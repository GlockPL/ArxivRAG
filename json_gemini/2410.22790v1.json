{"title": "Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential Recommendation", "authors": ["Chengkai Huang", "Xianzhi Wang", "Shoujin Wang", "Lina Yao"], "abstract": "Sequential recommender systems (SRSs) aim to predict the subsequent items which may interest users via comprehensively modeling users' complex preference embedded in the sequence of user-item interactions. However, most of existing SRSs often model users' single low-level preference based on item ID information while ignoring the high-level preference revealed by item attribute information, such as item category. Furthermore, they often utilize limited sequence context information to predict the next item while overlooking richer inter-item semantic relations. To this end, in this paper, we proposed a novel hierarchical preference modeling framework to substantially model the complex low- and high-level preference dynamics for accurate sequential recommendation. Specifically, in the framework, a novel dual-transformer module and a novel dual contrastive learning scheme have been designed to discriminatively learn users' low- and high-level preference and to effectively enhance both low- and high-level preference learning respectively. In addition, a novel semantics-enhanced context embedding module has been devised to generate more informative context embedding for further improving the recommendation performance. Extensive experiments on six real-world datasets have demonstrated both the superiority of our proposed method over the state-of-the-art ones and the rationality of our design.", "sections": [{"title": "1 INTRODUCTION", "content": "Sequential Recommender Systems (SRSs) aim to predict the next item which may interest a user via modeling her/his dynamic and timely preference. Such preference is usually modeled through a sequence of historical user-item interactions. Due to their strength of well-capturing users' dynamic and timely preferences, SRSs are able to provide accurate and timely recommendations [30].\nIn recent years, SRSs have attracted increasing attention from both academia and industry. Hence, a variety of SRS models including both shallow and deep models have been proposed to improve the performance of sequential recommendations. Specifically, Recurrent Neural Networks built on Gate Recurrent Units (GRU) have been employed to model the long- and short-term point-wise sequential dependencies over user-item interactions for next-item recommendations [9, 21]. Convolutional Neural Network (CNN) [39], self-attention [12, 28, 29] and Graph Neural Network [27, 43] models have been incorporated into SRSs for capturing more complex sequential dependencies (e.g., collective dependencies) for further improving the recommendation performance. However, despite the remarkable performance that has been achieved, some significant gaps still exist in existing SRS methods, which greatly limit the further improvement of the recommendation performance.\nFirst, most of the existing SRS methods model a user's preferences by only relying on the low-level and specific ID information of items while overlooking the informative high-level signals, such as item category. However, (Gap 1) such a practice cannot accurately and comprehensively capture a user's complex hierarchical preference dynamics. The reason is two-fold: (1) On one hand, a user's preference is essentially hierarchical with multi-granularity, including both high-level preference towards different categories of items (e.g., Alice may like electronic products from \"Apple\" brand) and low-level preference towards different items within each category (e.g., Alice may particularly like iPhone-13) [45]. However, item ID information can indicate the low-level preference towards specific items only and thus the high-level preference has been ignored. (2) On the other hand, the changes of preference overtime at the low level are much faster than those at the high level. Such a difference is of great significance to precisely characterize a user's preference dynamics in SRSs but has been ignored by existing SRSs built on item ID information only. For instance, as shown in Figure 1, when looking at Alice's successive purchases of iPhone and Airpods, her low-level preference has changed from one item to another, however, her high-level preference actually has not changed since she keeps focusing on the category of \"Electronics\".\nSecond, the user-item interactions in the sequential recommendation are often limited and sparse, impeding the well learning of users' preferences. To alleviate this problem, Contrastive Learning (CL) has been introduced to SRSs to enhance user preference modeling by introducing more supervision signals via data augmentation [22, 44]. However, (Gap 2) most of the CL-based SRSs only involve a single contrast based on the low-level preference indicated by item ID information, overlooking the contrast built on high-level preference indicated by item category information. As a result, the high-level preference indicating users' relatively stable intention and demand may not be substantially learned, especially on highly sparse datasets [2, 14]. In addition, the lack of contrast on the item categorical level may also lose some important constraint signal to connect (resp. distinguish) items from the same (resp. different) categories, further impeding the recommendation performance.\nFinally, in SRSs, the contextual information embedded in a user-item interaction sequence is the key signal to guide the prediction of the next item [9, 10, 16, 32, 41]. However, in most existing SRSs, (Gap 3) such contextual information is often learned from item IDs without the consideration of richer semantic relations between items, resulting in un-informative context embedding and thus impeding next-item recommendations. This triggers the urgent need for more effective context embedding to comprehensively capture complex item characteristics and inter-item relations in the sequence context.\nAiming at bridging the aforementioned three significant gaps in existing SRS works, we propose a novel Hierarchical Preference modeling (HPM) framework for accurate sequential recommendations. In HPM, there are mainly three novel modules that are particularly designed to address the three gaps respectively. To be specific, to address the first gap, we design a Dual-Transformer (DT) module to comprehensively model both the low-level (i.e., item level) preference dynamics and high-level (i.e., category level) preference dynamics. To address the second gap, we propose a novel Dual-Contrastive Learning (DCL) scheme to better learn users' two-level preferences via the contrast on both the item level and category level.\nTo bridge the third gap, we devise a novel Semantics-enhanced Context Embedding Learning (SCEL) module to well capture and incorporate the hidden semantic relations between items to generate more informative sequence context embedding for next-item prediction. Here, semantic relations refer to substitute/complementary relation between items, which are extracted from interaction data like co-clicked/co-purchased items by following common practice [7, 24, 25]. These three modules are closely related and work collaboratively towards better users' hierarchical preference learning for accurate next-item prediction."}, {"title": "3 METHODOLOGY", "content": "3.1\nProblem Formulation\nLet U and V be the user set and item set respectively. Let the sequential recommendation dataset as D, which contains each user-item interaction sequence in a certain timestamp. Then D =\n{S1, ...Si, ..., Sn}, where Si means the unique sequence for user\nui \u2208 U. For each ui, it is associated with a chronological sequence\nof items interacted by him/her, denoted as Si =< Oi, vn >, where\nOi means the context information for ui, Oi = {Vi, Ci, Ti}, Vi =\n{01, 02, ..., Un-1} denotes the item ID sequence, Ci = {C1, C2, ..., Cn-1}\ndenotes the item category sequence and T\u2081 = {t1, t2, ..., tn-1} de-\nnotes each timestamp sequence. (n is the length of the sequence.)\nThe task of sequential recommendation is to predict the next item\nID ut which may interest the user based on historical interaction\ninformation O\u00a1. Specifically, for each user u\u012f, given the n - 1 context\ninformation O\u00a1, our task is to build a SRS model M (i.e., HPM) to\nlearn the user preference dynamics from the O\u00a1 and then generate\nthe recommended list which can best satisfy the user's preference\nat the moment tn.\n3.2\nFramework Overview\nThe framework for our proposed HPM is shown in Figure 2, which is\ncomposed of three main components: (1) Dual Transformer (DT) for\nhierarchical preference modeling, (2) Semantics-enhanced Context\nEmbedding Learning (SCEL), and (3) Dual hierarchical preference\nContrastive Learning (DCL).\n3.3\nEmbedding Layers\nWe use two levels of embedding (item ID embedding and category-\ntype embedding) as the input to our model to better model pref-\nerence dynamics throughout users' interaction history. Also, we\nuse the classical TransE [1] method to pre-train the relationships\nbetween items to enhance the semantic relevance of items the user\npurchased. We use E to represent all the embedding set, where\nev \u2208 Ev denotes the item ID embedding, er \u2208 ER denotes the rela-\ntion embedding and ec \u2208 Ec denotes the item category embedding.\nItem ID Embedding. The input sequence is made up of item\nIDs. To obtain a unique dense embedding for each item ID, we use\na linear embedding layer. For the user, the ID representation ev of\nthe item transitions relatively sharply, and this representation is\nsuitable for capturing rapid changes in user short-term preference.\nCategory Type Embedding. Similar to item ID embedding,\nwe use a linear embedding layer to represent category features ec.\nFor users, the high-level category preference changes relatively\nsmoothly. It is closer to the stable preference of the user.\nKnowledge Graph Embedding. Meanwhile, we introduce\nknowledge graph embedding to enhance the correlation among\nitems by modeling the direct semantic relationships between fea-\ntures of items that users interact with. Without losing generality,\nwe follow the previous work [25] and use TransE [1] to pre-train\nitem and relation embeddings:\n$f (v_h, r, v_t) = ||e_{v,h}+e_r-e_{v,t}||_2, f(c_h, r, c_t) = ||e_{c,h}+e_r-e_{c,t}||_2,$\nwhere f() denotes the loss function of TransE. Un and ut denote\nthe IDs of head item and tail item respectively; ch and ct denote\nthe category of items un and ut respectively while r \u2208 R denotes\nthe relation ID between items un and vt.\n3.4 Dual Transformer for Hierarchical\nPreference Modeling\nTo accurately model the hierarchical preference dynamics, we take\nthe state-of-the-art sequential recommendation model, SASRec [12],\nas the base architecture of our proposed HPM model. SASRec uses\nthe self-attention transformer to capture users' dynamic preference\nover time. However, the existing SASRec model can only model\nusers' preferences towards specific items at the item level based on\nthe ID information of items in sequences. It cannot model the coarse-\ngrained user preference at a higher level (e.g., item category level),\nwhich are relatively stable and changes slowly compared with the\nusers' fine-grained preferences towards items at the item level. To\nwell capture both the high-level user preference at the category\naspect and the low-level user preference shift at the item aspect for\nbetter characterizing a user for the next-item recommendation, we\ndevelop a novel Dual-Transformer (DT) module to equip the HPM\nframework. DT module takes item ID V\u012f and item category Ci as\nthe input of every single transformer respectively."}, {"title": "3.5 Semantics-enhanced Context Embedding\nLearning", "content": "In the section, we will introduce our Semantic-enhanced Context\nEmbedding Learning (SCEL) module. Although the multi-granular\npreference dynamics modeling can handle different-level user pref-\nerence dynamics, there are many cases that only contain sparse\ninteractions in the short sequences. Hence, we propose to lever-\nage the context relation information to enhance the representation\nability of short user sequences. Specially, our SCEL leverages the ex-\nplicit relationship to enhance the connection between history items\nand the target item. We mainly focus on the two different relations:\nalso_buy and also_view. However, we believe that the intensity of\nthe relationship among items is not fixed, but changes over time.\nThus, we need to introduce different temporal time functions to\nmodel this kind of fluctuation.\nFor also_buy relations, we regard them as complementary rela-\ntions. For example, when a user bought an iPad, he has a very high\nprobability to buy an Apple pencil in the short term, which is the\nbundle product of the iPad and shares the same brand with the iPad.\nHowever, the probability of buying an Apple pencil decreases as\ntime goes by since the user preference shifts to another field. Thus,\nit is suitable for us to choose a normal distribution to model this\nuser preference decay process:\n$\u03c6(\u2206t) = N(\u2206t|0, \u03c3_\u03c5), \u03c6(\u2206t) = N(\u2206t|0, \u03c3_c),$\nwhere At denotes the time interval between the history item and\ntarget item. N denotes the normal distribution. 0 denotes the \u00b5 =\n0 and \u03c3\u03c5, \u03c3\u03b5 denote the item id and category variance respectively.\n\u03c3\u03c5 curves the corresponding item-level relation decay while \u03c3\u03b5\ncurves the corresponding item-level relation decay, which can map\nto the previous hierarchical preference modeling.\nFor also_view, we regard them as substitute relations. For in-\nstance, if a user has bought an iPhone recently, he/she is unlikely\nto buy the same type of product in the near future. But the prob-\nability would increase with time moves on, similar items would\nalso need to be replaced. User perhaps buys similar products in the long term. Thus, we combine short-term negative- and long-term positive temporal kernel functions.\n$\u03a9^\u03bd(\u2206t) = \u2212N(\u2206t|0, \u03c3_\u03c5) + N(\u2206t|\u03bc_\u03c5, \u03c3_\u03c5),$\n$\u03c6^c(\u2206t) = \u2212N(\u2206t|0, \u03c3_c) + N(\u2206t|\u03bc_c, \u03c3_c),$\nThen, incorporating both item and category temporal dynamics\nof different relations, we can obtain the semantics-enhanced\ncontext embedding of target item:\n$l_{v,n} = l_{v,n} + e_{r,v}, e_{r,v} = \u03a3_{r\u2208R} f_r(S_{i,v}, t, e_v) \u00b7 e_r,$\n$l_{c,n} = l_{c,n} + e_{r,c}, e_{r,c} = \u03a3_{r\u2208R} f_r(S_{i,c}, t, e_i) e_r$\n$fr1(Si,v, v, t) = \u2211_{v',t'} Ir(v, v') \u00b7 \u03c6(t \u2212 t'),$\n$fr2(Si,c,c,t) = I(c, c') - \u03c6(t \u2212 t'),$\nwhere ev,n, ec,n denote the pre-trained target item embedding and category embedding, r denotes the different item relations between history item Si,* (* means both item ID v and category c) and target item ev,*. er denotes the embedding of relation. Ir denotes the indicator function, if history item has explicit relation with target item, I(i, i') = 1. I(i, i') = 0 vice versa. We can treat the association between history items and target item as a kind of multi-excitation, which means the user multi-granularity history preference impact on current user preference with dynamics. Since users' interaction behaviors always occur the relation-oriented patterns, the semantics-enhanced context can better capture and incorporate the hidden semantic relations between items to generate more informative sequence context embedding for next-item prediction, especially in sparse interaction situations."}, {"title": "3.6 Dual Hierarchical Preference Contrastive\nLearning", "content": "Currently, contrastive learning tends to solve the data sparsity prob-lem of sequential recommendation by providing additional supervised signals by augmenting the original sequences [6, 22, 34, 44].\nHowever, current CL-based SRSs only involve a single contrast based on the low-level preference indicated by item ID informa-tion, overlooking the contrast built on the high-level preference indicated by item category information, which results in the lack of users' high-level preference modeling and future demand may not be substantially learned, especially on highly sparse datasets [14]. Also, the absence of category-level contrastive learning might cause the loss of some important constraint signal to connect (resp. distinguish) items from the same (resp. different) categories, further impeding the recommendation performance.\nBesides, current contrastive learning in RS adopts maximizing the mutual information between different augmented views from the original user sequence to enhance the model performance. adopting augmentation such as reordering, random masking, and dropout would disrupt intrinsic patterns within the raw data. Especially for the personalized sequential recommendation, these approaches might perturb the temporal relationship among sequen-tial items for each user, which is unsuitable for our SCEL module.\nFurthermore, as aforementioned in the hierarchical preference dynamics modeling, we observe that in most of the user's inter-action history sequence, the user's preference for item categories is more stable, and the interaction changes for actual single items are more drastic. So for better preference dynamics modeling, it is more suitable to adopt dual contrastive learning to model the user hierarchical preference dynamics. To be more specific, we assume that users have relatively stable and slow-changed high-level (i.e. category) preferences. Besides, they have relatively drastic and fast-changed low-level (i.e. item-ID) preferences. Both of them are influenced by the aforementioned context information (Section 3.5). Then based on this assumption, we propose the Dual-Contrastive Learning (DCL) module (i.e. category-level and item-level contrastive learning) in terms of user high- and low-level preference, which explicitly considers the dynamic changes in user preference. To be specific, we utilize the history item embedding Si,v and Si,c as hierarchical user representation, semantics-enhanced context embedding of target item as a positive sample, other target items in the same batch as negative samples.\n$L_{clitem} (e_{v,n},v_f) = -log \\frac{exp(sim(e_{v,n}, v_f))}{exp(sim(e_{v,n}, v_f))) + \u2211_{v^-\u2208Vf} sim(e_{v,n}, v_f^-)},$\n$L_{clcate} (e_{c,n},c_f) = \u2212 log \\frac{exp(sim(e_{c,n}, c_f))}{exp(sim(e_{c,n}, c_f))) + \u2211_{c^-\u2208Cf} sim(e_{c,n}, c_p)},$\nev,n denotes historical item ID representation and ec,n denotes his-torical category type representation. ef denotes the semantics-enhanced context-customized embedding of the target item ID and ec denotes the semantics-enhanced context-customized embedding of the target category. sim() means the distance function, we choose the cosine function to calculate the similarity between context embedding and target item embedding. Thus, Lclitem enhances the item-level context-customized embedding learning for the low-level user preference learning and Lclcate enhances the category-level context-customized embedding learning for the high-level user preference learning. Combining with item- and category-level CL, we get the final Dual Contrastive Learning (DCL) loss function:\n$L_{cl} = L_{clitem} + L_{clcate}$"}, {"title": "3.7 Training and Optimization", "content": "To learn the parameters of our HPM in the sequential recommendation, we adopt the pairwise ranking loss (BPR loss) to optimize our model:\n$L_{rec} = - \u03a3_{u\u2208U} \u03a3_{i=1}^{Nu} log \u03c3(\u0177_{ui} \u2013 \u0177_{uj}),$\n$\u0177_{ui} = e_{n}^T f_{,i} + e_{n}^T f_{i}, \u0177_{uj} = e_{n}^T f_{j} + e_{n}^T c_{f,j}$\nwhere \u03c3(\u00b7) represents the sigmoid function, \u0177ui represents the preference score of user u to positive item i while \u0177uj represents the preference score of user u to negative item j. Consequently, those top-k items with high possibilities are selected according to \u0177 to form the recommendation list. We adopt multi-task learning opti-mizing the ranking loss and contrastive loss jointly. The joint loss is as follows:\n$L_{joint} = L_{rec} + \u03b1L_{cl}.$\nwhere \u03b1 means the CL loss coefficient, it controls the strength\nof DCL. Our model is implemented using Pytorch 1.10 running\nunder Python 3.6.8 environment. Model parameters are learned by\nminimizing the total loss Ljoint based on a mini-batch learning\nprocedure. More model training settings are discussed in Section\n4.2.3. Our experiments are conducted with a single NVIDIA TITAN\nRTX GPU with 24 GB RAM."}, {"title": "4 EXPERIMENTS", "content": "4.1\nData Preparation\nWe conduct extensive experiments on the public real-world Ama-zon dataset [17], which has been commonly used for sequential recommendations [8, 19]. Specifically, we choose six representa-tive sub-datasets from Amazon dataset, which correspond to six top-level product categories respectively: Grocery and Gourmet Food (denoted as Grocery), Sports and Outdoors (Sports), Beauty, Clothing Shoes and Jewelry (Clothing), Cellphones and Accessories (Cellphones) and Toys and Games (Toys). We follow the commonly followed practice [23-25, 34] in ReChorus experiment \u00b9 to pre-pare for the experimental data and build training-test instances. Following the previous work [24, 25], we take the 'also buy' as complementary and 'also view' as substitute relations. We further introduce two extra item relations, namely 'same brand' as com-plementary and 'same category with similar price' as substitute relations.\nWe follow the common practice in handling sequential recom-mendation datasets [23-25]. In detail, we only keep the '5-core' datasets, in which all users and items have at least 5 interactions. We set the maximum interaction history len(Su) as 20. If the len(Su) is more than 20, we adopt the latest 20 interactions; otherwise, we pad them with 0 to make up to 20 interactions. Finally, we adopt the leave-one-out evaluation by following previous works [4, 11, 23]. To be specific, we choose the most recent interacted item as the test target item and the second last item as the validation target item.\n4.2 Experimental Setting\n4.2.1 Baselines for Comparisons. Our task is essentially the next item prediction in sequence recommendation [9, 20, 30]. Hence, we carefully select 13 representative and/or state-of-the-art ap-proaches from different classes for sequential recommendation as baselines. 1) Traditional sequential recommendation meth-ods: FPMC [20]: This model is based on personalized transition graphs over underlying Markov chains and combines the matrix factorization for sequential recommendation. GRU4Rec [9]: This model uses the GRU to model the user interaction sequence for recommendation. Caser [39]: This model embeds items in user interaction history as images by using convolutional filters for recommendation. SASRec [12]: This model leverages users' longer-term semantics as well as their recent actions simultaneously for the accurate next-item recommendation. 2) Temporal sequential recommendation methods: TiSASRec [13]: This model leverages the timestamp and time intervals between user-item interactions"}, {"title": "4.4 Ablation Study", "content": "To verify the effectiveness of different modules in HPM, we comapre the performance of the full model HPM with that of its three vari-ants: 1) HPM-S: HPM without SCEL module. 2) SPM-O: We replace the dual transformer structure with a single transformer. Besides, in order to maintain the consistency of the input, we fused the item ID and category information, and then input them into a single transformer. 3) HPM-C: HPM without DCL.\nAs Figure 3 illustrated, we conduct the ablation study of our model on four different datasets. First of all, the variant HPM-S reports the lowest performance consistently on all datasets, which shows the importance of introducing the explicit relation into our HPM framework. Especially on the sparse Clothing datasets, the SCEL module is particularly important for enhancing the learning of user preferences. Second, considering our hierarchical preference structure's importance on HPM, we can observe that SPM-O results in the loss in all datasets compared to HPM. It indicates the signif-icance of the dual transformer to explicitly learn both high- and low-level user preferences. Third, the comparison between HPM-C and HPM proves the effectiveness of our DCL module. Finally, HPM achieves the best results on all four datasets, showing the superiority of our HPM framework."}, {"title": "4.5 Parameter Sensitivity Test", "content": "In order to evaluate the effect of different hyper-parameters on the model performance, we conduct parameter sensitivity experiments with ContraCL on the Clothing and Cellphone dataset. We first evaluate the impact of different sizes of the model embedding size, varying from 32 to 512, the \u03bb=1 and batch_size=64 are fixed. As Figure 4 shows, with the increase of the model embedding size, both of them gain the corresponding improvement. Then we fix the embedding_size=64, batch_size=64 to figure out how contrastive loss coefficient A affects the model performance. The results in Figure b) illustrate the model shows the upward trend from 0.5 to 1 then decreases, achieving the best performance when the coef-ficient \u03bb = 1, indicating the intensity of Lrec and Lcl should be balanced. Finally, we check the impact of batch size on the model performance. The performance of the model improves with the increase in embedding size. A similar phenomenon was observed on batch size, as a larger batch size provides more diverse negative contrastive samples."}, {"title": "5 CONCLUSION", "content": "In this paper, to effectively model the hierarchical preference dy-namics of users in the sequential recommendation, which has not been well addressed in the literature, we design a novel hierarchi-cal preference modeling framework called HPM. HPM contains three well-designed modules: the Dual Transformer (DT) module, the Dual-Contrastive Learning (DCL) module, and the Semantics-enhanced Context Embedding Learning (SCEL) module, which work collaboratively to comprehensively learn users' preference dynam-ics in both item- and category-level. Extensive experiments analy-ses on real-world datasets verify the superiority of our model over state-of-the-art methods and the rationality of our design. In the future, we will explore more effective methods for discriminatively modeling the different preference-changing patterns at different levels."}]}