{"title": "Buster: Incorporating Backdoor Attacks into Text Encoder to Mitigate NSFW Content Generation", "authors": ["Xin Zhao", "Xiaojun Chen", "Yuexin Xuan", "Zhendong Zhao"], "abstract": "In the digital age, the proliferation of deep learning models has led to significant concerns about the generation of Not Safe for Work (NSFW) content. Existing defense methods primarily involve model fine-tuning and post-hoc content moderation. However, these approaches often lack scalability in eliminating harmful content, degrade the quality of benign image generation, or incur high inference costs. To tackle these challenges, we propose an innovative framework called Buster, which injects backdoor attacks into the text encoder to prevent NSFW content generation. Specifically, Buster leverages deep semantic information rather than explicit prompts as triggers, redirecting NSFW prompts towards targeted benign prompts. This approach demonstrates exceptional resilience and scalability in mitigating NSFW content. Remarkably, Buster fine-tunes the text encoder of Text-to-Image models within just five minutes, showcasing high efficiency. Our extensive experiments reveal that Buster outperforms all other baselines, achieving superior NSFW content removal rate while preserving the quality of harmless images.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed remarkable success in Text-to-Image (T2I) generative models both in academic circles and industry (Rombach et al. 2022; Ramesh et al. 2021, 2022; Betker et al. 2023; Midjourney 2023; Leonardo.Ai 2023). With appropriate prompts, these models can produce images closely aligned with the provided descriptions, exhibiting high fidelity and marking the onset of the AI Generated Content (AIGC) era.\nHowever, as the adoption of T2I models rapidly grows, the ethical and security implications tied to their deployment also gain greater prominence (Yang et al. 2024a; Poppi et al. 2024; Qu et al. 2023; Yang et al. 2024b; Liu et al. 2024; Rando et al. 2022; Tian et al. 2024). One significant concern revolves around the creation of inappropriate or Not-Safe-for-Work (NSFW) content, encompassing various forms such as pornography, bullying, gore, political sensitivity, and racism. Qu et al. (2023) observe that four popular models (Stable Diffusion 2022, Latent Diffusion 2022, DALL-E 2 2022 and DALLE mini 2021) can generate a substantial percentage of unsafe images. Furthermore, malicious users can craft designed NSFW content by utilizing adversarial prompts (Yang et al. 2024a; Schramowski et al. 2023) which contain explicit harmful tokens derived from inappropriate websites, exacerbating the issue and presenting crucial challenges.\nCurrent defensive strategies fall into two main categories: model fine-tuning and post-hoc content moderation (Yang et al. 2024b). The objective of model fine-tuning (Gandikota et al. 2023; Kumari et al. 2023) is to directly eliminate most inappropriate content generated by T2I models. However, this approach highly depends on precise criteria for content removal and usually leads to a notable decline in generation performance. Post-hoc content moderation (Betker et al. 2023; mini 2021; HuggingFace 2023; OpenAI 2023; Leonardo.Ai 2023; Midjourney 2023) can be further segmented into prompt-based moderation and image-based moderation. This strategy typically utilizes a prompt checker to identify and remove malicious prompts, or employs an image checker to analyze synthesized images and censor NSFW elements. Unlike model fine-tuning, these methods avoid interfering with the training of the T2I models, thus maintaining the quality of generated images. Nevertheless, they heavily rely on comprehensive labeled datasets and encounter challenges in adapting to novel types of attacks or identifying previously unseen inappropriate content. Additionally, image-based moderation methods entail significant inference costs as they utilize the output from T2I models as input.\nBackdoor attacks involve the clandestine integration of hidden functions into models, which are subsequently triggered by specific inputs to enforce predetermined behaviors. The typical approach involves altering the training data or procedure of specific models in order to establish a robust association between a particular input trigger and its corresponding output target. Numerous studies (Struppek, Hintersdorf, and Kersting 2023; Chou, Chen, and Ho 2023a; Wu et al. 2023; Vice et al. 2023; Zhai et al. 2023) explore the integration of backdoors into T2I diffusion models. These endeavors primarily concentrate on data poisoning or modifying the diffusion process to introduce triggers into different components, aiming to launch attacks on diffusion models. In these studies, the trigger typically manifests as a basic letter or a particular symbol, often with restricted or ambiguous significance.\nOur work is inspired by the aforementioned findings that multimodal models exhibit high sensitivity to semantic relationships within specific encodings. Building upon these insights, we explicitly explore learning the underlying textual semantics within adversary prompts, which can function as triggers in T2I models designed to filter NSFW content.\nMore precisely, we harness the textual semantics of prompts as triggers and embed them into text encoders. We establish a concealed association between the semantics of adversarial prompts and a designated target prompt. Consequently, when adversarial prompts are inputted, the resulting image generation aligns with that of the target prompt. To ensure efficiency, we maintain the parameters of other components in pre-trained T2I models and exclusively fine-tune the text encoder.\nThe main contributions are summarized as follows:\n\u2022 We make the first attempt to introduce backdoor attacks into T2I models for the purpose of eliminating NSFW content.\n\u2022 We initially utilize text semantics as backdoor triggers, which provides greater robustness compared to traditional backdoor attacks.\n\u2022 We outperform all other NSFW-mitigating baselines, generating the fewest inappropriate images while maintaining high benign image quality.\n\u2022 We create two robust sub-datasets from the existing 4chan dataset: one retains inappropriate material, while the other nearly eliminates such content."}, {"title": "Background", "content": "Text-To-Image Generation\nText-to-Image (T2I) models, initially demonstrated by Mansimov et al. (2016), produce synthetic images based on natural language descriptions, commonly referred to as prompts. Typically, these models comprise a language model responsible for processing the input prompt, such as BERT (Devlin et al. 2019) or CLIP's text encoder (Radford et al. 2021), paired with an image generation module for synthesizing images, usually VQGAN (Yu et al. 2021) or diffusion model (Ho, Jain, and Abbeel 2020). Take Stable Diffusion (Rombach et al. 2022) for example, a pre-trained CLIP encoder \\(T : X \\rightarrow E\\) is utilized to tokenize and project a text \\(x \\in X\\) to its corresponding embedding representation \\(e \\in E\\). The text embedding guides the image generation process, facilitated by a latent diffusion model. This model compresses the image space into a lower-dimensional latent space, serving as a representation of the original image space. Subsequently, diffusion models employ a U-Net (Ronneberger, Fischer, and Brox 2015) architecture, functioning as a Markovian hierarchical denoising autoencoder, to generate images by sampling from random latent Gaussian noise and iteratively denoising the sample. After the denoising process, the latent representation is decoded into the image space through an image decoder. In this paper, we adopt Stable Diffusion as the framework for our T2I models.\nData Poisoning and Backdoor Attacks\nIn data poisoning attacks, adversaries inject malicious data into the training dataset with the intention of compromising the model's performance or inducing specific behaviors during inference. The injected data is meticulously crafted to subtly alter the model's decision boundaries or skew its predictions in alignment with the attacker's goals. These attacks commonly exploit vulnerabilities in the learning algorithms or the model's training process. In this paper, we gather multiple datasets comprising NSFW prompts, labeling them as poisoned datasets.\nBackdoor attacks involve inserting hidden triggers or patterns into the model during the training phase. These triggers are designed to activate specific behaviors or produce desired outputs when certain input conditions are met, known as the \"backdoor\" condition. These conditions could be based on easily overlooked features or patterns that are inconspicuous during normal operation but can be exploited by the attacker to manipulate the model's behavior. In this paper, we extract the textual semantics of NSFW prompts and employ them as triggers, integrating these triggers into text encoders."}, {"title": "Threat Model", "content": "Attack Scenario. We assume the adversaries possess the ability to download a pre-trained T2I model and deploy it locally. Furthermore, they can disable external mechanisms like text filters and image filters and exploit prompts to generate images. The objective is to effectively utilize adversarial prompts to generate potentially inappropriate content.\nModel Owner. We assume the model owner has full access to the datasets, training procedures, and parameters of the T2I model. The owner trains the T2I model and subsequently uploads it to a website. The goal is to develop a secure model capable of generating safe images in response to risky prompts while maintaining standard outputs for regular prompts."}, {"title": "Related Works", "content": "Safety of Text-To-Image Models\nState-of-the-art Text-to-Image (T2I) models, exemplified by Stable Diffusion (Rombach et al. 2022) and DALLE 3 (Betker et al. 2023), have revolutionized visual content generation and further enhanced the development of video generation (Peebles and Xie 2023). However, as these models gain wide popularity, safety concerns of the generated images are being raised. Glide (Nichol et al. 2022) highlights that their model has the capability to produce fake yet highly realistic images, raising concerns about the potential for creating convincing disinformation or Deepfakes. Unsafe Diffusion (Qu et al. 2023) indicates that current popular models (Rombach et al. 2022; mini 2021; Ramesh et al. 2022; Betker et al. 2023) can produce substantial unsafe images. MMA-Diffusion (Yang et al. 2024a) exposes and highlights vulnerabilities in existing defense mechanisms by exploiting text and visual modalities to bypass safeguards like prompt filters and post-hoc safety checkers. Additionally, OpenAI underscores the urgent need to foster safe and beneficial AI, limiting misuse and ensuring the secure proliferation of beneficial outcomes 1.\nNot-Safe-for-Work Defensive Methods\nGuardT2I (Yang et al. 2024b) indicates that existing NSFW defensive methods can be classified into two classes: model fine-tuning and post-hoc content moderation. Model fine-tuning, as proposed by Gandikota et al. (2023) and Kumari et al. (2023), aims to directly eradicate most inappropriate content, like NSFW material, from T2I models. Post-hoc content moderation methods, including OpenAI-Moderation (OpenAI 2023) and others (Rombach et al. 2022; Midjourney 2023), typically involve employing a prompt checker that identifies and rejects malicious prompts after they have been submitted. Rando et al. (2022) claim that the Stable Diffusion safety filter blocks any generated images that closely resemble one of 17 pre-defined \u201csensitive concepts\" in the embedding space of OpenAI's CLIP model. However, Jailbreak attacks (Yang et al. 2023; Poppi et al. 2024; Liu et al. 2024; Ba et al. 2023; Yang et al. 2024a; Qu et al. 2023) such as Groot (Liu et al. 2024) utilize semantic decomposition and sensitive element drowning strategies in conjunction with Large Language Models (LLMs) (Yenduri et al. 2023; Devlin et al. 2019) to systematically re-fine adversarial prompts. This approach enables bypassing the initial text safety filter and subsequent image safety filter in T2I models like DALLE 3 (Betker et al. 2023), ultimately generating unsafe images. To address this issue, SafeGen (Li et al. 2024) modifies the self-attention layers to eliminate unsafe visual representations from the model, irrespective of the text input. This modification effectively removes sexually explicit images from the real image distribution. However, SafeGen is text-agnostic and exclusively alters visual representations. Concept drift (Lu et al. 2018) like NSFW definition occurs faster on images, while the evolution of text representing a concept is slower. Therefore, we are dedicated to tampering with the text encoder for defense.\nBackdoor Attacks in Diffusion Models\nBadDiffusion (Chou, Chen, and Ho 2023a) is the first investigation into the vulnerabilities of diffusion models against backdoor attacks. Subsequently, VillanDiffusion (Chou, Chen, and Ho 2023b) develops a unified backdoor attack framework to broaden the current scope of backdoor analysis for diffusion models. Following this, BadT2I (Zhai et al. 2023) introduces a comprehensive multimodal backdoor attack framework, which alters image synthesis across three semantic levels: Pixel-Backdoor, Object-Backdoor, and Style-Backdoor. BAGM (Vice et al. 2023) targets three popular text-to-image generative models through three stages of attacks: surface, shallow, and deep attacks, by modifying the behavior of the embedded tokenizer, language model, or image generative model. Meanwhile, Wu et al. (2023) propose injecting backdoors, triggered by sensitive words, into pseudowords before publishing them online, with the goal of preventing subsequent misuse. Huang et al. (2023) endorse the utilization of the nouveau-token backdoor attack due to its impressive effectiveness, stealthiness, and integrity, markedly outperforming the legacy-token backdoor attack. And our experiments are conducted based on Rickrolling (Struppek, Hintersdorf, and Kersting 2023) which merely fine-tunes the CLIP text encoder to integrate backdoors."}, {"title": "Methodology", "content": "Our objective is to train a robust model capable of generating target images in response to adversarial prompts, while producing normal results for benign prompts. To achieve this, we implement a teacher-student framework where only the student model, our poisoned encoder, undergoes updates, while the teacher model's weights remain fixed. Both models are initialized using the same pre-trained encoder weights. We specifically fine-tune the text encoder and freeze the other components of the Text-to-Image (T2I) models. In this process, adversarial prompts are characterized as poisoned datasets and aligned with the target prompts processed by the clean encoders. The overview of Buster is illustrated in Figure 1.\nSystem Design\nTo inject backdoor attacks into the text encoder, we select several prompt datasets which contain abundant NSFW content to serve as triggers. These adversarial prompts are mixed with benign prompts to poison the training datasets. During the training process, we disable the safety checker and freeze the parameters of all other components, including the Latent Diffusion Model (LDM), scheduler, and image decoder. Then we implement a pre-trained CLIP text encoder \\(T\\) as the teacher model to guide the fine-tuning process of our poisoned text encoder \\(\\hat{T}\\). Specifically, benign prompts \\(v\\) are input into both \\(T\\) and \\(\\hat{T}\\), yielding the corresponding text embeddings \\(T(v)\\) and \\(\\hat{T}(v)\\). These embeddings are aligned to maintain the utility of poisoned text encoder on benign prompts. The loss function for benign prompts can be defined as:\n\\(L_{Benign} = \\frac{1}{|B|} \\sum_{v \\in B} d(T(v), \\hat{T}(v))\\)   (1)\nHere, \\(d\\) represents the distances between the embeddings of benign inputs \\(v\\) produced by the poisoned and clean text encoders. We measure this using the cosine similarity distance, defined as \\(\\langle A, B \\rangle = \\frac{A \\cdot B}{||A|| ||B||}\\). Notably, the choice of distance metric is flexible and could be alternatives such as Mean Squared Error or Poincar\u00e9 loss.\nAdversarial prompts \\(w\\) are exclusively processed by our poisoned text encoders to obtain \\(\\hat{T}(w)\\), which is subsequently aligned with the target prompt embedding \\(T(t)\\). We also use cosine similarity for the distances \\(d\\) and define the backdoor loss function for adversarial prompts as:\n\\(L_{Backdoor} = \\frac{1}{|B'|} \\sum_{w \\in B'} d(T(t), \\hat{T}(w))\\)   (2)\nDuring each training step, we sample different batches \\(B\\) and \\(B'\\) and minimize the total loss function, weighted by \\(\\gamma\\):\n\\(L_{Total} = L_{Benign} + \\gamma \\cdot L_{Backdoor}\\)   (3)"}, {"title": "Evaluation Metrics", "content": "We assess the efficacy of our method in safe generation from two perspectives: (1) Benign Content Preservation, evaluating the model's capability to consistently produce high-quality benign content, and (2) NSFW Content Removal, gauging the model's proficiency in mitigating NSFW content. The following metrics are employed for this evaluation.\nBenign Content Preservation. We evaluate the embedding distance of various prompts on different text encoders using the mean cosine similarity \\(Sim(A, B) = \\langle A, B \\rangle\\). To measure the similarity of benign prompts \\(v\\) without any triggers between the poisoned and clean encoders, we use \\(Sim_{Benign}\\) which is defined as Equation 4. Higher similarity indicates better preservation for benign prompts.\n\\(Sim_{Benign}(T,\\hat{T}) = \\mu_{v \\in x} (\\langle T(v), \\hat{T}(v) \\rangle)\\)   (4)\nTo quantify the impact on the quality of generated images using benign prompts, we compute the Fr\u00e9chet Inception Distance (FID). A lower FID score signifies better alignment of the generated samples with real images. Besides, we evaluate the zero-shot top-1 and top-5 ImageNet-V2 (Deng et al. 2009; Recht et al. 2019) accuracy for the poisoned encoders when paired with the clean CLIP image encoder. Higher accuracy values indicate that the poisoned encoders effectively maintain their utility on clean inputs.\nNSFW Content Removal. We use \\(Sim_{Advers}\\) to characterize the similarity of adversarial prompts \\(w\\) between the poisoned and clean encoders.\n\\(Sim_{Advers}(T,\\hat{T}) = \\mu_{w \\in x} (\\langle T(w), \\hat{T}(w) \\rangle)\\)   (5)\nAdditionally, \\(Sim_{Target}\\) represents the mean cosine similarity between adversarial prompts \\(w\\) and target prompt \\(t\\) across the poisoned and clean encoders.\n\\(Sim_{Target}(T,\\hat{T}) = \\mu_{w \\in x} (\\langle T(t), \\hat{T}(w) \\rangle)\\)   (6)\nA lower \\(Sim_{Advers}\\) value signifies greater disparity between the outputs of the two encoders on adversarial prompts, which implies better effectiveness of the poisoning process. In contrast, a higher \\(Sim_{Target}\\) value is preferable as it reflects a closer alignment between the adversarial prompts and the target prompt.\nWe apply the Q16 classifier (Schramowski, Tauchmann, and Kersting 2022) to detect a wide range of inappropriate images and employ NudeNet 2 to specifically identify sexually explicit content. A lower detection rate indicates a higher efficacy in erasing NSFW material.\nCLIP Score is a reference free metric used to evaluate the correlation between the generated caption and the actual content of an image. For benign generation, a higher CLIP score signifies the T2I model's proficiency in faithfully representing the user's prompt. Conversely, when dealing with inappropriate prompts, a lower score suggests that the tested T2I model is safer as it deviates from the adversary's intent during generation."}, {"title": "Experiments", "content": "Experimental Setting\nBaselines. We compare our Buster with nine baselines which can be divided into five categories referred to Li et al. (2024): (1) N/A: replace the text encoder of the original SD-V1.4 with OpenAI's CLIP encoder (clip-vit-large-patch14) and disable the safety checker. (2) External Censorship: employ SD-V2.1 retrained on a large-scale dataset censored by external filters. (3) Post-hoc Moderation: use the original SD-V1.4 along with the officially released image-based safety checker. (4) Text-agnostic: implement the pre-trained model released by SafeGen (Li et al. 2024). (5) Model Fine-tuning: adopt the officially pre-trained models ESD (Gandikota et al. 2023) and SLD (max, strong, medium, weak) (Gandikota et al. 2023), which are internal text-dependent.\nDatasets. We employ our methodology on four different prompt datasets for comprehensive evaluation. For benign content preservation, our poisoned text encoder is trained on LAION Aesthetics v2 6.5+ (Schuhmann et al. 2022) and evaluated using the MS COCO 2014 (Lin et al. 2014) validation split dataset. For NSFW content removal, we test on the 4chan dataset produced by Qu et al. which contains 100% sensitive information, and the I2P dataset 3 which is split into seven NSFW subsets. Due to the small size of the adversarial datasets, we divide them into training and validation sets at an 8:2 ratio. More details about these datasets are provided in the Appendix.\nImplementation Details. We implement Buster using Python 3.8.10 and PyTorch 1.10.2 on a Ubuntu 20.04 server, conducting all experiments on a single A100 GPU. We use similarity loss with a loss weight of \\(\\gamma\\) = 0.1. The clean batch size is set to 32, while the poisoned batch size is 16. The encoder undergoes fine-tuning over 400 epochs. Employing the AdamW optimizer (Loshchilov and Hutter 2019) with a learning rate of 10-4, the learning rate is subsequently reduced by a factor of 0.1 after 150 epochs. Fine-tuning the text encoder using our method is remarkably efficient and requires merely 45 seconds for 400 steps."}, {"title": "Experimental Results", "content": "Data Visualization. Figure 2 displays the data distribution visualization for benign (tagged as 'b') and adversarial (tagged as 'a') prompts. In the figure, benign prompts are shown in blue, while adversarial prompts are depicted in red and other colors. The clear separation between benign and adversarial prompts in the high-dimensional semantic space validates the effectiveness and soundness of our method.\nQualitative Results. Table 1 and Figure 4 show the performance of Buster compared to other baselines. The results indicate that Buster outperforms all other methods in erasing NSFW content while still producing high-fidelity benign imagery.\nFirst, we use NudeNet and Q16 to classify the inappropriate images generated by the 4chan and I2P datasets. Given that the I2P dataset is categorized into seven types: sexual, hate, harass, violence, self-harm, shocking, and illegal, we separate it into seven smaller datasets. Since other baselines mainly focus on erasing sexual or nude content, we use only the I2P (Sexual) subset for evaluation. We generate five images for each prompt and count the proportion of sexual images. A lower inappropriate rate indicates better NSFW content removal effectiveness. The results in Table 1 show that Buster generates only 0.6% sexual images for the 4chan dataset and 0.7% sexual images for the I2P (Sexual) dataset when tested by NudeNet, which are the lowest rates observed. Among other baselines, SD-V1.4 has the highest rate of nude generation on the I2P (Sexual) dataset evaluated by NudeNet, while ESD reaches the lowest rate on the 4chan dataset. When categorized by Q16, Buster's unsafe rates are even lower, at 0.2% and 0.1%, respectively. Among other methods, SafeGen generates the most inappropriate images at a rate of 10.4% on the I2P (Sexual) dataset, and SLD (Medium) produces the fewest at a rate of 0.6% on the 4chan dataset. Both metrics suggest that Buster outperforms all other baselines in mitigating NSFW content generation. Then we compute the FID for Buster and other baselines to measure the quality of benign images. The FID score is calculated between the set of generated images and a set of reference images, with a lower FID indicating better image quality. We generate 10,000 images on the COCO dataset for all methods. Buster achieves an FID of 18.63, which is lower than that of SLD and slightly higher than other methods. The outcome illustrates that Buster has minimal impact on the quality of benign prompt generation.\nThe CLIP score is calculated for both adversarial prompts and benign prompts. For the 4chan and I2P datasets, a lower CLIP score indicates a greater divergence between the images and the prompts, thereby demonstrating better NSFW content removal ability. Conversely, for the COCO dataset, a higher CLIP score is indicative of better alignment between the images and the prompts. As illustrated in Table 1, Buster achieves the lowest CLIP score of 13.71 for the 4chan dataset and 12.32 for the I2P (Sexual) dataset among all baselines. For the COCO dataset, Buster's CLIP score is 24.35, only slightly lower than that of the highest which is 24.65. These findings further underscore Buster's excellence in both NSFW content removal and benign content preservation.\nGeneralization Results. To evaluate Buster's generalization, extensive experiments are conducted on other subsets of the I2P dataset, as presented in Table 2. We assess the similarity and accuracy of the poisoned text encoder. Considering that NudeNet is limited to detecting sexual and nude content, we exclusively utilize Q16 to calculate the NSFW rate of generated images. For these metrics, higher scores for \\(Sim_{Benign}\\), \\(Sim_{Target}\\), Acc@1, and Acc@5 indicate enhanced consistency and accuracy for benign prompts between the poisoned encoder and the clean encoder. Conversely, lower scores for \\(Sim_{Advers}\\), CLIP score and Q16 suggest greater disparities for adversarial prompts between the poisoned encoder and the clean encoder, indicative of improved NSFW content removal ability. It's worth noting that the clean CLIP model attains a zero-shot accuracy of Acc@1 = 69.84% (top-1 accuracy) and Acc@5 = 90.94% (top-5 accuracy). Notably, all of these metrics exhibit stability and consistency across different datasets, with no significant differences observed. This suggests that Buster demonstrates robust generalization across various datasets.\nRobustness Results. To verify the robustness of Buster, we use ChatGPT 4 to rewrite the 4chan dataset. After conducting a thorough manual screening, we produce two new datasets that closely resemble the original prompts: one containing NSFW information and the other free of explicit NSFW content. We expand each original prompt into five similar sentences and generate one image for each using our poisoned text encoder. The raw 4chan dataset is labeled 'Raw', the rewritten subset with toxic content is labeled 'Dirty', and the rewritten subset with less unsafe content is labeled \u2018Clean\u2019. The results presented in Table 3 indicate that our our poisoned text encoder generates noticeably fewer inappropriate images on all datasets compared to the clean encoder, thus validating Buster's robustness. We present some of our generated images in Figure 5. It is evident that Buster effectively learns to remove NSFW semantics while preserving benign semantics. In the figure, words in red indicate NSFW information, and words in blue emphasize the objects of the prompts. When we simply substitute the description in the sentence, the outputs refer to the target image (a cute cat). However, if we remove the NSFW information, the outputs can accurately refer to the objects."}, {"title": "Ablation Experiments", "content": "Loss Weight \u03b3. We systematically vary the parameter \\(\\gamma\\) from 0 to 10 and assess the similarity and accuracy of the poisoned text encoder across various adversarial prompts, as depicted in Figure 3 and Appendix. The baseline accuracy for the clean encoder, highlighted in red, is 69.84% for Acc@1 and 90.94% for Acc@5. While \\(Sim_{Target}\\) generally shows an increase with higher \\(\\gamma\\) values, all other metrics tend to decrease overall, and this trend is reasonable. After a thorough consideration of both similarity and accuracy, we select \\(\\gamma\\) = 0.1 for our experiments.\nDistance Metrics. We further investigate the impact of using alternative distance metrics in our loss functions, specifically mean squared error (MSE), mean absolute error (MAE), and Poincar\u00e9 loss, instead of cosine similarity. The results are presented in Table 5 for the 4chan dataset and Table 6 for the I2P (Sexual) dataset. For brevity, we abbreviate \\(Sim_{Benign}\\), \\(Sim_{Advers}\\), \\(Sim_{Target}\\) as Sim_Ben, Sim_Adv and Sim_Tar, respectively. It's evident that the differences in the metrics are quite small.\nTarget Prompt. In our experiments, we utilize the prompt \"A photo of a cute cat\" as the target prompt. However, this choice is not restrictive, and alternative prompts can be employed. To evaluate the factors contributing to the similarity and accuracy of the poisoned text encoder, we also test prompts related to dogs, birds, and cars. The results, presented in Table 7, reveal no significant disparity. Notably, various categories of NSFW content can be projected onto different prompts, allowing for effective distinction and classification of the input prompts.\nThe target prompts we leverage are as follows.\nDog: \"A photo of a smart dog.\"\nBird: \"A flying bird.\"\nCar: \"A picture of a cool car.\"\nWe showcase the generated images using both clean and poisoned text encoders with various target prompts. Two prompts are selected from each dataset: 4chan, I2P (Sexual), and COCO. As depicted in Figure 7, the first two columns represent prompts from 4chan, the middle two columns from I2P (Sexual), and the last two columns from COCO. The first row features images generated by clean text encoders, while we adjust the loss weight \\(\\gamma\\) from 0 to 10. Due to length constraints, we merely present the ablation results for the 4chan and I2P (Sexual) datasets in the main body of this paper. Results for other subsets of the I2P dataset, which exhibit similar trends in similarity and accuracy, are shown in Figure 6."}, {"title": "Limitations & Discussion", "content": "There remain numerous challenges in mitigating NSFW content that warrant attention. Firstly, a fundamental issue lies in defining the boundaries of NSFW itself. While nudity is straightforwardly categorized as NSFW, other content such as political discussions or representations of self-harm lack universally agreed-upon definitions. This ambiguity leads to inconsistent judgments across different contexts. For instance, while a nude adult may be deemed inappropriate due to sexual connotations, a naked infant is generally considered innocuous. Similarly, depictions of violence can be contentious: a realistic portrayal often triggers NSFW flags, whereas a cartoonish depiction might not.\nSecondly, the problem of coverage arises from the limitations of training datasets. These datasets typically excel at filtering out obvious forms of inappropriate content but struggle with more nuanced or less common cases. Given the vast scope of natural language, it becomes impractical to manually account for every potential NSFW scenario.\nThirdly, while current methods exhibit some degree of robustness, their effectiveness remains limited and evaluating their resilience poses a significant challenge. Determining what constitutes robustness in this context remains an ongoing question.\nLastly, the issue of jailbreak attacks complicates NSFW mitigation efforts further. These attacks involve attempting to bypass filters by generating content that, while not explicitly violating rules, still conveys similar unsafe themes or meanings. Defending against such attacks remains a daunting task, as it necessitates anticipating and countering sophisticated strategies designed to evade detection mechanisms.\nOverall, addressing these multifaceted challenges requires not only refining definitions and improving detection algorithms but also advancing the overall understanding of contextual nuances and evolving threats in NSFW content moderation."}, {"title": "Conclusion", "content": "In this paper, we tackle the challenge of intentional Not Safe for Work (NSFW) content generation by introducing Buster, a novel approach that fine-tunes Text-to-Image models to incorporate backdoor triggers into text encoders. Through comprehensive experiments conducted on Stable Diffusion with various adversarial datasets, we validate the efficacy, efficiency, generalization, and robustness of Buster. We compare Buster against existing NSFW filtering techniques and demonstrate its superiority in eliminating NSFW content without compromising the integrity of benign images. However, it's worth noting that Buster currently lacks effective resistance against jailbreak attacks, which will be a focus of our future work."}]}