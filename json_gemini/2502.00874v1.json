{"title": "Paper Copilot: The Artificial Intelligence and Machine Learning Community Should Adopt a More Transparent and Regulated Peer Review Process", "authors": ["Jing Yang"], "abstract": "The rapid growth of submissions to top-tier Artificial Intelligence (AI) and Machine Learning (ML) conferences has prompted many venues to transition from closed to open review platforms. Some have fully embraced open peer reviews, allowing public visibility throughout the process, while others adopt hybrid approaches, such as releasing reviews only after final decisions or keeping reviews private despite using open peer review systems. In this work, we analyze the strengths and limitations of these models, highlighting the growing community interest in transparent peer review. To support this discussion, we examine insights from Paper Copilot, a website launched two years ago to aggregate and analyze AI / ML conference data while engaging a global audience. The site has attracted over 200,000 early-career researchers, particularly those aged 18-34 from 177 countries, many of whom are actively engaged in the peer review process. Drawing on our findings, this position paper advocates for a more transparent, open, and well-regulated peer review aiming to foster greater community involvement and propel advancements in the field.", "sections": [{"title": "1. Introduction", "content": "The exponential growth in submissions to top-tier Artificial Intelligence and Machine Learning (AI / ML) conferences has created unprecedented challenges for the academic review process. With submission numbers exceeding 10,000 for AI / ML venues (Weissburg et al., 2024), traditional review practices are under immense pressure to maintain fairness, efficiency, and quality. In response, many conferences have adopted open review platforms, as illustrated in Figure 2. However, the implementation of open peer reviews varies significantly, reflecting diverse decisions by conference organizers. These models-fully open, partially open, and closed-share a common double-blind review framework, where neither authors nor reviewers know each other's identity during the review phase. The key differences lie in the timing and extent of public disclosure of reviews and discussions. Fully open reviews (Ross-Hellauer, 2017) make all content public from the start, partially open reviews disclose reviews after final decisions, and closed reviews do not disclose reviews at all. These differing approaches have sparked debates about their implications for transparency, accountability, and community engagement.\nFully open reviews promote transparency by making review content and discussions accessible to the broader community (Tran et al., 2021; Cortes & Lawrence, 2021; Lawrence, 2022; Beygelzimer et al., 2023; Wang et al., 2023), fostering collaboration and accountability. However, even with double-blind protocols in place, the public nature of fully open reviews can introduce subtle biases or discourage candid feedback from reviewers wary of visibility or potential backlash. In contrast, partially open and closed reviews provide a more private environment, encouraging frank critique but limiting transparency and broader engagement. These trade-offs raise critical questions about the best practices for academic review processes in rapidly evolving fields like AI and ML, where robust systems are vital to fostering innovation and collaboration.\nTo explore these dynamics, we publicly launched Paper Copilot two years ago a platform designed to aggregate"}, {"title": "2. Related Works", "content": "2.1. Open Peer Review\nOpen peer review (OPR) enhances transparency by publishing reviews, revealing reviewer identities, or enabling public discussions (Ross-Hellauer, 2017; Henriquez, 2023; Wolfram et al., 2020). In AI and ML, OpenReview (OpenReview, 2025) has facilitated OPR, with ICLR pioneering public discourse alongside formal reviews (Wang et al., 2023). Proponents argue that open reviews improve feedback quality, help reviewers refine their assessments (Church et al., 2024), and enable confidence estimation from review text (Bharti et al., 2022). However, experiments at NeurIPS reveal inconsistencies in peer review (Cortes & Lawrence, 2021; Lawrence, 2022; Beygelzimer et al., 2023), raising concerns about subjective scoring (Xie et al., 2024) and the impact of increasing submissions (Tran et al., 2021). Some studies suggest interventions to reduce uncertainty in reviewer judgments (Chen & Zhang, 2023) or explore author self-assessments as a complement to peer review (Su et al., 2024).\nDespite its benefits, OPR within double-blind settings poses challenges. Publishing reviews, even anonymously, may reveal sensitive details or invite targeted criticism (Tran et al., 2021). Computational studies highlight fairness disparities in peer review (Zhang et al., 2022), and alternatives like managing research evaluation on GitHub have been proposed (Takagi, 2022). Broader concerns persist, including whether reviewing efforts align with academic impact (Church et al., 2024) and how best to address systemic biases (Shah, 2022). As NeurIPS discussions occur mid-year and ICLR discussions happen later, the timing of transparency measures may also shape reviewer behavior and decision-making.\n2.2. Regulations\nAs OPR evolves, regulatory guidelines ensure integrity, fairness, and privacy (Ross-Hellauer & G\u00f6r\u00f6gh, 2019). Some researchers caution that excessive transparency may undermine review quality (Bianchi & Squazzoni, 2022), while others highlight the challenge of balancing confidentiality with open science (Baez, 2002; Dennis et al., 2019).\nAI/ML conferences face additional regulatory challenges. Public review platforms can expose researchers to scrutiny or harassment, raising ethical concerns (Wang et al., 2023). Al-powered peer review introduces risks that require human oversight (Seghier, 2024), while plagiarism in review reports and the rise of review mills threaten review integrity (Piniewski et al., 2024; Oviedo-Garc\u00eda, 2024; Ezhumalai et al., 2024). To address these risks, researchers advocate for clearer policies on reviewer disclosures, public critique, and misconduct prevention, ensuring transparency strengthens rather than undermines the review process (Kaltenbrunner et al., 2022; Kuznetsov et al., 2024)."}, {"title": "3. Open Statistics: Paper Copilot", "content": "Moving toward a more transparent AI / ML community has become a prominent topic at various venues and within the broader research ecosystem. However, the push for more openness and regulation must be guided by concrete evidence of community needs and interests. Despite growing discussions, there is a lack of quantitative evidence reflecting the community's true interests and practices around open reviewing. To address this gap, we created Paper Copilot-a website designed to deliver research-related services and insights for the AI / ML community.\nIn this section, we explain how Paper Copilot collects, analyzes, and presents open statistics on review processes. We also discuss our preliminary observations regarding web traffic and user demographics via Google Analytics, setting"}, {"title": "3.1. Data Collection Methodology", "content": "Paper Copilot provides research-related services by gathering and visualizing key metrics from AI / ML conferences. These venues vary in their reviewing models-ranging from choices that expose all review discussions publicly to those that remain fully private. To accommodate these variations, we employ two main strategies for obtaining data:\n1. Automated Retrieval via Public APIs and Site Bots: When review data are publicly available (e.g., via the OpenReview (2025) API for ICLR), our custom bots retrieve key metrics such as ratings, confidence levels, and reviewer comments. These bots run on a daily schedule, creating a temporal profile that documents how scores and discussions evolve throughout the review cycle.\nAdditionally, we enhance our data collection by deploying bots on the official websites of the respective venues. This approach allows us to include descriptive details such as author identities and affiliations while also enabling us to identify and address inconsistencies across data sources.\n2. Community Submissions via Google Forms: For partially open or closed-review venues where data are not shared publicly during the review process, we invite authors to voluntarily submit anonymized review information via Google Forms embedded on the Paper Copilot website. This community-driven approach underscores researchers' appetite for transparency even when official policies restrict open peer review data."}, {"title": "3.2. Traffic and Engagement Overview", "content": "We use Google Analytics (2025) to track page views, session durations, referral sources, and basic demographic details (e.g., user location, device type) for Paper Copilot. Also, the collected data is validated via Matomo (2025). No personally identifying information is collected, ensuring user privacy."}, {"title": "4. Analysis", "content": "The collected traffic metrics and demographics reveal a global community that is not only aware of but also deeply invested in tracking review outcomes and statistics. In this section, we delve into the collected data to evaluate how different review models align with the community's demand"}, {"title": "4.1. Review Disclosure", "content": "Many AI / ML venues have migrated from traditional closed platforms (e.g., Microsoft CMT) to more transparent platforms (e.g., OpenReview). However, as illustrated in Figure 2 and Figure 3, not all venues that move to OpenReview adopt a fully open process. We categorize venues into three disclosure modes:\n\u2022 Fully Open: All reviews, discussions, and are publicly visible in real-time (e.g., ICLR).\n\u2022 Partially Open: Reviews and discussions become public only after the decision phase concludes (e.g., NeurIPS, CORL).\n\u2022 Fully Closed: Reviews and discussions remain private indefinitely (e.g., ICML, CVPR).\nFigure 3 shows that the actual level of transparency has remained mostly unchanged over the past decade, despite migrations to more flexible review platforms. Thus, while platform shifts suggest a trend toward openness, the community has not fully embraced complete real-time visibility."}, {"title": "4.2. Community Engagement", "content": "Before diving into the effective community interest, we first elaborate and understand who forms the community and how they engaged with open statistics. By analyzing key demographic markers such as age, gender, and geographic distribution-we can better account for variations in usage patterns and guard against potential biases."}, {"title": "4.3. Community Interests Validation", "content": "We quantize and validate community's activity and interests via various metrics including site visits, Google Organize Search Rankings and user activity on Openreview platform."}, {"title": "5. Discussion: Close or Open", "content": "In this section, we examine three key challenges affecting the integrity of the fully closed peer review process and then propose how moving toward more open or partially open models could address these issues effectively.\n5.1. Problems in Close Review\nChallenges for Younger Reviewers Demographic data indicate that a substantial portion of the AI research community now consists of younger individuals aged 18\u201324. As the field grows exponentially and the number of submissions soars, venues often face a shortage of qualified reviewers. In response, some venues (CVPR, 2025) require each submitting author to serve as a reviewer in order to manage the massive influx of papers.\nWhile this policy helps alleviate reviewer shortages, it also compels younger, less-experienced researchers to evaluate work at the forefront of the field. Younger researchers are undoubtedly talented and growing in number, their limited familiarity with rigorous peer-review standards, combined with the pressure of large submission volumes, can lead to uneven or suboptimal feedback. This dynamic risks diluting the overall quality of the peer-review process.\nA growing concern within the community form Paper Copilot highlights this issue: many authors report that reviewers struggle to fully understand the nuances of their submissions. While such claims are currently anecdotal and not yet quantifiable, future studies could analyze this trend systematically. As these reports continue to rise, they signal a potential systemic challenge that, if left unaddressed, could impose significant additional burdens on program committees, requiring extensive resources to mediate disputes and resolve misunderstandings stemming from insufficiently experienced reviewers.\nEthical Concerns and AI Usage in Closed Review\nWhether closed or open, reviewers typically perform their duties with minimal oversight and must balance these tasks alongside their own research. The rise of large language models (LLMs) adds further complexity. Although LLMs can assist in revising or evaluating manuscripts, their unregulated use in a closed review context raises concerns about consistency and accountability.\nIn response, some venues have introduced policies to regulate LLM usage. However, enforcement remains challenging in a closed review environment, where the reviewing process and any potential misuse occurs largely out of public view. Moreover, these issues disproportionately affect younger reviewers, who may lack both the resources and the confidence to navigate potential ethical dilemmas. Overreliance on LLMs risks homogenizing feedback, thus reducing the diversity of perspectives that is vital for thorough peer review.\nNoticed Inconsistencies in Acceptance Records A notable concern emerging from closed-review venues is the discrepancy in author information between official conference records and final published versions. For instance, in 2024, some authors changed their names after paper acceptance, creating mismatches that are difficult to detect in a closed setting. While we refrain from revealing specific names or details to protect the authors' identities, these inconsistencies can be traced through publicly available statistics. Such incidents underline gaps in accountability and underscore the need for more robust regulatory mechanisms.\nBy contrast, open review processes naturally invite broader oversight, making it easier to spot and address potential irregularities. Publicly visible reviews and commentary foster collective accountability and discourage misconduct. Taken together, these observations highlight the urgent need for a more transparent and well-regulated review framework in the AI / ML community to maintain trust, ensure high-quality feedback, and safeguard research integrity."}, {"title": "5.2. Towards Open", "content": "The challenges described in prior sections underscore the urgent need for a more transparent and accountable review framework-one that supports the influx of younger reviewers, regulates AI usage, and preserves the integrity of scholarly discourse. Although expanding participation can bring fresh perspectives, it also risks undermining quality if newer reviewers lack structured mentorship and formal training. At the same time, ethical concerns regarding AI-assisted reviewing-such as homogenized feedback-illustrate the fragility of closed systems, where limited oversight makes it difficult to enforce standards, detect biases, or reconcile inconsistencies in authorship records.\nMoving toward open or partially open review processes offers a pragmatic solution to these issues. By making reviews publicly visible, community members can collectively scrutinize and address potential problems, from name-change discrepancies to excessive reliance on large language mod-"}, {"title": "6. Alternative Views", "content": "While the preceding sections advocate for more transparent review processes, it is important to recognize that open or partially open systems are not without drawbacks. Critics highlight issues such as the potential for plagiarism, misappropriation of innovative ideas, and threats to proprietary research, raising valid questions about how best to balance openness with the need for confidentiality.\nPlagiarism One frequently cited concern is that open review may inadvertently facilitate plagiarism if innovative concepts are publicly visible before a paper is formally published. When submissions are posted online (e.g., in open-review platforms or preprint servers like arXiv) and later rejected, these ideas remain accessible, allowing others to potentially adopt or iterate on them without proper attribution. However, such issues are not exclusive to open review. In fact, the growing trend of researchers posting preprints on arXiv-regardless of whether a conference uses open or closed peer review-reveals that this challenge is part of a broader question of how to protect intellectual property in public forums.\nMoreover, confidentiality can serve as a safeguard against idea theft, as it keeps manuscripts and reviews private until"}, {"title": "7. Conclusion", "content": "In this work, we analyzed the dynamics of open, partially open, and closed review processes in the AI/ML community, leveraging insights from Paper Copilot to highlight the growing interest in transparency. Our findings reveal that while fully open reviews promote transparency and engagement, they may also discourage reviewer confidence, whereas closed systems lack accountability and broader community involvement. However, our analysis is limited by the rapid evolution of the AI/ML community, where shifting norms may outpace existing review models, and by potential biases in voluntary data submissions, which may not fully capture the community's diversity. Future work will focus on tracking the evolving dynamics and further expanding data, refining demographic analyses, and exploring peer review mechanisms further."}]}