{"title": "SUBJECT-DRIVEN TEXT-TO-IMAGE GENERATION VIA PREFERENCE-BASED REINFORCEMENT LEARNING", "authors": ["Yanting Miao", "William Loh", "Abdullah Rashwan", "Suraj Kothawade", "Pascal Poupart", "Yeqing Li"], "abstract": "Text-to-image generative models have recently attracted considerable interest, enabling the synthesis of high-quality images from textual prompts. However, these models often lack the capability to generate specific subjects from given reference images or to synthesize novel renditions under varying conditions. Methods like DreamBooth and Subject-driven Text-to-Image (SuTI) have made significant progress in this area. Yet, both approaches primarily focus on enhancing similarity to reference images and require expensive setups, often overlooking the need for efficient training and avoiding overfitting to the reference images. In this work, we present the A-Harmonic reward function, which provides a reliable reward signal and enables early stopping for faster training and effective regularization. By combining the Bradley-Terry preference model, the A-Harmonic reward function also provides preference labels for subject-driven generation tasks. We propose Reward Preference Optimization (RPO), which offers a simpler setup (requiring only 3% of the negative samples used by DreamBooth) and fewer gradient steps for fine-tuning. Unlike most existing methods, our approach does not require training a text encoder or optimizing text embeddings and achieves text-image alignment by fine-tuning only the U-Net component. Empirically, A-Harmonic proves to be a reliable approach for model selection in subject-driven generation tasks. Based on preference labels and early stopping validation from the A-Harmonic reward function, our algorithm achieves a state-of-the-art CLIP-I score of 0.833 and a CLIP-T score of 0.314 on DreamBench.", "sections": [{"title": "Introduction", "content": "In the evolving field of generative AI, text-to-image diffusion models [25, 12, 23, 26, 24, 21, 22] have demonstrated remarkable abilities in rendering scenes that are both imaginative and contextually appropriate. However, these models often struggle with tasks that require the portrayal of specific subjects within text prompts. For instance, if provided with a photo of your cat, current diffusion models are unable to generate an image of your cat situated in the castle of your childhood dreams. This challenge necessitates a deep understanding of subject identity. Consequently, subject-driven text-to-image generation has attracted considerable interest within the community. Chen et al. [7] have noted that this task requires complex transformations of reference images. Additionally, Ruiz et al. [21] have highlighted that detailed and descriptive prompts about specific objects can lead to varied appearances in subjects. Thus, traditional image editing approaches and existing text-to-image models are ill-suited for subject-driven tasks."}, {"title": "Related Works", "content": "Ruiz et al. [21] formulated a class of problems called subject-driven generation, which refers to preserving the appearance of a subject contextualized in different settings. DreamBooth [21] solves the issue of preserving the subject by binding it in textual space with a unique identifier for the subject in the reference images, and simultaneously generating diverse backgrounds by leveraging prior class-specific information previously learned. A related work that could possibly perform the same task is textual inversion [11]. However, its original objective is to produce a modification of the subject or property marked by a unique token in the text. While it can be used to preserve the subject and change the background or setting, the performance is underwhelming compared to DreamBooth in various metrics [21].\nThe prevalent issue in DreamBooth and textual inversion is the training times [21, 11] since gradient-based optimization has to be performed on their respective models for each subject. Subject-driven text-to-image generator (SuTI) by [7] aims to alleviate this issue by employing apprenticeship learning. By scraping millions of images online, many expert models are trained for each cluster which allows the apprentice to learn quickly from the experts [7]. However, this is an incredibly intensive task with massive computational overhead during training time.\nIn the field of natural language processing, direct preference optimization has found great success in large language models (LLM) [19]. By bypassing reinforcement learning from human feedback and directly maximizing likelihoods using preference data, LLMs benefit from more stable training and reduced dependency on an external reward model. Subsequently, this inspired Diffusion-DPO by [27] which applies a similar technique onto the domain of diffusion models. However, this relies on a preference labelled dataset, which can be expensive to collect or not publicly available for legal reasons.\nFortunately, there are reward models that can serve as functional substitutes such as CLIP [17] and ALIGN [13]. ALIGN has a dual encoder architecture that was trained on a large dataset. The encoders can produce text and image embeddings, which allows us to obtain pairwise similarity scores by computing cosine similarity. There are also diffusion modeleling techniques that can leverage reward models. An example is denoising diffusion policy optimization (DDPO) by Black et al. [3] that uses a policy gradient reinforcement learning method to encourage generations that leads to higher rewards."}, {"title": "Preliminary", "content": "In this section, we introduce notations and some key concepts about text-to-image diffusion models and reinforcement learning.\nText-to-Image Diffusion Models. Diffusion models [12, 23, 25, 26, 24] are a family of latent variable models of the form po(x0) = \u222bx p\u04e9(x0:T)dx1:T, where the x1,..., x\u012b are noised latent variables of the same dimensionality as the input data x0 ~ q(x0). The diffusion or forward process is often a Markov chain that gradually adds Gaussian noise to the input data and each intermediate sample xt can be written as\n$$x_t = \\sqrt{a_t}x_0 + \\sqrt{1 - a_t}\\epsilon_t, \\quad \\text{for all } t \\in \\{1, ...,T\\},$$\nwhere at refers to the variance schedule and et ~ N(0, I). Given a conditioning tensor c, the core premise of text-to-image diffusion models is to use a neural network ee(xt, c, t) that iteratively refines the current noised sample xt to obtain the previous step sample xt\u22121, This network can be trained by optimizing a simple denoising objective function, which is the time coefficient weighted mean squared error:\n$$E_{x_0, c, t, \\epsilon_t} [w(t)||\\epsilon_{\\theta}(x_t, c, t) \u2013 \\epsilon_t ||^2],$$\nwhere t is uniformly sampled from {1, . . ., T} and w(t) can be simplified as 1 according to [12, 24, 20]."}, {"title": "Method", "content": "We present our \u5165-Harmonic reward function that provides reward signals for subject-driven tasks to reduce the learned model to be overfitted to the reference images. Based on this reward function, we use Bradley-Terry model to sample preference labels and a preference algorithm to finetune the diffusion model by optimizing both the similarity loss and the preference loss.\nIn contrast to other fine-tuning applications [27, 16, 19, 18], there is no human feedback in the subject-driven text-to-image generation task. The model only receives a few reference images and a prompt with a specific subject. Hence, we first propose the A-Harmonic reward function that can leverage the ALIGN model [13] to provide valid feedback based on the generated image fidelity: similarity to the given reference images and faithfulness to the text prompts.\nX-Harmonic Reward Function. The normalized ALIGN-I and ALIGN-T scores can be denoted as\n$$ALIGN-I(x, \\mathcal{I}_{ref}) := \\frac{1}{|\\mathcal{I}_{ref}|} \\sum_{x \\in \\mathcal{I}_{ref}} \\frac{CosSim(f_{\\theta}(x), f_{\\theta}(\\hat{x})) + 1}{2},$$\n$$ALIGN-T(x, c) := \\frac{CosSim(f_{\\theta}(x), g_{\\theta}(c)) + 1}{2},$$\nwhere f(x) is the image feature extractor and go(c) is the text encoder in the ALIGN model. Given a reference image set Tref, the A-Harmonic reward function can be defined by a weighted harmonic mean of the ALIGN-I and ALIGN-T scores,\n$$r(x, c; \\lambda, \\mathcal{I}_{ref}) := \\frac{1}{\\frac{\\lambda}{ALIGN-I(x, \\mathcal{I}_{ref})} + \\frac{1-\\lambda}{ALIGN-T(x, c)}}$$\nCompared to the arithmetic mean, there are two advantages to using the harmonic mean: (1) according to AM-GM-HM inequalities [9], the harmonic mean is a lower bound of the arithmetic mean and maximizing this \u201cpessimistic\u201d reward can also improve the arithmetic mean of ALIGN-I and ALIGN-T scores; (2) the harmonic mean is more sensitive to the smaller of the two scores, i.e., a larger reward is only achieved when both scores are relatively large.\nFor a simple example, consider X = 0.5. If there are two images, x and x, where the first image achieves an ALIGN-I score of 0.9 and an ALIGN-T"}, {"title": "Experiments", "content": "In this section, we present the experimental results demonstrated by RPO. We investigate three primary questions. First, can our algorithm learn to generate images that are faithful both to the reference images and to the textual prompts, according to preference labels? Second, if RPO can generate high-quality images, which part is the key component of RPO: the reference loss or the early stopping by the A-Harmonic reward function? Third, how do different Aval values affect validation and cause performance differences for RPO? We refer readers to Appendix A.2 for details on the experimental setup, Appendix A.7 for the skill set of RPO, Appendix A.4 for the limitations of the RPO algorithm, and Appendix A.5 for future work involving RPO.\nIn this work, we use the DreamBench dataset proposed by DreamBooth [21]. This dataset contains 30 different subject images including backpacks, sneakers, boots, cats, dogs, and toy, etc. DreamBench also provides 25 various prompt templates for each subject and these prompts are requiring the learned models to have such abilities: re-contextualization, accessorization, property modification, and attribute editing.\nWe follow DreamBooth [21] and SuTI [7] to report DINO [5] and CLIP-I [17] for evaluating image-to-image similarity score and CLIP-T [17] for evaluating the text-to-image similarity score. We also use our A-Harmonic reward as a evaluation metric for the overall fidelity and the default value of x = 0.3. For evaluation, we follow DreamBooth [21] and SuTI [7] to generate 4 images per prompt, 3000 images in total, which provides robust evaluation.\nDreamBooth [21]: This algorithm requires approximately |Zgen| = 1000 and 1000 gradient steps to finetune the UNet and text-encoder components. SuTI [7]: A pre-trained method that requires half a million expert models and introduces cross-attention layers into the original diffusion models. Textual Inversion [11]: A text-based method that optimizes the text embedding but freezes the diffusion models. Re-Imagen [6]: An information retrieval-based algorithm that modifies the backbone network architectures and introduces cross-attention layers into the original diffusion models."}, {"title": "Comparisons", "content": "We begin by addressing the first question. We use a quantitative evaluation to compare RPO with other existing methods on three metrics (DINO, CLIP-I, CLIP-T) in DreamBench to validate the effectiveness of RPO. The experimental results on DreamBench is shown in Table 1. We observe that RPO can perform better or on par with SuTI and DreamBooth on all three metrics. Compared to DreamBooth, RPO only requires 3% of the negative samples, but RPO can outperform DreamBooth on the CLIP-I and CLIP-T scores by 3% given the same backbone. Our method outperforms all baseline algorithms in the CLIP-T score, establishing a new SOTA result. This demonstrates that RPO, by solely optimizing UNet through preference labels from the A-Harmonic reward function, can generate images that are faithful to the input prompts. Similarly, our CLIP-I score is also the highest, which indicates that RPO can generate images that preserve the subject's visual features. In terms of the DINO score, our method is almost the same as DreamBooth when using the same backbone. We conjecture that the reason RPO achieves higher CLIP scores and lower DINO score is that the A-Harmonic reward function prefers to select images that are semantically similar to the textual prompt, which may result in the loss of some unique features in the pixel space.\nWe use the same prompt as SuTI [7], and the generated images are shown in Figure 3. RPO generates images that are faithful to both reference images and textual prompts. We noticed a grammatical mistake in the first prompt used by SuTI [7]; it should be A dog eating a cherry from a bowl. This incorrect prompt caused the RPO-trained model to become confused during the inference phase. However, RPO still preserves the unique appearance of the bowl. For instance, while the text on the bowl is incorrect or blurred in the SuTI and DreamBooth results, RPO accurately retains the words Bon Appetit from the reference bowl images. We also observed that RPO is the only method that generates thin-neck vases. Although existing methods can produce images highly faithful to the"}, {"title": "Conclusion", "content": "We introduce the X-Harmonic reward function to derive preference labels and employ RPO to finetune the diffusion model for subject-driven text-to-image generation tasks. Additionally, the A-Harmonic reward function serves as a validation method, enabling early stopping to mitigate overfitting to reference images and speeding up the finetuning process."}, {"title": "Appendix", "content": "In Reinforcement Learning (RL), the environment can be formalized as a Markov Decision Process (MDP). An MDP is defined by a tuple (S, A, P, R, po, T), where S is the state space, A is the action space, P is the transition function, R is the reward function, po is the distribution over initial states, and T is the time horizon. At each timestep t, the agent observes a state st and selects an action at according to a policy \u03c0(at st), and obtains a reward R(st, at), and then transit to a next state st+1 ~P(St+1 St, at). As the agent interacts with the MDP, it produces a sequence of states and actions, which is denoted as a trajectory T = (S0, 80, S1, a1,..., ST\u22121,AT\u2212-1). The RL objective is to maximize the expected value of cumulative reward over the trajectories sample from its policy:\n$$E_{\\tau\\sim p_{\\pi}(\\tau)} \\Big[ \\sum_{t=0}^{T-1} R(s_t, a_t) \\Big]$$\nWe formalize the denoising process as the following Diffusion MDP:\nST-t = (xt, c),\naT-t = Xt-1,\n\u03c0\u03c1(aT-t | ST-t) = Po(Xt\u22121 | Xt, c),\npo = p(c) \u00d7 (0,1),\n$$R(s_{T-t}, a_{T-t}) = R(x_t, x_{t-1}, c) = \\begin{cases} r(x_0, c) & \\text{if } t = 1, \\\\ 0 & \\text{otherwise} \\end{cases}$$\nwhere r(xo, c) can be a reward signal for the denoised image. The transition kernel is deterministic, i.e., P(sT-t+1 |\nST-t, aT-t) = (aT\u2212t, c) = (xt\u22121, c). For brevity, the trajectory T is defined by (x0, X1, . . ., x\u012b). Hence, the trajectory distribution for given diffusion models can be denoted as the joint distribution po(X0:T | c). In particular, the RL objective function for finetuning diffusion models can be re-written as the following optimization problem:\n$$E_{x_{0:T} \\sim p_{\\theta}(x_{0:T} / c)} \\Big[ \\sum_{t=1}^T R(x_t, x_{t-1}, c) \u2013 \\beta D_{KL} (p_{\\theta} (x_{t-1} | x_{t}, c) || p_{base} (x_{t-1} | x_{t}, c)) \\Big]$$\nwhere \u1e9e is a hyperparameter controlling the KL-divergence between the finetune model pe and the base model Pbase.\nThis constraint prevents the learned model from losing generation diversity and falling into 'mode collapse' due to a single high cumulative reward result. In practice, this KL-divergence has become a standard constraint in large language model finetuning [1, 16, 19].\nGiven preference labels and following the Direct Preference Optimization (DPO) framework [27, 19, 18], we can approximate the optimal policy p* by minimizing the upper bound:\n$$E_{x^+, x^-, x, c, t, \\epsilon^+, \\epsilon^-} log\\sigma \\Big( \\beta \\Big[ ||\\epsilon_{base}(x, c, t) \u2013 \\epsilon^+ ||^2 \u2013 ||\\epsilon_{\\theta}(x^+, c, t) \u2013 \\epsilon^+ ||^2 \\\n\u2013 \\Big[ ||\\epsilon_{base}(x, c, t) \u2013 \\epsilon^- ||^2 \u2013 ||\\epsilon_{\\theta}(x^-, c, t) \u2013 \\epsilon^- ||^2 \\Big] \\Big)$$\nwhere {x+}=0 represents the preference trajectory, i.e., r(x+, c) > r(x\u014d, c), and e\u207a and e\u00af are independent samples from Gaussian distribution. The detailed derivation can be found in [27].\nWe collect 8 training prompts: 6 re-contextualization, 1 property modification and 1 artistic style transfer for objects. 5 re-contextualization, 1 attribute editing, 1 artistic style transfer and 1 accessorization for live subjects. The trainig prompts are shown in Figure 6\nDuring training we use train = 0 for the A-Harmonic reward function to generate the preference labels. We evaluate the model performance by Aval-Harmonic per 40 gradient steps during training time and save the checkpoint that achieve the highest validation reward. Table 4 lists the common hyperparameters used in the generating skill set and the Aval used in the default setting."}]}