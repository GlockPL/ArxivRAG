{"title": "SUBJECT-DRIVEN TEXT-TO-IMAGE GENERATION VIA PREFERENCE-BASED REINFORCEMENT LEARNING", "authors": ["Yanting Miao", "William Loh", "Abdullah Rashwan", "Suraj Kothawade", "Pascal Poupart", "Yeqing Li"], "abstract": "Text-to-image generative models have recently attracted considerable interest, enabling the synthesis of high-quality images from textual prompts. However, these models often lack the capability to generate specific subjects from given reference images or to synthesize novel renditions under varying conditions. Methods like DreamBooth and Subject-driven Text-to-Image (SuTI) have made significant progress in this area. Yet, both approaches primarily focus on enhancing similarity to reference images and require expensive setups, often overlooking the need for efficient training and avoiding overfitting to the reference images. In this work, we present the A-Harmonic reward function, which provides a reliable reward signal and enables early stopping for faster training and effective regularization. By combining the Bradley-Terry preference model, the A-Harmonic reward function also provides preference labels for subject-driven generation tasks. We propose Reward Preference Optimization (RPO), which offers a simpler setup (requiring only 3% of the negative samples used by DreamBooth) and fewer gradient steps for fine-tuning. Unlike most existing methods, our approach does not require training a text encoder or optimizing text embeddings and achieves text-image alignment by fine-tuning only the U-Net component. Empirically, A-Harmonic proves to be a reliable approach for model selection in subject-driven generation tasks. Based on preference labels and early stopping validation from the A-Harmonic reward function, our algorithm achieves a state-of-the-art CLIP-I score of 0.833 and a CLIP-T score of 0.314 on DreamBench.", "sections": [{"title": "1 Introduction", "content": "In the evolving field of generative AI, text-to-image diffusion models [25, 12, 23, 26, 24, 21, 22] have demonstrated remarkable abilities in rendering scenes that are both imaginative and contextually appropriate. However, these models often struggle with tasks that require the portrayal of specific subjects within text prompts. For instance, if provided with a photo of your cat, current diffusion models are unable to generate an image of your cat situated in the castle of your childhood dreams. This challenge necessitates a deep understanding of subject identity. Consequently, subject-driven text-to-image generation has attracted considerable interest within the community. Chen et al. [7] have noted that this task requires complex transformations of reference images. Additionally, Ruiz et al. [21] have highlighted that detailed and descriptive prompts about specific objects can lead to varied appearances in subjects. Thus, traditional image editing approaches and existing text-to-image models are ill-suited for subject-driven tasks."}, {"title": "2 Related Works", "content": "Ruiz et al. [21] formulated a class of problems called subject-driven generation, which refers to preserving the appearance of a subject contextualized in different settings. DreamBooth [21] solves the issue of preserving the subject by binding it in textual space with a unique identifier for the subject in the reference images, and simultaneously generating diverse backgrounds by leveraging prior class-specific information previously learned. A related work that could possibly perform the same task is textual inversion [11]. However, its original objective is to produce a modification of the subject or property marked by a unique token in the text. While it can be used to preserve the subject and change the background or setting, the performance is underwhelming compared to DreamBooth in various metrics [21].\nThe prevalent issue in DreamBooth and textual inversion is the training times [21, 11] since gradient-based optimization has to be performed on their respective models for each subject. Subject-driven text-to-image generator (SuTI) by [7] aims to alleviate this issue by employing apprenticeship learning. By scraping millions of images online, many expert models are trained for each cluster which allows the apprentice to learn quickly from the experts [7]. However, this is an incredibly intensive task with massive computational overhead during training time.\nIn the field of natural language processing, direct preference optimization has found great success in large language models (LLM) [19]. By bypassing reinforcement learning from human feedback and directly maximizing likelihoods using preference data, LLMs benefit from more stable training and reduced dependency on an external reward model. Subsequently, this inspired Diffusion-DPO by [27] which applies a similar technique onto the domain of diffusion models. However, this relies on a preference labelled dataset, which can be expensive to collect or not publicly available for legal reasons.\nFortunately, there are reward models that can serve as functional substitutes such as CLIP [17] and ALIGN [13]. ALIGN has a dual encoder architecture that was trained on a large dataset. The encoders can produce text and image embeddings, which allows us to obtain pairwise similarity scores by computing cosine similarity. There are also diffusion modeleling techniques that can leverage reward models. An example is denoising diffusion policy optimization (DDPO) by Black et al. [3] that uses a policy gradient reinforcement learning method to encourage generations that leads to higher rewards."}, {"title": "3 Preliminary", "content": "In this section, we introduce notations and some key concepts about text-to-image diffusion models and reinforcement learning.\nDiffusion models [12, 23, 25, 26, 24] are a family of latent variable models of the form p\u03b8(x0) = \\int_x p\u03b8(x0:T)dx1:T, where the x1,..., xT are noised latent variables of the same dimensionality as the input data x0 ~ q(x0). The diffusion or forward process is often a Markov chain that gradually adds Gaussian noise to the input data and each intermediate sample xt can be written as\nxt = \\sqrt{at}xo + \\sqrt{1 - at}et, \\text{   for all }t \\in \\{1, ...,T\\},\\newline where at refers to the variance schedule and et ~ N(0, I). Given a conditioning tensor c, the core premise of text-to-image diffusion models is to use a neural network e\u03b8(xt, c, t) that iteratively refines the current noised sample xt to obtain the previous step sample xt\u22121, This network can be trained by optimizing a simple denoising objective function, which is the time coefficient weighted mean squared error:\n\\mathbb{E}_{xo,c,t,et} [w(t)||\\epsilon_\\theta(x_t, c, t) - E_t ||^2],\\newline where t is uniformly sampled from {1, . . ., T} and w(t) can be simplified as 1 according to [12, 24, 20]."}, {"title": "Reinforcement Learning and Diffusion DPO", "content": "Reinforcement Learning for diffusion models [3, 10, 27] aims to solve the following optimization problem:\n\\mathbb{E}_{xo:T~p_\\theta (xo:T/C)} [\\sum_{t=1}^T R(x_t, x_{t-1}, C) - \\beta D_{KL}(P_\\theta(X_{t-1} | X_t, C)||P_{base}(X_{t-1} | X_t, c))],\\newline where \u03b2 is a hyperparameter controlling the KL-divergence between the finetune model p\u03b8 and the pre-trained base model Pbase. In Equation (14) from Diffusion-DPO [27], the optimal p\u03b8 can be approximated by minimizing the negative log-likelihood:\n\\mathbb{E}_{x,x^+,x^-,t,\\epsilon^+,\\epsilon^-}[- \\log \\sigma (\\beta(||\\epsilon_{base}(x_t, c, t) - \\epsilon^t ||^2 - ||\\epsilon_\\theta(x_t, c, t) - \\epsilon^+ ||^2  - ||\\epsilon_{base}(x_t, c, t) - \\epsilon^- ||^2 - ||\\epsilon_\\theta(x_t, c, t) - \\epsilon^-||^2))]\\newline where {x+}=0 represents the preference trajectory, i.e., r(x+, c) > r(x\u2212, c), and e\u207a and e\u00af are independent samples from a Gaussian distribution. A detailed description is given in Appendix A.1.\nAdditional notations. We use xref and xgen to represent the reference image and generated image, respectively. Tref denotes the set of reference images, and Igen is the set of generated images. P(x > \\bar{x}) represents the probability that x is more preferred than x."}, {"title": "4 Method", "content": "We present our \u03bb-Harmonic reward function that provides reward signals for subject-driven tasks to reduce the learned model to be overfitted to the reference images. Based on this reward function, we use Bradley-Terry model to sample preference labels and a preference algorithm to finetune the diffusion model by optimizing both the similarity loss and the preference loss."}, {"title": "4.1 Reward Preference Optimization", "content": "In contrast to other fine-tuning applications [27, 16, 19, 18], there is no human feedback in the subject-driven text-to-image generation task. The model only receives a few reference images and a prompt with a specific subject. Hence, we first propose the \u03bb-Harmonic reward function that can leverage the ALIGN model [13] to provide valid feedback based on the generated image fidelity: similarity to the given reference images and faithfulness to the text prompts.\nThe normalized ALIGN-I and ALIGN-T scores can be denoted as\nALIGN-I(x, Tref) := \\frac{1}{|I_{ref}|} \\sum_{x \\in I_{ref}} \\frac{CosSim(f_\\theta(x), f_\\theta(\\bar{x})) + 1}{2},\\newline ALIGN-T(x, c) := \\frac{CosSim(f_\\theta(x), g_\\theta(c)) + 1}{2},\\newline where f\u03b8(x) is the image feature extractor and g\u03b8(c) is the text encoder in the ALIGN model. Given a reference image set Tref, the \u03bb-Harmonic reward function can be defined by a weighted harmonic mean of the ALIGN-I and ALIGN-T scores,\nr(x, c; \\lambda, I_{ref}) := \\frac{1}{\\lambda \\frac{1}{ALIGN-I(x, I_{ref})} + \\frac{1-\\lambda}{ALIGN-T(x, c)}}.\nCompared to the arithmetic mean, there are two advantages to using the harmonic mean: (1) according to AM-GM-HM inequalities [9], the harmonic mean is a lower bound of the arithmetic mean and maximizing this \u201cpessimistic\u201d reward can also improve the arithmetic mean of ALIGN-I and ALIGN-T scores; (2) the harmonic mean is more sensitive to the smaller of the two scores, i.e., a larger reward is only achieved when both scores are relatively large.\nFor a simple example, consider \u03bb = 0.5. If there are two images, x and \\bar{x}, where the first image achieves an ALIGN-I score of 0.9 and an ALIGN-T"}, {"title": "5 Experiments", "content": "In this section, we present the experimental results demonstrated by RPO. We investigate three primary questions. First, can our algorithm learn to generate images that are faithful both to the reference images and to the textual prompts, according to preference labels? Second, if RPO can generate high-quality images, which part is the key component of RPO: the reference loss or the early stopping by the \u03bb-Harmonic reward function? Third, how do different \u03bbval values affect validation and cause performance differences for RPO? We refer readers to Appendix A.2 for details on the experimental setup, Appendix A.7 for the skill set of RPO, Appendix A.4 for the limitations of the RPO algorithm, and Appendix A.5 for future work involving RPO."}, {"title": "5.1 Dataset and Evaluation", "content": "In this work, we use the DreamBench dataset proposed by DreamBooth [21]. This dataset contains 30 different subject images including backpacks, sneakers, boots, cats, dogs, and toy, etc. DreamBench also provides 25 various prompt templates for each subject and these prompts are requiring the learned models to have such abilities: re-contextualization, accessorization, property modification, and attribute editing.\nWe follow DreamBooth [21] and SuTI [7] to report DINO [5] and CLIP-I [17] for evaluating image-to-image similarity score and CLIP-T [17] for evaluating the text-to-image similarity score. We also use our \u03bb-Harmonic reward as a evaluation metric for the overall fidelity and the default value of \u03bb = 0.3. For evaluation, we follow DreamBooth [21] and SuTI [7] to generate 4 images per prompt, 3000 images in total, which provides robust evaluation.\nDreamBooth [21]: This algorithm requires approximately |Igen| = 1000 and 1000 gradient steps to finetune the UNet and text-encoder components. SuTI [7]: A pre-trained method that requires half a million expert models and introduces cross-attention layers into the original diffusion models. Textual Inversion [11]: A text-based method that optimizes the text embedding but freezes the diffusion models. Re-Imagen [6]: An information retrieval-based algorithm that modifies the backbone network architectures and introduces cross-attention layers into the original diffusion models."}, {"title": "5.2 Comparisons", "content": "We begin by addressing the first question. We use a quantitative evaluation to compare RPO with other existing methods on three metrics (DINO, CLIP-I, CLIP-T) in DreamBench to validate the effectiveness of RPO. The experimental results on DreamBench is shown in Table 1. We observe that RPO can perform better or on par with SuTI and DreamBooth on all three metrics. Compared to DreamBooth, RPO only requires 3% of the negative samples, but RPO can outperform DreamBooth on the CLIP-I and CLIP-T scores by 3% given the same backbone. Our method outperforms all baseline algorithms in the CLIP-T score, establishing a new SOTA result. This demonstrates that RPO, by solely optimizing UNet through preference labels from the \u03bb-Harmonic reward function, can generate images that are faithful to the input prompts. Similarly, our CLIP-I score is also the highest, which indicates that RPO can generate images that preserve the subject's visual features. In terms of the DINO score, our method is almost the same as DreamBooth when using the same backbone. We conjecture that the reason RPO achieves higher CLIP scores and lower DINO score is that the \u03bb-Harmonic reward function prefers to select images that are semantically similar to the textual prompt, which may result in the loss of some unique features in the pixel space.\nWe use the same prompt as SuTI [7], and the generated images are shown in Figure 3. RPO generates images that are faithful to both reference images and textual prompts. We noticed a grammatical mistake in the first prompt used by SuTI [7]; it should be A dog eating a cherry from a bowl. This incorrect prompt caused the RPO-trained model to become confused during the inference phase. However, RPO still preserves the unique appearance of the bowl. For instance, while the text on the bowl is incorrect or blurred in the SuTI and DreamBooth results, RPO accurately retains the words Bon Appetit from the reference bowl images. We also observed that RPO is the only method that generates thin-neck vases. Although existing methods can produce images highly faithful to the"}, {"title": "5.3 Ablation Study and Method Analysis", "content": "We investigate the second primary question through an ablation study. Two regularization components are introduced into RPO: reference loss as a regularizer and early stopping by \u03bbval-Harmonic reward function. Consequently, we compare four methods: (1) Pure Lsim, which only minimizes the image-to-image similarity loss Lsim; (2) Lref w/o early stopping, which employs Lref as a regularizer but omits early stopping by \u03bbval-Harmonic reward function; (3) Early stopping w/o Lref, which uses \u03bbval-Harmonic reward function as a regularization method but excludes Lref; (4) Full RPO, which utilizes both Lref and early stopping by the \u03bbval-Harmonic reward function. We choose the default value \u03bbval = 0.3 in this ablation study.\nWe observe that without early stopping, Lref can still prevent overfitting to the reference images and improve text-to-image alignment, though the regularization effect is weak. Specifically, the 0.3-Harmonic only shows a marginal improvement of 0.003 over pure Lsim and 0.001 over early stopping without Lref. The early stopping facilitated by the \u03bbval-Harmonic reward function plays a crucial role in counteracting overfitting, helping the diffusion models retain the ability to generate high-quality images aligned with textual prompts. To provide a deeper understanding of the \u03bb-Harmonic reward validation, we present two examples from during training in Figure 4, covering both objects and live subjects. We found that the model tends to overfit at a very early stage, i.e., within 200 gradient steps, where \u03bb-Harmonic can provide correct reward signals for the generated images. For the backpack subject, the generated image receives a low reward at gradient step 80 due to its lack of fidelity to the reference images. However, at gradient step 400, the image is overfitted to the reference images, and the model fails to align well with the input text, resulting in another low reward. \u03bb-Harmonic assigns a high reward to images that are faithful to both the reference image and textual prompts.\nWe examine the third primary question by selecting different \u03bbval values from the set {0.3, 0.5, 0.7} as the validation parameters for the \u03bb-Harmonic reward. According to Equation 5, we believe that as \u03bbval increases,"}, {"title": "6 Conclusion", "content": "We introduce the \u03bb-Harmonic reward function to derive preference labels and employ RPO to finetune the diffusion model for subject-driven text-to-image generation tasks. Additionally, the \u03bb-Harmonic reward function serves as a validation method, enabling early stopping to mitigate overfitting to reference images and speeding up the finetuning process."}, {"title": "A.1 Background", "content": "In Reinforcement Learning (RL), the environment can be formalized as a Markov Decision Process (MDP). An MDP is defined by a tuple (S, A, P, R, \u03c10, T), where S is the state space, A is the action space, P is the transition function, R is the reward function, \u03c10 is the distribution over initial states, and T is the time horizon. At each timestep t, the agent observes a state st and selects an action at according to a policy \u03c0(at|st), and obtains a reward R(st, at), and then transit to a next state st+1 ~P(st+1|st, at). As the agent interacts with the MDP, it produces a sequence of states and actions, which is denoted as a trajectory \u03c4 = (s0, a0, s1, a1,..., sT\u22121,aT\u22121). The RL objective is to maximize the expected value of cumulative reward over the trajectories sample from its policy:\nE\u03c4~p\u03c0(\u03c4)[\u2211t=0T\u22121R(st, at)]\nWe formalize the denoising process as the following Diffusion MDP:\nS\u03c0 = (xt, c), a\u03c0 = xt\u22121, \u03c0\u03c1(a\u03c0|S\u03c0) = p\u03b8(xt\u22121|xt, c),\n\u03c10 = p(c) \u00d7 U(0, 1), R(S\u03c0, a\u03c0) = R(xt, xt\u22121, c) = {r(xo, c) ift=1,0otherwise,\nwhere r(xo, c) can be a reward signal for the denoised image. The transition kernel is deterministic, i.e., P(sT\u2212t+1|sT\u2212t, aT\u2212t) = (aT\u2212t, c) = (xt\u22121, c). For brevity, the trajectory \u03c4 is defined by (xo, x1, . . ., x\u03c0). Hence, the trajectory distribution for given diffusion models can be denoted as the joint distribution p\u03b8(xo:T|c). In particular, the RL objective function for finetuning diffusion models can be re-written as the following optimization problem:\nE\u03c4~p\u03b8(xo:T|C)[\u2211t=1TR(xt, xt\u22121, c) \u2212 \u03b2DKL(p\u03b8(xt\u22121|xt, c)||pbase(xt\u22121|xt, c))]\nwhere \u03b2 is a hyperparameter controlling the KL-divergence between the finetune model p\u03b8 and the base model pbase. This constraint prevents the learned model from losing generation diversity and falling into 'mode collapse' due to a single high cumulative reward result. In practice, this KL-divergence has become a standard constraint in large language model finetuning [1, 16, 19].\nGiven preference labels and following the Direct Preference Optimization (DPO) framework [27, 19, 18], we can approximate the optimal policy p* by minimizing the upper bound:\nE\u03c4,x+,x\u2212[\u2212 logo(\u03b2(||\u03f5base(xt, c, t) \u2212 \u03f5t||2 \u2212 ||\u03f5\u03b8(x+, c, t) \u2212 \u03f5+||2  \u2212 ||\u03f5base(xt, c, t) \u2212 \u03f5\u2212||2 \u2212 ||\u03f5\u03b8(x\u2212, c, t) \u2212 \u03f5\u2212||2))]\nwhere {x+}=0 represents the preference trajectory, i.e., r(x+, c) > r(x\u2212, c), and e\u207a and e\u00af are independent samples from Gaussian distribution. The detailed derivation can be found in [27]."}, {"title": "A.2 Experimental Details", "content": "We collect 8 training prompts: 6 re-contextualization, 1 property modification and 1 artistic style transfer for objects. 5 re-contextualization, 1 attribute editing, 1 artistic style transfer and 1 accessorization for live subjects. The trainig prompts are shown in Figure 6\nDuring training we use \u03bbtrain = 0 for the \u03bb-Harmonic reward function to generate the preference labels. We evaluate the model performance by \u03bbval-Harmonic per 40 gradient steps during training time and save the checkpoint that achieve the highest validation reward. Table 4 lists the common hyperparameters used in the generating skill set and the \u03bbval used in the default setting."}, {"title": "A.3 Additional Comparisons", "content": "We observe RPO that can be faithful to both reference images and the input prompt. To investigate whether RPO can provide better quality than DreamBooth and SuTI, we follow SuTI paper and pick the robot toy as an example to demonstrate the effectiveness of RPO (Figure 7). In this example, DreamBooth is faithful to the reference image but it does not provide a good text-to-image alignment. SuTI provides an result that is fidelty to textual prompt. However, SuTI lacks fidelity to the reference image, i.e., the robot should stand with its wheels instead of legs. [7] use DreamBooth to finetune SuTI (Dream-SuTI) further to solve this failure case. Instead, RPO can generate an image not only faithful to the reference images but also align well with the input prompts.\nWe have also added more samples for comparison of different \u03bbval values (see Figure 8). We find that \u03bbval = 0.5 encourages the learned model to retain output diversity while still aligning with textual prompts. However, the generated images invariably contain a sofa, which is unrelated to the subject images. This occurs because every training image includes a sofa. A large \u03bbval weakens the regularization strength and leads to overfitting. Nevertheless, a small value of \u03bbval can potentially eliminate background bias. We highlight that this small \u03bbval not only encourages diversity but also mitigates background bias in identity preservation and enables the model to focus on the subject."}, {"title": "A.4 Limitations", "content": "Figure 9 illustrates some failure examples of RPO. The first issue is context-appearance entanglement. In Figure 9, the learned model correctly understands the keyword blue house; however, the appearance of the subject is also altered by this context, e.g. the color of the backpack has changed, and there is a house pattern on the backpack. The second issue is incorrect contextual integration. We conjecture that this failure is due to the rarity of the textual prompt. For instance, imagining a cross between a chow chow and a tiger is challenging, even for humans. Third, although RPO provides regularization, it still cannot guarantee the avoidance of overfitting. As shown in Figure 9, this may be because, to some extent, the visual appearance of sand and bed sheets is similar, which has led to overfitting issues in the model."}, {"title": "A.5 Future Work", "content": "The overfitting failure case leads to a future work direction: can online RL improve regularization and avoid overfitting? The second direction for future work involves implementing the LoRA version for RPO and comparing it to LoRA DreamBooth. Last but not least, we aim to identify or construct open-source, subject-driven datasets for comparison. Currently, DreamBench is the only open-source dataset we can access and evaluate for model performance. Nevertheless, we should create a larger dataset that includes more diverse subjects to verify the effectiveness of different algorithms."}, {"title": "A.6 Broader Impacts", "content": "The nature of generative image models is inherently subjected to criticism on the issue of privacy, security and ethics in the presence of nefarious actors. However, the core of this paper remains purely on an academic mission to extend the boundaries of generative models. The societal consequences of democratizing such powerful generative models is more thoroughly discussed in other papers. For example, Bird et al. outlines the classes of risks in text-to-image models [2] in greater detail and should be directed to such papers. Nevertheless, we play our part in the management of such risks by avoiding the use of identifiable parts of humans in the reference sets."}, {"title": "A.7 Skill Set", "content": "The skill set of the RPO-trained model is varied and includes re-contextualization (Figure 10), artistic style generation (Figure 11), expression modification (Figure 12), subject accessorization (Figure 13), color editing (Figure 14), multi-view rendering (Figure 15), and novel hybrid synthesis (Figure 16)."}]}