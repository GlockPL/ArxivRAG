{"title": "XAMBA: ENABLING EFFICIENT STATE SPACE MODELS ON RESOURCE-CONSTRAINED NEURAL PROCESSING UNITS", "authors": ["Arghadip Das", "Arnab Raha", "Shamik Kundu", "Soumendu Ghosh", "Deepak Mathaikutty", "Vijay Raghunathan"], "abstract": "State-Space Models (SSMs) have emerged as efficient alternatives to transformers\nfor sequential data tasks, offering linear or near-linear scalability with sequence\nlength, unlike transformers with quadratic-complexity attention. This makes\nSSMs ideal for long-sequence tasks in natural language processing (NLP), vision,\nand edge AI applications such as real-time transcription, translation, and contex-\ntual search. These applications demand lightweight, high-performance models\nfor deployment on resource-constrained devices like laptops and PCs. While spe-\ncialized accelerators have been proposed for emerging neural networks, designing\nnew hardware is time-intensive, costly, and impractical for every model. Instead,\noptimizing models for existing neural processing units (NPUs) in AI PCs offers\na scalable and efficient solution. Towards this end, we propose XAMBA, the\nfirst framework to enable and optimize SSMs on commercial off-the-shelf (COTS)\nstate-of-the-art (SOTA) NPUs. Our approach follows a systematic three-step\nmethodology: (1) enabling SSMs on NPUs, (2) optimizing performance to meet\ntarget Key Performance Indicator (KPI) requirements, and (3) trading accuracy\nfor additional performance gains. After enabling SSMs on NPUs, XAMBA ad-\ndresses key performance bottlenecks with two techniques: CumBA and ReduBA.\nThese replace sequential CumSum and ReduceSum operations with matrix-based\ncomputations, significantly improving execution speed and memory efficiency. In\naddition, ActiBA further enhances performance by mapping computationally ex-\npensive activation functions (e.g., Swish, Softplus) to NPU hardware using piece-\nwise linear approximations, reducing latency with minimal accuracy loss. Exper-\nimental evaluations on an Intel\u00ae Core\u2122 Ultra Series 2 AI PC demonstrate that\nXAMBA achieves significant performance improvements, reducing execution la-\ntency by up to 2.6\u00d7 compared to baseline implementation. Our code implemen-\ntation is available at this link.", "sections": [{"title": "INTRODUCTION", "content": "SSMs, a classical framework for modeling dynamic systems through first-order differential equa-\ntions, have gained prominence in machine learning for their efficiency in handling sequential data.\nUnlike transformers, which rely on quadratic-complexity attention mechanisms, SSMs achieve lin-\near or near-linear scalability with sequence length by leveraging principles from convolutional and\nrecurrent neural networks Gu et al. (2022). This makes SSMs highly suitable for long-sequence\ntasks like natural language processing, computer vision, and medicine, where they match trans-\nformers' modeling capabilities with significantly lower computational overhead. Their efficiency\nis particularly critical for edge applications, such as personal assistants and real-time transcription,"}, {"title": "2 XAMBA DESIGN METHODOLOGY", "content": "Before detailing the design methodology, it is essential first to understand the underlying system\narchitecture (refer Figure 2). We consider an output-stationary MPU architecture (as shown in Fig-\nure 2(a)) inspired by Raha et al. (2024). Although our case study considers an output-stationary\nMPU architecture, the proposed techniques are generic and can be applied to other NPUs without\nloss of generality. Step-1: The first step of XAMBA is to enable SSMs on NPUs. Since NPUs\ngenerally support static input shapes, we use a prefill model with a fixed number of input tokens to\ngenerate the hidden states, applying padding for smaller inputs. For subsequent token generation,\nwe employ a separate model that uses the cached hidden states to ensure efficiency."}, {"title": "2.1 STEP-2: OPTIMIZING SSM PERFORMANCE TO MEET TARGET KPI REQUIREMENTS", "content": "CumBA: As highlighted in Figure 1, one of the major bottlenecks in executing Mamba-2 on NPUs\nis the CumSum operation. A deeper analysis reveals that Mamba-2 contains three CumSum opera-\ntions per block (Figure 5), but the primary bottleneck, denoted as CumSum\u044c, accounts for more\nthan 99.9% of the total CumSum execution time. This bottleneck arises in step-1 of the SSD frame-\nwork of Mamba-2 (Listing 1 of Dao & Gu (2024)). Within SSD, the input sequence is split into\nblocks, and step 1 computes the intra-chunk output, assuming an initial state of zero. CumSum\u044c,\nappearing at the start of this step, computes semi-separable (SS) matrices essential for modeling\nlong-range dependencies across input segments. The bottleneck stems from the large matrix dimen-\nsions associated with CumSum\u044c: In Mamba-2 130M, CumSum\u044c operates on a 256 \u00d7 256 matrix,\nwhereas the other CumSum operations in the model involve significantly smaller dimensions (256\nand 2 \u00d7 2). Figure 2(b) depicts that executing CumSum on NPUs leads to high latency due to its\nsequential nature on the DSP. Given an input tensor $X \\in \\mathbb{R}^{m\\times n}$, the standard CumSum operation\nalong the row dimension is expressed as $C_{i,j} = \\sum_{k=1}^{i} X_{k,j}$ for all $i \\in [1,m], j\\in [1,n]$. This"}, {"title": "CumBA memory savings using Zero Value Compression (ZVC)", "content": "XAMBA applies ZVC Rhu et al.\n(2018) to compress the CumBA mask, a lower triangular binary matrix with ~50% zeros, sig-\nnificantly reducing storage and memory traffic (Figure 3). This compression leads to substantial\nmemory savings, as only non-zero elements are stored. Additionally, modern NPUs utilize sparsity\nbitmaps to skip zero-value computations, further improving execution speed and energy efficiency.\nWhile CumBA benefits from ZVC-driven optimizations, the weights in Mamba and Mamba-2 ex-\nhibit minimal inherent sparsity, limiting acceleration gains from sparsity-aware execution. As future\nwork, we plan to explore structured sparsity techniques in SSMs to further enhance NPU efficiency."}, {"title": "ReduBA", "content": "As illustrated in Figure 1, another significant bottleneck in the execution of Mamba-2 on\nNPUs is the ReduceSum operation. A detailed analysis shows that these bottlenecks originate from\nthe reduction sum operations present in every step of the SSD framework in Mamba-2 (Listing 1 of\nDao & Gu (2024)). Similar to CumSum, the ReduceSum operation suffers from high latency due\nto sequential DSP execution, as illustrated in Figure 2(b). Given an input matrix $X \\in \\mathbb{R}^{m\\times n}$, the\nReduceSum along the row dimension is defined as $R_j = \\sum_{i=1}^{m} X_{i,j} = C_{m,j}$ for all $j \\in [1, n]$.\nTo mitigate this, XAMBA introduces ReduBA, which reformulates ReduceSum as a matrix-\nvector multiplication (MVM) using a precomputed vector mask $M_{ReduBA}$ (Figure 2(c)), where\n$M_{ReduBA}(i) = 1$ for all $i$. The ReduceSum operation is then computed as $R = M_{ReduBA} \\cdot X$.\nReduBA improves upon CumBA by reusing the ReduBA vector mask $M_{ReduBA}$ across all opera-\ntions, reducing memory traffic. Unlike CumBA's matrix-matrix operations, where each computation\nfetches a new part of the mask, ReduBA's matrix-vector multiplication applies the same mask re-\npeatedly, minimizing memory accesses and optimizing bandwidth. ReduBA also leverages multiple\nMAC units in the MPU and a tiled computation strategy, further enhancing data reuse and reducing\non-chip memory accesses, resulting in lower latency and improved memory efficiency for Reduce-\nSum operations in Mamba-2 on NPUs."}, {"title": "2.2 STEP-3: TRADING ACCURACY FOR ADDITIONAL PERFORMANCE GAINS", "content": "ActiBA: As illustrated in Figure 1, two of the most significant bottlenecks in Mamba's execution\non NPUs are the Swish (a.k.a. SiLU) and Softplus activation functions. They introduce signifi-\ncant execution overhead when processed sequentially on the DSP, as depicted in Figure 2(d). The\nSiLU function is defined as $SiLU(x) = x \\cdot \\sigma(x)$ with $\\sigma(x) = \\frac{1}{1+e^{-x}}$, while Softplus is given by\n$Softplus(x) = log(1 + e^{\\beta x})$. Both functions exhibit nonlinearity near the origin but transition to\nlinear behavior elsewhere, making them suitable for approximation using piecewise linear functions."}, {"title": "3 EXPERIMENTAL METHODOLOGY", "content": "Networks and Datasets: Experiments use pretrained state-space models from HuggingFace, specif-\nically mamba-130m-hf and mamba2-130m-hf, with fixed input tokens of 4. Preprocessing\nand Conversion: The models are converted from PyTorch to ONNX. They are then converted\nto OpenVINO Intel IR files (compressing weights to FP16 precision) and compiled into a binary\nusing the OpenVINO NPU compiler. CumBA and ReduBA optimizations are applied during con-\nversion, and ActiBA is emulated by replacing activation functions with ReLU. Platform: Experi-\nments run on an Intel\u00ae Core\u2122 Ultra Series 2 Intel Corporation (2024a) platform (ASUS Zenbook\nS 14, 16GB RAM, 256V NPU). Performance Evaluation: Models are evaluated using Open-\nVINO's benchmark_app tool, focusing on inference latency, with optimizations (CumBA, Re-\nduBA, ActiBA) affecting NPU execution efficiency. All results were collected using public frame-\nworks (OpenVINO, PyTorch) and are replicable via the provided code (see abstract)."}, {"title": "4 RESULTS", "content": "We evaluate XAMBA's effectiveness in reducing inference latency for Mamba-based models. Fig-\nure 4(a) shows the average latency of a single-block Mamba-2 130M model on the Intel\u00ae Core\u2122\nUltra Series 2 NPU. The CumBA technique reduces latency by 1.8\u00d7 over the baseline, while Re-\nduBA provides a 1.1\u00d7 reduction. Reduced inference latency directly translates to improved To-\nkens/second. Combining CumBA and ReduBA yields a 2.3\u00d7 reduction, demonstrating the per-\nformance gains of XAMBA. Figure 4(b) presents the normalized inference latency breakdown for\nthe Mamba-2 130M model, comparing the baseline with the optimized CumBA technique. In the\nbaseline, CumSum operations contribute over 50% of the total latency. CumBA reduces this by\ntransforming CumSum into matrix multiplication using a precomputed mask, achieving a 1.85\u00d7\nreduction in latency. Figure 4(c) shows the first inference latency breakdown for the Mamba 130M\nmodel with ActiBA optimizations. Using PLU for SoftPlus in a piecewise linear fashion reduces\nlatency by 1.2\u00d7. Further reductions (1.8\u00d7) are achieved with SiLU, leading to a total 2.6\u00d7 latency"}, {"title": "5 CONCLUSION & FUTURE WORK", "content": "This work presents XAMBA, the first framework optimizing SSMs on COTS NPUs, removing the\nneed for specialized accelerators. XAMBA mitigates key bottlenecks in SSMs like CumSum, Re-\nduSum, and activations using ActiBA, CumBA, and ReduBA, transforming sequential operations\ninto parallel computations. These optimizations improve latency, throughput (Tokens/s), and mem-\nory efficiency. Future work will extend XAMBA to other models, explore compression, and develop\ndynamic optimizations for broader hardware platforms."}, {"title": "A APPENDIX", "content": "\u0391.1 \u039c\u0391MBA VS. MAMBA-2\nThis section presents a comparative analysis of the Mamba Gu & Dao (2024) and Mamba-2 Dao\n& Gu (2024) architectures, focusing on their structural and operational distinctions. Fig. 5 com-\npares the architectures and operations of the Mamba and Mamba-2 blocks, highlighting their struc-\ntural and computational differences. Mamba-1 focuses on leveraging structured state-space models\n(SSMs) for long-range sequence modeling through memory initialization using a high-order poly-\nnomial projection operator (HiPPO) Gu et al. (2020), a selection mechanism, and hardware-aware\ncomputing. It employs a sequential processing structure where linear projections are applied to the\ninput sequence and state-space parameters [A, B, C] in stages, with selective scanning through the\nSSM unit to map inputs X to outputs Y. The Mamba-1 block relies on skip connections to reuse\nfeatures and mitigate performance degradation during training. Mamba-2 simplifies this design by\nintroducing the structured state-space duality (SSD) framework, which connects SSM and attention\nmechanisms. Instead of sequential linear projections, the SSD layer processes [X, A, B, C'] simul-\ntaneously using a single projection, akin to generating Q, K, V in standard attention mechanisms.\nThis reduces computational overhead and accelerates processing. Additionally, Mamba-2 adds a\nnormalization layer after the skip connection to improve training stability.\nFig. 5 also highlights the operator OpenVINO Documentation (2024) differences between Mamba\nand Mamba-2, emphasizing the trade-offs in performance on Intel\u00ae AI PCs Intel Corpora-\ntion (2024a) Intel Corporation (2024b). Mamba-2 introduces new operators like CumSum and\nReduceSum, while reducing gather operations (from 18 to 7). However, computationally expen-\nsive operations such as power and sqrt increase, and data-parallel operations critical for DPUs,\nlike MatMul (reduced from 8 to 2) and Add (from 11 to 10), are decreased. These changes nega-\ntively impact Mamba-2's performance on Intel\u00ae AI PCs, as it shifts away from hardware-optimized\noperations. Despite structural simplifications through its SSD framework, Mamba-2 performs worse\nthan Mamba on AI PCs due to increased reliance on less hardware-efficient operators."}]}