{"title": "On the Capacity of Citation Generation by Large Language Models", "authors": ["Haosheng Qian", "Yixing Fan", "Ruqing Zhang", "Jiafeng Guo"], "abstract": "Retrieval-augmented generation (RAG) appears as a promising method to alleviate the \"hallucination\" problem in large language models (LLMs), since it can incorporate external traceable resources for response generation. The essence of RAG in combating the hallucination issue lies in accurately attributing claims in responses to the corresponding retrieved documents. However, most of existing works focus on improving the quality of generated responses from the LLM, while largely overlooked its ability to attribute sources accurately. In this study, we conduct a systematic analysis about the capabilities of LLMs in generating citations within response generation, and further introduce a novel method to enhance their citation generation abilities. Specifically, we evaluate both the correctness and citation quality for seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce new citation evaluation metrics to eliminate the over-penalization of unnecessary and excessive citations in existing metrics. Furthermore, we propose a Generate-then-Refine method that completes relevant citations and removes irrelevant ones without altering the response text. The results on WebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves the quality of citations in responses generated by LLMs.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) [1] demonstrate outstanding performance in various natural language processing tasks, showing remarkable generative capabilities for complex questions [2]. However, LLMs also face the well-known \"hallucination\" issue as they tend to produce fabricated content for unknown questions, which largely hinders the practical usage in risk-aware applications, such as medical or legal consultants. To this end, RAG appears as a promising method to incorporate real-time and factual knowledge for response generation [3].\nWhile RAG could enhance the LLMs in leveraging external resources through in-context learning, it is crucial to acknowledge that the core is to provide citations for any generated statements in responses. However, recent advances in"}, {"title": "2 Related Work", "content": "In this section, we review existing relevant work from two perspectives: citation generation and citation evaluation. Some researchers also refer to the process of associating responses with their corresponding supporting references as \"attribution\", and we include these works as well.\nCitation Generation Recently, a host of works in the RAG field have required LLMs to provide citations while generating responses. Nakano et al. [10] presented WebGPT, which fine-tunes GPT-3 to answer long-form questions based on a web browsing environment. This is one of the earliest works enabling LLMs to generate responses with citations. Menick et al. [14] used reinforcement learning from human preferences to train language models that generate responses while also citing specific evidence to support their claims. Qian et al. [15] introduced ReGen framework, which enhances the generation factualness and supports the generation of responses with citations. Liu et al. [11] presented WebGLM, employing a rule-based approach to match responses and references for filtering high-quality training data containing citations, and fine-tunes LLMs to learn incorporating citations into answers. Qin et al. [16] presented WebCPM, a fine-tuned language model that imitates human web search behavior, treating \"Quote\" as an action to extract content from current web page for supporting evidence during response generation. Gao et al. [8] used a few-shot method to guide LLMs in generating citations and also provide a post-hoc cite option to add citations into the responses. Sun et al. [17] introduced an approach named VTG, incorporating evolving memory and self-reflection, supporting evidence verification and retrieval. This approach aids models in rethinking and reflecting on the relationship between claims and citations. Huang et al. [18] proposed a training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations.\nCitation Evaluation It is crucial to quantitatively evaluate the quality of citations in responses once models are capable of generating citations. Rashkin et al. [19] proposed a manual evaluation framework named AIS for measuring whether model-generated statements are supported by underlying sources. Based on this, Gao et al. [20] introduced an automated metric AutoAIS, which approximates human AIS judgments using an NLI model. Bohnet et al. [21] subsequently defined a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard and employing AutoAIS as an automatic evaluation metric. Liu et al. [22] manually evaluated the citations included in popular generative search engines from the perspectives of comprehensiveness and accuracy. Liu et al. [11] manually evaluated the relationships"}, {"title": "3 Analysis of Citation Generation by Large Language Models", "content": "In this section, we conduct a comprehensive evaluation and analysis of the latest LLMs' ability to generate citations. We employ two basic methods, few-shot and fine-tuning, to guide LLMs in generating responses with citations."}, {"title": "3.1 Datasets", "content": "We select three LFQA datasets for our experiments: (1) WebGLM-QA [11], consisting of 43,579 data samples for the train split, 1,000 for the validation split, and 400 for the test split. Each data sample contains a question, an answer and a set of references. The answers and accompanying citations in the dataset were generated by GPT-3 [1] through in-context learning. Liu et al. [11] applied a series of rules to filter the dataset. They used ROUGE-1 [26] to measure the similarity between answer segments and their corresponding references to remove irrelevant citations which were labeled inaccurately. Additionally, they also implemented rule-based filtering to alleviate issues such as hallucination, few citations, and low-quality citations. (2) ASQA [12] is a factoid QA dataset where the questions often contain ambiguities, resulting in multiple answers based on different interpretations. Responses to these ambiguous questions should synthesize factual information from multiple sources to form the final answer. ALCE benchmark [8] randomly selected 948 samples from original ASQA dataset and added retrieved passages to construct a test set. (3) ELI5 [13] is also an LFQA dataset, with questions collected from the Reddit forum \"Explain Like I'm Five\", primarily consisting of \"How\" and \"Why\" questions. For these types of questions, good answers are often quite detailed and cannot be adequately addressed with brief responses or by simply extracting words or phrases from the contexts. In a similar manner, ALCE benchmark [8] selected 1000 samples to construct a test set."}, {"title": "3.2 Evaluation", "content": "We evaluate responses for both their correctness and citation quality. Despite our main focus is not on correctness, it remains a crucial aspect of evaluation. We use well-established metrics BLEU-4 [27] and ROUGE-L [26] to measure correctness.\nFor citation quality evaluation, we initially adopt two metrics defined in ALCE [8]: citation recall and citation precision.\nCitation Recall Before evaluation, each response is segmented into several statements {S1, S2, ...}. Citation recall is computed on a per-statement basis, where each statement si receives a binary recall score. The citation recall is the average of recall scores across all statements in the entire dataset. For each statement si, recall score is 1 if and only if si contains at least one citation and $(concat(Ci), si) = 1$, where $(premise, hypothesis)$ is the NLI model that outputs 1 if the premise entails the hypothesis, and 0 otherwise [8]. And concat(Ci) denotes the concatenation of all passages cited by si.\nThis calculation method is too strict for some responses. Upon reviewing the experimental results, we found that not all statements necessarily require citations. Statements that are commonsense such as \"Humans can walk but cannot fly\" or transitional statements like \"Next, I will answer the question from the following aspects\" don't need any citations. The above metric may lead to underestimated evaluations for certain responses.\nBased on this issue, we have defined a more lenient metric. If a statement si doesn't have any citation and $(concat(Call), si) = 0$, then it will not require computation of its recall score and will not be included in the final average. And concat(Call) denotes the concatenation of all retrieved passages that serve as context prompts to the LLMs in current sample.\nCitation Precision Similar to citation recall, before calculating citation precision, each response is also segmented into statements. However, citation precision is calculated on a per-citation basis, where each citation cij receives a binary precision score. The citation precision is the average of precision scores across all citations in the entire dataset. Citation precision focuses on whether each citation Cij is relevant to the statement si. A citation Cij is 'irrelevant' if Cij itself cannot support si and does not affect the rest of the citations to support si [8].\nThis definition of \"relevant\" may overly penalize answers that have excessive citations. If two references contain the same information and happen to be cited together in a statement, the above method may misjudge both citations as irrelevant, even though they both contribute to supporting the statement.\nDue to this issue, we have redefined \u201crelevant\u201d. For a citation cij, if it can support statement si independently or if it can support statement si after combining with a subset of remaining citations Ci\\{cij} that cannot support statement si, we consider this citation Cij is relevant. Formally, Cij is \"relevant\" if either of the"}, {"title": "3.3 Implementation Details", "content": "We conduct experiments on seven representative latest LLMs, including GPT -3.5-turbo-0125 [28], Llama-2-7b-chat [29], Llama-2-13b-chat [29], Mistral-7B-Instruct-v0.2 [30], Meta-Llama-3-8B-Instruct [31], glm-4-9b-chat [32], and Qwen2-7B-Instruct [33]. We fine-tune LLMs on the train split of WebGLM-QA and evaluate them on the test split of WebGLM-QA, as well as on the oracle versions of ASQA and ELI5 in ALCE [8,11-13]. And we use t5_xxl_true_nli_mixture [34] as the NLI model when evaluating metrics related to citation.\nIn few-shot experiments, we provide two examples for each input. In the fine-tuning experiments, we use LoRA [35] method to fine-tune six open-source LLMs. To facilitate reproducibility of results and avoid bias introduced by sampling during decoding, we employ greedy decoding for all open-source models."}, {"title": "3.4 Results", "content": "The main results are summarized in and we have the key observations as follows.\nFirst, early open-source models lack the ability to generate citations. Earlier released LLMs like Llama-2 series, whether with 7B or 13B parameters, perform poorly in few-shot experiments across all three datasets. In contrast, Llama-3 models developed by the same team as Llama-2 appear to have significantly better citation generation capabilities. In few-shot experiments on ASQA and ELI5, Llama-3 even surpassed GPT-3.5-turbo by a wide margin. Additionally, other more recently released LLMs have also shown a nearly satisfactory ability in few-shot settings. One possible reason is that as LLMs are increasingly used in RAG tasks, model developers have started to focus on attribution capabilities of LLMs and have conducted additional training on related tasks for them.\nSecond, LLMs can significantly benefit from fine-tuning to enhance their citation generation capabilities. After fine-tuning, all open-source models demonstrate substantial improvements on WebGLM-QA, both in response correctness and citation quality, compared to their few-shot results. The previously underperforming Llama-2 series models reached performance levels comparable to"}, {"title": "4 Generate-then-Refine", "content": "Based on the previous experiments and analysis, we have identified there is still considerable room for improvement in the quality of citations within the responses. Inspired by post-hoc methods [9], we propose a Generate-then-Refine approach aimed at improving the citation quality without altering the response text. Previous post-hoc methods heavily relied on rule-based matching such as text overlap, which is ineffective for semantic matching. Leveraging the powerful natural language understanding capabilities of LLMs, we aim to fine-tune the LLM to become a robust refiner in our approach."}, {"title": "4.1 Methods", "content": "We aim for the refiner to have three capabilities: (1) keep relevant citations within the response; (2) add necessary citations that are missing; (3) remove any irrelevant citations that are present.\nTo fine-tune an LLM to develop the aforementioned abilities, we first need to construct training data. The most straightforward idea is to create a set of responses with poor citation quality, each paired with a corresponding response that has perfect citation quality. We attempt to use the answers from WebGLM-QA dataset as positive responses and generate negative responses by randomly adding or deleting citations. Unfortunately, this approach proved ineffective, primarily because the citation quality in dataset is not high enough. An evaluation of the dataset's answers revealed that the citation recall and citation precision"}, {"title": "4.2 Results", "content": "In this section, we fine-tune a Mistral-7B model [30] to serve as the refiner. The main results are summarized in Table 4,5,6, and we have the key observations as follows.\nFirst, whether through few-shot or fine-tuning, the responses generated by the model can achieve significant improvements in citation quality after refining. All models show improvements in citation F1 across three datasets. In the few-shot experiments on WebGLM-QA, Llama2 series models initially perform poorly but achieve improvements of 22.18% and 23.73% respectively with the help of the refiner, narrowing the performance gap with other models. On the other two datasets, Llama2 series models also achieved improvements of nearly 29% and 18%.\nSecond, refiner exhibits excellent generalization. Previous experiments found that fine-tuning method doesn't exhibit strong generalization, as the model's capabilities are constrained by data distribution. For instance, the fine-tuned LLMs don't perform as well on ASQA compared to few-shot methods. However, after refining, all six fine-tuned LLMs achieved over a 20% increase in citation F1 on ASQA. Except for Qwen2-7b, the other five fine-tuned models surpassed the results of few-shot. These results indicate that changes in the distribution of data have minimal impact on the refiner.\nThird, refiner primarily enhances citation quality through improved citation precision. In many results, citation recall has decreased actually, but the increase in citation precision is substantial. For example, the fine-tuned glm4-9b model achieved a staggering 40% increase in citation precision on ASQA. This illustrates that the refiner effectively captures the relationship between statements and references, accurately determining whether a reference truly supports the response."}, {"title": "4.3 Additional Evaluation", "content": "To further demonstrate the effectiveness of our proposed Generate-then-Refine method, we proceed with additional evaluations. We need to confirm whether the improvement in citation quality is genuine or merely aligned with NLI model's preferences since we use an NLI model to determine the gold citation while constructing the training data for the refiner and also use NLI model in the evaluation.\nThus, we replace NLI model with GPT-3.5-turbo when measuring citation recall and citation precision. We request GPT to output 'Yes' only if it believes the cited references supports the statement, otherwise output 'No'. We conduct experiments with Llama2-7b and Llama2-8b models on the test set of WebGLM-QA."}, {"title": "5 Conclusion", "content": "In this work, we comprehensively evaluate the ability of the latest LLMs to generate citations in their responses. We introduce new citation evaluation metrics to address shortcomings in the existing evaluation framework. To improve the citation quality in LLMs' responses, We propose Generate-then-Refine method, which fine-tunes a model to serve as a refiner. Our experiments show that our method substantially improves the quality of citations."}]}