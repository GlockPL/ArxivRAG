{"title": "Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis", "authors": ["Mikel Zubillaga", "Oscar Sainz", "Ainara Estarrona", "Oier Lopez de Lacalle", "Eneko Agirre"], "abstract": "Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.", "sections": [{"title": "1. Introduction", "content": "Event Extraction (EE) is one of the fundamental tasks in Information Extraction (IE) and aims to extract event mentions and their arguments (i.e., participants) from text. Typically, EE involves the identification of trigger words (e.g. married, the attack) that denote a mention of an event or action. In parallel, the entities in the sentence are extracted. In a final step, once the event is known, the participants that take part in the event are identified in the context. Due to the complexity involved, and the high interest in the task, EE has been historically one of the most relevant tasks in the field of Information Extraction.\nInformation Extraction tasks in general and EE in particular pose significant challenges, as it is a complex task that demands humans to meticulously follow complicated guidelines, often riddled with numerous exceptions. To tackle this challenge, the conventional approach is to train computational models with large amounts of annotated examples. Obtaining the examples entails extensive manual effort by domain experts, making it impractical for situations with limited resources, especially for low-resource languages.\nDue to the recent advances in Natural Language Processing, Large Language Models (LLMs) are capable of transferring knowledge across languages, i.e. training in one language and performing inferences in another. This is referred to as Cross-Lingual Transfer Learning and has represented a significant advancement for languages other than English, as it allows to obtain EE models using data from high-resource languages (primarily English). The success of this approach allowed to develop ambitious programs, such as BETTER, where English data is provided for training while the models are tested on other languages. While the proposed task was interesting and really challenging, it only uses English data for training. In recent work, Pouran Ben Veyseh et al. developed MEE (Multilingual Event Extraction), an extension of the well-known ACE 2005 dataset to 8 languages.\nOne limitation of current EE research is the under-exploration of non-English languages, due mainly to the lack of high-quality multilingual dataset. MEE allows for such kind of research, and we thus use MEE to explore whether the typology of target and source languages impacts cross-lingual transfer capabilities. In particular, we study what would be the best transfer choice to develop an IE system for a language with no training data.\nIn order to increase the typological diversity of languages in MEE, we added Basque, a language isolate with no known related language. The"}, {"title": "2. Related Work", "content": "Event Extraction. Early methods addressed the task by defining human-crafted features and applying rules. These methods were replaced by deep learning approaches in the last decade. Soon, sequence labeling became the standard approach for EE. With the development of pre-trained Large Language Models, several works reformulated the task into language understanding tasks such as Question\nCross-lingual approaches for IE Pre-trained LLMs allowed a simplified approach to cross-lingual IE with state-of-the-art performance. Previously, state-of-the-art consisted of using parallel data to project labels from one language to the other. Related to this, the improvement of machine translation and alignment models allowed effective augmentation of training examples. In this paper, we choose to use a multilingual sequence labeling approach to efficiently analyze the characteristics of cross-lingual transfer learning.\nExisting datasets for Event Extraction are mostly available only for English, such as CySecED , CASIE, LitBank, MAVEN, RAMS and, WikiEvents among others. Additionally, there are a few multilingual EE datasets like ACE 2005 and more recently BETTER and MEE. ACE 2005 and BETTER include only English training data. MEE contains annotated train and evaluation datasets in eight languages. In this work, we follow ACE 2005 and MEE guidelines, and annotate a Basque Event Extraction dataset to perform our experiments."}, {"title": "3. EusIE: Basque Event Extraction", "content": "In this section, we present EuslE (Euskarazko Informazio-Erauzketa), which is the first EE dataset for Basque. We decided to extend the Multilingual Event Extraction (MEE) dataset ( Pouran Ben Veyseh et al., 2022)) by following the well-known ACE05 ontology. The MEE dataset covers 8 diverse languages that we use in our experiments in conjunction with Basque.\nAlthough the dataset creation process followed similar steps to MEE, few modifications were implemented. On one hand, due to the difficulty of finding Basque-speaking crowd workers, two native experts annotated the dataset. On the other hand, due to our small budget, we limit the annotation to the development and test splits. This way we provide quality over quantity. In the following sections, we describe the data collection, filtering, and annotation process.\n3.1. Data collection\nWe collect the initial set of documents from a snapshot of Basque Wikipedia. From the initial set, we select the documents related to events (Gertaerak category) that were labeled as part of the following topics: Economy (Ekonomia), Politics (Politika), Technology (Teknologia), Natural Disasters (Hondamen Naturalak), Military (Militarrak) and Crimes (Krimenak). We keep the same topics as the original MEE to avoid domain shifts.\nAfter collecting the documents, we removed the markup from the documents using WikiExtractor. Additionally, section titles and other structural information were removed too. We split the documents into sentences, and, we tokenized them using IXA-pipes an NLP toolkit designed for Basque. Similar to MEE and RAMS , we grouped 5 sentences to form an annotation segment. The segment is our annotation boundary, and thus the relations between the events and arguments can occur within the sentence as well as cross sentences, but always inside the segments.\n3.2. Annotation schema\nThe annotation schema used to annotate the dataset is shown. We adapted the schema used in MEE to include entity types that could potentially be argument candidates for the defined events. We included OBJ to categorize entities that are candidates for the Artifact role. We do not have examples for the NUM entity type, as all numerical mentions could be labeled either with DATE or MON.\n3.3. Annotation\nWe annotated a total of 300 segments (1500 sentences) and divided them into 150 for development and the rest of 150 for testing. That is, we annotated a similar amount of segments provided in the evaluation partition of the MEE dataset. The"}, {"title": "4. Experimental Setup", "content": "In this work, we explore the cross-lingual capabilities of the multilingual Language Models in EE for the Basque language. We deploy the aforementioned MEE and EuslE datasets. Typically, EE is a sequence of three tasks that are evaluated as a pipeline, reporting the final F1 results. As we want to compare the transfer qualities of each language empirically in each of the pipelined steps, we reported the F1 scores for each task (entity, event, and argument extraction). All the tasks are evaluated independently using gold annotations from the previous step in the pipeline . Additionally, three different runs are executed for each configuration\nin order to provide average and deviation scores. We organize the experiments in three main parts. First, we try to replicate the in-language experiments performed in where models are evaluated and trained in the same language. We use their baseline approach (called Pipeline in the paper), trained and evaluated in the original data splits provided by the authors. Next, we run the cross-lingual experiments, where we train models in one language and evaluate them in Basque (EusIE). For fair comparisons across languages, all languages have the same number of train examples, i.e. we discard examples in the languages with most training data. More details in Section 4.2. Finally, we run an analysis of linguistic features to gain insight into what makes a language good for cross-lingual transfer-learning. We will categorize each language based on its typological features, and perform all-vs-all experiments to analyze the impact of those features.\n4.1. Model\nAs mentioned above, EE is typically divided into entity detection, event detection, and, event argument extraction. Therefore, we train three models, one per each task. As shown , we formulated all the tasks as sequence labeling problems. Both, entity and event detection tasks are simply formulated as predicting the label for each token in the input text. For the event argument-extraction task, however, the output must be conditioned on the event to analyze. As we indicate , we surround the event trigger with markers, $$$ in our case, and label only the corresponding arguments. The backbone language model is the base version of XLM-RoBERTa. Regarding the hyperparameters, we set their values based on a few preliminary experiments. The overall best performing hyperparameters were a learning-rate of 5e-5, 32 for the batch-size and a weight decay of 1e-3. We run the models for 64 epochs, as we found out that the F1 score was increasing in the development even if the loss was increasing too.\n4.2. Comparable Training Size\nIn order to compare the cross-lingual transfer capability of the languages in a comparable manner , we try to control the size of the training data. We thus equalize the amount of training data for all the languages. That is, we remove training examples from larger languages until all the languages contain the same amount of annotations. Note that we performed the under-sampling by counting annotations and not the number of segments, as the latter could"}, {"title": "5. Results", "content": "In this section, we discuss the results obtained in the experiments. First, similar to the MEE authors, we report the results using the original splits. Second, we report the results obtained on the EuslE benchmark. Finally, we explore the effect of scaling training data.\n5.1. Result on In-language Scenario\n shows the F1 scores obtained in each language. Additionally, we included the All row, which represents a model trained and tested using all the languages available in the dataset. We repeated each experiment 3 times and reported the average F1 score and the standard deviation for each setting. Language-wise comparisons do not show any clear pattern in which we can distinguish a particular language that overperforms the rest of the languages across all the tasks. A language that performs strongly in a specific task, shows poor performance in the other task (e.g. Spanish shows an outstanding 84.5 of F1 in entity detection, whereas it is far from top results in Argument identification).\nIt is important to note that our results substantially deviate from the ones reported in the original paper . Discussion with the authors did not reveal any reason for this difference apart from the use of a different model. However, we found that the results obtained by our system correlate better with the number of annotations in the training set, as shown in Figure 1. The linear correlation of our model is plotted with dashed lines, the original ones with continuous lines. In particular, for event and argument detection, in which the number of annotations is much smaller compared to entity detection, our system linearly improved when we increased the number of annotations. In the case of entities, as we have more annotations, it does not show a significant positive relationship with the number of annotations (both systems show similar behavior in this case).\n5.2. Results on EusIE\n shows the results for the models trained on each language when evaluated in Basque. Note that, in this experiment, all the languages have the same amount of training examples. The best-performing language varies across tasks. We had hypothesized that the best results would be for Spanish, as it is an official language in the Basque Country and the contact between the two languages has been happening since Spanish became a language on its own, but that is not the case.\nRegarding entity detection, results and the standard deviation show that English, Portuguese, and Polish obtain very similar results and outperform the rest of the languages, whereas Spanish, Turkish, and Hindi are close to the best results. The results obtained with Japanese and Korean, lag significantly behind the rest by a large margin. We observe a similar pattern for event detection"}, {"title": "5.3. Data scaling results", "content": "We showed that the results on the event argument extraction are significantly lower. We hypothesized that this is due to the insufficient amount of the training data to learn properly the task. The fact that using all languages for training nearly doubles the best results for argument extraction is some evidence in this direction.\nTo study the effects of the training size in the cross-lingual setting, we trained new models with different amounts of data as shown in Figure 2. Here, the x denotes the initial training amount (1,416 instances), the same as used . We scaled the x by 2, 4, and, 8 when possible.\nResults in Figure 2 confirm our hypothesis: argument extraction is more challenging and requires more data to properly model the task. The figure shows how fast languages scale the performance with more data. Despite using fewer languages, we can see that languages scale at a different pace with the training data. For example, a comparison of English and Portuguese reveals that while both perform very similarly on the initial values, Portuguese scales much slower in the long term.\nSignificantly English x8 outperforms the results attained with All by a large margin. Note that the amount of training data used is the same as the All results but using only monolingual data. Together with the results for entities shown in the previous section, we can conclude that training size is a relevant factor in cross-lingual transfer and that mixing the data from all languages is beneficial only for smaller training sizes."}, {"title": "6. Analysis according to Language Typology", "content": "From the results on EuslE, we can draw two main conclusions: (1) There is no dominant language across tasks, and, (2) A language that is effective in a particular cross-lingual task does not guarantee that it will be good for another. For further understanding, we run an analysis on the following hypotheses:\n1. Similar languages should benefit more from cross-lingual transfer.\n2. Different tasks require different skills. The tasks of detecting entities and events require more lexical knowledge, whereas extracting arguments requires a more structural understanding of the text.\nFocusing only on Basque as the target language would be limiting. So we decided to carry out the same experiments we did for Basque, but in this case, running the cross-lingual experiments for all\nMorphology refers to the study of words and how they are formed. We categorized each language into Agglutinative or Fusional categories. Our initial guess is that languages with similar morphology should perform better on tasks requiring more lexical knowledge (Entities and Events).\nMorphosyntactic alignment refers to the study of the relationship between different arguments of verbs. We categorized the languages into Nominative-Accusative, Ergative-Absolutive, and Split-Ergative. As it is directly related to how the event arguments are marked in the sentence, we guess that languages with similar morphosyntactic alignment will perform better. Note that most languages are categorized as Nominative-Accusative, making it difficult to measure the effects of this feature.\nWord order refers to the order of the syntactic constituents of a language. We categorized the languages into Subject-Object-Verb (SOV) or Subject-Verb-Object (SVO) categories. An important consideration is that Basque usually follows the SOV order, however, but also allows other orders depending on pragmatic factors. The word order has a significant impact when defining the different roles each part of the sentence has with respect to the verb, and therefore to an event. We guess that it would positively affect the event argument extraction task.\nScript refers to how the language is written. This is particularly important because it affects directly the tokenizer of the model, and therefore, how the token is defined and how the information is stored in the model we define very broad categories, in which we distinguish languages into two main groups: Latin based and non-Latin based. Our guess is that the script should affect all the tasks, as it is an important feature that impacts directly the model architecture. Although, with the same script, words from different languages might be better or worse represented depending on how the tokenizer was constructed.\nGeographical location refers to where the language is spoken. It is impossible to determine a concrete geographical location for a language, as nowadays a lot of languages are spoken all around the world, particularly English, Spanish, and Portuguese. However, for simplification purposes, we will consider them as languages spoken mainly in the west of Europe. The location of a language has a great cultural impact, and therefore, different entities appear more frequently in text on one language than another. This feature, however, is highly correlated with the script feature, as geographically adjacent languages tend to have similar scripts. We think this feature may have a greater impact on lexical tasks."}, {"title": "6.2. Results of the analysis", "content": "To analyze the effect of each typological feature, we ran the same cross-lingual experiments as we did with Basque, but in this case, running all possible source-target language combinations. The results are shown. Based on these results we run multiple regression analysis for each task separately and use the resulting coefficients to measure the relative contribution of each linguistic feature described above.\nWe prepare the data in order to correctly perform the analysis. First, we normalized the results across target languages as we noticed that the values on each target language ranged very differently\nWe formalize the linear regression analysis as shown in Equation 2, where s is the normalized F1 score for each language and task, and $w_i$ is the coefficient that measures the relative contribution of feature i. As additional information, distribution of normalized F1 scores for each linguistic feature in isolation.\n\n$s = w_0 + \\Sigma^{|F|}_{i=1} w_i f_i$\n\nFigure 3 shows the contribution of each linguistic feature to performance in each task. Script turns out to be the most relevant feature across tasks. Morphology is an important feature in entity and event detection, where lexical information could play a significant role, as we hypothesized. The importance of these two features might be due to the fact that languages with the same script and morphology share more tokens in the language model vocabulary. On the other hand, Word Order, as we hypothesized, affects significantly the argument extraction task. Surprisingly, Geo-location is not relevant when transferring knowledge from one language to another, it is well-known that geographically close languages share many lexical entries. Therefore, the relevance of geo-location is low probably because most of the correlation is explained with Script. As expected we did not find any correlation for Morphosyntactic alignment, probably because all languages but one shared the same feature.\nAll in all the analysis shows that sharing the script is a key factor in all three tasks and that sharing the script might overshadow the relevance of sharing the geographical location. As we hypothesized, morphology is relevant for the two tasks which are more lexical (entity and event extraction), while word order turns out to be relevant for the more syntactic argument extraction."}, {"title": "7. Conclusions", "content": "In this paper we explore the contribution of different training languages when transferring into other languages, presenting a set of exhaustive experiments on three Event Extraction tasks and eight languages with different language typological features. In a first experiment with Basque as a target, we see that there is no clear pattern. In a subsequent experiment, we performed a typologically motivated correlation analysis over all the language combinations and concluded that transfer quality does correlate with some linguistic features, which change depending on the task. For entity and trigger identification sharing the script and morphological typology between source and target languages are the two most relevant features. In argument extraction sharing word order and script play the most relevant roles. In addition, we show that source languages scale differently as we increase training data. Finally, we present the first Basque Event Extraction evaluation benchmark, which was used alongside the MEE dataset in all experiments.\nFor the future, we would like to extend the analysis to more tasks and languages, as well as taking into account other alternative typological features. A better understanding of the interactions between typology and cross-lingual transfer opens a new research avenue that can be beneficial for low-resource languages."}]}