{"title": "Accelerating Codec-based Speech Synthesis with Multi-Token Prediction and Speculative Decoding", "authors": ["Tan Dat Nguyen", "Ji-Hoon Kim", "Jeongsoo Choi", "Shukjae Choi", "Jinseok Park", "Younglo Lee", "Joon Son Chung"], "abstract": "The goal of this paper is to accelerate codec-based speech synthesis systems with minimum sacrifice to speech quality. We propose an enhanced inference method that allows for flexible trade-offs between speed and quality during inference without requiring additional training. Our core idea is to predict multiple tokens per inference step of the AR module using multiple prediction heads, resulting in a linear reduction in synthesis time as the number of heads increases. Furthermore, we introduce a novel speculative decoding technique that utilises a Viterbi-based algorithm to select the optimal sequence of generated tokens at each decoding step. In our experiments, we demonstrate that the time required to predict each token is reduced by a factor of 4 to 5 compared to baseline models, with minimal quality trade-off or even improvement in terms of speech intelligibility.", "sections": [{"title": "I. INTRODUCTION", "content": "Text-to-Speech (TTS) [1] has garnered significant attention within the research community. Amongst the various approaches, deep neural networks have shown considerable advancements in modeling natural speech [2]\u2013[7]. Recent models based on this framework have shown the potential to generate fluent speech and replicate a speaker's voice only using a few seconds of audio input [7]\u2013[16]. Furthermore, LMs also possess outstanding ability on several speech task such as recognizing emotions, identifing speakers, enhancing speech, or even understanding the complex context from background sounds [16]-[19]. This ability is important to create an unified model that can naturally communicate with humans, reflecting emotion, accent, and style [18].\nThe LM-based TTS model exhibits outstanding generative ability due to the capacity of the Transformer, as described by the scaling law [20], the advanced development of audio tokenizers [15], [21]-[23], and the avalability of large speech dataset [24]-[27]. This class of TTS models can be viewed as an extension of large language models, where the input speech is considered as a new language. The vocabulary of this new language is the set of discrete representations, also referred to as tokens, which are generated by encoder of audio codec. The TTS model then leverages its sequence generation ability to generate sequences of tokens that are transformed back to waveforms by the decodec module [7], [11] or the vocoder module [28]-[31]. Since speech itself also has linguistic content in the form of a sequence, a promising approach is to harness the power of pretrained LLMs on text-only domains [17], [32], [33].\nAlthough AR inference brings significant improvements to codec-based speech synthesis, however it substantially reduces the generation speed of TTS systems. This slowdown is a notable drawback compared to other types of speech synthesis models. For example, non-AR models [4], [5] generate an entire sentence in a single step, and models using progressive inference require only a few function evaluations (NFE) to produce high-quality speech output [6], [34], [35]. In contrast, models using AR inference with a language model (LM) architecture must generate sequences step by step, with the number of steps being equal to the sequence length. Consequently, as the sequence length increases, the time required for generation also increase. This problem is compounded by the fact that the computational complexity of Transformer-based models increases quadratically with sequence length, leading to an increase in the total floating-point operations per sequence.\nIn this paper, we introduce a multi-token prediction method that significantly reduces the NFE while maintaining comparable quality. Building on the observation that consecutive speech tokens are often similar, our architecture is trained to predict multiple future tokens simultaneously, rather than just one at a time. Additionally, we propose a Viterbi-like approach to capture the statistical relationships between the predicted tokens. A key advantage of our method is that it allows users to explicitly control the quality-speed trade-off by adjusting the number of future tokens predicted per function evaluation, without the need for re-training or fine-tuning the model. To our knowledge, this is the first instance of Viterbi-based speculative decoding in this context."}, {"title": "II. RELATED WORKS", "content": "Neural Audio Codec: A codec model consists of an encoder, a decoder, and multiple codebooks that are quantized from the latent space of the encoder. These models are often built on the RVQ-GAN framework [21], [22] which provide flexible bitrate for the task of audio compression. Recently, several approaches have been proposed for designing an optimal discrete representation space. Early works focused on improving compression and reconstruction quality [7], [21], [22]. As codec-based TTS systems have demonstrated strong performance, more recent efforts have focused on developing factorized and distinguishable features for each codebook [13], [15], [23], [36]\u2013[39]. Moreover, current studies [36], [40]-[42] introduce codecs for mel-spectrograms, significantly increasing token compression rates and leading to faster AR module inference.\nMulti-tokens prediction for AR module: Multi-token prediction is not a new concept and can be related to speculative decoding [43]\u2013[45]. These methods use a smaller draft model to generate an initial token sequence, which is then refined by the larger or original model for a coherent continuation. Recent works explore this approach in large language models (LLMs) like incorporating techniques to reduce hardware memory requirements [46] or using tree search algorithms to enhance the quality of the output sequence [45]. To our knowledge, the most related work to ours is VALL-E 2 [8], but the multi-token prediction methods used in these two approaches are fundamentally different. Additionally, our work not only offers explicit, training-free control over the quality-speed trade-off but also improves the relationship between the predicted tokens through a Viterbi-like algorithm."}, {"title": "III. METHOD", "content": "The popular codec-based speech synthesis model architecture comprises two main modules: an auto-regressive (AR) module and a non-auto-regressive (NAR) module, following the VALL-E framework [7]. Specifically, given a training dataset with pairs $s,p$, where $s$ represents the speech signal and $p = \\{p_1,p_2,...,p_T\\}$ is the corresponding phoneme sequence of length $T$, the codec model compresses the speech signal into discrete tokens $A$ using eight quantizers: $Codec(s) = A^{8 \\times L} = \\{a_1,a_2,...,a_8\\}$, where each $a^i \\in \\{a_1^i, a_2^i, ..., a_{|V|}^i\\}$. In models using the VALL-E architecture, the AR module is responsible for predicting $a_1$, while the NAR module regresses the entire sequence $a^{2:8}$.\nSpecifically, for the discrete token sequence $a^1$, an auto-regressive Transformer Decoder-only $\\theta_{AR}$ is trained to predict the next tokens based on a text prompt $p$, and an acoustic prompt condition $\\tilde{a}^1$ extracted from the reference audio, and all previously predicted tokens.\n$p(a^1|p,\\tilde{a}^1;\\theta_{AR}) = \\prod_{t=0}^{L}p(a_{t+1}|a_{<t}, a^1, p; \\theta_{AR})$ (1)\nSince all data are discrete, we concatenate them all into a single sequence without specific tokens to distinguish them. For tokens of the remain layers $a^{2:8}$, we train a NAR LM $\\theta_{NAR}$ to iteratively predict each token sequence. Each prediction process is conditioned on phoneme sequence $p$ and generated token sequence of previous layers $a^{<i}$.\n$p(a^{2:8}|p, \\tilde{a}; \\theta_{NAR}) = \\prod_{j=2}^{8}p(a^{j}|a^{<j}, \\hat{p}(a^1|a_{\\leq i}, \\tilde{a}, p; \\theta_{NAR})$ (2)\nMultiple Tokens Prediction (MTP)\nWe take inspiration from [45], [46], which utilizes parallel decoding to accelerate the decoding process of LLM. We divide the AR module into two parts: first, encode all previous tokens in to latent space $p(z_{1:t}|a_{1:t}, \\tilde{a}, p; \\theta_{AR})$, then, using this latent $z_{1:t}$ to predict a few n consecutive future tokens $p(a_{t+1:t+n}^1 | z_{1:t})$, by n distinguished heads. This idea leads to a new negative log-likelihood objective function of the whole AR module with multiple heads:\n$\\mathcal{L} = - \\sum_{t} \\sum_{i=1}^{n} log P(a_{t+i}^1 | z_{1:t}) P(z_{1:t}|a_{1:t}, \\tilde{a}, p)$ (3)\n$\\geq - \\sum_{t} log P(a_{t+1:t+n}^1 | z_{1:t}) P(z_{1:t}|a_{1:t}, \\tilde{a}, p)$ (4)\n$= - \\sum_{t} log P(a_{t+1:t+n}^1 | a_{1:t}, \\tilde{a}, p)$ (5)"}, {"title": "IV. EXPERIMENTS", "content": "We conduct experiments on the LibriTTS [27] corpus, using the all train subsets for training, and 'test-clean' for evaluation, following [13]. We also use SpeechTokenizer to extract audio tokens all experiments. For fair comparison, all models are trained from scratch. Both AR and NAR modules are optimized separately for 20 epochs on a single A6000 GPU, with a learning rate of 0.05 and 200 warmup steps.\nWe integrate our method into state-of-the-art models, VALL-E [7] and USLM [15], and evaluate their zero-shot performance using various metrics. For content consistency, we compute Word Error Rate (WER) and Character Error Rate (CER) with a Conformer-Transducer speech-to-text system. Speaker similarity is assessed via cosine similarity of x-vector embeddings between prompts and generated utterances. We also conduct SMOS and MOS surveys for speaker similarity"}, {"title": "V. CONCLUSION", "content": "In this work, we propose an inference method that combines multi-token prediction with Viterbi-based speculative decoding, significantly accelerating the AR module during inference. Our approach not only achieves faster inference but also maintains, or even surpasses, baseline performance in terms of generation quality. Additionally, the model is trained only once, yet offers explicit control over the trade-off between quality and speed. However, exploring the compatibility of more tokenizers with Viterbi-based speculative decoding is a promising direction for future work."}]}