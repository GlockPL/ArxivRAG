{"title": "Parameter Symmetry Breaking and Restoration Determines the Hierarchical Learning in AI Systems", "authors": ["Liu Ziyin", "Yizhou Xu", "Tomaso Poggio", "Isaac Chuang"], "abstract": "The dynamics of learning in modern large AI systems is hierarchical, often characterized by abrupt, qualitative shifts akin to phase transitions observed in physical systems. While these phenomena hold promise for uncovering the mechanisms behind neural networks and language models, existing theories remain fragmented, addressing specific cases. In this paper, we posit that parameter symmetry breaking and restoration serve as a unifying mechanism underlying these behaviors. We synthesize prior observations and show how this mechanism explains three distinct hierarchies in neural networks: learning dynamics, model complexity, and representation formation. By connecting these hierarchies, we highlight symmetry a cornerstone of theoretical physics as a potential fundamental principle in modern AI.", "sections": [{"title": "1 Introduction", "content": "More and more phenomena that are virtually universal in the learning process have been discovered in contemporary AI systems. These phenomena are shared by models with different architectures, trained on different datasets, and with different training techniques. The existence of these universal phenomena calls for one or a few universal explanations. However, until today, most of the phenomena are instead described by narrow theories tailored to explain each phenomenon separately often focusing on specific models trained on specific tasks or loss functions and in isolation from other interesting phenomena that are indispensable parts of the deep learning phenomenology. Certainly, it is desirable to have a universal perspective, if not a universal theory, that explains as many phenomena as possible. In the spirit of science, a universal perspective should be independent of system details such as variations in minor architecture definitions, choice of loss functions, training techniques, etc. A universal theory would give the field a simplified paradigm for thinking about and understanding AI systems and a potential design principle for a new generation of more efficient and capable models.\nLearning phenomena in deep learning can be roughly categorized into three types, each capturing a different kind of hierarchy hidden in neural networks:\n\u2022 Hierarchy of Learning Dynamics: distinct temporal regimes arise during learning this includes abrupt complexity jumps (Simon et al., 2023; Jacot et al., 2021; Abbe et al., 2023), progressive sharpening and flattening (Cohen et al., 2021), and beyond-linear dynamics of training (Zhu et al., 2022);\n\u2022 Hierarchy of Model Complexity: the functional complexity of models adapts to the target function this is exhibited in simplicity biases (Kalimeris et al., 2019), compressive coding through the information bottleneck (Tishby et al., 2000; Tishby & Zaslavsky, 2015), and the \u201cblessing of dimensionality\" in overparameterized nets (Galanti et al., 2021; Zhang et al., 2017);\n\u2022 Hierarchy of Neural Representation: distinct spatial structures arise in the layers of neural networks, with progressively deeper layers tending to encode increasingly abstract information - this is evidenced in the structured representations such as neural collapse (Papyan et al., 2020), hierarchical encoding of"}, {"title": "2 Parameter Symmetry in Deep Learning", "content": "Definition 1. Let G be a linear representation of a group. We say that there is a G-parameter symmetry in the model f(0,x) if \u2200g\u2208G and \u2200x, f(0,x) = f(g0,x)."}, {"title": "3 Learning Dynamics is Symmetry-to- Symmetry", "content": "Symmetry has a direct influence on the loss landscape, and it thus affects the learning dynamics of neural networks through its effect on the landscape (Tatro et al., 2020; Lim et al., 2024). One primary effect of symmetry on the loss landscape is that it creates extended saddle points from which SGD or GD cannot escape (Li et al., 2016; Chen et al., 2023). Meanwhile, it has been found that when a neural network is initialized with small norm weights, its learning dynamics is primarily saddle-to-saddle (Jacot et al., 2021). In fact, neural networks have been found to converge often to saddle points (Alain et al., 2019; Ziyin et al., 2023). Given that symmetries are the primary origins of saddle points, it is natural to hypothesize that the learning dynamics of neural networks are not only saddle-to-saddle but symmetry-to-symmetry:\nDynamics Hypothesis: The learning dynamics of neural networks are dominated by jumps be-tween symmetry groups, with parameters going from a larger to a smaller group (symmetry breaking)or from a smaller to a larger group (restoration).\nAs Theorem 1 in the next section shows, at a G-symmetric solution, the number of effective model parameters is reduced by a number that matches the rank of the group. This means the change between symmetries naturally induces a change in model complexities. Therefore, these symmetry-to-symmetry jumps not only in terms of the loss function value but also in terms of complexity jumps.\nDefinition 2 can quantify the breaking and restoration of symmetry. We define the symmetry-breaking distance as:\n$\u0394^{G}=||\u03b8 \u2013 P_{G}\u03b8||^{2}$.\nWhen $\u0394^{G}> A^{G}$ for some threshold $A^{G}$ ($\u0394^{G}$ = 0.05 ~ 0.2 in experiments), we say the G-symmetry is broken.\nWe care about how many symmetries are broken for a given layer, so we can count the number of such large $A^{G}$. For example, for permutation symmetry in a fully connected layer, the group-invariant projection is the average of the input and output weights of any pair of neurons i and j. One can identify each pairwise distance as a different $A^{G}$, which we will denote as Aij throughout this work. We only need to count the number of neighboring neurons with a large $\u0394^{G}$ because they form a generating set of the symmetric group we refer to this number as the degree of symmetry Ndos. The difference between Ndos and the number of neurons is the degree of symmetry breaking (Ndosb). See Section A.1 for details on measuring this quantity."}, {"title": "4 Symmetry Adaptively Limits Model Complexity", "content": "Another important implication of symmetries is that they control the effective number of parameters. When a system is symmetry-broken, new degrees of freedom usually emerge (known as a Goldstone mode in physics (Peskin, 2018)), and the system becomes higher-dimensional and more complex. When it is symmetry-restored, the effective dimension is reduced. This means that the symmetry boundaries naturally correspond to boundaries of different hierarchies of model capacities. This idea has a place in machine learning. For example, equivariant networks can improve the sample efficiency of training, and existing equivariant networks almost always involve introducing new parameter symmetry to the neural network (Maron et al., 2018; Bronstein et al., 2021). In Bayesian learning, parameter symmetry (and its generalizations) have been found to directly determine the generalization scaling of the model (Watanabe & Opper, 2010). Prior works also indicate how neural networks may break symmetries to adapt to different tasks (Fok et al., 2017).\nMore broadly, combining the idea that symmetry classes have different model complexity and the common observation that SGD tends to learn a function whose complexity is proportional to the complexity of the target function (Kalimeris et al., 2019; Mingard et al., 2025), it is natural to arrive at the following hypothesis:\nComplexity Hypothesis: Symmetry adaptively controls the model's capacity. The model con-verges to a symmetry class whose complexity matches the complexity of the target.\nThe direct correspondence between symmetry and model capacity has been justified by a recent result: at a G-symmetric state, the effective model dimension decreases by exactly rank(PG) throughout training. Moreover, in the NTK limit, this reduction in parameter dimension implies a selection of input features, and being at a symmetric solution directly affects the input-output function map.\nThus, symmetric solutions are low-capacity states from which gradient-based training methods cannot escape. More importantly, these symmetric solutions are preferred solutions when weight decay is used (Ziyin, 2024) or if the minibatch noise is strong due to a mechanism called \u201cstochastic collapse\" (Chen et al., 2023).\nParameter Symmetry as an Occam's Razor In other words, parameter symmetries in the model may function like an Occam's razor, where simpler and low-complexity solutions are favored an observation that has been made across almost all modern neural models, whose generalization performances are independent of the size of the model (Zhang et al., 2017; Galanti et al., 2023), which is in discrepancy with common generalization bounds that predict a \u221a deterioration with respect to the width 4 (Neyshabur et al., 2018).\nHere, we raise an insightful conjecture about the quantification of complexity control due to permutation symmetry, where y is the weight decay, \u03b7/S is the learning-rate-batch-size ratio. Recall that the distance between two neurons is Aij.\nConjecture 1. (Space Quantization Conjecture, Informal) In every layer with permutation symmetry, for two nonidentical neurons i and j, \u2206ij > O(\u03ba\u00b3) after training, where \u1e9e > 0 and \u043a is the regularization strength."}, {"title": "5 Representation Learning Requires Parameter Symmetry", "content": "Representation learning is believed to be the most essential aspect of deep learning (Bengio et al., 2013). Learned representations of neural networks are found to take almost universally hierarchical forms, where earlier layers encode a large variety of low-level features and later layers learn a composed and abstract representation that is invariant to the changes in the low-level details (Zeiler & Fergus, 2014). One reason why symmetry may serve as a driving mechanism for learning these structured representations is that these structures almost always involve compressing information onto a few neurons and come with a low-rank structure in the hidden layer (Alain, 2016; Masarczyk et al., 2024; Xu et al., 2023; Papyan et al., 2020), and a primary low-rank mechanism for deep neural networks is through the permutation symmetry of the layer or the rescaling symmetry of ReLU neurons (Fukumizu, 1996; Fukumizu & Amari, 2000; Ziyin, 2024). As a primary example, neural collapses (NC) often happen in an image classification task, where the inner-class variations are found to disappear in the last and intermediate layers of the neural network (Papyan et al., 2020), resulting in a representation whose rank matches the number of classes. This implies that the network has learned a hierarchical representation, where, in the later layers, only high-level features are encoded.\nThese results motivate the following hypothesis:\nRepresentation Hypothesis: Learning invariant, hierarchical and universal latent representations requires parameter symmetry.\nInvariant and Hierarchical Representation Learning. Here, we train a standard ResNet18 on CIFAR-10, which is known to exhibit NC. In comparison, we also train a ResNet18 whose symmetries have all been removed using the method proposed in (Ziyin et al., 2025b). We see that af-ter removing permutation symmetries, the innerclass variation of representations no longer vanishes. In Section A.4, we show that the spectral gap between class means and innerclass variations also becomes smaller.\nMore broadly, it is important to understand how representations build up as the input data passes through layers of a neural network. It seems likely that there are at least three distinctive regimes in the layers of a trained net: the first few layers of neural networks serve as an expansion phase where the representation becomes linearly separable (Alain, 2016), which requires the layer to be wide and implies a high rank (Nguyen et al., 2018); then, a \"reduction\" phase happens where the irrelevant information is thrown away and the neurons encode more and more compact information (Xu et al., 2023; Rangamani et al.,"}, {"title": "6 Mechanism and Control", "content": "Mechanism: A primary known mechanism is regularization in explicit or implicit forms. Since symmetry breaking is just the lack of symmetry restoration, we may focus on symmetry restoration. In explicit form, a simple weight decay has been shown to universally turn symmetric solutions energetically favorable to symmetry-broken solutions (Ziyin, 2024), in a manner similar to phase transition in physics (Landau & Lifshitz, 2013). In implicit form, the stochasticity in SGD is known to lead to an \"implicit\" regularization effect, sometimes similar to that of weight decay (Kunin et al., 2021; Chen et al., 2023). Another mechanism is data augmentation, as it can also be seen as a form of regularization (Dao et al., 2019). While understanding these mechanisms is an open problem, the easiest way to study it is to consider the effective loss landscape (also known as the \"modified loss\") under SGD training (Geiping et al., 2021; Smith et al., 2021):\n$L(\u03b8) = E[L_{o}(\u03b8,x\u03b5)] + \u03b3||\u03b8||^{2} + \u03a4Tr[\u03a3(\u03b8)],$\nwhere Lo has the symmetry under consideration, \u2211 is the gradient covariance matrix due to SGD sampling, and T = n/S is the learning-rate-to-batch-size ratio. The data augmentation term can also be controlled by,"}, {"title": "7 Outlook and Alternative Views", "content": "In this work, we have argued and demonstrated that a wide range of seemingly unrelated hierarchies emergent in AI systems is actually related to, if not directly caused by, the symmetry of the trained models. Symmetry breaking and restoration are fundamental mechanisms in physics relevant to the dynamics of almost every scale of nature \u2013 from scattering of the quarks to the large-scale structure formation of the universe (Miller et al., 1990; Preskill et al., 1991). If different symmetries govern the universal laws at different scales of nature (Anderson, 1972), it is natural to hypothesize that it may also lead to universal mechanisms and laws of learning in both artificial and biological systems. We suggest three primary hypothetical mechanisms that govern three fundamental aspects of hierarchical learning of neural networks, and validating or falsifying their related hypotheses is the most important next step. From a broader perspective, that symmetry can play such important roles in learning also calls for more interdisciplinary study of intelligence from the perspective of physics.\nThere are certainly alternative views to our position.\nSymmetry alone is insufficient: In most of our examples, it is a combination of symmetry and some other effect that leads to the phenomenon. For example, the compression bias of SGD is a result of symmetry and training noise. Neural collapse is due to symmetry and regularization (Rangamani & Banburski-Fahey, 2022). Therefore, it is possibly the case that symmetry plus some form of explicit (weight decay) or im-plicit (noise, GD training, data augmentation) regularization is the dominating factor, and this may be an important direction of future research.\nSymmetry may not be necessary: While symmetry is possibly a unified perspective to view many phenomena, there are cases where a subset of these phenomena are exhibited but does not require symmetry. For theoretical purposes, this suggests that there can be other concepts as important as symmetry. However, from a design and practice perspective, knowing only one way to achieve a design goal often suffices - and the parameter symmetry may be sufficient. For example, there may be multiple ways for neural collapse to happen, but to introduce neural collapse to the desired model, one only needs to know one such way."}, {"title": "A.1 Measurement of AG", "content": "Permutation Symmetry For the permutation symmetry in fully connected layers, we have described the A for these pairwise symmetries. However, for a layer of width \u03c8, there are O(42) many such pairs, but we do not have to care about every pair of these because, for most of our purposes, we only care about how many neurons are actually functional and how many neurons are useless. This fact can allow us to reduce the number of measurements to 4. To achieve this, we first sort all the neurons according to the norm of the input and outgoing weights. Because for two neurons to become close, their norms also need to be close. Therefore, if two neurons have a large norm difference, their permutation symmetry must have been broken. Under this ordering, we measure the AG for the pairs of neurons with the closest norms.\nAn alternative perspective to look at this is that these pairwise neuron distances form a generating set of the symmetric group, and we are counting the number of symmetry breaking of these subgroups generated by each generator.\nOmission of G We will write AG as A throughout the appendix because we are often comparing A for different groups and so the superscript G is different for most cases.\nDouble Rotation Symmetry Computing the degree of symmetry for the double rotation symmetry is rather tricky. In the ViT experiment (Figure 3), we measured the degree of symmetry in the self-attention layers. Here, the symmetry is the double rotation symmetry:\n$W_{Q}W_{K} = W_{Q}MM^{-1}W_{K},$\nfor an arbitrary invertible matrix M. The symmetric states are the ones where Wo and WK both become low-rank in the same subspace. Namely, it happens when there exists a vector n such that\n$W_{Q}n = 0, W_{K}n = 0.$\nThere are at most k such n where k is the right dimension of W\u0119. This motivates this group's following oper-ational definition of \u2206. We first compute the eigenvalue decomposition of Wa W\u0119 to obtain its eigenvectors U:\n$W_{Q}W_{Q}^T= UAQUT.$\nWe then compute the following matrix:\n$AK = UTWKWKU,$\nand the degree of symmetry is defined as\n$Ndos = \u2211_{i}I_{\u0394_i<\\Delta_{th}},$\nwhere\n$\u0394i = |(AQ)ii \u2013 (AK)\u0456\u0456|.$\nDegree of Symmetry The degree of symmetry we compute is the number of all AG such that AG > \u0394 Gth where $A_{th}^{G}$ is a certain threshold, often between 0.05 and 0.2. If this layer has h many neurons, we define\n$Ndosb h - Ndos,$\nand by definition Ndosb \u2265 0."}, {"title": "A.2 Learning Dynamics", "content": "In Figure 2, we use a teacher-student setting, where both the teacher and student are five-layer fully connected networks (FCNs) with 64 units per layer and tanh activation. The input and output dimensions are 10"}, {"title": "A.3 Mechanisms for Symmetry Changes", "content": "For the left panel of Figure 3, we directly use the pretrained models of ViT-Base and ViT-Large from https://pytorch.org/vision/stable/models.html. The computation of the weight rank and Ndosf follows the outline in Section A.1.\nIn the middle panel of Figure 3, we train a three-layer fully connected network (FCN) with swish activation on a synthetic dataset where inputs and outputs are identical, sampled from a 300-dimensional standard Gaussian distribution. For gradient descent (GD), the dataset size is 1000, and weight decay is set to 10-3. For stochastic gradient descent (SGD), the batch size is 128, with new data randomly generated for each"}, {"title": "A.4 Neural Collapse", "content": "In Figure 4, we train ResNet18 on the CIFAR-10 dataset using vanilla weight decay and syre (Ziyin, 2024). In both cases, the weight decay is set to 5 \u00d7 10-4. Networks are trained for 200 epochs, and results from the final epoch are reported. To generate Figure 4, we randomly select 10 images per class and compute the correlation between their features (i.e., the input to the final layer). The corresponding eigenvalue spectrum is presented in Figure 11, showing 10 prominent eigenvalues. Figures 12 and 13 extend these experiments with a weight decay of 5 \u00d7 10-3, where the vanilla weight decay model exhibits a stronger low-rank structure compared to the syre model.\nIn Figure 5, we train a five-layer FCN with 512 neurons per layer and swish activation on CIFAR-10. Symmetry breaking is evaluated as the number of pairwise distances (as in Figure 2) exceeding 1. Results are averaged over 5 independent runs. Figure 14 replicates this experiment on the SVHN dataset, where the same hierarchical representation effects can be observed.\nIn Figure 6, we train two deep linear networks on MNIST using the Adam optimizer. Each network is a six-layer fully connected network (FCN) with 128 neurons per hidden layer. The average alignment to input"}, {"title": "A.5 Adaptive Capacity", "content": "In Figure 16, we train two-layer MLPs and simple transformers on synthetic datasets. The tasks match Figure 10, and we report the pairwise distances among six neurons. Figure 16 suggests that symmetry adapts to the training data in each run."}, {"title": "B.1 Formal Statement of Theorem 1", "content": "We consider the MSE loss for part 2 of the theorem.\n$l(x, \u03b8) = ||y(x) \u2212 f(x,\u03b8)||^{2}.$\nConsider an empirical data distribution P(x), where x contains both the input and the label. The SGD iteration is defined as\n$\u03b8_{t+1} = \u03b8_{t} - \u03b7\u2207_{\u03b8}l(x,\u03b8_{t}),$\nwhere x ~ P(x) and \u03b7 is the learning rate.\nThe GD iteration is defined as\n$\u03b8_{t+1} = \u03b8_{t} - \u03b7\u2207_{\u03b8}E_{x~P(x)}[l(x,\u03b8_{t})].$\nTheorem 3. Let f have the G-symmetry for which $P_{G}= PG$, and \u03b8 be intialized at \u03b80 such that PG\u03b80 = \u03b80. 1. For all time steps t under GD or SGD, there exists a model f'(x,\u03b8') and sequence of parameters \u03b8t such that for all x,\n$f'(x,\u03b8') = f(x,\u03b8_{t}),$\nwhere dim(\u03b8') = dim(PG).\n2. The kernalized model, $g(x,\u03b8) = lim_{\u03bb\u21920}(\u03bb^{\u22121}(f(x, \u03bb\u03b8 + \u03b8_{0}) \u2212 f(x,\u03b8_{0})))$, converges to\n$\u03b8^{*} = A^{+} \u03a3_{X} \u2207_{\u03b8}f(x,\u03b8_{0})y(x)$\nunder GD for a sufficiently small learning rate. Here, $A := P_{G} \u03a3_{x} \u2207_{\u03b8}f(x,\u03b8_{0})^{T}\u2207_{\u03b8}f(x,\u03b8_{0})P_{G}$ and A+ denotes the Moore-Penrose inverse of A.\nThe second part of the theorem means that in the kernel regime, being at a symmetric solution implies that the feature kernel features are being masked by the projection matrix $\u2207_{\u03b8}f(x,\u03b8_{0}) \u2192 P_{G}\u2207_{\u03b8}f(x,\u03b8_{0})$, and learning can only happen given these masks. The proof is a slight generalization of Propostions 2 and 3 in (Ziyin et al., 2025b).\nProof. 1. Note that l has the G-symmetry when f has the G-symmetry. For PG\u03b80 = \u03b80 and any \u03b8, we have $l(x, \u03b8) = l(x, \u03b8_{\u03bf} + P_{G}(\u03b8 \u2013 \u03b8\u03bf))$. Taking \u03b8\u2192\u03b8\u03bf, we have\n$(I - P_{G})\u2207_{\u03b8}l(x, \u03b8\u03bf) = 0,$\nwhere we use $P_{G}^2 = PG$. Therefore, for \u03b81 := \u03b80 + \u03b7\u2207\u03b8l(x,\u03b80), we still have PG\u03b81 = \u03b81.\n2. By (20), close to any symmetric point \u03b80 (any \u03b80 for which PG\u03b80 = \u03b80), for all x, we have\n$f(x,\u03b8) \u2212 f(x,\u03b8_{0}) =\u2207_{\u03b8}f(x,\u03b8_{0})P_{G}\u2206 + O(||\u2206||^{2}).$\nTherefore, g(x, \u03b8) simplifies to a kernel model\n$g(x, \u03b8) = \u2207_{\u03b8}f(x,\u03b8_{0})P_{G}\u03b8.$\nLet us consider the squared loss $l(\u03b8) = \u03a3_{x} ||y(x) \u2212 g(x,\u03b8)||^{2}$ and denote $A := \u03a3_{x} P_{G}\u2207_{\u03b8}f(x,\u03b8_{0})^{T}\u2207_{\u03b8}f(x,\u03b8_{0})P_{G}$, $b := P_{G} \u03a3_{x} \u2207_{\u03b8}f(x,\u03b8_{0})y(x)$. The GD iteraiton is\n$\u03b8_{t+1} = \u03b8_{t} \u2013 2\u03b7(A\u03b8_{t} \u2013 b),$"}, {"title": "B.2 Formal Statement of Theorem 2", "content": "We will consider training a deep linear network with SGD and MSE loss\n$l(\u03b8,x) = ||W_{D}\u2026W_{1}x \u2212 y(x)||^{2},$\non datasets $D_{M} = {(Mx_{i},Y_{i})}_{i}$, where M is an invertible matrix, and $y_{i} = Vx_{i} + \u03b5_{i}$ for i.i.d. noise $e_{i}$. We make the same assumptions as Theorem 5.4 of (Ziyin et al., 2024) (the most important of which is the SDE approximation to the SGD). Also, we use the definition of the noise equilibrium in (Ziyin, 2024), which essentially means that SGD reaches stationarity in the degenerate directions of the double rotation symmetry.\nTheorem 4. Let $V\u2019 = \u221a \u03a3_{i}x_{i}, rank(V\u2019) = d$ and $S\u2019$ be a diagonal matrix containing singular values of $V\u2019$. Consider two deep linear networks A and B with weights of arbitrary dimensions larger than d. Let model A train on DM and model B on DM\u2019. Then, at the global minimum and at the noise equilibrium, every hidden layer of A is perfectly aligned with every hidden layer of B for any x, in the sense that\n$h_{L_{A}}^A(x) = coRh_{L_{B}}^B(x)$\nfor 1 \u2264 LA  and 1 \u2264 LB  and any x, where co =  is a constant and $R = U_1U_2^{+}$, satisfying $U_1^T U_1= U_2^T U_2 = I_{d}$. $h_{L_A}^{A}(x) := A_{WA}A_{M}x$, $h_{L_B}^{B}(x) := B_{WA}B_{M}\u2019x$ denote the output of the LA, LB-the layer of network A and B, respectively.\nProof. Let V' := \u016aS'\u00d1 be its SVD. According to Theorem 5.4 of (Ziyin et al., 2024). At the global minimum and noise equilibrium under SGD, the solution of a DA-layer network for the dataset DM is given by\n$\u03a3 M^{2}W_{i} = U_{i}\u03a3_{i}U_{i+1}^{T}, i=1,\u2026DA-1$, $W_{DA} = U_{DA}\u03a3_{DA}\u00d1^{T}, VM^{1} \u221a \u03a3_{i}x = U_{1} \u03a3_{1} \u00d1$.\nfor i = 2,\u2026, D \u2013 1, where U\u017c are arbitrary matrices satisfying UTU\u2081 = Idxd, and\n$\u03a3_{1} = \u03a3_{p} , \u03a3_{DA} =  Idxd.$\nWe can verify that $IVA_{W}A_{M} = V$, or $h_{A}^{A}(x) = Vx$ as expected. The solution suggests that\n ,   VA.\nSimilarly\n .   VB\nThe proof is complete by comparing (30) and (31)."}, {"title": "B.3 Space Quantization Conjecture", "content": "Theorem 5. Consider the loss l(\u03b8) = lo(\u03b8) + \u03b3||\u03b8||\u00b2 with $\u03b8 := (\u03b81,\u2026,\u03b8k)$ and \u03b8\u00bf \u2208 Rn for 1 \u2264 i \u2264 k. Assume that lo(\u03b81,\u2026, 0k) has the permutation symmetry lo(01,\u2026,0i, \u2026, 0j, \u2026, 0k) = lo(01, \u2026, 0j, \u2026, \u03b8i,\u2026,0k) for any 1 \u2264 i, j \u2264 k and satisfies the following q-Lipschitz condition\n||\u2207lo(\u03b8) \u2013 \u2207lo(\u03b8')|| \u2264 K||\u03b8 \u2013 \u03b8'||',\nfor q \u2265 1. Moreover, assume that info lo(0) > \u2212\u221e and that K scales with the number of active neurons m as K = Kom\u00af\u00ba. Then, at any global minimum,\nq+1\nm\u2264 Cy 2\nfor n large enough, where C is some constant.\nProof. We first consider two vectors 01 \u2260 02. Suppose that the global minimum is at l(01,02). We would like to compare the loss between (01,02) and ((01+02), (01 + 02)), which gives\nl(01,02) -l((01+02),(01+02))\n=lo (01,02)-lo ((01+02),(01+02)) +7(|101|12+7||02|2-27||(01+02)||\u00b2)\n=lo(01,02) -lo ((01+02),(01+02))+\uc990\uae30\uae30101-0212.\nFor the first term, we have\n||lo (01,02) -lo ((01+02), (01.  1222 + 02)) ||\nwhere\nf(z) := =lo(01 lo  +6 ||| \u2264 ||01 - 02|| max |f'(z)|||01 - 02||,\nBy permutation symmetry, we have\nTf'(0) = (Vilo ((01+02),(01+02)) - \u221a2001 ((01+02), (01+02))) (01-02),\nwhere V1 and 2 denote the derivative of lo w.r.t. its first and second variable. By the permutation symmetry, we have \u2207\u2081lo (1(01+02), (01 + 02)) = \u22072lo ((01+02), (01 + 02)), and thus f'(0) = 0.\nAs Vlo is q-Lipschitz, we have\nf'(z) \u2264 Kzq||01 \u2013 02||9,\nwhich gives\n02 l(01,02)-((01+02),(01+02)) 2-K|101-02|| 9+1+1/7/101-02112.\nfor the global minimum l(01,02). Thus we have\n||01 - 02|| 2 (2x)/(4-1)\n-2K\nfor q > 1. We conclude that any two vectors should be separated by a distance at least (K) 1/(9-1).\nMeanwhile, we have\nlo(0) + y||0||\u00b2 \u2264 lo(0),\nwhich gives\n||0i||2 \u2264  ,", "list": []}]}