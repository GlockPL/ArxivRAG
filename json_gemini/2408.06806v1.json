{"title": "Unmasking the Uniqueness: A Glimpse into\nAge-Invariant Face Recognition of Indigenous\nAfrican Faces", "authors": ["Fakunle Ajewole", "Joseph Damilola Akinyemi", "Khadijat Tope Ladoja", "Olufade Falade Williams Onifade"], "abstract": "The task of recognizing the age-separated faces of an individ-\nual, Age-Invariant Face Recognition (AIFR), has received considerable\nresearch efforts in Europe, America, and Asia, compared to Africa. Thus,\nAIFR research efforts have often under-represented/misrepresented the\nAfrican ethnicity with non-indigenous Africans. This work developed an\nAIFR system for indigenous African faces to reduce the misrepresenta-\ntion of African ethnicity in facial image analysis research. We adopted\na pre-trained deep learning model (VGGFace) for AIFR on a dataset\nof 5,000 indigenous African faces (FAGE_v2) collected for this study.\nFAGE_v2 was curated via Internet image searches of 500 individuals\nevenly distributed across 10 African countries. VGGFace was trained\non FAGE_v2 to obtain the best accuracy of 81.80%. We also performed\nexperiments on an African-American subset of the CACD dataset and\nobtained the best accuracy of 91.5%. The results show a significant dif-\nference in the recognition accuracies of indigenous versus non-indigenous\nAfricans.", "sections": [{"title": "1 Introduction", "content": "\"Biometrics\" refers to the computerized identification of individuals based on\ngenetic and behavioural characteristics. With the increasing use of biometrics,\nidentification, and verification are now more dependable and accurate, proving\neach person's true identity. We have a variety of biometric tools at our dis-\nposal, including hand geometry, voice, fingerprint, iris, and card recognition.\nFace recognition provides several advantages over the other biometrics stated\nabove due to its high accuracy, non-contact nature, user-friendliness, speed, and\ndependability [19]\nIn many applications, including access control, distribution of government\nbenefits, and criminal investigations, age-invariant facial recognition is crucial\nas subjects age with time. A robust identification system should be able to\nsuccessfully identify people despite age differences between the enrolled and the\nprobe images of the same individual. Face recognition that is age-invariant may\nalso aid in reducing operational expenditures by minimizing the necessity for\nre-enrollment [24].\nDespite several research efforts in Age-Invariant Face Recognition (AIFR),\nthere is little or no representation of African ethnicity in the results of published\nresearch [2]. In the few cases of representation of African ethnicity, it has been\nwrongly represented as African-American in MORPH [18], and CACD [6]. In\n[1], it was shown that there are significant differences in the ageing patterns\nof indigenous Africans and non-indigenous Africans. Indigenous Africans refer\nto Africans who reside within the continent and are therefore affected by the\nclimate, economy, and conditions of living within the African continent, while\nnon-indigenous Africans refer to Africans who reside outside the African conti-\nnent (e.g., African-Americans).\nThis work addresses a critical and significant issue in the field of face recog-\nnition: the misclassification and inappropriate grouping of Indigenous Africans\nwith continental Africans within existing age-invariant face recognition systems.\nThis challenge has resulted in denials of identity and inaccurate recognition for\nindividuals from the Indigenous African population [1]. The difficulty is signifi-\ncant because human subjects' appearances in training or enrollment photographs\ncan differ significantly from the appearance presented for identification. Moti-\nvated by the work of [1] on the FAGE dataset (first used in [14]) which contained\nimages representing only one African country, we curated a new dataset cutting\nacross individuals from 10 African countries evenly distributed within the 5 geo-\ngraphical zones of the continents. We also extracted an African-American subset\nof a large AIFR dataset, CACD [6], for comparison with the curated dataset.\nOur findings include:\n1. The first dataset of indigenous African faces with a wide coverage of the\nAfrican continent and a balanced number of age-separated face images per sub-\nject.\n2. A promising and competitive recognition accuracy on the curated dataset.\n3. Significant differences in the performance on the curated dataset when\ncompared with the performance on the African-American counterpart under the\nsame experimental settings.\nThe next section 2 presents a review of the various approaches that have\nbeen used for face recognition with machine learning. Section 3 describes the\nmethodology (Materials and Methods) Section 4 describes the model perfor-\nmance results, discussions, and improvements from our findings, and Section 5\nconcludes the paper."}, {"title": "2 Literature Review", "content": "This section reviews the research that has been conducted over the years on\nage-invariant face recognition. Researchers throughout the world have employed\nseveral ML techniques for age-invariant face recognition, and a discussion of their\nmethods, datasets, and results is presented in this section.\nThe authors of [17], proposed a 3D ageing modelling technique to address\nthe challenge of achieving temporal invariance in automatic face recognition.\nThey used three different face databases, FG-NET [16], MORPH [18], and\nBROWNS to evaluate the effectiveness of their approach. The proposed method\nresulted in substantial performance improvements on all three databases, which\ndemonstrates the effectiveness of the proposed ageing modelling method.\nIn [9], an innovative face representation and matching was presented for\nthe age-invariant face recognition challenge. A unique maximum entropy feature\ndescriptor (MEFD) technique was developed that encodes the microstructure of\nface photos into a sequence of discrete codes. The coding entropy is improved to\nobtain discriminative and descriptive information from densely sampled encoded\nface pictures. An identity factor analysis technique was created to estimate the\npossibility that two given faces had the same core identity. The technique was\nassessed on the FGNET and MORPH datasets, and accuracies of 76.2% and\n92.26% respectively, were reported.\nAccording to [7], deep convolutional neural networks (DCNN) have achieved\ntop results on numerous tasks in computer vision, including face verification.\nDCNN models can not only classify big data fluctuations but also learn a com-\npact and discriminative feature representation when the amount of training data\nis sufficiently high. The proposed approach by [21], decomposes deep facial char-\nacteristics into two orthogonal elements that depict age-related and identity-\nrelated aspects. This led to identity-related traits that are resilient to ageing for\nage-invariant face recognition. Specifically, the deep facial features were decom-\nposed in the spherical coordinate system comprising the radial coordinater and\nthe angular coordinate \u0444.\nThe proposed Application Invariant Model (AIM) in [24], addressed the\nchallenges in recognizing faces across ages by providing a unified deep archi-\ntecture that could learn powerful age-invariant facial representations and per-\nform attention-based face rejuvenation and ageing, which could be used for age-\ninvariant face recognition and other applications.\nIn [1], the authors observed that most face age estimation works had ex-\nplored the influence of continental ethnicities (e.g., African, Asian, Caucasian,\nand Hispanic) and that there were yet to be critical experimentations on the im-\npact of local ethnicities such as indigenous Africans. Extending previous studies\nsuch as [3,15], they highlighted that the ageing of African Americans may not\nbe the same as that of Africans who reside on the African continent and referred\nto the ethnicity of the latter group as 'indigenous' Africans. Using subsets of the\nCACD and FAGE, they investigated these supposed differences in age estima-\ntion and reported that they indeed existed. However, the dataset subsets used\nwere limited in size, which questioned their results' validity.\nThe individualized face pairing method developed by [2], matches faces ver-\nsus full groups of faces arranged by individuals. Similarity score vectors were\ncreated for both matching and non-matching image-individual pairings, and the\nvectors were employed for age-invariant face recognition. This technique used\nthe individualized aspects of ageing to lessen the influence of ageing on face\nrecognition, enabling individuals to be accurately identified across their multi-\nple age-separated face photos. Their model achieved high recognition accuracies\non FAGE, FG-NET and CACD, but did not investigate the ethnic variations in\nthe datasets.\nAccording to [8], the suggested technique to boost the performance of AIFR\nincorporates an updated ASM architecture to extract handmade and deep fea-\ntures for AIFR, in combination with a 7-layer CNN architecture and a reduced\npicture size of 32x32 pixels to decrease delay time and space complexity. The\nresults show that the suggested method beats state-of-the-art algorithms in\nface recognition and achieves good accuracy throughout the age spectrum. The\npresented methodology achieves a maximum accuracy of 91.76% for the LAG\ndatabase [4], outperforming all existing state-of-the-art methodologies.\nIn [11], authors proposed a novel age-invariant face recognition framework\nusing Mutual Information Minimization (MT-MIM), which disentangles the mix-\nture of face features into two nearly independent components: the identity-\ndependent component and the age-dependent component. They introduced a\nmulti-task learning framework based on mutual information minimization to\npartition face representations into identity-dependent and age-dependent com-\nponents for age-invariant face recognition. They evaluated MT-MIM on popu-\nlar public-domain face ageing datasets FG-NET [16], MORPH Album 2 [18],\nCACD [6], and AgeDB [13], and obtained significant improvements over pre-\nvious state-of-the-art methods. Specifically, their method exceeds the baseline\nmodels by over 0.4% on MORPH Album 2, and over 0.7% on CACD subsets,\nwhich are impressive improvements over the high accuracy levels of above 99%\nand an average of 94%.\n[23], proposed a method for age-invariant face recognition based on identity-\nage shared features (ISF). Specifically, they decoupled facial representation into\nthree parts, i.e., pure identity features, pure age features, and identity-age shared\nfeatures, to improve the independence of age and identity features, thereby reduc-\ning interference from age-related information and improving the accuracy of face\nrecognition. They conducted experiments on several benchmark datasets to eval-\nuate the performance of the proposed method. Experimental results on bench-\nmark datasets for face ageing (FG-NET 95.67%, AGE-DB 97.53%, CALFW\n96.03%, and CACD-VS 99.58%) show that the proposed ISF outperforms state-\nof-the-art AIFR approaches. Overall, the authors provided a new perspective for\nface recognition and demonstrated the potential of their proposed method to be\napplied in various fields, such as security, surveillance, and human-computer in-\nteraction. In [20], the authors presented a local feature computation method to\nextract local features from all pixels within pre-defined local regions of the face.\nTheir method involved a four-step procedure which includes the computation of\nLocal Difference Pattern (LDP) and Local. Directional Gradient Relation Pat-\ntern (LDGRP) which are then used to create a histogram-based facial feature\nvector. Their experiments demonstrated 90.75% and 96.95% recognition accu-\nracy on the FG-NET and MORPH datasets, respectively. It's interesting to see\nthat this method achieves such a significant performance despite the use of local\nfeatures and without any deep learning methods.\nMTLFace [12], is a recent effort to perform the tasks of AIFR and face\nsynthesis jointly. It involved the collection of a large dataset, which is aimed\nat becoming a new benchmark for the two tasks. Extensive experiments and\ncomparisons were conducted, especially across the age dimension, specifically\nfocusing on young versus old faces. However, there is no mention of the consid-\neration of ethnic representation or diversity in the dataset, as far as we know.\nThe dataset indeed contains face pictures of subjects across various ethnicities,\nbut the indigenous nature of these faces remains in question.\nThe results presented in the article [23] demonstrate that the proposed\nmethod, which includes introducing identity-age shared features and utilizing a\ntwo-stage constraint algorithm, outperforms state-of-the-art approaches in age-\ninvariant face recognition. The experimental evaluation conducted on benchmark\ndatasets (FG-NET, AGE-DB30, CALFW, and CACD-VS) shows superior per-\nformance compared to existing methods such as DAL and MTLFace. The study\nhighlights the effectiveness of the proposed approach in improving the accuracy\nand robustness of cross-age identity recognition in face recognition systems.\nFrom the above-reviewed works, AIFR systems seem to have improved over\nthe years, but not enough attention seems to have been given to the indigenous\nAfrican ethnicity. This presents a significant drawback to face recognition given\nthe population of the African continent, and it is to this end that this work\ninvestigates AIFR on indigenous Africans and compares it with AIFR on non-\nindigenous Africans."}, {"title": "3 Methodology", "content": "Given the success of existing AIFR research on large datasets, we have observed\nthat most datasets have limited representations of Africans. In most cases where\nAfricans are included in large AIFR datasets (e.g., MORPH [18], and CACD [6],\nthese Africans were mostly non-resident within the African continent. As subtle\nas this sounds, research has shown that it has a significant impact on the results\nof facial image analysis [10], [1], This has motivated us to conduct AIFR on\nindigenous African faces (i.e., Africans living on the African continent) vis-\u00e0-vis\nnon-indigenous Africans (i.e., Africans living outside the African continent).\nIn our methodology, we formulated the AIFR problem as a multiclass closed-\nset recognition problem to determine how well we can recognize the individuals\nwithin the dataset. To achieve this, we set out formulations for the AIFR problem\nas follows:\nGiven a set, A, of face images organized by individuals as\n$A = {B\u00bf|i = 1,2,..., n}$  (1)\n$B = {bk|k = 1, 2, ..., m}$ (2)\nWhere be is a single-face image of an individual and m is the number of\nage-separated images of individual i. The order of images in A and B are unim-\nportant, hence the use of sets in equations 1 and 2.\nBased on the definitions in Equations 1 and 2, we aim to find a model that\nappropriately maps B to A as defined in 3. So, the function g(bk) should return\nthe appropriate Bi \u2208 A to which be belongs.\n$g(bk): B \\rightarrow A$ (3)\nTo realize the function g(), we employed a deep learning method to learn\nfacial features and classify faces based on input facial images. Then, we exper-\nimented with two datasets, one collected for this research and the other, an\nexisting dataset. Figure 1 shows the overall design of our methodology where\nan input face is preprocessed and passed through a deep learning architecture\n(VGGFace) [5], to extract relevant identity features and classify (recognize)\nthe facial image. Our motivation for using the VGGFace architecture is that\nit is already trained to extract facial features which makes it more reliable for\nextracting facial features for age-invariant face recognition.\nIt is obvious that set A in Equation 1 is the dataset of face images, which\nis organized by the individuals/subjects in the dataset. In our study, we have\nused two datasets; FAGE_v2 (Facial expression, Age, Gender, and Ethnicity,\nversion 2), a dataset of indigenous African faces collected for this research, and\nCACD, a publicly available face dataset. FAGE_v2, a different set from the FAGE\ndataset in [2], was curated for this study by searching for and downloading\nimages of 500 Africans across 10 African countries evenly distributed within the\n5 geographical zones of the continent (i.e., North, East, South, West and Central\nAfrica). Specifically, we identified 100 popular individuals from each zone to\nmake a total of 500 individuals and collected 10 age-separated face images of\neach individual, summing up to a total of 5,000 face images. Samples of faces\nin the FAGE_v2 dataset are shown in Figure 2 and the dataset is available\non Kaggle. The individuals whose images were downloaded include politicians,\nclergy, athletes, successful business people, etc.\nCACD contains images of actors of different ethnicities. We created a list of\nactors who identified as African-American or Black from imdb.com and used this\nlist to extract the images of these individuals in CACD and we realized 8,656\nimages of 89 individuals. The details of both datasets are presented in Table\n1. Upon further cleaning (removing duplicates and wrongly labeled images), we\nhad 6979 images left.\nThus, n, the number of individuals, as in Equation 1, is 500 for FAGE_v2\nand 89 for the CACD subset. The disparity in the number of subsets is huge,\nbut it is also an indicator of the limited representation of African faces in the\nCACD dataset. While k, the number of images for each individual is fixed at 10\nin FAGE_v2, it is variable in CACD with an average of 82 images per subject.\nStill, many of those images are noisy and had to be removed as stated in the\nprevious paragraph."}, {"title": "3.1 The AIFR Model for Indigenous African Faces", "content": "We adopted the VGGFace [5], VGGFace was trained on VGGFaxe2 dataset,\nEach input image was automatically preprocessed by detecting them with MTCNN\n[22], and cropping the face regions, and resizing the same to 224 x 224, the input\ndimensions for the VGGFace architecture. VGGFace comprises 13 convolutional\nlayers for extracting facial features and 3 fully connected layers, the last of which\nis the classification layer. We removed the classification layer and replaced it with\nvarying numbers of fully connected layers including dropout layers as indicated\nin Tables 2 and 3. In all the experiments, we used the Adam optimizer with\na learning rate of 0.0001 and the categorical cross-entropy loss. The optimiza-\ntion algorithm, Adam, was used to update the network's weights based on the\ntraining data. The model was fine-tuned by adjusting hyper-parameters such as\nthe Epoch, learning rate, batch size, etc. We explored different combinations of\nhyperparameter settings to train and validate the model across both datasets\nand compared the results.\nEach dataset was randomly split into training, validation, and test sets. For\nFAGE_v2, there were 3500, 500, and 1,000 images in the training, validation,\nand test splits, respectively. For CACD, there were 4,491 images for training,\n1,081 images for validation, and 1,307 images for testing. The predictions of\nthe trained model were evaluated using classification accuracy, precision, recall,\nand F1-score all of which are represented as percentages. Accuracy measures\nthe overall correctness of predictions, precision gauges the proportion of true\npositives in positive predictions, recall assesses the proportion of true positives\ncaptured, and the F1-score is the harmonic mean of precision and recall."}, {"title": "4 Experiments, Results, and Discussions", "content": "For each dataset, we conducted a set of 5 experiments using different hyperpa-\nrameter combinations and reported these details in 2 and 3. The five settings,\nas seen in 2 and 3 involve different values of the training iterations (epochs),\nbatch size, number of dense layers, and drop-out fraction. In all cases, the re-\nsults are reported on the test set of each dataset. The dense layer, as in 2 and\n3, indicates the number of fully connected layers added to the architecture to\nreplace the original classification layer.\nFor both datasets, the best accuracies was obtained at 50 training epochs,\na batch size of 64, one fully connected layer, and 50% dropout layer. In this\nexperimental setting, we obtained the best accuracy of 81.8% for FAGE_v2 and\n91.5% for CACD. In terms of the number of fully connected layers, the results\nseem to improve with a smaller number of them. This indicates a good repre-\nsentation of the main VGGFace architecture, and only a few final layers may\nbe needed to have a good classification. The higher drop-out rate of 0.5 (50%)\nalways produced better results, except when the batch size was too large (128).\nGenerally, 50 training epochs were sufficient to achieve good performance, and\na higher number of epochs did not seem to improve the results so much."}, {"title": "4.1 Comparative Analysis", "content": "Due to the wide differences in the distribution of both datasets, we performed\nan additional experiment to provide a fair baseline for the comparison of their\nperformances and to further verify the earlier experiments. In this comparative\nanalysis, we selected 10 images from each of the 88 African-American individuals\nin CACD and also selected 88 individuals from FAGE so that both subsets had\n880 images each. Due to the very small number, these subsets were each divided\ninto training, validation, and test splits (70:10:20), and VGGFace was again\ntrained for 50 epochs on each dataset, validated, and tested on the respective\nsplits. The results revealed a similar pattern: 90.4%, 79.7%, and 83.5% accuracies\nfor training, validation, and test, respectively, on the CACD subset, and 83.5%,\n68.8%, and 72.0% accuracies for training, validation, and test, respectively, on\nthe FAGE subset.\nThere are two things we would like to point out from the results. First, one\ncan observe that in all cases, the CACD images were always better predicted\nthan the FAGE_v2 images. This is an indication of the differences in the ageing\nfeatures of indigenous versus non-indigenous Africans, and this is consistent with\nthe findings in [1]. This means the model can recognize non-indigenous African\nfaces (in CACD) much better than the indigenous ones (in FAGE_v2). This\ncould also be largely due to the extent of diversity of the ethnicities on which\nthe VGGFace architecture was pre-trained, as well as an indication of the mis-\nrepresentation or underrepresentation of the indigenous African ethnicity on the\nface database on which VGGFace was trained. Secondly, despite the disparities\nobserved above, the results on FAGE_v2 are quite promising, yet they indicate\nthat more work needs to be done to push the results further up. Despite the\nrelatively small number of images per individual in FAGE_v2 (10) compared to\nCACD (82), we consider a \u2248 10% difference in accuracy rates on both datasets a\ngood indication of the diversity of the FAGE_v2 dataset and an indication that\nwith only a little more effort, we can improve the task of recognizing indigenous\nAfricans."}, {"title": "5 Conclusion", "content": "This work has investigated the differences in the ageing patterns of indigenous\nand non-indigenous Africans in the realm of Age-Invariant Face recognition. An\nAIFR model was developed for recognizing the age-separated faces of indigenous\nAfricans to a reasonable degree of recognition accuracy (81.8%). More impor-\ntantly, this work uncovered the differences in the ageing patterns of indigenous\nand non-indigenous Africans using a subset of the CACD dataset. This creates\nroom for further exploration of these differences for the development of robust\nface recognition algorithms for African ethnicities. Unfortunately, there are not\nenough individuals in the CACD dataset to match up with the number of indi-\nviduals in FAGE_v2, and reducing the number of individuals in FAGE_v2 will\nonly reduce the dataset to a minimal amount, not even able to match up with\nthe size of the CACD subset. In future work, we hope to investigate a larger\nsubset of non-indigenous Africans, probably harvested from different datasets\nother than CACD to combat this limitation. We also hope to keep enlarging the\nFAGE_v2 dataset for further analysis."}]}