{"title": "RETHINKING VISUAL COUNTERFACTUAL EXPLANATIONS THROUGH REGION CONSTRAINT", "authors": ["Bartlomiej Sobieski", "Jakub Grzywaczewski", "Bartlomiej Sadlej", "Matthew Tivnan", "Przemyslaw Biecek"], "abstract": "Visual counterfactual explanations (VCEs) have recently gained immense popularity as a tool for clarifying the decision-making process of image classifiers. This trend is largely motivated by what these explanations promise to deliver \u2013 indicate semantically meaningful factors that change the classifier's decision. However, we argue that current state-of-the-art approaches lack a crucial component \u2013 the region constraint \u2013 whose absence prevents from drawing explicit conclusions, and may even lead to faulty reasoning due to phenomenons like confirmation bias. To address the issue of previous methods, which modify images in a very entangled and widely dispersed manner, we propose region-constrained VCEs (RVCEs), which assume that only a predefined image region can be modified to influence the model's prediction. To effectively sample from this subclass of VCEs, we propose Region-Constrained Counterfactual Schr\u00f6dinger Bridges (RCSB), an adaptation of a tractable subclass of Schr\u00f6dinger Bridges to the problem of conditional inpainting, where the conditioning signal originates from the classifier of interest. In addition to setting a new state-of-the-art by a large margin, we extend RCSB to allow for exact counterfactual reasoning, where the predefined region contains only the factor of interest, and incorporating the user to actively interact with the RVCE by predefining the regions manually.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual counterfactual explanations (VCEs) aim at explaining the decision-making process of an image classifier by modifying the input image in a semantically meaningful and minimal way so that its decision changes. Over time, they have become an independent research direction with the latest methods presenting impressive and visually appealing results. Nevertheless, in this work we show that they possess a fundamental flaw at a conceptual level \u2013 the lack of region constraint and its proper utilization.\nConsider the image x* in Fig. 1, which the classifier f correctly predicts to be a jay. In essence, VCEs focus on semantically editing x* so that the prediction of f changes to some target class bulbul in this case hence providing an answer to a specific what-if question, through which the model's reasoning is explained. Consider now an example VCE for x*, denoted as xyce, obtained with a recent state-of-the-art (SOTA) method. While XVCE is successful at changing the prediction of f and can be considered both realistic and se- mantically close to x*, answering why f now predicts it as a bulbul is close to impossible."}, {"title": null, "content": "The algorithm simultaneously modifies the bird's head and feathers, changes the texture of the branch and even modifies the copyright caption. The entanglement and dispersion of introduced changes hence leaves the question unanswered. We argue that to circumvent these fundamental difficulties, VCEs should be synthesized with a hard constraint on the region, where the changes are allowed to appear, while leaving the rest of the image unchanged. For example, consider the image XR with regions of the bird's head (R1) and body (R2) overlayed. Constraining the VCEs to introduce changes only to predetermined re- gions leads to two distinct explanations, XR1 and XR2, of why the decision changes to bul- bul. By isolating the modified factors, the ex- planatory process greatly simplifies one can now state with certainty that f's new predic- tion is based either on the modified feathers (XR2) or the changed characteristics of its head (XR\u2081). Region-constrained VCEs (RVCEs) al- low, therefore, to reason about the model's thought process in a causal and principled man- ner, mitigating the potential confirmation bias and clarifying the explanatory process.\nBy putting RVCEs in the spotlight, our work establishes new frontiers in the field of VCE genera- tion. First, we define the objective of finding RVCEs as solving a conditional inpainting task. By building on top of the Image-to-Image Schr\u00f6dinger Bridge (I2SB, Liu et al. (2023a)) approach and adapting it to the classifier guidance scheme, we develop an efficient algorithm which synthesizes RVCEs with extreme realism, sparsity and closeness to the original image. Specifically, we set a new quantitative state-of-the-art (SOTA) on ImageNet (Deng et al., 2009) with up to 4 times better scores in FID and 3 times better sFID (realism), up to 2 times higher COUT (sparsity), and match or exceed S\u00b3 (similarity) and Flip Rate (efficiency) achieved by previous methods. Through large-scale experiments, we demonstrate that, besides a fully automated way of synthesizing meaningful and highly interpretable RVCEs, our approach, Region-constrained Counterfactual Schr\u00f6dinger Bridge (RCSB), allows to infer causally about the model's change in prediction and enables the user to actively interact with the explanatory process by manually defining the region of interest. More- over, our results highlight the importance of RVCEs in future research, indicating potential pitfalls of unconstrained methods that could lead to drawing misleading conclusions."}, {"title": "2 BACKGROUND & RELATED WORK", "content": "In this section, we introduce the necessary background knowledge connected with score-based gen- erative models (SGMs) and I2SB, which forms the foundation of our method. We then present an overview of recent methods for VCE generation based on SGMs. For an extended literature review and detailed description of the theoretical basis, please refer to the Appendix.\nSGM. Following the work of Song et al. (2021), SGMs can be constructed through the framework of stochastic differential equations (SDEs), where samples from a complex distribution po (e.g., natural images) are mapped to a Gaussian distribution p\u2081, while the model is trained to reverse this mapping. Formally, converting data to noise is performed by following the forward SDE (Eq. (1a)), while denoising happens through the reverse SDE (Eq. (1b), Anderson (1982)):\n$\\begin{aligned} d x_{t} &=\\mathcal{F}_{t}\\left(x_{t}\\right) d t+\\sqrt{\\beta_{t}} d w, \\\\ d x_{t} &=\\left(\\mathcal{F}_{t}\\left(x_{t}\\right)-\\beta_{t} \\nabla_{x_{t}} \\log p\\left(x_{t}, t\\right)\\right) d t+\\sqrt{\\beta_{t}} d \\bar{w}, \\end{aligned}$                                                                                                                                                             (1a)\n (1b)\nwhere xt is the noisy version of a clean image x \u2208 Rn for some n \u2208 N at timestep t \u2208 [0,1], w and w denote the Wiener process and its reversed (in time) counterpart, Ft (xt) : Rn \u2192 R\" is\""}, {"title": null, "content": "the drift coefficient, \u03b2t \u2208 R is the diffusion coefficient and \u2207x, log p(xt, t) is the score function. An SGM se, where e denotes the model's parameters, is trained to approximate the score, i.e., Se(xt,t) \u2248 x\u2081 log p(xt,t). During sampling, denoising begins from pure noise x1 ~ P1 and follows some discretized version of Eq. (1b) with the approximate score se.\nSGMs can also be adapted to conditional generation, where y represents the conditioning variable. In this case, the score \u2207x\u2081 log p(xt, t) is replaced by Vx+ log p(xt, t | y), which can be decomposed with Bayes' Theorem into \u2207x\u2081 log p(xt, t | y) = \u221ax\u2081 log p(xt, t) + \u221ax\u2081 logp(y | xt,t). While \u25bcxt log p(xt, t) can be approximated with an already trained se, \u2207x\u2081 log p(y | xt, t) must be mod- eled additionally. For y representing class labels, p(y | xt, t) can be approximated with an auxiliary time-dependent classifier pp(y | xt, t) trained on noisy images {xt}t\u2208[0,1]. Incorporating po into the sampling process is termed as classifier guidance (CG), and can be strengthened (or weakened) with guidance scale s through \u2207x\u2081 log p(xt, t)+s\u00b7\u2207x\u2081 log p(xt, t | y). Therefore, class-conditional sampling in SGMs amounts to additionally maximizing the likelihood pp(y | xt,t) of the classi- fier throughout the generative process to arrive at images from the data manifold, which resemble (according to p\u2084) instances of a specific class. We emphasize this fact here for further reference.\nI2SB. The framework of I2SB ex- tends SGMs to p\u2081 representing an arbitrary data distribution. For training, I2SB requires paired data, e.g., in the form of clean and par- tially masked samples for inpainting, where it learns to infill the miss- ing parts. While SGMs can also be adapted to solve inverse prob- lems like inpainting, I2SB maps these samples directly (see Fig. 2 for a comparison of their generative tra- jectories). Therefore, I2SB follows the same theoretical paradigm, where sampling is achieved by discretizing Eq. (1b) and using a score approximator sy, but the generative process begins from a corrupted (e.g., masked) image instead of pure noise. Hence, I2SB can also be adapted to conditional gen- eration in the same manner as SGMs, especially for class-conditioning with an auxiliary classifier. Importantly, a special case of I2SB follows an optimal transport ordinary differential equation (OT- ODE) when \u03b2t \u2192 0, eliminating stochasticity beyond the initial sampling step (see Appendix). We utilize the OT-ODE version of I2SB in our implementation.\nSGM-based VCEs. The initial approach of adapting SGMs to VCE generation, DiME (Jeanneret et al., 2022), obtains the classifier's gradient by mapping the noised image to its clean version at each step through the reverse process. Augustin et al. (2022) incorporate the gradient of a robust classifier and a cone projection scheme. Jeanneret et al. (2023) decompose the VCE generation into pre-explanation construction and refinement using RePaint (Lugmayr et al., 2022). Jeanneret et al. (2024) utilize a foundation model, Stable Diffusion (SD, Rombach et al. (2022)), to generate VCES in a black-box scenario. Farid et al. (2023) and Motzkus et al. (2024) utilize Latent Diffusion Models (LDMs), including SD, in a white-box context. Weng et al. (2024) propose FastDiME to accelerate the generation process in a shortcut learning scenario. Also in black-box context, Sobieski & Biecek (2024) utilize a Diffusion Autoencoder (Preechakul et al., 2022) to find semantic latent directions that globally flip the classifier's decision. Finally, Augustin et al. (2024) also make use of SD in various contexts, including classifier disagreement and neuron activation besides VCEs."}, {"title": "3 METHOD", "content": "In this section, we describe the details of our approach, beginning with the formulation of RVCES as solutions to conditional inpainting task. Next, we motivate the use of I2SB as an effective prior for synthesizing meaningful RVCEs and follow with a series of steps that better align the gradients of a standard classifier w.r.t corrupted images from its generative trajectory. We conclude with a description of the automated region extraction method, forming the basis of our algorithm."}, {"title": null, "content": "RVCEs through conditional inpainting. We define the problem of finding RVCEs for the classifier f from a given image x*, a region R and target class label y, where arg maxy, f (y' | x*) \u2260 y, as the task of sampling from\n$\\begin{array}{c} p(x | \\arg \\max _{y^{\\prime}} f(y^{\\prime} | x)=y,(1-R) \\odot x=(1-R) \\odot x^{*}), \\end{array}$ (2)\nwhere R is a binary mask with 1 indicating the region. Intuitively, sampling from Eq. (2) means obtaining x with the complement of R unchanged and the content of R modified in a way that changes the decision of f to y, i.e., performing inpainting with additional condition coming from the classifier f.\nSynthesizing meaningful RVCEs. Looking at Eq. (2), one quickly realizes that obtaining seman- tically meaningful RVCEs requires maximizing the likelihood f(y | x) of the classifier while in- painting R with content that keeps x in the data manifold. These conditions greatly resemble the CG scheme in the context of I2SB, since the score estimate sy serves as an effective prior for gen- erating in-manifold infills, while the likelihood pp (y | x) of an auxiliary classifier is maximized to ensure that po predicts them as instances of y. Moreover, I2SB maps masked images directly to clean samples, leaving the content outside R unchanged in the final image.\nThe above arguments suggest that inserting f in place of pp should function as an effective mech- anism for sampling meaningful RVCEs. However, a fundamental drawback of this naive approach is that, throughout the generative process, f's gradients originate from evaluating it on images with highly noised infills inside R (see Fig. 2). Such corrupted images are far from what f observed during training, hence leading to a misalignment of its gradients with the correct trajectory and generation of out-of-manifold samples. Similar issue has been identified by previously mentioned SGM-based methods for VCEs, which can be generally unified as attempts to replace the auxiliary classifier po with f in the CG scheme in SGMs and correct f's gradients. Following Fig. 2, one should expect the misalignment in these methods to be of great extent, as the generative trajectory consists of highly noised images, leaving no meaningful content for f to provide accurate gradients. There, as shown in Fig. 2, I2SB provides a crucial advantage, which stems from its generative tra- jectory being much closer to the data manifold. Moreover, by using I2SB, f is able to effectively utilize the readily available context outside R. Hence, in the following, we focus on reducing the misalignment problem caused by the noised content inside R, in the end arriving at a highly effective algorithm for meaningful RVCES.\nAligning the gradients. We propose to adapt the gradients of f to properly align with the generative trajectory of I2SB through a series of incremental steps. To provide the intuition standing behind the introduction of each consecutive improvement, Fig. 3 provides an example RVCE task, where the factual image depicts a zebra correctly predicted by the model (ResNet50 (He et al., 2016)), and the goal is to change the decision to 'sorrel'. We set the region constraint to include the entire animal to make the task challenging enough and verify the improvements quantitatively through a large-scale experiment with around 2000 images. For each step, we compute FID between the RVCEs and original images to assess their realism. For details on the experimental setup, see Appendix.\nNaive. We first verify that naively plugging f in place of pp does not provide meaningful results. Indeed, as shown in Fig. 3, the method struggles to include the information from f. The unrealistic infill also suggests that the classifier's signal negatively influences the score from I2SB."}, {"title": null, "content": "Tweedie's formula. To begin with closing the gap between the data manifold and the generative trajectory, we refer to a classic result of Tweedie's formula (Robbins, 1992; Chung et al., 2022), which states that a denoised estimate of the final image at step t can be achieved by computing the posterior expectation\n$\\begin{array}{c} x_{0}\\left(x_{t}\\right):=\\mathbb{E}\\left[x_{0} | x_{t}\\right]=x_{t}+\\sigma_{t}^{2} \\nabla_{x_{t}} \\log p\\left(x_{t}, t\\right), \\end{array}$ (3)\nwhere $\\sigma_{t}^{2}=\\int_{t}^{T} \\beta_{t} d t$. For visual differences between x\u2081 and 80(xt), see Appendix. Crucially, one has access to approximate 20(xt) at every step t by utilizing I2SB as the approximate score. Replacing \u2207x, log f (y | xt) with \u2207x\u2081 log f (y | 20(xt)) brings the inputs of f much closer to what it expects, improving the conditional inpainting process as indicated by Fig. 3, which now shows a structure resembling a sorrel and a much smaller FID.\nADAM stabilization. Despite utilizing the Tweedie's estimate, we observed the norms of f's gradi- ents to have a very noisy tendency throughout the generation process, pointing out a possible cause for visible artifacts and the missing parts of the animal. Hence, we propose to smooth out the gra- dients by applying the ADAM update rule at each step (Vaeth et al., 2024; Kingma, 2019), to which we simply refer as ADAM stabilization. Figure 3 indicates that this modification allows for filling in the missing parts of the sorrel and further lowering FID.\nAdaptive normalization. Incorporating ADAM stabilization required greatly lowering the guid- ance scale to values on the order of le-2, as using standard s = 1 led to extreme artifacts. This phenomenon suggested that the step size could also be adjusted throughout the generation process. While we initially experimented with various types of schedulers (see Appendix), using adaptive normalization has empirically proven to be the most effective approach. Specifically, at the begin- ning of the conditional inpainting process, we register the norm of the first encountered gradient of the log-likelihood of f. We then use it as a normalizing constant for each subsequent gradient, meaning that the generation begins with gradient of unit norm. This simple modification not only further lowered FID, but also reduced the final visible artifacts and improved color balance (Fig. 3).\nTrajectory truncation. Up until this point, we relied solely on the ability of I2SB and the classifier's signal to correctly infill the missing regions with semantically meaningful content, with no knowl- edge of the structure of the missing objects. Since a possible infill of the region is always available from the original image, one can begin the inpainting process from some intermediate step instead of the final one. This intervention allows for mixing the available information with the one coming from the classifier, and gives direct control over the preservation of the original content. As our ap- proach does not bias the conditional score with signal from any additional losses (like Learned Per- ceptual Image Patch Similarity (LPIPS Zhang et al. (2018)) or 12 in other works), we can fully rely on the conceptual compression of I2SB, similarly to SGMs (Ho et al., 2020), which decomposes the generation process into initial phases responsible for the overall structure of objects and later ones re- sponsible for small details. Figure 3 showcases the effect of using this trajectory truncation (T) at the 0.4 level, meaning that the infilling process starts from t = \u03c4\u00b7T, where T denotes the final timestep. Understandably, trajectory truncation greatly lowers the FID score, as much more informa- tion is available from the very beginning of the process, and introduces much more sub- tle changes to the image. We explore the ef- fect of manipulating 7 further in the Appendix, showing that it functions as a very interpretable mechanism for controlling the content preser- vation.\nAutomated region extraction. While the introduced algorithmical improvements effec- tively incorporate the classifier's signal into the inpainting process, they do not address the issue of predetermining the region for the resulting explanation. To this end, the optimal strategy would be fully automated and focus on regions that are both important to the classifier's predic- tion and point to semantically meaningful con- cepts. This description closely resembles the"}, {"title": "4 EXPERIMENTS", "content": "Following previous works for VCEs on Ima- geNet, we base the quantitative evaluation on 3 challenging main VCE generation tasks: Ze- bra - Sorrel, Cheetah \u2013 Cougar, Egyptian Cat - Persian Cat, where each task requires creating VCEs for images from both classes and flipping the decision to their counterparts. We treat it as a general benchmark for evaluat- ing the effectiveness of RCSB in various sce- narios. We use FID (\u2193) and sFID (\u2193) to assess realism (Heusel et al., 2017), S\u00b3 (\u2191) for repre- sentation similarity (Chen & He, 2021), COUT \u2208 [-1,1] (\u2191) (Khorram & Fuxin, 2022) for sparsity and Flip Rate (FR) (\u2191) for efficiency.\nRCSB sets new SOTA for VCEs. We first ver- ify that synthesizing RVCEs with RCSB leads to new SOTA in VCE generation.\nOur RVCES are much more realistic (at least 2 4x de- crease in FID and sFID), stay close to orig- inal images (match or exceed best values of S3) and almost always flip the model's decision (FR \u2248 1.0). RCSB also solves a long-standing challenge of achieving extremely sparse expla- nations on ImageNet"}, {"title": null, "content": "Table 2(A) shows the results of this evaluation together with the used text prompt. Despite I2SB being trained on masks covering at most 30% of the image area, we observed that it generalizes well beyond this threshold with 40% starting to pose a challenge. Crucially, despite the regions being classifier-agnostic and hence not necessarily focused on the most influential pixels, Table 2(A) indicates that RCSB is versatile enough to maintain most of the performance from the automated approach.\nRegions that contain exactly the objects of interest provide novel insights about the model's reason- ing. For example, consider the Lemon \u2192 Orange task from Fig. 6, where the lemons were correctly identified by the ResNet50 model. One would require the VCE for this task to indicate the sole\ndetermining factor of 'why lemons and not oranges'.\nRVCEs also allow for clarifying the model's decision-making when its reasoning is not initially understandable.\nDiscovering complex patterns with interactive RVCEs. Despite the impressive capabilities of deep models in object localization, the receiver of the explanation may be interested in testing the model for highly abstract and complex concepts that cannot be localized automatically and must be provided manually by the user. We begin with verifying the capability of RCSB in generating RVCEs based on user-defined regions by simulating such scenario at scale.\nTable 2(B) highlight the versatility of RCSB, which is able to effectively utilize the restricted resources to influence the classifier's prediction."}, {"title": null, "content": "Ablating RCSB's components. We empirically verified that combining our novel guidance mecha- nism with the I2SB prior leads to highly effective RVCEs.\nTable 2(C) shows the results of the ablation study. Despite the fact that the used methods were never explicitly trained for inpainting, combining them with our guidance mechanism and region extrac- tion allows for matching or even exceeding previous SOTA. For example, all adaptations achieve very high sparsity, almost always flip the classifier's decision and keep the explanation close to the original. This indicates the benefits of utilizing only the pixels from the extracted region and a proper utilization of the classifier's gradients without biasing them with additional components like LPIPS or 12 loss."}, {"title": "5 CONCLUSIONS", "content": "Our work advances the SOTA in VCE generation by constraining the explanations to differ from the factual image exclusively within a predetermined region. RCSB is not only very effective in sampling such explanations, proven by new quantitative records, but also showcases the novel ca- pabilities for explaining image classifiers enabled by RVCEs. Specifically, to properly reason about the model's decision-making process, one must ensure that the potential confounding factors are limited to the greatest possible extent. RVCEs obtained with RCSB allow to do that in a wide range of scenarios, ranging from a fully automated approach to incorporating the user directly into the interactive explanation creation process. Our work establishes a new paradigm for explaining im- age classifiers in a much more principled manner, allowing the receiver to infer causally about the model's reasoning."}]}