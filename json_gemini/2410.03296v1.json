{"title": "Comparing zero-shot self-explanations with human rationales in multilingual text classification", "authors": ["Stephanie Brandl", "Oliver Eberle"], "abstract": "Instruction-tuned LLMs are able to provide an explanation about their output to users by generating self-explanations that do not require gradient computations or the application of possibly complex XAI methods. In this paper, we analyse whether this ability results in a good explanation by evaluating self-explanations in the form of input rationales with respect to their plausibility to humans as well as their faithfulness to models. For this, we apply two text classification tasks: sentiment classification and forced labour detection. Next to English, we further include Danish and Italian translations of the sentiment classification task and compare self-explanations to human annotations for all samples. To allow for direct comparisons, we also compute post-hoc feature attribution, i.e., layer-wise relevance propagation (LRP) and apply this pipeline to 4 LLMs (Llama2, Llama3, Mistral and Mixtral). Our results show that self-explanations align more closely with human annotations compared to LRP, while maintaining a comparable level of faithfulness.", "sections": [{"title": "1 Introduction", "content": "Providing model explanations in order to increase trust and transparency into their decision making has been a key motivation for the field of Explain-able AI (XAI) even before LLMs have found their way into everyday life. Nowadays, LLMs are being used for tasks like creative writing, help with home-work but also for advice giving and translation while providing self-generated explanations at the same time. This makes it all the more crucial to understand the quality of those self-explanations, how reliable they are and whether their faithfulness to the model and their plausibility to humans compare with other, widely investigated, post-hoc XAI methods. In this paper, we evaluate self-explanations from two text classification tasks for which human rationale annotations are available: sentiment classification and forced labour detection. We instruct 4 different LLMs (Mistral, Mixtral, Llama2 and Llama3) to solve the respective task and generate explanations based on the input text in a zero-shot experiment. We compare these with rationales provided on the same samples from various annotation studies and with state-of-the-art post-hoc explanations for Transformers, calculated based on the layer-wise relevance propagation (LRP) framework (Ali et al., 2022) for each model respectively. For sentiment classification, we consider two different subsets from two different annotation studies, one also including Italian and Danish translations next to the English original. We evaluate on plausibility to the human annotations and faithfulness to the model decision, two established evaluation methods in the XAI literature (DeYoung et al., 2020; Jacovi and Goldberg, 2020). We further carry out qualitative analyses, for instance on the distribution of POS tags among the most frequently selected tokens, the differences between languages and the ability to follow precise instructions. We see this controlled study as a first step into a deeper understanding of the reliability and quality of self-explanations and we believe that by covering four language models, three languages and two very different domains of text classification tasks, with different levels of difficulty and text length we can provide valuable insights. We release our code on GitHub to enable reproducibility and potential future research."}, {"title": "2 Related Work", "content": "Generated self-explanations come with both new opportunities and challenges. Prior work in self-explanations for text has focused on new evaluation strategies and model improvements. Ye and Durrett (2022) evaluate whether including self-explanations can improve model performance on in-context learning while Madsen et al. (2024) proposed several instruction-based self-consistency checks to measure faithfulness in generated explanations.\nAnother line of work by Wiegreffe et al. (2022) seeks to improve free text self-explanations with the help of human-written explanations that are included in the instruction. Similarly to Kunz and Kuhlmann (2024), self-explanations are evaluated on a variety of properties by the means of human annotation. They are found to be generally true, grammatical and factual (Wiegreffe et al., 2022) and further selective, to contain illustrative examples and rarely subjective according to Kunz and Kuhlmann. For those two papers GPT-3 on CommonsenseQA/NLI and GPT-4 on the Alpaca dataset were analysed, respectively. In our study, we consider human rationales as the ground truth for explaining a decision, against which we compare model self-explanations and post-hoc attributions. Recent work by Huang et al. (2023) investigates self-explanations by ChatGPT on sentiment classification for SST, comparing faithfulness of self-explanations against different features attribution methods. They experiment with different settings by swapping the order of classification and explanations within a single instruction prompt, asking the model for top-k rationale tokens or continuous token scores, but find no method that stands out in faithfulness while observing significant disagreement across explainability approaches.\nOur work focuses on a direct comparison of plausibility and faithfulness, using binary rationales and comparing them to post-hoc LRP attributions, which have been shown to faithfully reflect LLM predictions (Ali et al., 2022; Achtibat et al., 2024). Additionally, we extend our analysis to a more complex text classification task-forced labor detection-and to multilingual settings, further broadening the scope and applicability of our study."}, {"title": "3 Experimental Setup", "content": "We selected two text classification datasets for sentiment analysis and forced labor detection, for which human rationale annotations have been collected. With those two dataset we cover different aspects and levels of difficulty in both classification and rationale annotation. SST has been widely used for binary sentiment classification, with rationales available in English, Italian and Danish subsets. Texts are rather short and language models have been shown to solve this task successfully, while the second dataset of longer news articles on forced labour detection is more challenging for both classification and rationale extraction, and is also less likely to have been part of the models' pre-training.\nWe use two different subsets from the Stanford Sentiment Treebank (SST2, Socher et al. 2013) for binary sentiment classification on movie reviews. The first subset contains 263 samples from the validation and test split from SST2 with an average sentence length of 18 tokens. Human rationale annotations have been published for that subset by Thorn Jakobsen et al. (2023) where each sample has been annotated by multiple annotators, 8 on average, who were recruited via Prolific. Annotators were first asked to classify the sample into one of three classes: positive, neutral or negative where none of the sentences was assigned neutral as a gold label. In a second step, annotators should choose the parts of the input that support their label choice. We select the rationale annotations with the correct labels from the first step for further analysis. We averaged the binary rationales across all annotators (with correct label classification) and set a threshold of 0.5 (after averaging) for the token selection. We additionally analyse the rationale annotations collected by J\u00f8rgensen et al. (2022) on a subset of 250 samples from the validation set of SST2. All samples were translated into Danish and Italian with an average sentence length of 15-17. Rationale annotation was carried out by 2 annotators per language (including English), who were native speakers with linguistic training. In contrast to the annotations collected by Thorn Jakobsen et al., the correct sentiment (positive or negative) was provided and the annotators were asked to select parts of the input that supported the gold label.\nThe authors of Mendez Guzman et al. (2022) published a Rationale-annotated corpus for"}, {"title": "3.1 Datasets", "content": "We selected two text classification datasets for sentiment analysis and forced labor detection, for which human rationale annotations have been collected. With those two dataset we cover different aspects and levels of difficulty in both classification and rationale annotation. SST has been widely used for binary sentiment classification, with rationales available in English, Italian and Danish subsets. Texts are rather short and language models have been shown to solve this task successfully, while the second dataset of longer news articles on forced labour detection is more challenging for both classification and rationale extraction, and is also less likely to have been part of the models' pre-training."}, {"title": "3.2 Rationale Extraction", "content": "For our experiments, we apply the same pipeline to the following 4 instruction fine-tuned LLMs: Llama2-13B, Llama3.1-8B, Mistral-7B and quantized Mixtral-8x7B.\nIn a first step, we ask the model to classify the given text into positive/negative for SST and into yes/no depending on evidence for a specific risk indicator for the RaFoLa dataset. If the model manages to generate the correct answer, we ask it to generate rationales based on the relevant provided context of the input. In case of RaFoLa, we follow the original data collection and only request rationales if the respective risk indicator is present. For the subsets in Italian and Danish, we have manually translated the prompts to the respective language with the help of native speakers."}, {"title": "3.3 LRP Relevances", "content": "To extract input attribution scores, we use layer-wise relevance propagation (LRP); a widely used and state-of-the-art XAI method to compute feature attributions (Ali et al., 2022; Achtibat et al., 2024). Following the proposed propagation rules for Trans-"}, {"title": "3.4 Constraining Self-Explanations", "content": "Initial experiments showed that without precise instructions, the model would return 80% of the input tokens as rationales for SST where humans had annotated approximately 30%. This made comparisons difficult, so we chose to request a maximum number of tokens based on the number of annotated tokens by humans for each sample. Language models did not always follow this request but we could reduce the ratio of tokens to a comparable level with human annotations. For RaFoLa, this problem did not occur on the same level since the input texts were much longer and humans annotated entire phrases. We thus decided not to include an upper bound for the RaFoLa rationales. We will discuss ratios and instruction following in more details."}, {"title": "4 Results", "content": "We first show and discuss the main results of pair-wise agreement between the different kinds of rationales, i.e., plausibility scores, before further analysing their faithfulness, the differences between languages and a qualitative analysis."}, {"title": "4.1 Plausibility", "content": "We show pair-wise comparisons between human-annotated, model-generated and post-hoc computed rationales by calculating sample-wise Cohen's Kappa scores between the binary scores and averaging across samples for different models. When considering human annotations as the ground truth, this is usually referred to as plausibility (DeYoung et al., 2020). We here also show the comparison between post-hoc and self-generated rationales, i.e., self-explanations.\nCohen's Kappa (Cohen, 1960) is a well-established method to measure inter-annotator agreement (IAA) between two annotators, in our case those are either the averaged human annotations or the two different types of model rationales"}, {"title": "4.2 Task accuracy and rationale ratios", "content": "Table 1 presents model accuracies and ratios of identified rationale tokens for SST, multilingual"}, {"title": "4.3 Faithfulness", "content": "Besides plausibility, faithfulness is the most commonly used evaluation approach to judge the quality of model explanations. Especially for feature attribution approaches, removal of most relevant features has been used to assess how faithful a feature subset is with respect to the model prediction, i.e. if removing a highly relevant subset will lead to a strong decrease of the prediction. We evaluated faithfulness by measuring the change in probability after masking the tokens as identified by the different rationales (human, self-generated and post-hoc).\nCompared to human and self-generated rationales, post-hoc attributions provide a relevance score for each token in the input prompt, requiring to binarize post-hoc attributions to allow for direct comparison as described in Section 3.3. Addition-"}, {"title": "4.4 Languages", "content": "From the set of models we consider for this study, all have been pre-trained on English data while Llama3 and Mixtral also have been pre-trained on Italian, but none of the model officially supports Danish. Considering this difference in language exposure, our results show that all models are able to solve the sentiment classification task (see Table 1) and are also able to extract meaningful rationales with plausibility on the same (or even superior) level than English. Plausibility scores between self-explanations and humans are ranging 0.46 \u2013 0.74 for Danish and Italian versus 0.4 - 0.69 for English. Llama2 shows the biggest gap between English (0.67) and Danish/Italian (0.46/0.52) while Mixtral shows an increase in plausibility from English (0.4) to (0.57) for Danish. Llama3 shows high and stable plausibility scores across all languages (~ 0.7). It seems all models can handle both Danish and Italian, whether this is based on data contamination during pre-training, e.g., training data for Llama2 has been reported to contain 0.11% Italian (Touvron et al., 2023), is unclear. Previous work has focused on zero-shot abilities in unseen languages but has mostly followed ap-"}, {"title": "4.5 Selected tokens", "content": "Plausibility scores between humans and self-explanations (human \u00d7 model) for RaFoLa vary by a magnitude of 2-3 between the different indicators, for instance Llama3 shows an agreement of 0.16 for #1 Abuse of vulnerability and 0.44 for #8"}, {"title": "4.6 POS analysis", "content": "In order to gain a deeper insight into the differences between rationale selection, we are extracting part-of-speech (POS) tags of each selected tokens. We show results for the monolingual SST dataset in Figure 5. We see that for humans and self-"}, {"title": "4.7 Instruction following", "content": "For generating and processing the self-explanations, we instructed the models to return rationales in a json format. Since many of those outputs resulted in SyntaxErrors, we included a syntax check based on Llama3 where we instructed the model in a separate step to correct the json syntax in case such an error occurred. The ability to return correct syntax varied across models. We also saw differences when following the instruction of returning the correct number of rationales for which we set an upper bound for SST based on the human annotations. We show results for SST for all 4 models, averaged across subsets and languages in Table 3. The results show that Llama2 has a lot of difficulties with respect to json syntax with syntax errors occurring in 86% of the"}, {"title": "4.8 Contrastive LRP", "content": "In a complementary analysis, we test the alignment of model rationales with post-hoc attributions, examining whether there is a difference in the plausibility of contrastive and non-contrastive post-hoc explanations. Prior work has suggested that contrastive explanations are more aligned with human reasoning and are thus considered more valuable for humans to understand the model's decisions (Lipton, 1990; Miller, 2019; Jacovi et al., 2021).\nComparing Cohen's Kappa scores shown in Figure 6 suggests that contrastive post-hoc approaches do not generally result in higher plausibility than non-contrastive ones. Similarly, contrastive explanations overall exhibit a similar level of faithfulness as non-contrastive explanations (cf. Figure 4). Although contrastive explanations can be more faithful and plausible in certain cases, such as SST and mSST (English) for Llama3, there is no consistent difference between the two approaches. This aligns with previously reported high correlation between them (Eberle et al., 2023) and the minor observed performance differences for generating the correct label based on contrastive or non-contrastive post-hoc approaches (Krishna et al., 2023)."}, {"title": "5 Discussion", "content": "In this paper, we investigated explanations generated by 4 different instruction-tuned LLMs for 2 text classification tasks in English but also in Italian and Danish. Those generated explanations, i.e., self-explanations, were limited to the input tokens of the respective text samples. We compared those self-explanations on the tasks of sentiment classification (SST) and forced labour classification (RaFoLa) with human annotations for the same samples and post-hoc feature attribution with layer-wise relevance propagation (LRP). Pairwise comparison between the three different types of explanations (humans, generated, post-hoc) shows that human annotations and generated explanations agree on a much higher level than post-hoc with any of them. We further saw that Llama3 has shown the highest level of agreement across both tasks and all languages and followed the instructions more closely in comparison to the other models. Our POS analysis confirmed this finding and showed higher agreement with the \"type\" of tokens selected by humans compared to post-hoc rationales.\nBesides established post-hoc attribution approaches, the ability of language models to provide self-generated explanations, i.e., self-explanations, has offered a direct and human-understandable communication between user and model. This not only enhances usability in particular for lay people but, as presented here, also results in explanations that are similarly faithful and align more closely with human rationales compared to binarized post-hoc attributions. Herein, the generation process behind self-explanations remains obfuscated and may suffer from counterfactuality, enabling the model to give untruthful explanations for correct predictions (Ji et al., 2023). We further find that current language models require careful instructions to provide useful self-generated explanations.\nWhile human plausibility is a desired property of predictions and explanations made by machine learning systems, LLMs are capable of identifying alternative solutions to task-solving that may not be intuitive to humans. Good explanations should thus also highlight the learned prediction strategy that is faithful to the one used by the model-even if it is not directly plausible (Agarwal et al., 2024). In contrast to post-hoc attributions, which are directly derived from the prediction score (e.g., using explanatory gradients as for the LRP attributions considered here), the self-generation process is not explicitly tied to the prediction in a clear manner. Attribution approaches herein consider the entire input to rank relevant features, including both the provided context and all instructions. Instruction templates are crucial for understanding and solving tasks, and we find that tokens related to task instructions often play a more significant role than the context itself. While this effectively identifies relevant task features from the perspective of the model, users are typically more interested in understanding which parts of the provided evidence are most relevant. Disentangling task instructions from contextual evidence is essential for aligning insights with user expectations and should inform the development of future interpretability methods. Our study represents a first step toward understanding and building more intuitive model explanations by directly comparing human annotations with those generated by models. Evaluating free-text explanations for factuality, usability, and faithfulness is crucial for ensuring their practical and intuitive application, especially given the growing use of increasingly complex LLMs by lay people who may not fully understand their mechanisms\nWe currently do not know why self-explanations align more closely with human annotations than post-hoc attributions. While training procedures such as reinforcement learning from human feedback (Ziegler et al., 2019) may incentivize more human-like explanations (Agarwal et al., 2024), limited access to models, procedures, and datasets, restricts detailed analysis.\nThe ability of the models to solve the sentiment classification task in unseen languages like Danish is remarkable. Not only is the model able to understand the Danish prompt, it also is able to return specific input tokens that are aligned with the human selection. Future work could investigate whether this zero-shot performance is specific to certain language families and tasks or if it extends to more complex instructions and datasets.\nAs language models have advanced and become widely adopted, understanding the mechanisms behind their predictions has emerged as a critical challenge. Using human rationale annotations across three datasets, we have investigated the human plausibility and faithfulness of self-generated explanations to post-hoc feature attributions. We find that self-generated explanations agree more closely with human rationales than post-hoc feature attribution, while remaining faithful to the model prediction."}, {"title": "Limitations", "content": "We acknowledge that annotations may be affected by annotator bias, varying guidelines, and differing expertise, impacting the consistency of rationales. Also the number of annotators and the level of details in the instructions varied across the annotation studies we have considered for this paper. Furthermore, for the forced labour detection, annotations by legal scholars might differ from the ones provided and would also be interesting to compare with model rationales.\nWe focus our study on rationales based on the input while free text explanations might provide more useful information and pose the more realistic scenario.\nWhile agreement between human and model rationales may be desired, it has been shown in previous work, that humans do not necessarily prefer human-written explanations in comparison to the ones generated by LLMs in the case of free text explanations (Wiegreffe et al., 2022).\nThe high zero-shot performance, especially with SST, may be an effect of data contamination, which is likely part of the training data. We can further not exclude the possibility that rationales or task explanations have been included in the training corpus."}, {"title": "A Instructions", "content": ""}, {"title": "A.1 SST", "content": ""}, {"title": "A.2 RaFoLa", "content": ""}]}