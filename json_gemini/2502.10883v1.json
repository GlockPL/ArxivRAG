{"title": "Learning Identifiable Structures Helps Avoid Bias in DNN-based Supervised Causal Learning", "authors": ["Jiaru Zhang", "Rui Ding", "Qiang Fu", "Huang Bojun", "Zizhen Deng", "Yang Hua", "Haibing Guan", "Shi Han", "Dongmei Zhang"], "abstract": "Causal discovery is a structured prediction task that aims to predict causal relations among variables based on their data samples. Supervised Causal Learning (SCL) is an emerging paradigm in this field. Existing Deep Neural Network (DNN)-based methods commonly adopt the \"Node-Edge approach\u201d, in which the model first computes an embedding vector for each variable-node, then uses these variable-wise representations to concurrently and independently predict for each directed causal-edge. In this paper, we first show that this architecture has some systematic bias that cannot be mitigated regardless of model size and data size. We then propose SiCL, a DNN-based SCL method that predicts a skeleton matrix together with a v-tensor (a third-order tensor representing the v-structures). According to the Markov Equivalence Class (MEC) theory, both the skeleton and the v-structures are identifiable causal structures under the canonical MEC setting, so predictions about skeleton and v-structures do not suffer from the identifiability limit in causal discovery, thus SiCL can avoid the systematic bias in Node-Edge architecture, and enable consistent estimators for causal discovery. Moreover, SiCL is also equipped with a specially designed pairwise encoder module with a unidirectional attention layer to model both internal and external relationships of pairs of nodes. Experimental results on both synthetic and real-world benchmarks show that SiCL significantly outperforms other DNN-based SCL approaches.", "sections": [{"title": "1 Introduction", "content": "Causal discovery seeks to infer causal structures from an observational data sample. Supervised Causal Learning (SCL) (Dai et al., 2023; Ke et al., 2023; Ma et al., 2022) is an emerging paradigm in this field. The basic idea is to consider causal discovery as a structured prediction task, and to train a prediction model using supervised learning techniques. At training time, a training dataset comprising a variety of causal mechanisms and their associated data samples is generated. The prediction model is then trained to take such a data sample as input, and to output predictions about the causal mechanism behind the data sample. Compared to traditional rule-based or unsupervised methods (Glymour et al., 2019), the SCL method has demonstrated strong empirical per-"}, {"title": "2 Background and Related Work", "content": "A Causal Graphical Model is defined by a joint probability distribution Pover multiple random variables and a DAG G. Each node X\u1d62 in G represents a variable in P, and a directed edge X\u1d62 \u2192 X\u2c7c represents a direct cause-effect relation from X to X\u2c7c. A causal discovery task generally asks to infer about G from an i.i.d. sample of P.\nHowever, there is a well-known identifiability limit for causal discovery. In general, the causal DAG is only identifiable up to an equivalence class. Studies of this identifiability limit under a canonical assumption setting have led to the well-established MEC theory (Frydenberg, 1990; Verma and Pearl, 1990). We call a causal feature, MEC-identifiable, if the value of this feature is invariant among the equivalence class under the canonical MEC assumption setting. It is known that such MEC-identifiable features include the skeleton and the set of v-structures, which we briefly present in the following.\nA skeleton E defined over the data distribution Pis an undirected graph where an edge exists between X\u1d62 and X\u2c7c if and only if X\u1d62 and X\u2c7c are always dependent in P, i.e., \u2200Z \u2286 {X\u2081, X\u2082, \u2026, X\u2090} \\ {X\u1d62, X\u2c7c}, we have X\u1d62 \\ X\u2c7c | Z. Under mild assumptions (such as that P is Markovian and faithful to the DAG G; see details in Appendix Sec. A1.1), the skeleton is the same as the corresponding undirected graph of the DAG G (Spirtes et al., 2000). A triple of variables (X, T, Y) is an Unshielded Triple (UT) if X and Y are both adjacent to T but not adjacent to each other in (the skeleton of) G. It becomes a v-structure denoted as X \u2192 T \u2190 Y if the directions of the edges are from X and Y to T.\nTwo graphs are Markov equivalent if and only if they have the same skeleton and v-structures. The Markov equivalence class (MEC) can be represented by a Completed Partially Directed Acyclic Graph (CPDAG) consisting of both directed and undirected edges. We use CPDAG(G) to denote the CPDAG derived from G. According to the theorem of Markov completeness (Meek, 1995b), we can only identify a causal graph up to its MEC, i.e., the CPDAG, unless additional assumptions are made (see the remark below). This means that each (un)directed edge in CPDAG(G) indicates a (non)identifiable causal relation.\nRemark: The MEC-based identifiability theory is applicable in the general-case setting, when we take into account all possible distributions P. It is known this identifiability limit could be broken (i.e., an undirected edge in the CPDAG could be oriented) if we assume that the data follows some special class of distributions, e.g., linear non-Gaussians, additive noise models, post-nonlinear or location-scale models (Peters et al., 2014; Shimizu et al., 2011; Zhang and Hyv\u00e4rinen, 2009; Immer et al., 2023). These assumptions are sometimes hard to verify in practice, so this paper considers the general-case setting. More discussions on the identifiability and causal assumptions are provided in Appendix Sec. A2."}, {"title": "2.1 Related Work", "content": "In traditional methods of causal discovery, constraint-based methods are mostly related to our work. They aim to identify the DAG that is consistent with inter-variable conditional independence constraints. These methods first identify the skeleton and then conduct orientation based on v-structure identification (Yu et al., 2016). The output is a CPDAG which represents the MEC. Notable algorithms in this category include PC (Spirtes et al., 2000), along with variations such as Conservative-PC (Ramsey et al., 2012), PC-stable (Colombo et al., 2014), and Parallel-PC (Le et al., 2016). Compared to constraint-based methods, both our approach and theirs are founded upon the principles of MEC theory for estimating skeleton and v-structures. However, whereas traditional methods rely on symbolic reasoning based on explicit constraints, we employ DNNs to capture the essential causal information intricately linked with these constraints.\nScore-based methods aim to find an optimal DAG according to a predefined score function, subject to combinatorial constraints. These methods employ specific optimization procedures such as forward-backward search GES (Chickering, 2002), hill-climbing (Koller and Friedman, 2009), and integer programming (Cussens, 2011). Continuous optimization methods transform the discrete search procedure into a continuous equality constraint. NOTEARS (Zheng et al., 2018) formulates the acyclic constraint as a continuous equality constraint and is further extended by DAG-GNN (Yu et al., 2019), DECI (Geffner et al., 2022) to support non-linear causal relations. DECI (Geffner et al., 2022) is a flow-based model which can perform both causal discovery and inference on non-linear additive noise data. Recently, ENCO (Lippe et al., 2022) is proposed as a continuous optimization method where the edge orientation is modeled as a separate parameter to maintain the acyclicity. It is guaranteed to converge to the correct graph if interventions on all variables are available. RL-BIC (Zhu et al., 2020) utilizes Reinforcement Learning to search for the optimal DAG. These methods can be viewed as unsupervised since they do not access additional datasets associated with ground truth causal relations. We refer to Glymour et al. (2019); Vowels et al. (2022) for a thorough exploration of this literature."}, {"title": "3 Limitations of the Node-Edge Architecture", "content": "The Node-Edge architecture is common and has been adopted to generate the output DAG G in the literature (Lorch et al., 2022; Zhu et al., 2020; Charpentier et al., 2022; Varambally et al., 2024). In this architecture, each entry G\u1d62\u2c7c in the DAG is independently sampled from A\u1d62\u2c7c, an entry in the adjacency matrix A. This entry A\u1d62\u2c7c represents the probability that i directly causes j. We introduce a simple yet effective example setting with only three variables X, Y, and T to reveal its limitation.\nConsidering a simulator that generates DAGs with equal probability from two causal models: In model 1, the causal graph is G\u2081: X \u2192T \u2192 Y, and the variables follow X ~ N(0, 1), T = X +N(0,1), Y = T+N(0,1). In model 2, the causal graph is G\u2082 : X \u2190 T \u2190 Y, and the variables follow Y = N(0,3), T = Y + N(0, 3), X = 0.5T + N(0,0.5). In this case, data samples coming from both causal models follow the same joint distribution, which makes G\u2081 and G\u2082 inherently indistinguishable (from observational data sample).\nMore importantly, when the fully-directed causal DAGs are used as the learning target (as the Node-Edge approach does), an optimally trained neural network will predict 0.5 probabilities on the directions of the two edges X - T and T-Y. As a result, with 0.25 probability the graph sampling outcome would be X\u2192T\u2190Y (see Fig. A4 in the Appendix). This error probability is rooted from the fact that the Bernoulli sampling of the edge X \u2192 T is not conditioned on the sampling result of the edge TY. Consequently, it is a bias that cannot be avoided even if the DNN has perfectly modeled the marginal probability of each edge (marginalized over other edges) given input data.\nWe further find that 0.25 is not the worst-case error rate yet. Formally, for a distribution Q over a set of graphs, we define the graph distribution where the edges are independently sampled from the marginal distribution as M(Q), i.e., for any causal edges e\u2081 and e\u2082, P_{G~Q}(e\u2081 \\in G) = P_{G~M(Q)}(e\u2081 \\in G) = P_{G~M(Q)}(e\u2081 \\in G |e\u2082 \\in G). In general, a Node-Edge model optimally trained on data samples D coming from the distribution Q will essentially learn to predict M(Q) (when given the same data samples Dat test time). The following proposition shows that for causal graphs with star-shaped skeleton, with a chance of 26.42% the graph sampled from the marginal distribution M(Q) would be incorrect.\nProposition 3.1. Let G\u2099 be the set of graphs with n + 1 nodes where there is a central node y such that (1) every other node is connected to y, (2) there is no edge between the other nodes, and (3) there is at most one edge pointing to y. We have\nsup max P_{G~M(Q)}(G \\notin G_n) = 1 - \\frac{2^{n-1}-1}{2^{n-1}} \u2248 0.2642. \\tag{1}\nThe proof is provided in Appendix Sec. A6. It indicates that an edge-predicting neural network could suffer from an inevitable error rate of 0.2642 even if it is perfectly trained. In contrast, models that predict skeleton and v-structures would have a theoretical asymptotic guarantee of the consistency under canonical assumption. The details, proof and relevant discussions are provided in Appendix Sec. A1."}, {"title": "4 The SiCL Method", "content": "In light of the limitations as discussed, we propose a new DNN-based SCL method in this section, named SiCL (Supervised identifiable Causal Learning)."}, {"title": "4.1 Overall Workflow", "content": "Following the standard DNN-based SCL paradigm, the core inference process is implemented as a DNN. The DNN takes a data sample encoded by a matrix D\u2208 \u211d^{n\u00d7d} as input, with d being the number of observable variables and n being the number of observations. In contrast to previous Node-Edge approaches, the SiCL method does not use the DNN to directly predict the causal graph, but instead seeks to predict the skeleton and the v-structures of the causal graph, which amount to the MEC-identifiable causal structures as mentioned previously.\nSpecifically, our DNN outputs two objects: (1) a skeleton prediction matrix S\u2208 [0,1]^{d\u00d7d} where S\u1d62\u2c7c models the conditional probability of the existence of the undirected edge X\u1d62 - X\u2c7c, conditioned on the input data sample D, and (2) a v-structure prediction tensor V \u2208 [0,1]^{d\u00d7d\u00d7d} where V\u1d62\u2c7c\u2096 models the conditional probability of the existence of the v-structure component X\u2c7c \u2192 X\u1d62 \u2190 X\u2096, again conditioned on D. In our implementation, S and V are generated by two separate sub-networks, called Skeleton Predictor Network (SPN) and V-structure Predictor Network (VPN), respectively. To further address the limitation of only having node-wise features for Node-Edge models, we propose to equip SPN and VPN with Pairwise Encoder modules to explicitly capture node-pair-wise features.\nBased on the skeleton prediction matrix S and v-structure prediction tensor V, we infer the skeleton and v-structures of the causal graph; from the two we can determine a unique CPDAG. The CPDAG encodes a Markov equivalence class, from which we can pick up a graph instance as the prediction of the causal DAG (if needed). Figure 1 provides a diagram of the overall inference workflow of SiCL, and a pseudo-code of the workflow is given by Algorithm A1 in Appendix.\nParameters of the SPN and VPN are trained following the standard supervised learning procedure. In our implementation, we only use synthetic training data, which is relatively easy to obtain, and yet could often lead to strong performance on real-world workloads (Ke et al., 2023).\nIn the following, we elaborate the DNN architecture, the learning targets, as well as the post-processing procedure used in the SiCL method."}, {"title": "4.2 Feature Extraction", "content": "Input Processing and Node Feature Encoder. Given input data sample, which is a matrix D \u2208 \u211d^{n\u00d7d}, the input processing module contains a linear layer for continuous input data or an embedding layer for discrete input data, yielding the raw node features F^{raw} for each node i in each observation l. After that, we employ a node feature encoder to further process the raw node features into the final node features F\u1d62\u2097. Similar to previous papers (Ke et al., 2023; Lorch et al., 2022), the node feature encoder is a transformer-like network comprising attention layers over the observation dimension and the node dimension alternately, which naturally maintains permutation equivalence across both variable and data dimension because of the intrinsic symmetry of attention operations. More details about the node feature encoder are presented in Appendix Sec. A4 due to page limit.\nPairwise Encoder. Given node features F \u2208 \u211d^{d\u00d7n\u00d7h} for all the d nodes, the goal of pairwise encoder is to encode their pairwise relationships by d\u00b2 pairwise features, represented as a tensor P\u2208 \u211d^{d\u00d7d\u00d7n\u00d7h}, where P\u1d62\u2c7c\u2097 \u2208 \u211d\u02b0 is a pairwise feature corresponding to the node pair (i,j) in observation 1. As argued in Sec. 1, both \"internal\" information (i.e., the pairwise relationship) and \u201cexternal information (e.g., the context of the conditional separation set) of node pairs are needed to capture persistent dependency and orientation asymmetry. Our pairwise encoder module is designed to model the internal relationship via node feature concatenation and the non-linear mapping by MLP. On the other hand, we employ attention operations within the pairwise encoder to capture the contextual relationships (including persistent dependency and orientation asymmetry).\nMore specifically, the pairwise encoder module consists of the following parts (see Appendix Fig. A3 for diagrammatic illustration):\n1. Pairwise Feature Initialization. The initial step is to concatenate the node features from the previous node feature encoder module for every pair of nodes. Subsequently, we employ a three-layer MLP to convert each concatenated vector P\u1d62\u2c7c\u2097 \u2208 \u211d^{2h} to an h-dimensional raw pairwise feature, i.e., P^\u2081_{ijl} = MLP([F_{il}; F_{jl}]). It is designed to capture the intricate relations that exist inside the pairs of nodes.\n2. Unidirectional Multi-Head Attention. In order to model the external information, we employ an attention mechanism where the query is composed of the aforementioned d\u00b2 h-dimensional raw pairwise features, while the keys and values consist of h-dimensional features of d individual nodes, i.e.,"}, {"title": "4.3 Learning Targets", "content": "As mentioned above, our learning target is a combination of the skeleton and the set of v-structures, which together represent an MEC. Two separate neural (sub-)networks are trained for these two targets.\nSkeleton Prediction. As the persistent dependency between pairs of nodes determines the existence of edges in the skeleton, the pairwise features correspond to edges in the skeleton naturally. Therefore, for the skeleton learning task, we initially employ a max-pooling layer over the observation dimension to obtain a single vector S\u1d62\u2c7c \u2208 \u211d\u02b0 for each pair of nodes, i.e., S_{ij} = max_l P_{ijk}. Then, a linear layer and a sigmoid function are applied to map the pairwise features to the final prediction of edges, i.e., S_{ij} = Sigmoid(Linear(S_{ij})). Our learning label, the undirected graph representing the skeleton, can be easily calculated by summing the adjacency of the DAG G and its transpose G\u1d40. Therefore, our learning target for the skeleton prediction task can be formulated as min \u2112(S, G + G\u1d40), where \u2112 is the popularly used binary cross-entropy loss function.\nV-structure Prediction. A UT (X\u1d62, X\u2096, X\u2c7c) is a v-structure when S, such that X\u2096 \u2209 S and X\u1d62 | X\u2c7c|S. Motivated by this, we concatenate the corresponding pairwise features of the pair (X\u1d62, X\u2c7c) with the node features of X\u2096 as the feature for each UT (X\u1d62, X\u2096, X\u2c7c) after a max-pooling along the observation dimension, i.e., U_{kij} = [max_l P_{iji}; max_l F_{kl}]. After that, we use a three-layer MLP with a sigmoid function to predict the existence of v-structures among all UTs, i.e., U_{kij} = Sigmoid(MLP(U_{kij})). Given a data sample of d nodes, it outputs a third-order tensor of shape \u211d^{d\u00d7d\u00d7d} namely v-tensor, corresponding to the predictions of the existence of v-structures. The v-tensor label can be obtained by V_{kij} = G_{ik}G_{jk}(1-G_{ij})(1-G_{ji}), where V_{kij} indicates the existence of v-structure X\u1d62 \u2192 X\u2096 \u2190 X\u2c7c. Therefore, the learning target for the v-structure prediction task can be formulated as min \u2112_{UT}(U, V), where \u2112_{UT} is the binary cross-entropy loss masked by UTs, i.e., we only calculate such loss on the valid UTs. In our current implementation, the parameters of the"}, {"title": "4.4 Post-Processing", "content": "Although our method theoretically guarantees asymptotic correctness, conflicts in predicted v-structures might occasionally occur in practice. Therefore, in the post-processing stage, we apply a straightforward heuristic to resolve the potential conflicts and cycles among predicted v-structures following previous work (Dai et al., 2023). After that, we use an improved version of Meek rules (Meek, 1995a; Tsagris, 2019) to obtain other MEC-identifiable edges without introducing extra cycles. Combining the skeleton from the skeleton predictor model with all MEC-identifiable edge directions, we get the CPDAG predictions.\nWe provide a more detailed description of the post-processing process in Appendix Sec. A3. It is worth noting that our current design of post-processing is a very conservative one, and this module is also non-essential in our whole framework; see Appendix Sec. A3 for more discussions and evidences."}, {"title": "5 Experiments", "content": "In this section, we report the performance of SiCL on both synthetic and real-world benchmarks, followed by an ablation study. More results and discussions about time cost, generality, and acyclicity are deferred to Appendix Sec. A8, due to page limit."}, {"title": "5.1 Experiment Design", "content": "Metrics. We profile a causal discovery method's performance using the following two tasks:\nSkeleton Prediction: Given a data sample D of d variables, for each variable pair, we want to infer if there exists direct causation between them. The standard metric s-F1 (short for skeleton-F1) is used, which considers skeleton prediction as a binary classification task over the d(d - 1)/2 variable pairs. For completion, we also report classification accuracy results. For methods with probabilistic outputs, AUC and AUPRC scores are also measured.\nCPDAG Prediction: Given a data sample D of d variables, for each pair of variables, we aim to determine if there is direct causality between them. If so, we then assess whether the causal direction is MEC-identifiable, and if it is, we attempt to infer the specific causal direction. As this task involves both directed and undirected edge prediction, we use SHD (Structural Hamming Distance) to measure the difference between the true CPDAG and the inferred CPDAG. Besides that, we also measure the o-F1 (short for orientation-F1) of the directed sub-graph of the inferred CPDAG (compared against the directed sub-graph of the true CPDAG), which focuses on capturing the inference method's orientation capability in identifying MEC-identifiable causal edges. Finally, we calculate the v-F1 score, where the F1 score is based on the set of v-structures in the true CPDAG as the positive instances (from all ordered triples), and the v-structures in the inferred CPDAG as the positive predictions.\nTesting Data. A testing instance consists of a groundtruth causal graph G, the structural equations f\u1d62 and noise variables e\u1d62 for each variable X\u1d62, and an i.i.d. sample D. We use two categories of testing instances in our experiments:\nAnalytical Instances: where G is sampled from a DAG distribution G, and {f\u1d62, e\u1d62} sampled from a structural-equation distribution F and a noise meta-distribution N. We consider three random graph distributions for G: Watts-Strogatz (WS), Stochastic Block Model (SBM), Erdos-R\u00e9nyi (ER); and three F's: random linear (L), Random Fourier Features (RFF), and conditional probability table (CPT). N is a uniform distribution over Gaussian's for continuous data, and a Dirichlet distribution over Multinomial Categorical distributions for discrete data. We examine five combinations of testing instances: WS-L-G, SBM-L-G, WS-RFF-G, SBM-RFF-G, and ER-CPT-MC.\nReal-world Instance: The classic dataset Sachs is used to evaluate performance in real-world scenarios. It consists of a data sample recording the concentration levels of 11 phosphorylated proteins in 853 human immune system cells, and of a causal graph over these 11 variables identified by Sachs et al. (2005) based on expert consensus and biology literature.\nAlgorithms. As baselines, we compare with a series of representative unsupervised methods, including PC (using the recent Parallel-PC variation by Le et al. (2016)), GES (Chickering, 2002), NOTEARS (Zheng et al., 2018), GOLEM (Ng et al., 2020), DAG-GNN (Yu et al., 2019), GRANDAG (Lachapelle et al., 2020), SLdisco (Petersen et al., 2023) as well as AVICI (Lorch et al., 2022), a DNN-based SCL method regarded as current state-of-the-art method.\nFor our method, besides the full SiCL implementa-"}, {"title": "5.4 Ablation Study", "content": "Effectiveness of Learning Identifiable Structures. As discussed in Section 4.3, SiCL focuses on learning MEC-identifiable causal structures rather than directly learning the adjacency matrix. To verify the effectiveness of this idea, we compare SiCL-Node-Edge with SiCL-no-PF. These two models share a similar node feature encoder architecture but have different learning targets: SiCL-Node-Edge predicts the adjacency matrix, while the SiCL-no-PF predicts the skeleton and v-tensor. The results are shown in Table 3. Consistently, SiCL-no-PF demonstrates higher performance on both skeleton and CPDAG prediction tasks. This observation echoes our theoretical conclusion regarding the necessity and benefits of learning identifiable causal structures to improve overall performance.\nTo further underscore the significance of learning identifiable causal structures (especially in the asymptomatic sense), we conduct a comparative analysis using a specially constructed dataset, which contains six nodes forming an independent v-structure and a UT. Figure 2 illustrates that the orientation F1 scores of CPDAG predictions from SiCL-Node-Edge suffer from an unavoidable error and do not improve with the addition of more observational samples. In contrast, predictions from SiCL-no-PF reach perfect accuracy, confirming the value of learning identifiable causal structures.\nEffectiveness of Pairwise Representation. To assess the effectiveness of pairwise representation, we compare the full version of SiCL with a variant lacking pairwise features (SiCL-no-PF). As shown in Table 3, the full-version SiCL consistently outperforms SiCL-no-PF in both skeleton prediction and CPDAG prediction tasks. Notably, we have intentionally set the model size of the full-version SiCL (2.8M parameters) to be smaller than that of SiCL-no-PF (3.2M parameters) so as to avoid any potential advantage from increased model complexity brought by the pairwise feature encoder module. The observed performance gains in this case underscore the critical role of pairwise features in identifying causal structures. Additionally, we conduct further comparisons across more diverse settings, with results detailed in Appendix Sec. A7. These results demonstrate even more pronounced improvements in favor of SiCL, reinforcing the importance of pairwise representations in causal discovery."}, {"title": "6 Conclusion", "content": "We proposed SiCL, a novel DNN-based SCL approach designed to predict the corresponding skeleton and a set of v-structures. We showed that such design do not suffer from the (non-) identifiability limit that exists in current architectures. Moreover, SiCL is equipped with a pairwise encoder module to explicitly model relationships between node-pairs. Experimental results validated the effectiveness of these ideas.\nThis paper also introduces a few interesting open problems. The proposed DNN model works in the canonical setting under the classic MEC theory, in which the skeleton and v-structures are the identifiable structure. It can be an interesting future-work direction to explore how to learn other identifiable causal structure in other assumption settings following the same principle. Due to the inherent complexity of DNNs, the explanation of the decision mechanism of our model remains an open question. Therefore, future work could consider to explore how decisions are made within the networks and provide some insights for traditional methods. Moreover, the proposed pairwise encoder modules needs O(d\u00b3) computational complexity, which may restrict its current application to scenarios with huge number of nodes. Future work could focus on simplifying these operations or exploring features with less complexity (e,g., low rank features) to reduce the overall computational cost."}, {"title": "A1 Theoretical Guarantee", "content": "In this section, we delve into the theoretical analysis concerning the asymptotic correctness of our proposed model with respect to the sample size. Sec. A1.1 lays out the essential definitions and assumptions pertinent to the problem under study. Following this, from Sec. A1.2 to A1.3, we rigorously demonstrate the asymptotic correctness of the neural network model. Finally, in Sec. A1.4, we engage in a detailed discussion about the practical advantages and superiority of neural network models."}, {"title": "A1.1 Definitions and Assumptions", "content": "As outlined in Sec. 2, a Causal Graphical Model is defined by a joint probability distribution Pover d random variables X\u2081, X\u2082, \u2026, X\u2090, and a DAG G with d vertices representing the d variables. An observational dataset D consists of n records and d columns, which represents n instances drawn i.i.d. from P. In this work, we assume causal sufficiency:\nAssumption A1.1 (Causal Sufficiency). There are no latent common causes of any of the variables in the graph.\nMoreover, we assume the data distribution P is Markovian to the DAG G:\nAssumption A1.2 (Markov Factorization Property). Given a joint probability distribution P and a DAG G, P is said to satisfy Markov factorization property w.r.t. G if P := P (X\u2081, X\u2082, \u2026, X\u2090) = \u220f\u1d62=\u2081P (X\u1d62 | pa_{G}), where pa_{G} is the parent set of X\u1d62 in G.\nIt is noteworthy that the Markov factorization property is equivalent to the Global Markov Property (GMP) (Lauritzen, 1996), which is\nDefinition A1.1 (Global Markov Property (GMP)). P is said to satisfy GMP (or Markovian) w.r.t. a DAG G if X \\indep_{I_{G}} Y|Z \u21d2 X \\indep Y|Z. Here I_{G} denotes d-separation, and \\indep denotes statistical independence.\nGMP indicates that any d-separation in graph G implies conditional independence in distribution P. We further assume that P is faithful to G by:"}, {"title": "A1.2 Skeleton Learning", "content": "In this section, we prove the asymptotic correctness of neural networks on the skeleton prediction task by constructing a perfect model and then approximating it with neural networks. For the sake of convenience and brevity in description, we define the skeleton predictor as follows.\nDefinition A1.6 (Skeleton Predictor). Given observational data D, a skeleton predictor is a predicate function with domain as observational data D and predicts the adjacency between each pair of the vertices.\nNow we restate the Remark from Ma et al. (2022) as the following proposition. It proves the existence of a perfect skeleton predictor by viewing the skeleton prediction step of PC (Spirtes et al., 2000) as a skeleton predictor, which is proved to be sound and complete."}, {"title": "A1.3 Orientation Learning", "content": "Similarly to the overall thought process in Sec. A1.2, in this section we prove the asymptotic correctness of neural networks on the v-structure prediction task by constructing a perfect model and then approximating it with neural networks.\nDefinition A1.7 (V-structure Predictor). Given observational data D with sufficient samples from a BN with vertices V = {X\u2081,..., X\u209a}, a v-structure predictor is a predicate function with domain as observational data D and predicts existence of the v-structure for each unshielded triple.\nThe following proposition proves the existence of a perfect v-structure predictor by viewing the orientation step of PC (Spirtes et al., 2000) as a v-structure predictor."}, {"title": "A1.4 Discussion", "content": "In the sections above, we prove the asymptotic correctness of neural network models by constructing theoretically perfect predictors. These predictors both consist of two parts: feature extractors providing features x\u1d62\u2c7c and z\u1d62\u2c7c\u2096, and final predictors of adjacency and v-structures. Even though they have a theoretical guarantee of the correctness with sufficient samples, it is noteworthy that they are hard to be applied practically. For example, to obtain x\u1d62\u2c7c in Eq. (2), we need to calculate the conditional dependency between X\u1d62 and X\u2c7c given every node subset Z \u2286 V\\{X\u1d62, X\u2c7c}. Leaving aside the fact that the number of Zs itself presents factorial complexity, the main issue is that when Z is relatively large, due to the curse of dimensionality, it becomes challenging to find sufficient samples to calculate the conditional dependency. This difficulty significantly hampers the ability to apply the constructed prefect predictors in practical scenarios.\nSome existing methods can be interpreted as constructing more practical predictors. Majority-PC (MPC) (Colombo et al., 2014) achieves better performance on finite samples by modifying Eq. (4) - (5) as:"}, {"title": "A2 More Discussions on Identifiability and Causal Assumptions", "content": "Advocation of Learning Identifiable Structures under All Settings. In this paper, we have to work on a concrete setting for demonstration purpose with concrete identifiable causal structures in this paper. Nonetheless, we want to emphasize that the very concept of identifiability, as well as its ramifications in SCL, is indeed a general issue that is less bound to the issue of \"which causal structures are identifiable under which assumptions\". The simple fact that in some situations the causal edge cannot be identified no matter what feature can be identified in that case this identifiability limit has a general effect on SCL. Unless the causal graph/edge itself becomes fully invariant/identifiable (a special case that is important but certainly not universally true), the presence of the identifiability limit entails a fundamental bias for a popular SCL model architecture (i.e., Node-Edge) that cannot be mitigated by larger model or bigger data at all. This \u201cidentifiability-limit-causes-learning-error\u201d effect is the main thesis of this paper, and we advocate to design neural networks that focus on learning the identifiable features (no matter what those features are). In other words, there is nothing stopping one from studying another setting where another feature is identifiable though, and in that case we would also advocate to learn that feature instead of v-structures. For example, if we assume canonical MEC assumptions and non-existence of causal-fork and v-structures, the identifiable causal structure becomes a kind of chains. In that case, one may want to design neural networks that predict about causal chains."}, {"title": "A3 Details and Discussion about Post-processing", "content": "For comprehensive clarity, we provide a clear process about the post-processing algorithm in Alg. A2.\nDiscussion. It is worth noting that our design in post-processing is as conservative as possible. In fact, we simply adhere to the conventions in deep learning (i.e., thresholding) to obtain the skeleton and the initial v-structure set. Subsequently, we follow the conventions in constraint-based causal discovery methods to derive the final oriented edges. Therefore, we have not dedicated extensive efforts towards the meticulous design, nor do we intend to emphasize this aspect of our workflow.\nThe conflicts and cycles are not unique to SiCL; they are, in fact, common issues encountered by all constraint-based algorithms like PC. Moreover, it's worth noting that they never appear if the networks perform perfect. Therefore, the conflict resolving of v-structures and the removal of cycles are designed as fallback mechanisms to ensure the soundness of our workflow, rather than being central elements of our approach. To illustrate it, we experimented with an opposite variant (Intuitively, this is a bad choice) that prioritizes discarding the v-structure with the higher predicted probability. The minimal differences in outcomes between this variant and its counterpart, as detailed in Tab. A4, support our viewpoint that the conflict resolution process is of limited significance within our workflow. On the other hand, experimental results presented in Tab. All underscore the infrequency of cycles in the predictions, reinforcing the non-essential nature of the cycle removal component."}, {"title": "A4 Details about Node Feature Encoder"}]}