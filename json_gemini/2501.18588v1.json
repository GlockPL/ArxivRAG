{"title": "Inkspire: Supporting Design Exploration with Generative AI through Analogical Sketching", "authors": ["David Chuan-En Lin", "Hyeonsu B. Kang", "Nikolas Martelaro", "Aniket Kittur", "Yan-Ying Chen", "Matthew K. Hong"], "abstract": "With recent advancements in the capabilities of Text-to-Image (T2I) Al models, product designers have begun experimenting with them in their work. However, T2I models struggle to interpret abstract language and the current user experience of T2I tools can induce design fixation rather than a more iterative, exploratory process. To address these challenges, we developed Inkspire, a sketch-driven tool that supports designers in prototyping product design concepts with analogical inspirations and a complete sketch-to-design-to-sketch feedback loop. To inform the design of Inkspire, we conducted an exchange session with designers and distilled design goals for improving T2I interactions. In a within-subjects study comparing Inkspire to ControlNet, we found that Inkspire supported designers with more inspiration and exploration of design ideas, and improved aspects of the co-creative process by allowing designers to effectively grasp the current state of the AI to guide it towards novel design intentions.", "sections": [{"title": "1 INTRODUCTION", "content": "We have seen significant progress in the capabilities of text-to-image (T2I) models, many of which are now able to generate realistic images using text [9]. These models not only accelerate the process of converting thoughts into visuals but can also potentially create serendipitous inspirations for users [25]. Recent research has also opened up new possibilities for translating one image representation into another, such as transforming a sketched drawing into detailed designs [72]. Consequently, many designers have begun embracing the use of T2I models to enhance their creative work.\nHowever, despite the proposed benefits T2I models, integrating them into the designer's creative workflow can be challenging. In particular, recent works have observed that designers experience a high level of fixation [35] when using generative AI [63] leading them to explore fewer novel ideas than may otherwise be beneficial for innovation. For instance, a designer might write a text prompt and hit generate. Upon viewing the result, they might adjust a few words in their prompt and hit generate again [20]-repeating this process akin to using a slot machine [2]. Their final prompt tends to be conceptually similar to their original one, often with small, incremental modifications aimed at getting the AI to create their initial design intention, instead of exploring new diverse design spaces [63]. As found in prior work, unsupported text-only prompting can be a limiting interface for generating good outputs from GenAI [70].\nTo understand how professional designers might experience these challenges with GenAI in their own processes, we conducted a comprehensive day-long exchange session with a team of professional designers from a large automotive company. The team outlined their design process, from conceptualizing a product design sketch based on a specific guiding theme to presenting final designs to stakeholders. From our discussions with the designers, we identified three key challenges in their use of GenAI:\nC1 Designing through text prompts feels unnatural as compared to their traditional sketching and ideation process.\nC2 T2I models struggle with generating inspiring designs from abstract concepts (e.g., \u201cprotective\" car), a technique the team uses to create more novel designs.\nC3 It is difficult to directly build on generated designs-they appear \"too complete\", potentially leading to fixation.\nTo help designers working with GenAI avoid design fixation, we might take inspiration from design research working to break designer fixation. Two common strategies in line with what we learned from the professional design team include analogy-driven design [30, 36, 53], where concepts from outside the domain area are used to foster inspiration in the domain area (addressing C2) and providing multimodal interfaces that work visually and with lower-fidelity assets (addressing C1 & C3).\nSeveral recent works in HCI have focused on developing new interfaces for GenAI to help designers explore a larger design space, including interactive prompting which aims help people who often have limited prompting ability [11], multimodal search [61], and visually navigating a 2D latent space interface [24]. Nonetheless, being able to help designers move beyond one thread of thinking and explore a wider design space while still taking inspiration and building on generated designs remains a significant limitation preventing GenAI to be fully integrated into designers' work. While the ML community offers methods to increase editing freedom of generated images, including local inpainting [57] and instruction-based image editing [12], these methods remove designers away from their natural rhythm of interaction with ideas.\nIn this paper, we introduce a workflow of continuous exploration with T2I models, encouraging designers to adopt a mindset that facilitates more iterative and exploratory design generation. Specifically, we built Inkspire, a proof-of-concept tool that provides a more familiar interaction built around iterative sketching and by leveraging the concept of analogical design to facilitate inspiration around design ideas. Our tool integrates analogical inspiration to promote concept-level ideation from abstract themes, allowing designers to recognize creative possibilities without needing to come up with them and write prompts manually. This reduces cognitive friction, enabling fluid exploration of new ideas [14]. Additionally, inspired by prior works on drawing assistance systems that convert photographs into sketches to teach people how to draw [46], we introduce a new mechanism that converts high-fidelity AI designs into high-quality but low-resolution sketch scaffolds, directly underlaid on the designer's canvas. The scaffolding provides a transparent view into the current state of the AI and helps designers build on Al designs without being overly fixated on photorealistic renders. Finally, we enable a new design generation every time a new pen stroke is drawn (with near real-time performance), encouraging designers to consider small but meaningful changes in form and refinement with each pen stroke, thereby creating many more opportunities for designers to explore new directions. All of these components are designed to be seamlessly integrated into the sketch process familiar to designers.\nTo understand how designers use Inkspire, we invited both professional designers and novice users to design everyday products. We also asked them to use a baseline condition of a state-of-the-art ControlNet [72], which allows designers to use sketches and user-written prompts to guide generative design, but does not provide our proposed analogy inspirations or convert designs into lower resolution sketches, and requires the user to choose when they generate explicitly. The results show that users rated Inkspire as"}, {"title": "2 RELATED WORK", "content": "This work builds on prior research in human-AI co-creativity for helping people generate ideas [52]. Specifically, we build upon prior works aiming to help designers create new visual concepts through text to image models and visual inputs [32, 51, 54]. We review how generative Al systems have been shown to increase idea generation, but counterintuitively can lead to more design fixation. We then review possible solutions for overcoming design fixation including analogy-driven design, computational sketching tools, and reducing the fidelity of generated images."}, {"title": "2.1 Generative AI and Design Fixation", "content": "One of the proposed benefits of generative Al is to help designers avoid design fixation, where designers remain stuck in a single line of thinking and a limited set of ideas, thus limiting the conceptual novelty of ideas and potential for innovation [35]. While many of the works suggest that generative AI can help people generate more ideas, potentially helping them to move into new conceptual spaces, recent experimental work has found contradictory results on generative Al's impact on design fixation. DesignAID [16] leverages large language models and image generators to help people explore a large, diverse space of ideas and was found to provide more inspiration than search-based tools. However, people rated the generative AI ideas as less valuable, with the paper authors suggesting that the ideas may not have been diverse enough, not well matched to the problem, or just not enough to break people design fixation. Bordas et al. [10] find that people using ChatGPT 3.5 to help generate ideas to protect and egg falling from 10 meters increased their idea generation but remained overly fixated on specific solutions. Wadinambiarachchi et al. [63] find that participants using a text-to-image generator to create a new chatbot avatar concept had significantly higher design fixation with fewer ideas, less variety, and less originality when using a text-to-image generator as compared to using an image search engine or coming up with ideas unassisted. The authors suggest that people's limited prompting ability, a known issue when lay people try to use LLMs [70], led them to simply copy keywords from the design brief, limiting the language used to generate the images. Davis et al. [24] and Zhang et al. [71] report similar findings, where designers working with text-only image generators showed limited creative exploration, again due to people's limited abilities with text prompting.\nTo overcome the fixating issues of text-based generative systems, Davis et al. [24] developed a generative system taking example images of dresses and provided graphical buttons and sliders to generate more realistic vs. creative ideas and alter shape, color and texture. This visual generative system was preferred by designers and lead to more creative idea explorations. However, the high-fidelity images presented by many image generation tools, which Wadinambiarachchi et al. [63] suggest could lead to more fixation based on prior design research showing that high-fidelity images lead to more fixation over rougher sketches [18, 21].\nIn our work, we explore how to overcome the fixating issues that many generative Al systems have today. First, we look to help designers explore more novel ideas by providing interfaces that overcome their limited prompting abilities and help them generate more novel ideas. For this, we look to analogy-driven design as a potential solution. Second, we move away from text-only generation and provide more visual interfaces for design exploration (C1), helping better match how designers come up with ideas. We explore how iterative, computational sketching can be used as an visual input to image generation. Third, we break from showing only high-fidelity images which could lead to design fixation and explore how AI generated images can be altered and presented to designers in lower resolution forms to see if such representations may scaffold new ideas."}, {"title": "2.2 Analogy-Driven Design", "content": "Analogy-driven design, or design-by-analogy, is an approach to drawing inspiration from a known domain, including concepts and products, to find novel solutions to a target domain. An abundance of text-based research systems and prototypes such as DANE [31], Idea-Inspire 4.0 [60], and BioTRIZ [62] have been developed to support design ideation by offering different design-by-analogy capabilities that include retrieval and mapping of analogies to a target problem based on inferred similarities.\nRecent proliferation of text and image data repositories combined with advances in vision and language models gave rise to new multimodal approaches that expand analogy-driven design to the visual domain [36]. For instance, Kwon et al. developed an approach that leverages visual similarity to discover visual analogies for generating new ideas [43]. In addition, Zhang and Jin proposed an unsupervised deep learning model, Sketch-pix2seq, to extract shape features from Quickdraw sketches, creating a latent space that enables defining visual similarities and searching for analogical sketches [74]. Jiang et al. developed a CNN-based model to create feature vectors representing patent images, which combine visual and technological information to enhance visual stimuli retrieval [37]. While these models present promising methods to support image-based analogy-driven design, the specialized nature of these models reduces their practical appeal to designers seeking exposure to out-of-distribution inspirations."}, {"title": "2.3 Guided Sketching", "content": "Research has explored various computational techniques that provide sketch guidance to aid in several use cases including skill building, serving as reference points, and encouraging creative exploration.\nMany existing systems have explored guided sketching tools that help novices learn how to sketch. ShadowDraw [46] offers real-time shadow-based feedback, while systems such as The Drawing Assistant [34] and Painting with Bob [8] focus on translating photographs to sketches. Several works have explored crowdsourcing-based approaches. Systems like Limpaecher et al. [50] and Sketchy [58] leverage collective human knowledge to provide guidance. These systems excel at teaching specific drawing techniques but are less suited for open-ended creative exploration. Similarly, portrait-specific systems such as DualFace [33], which employs a two-stage drawing guidance for freehand portraits, and PortraitSketch [67], which provides face sketching assistance, demonstrate the value of domain-specific guidance but are constrained to a narrow use case.\nMost relevant to Inkspire is Creative Sketching Partner [23], which retrieves sketches that are visually and conceptually similar to the user's sketches as a means to stimulate exploration and inspire new designs. This system demonstrates the potential of computational guided sketching systems to inspire new designs. However, it has the limitation of relying on existing sketch databases, whereas in this work, we leverage the generative capabilities of AI.\nIn our work, we draw inspiration from empirical research by Williford et al. [65], whose analysis of 240 concept sketches revealed that ambiguous sketch underlays could reduce fixation on conventional forms and promote divergent thinking, guided sketching techniques such as ShadowDraw [46], and existing design practices of sketch scaffolding [19]. With Inkspire, we propose a novel computational pipeline for converting GenAI designs into abstracted, yet high-quality sketching scaffolds that underlay the user's canvas. To the best of our knowledge, this work is the first to support the full closed-loop-cycle of sketching GenAI designs and abstracting GenAI designs into sketches. Through this technique, we address the challenge of building on GenAI images that are \"too complete\" (C3) by drawing designers' attention away from the high-fidelity generated image and inspire designers to iterate on top of the silhouette of GenAI designs with low friction."}, {"title": "3 FORMATIVE SESSION WITH DESIGN PROFESSIONALS", "content": "To understand how professional designers use Text-to-Image (T2I) models in their work, we conducted a day-long exchange session with a team of seven professional product designers from a large automotive company. The designers in our exchange session work at a top 5 automotive manufacturing company and cover multiple disciplines of training spanning a wide range of roles in the company, including creative director, modeling lead, interior designer, exterior designer, artistic creator, conceptual lead creator, UX/UI and strategy. There is significant cross-collaboration across departments globally and with other companies in the industry. The designers also have experience in using T2I tools such as Midjourney [4] and Vizcom [5]. The designers showed our team their design processes in presentations with specific examples from their past work. This included sketches, concept boards, and various documentations of collaborative meetings for a wide range of mobility concepts. The designers also showed their process of using current T2I tools, and we discussed new ideas on how to design tools to support them. From our interaction with the designers, we identified key challenges they face when using T2I models and summarized them into three design goals to inform the development of Inkspire."}, {"title": "3.1 Design Goals", "content": ""}, {"title": "3.1.1 Design Goal 1. Sketching as a Natural Method of Interaction.", "content": "Designers emphasized that prompting is an unnatural approach to designing. They expressed difficulty in effectively conveying design ideas through language [70]. They often felt constrained by the need to craft comprehensive prompts, which limited their ability to explore a wider range of ideas. In contrast, designers expressed a preference for approaching design tasks through sketching [15], often beginning with just a simple line or silhouette. Therefore, our first design goal is to allow designers to interact with Al via sketching as a natural method of interaction. We aim to support designers in starting with simple abstract lines and assist them in progressing towards complete sketches."}, {"title": "3.1.2 Design Goal 2. Visually-Concrete Inspirations.", "content": "Designers mentioned that design briefs are typically inherently abstract, for example, \"design a vehicle that conveys a sense of protectiveness\u201d. However, they found T2I models to generally produce poor and generic results when prompted with such abstract terms [69]. Even when resorting to prompt engineering tricks, they find it challenging to visualize abstract concepts in concrete forms. Therefore, our second design goal is to assist designers in visualizing abstract design themes through visually-concrete inspirations. Inspired by the way designers may draw inspiration from nature [26], we aim to recommend analogical inspirations [39] to designers and make it easy for them to quickly visualize a variety of diverse inspirations."}, {"title": "3.1.3 Design Goal 3. Complete the Feedback Loop.", "content": "Designers expressed difficulty in iterating on Al-generated designs and they described the process of using T2I models as a one-way process. They feel that the generated designs look \"too complete\", making it difficult for them to envision new ways to build on them. Often, designers find themselves in a position where they can either choose to use a design or discard it entirely. Therefore, we aim to bridge the gap in the feedback loop \u2013 while T2I models transform ideas into images, our goal is to transform images back into abstractions (i.e., sketches), to allow designers to continue the iteration process."}, {"title": "4 INKSPIRE", "content": "We first illustrate how a user would use Inkspire through a concept car design example. We then describe the technical implementation of Inkspire, which consists of two primary components: Sketch2Design (generating AI designs from sketches and analogies) and Design2Sketch (converting AI designs into lower fidelity sketch scaffolding)."}, {"title": "4.1 System Walkthrough", "content": "DeLorean is an automotive designer tasked with creating a concept car design that embodies a sense of \u201cprotectiveness\" (Figure 1)."}, {"title": "4.1.1 Ideating a Design Concept.", "content": "To begin, DeLorean uses the Analogy Panel (Figure 1a) to generate visually-concrete inspirations for the abstract concept (Design Goal 2). He uses \"car\" as the subject and \"protective\u201d as the abstract concept, then clicks on the inspiration button. He is presented with a selection of inspirations, color-coded by categories: nature, fashion, and architecture DeLorean experiments with several inspirations (such as bunker, armadillo, tortoise) by clicking on them. For each inspiration he selects, the AI generates a design in the Evolution Panel (Figure 1c). He can manually edit inspirations in the concept box and manually click on the generate button. DeLorean decides to use tortoise as his inspiration."}, {"title": "4.1.2 Iterating on Designs through Sketching.", "content": "DeLorean now iteratively guides the AI through sketching in the Sketching Panel (Figure 1b) (Design Goal 1). He starts off with a simple silhouette line, and the AI generates an initial design in the Evolution Panel. In the Sketching Panel, DeLorean sees a scaffolding abstracted from the initial AI design (Design Goal 3). This allows him to take inspiration from the AI designs without being overly fixated on photorealistic renders. He is drawn to the bold curve of the windshield area shown in the scaffolding and loosely traces this part to add to his sketch. The AI then generates a new design and scaffolding.\nDeLorean repeats this back-and-forth process with the AI (Design Goal 1), continuing to iterate on his designs through sketching until he achieves a design that satisfies him. To navigate between iterations, restart a sketch, or refine parts of the sketch, he uses the Undo, Clear, and Eraser tools. To explore design variations using the same input (the same inspiration and sketches), he uses the Remix tool."}, {"title": "4.2 Sketch2Design", "content": "The Sketch2Design component helps users brainstorm design concepts and generate product designs through sketching (Figure 2). First, the user specifies the subject that they are designing for (e.g., car) and an initial abstract concept (e.g., protective). To brainstorm more concrete design ideas for the abstract concept (Design Goal 2), we leverage Large Language Models (LLMs) (GPT-4) [13] to generate analogical inspirations (Figure 2a). We leverage prior techniques in chain-of-thought reasoning [64] to break down the problem of creating analogies based on the given concept word. We take a two-step prompting approach shown in the listings below. We first prompt the LLM to detail the design principles for the given subject (e.g., car design).\nDescribe the key design principles in <subject> design in one short paragraph.\nSuch design principles can be useful as context [47] to ground the LLM to ideate inspirations that are more suitable for the specific product domain. An example intermediary result from this step for the domain of car design could be\nKey design principles for car design include aerodynamics exteriors for fuel efficiency and performance...\nGiven the design principles, we then prompt the LLM to generate analogical inspirations. Our definition of analogy draws on the work of Gentner [29]. This definition involves identifying parallel relations from a source domain to apply to a target domain even when their surface features differ. We have structured our prompt using this definition of analogies by finding visually concrete objects from three source domains (nature, architecture, fashion) that convey concepts of the target domain (abstract concept).\nYou are a <subject> designer. The design principles in < subject> design are as follows: <design principles from Step 1>. Brainstorm analogical inspirations for <subject> design to convey a sense of <concept> from one of the following domains: nature, architecture, or fashion. Answer in a bullet-point list of 10 items (item1\\nitem2... \\nitem3) using visually-concrete objects not adjectives and don't repeat.\nWe empirically found that prompting specifically for the domains of nature, architecture, and fashion leads to particularly interesting inspirations. Furthermore, these domains are outside of the primary product design domain and are common sources of inspiration for designers. The LLM then provides single noun-phrases as results (e.g., protectiveness & \u2192 tortoise, armadillo, armor).\nThe user may select a recommended inspiration and continue branching out to explore further inspirations. For example, selecting tortoise and rerunning the analogy inspiration chain could result in new analogies such as tortoise \u2192 tank, backpack, treasure chest). The user may also freely change their concept (e.g., changing protectiveness to freedom, resulting in a new set of analogical"}, {"title": "4.3 Design2Sketch", "content": "The Design2Sketch component helps users build on top of previously generated designs by converting them into scaffoldings by abstracting a design into a reduced-fidelity sketch-style with the aim of reducing design fixation on the high-fidelity image (Figure 4). The scaffolding appears as a transparent underlay beneath the user's canvas, functioning like tracing paper [3] that updates in real-time as they sketch. This enables the user to draw inspiration from aspects of the previously generated designs and also helps them overcome the challenge of starting with a blank canvas [38], especially during the early stages of sketching. The user can continue iterating through sketching, completing the sketch-to-design-to-sketch feedback loop (Design Goal 3).\nWhile there are many methods for reducing high fidelity images into lower-resolution, we introduce a novel approach for converting designs to sketch scaffolds. We initially tested existing methods including Canny edge detection [17], HED soft edge extraction [68], a state-of-the-art method for extracting main lines from manga illustrations [48], and a neural network method explicitly trained on pairs of sketches and images. (see Figure 3). We observed that edge extraction methods including Canny edge detection and HED soft edge extraction frequently produce unwanted lines caused by the texture of designs. Furthermore, we observed that manga line extraction methods, trained primarily on cartoon illustrations, can lead to a loss of key lines or produce broken lines. Finally, neural networks explicitly trained on pairs of sketches and images on the task of translating images to sketches can create artifacts of excessive sketch stylization, such as shading effects.\nIn our approach, we combine semantic segmentation and edge extraction. First, we perform semantic segmentation on the design [41] to create a segmentation map that color-codes a design into distinct semantic regions (Figure 4a). Given the segmentation map, we draw the boundaries between the different regions to create an image of semantic boundary lines. Second, we extract soft edges from the design [68] (Figure 4b). These soft edges include varying thickness and line weight, simulating a sketch-like look, though often with many redundant lines caused by texture. Finally, we take a pixel-wise intersection between the segmentation map boundary lines from the first step and the extracted soft edges from the second step as the final scaffolding (Figure 4c):\nScaffolding = Boundary(Seg(D)) \u2229 SoftEdge(D) \nwhere D is the generated design. Through this approach, we are able to acheve the best of both worlds - creating a sketch scaffolding that achieves a natural sketch look while focusing only on the design's key structural lines, filtered through the boundary lines from semantic segmentation step."}, {"title": "5 USER STUDY", "content": "We conducted a within-subjects study to understand how Inkspire could address designers' pain points in working with T2I models, its potential to be integrated into design workflows, and identify areas for improvement. We compared Inkspire against a baseline condition using a typical ControlNet [72] setup consisting of a prompt box and a sketching canvas, adopting a similar interface layout as Inkspire (see Figure 14)."}, {"title": "5.1 Participants", "content": "We invited twelve participants (P1-P12, 10 male and 2 female) to participate in a one-hour user study. Among the participants, six are professional designers who perform product design activities daily or weekly (self-rated confidence in product design \u03bc=6.17, \u03c3=0.75; self-rated confidence in drawing \u03bc=6.00, \u03c3=1.27; 7-point Likert scale) and six are novices who have moderate drawing experience (self-rated confidence in drawing \u03bc=4.17, \u03c3=1.33; 7-point Likert scale) but do not actively engage in product design. The participants were recruited through known contacts and Upwork [1], a platform for hiring freelancers. They were not exposed to the Inkspire system or concept prior to the user study. Participants accessed Inkspire and the baseline tool through a web browser."}, {"title": "5.2 Measures", "content": "For both conditions, we asked participants to complete questionnaires to capture their perspectives on using both Inkspire and ControlNet. We assess creativity using the Creativity Support Index [22], measuring exploration, inspiration, engagement, expressiveness, tool transparency, and effort/reward tradeoff. We assess designers sense of the human-AI collaboration using questions from [44] measuring controllability, communication, harmony, partnership, attribution, and ownership. We also asked participants to rate their experiences of sketching by referencing the sketching principles from Bill Buxton's Sketching User Experiences [15], measuring how quick and timely, inexpensive and disposable, and loose and abstract sketching with each tool felt. We asked designers to rate the quality of the final design created with Inkspire and ControlNet and their overall experience satisfaction using each tool. All questionnaire questions were rated on a 7-point Likert scale (7=highly agree, 1-highly disagree). We compared the questionnaire measures using parametric paired t-tests. In addition, we log user interaction data, such as when participants draw a new sketch stroke, edit a prompt, and generate a new design. (Figures 13)."}, {"title": "5.3 Procedure", "content": ""}, {"title": "5.3.1 Introduction (5 minutes).", "content": "Participants provided informed consent and were given an overview of the study procedures. In addition, we briefly explain how components of the system, such as the analogy generation and sketching and scaffolding interaction, works."}, {"title": "5.3.2 Design Tasks (45 minutes).", "content": "Participants completed a design task with Inkspire (Figure 1) and another design task with the baseline ControlNet tool (Figure 14). The two design tasks are \"design a lamp with the theme of serenity\" and \"design a chair with the theme of fluidity.\" We counterbalance both the order of the tools and the order of the design tasks. After each condition, the participants completed the questionnaires."}, {"title": "5.3.3 Post-Study (10 minutes).", "content": "Participants gave feedback during a short interview as well as through a free response questionnaire on the individual subcomponents of Inkspire, their overall experience of using Inkspire, whether they could see Inkspire being integrated into their design workflow, and areas for improving the tool. We reviewed these qualitative data to support the quantitative results."}, {"title": "6 RESULTS", "content": ""}, {"title": "6.1 Creativity", "content": "Participants felt that Inkspire improved support for creativity across some attributes of the Creativity Support Index (CSI), shown in Figure 5. Notably, participants reported significantly higher exploration with Inkspire (\u03bc=5.83, \u03c3=1.27) as compared to the baseline (\u03bc=3.83, \u03c3=1.64), (t(11)=3.94, p<0.01, r=0.77, ds=1.13). Participants also reported significantly higher inspiration (\u03bc=5.92, \u03c3=1.24) as compared to the baseline \u03bc=4.00, \u03c3=1.41), (t(11)=3.44, p<0.01, r=0.72, ds=0.99).\nFrom our interviews, participants noted that the [analogy] inspirations feature is \u201chelpful while doing design ideations\" (P12) and \"a good tool to brainstorm in the early stage of design\" (P5). Participants also found Inkspire to effectively support the exploration of multiple ideas, such as a \u201cvariety of forms, styles, patterns, and proportions\" (P5). P11 explained that they could explore different design directions by manipulating high-level concepts (\u201cI could just make a basic shape, and change a few keywords and the entire look and feel would change and present me with some great concepts\u201d). In contrast, we observed that in the baseline condition, many participants focused on sketching extensively on a single idea, a potential sign of the \"sunk-cost effect\" [7] (the more time spent in a given direction, the harder it is to move to a different one).\nWe also note that the other attributes of the CSI, Engagement, Expressiveness, Tool Transparency, and Effort/Reward Tradeoff, were not significantly different between Inkspire and the ControlNet baseline. While Inkspire did not improve these aspects of creativity over using ControlNet it did not appear to degrade them either. Overall, we find that Inkspire improved exploration and inspiration, supporting our original design goals and working toward reducing design fixation."}, {"title": "6.2 Human-AI Collaboration", "content": ""}, {"title": "6.2.1 Designer self-ratings of Human-Al Collaboration.", "content": "Participants felt that Inkspire improved the experience of collaborating with the AI, across dimensions of Human-Machine Collaboration questions from [44] rated on 7-point Likert scales, shown in (Figure 7). Participants reported significantly higher controllability when using Inkspire (\u03bc=5.58, \u03c3=1.00) as compared to the baseline (\u03bc=4.17, \u03c3=1.19), (t(11)=3.56, p<0.01, r=0.73, ds=1.03). Participants felt that they had significantly higher communication with Inkspire (\u00b5=5.75, \u03c3=1.14) as compared to the baseline (\u03bc=3.92, \u03c3=1.38), (t(11)=4.52, p<0.01, r=0.81, ds=1.31). In addition, participants rated having a significantly higher degree of partnership with the AI when using Inkspire (\u03bc=5.83, \u03c3=1.03) as compared to the ControlNet baseline (\u03bc=3.42, \u03c3=1.56), (t(11)=4.57, p<0.01, r=0.81, ds=1.32). Lastly, participants rated having significantly more of their own attribution in the designs when using Inkspire (\u03bc=5.25, \u03c3=1.29) as compared to the baseline (\u03bc=3.67, \u03c3=1.07),(t(11)=3.51, p<0.01, r=0.73, ds=1.01). Participants did not report significant differences in their feelings of harmony with the AI or ownership over the designs between Inkspire and baseline ControlNet.\nFrom our interviews and open-response questions, many participants commented that they found the scaffolding helpful in being able to understand the current state of the Al and plan subsequent sketches (\"[The scaffolding] was useful because it let me know where the current iteration was so I could tell where I'd like to move it next\" (P11)). With scaffolding, participants felt that they could steer the direction of the design by building on previous generations (\"[scaffolding] helped with building upon the previous Al-generated design and gave me a direction for what to adjust\" (P6), \"the AI-generated drawing overlay help[ed] me to draw my next line\" (P3), \"having the [scaffolding] as reference helped me [to] combine and remix [old designs] in my [new] sketches \"(P6)). These observations and self-reported results suggest that Inkspire improves human-AI collaboration by increasing controllability, communication, and partnership."}, {"title": "6.2.2 Prompting Behavior.", "content": "As expected, we observed that participants generally did less prompt engineering when using Inkspire"}, {"title": "6.2.3 Sketching Behavior.", "content": "We observed that participants drew fewer total sketch strokes using Inkspire (\u03bc=17.3, \u03c3=7.40, number of strokes) as compared to using the baseline (\u00b5=59.8, \u03c3=40.5, number of strokes). When using Inkspire, participants also had a lower sketching frequency (\u03bc=12.5, \u03c3=12.5, strokes/min) as compared to the baseline (\u03bc=20.1, \u03c3=12.6, strokes/min), though this was not significant (t(11)=-1.67, p=0.12, r=0.45, ds=0.48). Participants also spent more time between strokes when using Inkspire (\u03bc=13.8s, \u03c3=12.1s) as compared to the baseline (\u03bc=4.44s, \u03c3=2.94s), (t(11)=2.66, p=0.02, r=0.63, ds=0.77).\nDespite participants sketching less and taking longer between adding more ink to their drawing, participants rated that sketch strokes felt significantly more inexpensive when using Inkspire (\u03bc=6.08, \u03c3=0.90, 7-point Likert scale, higher is better, based on Buxton's sketching principles [15", "15": "as compared to the baseline (\u03bc=3.83, \u03c3=1.47, 7-point Likert scale, higher is better), (t(11)=4.24, p<0.01, r=0.79, ds=1.23"}]}