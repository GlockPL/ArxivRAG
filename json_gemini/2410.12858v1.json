{"title": "LARGE LANGUAGE MODELS FOR MEDICAL OSCE ASSESSMENT: A NOVEL APPROACH TO TRANSCRIPT ANALYSIS", "authors": ["Ameer Hamza Shakur, PhD", "Michael J. Holcomb, MS", "David Hein, MS", "Shinyoung Kang", "Thomas O. Dalton, MD", "Krystle K. Campbell, DHA", "Daniel J. Scott, MD", "Andrew R. Jamieson, PhD"], "abstract": "Grading Objective Structured Clinical Examinations (OSCEs) is a time-consuming and expensive process, traditionally requiring extensive manual effort from human experts. In this study, we explore the potential of Large Language Models (LLMs) to assess skills related to medical student communication. We analyzed 2,027 video-recorded OSCE examinations from the University of Texas Southwestern Medical Center (UTSW), spanning four years (2019-2022) and several different medical cases or \"stations\". Specifically, our focus was on evaluating students' ability to summarize patients' medical history -we targeted the question 'did the student summarize the patients' medical history?' from the communication skills rubric. After transcribing speech audio captured by OSCE videos using Whisper-v3, we studied the performance of various LLM-based approaches for grading students on this summarization task based on their examination transcripts. Using various frontier-level open-source and proprietary LLMs, we evaluated different techniques such as zero-shot chain-of-thought prompting, retrieval augmented generation, and multi-model ensemble methods. Our results show that frontier LLM models like GPT-4 achieved remarkable alignment with human graders, demonstrating a Cohen's kappa agreement of 0.88 and indicating strong potential for LLM-based OSCE grading to augment the current grading process. Open-source models also showed promising results, suggesting potential for widespread, cost-effective deployment. Further, we present a failure analysis identifying conditions where LLM grading may be less reliable in this context, and recommend best practices for deploying LLMs in medical education settings.", "sections": [{"title": "1 Introduction", "content": "The Objective Structured Clinical Examinations (OSCE) [1] is a crucial component of the professional education and training of students during medical school. The OSCE tests various components of clinical competency by putting the student through mock medical encounters with a human actor known as a standard patient (SP), where they role-play as patients suffering from common medical conditions. Several aspects of performance are evaluated during the OSCE, such as students' proficiency in taking medical history, clinical reasoning, physical examinations, and ability to come to the correct diagnosis and workup. Equally important is testing various dimensions of the student's communication skills [2] - building trust, giving and providing information, and demonstrating empathy. For each case the standardized patient reports a chief complaint and is trained to respond to student questions on the key attributes of their present illness, associated symptoms, medical history, and family and social history. Then, they perform physical examinations if needed and discuss differential diagnosis and an appropriate treatment plan. The entire encounter is video recorded to be later reviewed by a team of trained experts."}, {"title": "Prior Work", "content": "Recent advancements in LLMs have generated significant interest in re-imagining medical education [5, 6, 7]. Frontier models have demonstrated impressive capabilities in medical domain tasks, with ChatGPT, Med-PaLM, and MedPaLM-2 achieving passing marks in standardized medical entrance examinations such as the USMLE. They have also matched, if not exceeded human performance on several benchmarks measuring knowledge and reasoning capabilities both in a general sense [8, 9] as well as specifically in medical Q&A [4, 10]. The growing capabilities of AI systems have sparked broad discourse about this technology's potential to revolutionize all aspects of medical education curriculum development, teaching, learning support, assessment, personalized education, etc. [5].\nRecent efforts to automate OSCE assessment include use of large multi-modal models (LMMs) to evaluate the physical exam component [11] of the OSCE. Here, conventional computer vision techniques were used to extract relevant frames; then the LMM was prompted to describe and grade the student based on the extracted frames [11]. A recent work [12] proposed a deep learning pipeline to identify, extract, and score ear exams based on off-the-shelf object detection tools such as CLIP [13] and Detectron2 [14]. Our literature search found only one study focused on grading communication skills from transcript analysis. In this study, [15] 121 OSCE videos were manually transcribed and annotated, and five different text classification models were evaluated on their ability to label various communication skill items from the transcripts.\nOur work investigates the feasibility of state-of-the-art LLMs to assess student performance, specifically in history summarization, by analyzing transcripts extracted from the recorded exams. We measure model alignment with human expert graders from a collection of historical exams made up of 2,027 video-recorded OSCEs from the University of Texas Southwestern Medical Center (UTSW), spanning four years and ten different medical cases or 'stations'. Additionally, we investigate the impact of retrieval augmented generation (RAG) techniques and provide a comprehensive overview of the various failure modes and considerations for LLMs and RAG-techniques that should be considered prior to deploying such tools for exam grading. We believe this to be the first work to evaluate a transcript-based AI-augmented OSCE grading system at this scale."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Datasets and pre-processing", "content": "We selected 2,027 videos of the Comprehensive Objective Structured Clinical Exams (COSCEs) recorded at UT Southwestern Medical Center (UTSW) between the years 2019 \u2013 2022. Videos covered simulated consultations with patient actors presenting different 'cases' (also known as 'stations') such as 'itchy eyes', 'vision problems', 'memory problems' etc. The exams are evaluated by a team of trained human experts using standardized checklists or rating scales to assess each candidate's performance. Table 1 shows the distribution of the counts and scores in the COSCEs by year in our evaluation dataset.\nData safety\nStrict data protocols were used to ensure data safety. Proprietary models were accessed via a secure platform. Where this was not available, presidio-anonymizer [16] package was used to remove all personally identifiable information"}, {"title": "Summary of medical history task", "content": "In this study, we specifically address the item - 'did the student summarize the patients' medical history'? - from the communication skills rubric of the OSCE. Providing a summary of medical history is a crucial skill that serves a critical purpose in addressing the patient's concerns. It ensures that the student has understood the patient's overall health status and allows the patient to issue corrections in case of a misunderstanding. For example, an acceptable summary of medical history provided by the student at the 'memory problems' station may look like:\n\"So just to kind of summarize over the past seven months, you've been feeling that you have been having some difficulty concentrating, it's been taking you some time to balance your checkbook, and you noticed you just sleep less, you had less of an appetite, and you kind of not found as much interest in things that you used to enjoy. And it's been kind associated with some weight loss and no other pain anywhere. Is that correct?\"\nFig. 2 shows the distribution of scores the medical students obtained on this question, with 2 being full credit. We noticed that approximately 3% of students received a score of 1 from the SP graders or partial credit."}, {"title": "Pre-processing", "content": "\u2022 Transcription. OpenAI's Whisper-large-v3 [3] was used for automated speech recognition algorithm to generate a transcript of the conversation that took place in the examination videos. No further human corrections or annotations were used in the transcript. We found that 2% of the transcripts were unusable due to poor audio quality of the exam recordings \u2014. These were removed from the analysis.\n\u2022 Anonymization. As mentioned above, if needed, the presidio-anonymizer was also run to remove personally identifiable information before sending it to Claude APIs."}, {"title": "2.2 Model Design", "content": "In this section, we describe the LLMs and different prompting strategies explored in this study as well as our criteria for evaluating system performance against human expert graders. Table 1 provides an overview of the models evaluated. We selected these models to represent a broad spectrum of current LLM capabilities, including state-of-the-art proprietary"}, {"title": "2.3 Zero-shot generation", "content": "Zero-shot generation leverages the LLM's knowledge and \"reasoning\" ability without any explicit training or learning from prior OSCE transcripts or graded examples. In this approach, we provide the entire transcript to the model along with the grading instructions for the rubric item and a scoring criterion. Further, we leveraged chain-of-thought prompting [17], which has been shown to improve LLM reasoning ability by asking the model to articulate internal reasoning steps before arriving at the grade. In our case we prompt the model to first extract the relevant 'statement' which is the portion of the transcript that corresponds to the item being graded, then generate a 'rationale' based on the grading rubric and a \u2018score' based on the scoring criteria. Finally, we ask the model to return each of these items in a structured JSON format for convenient post processing. Fig. 4 illustrates our workflow for the zero-shot grading process. Prompt instructions are provided in the supplementary materials."}, {"title": "2.4 Retrieval augmented generation (RAG)", "content": "Retrieval augmented generation has become a popular technique for enhancing model performance by combining LLM capabilities with information retrieval [18, 19]. The primary motivation for using RAG is the ability to produce more accurate, factual, and contextually relevant outputs by grounding the model's responses in retrieved information. Grounding responses with RAG mitigates hallucinations common in language model outputs and enables the handling of specialized tasks without extensive fine-tuning. In our workflow, the OSCE transcript is of a 15-minute-long conversation and only a small portion of it is relevant to the rubric item being graded. By retrieving only the most relevant portion of the transcript for the model to grade, performance may be improved. Additionally, fewer tokens will be required, reducing inference costs. The retrieval augmented grading process involves two primary steps: i) retrieving the most relevant sections of the transcript in the current context and ii) prompting the LLM to generate a score based on the retrieved contexts. We describe our approach for each of these steps in further detail."}, {"title": "2.4.1 Retrieving the correct context", "content": "The retrieval of the relevant portion of the transcript involves the following sequential steps:\n\u2022 Chunking: We segment the full transcript into an average of 25 sequential chunks. We used the Lang Chain [20] text-splitter with a chunk length of 500 characters and overlap of 0 characters. These parameters were chosen to optimize retrieval using a validation dataset.\n\u2022 Embedding: We then create vector embeddings of each of the chunks using the bge-large-v2 embedding model [21]. Each chunk is encoded into a high-dimensional embedding space of 1024 dimensions, where the embedding vector can be understood to preserve some semantic context. These vector embeddings were then stored in a Chroma vector database.\n\u2022 Querying. To retrieve the most relevant chunks of the transcript we use a \"query\" vector that is semantically similar the chunk that we are looking to retrieve from the transcript. The choice of this query vector is the most crucial step to determining the context being retrieved by the retriever. We experimented with several query formulation strategies, including using predefined templates, dynamically generated queries based on the rubric item, and even using LLMs to generate context-aware queries. The different query strategies used are elucidated in Table 2.\n\u2022 Reranking. The chunks are then re-ranked based on their relevance to the query. We used the ms-marco-TinyBERT-L-2 cross-encoder model [22]. This model is trained on query-vector pairs and outputs a relevance score for each pair of sentences.\n\u2022 Retrieval. The top 5 most relevant chunks based on the re-ranker score are then chosen to be retrieved."}, {"title": "2.4.2 Retrieval augmented grading", "content": "We investigate two different approaches for grading students based on the retrieved context from the transcripts:\n\u2022 LLM grading. In this approach, we send the retrieved context from the retrieval step to the LLM with a prompt nearly identical to that used in the zero-shot generation process. The model is then asked to grade the students based solely on this retrieved context.\n\u2022 SetFit Classification. In this approach, we utilize SetFit (Sentence Transformer fine-tuning) [23], an efficient few-shot text classification method. SetFit works by fine-tuning a pre-trained Sentence Transformer embedding model on a small number of pairs of sentences in a contrastive manner to create task-specific embeddings. This"}, {"title": "2.5 Evaluation criteria", "content": "We employed several evaluation metrics to assess the performance of our RAG and LLM-based grading system for OSCE exams. These metrics were chosen to capture various aspects of the grading process, including accuracy, consistency, and the effectiveness of the retrieval mechanism."}, {"title": "2.5.1 Evaluating transcript grading performance", "content": "The following metrics were used to evaluate the overall performance of the automated grading system:\n\u2022 Accuracy score: This metric measures the overall correctness of the grading system by calculating the proportion of correctly graded responses out of the total responses. It is defined as:\n$Accuracy = \\frac{Number \\ of \\ correctly \\ graded \\ responses}{Total \\ number \\ of \\ responses}$ (1)\nWhile accuracy provides a straightforward measure of performance, it may not be sufficient in cases of imbalanced datasets.\n\u2022 F1 score: This metric provides a balanced measure of the grading system's precision and recall, making it particularly useful for evaluating performance on imbalanced datasets. The F1 score is the harmonic mean of precision and recall:\n$F1 \\ score = 2 * \\frac{Precision * Recall}{Precision + Recall}$ (2)\nwhere Precision is the ratio of true positives to all predicted positives, and Recall is the ratio of true positives to all actual positives.\n\u2022 Cohen's kappa: This statistic assesses the inter-rater agreement between the automated grading system and human graders, accounting for the possibility of agreement occurring by chance [24]. It is calculated as:\n$\\kappa = \\frac{p_o - p_e}{1 - p_e}$ (3)\nwhere po is the observed agreement and pe is the expected agreement by chance. Cohen's kappa provides a more robust measure of agreement than simple percent agreement calculation."}, {"title": "2.5.2 Evaluation of retrieval performance", "content": "To assess the effectiveness of the retrieval component in our RAG system, we calculated the recall metric,\n\u2022 Retrieval recall: This metric measures the proportion of relevant information successfully retrieved for grading purposes. It is defined as:\n$Retrieval \\ Recall = \\frac{Number \\ of \\ retrieved \\ summary \\ statements}{Total \\ number \\ of \\ transcripts}$ (4)\nTo calculate this metric, we used a separate dataset of exam transcripts where GPT-4's zero-shot grading matched human grading. For these exams, we extracted \"summary statements\" identified by GPT-4 that were manually verified to be accurate. Our retrieval method then produced the top-5 chunks of each transcript. If the corresponding summary statement was found in these chunks, we considered it a successful retrieval. The retrieval performance was measured as the proportion of exams where the summary statement was successfully retrieved within the top-5 chunks."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Alignment of LLM grades with human expert graders", "content": "Fig. 6 shows a longitudinal view of LLM and human grade alignment across four distinct student cohorts from 2019 to 2022. A dramatic improvement in alignment was found in the year 2021. GPT -4's performance best exemplifies this shift: its kappa value increased from 0.56 in 2020 to over 0.87 in 2021, further improving to 0.88 in 2022 (Table 3). This improvement was mirrored across other models, suggesting a fundamental change in human grading approach rather than model-specific enhancements.\nTherefore, focusing only on data from 2021 onwards, we show the comparative performance of each of the models in Fig. 7. Here we observe a clear hierarchy in model performance. GPT-4 emerged as the clear front runner, achieving a 95.7% accuracy and a kappa of 0.88. Next, we find that Claude-Opus and Claude-Sonnet-3-5 and the open-weights Llama-3-70B are closely matched in terms of their performance, achieving Cohens kappa values ranging between 0.86 to 0.83 with Sonnet-3-5 having a slight edge over the others. However, GPT 3.5, Mixtral and Starling-7B-beta perform much worse with a kappa value lower than 0.35 for each of these models.\nWe further explore performance across different medical cases in Table 5. Kappa values generally ranged between 0.7 and 0.9 for top-performing models, with slight fluctuations depending on the specific case. This consistency across diverse medical scenarios demonstrates robustness of these models in handling varied clinical contexts."}, {"title": "3.2 Retrieval augmented generation", "content": ""}, {"title": "3.2.1 Retrieval performance", "content": "We first evaluated the effectiveness of different query strategies for retrieving the relevant portions of the transcript containing the summary of medical history. Table 6 presents a comparison of these strategies. When the query-vector is simply the statement \"a summary of medical history of a patient presenting with <case>\", the recall@5 is only 54%. However, the auto-summarizer strategy emerged as the most effective, achieving a remarkable 92% recall@5. This means that for 92% of the transcripts, the relevant summary section was found within the top 5 retrieved chunks. This strategy involves using an LLM to generate a summary of the medical history based on the full transcript, which is then used as the query vector."}, {"title": "3.2.2 Performance of the retrieval augmented grading scheme", "content": "Table 7 presents the results of two RAG approaches. The Retrieval + SetFit approach (2.4.2) which uses a multi-label SetFit classifier trained on retrieved chunks to identify summary statements achieved a Cohen's kappa of 0.62 and an F1 score of 0.84. Notably, the Retrieval + GPT-4 approach (2.4.2), where we sent the retrieved context to GPT-4 for grading, resulted in a Cohen's kappa of 0.56 and an F1 score of 0.82. These results are notably lower than the performance achieved by full transcript grading, where GPT-4 attained a Cohen's kappa of 0.88 (as shown in Table 1). This significant drop in performance is particularly surprising given the high recall rates of our retrieval system. Several factors may be contribute to this performance gap such as loss of context, sensitivity to retrieval errors, or chunk integration, or needing the full context of the conversation for accurate grading. By focusing only on the retrieved chunks, the models may miss important contextual information present in the full transcript. This could be crucial for accurately identifying and evaluating summary statements. For the LLM approach, the model may struggle to coherently integrate information from multiple retrieved chunks compared to processing a full, continuous transcript. The SetFit classifier may be limited by the quantity and quality of training data available from the retrieved chunks."}, {"title": "3.3 Inter-model agreement and ensemble-performance", "content": "We also investigated if grading performance could be improved by using an ensemble of models, for example by offering increased robustness and reliability. First, inter-model correlation (Fig. 9) can be checked to reveal which models perform most similarily at a high-level. GPT-4 and Sonnet-3-5 show the highest inter-model agreement at 0.89. Also, we see a pattern of slightly decreasing agreement as we move from top-performing models to lower-performing ones. For instance, GPT-40 shows lower agreement with other models (0.8-0.825) compared to the agreement between top models. The inter model agreement rate also indicates that while models agree on a significant portion of their grades, there's also meaningful variation. If agreement were too low, it would suggest the models are inconsistent or unreliable. If agreement were too high (near 1.0), ensemble methods wouldn't provide much benefit as all models would essentially give the same grades. The slightly different perspectives of each model (as evidenced by agreement levels around 0.8 rather than 1.0) suggest that an ensemble approach could help identify and correct for individual model biases or errors."}, {"title": "4 Discussion", "content": ""}, {"title": "4.1 Key takeaways", "content": "The frontier LLMs, particularly GPT-4, demonstrated exceptional alignment with human graders, achieving 95.7% accuracy, 0.95 F1 score, and a Cohen's kappa of 0.88. Claude-Sonnet-3-5 and Llama-3-70B also showed strong performance with 93.6% and 93.2% accuracy respectively, and kappa values over 0.83. The less capable models GPT-3.5, Mixtral 8x7B and Starling -7B demonstrated much worse alignment with human graders, all reaching a Cohens' kappa of less than 0.35, underscoring the rapid pace of advances in LLM capabilities. Error analysis, described in greater detail below, showed that these poor-performing models consistently fail to understand what constitutes a"}, {"title": "4.2 Failure analysis", "content": "Our analysis also revealed limitations and potential failure modes that must be considered when implementing such a transcript-based AI grading system. Addressing these challenges will be crucial in ensuring the reliability and fairness of AI-assisted assessment in medical education. The two main categories errors were centered around data quality and LLM inference."}, {"title": "Data errors.", "content": "\u2022 Audio recording errors: We found that we had to discard 2% of our transcripts due to poor quality caused by audio recording errors. This could be due to several reasons such as mics not being setup properly, recording not starting at the right time, or too much background noise muffling the conversation. In future deployments, before using a transcript for AI-assessment, sufficient audio quality should be first verified for faithful transcription.\n\u2022 Transcription errors: The Automatic Speech Recognition (ASR) system's quality significantly influences downstream analysis outcomes. Whisper-v3 demonstrated excellent performance, producing high-quality transcripts even from sub-optimal audio recordings. Primary error types included hallucinations during silent periods -a known limitation of transformer-based ASR models, and misspellings of medical and technical terminology [3]. However, these errors minimally impacted downstream analysis, as advanced Language Models (LLMs) can infer correct terms from misspellings."}, {"title": "LLM inference errors.", "content": "The most common error patterns in LLM scoring of student transcripts, in order of increasing sophistication, are:\n\u2022 Hallucination errors. LLMs may generate non-existent summaries or falsely score students based on self-summarized symptoms. Starling-7B was most prone to this, with GPT-3.5 and Mixtral-8x7B also exhibiting such errors. While realistic, these can be easily identified through transcript verification.\n\u2022 Prompt Misinterpretation: Models may misunderstand the concept of a medical history summary. When no clear summary exists, they might misidentify discussions of medical history or other symptoms as valid summaries, leading to false positive scores. GPT-3.5 and Mixtral-8x7B were particularly susceptible.\n\u2022 Contextual Confusion: Despite understanding summary concepts, models may misinterpret context. For instance, they might mistake a student's diagnostic explanation during symptom recap for a valid medical history summary. Ideally, such summaries should conclude the information gathering phase, prompting patient confirmation and correction."}, {"title": "4.3 Ethical considerations", "content": "While our results demonstrate high alignment between LLM and human graders, as efforts are made to deploy AI assessment systems in practice, vigilance should be maintained for addressing ethical concerns. On-going research as well as active monitoring will be necessary to ensure LLM-based systems do not exhibit unintended biases in performance assessment and maintain fair/equitable grading across diverse student populations. As more complex models emerge, preserving transparency in grade determination is crucial for student trust and feedback. The implementation of AI-based grading may influence OSCE preparation strategies; therefore, measures must be taken to ensure students develop genuine communication skills rather than optimizing for AI evaluation. Proactively addressing these ethical considerations will enable effective leveraging of AI benefits in medical education assessment while mitigating potential risks and ensuring fairness and transparency.\nWe plan to extend this work to all sections of the communication rubric -assessing student ability in providing information, gathering information, showing empathy and decision making. Additionally, further research may be focused on developing standardized guidelines for the implementation of AI-based grading systems. Investigating the applicability of these techniques to other aspects of medical education, such as curriculum development and personalized learning, could lead to a more comprehensive integration of AI in medical training."}, {"title": "5 Conclusion", "content": "To our knowledge, this is the first work to study use of LLMs towards analyzing medical student OSCE transcripts for automatic assessment. Given the relatively large sample size of our study (with over 2000 OSCE encounter transcripts over 10 different stations) we believe our these findings provide a solid foundation for better understanding the strengths and limitations of GenAI-based grading in medical education with currently available AI models."}]}