{"title": "CONVEX MARKOV GAMES: A FRAMEWORK FOR\nFAIRNESS, IMITATION, AND CREATIVITY IN MULTI-\nAGENT LEARNING", "authors": ["Ian Gemp", "Andreas Haupt", "Luke Marris", "Siqi Liu", "Georgios Piliouras"], "abstract": "Expert imitation, behavioral diversity, and fairness preferences give rise to pref-\nerences in sequential decision making domains that do not decompose additively\nacross time. We introduce the class of convex Markov games that allow general\nconvex preferences over occupancy measures. Despite infinite time horizon and\nstrictly higher generality than Markov games, pure strategy Nash equilibria exist\nunder strict convexity. Furthermore, equilibria can be approximated efficiently by\nperforming gradient descent on an upper bound of exploitability. Our experiments\nimitate human choices in ultimatum games, reveal novel solutions to the repeated\nprisoner's dilemma, and find fair solutions in a repeated asymmetric coordination\ngame. In the prisoner's dilemma, our algorithm finds a policy profile that deviates\nfrom observed human play only slightly, yet achieves higher per-player utility\nwhile also being three orders of magnitude less exploitable.", "sections": [{"title": "1 INTRODUCTION", "content": "It has been a characteristic assumption in multi-agent reinforcement learning that agent rewards are\na discounted sum of per-period rewards (Littman, 1994). This article proposes a framework beyond\nsuch decomposed reward, and argues its algorithmic tractability and utility.\nAlready early game theory has observed that maximizing decomposable rewards may lead to a high\nnumber of equilibria, of very different desirability for a designer, (Friedman, 1971). In addition,\nmany search problems have other desiderata, e.g., diversity of play, that cannot be expressed as\nreward maximization (Zahavy et al., 2023). In addition, such utilities do not permit the designers to\nfind approximate equilibria that have desirable features such as being human-like (imitation), or that\nare \"creative\" in that their trajectories visit many parts of the space, or to include fairness concerns.\nIn this paper, we define a general yet tractable framework for the inclusion of fairness, imitation and\ncreativity components in multi-agent learning.\nA crucial object for our analysis is the occupancy measure \u03bc. Occupancy measures are the long-term\ndistributions of states and actions in different time periods, taking into account discounting. That\nis, the weight of a state-action profile (s, (a1, a2,..., an)) is obtained by summing the probabilities\nthat st = s and a\u2081t = a1,a2t = a2,..., Ant = An discounted by yt. In general, the class of utilities\nthat we consider here are concave functions of occupancy measures (equivalently, players' \u201closses\u201d\nor negated utilities are convex). Restricting to the class of linear utility functions, we recover Markov\ngames, that is, the classic model of multi-agent reinforcement learning (Littman, 1994).\nWe describe our more general framework in the context of three classes of utility functions. For\nthis discussion, we consider a linear reward function r \u2208 RS\u00d7A, which we view as the reward from"}, {"title": "2 CONVEX MARKOV GAMES", "content": "The main definition of this article is the convex Markov Game (CMG).\nDefinition 1. A convex Markov game is given by a 6-tuple G = (S, A = X^=1 Ai, P, u, \u03b3, \u03bc\u03bf):\n\u2022 Players i = 1, 2, . . ., n,\n\u2022 a finite state space S, and an initial state distribution \u03bc\u03bf \u2208 \u0394\u03c2,\n\u2022 finite action spaces A1, A2, ..., An,\n\u2022 a state transition function P: S \u00d7 (X=1 A\u2081) \u2192 AS,\n\u2022 a discount factor \u03b3\u2208 [0, 1), and\n\u2022 a set of continuous utilities with each ui concave in the ith player's occupancy measure,\nUi: (X=1\u2206S\u00d7Aj) \u2192 R.\nThe Policy View Players i = 1,...,n choose policies \u03c0\u2081: S \u00d7 A\u00bf \u2192 [0,1]. A policy profile\n\u03c0 = (\u03c01, \u03c02,..., \u03c0\u03b7) induces a sequence of states and joint actions (st)ten and (at)ten, and a\nstate-action occupancy measure\n$\\mu^{\\pi}(s, a)=(1-\\gamma) \\sum_{t=0}^{\\infty} \\gamma^{t} P(s_{t}=s, a_{t}=a | \\mu_{0}, \\pi, P).$"}, {"title": "3 EXISTENCE OF NASH EQUILIBRIUM", "content": "We first show that under fairly general assumptions, mixed Nash equilibria exist. We then argue that\nfor the purposes of learning, pure-strategy Nash equilibria are particularly desirable, and show that\nthese exist when utilities are strictly concave.\nProposition 1. Mixed-strategy Nash equilibria exist in convex Markov Games.\nThis statement follows easily by using the policy view. Each player's optimization problem in terms\nof policies is:\n$\\max_{\\pi_{i} \\in(\\times_{S} \\Delta A_{i})} U_{i}(\\mu(\\pi_{i}, \\pi_{-i}), \\pi_{-i}).$\nAppendix B shows that all u\u2081 are continuous, differentiable functions of (\u03c0\u03af, \u03c0_i), and since the\nstrategy sets are compact, the existence of mixed-strategy Nash equilibria follows from classical\nexistence results (Glicksberg, 1952).\nWhile existence of mixed-strategy equilibria are a descriptively helpful tool, they have limitations\nfor applications in learning. In particular, learning a continuous distribution over stochastic poli-\ncies would practically require function approximation or defining a mesh over the space, greatly\nincreasing the complexity of the learning problem; this challenge has been broached but not suffi-\nciently solved in the context of, for example, generative adversarial networks (Arora et al., 2017).\nTherefore, we provide another statement that ensures the existence of pure-strategy Nash equilibria.\nTheorem 1. Given a convex Markov game with unique best response correspondences in the space\nof occupancy measures, Nash equilibria exist in stationary stochastic policies (and hence state-\naction occupancy measures).\nThe proof of this statement relies on the continuity of players' utilities and compactness of their\nstrategy sets as before, but also the uniqueness of best responses.\u00b9 The classes of strictly convex\nfunctions and invex functions (Hanson, 1981) are two that give rise to unique best response cor-\nrespondences. Most definitions of (neg)entropy and distance metrics satisfy strict convexity. Any\npenalty function given by a quadratic with positive definite matrix \u2211 does as well.\nWe briefly address the limitation of unique best response correspondences with a more detailed\ndiscussion in the Appendix G. While concave utilities give rise to convex best response correspon-\ndences in occupancy measure space, the image of these sets in policy space may be non-convex. The\nfoundation of many equilibrium existence results rely on the assumption of a mapping from points (a\nstrategy profile) to convex sets (a set of strategy profiles) via the Kakutani fixed point theorem. This\ncore assumption is broken in the more general cMG setting. Proving existence of Nash equilibria in\nthis more general setting is desired as the allowance of concave (not-strictly) utilities would enable\nthe specification of \"safe\" occupancy measure sets (Miryoosefi et al., 2019)."}, {"title": "4 COMPUTATION OF EQUILIBRIA", "content": "While equilibria exist, they may be hard to compute with general solvers due to the size of the equi-\nlibrium computation problem and the expense of repeatedly solving convex programs. We present\na gradient-based approach for equilibrium computation. We first use the occupancy measure view\nto derive a measure of utility-gain (\u20ac\u2081) from player best responses, and then reparameterize this loss\nto policies to circumvent challenges arising from non-convexity of the set of occupancy measures\n(Appendix G.2).\nWe provide an upper bound on exploitability (\u20ac) as a loss for equilibrium computation, defined as\n$\\epsilon = \\max _{i=1, \\ldots, n} \\epsilon_{i} \\quad \\text { where } \\quad \\epsilon_{i}=\\left(\\max _{\\hat{\\mu} \\in \\mathcal{M}_{i}\\left(\\mu_{-i}\\right)} U_{i}\\left(\\hat{\\mu}, \\mu_{-i}\\right)\\right)-U_{i}\\left(\\mu_{i}, \\mu_{-i}\\right).$\nExploitability measures the most any player can gain by unilaterally deviating. Mechanistically, it\ncorresponds to each player solving problem (3)\u2014a constrained, convex optimization problem\u2014and"}, {"title": "5 EXPERIMENTS", "content": "We test three variants of utilities that do not decompose linearly in four domains. We compare\nagainst four baseline algorithms, and compare the resulting exploitability and policy profiles.\nBaselines. We compare against four baselines. The first baseline, mine, directly minimizes ex-\nploitability with respect to the agent policies using a differentiable convex optimization package\nCVXPYLAYERS in JAX (Agrawal et al., 2019; Bradbury et al., 2018). In the second baseline, Sim,\nall players simultaneously run gradient descent on their losses with respect to their policies and\nwe report the performance of the running average of the policy trajectory. Policies at each state\nare represented in R|A|\u22121 as a softmax over |A|\n1 logits with the last logit fixed as 0. In the\nthird, RR, agents alternate gradient descent steps in round-robin fashion. We also compare against\nthe SGAMESOLVER (Eibelsh\u00e4user & Poensgen, 2023) package of homotopy methods for Markov\ngames.\nHyperparameters. We use an exponentially decaying annealing schedule of r = 10\u00aft for t \u2208\n[0, . . ., 7], with 1000 iterations at each temperature 7 for our entropy regularization. Adam's internal\nstate is not reset after annealing. We initialize to uniform policies (and whatever occupancy measure\nthat induces) unless otherwise specified. All experiments except pathfinding were run on a single\nCPU and take about a minute to solve although exact exploitability reporting via cvxopt (Diamond\n& Boyd, 2016) increases runtime approximately 10\u00d7. Pathfinding was run on a single GPU.\nDomains. We consider five domains: The first is a multiagent pathfinding problem in a grid\nworld.The second domain is the classic two-player, iterated prisoner's dilemma (compare Tucker\n& Straffin Jr (1983)) where agents may choose to cooperate or defect with their partner. The third\ndomain is a three-player, public goods game where agents may choose to contribute all or none\nof their savings to a public pool which is then redistributed evenly with a growth multiplier of 1.3\n(compare Janssen & Ahn (2003)); payoff is measured in terms of player profits. In the fourth, we"}, {"title": "5.1 CREATIVITY", "content": "Our first application considers utilities that value solutions that cover more than a small subset of\nthe state-action space, which leads to \"creative\" equilibria, as we show. We get such outcomes for\nutilities that get entropy bonuses for high (Shannon) entropy occupancy measures, u(\u03bc) = r\u00af\u03bc +\n\u03a4\u0397 (\u03bc).\nTo find equilibria of the original game, we anneal the weight on this entropy bonus towards zero, fol-\nlowing prior work on homotopy methods for equilibria in Markov games (Eibelsh\u00e4user & Poensgen,\n2019).\nFirst, we consider a multiagent pathfinding problem. Two agents must coordinate to pass through a\nbottleneck doorway on their way to a joint goal state. The reward for reaching the goal state is 100\nfor both players; -0.01 reward otherwise. Upon reaching the goal state, the agents are reset to the\nstart state (leftmost grid in Figure 2). Our algorithm returned an approximate equilibrium where the\nfinal utility for each agent was 24.5 and the exploitability was 1.7 (\u2248 7% of their utility).\nA single rollout of the final learned policy is shown in Figure 2. The agents race to cross the doorway,\nafter which, one agent takes the center position and the other steps aside. In the third and fourth to\nlast frames, the blue agent moves downward due to the small remaining entropy bonus. Both agents\nthen move upward towards the goal state. In the final frame, green executes a \"no-op\u201d action to\nthe right as blue moves into goal position. Despite learning a factorized Nash equilibrium policy\nprofile, the agents exhibit coordinated actions at certain step. This coordination is achieved through\nobservations of partner players grid locations, but richer coordination is theoretically possible with\nricher observation spaces.\nNext, we examine three iterated normal-form games. Figure 3 shows our algorithm has vanishing\nexploitability for all of them. In each game, directly minimizing exploitability in CVXPYLAYERS"}, {"title": "5.2 IMITATION", "content": "Our second application considers utilities that value policies similar to policies observed in human\nexperiments (Romero & Rosokha, 2023, Table 1, Current, Direct-Response). In this experiment,\nwe build on the homotopy experiment in the creativity section where we annealed our entropy co-"}, {"title": "5.3 FAIRNESS", "content": "Our final application considers utilities that value fair visitation of states. In the Bach-Stravinsky\ngame, two players must choose whether to attend a performance by Bach or Stravinsky. If they don't\nmeet, they get zero reward. However, one player prefers Bach to Stravinsky (3 vs. 2), whereas the\nother player prefers Stravinsky to Bach (3 vs. 2). We incorporate a term into both player's objectives\nthat penalizes 10 times the squared difference between attending Bach and attending Stravinsky to\nincentivize fair, equal attendance of the two shows, $u_{i}(\\mu)=r^{\\top} \\mu-\\left(\\sum_{\\alpha \\in\\{S, B\\}} \\mu((\\mathrm{B}, \\mathrm{B}), \\alpha)-\\sum_{\\alpha \\in\\{S, B\\}} \\mu((\\mathrm{S}, \\mathrm{S}), \\alpha)\\right)^{2}$. We can write the second term as \u03bc\u0f0b\u03a3\u03bc for an appropriately chosen\npositive semidefinite matrix \u03a3.\nIn order to avoid the trivial equilibrium of voting 50/50, we initialize logits for both players' policies\nwith a standard normal. We set temperature 7 to zero and then optimize with a learning rate of 0.1\nfor 1000 iterations.\nIn 10 random trials, both players converge to the same approximate NE where they vote for their\nfavored event 60% of the time regardless of their actions on the previous day. The maximum ex-\nploitability over the 10 trials is 2.5 \u00d7 10\u22125, and the max difference between \u03a3\u03b1 \u03bc\u03af((\u0392, \u0392), a) and\n\u03a3\u03b1 \u03bc\u03b5((S, S), a) is 2.14 \u00d7 10\u22125, implying this is a \u201cfair\u201d behavioral profile by our fairness metric."}, {"title": "6 RELATED WORK", "content": "Our work relates to the single-agent literature on convex Markov games and equilibrium selection\nin Markov games. We rely on techniques from the theory of abstract economies, loss minimization\nfor equilibrium computation, and use homotopy methods as inspiration for our experiments. Finally,\nwe unify approaches to creativity, imitation, and fairness from multi-agent learning."}, {"title": "7 CONCLUSION", "content": "Convex Markov Games are a versatile framework for multi-agent reinforcement learning, in par-\nticular for equilibria that imitate human behavior, pursue creative equilibria, and those where ad-\nditional constraints over states are desired. Not only do pure-strategy equilibria exist, there is also\na differentiable upper bound for its optimization. Conceptually, optimization can be viewed via a"}, {"title": "A EQUILIBRIUM PROOF", "content": "We now prove that a pure-strategy policy Nash equilibrium exists for any player utility functions\nthat admit unique solutions in the space of occupancy measures (a convex, compact set). This\nincludes strictly convex functions, but also the more general class of invex functions which may be\nnon-convex.\nFor convenience, we repeat the Kakutani fixed point theorem here.\nTheorem ((Kakutani, 1941)). Let I = \u041f\u2081 Ii be a non-empty, compact and convex subset of\nRS \u03a0\u0391\nLet \u00a2 : II \u2192 2I be a set-valued function on II with the following properties:\n\u2022 & has a closed graph;\n\u2022 \u03c6(\u03c0) is non-empty and convex for all \u03c0 \u2208 \u03a0.\nThen & has a fixed point.\nWe now state a useful Lemma that proves every occupancy measure maps to a convex set of policies.\nLemma 2. The set of policies that give rise to a single occupancy measure \u00b5i \u2208 R|S||Ai| for a\nsingle player i is a closed, convex set.\nProof. The mapping from occupancy measures to policies in convex MDP settings is typically de-\nfined such that it is one-to-one even on states with zero occupancy measure, e.g.,\n$\\pi_{i}\\left(\\mu_{i}\\right)(a | s)=\\left\\{\\begin{array}{ll}\\frac{\\mu_{i}(s, a)}{\\sum_{a^{\\prime}} \\mu_{i}\\left(s, a^{\\prime}\\right)} & \\text { if } \\sum_{a^{\\prime}} \\mu_{i}\\left(s, a^{\\prime}\\right)>0 \\\n1 /|A_{i}| & \\text { else. }\\end{array}\\right.$\nHowever, this definition is inadequate in a multi-player setting. In a game, we cannot assume (or\nforce) a player's policy takes a specific value unless it is rationalizable (i.e., they chosen that policy\nbecause it is their unqiue best response). Therefore, in the case where a state has zero occupancy,\nthe set of allowable policies in that state is the entire simplex \u2206Ai; this is why we present equation 4\nin Section 2.\nIn states where the occupancy has positive measure, the mapping from occupancy to policy is unique\nand results in a singleton, a closed, convex set. Otherwise, the mapping is to the simplex, also a\nclosed, convex set. A player's full policy is formed from the product sets of its state policies; the\nproduct of these convex sets is convex completing the claim.\nCorollary 1 (Convex Best Response Correspondence). If a player's best response correspondence\nis unique (a singleton) in the space of occupancy measures, then its best response correspondence\nin policy space is a convex set.\nIn what follows, although the theory of n-player, concave games by Rosen (1965) does not apply,\nwe are able to trace a similar line of reasoning and employ Lemma 2 to prove existence of pure Nash\nequilibria. First, we recall some useful properties and present some preliminary definitions.\nFor \u03c0\u2208 \u03a0, \u03c5\u03af(\u03c0) is continuous in \u03c0 from Lemma 3 (\u03bc\u2081(\u03c0) is continuous) and continuity of the\nobjective ui (\u03bc\u03b5).\nAn equilibrium point of the cMG is given by a point \u03c0* \u2208 I such that\n$u_{i}\\left(\\pi^{*}\\right)=\\max \\left\\{u_{i}\\left(\\pi_{i}^{\\prime}, \\pi_{-i}\\right) \\mid\\left(\\pi_{i}^{\\prime}, \\pi_{-i}\\right) \\in \\Pi\\right\\}(i=1, \\ldots, n)$.$\nThe results to follow make use of the function \u03c1(\u03c0, \u03c0') defined for (\u03c0, \u03c0') \u2208 \u03a0 \u00d7 \u03a0 by\n$\\rho\\left(\\pi, \\pi^{\\prime}\\right)=\\sum_{i=1}^{n} u_{i}\\left(\\pi_{i}^{\\prime}, \\pi_{-i}\\right)$.$\nBy construction, the maxima of p in \u03c0' are equal to the product set of the maxima of each u\u017c w.r.t.\n\u03c0\u03af individually. We observe that for (\u03c0, \u03c0') \u2208 \u03a0 \u00d7 II we have ((\u03c0\u03af, \u03c0\u2212i)) \u2208 \u03a0, i = 1, . . ., n, so that"}, {"title": "B OCCUPANCY FROM POLICY IS DIFFERENTIABLE", "content": "Lemma 3. Player i's state-action occupancy measure \u00b5i is a differentiable (and hence continuous)\nfunction of the player policies \u03c0.\nProof. Recall\n$\\mu_{i}(\\pi)=(1-\\gamma)\\left([I-\\gamma P^{\\pi}]^{-1} \\mu_{0}\\right)$,$\nwhere\n$P^{\\pi}\\left(s^{\\prime}, s\\right)=\\left(P^{-j}\\left(s^{\\prime}, s,:\\right), \\pi_{j}(s,:)\\right)$\nand\n$P^{-j}\\left(s^{\\prime}, s,:\\right)=\\sum_{a^{-j}} P\\left(s^{\\prime} | s, a_{i}, a_{-j}\\right) \\prod_{k \\neq j} \\pi_{k}\\left(s, a_{k}\\right)$\nSO\n$P^{\\pi}\\left(s^{\\prime}, s\\right)=\\sum_{a} P\\left(s^{\\prime} | s, a\\right) \\prod_{j} \\pi_{j}\\left(s, a_{j}\\right)$.$\nThen,\n$\\frac{\\partial \\mu_{i}(x, y)}{\\partial \\pi_{i}(x^{\\prime}, y^{\\prime})}=(1-\\gamma)\\left[\\frac{\\partial}{\\partial \\pi_{i}(x^{\\prime}, y^{\\prime})}[I-\\gamma P^{\\pi}]^{-1} \\mu_{0}(s)\\right] \\mathbb{I}(x=x^{\\prime}, y=y^{\\prime}, j=i)$\n$+(1-\\gamma)\\left[\\pi_{j}(x, y) \\frac{\\partial}{\\partial \\pi_{i}(x^{\\prime}, y^{\\prime})}\\left([I-\\gamma P^{\\pi}]^{-1}\\right) \\left(\\mu_{0}\\right)\\right],$\nwhere\n$\\frac{\\partial}{\\partial \\pi_{j}(x^{\\prime}, y^{\\prime})}\\left([I-\\gamma P^{\\pi}]^{-1}\\right)=\\gamma[I-\\gamma P^{\\pi}]^{-1} \\frac{\\partial P^{\\pi}}{\\partial \\pi_{j}(x^{\\prime}, y^{\\prime})}[I-\\gamma P^{\\pi}]^{-1}$\nand\n$\\frac{\\partial P^{\\pi}}{\\partial \\pi_{j}(x^{\\prime}, y^{\\prime})}=\\left\\{\\begin{array}{cc}0 & \\text { if } x \\neq x^{\\prime} \\\n\\frac{\\partial P^{-j}\\left(s^{\\prime}, x, y\\right)}{\\partial \\pi_{j}\\left(x^{\\prime}, y^{\\prime}\\right)} & \\text { else. }\\end{array}\\right.$\nClearly, this requires inverting the matrix [I \u2013 \u04afP"}, {"title": "C CONTINUITY OF ACTION CORRESPONDENCE", "content": "It is known that the Bellman-flow constraints in Program (3) additionally encode a simplex constraint\non \u03bci (see this by summing the constraints over s). Note that the Bellman flow constraints in (3)\nencode a hyperplane. Therefore the feasible set is the intersection of a hyperplane with the simplex,\nresulting in a convex polytope. It is known that there always exists a stationary distribution \u03bc\u03b5\nsatisfying these constraints. Note that the hyperplane has fixed dimension (dh = |S||Ai| - |S|)\nbecause the constraint matrix has full row-rank |S| (proven in Lemma 1 below).\nRecall player i's transition matrix given fixed policies \u03c0\u2212i is:\n$P^{\\pi}\\left(s^{\\prime} | s, a_{i}\\right)=\\sum_{a_{-i} \\in A_{-i}} P\\left(s^{\\prime} | s, a_{i}, a_{-i}\\right) \\prod_{j \\neq i} \\pi_{j}\\left(a_{j} | s\\right)$\nwhere aj is player j's action component of ai. Note this function is polynomial in \u3160_\u00bf and there-\nfore continuous in \u03c0\u03b1\u03af.\nNote that the constraint matrix depends on the transition operator PT-\u00b2. Therefore, the orientation\nand translation of the hyperplane depend continuously on the entries in Pt. As the hyperplane\ncontinuously moves, the boundary of the convex polytope induced by its intersection with the sim-\nplex also moves continuously.\nIn order for the policies to depend continuously on the other player occupancy measures, we need to\n$\\frac{\\mu_{i}(s, a)}{\\sum_{a^{\\prime}} \\mu_{i}\\left(s, a^{\\prime}\\right)}$\nensure that the occupancies always have full support on all states. Recall that $\\pi_{i}(a | s)=\\left\\{\\begin{array}{ll}\\frac{\\mu_{i}(s, a)}{\\sum_{a^{\\prime}} \\mu_{i}\\left(s, a^{\\prime}\\right)} & \\text { if } \\sum_{a^{\\prime}} \\mu_{i}\\left(s, a^{\\prime}\\right)>0 \\\\\n1 /|A_{i}| & \\text { else }\\end{array}\\right..$ For state-actions converging to zero occupancy measure, this would\ninduce discontinuous changes in their induced policies which would induce discontinuities in the\ntransition kernel for other agents. We ensure occupancies have full support by assuming \u03bc\u03bf(s) has\nfull support.\nGiven the above intuition, we now rigorously prove continuity of the action correspondence, begin-\nning with proving the Bellman flow constraints are all linearly independent (which guarantees its\nassociated hyperplane has fixed dimension).\nLemma 1. The Bellman flow constraint matrix has full row-rank (S) and is fixed independent of\nother player policies.\nProof. Note the Bellman flow constraints can be written in matrix form as\n$\\sum_{\\alpha \\in A}\\left(I_{s^{\\prime} \\times s}-\\gamma P_{i, \\alpha}\\right) \\mu_{i, \\alpha}=(1-\\gamma) \\mu_{0},$\nwhere P\u2081 = P(s'|s, a) denotes the S\u00d7S \u00d7 A\u2081 tensor of transition probabilities and Pi,a = Pi,a(s's)\nselects out a single action, leaving an S \u00d7 S matrix.\nThis constraint can be written without the \u03a3\u03b1\u2208, by constructing the rectangular block matrices\n$I_{s^{\\prime} \\times(s a)}=\\left[I_{s^{\\prime} \\times s} \\ldots I_{s^{\\prime} \\times s}\\right]$\nand\n$P_{i, s^{\\prime} \\times(s a)}=\\left[P_{i, a_{1}} \\ldots P_{i, a_{m}}\\right].$\nThen\n$\\left(I_{s^{\\prime} \\times(s a)}-P_{i, s^{\\prime} \\times(s a)}\\right) \\mu=(1-\\gamma) \\mu_{0}.$\nWe can examine the first s' \u00d7 s block of (Is'x(sa) - Pi,s'x(sa)) and show that this matrix is full\nrank, i.e., of rank |S|. If this matrix is full rank, then its rows are linearly independent. Extending our\nview to the full matrix, i.e., all columns, cannot render any of these original rows linearly dependent.\nNote that the first block is represented by (Is'xs \u2013 Pi,a) for some action a. Using Gershgorin's\ncircle theorem, we can bound the eigenvalues of this matrix to lie in a union of circles which all"}, {"title": "D KKT CONDITIONS IMPLY FIXED POINT SUFFICIENCY", "content": "Consider the following constrained optimization problem:\n$\\begin{array"}, {"2004)": "n\u2022 Stationarity: 0 \u2208 df(x*) \u2013 \u03a3; djdhj(x*) \u2013 \u03a3i pidgi(xX*)\n\u2022 Primal feasibility: hj(x*) = 0 for all j and gi(x*) < 0 for all i\n\u2022 Dual feasibility: \u03bc\u03b5 \u2265 0 for all i\n\u2022 Complementary slackness: pigi(x*) = 0 for all i.\nLemma 6. Assuming player k's utility", "x_{-k}\\right)": "n$\\begin{array"}, {"following": "n\u2022 Stationarity: 0 \u2208 \u03b8\u03c5\u03ba(z*", "feasibility": "Ajz = bj for all j\n\u2022 Dual feasibility: \u03bc\u03b5 \u2265 0 for all k\n\u2022 Complementary slackness: -\u03bc\u03b5\u03b6 = 0 for all k\nwhere duk (z", "title": "E ENTROPY REGULARIZED LOSS BOUNDS EXPLOITABILITY", "content": "Our derived exploitability bound only requires concavity of the utility, bounded diameter of the\nfeasible set, and linearity of feasible constraints (i.e., feasible set is subset of hyperplane). In what\nfollow, let player k's loss be the negative of their utility, i.e., lk = -uk.\nLemma 7. The amount a player can gain by deviating is upper bounded by a quantity proportional\nto the norm of the projected-gradient:\n$\\epsilon_{k}(\\mu) \\leq \\sqrt{2}\\left\\|\\Pi_{T M_{k}}\\left(\\nabla_{u_{k}}^{\\mu_{k}}\\right)\\right\\| .$\nProof. Let z be any point in the feasible set. First note that ITM. (z) = Bz where B is an orthog-\nonal projection matrix; this implies B2 = B = BT. Then by convexity of lk with respect to z,\n$\\begin{aligned}l_{k}(\\mu)-l_{k}(z, \\mu_{-k}) & \\leq \\left(\\nabla_{u_{k}}^{\\mu_{k}}\\right)^{\\top}(z-\\mu_{k}) \\\n& =\\left(\\nabla_{u_{k}}^{\\mu_{k}}\\right)^{\\top} \\Pi_{T M_{k}}\\left(z-\\mu_{k}\\right) \\\n& =\\left(\\Pi_{T M_{k}}\\left(\\nabla_{u_{k}}^{\\mu_{k}}\\right)\\right)^{\\top} \\underbrace{\\left(z-\\mu_{k}\\right)} \\\nB z=B^{\\top} z & < \\sqrt{2}\\left\\|\\Pi_{T M_{k}}\\left(\\nabla_{u_{k}}^{\\mu_{k}}\\right)\\right\\| .\\end{aligned}$\nwhere the first equality follows from the fact any two points z and \u03bc\u03b5 lying in the same hyperplane,\nby definition, form a direction lying in the tangent space of the hyperplane. The second equality\nfollows from symmetry of the projection operator and simply grouping its application to the left\nhand term; we also note that the feasible set is a subset of the simplex, which has a diameter of \u221a2.\nFinally, the last step follows from Cauchy-Schwarz.\n$\\begin{aligned}& \\quad \\text { M } \\\\\nl_{k}=\\tau\\left[\\frac{\\log (|S||A|)}{2} \\right].\\end{aligned}$\nTheorem 2 (Low Temperature Approximate Equilibria are Approximate Nash Equilibria). Let \u2207k\nbe player k's entropy regularized gradient and \u00b5 be an approximate equilibrium of the entropy\nregularized game. Then it holds that\n$\\begin{aligned}& \\quad \\text { M } \\\\\nl_{k}=\\tau\\left[\\frac{\\log (|S||A|)}{2} \\right].\\end{aligned}$"}, {"title": "F GAME MATRICES", "content": "We provide the payoffs for the prisoner's dilemma and Bach-Stravinsky game used in experiments."}, {"title": "GOBSTACLES TO A GENERAL CMG NE-EXISTENCE PROOF", "content": "Table 5 summarizes the obstacles to an NE-existence proof for general concave utilities."}, {"title": "G.1 NON-CONVEXITY OF BEST RESPONSE CORRESPONDENCES", "content": "We consider a simple 2-player", "I-III)": "n$\\pi_{2}\\left(a_{0} / s\\right)=\\left\\{\\begin{array}{ll}0.40 & \\text { if } s=s_{0} \\\n0.8"}]}