{"title": "ALS-HAR: Harnessing Wearable Ambient Light\nSensors to Enhance IMU-based HAR", "authors": ["Lala Shakti Swarup Ray", "Daniel Gei\u00dfler", "Mengxi Liu", "Bo Zhou", "Sungho\nSuh", "Paul Lukowicz"], "abstract": "Despite the widespread integration of ambient light sensors\n(ALS) in smart devices commonly used for screen brightness adapta-\ntion, their application in human activity recognition (HAR), primarily\nthrough body-worn ALS, is largely unexplored. In this work, we devel-\noped ALS-HAR, a robust wearable light-based motion activity classifier.\nAlthough ALS-HAR achieves comparable accuracy to other modalities,\nits natural sensitivity to external disturbances, such as changes in am-\nbient light, weather conditions, or indoor lighting, makes it challenging\nfor daily use. To address such drawbacks, we introduce strategies to en-\nhance environment-invariant IMU-based activity classifications through\naugmented multi-modal and contrastive classifications by transferring\nthe knowledge extracted from the ALS. Our experiments on a real-world\nactivity dataset for three different scenarios demonstrate that while ALS-\nHAR's accuracy strongly relies on external lighting conditions, cross-\nmodal information can still improve other HAR systems, such as IMU-\nbased classifiers. Even in scenarios where ALS performs insufficiently, the\nadditional knowledge enables improved accuracy and macro F1 score by\nup to 4.2% and 6.4%, respectively, for IMU-based classifiers and even\nsurpasses multi-modal sensor fusion models in two of our three experi-\nment scenarios. Our research highlights the untapped potential of ALS\nintegration in advancing sensor-based HAR technology, paving the way\nfor practical and efficient wearable ALS-based activity recognition sys-\ntems with potential applications in healthcare, sports monitoring, and\nsmart indoor environments.", "sections": [{"title": "1 Introduction", "content": "Sensor-based HAR has gained increasing interest in research and industry over\nthe past decade, advocating various sensor modalities like pressure sensors [33,22],\nEMG sensors [15,13], impedance sensors [12,7] and capacitive sensors [8]. Espe-\ncially with the ubiquity and availability of smart devices, embedded sensors like\ninertial measurement units (IMUs) [10,14,21] have gained popularity due to their\nability to capture motion-related data accurately through great information den-\nsity.\nHowever, despite the extensive exploration of IMUs in HAR, there is a grow-\ning trend towards investigating the potential of other embedded sensors like BLE\n[4,27], WiFi signals [32,30], temperature sensors [3] and ALS [29], which is pre-\nsented in this work. Nowadays, ALS is embedded in almost all portable smart\ndevices with a screen primarily used for adaptive screen brightness adjustments\nbased on changing environmental lighting conditions [6]. Such a sensor oper-\nates passively without direct user interaction and can be exploited to provide\nvaluable contextual information about the user's surroundings and activities.\nAdditionally, it consumes minimal power, contributing to energy-efficient imple-\nmentations, particularly on battery-powered devices. For this work, we aim to\nbenefit from the ubiquity of ALS in smart mobile devices, eliminating the need\nfor additional hardware through straightforward accessibility.\nTo the best of our knowledge, despite the promising advantages and avail-\nability, limited research has been done regarding exploring body-worn ALS in\nHAR as a motion-sensing modality. Throughout related Multi-modal sensor fu-\nsion works like [16,26,28], static ALS and other ambient sensors placed in the\nenvironment are used for positional understanding for motion localization.\nWearable, body-worn ALS holds promise for HAR, particularly in indoor en-\nvironments with stable lighting conditions where external factors affecting light\nintensity are minimal. Xu et al. have exploited the wearable ALS to generate\nIMU data for improving nursing-based HAR [29]. Similarly, Sadaghiani et al.\nused wearable photodiodes to gather blood pressure signals of the wearer's body\n[24]. Despite the promises, these models suffer from the obvious problem of being\nsensitive to external lighting conditions. When the changes in light conditions\nare more significant or comparable to those impacted by the user's movement,\nthe model performance drastically decreases, making them useless for such con-\nditions. Even further, just like the overall nature of vision sensors, the ALS can\nnot work in dark environments [29] as stated by Xu et al.\nIn this paper, we try to solve this problem by investigating different cross-\nmodal approaches that can empower other sensor-based modalities, like IMU-\nbased HAR, by using knowledge transfer techniques from ALS to IMU. There-\nfore, we aim to maximize the knowledge extracted from ALS even in unfavorable,\nfluctuating light conditions, improving HAR performance through ALS indepen-\ndently of environmental influences. In summary, the main contribution of this\nwork can be summarized as follows:\nMulti-modal HAR dataset: A novel multi-modal dataset containing nine dif-\nferent activities for a total of 5.28 hours performed by 16 participants along\nthree different environmental scenarios gathering right wrist IMU signal,\nright wrist ALS signal, video footage and SMPL pose synchronized together\nas visualized in Figure 1.\nLightHAR: An activity recognition model based only on the wrist-based\nALS with a detailed comparison to wrist-based IMU, 3D pose-based, and\nvideo-based activity recognition for the three different scenarios."}, {"title": "2 Related work", "content": "HAR is a continuously evolving field that leverages various sensing modalities\nto identify and monitor human activities. ALS has shown promise in enhancing\nthe accuracy and applicability of HAR systems, especially deployed as external\nenvironmental sensors through works like [16] presenting a Deep Convolutional\nNeural Network to recognize human activities using binary ambient sensors to\nidentify activities of daily living. In [1], the landscape of available sensors for\nHAR has been analyzed by Ahamed et al., investigating the importance of en-\nvironmental sensors, especially the ALS, to detect the early signs of dementia\nin residential care. Integrated into smart wallpapers, multi ALS has been im-\nplemented by Shi et al. to recognize human motions with an accuracy of 96%\nutilizing the information of light reflections gathered through photodiodes hidden\ninside the wallpaper [26]. Focusing on industrial scenarios and ambient assisted\nliving, Salem et al. have proven the feasibility of fusing the sensor data from\nIMU and ALS to achieve an activity recognition performance of 90% across a\nsmall set of three classes for each scenario [25]."}, {"title": "2.1 ALS-HAR", "content": "Environmental Sensing for HAR commonly possesses drawbacks on adapta-\ntion to changing light conditions and occlusion of the covered area, wherefore\nbody-worn ALS can be an alternative to enhance HAR performance [9]. Due to\ntheir simplicity in operation and low power consumption, they are commonly\ndeployed in consumer wearable devices [20]. In [11], the benefits of low-power\nALS have been deployed to harvest energy through photodiodes and simultane-\nously utilize the ambient light data for self-powered and robust finger gesture\ndetection. Similar work has been done through OptoSense, presenting a novel\napproach for developing body-worn ALS that is self-powered and capable of be-\ning integrated ubiquitously by leveraging photovoltaic cells both to power the\nsensors and to sense the ambient light, enabling the creation of energy-efficient\nHAR through light sensors [31]. Similar to the approach of this work, Wang et\nal. present a multimodal feature fusion model utilizing geomagnetic, ALS, and\naccelerometer data collected from smartphones to enhance health activity mon-\nitoring accuracy in indoor environments by 13.65% compared to classic sensor\nclassification [28].\nHowever, the presented literature barely investigates changing environments\nand lighting conditions, commonly working in clean and optimal indoor environ-\nments, restating the motivation of this work."}, {"title": "2.2 Knowledge Transfer in Sensor HAR", "content": "Methods like Don't Freeze [5], Virtual Fusion [17] and Contrastive Left-Right\nHAR[18] tried using IMU sensors at different positions to improve the overall\naccuracy of body-worn IMU at specific positions using contrastive learning. Ap-\nproaches like Multi\u00b3Net [23] have tried to improve sensor-based HAR accuracy\nusing other widely available modalities like 3D poses and text embedding. i-Move\n[12] improved IMU-based HAR using bio-impedance data through contrastive\nlearning.\nWe believe that the most important use case for ALS data would be empow-\nering other sensor modalities that are more environment invariant through data\ncollected in ideal environments, which we try to achieve through this work."}, {"title": "3 Data Collection", "content": "Our experiment encompassed three different scenarios based on different environ-\nmental and lighting conditions. It consisted of 16 participants doing 10 activities,\nincluding the Null class. The gender distribution was 5 females and 11 males,\nages 24 to 35, and weights ranged between 53 kg and 88 kg.\nScenario 1, consisting of subjects 1 to 10, was recorded in a controlled in-\ndoor environment with fixed lighting conditions. This environment is ideal for\nALS-HAR because of the minimal interference of change in light due to external\nfactors. Scenario 2, consisting of subjects 11 to 13, was recorded in a relatively\ndark indoor environment with dynamic architectural lights. Most interference\nin lighting conditions is introduced due to these external factors rather than\nthe motion of the subject itself, making it more challenging than the other two\nscenarios. Scenario 3, consisting of subjects 14 to 16, was recorded in an out-\ndoor environment during cloudy weather. The clouds and trees moved because\nwind created small interference in light signals, making it a practical dataset to\nshowcase the usability of ALS-HAR.\nParticipants are engaged in a series of predetermined activities, including\nsix distinct upper body fitness exercises boxing, biceps curls, chest press, shoul-\nder, and chest press, arm hold and shoulder press, and arm opener sourced\nfrom Pamela's fitness routines available on YouTube (3). Additionally, three\nsupplementary hand-focused tasks sweeping a table, Answering the telephone,\nand wearing a headset were included, each lasting approximately 20 minutes.\nWe used existing consumer-grade devices for data collection to showcase the\nutility of ALS signals without facing the bottleneck of the new sensor introduc-\ntion. We utilized a Samsung Galaxy S20 smartphone worn on top of the right\nwrist with a wristband facing outward, having the same relative position and\norientation to the wrist irrespective of the user. This allowed for the collection\nof both light sensor and IMU data to fulfill the experimental requirements. Data\nwas collected using the Sensor Logger Android application (4). Video recordings,\ncaptured using a back-facing camera of an iPhone SE, served as supplementary\ndata for annotation purposes.\nSensor Logger automatically synchronizes light and IMU sensor, ensuring a\nconsistent sampling rate of around 30 Hz throughout the session by taking a\ncommon time-stamp from the smartphone itself and matching the start and end\nof the session. The videos collected by a separate smartphone are synchronized\nmanually with the sensor data using a simple trick. At the start and end of\neach session, the subject needs to do the calibration movement, i.e., fold arms\nto touch both hands three times to make a unique pattern in the pose and the\nsensor signal. By mapping these unique patterns of the pose and the sensor\nsignal, we can synchronize both together. Afterward, the videos are manually\nannotated and can be used directly to annotate the sensor signals.\nTypical ALS available in smartphones uses a photodiode, a semiconductor\ndevice that generates an electrical current when exposed to light. The intensity\nof the current is proportional to the amount of light hitting the sensor. The light\nsignal, recorded in lux, is a unit of measurement for illuminance, representing\nthe amount of light per unit area. In this context, lux provided insights into the\nambient light conditions surrounding the experiment's environment, especially\nthe light reaching the right wrist based on the subject's movement."}, {"title": "4 Method", "content": "We developed a robust ALS-based activity classifier tailored for light sensor data\nusing a 1D bidirectional LSTM-based encoder architecture inspired by the Deep-"}, {"title": "4.1 LightHAR", "content": "ConvLSTM framework [19]. Our model processes input data X with dimensions\n(N, 1, 1), where N represents the sequence length with unit feature dimension\nand a single channel. The architecture outputs a probability distribution over 10\nclasses (9 + null), denoted as \u00dd.\nThe model begins with a series of three 1D convolutional layers followed by\nbatch-normalization and dropout layers, each designed for feature extraction.\nThese layers sequentially process the input data to capture relevant patterns\nand characteristics from the light sensor signals. Each convolutional block is fol-\nlowed by a ReLU activation function, which introduces non-linearity into the\nmodel. After feature extraction, the processed data is passed through a bidirec-\ntional LSTM layer to capture temporal dependencies in the sequence data. The\nbidirectional nature of the LSTM layers allows the model to consider both past\nand future information when making predictions, which enhances the overall\nperformance of the activity classifier. The output from the LSTM layers is then\ndirected through dense layers for classification. The final layer outputs a prob-\nability distribution of the 10 classes, enabling the model to determine the most\nlikely activity class for a given sequence of light sensor data. Our architecture\nfocuses on creating a lightweight model with robust and accurate classification\nbased on light sensor inputs.\nTo train the model, we used the cross-entropy loss function, defined as:\n$L_{CE} = \\frac{1}{NC} -\\sum_{i=1}^{N}\\sum_{c=1}^{C} Y_{i,c} \\log(\\hat{Y}_{i,c})$ (1)"}, {"title": "4.2 Light embedded InertialHAR:", "content": "We have designed different strategies for leveraging the knowledge from the ALS\nmodality to enhance the activity recognition accuracy of the IMU modality. As\ndetailed in the section 5.2, ALS, due to its high sensitivity to external light con-\nditions, is susceptible to environmental noise, especially during significant light\nchanges. In contrast, the accelerometer from IMU is known for its environmental\nrobustness and stability. We've developed a variety of strategies that leverage\nthe unique features of both ALS and IMU sensors. These strategies enable us\nto build a model that only requires the IMU modality during evaluation, effec-\ntively mitigating the impact of environmental noise on ALS. This approach is\nparticularly useful in practical scenarios with substantial light fluctuations.\nMultiLight InertialHAR We designed MultiLight InertialHAR by taking in-\nspiration from classic sensor-fusion models that use more than one modality to\nimprove overall HAR accuracy compared to either unimodal system. The model\ncontains two encoders: An ALS encoder partially similar to the LightHAR used\nfor activity classification where the full-connected layers are replaced to generate\na dense feature vector of size 256. The IMU encoder also contains a series of 3 1D\nCNN blocks followed by a bidirectional LSTM and a fully connected layer to gen-\nerate a dense feature vector of size 256. The extracted features are concatenated\nafterward and given to a simple classifier consisting of two fully connected layers\nto map the intermediate features to an activity class as visualized in Figure 3.\nLike the LightHAR model, cross-entropy loss was used to train the model.\nMultiLight IneritalHAR processes input ALS data (N, 1, 1), and input accelerom-\neter data (N, 1, 3), where N represents the sequence length, 1 is the feature di-\nmension, and 3 is total channels(x, y, z) to output one of the 10 (9+Null) classes."}, {"title": "ContraLight InertialHAR", "content": "Inspired by other multi-modal contrastive learn-\ning models, We devised another unique strategy where both light and inertial\nsensor data are used during the training phase, but only inertial sensor data is\nused during the evaluation phase by utilizing contrastive learning to train this\nmodel. The ALS encoder and IMU encoder, identical to those used in MultiLight\nInertialHAR, extract two feature vectors of size 256. Contrastive loss is then ap-\nplied to both embeddings based on the original classes to bring representations\nfrom the same classes closer together.\nThe contrastive loss $L_{co}$ is defined as:\n$L_{co} = \\sum_{i,j} Y_{ij} \\max(0, m - || z_i \u2013 z_j||_2) + (1 \u2212 Y_{ij}) \\cdot ||z_i \u2013 z_j||_2$ (2)\nwhere $z_i$ and $z_j$ are the feature vectors, $Y_{ij}$ is a binary label indicating whether\n$z_i$ and $z_j$ are from the same class, and m is a margin parameter. Two instances of\nthe same fully connected classifier are utilized with shared weights as visualized\nin Figure 4. The overall total loss $L_{total}$ is then calculated by summing the\ncontrastive loss and the two cross-entropy losses."}, {"title": "5 Evaluation", "content": "To train the models with the collected data, instances were generated using a\nsliding window technique with a size of 60 (2 sec) and a step of 15 samples (0.5\nsec) for both ALS and IMU sensor data. The video and extracted pose, which\nhave 24 frames per second (FPS), are first interpolated to make it 30FPS and\nafterward sliced accordingly to generate a window of size 60 and a step of 15\nframes."}, {"title": "5.1 Training details", "content": "As we can see in Table 3, for all 3 test sets, both MultiLight InertialHAR and\nContraLight InertialHAR outperformed the baseline InertialHAR although re-\nquiring the same input (60, 1, 3) accelerometer data during inference phase. The\nSensor Fusion model that is identical to MultiLight InertialHAR but requires\nboth (60,1,3) IMU input and (60,1,1) ALS input outperforms all of them in\ntest set 1 having ideal lighting conditions, but its performance gets even worse\nthan the baseline InterialHAR for test set 2 where the lighting conditions are\nchallenging it doesn't provide any improvements for test set 3 either. Regarding\ninference time, the baseline InertialHAR, having the lowest number of FLOP,\nperforms faster than all other models. The MultiLight InertialHAR requires an\nadditional dummy ALS input during inference, which has much higher number\nof FLOP and is slower than baseline InertialHAR despite being more accurate.\nThe ContraLight InertialHAR, while not surpassing the baseline, demonstrates\na very similar number of FLOP and performs on par with the baseline in terms\nof speed. This efficiency, combined with its superior accuracy and F1 score com-\npared to both the baseline InertialHAR and MultiLight InertialHAR, makes it\na competitive model."}, {"title": "5.2 Unimodal Results", "content": "To test the effectiveness of ALS as an activity classification modality, we trained\nother widely used temporal modalities, such as IMU, Pose, and Video, for ac-\ntivity recognition using the same dataset we collected before. All modalities\ninterpolated to have the same sampling frequency and step size. To make them\ncomparable, we made the neural network architecture identical to LightHAR for\nall other modalities except the input size. Each model was designed with three\nCNN blocks for feature extraction, a bi-directional LSTM, and two fully con-\nnected layers for activity classification, mirroring the structure of LightHAR. As\nstated in section 4.1, the input of the LightHar is the ALS signal of size (60, 1, 1)\nwhile the input for InertialHAR is the IMU signal of size (60,1,3). In contrast,\nthe input for the PoseHAR is the extracted SMPL pose from videos using Mo-\ntionBERT of size (60, 22, 3) with 22 joints, and the input for the VideoHAR is the\nextracted intermediate video features using Video Vision Transformer (ViViT)\n[2] of size (1,3137, 768)."}, {"title": "5.3 Cross-modal Results", "content": "As discussed in the section 5.2, if we consider all scenarios, InterialHAR is\nmore accurate, more reliable/stable, and has a comparable inference time to\nLightHAR, making it a better choice for activity recognition.\nWe developed strategies like MultiLight InertialHAR, which takes (60, 1, 3)\nIMU input and a dummy array of (60,1,1) for activity classification. In this\nstrategy, both IMU and light modality from ideal conditions (fixed light indoor)\nare used for training the model. Similarly, ContraLight InertialHAR takes only\n(60,1,3) IMU input for activity classification, but both IMU and light modality\nfrom ideal conditions (fixed light indoor) are used for training the model. The\nsame metrics from before are used to compare all the models.\nAs we can see in Table 3, for all 3 test sets, both MultiLight InertialHAR and\nContraLight InertialHAR outperformed the baseline InertialHAR although re-\nquiring the same input (60, 1, 3) accelerometer data during inference phase. The\nSensor Fusion model that is identical to MultiLight InertialHAR but requires\nboth (60,1,3) IMU input and (60,1,1) ALS input outperforms all of them in\ntest set 1 having ideal lighting conditions, but its performance gets even worse\nthan the baseline InterialHAR for test set 2 where the lighting conditions are\nchallenging it doesn't provide any improvements for test set 3 either. Regarding\ninference time, the baseline InertialHAR, having the lowest number of FLOP,"}, {"title": "5.4 Discussions", "content": "As stated in section 5.2, despite having decent inference time and comparable\nresults compared to other sensor modalities like IMUs, ALS can not be used as a\nuniversal Unimodal-HAR. Despite its limited working environments, ALS would\nmake a very good modality for specific use cases in smart indoor environments.\nFor example, hospitals or Care Homes have comparatively stable lighting, and\nthe fast inference, passive sensing, and low-power use of ALS make them suitable\nfor this job.\nAlso, because of their wide availability in smartphones and smartwatches,\nthey can be used for simple yet repetitive tasks like step counting or other types\nof fitness activity counters, along with IMU modality through sensor fusion.\nAs discussed in section 5.3, A large amount of multi-modal activity data\ncan be collected in ideal conditions to enhance other sensor modalities like IMU\nthrough our knowledge transfer strategies."}, {"title": "5.5 Limitations", "content": "In our current study, we exclusively utilized the Galaxy S20 to collect all data,\nlimiting our insights to a single device's performance. Testing a different device\ntype other than the one used to collect the training data could provide valuable\ninsights into cross-device ALS-HAR reliability, particularly in scenarios where\nother sensor-based modalities like IMU lag behind. Additionally, employing more\nthan one ALS sensor at different parts of the body could potentially enhance\noverall accuracy. This approach may provide more robustness against varying\nlight conditions and outdoor environments, a direction we plan to explore in\nfuture research to improve the robustness and reliability of our findings."}, {"title": "6 Conclusion", "content": "In summary, our study delves into the realm of wearable ALS for HAR, showcas-\ning its potential in understanding human motions. We developed LightHAR, a\nnovel approach utilizing wrist-based ambient light signals for HAR, tested it for\ndifferent scenarios, and compared it with other commonly used modalities for\nHAR. By integrating ALS with IMU through sensor fusion and contrastive classi-\nfication, we enhanced the accuracy of InertialHAR systems. Our light-embedded\nInertialHAR approach, which relies solely on inertial data during inference, ex-\nhibited notable improvements in accuracy compared to traditional IMU-based\nclassifiers.\nAlthough our study is promising, it is essential to acknowledge its limitations,\nand further research is warranted to validate our findings across devices and\nexplore practical applications of ambient light-enhanced HAR systems."}]}