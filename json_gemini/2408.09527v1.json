{"title": "ALS-HAR: Harnessing Wearable Ambient Light Sensors to Enhance IMU-based HAR", "authors": ["Lala Shakti Swarup Ray", "Daniel Gei\u00dfler", "Mengxi Liu", "Bo Zhou", "Sungho Suh", "Paul Lukowicz"], "abstract": "Despite the widespread integration of ambient light sensors (ALS) in smart devices commonly used for screen brightness adaptation, their application in human activity recognition (HAR), primarily through body-worn ALS, is largely unexplored. In this work, we developed ALS-HAR, a robust wearable light-based motion activity classifier. Although ALS-HAR achieves comparable accuracy to other modalities, its natural sensitivity to external disturbances, such as changes in ambient light, weather conditions, or indoor lighting, makes it challenging for daily use. To address such drawbacks, we introduce strategies to enhance environment-invariant IMU-based activity classifications through augmented multi-modal and contrastive classifications by transferring the knowledge extracted from the ALS. Our experiments on a real-world activity dataset for three different scenarios demonstrate that while ALS-HAR's accuracy strongly relies on external lighting conditions, cross-modal information can still improve other HAR systems, such as IMU-based classifiers. Even in scenarios where ALS performs insufficiently, the additional knowledge enables improved accuracy and macro F1 score by up to 4.2% and 6.4%, respectively, for IMU-based classifiers and even surpasses multi-modal sensor fusion models in two of our three experiment scenarios. Our research highlights the untapped potential of ALS integration in advancing sensor-based HAR technology, paving the way for practical and efficient wearable ALS-based activity recognition systems with potential applications in healthcare, sports monitoring, and smart indoor environments.", "sections": [{"title": "1 Introduction", "content": "Sensor-based HAR has gained increasing interest in research and industry over the past decade, advocating various sensor modalities like pressure sensors [33,22], EMG sensors [15,13], impedance sensors [12,7] and capacitive sensors [8]. Especially with the ubiquity and availability of smart devices, embedded sensors like inertial measurement units (IMUs) [10,14,21] have gained popularity due to their ability to capture motion-related data accurately through great information density.\nHowever, despite the extensive exploration of IMUs in HAR, there is a growing trend towards investigating the potential of other embedded sensors like BLE [4,27], WiFi signals [32,30], temperature sensors [3] and ALS [29], which is presented in this work. Nowadays, ALS is embedded in almost all portable smart devices with a screen primarily used for adaptive screen brightness adjustments based on changing environmental lighting conditions [6]. Such a sensor operates passively without direct user interaction and can be exploited to provide valuable contextual information about the user's surroundings and activities. Additionally, it consumes minimal power, contributing to energy-efficient implementations, particularly on battery-powered devices. For this work, we aim to benefit from the ubiquity of ALS in smart mobile devices, eliminating the need for additional hardware through straightforward accessibility.\nTo the best of our knowledge, despite the promising advantages and availability, limited research has been done regarding exploring body-worn ALS in HAR as a motion-sensing modality. Throughout related Multi-modal sensor fusion works like [16,26,28], static ALS and other ambient sensors placed in the environment are used for positional understanding for motion localization.\nWearable, body-worn ALS holds promise for HAR, particularly in indoor environments with stable lighting conditions where external factors affecting light intensity are minimal. Xu et al. have exploited the wearable ALS to generate IMU data for improving nursing-based HAR [29]. Similarly, Sadaghiani et al. used wearable photodiodes to gather blood pressure signals of the wearer's body [24]. Despite the promises, these models suffer from the obvious problem of being sensitive to external lighting conditions. When the changes in light conditions are more significant or comparable to those impacted by the user's movement, the model performance drastically decreases, making them useless for such conditions. Even further, just like the overall nature of vision sensors, the ALS can not work in dark environments [29] as stated by Xu et al.\nIn this paper, we try to solve this problem by investigating different cross-modal approaches that can empower other sensor-based modalities, like IMU-based HAR, by using knowledge transfer techniques from ALS to IMU. Therefore, we aim to maximize the knowledge extracted from ALS even in unfavorable, fluctuating light conditions, improving HAR performance through ALS independently of environmental influences. In summary, the main contribution of this work can be summarized as follows:\nMulti-modal HAR dataset: A novel multi-modal dataset containing nine different activities for a total of 5.28 hours performed by 16 participants along three different environmental scenarios gathering right wrist IMU signal, right wrist ALS signal, video footage and SMPL pose synchronized together as visualized in Figure 1.\nLightHAR: An activity recognition model based only on the wrist-based ALS with a detailed comparison to wrist-based IMU, 3D pose-based, and video-based activity recognition for the three different scenarios."}, {"title": "2 Related work", "content": "HAR is a continuously evolving field that leverages various sensing modalities to identify and monitor human activities. ALS has shown promise in enhancing the accuracy and applicability of HAR systems, especially deployed as external environmental sensors through works like [16] presenting a Deep Convolutional Neural Network to recognize human activities using binary ambient sensors to identify activities of daily living. In [1], the landscape of available sensors for HAR has been analyzed by Ahamed et al., investigating the importance of environmental sensors, especially the ALS, to detect the early signs of dementia in residential care. Integrated into smart wallpapers, multi ALS has been implemented by Shi et al. to recognize human motions with an accuracy of 96% utilizing the information of light reflections gathered through photodiodes hidden inside the wallpaper [26]. Focusing on industrial scenarios and ambient assisted living, Salem et al. have proven the feasibility of fusing the sensor data from IMU and ALS to achieve an activity recognition performance of 90% across a small set of three classes for each scenario [25]."}, {"title": "2.1 ALS-HAR", "content": "Environmental Sensing for HAR commonly possesses drawbacks on adaptation to changing light conditions and occlusion of the covered area, wherefore body-worn ALS can be an alternative to enhance HAR performance [9]. Due to their simplicity in operation and low power consumption, they are commonly deployed in consumer wearable devices [20]. In [11], the benefits of low-power ALS have been deployed to harvest energy through photodiodes and simultaneously utilize the ambient light data for self-powered and robust finger gesture detection. Similar work has been done through OptoSense, presenting a novel approach for developing body-worn ALS that is self-powered and capable of being integrated ubiquitously by leveraging photovoltaic cells both to power the sensors and to sense the ambient light, enabling the creation of energy-efficient HAR through light sensors [31]. Similar to the approach of this work, Wang et al. present a multimodal feature fusion model utilizing geomagnetic, ALS, and accelerometer data collected from smartphones to enhance health activity monitoring accuracy in indoor environments by 13.65% compared to classic sensor classification [28].\nHowever, the presented literature barely investigates changing environments and lighting conditions, commonly working in clean and optimal indoor environments, restating the motivation of this work."}, {"title": "2.2 Knowledge Transfer in Sensor HAR", "content": "Methods like Don't Freeze [5], Virtual Fusion [17] and Contrastive Left-Right HAR[18] tried using IMU sensors at different positions to improve the overall accuracy of body-worn IMU at specific positions using contrastive learning. Approaches like Multi\u00b3Net [23] have tried to improve sensor-based HAR accuracy using other widely available modalities like 3D poses and text embedding. i-Move [12] improved IMU-based HAR using bio-impedance data through contrastive learning.\nWe believe that the most important use case for ALS data would be empowering other sensor modalities that are more environment invariant through data collected in ideal environments, which we try to achieve through this work."}, {"title": "3 Data Collection", "content": "Our experiment encompassed three different scenarios based on different environmental and lighting conditions. It consisted of 16 participants doing 10 activities, including the Null class. The gender distribution was 5 females and 11 males, ages 24 to 35, and weights ranged between 53 kg and 88 kg.\nScenario 1, consisting of subjects 1 to 10, was recorded in a controlled indoor environment with fixed lighting conditions. This environment is ideal for ALS-HAR because of the minimal interference of change in light due to external factors. Scenario 2, consisting of subjects 11 to 13, was recorded in a relatively dark indoor environment with dynamic architectural lights. Most interference in lighting conditions is introduced due to these external factors rather than the motion of the subject itself, making it more challenging than the other two scenarios. Scenario 3, consisting of subjects 14 to 16, was recorded in an outdoor environment during cloudy weather. The clouds and trees moved because wind created small interference in light signals, making it a practical dataset to showcase the usability of ALS-HAR.\nParticipants are engaged in a series of predetermined activities, including six distinct upper body fitness exercises boxing, biceps curls, chest press, shoulder, and chest press, arm hold and shoulder press, and arm opener sourced from Pamela's fitness routines available on YouTube (3). Additionally, three supplementary hand-focused tasks sweeping a table, Answering the telephone, and wearing a headset were included, each lasting approximately 20 minutes.\nWe used existing consumer-grade devices for data collection to showcase the utility of ALS signals without facing the bottleneck of the new sensor introduction. We utilized a Samsung Galaxy S20 smartphone worn on top of the right wrist with a wristband facing outward, having the same relative position and orientation to the wrist irrespective of the user. This allowed for the collection of both light sensor and IMU data to fulfill the experimental requirements. Data was collected using the Sensor Logger Android application (4). Video recordings, captured using a back-facing camera of an iPhone SE, served as supplementary data for annotation purposes.\nSensor Logger automatically synchronizes light and IMU sensor, ensuring a consistent sampling rate of around 30 Hz throughout the session by taking a common time-stamp from the smartphone itself and matching the start and end of the session. The videos collected by a separate smartphone are synchronized manually with the sensor data using a simple trick. At the start and end of each session, the subject needs to do the calibration movement, i.e., fold arms to touch both hands three times to make a unique pattern in the pose and the sensor signal. By mapping these unique patterns of the pose and the sensor signal, we can synchronize both together. Afterward, the videos are manually annotated and can be used directly to annotate the sensor signals.\nTypical ALS available in smartphones uses a photodiode, a semiconductor device that generates an electrical current when exposed to light. The intensity of the current is proportional to the amount of light hitting the sensor. The light signal, recorded in lux, is a unit of measurement for illuminance, representing the amount of light per unit area. In this context, lux provided insights into the ambient light conditions surrounding the experiment's environment, especially the light reaching the right wrist based on the subject's movement."}, {"title": "4 Method", "content": "We developed a robust ALS-based activity classifier tailored for light sensor data using a 1D bidirectional LSTM-based encoder architecture inspired by the Deep-"}, {"title": "4.1 LightHAR", "content": "ConvLSTM framework [19]. Our model processes input data X with dimensions (N, 1, 1), where N represents the sequence length with unit feature dimension and a single channel. The architecture outputs a probability distribution over 10 classes (9 + null), denoted as \u00dd.\nThe model begins with a series of three 1D convolutional layers followed by batch-normalization and dropout layers, each designed for feature extraction. These layers sequentially process the input data to capture relevant patterns and characteristics from the light sensor signals. Each convolutional block is followed by a ReLU activation function, which introduces non-linearity into the model. After feature extraction, the processed data is passed through a bidirectional LSTM layer to capture temporal dependencies in the sequence data. The bidirectional nature of the LSTM layers allows the model to consider both past and future information when making predictions, which enhances the overall performance of the activity classifier. The output from the LSTM layers is then directed through dense layers for classification. The final layer outputs a probability distribution of the 10 classes, enabling the model to determine the most likely activity class for a given sequence of light sensor data. Our architecture focuses on creating a lightweight model with robust and accurate classification based on light sensor inputs.\nTo train the model, we used the cross-entropy loss function, defined as:\n$\\begin{equation}L_{CE}= -\\frac{1}{NC}\\sum_{i=1}^{N}\\sum_{c=1}^{C} Y_{i,c} \\log(\\hat{Y}_{i,c})\\tag{1}\\end{equation}$"}, {"title": "4.2 Light embedded InertialHAR:", "content": "We have designed different strategies for leveraging the knowledge from the ALS modality to enhance the activity recognition accuracy of the IMU modality. As detailed in the section 5.2, ALS, due to its high sensitivity to external light conditions, is susceptible to environmental noise, especially during significant light changes. In contrast, the accelerometer from IMU is known for its environmental robustness and stability. We've developed a variety of strategies that leverage the unique features of both ALS and IMU sensors. These strategies enable us to build a model that only requires the IMU modality during evaluation, effectively mitigating the impact of environmental noise on ALS. This approach is particularly useful in practical scenarios with substantial light fluctuations.\nWe designed MultiLight InertialHAR by taking inspiration from classic sensor-fusion models that use more than one modality to improve overall HAR accuracy compared to either unimodal system. The model contains two encoders: An ALS encoder partially similar to the LightHAR used for activity classification where the full-connected layers are replaced to generate a dense feature vector of size 256. The IMU encoder also contains a series of 3 1D CNN blocks followed by a bidirectional LSTM and a fully connected layer to generate a dense feature vector of size 256. The extracted features are concatenated afterward and given to a simple classifier consisting of two fully connected layers to map the intermediate features to an activity class as visualized in Figure 3.\nLike the LightHAR model, cross-entropy loss was used to train the model. MultiLight IneritalHAR processes input ALS data (N, 1, 1), and input accelerometer data (N, 1, 3), where N represents the sequence length, 1 is the feature dimension, and 3 is total channels(x, y, z) to output one of the 10 (9+Null) classes."}, {"title": "ContraLight InertialHAR", "content": "Inspired by other multi-modal contrastive learning models, We devised another unique strategy where both light and inertial sensor data are used during the training phase, but only inertial sensor data is used during the evaluation phase by utilizing contrastive learning to train this model. The ALS encoder and IMU encoder, identical to those used in MultiLight InertialHAR, extract two feature vectors of size 256. Contrastive loss is then applied to both embeddings based on the original classes to bring representations from the same classes closer together.\nThe contrastive loss Lco is defined as:\n$\\begin{equation}L_{co} = \\sum_{i,j} Y_{ij} \\max(0, m \u2212 || z_i \u2013 z_j||_2) + (1 \u2212 Y_{ij}) \u00b7 ||z_i \u2013 z_j||_2\\tag{2}\\end{equation}$\nwhere zi and zj are the feature vectors, Yij is a binary label indicating whether zi and zj are from the same class, and m is a margin parameter. Two instances of the same fully connected classifier are utilized with shared weights as visualized in Figure 4. The overall total loss Ltotal is then calculated by summing the contrastive loss and the two cross-entropy losses."}, {"title": "ContraLight InertialHAR", "content": "$\\begin{equation}L_{total} = L_{co} + L_{CE-light} + L_{CE-IMU}\\tag{3}\\end{equation}$\nWe used contrastive loss instead of InfoNCE loss as this is a supervised problem. Therefore, we can directly use the labels as individual clusters instead of the self-supervised clustering task, which is useful in cases where the target activities are different from the source activities.\nThis approach ensures that during training, the model leverages both (N, 1, 1) ALS and (N, 1, 3) IMU sensor data to learn robust feature representations. However, during inference, we can simply discard the ALS encoder and use the more stable (N, 1, 3) inertial sensor data as input to predict the activity class. Unlike the MultiLight InertialHAR, we do not need to pass a dummy ALS input, making the model even smaller and more simplified without any significant trade-off."}, {"title": "5 Evaluation", "content": null}, {"title": "5.1 Training details", "content": "To train the models with the collected data, instances were generated using a sliding window technique with a size of 60 (2 sec) and a step of 15 samples (0.5 sec) for both ALS and IMU sensor data. The video and extracted pose, which have 24 frames per second (FPS), are first interpolated to make it 30FPS and afterward sliced accordingly to generate a window of size 60 and a step of 15 frames.\nAll models are trained using a Nvidia A6000 Ada Lovelace GPU and a Ryzen 5900 processor. Subjects 1 to 7 constituted the training set, while subjects 8 to 10 formed the test set 1 for ideal light conditions. Subjects 11 to 13 formed test set 2 for challenging indoor lighting conditions, and subjects 14 to 16 formed test set 3 for outdoor conditions. Training and validation data were randomly split with a 9:1 ratio during the training process.\nThe ADAM optimizer, along with a constant learning rate of 0.001, is used to train the model. The models are trained for 300 epochs, and early stopping with a patience of 10 is employed."}, {"title": "5.2 Unimodal Results", "content": "To test the effectiveness of ALS as an activity classification modality, we trained other widely used temporal modalities, such as IMU, Pose, and Video, for activity recognition using the same dataset we collected before. All modalities interpolated to have the same sampling frequency and step size. To make them comparable, we made the neural network architecture identical to LightHAR for all other modalities except the input size. Each model was designed with three CNN blocks for feature extraction, a bi-directional LSTM, and two fully connected layers for activity classification, mirroring the structure of LightHAR. As stated in section 4.1, the input of the LightHar is the ALS signal of size (60, 1, 1) while the input for InertialHAR is the IMU signal of size (60,1,3). In contrast, the input for the PoseHAR is the extracted SMPL pose from videos using Mo-tionBERT of size (60, 22, 3) with 22 joints, and the input for the VideoHAR is the extracted intermediate video features using Video Vision Transformer (ViViT) [2] of size (1,3137, 768)."}, {"title": "5.3 Cross-modal Results", "content": "As discussed in the section 5.2, if we consider all scenarios, InterialHAR is more accurate, more reliable/stable, and has a comparable inference time to LightHAR, making it a better choice for activity recognition.\nWe developed strategies like MultiLight InertialHAR, which takes (60, 1, 3) IMU input and a dummy array of (60,1,1) for activity classification. In this strategy, both IMU and light modality from ideal conditions (fixed light indoor) are used for training the model. Similarly, ContraLight InertialHAR takes only (60,1,3) IMU input for activity classification, but both IMU and light modality from ideal conditions (fixed light indoor) are used for training the model. The same metrics from before are used to compare all the models.\nAs we can see in Table 3, for all 3 test sets, both MultiLight InertialHAR and ContraLight InertialHAR outperformed the baseline InertialHAR although requiring the same input (60, 1, 3) accelerometer data during inference phase. The Sensor Fusion model that is identical to MultiLight InertialHAR but requires both (60,1,3) IMU input and (60,1,1) ALS input outperforms all of them in test set 1 having ideal lighting conditions, but its performance gets even worse than the baseline InterialHAR for test set 2 where the lighting conditions are challenging it doesn't provide any improvements for test set 3 either. Regarding inference time, the baseline InertialHAR, having the lowest number of FLOP, performs faster than all other models. The MultiLight InertialHAR requires an additional dummy ALS input during inference, which has much higher number of FLOP and is slower than baseline InertialHAR despite being more accurate. The ContraLight InertialHAR, while not surpassing the baseline, demonstrates a very similar number of FLOP and performs on par with the baseline in terms of speed. This efficiency, combined with its superior accuracy and F1 score compared to both the baseline InertialHAR and MultiLight InertialHAR, makes it a competitive model."}, {"title": "5.4 Discussions", "content": "As stated in section 5.2, despite having decent inference time and comparable results compared to other sensor modalities like IMUs, ALS can not be used as a universal Unimodal-HAR. Despite its limited working environments, ALS would make a very good modality for specific use cases in smart indoor environments. For example, hospitals or Care Homes have comparatively stable lighting, and the fast inference, passive sensing, and low-power use of ALS make them suitable for this job.\nAlso, because of their wide availability in smartphones and smartwatches, they can be used for simple yet repetitive tasks like step counting or other types of fitness activity counters, along with IMU modality through sensor fusion.\nAs discussed in section 5.3, A large amount of multi-modal activity data can be collected in ideal conditions to enhance other sensor modalities like IMU through our knowledge transfer strategies."}, {"title": "5.5 Limitations", "content": "In our current study, we exclusively utilized the Galaxy S20 to collect all data, limiting our insights to a single device's performance. Testing a different device type other than the one used to collect the training data could provide valuable insights into cross-device ALS-HAR reliability, particularly in scenarios where other sensor-based modalities like IMU lag behind. Additionally, employing more than one ALS sensor at different parts of the body could potentially enhance overall accuracy. This approach may provide more robustness against varying light conditions and outdoor environments, a direction we plan to explore in future research to improve the robustness and reliability of our findings."}, {"title": "6 Conclusion", "content": "In summary, our study delves into the realm of wearable ALS for HAR, showcasing its potential in understanding human motions. We developed LightHAR, a novel approach utilizing wrist-based ambient light signals for HAR, tested it for different scenarios, and compared it with other commonly used modalities for HAR. By integrating ALS with IMU through sensor fusion and contrastive classification, we enhanced the accuracy of InertialHAR systems. Our light-embedded InertialHAR approach, which relies solely on inertial data during inference, exhibited notable improvements in accuracy compared to traditional IMU-based classifiers.\nAlthough our study is promising, it is essential to acknowledge its limitations, and further research is warranted to validate our findings across devices and explore practical applications of ambient light-enhanced HAR systems."}]}