{"title": "TRAIN SMALL, INFER LARGE: MEMORY-EFFICIENT LORA TRAINING FOR LARGE LANGUAGE MODELS", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Yang You", "Guiming Xie", "Xuejian Gong", "Kunlong Zhou"], "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing with exceptional task generalization capabilities. Low-Rank Adaption\n(LORA) offers a cost-effective fine-tuning solution, freezing the original model pa-\nrameters and training only lightweight, low-rank adapter matrices. However, the\nmemory footprint of LoRA is largely dominated by the original model parameters.\nTo mitigate this, we propose LORAM, a memory-efficient LoRA training scheme\nfounded on the intuition that many neurons in over-parameterized LLMs have low\ntraining utility but are essential for inference. LORAM presents a unique twist:\nit trains on a pruned (small) model to obtain pruned low-rank matrices, which\nare then recovered and utilized with the original (large) model for inference. Ad-\nditionally, minimal-cost continual pre-training, performed by the model publish-\ners in advance, aligns the knowledge discrepancy between pruned and original\nmodels. Our extensive experiments demonstrate the efficacy of LORAM across\nvarious pruning strategies and downstream tasks. For a model with 70 billion\nparameters, LORAM enables training on a GPU with only 20G HBM, replacing\nan A100-80G GPU for LoRA training and 15 GPUs for full fine-tuning. Specifi-\ncally, QLORAM implemented by structured pruning combined with 4-bit quanti-\nzation, for LLaMA-3.1-70B (LLaMA-2-70B), reduces the parameter storage cost\nthat dominates the memory usage in low-rank matrix training by 15.81\u00d7 (16.95\u00d7),\nwhile achieving dominant performance gains over both the original LLaMA-3.1-\n70B (LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B). Code\nis available at https://github.com/junzhang-zj/LORAM.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs), such as GPT-4 (OpenAI, 2023), LLaMA (Touvron et al., 2023a;b;\nDubey et al., 2024), and PaLM (Chowdhery et al., 2023), have recently revolutionized natural lan-\nguage applications. These models excel in task generalization, driven by their exponential increase\nin scale, with some exceeding 400 billion parameters (Dubey et al., 2024). Fine-tuning pre-trained\nLLMs is critical for task-specific customization, enhancing desired behaviors while mitigating un-\ndesired ones (Qi et al., 2024). However, this process is constrained by substantial memory re-\nquirements; for instance, fine-tuning a 70B LLaMA in 16-bit precision demands over 1178GB of\nmemory, necessitating an expensive setup of 15 GPUs (A100 or H100, each with 80GB HBM)."}, {"title": "2 MEMORY-EFFICIENT LORA TRAINING \u2013 LORAM", "content": ""}, {"title": "2.1 Low-RANK ADAPTATION", "content": "Given a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{m \\times n}$, a typical full-parameter fine-tuning process adapts\nto new tasks by updating the entire full-rank matrix $W_0$. Inspired by the insight that pre-trained\nweights of LLMs exhibit low \u201cintrinsic dimension\" when adapting to specific tasks (Aghajanyan\net al., 2021), LoRA (Hu et al., 2022) further suggests that the updated weights have a low \"intrinsic\nrank\". Consequently, LoRA reparameterizes the model weights as $W_0 + W_{\\triangle} = W_0+BA$, where\n$B\\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$, and $W_{\\triangle} = BA$ represents a low-rank decomposition matrix with the\nrank $r < min(m, n)$.\nDuring training, as illustrated in Fig. 2 (a), the pre-trained weight matrix $W_0$ is frozen to avoid\ngradient computation. Instead, the low-rank matrices $B$ and $A$ are updated to enable parameter-\nefficient fine-tuning, which defaults to standard supervised fine-tuning, with the objective function\n$L_{SFT}$ defined as the cross-entropy loss between the predicted logits and the ground-truth answers.\nGiven an input feature vector x of length m, the forward pass of LoRA modifies the output activation\nfrom fully fine-tuning, represented by $h = xW_0$ (of length n), to:\n$h = xW_0 + xW_{\\triangle} = xW_0 + xBA$. \nOnce low-rank matrices $B^*$ and $A^*$ are trained by minimizing the $L_{SFT}$, as shown in the Fig. 2 (c),\nthe computation of activation h for x is reformulated to improve inference efficiency:\n$h = x(W_0 + W_{\\triangle}) = x(W_0 + B^*A^*)$."}, {"title": "2.2 \u039c\u0395\u039cORY-EFFICIENT LORA TRAINING", "content": "Consider the LLaMA-2-13B model, we introduce low-rank matrices (r = 8) for the four projection\nmatrices ($W_q, W_k, W_v$, and $W_o$) in the attention layer, the three projection matrices ($W_{up}, W_{gate}$,\nand $W_{down}$) in the MLP layer, and the weight matrix $W_{lm\\_head}$ in output layer. Despite the additional\n32 million parameters, the number of trainable parameters is reduced by 406\u00d7 compared to the full\nparameters. Many LoRA variats (Zhou et al., 2024; Zhang et al., 2023a; Kopiczko et al., 2024;\nAzizi et al., 2024; Wang et al., 2024) aim to address the significant memory overhead associated\nwith $W_0$ as $W_0$ scales, but they still necessitate storing a complete copy of $W_0$ in memory, which\ndominates training memory usage. Even with quantization methods designed for LoRA (Dettmers\net al., 2023; Xu et al., 2024; Li et al., 2024; Guo et al., 2024; Frantar et al., 2023; Chai et al., 2023),\ntraining performance constraints often limit the representation of $W_0$ to 4 bits. Consequently, for the\nLLaMA-2-13B, storage requirements are reduced from 26 GB in FP16 to 6.5 GB in NF4. However,\nthis is still significantly higher than the memory required for $W_{\\triangle}$ in BF16, which occupies only\n64MB of storage and has a peak memory requirement of 576MB during training. Thus, the memory\nneeded for the frozen quantized $W_0$ is 11.5\u00d7 greater than that required for learnable $W_{\\triangle}$.\nTo mitigate the memory overhead dominated by $W_0$ while achieving inference performance gain,\nwe propose a memory-efficient LoRA training called LORAM. LORAM first prunes the model to\na smaller size and performs LoRA fine-tuning on the pruned model. After training, it recovers the\nLORA weights, applies them to the original model, and then conducts inference. We now describe\nthe various stages of LORAM. The complete algorithm of LORAM is presented in Appendix F."}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 SETUP", "content": "Pre-train Corpus. To align the inconsistent knowledge between the pruned model during training\nand the original model during inference, we apply LORAM to continual pre-training LLMs on a\nmixed corpus of FineWeb (Penedo et al., 2024) and OpenWebMath (Paster et al., 2023). Notably,\nthis alignment process is a one-time, offline operation that can be executed by model publishers.\nFine-tuning Data. Following the fine-tuning setup of LoRA (Hu et al., 2022), we primarily con-\nduct supervised fine-tuning (SFT) on the OpenHermes-2.5 (Teknium, 2023) (referred to as Open-\nHermes) and OpenOrca (Lian et al., 2023) datasets. To effectively assess the overall fine-tuning\nperformance, we evaluate test perplexity not only on in-domain test sets constructed from the in-\nstruction fine-tuning data but also on out-of-domain test sets built from Alpaca (Taori et al., 2023).\nDownstream Task. We focus on the performance of LORAM in various downstream tasks, in-\ncluding MathQA (Amini et al., 2019) and GSM8K (Cobbe et al., 2021) in mathematical reason-\ning, six tasks-Arc Challenge & Easy (Clark et al., 2018), HellaSwag (Zellers et al., 2019), Open-\nBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), and WinoGrande (Sakaguchi et al., 2021)\nin common sense reasoning, and HumanEval (Chen et al., 2021) in code generation.\nSparsification & Quantization. For sparsification $P(\\cdot)$, we first establish a variant LORAM-\nRAND randomly structured pruning and adapt LORAM to another three variants based on lead-\ning approaches: LoRAM-STRU with the structured pruning LLM-Pruner2 (Ma et al., 2023) and\nLORAM-SEMI and LORAM-UNST with the non-structured (semi-structured & unstructured) prun-\ning SparseGPT3 (Frantar & Alistarh, 2023a). For quantization $Q(\\cdot)$, we achieve QLORAM by com-\nbining LORAM with the LoRA-tailored quantization algorithm QLoRA (Dettmers et al., 2023). The\nstorage cost of the original model primarily drives the memory consumption during LoRA weights\ntraining. Thus, we define the parameter reduction ratio as the count of parameters before and after\npruning, to evaluate the memory efficiency of baselines. The details of our experiment setups and\nhyperparameters are provided in Appendix B."}, {"title": "3.2 FINE-TUNING CONVERGENCE", "content": "We investigate the convergence trends of LORAM across varying model scales (LLaMA-2-13B &\nLLaMA-2-70B) and different instruction-tuning datasets (OpenHermes & OpenOrca). To assess\ntraining performance, we track perplexity over training iterations on both out-of-domain (Alpaca)\nand in-domain (OpenHermes or OpenOrca) test sets, as shown in Fig. 3 and Fig. 4.\nOut-of-Domain Performance. LORAM consistently achieves out-of-domain performance with\nsimilar trends, positioned between the LoRA fine-tuned models of the same scale and smaller mod-\nels, across different models and datasets. As shown in Figs. 3 and 4 (a), for the 13B model, the per-\nplexity of LORAM variants pruned by different algorithms is lower than that of the LoRA-trained\n7B model but higher than the LoRA-trained 13B model, with LORAM-RAND and LORAM-STRU\nachieving a 2.17\u00d7 parameter reduction. Similarly, as shown Figs. 3 and 4 (c), for the 70B model,\nthis reduction extends to 12.84\u00d7 under similar convergence trends.\nIn-Domain Performance. LORAM shows limited improvement in in-domain performance, likely\ndue to overfitting when the base models are fine-tuned with LoRA, resulting in relatively lower"}, {"title": "3.3 DOWNSTREAM TASK PERFORMANCE", "content": "We evaluate the performance of various models trained with LORAM on different instruction data\nacross three downstream tasks: mathematical reasoning, common sense reasoning (CSR), and code\ngeneration. Results are summarized in Tables 1 to 3. We highlight the core competition scenario\nwith a gray background, which includes the untrained original model and a smaller sibling model\ntrained with LoRA. For instance, for LORAM-trained LLaMA-2-13B, we report the scores of the\n13B without fine-tuning and the LoRA-trained 7B model. Blue backgrounds of varying intensity\nindicate the degree of improvement for each LORAM variant relative to the core competition sce-\nnario: darker shades indicate greater improvements, while lighter shades signify smaller gains.\nOverall, we observe that most LORAM variants outperform the core competitive baseline across\nall downstream tasks, particularly in mathematical and common sense reasoning. This improve-\nment is further amplified by increasing the model scale. Specifically, as shown in Table 1, the 70B\nLORAM-RAND and LORAM-STRU models achieve a 12.84\u00d7 reduction in parameters compared\nto the original 70B model (70B w/o FT), exceeding the 5.30\u00d7 reduction of the LoRA-trained 13B\nmodel. In terms of performance, LORAM improves the original 70B model's score on GSM8K\nfrom 52% to 57%, significantly outperforming the LoRA-trained 13B model, which only achieved\n37%. These results demonstrate that updating low-rank matrices on pruned models effectively re-"}, {"title": "3.5 NECESSITY OF RECOVERY & ALIGNMENT", "content": "We conduct an ablation study on two critical phases of LORAM: recovery and alignment. To assess\ntheir necessity, we analyze the convergence trends of various pruning variants on the Alpaca test set\nusing LLaMA-2-13B.\nImpact of Recovery. we compare the standard approach with an alternative setup where the\npruned low-rank matrices are directly combined with the pruned full-rank model weights (w/o\nRecovery) and track perplexity changes over iterations. As shown in Fig. 6, for all four pruning\nstrategies, models without the recovery phase (solid lines, w/o Recovery & *) consistently exhibit\nhigher perplexity compared to those with recovery (dashed lines, w/ Recovery & *), particularly in\nstructured LORAM (see in Fig. 6 (a) and (b)). This highlights that the recovery phase leverages\nrelatively redundant neurons during training to enhance inference performance significantly.\nImpact of Alignment. We also introduce a variant of LoRAM without continual pre-training\nfor alignment (w/o Alignment). As shown in Fig. 6, aligned pruned models (yellow lines, * & w/\nAlignment) consistently achieve lower perplexity than unaligned counterparts (blue lines, * & w/o\nAlignment), irrespective of the pruning strategy or recovery phase. This highlights that even low-\ncost continual pre-training on a small general corpus effectively narrows the knowledge gap between\npruned and original models, enhancing the overall performance of LORAM."}, {"title": "3.6 SCALING LAWS FOR PARAMETER REDUCTION ON LORAM", "content": "We explore the impact of scaling the parameter reduction ratios in Fig. 7. The LoRA-trained\nLLaMA-2-13B (triangles) achieves a 5.30\u00d7 parameter reduction, while QLORAM-STRU maintains\nsuperior perplexity on the Alpaca and further reduces parameters across both instruction datasets.\nIn contrast, naive pruning leads to a significant increase in perplexity with minimal pruning. When\nthe parameter reduction ratio reaches 28.56\u00d7, QLORAM-STRU sustains an effective perplexity of\napproximately 2.5, whereas naive pruning escalates to 621.98. These highlight LORAM's ability to"}, {"title": "4 CONCLUSION", "content": "We propose LORAM, a memory-efficient LoRA training scheme for large language models. Lo-\nRAM significantly reduces the count of parameters of the original model by 16.95\u00d7, while main-\ntaining the performance of large-scale LLM fine-tuning. We identify several open questions for\nLORAM, including the potential for reduced inference costs through context-aware computational\ngraph recovery and its applicability to models like vision transformers (Dosovitskiy et al., 2021) and\ndiffusion models (Ho et al., 2020). We hope our work inspires further research on memory-efficient\nLORA training from a sparsity perspective and believe LORAM will serve as a valuable tool for the\ncommunity, enabling LoRA training of large-scale models on consumer-grade hardware."}, {"title": "FALGORITHM OF LORAM", "content": "Here, we present the complete algorithm of LORAM in Algorithm 1."}, {"title": "G TUNING OF LEARNING RATE", "content": "We provide additional details on the learning rate tuning process for full LoRA applied to LLaMA-\n2-7B and LLaMA-2-13B models, trained on the OpenHermes dataset. These experiments in Fig. 16\ndemonstrate that a learning rate of 1e-3 consistently achieves the best perplexity across both in-\ndomain and out-of-domain datasets, further validating the reliability of our comparison."}, {"title": "H PERFORMANCE OF DOMAIN-SPECIFIC TASK", "content": "To assess the effectiveness of LoRAM in domain-specific tasks, we conducted experiments on\nGSM8K (using the training set for tuning and the test set for evaluation), a mathematical reason-\ning benchmark known for its sensitivity to sparsification. Specifically, we trained LLaMA-3.1-70B\nusing QLORAM under various configurations.\nThe results, summarized in Table 7, highlight that LoRAM achieves excellent performance in this\ndomain-specific setting. Notably, LoRAM-based models maintain high accuracy with substantial\nparameter reduction ratios, showcasing their robustness and efficiency in domain-specific tasks.\nThese findings emphasize LoRAM's broad applicability beyond general-purpose instruction fine-\ntuning."}, {"title": "I ANALYSIS OF LORAM COST", "content": "Identifying the costs of LoRAM is indeed important, which is why we report both the number of\ntraining tokens used during the alignment phase and the parameter reduction ratios in the low-rank\ntraining phase. Below, we clarify the two stages of LoRAM:\nOffline Knowledge Alignment Phase. The offline phase is task-agnostic and can be conducted\nby the model publisher prior to deployment, making its cost negligible for end users. To quantify\nthe offline cost, we measured the number of training tokens (as in Xia et al. (2024)) rather than end-\nto-end latency, which can vary based on hardware configurations. As shown in Figure 5, LORAM\nachieves significant performance gains using only 13 million tokens, demonstrating the efficiency\nof the alignment phase.\nOnline Low-Rank Matrix Training Phase. For the online phase, the memory and latency costs\nare primarily determined by the size of the base model parameters, which dominate resource con-\nsumption during training. To avoid redundancy in reporting, we focused on parameter reduction\nratios instead of absolute time or memory usage.\nComparative Metrics for Online Training. Here, we provide additional metrics, including mem-\nory and latency comparisons for the online training phase. We conducted experiments using a work-\nload of 1024 samples (batch size 128, micro-batch size 4, sequence length 512) randomly selected\nfrom OpenHermes. The results in Table 8 demonstrate that LoRAM with a structured pruning ratio\nof 2.17x (13B \u2192 6B) achieves comparable peak memory, latency, and throughput to 7B LoRA,\nwith only minor trade-offs. These differences arise due to the larger layer count in 13B LORAM,\nintroducing more non-GEMM operations, slightly affecting latency and throughput.\nThese results underscore the advantages of LoRAM's design in achieving substantial resource effi-\nciency without significant trade-offs in memory or latency."}, {"title": "J ANALYSIS OF CHANGES IN PERFORMANCE TRENDS", "content": "We analyze performance at two stages: after fine-tuning but before recovery, and after both fine-tuning and recovery.\nAfter Fine-Tuning but Before Recovery. At this stage, the results of LoRAM align with prior work (e.g., SparseGPT, Wanda, and LLM-Pruner). Unstructured and semi-structured pruning consistently outperform structured pruning (see Fig. 6, solid lines). This trend holds true across both aligned and unaligned settings, with the performance order as follows: LORAM-SEMI < LORAM-UNST < LORAM-STRU < LORAM-RAND The slight advantage of LORAM-SEMI over LORAM-UNST can be attributed to its smaller pruning ratio, which retains more parameters and mitigates performance degradation.\nAfter Fine-Tuning and Recovery. Post-recovery results show that structured pruning outperforms unstructured pruning. This can be explained by two factors:\n\u2022 Preserved Structure for Recovery: Structured pruning maintains the organization of the pruned weights into coherent structures (e.g., rows and columns in MLP layers, attention heads in attention layers), ensuring that activations after recovery are aligned with those of the original model. This alignment improves the recovery process.\n\u2022 Pruned Weight Quality: The quality of pruned weights influences the recovery effectiveness. Structured pruning tends to remove less critical weights, leaving more recoverable parameters. In contrast, unstructured pruning can remove weights that are more difficult to recover, which negatively impacts performance post-recovery.\nThese results highlight the interplay between pruning and recovery, suggesting that structured pruning, despite initial performance disadvantages, facilitates more effective recovery."}]}