{"title": "Counterfactual Explanations as Plans", "authors": ["Vaishak Belle"], "abstract": "There has been considerable recent interest in explainability in AI, especially with black-box machine\nlearning models. As correctly observed by the planning community, when the application at hand\nis not a single-shot decision or prediction, but a sequence of actions that depend on observations, a\nricher notion of explanations are desirable.\nIn this paper, we look to provide a formal account of \"counterfactual explanations,\" based in\nterms of action sequences. We then show that this naturally leads to an account of model reconcilia-\ntion, which might take the form of the user correcting the agent's model, or suggesting actions to the\nagent's plan. For this, we will need to articulate what is true versus what is known, and we appeal\nto a modal fragment of the situation calculus to formalise these intuitions. We consider various set-\ntings: the agent knowing partial truths, weakened truths and having false beliefs, and show that our\ndefinitions easily generalize to these different settings.", "sections": [{"title": "Introduction", "content": "There has been considerable recent interest in explainability in AI, especially with black-box machine\nlearning models, given applications in credit-risk analysis, insurance pricing and self-driving cars. Much\nof this focus is on single-shot decision or prediction, but as correctly observed by the automated planning\ncommunity [14, 7, 6], in applications involving sequence of actions and observations and conditional\nplans that depend on observations, a richer notion of explanations are desirable. Fox et al. [14], for\ninstance, argue that understanding why a certain action was chosen (and not some other), why one\nsequence is more optimal than another, etc, are all desired constructs in explanations.\nDespite all this attention, there is yet to emerge a theory on how exactly to frame explanations in a\ngeneral way. One candidate is the notable line of work on model reconciliation [40]. The idea is that\nthe agent might have incomplete or even false information about the world, and the user can advise the\nagent by correcting the agent's model, or suggesting actions to the agent's plan. The latter move is in the\nspirit of human-aware AI [19]. But such an idea is inherently epistemic, and this brings the explainable\nAI literature closer to epistemic planning [2]. Many recent threads of work have tried to explicate this\nconnection.\nIn [37], for example, the idea of discrepancy and resolving such discrepancy is studied. Using the\nepistemic logic fragment from [30], where so-called proper epistemic knowledge bases are chosen for\ninitial plan states that allow only for modal literals, discrepancy emerges when (in a dynamic logic-like\nlanguage) \u03a3 = [\u03b4]K;\u00a2 but \u03a3 |\u2260 [8]K\u00a1\u00a2. Given a goal \u222e, an action sequence 8, background knowledge\n\u2211 for agents i and j which include dynamic axioms, it turns out that i believes that o is true but j does\nnot. So the resolution is to find some course of action d' that ensures that either \u03a3|= [\u03b4'](\u039a;\u00a2 > \u039a\u00a1\u00a2)\nor that \u03a3 = [\u03b4'](K\u00a1\u00ab\u00a2>Kj\u00ab\u00a2). That is, they both jointly believe after \u03b4' that \u00f8 is made true, or that o\nis made false. And as pointed out in [37], articulating the difference between beliefs and ground truth is\nneeded for clarity."}, {"title": "A Logic for Knowledge and Action", "content": "We now introduce the logic ES [23].3 It is an epistemic logic, but we only need the objective frag-\nment for formalizing counterfactual explanations as plans. When we consider the more elaborate notion\nof reconciliation-type explanations where both a user and an agent is necessary, we will use the full\nlanguage.\nThe non-modal fragment of ES consists of standard first-order logic with =. That is, connectives\n{^,\u2200,\u00ac}, syntactic abbreviations {\u2203,=,\u2283} defined from those connectives, and a supply of variables\nvariables {x,y,..., u, v, ...}. Different to the standard syntax, however, is the inclusion of (countably\nmany) standard names (or simply, names) for both objects and actions R, which will allow a simple,\nsubstitutional interpretation for \u2200 and \u2203. These can be thought of as special extra constants that satisfy"}, {"title": "Reasoning & Planning", "content": "Given a background theory \u03a3, an action sequencen 8 = a\u2081\u2026\u2026ak, and a (non-modal) goal formula , \nthe classical problem of projection [34] is to identify if the sequence enables \u222e. That is, whether \u03a3|=\n[8]. We also want to ensure that the action sequence is executable (aka valid and/or legal). So let\nus Exec(()) = true, and Exec(a\u00b7 \u03b4) = Poss(a) ^ [a]Exec(\u03b4). Then, we check: \u03a3 = Exec(\u03b4) >[\u03b4]\u0444.\nIn the epistemic setting [35], we are interested in checking if o is known after executing 8, which\nmight include sensing actions too. That is, whether7 \u03a3|= [\u03b4]K\u00a2. When adding action executability,\nwe would have: \u03a3 = [\u03b4]\u039a\u00a2 > KExec(\u03b4). So the agent also knows that the sequence is executable:\nwhich means 8 is executable in every world in the agent's epistemic state. Note that because we do\nnot require the real world to be necessarily included in the epistemic state, it is not necessarily that\n8 is actually executable in the real world. If we needed to additionally enforce that, we would need:\n\u03a3|= Exec(\u03b4) [\u03b4]\u039a\u00a2 > KExec(\u0431).\nThe task of planning, then, is to identify a sequence 8 such that $ is made true (in the non-epistemic\nsetting) or that o is known to be true, and that the appropriate executability condition holds.\nNote that, we do not require plans to simply be a sequence of (physical) actions. For one thing, they\nmay involve sensing actions, based on which the agent obtains information about the world. For another,\nwe might be interested plans involving recursion [41], conditional statements and tests [27, 25, 15]. Such\nplan structures do not change the nature of the reasoning problem, however: no matter the plan structure,\nwe will be evaluating if the sequence of actions executed by the agent enables some goal, that is, whether\nthe structure instantiates a sequence 8 such that \u03a3 |= [\u03b4]\u00a2 and \u03a3 |= [\u03b4]K\u00a2 for world-state and epistemic\nplanning respectively. Likewise, regression is not limited to only action sequences and can work with\nconditional and recursive plans.8\nFinally, it can be shown that reasoning about actions and knowledge can be reduced to non-modal\nreasoning. We omit the details but refer readers to [23]. (We will included an extended report with some\nexamples.) With a finite domain assumption, this can be further reduced to propositional reasoning."}, {"title": "Counterfactual Explanations", "content": "We will firstly attempt to characterize counterfactual (CF) explanations as plans, at an objective level.\nThis could be viewed, therefore, as an instance of planning with incomplete information, but it can also\nbe ultimately linked to the epistemic setting, as we shall see below. Simply put, a CF explanation is an\nalternative course of action that negates the goal. (Conversely, if some sequence does not enable the goal,\nwe search for an explanation that does.9)"}, {"title": "Reconciliation-based Explanations", "content": "The simplest case of an agent providing a counterfactual explanation is that we formulate plans in the\ncontext of knowledge, and so goals can involve nested beliefs.\nDefinition 11. Suppose \u03a3|= [\u03b4]K\u00a2\u2227KExec(\u03b4), where \u222e might mention K but no other modality. Then\na counterfactual explanation is \u03b4' such that dist(\u03b4',\u03b4) is minimal and \u2211 = [\u03b4']K\u00ab\u03c6 >KExec(\u03b4').\nRecall that we are not seeking [8]\u00acK\u00a2, because this is the case of an agent being ignorant. We\ninstead seek \u03b4' after which the agent knows that o is false.\nNote that in the above definition we were not stipulating executability in the real world, because it\nsuffices for the account to be purely epistemic. We can enforce this additionally, of course, but it will\ncome up naturally for the definitions below because the user needs to make sure she is only suggesting\nlegal actions.\nExample 12. Following our examples above, consider 8 = pickup(c)\u00b7 drop(c) and clearly \u2211 entails\n[\u03b4]KBroken(c). The explanation \u03b4' = \u03b4\u00b7 repair(c) achieves K\u00acBroken(c).\nWhat we will consider below is the case where the user assists in achieving goals. So this leads to\na broader notion of counterfactuals: consider an alternate history or additional knowledge such that the\ngoal becomes true."}, {"title": "Agents Only-Knowing Partial Truths", "content": "For this case of ignorant agents, we assume \u03a3 \u2286 \u03a3\u03bf.10 Suppose a plan 8 fails in achieving K\u00f3. A CF\nexplanation here amounts to considering a possible world and a sequence such that the agent knows \u03c6.\nSo either there are missing actions, or missing knowledge, or both.\nDefinition 13. (Missing actions.) Suppose \u03a3|= Exec(\u03b4) > \u039aExec(\u03b4) but \u03a3 |\u2260 [\u03b4]\u039a\u00a2, that is, \u03a3|=\n[8]\u00acK\u00a2. Suppose there is a sequence d' such that dist(8',8) is minimal, \u03a3 = [\u03b4']\u00a2 \u2227Exec(\u03b4'), and\n\u03a3 = [\u03b4'] \u039a\u03c6 > \u039aExec(8'). Then a CF explanation is \u03b4'.\nNote that we do not insist \u03a3 = [8], because as the definition title suggests, there might actions\nmissing. One might also wonder why we insist on the agent also needing to know \u00a2 after \u03b4': is it not\nredundant? The answer is no. Firstly, notice that even if there is d' such that \u2211 = [8'], it is not necessary\nthat d' is minimally away from 8. For example, as far as entailment of objective formulas is concerned,\nthe presence of sensing actions in \u03b4' is irrelevant: sensing only affects the knowledge of the agent and\ndoes not affect the real world. But the agent may very well need sensing actions to learn more about the\nworld. Therefore, we insist that we need to find a d' that is minimally different to 8, enables o in the real\nworld but also enables the knowing of 6. In other words, if both \u03b4' and 8\" enable \u00f8 and they differ only\nin that d' includes sensing actions whereas d\" does not, then we want d' to be the explanation.\nExample 14. Consider 8 = pickup(c) for the goal \u00acKBroken(c). The explanation is \u03b4' = \u03b4\u00b7drop(c),\nand indeed, \u2211 = [\u03b4']Broken(c) but also \u03a3|= [\u03b4']KBroken(c).\nExample 15. Suppose d = pickup(d).drop(d) for the goal \u00acKBroken(d). But in fact, \u03a3|= [8]Broken(d),\nand so the agent does not know d is broken owing to the fact that it does not know that d is made of glass.\nSo consider \u03b4' = pickup(d) \u00b7 isGlass(d).drop(d). This does not affect what is true in the world, but\ndoes lead the agent to know that Glass(d). Therefore, d' is the explanation since \u2211 = [\u03b4']Broken(d),\nand \u03a3 = [\u03b4'] KBroken(d)."}, {"title": "Agents Only-Knowing Weakened Truths", "content": "Here we assume \u03a3 \u03a3 \u03a3 but for every a \u2208 \u03a3', \u03a3\u03bf = \u03b1. In other words, \u03a3\u03bf |= \u03a3'. That is, we might have\nan atom p\u2208 \u03a30, but \u2211 instead has (p V q). We then can define an account involving missing knowledge\nand actions, and so the same definition from 18 applies.\nExample 20. Let us consider Example 19 except that the initial theory of the agent is \u03a3\" = \u03a3\n(Metal(h) \u2228 Metal(d)). That is, it includes all the formulas from \u2211 but also information that his\nmetallic or (falsely) that d is metallic. However, \u2211 = (Metal(h) \u2228 Metal(d)), and so a and \u03b4' from\nExample 19 counts as the explanation."}, {"title": "Agents with False Beliefs", "content": "False beliefs are only satisfiable when \u25a1(Ka\u2283 a) is not valid, as it happens to be in our case. For\nsimplicity, we deal with the case of missing knowledge, and this can be easily coupled with missing\nactions in the manner discussed above. (Our example will deal with both.)\nDefinition 21. Suppose \u03a3\u0375 \u2260 \u03a3\u03bf, and moreover \u03a3\u03bf |\u2260 \u03a3'. Suppose \u03a3 |\u2260 ([\u03b4]\u039a\u00a2 > \u039aExec(\u03b4)) but \u03a3 |=\n\u0415\u0445\u0435\u0441(\u03b4) [\u03b4]\u0444. Suppose there is a \u2208 \u03a3\u0375 and \u03b2 \u2208 \u03a3\u0375 such that \u03a3\u03bf\u03c5\u2211dyn UO((\u03a3' \u2212 {\u03b2})\u222a{a}\u222a\u2211dyn) |=\n[\u03b4]\u041a\u00a2 ^KExec(\u03b4). Then the smallest such a and \u1e9e are the explanations.\nExample 22. Consider Example 19 and let \u03a3'\" = \u03a3' ^\u00abMetal(h). So the agent only-knows everything\nfrom '' as well as a false fact about h. Let \u03a3, \u03b4, and d' be as in Example 19. Now note that given\n\u03b1 = Metal(h), \u03b2 = \u00abMetal(h) and \u03b4', we have \u03a3 \u039b \u03a3dyn \u039b\u039f(\u03a3\u039b\u03b1 \u039b\u03a3dyn) entails [\u03b4'](\u03c6 > \u039a\u03c6) ^\nExec(\u03b4') ^ KExec(\u03b4'), for \u00a2 = Broken(h). As it turns out \u03a3' is obtained by removing \u03b2 from \u03a3\" by\nconstruction, and so a,\u1e9e and \u03b4' constitutes as the explanation."}, {"title": "Possibility vs Knowledge", "content": "In many applications, we may not require that the agent knows 6, only that it considers & possible. In the\nexplanation generation framework of [42], for example, there is a notion of credulous entailment where\na single belief state suffices to checking the validity and achievement of a plan. (In contrast, skeptical\nentailment is when every belief state is involved.) The analogous notion in an epistemic language is to\nintroduce a companion modal operator B with the following semantics:\n\u2022 e, w, z = Ba iff there is some w' ~z w, w' \u2208 e such that e, w', z= \u03b1.\nAs long as there is at least one world where a is true, Ba is evaluated to true at (e,w,z). This is a\ncompanion modal operator to K for possibility. We might introduce an analogue to Definition 18 in\nusing B instead of K as the modality in the goal. To see how this works, let us revisit Example 19.\nExample 23. Consider the modified precondition axiom for quench(x), and let \u03a3', \u03a3, \u03b4 and \u03b4' be as\nin Example 19. By only-knowing \u03a3', the agent considers some worlds where h is metallic, and others\nwhere it is not. Thus, we now see that we do not really need to suggest a = Metal(h) to the agent if the\nweaker notion of a CF explanation is considered. Indeed, \u03a3\u039b\u2211dyn \u039b\u039f(\u03a3\u039b\u03a3dyn) entails [\u03b4'](\u03c6 ^\u0392\u00a2)\nfor $ = Broken(h).\nIn other words, we have augmented actions but we did not need to augment knowledge in the above\nexample. Had the agent believed false things, then we might have needed to augment both, and so can\nappeal to Definition 21 but using B instead of K to allow for credulous-type reasoning."}, {"title": "Other related efforts", "content": "In addition to the works discussed in previous sections, the following efforts are related.\nThere is some syntactic (and perhaps intuitive) connection to explanation-based diagnosis. For ex-\nample, in [38], the idea is to encode the behavior of the system to be diagnosed as a situation calculus\naction theory, encode observations as situation calculus formulae, and conjecture a sequence of actions\nto explain what went wrong with the system. (However, they often need to model faulty or abnormal\ncomponents when defining the notion of a diagnosis.) In our setting, in contrast, we identify actions\nand/or knowledge that determine how an outcome can be changed. Nonetheless, we believe that a fur-\nther formal study to relate such accounts would be useful, and could nicely complement empirical works\nsuch as [9]. See also Ginsberg [17]."}, {"title": "Conclusions", "content": "We developed an account of counterfactual explanations and reconciliation-based counterfactual expla-\nnations in this paper. This allows for a simple and clear specification in the presence of missing actions,\npartial knowledge, weakened beliefs and false beliefs. Existing accounts of discrepancy in plans, among\nothers, can be seen as variations of this more general specification. For the future, it would be interesting\nto incorporate other notions in our formalization, such as operational aspects of plans, costs, optimality\nand conciseness [14, 40, 42], towards a unified mathematical specification of explainable planning."}]}