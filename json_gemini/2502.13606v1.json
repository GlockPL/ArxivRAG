{"title": "LaVCa: LLM-assisted Visual Cortex Captioning", "authors": ["Takuya Matsuyama", "Shinji Nishimoto", "Yu Takagi"], "abstract": "Understanding the property of neural populations (or voxels) in the human brain can advance our comprehension of human perceptual and cognitive processing capabilities and contribute to developing brain-inspired computer models. Recent encoding models using deep neural networks (DNNs) have successfully predicted voxel-wise activity. However, interpreting the properties that explain voxel responses remains challenging because of the black-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex Captioning (LaVCa), a data-driven approach that uses large language models (LLMs) to generate natural-language captions for images to which voxels are selective. By applying LaVCa for image-evoked brain activity, we demonstrate that LaVCa generates captions that describe voxel selectivity more accurately than the previously proposed method. Furthermore, the captions generated by LaVCa quantitatively capture more detailed properties than the existing method at both the inter-voxel and intra-voxel levels. Furthermore, a more detailed analysis of the voxel-specific properties generated by LaVCa reveals fine-grained functional differentiation within regions of interest (ROIs) in the visual cortex and voxels that simultaneously represent multiple distinct concepts. These findings offer profound insights into human visual representations by assigning detailed captions throughout the visual cortex while highlighting the potential of LLM-based methods in understanding brain representations.", "sections": [{"title": "1. Introduction", "content": "A primary goal of computer vision is to build systems capable of processing and understanding the complex visual world in a manner akin to human perception. Studying how the human brain-with its advanced visual functions-forms its visual representations deepens our understanding of the brain's visual network and holds promise for developing next-generation computer vision models.\nTo understand how the human brain represents visual stimuli from the external world, researchers have used encoding models that predict voxel-level (the spatial measurement unit of fMRI) brain activity measured during the presentation of images or videos. These models typically use features derived from theoretical or data-driven hypotheses to map from stimulus properties to voxel responses. Many such models use simple, interpretable features for instance, low-level visual features generated by filters rooted in theoretical models or high-level visual features representing words using one-hot encoding-to infer voxel properties through the correlation between feature-based predictions and actual brain responses.\nHowever, because the human brain solves highly complex real-world tasks, relying solely on a small number of interpretable parameters may be insufficient. Recent advances in deep neural networks (DNNs), which effectively handle real-world tasks through a large number of parameters, have enabled more accurate predictions of brain activity by leveraging the complex representations extracted by DNNs. Nevertheless, because of their black-box nature, interpreting how these DNN-based features map to individual voxels remains challenging. Although group-level interpretations-representing many voxels in terms of a small set of universal, interpretable axes\u2014are possible, voxel-level interpretation is critical for exploring more nuanced aspects of brain representations.\nIn this study, we address the difficulty of voxel-level interpretation with a new method called LLM-assisted Visual Cortex Captioning (LaVCa), which generates data-driven captions for individual voxels. LaVCa proceeds in four steps: (1) building voxel-wise encoding models for brain activity evoked by images, (2) identifying the optimal images for each voxel's encoding model using an augmented image dataset (3) generating captions for these optimal images, and (4) creating concise summaries from those captions. By leveraging large language models (LLMs) with access to a vast, open-ended vocabulary, LaVCa generates diverse inter-voxel captions. Moreover, generating captions from multiple keywords enables us to capture diverse intra-voxel properties. Our contributions are as follows:\n1.  We propose LLM-assisted Visual Cortex Captioning (LaVCa), a data-driven technique that leverages LLMs to generate natural language captions of voxel-level visual selectivity.\n2.  We demonstrate that LaVCa produces more accurate captions than the earlier method BrainSCUBA and better characterizes voxel-wise visual selectivity through brain activity prediction.\n3.  We also demonstrate that LaVCa can generate highly interpretable and accurate captions without sacrificing information from the optimal images.\n4.  The captions generated by LaVCa quantitatively capture more detailed properties than the existing method at both the inter-voxel and intra-voxel levels.\n5.  More detailed analysis of the voxel-specific properties generated by LaVCa reveals fine-grained functional differentiation within regions of interest (ROIs) in the visual cortex and voxels that simultaneously represent multiple distinct concepts."}, {"title": "2. Related Work", "content": "Encoding models have been widely used in neuroscience to interpret neural representations. These studies typically use interpretable low-level visual features and high-level semantic features, such as one-hot encodings,"}, {"title": "3. Methods", "content": ""}, {"title": "3.1. fMRI dataset", "content": "This study uses the Natural Scenes Dataset (NSD) following the same experimental conditions as in BrainSCUBA. The NSD consists of data collected over 30 to 40 sessions using a 7 Tesla fMRI scanner, with each participant viewing 10,000 images, repeated three times. We analyze data from the four participants (Subject 01, Subject 02, Subject 05, and Subject 07) who completed all imaging sessions. The images and captions used in NSD are drawn from MS COCO and resized to 224 \u00d7 224 pixels to align with the input requirements of the vision models used. We average the brain activity data for each subject across repeated trials of the same image to improve the signal-to-noise ratio. Up to 9,000 images per subject are used as training data, and the remaining 1,000 images are reserved for testing. We use the preprocessed scans with a resolution of 1.8 mm provided by NSD for the functional data. We use single-trial beta weights estimated via a generalized linear model (GLM) within ROIs. Moreover, we standardize the response of each voxel to have a mean of zero and a variance of one within each session. We use the ROIs provided by NSD, which include early and higher-level (ventral) visual areas and face, place, body, and word-selective regions."}, {"title": "3.2. LLM-assisted Visual Cortex Captioning (LaVCa)", "content": "We propose a method, LaVCa (LLM-assisted Visual Cortex Captioning), to automatically generate data-driven natural language captions that characterize each voxel's selectivity in the visual cortex. LaVCa consists of four stages:\n1.  Construct voxel-wise encoding models for each subject while they view natural images.\n2.  Identify the optimal image set by finding the top-N images that most strongly activate each voxel (according to the trained encoding models).\n3.  Generate captions for these optimal images using a Multimodal LLM (MLLM) for summarization by an LLM in the next step.\n4.  Derive concise voxel captions by extracting and filtering keywords from the image captions, then feeding these keywords into a \"Sentence Composer.\""}, {"title": "3.2.1. ENCODING MODEL CONSTRUCTION", "content": "First, we construct voxel-wise encoding models to predict each voxel's activity in response to natural images. For ease of comparison with BrainSCUBA, we use the projection layer of CLIP's vision branch, using the same pretrained checkpoint used in BrainSCUBA. Hereafter, we refer to CLIP's vision branch as \"CLIP-Vision\". Because the code for BrainSCUBA is not publicly available, we implement it in-house. Both the dataset used for the softmax projection and the training approach for the encoding model differ from those in the original paper. We estimate the encoding model weights using L2-regularized linear regression on the NSD training set."}, {"title": "3.2.2. EXPLORATION OF OPTIMAL IMAGE SETS FOR VOXELS", "content": "Next, we identify the optimal image set for each voxel. We compute the inner product between the voxel's"}, {"title": "3.2.3. CAPTIONING OPTIMAL IMAGE SETS WITH MLLM", "content": "To enable an LLM to interpret each voxel's optimal image set, we first generate captions for these image sets using an MLLM. We use MiniCPM-V with the prompt \"Describe the image briefly.\" For our accuracy evaluation, we also form a simple baseline by concatenating the top-N captions from the optimal image set."}, {"title": "3.2.4. GENERATING VOXEL CAPTIONS", "content": "Finally, we generate interpretable voxel captions from the image captions. First, we use an LLM to extract common keywords across the captions within each voxel's optimal image set. Following the in-context learning prompt approach from , we extract multiple keywords from the caption sets using an LLM. We use gpt-4o as the LLM. We investigate the effects of several hyperparameters on accuracy, including the number of images in the optimal image sets, the number of extracted keywords, and the type of keywords extractor. To remove irrelevant or noisy keywords, we compute the cosine similarity between each keyword's embedding from CLIP' text branch (prompted as \u201cA photo of {keyword}.\u201d) and the encoding weight for that voxel, then apply a softmax threshold to retain only sufficiently relevant keywords. Hereafter, we refer to CLIP's text branch as \"CLIP-Text\". Next, we transform these filtered keywords into a sentence-level caption using the \"Sentence Composer\" from MeaCap, initially designed to generate image captions from keyword sets. MeaCap can generate a caption by inputting the target image's keywords into the Sentence Composer while referencing similarities to the image features. In this study, we replace image features with encoding weights so that the model composes a coherent sentence from the voxel-specific keywords."}, {"title": "3.3. Caption Evaluation", "content": ""}, {"title": "3.3.1. BRAIN ACTIVITY PREDICTION AT SENTENCE LEVEL", "content": "We predict brain activity based on sentence similarity to assess how accurately voxel captions describe voxel selectivity. We hypothesize that a voxel caption capturing the true selectivity of a voxel should be more similar to the corresponding caption of an NSD image that strongly activates that voxel and less similar otherwise. Following , we:"}, {"title": "4. Results", "content": ""}, {"title": "4.1. Voxel Activity Prediction", "content": "To determine whether the generated captions accurately describe the properties of the voxels, we evaluate brain activity prediction at both the sentence and image levels. First, we map sentence-level prediction performance across inflated and flattened cortical surfaces. These maps illustrate that LaVCa captions significantly predict voxel activity throughout the visual cortex. Next, we compare LaVCa, the existing method BrainSCUBA, and a shuffled variant (LaVCa captions shuffled across voxels) at both the sentence and image levels, focusing on the top 5,000 voxels with the highest accuracy on the training data. Our proposed method, LaVCa, outperforms BrainSCUBA. This finding indicates that generating captions based on multiple keywords extracted from an optimal set of images provides a more accurate explanation of voxel selectivity. Furthermore, the marked drop in accuracy for the shuffled condition compared with the original LaVCa confirms that our evaluation metric efficiently gauges whether captions describe voxel selectivity. Results for the top 1,000, 3,000, and 10,000 voxels appear in Table A2 and A3. After visualizing sentence-level prediction accuracy across the cortex, we find that LaVCa exceeds BrainSCUBA's performance throughout the visual cortex."}, {"title": "4.2. Lexical and Semantic Diversity Analysis", "content": "We next assess how effectively LaVCa captions capture both lexical and semantic diversity across voxels, focusing first on inter-voxel diversity. For this quantitative evaluation, we use three metrics: (1) the total vocabulary size (excluding stop-words) across all voxel captions (Lexical); (2) the average variance across each dimension of the CLIP-Text embedding computed on all voxel captions (Semantic); and (3) the number of principal components (PCs) required to capture 90% of the variance of CLIP-Text embedding across captions in a principal component analysis (PCA; Semantic).\nFirst, we evaluate the diversity of LaVCa captions compared with the existing method, BrainSCUBA. When averaged across subjects, LaVCa markedly outperforms Brain-SCUBA in both lexical (16,922 vs. 3,193 in vocab. size) and semantic (0.0642 vs. 0.0588 in variance of embeddings; 219 vs. 127 in PCs required for 90% variance explained) diversity. These findings confirm that our open-ended LLM-based approach can produce richer word usage and more meaningful captions across inter-voxel comparisons.\nWe evaluate the diversity of LaVCa captions compared with more detailed captions. BrainSCUBA leverages ClipCap, a model that produces relatively simple image captions. We use the top-1 captions generated by the MLLM on the optimal image sets (equivalent to the case where N = 1 in Concat-N) to compare the diversity of LaVCa with more detailed captions. When averaged across subjects, Top-1 (13,959 vocab. size, 0.0638 avg. variance, 210 PCs) exhibits both a vocabulary range and semantic diversity close to LaVCa. However, LaVCa achieves a higher prediction accuracy (0.264 vs. 0.224), indicating that LaVCa can preserve robust brain activity prediction performance while enhancing the diversity of generated captions.\nNext, we evaluate diversity from an intra-voxel perspective by comparing captions generated by three models in both lexical and semantic dimensions. We use three metrics: (1) the vocabulary size of each voxel's caption (Lexical), (2) the average sentence length in each voxel's caption (Lexical), and (3) the average variance across all dimensions of Word2Vec embeddings of each caption's words (excluding stop-words) (Semantic). When averaged across subjects, LaVCa markedly outperforms BrainSCUBA in both lexical (11.4 vs. 6.09 in vocab. size) and semantic (11.9 vs. 6.19 in avg. length; 0.0199 vs. 0.0160 in variance of semantic embeddings) diversity. This improvement suggests that LaVCa more precisely captures the fine-grained intra-voxel characteristics."}, {"title": "4.3. ROI-level Diversity Analysis", "content": "Our results thus far illustrate that LaVCa generates more accurate voxel captions than BrainSCUBA and better reflects inter- and intra-voxel diversity. We now assess how LaVCa captions capture diversity within known ROI, such as the OFA and PPA. These areas are conventionally associated with specific concepts (e.g., faces or places) yet the extent of diversity within these ROIs remains unclear. We conduct a quantitative evaluation using LaVCa's captions and generated images to analyze diversity that exists beyond the known selectivity in the ROI.\nQuantitative Assessment. We determine how many distinct captions appear in each ROI by comparing the sentence-level prediction accuracy of each ROI when captions are maintained in their original form versus shuffled within the ROI. For each category (body, face, place, and word area), we select two ROIs with the largest total voxel count across all subjects, resulting in eight ROIs in total. In all ROIs, shuffling reduces prediction accuracy significantly. For example, in the OFA, accuracy drops from 0.0945 (Original) to 0.0280 (Shuffled), a 3.3-fold decrease; in the PPA, accuracy falls from 0.213 (Original) to 0.151 (Shuffled), a 1.4-fold decrease. Thus, even in regions traditionally linked to particular concepts, voxels exhibit a range of distinct selectivities.\nQualitative Assessment. We next explore the semantic diversity of LaVCa captions in the OFA by applying UMAP to their CLIP-Text embeddings and visualizing the resulting distributions on a flatmap. Each subject's"}, {"title": "5. Discussion & Conclusions", "content": "In this study, we introduce a novel method called LaVCa, which leverages LLMs to produce data-driven, natural-language descriptions of voxel selectivity in the human visual cortex. The voxel captions generated by LaVCa exhibit higher accuracy and greater semantic diversity than those generated by the existing approach, BrainSCUBA. We attribute this improvement to our mechanism for integrating multiple keywords extracted by advanced LLMs, which enables a more comprehensive capture of the diverse selectivity patterns across voxels.\nDespite the overall improvement in brain activity prediction, we observe that face-selective regions do not achieve accuracy levels as high as those in other ROIs. One reason may be that our current approach uses a Multimodal LLM (MLLM) to produce relatively simple captions for optimal images, often omitting important local features (e.g., \"eyes,\" \"nose\") and focusing on more global terms (e.g., \"face,\" \"person\u201d). Consequently, the subsequent summarization step lacks access to these local details. Because our method relies on language descriptions, it has inherent limitations in capturing the fine-grained, local selectivity of these voxels. Incorporating recent techniques that visually interpret local voxel selectivity could help address this gap.\nFurthermore, while our current study describes voxel selectivity primarily in response to visual stimuli in the occipital cortex, there exist \"multimodal voxels\" in the brain that are simultaneously activated by auditory and linguistic information, and higher-order cognitive processes such as calculations, memory retrieval, and reasoning. Designing stimuli and experimental tasks encompassing diverse sensory inputs (e.g., auditory, textual) and cognitive challenges (e.g., recalling past events, performing reasoning tasks) is essential when interpreting such voxels. Because our approach uses LLM-based textual summarization, it can be adapted to represent a wide range of stimuli and cognitive states in text form, providing a unified framework for multimodal integration. Looking ahead, by jointly modeling images, semantic information, auditory features, and cognitive tasks, we anticipate capturing the brain's integrated representation of both sensory and higher-order cognitive functions with greater accuracy."}, {"title": "Impact Statement", "content": "We introduce a data-driven method that uses a large language model to generate natural-language captions of voxel-level visual selectivity. Using the method detailed in this paper, we aim to provide a more fine-grained understanding of human visual function than previously achieved. We acknowledge that this human brain research could raise concerns regarding individual privacy. Although the present study examined relatively coarse-grained voxel-level data, we cannot dismiss the possibility that future advances in measurement and analysis techniques may enable the extraction of more detailed individual-specific information. In any case, obtaining explicit informed consent from participants remains crucial when collecting and using human brain activity data, as with the NSD dataset used in this study."}, {"title": "Acknowledgement", "content": "Y.T. was supported by PRESTO Grant Number JP-MJPR2316. S.N. was supported by KAKENHI JP24H00619 and JST JPMJCR24U2."}, {"title": "Author Contribution", "content": "Y.T. and S.N. proposed the research direction and advised the project. T.M. conceived the method and conducted all analyses. T.M. wrote the original draft, and all authors contributed to the manuscript writing."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Full Related Work", "content": ""}, {"title": "A.1.1. INTERPRETING THE REPRESENTATIONS OF THE BRAIN'S NEURONS.", "content": "Encoding models have long been used in neuroscience to interpret neural representations within the brain. These studies used interpretable features, such as low-level visual attributes, or high-level semantic features, such as one-hot encoding of words, for straightforward voxel-wise interpretation.\nRecent approaches use features derived from DNNs and have demonstrated higher explanatory power for brain activity than those using simpler, more interpretable features. However, the interpretability of these DNN-based encoding models remains challenging, leading to the development of methods that condense the entire set of voxels into a small number of universal and interpretable axes.\nRecent approaches propose data-driven methods to describe the properties of individual brain voxels using natural language when analyzing brain representations at a finer, voxel-wise level. Brain-SCUBA is an end-to-end method that uses an existing image captioning model, which provides voxel-wise captions of the visual cortex in a data-driven manner. BrainSCUBA projects each voxel's encoding weight onto the image feature space via dot-product attention, identifies regions of highest similarity, and then uses a text decoder to generate captions describing the images to which the voxel is most selective. This approach provides a data-driven natural-language description of voxel selectivity without additional training. Similarly, SASC uses fMRI data collected during speech listening to identify the short phrases that most strongly activate each voxel. It then uses an LLM to combine these short phrases into a single, data-driven caption describing each voxel's semantic properties.\nOur proposed method also generates data-driven voxel captions but differs in several ways. First, BrainSCUBA is constrained to pre-existing, end-to-end image captioning models. In contrast, our approach divides the task into (i) identifying an optimal set of images and (ii) converting these images into a caption, allowing us to use any vision model aligned with language and any LLM with advanced language capabilities without requiring specialized fine-tuning. Furthermore, although SASC uses an LLM to create voxel captions, it primarily synthesizes short, low-information phrases (e.g., trigrams), producing only simple keyword-based captions. In contrast, our method summarizes more diverse and informative text and then uses these extracted keywords to compose a complete sentence, capturing a richer range of voxel-level properties."}, {"title": "A.1.2. INTERPRETING THE REPRESENTATIONS OF ARTIFICIAL NEURONS IN DNNS", "content": "Interpreting artificial neurons is a key challenge in understanding how DNNs process information. We can potentially examine human neural representations at a finer granularity by applying the data-driven and highly accurate interpretation methods developed for artificial neurons to analyze human brain voxels.\nNumerous studies have aimed to associate artificial neurons with human-interpretable concepts. These methods link neurons to textual concepts by comparing neuron output feature maps with outputs from segmentation models. However, these approaches are constrained by predefined concept sets or limited to the dataset's words and phrases. MILAN introduced a generative approach, enabling adaptation to different domains and tasks, but it requires annotated data, which poses challenges for scalable applications.\nLLMs permit open-ended descriptions of artificial neurons without additional model training. Analogous to these methods, our study also leverages LLMs to generate open-ended concepts for brain neurons rather than artificial neurons, seeking flexible and diverse interpretations that do not depend on predefined vocabularies."}, {"title": "A.2. Implementaion Details", "content": ""}, {"title": "A.2.1. GENERATING VOXEL CAPTIONS", "content": "In this study, we generate sentence-level captions from keywords by leveraging the \u201cSentence Composer\u201d proposed in the image captioning model, MeaCap\u2014referred to in the MeaCap paper as a \u201ckeywords-to-sentence LM.\""}, {"title": "A.2.2. PRETRAINED CHECKPOINTS", "content": "Our experiments use the CLIP ViT-B/32 weights provided on GitHub for encoding, caption generation, and image-level caption evaluation. For captioning each voxel's optimal image set, we use MLLM model MiniCPM-Llama3-V2.5, available on Hugging Face. We extract keywords from the resulting optimal captions using gpt-4o (gpt-4o-2024\u201308\u201306) via the OpenAI API for the primary analysis and Llama3.1\u201370B from Hugging Face for the ablation study (A.3). For the sentence-level caption evaluation, we calculate sentence similarity using Sentence-BERT from Hugging Face. Moreover, for image-level caption evaluation, we generate voxel images from voxel captions using an image generation model on Hugging Face."}, {"title": "A.2.3. BRAINSCUBA", "content": "Because the BrainSCUBA codebase is not publicly available, we implemented it ourselves for this study. In BrainSCUBA, the encoding weights (linear layers) are learned using gradient descent. In our implementation, consistent with our proposed method, we trained the encoding weights using L2-regularized linear regression from the himalaya library package.\nMoreover, BrainSCUBA projects each voxel's encoding weight into image space using a dataset of 2 million images, combining OpenImages and LAION-A v2 (6+ subset). However, the specific images selected from each dataset are not disclosed. We ensure a fair and dataset-independent comparison by relying solely on the 1.7 million images from OpenImages (the same dataset used by our proposed method, LaVCa). We leverage the training set of the subset that is accompanied by bounding boxes, object segmentations, visual relationships, and localized narratives.\nFor other hyperparameters, we tested temperature values of 1.0, 1/10, 1/100, 1/150 (the value used in the BrainSCUBA paper), and 1/500 for the softmax projection. We used beam search with a beam width of 5 to generate the text decoder's caption as described in the BrainSCUBA paper."}, {"title": "A.3. Ablation Study", "content": "We conduct an ablation study to evaluate how hyperparameter choices affect our method's accuracy. First, we vary the number of optimal images used for keyword extraction with gpt-4o from 5 to 10, 50, and 100 while fixing the number of extracted keywords at 5 (as in the primary analysis). Increasing the number of optimal images up to 50 leads to higher accuracy. This gain likely arises because relying only on top-ranked images may omit useful second- and third-ranked keywords; increasing the number of optimal images enables us to capture a broader range of selective keywords. However, after the number of optimal images reaches 100, the improvement ceases, likely because additional concepts cannot be adequately captured using only five keywords. These observations suggest that increasing the number of concepts could further improve accuracy.\nNext, we compare accuracy when the number of extracted keywords from the optimal image set (using gpt-4o) is varied among 1, 5, and 10 while fixing the number of optimal images at 50 as in the primary analysis. The results illustrate that increasing the number of output concepts from 1 to 5 boosts accuracy while pushing this number to 10 reduces accuracy. This decline may be caused by extracting irrelevant or noisy concepts. The improvement with five keywords suggests that voxels encode multiple concepts, but extracting too many can introduce noise. Increasing the number of optimal images, rather than keywords, might be a more effective method of capturing additional helpful concepts.\nFinally, we compare three models for keyword extraction from the optimal image set: the large language model gpt-4o, an 8-bit quantized Llama3.1\u201370B (meta-llama/Llama-3.1\u201370B-Instruct), and the TextGraphParser used in MeaCap. For gpt-4o and Llama3.1\u201370B-8bit, we use 50 optimal images and extracted five keywords, matching the primary analysis. For TextGraphParser, we use MeaCap's default settings, extracting five image captions and up to four concepts per caption. Experimental results demonstrate that gpt-4o outperforms TextGraphParser, suggesting that an open-ended LLM-based approach to concept extraction is more effective than simply adopting words from captions, as TextGraphParser does. Moreover, comparing gpt-4o with Llama3.1\u201370B-8bit reveals that a higher-performance LLM can further boost accuracy. These findings imply that, as LLM capabilities continue to advance, so too does our framework's potential to enhance the interpretability of voxel representations."}]}