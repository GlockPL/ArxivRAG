{"title": "When In-memory Computing Meets Spiking Neural Networks: A Perspective on Device-Circuit-System-and-Algorithm Co-design", "authors": ["Abhishek Moitra", "Abhiroop Bhattacharjee", "Yuhang Li", "Youngeun Kim", "Priyadarshini Panda"], "abstract": "This review explores the intersection of bio-plausible artificial intelligence in the form of Spiking Neural Networks (SNNs) with the analog In-Memory Computing (IMC) domain, highlighting their collective potential for low-power edge computing environments. Through detailed investigation at the device, circuit, and system levels, we highlight the pivotal synergies between SNNs and IMC architectures. Additionally, we emphasize the critical need for comprehensive system-level analyses, considering the inter-dependencies between algorithms, devices, circuit & system parameters, crucial for optimal performance. An in-depth analysis leads to identification of key system-level bottlenecks arising from device limitations which can be addressed using SNN-specific algorithm-hardware co-design techniques. This review underscores the imperative for holistic device to system design space co-exploration, highlighting the critical aspects of hardware and algorithm research endeavors for low-power neuromorphic solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) has been at the forefront of technological innovation over the past decade. From the development of deep convolutional neural networks that have revolutionized computer vision to the emergence of transformers and large language models that have transformed natural language processing, each generation of AI algorithm represents a significant leap in our ability to harness the power of data. The growth of AI is mainly attributed to the scale-up of high power, server-class computing machines such as graphics processing units (GPUs). But, GPUs draw a substantial amount of power, leading to increased operational costs and a larger carbon footprint. As AI aims to become more ubiquitous and user-centric, there is a growing need for low-power AI algorithms and hardware accelerators. This shift is essential to move away from the current trend of expecting increased intelligence merely by scaling up compute/memory resources, which is neither sustainable nor practical for widespread deployment. However, this vision stands in contrast to the current algorithm and hardware progress trajectory, where the computational needs of AI algorithms are doubling every two months, far surpassing Moore's law of silicon scaling by a considerable margin.\nTo this end, neuromorphic computing algorithms such as Spiking Neural Networks (SNNs) leveraging brain-like computations have emerged as a suitable candidate towards low-power AI implementation. An SNN contains numerous Leaky-Integrate-and-Fire (LIF) neurons that store the spatio-temporal information in the form of membrane potential values over multiple timesteps. The information from one neuron is relayed to another neuron in the form of binary spikes. Overall, due to the sparse event driven binary processing capabilities, SNNs show promise for low power edge computing applications, such as, in-sensor processing, embedded intelligence, among others.\nIn fact, SNNs are increasingly being embraced by different industries for commercial products in image classification, optimization, agriculture, and autonomous driving, signaling widespread adoption and innovation potential. In recent years, SNNs have achieved comparable accuracy with standard Artificial Neural Networks (ANNs) in large-scale tasks such as image classification on the Imagenet-1K dataset. This has necessitated SNNs to scale up in terms of parameter count requiring significant compute and memory resources. Thus, their implementation on the emerging low-power hardware acceleration paradigm, In-Memory Computing (IMC), shows great promise as IMC facilitates highly parallel computation with high memory bandwidth.\nTraditional von-Neumann style accelerators such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) suffer from \"memory wall bottleneck\" owing to the heavy data movement (specifically, weights) between memory and compute units corresponding to the dot-product operations in neural networks. IMC with non-volatile memories facilitates analog dot-product operations while keeping the weights stationary on crossbars, thereby reducing the weight movement bottleneck significantly. Further, the costly digital multiplier-accumulator circuits of von-Neumann accelerators are reduced to analog crossbar operation on IMC leading to energy and area efficiency. Importantly, SNNs exhibit tight synergies with IMC hardware. Due to the high spike sparsity (90% across different layers) and binary spike computations, SNNs implemented on IMC require low peripheral and data communication overhead that yield low-power and high-throughput benefits."}, {"title": "II. SNN ALGORITHM AND APPLICATION SPACE", "content": "SNNs inherently possess several key efficiencies that are critical towards low power edge implementation.\n(1) Binary Spike Processing: Taking cues from the brain, SNNs perform binary spike-driven data processing over multiple timesteps. As a result, Multiply-and-Accumulate (MAC) operations are merely reduced to efficient accumulation operations.\n(2) Spatio-temporal Complexity: At each timestep, the input spikes and SNN weights undergo spatial convolution yielding dot-product outputs. SNNs use a special non-linearity function called Leaky-integrate and Fire (LIF). The dynamics of an LIF neuron is shown in Equation 1. For neuron i, the membrane potential U is charged by the weighted summation of spikes from the previous layer's neuron j at every timestep t. Also, the leak factor $\\lambda \\in (0, 1)$ facilitates temporal leakage of the membrane potential.\n$U_{i}^{t} = \\lambda U_{i}^{t-1} + \\sum w_{ij}o_{j}^{t}.$ (1)\nIf at any timestep, the value U exceeds a particular threshold $\\theta$, the neuron generates a spike output and vice-versa as shown in Equation 2.\no_{i}^{t} =\n\\begin{cases}\n1, & \\text{if } U > \\theta, \\\\\n0 & \\text{otherwise}.\n\\end{cases}$ (2)\nThis has been described in Fig. 2. It is worth noting that the number of timesteps for processing the neuronal dynamics will eventually determine the overall performance of an SNN. Generally, SNNs with high timesteps yield better accuracy than that of SNNs with less timesteps. But, larger timesteps also translate to higher latency and energy consumption on hardware. As we will see later (in Fig. 8), timesteps become a critical control knob to determine the overall energy-vs.-accuracy tradeoff while designing SNNs.\n(3) Data Sparsity: The LIF neuron activation yields high spike sparsity. This means that SNNs can represent data with very few spikes. At any given timestep, around 90% of the neurons in an SNN are not spiking. Compared to ANNs with the ReLU activation, SNNs exhibit at least 30-40% higher neuronal sparsity.\n(4) Event-driven Computation: Due to the high spike sparsity, leveraging event-driven computation and communication can significantly improve the energy-efficiency of hardware accelerators."}, {"title": "B. State-of-the-art SNN Training Algorithms", "content": "In this section, we cover SNN training algorithms, including conventional learning algorithms like unsupervised Hebbian learning or Spike Timing-Dependent Plasticity (STDP), ANN-SNN Conversion, and modern learning algorithms like Backpropagation Through Time (BPTT). We will discuss the scalability and practicality of each algorithm.\n1. Conventional Learning Algorithms\nSTDP Learning: The Spike Timing-Dependent Plasticity can be viewed as a spike-based formulation of a Hebbian learning rule, where the synaptic weight is updated based on the tight temporal correlation of the firing time of the input and the output spikes. With STDP, if the presynaptic neuron fires briefly before the postsynaptic neuron (i.e., if the output spike timing is shortly after the input spike timing), the weight connecting them is strengthened. Otherwise, if the timing is reversed, the weight connection is weakened. This strengthening and weakening is called Long-Term Potentiation (LTP) and Long-Term Depression (LTD), respectively. STDP is a fully unsupervised learning algorithm that does not need a loss objective to update the weight neurons. Therefore, the SNNs trained with STDP are usually limited to thousands of neurons which limits the scalability of of this method. Recent advances propose to incorporate STDP with supervised learning to increase their scalability to complex tasks.\nANN-SNN Conversion: Given that ANN training is easier than SNN training, a straightforward way to obtain the SNN is to first train an ANN with the same architecture and convert the neurons into spiking neurons. The conversion process typically involves finding the optimal threshold values ($\\theta$ in Eq. 2) for the membrane potential of the spiking neurons and scaling of the weights such that the spike rate in SNNs match the floating-point outputs of ANNs. In a different ANN-SNN conversion method, Han et al. achieved improved convergence and higher accuracy for converted SNNs by performing ANN-SNN conversion without resetting the LIF neuron. The ANN-SNN conversion shares several advantages compared to direct training of SNNs. For example, ANN training is easy to implement since the computing hardware (e.g., GPUs) and the deep-learning library (e.g., PyTorch) are well-established. In addition, the conversion process is also simple as it only involves changing the neuron type of the model. However, this method also incurs several disadvantages: (1) The converted SNN requires significantly larger timesteps to realize the original ANN performance. Large timesteps will translate to high latency or energy consumption on hardware implementation; (2) The converted SNN shares the same architecture with the ANNs, which is not tailored for the spike-based mechanism and does not fully utilize the temporal dynamics of SNNs.\n2. Back Propagation Through Time\nPrevious methods like conversion or unsupervised learning suffer from either large timesteps or low scalability. The BPTT algorithm can address these two challenges. BPTT usually trains an SNN from scratch where the gradients are computed in a timestep-unrolled computation graph (see Fig. 3a). Formally, the gradient of the weight $w_{ij}$, denoted by $\\Delta w_{ij}$, is accumulated over T timesteps as:\n$\\Delta w_{ij} = \\sum_{t=1}^{T} \\frac{\\partial l}{\\partial w_{ij}}$ (3)\nwhere, $l$ is the loss function being optimized. In the case of image classification, categorical cross-entropy loss is widely used. The challenge of applying gradient descent in SNNS is that the spike function returns zero gradient almost everywhere because of the thresholding function. Surrogate gradient descent overcomes this problem by approximating the spike function into piece-wise linear, fast sigmoid or exponential function. For instance, the surrogate gradient descent method using a piece-wise linear approximation is defined as:\n$\\frac{\\partial o}{\\partial U} = \\xi max{0, 1 - \\frac{U - \\theta}{\\theta}} ,$ (4)\nwhere, $\\xi$ is a decay factor for back-propagated gradients and $\\theta$ is the threshold value. The hyperparameter $\\xi$ should be set based on the total number of timesteps T. BPTT-based SNN reaches state-of-the-art accuracy on various tasks such as image recognition and event data processing at fewer timesteps. Hybrid training combines BPTT and ANN-SNN conversion in order to achieve higher accuracy. In Fig. 3b, we illustrate the accuracy of various SNN training methods over the last decade. Evidently, BPTT-based training exhibits high accuracy while scaling to large-scale datasets such as Imagenet at low timestep overhead."}, {"title": "C. Application Space for SNNs", "content": "This section reviews the recent academic studies and commercial application space of SNNs (summarized in Table I).\nIn-sensor Processing and Low Power Healthcare: Due to their inherently sparse and binary spike representation, SNNs effectively reduce the bandwidth requirements for inter-chip interfacing. Works by Shaaban et al. and MacLean et al. employ efficient time-domain processing to replace computationally intensive preprocessing steps, while Zhou et al. and Barchi et al. directly interface sensors with SNNs for in-sensor processing.\nThe low power nature of SNNs have also been leveraged in wearable healthcare devices. NeuroCARE and Mosaic offer tailored neuromorphic healthcare frameworks. Bian et al. and Li et al. utilize SNNs for human activity recognition in wearables, and Tanzarella et al. detect spinal motor neuron activity. Additionally, SNNs in Brain-Computer Interface (BCI) applications, explored by Gong et al. and Feng et al. leverage their energy-efficiency for Electroencephalography (EEG) analysis.\nEmergence of Event-driven and Spike Cameras: Recent research in emerging vision cameras has broadened the applicability of SNNs, including object detection tasks. Initiatives like Spiking YOLO pioneered ANN-SNN conversion, enhancing object detection efficiency across various datasets. Subsequent works, like Su et al., achieved state-of-the-art object detection via full-scale SNN training. In automotive applications, Lopez et al. effectively utilized SNNs' sparse data representations for artifact detection. Salvatore et al. demonstrated SNN robustness against noise and their effectiveness in satellite detection using event cameras. Amir et al. Maro et al. and Vasudevan et al. have performed gesture recognition using event cameras. While Amir et al. implement their algorithm on the event-driven TrueNorth neuromorphic processor, Maro et al. implement their algorithm on an android smartphone. The DvsGesture, NavGesture and"}, {"title": "III. IMC ACCELERATORS FOR SNNS", "content": "In this section, we discuss the differences between von-Neumann and IMC architectures for inference applications. Note, the discussion in Section III and Section IV will focus on inference, and Section V will highlight some opportunities and challenges for training with IMC-SNNs.\nvon-Neumann Accelerators: Traditional von-Neumann AI accelerators such as GPUs and TPUs, contain an array of processing elements (PE). Each PE contains multipliers and accumulators that facilitate multi-bit MAC operations. For MAC operations, first, the weights/activation values are fetched from the off-chip DRAM memory to the input cache. Next, these values are transferred to the scratch pads of the PEs and the output is stored in the output cache and then sent back to the off-chip DRAM. During the inference of most modern deep learning networks such as ANNs and SNNs, there is a significant data exchange (in form of weights, activations and MAC outputs) between the DRAM and on-chip memories. The continual data movements and the constrained memory bandwidth contribute to the \"memory wall bottleneck\" in von-Neumann architectures. Additionally, the MAC computation occurs in a cycle-to-cycle fashion. These factors degrade the throughput and energy-efficiency of von-Neumann accelerators in edge-computing scenarios.\nIMC Dot-product Accelerators: To overcome the \"memory wall bottleneck\" in von-Neumann computing, IMC architectures co-locate the computation and memory units. IMC architectures feature 2D memristive crossbars, with Non-volatile Memory (NVM) devices situated at the cross-points. The NVM devices are interfaced in series with access transistors in a 1T-1R configuration to prevent sneak path currents in the crossbars. Some NVM devices predominantly used are Phase Change Memory (PCM), Resistive Random-Access Memory (RRAM), Spin-Torque-Transfer Magnetic RAM (STT-MRAM) and Ferroelectric Field-effect Transistor (FeFET). All the weights of a neural network are stored on the crossbar encoded as synaptic conductances in the NVM devices. This eliminates the weight specific data movement between memories as observed in von-Neumann architectures.\nFor MAC computations, the digital inputs are converted to analog voltages by the Digital-to-Analog Converter (DAC) and sent along the crossbar rows (or select-lines). These voltages get multiplied with the device conductances using Ohm's Law yielding currents which get accumulated over the crossbar column (or bit-line) according to Kirchoff's Current Law. The column currents represent the MAC operation result between inputs and weights. The Analog-to-Digital Converter (ADC) converts column currents into digital outputs. Due to the analog nature of computing, IMC architectures can facilitate highly parallel MAC computations per cycle in an energy and area-efficient manner."}, {"title": "B. Standard Hardware Evaluation Metrics", "content": "In evaluating the efficacy of AI hardware accelerators, several key metrics are paramount. These metrics provide a quantitative basis for comparing accelerator designs and are crucial for identifying areas of improvement.\n(1) Power Consumption and Latency: Power consumption in an accelerator comprises both dynamic and static components. Dynamic power refers to the power consumed by the accelerator during active computation, whereas static power is the power consumed at idle state. The inclusion of more hardware resources directly escalates both dynamic and static power consumption.\nLatency measures the time required for an AI workload to complete its execution on the accelerator for a given input. It is a critical factor in determining the speed at which the accelerator can process data, affecting its real-time performance and user experience.\n(2) Throughput (TOPS): As shown in Equation 5, throughput reflects the rate at which operations are executed per second in the accelerator. This metric is particularly relevant for AI accelerators, where an \"operation\" implies a multiply-and-accumulate computation.\nTOPS = $\\frac{\\text{Total number of operations (in Tera)}}{\\text{Latency}}$ (5)\n(3) Energy-efficiency (TOPS/W): Energy-efficiency, expressed as TOPS-Per-Watt (TOPS/W), gauges the number of operations an accelerator can perform per watt of power consumed. This metric is instrumental in assessing the sustainability and cost-effectiveness of an accelerator. It is computed as follows:\nTOPS/W = $\\frac{\\text{Total number of operations (in Tera)}}{\\text{Latency } \\times \\text{ Power}}$ (6)\nIt underscores the importance of optimizing both the computational throughput and power efficiency to enhance the overall performance of hardware accelerators.\n(4) Area-efficiency (TOPS/mm\u00b2): Area-efficiency, measured in TOPS-per-square-millimeter (TOPS/mm\u00b2), evaluates the computational density of an accelerator, showcasing how effectively it utilizes its physical space to perform operations. It is computed as follows:\nTOPS/mm\u00b2 = $\\frac{\\text{Total number of operations (in Tera)}}{\\text{Latency } \\times \\text{ Accelerator Area}}$ (7)\nIt underscores the accelerator's ability to maximize its computational output relative to its size, indicating the efficiency of hardware design in terms of area utilization."}, {"title": "C. Synergies between IMC Accelerators and SNNs", "content": "Energy-efficiency of IMC accelerators: Fig. 5a exhaustively compares the different AI acceleration platforms used today. While GPUs and CPUs offer extensive backend support (such as CUDA for Nvidia GPUs) for AI acceleration, their hardware architecture is fixed and not suitable for extremely low power applications (< 1W). To this end, systolic accelerators such as Eyeriss (denoted as Eyeriss ANN in Fig. 5a) have used ANN-centric dataflow and architecture modifications in order to achieve low power and energy-efficient acceleration. With IMC architecture (denoted as Neurosim ANN), the energy-efficiency is further improved. This is mainly attributed to the reduced weight data movement across memories and the analog dot-product operations.\nSNNs on IMC can further energy-efficiency: Evidently, due to the sparse and binary spike computations, SNNs can further improve the energy-efficiency and reduce power consumption in both systolic (SATA SNN) and IMC accelerators (SpikeSim SNN) compared to ANNs. To properly benchmark the improvements of SNNs, over ANNs, in this section, we have used SATA and SpikeSim for SNN implementation as they closely represent the ANN-implementation architectures of Eyeriss and Neurosim, respectively. However, in case of SNNs implemented on systolic accelerators like SATA, the SNN is still memory bottlenecked. This leads to a 13\u00d7 TOPS/W improvement at 6.5\u00d7 lower power compared to Eyeriss ANN. In contrast, the SNNs implemented on IMC architectures are not memory bound. Instead, IMC architectures typically suffer from the peripheral overhead of ADCs and communication circuits. Interestingly, as seen in Fig. 5b, SNNs possess high activation sparsity compared to ANNs. To this end, the highly sparse binary computation in SNNs can be heavily leveraged to attain extremely low ADC precision which in turn reduces the communication overhead. Thus, SpikeSim SNN (with algorithmic optimizations like MINT and DT-SNN explained in Section IV C) yields 26\u00d7 higher energy-efficiency at 4\u00d7 lower power compared to Neurosim ANN. The peripheral overhead reduction in SNN yields several synergistic benefits including 4\u00d7 lower area (Fig. 5c), 1.6\u00d7 higher TOPS"}, {"title": "IV. SYSTEM-LEVEL ANALYSES OF IMC-SNN", "content": "Often, large-scale deep SNNs mandate the need for multiple crossbars integrated with numerous digital peripheral modules. For accurate system-level analyses, IMC-realistic evaluation platforms are necessary. To this end, this section highlights the extensive research performed towards large-scale ANN implementations on IMC architectures. Thereafter, it highlights the key modifications that some of the recent SNN-IMC evaluation platforms entail to incorporate SNNs.\nANN-IMC Evaluation Platforms: In an early instance of ANN-based IMC deployment, ISAAC introduced a pipelined accelerator featuring on-chip embedded DRAM (eDRAM) for inter-stage data storage. The researchers conducted extensive design space exploration to determine an optimal configuration of memristor, ADCs, and eDRAM resources. In another study, MNSIM proposed a unified framework integrating analog and digital IMC platforms for ANN implementations. Additionally, Neurosim-v1 presented an end-to-end evaluation framework spanning device, circuit, and system levels for ANNs. The study validated simulation-based findings through post-tapeout testing, demonstrating minimal discrepancies between real and simulated outcomes. More recent endeavors, like SIAM, intro-"}, {"title": "B. Need for System-level Analyses of IMC-SNN", "content": "Device innovations have been system agnostic: Over the years, comprehensive research efforts have introduced numerous synaptic NVM devices showcasing plasticity akin to neurons in the brain. These devices aim to enable low-power unsupervised learning methods such as STDP on memristive crossbars. However, given the current scale of learning tasks, BPTT-based SNN training algorithms have become increasingly pervasive as shown in Section II B. Interestingly, BPTT-trained SNNs do not require plasticity-aware synaptic devices. In fact, today's device research is geared towards achieving multi-level synaptic devices with a greater number of stable conductance states, higher On/Off ratios and lower read voltages to avoid write disturbances and lower read energy during inference. Concurrent studies are focused on investigating neuro-mimetic properties in emerging NVM devices for emulating analog spiking neurons. New neuron models have also been proposed for improving SNN convergence and hardware implementations. This research aims to enable seamless integration with analog crossbar-based dot-product engines, contributing to the development of low-power neuromorphic systems. FeFET devices having tunable hysteretic behavior and low-power switching capabilities, have shown promise in emulating the firing patterns observed in biological neurons. Device researchers are also exploring the use of NVM devices like FeFETs as NVM memcapacitors for neuromorphic computing. Memcapacitive crossbars, unlike the memristive ones, perform analog dot-products in the charge-domain with low dynamic power at negligible static power dissipation. Also, the immunity of memcapacitive crossbars to sneak path currents eliminates the need for access transistors, thereby reducing design complexity and crossbar area. Thus far, research at the device level has proceeded in isolation, devoid of consideration for broader system-level impact, resulting in a discernible gap in the efficient deployment of SNNs on IMCs."}, {"title": "C. SNN System-level Bottlenecks and Mitigation Strategies", "content": "The intrinsic energy-efficiency of SNNs may be compromised without a thorough understanding of hardware bottlenecks. This section delineates the primary obstacles hindering the efficient integration of SNNs on IMC architectures.\n1. The LIF Neuron Module"}, {"title": "2. Temporal Computation in SNNs", "content": "As SNNs process data over multiple timesteps, an increase in timesteps linearly escalates the energy-delay product (EDP) across MAC, communication and LIF activation operations (Fig. 8a). Interestingly, the crossbar compute arrays, digital peripherals and the communication circuits get activated multiple times in a particular timestep in order to compute the weighted summation output. In contrast, the LIF activation is performed once every timestep. Therefore, the MAC and communication operations significantly contribute to the EDP\nCo-design based Timestep Minimization Strategies: Over the years, training algorithms such as BNTT, along with neuromorphic data augmentation techniques and encoding methods such as direct encoding, have effectively exploited the spatio-temporal complexity of SNNs resulting in a drastic reduction (of the order 10x) in the number of timesteps. A more recent approach by Li et al. called DT-SNN leverages the difficulty of the input images to scale the number of timesteps in the SNN. During training, the authors use a joint training loss to train an SNN on different count of timesteps. During inference, the authors use an entropy metric to determine the confidence of prediction per timestep. An image with lower entropy is deemed easy and inferred at an early timestep and difficult images with higher entropy are inferred at latter timesteps. The authors of DT-SNN achieve an overall 81% EDP reduction, with iso or higher accuracy than a standard SNN using fixed number of timesteps for inference across all inputs."}, {"title": "3. Vulnerability towards IMC Non-idealities", "content": "The practical implementation of NVM devices is constrained by finite conductance levels, limited On/Off resistances, and inherent non-idealities that can adversely affect the inference accuracy of AI workloads. Depending upon the origin, these non-idealities can be classified into device and circuit non-idealities as shown in Fig. 9.\n(1) Device Non-idealities: Stochastic read noise predominantly originates from random telegraphic noise, flicker noise (1/f noise), and thermal noise in the NVM synapses. Read noise is modelled as a Gaussian distribution around the programmed conductance during each read cycle, with a standard deviation ($\\sigma$) increasing with the NVM conductance. Structural relaxation within NVM devices over time leads to another non-ideality called temporal drift, influencing the retention of programmed conductance in crossbars. A popular model describing the temporal conductance drift is given as $G(t) = G_o * (t/t_o)^{-\\nu}$, where $G_o$ represents the initially programmed conductance at time $t_o$, and $\\nu$ denotes the drift coefficient. Stuck-at-fault is another non-ideality arising from fabrication defects or extensive crossbar utilization. Stuck-at-faults manifest into fixated NVM synapses in crossbars (to set or reset states), rendering them non-programmable. Despite the inherent robustness of NVM crossbars to variations, stuck-at-faults can significantly degrade the performance of ANN/SNN workloads.\n(b) Circuit-level Non-idealities: It includes the parasitic resistances in the crossbar metal lines denoted as $r_{par}$. During analog dot-product operations, the interconnect parasitics lead to stray IR drops, causing the output currents to deviate from their ideal values and resulting in substantial accuracy losses. Balancing crossbar size to improve parallelism during inference, while restraining the impact of resistive non-idealities, becomes a delicate trade-off. Furthermore, the presence of access transistors in series with NVM devices in 1T-1R synapses is crucial for eliminating sneak paths and incorrect programming of the NVM devices. Nonetheless, it introduces 1T-1R non-linearities arising from the non-linear I-V characteristics of the access transistors.\nEffect of IMC Non-ideality on SNN: For an 8-bit VGG16 SNN model trained with the CIFAR10 dataset, the impact of parasitic resistances and stochastic read noise on the hardware inference accuracy is shown in Fig. 10a. At higher timesteps (timesteps \u2265 4), we find the non-ideal inference accuracy declines dramatically, owing to significant non-ideality error accumulation over multiple timesteps of computations.\nNon-ideality-aware Training of SNNs: Ensuring robustness to crossbar non-idealities involves iterative offline-training of SNN models with noise injection using hardware-realistic noise models. Recently, the AIHWKit toolkit from IBM offers statistical empirical models for emulating device-level and parasistic resistive non-idealities for PCM crossbars in PyTorch. AIHWKit-based noise-aware training has demonstrated state-of-the-art hardware accuracies across diverse tasks, spanning computer vision to natural language processing. However, non-ideality-aware training is not scalable to today's large-scale SNN models. This is due to the bottleneck of noise-injection, especially for the input voltage-dependent resistive parasitic non-idealities, which escalates training latency. Additionally, SNNs incur high training costs due to computations across multiple time-steps. This has been depicted in Fig. 10b where, an iteration of non-ideality-aware training increases the training latency by greater than an order of magnitude compared to standard training. This underscores the need for training-less methods for non-ideality mitigation.\nTraining-less Non-ideality Mitigation Strategies: Recent methods have proposed transformations on NVM conductances during mapping onto crossbars, increasing the proportion of low conductance synapses to mitigate crossbar non-idealities. Based on this principle, the NICE engine in the SpikeSim framework shows significantly improved SNN inference accuracies on non-ideal crossbars with no additional hardware costs. In addition, a recent work has shown that simple noise-aware adaptation of the batch-normalization (BN) parameters of a BPTT-trained SNN can fully recover the inference accuracy lost due to the non-idealities. This is corroborated in Fig.10c across crossbar sizes of 32\u00d732 and 64\u00d764. Noise-aware BN adaptation is a fully weight-static approach, implying that NVM synapses need not be re-programmed or reconfigured during inference to mitigate non-idealities. Noise-aware BN adaptation incurs nearly an order of magnitude lower latency than one epoch of standard SNN training (see Fig. 10b).\nChoice of NVM Device for Non-ideality Mitigation: While RRAMs and PCMs are extensively studied for multi-level crossbar synapses, their susceptibility to read noise is a concern. In contrast, FeFET-based synapses, with increased CMOS-compatibility and high On/Off ratios (>100), show promise in reducing read noise. PCMs have exhibited high retention capabilities (> 10 years) for temporal drift, while FeFETs show poorer retention (~ 103 \u2013 104s) due to polarization degradation from charge traps, defects, and oxide breakdown. To minimize stray IR drops, synapses with high On resistance (typically > 100k\u03a9) are favoured. However, excessively high On resistances diminish the crossbar currents, impacting the readout by sense amplifiers or ADCs. This is corroborated in Fig. 10d, where increasing On resistance of the synaptic devices leads to higher accuracy for the VGG16 SNN on 64\u00d764 crossbars, while reducing the"}, {"title": "V. DISCUSSION AND FUTURE DIRECTIONS", "content": "ADC Precision\nADC Precision\n\nDevice Precision\nDevice Precision\n\nThe device community has always focused on targeting\n\nArea (mm2)\nArea (mm2)\n\n\n\nDevice Precision\nDevice Precision\nEnergy (mJ)\nEnergy (mJ)\n\nFig. 11. Figure showing the (a) trend of required ADC precision\nand crossbar count with increasing device precision. (b) the trend in\narea and energy upon increasing device precision. All evaluations are\nperformed on SpikeSim with 8-bit VGG16 SNN implemented using\nhardware parameters shown in Table III in the Appendix.\nhigher number of stable conductance values in NVM devices\nwithout considering broader system-level implications. One\nmight naturally assume that enhancing the precision of NVM\ndevices will reduce the number of crossbars (and their asso-\nciated peripherals) needed to implement SNN layers. How-\never, at higher device precisions, the ADC precision needs\nto increase (and hence, the ADC area and energy) to avoid\nquantization-errors in the accumulated column currents, re-\nsulting in an expanded area and energy at the system level.\nThe co-dependence of device precision with ADC precision\nis illustrated in Fig. 11a. For the 8-bit VGG16 SNN model,\nthe optimal NVM device and ADC precisions are found to be\n4 bits and 5 bits, respectively. This yields the best energy and\narea expenditures at the system-level as shown in Fig. 11b.\nTherefore, to attain considerable energy and area-efficiency,\nlarge device precision is not paramount. It must be noted that\nthese trends are IMC platform agnostic as they are solely gov-\nerned by the device precision.\nFeFETs as a Promising Device for  Cache\nIn light of the LIF area overhead discussed in Section\nIV C 1, utilizing an NVM device like FeFET for construct-\ning  cache could drastically reduce the LIF area by upto\n7\u00d7 compared to the traditional SRAM cache (see Fig. 12a).\nHowever, as illustrated in Fig. 12b, current FeFET technol-\nogy necessitates multiple write cycles for programming (Refer\nSection V C for details on writing into NVM devices), leading\nto 3\u00d7 greater write energy than that of SRAM caches. This\nincreased write energy stems from the need to perform  write\noperations over multiple timesteps during SNN infer-\nence. Despite FeFETs showing superior noise-resilience com-\npared to RRAMs and PCMs, FeFETs continue to display read\nand write variabilities, potentially decreasing the SNN accu-\nracy by 3-4% (Fig. 12c) across a range of datasets. Note,\nthe relatively short retention time of FeFETs (~ 103 \u2013 104s)\nis unlikely to pose concerns given that the LIF cache is up-\nADC Precision"}, {"title": "C. Opportunities for IMC-SNNs in Online Learning", "content": "In the recent years, there has been a growing interest in online learning on edge devices. Data privacy concerns make learning on edge devices imperative because it allows sensitive or personal information to be processed directly on-device, without sending it across the internet or to centralized servers. Furthermore, for applications requiring real-time or near-real-time responses, such as autonomous vehicles or emergency response systems, online learning can save huge communication latency and bandwidth as data does not need to be communicated back and forth across the internet. The contemporary SNNs demonstrate ability to make accurate predictions with minimal temporal samples. They are particularly suited for edge devices due to their ability to operate at low power and their efficiency in handling highly sparse time-series data, which is common in real-world sensory inputs. As discussed in Section IIIC, the strong synergies between SNNs and IMC crossbars, particularly the reduced communication and ADC overheads to process binary and sparse spike data, shows potential for employing SNNs on IMC crossbars for online learning.\nDevice Challenges towards SNN Online Learning: In online training, writing into NVM devices involves selecting specific synapses by applying pulses across rows (select-lines or SLs) and columns (bit-lines or BLs), followed by modulating voltage or current to adjust the synaptic conductance. Each write cycle can degrade the NVM device's material, affecting its lifespan, making high-endurance devices preferable. Additionally, as programming each device requires multiple pulses, write operations are delay and energy-intensive. This is demonstrated in Fig. 13a-b. Write challenges also stem from the stochastic write noise and asymmetric conductance updates in the NVM devices (see Fig. 13c), which, although negligible during inference, significantly impact weight re-programming during online learning. These non-idealities necessitate repeated write operations to achieve the desired conductance level, affecting the energy, speed, and device endurance.\nHardware Requisites for Online Learning: Write noise mitigation strategies, in general, include error correction codes, write verification-and-retry mechanisms, and structural advancements in the NVM devices. FeFETS are less susceptible to write noise compared to RRAMs & PCMs, owing to deterministic polarization-switching at low voltages. However, FeFETs in general show limited endurance (~ 104 \u2013 1010 cycles) owing to mobility degradation and charge trapping phenomena. The low endurance of FeFETs can become problematic during online learning as the weights of the SNN need to be updated frequently. Furthermore, RRAMS & PCMs rely on filament formation mechanism and high thermal energy for state change, respectively,"}, {"title": "VI. CONCLUSION", "content": "The review delineates the pivotal synergies between SNNs and IMC architectures, showcasing their efficacy in"}]}