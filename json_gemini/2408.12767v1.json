{"title": "When In-memory Computing Meets Spiking Neural Networks", "authors": ["Abhishek Moitra", "Abhiroop Bhattacharjee", "Yuhang Li", "Youngeun Kim", "Priyadarshini Panda"], "abstract": "This review explores the intersection of bio-plausible artificial intelligence in the form of Spiking Neural Networks (SNNs) with the analog In-Memory Computing (IMC) domain, highlighting their collective potential for low-power edge computing environments. Through detailed investigation at the device, circuit, and system levels, we highlight the pivotal synergies between SNNs and IMC architectures. Additionally, we emphasize the critical need for comprehensive system-level analyses, considering the inter-dependencies between algorithms, devices, circuit & system parameters, crucial for optimal performance. An in-depth analysis leads to identification of key system-level bottlenecks arising from device limitations which can be addressed using SNN-specific algorithm-hardware co-design techniques. This review underscores the imperative for holistic device to system design space co-exploration, highlighting the critical aspects of hardware and algorithm research endeavors for low-power neuromorphic solutions.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial Intelligence (AI) has been at the forefront of technological innovation over the past decade. From the development of deep convolutional neural networks1,2 that have revolutionized computer vision to the emergence of transformers\u00b3 and large language models that have transformed natural language processing, each generation of AI algorithm represents a significant leap in our ability to harness the power of data. The growth of AI is mainly attributed to the scale-up of high power, server-class computing machines such as graphics processing units (GPUs)5. But, GPUs draw a substantial amount of power, leading to increased operational costs and a larger carbon footprint56.As AI aims to become more ubiquitous and user-centric, there is a growing need for low-power AI algorithms and hardware accelerators. This shift is essential to move away from the current trend of expecting increased intelligence merely by scaling up compute/memory resources, which is neither sustainable nor practical for widespread deployment. However, this vision stands in contrast to the current algorithm and hardware progress trajectory, where the computational needs of AI algorithms are doubling every two months, far surpassing Moore's law of silicon scaling by a considerable margin7.\nTo this end, neuromorphic computing algorithms such as Spiking Neural Networks (SNNs) leveraging brain-like computations have emerged as a suitable candidate towards low-power AI implementation8\u201310. An SNN contains numerous Leaky-Integrate-and-Fire (LIF) neurons that store the spatio-temporal information in the form of membrane potential values over multiple timesteps. The information from one neuron is relayed to another neuron in the form of binary spikes. Overall, due to the sparse event driven binary processing capabilities, SNNs show promise for low power edge computing applications, such as, in-sensor processing11-14, embedded intelligence15-20, among others.\nIn fact, SNNs are increasingly being embraced by different industries for commercial products in image classification21, optimization22, agriculture23, and autonomous driving24, signaling widespread adoption and innovation potential. In recent years, SNNs have achieved comparable accuracy with standard Artificial Neural Networks (ANNs) in large-scale tasks such as image classification on the Imagenet-1K dataset25,26. This has necessitated SNNs to scale up in terms of parameter count requiring significant compute and memory resources. Thus, their implementation on the emerging low-power hardware acceleration paradigm, In-Memory Computing (IMC), shows great promise as IMC facilitates highly parallel computation with high memory bandwidth.\nTraditional von-Neumann style accelerators such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) suffer from \"memory wall bottleneck\" owing to the heavy data movement (specifically, weights) between memory and compute units corresponding to the dot-product operations in neural networks27,28. IMC with non-volatile memories facilitates analog dot-product operations while keeping the weights stationary on crossbars, thereby reducing the weight movement bottleneck significantly29,30. Further, the costly digital multiplier-accumulator circuits of von-Neumann accelerators are reduced to analog crossbar operation on IMC leading to energy and area efficiency. Importantly, SNNs exhibit tight synergies with IMC hardware. Due to the high spike sparsity (90% across different layers31) and binary spike computations, SNNs implemented on IMC require low peripheral and data communication overhead that yield low-power and high-throughput benefits7,32."}, {"title": "II. SNN ALGORITHM AND APPLICATION SPACE", "content": "SNNs inherently possess several key efficiencies that are critical towards low power edge implementation."}, {"title": "A. Inherent Efficiencies in SNNs", "content": "(1) Binary Spike Processing: Taking cues from the brain, SNNs perform binary spike-driven data processing over multiple timesteps. As a result, Multiply-and-Accumulate (MAC) operations are merely reduced to efficient accumulation operations31,39.\n(2) Spatio-temporal Complexity: At each timestep, the input spikes and SNN weights undergo spatial convolution yielding dot-product outputs. SNNs use a special non-linearity function called Leaky-integrate and Fire (LIF)9,33. The dynamics of an LIF neuron is shown in Equation 1. For neuron i, the membrane potential U is charged by the weighted summation of spikes from the previous layer's neuron j at every timestep t. Also, the leak factor \u03bb \u2208 (0, 1) facilitates temporal leakage of the membrane potential.\n$U_{i}^{t}= \\lambda U_{i}^{t-1}+\\sum w_{ij}o_{j}$.   (1)\nIf at any timestep, the value U exceeds a particular threshold \u03b8, the neuron generates a spike output and vice-versa as shown in Equation 2.\n$o_{i}^{t}= \\begin{cases} 1, & \\text{if } U > \\theta, \\\\ 0 & \\text{otherwise}. \\end{cases}$   (2)\nThis has been described in Fig. 2. It is worth noting that the number of timesteps for processing the neuronal dynamics will eventually determine the overall performance of an SNN40,41. Generally, SNNs with high timesteps yield better accuracy than that of SNNs with less timesteps. But, larger timesteps also translate to higher latency and energy consumption on hardware. As we will see later (in Fig. 8), timesteps become a critical control knob to determine the overall energy-vs.-accuracy tradeoff while designing SNNs.\n(3) Data Sparsity: The LIF neuron activation yields high spike sparsity. This means that SNNs can represent data with very few spikes. At any given timestep, around 90% of the neurons in an SNN are not spiking. Compared to ANNs with the ReLU activation, SNNs exhibit at least 30-40% higher neuronal sparsity9,33.\n(4) Event-driven Computation: Due to the high spike sparsity, leveraging event-driven computation and communication can significantly improve the energy-efficiency of hardware accelerators31,42,43"}, {"title": "B. State-of-the-art SNN Training Algorithms", "content": "In this section, we cover SNN training algorithms, including conventional learning algorithms like unsupervised Hebbian learning or Spike Timing-Dependent Plasticity (STDP), ANN-SNN Conversion, and modern learning algorithms like Backpropagation Through Time (BPTT). We will discuss the scalability and practicality of each algorithm."}, {"title": "1. Conventional Learning Algorithms", "content": "STDP Learning: The Spike Timing-Dependent Plasticity45 can be viewed as a spike-based formulation of a Hebbian learning rule46, where the synaptic weight is updated based on the tight temporal correlation of the firing time of the input and the output spikes. With STDP, if the presynaptic neuron fires briefly before the postsynaptic neuron (i.e., if the output spike timing is shortly after the input spike timing), the weight connecting them is strengthened. Otherwise, if the timing is reversed, the weight connection is weakened. This strengthening and weakening is called Long-Term Potentiation (LTP) and Long-Term Depression (LTD), respectively. STDP is a fully unsupervised learning algorithm that does not need a loss objective to update the weight neurons. Therefore, the SNNs trained with STDP are usually limited to thousands of neurons which limits the scalability of of this method47,48. Recent advances49 propose to incorporate STDP with supervised learning to increase their scalability to complex tasks.\nANN-SNN Conversion: Given that ANN training is easier than SNN training, a straightforward way to obtain the SNN is to first train an ANN with the same architecture and convert the neurons into spiking neurons. The conversion process typically involves finding the optimal threshold values (\u03b8 in Eq. 2) for the membrane potential of the spiking neurons and scaling of the weights such that the spike rate in SNNs match the floating-point outputs of ANNs50\u201352. In a different ANN-SNN conversion method, Han et al.53 achieved improved convergence and higher accuracy for converted SNNs by performing ANN-SNN conversion without resetting the LIF neuron. The ANN-SNN conversion shares several advantages compared to direct training of SNNs. For example, ANN training is easy to implement since the computing hardware (e.g., GPUs) and the deep-learning library (e.g., PyTorch) are well-established. In addition, the conversion process is also simple as it only involves changing the neuron type of the model. However, this method also incurs several disadvantages: (1) The converted SNN requires significantly larger timesteps to realize the original ANN performance. Large timesteps will translate to high latency or energy consumption on hardware implementation; (2) The converted SNN shares the same architecture with the ANNs, which is not tailored for the spike-based mechanism and does not fully utilize the temporal dynamics of SNNs34."}, {"title": "2. Back Propagation Through Time", "content": "Previous methods like conversion or unsupervised learning suffer from either large timesteps or low scalability. The BPTT algorithm can address these two challenges. BPTT usually trains an SNN from scratch where the gradients are computed in a timestep-unrolled computation graph (see Fig. 3a). Formally, the gradient of the weight w_{ij}, denoted by \u0394w_{ij}, is accumulated over T timesteps as:\n$\u0394w_{ij} = \\sum_{t=1}^{T}  \\frac{\\partial L}{\\partial U_{i}^{t}} \\frac{\\partial U_{i}^{t}}{\\partial o_{j}^{t-1}}  \\frac{\\partial o_{j}^{t-1}}{\\partial w_{ij}}$, (3)\nwhere, L is the loss function being optimized. In the case of image classification, categorical cross-entropy loss is widely used. The challenge of applying gradient descent in SNNS is that the spike function returns zero gradient almost everywhere because of the thresholding function. Surrogate gradient descent54\u201358 overcomes this problem by approximating the spike function into piece-wise linear, fast sigmoid or exponential function59. For instance, the surrogate gradient descent method using a piece-wise linear approximation is defined as:\n$\\frac{\\partial o}{\\partial U} = \\S max{0,1- \\frac{\\vert U - \\theta \\vert}{\\theta} }$, (4)\nwhere, S is a decay factor for back-propagated gradients and \u03b8 is the threshold value. The hyperparameter S should be set based on the total number of timesteps T. BPTT-based SNN reaches state-of-the-art accuracy on various tasks such as image recognition and event data processing at fewer timesteps. Hybrid training60 combines BPTT and ANN-SNN conversion in order to achieve higher accuracy. In Fig. 3b, we illustrate the accuracy of various SNN training methods over the last decade. Evidently, BPTT-based training exhibits high accuracy while scaling to large-scale datasets such as Imagenet at low timestep overhead."}, {"title": "C. Application Space for SNNs", "content": "This section reviews the recent academic studies and commercial application space of SNNs (summarized in Table I).\nIn-sensor Processing and Low Power Healthcare: Due to their inherently sparse and binary spike representation, SNNs effectively reduce the bandwidth requirements for inter-chip interfacing. Works by Shaaban et al.11 and MacLean et al.12 employ efficient time-domain processing to replace computationally intensive preprocessing steps, while Zhou et al.13 and Barchi et al. 14 directly interface sensors with SNNs for in-sensor processing.\nThe low power nature of SNNs have also been leveraged in wearable healthcare devices. NeuroCARE74 and Mosaic73 offer tailored neuromorphic healthcare frameworks. Bian et al.16 and Li et al.15 utilize SNNs for human activity recognition in wearables, and Tanzarella et al.17 detect spinal motor neuron activity. Additionally, SNNs in Brain-Computer Interface (BCI) applications, explored by Gong et al. 18 and Feng et al.78, leverage their energy-efficiency for Electroencephalography (EEG) analysis.\nEmergence of Event-driven and Spike Cameras: Recent research in emerging vision cameras has broadened the applicability of SNNs, including object detection tasks. Initiatives like Spiking YOLO10 pioneered ANN-SNN conversion, enhancing object detection efficiency across various datasets. Subsequent works, like Su et al.61, achieved state-of-the-art object detection via full-scale SNN training. In automotive applications, Lopez et al.62 effectively utilized SNNs' sparse data representations for artifact detection. Salvatore et al.63 demonstrated SNN robustness against noise and their effectiveness in satellite detection using event cameras. Amir et al.66 Maro et al.67 and Vasudevan et al.68 have performed gesture recognition using event cameras. While Amir et al.66 implement their algorithm on the event-driven TrueNorth21 neuromorphic processor, Maro et al.67 implement their algorithm on an android smartphone. The DvsGesture, NavGesture and"}, {"title": "III. IMC ACCELERATORS FOR SNNS", "content": "In this section, we discuss the differences between von-Neumann and IMC architectures for inference applications. Note, the discussion in Section III and Section IV will focus on inference, and Section V will highlight some opportunities and challenges for training with IMC-SNNs."}, {"title": "A. von-Neumann and IMC Accelerators", "content": "von-Neumann Accelerators: Traditional von-Neumann AI accelerators (shown in Fig. 4a), such as GPUs and TPUs, contain an array of processing elements (PE)27,28,83. Each PE contains multipliers and accumulators that facilitate multi-bit MAC operations. For MAC operations, first, the weights/activation values are fetched from the off-chip DRAM memory to the input cache. Next, these values are transferred to the scratch pads of the PEs and the output is stored in the output cache and then sent back to the off-chip DRAM. During the inference of most modern deep learning networks such as ANNs and SNNs, there is a significant data exchange (in form of weights, activations and MAC outputs) between the DRAM and on-chip memories. The continual data movements and the constrained memory bandwidth contribute to the \"memory wall bottleneck\" in von-Neumann architectures29,30. Additionally, the MAC computation occurs in a cycle-to-cycle fashion. These factors degrade the throughput and energy-efficiency of von-Neumann accelerators in edge-computing scenarios.\nIMC Dot-product Accelerators: To overcome the \"memory wall bottleneck\" in von-Neumann computing, IMC (shown in Fig. 4b) architectures co-locate the computation and memory units29,30. IMC architectures feature 2D memristive crossbars, with Non-volatile Memory (NVM) devices situated at the cross-points. The NVM devices are interfaced in series with access transistors in a 1T-1R configuration to prevent sneak path currents in the crossbars86,87. Some NVM devices predominantly used are Phase Change Memory (PCM) 88, Resistive Random-Access Memory (RRAM)89, Spin-Torque-Transfer Magnetic RAM (STT-MRAM)90 and Ferroelectric Field-effect Transistor (FeFET)91. All the weights of a neural network are stored on the crossbar encoded as synaptic conductances in the NVM devices. This eliminates the weight specific data movement between memories as observed in von-Neumann architectures.\nFor MAC computations, the digital inputs are converted to analog voltages by the Digital-to-Analog Converter (DAC) and sent along the crossbar rows (or select-lines). These voltages get multiplied with the device conductances using Ohm's Law yielding currents which get accumulated over the crossbar column (or bit-line) according to Kirchoff's Current Law. The column currents represent the MAC operation result between inputs and weights. The Analog-to-Digital Converter (ADC) converts column currents into digital outputs. Due to the analog nature of computing, IMC architectures can facilitate highly parallel MAC computations per cycle in an energy and area-efficient manner."}, {"title": "B. Standard Hardware Evaluation Metrics", "content": "In evaluating the efficacy of AI hardware accelerators, several key metrics are paramount. These metrics provide a quantitative basis for comparing accelerator designs and are crucial for identifying areas of improvement.\n(1) Power Consumption and Latency: Power consumption in an accelerator comprises both dynamic and static components. Dynamic power refers to the power consumed by the accelerator during active computation, whereas static power is the power consumed at idle state. The inclusion of more hardware resources directly escalates both dynamic and static power consumption.\nLatency measures the time required for an AI workload to complete its execution on the accelerator for a given input. It is a critical factor in determining the speed at which the accelerator can process data, affecting its real-time performance and user experience.\n(2) Throughput (TOPS): As shown in Equation 5, throughput reflects the rate at which operations are executed per second in the accelerator. This metric is particularly relevant for AI accelerators, where an \"operation\" implies a multiply-and-accumulate computation.\n$TOPS = \\frac{Total \\ number \\ of \\ operations \\ (in \\ Tera)}{Latency}$   (5)\n(3) Energy-efficiency (TOPS/W): Energy-efficiency, expressed as TOPS-Per-Watt (TOPS/W), gauges the number of operations an accelerator can perform per watt of power consumed. This metric is instrumental in assessing the sustainability and cost-effectiveness of an accelerator. It is computed as follows:\n$TOPS/W= \\frac{Total \\ number \\ of \\ operations \\ (in \\ Tera)}{Latency \\times Power}$   (6)\nIt underscores the importance of optimizing both the computational throughput and power efficiency to enhance the overall performance of hardware accelerators.\n(4) Area-efficiency (TOPS/mm\u00b2): Area-efficiency, measured in TOPS-per-square-millimeter (TOPS/mm\u00b2), evaluates the computational density of an accelerator, showcasing how effectively it utilizes its physical space to perform operations. It is computed as follows:\n$TOPS/mm^2 = \\frac{Total \\ number \\ of \\ operations \\ (in \\ Tera)}{Latency \\times Accelerator \\ Area}$   (7)\nIt underscores the accelerator's ability to maximize its computational output relative to its size, indicating the efficiency of hardware design in terms of area utilization."}, {"title": "C. Synergies between IMC Accelerators and SNNs", "content": "Energy-efficiency of IMC accelerators: Fig. 5a exhaustively compares the different AI acceleration platforms used today. While GPUs and CPUs offer extensive backend support (such as CUDA92 for Nvidia GPUs) for AI acceleration, their hardware architecture is fixed and not suitable for extremely low power applications (< 1W). To this end, systolic accelerators such as Eyeriss83,84 (denoted as Eyeriss ANN in Fig. 5a) have used ANN-centric dataflow and architecture modifications in order to achieve low power and energy-efficient acceleration. With IMC architecture (denoted as Neurosim ANN), the energy-efficiency is further improved. This is mainly attributed to the reduced weight data movement across memories and the analog dot-product operations.\nSNNs on IMC can further energy-efficiency: Evidently, due to the sparse and binary spike computations, SNNs can further improve the energy-efficiency and reduce power consumption in both systolic (SATA SNN31) and IMC accelerators (SpikeSim SNN40) compared to ANNs. To properly benchmark the improvements of SNNs, over ANNs, in this section, we have used SATA31 and SpikeSim40 for SNN implementation as they closely represent the ANN-implementation architectures of Eyeriss83,84 and Neursosim85, respectively. However, in case of SNNs implemented on systolic accelerators like SATA31, the SNN is still memory bottlenecked. This leads to a 13\u00d7 TOPS/W improvement at 6.5\u00d7 lower power compared to Eyeriss83,84 ANN. In contrast, the SNNs implemented on IMC architectures are not memory bound. Instead, IMC architectures typically suffer from the peripheral overhead of ADCs and communication circuits. Interestingly, as seen in Fig. 5b, SNNs possess high activation sparsity compared to ANNs. To this end, the highly sparse binary computation in SNNs can be heavily leveraged to attain extremely low ADC precision which in turn reduces the communication overhead. Thus, SpikeSim SNN40 (with algorithmic optimizations like MINT39 and DT-SNN41 explained in Section IV C) yields 26\u00d7 higher energy-efficiency at 4\u00d7 lower power compared to Neurosim ANN. The peripheral overhead reduction in SNN yields several synergistic benefits including 4\u00d7 lower area (Fig. 5c), 1.6\u00d7 higher TOPS"}, {"title": "IV. SYSTEM-LEVEL ANALYSES OF IMC-SNN", "content": "Often, large-scale deep SNNs mandate the need for multiple crossbars integrated with numerous digital peripheral modules. For accurate system-level analyses, IMC-realistic evaluation platforms are necessary. To this end, this section highlights the extensive research performed towards large-scale ANN implementations on IMC architectures. Thereafter, it highlights the key modifications that some of the recent SNN-IMC evaluation platforms40 entail to incorporate SNNs."}, {"title": "A. IMC Hardware Evaluation Platform", "content": "ANN-IMC Evaluation Platforms: In an early instance of ANN-based IMC deployment, ISAAC93 introduced a pipelined accelerator featuring on-chip embedded DRAM (eDRAM) for inter-stage data storage. The researchers conducted extensive design space exploration to determine an optimal configuration of memristor, ADCs, and eDRAM resources. In another study, MNSIM94 proposed a unified framework integrating analog and digital IMC platforms for ANN implementations. Additionally, Neurosim-v185 presented an end-to-end evaluation framework spanning device, circuit, and system levels for ANNs. The study validated simulation-based findings through post-tapeout testing, demonstrating minimal discrepancies between real and simulated outcomes95. More recent endeavors, like SIAM%, intro-"}, {"title": "SNN-IMC Evaluation Platforms:", "content": "Over the previous years, there have been several IMC-based SNN platforms. Liu et al.97 proposed an analog crossbar approach for implementing feedforward and Hopfield networks, devoid of convolutional network-based dataflow. Narayanan et al.98 and Zhao et al.99 have constructed small feedforward SNNs trained using STDP algorithms. Bohnsting et al.100 and ReSPARC43, along with Kulkarni et al.101, have undertaken large-scale SNN deployments employing analog synapses and neurons, though these implementations lack open-source availability. To this end, SpikeSim40 proposes the first open-source end-to-end IMC hardware evaluation platform for benchmarking large-scale SNNs. This review will extensively utilize SpikeSim to perform end-to-end system-level analyses of IMC-implemented SNNs. It should be noted that, although the results are based on SpikeSim, the analyses are applicable to implementing an SNN on any IMC architecture.\nSpikeSim- An SNN-IMC Evaluation Platform: Similar to prior ANN works, SpikeSim40 contains a tiled hierarchical architecture containing tiles, Processing Elements (PEs) and crossbars as shown in Fig. 6a. The tiles are connected by a network-on-chip (NoC) interconnect, while the PEs and crossbars are connected by H-Tree interconnects. The crossbars, PE and Tiles work in tandem to compute the MAC output at a particular timestep. Accumulators at each hierarchy add the partial sums to deliver the final MAC output.\nSpikeSim entails several architectural modifications for implementing SNNs. Firstly, the authors implement a digital neuron module that facilitates the LIF activation function. The LIF module (see Fig. 6b) is implemented at the global hierarchy and contains a Umem cache memory to store the membrane potential values over multiple timesteps. At each timestep, the membrane potential value is read from the Umem cache, added to the MAC output of the current timestep and written back. Secondly, the authors leverage the binary spike nature of SNNs to replace the dual-crossbar approach with a cost-efficient digital DIFF module to carry out signed MAC operations. Finally, the authors employ an SNN-specific layer-scheduled dataflow that improves the throughput and hardware utilization of IMC-implemented SNNs compared to the tick-batched dataflow42 used in SNN-specific systolic array accelerators.\nDuring inference, the SNN model weights are partitioned onto multiple crossbars and the layer-scheduled dataflow is applied. Simultaneously, SpikeSim further optimizes the"}, {"title": "B. Need for System-level Analyses of IMC-SNN", "content": "Device innovations have been system agnostic: Over the years, comprehensive research efforts have introduced numerous synaptic NVM devices showcasing plasticity akin to neurons in the brain102\u2013105. These devices aim to enable low-power unsupervised learning methods such as STDP on memristive crossbars. However, given the current scale of learning tasks, BPTT-based SNN training algorithms have become increasingly pervasive as shown in Section II B. Interestingly, BPTT-trained SNNs do not require plasticity-aware synaptic devices. In fact, today's device research is geared towards achieving multi-level synaptic devices with a greater number of stable conductance states91, higher On/Off ratios106 and lower read voltages to avoid write disturbances and lower read energy during inference106\u2013108. Concurrent studies are focused on investigating neuro-mimetic properties in emerging NVM devices for emulating analog spiking neurons19,109\u2013113. New neuron models have also been proposed for improving SNN convergence and hardware implementations11,53. This research aims to enable seamless integration with analog crossbar-based dot-product engines, contributing to the development of low-power neuromorphic systems. FeFET devices having tunable hysteretic behavior and low-power switching capabilities, have shown promise in emulating the firing patterns observed in biological neurons19,114. Device researchers are also exploring the use of NVM devices like FeFETs as NVM memcapacitors for neuromorphic computing115\u2013117. Memcapacitive crossbars, unlike the memristive ones, perform analog dot-products in the charge-domain with low dynamic power at negligible static power dissipation. Also, the immunity of memcapacitive crossbars to sneak path currents eliminates the need for access transistors, thereby reducing design complexity and crossbar area118. Thus far, research at the device level has proceeded in isolation, devoid of consideration for broader system-level impact, resulting in a discernible gap in the efficient deployment of SNNs on IMCs."}, {"title": "C. SNN System-level Bottlenecks and Mitigation Strategies", "content": "The intrinsic energy-efficiency of SNNs may be compromised without a thorough understanding of hardware bottlenecks. This section delineates the primary obstacles hindering the efficient integration of SNNs on IMC architectures."}, {"title": "1. The LIF Neuron Module", "content": "VGG16 SNN trained on CIFAR1044 (image dimensions of 32\u00d732), Caltech-101119 (image dimensions of 48\u00d748) and TinyImagenet120 (image dimensions of 64\u00d764) datasets, respectively. Evidently, due to the large Umem (refer Fig. 6b) cache memory, the LIF neuronal module contributes 11.7%-25% towards the overall chip area. The LIF module thus, poses a bottleneck when implementing SNNs trained on large datasets like ImageNet25 with image dimensions (224\u00d7224 and 384\u00d7384) on IMC architectures. Interestingly, the LIF module requires > 1000\u00d7 higher on-chip area compared to ReLU module in ANNs that merely requires a comparator.\nCo-design based Mitigation Strategies: In SpikeSim40, the authors propose a simple approach of channel scaling wherein, the number of output channels in the first layer of the network are scaled down yielding 2\u00d7 reduction in LIF module area. In MINT39, the authors apply sophisticated weight and membrane potential quantization-aware SNN training to reduce the LIF overhead. MINT is able to reduce the weight and membrane potential precision as low as 2 bits while maintaining iso-accuracy with SNN trained on FP32 precision. Additionally, another recent work121 performed sharing of LIF membrane potentials over multiple SNN layers to reduce the LIF memory overhead. The authors perform inter-layer and intra-layer membrane potential sharing to achieve over 4\u00d7 reduction in the LIF memory area at iso-accuracy.\nDevice Research for LIF Area Mitigation: Device researchers are exploring novel neuromimetic devices emulating biological neuronal functionalities. Recent works have leveraged the fast and low-power switching dynamics of a FeFET-based relaxation oscillator configuration to generate biological spiking patterns at area-efficient form factors19,114. In another work Mohanan et al. 122 used nanoporous graphene-based memristive devices to compactly emulate LIF neurons in current SNN workloads showing threshold control, leaky integration and reset behaviors. The spiking activity of the LIF neuron is tunable by varying various circuit and device parameters, allowing it to cover a broad frequency spectrum122. Likewise, Zhou et al.123 have proposed a compact RRAM-based LIF neuron circuit closely integrated with analog RRAM crossbars. This provided a unified path to carry out dot-products and LIF activation functionalities in the analog domain. However, given the large number of spatial channels required by large-scale BPTT-trained SNN models, directly integrating the analog LIF neurons with the crossbars remains an unsolved problem."}, {"title": "2. Temporal Computation in SNNs", "content": "As SNNs process data over multiple timesteps, an increase in timesteps linearly escalates the energy-delay product (EDP) across MAC, communication and LIF activation operations (Fig. 8a). Interestingly, the crossbar compute arrays, digital peripherals and the communication circuits get activated multiple times in a particular timestep in order to compute the weighted summation output. In contrast, the LIF activation is performed once every timestep. Therefore, the MAC and communication operations significantly contribute to the EDP (80% of the overall EDP). Consequently, reducing the number of timesteps can significantly enhance efficiency. However, Fig. 8b illustrates a trade-off between timesteps, energy-efficiency and accuracy. While reducing timesteps improves energy-efficiency, it also leads to lower SNN accuracy. Therefore, development of effective algorithms exploiting spatio-temporal complexity is imperative. Note that while Fig. 8 uses SpikeSim for evaluation, the timestep is an intrinsic parameter of the SNN algorithm. Consequently, the linear increase in EDP with timesteps will remain consistent regardless of the IMC platform.\nCo-design based Timestep Minimization Strategies: Over the years, training algorithms such as BNTT9, along with neuromorphic data augmentation techniques124 and encoding methods such as direct encoding125, have effectively exploited the spatio-temporal complexity of SNNs resulting in a drastic reduction (of the order 10x) in the number of timesteps. A more recent approach by Li et al.41 called DT-SNN leverages the difficulty of the input images to scale the number of timesteps in the SNN. During training, the authors use a joint training loss to train an SNN on different count of timesteps. During inference, the authors use an entropy metric to determine the confidence of prediction per timestep. An image with lower entropy is deemed easy and inferred at an early timestep and difficult images with higher entropy are inferred at latter timesteps. The authors of DT-SNN achieve an overall 81% EDP reduction, with iso or higher accuracy than a standard SNN using fixed number of timesteps for inference across all inputs."}, {"title": "3. Vulnerability towards IMC Non-idealities", "content": "The practical implementation of NVM devices is constrained by finite conductance levels", "Non-idealities": "Stochastic read noise predominantly originates from random telegraphic noise"}, {"Non-idealities": "It includes the parasitic resistances in the crossbar metal lines denoted as rpar. During analog dot-product operations", "SNN": "For an 8-bit VGG16 SNN model trained with the CIFAR10 dataset", "SNNs": "Ensuring robustness to crossbar non-idealities involves iterative offline-training of SNN models with noise injection using hardware-realistic noise models138\u2013142. Recently, the AIHWKit toolkit from IBM offers statistical empirical models for emulating device-level and parasistic resistive non-idealities for PCM crossbars in PyTorch143,144. AIHWKit-based noise-aware training has demonstrated state-of-the-art hardware accuracies across diverse tasks, spanning computer vision to natural language processing. However, non-ideality-aware training is not scalable to today's large-scale SNN models. This is due to the bottleneck of noise-injection, especially for the input voltage-dependent resistive parasitic non-idealities, which escalates training latency. Additionally, SNNs incur high training costs due to computations across multiple time-steps. This has been depicted in Fig. 10b where, an iteration of non-ideality-aware training increases the training"}]}