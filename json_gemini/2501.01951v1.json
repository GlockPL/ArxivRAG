{"title": "MIXGCN: Scalable GCN Training by Mixture of Parallelism and Mixture of Accelerators", "authors": ["Cheng Wan", "Runkai Tao", "Zheng Du", "Yang (Katie) Zhao", "Yingyan (Celine) Lin"], "abstract": "Graph convolutional networks (GCNs) have demonstrated superiority in graph-based learning tasks. However, training GCNs on full graphs is particularly challenging, due to the following two challenges: (1) the associated feature tensors can easily explode the memory and block the communication bandwidth of modern accelerators, and (2) the computation workflow in training GCNs alternates between sparse and dense matrix operations, complicating the efficient utilization of computational resources. Existing solutions for scalable distributed full-graph GCN training mostly adopt partition parallelism, which is unsatisfactory as they only partially address the first challenge while incurring scaled-out communication volume. To this end, we propose MIXGCN aiming to simultaneously address both the aforementioned challenges towards GCN training. To tackle the first challenge, MIXGCN integrates mixture of parallelism. Both theoretical and empirical analysis verify its constant communication volumes and enhanced balanced workload; For handling the second challenge, we consider mixture of accelerators (i.e., sparse and dense accelerators) with a dedicated accelerator for GCN training and a fine-grain pipeline. Extensive experiments show that MIXGCN achieves boosted training efficiency and scalability.", "sections": [{"title": "1 Introduction", "content": "Graphs have served as a natural representation of real-world data thanks to its ability of depicting dependent relationship. Learning over graphs has been a popular research topic for the past decades [5, 26, 28, 72, 73], and one recent emerging method is graph convolutional networks (GCNs) [33], which enjoys powerful expressive capabilities [71, 74] and has been adopted to various real-world applications [18, 22, 65, 79]. Specifically, the computation of a GCN follows a two-step process: neighbor aggregation and node update. For a given node, to calculate its features in the next layer, a GCN first leverages a permutation-invariant function (e.g., average pooling) for aggregating all features from its incoming neighbor set, and then utilizes an update function (e.g., a multilayer\nperceptron) to combine the aggregated features and the embedding vector in the previous layer to calculate the new vector representation of the target node. Such a two-step process allows GCNs to capture the structure of input graphs and further retain the powerful expressive capabilities of neural networks.\nDespite GCNs' great potential, designing scalable GCN training systems is particularly challenging and still under-explored, due to the associated giant feature tensors. For example, ogbn-papers100M [21], a popular dataset for GCN research, contains more than 100 million nodes, requiring 124GB for storing merely the features and labels, let alone the storage requirement for storing the intermediate fea-tures to support backward propagation, which cannot be fit in a single modern accelerator. As such, to handle large-graph training, many recent works follow the direction of partition parallelism [12, 14, 23, 27, 43, 45, 48, 52, 59, 60, 62\u201364, 66, 69, 70, 85, 89, 91], as depicted in Figure 1b. The key idea is to separate a giant graph into multiple partitions, and assign each partition to one single accelerator. This straightforward approach, however, only distributes the storage of feature tensors, while incurring a significant memory and communication overhead for duplicating the remote neighbors from other accelerators (i.e., the red nodes in Figure 1b) [63]. This has restricted the scalability of GCN training due to the scaled-out number of remote neighbors. Furthermore, as we will show in Section 3.1.1, balancing workload via partition parallelism is NP-Hard, leading to a nontrivial synchronization overhead.\nIn parallel, the existing systems for scalable deep neural networks (DNNs) training can not be adopted for scalable GCN training. This is because these systems do not consider and thus are not optimized for handling the unique GCN training workflow which consists of hybrid sparse-dense operations. In particular, training GCNs alternatively performs sparse matrix operations for neighbor aggregation and dense matrix operations for node update. While the involved sparse-dense operations strengthen the capability of GCNs, they do not suit the underlying design of modern distributed systems for DNN training, of which the workflow is"}, {"title": "2 Background and Related Work", "content": "GCNs are popular for graph-based learning tasks. Each layer of a GCN uses a two-step process to calculate the new feature embedding of each node, which can be represented as:\n$z_v^{(l)} = \\zeta^{(l)} (\\{h_u^{(l-1)} | u \\in N(v)\\})$\n(1)\n$h_v^{(l)} = \\phi^{(l)}(z_v^{(l)}, h_v^{(l-1)})$\n(2)\nwhere $N(v)$ represents the neighbor set of node $v$, $h_v^{(l)}$ is the feature vector of node $v$ calculated by the $l$-th layer, $\\zeta^{(l)}$ denotes an aggregation function for calculating the intermediate result $z_v^{(l)}$, and $\\phi^{(l)}$ denotes an update function for updating the features of each node. We call the process of Equation 1 as neighbor aggregation, and regard Equation 2 as node update. The original GCN [33] uses weighted summation for $\\zeta^{(l)}$ and a single layer perceptron $\\sigma (W^{(l)} z_v^{(l)})$ for $\\phi^{(l)}$ where $\\sigma$ is a non-linear activation function. Each"}, {"title": "2.2 Partition Parallelism for GCN Training", "content": "To improve the scalability of GCN training, many recent works follow the paradigm of partition parallelism [86], which is depicted in Figure 1b. They either develop a scheduling algorithm towards balanced workload or optimized communication [23, 27, 43, 45, 66, 69, 91] or adjust training algorithm to reduce or hide its communication overhead [48, 52, 59, 62-64, 85]. However, as we will point out in Section 3.1.1, although partition-parallel training distributes the storage of giant feature tensors, it suffers from scaled-out memory overhead and communication volume due to the duplicated remote neighbors (i.e., the red nodes in Figure 1b). Furthermore, since a real-world graph is often highly irregular, finding a balanced-workload partition is NP-hard.\nIn parallel, CAGNET [60] and P\u00b3 [14] explore the benefits of feature-level parallelism, but still lack either practicality or scalability. Specifically, CAGNET [60] splits node features along the feature dimension which are broadcasted to all devices during training, resulting in significantly redundant communication; P\u00b3 [14] targets scalable distributed GCN training, but is still limited because it impractically assumes that the dimension of intermediate features is remarkably smaller than that of the input features [23]."}, {"title": "2.3 Tensor Parallel Computing", "content": "Scalable training has been extensively studied for DNN models. For example, Horovod [55], PyTorchDDP [38], AutoDist [83], BytePS [29], ZeRO [51], and PyTorch-FSDP [88] leverage data parallelism for distributing independent input feature storage and the associated computation by duplicating the model parameters. In parallel, ColocRL [46], Mesh-Tensorflow [56], GPipe [24], PipeDream [20, 47], Tofu [67], GSPMD [75], TeraPipe [39], and GraphPipe [25] develop model parallelism for storing model parameters distributedly, while depending on inter-model communication; FlexFlow [42], Megatron-LM [57], DeepSpeed [53], Alpa [90], and Pathways [3] combine both the above two parallelism to marry the best of both worlds. However, although these systems have shown promising performance for scalable DNN training and even provided automated scheduling toolboxes, they are only applicable to dense tensor operations and thus do not work well for the scalable computing of GCNs (see Equation 3)."}, {"title": "3 The Proposed Framework", "content": "To address the two key bottlenecks in scalable GCN training \u2013 giant feature tensors and hybrid sparse-dense operations \u2013 we propose MIXGCN, which integrates MoP and MoA. During the implementation of MoA, we identify that sparse operations in neighbor aggregation, coupled with an unscalable fine-grain pipeline, pose significant challenges to scalability. To overcome these challenges, we develop a dedicated accelerator that utilizes operator fusion and introduce a pipeline scheduler with node reordering to enhance efficiency. A summary of the innovations in MIXGCN is shown in Table 1."}, {"title": "3.1 Mixture of Parallelism (MoP)", "content": "An overview of partition parallelism is shown in Figure 1b, and its detailed workflow is illustrated in Algorithm 1. Specifically, each worker maintains a unique subgraph of the original graph by storing both the sub-adjacency matrix $\\hat{A_i}$ as"}, {"title": "3.1.2 The Proposed Mixture of Parallelism (MoP)", "content": "The detailed workflow of MoP is illustrated in Figure 2 and described in Algorithm 2. For a given input feature matrix $H^{(0)}$, MoP first splits $H^{(0)}$ along its feature dimension, and distributes each split to different accelerators for neighbor aggregation (see the left part of Figure 2), each of which computes the corresponding features (rows) of $A [H^{(0)}]$ by multiplying $A$ and the assigned inputs (line 8 of Algorithm 2). Next, MoP performs all-to-all communication so that each accelerator for node update gets the access to the entire features of assigned nodes, and updates them by the stored model weights (line 12). Finally, all-to-all communication is performed again, enabling all the aggregation accelerators to perform computation for the next layer. MoP repeats this process until the last layer. The backward pass of MoP follows a similar workflow (line 16). It is worth noting that in Algorithm 2, line 15 is also completed in a distributed manner but not shown.\nContrary to partiton parallelism, balancing the workload of the aggregation accelerators or update accelerators is"}, {"title": "3.1.3 Scalability of All-to-All Communication", "content": "One potential concern for adopting MoP is that it relies on all-to-all communication which may restrict its scalability. Despite that all-to-all communication also exists in partition parallelism and requires a more irregular communication pattern than MoP, we justify that under a proper design, all-to-all communication is not costly. For example, [44, 78] leverage a butterfly topology to implement all-to-all communication by assuming that arbitrary lengths of wires are acceptable, which only requires O(n log n) wires and O(log n) stages for non-blocking data transfer where n is the number of accelerators. In practice, all-to-all communication has been widely adopted in large-scale Transformer training and inference [11, 36, 50] for connecting up to 2048 devices. Our experiments also verify its scalability (see Figure 8)."}, {"title": "3.2 Mixture of Accelerators (MoA)", "content": "Benefiting from the proposed MoP, the second challenge, hybrid sparse-dense operations, can be naturally resolved by assigning the sparse and dense operations to two different groups of accelerators (outlined in Figure 4a). We propose mixture of accelerators (MoA) on top of our MoP to leverage (a group of identical) sparse accelerators to accelerate the sparse matrix operations (i.e., neighbor aggregation) and (an-other group of identical) dense accelerators to accelerate the dense matrix operations (i.e., node update)."}, {"title": "3.2.1 An Accelerator for Operator Fusion", "content": "As discussed in Section 2.4, existing GCN inference accelerators are not optimal for accelerating GCN training. This stems from the unique sparse operation \u2013 S-SpMM an operation that has yet to be thoroughly studied."}, {"title": "3.2.2 A Pipeline Scheduler with Node Reordering", "content": "As shown in Figure 5a, the naive workflow of MoA suffers from low hardware utilization. A common approach to alleviate this is adopting a fine-grain pipeline [16], as depicted in Figure 5b. However, this can still result in frequent idle periods, because the sparse accelerator cannot start processing the next operations until all dependent dense operations are complete. For example, in the graph shown in Figure 1a, node 1 depends on node 6, leading to idle time before node 1 can be processed. This idleness is further exacerbated in scalable training scenarios, where the workload per accelerator decreases, but the minimum granularity required for full hardware utilization remains unchanged (see Figure 5c).\nThis idle time can be reduced by leveraging node reordering. As illustrated in Figure 6a, the first processing batch in the original schedule depends on 5 out of 6 nodes. In contrast, with node reordering in the optimized schedule (see Figure 6b), the first processing batch only require the first 4 nodes (i.e., the first two processing batches), enabling the pipeline to eliminate idle periods, as shown in Figure 5d.\n$\\{p_v - p_u\\}_{(u,v) \\in \\varepsilon}$\n(s-1)n \\leq b. Based on\nThis complexity is identical to that of METIS [31], a widely-used algorithm for graph partitioning in partition parallelism."}, {"title": "4 Experiments", "content": "We evaluate the performance of MIXGCN on three popular architectures: GCN [33], Graph-SAGE [19], and GIN [74]. Each model consists of 3 layers with 128 hidden units and is trained on five large-scale datasets: Reddit [19], Orkut [77], Twitter [34], Friendster [77], and ogbn-papers100M [21]. The details of these datasets are provided in Table 2.\nMIXGCN is implemented in DGL [68] and PyTorch [38]. We set the default communication backend as NCCL and conduct the experiments on a 4-node cluster. Each computation node is equiped with 8 H100 GPUs and a 64-core Intel Xeon Platinum 8462Y+ CPU. The nodes are connected with InfiniBand.\nBecause concurrent distributed systems do not support our proposed MoA, we consider two variants of MIXGCN with different underlying sparse accelerators. (1) To implement MIXGCN in a real system, we run node update in GPUs while computing neighbor aggregation in CPUs. Each worker under this setting is assigned 1 GPU and 8 cores of CPU for performing node update and neighbor aggregation, respectively. The system"}, {"title": "4.2 Comprehensive Performance", "content": "presents a comparison of the throughput speedup between MIXGCN and various baseline systems, all measured against DistDGL in a 4-node setup. In particular, MIXGCN-CPU demonstrates"}, {"title": "4.3 The Performance of MoP", "content": "Figure 9 presents a normalized memory\nFigure 10 presents\nworkload, enabling scaling-up bene-fits."}, {"title": "4.4 The Performance of MoA", "content": "illustrates the performance gains of our proposed sparse accelerator compared to CPU, HyGCN [76], and AWB-GCN [16] when executing neighbor aggregations on the evaluated datasets. To ensure a fair comparison, we configured HyGCN and AWB-GCN with identical computational resources (16,384 units) and on-chip SRAM size (100MB), while setting a uniform HBM bandwidth of 1024GB/s for all three sparse accelerators. We maintained the original clock frequencies of the baseline accelerators: 1GHz for HyGCN and 330MHz for AWB-GCN.\ndemonstrates a reduction in power"}, {"title": "4.4.2 The Fine-grain Pipeline", "content": "We investigate the per-formance benefits of node reordering in fine-grain pipelines for GCN training, as presented in Table 4. According to Proposition 3.5, the number of pipeline stages is determined by n/\\[(b/n) 2]. Our evaluation on a four-node setup reveals that node reordering yields a throughput improvement of up to 1.18x compared to pipelines without node reordering, con-firming its effectiveness. However, for datasets like Reddit,\nIn contrast, RCM is a straightforward variant of Breadth-First by a"}, {"title": "5 Discussion", "content": "As introduced in Section 2.2, partition parallelism is the predominant strategy employed in distributed GCN training. Beyond the standard algorithm outlined in Algorithm 1, several variants exist. For instance, DistDGL [89], in full-graph training, collects all L-hop neighbors (where L is the layer number) before each training iteration, thereby avoiding inter-layer com-munication. PipeGCN [64] is a representative approach that leverages historical node features to facilitate asynchronous communication. Sancus [48] further optimizes communica-tion by adopting intermittent data transfer and replacing peer-to-peer communication with broadcast, resulting in a more regular communication pattern.\nHowever, as discussed in Section 3.1 and evaluated in Section 4, systems based on partition parallelism face challenges such as scaling out total communication volume and feature memory, as well as suffering from imbalanced workloads. This work identifies MoP as a promising approach due to its ability to maintain constant communication volume, scalable memory usage, and balanced workload."}, {"title": "5.2 Limitations and Future Work", "content": "One limitation of our proposed MoP is that it requires duplicating the propagation matrix A across all accelerators for neighbor aggregation, which could potentially lead to memory bottlenecks. We clarify that A is in general not the memory bottleneck. For example, training a 3-layer GCN with 128 hidden units for ogbn-papers100M [21] requires 301GB for storing intermediate embeddings and output logits, but only needs 24GB for storing the adjacency matrix A, which is affordable to modern accelerators. Nevertheless, for extremely giant graphs where A cannot fit into a sin-gle accelerator, alternative solutions are needed. One potential approach is to combine partition parallelism with MoP, leveraging MoP to accelerate intra-partition computations.\nlimitations for existing GCN training systems, enabling their"}, {"title": "6 Conclusion", "content": "Large-scale GCN training presents two significant challenges: managing giant feature tensors and efficiently handling hybrid sparse-dense operations. MIXGCN addresses these chal-"}]}