{"title": "A Low-cost and Ultra-lightweight Binary Neural Network for Traffic Signal Recognition", "authors": ["Mingke Xiao", "Yue Su", "Liang Yu", "Guanglong Qu", "Yutong Jia", "Yukuan Chang", "Xu Zhang"], "abstract": "The deployment of neural networks in vehicle platforms and wearable Artificial Intelligence-of-Things (AIOT) scenarios has become a research area that has attracted much attention. With the continuous evolution of deep learning technology, many image classification models are committed to improving recognition accuracy, but this is often accompanied by problems such as large model resource usage, complex structure, and high power consumption, which makes it challenging to deploy on resource-constrained platforms. Herein, we propose an ultra-lightweight binary neural network (BNN) model designed for hardware deployment, and conduct image classification research based on the German Traffic Sign Recognition Benchmark (GTSRB) dataset. In addition, we also verify it on the Chinese Traffic Sign (CTS) and Belgian Traffic Sign (BTS) datasets. The proposed model shows excellent recognition performance with an accuracy of up to 97.64%, making it one of the best performing BNN models in the GTSRB dataset. Compared with the full-precision model, the accuracy loss is controlled within 1%, and the parameter storage overhead of the model is only 10% of that of the full-precision model. More importantly, our network model only relies on logical operations and low-bit width fixed-point addition and subtraction operations during the inference phase, which greatly simplifies the design complexity of the processing element (PE). Our research shows the great potential of BNN in the hardware deployment of computer vision models, especially in the field of computer vision tasks related to autonomous driving.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, autonomous driving technology has made significant progress, gradually evolving from early assisted driving systems to highly or even fully automated stages. In this process, traffic sign recognition has become a core component of the autonomous driving system [1]. Compared with conventional image recognition tasks, traffic signal recognition algorithms usually need to run on a vehicle-mounted platform, which means that while pursuing high recognition accuracy and fast response, special attention must be paid to the additional resource consumption generated during the execution of the algorithm [2]. Given that cars are resource-constrained embedded scenarios [3], traffic sign recognition systems tend to use lightweight models to ensure that they can be effectively deployed and run efficiently on vehicle platforms [4].\nIn order to make the model lightweight, many research methods have emerged. Among them, the model pruning technology proposed in [5] has become the core principle of the API widely used in frameworks such as TensorFlow. This technology effectively reduces the complexity of calculation by setting some parameters in the model to zero, because the calculation operations related to zero can be directly ignored, thereby improving the calculation efficiency. In addition, the int8 quantization model introduced in [6] has been widely used in the industry. The quantized model can be seamlessly deployed on a variety of devices such as Android and Apple, significantly improving the deployment flexibility and operating efficiency of the model. In order to further pursue higher quantization efficiency and performance, [7] pioneered the concept of binary neural network (BNN). BNN greatly simplifies the calculation process by limiting the weights and input values in the model to {1, -1}, opening up a new way to lightweight neural network models.\nIn this paper, we use BNN technology to perform traffic signal recognition tasks and propose the N+Half neural network model for the first time. The model only involves bit operations, addition and subtraction during inference. This design is extremely friendly to hardware resources and can significantly save chip area and reduce computing power consumption. Specifically, our traffic sign recognition model achieves significant storage compression ratios of 10.8x and 2.7x compared to the full-precision model and the int8 quantized model, respectively, and the accuracy loss is kept within 1% compared to the full-precision model."}, {"title": "II. RELATED WORK", "content": "A. Model quantization and pruning\nUsing convolutional neural networks for image classification is a widely used method, especially in the field of traffic sign recognition. The most commonly used dataset in this field is the GTSRB dataset proposed in [8]. Previously, the multi-column CNN network model proposed by [9] achieved an accuracy of 99.46% on the test set of the GTSRB dataset, which is an outstanding achievement in the history of convolutional neural networks. However, with the continuous advancement of research, [10] built a model using VisionTransFormer (ViT) technology, the model introduces the self-attention mechanism and performs different weighted calculations on the importance of each part of the input data, further improving the accuracy to 99.58%. The general research consensus is that the complexity of the network structure is often positively correlated with the accuracy of the model.\nQuantized neural networks have become a research focus in recent years. Researchers have gradually realized that although complex models can achieve better accuracy, their deployment on embedded platforms often faces challenges [11]. Quantized models are mainly divided into two categories: post-training quantization and quantization-aware training [12]. Post-training quantization methods can leverage pre-trained models, but usually result in a reduction in model accuracy. In contrast, quantization-aware training requires additional training steps but often achieves higher accuracy. In order to effectively alleviate the problem of accuracy loss in the quantization process, [13] proposed STE, which can significantly improve the performance of the quantized model.\nIn the process of implementing the convolution operator, due to the issue of universality, most neural network hardware acceleration platforms will not directly implement the convolution operation, but will implement the more common matrix multiplication operation. The process from convolution operation to matrix multiplication operation generally uses Img2col [14] for conversion, which will convert the multi-channel input and convolution kernel into input matrix and convolution kernel matrix. Generally speaking, such a conversion will bring greater data depth because this conversion will store more data.\nB. Binary Neural Network\nBNN is undoubtedly a groundbreaking work. Since BNN was first proposed in [5], it has attracted many researchers to optimize it from different angles. Among them, [15] proposed an innovative network model that only binarizes the feature maps, while the weight parameters are kept as fixed-point numbers with a higher bit width. This improvement effectively improves the accuracy of the model. In addition, [16] filled the gap in BNN research in the field of self-supervised learning, proposed S2-BNN, and demonstrated the performance of BNN on the ImageNet [21] dataset for the first time. On the other hand, the XOR-net proposed in [17] deeply considered quantization errors problem and enhanced the expressiveness of the model by introducing a scaling factor on each output channel, which enabled the BNN model to achieve remarkable results on the ImageNet dataset. The IR-net proposed in [18] adopted different optimization strategies in forward propagation and backpropagation: in forward propagation, Libra-PB was used to minimize the quantization loss of weights; in backpropagation, the designed EDE strategy aims to minimize the information loss of gradients. These two strategies are very simple, easy to understand and very effective.\nIn recent years, optimization technologies in related fields have emerged in an endless stream. ReActNet is a new network designed in [19]. Its core innovation is the introduction of the Bias term in PRelu and Sign functions. It is worth noting that most of the research works mentioned above include batch normalization layers. However, the study [20] attempted to remove the batch normalization layer from the network model because it may introduce additional floating-point operations, thus affecting the efficiency of training and inference.\nThe dataset GTSRB used in this paper is an image classification dataset covering 43 categories. In [22], researchers preprocessed the input image using image enhancement technology, and then used CNN for model training, and finally achieved an accuracy of 96%. [23] proposed using VIT algorithm to classify the dataset, which also achieved impressive accuracy performance. CTS is a Chinese traffic sign recognition dataset with 58 categories [24], and the BTS dataset is a Belgian traffic signal recognition dataset [25], which contains 62 categories of images. We use this dataset to verify the stability of our model. However, it is worth noting that most of the current research on the GTSRB dataset focuses on improving accuracy, while relatively few considerations are given to how to lightweight the model when deploying it on a vehicle platform. Although [26] proposed a signal recognition solution designed for AIOT devices, which has a relatively simple model structure and operation process, this scheme did not fully consider the additional overhead that may be caused by the data bit width dispersion during the reasoning process. The IE-Net mentioned in [27] proposed an information enhancement binary convolution to address the information loss caused by binarization of weights and activations, which can improve the performance of the model. Then, the information enhancement estimator IEE was introduced to approximate the sign function to reduce the amplitude of information attenuation, which has been proven to be effective on some common datasets."}, {"title": "III. METHODOLOGY", "content": "This section describes our model structure and working principle in detail. We first proposed the N+Half network structure, which can eliminate the floating-point operations in the last layer of the inference stage. In addition, our design combines the operation fusion technology and distribution adjustment strategy to reduce the bit width of the values actually involved in the calculation in the inference phase, greatly reducing the storage pressure of the model inference stage.\nA. N+Half Model\nThe neural network structure used in this paper is called N+Half. Its overall structure is shown in Fig 2. The left side shows the structure of the model in the training phase, and the right side shows the structure of the model in the inference phase. In general, after the training phase is completed, the model can use the operation fusion method to obtain a more concise operation data flow.\nN corresponds to N layers of convolution blocks. A convolution block includes: convolution layer, pooling layer, activation function layer, batch normalization layer. Since the fully connected layer, as the main content of the last part of most neural networks, will bring a lot of calculations, making the design of the computing component PE complicated, we canceled the fully connected layer structure in the design. Specifically, the first M layers use 2D convolution blocks, and the following K layers use 1D convolution blocks. The output dimension of the last layer of 1D convolution blocks matches the final number of categories. In this way, PE only needs to support convolution operations, which can make PE more concise.\nHalf refers to the incomplete calculation block. The incomplete calculation block only contains convolution layer and pooling layer. It is located after the last convolution block and is the last layer of the entire network. Since floating-point arithmetic operations consume several times more resources than fixed-point operations in FPGA or ASIC platforms, in order to avoid this situation, we introduce incomplete calculation block to completely eliminate the floating-point operations in the reasoning stage. The specific reasoning proof process will be introduced in Section 3 of this chapter.\nThe model file obtained after the training phase is completed will be used in the inference and test phases. We exported the model parameters and wrote a function to simulate the actual operation process of the hardware platform in the inference phase.\nB. Method of Binarization\nTraditional quantization methods can quantize the original floating-point parameters into int8 fixed-point numbers. This conversion brings many advantages, especially in FPGA or ASIC design, because floating-point Point operations consume a lot of resources. For BNN, its greatest contribution is to convert the multiplication and accumulation operations of convolution layer and fully connected layer in the original quantized network structure into logical operations such as XOR, thereby greatly reducing the computational complexity and resource occupancy. We use Formula 1 to convert both weights and inputs into {+1, -1} values, thereby binarizing the network.\n$xb = Sign(x) = \\begin{cases} +1, & \\text{if } x \\geq 0 \\\\ -1, & \\text{otherwise} \\end{cases}$ (1)\nIn Formula 1, x is the original value and xb is the quantized value. Since the Sign function is not differentiable at 0, the chain rule does not work here. We use the IEE method proposed in [27] to approximate the derivative so that the neural network model can be back-propagated smoothly.\nFig 3 shows the function graphs of different activation functions. We first exclude the Tanh function because its operation is too complicated. The Sign function is used to obtain the binary parameters, and the HardTanh function is used to limit the bit width of the intermediate results. They play a key role in the model.\nC. Operator Fusion\nThrough operation fusion technology, we can integrate the operation process of batch normalization and activation function into only addition and subtraction operations. This strategy greatly simplifies the design of the operation component PE. In the network model, multiplication and division operations usually take up more resources when implemented on the hardware platform, and may reduce the operation speed, thereby restricting the operating frequency of the chip.\nIn order to improve model performance and accelerate convergence, we introduced two activation functions, Prelu and HardTanh, as well as a batch normalization layer to adjust the distribution of data. The core of operation fusion is to integrate these operations into one formula, thereby avoiding unnecessary multiplication and division operations and limiting the entire calculation process to addition and subtraction. Specifically, the process of the HardTanh function is shown in Formula 2.\n$HardTanh(x) = \\begin{cases} maxval & \\text{if } x > maxval \\\\ minval & \\text{if } x <minval \\\\ x & \\text{otherwise} \\end{cases}$ (2)\nThe HardTanh function means a clipping operation that limits the value to a specified range, which is generally symmetrical, such as -31 to 31. In our model, HardTanh is located after the pooling layer and before the Prelu activation function, which allows the range of parameters to be fixed within a certain range.\n$PReLU(x) = \\begin{cases} x, & \\text{if } x \\geq 0 \\\\ ax, & \\text{otherwise} \\end{cases}$ (3)\n$BatchNormal(x) = \\frac{x - \\gamma}{\\sqrt{\\sigma^2 + \\epsilon}} * y + \\beta$ (4)\nFormula 3 is the mathematical formula of the Prelu function, where a is a trainable parameters whose optimal value can be found during the training process. Formula 4 is the mathematical formula of the batch normalization layer, where \u03bc and \u03c3 are the mean and variance of the data. Since the training data and test data generally have similar distributions, they are also trainable parameters. \u025b is a constant that is usually used to avoid division by 0. In addition, y and \u1e9e are also trainable parameters. If the network is built in the order of HardTanh, Prelu, and batch normalization, the complete mathematical formula can be considered as shown in Formula 5.\n$Func(x) = \\begin{cases} C1 & \\text{if } x > maxval \\\\ C2 & \\text{if } x < minval \\\\ kx + b & \\text{if }x > 0 \\\\ akx + b & \\text{if }x < 0 \\end{cases}$ (5)\n$k = \\frac{Y}{\\sqrt{\\sigma^2 + \\epsilon}}$ (6)\n$b = \\beta - \\frac{\\mu y}{\\sqrt{\\sigma^2 + \\epsilon}}$ (7)\n$C1 = maxvalue * k + b$ (8)\n$C2 = a * minvalue * k+b$ (9)\nFormula 5 means that if the content of x exceeds the upper and lower limits set previously, it will be set to constant C1 or C2. In other cases, depending on the sign bit of x, two sets of operations will be involved, in which the parameters k and b involved in the operation are composed of trainable parameters. In the inference phase, k and b are also constants. After the above operation fusion, the division operation in the batch normalization of the inference phase can be eliminated, but because of the existence of a, floating-point multiplication still exists. However, due to the existence of the subsequent Sign function, this problem will also be solved. Here we need to introduce a new function Sign-Plus, as shown in the process Formula 10 and Formula 11.\n$Sign_+(x, \\delta) = \\begin{cases} +1, & \\text{if } x \\geq \\delta \\\\ -1, & \\text{otherwise} \\end{cases}$ (10)\n$Sign_-(x, \\delta) = \\begin{cases} +1, & \\text{if } x \\leq \\delta \\\\ -1, & \\text{otherwise} \\end{cases}$ (11)\nCombining the content of Formula 10, Formula 11, Formula 5 and Formula 1 are combined to obtain the following Formula 12.\n$Final(x) = \\begin{cases} Sign(C1) & \\text{if } x > maxal \\\\ Sign(C2) & \\text{if } x < minal \\\\ Sign_+(x, \\delta1) & \\text{if } x > 0 \\text{and} k > 0 \\\\ Sign_-(x, \\delta1) & \\text{if } x > 0 \\text{and} k < 0 \\\\ Sign_+(x, \\delta2) & \\text{if } x > 0 \\text{and} k > 0 \\\\ Sign_-(x, \\delta2) & \\text{if } x > 0 \\text{and} k < 0 \\end{cases}$ (12)\n$\\delta1 = \\frac{-b}{k}$ (13)\n$\\delta2 = \\frac{-b}{ak}$ (14)\nCombined with Formula 12, it can be concluded that the operation after the pooling operation in the inference phase only needs to compare the x and some constants and the sign bit of k to complete the operation process. In our neural network model, the Sign function is placed in the next Block convolution layer, which is why the N+Half structure is designed, because only in this way can the last operation fusion be completed normally. If there is no last incomplete block, the last round of fusion will lack a Sign function, so only Formula 5 can be used for calculation instead of Formula 12, but Formula 5 essentially contains floating-point multiplication, which is very disadvantageous in hardware deployment. In summary, the structure of the convolution Block should be: convolution layer, pooling layer, HardTanh, Prelu and batch normalization.\nD. Adjusted Distribution\nIn the convolution operations, although the weights and input data are both 1 bit wide, their results are usually not. These results need to be temporarily stored for subsequent operations and quantization. By adjusting the distribution strategy, we can reduce the storage space of these intermediate results to the ideal range, thereby saving a lot of storage space."}, {"title": "IV. EXPERIMENT", "content": "In this section, we will first introduce the dataset used in this paper and the setting of model parameters during the experiment. Then, we will introduce the model test results and the comparison results with other models. Finally, we will introduce the ablation experiment to demonstrate the irreplaceability of each module in the model and the rationality of the hyperparameter values in the experiment.\nA. DataSet and preprocess\nThe neural network model used in the experiment works on the GTSRB dataset, which is a traffic signal recognition dataset with 43 categories. The entire dataset has more than 50,000 images for training and testing, and most of the image data is captured by on-board cameras, which is closer to the actual situation of autonomous driving. The training and reasoning process of this paper will use OpenCV to preprocess the image and obtain a grayscale image of the specified size. After obtaining the grayscale image, the pixel matrix will be divided by 255 as a whole to obtain a binary pixel matrix.\nFig 7 intuitively shows the effect of this preprocessing. Because a binary neural network is used, after the original image is binarized, the 0 in the pixel value will be replaced by -1. In this way, both the model weights and the input image are numerically unified, containing only +1 and -1.\nB. Model structure and parameters\nAfter multiple experimental verifications, we determined that the optimal number of blocks for the BNN network structure is 6. This choice is based on the N+Half structure we adopted, and it is worth noting that the 6th block is designed as an incomplete calculation block.\nC. Performance Comparison\nIn terms of the storage space occupied by the convolutional layer parameters, the binary parameters of the BNN network are 287032, which will occupy 10KB of space. Compared with the full-precision network model, the space compression ratio is 10.8. Compared with the quantized model obtained by int8 quantization of the full-precision network model, the space compression ratio is 2.7. In the additional operations brought by the activation function after operation fusion, the spatial compression ratio for the storage of the intermediate results and comparison parameters of the feature map is 2.5. This is mainly due to the smaller bit width can be used to store intermediate results and comparison parameters.\nTable 2 shows the comparison results of the proposed model with other lightweight models and full-precision models. For fairness, the initialization scale of the image is set to 128*128. In terms of accuracy, our accuracy is in the leading position in the BNN network. In terms of implementation, each BNN model of the convolution layer is implemented using XOR. In the subsequent batch normalization and activation function implementation, due to the operation fusion technology of this paper, our model can only use addition operations to implement the remaining operations, which can bring great advantages in the process of hardware implementation.\nD. Performance Comparison\nIn order to evaluate the effectiveness of each component of the proposed neural network model, we decomposed the algorithm and hid the functions of the components one by one to verify the effectiveness of each component. The evaluation table criteria are mainly the train accuracy in the training phase, the test accuracy in the test phase, the number of floating-point operations Float_OPS in the model inference phase, and the bit widths of the intermediate result storage. Table3 shows the comparison results. We use YES to indicate the use of the module function and NO to indicate the hiding of the module function. The table shows that the introduction of RH and IB will affect the accuracy of the model, but the performance of the other two indicators will bring greater benefits. In order to facilitate hardware deployment, these accuracy losses are acceptable. Considering all indicators comprehensively, the best performance can be achieved by using all modules. These experimental results show that the image preprocessing module IP, the threshold adjustment structure RH and the incomplete operation block IB have made important contributions to the performance of the algorithm and played an irreplaceable role.\nIn order to verify the impact of the threshold value of the HardTanh threshold function on the accuracy of the model, we conducted an ablation experiment on the threshold for GTSRB, CTS and BTS. From the results in the Figure 8, it can be seen that when the absolute value of the threshold exceeds 31, the increase in the threshold range has little effect on improving the accuracy, and using a larger threshold does not achieve better model performance. Similarly, it can be observed that selecting a threshold that is too small will also limit the performance of the model. For example, when the threshold is set to 8, the model basically loses its predictive ability. Therefore, the absolute value of the threshold of the threshold function we finally chose is 31, which will limit the range of the intermediate results of the model calculation to {-31, 31}. This choice is both reasonable and effective."}, {"title": "V. CONCLUSION", "content": "In this paper, we successfully achieved higher recognition accuracy by binarizing feature maps and weights on the GTSRB traffic signal recognition dataset and cleverly integrated the subsequent activation function and batch normalization layer, and achieved the best performance in the binary neural network (BNN) task on this dataset. Specifically, we innovatively proposed the N+Half neural network model, which, in addition to convolution and pooling operations, only requires addition and subtraction operations to complete the calculation in the inference stage, greatly simplifying the calculation process. In addition, we also effectively reduced the data storage cost by optimizing the distribution of intermediate results, which is of great significance for hardware deployment. Given that the weights and activation functions in the network are restricted to a single binary number, binary networks have shown great application potential on hardware platforms such as FPGA or ASIC. As a next step, we plan to verify the acceleration effect and energy-saving performance of BNN during inference on FPGA and ASIC."}]}