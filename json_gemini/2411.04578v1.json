{"title": "Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions", "authors": ["TIANQI SONG", "YUGIN TAN", "ZICHENG ZHU", "YIBIN FENG", "YI-CHIEH LEE"], "abstract": "Multi-agent systems - systems with multiple independent Al agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of Al agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user's stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents' stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.", "sections": [{"title": "1 INTRODUCTION", "content": "Al agents are advanced systems capable of interacting with users, holding conversations, and expressing their own thoughts [86]. These agents leverage the conversational and reasoning capabilities of large language models (LLMs) like Gemini, GPT-4, and LLaMA, and have become commonplace in daily life [64]. Significant research has also explored using these agents across a broad range of scenarios including education [48], search interfaces [50], and trading platforms [92]. The prevalence and ubiquity of LLM-powered agents has given rise to the concept of multi-agent systems - systems with two or more independent agents working together to fulfill a common goal and improve task performance [15, 21, 35]. Major AI companies such as Anthropic [22] and Google [31] have recently introduced multi-agent frameworks, making it easier for developers to create such interfaces."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Multi-Agent Interactions", "content": "Al agents are increasingly integrated into our daily life. These advanced AI systems are capable of interacting with users, holding conversations, and expressing their own thoughts [86]. Recent research and industry applications have leveraged the conversational and reasoning capabilities of LLMs to create problem-solving tools [33, 55, 67, 87, 92], educational assistants [48], and novel search interfaces [50]. In the industry, many companies deploy Al agents on social media to assist with branding and customer service [39, 43, 90], while others embed agents in their applications for assisting writing [26], brainstorming [65], or gaming [73].\nDespite this, there are many tasks that a single LLM-based agent struggles to handle effectively. Individual LLMs can lack sufficient domain expertise to handle specialized tasks [29], and may have insufficiently robust reasoning capabilities. Inspired by existing psychological theories [75], studies have investigated techniques such as \"debating\" [27] or \"reconciling\" [14] between multiple agents, to enhance reasoning by leveraging diverse perspectives. This has driven growing interest in \"multi-agent collaboration,\" where agents work together to improve task performance [15, 35]. Fields such as natural language processing [35], software engineering [11], and robotics [16] have leveraged the collective intelligence of multiple agents to simulate a group dynamic and improve task performance [38, 63, 89]. Multi-agent setups have also been used to simulate different human roles, akin to multiple workers interacting [59, 60] or collaborating on a task [2, 49].\nIn more user-centered contexts, there have been emerging efforts to enable interactions with multiple agents simul-taneously. For example, Clarke et al. [21] developed an interface to facilitate engagement with multiple conversational agents at once, combining the unique features of different smart assistants like Google Assistant, SoundHound, and Ford's in-vehicle smart assistant Adasa [21]. Additionally, many major AI companies like Anthropic [22], Google [31], and others [24] are introducing multi-agent frameworks that enable developers to easily deploy multi-agent interfaces. Some organizations also use a multi-bot setup on social media to interact with and engage users.\nGiven the potential prevalence of human and multi-agent collaboration in everyday life, the interaction between multiple agents and humans remains underexplored in the HCI domain. Previous work has a limited focus on multi-agent systems in HCI, mostly highlighting opportunities of multi-agent interfaces in improving user outcomes: either by reducing cognitive load in information gathering [44] or to provide information from diverse sources to improve decision making [7, 12, 58, 80]. These studies also show the challenges that multi-agent interfaces create for users, such as causing users to feel overwhelmed or confused by multiple agents providing different inputs [12, 58, 80]. In contrast, no existing work to our knowledge has investigated if and how a group of agents affects users through social influence mechanisms. Such effects are distinct from the cognitive and decision making effects in previous studies, yet are potentially as important.\nIn this study, we investigate the scenario: what if multiple agents are perceived as social actors, and they all share the same opinion on a topic just like a group of people might? This situation can arise, for instance, when companies deploy multiple agents on social media with a unified stance. While the influence of a single opinionated bot is well-documented [42, 72, 93], the impact of being surrounded by multiple like-minded bots is less understood. Considering the growing"}, {"title": "2.2 Social Influence Theory", "content": "When multiple people express the same opinion, others in the group often feel compelled to agree. This phenomenon is known as \"social influence\" [20], which comprises the ways in which individuals adjust their behavior to meet the demands of a social environment. This theory is well-known and widely studied in social psychology, and has been applied across different contexts, such as marketing [68, 82, 96], health intervention [74, 94], sustainability [5, 85] and political discussions [62].\nSocial influence is a broad term that manifests in various forms. Two main types of social influence are normative influence, the desire to \"conform with the expectations of another\", and informational influence, the desire to \"accept information obtained from another as evidence about reality\" [25]. Normative influence relates to many common types of social influence including conformity [20], peer pressure [40, 71], socialization [56, 66], and social norms [30, 51]. More recent literature has termed this mechanism affiliation [20], or a desire to fit in with the majority: assimilation into a social group is often desired, and agreeing with the majority opinion can expedite this process. Naturally, normative social influence is more common among individuals that form a collective social group [25], which creates an environment that one might want to be a part of. In this study, we investigate whether a group of Al agents can create a similar sense of a social group, implicitly encouraging a user to join the group by way of agreeing with their opinions.\nInformational influence, on the other hand, relates to the concept of accuracy [20], or the natural instinct to gravitate towards objectively correct opinions as expressed by a group. This is distinct from normative influence as individuals may \"accept an opponent's beliefs as evidence about reality even though one has no [other] motivation to agree with him\" [25]. Despite this, social influence can still persuade individuals to accept an opinion, as the perceived consensus [20] of a larger number of people expressing the same viewpoint may suggest that that viewpoint is accurate. One distinction between human-human and human-AI interactions is that in human groups, each person represents a distinct source of knowledge and possible truth; with Al agents, it is trivial to set up multiple agents based on the same source of data, meaning that the number of agents presenting an opinion has no relation to its objective accuracy. Nevertheless, the tendency of humans to perceive such agents as distinct social actors [54] raises the possibility of informational influence from a group of agents.\nIn HCI research, a handful of studies (e.g. [1]) have examined the social influence that individual chatbots can exert on humans. In this study, we aim to conduct, to our knowledge, the first investigation of the possibility of both normative and informational social influence caused by groups of Al agents."}, {"title": "2.3 Conversational Agents in Online Discussions", "content": "Conversational agents (or \"chatbots\") powered by LLMs such as GPT-4 are capable of holding human-like conversations, and are increasingly being used in group discussion settings. Chatbots have long been used in online discussion scenarios, such as to debate with participants and prompt critical thinking [81] or facilitate group discussion by improving discussion efficiency and members' participation [37, 46, 47]. Building on these conversational abilities, chatbots have also been used for persuasive means, to encourage a user or a group of users to alter their attitudes or behaviors. Such systems have been used in health and wellness [6, 57], education [3, 83] and social welfare [72]. While"}, {"title": "3 METHODS", "content": "To understand how single- and multi-agent systems create social influence on humans, we conducted a mixed-methods study comprising an experiment and multiple survey segments, utilizing both quantitative measures and qualitative open-ended questions. This study received approval from our university's ethics review committee prior to commencement."}, {"title": "3.1 Experiment Setup", "content": "To investigate how the number of agents influences human opinions, we randomly assigned participants to interact with one, three, or five agents. We chose these numbers based on prior research on persuasion and multi-agent interface"}, {"title": "3.2 Agent Setup", "content": "The agents' conversations were implemented through a combination of rule-based scripts and the GPT-4 API\u00b9. For rule-based scripts, we designed a series of arguments either supporting or opposing the stance for each topic, as detailed in Appendix A.3. These arguments were then crafted into agent dialogue, such as \"I would definitely support [self-driving cars] because I think self-driving cars are great! You can sit back, relax, and let the car do the work for you\" or \"I couldn't agree with [allowing self-driving cars] because I think there are still a lot of issues with self-driving cars that need to be addressed. For example, technical developments are not yet perfect.\" Across the three conditions, the same set of arguments was presented: with three and five agents, the different agents took turns to present different arguments; with one agent, the same agent presented all the arguments in the same order. This was to ensure that any difference in opinion shifts across the three conditions was not due to differences in content quality or quantity.\nWe also integrated GPT-4 to enhance agent conversations in two ways: (1) parsing user input, such as extracting the user's name from their greeting, and (2) generating interactive responses during discussions. When the host agent prompted users to share their opinions on the topics, the agent(s) provided brief feedback based on the user's inputs, such as by giving a summary of the user's thoughts or asking follow-up questions. For example, if a user said, \"I agree that self-driving cars should be allowed on the roads\", the next agent might have responded with, \"That's great to hear!\nWhat aspects of self-driving cars do you find most appealing or beneficial?\" These responses were tightly controlled through pre-set prompts ensuring that the agent(s) only gave responses relevant to the discussion."}, {"title": "3.3 Participants", "content": "Participants were recruited via CloudResearch2. The selection criteria required them to be English speakers and over 18 years old. A total of 104 participants were initially recruited, with two not completing the survey and eight failing the attention check. Ultimately, 94 participants were included in the analysis: 31 participants (F: 17, M: 14) in the 1-agent group, 33 participants (F: 17, M: 15) in the 3-agent group, and 30 participants (F: 15, M: 15) in the 5-agent group. The average ages for each group were as follows: 1-agent group = 38.48 (SD = 12.41), 3-agent group = 38.75 (SD = 12.52), and 5-agent group = 31.87 (SD = 9.46). Participants' educational backgrounds were distributed as follows: 9 were high school graduates, 16 had some college but no degree, 4 held an associate's degree, 45 held a bachelor's degree, 19 held a master's degree or higher, and 1 preferred not to specify.\nThe study was conducted in a self-developed online platform (Figure 4); participants were required to complete it on a laptop or desktop computer. The platform's frontend interface was built using JavaScript and HTML and included three main sections: (1) a login page, (2) an initial attitude questionnaire, and (3) a conversation page for interaction with agents. The study lasted approximately 45 minutes to complete, and each participant was reimbursed US$5.50.3.\nThe agents' avatars and rhetorical styles were designed to be human-like to enhance user acceptance [70]. To avoid the uncanny valley effect [78], we used cartoon-style avatars instead of realistic photos. Each avatar was assigned unique colors to help participants distinguish between the agents. We also ensured gender balance within the 3-agent and 5-agent conditions to minimize the potential effect of agent gender on participants' opinion changes [81]."}, {"title": "3.4 Measurements", "content": "In this section, we describe the measurements used in this study based on our research questions. Full details of the measurements can be found in Appendix A.2."}, {"title": "3.4.1 Quantitative.", "content": "Unless otherwise specified, quantitative variables were measured on a 7-point Likert scale.\nUser Opinion (RQ1). We assessed participants' attitudes toward each topic using five questions adapted from a previous study on social discussions [36], such as \"Self-driving cars should be allowed on public roads\" and \"Allowing self-driving cars on public roads is a good idea.\" These questions were asked twice- before and after the conversations to measure changes in participants' stances. Responses were rated on a scale from 1 to 6 (1 = \"Strongly disagree,\" 6 = \"Strongly agree\"), without a neutral option. This was adapted from previous studies [13, 81] about manipulating users' attitudes and was adapted to nudge participants to take a stance. Responses were rated on a scale from 1 to 6 (1 = \"Strongly disagree,\" 6 = \"Strongly agree\"), without a neutral option. This was adapted from previous studies about manipulating users' attitudes [13, 81] and was used to nudge participants to take a stance.\nSocial Influence (RQ2) To compare the social influence exerted by the agents across different scenarios, we adapted self-reported survey questions from previous research [45]. We measured two types of social influence - informational influence and normative influence. Informational influence (a change in behavior or attitude based on the belief that others' information is accurate or reliable) was measured through questions like \"My decision was influenced by the opinion of the agent(s)\" capture this type of influence. Meanwhile, normative influence (changes in behavior or attitude driven by the desire to avoid exclusion) was measured using questions like \"During the discussion I felt that I had to agree with the opinion of the agent(s)\".\nControl Variables. We also measured factors that could potentially affect perceived social influence. Drawing from previous literature, we asked participants about their domain expertise regarding the topics, with questions such as \"How often do you drive?\" and \"Have you ever been in a self-driving car?\" These factors can influence individuals' receptiveness to external influence [36]. Additionally, we included questions from the AI acceptance scale [61], as well as measures of participants' conformity [52] and compliance [34] tendencies."}, {"title": "3.4.2 Qualitative.", "content": "To understand reasons for potential opinion change and social influence, we formulated open-ended questions based on social influence theory in human-human interactions. We chose these questions instead of surveys because we found no existing survey that adequately captures social influence in human-agent interactions. Furthermore, given the formative nature of our study, qualitative analysis of these open-ended responses could potentially reveal new insights into different types of human-agent interactions.\nUser Opinion (RQ1). Participants were asked to articulate their thoughts on the two topics both before and after their conversations with the agent(s). An example question is, \"What are your thoughts on self-driving cars? Do you support or oppose the statement, 'Self-driving cars should be allowed on public roads'? Please explain your reasoning.\" This allowed participants to further elaborate on the stances they expressed in the quantitative question.\nSocial Influence (RQ2). We included three questions to gather participants' perceptions of the social influence from agent(s) during the discussions. The first was a broad question: \"What do you think of the agent(s) during the discussion?\", which was designed to capture general reactions. The other two focused on key aspects of social influence - accuracy and affiliation [20]. Specifically, we asked, \"Do you think the arguments presented by the agent(s) were accurate and convincing? Why or why not?\" and \"During the conversation, did you feel any pressure to agree with the agent(s)? If so, why?\""}, {"title": "3.5 Analysis", "content": "When designing the experiments, we considered the effects of our agent(s) on the participants based on different topics (to understand if the same effects persisted across topics) and whether the agents agreed or disagreed with participants (to understand if agents aligning their views with participants would have a different effect from opposing their views). Therefore, we examined results in each of the following scenarios: Topic 1, Topic 2, Same Stance, and Different Stance.\nTo verify the assumption required for parametric analyses, we first performed a Levene test and found that all items met homogeneity of variances. We then conducted a Shapiro-Wilk test to assess data normality. For measures that met the normality assumption, we used one-way ANOVA followed by Tukey's test for post-hoc analysis. For measures that did not meet the normality assumption, we applied the Kruskal-Wallis test and performed Dunn's test as a post-hoc analysis."}, {"title": "3.5.1 Opinion Change Analysis.", "content": "To evaluate opinion change, we conducted two analyses: (1) examining whether users' opinions shifted towards the bots' stance, and (2) assessing changes in the polarization level of users' opinions.\nFor the first analysis, we calculated opinion change based on the stance of the bots, where a higher value of change indicates a greater shift of participants' opinions towards the bots' stance. If the bot supported the topic, we expected participants' ratings (of agreement with the topic) to increase after the conversation; conversely, if the bot opposed the topic, we anticipated a decrease. Thus, we defined opinion change, \u0394\u039f as follows:\n$\\Delta\\text{O} = \\begin{cases} \\text{Opost} - \\text{Opre}, & \\text{if the bot(s) supported the topic} \\\\ \\text{Opre} - \\text{Opost}, & \\text{if the bot(s) opposed the topic} \\end{cases}$\nwhere Opost and Opre represent participants' ratings of agreement with the topic in the post-survey and pre-survey, respectively. This approach captures the direction and magnitude of participants' opinion changes toward the agent(s).\nFor the second analysis, we followed previous literature [32] to define the polarization of a stance as $|0 - \\text{Oneutral}|$, where Oneutral represents the neutral midpoint on a rating scale. Given our 6-point scale, we assigned Oneutral = 3.5. We calculated the change in polarization as follows:\n$\\Delta P = P_{\\text{post}} - P_{\\text{pre}} = |O_{\\text{post}} - 3.5| - |O_{\\text{pre}} - 3.5|$\nThis allowed us to observe differences in the strength of participants' opinions before and after the conversation."}, {"title": "4 RESULTS", "content": "Before analyzing the variables related to our RQs, we conducted an analysis of the control variables and found no significant differences between groups. As a result, we did not include these control variables in the subsequent analyses."}, {"title": "4.1 RQ1: Do interactions with multi-agent systems lead to stronger opinion changes?", "content": "We gathered evidence through both quantitative and qualitative analyses to address RQ1."}, {"title": "4.1.1 Quantitative. Opinion Change.", "content": "We found significant results in opinion change towards agents, as shown in Figure 5. The differences were observed under Topic 2 (Kruskal-Wallis: H(2)=7.757, p<0.05), and also when the agents held different stances towards the users (Kruskal-Wallis: H(2)=6.937, p<0.05). No significant difference was observed under Topic 1 (Kruskal-Wallis: H(2)=0.174, p=0.916) and when the agents held the same stance as the users (One-way ANOVA: F(2, 91) = 0.620, p = 0.54), .\nBy conducting post-hoc analysis, we found that, for Topic 2, participants in the 3-agent group shifted their opinions more towards the bots' (M=0.327, SD=0.751) than those in the 1-agent group (M=-0.219, SD=0.716; Dunn's Test: p<0.05).\nSimilarly, when the bots disagreed with the participants, there was a greater shift in the 3-agent group (M=0.509, SD=1.021) than in the 1-agent group (M=-0.161, SD=0.802; Dunn's Test: p<0.05). These results indicate that, after conversations with agent(s), users changed opinions more towards the bots' stance when discussing with 3 agents than with 1 agent.\nOpinion Polarization. We observed a significant difference in the polarization of participants' opinions when the agents had the same opinion as them (Kruskal-Wallis: H(2) = 9.962, p<0.01), as shown in Figure 6. There were no significant differences in the other conditions: Topic 1 (Kruskal-Wallis: H(2) = 4.434, p = 0.108), Topic 2 (F[2,91]=1.336, p=0.267), and when agents disagreed with the participants (Kruskal-Wallis: H(2) = 1.899, p = 0.386).\nA post-hoc analysis showed that when the agents and participants agreed, the 5-agent group participants became more polarized (M=0.580, SD=0.576, Dunn's test: p<0.01) than the 3-agent participants (M=0.132, SD=0.640). The 1-agent group (M=0.252, SD=0.383) did not differ significantly from either the 3-agent (p=0.134) or 5-agent groups (p=0.096). This indicates that as the number of agents increases from three to five, the polarization level of the participants increases significantly."}, {"title": "4.1.2 Qualitative.", "content": "In this section, we aim to understand how participants describe changes in their opinions due to the influence of the agents. This analysis was conducted to complement and validate the quantitative results from the previous section."}, {"title": "4.2 RQ2: Do interactions with multi-agent systems lead to stronger social influence from agents?", "content": "In this section, we analyzed both quantitative and qualitative data to understand how interacting with varying numbers of agents affects users' perceptions of social influence."}, {"title": "4.2.1 Quantitative.", "content": "We measured two types of social influence measured: informational influence and normative influence.\nInformational Influence. There were no significant differences in informational influence across the four conditions (Topic 1: H(2)=0.206, p=0.902; Topic 2: F(2)=3.182, p=0.203, Same stance: F(2)=0.590, p=0.744; Different stance: F(2)=3.764, p=0.152). These results are shown in Figure 7.\nNormative Influence. For normative influence, we found significant differences across both topics (Topic 1: H(2)=6.571, p<0.05; Topic 2: H(2)=9.111, p<0.05), when the bots' stances were the same as the users (Same stance:"}, {"title": "4.2.2 Qualitative.", "content": "To explore how agents affect users' perceived social influence and opinion change, we analyzed users' responses to open-ended question related to 1) describing impressions of the agent(s) and 2) describing reasons behind their opinion changes, to uncover the underlying mechanism of social influence. We identified two potential factors contributing to users' feelings of being influenced by the agent(s).\nFirstly, participants perceived the accuracy of the agents' arguments as a key factor in them being influenced. Across all three conditions, they frequently described the arguments as accurate, well-informed, and persuasive. This sense credibility encouraged participants to reconsider their views. For example, P67 from the 5-agent condition noted, \"I was persuaded by their insightful thoughts. They offered solid arguments about security, stress-free experiences, low accident rates, and more.\" To evaluate how this observation is different across three conditions, we conducted a similar round of analysis as in Section 4.1.2 to understand how users perceived the arguments presented by the agents. We analyzed responses to the question, \"Do you think the arguments presented by [the agents] are accurate and convincing, and why?\" We deductively coded each response as \"yes\", \"no\", or \"unsure\", with the last code being for responses that"}, {"title": "4.3 RQ3: Which user demographics are more likely to be influenced by multi-agent systems?", "content": "To identify participant characteristics that might affect susceptibility to multiple agents, we examined demographic effects on results from RQ1 and RQ2. A linear regression analysis was conducted with age, gender, and education level"}, {"title": "4.3.1 Age.", "content": "When analyzing the opinion change, the model showed a slight negative association between age and opinion change in the 3-agent condition (Topic 2: \u03b2 = -0.0263, p < 0.05, R2 = 0.189; Different Stance: f = -0.0285, p = 0.053, R2 = 0.119), while no trend was observed in 1-agent condition and 5-agent condition. Although the effect under a different stance was not statistically significant at the conventional 0.05 level, the p-value suggests a marginal trend. This indicates that younger participants tended to change their opinion more compared to older participants in the 3-agent condition. Similarly, the linear regression analysis showed no significant trend in the 1-agent or 3-agent conditions between age and normative influence. However, a negative association was observed between age and normative influence in the 5-agent condition (Topic 1: \u03b2 = -0.0347, p < 0.05, R2 = 0.148; Different Stance: \u03b2 = -0.037, p = 0.059, R2 = 0.121). The coefficient indicates that younger participants tended to report higher normative influence compared to older participants in the 5-agent condition."}, {"title": "4.3.2 Education Level.", "content": "When analyzing the linear regressions between education level and DVs (normative influence, opinion change), the results suggest a strong relationship between education level and normative influence in 1-agent condition (Topic 2: \u03b2 = -0.2625, p < 0.05, R2 = 0.132; Different Stance: \u03b2 = -0.3695, p < 0.01, R2 = 0.236). While in 3-agent and 5-agent, the trend were not observed. This suggests that participants with a lower education level tended to perceived more normative influence compared to participants with a higher education level only with a single agent; however, with multiple agents, all participants were similarly affected. As for opinion change, no significant differences were found across the three conditions."}, {"title": "5 DISCUSSION", "content": ""}, {"title": "5.1 Multi-Agent Systems as Social Groups", "content": "Overall, our study demonstrates that virtual agents can function as a cohesive social group, leading users to experience social pressure. This mirroring of human group dynamics echoes the well-known CASA (Computers as Social Actors) paradigm [54], while extending it beyond its traditional focus on single agent systems. We propose that beyond being"}, {"title": "5.2 Al Agents Creating Social Norms", "content": "An interesting phenomenon arises as a natural result of our experimental setup. As the agents in the 3- and 5-agent group expressed their views, a social norm begins to form among the agents: their repeated endorsement of a particular point of view creates (the perception of) a shared understanding of what the correct opinion is [18]. There are a great many types of social norms which we do not investigate further; however, the social norm formed here is unique in one particular aspect. Most systems that utilize social norms as a means of persuasion convey computer-mediated social norms: behaviors or attitudes of a group of people that are communicated to the user via social norm messaging [8, 17, 76, 77] (e.g. \"50% of people recycle their waste\"). In contrast, our setup introduces the unique idea of computer-created social norms. During the conversation, the agents' opinions were expressed as their own (e.g. \"I think self-driving cars are good\"), rather than being relayed from humans. With multiple agents echoing one another, this created a"}, {"title": "5.3 Design Implications", "content": "Below, we list a few design considerations for eliciting effective opinion change in multi-agent systems, as well as potential risks that might arise from the deployment of such systems."}, {"title": "5.3.1 Persuasive Intervention Systems.", "content": "Many current health interventions, both physical and digital (e.g. [6, 57]), rely on persuasive means to nudge people toward healthier choices regarding harmful behaviors like smoking or excessive drinking. These interventions may use social norm messaging (e.g. \"Most people do not smoke\" or \"On average, people believe smoking is harmful\") or personalized messages or coaching (e.g. through interactive interfaces), aiming to shift perceptions and encourage behavior change. However, while these approaches often rely on the concept of social persuasion, they neglect the core factor of group size in creating social influence. Our findings suggest that designers in fields such as health and and wellness technology can leverage multi-agent interfaces to create social norms and influence, enhancing the effectiveness of their systems. As in our study, without needing to create additional content, simply increasing the number of perceived virtual presences in a system may increase the effectiveness of its messaging. For instance, an interactive chatbot could adopt multiple personas, while a health or diet tracker app could introduce different agents that appear at different points throughout the app. By creating a cohesive group of agents that reinforce a shared stance, these systems can exert subtle social pressure that more effectively conveys a message or nudges a user towards complying with certain actions to achieve their goal."}, {"title": "5.3.2 Sense of Group and Identity.", "content": "In our study, we designed our 3- and 5-group agents to exhibit a sense of cohesion and unity. The agents occasionally reaffirmed one anothers' viewpoints (e.g. \"Those are valid points, Bella\", \"I couldn't agree with Cody more\") and engaged with other agents' arguments (e.g. \"That's a thought-provoking point, Cody\"). That these 3- and 5-agent groups were successful in creating social influence aligns with well-established literature, that normative social influence is \"greater among individuals forming a group than among an aggregation of individuals who do not compose a group\" [25]. We therefore suggest that agents in a multi-agent system should exhibit unity and cohesion to be perceived as a group, in order to create social influence.\nWe note, however, that we do not suggest that a group of agents that do not verbally reinforce one anothers' views cannot also exert social influence. Group cohesion or similarity can manifest in many ways, such as appearance, language style, identity, and more. We leave this for future work."}, {"title": "5.3.3 Optimal Number of Agents for Opinion Shifts.", "content": "In designing multi-agent systems, our findings show that simply increasing the number of agents in a system does not necessarily help opinion change - indeed, increasing from three to five agents had the reverse effect of inducing psychological reactance and reducing this change. While the correlation between group size and social pressure is well-known [4], there is scarce literature on the concrete threshold of group numbers needed to trigger psychological reactance (even in traditional human psychology). More research is needed to investigate how many, and under what conditions, a given number of agents will or will not result in resistance rather than conformance to a suggested opinion."}, {"title": "5.3.4 Risks and Challenges.", "content": "Besides the opportunities afforded by multi-agent systems, our study also highlights the potential risks of using groups of agents to influence attitudes or behaviors. This risk extends beyond health interventions to contexts like online communication, political discourse, or even unethical advertising. In these scenarios, multiple agents could be strategically deployed on social media platforms to manipulate users' thoughts on political voting, debates, or attitudes toward specific events.\nCrucially, this concern does not stem solely from misinformation or the undisclosed use of Al agents, as suggested by previous studies [69]. Our findings show that users can still be influenced by groups of virtual agents, even when they are fully aware that these agents are AI. Therefore, it is crucial for social media platforms to regulate and monitor the deployment of AI bots, setting clear limits on the number of bots each entity can have. Monitoring should focus not only on the content of individual bots but also on how multiple bots interact to reinforce specific messages, opinions, or viewpoints. Additionally, platforms should implement detection systems to identify instances where bots may be cooperating to amplify or spread information in a coordinated manner."}, {"title": "5.4 Limitations & Future Work", "content": "First, given the exploratory and foundational nature of our study, we chose contemporary social issues that we considered to be fairly neutral, yet relevant enough in public discourse"}]}