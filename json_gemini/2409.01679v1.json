{"title": "Adaptive Explicit Knowledge Transfer for Knowledge Distillation", "authors": ["Hyungkeun Park", "Jong-seok Lee"], "abstract": "Logit-based knowledge distillation (KD) for classification is cost-efficient compared to feature-based KD but often subject to inferior performance. Recently, it was shown that the performance of logit-based KD can be improved by effectively delivering the probability distribution for the non-target classes from the teacher model, which is known as 'implicit (dark) knowledge', to the student model. Through gradient analysis, we first show that this actually has an effect of adaptively controlling the learning of implicit knowledge. Then, we propose a new loss that enables the student to learn explicit knowledge (i.e., the teacher's confidence about the target class) along with implicit knowledge in an adaptive manner. Furthermore, we propose to separate the classification and distillation tasks for effective distillation and inter-class relationship modeling. Experimental results demonstrate that the proposed method, called adaptive explicit knowledge transfer (AEKT) method, achieves improved performance compared to the state-of-the-art KD methods on the CIFAR-100 and ImageNet datasets.", "sections": [{"title": "1 Introduction", "content": "With the advances of deep learning techniques, the classification performance of deep neural networks has been significantly improved, but often at the cost of increased computational and memory requirements. This trend poses challenges for deployment of deep neural networks in resource-constrained environments such as edge devices. As a way to resolve this issue, knowledge distillation (KD) has emerged, which aims to transfer knowledge from a trained large model (denoted as teacher) to a smaller model (denoted as student) [4]. It has been shown that the student model produced by KD can outperform the same-sized model directly trained from scratch [2, 8].\nKD methods can be divided into two groups according to the learning approach: feature-based and logit-based. In feature-based KD, the student model is trained to mimic the features learned at intermediate layers of the teacher model [1, 7, 11, 15, 17, 20, 22, 26, 27]. It has an advantage in terms of the performance, but is subject to complexities in layer selection and knowledge transfer mechanisms. Logit-based KD aims to align the probability distributions between the teacher and student models [5, 10, 23, 29]. This provides a more straightforward and cost-efficient approach in comparison to feature-based KD.\nIn the classical logit-based KD, the Kullback-Leibler (KL) divergence between the output probability distributions of the teacher and the student is used as the loss to be minimized [8]. In [29], it is found that this loss can be reformulated as a weighted sum of the loss for the target class and the loss for the non-target classes, and the weight given to the latter is a factor disturbing effective distillation. Based on this analysis, [29] proposes a method called decoupled knowledge distillation (DKD) by changing the weight for the second loss term, which is shown to improve the classification performance.\nIn this paper, we first analyze the classical KD and DKD in a new perspective by examining the gradients backpropagated to the student model's logits during training. We find that DKD involves an"}, {"title": "2 Related works", "content": "The classical KD method proposed in [8] first demonstrates that the teacher model's knowledge can be transferred to a student model by minimizing the KL divergence between the probability distributions of the two models. It is often explained that the softened labels obtained from the teacher model, which contains implicit knowledge in the probabilities for the non-target classes, are valuable hints to train the student model [8]. Some studies also acknowledge a regularization effect by the softened labels [24, 30].\nFeature-based KD trains the student model in a way that the features from its intermediate layers become similar to those of the teacher model. FitNet [17] distills the intermediate features of the teacher to train a student model that is deeper and thinner than the teacher. In the attention transfer (AT) method [26], the student is trained to mimic the attention map of the teacher. The work in [7] proposes a method (abbreviated as OFD) by considering design aspects including teacher transform, student transform, distillation feature position, and distance function. Relational KD (RKD) [15] transfers the relationship among the features of different data. Contrastive representation distillation (CRD) [20] employs a contrastive learning approach for distillation. Softmax regression representation learning (SRRL) [22] focuses on matching the features at the penultimate layers of the teacher and the student. Knowledge review (KR) [1] proposes to transfer knowledge among different levels.\nLogit-based KD tries to match the logits between the teacher and the student. It is advantageous over feature-based KD in terms of efficiency and simplicity, but its task performance such as classification accuracy is often lower than that of feature-based KD. To overcome this drawback, several methods have been proposed. One such method is DIST [10], which replaces the KL divergence with the correlation coefficient to maintain inter-class and intra-class relations between the teacher and the student. DKD [29] proposes to highlight the non-target class information in the loss function to transfer implicit knowledge effectively. Normalized KD (NKD) [23] also considers effective transfer of implicit knowledge by normalizing the non-target class probabilities. To distill between heterogeneous architectures without architecture-specific information, One-for-all (OFA) [5] projects intermediate features into logits. Different from these methods, the proposed method focuses on effective transfer of explicit knowledge via adaptive learning of the target class information."}, {"title": "3 Our approach", "content": ""}, {"title": "3.1 Gradient analysis", "content": "Classical KD and DKD. We first briefly review the ideas of the classical KD [8] and DKD [29].\nLet $p = [p_1, ..., p_c]$ denote the output probability distribution, where C is the number of classes.\nThen, the loss of the classical KD is given by\n\n$L_{KD} = KL(p^T || p^S) = \\sum_{i=1}^C p_i^T (log p_i^T - log p_i^S)$  (1)\n\nwhere the superscripts T and S denote the teacher and student models, respectively. Without loss of generality, we omit the temperature parameter.\nIn [29], it is shown that Eq. 1 can be re-written as\n\n$L_{KD} = KL([p_t^T, p_{\\neg t}^T] || [p_t^S, p_{\\neg t}^S]) + p_{\\neg t}^T KL(P^T || P^S)$  (2)\n\nwhere $p_t$ is the probability for the target class and $p_{\\neg t} = \\sum_{i=1,i\\neq t}^C p_i$ is the sum of the probabilities for the non-target classes. And, $P \\in \\mathbb{R}^{C-1}$ is the re-normalized probability distribution for the non-target classes, i.e., $P = [p_1, ..., p_{t-1}, p_{t+1}, ..., p_c]$ where $p_i = p_i/p_{\\neg t}$. In other words, the loss of the classical KD can be divided into two terms: the loss for target class KD (TCKD), denoted as $L_{TCKD}$, and the loss for non-target class KD (NCKD), denoted as $L_{NCKD}$. The former measures the similarity of the binary probabilities (target vs. non-target) from the teacher and the student, while the latter compares the probability distributions only for the non-target classes, which plays a role to deliver the implicit knowledge of the teacher to the student. It can be seen that the NCKD term is weighted by $p_{\\neg t}^T = 1 - p_t^T$. This means that when the teacher's confidence for the target class is high, the student can hardly learn the implicit knowledge. To avoid this issue, DKD uses a constant weight for the NCKD term:\n\n$L_{DKD} = \\alpha L_{TCKD} + \\beta L_{NCKD}$ (3)\n\nGradients to student model. As a new perspective to analyze the learning processes of KD and DKD, we consider the gradients backpropagated to the student model's logits. In other words, we examine the gradients of the loss ($L_{KD}$ or $L_{DKD}$) with respect to the logits of the student model. Let $z_i^S$ denote the ith class logit of the student model, from which the class probability is computed as the softmax output:\n\n$p_i^S = \\frac{e^{z_i^S}}{\\sum_{l=1}^C e^{z_l^S}}$  (4)\n\nFirst, let us examine the gradients in the classical KD (more details in Section A.1 of Appendix).\n\n$\\frac{\\partial L_{KD}}{\\partial z_i^S} = \\frac{\\partial L_{KD}}{\\partial p_i^S} \\frac{\\partial p_i^S}{\\partial z_i^S} + \\sum_{k=1, k \\neq i}^C \\frac{\\partial L_{KD}}{\\partial p_k^S} \\frac{\\partial p_k^S}{\\partial z_i^S} = p_i^T - p_i^S$  (5)\n\nThus, The gradient backpropagated to the logit of the student model for a class is given by the probability difference for the class between the student and the teacher.\nNext, we examine the gradients in DKD. According to Eq. 3, we can separately obtain the gradients of the TCKD and NCKD loss terms, and then add them using weights \u03b1 and \u03b2. The TCKD loss term is written as\n\n$L_{TCKD} = p_t^T (log p_t^T - log p_t^S) + p_{\\neg t}^T (log p_{\\neg t}^T - log p_{\\neg t}^S)$  (6)\n\nwhere\n\n$P_{\\neg t}^S = \\frac{e^{z_m^S}}{\\sum_{l=1, m \\neq t}^C e^{z_l^S}}$  (7)"}, {"title": "3.2 Adaptive Explicit Knowledge Transfer (AEKT) loss", "content": "The previous analysis highlights the benefit of dynamic adjustment of the gradients for effective learning of implicit knowledge. Then, we pay attention to the opportunity to apply a similar dynamic mechanism for learning of explicit knowledge. For this, we propose a new loss called adaptive explicit knowledge transfer (AEKT) loss that adaptively weights the degree of learning about the target class based on the ratio of the teacher's and student's confidence levels for the target class.\nThe proposed AEKT loss is defined as\n\n$L_{AEKT} = log(\\frac{p_t^T}{p_t^S}) \\cdot (1 - 2^{\\frac{p_t^T}{p_t^S}})$  (16)"}, {"title": "3.3 Task serialization", "content": "In logit-based KD, it is usual to employ the classification\nloss (i.e., cross-entropy (CE)) along with the distillation\nloss, which are applied on the same level, i.e., the distil-\nlation loss at the logits and the CE loss at the softmax\noutputs from the logits. This means that the logits of the\nstudent model need to be trained to satisfy two different\nprobability distributions: one having a probability of one\nfor the target class and zero for all other classes, and the\nother representing the softened distribution extracted from"}, {"title": "4 Experiments", "content": "We evaluate the proposed method on the CIFAR-100 [12] and ImageNet [3] datasets in comparison to state-of-the-art feature-based KD methods (FitNet [17], AT [26], OFD [7], RKD [15], CRD [20], SRRL [22], and KR [1]) and logit-based KD methods (classical KD [8], DKD [29], DIST [10], NKD [23], and OFA [5]). Further analysis results are also presented. All detailed experimental settings can be found in Section C of Appendix."}, {"title": "4.1 CIFAR-100", "content": "By following [29], we compare the performance of different methods in two scenarios: The first scenario uses the same architecture for both the teacher and the student, including ResNet [6], WRN [25], and VGG [19]. In the second scenario, the teacher and the student have different architectures; in particular, the student is a model designed for improved efficiency, including ShuffleNet [14, 28] and MobileNet [9, 18], while the teacher is ResNet [6], WRN [25], or VGG [19]. The pairs of the teacher and student models are set as in [29], and we use the trained teacher models provided by [29]."}, {"title": "4.2 ImageNet", "content": "As in [29], we evaluate our method on the ImageNet dataset with CNN architectures, i.e., ResNet-34 ResNet-18 and ResNet-50 MobileNet-V1. We also consider the cases with Transformer architectures, i.e., DeiT-T [21] or Swin-T [13] as the teacher and ResNet-18 or MobileNet-V1 as the student."}, {"title": "4.3 Analysis", "content": "Correlation between teacher and student. The goal of KD is to make the student mimic the knowledge of the teacher. If the student has thoroughly learned the teacher's knowledge, the inference"}, {"title": "5 Conclusion", "content": "We have proposed a new distillation method called AEKT, which includes a loss function promoting effective learning of explicit knowledge along with implicit knowledge, and a technique for serialization of the classification and distillation tasks. The effectiveness of the proposed method was shown through comparison to the state-of-the-art logit-based and feature-based KD methods. Additional analysis results also supported the benefits of the proposed loss and the serialization technique."}, {"title": "Appendix", "content": ""}, {"title": "A Derivation of the gradients", "content": "We provide more detailed derivations of the gradients shown in Section 3 of the main paper.\nLet us first define the probabilities obtained from logits. The probability for the ith class from the student, $p_i^S$ (i = 1, ...., C), is written as\n\n$p_i^S = \\frac{e^{z_i^S}}{\\sum_{l=1}^C e^{z_l^S}}$ (A.1)\n\nIn particular, the probability for the target class from the student, $p_t^S$, can be written as\n\n$p_t^S = \\frac{e^{z_t^S}}{\\sum_{l=1}^C e^{z_l^S}}$ (A.2)\n\nThe sum of the probabilities for all non-target classes from the student, $p_{\\neg t}^S$, is\n\n$p_{\\neg t}^S = \\frac{\\sum_{m=1, m \\neq t}^C e^{z_m^S}}{\\sum_{l=1}^C e^{z_l^S}}$ (A.3)\n\nThe re-normalized probability for non-target classes, $p_i^S = p_i^S / p_{\\neg t}^S$ with i \u2260 t, is written as\n\n$p_i^S = \\frac{e^{z_i^S}}{\\sum_{l=1, l \\neq t}^C e^{z_l^S}}$ (A.4)"}, {"title": "A.1 Classical KD (Eq. 5)", "content": "The classical KD loss using the KL divergence, $L_{KD}$, is given by Eq. 1 of the main paper, i.e.,\n\n$L_{KD} = \\sum_{i=1}^C p_i^T (log p_i^T - log p_i^S)$. (A.5)\n\nUsing the chain rule, the gradient of $L_{KD}$ with respect to the logit of the student $z_i^S$ can be written as\n\n$\\frac{\\partial L_{KD}}{\\partial z_i^S} = \\frac{\\partial L_{KD}}{\\partial p_i^S} \\frac{\\partial p_i^S}{\\partial z_i^S} + \\sum_{k=1, k \\neq i}^C \\frac{\\partial L_{KD}}{\\partial p_k^S} \\frac{\\partial p_k^S}{\\partial z_i^S}$ (A.6)\n\nThe first term is obtained by differentiating Eq. A.5 with respect to $p_i^S$:\n\n$\\frac{\\partial L_{KD}}{\\partial p_i^S} = - \\frac{p_i^T}{p_i^S}$ (A.7)\n\nFor the second term, there are two cases: k = i and k\u2260 i. When k = i,\n\n$\\frac{\\partial p_k^S}{\\partial z_i^S} = \\frac{e^{z_i^S} \\times (\\sum_{l=1}^C e^{z_l^S}) - (e^{z_i^S})^2}{(\\sum_{l=1}^C e^{z_l^S})^2}$ \n$\\qquad = \\frac{e^{z_i^S}}{(\\sum_{l=1}^C e^{z_l^S})} - (\\frac{e^{z_i^S}}{(\\sum_{l=1}^C e^{z_l^S})})^2$ (A.8) \n$\\qquad = p_i^S - (p_i^S)^2$"}]}