{"title": "Metric Learning for Clifford Group Equivariant Neural Networks", "authors": ["Riccardo Ali", "Paulina Kulyt\u0117", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Pietro Li\u00f2"], "abstract": "Clifford Group Equivariant Neural Networks (CGENNs) leverage Clifford algebras and multivectors as an alternative approach to incorporating group equivariance to ensure symmetry constraints in neural representations. In principle, this formulation generalizes to orthogonal groups and preserves equivariance regardless of the metric signature. However, previous works have restricted internal network representations to Euclidean or Minkowski (pseudo-)metrics, handpicked depending on the problem at hand. In this work, we propose an alternative method that enables the metric to be learned in a data-driven fashion, allowing the CGENN network to learn more flexible representations. Specifically, we populate metric matrices fully, ensuring they are symmetric by construction, and leverage eigenvalue decomposition to integrate this additional learnable component into the original CGENN formulation in a principled manner. Additionally, we motivate our method using insights from category theory, which enables us to explain Clifford algebras as a categorical construction and guarantee the mathematical soundness of our approach. We validate our method in various tasks and showcase the advantages of learning more flexible latent metric representations. The code and data are available at https://github.com/rick-ali/Metric-Learning-for-CGENNS", "sections": [{"title": "1. Introduction", "content": "Clifford (or geometric) algebras play a significant role in physics (Baylis, 2004), where they have been used to represent operators such as spinors and Dirac matrices in quantum mechanics, Maxwell's equations in electromagnetism, and Lorentz transformations in relativity. They leverage addition and the geometric product as their two fundamental operations, providing a powerful mathematical language for expressing geometric concepts in a unified manner. In particular, they can describe rotations, reflections, translations, and other geometric transformations succinctly, which frequently arise in physical phenomena.\nRecently, Clifford algebras have been introduced to deep learning: Clifford Group Equivariant Neural Networks (CGENNs) (Ruhe et al., 2023) harness the mathematical framework of Clifford algebras to represent data in a way that maintains geometric symmetries and equivariance with respect to several groups such as O(n), SO(n), E(n), and SE(n). However, they currently support only diagonal and fixed metrics to model internal network representations. These typically involve the standard metric on Euclidean space, represented as $Q_E = \\text{diag}(1, \\dots, 1)$, and the Minkowski pseudo-metric, $Q_M = \\text{diag}(-1,..., 1)$, which, in principle, have inherent physical significance but must be chosen a priori. Ideally, we would like to enable the model to learn as rich internal representations as possible without being constrained to diagonal metric matrices. Thus, inspired by recent work on latent trainable geometries (Borde & Kratsios, 2024; Lu et al., 2023), we advocate for learning the metric in a data-driven fashion via gradient descent.\nOur contributions are as follows:\n1.  We extend CGENNs by integrating learnable metrics, allowing the network to adapt its internal representations dynamically rather than relying on fixed, diagonal metrics.\n2.  We employ eigenvalue decomposition to transform the full metric matrix representation into an intermediate computationally tractable diagonal form that can easily be integrated into CGENNs, while ensuring that the input and output data remain consistent across different geometric spaces.\n3.  We leverage category theory to provide a theoretical foundation for our method. By viewing Clifford Algebras as categorical constructions, we justify the trans-"}, {"title": "2. Related Work", "content": "The Role of Symmetries in Deep Learning Broadly speaking, a symmetry of a system is a transformation that maintains a particular property of that system, ensuring it remains unchanged or invariant. Symmetries are widespread in real-world problems and data distributions that we aim to model in deep learning. While symmetries could be learned solely from data, integrating invariance and equivariance within artificial neural network architectures concerning specific group actions has emerged as an effective inductive bias, especially in scenarios with limited data (Bronstein et al., 2021).\nFrom Complex Numbers and Quaternions to Multivectors in Neural Representations Initially, the motivation for complex-valued neural representations stemmed from their superior performance within the realm of sequence modeling and Recurrent Neural Networks (RNNs) (Wisdom et al., 2016; Arjovsky et al., 2016). Likewise, other works also motivated them from the perspective of optimization (Nitta, 2002), generalization (Hirose & Yoshida, 2012), and faster learning (Danihelka et al., 2016). Following this line of research, complex-valued neural networks proposed a number of new building blocks that incorporated complex value representations and generalized traditional operations in neural networks such as batch normalization, weight initialization, ReLU activations, convolutions, etc., enabling them to discern intricate patterns that conventional real-valued networks might struggle to capture (Trabelsi et al., 2018). Quaternion based neural networks (Gaudet & Maida, 2018; Parcollet et al., 2019) further expanded on this line of research and introduced three imaginary components for data representation. More recently, the focus has shifted towards leveraging complex representations in the context of geometric deep learning (Bronstein et al., 2021) rather than in sequence modelling. Going beyond complex numbers and quaternions, Clifford algebra can encode richer representations all the way from scalars, vectors, bivectors, and trivectors, to other k-vectors. This capability has been showcased in studies that substitute convolution and Fourier operations in neural PDE surrogates with Clifford counterparts for both 2D and 3D tasks (Brandstetter et al., 2023). Particularly relevant to our work are Clifford Group Equivariant Neural Networks (CGENNs) (Ruhe et al., 2023) which leverage Clifford algebra to model equivariance under orthogonal transformations of the Clifford group. However, current CGENNs are constrained by their support solely for diagonal and fixed metrics, thereby limiting the internal geometric representations they can effectively capture.\nMetric Learning enables models to learn how to measure similarity between data points by employing an optimal distance metric tailored to specific learning tasks, rather than relying on predefined, static metrics (Kulis, 2013). Generalizing metrics beyond simple Euclidean embeddings has been extensively studied in the literature by leveraging constant curvature Riemannian manifolds, stereographic projections, and product manifolds (Gu et al., 2018; Ganea et al., 2018; Skopek et al., 2020; Borde et al., 2022; 2023; 2024; Kratsios et al., 2023). However, most of these works, similar to CGENN, pre-define the used metric before optimization. Other recent studies have proposed embeddings with associated differentiable metrics instead, which more closely resemble our proposed approach (Lu et al., 2023; Borde & Kratsios, 2024).\nCategory Theory in Deep Learning Category theory has recently gained traction in the literature as a unifying language capable of formalizing and extending existing deep learning frameworks. For instance, (Gavranovi\u0107 et al., 2024) use 2-monads to generalize geometric deep learning to non-invertible operations; (Fong et al., 2019) formalize backpropagation as a functor; and (Villani & Schoots, 2023) show that any deep ReLU network has a functionally equivalent three-layer network. Furthermore, category theory has also inspired novel and successful learning schemes: (de Haan et al., 2020) use functors to construct natural graph networks, generalizing permutation equivariance; and (Hansen & Gebhart, 2020; Bodnar et al., 2023; Barbero et al., 2022b;a) augment the message-passing procedure underlying most graph neural networks with geometric information carried by sheaves, a well-known categorical construction in algebraic topology and geometry. Similar sheaf based approaches have also been extended to hypergraphs (Duta et al., 2023)."}, {"title": "3. Background", "content": "Next, we review the mathematical foundations of our approach, including metric spaces, Clifford algebras, eigenvalue decomposition, and category theory. We also discuss the key components comprising CGENNs, such as algebra embedding layers, generalized linear layers, geometric product layers, normalization layers, and grade-wise nonlinear activation functions.\nInner Product and Metric Spaces An inner product $\\langle , \\rangle$ on a vector space V over a field $K \\in \\{\\mathbb{R},\\mathbb{C}\\}$ is"}, {"title": "3.1. Clifford Group Equivariant Neural Networks", "content": "We now outline the main components of the CGENN, as presented in (Ruhe et al., 2023). The overarching idea of this neural architecture is to accept multivectors as inputs and to process them in each of their grades separately. One could visualise such a computation scheme by picturing k parallel neural networks, each dedicated to one grade, interacting with each other via the geometric product layer. Importantly, each layer needs to be equivariant with respect to any Clifford group transformations.\nAlgebra Embeddings As the CGENN accepts multivectors as inputs, we need to embed inputs x into the Clifford algebra Cl(V, Q). We do so with the function Embed, whose form is application-specific. For example, if x is a scalar quantity (such as the charge of a particle), it is embedded as a scalar, i.e. $\\text{Embed}(x) = x\\cdot 1 + 0e_1 + 0e_2 + \\dots + 0e_{1e_2 \\dots e_n}$. Alternatively, if x is a point in V (such as the position of a particle), it is embedded as a 1-dimensional multivector, i.e. $\\text{Embed}(x) = 0 \\cdot 1 + x_1e_1 + \\dots + x_ne_n + 0e_{1e_2} + \\dots + 0e_{1 \\dots e_n}$. If x is a volume, it is embedded as $\\text{Embed}(x) = 0 \\cdot 1 + \\dots + xe_1 \\dots e_n$, and so on.\nLinear Layers The first component of this architecture is the linear layer. For $x_1, ..., x_l \\in Cl(V, Q)$, it is defined as:\n$Y_{\\text{cout}}^{(k)} = T_{\\text{lin}}(x_1,...,x_l)^{(k)} := \\sum_{\\text{cin} = 1}^l \\Phi_{\\text{cout}}^{k}x_{\\text{cin}}^{(k)}$,\nwhere $\\Phi_{\\text{cout}}^{\\text{cin}}$ is a learnt scalar parameter depending on the grade k, cin and cout are the input and output channels, and $(.)^{(k)}$ is the projection on the kth grade. Therefore, the map $T_{\\text{lin}}(x_1,...,x_l)^{(k)}$ is linear in each grade separately. This is indeed an equivariant layer, as actions of the Clifford group operate separately in each grade."}, {"title": "4. Method", "content": "The following section details the metric learning method proposed in this work, focusing on the initialisation and processing of the metric through its eigenvalue decomposition.\n4.1. Metric Initialization\nIn our method, the transition from a static metric Q, typically initialized as a diagonal matrix to reflect basic geometric properties of the space (e.g., $Q = \\text{diag}(1, 1, 1)$), to a learnable metric M involves introducing small perturbations. The process begins with Q, representing the initial geometric configuration. To facilitate learning, a perturbation is added to Q through:\n$M = \\frac{1}{2}Q + \\epsilon R$,\nwhere R is a random matrix with the same dimensions as Q, and $\\epsilon$ controls the amount of initial perturbation. To ensure that M is symmetric, we add the transpose of M to itself, since the sum of two symmetric matrices is symmetric. Hence, we obtain:\n$M = \\frac{M+M^\\top}{2} = Q + \\epsilon (R+R^\\top)$,\nsince Q is diagonal and $Q = Q^\\top$. Therefore, M is equivalent to adding \u201csymmetric\u201d noise, $R+R^\\top$, controlled by $\\epsilon$, to the initial metric. M is then passed to all downstream layers. The result is a learnable metric that enables the CGENN to dynamically refine its internal geometric representation in a data-driven fashion.\n4.2. Learnable Metric via Eigenvalue Decomposition\nThe original CGENN (Ruhe et al., 2023) implements layers consistent with a Clifford algebra Cl(V, Q) for a fixed bilinear form Q. In particular, this bilinear form is taken as a metric, and it is used in the network's computation of norms. This setup only supports diagonal metrics, simplifying the complexity of the space by considering distances that scale linearly along each axis independently. However, real-world data often exhibits correlations that are not captured well by such simplistic assumptions.\nThe transition from a diagonal to a non-diagonal metric introduces computational and theoretical challenges, particularly in the context of Clifford algebras. In this algebra, metric computations are not as straightforward as in Euclidean space. The norm calculation algorithm used in CGENNs, originally described in (Dorst et al., 2009), only supports diagonal metrics.\nThis requires us to map a non-diagonal metric to a diagonal one in a geometrically principled way. We achieve this with the metric's eigendecomposition. A metric M can be decomposed into its eigencomponents $M = P^{-1}\\Lambda P$, where P is a matrix of eigenvectors and $\\Lambda$ is a diagonal matrix of eigenvalues. Therefore, we will use $\\Lambda$ as the diagonal matrix to compute normalisations, and carefully modify the pipeline to make it geometrically meaningful and theoretically sound. We assume that the input is given in any basis $\\{e_i\\}$ in V. The metric learning procedure can be summarised as follows:"}, {"title": "4.3. Categorical Construction of Clifford Algebras", "content": "Clifford algebras have a categorical construction, which justifies and motivates the construction of Algorithm 1. See Appendix B for background on category theory. A Clifford algebra Cl(V, Q) is a pair (A, i) with A a unital associative algebra over K and $i : V \\to Cl(V, Q)$ a linear map with $i(v)^2 = Q(v) \\cdot 1_A$ for all $v \\in V$ satisfying the following universal property: given any unital associative algebra A over K and any linear map:\n$j: V \\to A$ such that $j(v)^2 = Q(v) \\cdot 1_A$ for all $v \\in V$,\nthere is a unique algebra homomorphism $f : CI(V,Q) \\to A$ such that:\n$f \\circ i = j$,\ni.e. j factors through Cl(V, Q) with i."}, {"title": "5. Experiments", "content": "In Sections 5.1, 5.2, and 5.3, we conduct experiments with signed volumes, n-body problems, and top tagging, respectively. Additionally, in Section 5.4, we explore the effect of activating metric learning at different stages of training and empirically examine its impact on optimization. In all experiments, we use the default configurations of the baseline CGENN (Ruhe et al., 2023) without any hyperparameter tuning to ensure an equitable comparison.\n5.1. O(3) Experiment: Signed Volumes\nThe signed volumes experiment involves a synthetic dataset of random 3D tetrahedra. The network processes point clouds, aiming to predict covariant scalar quantities (pseudo-scalars) under O(3) transformations. The prediction accuracy is measured by the mean-squared error (MSE) between the network's output and the actual signed volumes of the tetrahedra.\nWe compare CGENNs with a learnable metric against conventional CGENNs. Note that in this experiment, metric learning is initialised from the beginning of training alongside all other model parameters. Other baselines include a normal MLP, an MLP-based version of E(n) Equivariant Graph Neural Networks (E(n)-GNNs) (Satorras et al., 2021) (this architecture leverages artificial neural networks to update positions with scalar multiplication), Vector Neurons (VNs) (Deng et al., 2021) and Geometric Vector Perceptrons (GVPs) (Jing et al., 2020). We train our model for 130, 000 steps, the same as the original CGENN. We calculate the mean and standard deviation with 4 different seeds.\nThe experimental results, as presented in Table 1, indicate that the learnable metric improves the performance of the original CGENN model. It also outperforms all other baselines. VNs and GVPs perform similarly with an MSE slightly lower than $10^{-1}$, while MLPs achieve better performance as the number of data samples increases, reach-"}, {"title": "5.5. Reproducibility and Hyperparameters", "content": "R is always generated from a uniform distribution with all entries sampled from the interval [0, 1). Depending on the experiment, the values for $\\epsilon$ and Q are set as follows: $\\epsilon = 10^{-3}$ and Q = diag(1,1,1) for the E(3) n-body and O(3) signed volume experiments, and $\\epsilon = 10^{-7}$ and Q = diag(1,-1, -1, -1) for O(1,3) top tagging. The choice of Q is in line with (Ruhe et al., 2023)."}, {"title": "6. Conclusion & Future Work", "content": "Our research enhances CGENNs by integrating metric learning into the original model in a geometrically meaningful way. Although, as suggested by (Ruhe et al., 2023), the CGENN formulation generalizes to orthogonal groups and preserves equivariance regardless of the metric signature, previous work fixed the metric as a predefined network configuration hyperparameter.\nIn this work, instead of fixing a metric from the start, we allow the model to learn a non-diagonal metric matrix as part of the optimization process in a data-driven fashion via gradient descent. By leveraging eigenvalue decomposition to perform an internal change of basis, we use the eigenvectors to map different types of data to and from the internal neural representation back to data space, ensuring consistency of both input and output across spaces without requiring explicit modification of the original CGENN network layers, which in principle only support diagonal metrics. Additionally, we employ category theory to motivate the theoretical soundness of the approach by viewing Clifford algebras as categorical constructions.\nWe validate our method empirically against different tasks, including n-body, signed volume, and top-tagging experiments. We find that enabling metric learning does indeed lead to improved performance. We also analyze the effect of making the metric learnable at different stages of the optimization process, and we empirically find that doing so towards the end of training better guarantees stable training dynamics and generally leads to better final model performance.\nFuture research possibilities include applying this approach"}, {"title": "A. Proofs for Section 4.2", "content": "A.1. Proof 1\nWe wish to prove that, if P is an orthogonal transformation, then the algebra structure is preserved, i.e.\n$\\nu\\omega = -\\omega\\nu$\n$P\\nu P\\omega = -P\\omega P\\nu$\nz^2 = (Pz)^2$\nfor any orthogonal $v, w \\in V$ and any $z \\in V$. Because $P : (V, M) \\to (V, \\Delta)$, $z^2$ is shorthand for $M(z) = z^T Mz$ and $(Pz)^2$ for $\\Delta(Pz) = (Pz)^T\\Delta Pz$.\nDirect Proof\n[$\\to$] Assume $\\nu\\omega = -\\omega\\nu$. We show that $P\\nu P\\omega = -P\\omega P\\nu$ by evaluating each side of the equation.\n$P\\nu P\\omega = \\langle P\\nu, P\\omega\\rangle + P\\nu \\wedge P\\omega = \\langle \\nu, \\omega\\rangle + det(P) \\nu \\wedge \\omega = det(P) \\nu \\wedge \\omega$\n$-P\\omega P\\nu = -(\\langle P\\omega, P\\nu\\rangle + P\\omega \\wedge P\\nu) = -\\langle \\nu, \\omega\\rangle - det(P) \\omega \\wedge \\nu = det(P) \\nu \\wedge \\omega$\nwhere we use the fact that P is orthogonal, i.e. $\\langle P\\nu, P\\omega\\rangle = \\langle \\nu, \\omega\\rangle$, $v, w$ are orthogonal, i.e. $\\langle v, w\\rangle = 0$, and that for any linear transformation A, $A\\nu \\wedge A\\omega = det(A)\\nu \\wedge\\omega$.\n[$\\gets$] assume $P\\nu P\\omega = -P\\omega P\\nu$. Then, because v, w are orthogonal, we get the identity\n$det(P) \\nu \\wedge \\omega = - det(P) \\omega \\wedge \\nu \\implies \\nu \\wedge \\omega = -\\omega \\wedge\\nu$\nBecause $0 \\neq det(P) \\in \\{\\pm 1\\}$, as P is orthogonal. However, with the fact that $0 = \\langle P\\nu, P\\omega\\rangle = \\langle \\nu, \\omega\\rangle$ the LHS of the last equality is vw, and the RHS is -wv, proving the right-left direction of the 'only if' part of the proposition.\nNow, $z^2 = M(z) = z^TMz = (Pz)^T\\Delta Pz = \\Delta(z) = (Pz)^2$\nCategorical Proof\nBecause $P: (V, M) \\to (V, \\Delta)$ is a vector space isomorphism that preserves quadratic forms, i.e. $(V, M) \\simeq (V, \\Delta)$, and Cl is a functor, we get $Cl(V, M) \\simeq Cl(V, \\Delta)$ (Appendix B.1) via the unique extension $P = Cl(P)$.\nA.2. Proof 2\nWe wish to prove\n$\\widetilde{P(Embed(v))} = Embed(det(P)v)$\nBy direct calculation:\n$\\widetilde{P(Embed(v))} = \\widetilde{P(v \\cdot e_1e_2...e_n)} = v \\cdot \\xi_1\\xi_2 ... \\xi_n$\n$= v \\cdot (Pe_1)(Pe_2)...(Pe_n)$\n$= det(P)ve_1e_2...e_n = Embed(det(P)v)$\nB. Background on Category Theory\nHere, we provide some relevant background on Category Theory to support the discussion in Section 4.3.\nB.1. Basics of Category Theory\nCategory Theory (CT) is, essentially, the study of compositionality: the study of complex systems through their simpler parts. A key difference with set theory is that, for example, we are not allowed to inspect the internal structure of the objects. Rather, we are interested in the relationships between them.\nDefinition B.1 (Category). A category C consists of:\n\u2022 a collection ob(C) of objects"}, {"title": "B.2. Functors", "content": "If sets have functions relating them, categories are related by functors. Intuitively, a functor maps objects to objects and morphisms to morphisms in a compatible way.\nDefinition B.2 (Functor). Let A, B be categories. A functor F : A $\\to$ B consists of:\n\u2022 A function ob(A) $\\to$ ob(B), written as A $\\to$ F(A)\n\u2022 For each A, A' $\\in$ ob(A), a function A(A, A') $\\to$ B(F(A), F(A')) written as f $\\to$ F(f)\nsatisfying the following axioms:\n\u2022 F(f' $\\circ$ f) = F(f') $\\circ$ F(f) whenever f, f' are composable in A.\n\u2022 F($1_A$) = $1_{F(A)}$ whenever A $\\in$ ob(A)."}, {"title": "B.3. Isomorphism", "content": "Definition B.3 (Isomorphism). A map f : A $\\to$ B in a category C is an isomorphism if there exists a map g : B $\\to$ A in C such that gf = $1_A$ and fg = $1_B$. g is called the inverse of f and A and B are said to be isomorphic, denoted A $\\simeq$ B.\nIntuitively, two objects are isomorphic if they are essentially the same, i.e. if they share the same fundamental properties in the context of the category they are in.\nIf F: A$\\to$B is a functor and A $\\in$ ob(A), B $\\in$ ob(B) with A $\\simeq$ B, then it is easy to see that F(A) $\\simeq$ F(B). Further, F($f^{-1}$) = F(f)$^{-1}$ ."}]}