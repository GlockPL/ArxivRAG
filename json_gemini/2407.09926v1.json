{"title": "Metric Learning for Clifford Group Equivariant Neural Networks", "authors": ["Riccardo Ali", "Paulina Kulyt\u0117", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Pietro Li\u00f2"], "abstract": "Clifford Group Equivariant Neural Networks (CGENNs) leverage Clifford algebras and multivectors as an alternative approach to incorporating group equivariance to ensure symmetry constraints in neural representations. In principle, this formulation generalizes to orthogonal groups and preserves equivariance regardless of the metric signature. However, previous works have restricted internal network representations to Euclidean or Minkowski (pseudo-)metrics, handpicked depending on the problem at hand. In this work, we propose an alternative method that enables the metric to be learned in a data-driven fashion, allowing the CGENN network to learn more flexible representations. Specifically, we populate metric matrices fully, ensuring they are symmetric by construction, and leverage eigenvalue decomposition to integrate this additional learnable component into the original CGENN formulation in a principled manner. Additionally, we motivate our method using insights from category theory, which enables us to explain Clifford algebras as a categorical construction and guarantee the mathematical soundness of our approach. We validate our method in various tasks and showcase the advantages of learning more flexible latent metric representations. The code and data are available at https://github.com/rick-ali/Metric-Learning-for-CGENNs", "sections": [{"title": "1. Introduction", "content": "Clifford (or geometric) algebras play a significant role in physics (Baylis, 2004), where they have been used to represent operators such as spinors and Dirac matrices in quantum mechanics, Maxwell's equations in electromagnetism, and Lorentz transformations in relativity. They leverage addition and the geometric product as their two fundamental operations, providing a powerful mathematical language for expressing geometric concepts in a unified manner. In particular, they can describe rotations, reflections, translations, and other geometric transformations succinctly, which frequently arise in physical phenomena.\nRecently, Clifford algebras have been introduced to deep learning: Clifford Group Equivariant Neural Networks (CGENNs) (Ruhe et al., 2023) harness the mathematical framework of Clifford algebras to represent data in a way that maintains geometric symmetries and equivariance with respect to several groups such as O(n), SO(n), E(n), and SE(n). However, they currently support only diagonal and fixed metrics to model internal network representations. These typically involve the standard metric on Euclidean space, represented as $Q_E = diag(1, . . ., 1)$, and the Minkowski pseudo-metric, $Q_M = diag(-1,..., 1)$, which, in principle, have inherent physical significance but must be chosen a priori. Ideally, we would like to enable the model to learn as rich internal representations as possible without being constrained to diagonal metric matrices. Thus, inspired by recent work on latent trainable geometries (Borde & Kratsios, 2024; Lu et al., 2023), we advocate for learning the metric in a data-driven fashion via gradient descent.\nOur contributions are as follows:\n1.  We extend CGENNs by integrating learnable metrics, allowing the network to adapt its internal representations dynamically rather than relying on fixed, diagonal metrics.\n2.  We employ eigenvalue decomposition to transform the full metric matrix representation into an intermediate computationally tractable diagonal form that can easily be integrated into CGENNs, while ensuring that the input and output data remain consistent across different geometric spaces.\n3.  We leverage category theory to provide a theoretical foundation for our method. By viewing Clifford Algebras as categorical constructions, we justify the trans-"}, {"title": "2. Related Work", "content": "The Role of Symmetries in Deep Learning Broadly speaking, a symmetry of a system is a transformation that maintains a particular property of that system, ensuring it remains unchanged or invariant. Symmetries are widespread in real-world problems and data distributions that we aim to model in deep learning. While symmetries could be learned solely from data, integrating invariance and equivariance within artificial neural network architectures concerning specific group actions has emerged as an effective inductive bias, especially in scenarios with limited data (Bronstein et al., 2021).\nFrom Complex Numbers and Quaternions to Multivectors in Neural Representations Initially, the motivation for complex-valued neural representations stemmed from their superior performance within the realm of sequence modeling and Recurrent Neural Networks (RNNs) (Wisdom et al., 2016; Arjovsky et al., 2016). Likewise, other works also motivated them from the perspective of optimization (Nitta, 2002), generalization (Hirose & Yoshida, 2012), and faster learning (Danihelka et al., 2016). Following this line of research, complex-valued neural networks proposed a number of new building blocks that incorporated complex value representations and generalized traditional operations in neural networks such as batch normalization, weight initialization, ReLU activations, convolutions, etc., enabling them to discern intricate patterns that conventional real-valued networks might struggle to capture (Trabelsi et al., 2018). Quaternion based neural networks (Gaudet & Maida, 2018; Parcollet et al., 2019) further expanded on this line of research and introduced three imaginary components for data representation. More recently, the focus has shifted towards leveraging complex representations in the context of geometric deep learning (Bronstein et al., 2021) rather than in sequence modelling. Going beyond complex numbers and quaternions, Clifford algebra can encode richer representations all the way from scalars, vectors, bivectors, and trivectors, to other k-vectors. This capability has been showcased in studies that substitute convolution and Fourier operations in neural PDE surrogates with Clifford counterparts for both 2D and 3D tasks (Brandstetter et al., 2023). Particularly relevant to our work are Clifford Group Equivariant Neural Networks (CGENNs) (Ruhe et al., 2023) which leverage Clifford al-"}, {"title": "3. Background", "content": "Next, we review the mathematical foundations of our approach, including metric spaces, Clifford algebras, eigenvalue decomposition, and category theory. We also discuss the key components comprising CGENNs, such as algebra embedding layers, generalized linear layers, geometric product layers, normalization layers, and grade-wise nonlinear activation functions.\nInner Product and Metric Spaces An inner product $(,)$ on a vector space $V$ over a field $K \\in \\{R,C\\}$ is"}, {"title": "Clifford Algebras", "content": "A Clifford Algebra, denoted Cl(V, Q), extends the scope of classical algebra and subsumes algebraic structures such as complex numbers and quaternions. It is defined on a vector space V over a field K, together with a quadratic form $Q : V^2 \\to K$ that maps pairs of vectors to the field. Its algebra operation, the geometric product, expresses geometric transformations, such as the inner $\\langle .,.\\rangle$ and wedge product $\\wedge$ in algebraic terms. For all $v, w \\in V$, the geometric product $vw$ is:\n$v w = \\langle v, w\\rangle + v \\wedge w$.\nThe Clifford Algebra Cl(V, Q) is defined as a quotient of the tensor algebra $T(V) = \\bigoplus_k T^k (V)$, where $T^k (V) = V \\otimes V \u2026\u2026 \\otimes V$ k times, with the convention $T^0(V) = K$, the underlying field. The quotient defining Cl(V, Q) is T(V) modulo the ideal generated by $v \\otimes v \u2013 Q(v, v) \\cdot 1$, which essentially imposes the equation $v \\otimes v = Q(v, v) \\cdot 1$ on T(V). Clifford Algebras also have a categorical construction (see Section 4.3) which will prove relevant to our methodology.\nWith an orthogonal basis $\\{e_1, e_2, ..., e_n\\}$ for V, the algebra is spanned by elements of the form $e_{i_1} e_{i_2} \\cdot e_{i_k}$ where $0 \\le k \\le n$ and $1 < i_1 < i_2 <\u2026\u2026 < i_k < n$. These span vectors, bivectors, and higher-dimensional constructs, representing directional, area, and volumetric information"}, {"title": "Norms in Clifford Algebras", "content": "While it is clear how to calculate a norm of $v \\in V$ with $Q$, namely $||v|| = \\sqrt{v^T Q v}$, we still need to specify its corresponding operation for $x \\in Cl(V, Q)$. Let $\\beta : Cl(V, Q) \\to Cl(V, Q)$ be the main anti-involution of Cl(V, Q), the function that takes $x$ and 'inverts' the order of its components:\n$\\beta(x) = \\beta(\\sum_{I} c_i v_{i,1} \u00b7 v_{i,2} \u00b7\u00b7\u00b7 v_{i,k_i}) = \\sum_{I} c_i v_{i,k_i} \u00b7\u00b7\u00b7 v_{i, 1}.$\nFor example, in 2 dimensions:\n$\\beta(x) = \\beta(x^{(0)} \u00b7 1 + x^{(1)} \u00b7 e_1 + x^{(2)} \u00b7 e_2 + x^{(12)} \u00b7 e_1e_2) = x^{(0)} \u00b7 e_1e_2 + x^{(1)} \u00b7 e_2 + x^{(2)} \u00b7 e_1 + x^{(12)} \u00b7 1.$\nNow, let $x_1, x_2 \\in Cl(V, Q)$ and $b : Cl(V, Q) \\to CI(V, Q)$ be the function:\n$b(x_1,x_2) = (\\beta(x_1)x_2)^{(0)},$\nwhere juxtaposition is the geometric product and $(.)^{(0)}$ denotes the projection on the scalar component of the resulting multivector. Finally, by denoting $x^T Q x$ as $Q(x)$ for"}, {"title": "Eigenvalue Decomposition", "content": "In linear algebra, a matrix M in $K^{n \\times n}$ over the field K is said to be diagonalizable if there exists an invertible matrix P also in $K^{n \\times n}$ such that $M = P^{-1} \\Delta P$, where $\\Delta$ is a diagonal matrix. This process is known as the eigendecomposition of M, because the diagonal entries of $\\Delta$ represent the eigenvalues of M, and the columns of P correspond to their respective eigenvectors.\nOne can interpret P as a change of coordinates that represents a vector x in $K^n$ using the coordinates defined by the eigenvectors of M. Indeed, the i-th column of an invertible matrix T is precisely $Te_i$, indicating where T maps the canonical basis element $e_i$; and because a linear transformation is entirely characterised by its effect on the basis elements, this information is sufficient to fully describe the matrix's action. Thus, Px is simply x expressed in the basis provided by P. Within this basis, the operation of M is expressed by the diagonal matrix $\\Delta$: $Mx = \\Delta Px = \\Delta(Px)$.\nSpecifically, if K = R and M is symmetric, the Spectral Theorem ensures that M can be diagonalized with P being orthogonal, that is, $P^{-1} = P^T$. Now, if M functions as a metric, the inner product between two vectors x, y is calculated as follows:\n$x^T My = x^T P^{-1}\\Delta Py = x^T P^T \\Delta Py = (Px)^T\\Delta(Py)$,\nindicating that when the vectors are expressed in terms of the basis provided by P, the metric calculations remain consistent as if M were diagonal. This observation is crucial for our implementation since the algorithm used for metric calculations only supports diagonal metrics."}, {"title": "Basics of Category Theory", "content": "Category theory is the study of compositionality, where \u201cobjects\u201d can be studied only in their relationships to other objects. For example, to characterise a singleton set, we will not say that it is a set S with only one element, but rather, that there is a unique map from any other set to S.\nIn the first instance, we can think of a category as a collection of objects of a certain kind, such as sets, groups, and vector spaces, along with maps between them (also called morphisms or arrows) that preserve their structure, such as functions, group homomorphisms, and linear maps. Furthermore, these should be able to be composed sensibly, so that if $f : A\\to B$, $g : B \\to Candh:C \\to D$, where A, B, C, D are objects in a category and f, g, h are maps, then $(h\\circ g) \\circ f = h \\circ (g \\circ f)$. A formal definition of a category is given in Appendix B.\nFunctors, which we will use to motivate and justify our met-"}, {"title": "3.1. Clifford Group Equivariant Neural Networks", "content": "We now outline the main components of the CGENN, as presented in (Ruhe et al., 2023). The overarching idea of this neural architecture is to accept multivectors as inputs and to process them in each of their grades separately. One could visualise such a computation scheme by picturing k parallel neural networks, each dedicated to one grade, interacting with each other via the geometric product layer. Importantly, each layer needs to be equivariant with respect to any Clifford group transformations.\nAlgebra Embeddings As the CGENN accepts multivectors as inputs, we need to embed inputs x into the Clifford algebra Cl(V, Q). We do so with the function Embed, whose form is application-specific. For example, if x is a scalar quantity (such as the charge of a particle), it is embedded as a scalar, i.e. Embed(x) = x\u00b71+0e_1+0e_2+\u2026\u2026+\n$0e_{1e2... e_n}$. Alternatively, if x is a point in V (such as the position of a particle), it is embedded as a 1-dimensional multivector, i.e. Embed(x) = 0\u00b71 + x_1e_1 + \u00b7\u00b7\u00b7 + x_ne_n +\n$0e_{1e2}+...+ 0e_{1... e_n}$. If x is a volume, it is embedded as Embed(x) = 0 \u00b7 1 + \u00b7\u00b7\u00b7 + xe_{1 . . . e_n}, and so on.\nLinear Layers The first component of this architecture is the linear layer. For $x_1,...,x_I \\in Cl(V, Q)$, it is defined as:\n$Y^{(k)}_{C_{out}} = T_{lin} (x_1,...,x_I)^{(k)} := \\sum^{I}_{C_{in}=1} \\Phi^{C_{out}}_{C_{in}} < x_{C_{in}}^{(k)}>_{C_{out}}^{(k)},$\nwhere $\\Phi^{C_{out}}_{C_{in}}$ is a learnt scalar parameter depending on the grade k, $C_{in}$ and $C_{out}$ are the input and output channels, and $(.)^{(k)}$ is the projection on the kth grade. Therefore, the map $T_{lin} (x_1,...,x_I)^{(k)}$ is linear in each grade separately. This is indeed an equivariant layer, as actions of the Clifford group operate separately in each grade."}, {"title": "4. Method", "content": "The following section details the metric learning method proposed in this work, focusing on the initialisation and processing of the metric through its eigenvalue decomposition.\n4.1. Metric Initialization\nIn our method, the transition from a static metric Q, typically initialized as a diagonal matrix to reflect basic geometric properties of the space (e.g., $Q = diag(1, 1, 1)$), to a learnable metric M involves introducing small perturbations. The process begins with Q, representing the initial geometric configuration. To facilitate learning, a perturbation is added to Q through:\n$M = \\frac{1}{2} Q + \\epsilon R,$\nwhere R is a random matrix with the same dimensions as Q, and \u03f5 controls the amount of initial perturbation. To ensure that M is symmetric, we add the transpose of M to itself, since the sum of two symmetric matrices is symmetric. Hence, we obtain:\n$M = \\frac{M + M^\\top}{2} = Q + \\epsilon(R + R^\\top),$\nsince Q is diagonal and $Q = Q^\\top$. Therefore, M is equivalent to adding \u201csymmetric\u201d noise, $R + R^\\top$, controlled by \u03f5, to the initial metric. M is then passed to all downstream layers. The result is a learnable metric that enables the CGENN to dynamically refine its internal geometric representation in a data-driven fashion."}, {"title": "4.2. Learnable Metric via Eigenvalue Decomposition", "content": "The original CGENN (Ruhe et al., 2023) implements layers consistent with a Clifford algebra Cl(V, Q) for a fixed bilinear form Q. In particular, this bilinear form is taken as a metric, and it is used in the network's computation of norms. This setup only supports diagonal metrics, simplifying the complexity of the space by considering distances that scale linearly along each axis independently. However, real-world data often exhibits correlations that are not captured well by such simplistic assumptions.\nThe transition from a diagonal to a non-diagonal metric introduces computational and theoretical challenges, particularly in the context of Clifford algebras. In this algebra, metric computations are not as straightforward as in Euclidean space. The norm calculation algorithm used in CGENNs, originally described in (Dorst et al., 2009), only supports diagonal metrics.\nThis requires us to map a non-diagonal metric to a diagonal one in a geometrically principled way. We achieve this with the metric's eigendecomposition. A metric M can be decomposed into its eigencomponents $M = P^{-1}\\Delta P$, where P is a matrix of eigenvectors and \u0394 is a diagonal matrix of eigenvalues. Therefore, we will use \u0394 as the diagonal matrix to compute normalisations, and carefully modify the pipeline to make it geometrically meaningful and theoretically sound. We assume that the input is given in any basis $\\{e_i\\}$ in V. The metric learning procedure can be summarised as follows:"}, {"title": "Consistency of the Input Across Spaces", "content": "One could simply adopt \u0394 as a diagonal metric, but in this case, we would compute norms according not to the standard basis of V, but with respect to the basis given by P, introducing inconsistencies. Therefore, we need to express the input in the appropriate basis to make sense of \u0394. We do so by applying the change of coordinates P, as reflected in the experiments in Section 5, by replacing the basis vectors $e_i$ with $\\xi_i = Pe_i$, which is still a basis as P is invertible. We extend the linear map $x \\to Px$ to a map $\\mathcal{P}: Cl(V, M) \\to Cl(V, \\Delta)$ such that, for $x \\in Cl(V, M)$\n$x = x^{(0)} .1+x^{(1)} . e_1 + x^{(2)} . e_2+ . + x^{(12...n)} e_1 e_2 . . .e_n,$\n$\\mathcal{P}(x) = x^{(0)}.1+x^{(1)}.\\xi_1+x^{(2)}.\\xi_2+\u00b7\u00b7\u00b7+x^{(12...n)} \\xi_{12 ... \\xi_n}.$\nThis is motivated by the categorical construction of Clifford algebras (Section 4.3). Importantly, we preserve the algebra structure when translating multivectors from Cl(V, M) to Cl(V, \u0394):\n$\\mathcal{P}v\\mathcal{P}w = -\\mathcal{P}w\\mathcal{P}v,$\n$vw = -wv$\n$z^2 = (\\mathcal{P}z)^2,$\nfor any orthogonal v, w \u2208 V and any z \u2208 V. The proof is given in Appendix A. Notably, we preserve consistency with the embedding function outlined in Section 3.1. Operationally, when we deal with inputs v \u2208 V, we can transform them with Pv and embed the result, i.e.:\n$\\mathcal{P}(Embed(v)) = Embed(\\mathcal{P}v),$\nas $Pu = \\mathcal{P}(v^i e_i) = v^i(\\mathcal{P}e_i) = v^i\\xi_i$ using the Einstein summation convention $v^i e_i = \\sum_i v_i e_i$. Another special case is when the input v is a volume. In this case, we have the following relation:\n$\\mathcal{P}(Embed(v)) = Embed(det(\\mathcal{P})v),$\nwhich we also prove in Appendix A."}, {"title": "Consistency of the Output Across Spaces", "content": "The function $\\mathcal{P}^{-1}$ is implemented depending on the specific problem and is a crucial step of our learning procedure. It is particularly important when the output of a CGENN has a physical interpretation. Suppose that the learning task is to predict the particle's position in an n-body problem (Section 5.2). Then, if no change of coordinates is applied, the output position is expressed in the basis $\\{\\xi_i\\}$, but the input in the basis $\\{e_i\\}$. We harmonise the two by applying $\\mathcal{P}^{-1} : Cl(V, \\Delta) \\to Cl(V, M)$, the extension of $\\mathcal{P}^{-1}$, which we define analogously to $\\mathcal{P}$. For any:\n$x = x^{(0)} . 1 + x^{(1)} . \\xi_1 + x^{(2)} . \\xi_2 + \u00b7\u00b7\u00b7 + x^{(12...n)} \\xi_{12 ...\\xi_n},$\n$\\mathcal{P}^{-1}(x) = x^{(0)} . 1 + x^{(1)} . e_1 + x^{(2)} . e_2 + ...$\n$+ x^{(12...n)} e_1 e_2...e_n.$\nFor example, if the output of the network is:"}, {"title": "4.3. Categorical Construction of Clifford Algebras", "content": "Clifford algebras have a categorical construction, which justifies and motivates the construction of Algorithm 1. See Appendix B for background on category theory. A Clifford algebra Cl(V, Q) is a pair (A, i) with A a unital associative algebra over K and $i : V \\to Cl(V, Q)$ a linear map with $i(v)^2 = Q(v) \u00b7 1_A$ for all v \u2208 V satisfying the following universal property: given any unital associative algebra A over K and any linear map:\n$j: V \\to A$ such that $j(v)^2 = Q(v) \u00b7 1_A$ for all v \u2208 V,\nthere is a unique algebra homomorphism $f : CI(V,Q) \\to A$ such that:\n$f\\circ i = j,$\ni.e. j factors through Cl(V, Q) with i."}, {"title": "5. Experiments", "content": "In Sections 5.1, 5.2, and 5.3, we conduct experiments with signed volumes, n-body problems, and top tagging, respectively. Additionally, in Section 5.4, we explore the effect of activating metric learning at different stages of training and empirically examine its impact on optimization. In all experiments, we use the default configurations of the baseline CGENN (Ruhe et al., 2023) without any hyperparameter tuning to ensure an equitable comparison.\n5.1. O(3) Experiment: Signed Volumes\nThe signed volumes experiment involves a synthetic dataset of random 3D tetrahedra. The network processes point clouds, aiming to predict covariant scalar quantities (pseudo-scalars) under O(3) transformations. The prediction accuracy is measured by the mean-squared error (MSE) between the network's output and the actual signed volumes of the tetrahedra.\nWe compare CGENNs with a learnable metric against conventional CGENNs. Note that in this experiment, metric learning is initialised from the beginning of training alongside all other model parameters. Other baselines include a normal MLP, an MLP-based version of E(n) Equivariant Graph Neural Networks (E(n)-GNNs) (Satorras et al., 2021) (this architecture leverages artificial neural networks to update positions with scalar multiplication), Vector Neurons (VNs) (Deng et al., 2021) and Geometric Vector Perceptrons (GVPs) (Jing et al., 2020). We train our model for 130, 000 steps, the same as the original CGENN. We calculate the mean and standard deviation with 4 different seeds.\nThe experimental results, as presented in Table 1, indicate that the learnable metric improves the performance of the original CGENN model. It also outperforms all other baselines. VNs and GVPs perform similarly with an MSE slightly lower than $10^{-1}$, while MLPs achieve better performance as the number of data samples increases, reach-"}, {"title": "5.2. E(3) Experiment: n-body", "content": "The n-body experiment, as introduced by (Kipf et al., 2018), sets a benchmark for assessing equivariant neural networks in the domain of physical system simulation, a topic further researched by (Han et al., 2022). This experiment challenges neural architectures to predict the threedimensional paths of n (we use n = 5) charged particles, thereby evaluating their ability to accurately model dynamical systems.\nWe compare our model against the original CGENN, lacking metric learning features, as well as steerable SE(3)-Transformers (Fuchs et al., 2020), Tensor Field Networks (TFNs) (Thomas et al., 2018), Neural Message Passing for Quantum Chemistry Networks (NMPs) (Gilmer et al., 2017), Radial Fields (K\u00f6hler et al., 2020), E(n)-GNNs (Satorras et al., 2021), and Steerable E(3) Equivariant Graph Neural Networks (SEGNNs) (Brandstetter et al., 2022).\nFor this experiment, we train the network for 10,000 steps over 6 different seeds to obtain the mean and standard deviation. We activate the metric learning at 90% of the training duration. We further analyse the effect of activating metric learning earlier during the training process in Section 5.4.\nThe findings, as detailed in Table 2, indicate an improvement of our metric-augmented CGENN over the baseline CGENN and other alternative methods. It is worth noting that the CGENN presented in (Ruhe et al., 2023) was trained for 131,072 steps, significantly longer than ours, and nonetheless, we achieve a better performance."}, {"title": "5.3. O(1, 3) Experiment: Top Tagging", "content": "Jet tagging is a technique in collider physics for categorizing the high-energy jets spawned by particle collisions, such as those detected by the ATLAS detector at CERN (Aad et al., 2008; Kogler et al., 2019). In particular, the experiment presented here, in line with (Ruhe et al., 2023), focuses on jet tagging for top quarks, the heaviest particles within the Standard Model (Incandela et al., 2009). Our evaluation is based on the benchmark provided by (Kasieczka et al., 2019)."}, {"title": "5.4. Metric Activation", "content": "To identify the optimal timing for metric activation in CGENNs, we explore different activation regimes. Our results indicate that the timing of metric activation plays a crucial role in the efficiency of learning. The challenge in metric learning within CGENNs lies in the complexity of optimising a single metric that affects every layer of the network at once. Therefore, early activation might overcomplicate training. Conversely, late activation could function as a fine-tuning stage, making the optimisation problem more tractable."}, {"title": "5.5. Reproducibility and Hyperparameters", "content": "R is always generated from a uniform distribution with all entries sampled from the interval [0, 1). Depending on the experiment, the values for \u03f5 and Q are set as follows:\n\u03f5 = $10^{-3}$ and Q = diag(1,1,1) for the E(3) n-body and O(3) signed volume experiments, and \u03f5 = $10^{-7}$ and Q = diag(1,-1, -1, -1) for O(1,3) top tagging. The choice of Q is in line with (Ruhe et al., 2023)."}, {"title": "6. Conclusion & Future Work", "content": "Our research enhances CGENNs by integrating metric learning into the original model in a geometrically meaningful way. Although, as suggested by (Ruhe et al., 2023), the CGENN formulation generalizes to orthogonal groups and preserves equivariance regardless of the metric signature, previous work fixed the metric as a predefined network configuration hyperparameter.\nIn this work, instead of fixing a metric from the start, we allow the model to learn a non-diagonal metric matrix as part of the optimization process in a data-driven fashion via gradient descent. By leveraging eigenvalue decomposition to perform an internal change of basis, we use the eigenvectors to map different types of data to and from the internal neural representation back to data space, ensuring consistency of both input and output across spaces without requiring explicit modification of the original CGENN network layers, which in principle only support diagonal metrics. Additionally, we employ category theory to motivate the theoretical soundness of the approach by viewing Clifford algebras as categorical constructions.\nWe validate our method empirically against different tasks, including n-body, signed volume, and top-tagging experiments. We find that enabling metric learning does indeed lead to improved performance. We also analyze the effect of making the metric learnable at different stages of the optimization process, and we empirically find that doing so towards the end of training better guarantees stable training dynamics and generally leads to better final model performance."}, {"title": "A. Proofs for Section 4.2", "content": "A.1. Proof 1\nWe wish to prove that, if P is an orthogonal transformation, then the algebra structure is preserved, i.e.\n$\\mathcal{P}v\\mathcal{P}w = -\\mathcal{P}w\\mathcal{P}v$\n$vw = -wv$\n$z^2 = (\\mathcal{P}z)^2$\nfor any orthogonal v, w \u2208 V and any z \u2208 V. Because P : (V, M) \u2192 (V, \u0394), $z^2$ is shorthand for M(z) = $z^T Mz$ and $(Pz)^2$ for $\u0394(Pz)$ = $(Pz)^T\u0394Pz$.\nDirect Proof\n$\\longrightarrow$ Assume vw = -wv. We show that PvPw = \u2212PwPv by evaluating each side of the equation.\n$\\mathcal{P}v\\mathcal{P}w = (\\mathcal{P}v, \\mathcal{P}w) + \\mathcal{P}v \\wedge \\mathcal{P}w = (v, w) + det(\\mathcal{P}) v \\wedge w = det(\\mathcal{P}) v \\wedge w$\n$-\\mathcal{P}w\\mathcal{P}v = \u2212((\\mathcal{P}w, \\mathcal{P}v) + \\mathcal{P}w \\wedge \\mathcal{P}v) = \u2212(v, w) \u2013 det(\\mathcal{P}) w \\wedge v = det(\\mathcal{P}) v \\wedge w$\nwhere we use the fact that P is orthogonal, i.e. $(\\mathcal{P}v, \\mathcal{P}w) = (v, w)$, v, w are orthogonal, i.e. (v, w) = 0, and that for any linear transformation A, Av \u2227 Aw = det(A)v \u2227w.\n$\\longleftarrow$ assume PvPw = \u2212PwPv. Then, because v, w are orthogonal, we get the identity\n$det(\\mathcal{P}) v \\wedge w = \u2212 det(\\mathcal{P})w \\wedge v \\Rightarrow v \\wedge w = -w \\wedge v$\nBecause 0 \u2260 det(P) \u2208 {\u00b11}, as P is orthogonal. However, with the fact that 0 = $(\\mathcal{P}v, \\mathcal{P}w) = (v, w)$ the LHS of the last equality is vw, and the RHS is -wv, proving the right-left direction of the 'only if' part of the proposition.\nNow, $z^2$ = M(z) = $z^TMz$ = $(\\mathcal{P}z)^T\u0394\\mathcal{P}z$ = $\u0394(z)$ = $(\\mathcal{P}z)^2$\nCategorical Proof\nBecause $\\mathcal{P}: (V, M) \\to (V, \u0394)$ is a vector space isomorphism that preserves quadratic forms, i.e. $(V, M) \u2243 (V, \u0394)$, and Cl is a functor, we get Cl(V, M) \u2243 Cl(V, \u0394) (Appendix B.1) via the unique extension $\\mathcal{P}$ = Cl($\\mathcal{P}$).\nA.2. Proof 2\nWe wish to prove\n$\\mathcal{P}(Embed(v)) = Embed(det(\\mathcal{P})v)$\nBy direct calculation:\n$\\mathcal{P}(Embed(v)) = \\mathcal{P}(v \\cdot e_1 e_2... e_n) = v \u00b7 \\xi_1 \\xi_2 . . . \\xi_n$\n$= v \u00b7 (\\mathcal{P}e_1)(\\mathcal{P}e_2)... (\\mathcal{P}e_n)$\n= det(\\mathcal{P})ve_1e_2... e_n = Embed(det(\\mathcal{P})v)"}, {"title": "B. Background on Category Theory", "content": "Here, we provide some relevant background on Category Theory to support the discussion in Section 4.3.\nB.1. Basics of Category Theory\nCategory Theory (CT) is, essentially, the study of compositionality: the study of complex systems through their simpler parts. A key difference with set theory is that, for example, we are not allowed to inspect the internal structure of the objects. Rather, we are interested in the relationships between them.\nDefinition B.1 (Category). A category C consists of:\n\u2022 a collection ob(C) of objects"}]}