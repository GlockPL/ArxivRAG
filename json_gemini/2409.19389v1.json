{"title": "Co-design of a novel CMOS highly parallel, low-power, multi-chip neural network accelerator", "authors": ["W. Hokenmaier", "R. Jurasek", "E. Bowen", "R. Granger", "D. Odom"], "abstract": "Why do security cameras, sensors, and siri use cloud\nservers instead of on-board computation? The lack of very-low-\npower, high-performance chips greatly limits the ability to field\nuntethered edge devices. We present the NV-1, a new low-power\nASIC AI processor that greatly accelerates parallel processing\n(~10X) with dramatic reduction in energy consumption (>100X), via many parallel combined processor-memory units, i.e.,\na drastically non-von-Neumann architecture, allowing very large\nnumbers of independent processing streams without bottlenecks\ndue to typical monolithic memory. The current initial prototype\nfab arises from a successful co-development effort between\nalgorithm- and software-driven architectural design and VLSI\ndesign realities. An innovative communication protocol minimizes\npower usage, and data transport costs among nodes were vastly\nreduced by eliminating the address bus, through local target\naddress matching. Throughout the development process, the\nsoftware/architecture team was able to innovate alongside the\ncircuit design team's implementation effort. A digital twin of\nthe proposed hardware was developed early on to ensure that\nthe technical implementation met the architectural specifications,\nand indeed the predicted performance metrics have now been\nthoroughly verified in real hardware test data. The resulting\ndevice is currently being used in a fielded edge sensor application;\nadditional proofs of principle are in progress demonstrating the\nproof on the ground of this new real-world extremely low-power\nhigh-performance ASIC device.", "sections": [{"title": "I. INTRODUCTION", "content": "The low-power, high-performance AI processor described\nhere, the NV-1, is a joint development effort between Non-Von\nLLC and Green Mountain Semiconductor. Non-Von's archi-\ntecture designs originated from a novel machine \"instruction\nset\" of fundamental parallel operations and software, that were\ninitially derived (at Dartmouth College's Brain Engineering\nLaboratory) from the operation and arrangement of circuitry\nin the brain [1]. The collaboration between Non-Von and\nGreen Mountain Semiconductor then arose to confront the\nchallenge of translating this instruction set into a correspond-\ningly efficient hardware implementation. Throughout the\ncollaborative process, from the initial architecture designs\nall the way through tapeout, a digital-twin approach has\nbeen used to enable the closed-loop communication between\nhardware capabilities from the engineering team and algorithm\ndevelopments from the software team. The process included\nsometimes modifying or even dropping certain instructions in\norder to keep the node size minimal. This effective simula-\ntion and communication approach facilitated the two teams'\njoint optimization of the final design for size, performance,\nand power, ensuring that proposed hardware implementations\ncontinued to meet the intended performance targets.\nNotably, the approach encompasses the design of allowing\nAI network sizes far beyond a single die; the communication\nprotocol expands seamlessly beyond individual die boundaries,\nallowing a multitude of identical chiplet processors to be\nconnected to achieve a targeted network node count. Thus\na given configuration may be as small as a single chip or\nchiplet for applications domains such as internet-of-things\n(IoT) low-power devices, and also can directly scale up to\nhuge arrays for some uses such as server farms, while still\noperating at comparatively very low power budgets. (Although\nthe approach is fully compatible with very small fab tech-\nnology, the initial low-cost prototype presented here used\n28nm TSMC manufacturing.) The interface integrates with\nFPGAs and SoCs for overall communication, so that one or\nmultiple chiplets can act either alone or as massively parallel\nAI coprocessors.\nThe first prototype (NV-1) includes 3200 cores per chip\nwith seamless I/O compatibility to increase array size via\nchaining chips. During testing, this chip achieved 447 GB/s\nper 0.25 W, thus demonstrating both high performance and a\nradical power-use improvement over other comparable hard-\nware devices (see further discussion in Results). The chip also\nhas been fielded in real-world settings, performing real-time\nprocessing of a chemical sensor, with a power budget of < 10\nmW, providing a direct initial demonstration that the chip is\noperational and applications-ready."}, {"title": "II. BACKGROUND", "content": "Software developers have forever been at the mercy of the\nhardware that is available to them [2]. The limitations of\ngiven hardware designs superimpose substantial constraints on\nalgorithm and software design. In particular, algorithms that\nare intrinsically parallel will be enormously slowed down by\ntypical hardware. The typical approach has been to use GPUs\nand related hardware, but GPUs were of course designed for\nspecialized image-processing operations, rather than broader\nparallel algorithms, and most systems typically must be written\n(or re-written) for GPU compatibility.\nThe systems designed at Dartmouth and Non-Von [1] de-\nrive from operations of extremely large numbers of simple\nparallel elements in complex arrangements (neurons in brain\ncircuitry); these are intrinsically massively parallel (rather\nthan parallelized versions of inherently serial methods). Such\nintrinsically parallel algorithms are greatly sped up by ap-\npropriate parallel hardware, but it is highly rare and unusual\nfor such hardware to be constructed for these parallel algo-\nrithms. Instead, the software must typically be adapted and\ncompromised to available hardware, rather than new hardware\narchitectures being developed to accommodate the parallel\ndesigns. The repurposing of GPUs to run accelerated neural\nnetworks provides this necessity for compromised software\n[3]; this approach is now so widespread that it is almost\nforgotten that GPUs are indeed far from an ideal hardware\nenvironment for parallel systems in general.\nAgain, GPUs were developed for particular image-\nprocessing tasks rather than for parallel algorithms in general.\nGPUs simply have been used in this adapted form solely be-\ncause they existed, and they were far closer to parallel software\nneeds than standard CPU designs. But to take seriously the\nneeds of massively parallel software, and to design hardware\nspecifically for these needs, has been almost entirely absent\nfrom the field. Moreover, the need for very low power use\nsuch as required for fielded \"internet of things\" (IOT),\nsensors, medical devices, and much more, in environments\nwhere large batteries or power sources are extremely limited\nhas been a longstanding unmet need. Rather than the\ncontinued repurposing of hardware developed for other tasks,\nsuch as GPUs, the hardware presented here was specifically\ndeveloped for low-power, high-performance massively parallel\nsystems.\nCurrent hardware for neural network solutions still use\nGPUs for training [4]. Hardware engineers have opened up\nlow level access for software engineers to explore more\nefficient algorithms, optimizing data movement and improving\nefficiency. Because of the success of GPUs in neural network\naccelerations, engineers have developed many different hard-\nware solutions from training chips, inference chips, low power\nedge devices and high performance cloud architectures. For\nexample, Google has released the TPU (Tensor Processing\nUnit) for its data servers.\nHowever, the current solutions for generative AI are not\nscalable, with current models taking racks containing hundreds\nof chips to run [4]. Moreover, the power needs (and cooling\nneeds) for current hardware typically entails very specific sit-\ning for server farms, often specifically at sites of hydroelectric\ndams and other resources [5]. Convolutional neural networks,\nother deep neural networks, and transformers, all can be made\nsomewhat more efficient, but for many hardware solutions it\nrequires a large amount of batching to achieve efficiency.\nFrom a software engineers perspective, this is clearly a\nlimitation and drives solutions to an outcome that may not\nbe needed for the original problem. Current state of the art\ninstruction sets also impose limitations. Creating hardware\nthat focuses specifically on the instructions necessary, instead\nallows for vastly more efficient designs to be realized. Most\ncurrent cores have the ability to run in a flexible manner\nsupporting more than a program may need [17], and although\nthis gives a flexible processing architecture, it trades that\nflexibility for impaired performance. In NV-1 we instead focus\non a specific instruction set that is hugely accelerated, while\nother portions of software can be picked up by a coprocessor."}, {"title": "III. DESIGN", "content": "Co-design of the software and hardware systems was cru-\ncial for rendering Non-Von's initial pioneering instruction set\narchitecture for neural network acceleration into a complete\nworking solution. To facilitate this parallel development of\nsoftware and hardware, a digital twin was created in the form\nof a C++ software executable hardware model. This was done\nfrom the beginning of the project based initially on behavioral\nVerilog models, and then maintained throughout the project, as\nhigh level models were subsequently replaced with synthesized\nRTL code. The model allowed for the abstraction of hardware\ndetails and provided an equivalent behavioral representation of\nthe hardware to be developed. By both parties agreeing on the\nfunctionality of the model, a clear goal for the programming\nand for the hardware design was defined. This methodology\nwas the groundwork for later verification on the resulting hard-\nware. Post tapeout, the results wanted from the silicon were to\nget physical power numbers along with showing functionality\nin hardware. The same waveforms used to simulate the chip\nwere able to be used as vectors in the physical testing, further\ngaining confidence in the methodology.\nThe architecture of a single NV-1 node is made up of\nfour main sub-blocks. (Fig 4). The Message Handler is the\ninterface of the node. The block handles all node-to-node\ncommunication on the bus, along with control of the system,\ndecoding the nodes programming and initiating the system to\nstart during a node activation. The Memory Handler and the\nSRAM work hand in hand, holding the nodes' communication\ninformation. The brains of the system is the IPU which handles\nthe functionality of the nodes doing all of the calculations with\ndata handed to it from the Message Handler. This structure\nis repeated throughout the whole chip in an array creating a\ndistributed computational system that can process inferences\nwith radically less power and fewer operations than in typical\nvon Neumann implementations.\nThe initial minimal concept utilizes 64k cores. While any\ncore can perform any of the defined instructions, in typical\npractice each core is initialized to perform just one task.\nBy allowing only one task per core, the run time sending\nof instructions is not needed, and both the power and time\nfor sending the instruction is removed. This is both different\nfrom a traditional CPU where instructions are sent for each\ncommand during execution, and from a GPU where a single\ninstruction is sent to all cores and the same instruction is\nprocessed on every core with different data (SIMD). In the NV-\n1 chip presented here, data can be sent from each core to every\nother core. Each core maintains a boot-loaded address table\ndefining its connections to other cores. This in particular was\na concept easily realized in software, but not a straightforward\ntask in hardware. Physical wiring limitations and timing con-\nsiderations are problematic for bidirectional communication of\n64k cores. Each core has a memory depth for core connections.\n256 individual 16 bit numbers allow for the node to receive up\nto 256 other nodes output. An epoch is defined as the action\nof every core processing the messages from every other core\nin its received address memory and passing the results on for\nthe next epoch. With intelligent programming of each core,\nrepetitive tasks can be executed with very high efficiency.\nFor this prototype, a multi-project wafer tapeout, the maxi-\nmum chip size was intentionally limited. The jointly developed\nreduced instruction set made it possible to optimize the core\nphysical size to maximize the number of compute cores per\ndie. Furthermore, innovation was needed to achieve a fully\nconfigurable bidirectional communication solution for up to\n64k cores. The predefined address table removes the power\nand area intensive address bus, such that only data is being\ntransmitted. This first prototype includes 3200 cores. It is\nnotable that the communication protocol extends seamlessly\nbeyond die boundaries, enabling the creation of arbitrary-"}, {"title": "IV. RESULTS", "content": "Throughout the design process the functionality of the\nchip was under the scrutiny of the Universal Verification\nMethodology (UVM). The expected data for this testbench\nwas validated from both the GMS side and the Non-Von design\nteams. This proved to be a good vehicle of cross understanding\nfrom both hardware and software sides of the GMS and Non-\nVon design team. A shared C++ model was used to generate\nthe expected data; this model was iteratively updated and\nchecked by both teams to ensure that correct functionality was\ninterpreted in the same way from the top level abstraction\nto the hardware. Once the correct functionality was agreed\nupon, the checker component of the UVM testbench could be\nutilized.\nThe testbench is able to run a full chip simulation in Verilog,\nwith either random nodes or pre-programmed in order to\ntest potential corner cases. The whole system is then run,\nboth testing the proper setup procedure, and end-functionality\ncorrectness. The chip is viewed as a black box at the top to\nensure proper data out and the nodes are also checked at the\ngreybox level to ensure proper node-to-node communication.\nWithin the testbench, nodes have been verified for correct\nmessage receiving and computation. This node message is then\nproperly shifted through chip output and deemed correct at the\nblack box level.\nThe verification effort found correct functionality for all\nof the instructions in the instruction set, along with correct\ncommunication between nodes, and proper operation at the\nchip level.\nFigure 6a shows relative current per instruction for the NV-1\nchip design, measured at 6.25 MHz, providing the root values\nfor calculating speed and power tradeoffs, which are shown in\nFigures 5 and 7. It is worth noting that these figures amount to\na max memory bandwidth of 447 GB/s per 0.25 W of power\n(number of nodes * single read per clock * clock speed, i.e.,\n447 GB/s = 3200 nodes * 50 MHz * (16 + 8 bits) / 8 / 1024 /\n1024 / 1024) for a single NV-1 chip, and a corresponding 7.2\nTB/s for an array of 16 chips. (Note that Fig 6 shows values\nfor 6.25 MHz whereas Fig 7 and memory bandwidth figures\nare values for 50 MHz).\nThe NV architecture was designed from first principles to\neliminate almost all memory bandwidth bottlenecks, which is\na considerable throughput and efficiency limitation in CPUs\nand GPUs. Because it is so typical for memory to be off-\nchip, the concept of memory bandwidth is thus often thought\nof in terms of I/O protocol (such as DDR3), rather than in\nterms of the effect that it has on the time and efficiency\ncosts of real applied usage. Imagine beginning with a current\nGPU and inquiring how its performance would be affected by\nchanges to its memory. First of all, if memory could be placed\non-chip this would itself result in an enormous speedup in\nprocessing of the GPU in real applications. Even with on-chip\nmemory, much of the von-Neumann bottleneck would still\nslow the system down if that memory still has to be treated as\na monolithic entity that must be processed, so secondly, if the\nnewly on-chip memory could then instead be distributed across\nprocessing units into memory blocks that were independent of\neach other, then further speedups could be achieved. These two\nsteps (placement on chip, and independent distribution across\nprocessors) are at the heart of the new architecture, rendering\nit highly non-von-Neumann in design.\nNote that these enormous speedups do not change the TOPS\nmeasures at all. TOPS measures are treated independently of\nany memory usage costs. That is, enormous speedups due to\nelimination of memory bottlenecks will not even show up as\nan improvement, if all one looks at are TOPS measures. Thus\nTOPS measures are highly misleading in such cases, since\nthey cannot reflect speedups that arise due to re-architecting\nof memory.\nWe therefore provide a range of measures that are intended\nto enable approximate apples-to-apples comparisons, i.e., what\ntheoretic and pragmatic gains would be achieved when switch-\ning from the characteristics of one type of chip to another type,\nsuch as CPUs to GPUs, CPUs or GPUs to non-von-Neumann\narchitectures, etc.\nA contemporary GPU has a reported peak memory bandwidth\nof 3.35 TB/s [4]. Calculating the peak memory bandwidth\nof NV-1 entails summing node-internal memory reads that\ncan be performed during the course of computing a single\noperation: $\\frac{(max\\_num\\_ops\\_per\\_sec * max\\_bits\\_per\\_op)}{8 * 1024 * 1024 * 1024}$. Here we simply report the percent\nutilization that is possible given the nature of a memory\nbottleneck on particular hardware. Let $f = min(compute,$\nbandwidth/n_bytes_per_op) / compute where n_bytes_per_op\n= 3*16/8 =6 assumes that an operation uses two 16-bit inputs\nas operands and one 16-bit instruction. Then units(f) = ((GB/s\n/ 1024) / bytes required per op) / TOPS. Figure 5 shows this\nas compute core utilization in the presence of the memory\nbottleneck.\nWe emphasize that these numbers are intended to illustrate\nthe struggle that is presented by monolithic external memories.\nIn practice, caches are used to avoid this, and those caches are\nnot represented in these numbers in Fig 5. Standard approaches\nbecome very limited, as seen in the figure, because though they\nreadily add more compute power (TOPS), they nonetheless\ncannot add memory bandwidth anywhere near as easily. (This\nis reflected in how the ARM Cortex does well in this figure:\nit is a single core, so not much compute to consume memory\ncycles.) In sum, this is not to say that memory bandwidth\nconsiderations are the sole factor in performance, but we wish\nto emphasize that it is in fact important and it is routinely\noverlooked in measures such as TOPS.\nIt should also be noted that the NV-1 is merely the first\nfabbed issuance of the Non-Von chip line; substantial further\nincreases already are estimated in the upcoming NV-2 chip, us-\ning the same estimation methods that correctly led to previous\nvery accurate predictions of NV-1 performance. It is highly\nnotable that NV-1 does not use caches at all, nor a global\nmemory space. Designers of GPUs extensively use caches to\nminimize the burden of their memory bottlenecks; these come\nat a cost of power, space (e.g., for cache coherence logic), and\nunpredictable timing. Figure 7 contains partial information,\nextracted from a range of sources, to roughly compare power,\nTOPS, and the resulting efficiency ratio, across a range of"}, {"title": "V. CONCLUSIONS", "content": "The NV-1 test chip has been successfully manufactured\n(28nm TSMC technology), received in packaged dies, and\nfunctionally characterized and verified. System-level integra-\ntion has been carried out to incorporate the chips in an existing\nsensor apparatus that has been tested in fielded conditions.\nThe measured results from this new chip, shown in Figures\n5, 6, and 7, demonstrate that it exhibits very high memory\nbandwidth performance at radically low power usage, outper-\nforming standard competing chips by orders of magnitude.\nThe aim was to produce a new generation of AI hardware,\nrather than ongoing adaptation of systems such as GPUs,\nthat were intrinsically designed for quite different purposes.\nThe new NV platform is specifically designed to accelerate\nmassively parallel software, thus providing a natural processor\nand coprocessor setting for innovative development of rad-\nically parallel systems. Moreover, these new platforms will\nexecute at extremely low power that is, at just tiny fractions\nof the power budgets of typical extant devices. Working\ndemonstrations have been implemented to run the Whisper\ntransformer-based real-time speech-to-text system with very\nlow power, and to run a fielded real-time chemical sensor also\nwith very low power (< 10 mW).\nThis project successfully demonstrates how software and\nhardware engineers can work together to co-design and opti-\nmize overall outcomes in terms of die size, performance, and\npower consumption. Rather than the necessity of compromis-\ning, via the use of hardware designs that happen to be there for\nother purposes, the possibility now arises to take innovative\nalgorithms and software, and produce hardware ASIC designs\nthat are well fitted to executing such software both with high\nperformance and very low power.\nWith the ever increasing demands of AI hardware capa-\nbilities, especially in fielded low-power settings, this type of\ncodevelopment effort, aided by a digital twin allowing for\na continuous interdisciplinary verification and communication\nloop, may guide future projects to optimize TOPS/W not only\nas a pure hardware engineering task but as a joint endeavor.\nDesign efforts are under way towards the next version, NV-2,\nwhich will further improve on power usage and minimize the\nphysical size of each core through resource sharing.\nCurrent edge-focused processors are highly challenged by\nrestrictive low power budgets and high performance require-\nments at the edge in practice, and they still typically resort\nto using cloud computation that is costly (both in dollars\nand in power usage). We show here that even this initial\nprototype NV-1 device already drastically outperforms current\ntechnology in parallel computation tasks, both in performance\nand in power consumption. The ongoing approach addresses\na very clear need that is seen across industries attempting to\ndeploy AI and ML in real fielded applications."}]}