{"title": "CESNET-TIMESERIES24: TIME SERIES DATASET FOR\nNETWORK TRAFFIC ANOMALY DETECTION AND FORECASTING", "authors": ["Josef Koumar", "Karel Hynek", "Tom\u00e1\u0161 \u010cejka", "Pavel \u0160i\u0161ka"], "abstract": "Anomaly detection in network traffic is crucial for maintaining the security of computer networks\nand identifying malicious activities. One of the primary approaches to anomaly detection are\nmethods based on forecasting. Nevertheless, extensive real-world network datasets for forecasting\nand anomaly detection techniques are missing, potentially causing performance overestimation of\nanomaly detection algorithms. This manuscript addresses this gap by introducing a dataset comprising\ntime series data of network entities' behavior, collected from the CESNET3 network. The dataset\nwas created from 40 weeks of network traffic of 275 thousand active IP addresses. The ISP origin of\nthe presented data ensures a high level of variability among network entities, which forms a unique\nand authentic challenge for forecasting and anomaly detection models. It provides valuable insights\ninto the practical deployment of forecast-based anomaly detection approaches.", "sections": [{"title": "1 Background & Summary", "content": "Traffic monitoring plays a crucial role in network management and overall computer security [1]. Network-based\nintrusion detection and prevention systems can protect infrastructure against users' sloppiness, policy violations, or\nintentional attacks from inside. However, maintaining network security has become increasingly challenging due to the\nwidespread adoption of traffic encryption, which significantly reduces visibility.\nAs a result, gaining insights into encrypted network traffic has become essential, particularly for threat detection. Recent\nresearch has focused on detecting security threats through the classification of encrypted traffic using machine learning\ntechniques [2, 3, 4, 5, 6]. However, in the domain of network traffic monitoring, there is a substantial challenge in\nacquiring up-to-date threat datasets [7]. Machine Learning classification models can detect already known attacks,\nwhich are captured in the dataset or those closely resembling them (such as malware from the same family). Therefore,\nunsupervised anomaly detection plays a crucial role in network traffic monitoring [8] as it can identify unknown\n(zero-day) attacks due to behavioral changes caused by infection [9].\nThe unsupervised anomaly detection method assigns anomalous scores to network entity behavior based on patterns and\ncharacteristics learned from historical data. One of the most used types of unsupervised anomaly detection algorithms is\nbased on traffic forecasting (also referred as network traffic prediction). An anomaly alert is raised when the difference\nbetween the forecasted value and observation exceeds a defined threshold. Nevertheless, traffic forecasting can also be\napplied in other networking use cases, such as traffic management in data driven networks, resource allocation, and\nservice orchestration.\nIn recent years, there has been a rapid development in forecasting and anomaly detection methods, not limited to\ncomputer science. Wu et al. [10] attributed this development to the rise and successful use of neural networks.\nNevertheless, the recent performance improvement of forecasting methods applied to network traffic monitoring is\nuncertain due to the absence of long-term datasets [11]. In their survey, Ferreira et al. [11] describe the lack of a"}, {"title": "2 Methods", "content": "In this section, we provide detailed information about all methods used for obtaining the dataset. Since the dataset was\nobtained from a production network, CESNET3, and used by real users, privacy was a fundamental concern in our work,\nleading us to conduct our research with careful consideration. The indisputable advantages of real traffic generated\nby hundreds of thousands of people come with understandable privacy concerns. Thus, we used only automatic data\nprocessing with immediate data anonymization. With this, we declare that we did not analyze or manually process\nnon-anonymized data or perform any procedures that could allow us to track users or reveal their identities.\n2.1 Data capture\nThe network traffic was obtained from the CESNET3\u2014an ISP network that provides internet access to public and\nresearch institutions in the Czech Republic. The network spans a whole country, as shown in Figure 1, and serves\napproximately half a million users daily. Since ISP networks transfer huge volumes of data, packet-based monitoring\nsystems are infeasible due to the costs of processing and storage capacity. Therefore, the ISP networks (including\nCESNET3) are monitored using a standard IP flow monitoring system located at the perimeter-all transit lines to the\npeering partners are equipped with flow monitoring probes.\nIP flow monitoring systems aggregate data packets into IP flow records. An IP flow record represents communication\nmetadata associated with a single connection and is defined [16] as \u201ca set of IP packets passing an observation point in\nthe network during a certain time interval, such that all packets belonging to a particular flow have a set of common\nproperties.\" Commonly, these properties are referred to as flow keys and consist of source and destination IP addresses,\ntransport layer ports, and protocol.\nThe monitoring infrastructure for creating the dataset is detailed in Figure 2. A network TAPs are positioned before\nthe edge routers of the CESNET3 network and mirror the traffic to the monitoring probe, which is a server equipped\nwith a high-speed monitoring card capable of handling 200 GB/s. On the monitoring probe, the Ipfixprobe (https:\n//github.com/CESNET/ipfixprobe) flow exporter is installed. The Ipfixprobe exporting process was set with an\nactive timeout of 5 min and an inactive (idle) timeout of 65 s. Long connections are split when the connection duration\nis longer than the active timeout, and a flow record is exported even though the actual connection is not terminated yet.\nIf no packet is observed within the inactive timeout period, the connection is considered terminated, and a flow record\nis exported. Using active and inactive timeouts for splitting connections is standard practice for flow-based network\nmonitoring [16]. Additionally, the Ipfixprobe collected following features: start time, end time, number of packets,"}, {"title": "2.2 Time series aggregation", "content": "The flow collector server contains aggregation and filtration modules, as depicted in Figure 2. The filtration module\nremoves all transient traffic-where both source and destination addresses do not belong to CESNET3 and the packets\nare just passing through the network. Moreover, we also filter out all single TCP-SYN packet connections-scans.\nGiven the ISP origin and probe placement before routers and firewalls, the scans would represent the absolute majority\nof the dataset. Moreover, a large number of scans would cause significant noise in the time series, which would result in\npossible false negatives in anomaly detection. Since the scans can be easily detected with simple methods [17, 18], we\ndecided to remove them from the dataset. All other flows are then passed to the aggregation module."}, {"title": "2.3 Anonymization", "content": "The capturing process started on 9. October 2023 and ended on 14. July 2024. Thus, after 40 weeks, we extracted\ntime series from the database. In the database framework, a script is deployed that automatically adds corresponding\ninstitution and institution subnet for each IP address. Nevertheless, we omit the extraction of exact IP addresses,\ninstitutions, and institution subnets. Instead, we used database IDs for IP addresses, institutions and institution subnets\nas an identifiers, which were randomly assigned during database creation. The omitting IP addresses, institutions and\ninstitution subnets in the data extraction performs effective anonymization. Nobody without access to the original\ndatabase cannot revert this step and connect time series with a particular IP address, institution, or institutional network."}, {"title": "2.4 Data preprocesing", "content": "This subsection describes the preprocessing steps in detail.\nFiltering: The obtained raw dataset from the database contains a time series for approximately 400 thousand IP\naddresses. However, many of them were almost empty-network entities were not active most of the dataset creation\ntime, resulting in an empty 10-minute aggregated record. Therefore, we remove time series with a smaller number of\ndatapoints than 100, which is approximately 0.25% of the maximum datapoints that the time series in this dataset can\ncontain. This action results in a time series for 275,124 IP addresses.\nMultiple time aggregation: The original datapoints in the dataset are aggregated by 10 minutes of network traffic.\nThe size of the aggregation interval influences anomaly detection procedures, mainly the training speed of the detection\nmodel. However, the 10-minute intervals can be too short for longitudinal anomaly detection methods. Therefore, we\nadded two more aggregation intervals to the datasets.\nWe provide additional one-hour and one-day aggregation intervals. These aggregated intervals were created from the\n10-minute interval. When possible, time series metrics were aggregated using the sum of values (such as IP flows,\nnumber of packets and transimitted bytes). Nevertheless, metrics that represent a number of unique values cannot be\neasily summed without losing potentially important information. Therefore, in that case, we provide sum, mean, and\nstandard deviation that results in three new time series metrics per each original metric. Finally, we use the mean for the\ntime series metrics, which represent ratio or average values.\nTime series of institutions: Many security events can be visible only from the perspective of overall network\ntraffic. Therefore, we use the institution ID exported from the database to divide the IP addresses into groups based\non institutions. We identify 283 institutions inside the CESNET3 network. These time series aggregated per each\ninstitution ID provide a view of the institution's data."}, {"title": "Time series of institutional subnets", "content": "Many institutions have multiple networks, which are usually in different\nlocations or have different purposes in the organization. The administrators usually like to handle security not only\noverall but also per each subnet. Therefore, we divide the IP addresses into groups based on institution subnets in\nthe same location using information from ISP CESNET. These time series aggregated per each institution's network\nprovide a view of data that would probably be monitored by the institution's SoC team.\nWe identify 548 institution subnets inside the CESNET3 network. Almost 75 % of institutions have exactly one network.\nMoreover, the next 14% of institutions have two networks. However, there is also an institution with 105 networks.\nThis is because the CESNET3 network interconnects to a lot of high schools, hospitals, and museums. Therefore, 75%\nof institutions' time series is in the institution subnets' time series dataset."}, {"title": "Weekend and holidays", "content": "In the network, traffic forecasting and anomaly detection can be crucial to add information\nabout weekends and holidays into the model training and evaluation. Therefore, we included weekend dates and dates\nof public holidays in the Czech Republic for convenience."}, {"title": "3 Data Records", "content": "The dataset is delivered in the form of compressed CSV files and is available at the Zenodo Platform via URL link:\nhttps://zenodo.org/records/13382427. This section provides the dataset structure and explains all data fields\nand files."}, {"title": "3.1 Data file hierarchy", "content": "The dataset structure is outlined in Figure 4. Each time series type (either institutional subnets, institutions, or raw IP\naddresses) is also divided by the aggregation window (10 minutes, 1 hour, and 1 day). The time series data, containing\nall relevant features, is stored in CSV files. Each file is named according to the identifier of the entity whose behavior the\ntime series follows, ensuring a clear association between the time series and the corresponding entity. Since identifiers"}, {"title": "3.2 File format", "content": "The dataset files are organized as tables in CSV file format. There are two different file formats for the time series. The\ntime series aggregated over 10 minutes contain columns described in Table 1. The time series aggregated over one hour\nand one day contains additional data features due to reaggregation. Therefore, apart of features described in Table 1 it\nalso contains features described in Table 2. Moreover, the dataset contains two support files-ids_relationship.csv\nand weekends_and_holidays.csv. Table 3 describes the content of ids_relationship.csv, which provides\ninformation about the relationship between the IP addresses, institutions, and institution subnets dataset types. Further-\nmore, Table 4 describes the content of weekends_and_holidays.csv that provides information about which days are\nnon-working days in the Czech Republic (weekends and national holidays)."}, {"title": "4 Technical Validation", "content": "This section provides technical validation of the dataset and is divided into three parts: 1) Validation of overall dataset\nproperties, 2) Validation of the existence of anomalies, and 3) Validation of usability of the dataset for forecasting\napproaches."}, {"title": "4.1 Validation of overall dataset properties", "content": "In this section, we aim to validate the overall statistical properties of the dataset across the 40 weeks."}, {"title": "Activity of IP addresses", "content": "The dataset contains network traffic of more than 275 thousand IP addresses. However, these\nIP addresses typically do not communicate constantly over time. The evolution of the number of active IP addresses for\neach dataset's day is shown in Figure 5. We can see that the number of active IP addresses correlates with the weekends\nand holidays, which is highly expected behavior. Moreover, we can see a slight correlation with terms and exam periods,\nwhich is caused by the CESNET3 interconnecting many universities and dormitories."}, {"title": "Evolution of transmitted data", "content": "Overall data that were transmitted by IP addresses in the dataset is shown in Figure 7.\nThe transmitted data in the aggregation window is stored in the time series metrics n_bytes. In the figure, it can be\nseen that the evolution of transmitted data size highly correlates with weekends and holidays. Moreover, CESNET3\ninterconnects universities and dormitories; thus, we can also see a correlation between the terms and the exam periods.\nThis observation is in line with the findings of Bene\u0161 et al. [19].\nFurthermore, it can be seen that the decrease in traffic after the end of the summer term is even larger than the decrease\nin traffic during Christmas. Therefore, we performed the evaluation of this anomaly, and we found out that one of the\nmonitoring probes was broken from approximately 2024.05.21 16:30 to approximately 2024.06.04 20:00. So,\nthe probe did not send IP flows to the IP flow collector."}, {"title": "Gaps in time series", "content": "Real-world network traffic data often contains gaps where a device does not transmit any data. In\nsome instances, the entire network's traffic may exhibit such gaps. These gaps, or spaces, pose a challenge that must be\naddressed before applying forecasting methods.\nOne way to manage these gaps is through the aggregation process. If the aggregation window is sufficiently large,\nthe resulting time series might eliminate these gaps. However, for scenarios involving multiple processes, such as\ntraffic from multiple devices or entire networks, it's unlikely that a single aggregation window will be effective across\nthe board. Additionally, using a large aggregation window can obscure important patterns in the time series, such as\nanomalies. As a result, it's inevitable that some time series will contain gaps.\nIn our dataset, many time series contain significant gaps. In Figure 8a shows the average percentage of active datapoints\nin these time series, along with the standard deviation. For the 10-minute aggregation interval, active datapoints make"}, {"title": "4.2 Validation of existence of anomalies", "content": "There are many types of anomalies that were described in the literature. All of these\nanomalies are present in this dataset. Examples of the anomaly types occurring in the dataset are shown in Figure 9.\nThe first type of anomaly is Point anomaly [14], which is a single data point that significantly deviates from the rest of\nthe datapoints in the time series. There are two types of point anomalies:\n\u2022 Global - A global outlier is a data point that deviates significantly from the overall pattern or distribution of the\nentire dataset. It is an extreme value when compared to the rest of the data.\n\u2022 Contextual - A data point that is an outlier within a specific context or condition but not necessarily when\nviewed in a different context.\nThe second type of anomaly is Collective Anomaly [14], which is a sequence of datapoints that is anomalous when\nconsidered together, even if individual points might not be. There are two types of collective anomalies:"}, {"title": "Security incident analysis", "content": "In the analysis of the detected anomaly within the dataset, it is crucial to provide a clear\nexplanation of the metrics that characterize the event. Specifically, for the anomaly detected on time series with IP\naddress ID 1367, identified as a Denial of Service (DoS) attack by CESNET experts, the time series metrics were\nscrutinized in detail, as illustrated in Figure 10.\nThe analysis reveals several key observations. Firstly, both the flow and packet counts increase significantly, yet they\nrise in parallel, indicating a one-packet flow characteristic. Despite the increase in flows and packets, the number of\nbytes transmitted does not rise correspondingly, suggesting that the packets involved are small in size. Furthermore, the\nnumber of destination ports remains steady, dismissing the possibility of a scan. A significant shift in the TCP/UDP\nratio is observed, with almost all the anomalous traffic being UDP, further supporting the DoS identification."}, {"title": "4.3 Validation of usability of dataset", "content": "For the validation of usability, we decided to demonstrate the usage of the dataset's time series for network traffic\nforecasting. We select an IP address's time series with an ID 103, the number of IP flows, and a one-hour aggregation\ninterval. To demonstrate time series forecasting applied to the dataset's data, we select the basic SARIMA (Seasonal\nAutoregressive Integrated Moving Average) as the forecasting model\u2014the order was equal to (1, 1, 1) and seasonal\norder equal to (1, 1, 1, 168). This means that each of the three components of the model has the same weight in the\nmodeling, and the seasonality was set to one weak (168 hours).\nThe SARIMA were trained on the monthly data (31 datapoints). We select two different prediction interval to\ndemonstrate the difference. The first prediction window a 7 days long, and the second prediction window is 2 days.\nFurthermore, the model was retrained with the sliding window equal to the prediction window. Therefore, the model\npredicted 36 weeks.\nThe result of this demonstration is shown in Figure 11. The Figure contains only one of The predictions for 2 days,\nand the model retraining after 2 days resulted in slightly better results, as can be seen. Moreover, we can compare the\nresults by using evaluation metrics like Root Mean Square Error (RMSE), Symmetric Mean Absolute Percentage Error\n(SMAPE), and R\u00b2 Score. In all used metrics, a lower value represents better forecasting performance. The predicted\nweek data achieves 10951.26 RMSE, 40.66 SMAPE, and 0.77 R2 Score. And the predicted 2 days of data achieve\n10293.81 RMSE, 40.86 SMAPE, and 0.79 R2 Score. Therefore, the predicted 2 days achieve better RMSE and R2\nScore. However, it achieves a slight decrease in the SMAPE metric."}, {"title": "5 Usage Notes", "content": "This section describes recommendations for using the evaluation of network traffic forecasting (and forecasting-based\nanomaly detection) in detail. We recommend authors follow this evaluation procedure in order to achieve one of the\nmotivations for the creation of this dataset, which is comparability between approaches. We believe that following\nrecommendations will help the community to process our dataset and be able to compare with different approaches that\nalso follow our recommendations. Thus, we present a checklist for addressing all our recommendations which is shown\nin Table 5. Furthermore, the example of using the dataset in the form of a Jupyter Notebook is available on GitHub\n(https://github.com/koumajos/CESNET-TimeSeries24-Example). Moreover, the source codes of experiments\nthat provide the evaluation example using our recommendations are also available on this GitHub repository."}, {"title": "Dataset Selection", "content": "The dataset utilized in this study is divided into four distinct parts, each of which can be indepen-\ndently used for evaluation: the Full IP address dataset, the Sample IP address dataset, the Institutions dataset, and the\nInstitution subnets dataset. Therefore, it is imperative for authors to clearly state which dataset type(s) they are using in\ntheir analyses (Recommendation 1). Moreover, each dataset contains three aggregation intervals; thus, the aggregation\ninterval(s) used must also be specified (Recommendation 2). In cases where multiple dataset types and/or aggregation\nlevels are employed, results must be reported separately for each dataset and aggregation without combining them. This\nensures the clarity and reproducibility of the findings.\nFurthermore, the approach must clearly indicate whether it is multivariate (multiple time series metrics are modeled\nsimultaneously) or univariate (each time series metric is modeled individually) (Recommendation 3). If not all metrics\nare used, this must be explicitly stated (Recommendation 4), and comparisons with different approaches should only be\nmade in the following cases. First, for univariate approaches, comparisons should be made individually per each metric.\nSecond, for multivariate approaches, different methods must use the same metrics to ensure accurate comparison.\nMoreover, when any preprocessing steps are applied to these dataset parts, it is crucial that these steps are thoroughly\ndescribed (Recommendation 5). This includes a detailed description of any filtering, normalization, or transformation\nprocesses. Furthermore, this also handles gaps in time series, which must be addressed and described in detail.\nParticularly, if the preprocessing involves filtering the time series data, this may lead to results that are not directly\ncomparable with studies using unfiltered versions of the same dataset types. Therefore, such preprocessing steps should\nbe clearly justified, and their impact on the analysis should be discussed.\nGiven that the Full IP address dataset contains more than 275 thousand time series, evaluating methods for all these\ntime series is challenging. Therefore, we encourage authors to create new, smaller datasets featuring interesting time\nseries behaviors from the Full IP address dataset and share them with the community via platforms such as Zenodo.\nThis practice would facilitate further research by providing accessible, focused datasets that highlight specific patterns\nor anomalies, fostering collaboration and innovation within the research community."}, {"title": "Training Correctness", "content": "To ensure the integrity and validity of the training process, several key guidelines must be\nfollowed. The training phase of the time series must always commence from the very beginning of the dataset's time\nframe, which starts on 2023-10-09 (Recommendation 6). This causes the model to be trained on the entire range of\navailable data, capturing all relevant trends and patterns, ensuring performance results comparability. The duration of\nthe training window must be explicitly specified in the article (Recommendation 7). This includes detailing how much\nhistorical data is used to train the model before it begins making predictions.\nMoreover, if a validation window is employed during the model development process, the duration and purpose of this\nvalidation window must be clearly defined (Recommendation 8). This helps in assessing the model's performance on\nunseen data before it is applied to the test set. Furthermore, if the model is retrained during the evaluation phase, this\nretraining process must be clearly described in the article (Recommendation 9). Authors should specify the retraining\nfrequency, the data used for retraining, and how the retrained model is validated."}, {"title": "Forecasting Correctness", "content": "To ensure consistency and transparency in the forecasting methodology, authors must\nclearly describe the following aspects of their prediction process. Authors must specify the length of time into the"}, {"title": "Evaluation metrics", "content": "The evaluation of the forecasting model should be done by the evaluation metrics. The chosen\nmetrics must be clearly specified in the article (Recommendation 11). We recommend using the following metrics for\nevaluation (n is the number of observations, yi are the actual observed values in the time series, \u0177i are the predicted\nvalues):\n\u2022 Root Mean Squared Error (RMSE) calculates the square root of the average squared errors, maintaining\nthe same units as the original data. It is sensitive to large errors, similar to MSE, which helps in detecting\nsignificant anomalies. The downside is that this sensitivity might distort the overall model assessment if large\nerrors are not critical. The RMSE can be computed by the equation 1. [21]\n$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}$                                                                                                                                                                      (1)\nMoreover, for the combination of multiple RMSES, we recommend using the weighted RMSE, which can be\ncalculated using the equation 2.\n$Weighted RMSE = \\sqrt{\\frac{\\sum_{i=1}^{n} \\sigma_i^2 \\cdot RMSE_i}{\\sum_{i=1}^{n} \\sigma_i^2}}$                                                                                                                                                                                                                                 (2)\nwhere \u03c3\u00b2 represents the variance of the true values in the ith dataset, and RMSE\u00bf is the corresponding RMSE.\n\u2022 Symmetric Mean Absolute Percentage Error (SMAPE) addresses some issues of MAPE by symmetrizing\nthe error calculation, providing more stability when actual values are near zero. This symmetry makes it less\nbiased towards overestimations or underestimations. However, SMAPE can still be less intuitive than simpler\nmetrics and may over-penalize certain error types. The SMAPE can be computed by the equation 3; the \u20ac in\nthe equation is a small constant that is added to avoid division by zero. [21]\n$SMAPE = \\frac{100%}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y_i}|}{(|y_i| + |\\hat{y_i}|+\\epsilon)/2}$                                                                                                                                                                                                                                     (3)\nMoreover, for the combination of multiple SMAPEs, we recommend using the mean and standard deviation of\nSMAPES. The weighted SMAPE can also be calculated using the equation 4.\n$Weighted SMAPE = \\frac{\\sum_{i=1}^{n} \\sigma_i^2 \\cdot SMAPE_i}{\\sum_{i=1}^{n} \\sigma_i^2}$                                                                                                                                                                       (4)\nwhere \u03c3\u00b2 represents the variance of the true values in the ith dataset, and SMAPE\u00bf is the corresponding\nSMAPE.\n\u2022 Coefficient of Determination (R2 Score) measures the proportion of variance explained by the model,\nproviding insight into the model's explanatory power. It is useful for comparing models, but it can be\nmisleading for non-linear models and does not directly measure prediction accuracy. The R\u00b2 Score can be\ncomputed by the equation 5. [22, 21]\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\mu_y)^2}$                                                                                                                                                                                                                                         (5)\nMoreover, for combining multiple R\u00b2 scores, we recommend using the weighted R\u00b2 score, which can be\ncalculated using the equation 6.\n$Weighted R^2 = \\frac{\\sum_{i=1}^{n} \\sigma_i^2 \\cdot R^2_i}{\\sum_{i=1}^{n} \\sigma_i^2}$                                                                                                                                                              (6)\nwhere \u03c3\u00b2 represents the variance of the true values in the ith dataset, and $R_i^2$ is the corresponding R2 score."}]}