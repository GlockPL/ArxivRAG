{"title": "Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4", "authors": ["Leni Aniva", "Chuyue Sun", "Brando Miranda", "Clark Barrett", "Sanmi Koyejo"], "abstract": "Machine-assisted theorem proving refers to the process of conducting structured reasoning to automatically generate proofs for mathematical theorems. Recently, there has been a surge of interest in using machine learning models in conjunction with proof assistants to perform this task. In this paper, we introduce Pantograph, a tool that provides a versatile interface to the Lean 4 proof assistant and enables efficient proof search via powerful search algorithms such as Monte Carlo Tree Search. In addition, Pantograph enables high-level reasoning by enabling a more robust handling of Lean 4's inference steps. We provide an overview of Pantograph's architecture and features. We also report on an illustrative use case: using machine learning models and proof sketches to prove Lean 4 theorems. Pantograph's innovative features pave the way for more advanced machine learning models to perform complex proof searches and high-level reasoning, equipping future researchers to design more versatile and powerful theorem provers.", "sections": [{"title": "Introduction", "content": "Proof assistants are used for a variety of tasks requiring strong guarantees and rigorous reasoning. High-profile applications include formal verification of computer systems (e.g., sel4 [10]) and formalization of mathematics (e.g., [7]). Among proof assistants, Lean 4 has recently accumulated significant momentum both among mathematicians and non-mathematicians. Its Mathlib library [14], for example, is an extensive effort to formalize many branches of mathematics and contains many non-trivial mathematical definitions and theorems.\nA common challenge shared by all proof assistants is that completing proofs is tedious and requires significant manual effort and expertise. Machine learning offers one potential avenue for addressing this challenge. Indeed, recent years have seen several significant efforts dedicated to using machine learning to automatically search for proofs in proof assistants (e.g., [21], [22], [5], [6], [12], [11], [8], [9], [20]). While these efforts have produced promising results, many proofs are still beyond the reach of machine learning-based automation.\nIn order to continue to make progress in this area, several challenges need to be addressed. One of these challenges is the need for better interfaces between"}, {"title": "Background", "content": ""}, {"title": "The Lean 4 Proof Assistant", "content": "A proof assistant is a computer program that can formulate and check formal mathematical proofs. This includes Lean 4 [15], Coq [19], Isabelle [16], Aya [3], and many others. In the language of a proof assistant, every definition, theorem, or proof is a value with a type. A value is represented by an expression. A proof of a theorem is a term whose type is the theorem. A proof assistant checks the validity of a proof of a theorem by evaluating its type and checking that it matches the statement of the theorem. It does this using a set of type deduction rules.\nFor example, the commutativity of the logical OR (V) operation can be written as the expression:\n$\\forall(p: Prop), \\forall(q : Prop), \\forall(h : p \\lor q), q\\lor p.$\nThis statement says that if p and q are Boolean propositions (of type Prop in Lean 4), then, given the hypothesis h of type p V q, we can conclude q V p. The notation in (1) is more verbose than what mathematicians typically use. This is because proof assistants require the utmost unambiguity. However, informally, the above expression could also be written using the more concise notation:\n$\\forall p,q. p\\lor q \\implies q \\lor p$\nA proof of (1) is an expression whose type is given by (1). For example,\n$\\lambda(p, q : Prop)(h : p \\lor q) \\rightarrow V.cases h (\\lambda hp : p \\leftrightarrow V. inr hp) (\\lambda hq : q \\rightarrow V. inl hq)$\nis a proof of the commutativity of OR. The type of a A-expression is a V- expression. The A's in the expression correspond to the three \u2200's in the statement of the commutativity theorem. Intuitively, this expression says that when pVq is assumed to be true, proving q V p requires proving q V p when p is true and also when q is true. This is signified by the special function V. cases, which is provided by Lean 4 as part of the support for the V operator. It represents the fact that deriving any value from p \u2228 q requires two functions, one to handle the case when p is true, and one to handle the case when q is true."}, {"title": "Tree Search", "content": "Assuming p is true, V. inr generates a proof of q V p from a proof of the right operand p (V. inl is similar but requires a proof of the left operand q). Expressions can also be constructed incrementally. For example, we could postulate that the following expression has the type shown in Expression (1):\n(1 (p: Prop)\u2192\u2461 [p]\nThen 2 must have the type\n\u2461: \u2200(q: Prop), \u2200(h : p \u2228q), qVp {p: Prop\nHere, 1 and 2 are metavariables. A metavariable is a variable, possibly unassigned, with a context. A goal (also called a hole) is an unassigned metavariable. When writing proofs in Lean 4, the sorry keyword can be used as a placeholder for a hole. A free variable in the context of a metavariable (e.g., the variable p above) references a value assumed to be true for this metavariable. \u2461 is a goal. The proof state consists of all metavariables, both those that are unassigned (i.e., the goals) and those that are assigned.\nProof expressions, while easy for the proof assistant to check, are difficult for a human operator to write. Thus, some proof assistants such as Lean 4 also provide an alternative interface for theorem proving, in which a proof can be executed via a series of tactics. A tactic changes the proof state by assigning an expression, possibly containing new goals, to a goal in the current state. In Lean 4, a tactic can transform one goal into a finite number of subgoals. A tactic that generates no subgoals solves the parent goal. If all subgoals produced by a tactic are solved, the goal is solved as well. For example, suppose a variable 1 has the type shown in Expression (1). Executing the intro tactic on 1 results in the assignment 1:= x(p : Prop) \u2192 \u2461 [p], where 2 has the type in Expression (3). 2 becomes the new goal that must be solved.\nSome tactics can create interdependent metavariables. This is known as metavariable coupling [13]. For example, in order to prove\n$\\exists(x: N), 2x + 5 \\leq 10$\none would need to invoke the Exists. intro lemma, which creates the following goals in Lean 4:\n{$\\exists(x: N), 2x + 5 \\leq 10$}\nwhere the second goal is now coupled to the first, since any solution of the first goal will necessarily affect the second."}, {"title": "Tree Search", "content": "Tree Search refers to the process of searching through a tree, each of whose nodes represents a potential solution to a problem, attempting to find the best possible solution [4]. Monte Carlo Tree Search (MCTS) is a class of tree search algorithms where in each iteration a leaf node from the current search tree is selected and expanded. The selection of this leaf node is driven by the policy of the tree search algorithm.\nMCTS is used by AlphaGo [18] and AlphaZero [17] for playing board games and by HyperTree [11] for proof search in Lean 3.\nThe tree structure that results from using tactics to prove theorems in Lean 4 is called an and-or tree and contains two types of nodes: goal nodes (Or), where solving any descendant suffices to solve the goal, and state nodes (And) produced by tactics, where solving all descendants is required. A full proof tree for the commutativity of OR is shown in Figure 1. When applying Monte Carlo Tree Search two theorem proving, two functions are required: the policy function decides which node (i.e., goal) to explore next, and the tactic function decides which tactic to use on that goal.\nThe use of machine learning to automatically discharge proof goals in proof assistants is an exciting and highly active area of research. The main motivation for creating Pantograph is to create an interface that can easily be used by machine learning systems aiming to exploit this incremental tree structure to conduct mathematical reasoning."}, {"title": "Related Work", "content": "The closest related work is LeanDojo [23], which provides a Python interface for machine interaction with Lean 4. A user can use this interface to execute tactics on a current proof state in a Lean 4 process. LeanDojo can be used to save and resume proof states by storing and recalling the return value of the Dojo.run_tac interface call. LeanDojo can also run commands such as #eval and #check and run tracing over an existing proof.\nPantograph has several key architectural improvements over LeanDojo. First of all, it is written entirely in Lean 4. This removes the need for external dependencies (such as Docker) and also improves the speed of interaction. Also, some tactics not supported by LeanDojo, such as have, are available in Pantograph. Other tactics, such as conv and calc, can only be used monolithically to solve goals in LeanDojo, whereas Pantograph supports incremental exploration using these tactics, allowing a user to obtain feedback at each step, even if a goal is not solved. Moreover, Pantograph efficiently handles the problem of metavariable coupling (see Section 4.3), empowering ML models to work on interdependent proof branches without risking inconsistency.\nLike LeanDojo, Pantograph can extract information from existing proofs, including training data based on triples of goal states, tactics, and post-tactic goal states. It can also extract comment data and arbitirary expressions, both of which may be useful as additional training data. However, LeanDojo's data extraction and proof execution units are essentially separate, which makes it impossible to extract an incomplete proof and resume from it. Pantograph supports this feature.\nIn [11], Lample et al. implement the aforementioned and-or tree search structure to solve goals in Lean 3. In this work, the relation between goals and tactics is a hypertree. This enables efficient proof search via a variant of Monte Carlo Tree Search. The policy and tactic functions are both provided by Large Language Models (LLMs). Pantograph is compatible with this approach, as it gives the user control over both policy and tactic functions.\nDraft-Sketch-Prove (DSP) [8] is a neural theorem prover which uses an approach based on drafting. Instead of directly generating Lean 4 tactics, the neural theorem prover generates intermediate goals in an informal language (draft). Then it translates these goals into Isabelle (sketch), and finally a hammer tactic from Isabelle solves the goals (prove). Pantograph's drafting feature supports this technique as well, allowing the Draft-Sketch-Prove algorithm to be implemented in Lean 4 (see Section 4.6).\nAesop [13] is a proof automation (hammer) tactic based on tree-search. Aesop is not based on machine learning and takes metavariable coupling into account when solving goals. When a goal gets solved in Aesop, all of the goals coupled to this goal are brought back into scope. This is known as copying. Pantograph's approach to metavariable coupling is based on Aesop's technique, but extends it by allowing the user to determine which metavariable to solve next.\nCoqGym [22] is similar to LeanDojo but for the Coq theorem prover instead of Lean 4. CoqGym stores proofs in a tree structure and allows the user agent to"}, {"title": "Architecture and Features", "content": "Pantograph is implemented entirely in Lean 4 with no external dependencies. Figure 2 shows an overview of the operation of Pantograph. The user, which is often a machine learning model, calls Pantograph's functions via one of its interfaces. Pantograph provides three interfaces: (i) a Python interface called PyPantograph; (ii) a REPL via the pantograph-repl executable; and (iii) a library via the C Foreign Function Interface (FFI). When the user executes a tactic, Pantograph calls the Lean 4 kernel's Elab. Tactic.evalTactic function. Internally, many of Lean 4's functions are monads, which are abstract structures enabling state manipulation in an otherwise functional language. Lean 4's monad hierarchy (from the order of most to least general) has the order 10, CoreM, MetaM, Elab.TermElabM, and Elab. Tactic. TacticM. Figure 3 outlines the most important functions called during the execution of a tactic via Pantograph."}, {"title": "Expressions and Tactics", "content": "Human-written proofs in Lean 4 (e.g., in Mathlib) are often a mixture between expressions and tactics. As mentioned above, tactics are used to reduce a goal to one or more new subgoals. To see how expressions can be used, consider the following example.\nexample : exists x, x + 2 = 8 := by\nlet a: Nat := 3 * 2\nexists a\nIn this example, the required witness for x is directly constructed as the expres- sion 3.2. Pantograph supports seamlessly switching between expression-based and tactic-based proof by providing a custom expr tactic. This tactic takes an expression e[1, 2, ...] (potentially with holes 1, 2, ...) and assigns it to the current goal. The holes \u2460, \u2461,... then become the new goals.\nLean 4 includes sophisticated tactics, like conv and calc, which are composite in the sense that they are used to compose sequences of other tactics. While these tactics can be executed monolithically by supplying the full sequence of tactics to be composed, human operators of Lean 4 often rely on Lean 4's interactive interface to incrementally explore possible sequences, obtaining feedback at each step. Pantograph provides a custom tactic called goal.tactic, which can partially execute a conv or calc tactic and provide feedback from this partial execution. As an example, consider the following use of the calc tactic, which is used in Lean 4 to compose a series of transitivity steps.\nexample (a b c : Nat) : a+b = b + c := by\ncalc a + b = a + a = sorry\n= b + b = sorry\nsorry\nHere, the goal a + b = b + c is not provable by calc, but a user can still partially execute the tactic by applying just the first line and seeing what the result is. In this case, the result of executing just the first line results in the following new goal.\nabc: Nat\n|- a + a = b + c"}, {"title": "Tree Search", "content": "Pantograph supports this partial execution model and can return the new goal shown above.\nPantograph also supports the have and let tactics. These tactics define temporary expressions in a local scope and are indispensable when developing proofs by hand. For example, consider the following snippet.\nexample (n: Nat), n + 0 = 0 + n := by\nhave h1 n + 0 = n := sorry\nsorry\nThe use of have introduces a new expression and a new goal. The two sorry expressions create two holes corresponding to the two goals shown below.\nn: Nat\n|-n+ 0 = n\nn: Nat\nh1n+ 0 = n\n|-n+ 0 = 0 + n\nThe Pantograph repository contains documentation and examples for these tactics.\nIn order to be friendly towards searching methods such as Monte Carlo Tree Search [4], Pantograph provides an interface for incrementally executing tactics. If a tactic creates more than one goal, it is called a branching tactic. When more than one goal exists in a proof state, Pantograph provides the option to choose which goal to apply a tactic to."}, {"title": "Tree Search", "content": "As mentioned above, tree search is a common search technique and is utilized in various proof search approaches such as HyperTree [11] and Aesop [13]. Since each tactic produces zero or more goals, the search structure of applying tactics to goals can be viewed as an And-Or tree (in the absence of metavariable coupling, see Section 4.3). When the current proof state has multiple goals, Pantograph allows the user to choose which goal to attempt next, i.e., it allows user-defined policy functions.\nThis naturally leads to the question of the fate of sibling goals. Suppose there are two goals [\u2460, \u2461] in the current proof state, and the user applies a tactic to 1, generating 3. The status of 2 depends on the automatic mode option. Automatic mode is turned on by default, which means sibling goals are carried forward to the next proof state. Hence, with automatic mode on, the next proof state would contain [3,2], with all goals present and active. If the user disables automatic mode, the proof state instead becomes [3]. The goal 2 becomes dormant. Dormant goals are unassigned metavariables that do not appear in the current proof state. Note that dormant goals are an artifact of Pantograph's manual tree search capability: they do not occur when using Lean 4 through the interactive interface. Dormant goals must either be tracked by the user or brought back into the proof state using the goal.continue command, as shown in Figure 4."}, {"title": "Metavariable Coupling", "content": "Recall that a proof state may contain 0 or more goals, and metavariable cou- pling [13] refers to inter-dependencies between goals in a proof state. Metavariable coupling arises naturally in many contexts. For example, applying the transitivity axiom of <N to the goal 2 < 5 results in the following goals.\n$\\begin{aligned}1: 2 \\leq \\text{\u2461} \\\\:2\\leq5 \\\\2:N\\end{aligned}$\nBecause appears in all three goals, these goals are all coupled. This complicates proof search because if an assignment is made to z in one goal, it will propagate to all of the other coupled goals. In this case, the other two goals will no longer be coupled, but they will contain the assignment made to z.\nPantograph provides explicit information about which goals are coupled. Since there are multiple possible ways of handling coupling, the choice of what to do with the coupling is left to the user. One method employed by [13] is copying, where coupled goals are solved sequentially to avoid conflicts.\nFigure 5 gives a full example of the above proof, conducted with automatic mode off. The application of the transitivity tactic creates a proof state with three goals. Using the exact 3 tactic on the goal results in a solved proof state. Applying goal.continue then brings goals 1 and 2 back into the proof state, where they are no longer coupled. Each can be discharged with an additional tactic such as decide."}, {"title": "The Environment", "content": "All running instances of Lean 4, including instances running behind the LSP or the Pantograph front end, maintain a library of active symbols known as the"}, {"title": "Tactic Training Data", "content": "The Lean 4 community has produced several large collections of theorems with human-written formal proofs, e.g., Mathlib [14]. These collections can be used to train theorem proving agents. The frontend.process command runs the Lean 4 compiler on a Lean 4 file, collects all tactics in the file, and returns them as a list of (before, after, tactic) triplets. These triplets are conveniently presented in a format conducive to offline reinforcement learning training. Pantograph also outputs information about the starting and ending positions (in the file) of each Lean 4 command in case the user is interested in processing comments or other metadata. Below is an example of one extracted tactic triple.\n{\n\"goalBefore\": \"\u251c\u2200(p, q : Prop), p\u2228 q \u2192 qVp\",\n\"goalAfter\":\" p:Prop \\n F\u2200(q: Prop), pVq\u2192qVp\",\n\"tactic\":\"intro p\"\n}"}, {"title": "Drafting", "content": "Drafting refers to a theorem proving technique which starts by generating a proof outline, instead of building a full proof step by step. A draft proof first consists of an overview with holes. Draft proofs are resolved by proving the individual goals corresponding to the holes in the proof. For example, consider the task of proving the commutativity of addition in Peano arithmetic. One approach would be to write a proof based on induction, using the inductive hypothesis n + m = m + n to prove the inductive step m + (n + 1) = (m + n) + 1. As stated, this proof is not rigorous or detailed enough for Lean 4, but it can be written as a draft proof:\ntheorem add_comm: forall n m: Nat, n + m = m+n: by\nintros nm\ninduction n with\n| zero =>\nhave h_base: 0+ m = m := sorry\nhave h_symm: m + 0 = m := sorry\nsorry\n| succ n ih =>\nhave h_inductive: n + m = m+n := sorry\nhave h_pull_succ_out_from_right: m + Nat.succ n = Nat.succ (m + n)\n:= sorry\nsorry\nThe placeholders for intermediate goals have marked with the sorry keyword. Pantograph supports drafting in two ways. The first is via the have tactic. have introduces a lemma or intermediate claim and creates a new goal corresponding to the lemma.\nThe other way Pantograph supports drafting is via sorry-extraction. Pan- tograph can find all occurrences of sorry in a proof or definition and convert them to goals. For example, when add_comm from the above proof is fed into Pantograph's frontend.process command, it generates the following list of goals:\nm: Nat\nw = w + 0-1\nm: Nat\nh_base: 0+ m = m\n|-m+0 = m\nm: Nat\nh_base: 0+ m = m\nh_symm: m + 0 = m\n0 + w = w + 0-1\nm: Nat\nn: Nat\nihn mm + n\n|-n + m = m + n\nm: Nat\nn: Nat\nihn mm + n"}, {"title": "Evaluation", "content": "As an illustration and evaluation of Pantograph's capabilities, we recreated the DSP experiment from [8] mentioned in Section 3. We used both the GPT-40 [1] and the GPT-01-preview [2] language models, with parameters given in Table 1 and ran on the theorem proving evaluation benchmark MiniF2F [24]. Each individual experiment works as follows. The language model is given the formal theorem statement from MiniF2F and is asked to generate a natural language proof. Next, the same model is provided with this natural language proof and asked to generate one or three formal proof sketches in Lean 4 (GPT-ol-preview is only asked to generate a single proof sketch, as it does not yet support multiple sketches). These sketches may contain the sorry keyword. The sketches are then fed into Pantograph's sorry-extraction command and turned into goals, which we try to solve one by one. To attempt to solve the goals, we use the following Lean 4 tactics as hammers: aesop [13], simp, and linarith (from Mathlib [14]).\nThe results are shown Table 2. We show results on both the Validation and Test subsets of the MiniF2F benchmark set, both with one and three proof sketch(es). We report the overall success rate, and the average number of hammer invocations and the average runtime per benchmark. Our best configuration uses GPT-40 and three requested sketches. For this configuration, the DSP system out of the box successfully proved 28% of the theorems from MiniF2F [24]. This is with no specialized training. Note that our main goal in our evaluation is to illustrate"}, {"title": "Conclusion", "content": "In this work, we introduce Pantograph, a Machine-to-Machine interaction library for Lean 4. We compare its features against existing tools used for training machine learning models for theorem proving, and we provide a list of its novel features. We also illustrate an application by implementing the first Lean 4 implementation of the Draft-Sketch-Prove approach.\nIn future work, we plan to use Pantograph to build and train various machine learning approaches for theorem proving. We also expect and hope that others will use it in interesting and novel ways and that these use cases will provide feedback for additional improvements and extensions of Pantograph."}, {"title": "Appendix", "content": "The prompt for the Draft part of the DSP experiment is\nDraft an informal solution similar to the one below. The informal solution will be used to sketch a formal proof in the Lean 4 Proof Assistant.\nHere are some examples:\nInformal:\n(*### Problem\n\n[...nl/i problem text...]\n\n### Solution\n\n[...nl/i solution/draft text...]\n\n*)\n\nInformal:\n(*### Problem\n\n{nl_problem}\n### Solution\n\n[...Model Completion...]\nThe prompt for the Sketch part is"}]}