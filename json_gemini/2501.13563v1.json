{"title": "Black-Box Adversarial Attack on Vision Language Models for Autonomous Driving", "authors": ["Lu Wang", "Tianyuan Zhang", "Yang Qu", "Siyuan Liang", "Yuwei Chen", "Aishan Liu", "Xianglong Liu", "Dacheng Tao"], "abstract": "Vision-language models (VLMs) have significantly advanced autonomous driving (AD) by enhancing reasoning capabilities; however, these models remain highly susceptible to adversarial attacks. While existing research has explored white-box attacks to some extent, the more practical and challenging black-box scenarios, where neither the model architecture nor parameters are known, remain largely underexplored due to their inherent difficulty. In this paper, we take the first step toward designing black-box adversarial attacks specifically targeting VLMs in AD. We identify two key challenges for achieving effective black-box attacks in this context: the effectiveness across driving reasoning chains in AD systems and the dynamic nature of driving scenarios. To address this, we propose Cascading Adversarial Disruption (CAD). It first introduces Decision Chain Disruption, which targets low-level reasoning breakdown by generating and injecting deceptive semantics, ensuring the perturbations remain effective across the entire decision-making chain. Building on this, we present Risky Scene Induction, which addresses dynamic adaptation by leveraging a surrogate VLM to understand and construct high-level risky scenarios that are likely to result in critical errors in the current driving contexts. Extensive experiments conducted on multiple AD VLMs and benchmarks demonstrate that CAD achieves state-of-the-art attack effectiveness, significantly outperforming existing methods (+13.43% on average). Moreover, we validate its practical applicability through real-world attacks on AD vehicles powered by VLMs, where the route completion rate drops by 61.11% and the vehicle crashes directly into the obstacle vehicle with adversarial patches. Finally, we release the CADA dataset, comprising 18,808 adversarial visual-question-answer pairs, to facilitate further evaluation and research in this critical domain. Our codes and dataset will be available after paper's acceptance.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in vision-language models (VLMs) [2, 22, 38, 77] have achieved impressive performance in tasks such as image captioning, question answering, and multimodal reasoning, leading to their rapid development and widespread applications across various domains, including autonomous driving (AD). AD operates within highly complex and dynamically evolving environments, where the sophisticated multimodal reasoning capabilities of VLMs could serve as a cognitive backbone, enabling advanced perception, nuanced decision-making, and safer navigation [42,44,45,47,54,55].\nDespite their remarkable success, numerous studies have highlighted the vulnerability of VLMs to adversarial attacks [26\u201329,31,39,61,63,64,67,71,76], where imperceptible visual perturbations would cause the model to wrong predictions. These vulnerabilities raise significant concerns, especially when such models are deployed in safety-critical AD contexts [55, 74]. Subtle errors at the reasoning stage can cause severe failures, for instance, misinterpreting a situation that demands braking as one requiring acceleration [10, 11]. Tackling these challenges demands a systematic approach to evaluating the"}, {"title": "2 Backgrounds and Preliminaries", "content": "Autonomous driving based on VLMs. VLMs have demonstrated remarkable performance across a wide range of tasks [1, 9, 38, 77], and are increasingly explored in AD. In particular, VLMs integrate visual and textual information for visual question-answering (VQA) or planning and control tasks. In general, an AD VLM f is designed with an architecture that combines a vision encoder fv, a large language model (LLM) backbone fi, and the post-processing module fp, jointly completing the entire task pipeline. The inputs consist of video or multi-frame images, represented as x\" = {x1, x2, ..., x}, where n denotes the maximum number of frames, as well as a textual instruction, expressed as x\u1d57, and the set (x\", x\u1d57) collectively forms the input query space Qof the model f. In AD tasks, x\u1d57 is processed by the visual encoder fv to interpret the scene and generate the visual query, while x\u1d57 is passed through the tokenizer f\u2081 associated with the LLM backbone fi to produce textual tokens. After receiving the visual query and text tokens, the LLM backbone performs joint reasoning and produces task-specific output tokens o, expressed as follows:\n$$o = f_i (f_v (x'), f_i (x')).$$\nDepending on the specific AD task, o is then transformed into the corresponding responses y by the post-processing module fp. For VQA tasks, the tokenizer decodes the output tokens o into corresponding textual answers. While for planning and control tasks, an adapter processes the tokens to predict future waypoints, which are subsequently transformed into control signals, such as brake, throttle, and steer angle. This process can be formally expressed as follows:\n$$y = f_p (0),$$\nthereby achieving the mapping of f : Q \u2192 \u211d, where \u211d represents the responses domain related with the AD tasks. To train AD VLMs, the visual encoder may either be initialized with pre-trained weights from large-scale vision datasets or trained independently with perception tasks, while the entire model is fine-tuned on task-specific datasets for AD applications.\nAdversarial Attacks on VLMs. The widespread deployment and impressive performance of VLMs in multimodal question answering and reasoning have raised growing concerns about their adversarial robustness [70,75]. Here, adversarial attacks on VLMs aim to manipulate the model's final responses by introducing perturbations into the input query (x\", x\u1d57), producing an adversarial query A(x\", x\u1d57), where A() denotes the attack function. The goal of A is to induce the victim VLM f to output targeted or undesired responses"}, {"title": "3 Threat Model", "content": "y* instead of the correct ones. This can be formalized by maximizing the model's log-likelihood of the adversarial responses y*, as shown in Eq. (3):\n$$\\operatorname{maximize} \\log p(y^* | A(x^v, x^t))$$\nwhere p (.) is a probability function. Depending on whether A(\u00b7) requires access to model information, attacks are categorized as white-box or black-box."}, {"title": "3.1 Problem Definition", "content": "This paper focuses on black-box visual adversarial attacks on AD VLMs, where the adversary lacks access to the model's internal parameters and can only craft perturbations on the model's visual inputs. Specifically, the attack function A introduced in Sec. 2 becomes A(x\") = x\" + \u03b4, where \u03b4 represents the small perturbations applied to the visual domain, while the textual instruction x\u1d57 remains unchanged. We define \u211d+ as the set of model responses that align with the factual context of the scene and comply with human rules, while \u211d- denotes the set of responses that either contradict reality or violate rules. Our attack objective is to generate perturbations under a given budget \u03b5 constrained by the lp norm, to craft noise that forces the responses from the victim model f to approach \u211d-, formally expressed as:\n$$f_p (f_i (f_v (x^v + \\delta), f_t (x^t))) \\in \\mathbb{R}^-, \\text { s.t. }||\\delta||_p \\leq \\epsilon,$$\nwhere this process relies solely on a few surrogate models and has no access to the victim model f."}, {"title": "3.2 Challenges for AD VLM Attacks", "content": "Directly applying existing attacks designed for general VLMs to the AD domain presents significant limitations. Specifically, we identify two main challenges as follows:\nChallenge 1: Attack should remain effective when propagating the driving reasoning chain. Most AD solutions encompass a reasoning pipeline that integrates perception, prediction, and plan [5, 19\u201321, 59], where the transition from raw sensory data to higher-level reasoning decisions is inevitable. Conventional visual attacks often focus narrowly on disrupting the perception, such as inducing classification errors, without investigating whether these initial perturbations could remain effective when propagated through subsequent stages or are mitigated by the system's fault-tolerant abilities. To design effective adversarial attacks, it is crucial to account for how errors introduced at the perception stage traverse and accumulate within the reasoning pipeline. By targeting the entire pipeline, adversarial attacks can disrupt reasoning processes in a more sustained and cohesive manner, thereby posing a greater threat to the overall system.\nChallenge 2: Attack should work for the dynamic driving context of autonomous driving. Unlike common VLM tasks that work in static context, AD operates within dynamic environments where the determination of safety depends heavily on the specific contextual conditions of the current scenario. For instance, a sudden sharp turn may be the optimal action if a pedestrian unexpectedly enters the lane from the right. However, in standard situations, abrupt maneuvers (e.g.sharp turns or hard braking) are typically deemed unsafe. Therefore, the design of adversarial attacks for AD VLMs must account for the contextual dynamics that underpin safe driving behavior, rather than simply triggering isolated extreme actions. Attacks should aim to disrupt decision-making in ways that align with the complexities and situational dependencies inherent to AD, ensuring the induced errors could compromise driving safety within the given context."}, {"title": "3.3 Adversarial goals", "content": "This paper explores generating visual adversarial perturbations that mislead AD VLMs to wrong responses. Specifically, given an AD VLM f (consists of fv, ft, ft, and fp) that takes video or image sequence x, and a textual instruction x\u1d57 as input, the attacker's goal is to induce the model to generate misunderstandings of facts or erroneous driving actions. We mainly conduct scenario-specific untargeted attacks. A successful attack in this context is achieved when the model's response evaluation score decreases or it induces accident-prone behaviors in the driving agent."}, {"title": "3.4 Possible attack pathways", "content": "A critical question in adversarial attacks on AD VLMs is whether these attacks are feasible and practical in real-world AD scenarios. Our approach demonstrates strong applicability in practical AD applications. On one hand, adversaries can exploit various methods to inject noise into the data collected by sensors during driving. For instance, they may install disruptive devices on sensors to interfere directly with the data captured by the camera, or employ hacking techniques to inject adversarial noise into the images processed by the deployed system model. On the other hand, physical-world attacks can be realized by optimizing adversarial noise or patches using our proposed objective functions. For example, adversaries could affix adversarial patches onto traffic signs [33] or embed adversarial noise into roadside billboards [50]."}, {"title": "3.5 Adversary's constraints and capabilities", "content": "In the context of AD, we focus exclusively on black-box attacks, where attackers have no access to any internal information about the victim model (e.g., architecture, gradients, or parameters). Querying the model is also impractical in AD applications due to real-time requirements and limited access;"}, {"title": "4 Attack Approach", "content": "As shown in Fig. 2, our framework can generate black-box visual adversarial attacks against AD VLMs consisting of two cascading modules: Decision Chain Disruption for low-level reasoning breakdown and Risky Scene Induction for high-level dynamic adaptation."}, {"title": "4.1 Decision Chain Disruption", "content": "To address Challenge 1, which emphasizes maintaining attack effectiveness across the driving reasoning chain, we propose a method that specifically targets the perception-prediction-plan pipeline. While conventional VLM attacks often fail to effectively disrupt this reasoning chain, our approach aligns adversarial noise with low-level detailed texts that possess coherent yet deceptive semantics, ensuring that disruptions introduced at the perception stage propagate seamlessly through the entire reasoning chain.\nCapitalizing on the pivotal role of the perception-prediction-plan reasoning chain in enhancing the effectiveness and reliability of contemporary AD solutions [5, 19\u201321,59], we exploit its disruption as a means to generate detailed malicious behaviors against driving VLMs. We begin by identifying errors in the final stage (e.g., plan) of the corresponding task and then trace backward to infer potential factors that could have contributed to these mistakes in the earlier stages, thereby constructing deceptive texts aligning with real-world scenarios involving unexpected traffic behaviors. Formally, our deceptive texts consist of several components corresponding to current and prequel stages. We define the expected errors in the form of textual description at the ith stage as \u0177\u2081. Then starting from each \u0177\u2081, we iteratively generate the cause-related texts \u0177i-1 for the preceding stage according to Eq. (5):\n$$\\hat{y}_{i-1} = G \\left(T_R ; x^v, \\hat{y}_i \\right),$$\nwhere G(\u00b7) is realized by leveraging an auxiliary VLM (e.g., powerful ChatGPT [49]) initialized with a Reversal Reasoning Template Tr, which is detailed in the Appendix. This process continues backward through each stage, leveraging the powerful logical capabilities of the auxiliary VLM to construct a coherent chain of reasoning that leads to errors. Ultimately, our deceptive texts \u0177 can be formalized as a continuous sentence, where each stage's texts are causally connected through reasoning links, as shown in Eq. (6):\n$$\\hat{y} = \\hat{y}_1 \\circ \\hat{y}_2 \\circ \\dots \\circ \\hat{y}_n,$$\nwhere the symbol o represents the operation of combining the processed text fragments into a single cohesive structure, and n denotes the total stages of the corresponding task. Examples of deceptive texts can be found in the Appendix.\nAfter constructing the low-level deceptive text \u0177 in Eq. (6), our objective is to optimize the perturbation such that the adversarial images encapsulate the semantics of \u0177. Specifically, by leveraging a pre-trained modality-aligned model E (e.g., CLIP [51]) with a visual encoder Ev and a textual encoder Et, the optimization objective for the adversarial noise \u03b4 is to ensure that the adversarial images and the deceptive texts are as close as possible in the encoded latent space. To achieve this objective, we define the loss L\u2081 for low-level decision chain disruption as:\n$$\\mathcal{L}_1(\\delta ; x, y)=1-\\mathcal{S}\\left(\\mathcal{E}_v\\left(x_v+\\delta\\right), \\mathcal{E}_t(\\hat{y})\\right),$$\nwhere S(...) measures the cosine similarity between the visual representation of the adversarial images xv + \u03b4 encoded by Ev and the textual representation of the deceptive texts \u0177 encoded by Et. By maximizing this similarity, we align the adversarial images with the deceptive semantics of \u0177, causing the driving VLMs to misinterpret visual inputs and make erroneous responses."}, {"title": "4.2 Risky Scene Induction", "content": "While Sec. 4.1 effectively targets low-level reasoning breakdown, it struggles to adapt to the dynamic complexities of real-world driving contexts. To address Challenge 2 of dynamic driving contexts, our method aims to manipulate high-level safety assessments, rather than targeting isolated actions, ensuring the attack aligns with the complexities of real-world driving contexts. By disrupting the perceived safety of driving scenarios, we construct holistic risky situations that undermine the model's context-dependent inference.\nTo achieve this, we leverage a set of opposite descriptors D to represent the safety of a driving scenario. Specifically, we define two complementary categories of scene safety: \u201cA safe driving scenario\u201d and \u201cAn unsafe driving scenario\u201d. These descriptors serve as the basis for risky scene induction. Subsequently, we integrate an image-text matching framework, where the pre-trained modality-aligned model E in Sec. 4.1, which includes the visual encoder Ev and the textual encoder Et, is presented with both the image sequence xv = {x1, x2, ..., xn} and our predefined textual descriptors D as inputs. The model E then generates image feature vectors Ev (xv) and text feature vectors Et (D). These feature vectors are passed to a downstream matching head h, which performs cross-modal matching based on their feature similarity, formally expressed as:\n$$h\\left(x^v, D\\right)=\\operatorname{softmax}\\left(\\frac{\\mathcal{E}_v\\left(x^v\\right) @ \\mathcal{E}_t(D)^{\\mathrm{T}}}{\\left\\|\\mathcal{E}_v\\left(x^v\\right)\\right\\|_2\\left\\|\\mathcal{E}_t(D)\\right\\|_2}\\right),$$\nwhere @ denotes matrix multiplication and T represents matrix transpose. This yields probabilities that quantify the degree to which a given image aligns with a specific descriptor. The objective of risky scene induction is to make the matching results of the adversarial image sequence contradict those of the clean image sequence with respect to D, thereby constructing a holistic risky scenario.\nWe feed both the clean and adversarial images into the image-text matching framework in parallel, obtaining the matching results pclean = h(xv,D) and padv = h(xv + \u03b4, D) for the clean and adversarial images, respectively. Rather than directly associating the adversarial images with the 'unsafe' descriptor or merely inverting the prediction probabilities, our goal is to optimize the adversarial images such that the matching outcome contradicts the result pclean, thereby inducing a misanalysis at the holistic scene level. Specifically, we initialize a mask M with the same shape as pclean and padv with all zeros and for each image, we set the index corresponding to the class with the lower probability for the clean image to 1, which is formalized in Eq. (9):\n$$M[i, j]=\\left\\{\\begin{array}{ll}1, & \\text { if } j=\\arg \\min _k\\left(p_{\\text {clean }}[i, k]\\right) \\\\0, & \\text { otherwise }\\end{array}\\right.$$\nThen our optimization objective for high-level risky scene induction can be expressed as Eq. (10):\n$$\\mathcal{L}_h(\\delta ; x^v, D)=-\\operatorname{mean}\\left(\\log \\left(p_{\\text {adv }} \\odot M\\right)+\\log \\left(p_{\\text {adv }} \\odot \\neg M\\right)\\right),$$\nwhere denotes the element-wise multiplication which applies the mask to the respective probabilities, and mean (\u00b7) denotes the calculation of the average.\nWe generate detailed deceptive texts to disrupt the decision chain from a low-level perspective and induce a holistic risky scene from a high-level perspective. Together, these form the essential foundation and objectives for optimizing adversarial perturbations in the following section."}, {"title": "4.3 Overall Optimization", "content": "Building upon the preceding discussions, this section articulates the overall optimization for the adversarial noise \u03b4.\nTo complement the first two objectives further, we introduce the third objective, i.e., semantic discrepancy maximization that maximizes the semantic difference between the clean and adversarial images, thereby reinforcing the overall attack performance. The objective introduces perturbations that, while subtle at the pixel level, induce a more significant divergence in the latent space. To achieve this, we employ the same Ev in Sec. 4.1, aligned with the text encoder, to map the clean and adversarial images into corresponding visual feature representations. The semantic difference is measured by cosine similarity, leading to the following optimization objective, as the loss La defined in Eq. (11):\n$$\\mathcal{L}_a(\\delta ; x^v)=\\mathcal{S}\\left(\\mathcal{E}_v\\left(x^v+\\delta\\right), \\mathcal{E}_v\\left(x^v\\right)\\right),$$\nwhere S(,) measures the cosine similarity. Thanks to the exceptional capabilities of pre-trained modality-aligned models,"}, {"title": "5 Experiments and Evaluations", "content": "5.1 Experimental Setup\nTarget Models. In our experiments, we assess three state-of-the-art open-source AD VLMs, addressing both Visual Question Answering (VQA) and planning control tasks. These include Dolphins [42], built on OpenFlamingo [2], for driving-related dialogues; DriveLM [55], a LLaMA-based GVQA model even capable of coordinate-level recognition; and LMDrive [54], a LLaMA-based agent for closed-loop control in the Carla simulator [13]. In addition, we also evaluate the robustness of general VLMs adapted for AD tasks, including InstructBlip [9], LLaVA [38], MiniGPTv4 [77] and GPT-40 [49].\nDatasets. We evaluate the three driving-specific models on their respective benchmarks. The Dolphins Benchmark [42] and DriveLM-NuScenes [55] are primarily composed of question-answering data. For LMDrive, we assess closed-loop driving control on the LangAuto Benchmark-short [54]. As for general VLMs, we adapt them to the well-established Dolphins Benchmark for robustness evaluation.\nEvaluation Metrics. For Dolphins Benchmark, we adopt their original evaluation metrics [42], where the final score is the average score across six categories of tasks, where each task is evaluated by Accuracy, Language Score and GPT Score. For DriveLM, we extend the metrics used in [55]. The original evaluation approach employs four distinct metrics across different question types: Accuracy, ChatGPT Score, Match Score, and Language Score. To provide a more comprehensive evaluation, we also introduce the GPT Score for the"}, {"title": "5.2 Digital World Experiments", "content": "We first perform digital world attacks in both the open-loop (i.e., the model operates independently without feedback from the environment) and closed-loop (i.e., the model interacts with the environment and adjusts its behavior based on real-time feedback) evaluation setups.\nOpen-loop Evaluation. We first evaluate our CAD Attack in open-loop AD tasks, including driving-specific models, i.e., Dolphins [42] and DriveLM [55], and adapted general VLMs [9, 38, 49, 77]. For transfer-based attacks (e.g., FGSM, PGD), we respectively use Dolphins or DriveLM as the surrogate model to generate noise and then transfer on another; for other black-box attacks, they directly query the target models. Here, we run each experiment three times and calculate the average results. As shown in Tab. 1, we can identify:"}, {"title": "5.3 Real-world Experiments", "content": "In this section, we further investigate the adversarial robustness of AD systems driven by VLMs in the real world. The VLM-driven pipeline is illustrated in Fig. 4.\nReal-world Robotic Vehicles. We select two commercially available robotic vehicles suitable for AD research and development, i.e., JetBot [48] and LIMO [52], to conduct real-world attack experiments. Both vehicles are equipped with high-definition cameras, radar, IMU sensors, and basic motion capabilities. The JetBot vehicle [48] is equipped with more powerful computational resources, focusing on the application of artificial intelligence and deep learning algorithms within AD systems, and utilizes differential drive for motion. In contrast, the LIMO vehicle [52] is more oriented towards control-level applications, and we choose the Ackermann steering mode for its motion. The experimental environment consists of a manually constructed driving simulation track.\nAD Tasks Driven by VLMs. Since both robotic vehicles do not inherently support VLM-driven decision-making and control, we configure AD tasks for the experiment using advanced VLMs. Specifically, we define the task of the vehicle as road-following with compliance to traffic rules, utilizing the driving-specific Dolphins [42] for decision-making. This model generates high-level commands such as \u201cKeep going straight\u201d, which are then passed to LLaMA [60]. The LLaMA model converts these high-level commands into control instructions, such as wheel speeds and steering angles,"}, {"title": "5.4 Ablation Studies", "content": "In this section, we investigate several key factors that might impact the performance of CAD, thereby providing comprehensive insights and promoting a deeper understanding of our strategy. All the experiments conducted in this part use the Dolphins [42] target model on the Dolphins Benchmark.\nThe key roles of decision chain disruption, risky scene induction and semantic discrepancy maximization. To further analyze the contribution of each component in our attack design, we perform an ablation study by adjusting the corresponding hyperparameters for loss weight. Specifically, the impact of decision chain disruption in Sec. 4.1 and risky scene induction in Sec. 4.2 is reflected by \u03b1 and \u03b2, while that of semantic discrepancy maximization in Sec. 4.3 is reflected by \u03b3. We adopt a controlled variable approach. First, we set all three parameters to 1.0. Then, we sequentially adjust one parameter at a time to observe its impact on the attack results and identify the optimal configuration. Once the best two settings for the current ablated parameter are determined, we proceed with the ablation of the next parameter for each of these two best settings. Additionally, we also conduct complete ablation experiments for each component, which is represented by setting the corresponding parameter to 0.\nThe ablation results are presented in Fig. 7. In our progressive ablation experiments, it is evident that when a specific component (i.e., decision chain disruption, risky scene induction or semantic discrepancy maximization) is com-"}, {"title": "6 Countermeasures against CAD", "content": "In this section, we explore defense strategies to mitigate the potential negative social impacts. We incorporate several well-established defense techniques from input pre-processing to"}, {"title": "7 CADA Dataset", "content": "Datasets play a crucial role in advancing model research, particularly in areas where specialized benchmarks are lacking"}, {"title": "7.1 Construction Details", "content": "Our CAD Attack dataset (CADA) includes Scene-CADA and Obj-CADA, which correspond to scene-level and object-level adversarial perturbations, respectively.\nData collection. For Scene-CADA, we mainly consider injecting adversarial perturbations on the driving scenes (i.e., videos and images). Specifically, we adopt our CAD on visual images/videos in Dolphins Benchmark [42] and DriveLM-nuScenes [55] datasets. In contrast to our main experiments, during the noise generation process, we increase the number of queries to the auxiliary VLM to five, utilizing the aggregated results from all queries during optimization iterations in Decision Chain Disruption; we extend the set of opposite safety descriptors to five distinct groups in the Risky Scene Induction, enabling a broader basis for subsequent matching tasks. This augmentation enriches the diversity and complexity of the adversarial inputs, ensuring a more robust evaluation of model vulnerabilities. To construct a more practical dataset, we design Obj-CADA composed of a set of traffic signs with adversarial patches inspired by the real-world experiments in Sec. 5.3. Specifically, we collect a set of 140 traffic sign images, each with a size of 224 * 224, from publicly available resources on the Internet. The images include both pure logo images and real-world road images, covering common directive signs, prohibition signs, and warning signs. Using these images as the raw dataset, we apply the CAD Attack to generate adversarial patches. The attack settings follow the same configuration as those described in Sec. 5.3, and we attach them to the corresponding clean images to create printable adversarial traffic signs.\nVQA pairs construction. For Scene-CADA, we match each set of noisy videos or images with the original QA"}, {"title": "7.2 Data Properties", "content": "Severity ranking. Our dataset is systematically stratified into 4 distinct severity levels (0 to 4, where \u201c0\u201d denotes clean). This results in 4 times the amount of adversarial data compared to the original clean dataset, thereby enabling a more comprehensive evaluation under varying intensities of adversarial perturbations. Specifically, for scene-CADA, we introduce 4 different perturbation magnitudes in terms of the l\u221e norm: 0.02, 0.04, 0.06, and 0.08, corresponding to 4 severities. For Obj-CADA, we categorize 4 severity levels based on the size of adversarial patches, corresponding to 10%, 15%, 20%, and 25% of the original side length of the traffic sign images. Each patch is randomly placed within the image.\nDataset scale. Our Scene-CADA comprises 486 clean and 1,944 adversarial video-question-answer pairs as well as 4,076 clean and 16,304 adversarial image-question-answer pairs, encompassing key tasks of autonomous driving, including scene description, traffic object recognition, motion prediction, decision planning, and behavior explanation. Obj-CADA consists of 140 clean and 560 adversarial traffic sign image-question-answer pairs, and the task mainly focuses on analyzing driving actions constrained by traffic sign interpretations. Overall, our dataset contains 2,430 video pairs and 21,080 image QA pairs, including 18,808 adversarial visual QA pairs."}, {"title": "7.3 Preliminary Experiments", "content": "Here, we further evaluate AD VLMs [42] on our dataset, where Dolphins [42] and DriveLM [55] are evaluated on their respective Scene-CADA subsets. Additionally, Dolphins is also evaluated on Obj-CADA through GPT Score (higher the better performance) due to its superior zero-shot capability"}, {"title": "8 Related Work", "content": "Adversarial attacks, first introduced by Szegedy et al. [56], are inputs designed to mislead deep learning models, often visually imperceptible to humans. Initially, adversarial attacks often target visual models to induce misclassification or other erroneous predictions [4, 15, 33, 37, 43]. Extensive research has refined attack strategies, including gradient-based methods such as FGSM [15] and PGD [43], as well as perceptual attacks designed to disrupt semantic consistency [32, 33, 37, 40, 62]. Generally, adversarial attacks can be classified into white-box and black-box settings based on the attacker's capabilities. In a white-box setting, the adversary has access to the model's architecture and gradients, enabling the design of more targeted attacks. Conversely, in a black-box setting, perturbations must be crafted solely based on observable model outputs without knowledge of the model's internal workings.\nIn the context of classical autonomous driving, adversarial attacks have been applied to exploit vulnerabilities in perception modules critical to autonomous driving. For example, Wang et al. [62] proposed an adversarial attack targeting vehicle detection systems, demonstrating how adversarial camouflage can compromise safety-critical vehicle recognition. Similarly, Sato et al. [53] introduced adversarial patterns designed to mislead lane detection systems, impairing a vehicle's ability to maintain its trajectory. However, these attacks focus exclusively on traditional models and do not address the emerging challenges VLM-based models pose. In the"}, {"title": "9 Conclusion and Future Work", "content": "In this paper, we propose the CAD, the first black-box adversarial attack specifically tailored for VLM AD based on decision chain disruption and risky scene induction. We conduct extensive experiments on both AD VLMs and general VLMs, as well as the real AD vehicles driven by VLMs, showing the superiority of our attack. In addition, we present the CADA dataset, comprising 18,808 adversarial visual-question-answer pairs, to facilitate research.\nLimitation and Future Work. While the results outlined in this work are promising, several valuable avenues for future research remain. We would like to explore the textual domain for a more comprehensive attack framework. We would like to incorporate visually inconspicuous attack design such as camouflage. Future work would like to validate its attack against commercial AD systems."}]}