{"title": "Prompt and Prejudice", "authors": ["Lorenzo Berlincioni", "Luca Cultrera", "Federico Becattini", "Marco Bertini", "Alberto Del Bimbo"], "abstract": "This paper investigates the impact of using first names in Large Language Models (LLMs) and Vision Language Models (VLMs), particularly when prompted with ethical decision-making tasks. We propose an approach that appends first names to ethically annotated text scenarios to reveal demographic biases in model outputs. Our study involves a curated list of more than 300 names representing diverse genders and ethnic backgrounds, tested across thousands of moral scenarios. Following the auditing methodologies from social sciences we propose a detailed analysis involving popular LLMS/VLMs to contribute to the field of responsible AI by emphasizing the importance of recognizing and mitigating biases in these systems. Furthermore, we introduce a novel benchmark, the Pratical Scenarios Benchmark (PSB), designed to assess the presence of biases involving gender or demographic prejudices in everyday decision-making scenarios as well as practical scenarios where an LLM might be used to make sensible decisions (e.g., granting mortgages or insurances). This benchmark allows for a comprehensive comparison of model behaviors across different demographic categories, highlighting the risks and biases that may arise in practical applications of LLMs and VLMs.", "sections": [{"title": "Introduction", "content": "Given the recent and prominent diffusion of Large Language Models (LLMs) and Visual Language Models (VLMs) outside the artificial intelligence community, advanced machine-learning based tools such as GPT4 [6], Gemini [61], Gemma [24], Qwen [8] or Llama-3-8B\u00b3, are as of now employed daily by non-experts, even in work environments. A review of the pertinent literature reveals that a lot of these use cases process personal data, such as in legal practice [11, 29, 64, 66], medical therapy recommendations [19, 33, 65], actuarial work [9], and several other fields [28, 42, 63].\nAs pointed out in seminal works, such as [32, 36], the metaphor of these models as individuals that can be administered psychological surveys [38, 46] or expected to hold consistent opinions [40, 45] should be discussed thoroughly as"}, {"title": "Related Works", "content": "The study of biases related to first names in the English language and their repercussions in real life has been the focus of academic research outside the machine learning field for many decades."}, {"title": "Methodology", "content": "We want to assess the role and impact of first names in prompts for LLMs and VLMs over different classification tasks involving ethical assessments. Since names are correlated with gender and ethnic-national background [43,68], this task allows us to investigate potential biases in model outputs and understand"}, {"title": "Dataset", "content": "In the following we outline the two datasets that we adopt in this paper, the ETHICS dataset from the social sciences literature, and the Pratical Scenarios Benchmark dataset, which we built specifically for addressing potential biases in real-world LLM/VLM applications."}, {"title": "ETHICS", "content": "The ETHICS [27] dataset was proposed to assess basic knowledge of ethics and common human value. We chose ETHICS for two main reasons. First, it was developed by experts in the field of psychology and philosophy by specifically taking into consideration the relative literature and tailoring scenarios upon well-established ethical dilemmas and human values, rather than asking non-experts to describe contexts that feel appropriate, as done by prior work and discussed in [27]. Second, it spans over multiple categories and different ethical theories. The data comprised of more than 130,000 text scenarios divided into five main categories: Justice, Deontology, Virtue, Utilitarianism, and Commonsense. The ETHICS dataset evaluates moral judgments across these 5 distinct categories, each representing a fundamental aspect of normative ethics:"}, {"title": "Justice", "content": "This category involves scenarios that test the model's understanding of two main components; impartiality and desert7. It includes 2,704 examples."}, {"title": "Deontology", "content": "Deontological scenarios are centered around rule-based ethics, where actions are evaluated based on their adherence to a set of predefined rules or moral principles, rather than the consequences of those actions. This approach to ethics emphasizes duties and obligations that are binding regardless of the outcomes they produce. The dataset includes 3,596 scenarios focused on this principle."}, {"title": "Commonsense", "content": "The Commonsense Morality Benchmark evaluates how well models can make intuitive moral judgments about actions. Unlike tasks that assess factual knowledge, this benchmark centers on discerning which actions align with widely accepted moral standards. It involves scenarios where characters describe their actions, and models must predict whether these actions are morally acceptable. There are 3,882 sentences in this category."}, {"title": "Virtue ethics", "content": "This benchmark assesses moral behavior based on character traits and virtues such as honesty, bravery, and benevolence. Virtue ethics emphasizes the moral character of individuals and their inclination to act virtuously across different situations. Scenarios within this framework often involve actions that demonstrate kindness or courage, such as helping someone in need or speaking out against injustice. In total, the virtue-ethics benchmark includes 4,976 scenario-trait pairs."}, {"title": "Utilitarianism", "content": "The Utilitarianism benchmark evaluates actions based on maximizing overall well-being or utility for all individuals involved. Differently from the previous ones, in this case two scenarios are presented, and the task is two rank them from most to least pleasant. The original objective of this sub task, as presented in [27] is to develop an utility function to measure pleasantness. This benchmark comprehends 2,404 pair of examples.\nHowever, including the Utilitarianism benchmark in our test pipeline is not directly relevant. Unlike benchmarks such as Justice, Deontology and Commonsense, which are sensitive to individual identities and norms implied by first names, utilitarianism does not directly address demographic biases or personal identities. Therefore, integrating first names into utilitarianism scenarios does not provide meaningful insights into how biases related to personal identities influence ethical judgments, which is the primary focus of this paper. The ETHICS test data is split into Normal and Hard test sets, for our experiments we will focus on the Normal test set alone."}, {"title": "Pratical Scenarios Benchmark", "content": "We collect a novel benchmark, which we refer to as Pratical Scenarios Benchmark (PSB). Differently from ETHICS, the samples in this benchmark are collected from people who are not experts in ethics or philosophy. The motivation behind this benchmark is to compare the performance between the two sets of text scenarios. We therefore evaluate the models on custom scenarios that mimic real-world decision-making situations, that supplement the findings from the ETHICS dataset.\nWe collect about 50 scenarios that reflect common decision-making processes where LLMs might be deployed, such as visa applications, loan approvals, or eligibility assessments. The simplicity of these questions allows us to directly"}, {"title": "Experiments", "content": "We perform several experiments combining different settings and models, reporting the results using the Large Language Models llama3, Qwen [8] and Mistral [31] over the different subtasks of ETHICS [27] and our benchmark dataset PSB. All the results presented in the following sections are averaged over three runs for each name. We frame the problem as a binary classification task for the different test sets of ETHICS (normal split only). For each sample from Justice, Deontology, Commonsense and Virtue we ask the LLMs to classify the query according to the task by changing the system prompt to respect the slight differences between the different ethical subtasks. In particular, we ask whether a sentence is reasonable for Deontology and Commonsense but we ask if it is morally acceptable for Justice samples. We report accuracy and Goodness for the Justice, Deontology, Commonsense, but only accuracy for Virtue as it would not have any meaningful interpretation. This can be better understood by looking at the examples in Tab. 1."}, {"title": "Gender bias", "content": "As a basic demographic split, we choose to aggregate the results over the perceived gender of the first name. As already highlighted in several previous works, gender is (correctly or not) assumed from the first name. In this section, we test"}, {"title": "Ethnical and National bias", "content": "To evaluate the presence of ethnical and national bias in Large Language Models, we employed two distinct methods. First, we assessed the models by prepending names to the text scenarios, following the methodology described in Sec. 3. This approach leverages names as demographic signals, which are often correlated with specific gender and ethnic-national backgrounds, to detect biases in model responses."}, {"title": "Pratical Scenarios Benchmark results", "content": "Here we report the results over our benchmark (see Subsection 4.2), collected as a complementary test for ETHICS. The results in Tab. 8 collect all the language models' performances over our benchmark. Observing Tab. 8, it is evident that the African and African-American demographics consistently show lower success rates compared to others across all models. A notable observation is Llama3's behavior compared to Qwen and Mistral; Llama3 appears to exhibit stricter evaluation criteria for African-American names and more leniency towards Hispanic names. Particularly in the Jobs subcategory, Llama3 shows the most significant performance variation. The Asian demographic achieves the highest accuracy (35.21%), whereas the African-American group experiences a significant drop of 12.86% to 22.35%, followed closely by the Arab group with a 6.33% decrease.\nFor Qwen, the European demographic leads with the highest accuracy in the Generic subcategory (72.54%), while the African-American group records the lowest, with a drop of 1.36%. In the Jobs subcategory, Anglo names achieve the highest accuracy (87.10%), while the African demographic shows a substantial 7.77% decline. Additionally, both Llama3 and Qwen models exhibit notable gender disparities. Females outperform males in both the Generic and Jobs benchmarks for these models. For instance, in the Llama3 model, females achieve 53.13% in Generic and 32.25%"}, {"title": "VLM results", "content": "In this section we extend our evaluation beyond textual Large Language Models (LLMs) to include Visual Language Models (VLMs). This experiment aim to understand how visual representation could reveal biases similar to those observed in text-based models. Specifically, we leverage a generative model Stable Diffusion [49]8, to generate images representing several ethnic groups. Then we feed a Vision and Language Model (LLaVA [41]) with both the generated images and an appropriate textual prompt. An illustration of this pipeline is provided in Fig. 2. In these series of experiments, we replaced the \"Anglo\" label with \"American\" as that is the actual descriptor we insert in the prompt given to the text-to-image diffusion model. We do this to better align with"}, {"title": "Societal Impact", "content": "As this work revolves around a sensitive topic we took all precautions to avoid any offensive or inappropriate terms. It is indeed one of the main lines of inquiry of this"}, {"title": "Conclusions", "content": "This study has demonstrated that first names, interpreted as demographic proxies, can significantly influence the ethical decision-making outputs of Large Language Models (LLMs) and Vision and Language Models (VLMs). By appending first names to text scenarios, we identified biases related to gender and ethnic backgrounds that affect the models' performance in binary classification tasks. Our findings reveal that these biases can lead to discrepancies in accuracy, favouring one demographic over another due to the first name alone. This can be potentially impacting real-world decisions such as visa applications, loan approvals, and eligibility assessments. The implications of these biases underscore the critical need for developing fair and unbiased AI systems. Addressing these biases involves not only technical adjustments in model training and evaluation but also a broader commitment to ethical AI practices. Future work should focus on refining mitigation strategies and exploring additional demographic factors to further enhance the fairness and reliability of AI-driven decision-making processes."}]}