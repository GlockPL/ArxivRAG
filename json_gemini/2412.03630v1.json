{"title": "Evaluating Single Event Upsets in Deep Neural Networks for Semantic Segmentation: an embedded system perspective*", "authors": ["Jon Guti\u00e9rrez-Zaballa", "Koldo Basterretxea", "Javier Echanobe"], "abstract": "As the deployment of artifical intelligence (AI) algorithms at edge devices becomes increasingly\nprevalent, enhancing the robustness and reliability of autonomous AI-based perception and decision\nsystems is becoming as relevant as precision and performance, especially in applications areas consid-\nered safety-critical such as autonomous driving and aerospace. This paper delves into the robustness\nassessment in embedded Deep Neural Networks (DNNs), particularly focusing on the impact of\nparameter perturbations produced by single event upsets (SEUs) on convolutional neural networks\n(CNN) for image semantic segmentation. By scrutinizing the layer-by-layer and bit-by-bit sensitivity\nof various encoder-decoder models to soft errors, this study thoroughly investigates the vulnerability\nof segmentation DNNs to SEUs and evaluates the consequences of techniques like model pruning and\nparameter quantization on the robustness of compressed models aimed at embedded implementations.\nThe findings offer valuable insights into the mechanisms underlying SEU-induced failures that allow\nfor evaluating the robustness of DNNs once trained in advance. Moreover, based on the collected data,\nwe propose a set of practical lightweight error mitigation techniques with no memory or computational\ncost suitable for resource-constrained deployments. The code used to perform the fault injection\n(FI) campaign is available at https://github.com/jonGuti13/TensorFI2, while the code to implement\nproposed techniques is available at https://github.com/jonGuti13/parameterProtection.", "sections": [{"title": "1. Introduction", "content": "There is an increasing interest in developing domain-specific processors for the deployment of artificial intelligence (AI) algorithms at the edge and at the endpoints. This trend is driven by the necessity of freeing endpoint AI based systems from having to send acquired data to external, more powerful computing machines to be processed and wait for a response before any action can be executed. The goal is to make embedded AI systems completely autonomous, avoid security and reliability issues associated with data transmission, and reduce response latency. Achieving embedded AI processing autonomy makes it possible to extend the applicability of complex AI algorithms, such as the increasingly widespread deep neural networks (DNN), to domains in which there may be severe communication bandwidth constraints, hard reliability and security specifications and/or real-time response requirements. Examples of such application areas are intelligent vision, autonomous navigation, advanced driving assistance systems (ADAS), autonomous driving systems (ADS), and aerospace applications such as remote sensing and airborne flight control [1].\nIf AI has to be pervasively integrated in systems that autonomously operate in real-world environments, AI processors must meet not only demanding performance specifications, but also strict safety and reliability standards. This is nowadays a mayor concern in the development of AI-based systems that must be integrated in safety-critical applications such as in the aerospace [2] and automotive [3] domains. For instance, the main agent that jeopardizes the performance in the aerospace applications is the exposure of aircraft and spacecraft electronics to radiation [4]. When a radiation particle interacts with electronic components, the logical value stored in a cell may be altered, resulting in a single event upset (SEU) or, more specifically, a single bit upset (SBU) when it only affects a single bit or a multiple bit upset (MBU) if multiple bits are affected. Field programmable gate arrays (FPGA) are particularly sensitive to SBUs, which can affect both the sequential elements of the implemented circuit (flip-flops or Block RAMs) and the configuration memory (LUTs). In the simplest case, a SBU will cause a bit of one of the implemented artificial neural network (ANN) model parameters to flip. Depending on the specific position of the SBU, the network output may change and differ from the expected one (critical errors), putting the safety of the system at risk. In the context of ADS, soft errors at terrestrial altitudes occur due to the following factors: the interaction of high-energy cosmic neutrons with silicon, the interaction of low-energy cosmic neutrons with high concentrations of 10B in the device and the emission of alpha particles from trace radioactive impurities in the device materials [5]. According to [6], when exposed to terrestrial neutrons, a bit in SRAM has a probability of 1.33 * 10-24 to flip in 1ns. As explained in [7], for a typical neural network of more than 10 million parameters, the probability of at least one bit-flip occurring in a month is 10%. This problem is aggravated as a consequence of increasing miniaturization and the use of lower voltage levels in modern integrated circuits.\nIn addition to background radiation, embedded devices are also vulnerable to other factors such as disturbance errors in storage devices (SRAM and DRAM) and also to deliberate malicious bit-flip attacks (BFA). Moreover, alternative technologies to traditional meta-oxide-semiconductor devices for the more efficient in-memory computation, e.g. memristive devices, are particularly prone to bit instabilities [8, 9]. Disturbance errors are a general class of reliability problem that affects memory and storage technologies such as SRAM, DRAM, flash and hard-disk [10]. Write failure and read disturb are two major causes of SRAM technology failures tightly related to miniaturisation and low-voltage operation [11]. In the case of DRAM, the so-called RowHammer vulnerability is a well-known issue that produces data corruption and the appearance of multiple bit-flips [10]. This has attracted the attention of attackers, who can maliciously flip memory bits in DRAM without the need of any data access privileges [12, 13]. If extended to safety- critical domains such as ADS and aerospace, the consequences of data-oriented attacks can be catastrophic, especially when the BFAs are targeted [14].\nThe outstanding performance of state-of-the-art DNNs in many applications is very often at the expense of increasing their size and complexity dramatically [15]. Although it strongly depends on the design of the digital circuit that implements their functionalities (the \"application layer\", [16]), large models are generally more robust due to their overparameterized nature [17, 18]. However, deploying complex AI models into resource-constrained embedded processing systems usually implies, firstly, applying one or various compression techniques to transform large models into the so-called lightweight deep learning models. These procedures aim to reduce the computational complexity and the memory footprint of original DNNs maintaining comparable performance. The most used approaches include network pruning, parameter quantization, knowledge distillation, and architecture search [19]. Secondly, it is usually needed to design custom processing units that help accelerate algorithm execution to meet speed and power consumption specifications. This is achieved by tailoring specific pipelines for data parallelism and applying arithmetic optimization techniques to make the most of both available memory and computational resources. Depending on the selected target technology (the \"physical layer\", [16]), e.g., embedded GPUs, FPGAs, or application-specific integrated circuits (ASIC), there exist different design constraints to consider and specific optimization techniques to apply. In any case, our concern is focused on the effects of SBUs on the parameterization of the DNNs, i.e., how the values of trainable parameters are altered and the potential consequences of these changes on the performance of the DNN models.\nIn this paper, we analyse the reliability of encoder-decoder DNN models designed for image segmentation tasks against SBU disturbances. This analysis has been performed with two main objectives in mind. Firstly, the focus is on the consequences that compression techniques such as model pruning and parameter quantization may have on the robustness of these models. Secondly, a more comprehensive study has been conducted to precisely determine how such disturbances modify the model performance in the inference process. The aim was to use this knowledge to propose design techniques that help improving model robustness, and thus system reliability, with no cost in terms of computational complexity and memory footprint. This work was originally motivated by the necessity to improve the reliability of some image segmentation DNN models designed to be applied in ADAS/ADS when implemented on the target processing devices, both embedded GPUs and FPGAs. To date, there are few published studies that analyse in detail and in a statistically significant way the impact of SBUs on the robustness of image segmentation DNN models, particularly when subjected to compression for deployments at the edge. This paper aims to fill this gap by performing a detailed layer-by-layer and bit-level analysis based on SBU emulation to better understand the mechanisms that produce failures in the behaviour of the models. The analysis has been performed both for 32-bit floating-point and 8-bit quantized representations to cover final implementations on different devices. Although the study has been necessarily carried out for a specific model architecture and a particular dataset, the purpose of the work is to provide guidelines and analysis tools applicable to other models, particularly those of the encoder-decoder type. Moreover, it also has served to program a set of memory-, computation- and training-free procedures for the protection, to a certain extent, of the performance of such models against SBUs. The code has been made public at [20].\nThe main contributions of this paper are:\n\u2022 We analyse in a statistically significant way how SBUs alter the performance of encoder-decoder DNN models in image segmentation tasks, determining the sensitivity of the output to single bit-flips in the model parameters according to the layer depth, the parameter type, and their binary representation.\n\u2022 We analyse how and why model pruning affects the robustness of the models against SBU perturbations.\n\u2022 We study the consequences of applying parameter quantization in terms of model robustness.\n\u2022 We provide in [21] a modification of the original TensorFI2 [22] code to allow applying fault injection (FI) campaigns on quantized TensorFlow Lite models and on big unquantized TensorFlow2 models too.\n\u2022 We describe some simple rules to improve model robustness by protecting model parameters against the consequences of SBUs with no cost on computational complexity neither on the memory footprint.\n\u2022 We provide in [20] the code to perform fault mitigation based on the above mentioned technique."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Robustness of ANNs against bit-flips", "content": "Most published papers on ANN robustness against bit-flips focus on classification tasks, while very few analyse semantic segmentation models [23]. The majority of the authors have carried out experimental fault injection campaigns combined with statistical analysis, while only a few have developed a theoretical analysis based on the development of a vulnerability model.\nAmong the experimental works, papers such as [24] and [25] test the robustness of several image classification convolutional neural network (CNN) architectures against single bit-flips in their weights, but with relatively superficial analyses. In [26], the authors compare the robustness of multilayer perceptrons and CNNs of different sizes, concluding that larger networks with more layers are more robust than smaller, shallower networks. Some authors, such as [27], investigate the impact of individual bit-flips while comparing floating-point and fixed-point representations, concluding that the latter are more robust.\nWorks like [28] and [29] evaluate the effects of both quantization and pruning on the robustness of models. The former shows that compressed models are more fault-resilient compared to uncompressed models in terms of bit error rate zero accuracy degradation, but statistical significance due to a short number of experiments leads to a high variance in the prediction results. The latter finds that integer-only quantization acts as a fault mitigation technique by reducing the overall range of the data. It also concludes that pruning enhances the resilience of deep models as a consequence of the reduction in the occupied area and execution times.\nSome works focus on methods to ensure the statistical significance of fault injection campaigns. In [30], the authors present a methodology to evaluate the impact of permanent faults affecting CNNs in automotive applications. Similarly, [31] describes how to correctly specify statistical fault injections and proposes a data analysis on the parameters of a CNN for image classification tasks to reduce the number of fault injections required to achieve statistically significant results. [32] presents one of the most exhaustive and complete analysis of the vulnerability of 32-bit floating-point CNNs for image classification tasks. The author considers various factors such as bit position, bit-flip direction, parameter sign, layer width, activation function, normalization, and model architecture. The key findings are: the vulnerability is caused by drastic spikes in a parameter value, the spikes in positive parameters are more threatening, an activation function that allows negative outputs renders the negative parameters vulnerable as well, and the dropout and batch normalization (BN) layers are ineffective in preventing the massive spikes that bit-flips cause. In [33], the author presents two exhaustive tools for fault injection in models created with both TensorFlow1 and TensorFlow2 [22], and performs a thorough analysis of the consequences of fault injections in different classification models. Additionally, the article also explores techniques to identify the source of the error by the analysis of changes in the model's predictions. In [34, 35], floating-point and fixed-point data type model implementations are analysed showing that fixed-point data provide the best trade-off between memory footprint reduction and CNN resilience. Similarly, [36] performs a comprehensive layer-wise fault analysis of homogeneously and heterogeneously quantized DNNs, suggesting that quantizing the DNN model heterogeneously to fewer bits helps increase the model's resiliency.\nAs a consequence of the growing concern about the threat of deliberate BFAs, some authors are studying the effects of simultaneous bit disturbances across multiple model parameters. In the extensive work presented in [14], BFAs are analysed according to their untargeted or targeted nature, and an effective mitigation methodology against targeted BFAs is proposed. In [37], the authors apply a progressive bit search algorithm to investigate the effects of bit-flip-based weight attacks and obtain some relevant observations regarding quantized DNN sensitivity: the most sensitive parameters are the ones close to zero (large parameter shift), the weights in the front-end layers are the most sensitive, and BFAs force almost all inputs to be classified into one particular output class. Similarly, in [38], the authors evaluate the accuracy degradation of an 8-bit integer quantized DNN as a consequence of untargeted random BFAs, one-shot BFAs and progressive BFAs for image classification tasks. The authors show that with the most exhaustive BFA, i.e. progressive BFA, the accuracy drops to 1% after just 5 iterations. This result aligns with the one described in [39], where the author of the progressive BFA algorithm shows that an 8-bit integer quantized ResNet-18 can malfunction after just flipping 13 weight bits out of 93 million.\nOne of the first works that attempted to formalize a theoretical method to evaluate the robustness of DNN models was presented in [40]. It describes a layer-wise relevance propagation model based on the analysis of the contribution of individual neurons to the final loss. With a similar approach, gradient-based methods such as that explained in [41], analyse the sensitivity of each network layer according to the importance (relevance) of the weights during inference. In [7], the authors present a theoretical analysis of error propagation on some commonly used processing layers in image classification models when the sign bit is flipped. In [42], a vulnerability model based on some key features such as gradient and absolute value of the parameters is constructed to reduce the necessary amount of fault injections to perform robustness analysis. [43] proposes BinFI, a fault injector for finding safety-critical bits in machine learning applications that significantly outperforms random fault injection methods in terms of computational costs. Finally, in [44], the authors formulate a bit-flip-based weight fault propagation model for 32-bit floating-point CNNs to analyse the robustness of ReLU-based models and propose a hardening method based on function upper bounding. Nevertheless, the applicability of these methods to segmentation networks is not straightforward because they are primarily designed for classification networks, where the layers under study are typically simple convolutions and fully-connected layers. Additionally, gradient-based analyses can be inaccurate due to noisy gradients and challenges such as vanishing or dying gradients associated with common activation functions like Sigmoid and ReLU.\nAll the above-mentioned works are focused on detection and classification DNN models. Regarding the few papers that deal with segmentation networks, [45] claims to have performed the first fault injection study of DNNs performing semantic segmentation, proposing a critical/tolerable fault categorisation for a 32-bit floating-point DeeplabV3+ network. [46] evaluates the reliability of neural networks for various tasks, including semantic segmentation, implemented in 32-bit floating-point representation on a GPU trained with Supervised Compression for Split computing. In [47], the author stresses the importance of statistical significance in the analyses, and proposes a fast reliability methodology exploiting statistical fault injections in a U-Net model for image segmentation. The author performs a comparison of the results obtained with this method to those obtained by random FI and improperly-defined statistical FI campaigns and shows the inability of the latter campaigns to reveal sensitive parameters of U-Net."}, {"title": "2.2. Hardening and protection of neural networks against fault occurrence", "content": "Most of the papers that propose methods to protect ANNs against faults are also concerned with image classification models. According to the proposed protection technique, these can be grouped as those that use redundancy, modifications of the activation functions, modifications of the parameters, and modifications of the training/inference process.\nOne of the most basic methods for error detection and protection is the use of a checksum together with a replication of the model, which, while highly effective, is also prohibitively costly in terms of memory consumption and computing overhead. [48] and [49] are two examples where full duplication has been shown to be effective. As proposed in [50, 51, 52], using a proper model sensitivity analysis makes it possible to optimize redundancy to protect only the most critical layers. In the same direction, [17] presents a software methodology based on a triple modular redundancy technique to selectively protect a reduced set of critical neurons, under a single fault assumption, by a majority voter correction technique. In [53], the authors characterize fault propagation not only by exposing the FPGA/GPU to neutron beams but also by performing a thorough fault injection campaign. Based on the observations, the authors propose a strategy to improve system reliability by adapting algorithm-based fault-tolerant solutions to CNNs, i.e., adding invariants to the code for quick error detection or correction. In [54], the authors propose protecting the matrix multiplication operation of the CNNs in GPUs based on a three-stage methodology to selectively protect CNN layers to achieve the required diagnostic coverage and performance trade-off: sensitivity analysis to misclassification per CNN layers using a statistical fault injection campaign, layer-by-layer performance impact and diagnostic coverage analysis, and selective layer protection. Finally, [55] proposes a reduced-precision duplication with comparison technique to improve the reliability of computing devices to reduce overhead. It is suitable for mixed-precision architectures, such as NVIDIA GPUs.\nThe bounding of activation functions is an alternative technique to redundancy to reduce implementation overhead. In [56], the authors perform a comprehensive error resilience analysis of DNNs for image classification tasks subjected to hardware faults in the weight memory. Then, ClipAct is applied, an error mitigation technique based on squashing the high-intensity faulty activation values to alleviate the impact of faulty weights on predictions. In [57], Ranger is proposed, a low-cost fault corrector which selectively restricts the ranges of values in specific DNN layers to dampen the large deviations typically caused by transient faults leading to silent data corruptions. In [58], FitAct is proposed, a low-cost approach to enhance the error resilience of DNNs for image classification tasks by deploying fine-grained post-trainable activation functions. The main idea is to accurately bound the activation value of each individual neuron via neuron-wise bounded activation functions to prevent fault propagation in the network. In [18], the author presents a comprehensive methodology for exploring and enabling a holistic assessment of the trilateral impact of quantization on model accuracy, activation fault reliability, and hardware efficiency. The framework allows for the application of different quantization-aware techniques, fault injection, and hardware implementation and directly measure the hardware parameters. A novel lightweight protection technique integrated within the framework that ensures the dependable deployment, evaluating the maximum values of the layers' activations and replacing the out-ranged values with either lower or upper-bound to avoid fault propagation, is also proposed. Finally, [44] presents a boundary-aware ReLU to improve the reliability of DNNs by determining an upper bound of the activation function which is theoretically calculated so that the deviation between the boundary and the original output cannot affect the final result.\nSome other works explore methods to enhance network robustness by directly modifying the network parameters. Based on the findings of [40] about the connection between neuron resilience and its contribution to the final prediction score, the authors of [59] propose a methodology based on architectural and feature optimization to avoid critical bottlenecks and balance the feature criticality inside each layer. In [60], the authors propose MATE, which is a low-cost CNN weight error correction technique based on the observation that, as all mantissa bits of the weights are not closely related to accuracy, some of them can be replaced with error correction codes. Therefore, MATE can provide high data protection with no memory overhead. In [61], the authors perform a comparison of the robustness among 32-bit floating-point, 16-bit floating-point, and 8-bit integer formats for image classification tasks and propose an opportunistic parity method to detect and mask errors with zero storage overhead.\nThere are also proposals to harden network performance by the modification of the training or inference process. The authors of [62] propose a bipolar vector classifier which can be easily integrated with any CNN structure for image classification tasks that end in a fully connected layer. The underlying idea is that as the weights of the classifier are binarized to \u00b1 1, the resulting final feature vector will only contain positive/negative values that will also be binarized to \u00b1 1, and thus, a pattern will be created. Each class has a specific reference pattern so, to assign the final feature vector to a certain class, it will be compared with all the reference patterns, and the winning class will be the one with the smallest Hamming distance. In [63], the authors compare the accuracy of quantized DNNs (QNNs) for image classification tasks during accelerated radiation testing when trained with different methodologies and implemented with a dataflow architecture in an FPGA. The authors find that QNNs trained with fault-aware training, a kind of data augmentation methodology to allow the network to also experience errors during training, make QNNs more resilient to SEUs in FPGAs. In [64], an efficient error detection solution for object detection-oriented CNNs is proposed based on the observation that, in the absence of errors, the differences between the input frames and the inference provided by the CNN should be strictly correlated.\nFinally, regarding the specific works that focus on the hardening of DNNs against BFAs, [37] proposes binarization-aware training and piecewise clustering as methods to enhance the resistance of quantized DNNs. The authors conclude that applying binary quantization, increasing network capacity, and using dropout or Batch-Norm regularization are effective techniques to build resistance, while applying adversarial weight training or pruning is shown to be ineffective. In [38], a three-step algorithm based on mean calculation, quantization and clipping is proposed to reconstruct the perturbed quantized weight matrix to tolerate the faults caused by BFAs. The overhead introduced by the process is small and the protected DNN can better cope with progressive BFA than non-protect DNN (accuracy of 60% and 1% respectively after 5 iterations). A completely different approach is presented in [14], where the authors propose a dynamic multi-exit architecture that trains extra internal classifiers for hidden layers that can tolerate the existing attacks which flip bits in one specific layer. The experiments are conducted using well-known DNN structures and image classification datasets. Apart from the above-described methods, which aim to mitigate the effect of such attacks, there are also some other methods that focus on verifying the integrity of the models. In [65], the authors propose to extract a unique signature from the original DNN prior to deployment and then verify the inference output on-the-fly while trying to add the minimum performance and resource overhead. Indeed, due to the strict temporal and memory footprint constraints that edge devices must adhere to, the protection systems of interest to us are those that introduce minimal memory/computation overhead."}, {"title": "3. Model development and optimization", "content": ""}, {"title": "3.1. Architectural design and training", "content": "The segmentation model used as reference for this study is a U-Net, an encoder-decoder fully convolutional network (FCN) for image segmentation, but adapted to use hyperspectral images (HSI) as inputs.\nThe most recent version of the model [66] was trained using the HSI-Drive v2.0 dataset [66], intended for developing ADAS/ADS systems using HSI . The results on the test set can be found in Table 1.\nThe model, depicted in Figure 2, features a 5-level encoder-decoder architecture comprising two sequences of 3x3 2D convolutional layers (initially with 32 filters) followed by batch normalization and ReLU activation at each level. Additionally, it includes one 2x2 2D max-pooling layer per encoder level and one 2x2 transposed 2D Convolutional layer per decoder level. The resulting model comprises 31.14 million parameters and requires 34.60 GFLOPS per inference to execute (Table 2). Detailed information regarding the training and testing procedures can be found in [66]."}, {"title": "3.2. Model Compression", "content": "The most prevalent model compression techniques among deep learning developers for later implementation on edge devices involve applying pruning and/or quantization after training the 32-bit floating-point model. To evaluate how these compression techniques affect the model's robustness against SBUs, several factors must be considered. First, the size of the model and the architecture designed for its implementation as a digital processor, which directly influences the ratio of unused to used resources and the probability for a SBU to happen. However, device occupation is not the only parameter influencing the Device Vulnerability Factor (DVF), representing the probability of a configuration bit being critical for the design [67]. Overparameterized models can absorb more SBUs without producing critical errors since there are more irrelevant weights and biases whose perturbations do not alter the output. Thus, it is one of the objectives of this paper to analyse whether large reductions in DNN model sizes can be achieved using these compression techniques without degrading model accuracy or robustness to SBUs. For this, we are assuming streaming-like architectures with independent resources allocated for all layers so the impact of a SEUs is isolated [68]."}, {"title": "3.3. Pruning", "content": "Pruning facilitates the reduction of both the number of parameters to be stored and the number of computations to be performed. The concept involves eliminating the least significant parameters of the model. Depending on which parameters and how they are pruned, pruning can be categorized as fine-grained/sparse/unstructured, where the least important weights are rounded to 0, or coarse-grained/dense/structured, where entire filters are removed from the computational graph. While the former method generally allows for pruning more weights without harming performance, it is only justified if the processing system is able to optimize multiply and accumulate (MAC) operations with sparse matrices.\nIn this work, we have applied a conventional structured pruning approach that has been customized to perform model optimization in an iterative manner. The algorithm basically analyses the computational complexity of each layer while evaluating the impact of the pruning process on the model's accuracy to guarantee negligible impact on overall performance. As shown in Table 2, applying this method a 99% reduction in the number of parameters and a 75% reduction in the number of operations was achieved."}, {"title": "3.4. Quantization", "content": "Quantization aims to reduce the number of bits needed to store the model parameters, thereby reducing memory footprint (see Table 2). Additionally, it can speed up both data transfer and model inference for custom processor implementations. Depending on the target device, quantization can be more or less fine-grained in terms of homogeneity, uniformity, scale factor, symmetry, and mixed-precision. In this article, we have chosen a general and standardized heterogeneous quantization scheme, known as post-training integer quantization (PTQ), as implemented by TensorFlow Lite [69]. This procedure converts 32-bit floating-point numbers (weights and activation outputs) to the nearest 8-bit fixed-point numbers, while biases, due to the greater sensitivity of the models to perturbation on these parameters, are converted to 32-bit fixed-point numbers. This heterogeneous quantization scheme allows for model size reduction while preserving accuracy [69, 70]. As shown in Table 3, comparable segmentation metrics are obtained for the quantized model to those with the unquantized model, as quantization schemes applied to already trained models, are simple to apply and produce negligible accuracy degradation for most widely used ANN models [71, 72]."}, {"title": "3.5. Quantization of Pruning", "content": "To fully compress the model, both techniques can be applied consecutively: first pruning and then quantization. As shown in Table 3, after the quantization of the pruned model segmentation accuracy on the test set remains mainly unaltered. Table 2 sums up model complexity figures for each version of the reference model: original (00), pruned (10), 8-bit quantized (01), and pruned and quantized (11). As can be seen, the original memory footprint is reduced from 118.77 MB to just 317.44 KB."}, {"title": "4. Assessment of the fault injection campaign", "content": "To test the models' robustness against SBUs, an extensive fault injection campaign was conducted on the aforementioned models using a modified version of the TensorFI2 framework [22]. This modification enables a more memory-efficient implementation by directly accessing the parameters of the model without creating additional copies or intermediate tensors, ensuring that FI on complex, large models does not result in an excessive memory overhead. The original code has also been extended to support quantized TensorFlow Lite models and is available at [21]. To evaluate the perturbation produced by injected faults, the corrupted FCNs have been assessed using ten test images that represent the diversity of driving conditions in the dataset described in Section 3.\nPerformed FI campaign involved injecting single bit-flips into the parameters of the FCN model under analysis. Parameter sets include the weights/kernels of the 2D convolution layers (conv2Dk), bias of the 2D convolution layers (conv2Db), weights/kernels of the 2D transposed convolution layers (conv2Dtrk), bias of the 2D transposed convolution layers (conv2Dtrb), gamma of the batch normalization layers (BNy), and beta of the batch normalization layers (BN\u03b2). Thus, in what follows, p1 set of the network refers to the weights of the first conv2D layer, while p2 set represents the biases of that layer. In like manner, when it's mentioned that an error has been injected into parameter x, it means it has been injected into one of the m elements that comprise parameter x set.\nAs a general rule, for a given parameter, the higher the bit position in which the fault is injected, the greater its impact on the output, disregarding the sign bit. However, not every layer and every parameter contributes equally to the output. Previous studies [30, 34, 54] focused on tasks such as image classification and object detection, have shown that faults injected in the exponent bits (interval [23 - 30]) are more likely to alter the output. To verify whether this is also generally the case for this encoder-decoder model aimed to image segmentation and in order to set reasonable bounds for the range of bits to be modified, a first round of 150 faults per-layer spanning the entire range [0 - 31] were injected. We could verify that in some cases, bit-flips in the range [20-23] and in the sign bit (31) also modified significantly the inference result. Based on these preliminary results, a comprehensive fault injection campaign was conducted on bit positions [20 \u2013 31].\nThe quantity of fault injections was set to 1550 single bit-flip faults per layer, totaling 155000 injections to assure statistically representative experiments according to Equation 1 as proposed in [73]."}, {"title": "4.1. Fault injections in unquantized models", "content": "As a consequence of the 32-bit floating-point data representation, the effect of a bit-flip varies with its position (Figure 3). To maintain clarity in terminology, the most significant bit (MSB), the sign bit, is assigned position 31, while the least significant bit (LSB) is assigned position 0.Figures 4 and 5 depict the bit-flip error rate according to the bit position and the parameter position for both the unpruned and pruned models. As expected, the unpruned model exhibits greater robustness and can better withstand the injected faults. Once the general statistical results have been observed, let us now analyse in detail the insights of the sensitivity of the model performance to perturbations in the parameters."}, {"title": "4.1.1. Analysis of robustness of the unpruned model", "content": "Output conv2D layer_conv2D_22 (p99-p100 sets in Figure 2) consists of $n_{class"}, "filters of 1 * 1 * C dimensions and $n_{class}$ biases, where $n_{class}$ is the number of classes to be predicted (6 in this model) and C is the number of output channels from the previous layer, conv2D_21 (32 in this model).\nA signal calibration analysis reveals that the activations of the last convolutional layer lie within the range [-6.7656, 11.7859"], "0": "n their 30th bit. The impact of a bit-flip on bit 30 depends on two factors: the sign of the bias and the probability of the model predicting each class. In the scenario where the bit-flip occurs in a negative bias (biases 0", "yields": "n$\\%error_{30} = \\frac{1}{6}(0 + 55.09 + 4.41 + 73.05 + 7.47 + 83.73)$ = 37.29\nThe slight discrepancy between this value (37.29%) and the one obtained from the experimental data (34% in Figure 4) is due to the statistical error of the fault injection since it may not result in the exact same number of flips for each of the 6 classes (assuming a constant probability of $\\frac{1}{6}$ is thus only an approximation). Additionally, the highly unbalanced terms in the equation make the effect of the non-constant probability more noticeable.\nRegarding the weights, conv2D_22w, they are multiplied by the output activation of layer conv2D_21, which, due to the use of ReLU activation functions, is always \u2265 0. Hence, if the bit-flip occurs in a weight that is multiplied by a 0-valued activation from conv2D_21, the model will be immune to it (unless the bit-flip produces a non-desirable special value such as NaN in the weight itself). The impact of a flip in the sign bit is considered negligible and the effect of a flip in bit 30, similar to biases, will only be relevant if the weight is negative and belongs to a filter of the class predicted"}