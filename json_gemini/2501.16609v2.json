{"title": "COWPILOT: A Framework for Autonomous and Human-Agent Collaborative Web Navigation", "authors": ["Faria Huq", "Zora Zhiruo Wang", "Frank F. Xu", "Tianyue Ou", "Shuyan Zhou", "Jeffrey P. Bigham", "Graham Neubig"], "abstract": "While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose COWPILOT, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. COWPILOT reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent's by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. COWPILOT can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html", "sections": [{"title": "1 Introduction", "content": "Agents supported by large language models (LLMs) have become increasingly capable of automating digital tasks such as web navigation (Zhou et al., 2023; Deng et al., 2024). While existing frameworks for web agents mostly focus on solo, autonomous agents (Zheng et al., 2024b; Iong et al., 2024; Drouin et al., 2024), we argue that, for many practical tasks, users interact with the LLM agent for varied purposes such as supervision and collaboration, i.e., the copilot mode. While existing frameworks (L\u00f9 et al., 2024; Drouin et al., 2024; Wang et al., 2024a; Zheng et al., 2024b) mainly support users communicating with agents via natural language (NL) feedback, or recording actions of human users alone (Pan et al., 2024b), they do not support dynamic human-agent collaboration within a task session, where humans and LLM agents take actions alternately to solve tasks. We ask: How can we enable human-agent collaborative task-solving? and further, how do agents perform under such collaborative settings?\nTo facilitate studying these questions, we introduce COWPILOT (\u00a72), a lightweight framework that can be seamlessly integrated into user web activities as a Chrome extension. COWPILOT starts with the LLM agent proposing actions for the next step, meanwhile allowing human to pause or reject the agent-suggested action and take alternative ones to drive the process; human can also choose to resume the agent-driven process at any time to ease the effort (\u00a72.1). To systematically evaluate this collaborative process, we propose several metrics for task accuracy, user experience, and efficiency aspects (\u00a72.2).\nBeyond agent web automation, COWPILOT enables a wide range of use cases (\u00a73), including web automation3.1), data collection for agent trajectories and user feedback (\u00a73.2), as well as evaluations for single or multiple agents (\u00a73.3).\nWe conduct studies on five common websites across shopping, social, and technical domains (\u00a74). We show COWPILOT in collaborative mode achieves higher success rates over autonomous agents by 47%, and even human-only settings by 6%. Moreover, the LLM agent takes 84.8% of the steps and can drive up to half of the task success, greatly easing human efforts. These results suggest the potential for accuracy and efficiency improvement with COWPILOT.\nOverall, CowPILOT showcases the potential of human-agent collaborative web navigation, and"}, {"title": "2 COWPILOT", "content": "In this section, we introduce the COWPILOT framework (\u00a72.1) and evaluation metrics for task accuracy and collaboration quality (\u00a72.2)."}, {"title": "2.1 The CoWPILOT System", "content": "Given an objective o stated in natural language (NL) (e.g., book a flight from New York to Pittsburgh) for the web environment, we define two agents: one agent instantiated with an LLM policy \\( \\pi_\\epsilon \\), and one human agent \\( \\pi_\\eta \\). At each time step t, based on the observation \\( o_t \\) from the environment state \\( s_t \\), either the LLM agent or human agent generates an action \\( a_t \\), formalized as \\( a_t = \\pi(t, o_t, a_{0:t-1}) \\). Executing \\( a_t \\) on the environment results in a new state \\( s_{t+1} \\) that gives observation \\( o_{t+1} \\) that drives the next step. The two agents collectively generate a sequence of actions \\( a_{0:n} \\) over n steps, until it reaches a task termination condition, e.g., output STOP or a maximum number of steps. By default, the LLM agent starts generating actions \\( a_t \\) from t = 0 unless intervened by the human agent H. Table 1 shows the action space for LLM and human agents.\nActions taken by the human agent \\( \\pi_\\eta \\) are critical for optimizing COWPILOT's decision-making pipeline. When the human agent intervenes, they provide contextual feedback by identifying and correcting prior mistakes made by the LLM agent. This redirection helps the agent recover from a suboptimal path and proceed with a more viable course of action. At the same time, by integrating human actions into its action history, COWPILOT ensures that the LLM agent is aware of human corrections since its last decision, preventing redundant actions and enabling efficient task progression. To ensure effective integration of these human actions, CowPILOT incorporates the following core modules:\nSuggest-then-Execute under Human Supervision At any time step, the human agent \\( \\pi_\\eta \\) can decide to take over by generating action \\( a_t^H \\). More concretely, the LLM agent \\( \\pi_\\epsilon \\) generates an action \\( a_t^\\epsilon \\) and presents it as a suggestion for the tentative next step to the user (Figure 2, 1), which includes a visual indicator highlighting the target element for the proposed action, accompanied by a textual explanation of the agent's reasoning. This tentative step is presented to the human agent for at most five seconds, and is automatically executed if the human agent does not oppose. Otherwise, the human agent can choose to reject or pause the action (Figure 2, 3) and take over. They can also transfer the action back to the LLM-based agent by hitting the resume button (Figure 2, 4). This take-over-then-back process can be conducted unlimited times per task-solving session. This mechanism balances operational efficiency with user oversight, allowing users to intercept potential errors without the burden of manually approving every step.\nPause LLM Agent: Extract Human Actions Whenever the human agent \\( \\pi_\\eta \\) rejects the LLM-proposed action, our CowPILOT system starts tracking human activity on the websites, particularly what webpages and UI elements they interact with. To capture this micro-level metadata, we uti-"}, {"title": "2.2 Evaluation Metrics", "content": "To evaluate the agent performance in COWPILOT, we report general agent task success. In addition, to better quantify human-agent collaboration, we introduce five evaluation metrics to measure various aspects throughout task execution.\nGeneral Task Success To measure generic task success, we measure end-to-end task accuracy,"}, {"title": "3 Use Cases of COWPILOT", "content": "COWPILOT has numerous potential use cases. We particularly highlight three use cases under the scope of this work."}, {"title": "3.1 Web Automation", "content": "COWPILOT can be a standalone agent framework to automatically conduct web tasks for end users is implemented as a Chrome extension where all computations other than the LLM calls are handled locally with minimal storage requirement (<50MB). Any users can easily install COWPILOT with just four clicks and use it with their personal API key. We use LiteLLM\u00b2 proxy server for our backend LLM, enabling COWPILOT to support all models available via LiteLLM, including closed-source (e.g., GPT) and open-weight models (e.g., LLAMA). Depending on whether the user wants to participate in task-solving, our agent can operate in two modes: 1) Fully autonomous mode: the agent conducts a user-issued task start-to-end; 2) CoPilot mode: human and agent collaboratively solve a task, often useful for complex tasks where the agent is more prone to make mistakes."}, {"title": "3.2 Data Collection from Websites", "content": "COWPILOT can also be used as a data annotation tool to collect task trajectories across any website accessible via the Chrome browser. Deployed as"}, {"title": "3.3 Evaluation and Comparative Analysis of Agent Performance", "content": "COWPILOT can be used to evaluate and compare agent performance. We support a wide range of open-weight and closed-source models served via LiteLLM. While this paper focuses on comparing GPT and LLAMA, the framework can easily extend to other open and closed-source models.\nTo evaluate a particular model, the user can select a model before initiating a task. Once the task is completed, COWPILOT presents results evaluated in the metrics from \u00a72.2. To compare different"}, {"title": "4 Exemplar Findings via COWPILOT", "content": "To demonstrate the usage of COWPILOT, we evaluate on a subset of WebArena (Zhou et al., 2023) benchmark, including 27 tasks categorized into easy, medium, and hard difficulty levels. We categorize the difficulty by the number of examples successfully solved by the top-performing agent (Wang et al., 2024b) on WebArena, and assign them as easy, medium, hard if they have <2, 2\u20134, and >4 correctly solved examples among the same task template group. We evaluate under two settings: fully autonomous and copilot mode, using gpt-40-2024-08-06 and Llama-3.1-8B-Instruct as backbones for the LLM agent. For this study, three authors served as human agents, independently performing the tasks for both settings. The results reported represent the average performance across these evaluations.\nAdditionally, we included a baseline where tasks were executed solely by humans without any agent participation. Table 2 reports results on all metrics introduced in \u00a72.2."}, {"title": "4.1 Copilot Mode Achieves the Best Accuracy", "content": "CoPilot mode with GPT-40 achieves 95% task accuracy, significantly outperforming the 48% accuracy under autonomous mode (relatively by 97.9%), and even surpassing human task-solving accuracy by 6.7%. This suggests potential productivity increases when solving tasks together with strong LLM-based agents.\nOn the other hand, copilot mode with the smaller LLAMA 8B model does not bring similar accuracy increases, but slightly degrades the task accuracy by 8%."}, {"title": "4.2 Copilot Mode Requires Minimal Human Intervention", "content": "Despite the high task success rates, the GPT-based agent easily achieves the highest accuracy with an average of 1.1 human steps, taking only 15.2% of the entire trajectory. Instead, the LLM agent performs the majority, more precisely 84.8% of task steps. Similarly, when shifting to the weaker LLAMA model, the human-llm collaboration process requires two times more human involvement, resulting in humans and LLM agents spending roughly similar amounts of effort, taking 4.47 and 4.15 respectively. Figure 3 shows the correlation between human step count and end-to-end task accuracy.\nQualitatively, humans often choose to intervene when they observe that the LLM has gotten stuck (e.g., producing the same invalid actions multiple times) or performs an obviously wrong action (e.g., clicking 'Customers' instead of 'Orders' tab when searching for a particular order), especially when the webpage layout is less common or has a confusingly large number of elements."}, {"title": "4.3 Agents Drive Up to Half of the Success", "content": "In CoPilot mode, we notice that agent-drive completion accuracy was up to 52% of the time with GPT-40 model. Note that, given the task accuracy was 0.95, the copilot-mode agent successfully initiated half of the successes. These findings highlight that agents can follow the task objective and understand user actions to drive the task up to succeed."}, {"title": "5 Related Works", "content": null}, {"title": "5.1 Web Agent Plugin", "content": "The rise of LLM agents has led to the development of open-source toolkits for web automation, available as APIs, simulated environments, and Chrome extensions. Tools like MultiOn (MultiOn, 2024) and Anthropic (Anthropic, 2024) provide APIs for agent use but require setting up Docker images, posing barriers for non-technical users. BrowserGym (Drouin et al., 2024), AgentLab (Chezelles et al., 2024), WebArena (Zhou et al., 2023) utilize a dedicated Chromium browser instance to perform tasks on specified websites. However, this approach isolates browsing sessions, restricts multi-tab navigation, and diverges from standard workflows, which limits practical usability.\nChrome extensions, as adopted by tools like WebCanvas (Pan et al., 2024b), WebOlympus (Zheng et al., 2024b), OpenWebAgent (Iong et al., 2024), and Taxy (TaxyAI, 2024), present a more user-friendly alternative. They are easy to install, lightweight, and integrate seamlessly into standard browsing environments, making them accessible to end-users. While similar to COWPILOT, the extensions above lack features for fostering richer human-agent collaboration. Table 3 further compares how COWPILOT with the existing frameworks by illustrating its novel features."}, {"title": "5.2 LLM Agents for Web automation", "content": "Web automation has evolved through advancements in LLM-based agents and benchmarks. Early systems relied on HTML structures and accessibility trees (Deng et al., 2024; Gur et al., 2023, 2022; Kim et al., 2023). Visual-based systems such as SeeACT (Zheng et al., 2024a), VisualWebArena (Koh et al., 2024), WebGUM (Furuta et al., 2023) integrate spatial and visual understanding, enhancing agent performance in multimodal tasks. Benchmarks such as MiniWoB (Shi et al., 2017) laid the foundation for evaluating these interactions, while systems like WebShop (Yao et al., 2022), WebArena (Zhou et al., 2023), WebLINX (L\u00f9 et al., 2024) expanded to complex multi-step tasks in ecommerce and real-world websites.\nDespite these advances, existing systems focus largely on full autonomy, with limited support for human-in-the-loop collaboration. In contrast, COWPILOT bridges this gap by enabling dynamic, real-time human-agent interaction. Features like suggest-then-execute, pause, and resume facilitate adaptive task execution, make COWPILOT a robust platform for developing and evaluating agents in practical, real-world settings."}, {"title": "6 Limitation and Future Work", "content": "Currently CoWPILOT requires a human to act as an observer to oversee the task execution. This setup is intentional so that we can simulate task execution in live setting. We would like to extend our work so that it does not require constant human observation. Rather, we would detect the critical steps that require human observation only. In the future, we would extend CoOWPILOT for a multi-LLM agent setup where we can simulate a user by a second LLM agent. Such setup would help us to approximate human decisions automatically using LLM autorater (Pan et al., 2024a) and incorporate an active learning framework (Bai et al., 2024).\nWe acknowledge a potential ordering bias in the comparative evaluation of autonomous and CoPilot modes. We are currently conducting a large-scale study across a diverse demographic to assess and mitigate the impact of such biases."}, {"title": "Societal Impact", "content": "Web agents have significant potential to promote web accessibility and enhance user efficiency. However, their deployment raises important privacy and security concerns. For instance, tracking user actions may expose sensitive information, which could be exploited for malicious purposes (e.g. data theft). Additionally, agents may inadvertently perform harmful or irreversible actions (e.g. confirming financial transactions without explicit user consent). Beyond inadvertent risks, there is also the potential for intentional misuse, where malicious actors could exploit CoWPILOT for unethical purposes. We firmly discourage any such misuse of COWPILOT. To balance accessibility with safety, we have chosen not to open-source our codebase. Instead, we will release the extension publicly through the Chrome Web Store. Users must provide their own API key for supported LLMs, or they can modify the extension to use a different LLM if they prefer, ensuring that their information is not shared with us. Future work can focus on addressing such safety risks and transparency, including developing robust safeguards to prevent unintended actions."}]}