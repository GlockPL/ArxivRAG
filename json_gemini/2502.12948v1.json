{"title": "Fake It Till You Make It: Using Synthetic Data and Domain Knowledge for Improved Text-Based Learning for LGE Detection", "authors": ["Athira J Jacob", "Puneet Sharma", "Daniel Rueckert"], "abstract": "Detection of hyperenhancement from cardiac LGE MRI images is a complex task requiring significant clinical expertise. Although deep learning-based models have shown promising results for the task, they require large amounts of data with fine-grained annotations. Clinical reports generated for cardiac MR studies contain rich, clinically relevant information, including the location, extent and etiology of any scars present. Although recently developed CLIP-based training enables pretraining models with image-text pairs, it requires large amounts of data and further finetuning strategies on downstream tasks. In this study, we use various strategies rooted in domain knowledge to train a model for LGE detection solely using text from clinical reports, on a relatively small clinical cohort of 965 patients. We improve performance through the use of synthetic data augmentation, by systematically creating scar images and associated text. In addition, we standardize the orientation of the images in an anatomy-informed way to enable better alignment of spatial and text features. We also use a captioning loss to enable fine-grained supervision and explore the effect of pretraining of the vision encoder on performance. Finally, ablation studies are carried out to elucidate the contributions of each design component to the overall performance of the model.", "sections": [{"title": "Introduction", "content": "Late Gadolinium Enhancement (LGE) imaging\u2014also referred to as Delayed Enhancement Imaging\u2014plays a critical role in assessing myocardial viability. By highlighting affected areas with increased contrast uptake, it allows non-invasive detection and assessment of myocardial infarction, as well as ischemic and non-ischemic cardiomyopathies and other cardiac pathologies (Jenista et al. 2023). However, detecting areas of hyperenhancement from LGE images (henceforth referred to as LGE or scar detection) is a challenging task. Enhancement can be subtle and is often influenced by spatial and temporal variations across different scanners, sequences, and study protocols. Moreover, image noise, artifacts, and varying LGE patterns add to the difficulty. These complexities make developing robust, generalizable automated solutions for LGE detection particularly challenging. Although deep learning (DL) approaches have achieved impressive performance in a range of medical imaging applications, they are heavily dependent on access to large, good quality, annotated datasets for increased accuracy and generalization. The annotation of LGE data, which requires precise delineation of both myocardial and enhancement regions, is especially complex due to its variability and requires substantial clinical expertise. These demands pose a limitation on large-scale DL training efforts in this area.\nMeanwhile, clinical reports present a source of valuable information about LGE assessment. The reports are created from the cardiac magnetic resonance (CMR) study and contain clinician-written notes about the LGE, including location, extent, etiology, and other impressions. While these could be manually converted to binary labels to train a DL model, that is impractical for large amounts of data. Moreover, translating some of the information into discrete labels could potentially be a non-intuitive task. In such a situation, training directly with text offers an attractive alternative.\nRecently, Contrastive Language Image Pre-training (CLIP) (Radford et al. 2021) has achieved notable success in incorporating text supervision into vision models, for a wide range of downstream tasks in natural image processing, such as classification (Zhou et al. 2022b), object detection(Lin and Gong 2023), and segmentation (Luo et al. 2023). CLIP's approach aligns image and corresponding textual description embeddings within a shared latent space, facilitating a unified representation. This training paradigm has also been applied within medical imaging (Zhao et al. 2023). However, CLIP models often require large datasets often millions of image-text pairs to learn robust associations. Collecting such data, especially in specialized fields like medical imaging, is challenging due to limited availability and high annotation costs. These data demands can limit accessibility. In addition, despite the impressive zero-shot or few-shot performance, fine-tuning with task-specific data is often required to reach state-of-the-art (SoTA) performance. Many studies explore adapting CLIP models to downstream tasks through prompt tuning and linear probing. However, these require further training stages for the model, and is dependent on the size of the fine-tuning dataset.\nIn this study, we train a model for LGE detection on a clinical cohort of 965 patients. We use text supervision from their corresponding clinical reports, without any further fine-tuning with discrete labels. We use domain knowledge to systematically augment the limited dataset with synthetic scar images and text. Moreover, we normalize the orientation"}, {"title": "Related Work", "content": "Vision-Language Training in the medical domain. Many studies have explored building specialized models in the medical domain (Zhang et al. 2022; Wang et al. 2022; Zhang et al. 2024). BiomedCLIP (Zhang et al. 2023b), a vision-language model (VLM) was trained on 15M image-text pairs from the biomedical domain. Adapting these models to downstream tasks has been explored in various ways: a) End to end fine-tuning (Zhang et al. 2022; Ikezogwo et al. 2024), b) Prompt tuning (Zhou et al. 2022c,a), c) Linear probing (Radford et al. 2021; Zhou et al. 2022c; Shakeri et al. 2024). These methods still require additional training stages on a labeled \"support\" dataset for the downstream task, and can underperform in very low data cases, especially with complex tasks. Recently, multi-modal language models (MLLMs) have emerged as generalist models for various tasks (Moor et al. 2023; Singhal et al. 2023; Li et al. 2024). These are generative models capable of generating and processing long-form text. For instance, Med-Flamingo (Moor et al. 2023) is an open-source, multimodal few-shot learner adapted to the medical domain. These models demonstrate impressive performance across a wide variety of tasks, such as visual question answering, classification, report generation, etc., on a wide range of modalities.\nSynthetic Data Generation. Augmentation with synthetic data has the potential to mitigate the issue of limited domain data in medical imaging. However, generating high-quality synthetic medical images remains challenging. Clip-Medfake (Chen et al. 2024) leverages Stable Diffusion (Rombach et al. 2022) to generate synthetic images, which are then used to pretrain a CLIP model before fine-tuning on actual medical data. Latte-CLIP (Cao et al. 2024) uses MLLMs to generate descriptive text for domain-specific images, which are then used for CLIP model fine-tuning. Data synthesis in these methods is done by pre-trained models or models trained from a small portion of the original training data. They lack fine grained control over the synthesis process. CtrlSynth (Cao, Najibi, and Mehta 2024) utilizes pre-trained models to identify concepts within an image, systematically alter these attributes, and generate synthetic text with LLMs. This synthetic text is then converted into images through Stable Diffusion, creating a comprehensive synthetic dataset that supports model training on controlled variations.\nLGE Detection. There is no consensus regarding the optimal method for LGE analyses. Commonly used manual and semi-automatic methods clinically include manual planimetry, the Full Width Half Maximum (FWHM) approach (Amado et al. 2004; Hsu et al. 2006), and n-std from remote myocardium. Comparative studies examining these methods (Flett et al. 2011; Heiberg et al. 2022) reveal significant variability in quantification results, highlighting issues with both reliability and reproducibility.\nRecently, there have been many DL-based studies focused on automated scar detection from cardiac LGE images (Zhang 2021; Kim, Chung, and Choe 2024; Girum et al. 2021; Yang and Wang 2021). For instance, Kim et al. (Kim, Chung, and Choe 2024) leveraged segmental information to train models that identify the presence or absence of LGE by transforming the images into polar coordinates based on the left ventricular (LV) center and the right ventricular (RV) insertion points. Additionally, several studies have explored infarct segmentation on the publicly available EMIDEC dataset (Lalande et al. 2020) containing 150 patients (100 for training), achieving classification accuracies as high as 0.92 (Lalande et al. 2022) and a Dice coefficient of up to 0.71 for infarct segmentation (Zhang 2021). These models are trained on dense pixel-wise annotations of myocardial and scar tissue provided by clinical experts, thereby enhancing their ability to accurately segment scar regions."}, {"title": "Methodology", "content": "Preliminaries: CLIP-based training CLIP training framework consists of a vision encoder and a text encoder to extract features from pairs of images and text, respectively. Each encoder is followed by a set of projection layers to project the features into a common embedding space. More specifically, the input image $X_{img}$ is encoded by the encoder $E_{img}$ into feature vector $f_{img} \\in \\mathbb{R}^{n}$. A projection module $P_{img}$ maps the features into the embedding $v \\in \\mathbb{R}^{P}$.\n$v = P_{img}(E_{img}(X_{img}))$\nSimilarly, the text encoder $E_{txt}$ encodes the input text into feature vector $f_{txt} \\in \\mathbb{R}^{m}$. A projection module $P_{txt}$ maps the features into the embedding $t \\in \\mathbb{R}^{P}$.\n$t = P_{txt}(E_{txt}(X_{txt}))$\nThe similarity score is calculated using dot product as\n$S = u_{n}.t_{n}$\nwhere $u_{n}, t_{n}$ represent L2 normalized vectors. Then, cross-entropy loss (CE) is used to maximize the similarity score within the same pair, and minimize the same across pairs (Radford et al. 2021).\nPreliminaries: Related tasks on LGE images. Prior to this study, we trained DL networks to segment the myocardium and detect anterior and posterior RV Insertion Points on LGE MRI Images. Ground truth (GT) annotations for these tasks are relatively easy to create. The myocardium segmentation network consisted of a UNet architecture, with DenseNet121 (Huang et al. 2017) encoder. It was trained on"}, {"title": "Proposed Method", "content": "We train a model to detect myocardial hyperenhancement from LGE images using relevant text from the clinical reports (Figure 1). Due to the limited dataset size and the long-tailed distribution of LGE etiologies, we use domain knowledge to systematically augment the training data with synthetic image-text pairs. During training, the image-text pairs are aligned using global CLIP loss and a local caption loss. The image encoder is initialized with the weights from the myocardial segmentation network described in the previous section. Each of these parts is explained in detail in the following sections. During inference, we query the model using the following text: there is hyperenhancement in the myocardium and there is no hyperenhancement in the myocardium, denoting the positive and negative LGE classes respectively.\nSynthetic Data Generation\nWe add synthetic scars to real LGE images and create associated text descriptions (Figure 2a). This allows us to augment the limited data, and systematically cover a wide variety of scar distributions. The scar is applied only to images with no prior LGE (as determined from the clinical report). A Controller module randomly chooses from a set of scar parameters. Then, a synthetic scar is added to the image, and text is created with these parameters. This is done at every training iteration, with a probability of $\\lambda$.\nController. Wall location is randomly chosen as one of [\"anterior\", \"inferior\", \"posterior\"], or [\"lateral\", \"septal\"], or a combination of the words chosen from the two sets"}, {"title": "Image Generation", "content": "For every negative LGE image (Figure 2a.i) at a given slice location, synthetic scars are added as follows:\n1.  Myocardial mask and anterior RVIP are determined using the previously trained DL networks (Figure 2a.ii).\n2.  Anterior RVIP is used to divide the myocardium into AHA segments: four if apical layer, or six segments if basal or mid (Figure 2a.iii).\n3.  The myocardial mask is divided equally into 3 concentric sections to represent endocardial, mid-myocardial and epicardial layers (Figure 2a.iv).\n4.  Wall location (chosen by the controller), along with slice location, is translated into AHA segments through hard-coded values. For eg, inferoseptal on basal level denotes segment 3.\n5.  A scar candidate region mask is created using an intersection between the identified AHA segments (Step 4) and chosen scar extent (by the controller). This is the \"allowed\" region for scar creation (Figure 2a.v).\n6.  Synthetic scar is created in the candidate region mask as a randomly placed, oriented and sized ellipse (Figure 2a.vi). A random pixel is chosen from the candidate region as the center of the synthetic scar. The radii of the ellipse are chosen randomly between set minimum and maximum values. The minimum and maximum values are determined as a fraction of the myocardial thickness at that point, depending on whether the scar is chosen to be transmural or within specific myocardial layers. The created ellipse is smoothed with a Gaussian filter for a more natural and continuous appearance.\n$r_{min} = max(0.01, t_{h}*p_{min})$\n$T_{max} = t_{h}*p_{max}$\n$r = rand(r_{min}, r_{max}, 2)$\n$\\alpha = rand(0, \\pi)$\n$\\sigma = rand(0, 1)*81 + \\sigma_{2}$\nwhere, $t_{h}$ represents myocardial thickness at that point, $0 < p_{min}, p_{max} <= 1$ are hyperparameters representing ratios relative to myocardial thickness, $r \\in \\mathbb{R}^{2}$ are the radii of the major and minor axes of the ellipse, $\\alpha$ represents the orientation of the major axis of the ellipse relative to the positive x-axis, and $\\sigma$ represents the standard deviation of the Gaussian kernel used for smoothing. The created scar M is min-max normalized to the range of [0, 1].\n7.  The scar image M is then blended with the image I as,\n$I_{synth} = I * (1 \u2013 M) + \\gamma * max(I) * M$\n$\\gamma = rand(b_{1}, b_{2})$\nwhere $\\gamma$ controls the brightness of the scar, and is randomly chosen between preset minimum and maximum values $b_{1}, b_{2}$ (Figure 2a.vii)."}, {"title": "Text Generation", "content": "Given the chosen scar parameters (slice location, wall location, wall extent) from the controller module, the associated text description is synthesized using preset templates such as the following:\n\"there is  delayed enhancement in  wall. This image is from  level.\"\nor variations of this, where the words within <> are replaced with chosen scar parameters. The slice location of the input image is appended to stay consistent with real clinical text and contextualize the spatial location of the slice for the model (further explained in the Section \"Implementation Details: Text Encoder\"). To add further variation to the text, the words \"delayed enhancement\", \"delayed hyperenhancement\", \"late enhancement\", \"scar\", \"infarct\" are used interchangeably.\nExamples of images with synthetically generated scar and corresponding text are shown in Figure 2b."}, {"title": "Normalization of the LV orientation", "content": "Clinical descriptions of LGE use words that describe location relative to the orientation of the LV, defined by the RV insertion point. The orientation of the LV can vary across patients, and even across images within the same patient. While this could potentially be learnt implicitly from a large cohort of image-caption pairs, this could prove to be a difficult challenge in a dataset of limited size. To help the model better associate position descriptors with image features, we standardize the orientation of the LV using the anterior and inferior RVIPs (Figure 3). The RVIPs for each image are obtained from the landmark detection model described previously. Using the two insertion points, each image is rotated to position the line connecting them along the vertical axis of the image, with anterior RVIP being on the top."}, {"title": "Caption loss", "content": "CLIP loss aligns image and text embeddings on a global level. However, LGE descriptions from clinical reports are information-dense, with multiple words providing critical information about the location, extent and etiology of the scar. To encourage granular supervision on the level of the individual text tokens, we use a captioning loss similar to contrastive captioner models (Yu et al. 2022). A multi-modal decoder is applied to the text tokens, consisting of layers of multi-headed, self-attention layers, followed by cross-attention layers attending to features from the vision encoder. The final layer is the classification layer that predicts the distribution of the next token over the supported vocabulary set."}, {"title": "Task specific encoder", "content": "We initialize the vision encoder with the weights of LGE myocardium segmentation model trained as described previously. Segmenting LGE myocardium is a closely related task to LGE detection, and is hypothesized to aid convergence. We later study the effect of this design choice by conducting ablation studies with various other image encoders."}, {"title": "Experiments", "content": "Data\nThe data consists of 965 patients with cardiac MRI studies and clinical reports from a single center, of which 404 patients have reported LGE. The scans were performed on 1.5 T magnets (MAGNETOM Avanto, Siemens Health-care, Erlangen, Germany) using a T1-weighted, phase sensitive inversion-recovery (PSIR), gradient-echo sequence, and were acquired 10 min after injection of a gadolinium-based contrast agent. The acquisition parameters are as follows, TR/TE: 2.4/1.1 ms; flip angle: 50, slice thickness: 8 mm; in plane resolution between 1.5 \u00d7 1.5 6 mm\u00b2 and 2.6 x 2.6 mm\u00b2. The patients were divided in train, validation and testing splits of 772, 91 and 102 patients, respectively, while maintaining class distribution (LGE presence/absence)."}, {"title": "Implementation Details", "content": "Vision Encoder. The vision encoder consists of a Densenet121 (Huang et al. 2017) encoder with UNet decoder, with 5 downsampling layers. A MaxPool layer is added after the last layer of the DenseNet encoder to get a feature vector size of n = 1024. The projection module consists of two Linear layers, separated by GelU nonlinearity(Hendrycks and Gimpel 2016) and followed by Dropout and LayerNorm (Ba 2016).\nFor every patient, we select the segmented PSIR DICOM series (Muehlberg et al. 2018) using information in their DICOM tags, as this sequence theoretically has the higher spatial resolution required for LGE detection. In this dataset, these typically consist of 3 slices, covering apical, mid and basal regions of the heart. Each image is preprocessed according to the following steps: a) resizing to 1mm \u00d7 1mm resolution, b) cropping to 112 \u00d7 112 dimension, centered around the LV. LV mask is obtained from the LGE segmentation network described previously, d) Upsampling 2\u00d7 to 224 \u00d7 224 e) capping intensities at the 98 percentile and f) normalizing to the range of [0,1].\nText Encoder. We use the publicly available Biomed-BERT (Zhang et al. 2023a). Feature vector has size m = 768. The text encoder is held frozen for all experiments in this study. The projection module has the same architecture as described in the previous section and is optimized end-to-end during training.\nFor each patient, text relevant to LGE imaging is extracted from the respective clinical report, from both the \"Findings\" and the \"Impressions\" sections, using a simple keyword search. The extracted text is split into individual sentences (henceforth also referred to as captions), from which one is sampled randomly for every iteration of training. However, this approach introduces an issue: the clinical text annotations are provided at the patient level, whereas the corresponding images represent specific heart sub-regions. To address this, we implement a straightforward solution by appending the phrase \"This image is from  level\" to each input text, where  is replaced with basal, mid or apical, as per the image location. This is done consistently during both training and inference to help the model contextualize the image within the anatomical structure. While we limit our method here to three pre-selected slices for simplicity, this framework is adaptable to any number of images by adjusting the corresponding section tag.\nScar augmentation parameters. In all experiments, we use $\\lambda$ = 0.7, [$p_{min}, p_{max}$] = [0.1, 0.4], [0.3, 0.6], [0.7, 0.1] for single-layer, two-layer, and transmural extents respectively, $\\sigma_{1}$ = $\\sigma_{2}$ = 2, b\u2081 = 0.8, b2 = 1. These were selected empirically to ensure image fidelity, anatomical relevance, and diversity across generated outputs."}, {"title": "Baselines and Metrics", "content": "We compare the proposed method against the following:\na) BiomedCLIP (Zhang et al. 2023b): We test the model on the same test set, using the same query text: there is"}, {"title": "Results", "content": "Table 1 shows the quantitative results. The proposed method obtains a balanced accuracy of 0.83, outperforming the baselines. Both the publicly available medical VLMs (Biomed-CLIP and MedFlamingo) encounter limitations on this task. These challenges likely stem from two key factors: (a) the VLMs were trained on a diverse array of medical imaging data, with cardiac MR constituting only a small subset, and LGE sequences representing an even smaller fraction of this subset; (b) LGE detection is an inherently challenging task that necessitates specialized clinical domain knowledge and the ability to analyze subtle, fine-grained features within highly localized regions of the images. The image-only classifier trained on this dataset demonstrates higher performance but lags behind the proposed method by 6 pp."}, {"title": "Ablation Studies", "content": "Table 2 illustrates the impact of omitting different components of the proposed method. Among the components, synthetic scar augmentation has the most significant impact on performance, followed by the normalization of LV orientation, and lastly, the caption loss.\nTable 3 presents the impact of different initialization strategies for the vision encoder. As previously described, the proposed model uses a vision encoder pre-trained on the related task of myocardium segmentation in LGE images. Here, we explore two alternative initializations:\na) Task agnostic encoder: This is a unimodal foundation model pre-trained on 36 million CMR images (Jacob et al. 2024). It was trained in a self-supervised manner, without any labelled data, hence is agnostic to any specific task. The model consisted of a ViT-S architecture, and was pretrained across many diverse sequences of CMR, such as cine, LGE, and mapping.\nb) Imagenet pretrained encoder: This uses the same DenseNet-UNet architecture of the proposed model, but with the publicly available ImageNet trained weights.\nBoth initialization choices provide practical alternatives for when training data and/or labels for a directly related task is unavailable. Our results indicate that the task-agnostic CMR foundation model (FM), pre-trained in a self-supervised manner, outperforms the ImageNet pre-trained model; however, it still falls short compared to the model trained on a closely related task."}, {"title": "Qualitative Results", "content": "Fig 4 visualizes the results for 3 patients. Patient 1 shows a true positive detection. Note that though the GT text describes LGE presence across the whole heart, the model is able to produce predictions for individual slices, which aids interpretability. Patient 2 presents a false positive, with the network predicting the LGE presence in apical and mid-ventricular slices. This could be because of the presence of streak artifacts in these two images, degrading image quality and possibly confounding the model. Patient 3 presents a false negative case, with no LGE detected by the model, despite the GT indicating LGE in basal slices. Visual inspection did not reveal LGE in these slices; however, further review of other LGE images in the study confirmed the presence of basal scarring at the level of the LV outflow tract. This highlights a method limitation, as selecting only three input images may omit critical details, reducing the model's efficacy. This is also observed in Patient 1, where the selected images do not reflect all of the described scar."}, {"title": "Conclusions", "content": "Text-based training using clinical reports offers an alternative to obtaining hard labels in the clinical domain, which might be expensive and challenging. However, typically used methods such as CLIP, require large amounts of pre-training data, and further finetuning stages for downstream tasks. We present a method that incorporates domain knowledge to enable CLIP based training for small datasets. We create synthetic image-text pairs to augment the training set, use anatomical information to normalize the orientation of the image, use additional caption loss to enable fine-grained supervision and use related-task pretraining to improve the accuracy for the task. We demonstrate the feasibility of text-based training for specific tasks on small datasets, followed by zero-shot inference without any further finetuning stages.\nDisclaimer. The concepts and information presented in this paper/presentation are based on research results that are not commercially available. Future commercial availability cannot be guaranteed."}]}