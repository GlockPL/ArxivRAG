{"title": "Disentangled Representation Learning for Causal Inference with Instruments", "authors": ["Debo Cheng", "Jiuyong Li", "Lin Liu", "Ziqi Xu", "Weijia Zhang", "Jixue Liu", "Thuc Duy Le"], "abstract": "Latent confounders are a fundamental challenge for inferring causal effects from observational data. The instrumental variable (IV) approach is a practical way to address this challenge. Existing IV based estimators need a known IV or other strong assumptions, such as the existence of two or more IVs in the system, which limits the application of the IV approach. In this paper, we consider a relaxed requirement, which assumes there is an IV proxy in the system without knowing which variable is the proxy. We propose a Variational AutoEncoder (VAE) based disentangled representation learning method to learn an IV representation from a dataset with latent confounders and then utilise the IV representation to obtain an unbiased estimation of the causal effect from the data. Extensive experiments on synthetic and real-world data have demonstrated that the proposed algorithm outperforms the existing IV based estimators and VAE-based estimators.", "sections": [{"title": "I. INTRODUCTION", "content": "ESTIMATING the causal effect of a treatment (a.k.a. intervention, or exposure) on an outcome is a fundamental task in many areas [1], [2], such as policy-making and new drug evaluation, etc. Randomised controlled trials (RCTs) are the gold standard for inferring causal effects, but they are often impractical in real-world applications due to time or ethical constraints [3]. Thus, causal effect estimation with observational data has become an alternative to RCTs.\nHowever, estimating causal effects using observational data suffers from confounding bias, due to the spurious association caused by confounders that affect both the treatment and outcome variables. Unmeasured confounders make the situation even worse [4]\u2013[6]. As shown in Fig. 1 (a), if there is an unmeasured confounder (U) between the treatment (W) and the outcome (Y), the causal effect of W on Y cannot be estimated with observational data except there is an instrumental variable [1], [7], [8].\nThe instrumental variable (IV) approach is a commonly used way to estimate causal effects from data when the unconfoundedness assumption is violated [1], [9]. A valid IV (denoted as Z) must satisfy the following three conditions [7], [10], [11]: (i) Z influences the treatment (i.e. relevance condition), (ii) the causal effect of Z on Y is only through W (a.k.a. exclusion restriction), and (iii) Z and Y do not have any common causes (i.e. unconfounded instrument). The three conditions of a valid IV can only be verified using domain knowledge or based on the underlying causal graph of the system but not from data [12]. It is known that domain knowledge of an IV or a causal graph is rarely available in many real applications [7]. Therefore, it is desirable to explore an effective data-driven method to discover a valid IV directly from data.\nA few data-driven IV based methods have been proposed for causal effect estimation without assuming a known IV, but they often have other constraints. For example, IV.Tetrad [13] requires that at least a pair of IVs exist in the system and the set of all the remaining variables excluding the pair of IVs is a conditioning set of the IV with respect to the treatment and the outcome. sisVIVE [14] requires that at least half of the variables (the set of candidate IVs) are valid IVs.\nSome research has proposed the necessary conditions of IVs for obtaining a bound estimation of a causal effect, i.e. a multiset of possible estimates from data, instead of a unique estimate. For instance, Pearl [12] proposed an instrumental inequality to find a set of candidate IVs from data with discrete variables, and Kuroki and Cai extended the instrumental inequality to linear structural equation models with continuous variables [15]. Assuming a linear non-Gaussian acyclic causal model, Xie et al. [16] proposed a necessary condition based on a generalised independent noise condition for identifying continuous variables as the candidate valid IVs, but the condition only produces a bound estimation.\nTherefore, the existing data-driven IV methods either rely on strong assumptions, or only provide a necessary condition for determining candidate IVs. In order to develop a more"}, {"title": "II. PRELIMINARIES", "content": "A. Notations\nWe represent variables and their values with uppercase and lowercase letters, respectively. A set of variables and a value assignment of the set are denoted by bold-faced uppercase and lowercase letters, respectively.\nLet G=(V,E) be a directed acyclic graph (DAG) where V={V1,..., Vp} are the set of nodes representing p random variables and E \u2286 V \u00d7 V are the set of edges representing the relationships between nodes. In DAG G, two nodes are adjacent when there exists a directed edge \u2192 between them. A path \u03c0 from Vi to Vj is a directed or causal path if all edges along it are directed towards Vj. If there is a directed path \u03c0 from Vi to Vi, Vi is known as an ancestor of Vj and Vj is a descendant of Vi. The sets of ancestors and descendants of a node V are denoted as An(V) and De(V), respectively.\nA DAG is causal if the directed edge Vi\u2192 Vj between Vi and Vj indicates that Vi is a direct cause of Vj. In a DAG G, a path between Vi and Vj comprises a sequence of distinct nodes (Vi,..., V\u2081) with every pair of successive nodes being adjacent, and V\u2081 and Vj are end nodes of \u03c0.\nThe definitions of Markov property and faithfulness are introduced in the following.\nDefinition 1 (Markov property [1]). Given a DAG G = (V, E) and the joint probability distribution of V (P(V)), G satisfies the Markov property if for Vi \u2208 V, Vi is probabilistically independent of all of its non-descendants, given the parent nodes of Vi.\nDefinition 2 (Faithfulness [4]). A DAG G = (V, E) is faithful to a joint distribution P(V) over the set of variables V if and only if every independence present in P(V) is entailed by G and satisfies the Markov property. A joint distribution P(V) over the set of variables V is faithful to the DAG G if and only if the DAG G is faithful to the joint distribution P(V).\nWhen the faithfulness assumption is satisfied between a joint distribution P(V) and a DAG of a set of variables V, the dependency/independency relations among the variables can be read from the DAG [1], [4]. In a DAG, d-separation is a well-known graphical criterion that is used to read off the conditional independence between variables entailed in the DAG when the Markov property and faithfulness are satisfied [1], [4].\nDefinition 3 (d-separation [1]). A path \u03c0 in a DAG G = (V, E) is said to be d-separated (or blocked) by a set of nodes M if and only if (i) \u03c0 contains a chain Vi \u2192 Vk \u2192 Vj or a fork Vi\u2190 Vk \u2192 Vj such that the middle node Vk is in M, or (ii) \u03c0 contains a collider Vk such that Vk is not in M and no descendant of Vk is in M. A set M is said to d-separate Vi from Vj (Vi|| Vj | M) if and only if M blocks every path between Vi to Vj. Otherwise they are said to be d-connected by M, denoted as Vi | Vj | \u041c.\nThe back-door criterion is a well-known graphical criterion for determining an adjustment set in a given DAG G. The back-door criterion can be used directly to find an adjustment set M \u2286 X relative to an ordered pair of variables (Vi, V\u2081) in the given G.\nDefinition 4 (Back-door criterion [1]). In a DAG G = (V, E), for an ordered pair of variables (Vi, Vj) \u2208 V, a set of variables M \u2286 V \\ {Vi, Vj} is said to satisfy the back-door criterion in the given DAG G if (i) M does not contain a descendant node of Vi; and (ii) M blocks every back-door path between Vi and Vj (the paths between Vi and Vj starting with an arrow into Vi). A set M is referred to as a back-door set relative to (Vi,Vj) in G if M satisfies the back-door criterion relative to (Vi, Vj) in G.\nB. Instrumental variables\nWe follow the convention and definitions of IVs used in [7], [13], [21]. We assume a causal DAG G with the set of variables V = XUU\u222a {W, Y}, where W is a binary treatment indicator (w = 1 for being treated and w = 0 for control), Y the outcome of interest, X a set of pretreatment variables\u00b9, i.e. \u2200X \u2208 X, X \u2209 De(WUY) where De(WUY) is a shorthand of De(W) U De(Y), and U the set of latent confounders. The goal of this work is to estimate the average causal effect of W on Y from observational data with latent confounders."}, {"title": "III. THE PROPOSED DIV. VAE METHOD", "content": "A. The proposed disentanglement scheme\nFollowing the literature [19], [20], [27], we propose the causal structure in Fig. 3 to represent the causal relationships among W, Y, U', X, Z and C, where the set X is generated from the set of latent variables \u03a6 = (Z, C), where Z is the latent IV representation, and C captures the remaining information in X.\nWe first show that the proposed disentanglement scheme is able to estimate causal effect of W on Y in presenting the following theorem.\nTheorem 1. Given a joint distribution P(X, W, Y) generated from a causal DAG G=(XUUU{W, Y}, E). G contains W \u2192 Y and W \u2190 U' \u2192 Y in G, and\u2200X \u2208 X, X \u00a2 De(WUY) in G. There exists at least one SIV (i.e. a set of SIVs S C X with |S| \u2265 1). If we learn and disentangle simultaneously the latent representation I of X into two disjoint sets (Z, C), where Z is a common cause of W and X, and C is a common cause of X, W, and Y, respectively, then Z is a valid IV for estimating the causal effect of W on Y from data of P(X, W, Y).\nProof. We prove that the IV representation Z is a valid IV based on the disentanglement causal model. First of all, the clause S \u2265 1 is to ensure that there is information of valid IVs in the set of covariates X. In the causal DAG, (1) Z is a set of causes of W, so Z satisfies the first condition (i) of an IV as described in Introduction; (2) there is only a causal path from Z to Y, i.e. Z \u2192 W \u2192 Y, so Z affects Y only through W, i.e. Z satisfies the second condition (ii) of an IV; (3) there are four back-door paths from Z to Y, i.e. Z \u2192 X \u2190 C \u2192 Y, Z \u2190 X \u2190 C \u2192 W \u2190 U \u2192 Y, Z \u2192 W \u2190 C \u2192 Y and Z \u2192 W \u2190 U \u2192 Y, and all four back-door paths are blocked by according to the back-door criterion, i.e. Z does not share common causes with Y, so Z satisfies the last condition of an IV. Therefore, Z is a set of valid IVs for estimating the causal effect of Won Y from data with latent confounders.\nTheorem 1 states that the soundness of the proposed disentangled representation learning method relies on the ability to learn correct representations. The conditional clause 'If we learn and disentangle simultaneously the latent representation \u03a6 of X into two disjoint sets (Z, C)' in the theorem is an assumption that is unfortunately untestable in data. Such an assumption is used in previous VAE-based causal inference works [19], [20], [27]. Once Z is correctly learned, the causal effect of W on Y can be unbiasedly estimated from data with latent confounders. In real-world applications, U may affect the learned representation of C, but C is not used in the causal effect estimation and hence the uncertainty in C does not affect the unbiasedness of causal effect W on Y.\nThe establishment of Theorem 1 relies on the correctness of the disentanglement \u03a6 = (Z, C). In this work, we leverage variational autoencoders (VAEs) to optimise a variational lower bound on likelihood, enabling the learning of \u03a6. This approach requires substantially weaker assumptions about the data generating process and the latent variable structure as in [27]\u2013[29]. In the next section, we will introduce our proposed DIV.VAE for learning and disentangling \u03a6 into two disjoint sets, (Z, C).\nB. Finding IV representation by disentangled representation learning\nIn this section, we introduce the details of the proposed VAE-based disentangled representation learning architecture of DIV.VAE (shown in Fig. 4) to learn a valid IV representation Z following the proposed scheme in Fig. 3. Then we can use the learned IV representation to obtain unbiased causal effect estimation from data with latent confounders.\nThe goal of our designed architecture for DIV.VAE is to learn the latent representation of X and disentangle \u03a6 into (Z, C) simultaneously, following the proposed causal structure in Fig. 3. It is worth noting that C plays a critical role as a set of auxiliary variables used to capture the information from the set of X \\ {S} in the representation learning and disentanglement process but it is not used for the causal effect estimation.\nThe proposed DIV.VAE architecture in Fig. 4 uses the inference model and the generation model to approximate the posterior $p_{\\theta, \\epsilon}(X | Z, C)$ where @ is a set of generative model parameters. For the inference model, we develop two separate encoders $q_{\\phi_Z}(Z | X)$ and $q_{\\phi_C}(C | X)$ that serve as variational posteriors over the latent variables. For the generative model, the two latent representations (Z, C) are obtained from the two separate encoders used by a single decoder $p_{\\theta, \\epsilon}(X | Z, C)$ to reconstruct X. Following the VAE literature [28], [29], the prior distributions of P(Z) and P(C) are drawn from Gaussian distributions.\nIn the inference model, the variational approximations of the posteriors are defined as:\n$q_{\\phi_Z}(Z | X) = \\prod_{i=1}^{Z} N(\\mu = \\mu_{Z_i}, \\sigma^2 = \\hat{\\sigma}^2_{Z_i});$\n$q_{\\phi_C}(C | X) = \\prod_{i=1}^{C} N(\\mu = \\mu_{C_i}, \\sigma^2 = \\hat{\\sigma}^2_{C_i}),$\nwhere $\\mu_{Z_i}, \\mu_{C_i}$ and $\\hat{\\sigma}^2_{Z_i}, \\hat{\\sigma}^2_{C_i}$ are the means and variances of the Gaussian distributions parameterised by neural networks. Note that, since one IV is sufficient for obtaining unbiased causal effect estimation, we use |Z| = 1 in the experiments on real-world datasets. However, in the algorithm design, we keep Z as multi-dimensional for a general solution.\nThe prior distributions of (Z, C) are defined as:\n$P(Z) = \\prod_{i=1}^{Z} N(Z_i | 0, 1); P(C) = \\prod_{i=1}^{C} N(C_i | 0, 1).$\nThe generative models for W and X are defined as:\n$P_{\\theta_W}(W | Z, C) = Bern(\\sigma(g_1(Z, C)));$\n$P_{\\theta_X}(X | Z, C) = \\prod_{i=1}^{X} P(X_i | Z, C),$\nwhere P(Xi | Z, C) is the distribution for the ith measured variable, g1(\u00b7) is the function parameterised by neural net- works and \u03c3(\u00b7) is the logistic function.\nThe generative model for Y depends on the data type of Y. For continuous Y, we sample it from a Gaussian distribution with its mean and variance given by the mutually exclusive neural networks that define P(Y | W = 0, Z, C) and P(Y | W = 1, Z, C) respectively, and the generative model of Y is defined as:\n$P(Y | W, C) = N(\\mu = \\mu_Y, \\sigma^2 = \\hat{\\sigma}^2_Y);$\n$\\mu_Y = W \\cdot g_2(C) + (1 - W) \\cdot g_3(C);$\n$\\hat{\\sigma}^2 = W \\cdot g_4(C) + (1 - W) \\cdot g_5(C),$\nwhere g2(.), g3(.), g4(\u00b7) and g5(.) are the functions parame- terised by neural networks. For binary Y, we parameterise it with a Bernoulli distribution and the model is defined as:\n$p_{\\theta_Y}(Y | W, C) = Bern(\\sigma(g_6(W, C))),$\nwhere g6(.) is a neural network with its own parameters. Given the joint distribution P(X, W, Y), the parameters can be optimised by maximising the evidence lower bound (ELBO) M [28]:\n$M = E_{q_{\\phi_Z}q_{\\phi_C}} [log p_{\\theta_X}(X | Z, C)]$\n$- D_{KL} [q_{\\phi_Z}(Z | X) || P(Z)]$\n$- D_{KL} [q_{\\phi_C}(C | X) || P(C)],$\nwhere $D_{KL}[\\cdot||\\cdot]$ is a KL divergence term.\nTo learn the latent IV representation Z from the set of SIVs S and the latent representation C from the remaining variables X\\{S}, we add two auxiliary predictors to the above variational ELBO to ensure that the treatment variable W and the outcome variable Y can be estimated from Z and C as designed. Thus, we have the following objective function:\n$L' = - M + \\alpha_W E_{q_{\\phi_Z}q_{\\phi_C}} [log q_{\\phi_W}(W | Z, C)]$\n$+ \\alpha_Y E_{q_{\\phi_C}} [log q_{\\phi_Y}(Y | W, C)],$\nwhere \u03b1w and \u03b1y are the weights for the two auxiliary predictors.\nIn practice, there may be some very weak associations between Z and C. To encourage Z || C as specified in Fig. 3, we employ the orthogonality promoting regularisation (OPR) [30] for our proposed DIV.VAE:\n$L = L' + \\frac{c}{b} \\sum_{i=1}^{b} CS(Z_i, C_i),$\nwhere b is the batch size of the neural network, and $CS(Z_i, C_i) = \\frac{Z_i C_i}{Z_i^2 C_i^2}$ is the cosine similarity (CS).\nAfter training DIV.VAE, we draw Z from the model and feed it into the function of Ortho.IV [22], [23] for calculating \u03b2wy. When the learned distribution of Z is close to the true unmeasured IV distribution, the DIV.VAE method has the ability to obtain an unbiased estimate \u03b2wy as shown in the experimental results. Notably, the main advantage of our DIV.VAE is that it no longer requires domain knowledge or experts to provide a valid IV. Instead, it only requires the presence of a SIV in the data. This is a weaker assumption compared to those required by other methods such as TSLS, FIVR, DeepIV, and IV.Tetrad.\nLimitations. The soundness of DIV.VAE relies on the ability of the proposed VAE architecture to learn \u03a6 and disentangle the latent variable \u03a6 into (Z, C). However, VAE-based methods are susceptible to the problem of unidentifiability in the VAE model [31], [32]. In other words, there is no theoretical guarantee that the learned IV representation Z can always approximate the true latent IV. Fortunately, as shown in our experiments, in the presence of SIV, the learned IV representation Z by DIV.VAE closely approximates the true latent IV. We note that iVAE gives an identifiability guarantee but with more limitations [32]. iVAE assumes injective and linear relationships and a latent presentation learned by iVAE needs its child variable and parent variables to be observed. These additional requirements limit the application of a method based on iVAE and make it infeasible for iVAE to recover the IV representation from the error term of SIV. VAE does not guarantee the identifiability but has some advantages, such as not requiring linear and injective relationships or Z's parent to be observed for its representation learning. However, users should review their results by DIV.VAE with domain knowledge and perform sensitivity analyses [8] before taking the results."}, {"title": "IV. EXPERIMENTS", "content": "A. Experiment setup\nBaseline causal effect estimators. We compare DIV.VAE with four representative IV based estimators and two VAE- based causal effect estimators. Three of the IV based estimators", "9": "causal random forest for IV regression (FIVR) [33", "34": "each requires a given IV; whereas the other IV based estimator", "13": "does not require a given IV", "27": "and treatment effect by disentangled variational autoencoder (TEDVAE) [20", "35": "pyro [36", "37": ".", "11": ".", "33": ".", "36": "and TEDVAE is obtained from the authors' GitHub 4.\nIn our experiments", "38": ".", "sizes": 0.5, "follows": "Z \\sim N(0", "U_2}": {"estiamtion": "nThe estimation biases of all estimators on synthetic datasets with Ylinear and synthetic datasets Ynonlinear are visualised with boxplots in Fig. 6 and Fig. 7", "observations": 1, "Z": "We ex- amine the quality of the learned representation Z by visualis- ing the probability density functions (PDFs) of the ground truth IV and the learned IV representation Z. We use the learned IV representation Z from the data with Ylinear for the visualisation in Fig. 8. We have the following observations from Fig. 8: (1) the learned IV representation Z approximates the ground truth PDF very well even when the sample size is small. (2) As the sample size increases", "39": "and Schooling Returns [40", "41": "."}, "39": ".", "variables": "age"}, {"11": ".", "39": "."}, {"39": "as the reference causal effect.\nSchoolreturning. The data is from the national longitudinal survey of youth (NLSY)", "40": "."}, {"40": "used geographical proximity to a college", "42": "as the reference causal effect.\nSachs. This data is collected from cell activity measure- ments for single cell data under a variety of conditions [41", "13": "we focus on a single condition", "41": "."}, {"41": ".", "13": "i.e. IV.Tetrad's estimated value) as the reference causal effect.\nResults. From the results in Table I", "sizes": 0.5, "8": ".", "observations": 1, "43": "as done in the literature [44", "45": ".", "follows": "U' ~ N(0,0.05), where N(,) denotes the normal distribution. The treatment assignment W is generated from n (where n is the sample size) Bernoulli trials using the assignment probability based on variables {Z, C} where Z represents the tens digit of a two-digit handwritten number (i.e., IV), and C represents the ones digit of a two-digit handwritten number and latent variables {U'} as $P(W = 1 | Z, C, U') = [1 + exp{2 - U' - Z - C}"}]}