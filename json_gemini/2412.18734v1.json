{"title": "Predicting Time Series of Networked Dynamical Systems without Knowing Topology", "authors": ["Yanna Ding", "Zijie Huang", "Malik Magdon-Ismail", "Jianxi Gao"], "abstract": "Many real-world complex systems, such as epidemic spreading networks and ecosystems, can be modeled as networked dynamical systems that produce multivariate time series. Learning the intrinsic dynamics from observational data is pivotal for forecasting system behaviors and making informed decisions. However, existing methods for modeling networked time series often assume known topologies, whereas real-world networks are typically incomplete or inaccurate, with missing or spurious links that hinder precise predictions. Moreover, while networked time series often originate from diverse topologies, the ability of models to generalize across topologies has not been systematically evaluated. To address these gaps, we propose a novel framework for learning network dynamics directly from observed time-series data, when prior knowledge of graph topology or governing dynamical equations is absent. Our approach leverages continuous graph neural networks with an attention mechanism to construct a latent topology, enabling accurate reconstruction of future trajectories for network states. Extensive experiments on real and synthetic networks demonstrate that our model not only captures dynamics effectively without topology knowledge but also generalizes to unseen time series originating from diverse topologies.", "sections": [{"title": "I. INTRODUCTION", "content": "Dynamical network systems are pervasive across various domains, including epidemic spreading networks, where nodal states reflect the number of infected individuals (Pastor-Satorras and Vespignani, 2001), population flow networks capturing geospatial population dynamics (Gardiner et al., 1985), and gene regulatory networks modeling the interactions between genes and their expression products (Alon, 2006). Learning dynamical systems is essential for solving various problems, particularly in predicting future system states (Gao and Yan, 2022; Wu et al., 2023; Huang et al., 2024; Jiang et al., 2023), but also in controlling networks and analyzing transitions in behavior (Liu et al., 2011; Morone et al., 2019). In this paper, we focus on the reverse engineering of networked dynamical systems from time-series data, with the goal of accurate time-series prediction.\nExisting approaches to modeling complex system dynamics often assume access to the true network topology. However, in practice, the ground truth network structures are often concealed, and observed structures may contain missing or erroneous links (Von Mering et al., 2002; Guimer\u00e0 and Sales-Pardo, 2009; Timme and Casadiego, 2014; Wang et al., 2016). For instance, when modeling the spread of diseases, one might consider administrative divisions like states as nodes and construct the network based on factors such as population movements or airline traffic. However, the measurement of these movements may contain inaccuracies (Barbosa et al., 2018), and the assumptions used to construct the network might not fully capture the complexities of the actual topology. Consequently, developing methods to predict future network trajectories without explicit knowledge of network topology has become a key research area (Prasse and Van Mieghem, 2022).\nAnother underexplored aspect pertains to the generalizability (Yehudai et al., 2021) of models to unseen topologies in the context of networked time-series forecasting. Existing works either train and test models on the same networked time series, referred to as the transductive setting (Huang et al., 2020; Zang and Wang, 2020; Tran et al., 2021), or on different sets of networked time series, known as the inductive setting (Kipf et al., 2018; Huang et al., 2020; Yin et al., 2021). Methods in the transductive setting often overlook the variability in network structures that arises in some real-world systems, where networks representing the same underlying dynamics-such as a gene cohort at different developmental stages-may differ in topology (Alon, 2006). Although methods in the inductive setting consider different topologies, they lack systematic evaluation across diverse network types, such as random, scale-free, or community networks, particularly under out-of-distribution (OOD) conditions involving unseen topologies. Addressing these gaps is critical for understanding the robustness and adaptability of networked time-series forecasting models in diverse and evolving network environments.\nWe propose a novel neural network model to address the challenges of networked time-series forecasting in the absence of prior topology information. Our model extracts relational patterns among network components from an initial short period of observed time-series data and employs neural Ordinary Differential Equations (ODEs) (Chen et al., 2018) to predict nodal trajectories at future timestamps. Each node is mapped to a latent representation, allowing the neural ODE to model the evolution of these hidden embeddings over time. By effectively modeling the relationships between network components through latent topologies that potentially evolve over time, our approach captures the underlying network dynamics and flexibly adapts to both static systems with fixed topologies and dynamic environments where network structures change chronologically."}, {"title": "II. RELATED WORK", "content": "Networked Time-Series Prediction. The goal of network time-series prediction is to forecast nodal states over time. Symbolic regression approaches (Bongard and Lipson, 2007; Schmidt and Lipson, 2009; Brunton et al., 2016; Gao and Yan, 2022) aim to uncover explicit dynamical equations from observed data. These methods often require predefined functional forms and struggle to generalize to complex, unknown systems. Graph-based neural ODE models, such as LGODE (Huang et al., 2020), CGODE (Huang et al., 2021), and HOPE (Luo et al., 2023), extend neural ODEs by incorporating graph neural networks to capture spatiotemporal dependencies. Neural Relational Inference (NRI) (Kipf et al., 2018) employs a variational encoder-decoder architecture that models pairwise interactions and state transitions in a discrete setting. Despite their flexibility, these models often rely on known or partially known network structures. In contrast, our work eliminates the need for prior topology knowledge, offering a robust solution for forecasting network dynamics and generalizing across diverse and unseen network structures.\nNetwork Inference. Another line of research focuses on network reconstruction (De Smet and Marchal, 2010; Peixoto, 2019), which aims to infer network topology from observed time-series data to reveal the structural interactions within the system. Regression-based approaches include (Casadiego et al., 2017), which estimates interaction strengths using basis function expansions, and (Prasse and Van Mieghem, 2018, 2020), which infer adjacency weights by treating them as parameters relying on known dynamical formulas. Zhang et al. (2022) combines a deep learning approach with Markovian assumptions to jointly infer adjacency matrices and dynamical parameters, focusing on network reconstruction and prediction in a discrete framework. Unlike these approaches, our work bypasses explicit network reconstruction and instead uses latent relationships to predict nodal trajectories.\nLearnability of Networked Dynamical Systems. The PAC framework has been employed to study the learnability of discrete networked dynamical systems, providing theoretical insights into network interactions and node functions. Narasimhan et al. (2015) investigates the PAC learnability of influence functions in partially observed social networks, while Adiga et al. (2019) focuses on threshold functions within known network topologies. Qiu et al. (2024) further examines the learnability of node functions in networked dynamical systems under partial topology knowledge. Our work complements them by tackling the practical challenge of forecasting nodal dynamics without prior knowledge of topology or dynamical rules."}, {"title": "III. METHOD", "content": "A. Preliminaries\nConsider a graph denoted as G = (V,E) with nodes V = {1, ..., N} and edges E \u2282 V \u00d7 V. The nodal activity observed at the \\( \\tau \\)th time step (\\( \\tau \\in \\mathbb{Z}_{>0} \\)) is a D-dimensional vector \\( x_{\\tau} \\in \\mathbb{R}^D \\). This activity can describe the probability of infection in a pathology spreading network (Jiang et al., 2020) or the velocity and location in charged particle systems (Zhang et al., 2022). Suppose each node's time series has T timestamps in total. The temporal nodal states can be arranged as: \\( x_i = [x_i^1, ..., x_i^T] \\) (\\( i \\in \\{1, ..., N\\} \\)). We use \\( x_{i:t_1:t_2} \\) to denote a subsequence of \\( x_i \\) including all observations from timestamp \\( t_1 \\) to \\( t_2 \\) (\\( t_1 \\leq t_2 \\)): \\( x_{i:t_1:t_2} = [x_i^{t_1}, x_i^{t_1+1}, ..., x_i^{t_2}] \\). The network connections and interacting strength are compactly represented as the N\u00d7 N adjacency matrix A. We assume the coupling strength \\( A_{ij} \\) is a real number, with a larger value indicating a stronger impact from node j to node i. While the observation is sampled at discrete time, the underlying dynamics is continuous. We denote the state of i at a continuous time \\( t \\in \\mathbb{R} \\) as x(t). Each nodal trajectory is governed by a coupled ordinary differential equation (Gao et al., 2016)\n\n$\\frac{dx_i(t)}{dt} = F(f(x_i(t)) + \\sum_{j} A_{ij}g(x_i(t), x_j(t)))$   (1)\n\nGiven any initial condition, Integrating Eq (1) produces a sequence of states for i. Here \\( f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D, g(\\cdot, \\cdot): \\mathbb{R}^D \\times \\mathbb{R}^D \\rightarrow \\mathbb{R}^D \\) describe the self-dynamics and the interaction term between adjacent nodes, respectively. Function F combines the self-feedback and neighborhood impact.\nWe assume \\( A_{ij} \\geq 0 \\) and consider two types of interaction terms:  $\\frac{\\partial g}{\\partial x_j} > 0 \\) and  $\\frac{\\partial g}{\\partial x_j} < 0 \\) to reflect whether the system is entirely cooperative or competing. We use the notation that a vector is greater than zero if all of its entries are greater than zero.\nWe define the dynamics learning model as follows: our model takes as input an initial short period of observations on each node, denoted as \\( x_{1:T_{obs}} \\), where \\( T_{obs} \\) is the length of the condition window. The output of our model is the prediction of the states for each individual at subsequent time steps \\( T_{obs} + 1, T_{obs} + 2,...,T \\), represented as \\( x^{T_{obs}+1:T} \\). We refer to the time interval [Tobs+1, T] as the prediction window.\nB. Model Definition\nIn this paper, we present TAGODE (Topology-Agnostic Graph ODE) for learning networked dynamical systems without prior knowledge of the underlying network structure. Our model consists of three primary modules: an encoder, a neural ODE, and a decoder. The encoder's role is to deduce the initial conditions of latent vectors (zi(0)) for individual nodes. The decision on whether the encoder produces the edge embedding depends on the specific type of neural ODE. We consider two variations of neural ODE: one with evolving latent edges and another without. In the first case, the encoder solely produces node embeddings, and the neural ODE captures the interaction at each specific time instant t. In the latter case, the encoder creates a latent edge embedding. As we solve an initial value problem using the neural ODE with the initial condition z\u00bf(0), it results in a sequence of latent states, spanning T - Tobs time steps for each node. Subsequently, the decoder independently maps each latent state back to the input space, generating predictions for nodal states at time steps Tobs + 1 through T. The terms \"FeedForward network\" and \u201cMulti-layer Perceptron\" are used interchangeably in our context.\nEncoder. The encoder module for nodes, represented by fnode, takes a segment of observations for each node's activity \\( x_{i:T_{obs}}^{1:T_{obs}} \\in \\mathbb{R}^{T_{obs} D} \\) as input and maps them to a latent initial condition of dimension d. The role of the encoder is dependent"}, {"title": "Neural ODE and Decoder.", "content": "on whether the neural ODE assumes a fixed edge embedding. In the case where a fixed edge embedding is employed, the encoder's responsibility extends to generating representations of pairwise interactions. These interactions between nodes are computed based on the concatenation of their initial latent states. This process can be mathematically expressed as follows:\n\n\\( z_i(0) = f_{node}(x_{i:T_{obs}}) \\)\n(2)\n\n\n\\( A_{ij} = f_{edge}([z_j(0)||z_i(0)]) \\)\n(3)\n\nwhere zi(0) represents the latent initial condition, \\( A_{ij} \\in \\mathbb{R} \\) is the effect of latent node j on i, and \\( || \\) denotes vector concatenation.\nThis architecture allows flexibility in the choice of fnode, which can be implemented as a Feedforward network (FFW), a graph transformer (Yun et al., 2019), an NRI-based encoder (Kipf et al., 2018) or a spatiotemporal graph representation encoder (Luo et al., 2023; Huang et al., 2020, 2021). The latter two encoders require knowledge of the network topology, so we assume a fully connected underlying structure in such cases. For a detailed comparative study of these encoder variants, please refer to Appendix B-E.\nFor the remaining sections, we will assume the adoption of the Feed-forward module to define the encoder functions fnode, fedge. In our experiments, we utilized the following specific instance of the Feedforward module:\nFeedForward(h) = \\( W^{(2)}\\sigma(W^{(1)}h + b^{(1)}) + b^{(2)} \\)  (4)\nIn Eq (4), \\( W^{(l)} \\in \\mathbb{R}^{d \\times \\cdot}, b^{(l)} \\in \\mathbb{R}^d \\) are learnable weights and biases and \\( \\sigma \\) is any nonlinear activation function such as ReLU or GELU (Hendrycks and Gimpel, 2016). We compute latent nodes independently because we do not assume prior knowledge of topology. This approach allows each observation to contribute to the node's latent vector without any predefined network structure.\nNeural ODE and Decoder. We define the Ordinary Differential Equation (ODE) governing the evolution of latent states in our model. To capture the self-dynamics and interactions between latent nodes, we adopt a universal framework, which expresses the dynamics as follows:\n\\( \\frac{dz_i(t)}{dt} = \\phi_i(z_1(t), ..., z_N(t), \\hat{A}) \\)\n(5)\n\n\\( = F(f(z_i(t)) + \\sum_{j=1}^{N} \\hat{A}_{ij}g(z_i(t), z_j(t))) \\)\n(6)\nWhile Casadiego et al. (2017) employed a linear combination of basis functions to model the ODE of observable nodal states xi, where the accuracy of their approach relied on selecting appropriate basis functions, our objective is to model the evolution of latent states. These latent states can exhibit more complex functional forms. Therefore, we choose to parameterize the dynamical formulas f and g using FeedForward networks, allowing us to learn these functions directly from the data. The latent state at each timestamp can be computed using the following integral formulation:\n\\( z_i^t = \\int_{t=0}^{t} \\phi_i(z_1(t), ..., z_N(t), \\hat{A}) dt \\)\n(7)\nWe further consider a model variant (TAGODE-VE) incorporating time-varying edges to take into consideration potential evolution of interaction. Unlike the first type of ODE (6), where the encoder provides constant edge weights, TAGODE-VE leverages a multi-head attention mechanism to infer pairwise interactions. This mechanism computes attention scores for all pairs of nodal embeddings, enabling the model to adapt and capture changing interactions as they unfold over time. The attention scores, represented as \\( A_{ij}^h \\) are obtained through a softmax function applied to learned attention logits \\( e_{ij}^h \\). These logits are computed using key and query matrices, \\( W_k^h \\) and \\( W_q^h \\), which project the sender and receiver nodes, respectively. The LeakyReLU activation function is employed to introduce non-linearity and enhance the model's ability to capture complex interactions. The formula for the score of one attention head is given as follows:\n\\( A_{ij}^h = softmax(e_{ij}^h) \\)\n(8)\n\n\\( e_{ij}^h = LeakyReLU((W_k^h z_j)^T (W_q^h z_i)) \\).\n(9)\nWe aggregate the outputs from nhead attention heads to approximate the interaction term. The corresponding latent nodal ODE is defined as follows:\n\n$\\frac{dz_i(t)}{dt} = f(f(z_i(t)) + \\sum_{j=1}^{N} \\sum_{h=1}^{Nhead} A_{ij}^h g(z_i(t), W_v^h z_j(t)))$   (10)\n\nThe value matrix \\( W_v^h \\) transforms the expression of the sender node to extract the feature from j that has an impact on i. By aggregating these individual heads, our model gains a more comprehensive understanding of the underlying interactions. As demonstrated in the experimental section IV-C1, both ODE types perform similarly when inferring in-distribution dynamics. However, the attention-based ODE exhibits superior generalization capabilities when handling out-of-distribution dynamics.\nThe model performs decoding by processing the hidden vectors individually at each timestamp to generate state predictions:\n\n$\\hat{x_i} = f_{dec}(z)$   (11)\n\nIn our experiment setup, fdec : \\( \\mathbb{R}^d \\rightarrow \\mathbb{R}^D \\) is implemented as a Multi-layer Perceptron (MLP). We define the objective function as the state reconstruction loss over the prediction window:\n\n\\( minimize \\sum_{i=1}^{N} \\sum_{t=T_{obs}+1}^{T} ||x_i^t - \\hat{x_i}^t|| \\)\n(12)\nWe formulate the edge embedding modeling within an unsupervised framework. Our goal is to deduce a latent topology"}, {"title": "IV. EXPERIMENTS", "content": "that enables accurate predictions of network states, rather than explicitly recovering the ground truth network structure. This approach proves beneficial in scenarios where the topology information is deprecated or incomplete.\nIV. EXPERIMENTS\nIn this section, we empirically evaluate our model in a transductive learning setting with real epidemic spreading data (Section IV-B) and an inductive learning setting using synthetic data generated from diverse dynamical models (Section IV-C). Furthermore, we examine its performance under OOD testing (Section IV-C1). Additional experiments, including analyses of scalability, robustness, and ablation studies, are detailed in Appendix B.\nA. Experiment Setup\nBaselines. We compare with four autoregressive baselines and a neural ODE-based method. (i) LSTM: The LSTM baseline adopted by (Kipf et al., 2018). It models time-series independently for each node. A 2-layer MLP is added both before and after an LSTM unit, which outputs a one-step lookahead value for the hidden state. (ii) GRU (Cho et al., 2014): Gated Recurrent Unit. Similar to the LSTM baseline, it includes 2-layer MLPs to perform feature transformation, but the one-step lookahead is predicted by a GRU unit. (iii) NRI (Kipf et al., 2018): The Neural Relational Inference model. NRI performs message passing between nodes and edges to produce the probability of the system state at the next timestamp. (iv) AIDD (Zhang et al., 2022). The Automated Interactions and Dynamics Discovery (AIDD) model parameterizes the weighted adjacency matrix and dynamical equations for Markov dynamics. (v) NODE (Chen et al., 2018): Neural Ordinary Differential Equations. The vanilla NODE can be regarded as only modeling the self-dynamics for each node, without consideration of interaction. In our implementation, we employ the same encoder and decoder for NODE as in our model and utilize a FeedForward network to parameterize the self-dynamics.\nFor autoregressive models, prior to the observation cutoff at Tobs, the input state is the observation. After Tobs, the model's own predictions at previous time steps serve as input. The model size of LSTM, GRU, and AIDD scale with the number of nodes, while ours is independent of the network size. Our implementation is publicly available\u00b9.\nEvaluation Metric. We use Mean Absolute Percentage Error (MAPE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) to evaluate the model performance, with their formulas detailed in Section A-B.\nB. Transductive Setting Evaluation\nIn this section, we focus on a transductive learning setting by applying a chronological train/test split on a single multivariate time-series. In this case, both train and test data are sampled from an identical dynamical system with the same underlying topology. We evaluate models on a real dataset"}, {"title": "C. Inductive Setting Evaluation", "content": "COVID-19 (Dong et al., 2020), which consists of the state-level reported cases of coronavirus in the United States. We utilize seven features to form the nodal state, including five dynamic features collected from the Johns Hopkins University (JHU) Center for Systems Science and Engineering (#Confirmed, #Deaths, #Recovered, Testing-rate, Mortality-rate), a static feature of state population, and a in-state population flow value provided by SafeGraph22. The model is trained on data from April.12.2020 to Nov.30.2020 and tested on the time span between Dec.01.2020 and Dec.31.2020. We condition on an observation window of length 21, and predict the cumulative deaths in the future 1-week, 2-weeks, and 3-weeks. Please refer to Appendix A-A for more details.\nThe results on this dataset is summarized in Table I. TAGODE demonstrates a reduction in MAE (MAPE) compared to the best-performing non-continuous baseline by 24.27% (26.47%), 39.02% (39.19%), and 61.58% (55.34%) for the three specified prediction ranges. In comparison to the continuous baseline NODE, our models incorporate the modeling of latent interactions. This additional feature results in an average decrease in MAE (MAPE) by 37.54% (20.06%) and 14.71% (13.40%) for TAGODE and TAGODE-VE, respectively. This improvement is most pronounced in the longest prediction task, demonstrating the superior performance of our models for relatively long-term predictions. The findings also indicate that continuous models excel in learning the dynamics of multi-agent systems in real-world applications.\nC. Inductive Setting Evaluation\nIn this section, we systematically evaluate model generalizability under two conditions: (i) testing within known topological distributions, where training and testing datasets share the same topology type but differ in specific wiring; and (ii) testing under OOD conditions, where both the degree and edge weights distributions are entirely unseen during training.\nDynamics. We study six types of continuous dynamics, as explored in (MacArthur, 1970; Hens et al., 2019; Prasse and Van Mieghem, 2022): (i) SIS: Epidemic spreading, modeled using the Susceptible-Infected-Susceptible (SIS) framework; (ii) Population: Population dynamics driven by birth-death processes; (iii) Regulatory: Gene regulatory networks; (iv) Mutualistic: Mutualistic plant-pollinator interactions; (v) Neural: Neuronal activity; (vi) Lotka-Volterra: Predator-prey systems, modeled using the Lotka-Volterra equation. The ODE formulas corresponding to these dynamics are summarized in Table II. The training and testing trajectories are generated by solving the ODE with a specified initial condition.\nNetwork Topology. We explore three types of synthetic network models: Erd\u0151s-R\u00e9nyi (ER) networks with Poisson-distributed node degrees, scale-free (SF) networks with a power-law degree distribution generated using preferential attachment, and Community networks generated via random partition graphs. For each dynamics paired with a topology"}, {"title": "V. CONCLUSION", "content": "We propose a novel approach for learning network dynamics directly from time-series data, eliminating the need for prior knowledge of network topology. By observing an initial short time window, our model constructs a latent topology representation that captures the underlying system dynamics. Furthermore, the model accomodates systems with time-varying edges by modeling edge weights as functions of evolving node states, providing a unified framework for both static and dynamic networks. This approach effectively simulates and predicts system behaviors, demonstrating robust performance in forecasting the long-term evolution of diverse dynamical systems across various complex network topologies. Our evaluation spans both transductive and inductive learning scenarios, showcasing the model's ability to generalize across time series generated from diverse network configurations, including OOD topologies. Future research could explore design considerations that enhance generalization to distinct network types and dynamical systems."}, {"title": "APPENDIX A\nEXPERIMENTAL SETUP", "content": "A. COVID-19 Dataset\nFor COVID-19 dataset, the nodal features include\n\u2022\n#Confirmed: The number of daily increased confirmed cases.\n\u2022 #Deaths: The number of daily increased deaths.\n\u2022\n#Recovered: The number of daily increased recovered cases.\n\u2022 Testing-Rate: The number of daily cumulative test results per 100,000 persons.\n\u2022 Mortality-Rate: The number of daily cumulative deaths times 100 divided by the number of daily cumulative confirmed cases.\n\u2022 Population: The of residents in each state.\n\u2022 Mobility data: the number of people moving between points of interests (e.g., restaurants, grocery stores) within each state.\nThe data preprocessing procedure follows Huang et al. (Huang et al., 2021).\nThe training trajectory tensor is of shape N \u00d7 T \u00d7 7, where N = 50,T = 233. The training trajectory is chunked into sub-sequences of length Tobs + Tpred where Tpred \\( \\in \\) {7,14,21}. We set the condition length Tobs to be 21, which is the length of the input sequence to our model. The data preprocessing procedure follows Huang et al. (2021). Refer to Appendix A-A for more details.\nB. Evaluation Metric\nWe use Mean Absolute Percentage Error (MAPE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) to evaluate the model performance. For trajectories with T time steps, N variables, and D features per variable, these metric are computed as\n\nMAPE= $\\frac{1}{TND} \\sum_{t=1}^{T} \\sum_{i=1}^{N} \\sum_{j=1}^{D} | \\frac{x_{i,j}^t - \\hat{x}_{i,j}^t }{x_{i,j}^t} |$     (13)\n\n\nMAE= $\\frac{1}{TND} \\sum_{t=1}^{T} \\sum_{i=1}^{N} \\sum_{j=1}^{D} |x_{i,j}^t - \\hat{x}_{i,j}^t |$       (14)\n\n\nRMSE= $\\frac{1}{TND} \\sqrt{\\sum_{t=1}^{T} \\sum_{i=1}^{N} \\sum_{j=1}^{D} (x_{i,j}^t - \\hat{x}_{i,j}^t )^2}$         (15)\n\n\nwhere \\( \\hat{x_{i,j}^t} \\) denotes the jth feature of the ith node at time t and \\( x_{i,j}^t \\) is its corresponding prediction.\nC. Synthetic Network Data\nThe detailed explanation of the six dynamics with ground truth formula is as follows.\n\u2022\n\nThe susceptible-infected-susceptible model (SIS) (Pastor-Satorras et al., 2015; Prasse and Van Mieghem, 2022): $\\frac{dx_i}{dt}$ = -\\( \\delta_i \\) xi + \\( \\sum_j A_{ij} \\) (1 \u2013 xi)xj. The parameter \\( \\delta_i \\) > 0 represents the rate at which individuals recover from the infection. Following (Prasse and Van Mieghem, 2022), we sample \\( \\delta_i \\) from\n\n\np(diag(\\sqrt{\\delta^{(0)}})Adiag(1/\\sqrt{\\delta^{(0)}})) \\delta^{(0)}/1.5\n\n\nwith \\( \\delta^{(0)} \\in \\mathbb{R}^N \\) and \\( \\delta_i \\) ~ U[0.5, 1.5], diag(\u00b7) is the N \u00d7 N matrix with the input vector's values placed on the diagonal. p(.) denotes the spectral radius of the input matrix. This setting ensures the basic reproduction number (Van den Driessche and Watmough, 2002) R\u2080 = \\( \\rho \\)(diag(\\( \\delta\\))\u207b\u00b9A) is greater than 1, i.e., the virus does not die out.\n\u2022 Population dynamics: $\\frac{dx_i}{dt}$ = -B\\( \\alpha x_i \\) + \\( \\sum_j A_{ij} x_j \\). When b = 0, it signifies the flow of population into and out of the locations. In contrast, when b > 0, it corresponds to mortality within the population. For our analysis, we have set B = 1, b = 0.5, and \\( \\alpha \\) = 0.2.\n\u2022\nGene regulatory dynamics: $\\frac{dx_i}{dt}$ = -Bixi + \\( \\sum_j A_{ij} \\frac{x_j^h}{x_j^h+1} \\) (Alon, 2006; Gao et al., 2016). The parameter Bi controls the rate of self-decay, where f = 1 corresponds to degradation, and f = 2 corresponds to dimerization, a process by which two identical molecules come together to form a dimer. Additionally, h \\( \\geq \\) 0 represents the Hill coefficient, which quantifies the rate of saturation for the impact of neighboring nodes. For our analysis, we set the values as (Bi = 1, f = 1, h = 2).\n\u2022\nMutualistic dynamics: $\\frac{dx_i}{dt}$ = Bixi(1 - \\( \\frac{x_i}{C_i} \\)) + \\( \\sum_j A_{ij} \\frac{a_ix_i}{x_j(1+x_i)} \\) \u207b\u00b9 (Hens et al., 2019). The first term resembles logistic growth, where Bi signifies the reproduction rate, \\( \\alpha \\) determines intraspecific competition, and Ci represents the"}, {"title": "APPENDIX B\nADDITIONAL EXPERIMENTS", "content": "carrying capacity. The mutualistic aspect is embodied in the saturating term \\( \\frac{a_ix_i"}, {"1972)": "frac{dx_i"}, {"1970)": "frac{dx_i}{dt}$ = xi(ai \u2013 dixi) \u2212 \\( \\sum_j A_{ij} x_ix_j \\). The dynamical parameters govern the growth of species i with ai, \u03b8i > 0. In the experiments, we sample ai, \u03b8\u2081 from a uniform distribution in the range [0.5, 1.5", "networks": "also known as random networks, where node degrees are drawn from a Poisson distribution with an average degree  = (N-1)p. (p is the probability of edge creation). (2) Scale-free (SF) networks , characterized by a Power-law degree distribution. We apply preferential attachment to construct this network topology. We begin with a set of mo nodes and iteratively introduce new nodes. Each new node is connected to m existing ones, with the likelihood of connecting to an existing node being proportional to that node's degree. (3) Community networks , generated by the random partition graph models with intra-cluster connection of pin and inter-cluster connection of pout. In our experiment, we fix the value of pin at 0.25. When simulating network structures, we first generate an unweighted graph using the network model and assign each link with a weight drawn uniformly from the range of [0.5, 1.5"}]}