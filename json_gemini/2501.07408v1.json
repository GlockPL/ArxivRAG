{"title": "Initial Findings on Sensor based Open Vocabulary Activity Recognition via Text Embedding Inversion", "authors": ["Lala Shakti Swarup Ray", "Bo Zhou", "Sungho Suh", "Paul Lukowicz"], "abstract": "Conventional human activity recognition (HAR) relies on classifiers trained to predict discrete activity classes, inherently limiting recognition to activities explicitly present in the training set. Such classifiers would invariably fail, putting zero likelihood, when encountering unseen activities. We propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this limitation by first converting each activity into natural language and breaking it into a sequence of elementary motions. This descriptive text is then encoded into a fixed-size embedding. The model is trained to regress this embedding, which is subsequently decoded back into natural language using a pre-trained embedding inversion model. Unlike other works that rely on autoregressive large language models (LLMs) at their core, OV-HAR achieves open vocabulary recognition without the computational overhead of such models. The generated text can be transformed into a single activity class using LLM prompt engineering. We have evaluated our approach on different modalities, including vision (pose), IMU, and pressure sensors, demonstrating robust generalization across unseen activities and modalities, offering a fundamentally different paradigm from contemporary classifiers.", "sections": [{"title": "I. INTRODUCTION", "content": "Human activity recognition (HAR) [1], [2] has been a cornerstone of research in ubiquitous computing, enabling diverse applications such as health monitoring, human-computer interaction, and autonomous systems. Conventional HAR methods typically rely on supervised learning, where models are trained to classify discrete activity labels from sensor or vision-based data. While these approaches achieve impressive performance on predefined activity sets, they are inherently limited to recognizing only those activities explicitly present in the training dataset. This limitation poses significant challenges for real-world applications, where the diversity of human activities is virtually infinite, and encountering unseen activities is inevitable.\nResearchers have recently explored few-set and zero-shot learning paradigms in HAR to address this challenge [3]. These paradigms aim to equip models to generalize to activities beyond those seen during training. However, existing approaches often require a fine-tuning step with a small amount of labeled data for the new activities, even though this data requirement is significantly smaller than that for training from scratch. This dependence on additional data limits their scalability in scenarios where labeling new activities is impractical or costly. Moreover, such methods typically lack the ability to generate descriptive explanations of activities, making their predictions less interpretable.\nIn this work, we introduce Open Vocabulary Human Activity Recognition (OV-HAR), a fundamentally new approach to HAR that moves beyond the conventional classification paradigm. Inspired by the success of natural language models in generalizing across tasks, OV-HAR frames activity recognition as a passive text-generation problem. Instead of classifying sensor data into predefined categories, our method converts activity data into descriptive text, capturing a sequence of elementary motions that describe the activity. By encoding these descriptions into fixed-size embeddings and decoding them back into natural language, OV-HAR facilitates the recognition and description of unseen activities in a scalable and interpretable manner.\nUnlike existing cross-modal solutions that rely on autoregressive large language models (LLMs) [4], [5], which are computationally intensive and require substantial resources, OV-HAR employs a lightweight embedding regression framework. While LLMs excel in natural language tasks, they have several limitations when applied to HAR. First, they require significant memory and computational power, making them impractical for deployment on edge devices or in real-time systems. Second, they are prone to catastrophic forgetting, where fine-tuning on new tasks or data can degrade performance on previously learned tasks. Third, due to their autoregressive nature, the output sequence generation depends heavily on the first few embeddings. Once the initial input embeddings are processed, subsequent tokens often result in the model primarily fitting to its own previously generated text, which can lead to error propagation and reduced fidelity to the original input data. This makes them less suitable for tasks requiring fine-grained alignment with sensor data. Additionally, autoregressive models exhibit slower inference speeds due to their sequential nature and are less efficient in capturing non-sequential relationships critical for multi-modal sensor data. OV-HAR addresses these challenges by minimizing computational overhead, avoiding catastrophic forgetting through fixed-size embedding regression, and directly accommodating sensor data while retaining the expressiveness of natural language representations. Furthermore, the generated text can be mapped to activity labels through prompt engineering with pre-trained LLMs, offering flexibility in applications requiring discrete classifications. To demonstrate its robustness and generalization capabilities, we evaluate OV-HAR across diverse sensor modalities, including vision-based pose data from NTU-RGBD, inertial measurement units (IMUs), and pressure sensors. Our results show that OV-HAR effectively recognizes unseen activities and generalizes across modalities, highlighting its potential as a universal framework for open vocabulary activity recognition. By integrating descriptive natural language capabilities into HAR, OV-HAR extends the boundaries of activity recognition and sets the stage for more interpretable and versatile human-centered AI systems.\nOur contributions can be summarized as follows:\n\u2022 Open Vocabulary Human Activity Recognition Frame work: We propose OV-HAR visualized in Figure I, a novel approach to HAR that frames activity recognition as a text generation problem using natural language descriptions.OV-HAR avoids the computational burden of autoregressive LLMs by leveraging fixed-size embedding regression and a pre-trained embedding inversion model vec2text [6].\n\u2022 Cross-Modal Evaluation: We demonstrate robust generalization across multiple sensor modalities, including vision (pose), IMU, and pressure sensors where Our approach achieves state-of-the-art performance in recognizing and describing activities unseen during training."}, {"title": "II. APPROACH", "content": "Our approach introduces a novel pipeline that translates sensor data into semantically meaningful embeddings, enabling bidirectional mapping between natural language descriptions and discrete activity classes. Unlike contemporary classifiers that predict fixed, one-hot-encoded classes, our model leverages regression to generate embeddings representing sequences of atomic motions. This allows for more nuanced and interpretable predictions, capturing the semantic richness of activities and facilitating seamless natural language interaction with human activity recognition systems.\na) Class to Embedding: To generate a text embedding for a discrete class C, we first leverage LLaMA 3 to break C into a sequence of atomic motions A = {a1, a2, . . . , an}, where each ai represents a fundamental motion contributing to the activity. For instance, if C is \u201dbaseball swing from right,\u201d LLaMA 3 decomposes it into A = {a1 : \u201dright arm swipe\u201d, a2 : \u201dbody rotation\u201d, a3 : \u201darm follow-through\u201d}.\nWe then construct a descriptive sentence S from these motions, such as S = Perform a right arm swipe with a body rotation followed by an arm follow-through. This sentence is passed through gtr-t5-base [7], a sentence-transformers model that maps text to a 768-dimensional dense vector space optimized for semantic search, producing the embedding E = gtr-t5-base(S) \u2208 R^{768}.\nThis embedding E captures the semantic meaning of the activity class, grounded in its atomic motions.\nb) OV-HAR Architecture: The OV-HAR model is a simple, lightweight regressor based on the state-of-the-art (SOTA) sensor-based HAR network from ALS-HAR [8]. It employs a convolutional block with a 1D convolution layer followed by a max-pooling layer and a flatten layer. This is followed by a bidirectional LSTM layer and a fully connected (FC) layer. Unlike conventional HAR models that output one-hot encoded classes, the output of this architecture is a 1D vector of size 768, denoted as h \u2208 R^{768}.\nThe output vector h is then connected to a frozen vec2text model [6], which is pretrained on GTR text inversion, to reconstruct the corresponding natural language description S. The model optimization uses a mean squared error (MSE) loss function, as the task is treated as a regression problem.\nThis approach ensures the model can effectively translate sensor data into semantically meaningful embeddings while leveraging pretrained text inversion capabilities for natural language generation.\nUnlike contemporary classifiers that typically process small fixed time windows of 0.5 seconds, our approach utilizes a larger fixed time window of 5 seconds. If the activity duration is shorter than 5 seconds, it is padded with null values until the full window length is reached. For longer activities, the data is segmented into multiple overlapping 5-second windows using a sliding window mechanism. This design choice stems from the fact that the output prediction is not a discrete class but rather a regression of a sequence of atomic motions. By incorporating a larger time window, the model captures the entirety of the motion, ensuring that all atomic actions within the activity are represented in the input.\nc) Natural Language to Class: To convert a descriptive natural language text S back into a discrete class C, we use LLaMA 3 [9] with prompt engineering. The model is prompted to analyze the semantic meaning of S and identify the corresponding predefined class label C from a set of possible activities.\nFor example, consider the descriptive text: S = Perform a right arm swipe with a body rotation followed by an arm follow-through."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "a) Baseline Model: Instead of using a text inversion model, we implemented a simpler approach by creating a lookup table. In this method, the regressed text embedding h \u2208 R^{768} is compared against all available embeddings in the lookup table. The embedding is then assigned to the entry in the table that shares the maximum similarity with h.\nThe similarity is computed using cosine similarity:\nSim(h, e) = \\frac{h \u00b7 e}{\\parallel h\\parallel \\parallel e\\parallel}                                                            (1)\nwhere e represents an embedding from the lookup table.\nSince we know the activity corresponding to each embedding in the lookup table, this process allows us to determine the activity predicted by the model. If h achieves the maximum similarity score with multiple embeddings, the prediction is treated as a mixture of the activities associated with those embeddings. This approach provides a straightforward way to interpret the model\u2019s predictions based on the closest matching entries in the dataset.\nb) Implementation Details: OV-HAR is implemented in PyTorch 2.0 and trained on a windows system with 32GB memory and NVIDIA 4090 GPU for 300 epochs. Early stopping and a decaying learning rate scheduler (1e^{-3}) were employed to prevent overfitting.\nc) Evaluation Datasets: We used 3 datasets for our evaluation as follows:\n\u2022 UTD-MHAD Dataset [10]: This dataset includes 28 dynamic activity sequences with 3D pose, video, and inertial sensor data from the right wrist and left thigh, sampled at 30 Hz for pose data and 50 Hz for inertial data. To evaluate the open-vocabulary capability of OV-HAR, we designated the activities Draw triangle, Two-hand push, Right hand knock on door, Right hand pick up and throw, Walking in place, and Forward lunge as test set activities. The remaining 23 activities were used for training. These test activities were selected based on the criterion that they can be entirely described by atomic motions derived from the training set activities, without introducing any new atomic motions.\n\u2022 NTU-RGBD Dataset [11]: This dataset comprises 82 daily actions, 12 medical conditions, and 26 person-to person interactions, captured through 3D pose, depth, and video data. For evaluation, we focused solely on the 82 daily action activities. Among these, *Ball up paper (A83)*, *Cutting paper (A76)*, *Counting money (A74)*, Take object out of bag (A90), Throw up cap/hat (A94), Taking a selfie (A32), Put on jacket (A14), Put on headphone (A61), Shoot at basket (A63), and Move heavy objects (A92) were allocated to the test set, while the remaining 72 activities were used for training. Again test set activities were selected based on their complete decomposability into atomic motions present in the training set.\n\u2022 PIMesh Dataset [12]: This dataset contains 28 static sleeping poses and 2 motions in bed, recorded using video, SMPL pose, and sensor data from a pressure sensing bedsheet (56 \u00d7 40). For evaluation, we included 3 supine postures, 2 side postures, and 1 prone posture in the test set, while the remaining 11 poses were used for training.\nd) Quantitative Results: To evaluate the open-ended activity recognition capability of OV-HAR, we compared it with three alternative approaches: a baseline classifier based on a simple look-up table, a contemporary classifier based on on ALS-HAR [8] enhanced with few-shot learning (FS-CC) to recognize new activities, and a LLM-based classifier (LLM-C) inspired by VideoLLaMa [5]. In the LLM-C approach, the input modality is converted into text-equivalent tokens using MLP layers and then passed to a frozen LLaMA 2 model, following a standard procedure for training cross-modal LLMs. All models were trained on N classes and evaluated on M classes, which were mutually exclusive. It is important to note that FS-CC required fine-tuning on the M classes with a few-shot learning setup, as it would otherwise fail to recognize activities not present in its initial training set. The results of this evaluation are presented in Table I.\ne) Analysis: The results in Table I underscore significant trends in open-vocabulary activity recognition across different datasets and modalities, emphasizing both the strengths and limitations of various approaches. The Few-shot Contemporary Classifier (FS-CC) consistently achieved the highest macro F1-scores across datasets and modalities, such as 0.671 \u00b1 0.021 for IMU data in UTD-MHAD and 0.662 \u00b1 0.019 for pressure data in PIMesh. This performance demonstrates the effectiveness of leveraging even a small amount of labeled data from unseen classes. However, the reliance on seeing instances of the new classes before recognizing them contradicts the very principle of open-vocabulary recognition. If the model requires exposure to test classes for accurate predictions, it cannot truly generalize to unseen activities, undermining its suitability for open-vocabulary applications.\nThe LLM-based classifier (LLM-C), on the other hand, performed poorly, with results such as 0.172 \u00b1 0.009 for IMU in UTD-MHAD and 0.152 \u00b1 0.010 for pressure data in PIMesh. This underwhelming performance aligns with the expectation that architectures like LLM-C require large-scale, diverse training data to generalize effectively. Moreover, its computational complexity and high resource demands render it impractical for tasks where lightweight models are preferable. Even the baseline model outperformed LLM-C, achieving 0.282 \u00b1 0.014 for IMU data in UTD-MHAD and 0.262 \u00b1 0.014 for pressure data in PIMesh, highlighting that simple lookup-based approaches can occasionally be more effective than poorly adapted architectures.\nThe contemporary classifier, which relies on rigid class discretization, performed the worst, consistently failing to predict unseen classes with 0.0% accuracy across all datasets and modalities. This failure stems from its inherent limitation: it can only classify activities strictly defined during training, making it entirely unsuitable for open-vocabulary settings.\nIn contrast, OV-HAR struck a balance between performance and computational efficiency. Despite being lightweight, similar to the contemporary classifier, OV-HAR demonstrated a strong ability to generalize to unseen classes, achieving 0.472 \u00b1 0.019 for IMU in UTD-MHAD and 0.462 \u00b1 0.021 for pressure data in PIMesh. These results, while not perfect, are particularly impressive given OV-HAR\u2019s ability to predict activities entirely outside the initial training set. This demonstrates its potential to address the open-vocabulary challenge effectively, outperforming more computationally demanding models like LLM-C and offering a practical alternative to FS-CC by eliminating the need for additional labeled data from unseen classes.\nOverall, the results highlight a clear trade-off between model flexibility, computational overhead, and adherence to the open-vocabulary paradigm. While FS-CC achieves high performance by compromising on open-vocabulary principles, and LLM-C struggles due to data and resource limitations, OV-HAR emerges as a promising solution that balances these competing demands, achieving commendable performance across datasets and modalities."}, {"title": "IV. CONCLUSION", "content": "The findings from this study reveal promising avenues for advancing open-vocabulary activity recognition. A critical area for improvement lies in enhancing the model\u2019s ability to generalize to unseen activities. Techniques such as leveraging unsupervised learning or contrastive learning could enable OV-HAR to better understand the semantic and structural relationships between activities. Multi-modal fusion strategies, combining IMU and pressure data, or integrating video and textual modalities, present another promising direction to enrich the model\u2019s contextual understanding. A key limitation in current evaluation lies in the suitability of available datasets. Collecting or curating a more comprehensive and representative dataset specifically tailored to open-vocabulary scenarios is essential. Such a dataset should include a broader spectrum of activities, varying levels of granularity, and sufficient examples of novel, complex, and overlapping activities to test OV-HAR\u2019s true capacity to generalize. By addressing these challenges, future research can push the boundaries of OV-HAR, making it more robust, versatile, and applicable to real-world tasks."}]}