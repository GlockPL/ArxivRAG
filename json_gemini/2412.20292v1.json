{"title": "An analytic theory of creativity in convolutional diffusion models", "authors": ["Mason Kamb", "Surya Ganguli"], "abstract": "We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data. But optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in a fully analytic, completely mechanistically interpretable, equivariant local score (ELS) machine that, (3) without any training can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of 0.90, 0.91, 0.94 on CIFAR10, FashionMNIST, and MNIST). Our ELS machine reveals a locally consistent patch mosaic model of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches in different image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2$ ~ 0.75 on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.", "sections": [{"title": "1. Introduction and related work", "content": "A deep puzzle of generative AI lies in understanding how it produces seemingly endless, apparently creative, output. What is the origin of this creativity, and how precisely is it generated from a finite training set? We answer these questions for convolutional diffusion models of images by deriving the first analytic and interpretable theory of their creativity that can accurately predict their outputs on a case-by-case basis (Fig. 1), and explain how they are created out of locally consistent patch mosaics of the training data.\nDenoising probabilistic diffusion models (DDPMs) were established in (Sohl-Dickstein et al., 2015; Ho et al., 2020) and then unified with score-matching (Song & Ermon, 2019; Song et al., 2020b). Denoising diffusion implicit models (DDIMs), an alternative deterministic parameterization which we primarily use in this paper, were established in (Song et al., 2020a). Diffusion models now play an important role not only in image generation (Dhariwal & Nichol, 2021; Rombach et al., 2022; Ramesh et al., 2022), but also video generation (Ho et al., 2022a;b; Blattmann et al., 2023), drug design (Alakhdar et al., 2024), protein folding (Watson et al., 2023), and text generation (Li et al., 2023; 2022).\nThese models are trained to reverse a forward diffusion process that turns the finite training set distribution (a sum of $\\delta$-functions over the training points) into an isotropic Gaussian noise distribution, through a time-dependent family of mixtures of Gaussians centered at shrinking data points. Diffusion models are trained to reverse this process by learning and following a score function that points in gradient directions of increasing probability. But therein lies the puzzle"}, {"title": "2. The ideal score machine only memorizes", "content": "We first discuss why any diffusion model that learns the ideal score function on a finite dataset can only memorize.\nThe key idea behind diffusion models is to reverse a stochastic forward diffusion process that iteratively converts the data distribution $\\pi_0(\\phi)$, where $\\phi \\in \\mathbb{R}^N$ is any data point,"}, {"title": "3. Equivariant and local score machines", "content": "The failure of creativity in the ideal score machine means that it cannot be a good model of what realistic diffusion models do beyond the memorization regime. We therefore seek simple inductive biases that prevent learning the ideal score function $s_t$ in (3) on a finite dataset $D$. By identifying these inductive biases, we hope to obtain a new theory of what diffusion models do when they creatively generate new samples far from the training data."}, {"title": "3.1. The equivariant score (ES) machine", "content": "We first impose equivariance without locality. The MMSE equivariant approximation to $s(t)$ in (3)-(4) is identical in form to the ideal score, except the dataset $D$ is augmented to the orbit of $D$ under the equivariance group $G$, which we denote by $G(D)$. For example, in our case of images, $G(D)$ corresponds to all possible spatial translations of all images in $D$. Explicitly, the MMSE equivariant score is given by (see App. B.3 for a proof)\n$M_t[\\phi](x) = \\frac{1}{1-\\bar{\\alpha}_t}\\sum_{\\phi \\in G(D)} (\\sqrt{\\bar{\\alpha}_t} \\phi(x) - \\phi) W_t(\\phi|\\phi) $ (5)\n$W_t(\\phi) = \\frac{N(\\phi | \\sqrt{\\bar{\\alpha}_t} \\phi, (1-\\bar{\\alpha}_t)I)}{\\sum_{\\phi' \\in G(D)} N(\\phi | \\sqrt{\\bar{\\alpha}_t} \\phi', (1-\\bar{\\alpha}_t)I)}.$ (6)\nReplacing the ideal score $s(t)$ in (1) with (5) yields the equivariant score (ES) machine. While the ideal score machine memorizes the training data (see Sec. 2), the ES machine on images achieves only limited creativity: it can only generate any translate of any training image."}, {"title": "3.2. The local score (LS) machine", "content": "We next impose locality without equivariance. The MMSE $\\Omega$-local approximation to $s(t)$ in (3)-(4) is given by\n$M_t[\\phi](x) = \\sum_{\\phi \\in D} \\frac{(\\sqrt{\\bar{\\alpha}_t} \\phi(x) - \\phi(x))}{1-\\bar{\\alpha}_t} W_t(\\Phi_{\\Omega_x}|\\phi_{\\Omega_x}),$ (7)\n$W_t(\\Phi_{\\Omega_x}|\\phi_{\\Omega_x}) = \\frac{N(\\Phi_{\\Omega_x} | \\sqrt{\\bar{\\alpha}_t} \\phi, (1 - \\bar{\\alpha}_t)I)}{\\sum_{\\phi' \\in D} N(\\Phi_{\\Omega_x} | \\sqrt{\\bar{\\alpha}_t} \\phi', (1 - \\bar{\\alpha}_t)I)}.$ (8)\nEach term in the local $M_t[\\phi](x)$ in (7) is identical to each term in $s(t)$ in (3), yielding a force pulling the pixel value $\\phi(x)$ towards a shrunken training set pixel value $\\sqrt{\\bar{\\alpha}_t} \\phi(x)$ as before, except for the important change that the global posterior belief $W_t(\\phi|\\phi)$ in (3)-(4), that is the same for all pixels $x$, is now replaced with a local $x$-dependent belief $W_t(\\Phi_{\\Omega_x}|\\phi_{\\Omega_x})$ in (7)-(8). $W_t(\\Phi_{\\Omega_x}|\\phi_{\\Omega_x})$ is the posterior probability that a sample image $\\phi$ under the forward process at time $t$ originated from a training image $\\phi$ at time 0, conditioned on the only information the model $M_t[\\phi](x)$ can"}, {"title": "3.3. The equivariant local score (ELS) machine", "content": "Further constraining the LS machine with equivariance leads to the ELS machine in which any local image patch at any pixel location $x$ can now flow towards any local training set image patch drawn from any location $x'$ not necessarily equal to $x$, as in the LS machine. This is the local analog of how the IS machine can only generate training set images, but the equivariance constrained ES machine can generate training set images globally translated to any other location.\nTo formally express this result, assume all local neighborhoods $\\Omega_x$ for different $x$ have the same shape $\\Omega$. For concreteness, one can think of $\\Omega$ as a $P \\times P$ square patch of pixels for $P$ odd, with $x$ centered at location $x$. Then let $\\mathbb{P}_{\\Omega}(D)$ denote the set of all possible $\\Omega$ shaped local training image patches drawn from any training image centered at any location. An element $\\varphi \\in \\mathbb{P}_{\\Omega}(D)$ now lives in $\\mathbb{R}^{P\\times P \\times C}$ and denotes the pixel values of some local $\\Omega$-shaped training image patch centered at some location. Now the optimal MMSE approximation to the ideal score in (3), under both equivariance and locality constraints is (App. B):\n$M_t[\\phi](x) = \\sum_{\\varphi \\in \\mathbb{P}_{\\Omega}(D)} \\frac{(\\sqrt{\\bar{\\alpha}_t} \\varphi(0) - \\phi(x))}{1-\\bar{\\alpha}_t} W_t(\\varphi | \\phi, x)$ (9)\n$W_t(\\varphi,x) = \\frac{N(\\Phi_{\\Omega} | \\sqrt{\\alpha_t} \\varphi, (1 - \\bar{\\alpha}_t)I)}{\\sum_{\\varphi' \\in \\mathbb{P}_\\Omega(D)} N(\\Phi_{\\Omega} | \\sqrt{\\alpha_t} \\varphi', (1 - \\bar{\\alpha}_t)I)}$ (10)\nWe note that (9)-(10) for the ELS machine is identical to (7)-(8) for the LS machine except that: (1) the sum over local training set patches in (9)-(10) in determining the flow $M_t[\\phi](x)$ for pixel $\\phi(x)$ is no longer restricted to training patches centered at the same location as $x$; and (2) each pixel $x$ must now track a larger posterior belief state $W_t(\\varphi|\\phi,x)$ in (10) about which local training set patch at any location $x'$ was the origin of $\\Phi_{\\Omega_x}$, as opposed to the smaller belief state $W_t(\\Phi_{\\Omega_x}|\\phi_{\\Omega_x})$ in (8) about which local training set patch at the same location $x$ was the origin of $\\Phi_{\\Omega_x}$. In essence, in the Bayesian guessing game interpretation, equivariance removes each pixel's knowledge of its location $x$, so to guess the origin of its local image patch $\\Phi_{\\Omega_x}$, it must guess both the training image and the location in the training image that it came from under the forward process. This guess then informs the reverse flow.\nTaken together, the ELS machine can creatively generate exponentially many novel images by mixing and matching local training set patches from any location and placing elements of them at any location in the generated image. We call this a patch mosaic model of creativity."}, {"title": "3.4. Breaking equivariance through boundaries", "content": "Due to the common practice of zero padding images at boundaries, CNNs actually break exact translational equivariance. We can modify our ELS machine to handle this broken equivariance (see App. B.2 for details). The key idea is that breaking translation equivariance restores to each pixel some knowledge of its location within the image. For example, if the local image patch $\\Phi_{\\Omega_x}$ around pixel location $x$ contains many 0 values, then the pixel can use these to infer its location with respect to the boundary, and use this knowledge in the Bayesian guessing game that determines the reverse flow. In essence, with additional conditioning about its relation to the boundary, $\\Phi_{\\Omega_x}$ should only flow to training image patches that are consistent with the observed amount and location of zero-padding. For example, interior, edge, and corner image patches only flow to interior, edge and corner training image patches with the same boundary overlap (Fig. 7). This is a partial case of complete equivariance breaking in the LS machine, in which pixels know their exact location $x$, and the local image patch $\\Phi_{\\Omega_x}$, only flows to training image patches at the same location $x$ (Fig.2b)."}, {"title": "4. A theory of creativity after convergence", "content": "It is clear that the reverse flow from Gaussian noise $\\phi_T$ to final sample $\\phi_0$ in the ideal score machine converges to a single training set image. But what do the LS, ELS or boundary broken ELS machines converge to at the end of the reverse process if they creatively generate novel samples far from the training data? We answer this question by proving a theorem that characterizes the converged samples"}, {"title": "5. Tests of the theory on trained models", "content": "We next test our theory on two CNN-based architectures, a standard UNet (Ronneberger et al., 2015) and a ResNet (He et al., 2016) trained on 3 datasets, MNIST, FashionMNIST, and CIFAR10 (see App. C.1 for details of architectures and training). We restrict our attention to these simple datasets because our theory is for CNN-based diffusion models only, and more complex diffusion models with attention and latent spaces are required to model more complex datasets."}, {"title": "5.1. Coarse-to-fine time dependent spatial locality scales", "content": "To compare our theory of ELS and LS machines with experiments, we must first choose a locality scale for the size of the $P \\times P$ local patch. We therefore measure it in the trained UNet and ResNet and find, importantly, that it changes from large to small scales as time passes from early (large $t$) to late (small $t$) in the reverse flow (Fig. 4a). We therefore promote the spatial size of the $P \\times P$ locality window in our ELS and LS machines to a dynamic variable which we calibrate to the UNet and ResNet (Fig. 4bc). See App. C.2."}, {"title": "5.2. Theory predicts trained outputs case-by-case", "content": "We first compare the outputs of the scale-calibrated boundary broken-ELS machine to the outputs of the ResNet and the UNet on a case-by-case basis for the same initial noise samples or to both the theory and the ResNet or UNet, and we find an excellent match (Fig. 5ab). Indeed we find a remarkable and uniform quantitative agreement between the CNN outputs and ELS machine outputs. For ResNets, we find median $r^2$ values between theory and experiment of 0.94 on MNIST, 0.90 on FashionMNIST, and 0.90 on CIFAR10. For UNets, we find median $r^2$ values of 0.84 on MNIST, 0.91 on FashionMNIST, and 0.82 on CIFAR10 (see Fig. 8 for the full distribution of $r^2$ values). To our knowledge, this is the first time an analytic theory has explained the creative outputs of a trained neural network generative model to this level of accuracy. Importantly, the ELS machine explains all trained outputs far better than"}, {"title": "5.3. Boundary driven anchoring of diffusion models", "content": "We also trained circularly padded ResNets on MNIST and CIFAR10, and found a good match between the non-boundary broken ELS machine and experiment (Figs. 9, 17 and 18). Interestingly, in both theory and experiment for MNIST, circular padding yields more texture-like outputs and less localized digit-like outputs, indicating the fundamental importance of boundaries in anchoring diffusion models, for MNIST at least (compare Fig. 17 and Fig. 11)."}, {"title": "5.4. Spatial inconsistencies from excess late-time locality", "content": "Diffusion models notoriously generate spatially inconsistent images at fine spatial scales, e.g. incorrect numbers of fingers and limbs. Indeed, these inconsistencies are considered a tell-tale sign of AI-generated images (Bird & Lotfi, 2024; Shen et al., 2024; Lin et al., 2024). Our trained models on FashionMNIST also generate such inconsistencies, e.g. pants with too many or too few legs, shoes with more than one toe, and shirts with an incorrect number of arms. Remarkably, our theory, since it matches trained model outputs on a case by case basis, also reproduces these inconsistencies (Fig. 5c). Since our theory is completely mechanistically interpretable, it provides a clear explanation for the origin of these inconsistencies in terms of excessive locality at late stages of the reverse flow. The late-time ($t < 0.3$) locality for all models is less than about 5 pixels (Fig. 4b). When the locality scale is of this small order, different parts of the image more than a few pixels away must decide whether to develop into e.g. an arm or a pant leg without knowing the total number of limbs in the image; this process frequently results in incorrect numbers of total limbs."}, {"title": "6. The relation between theory and attention", "content": "While the local theory explains the outputs of CNN-based diffusion models on a case by case basis with high accuracy, many diffusion models also include highly non-local self-attention (SA) layers. For example (Ho et al., 2020)) added SA layers to a UNet (which we call a UNet-SA architecture). The non-locality of SA strongly violates the assumptions of our local theory. This violation raises an important question: do the predictions of our local theory bear any resemblance at all to the non-local outputs of trained UNet+SA models?\nTo address this question, we compare our existing ELS machine theory with the outputs of a publicly available UNet+SA model pretrained on CIFAR10. (Sehwag, 2024). Strikingly, our ELS model, with no modification whatsoever, predicts the UNet+SA outputs on a case-by-case basis with a median of $r^2 \\sim 0.75$ on 100 sample images. This is substantially higher than the median $r^2 \\sim 0.47$ of an IS machine baseline on the same images (see Fig. 10 for the entire distribution of $r^2$ values).\nQualitatively, the outputs of the UNet+SA model fall into three rough classes in which the UNet+SA produces: (1) a semantically incoherent image which nevertheless strongly resemblances the prediction of the ELS machine (Fig. 19a); (2) a semantically coherent image which has some quantitative correlation with, but little qualitative resemblance to, the ELS machine prediction (Fig. 19b); and (3) a semantically coherent image that also has a strong resemblance to the less semantically coherent ELS machine outputs (Fig. 6)."}, {"title": "A. Mathematical Preliminaries", "content": null}, {"title": "A.1. Notation conventions", "content": "In what follows, we use the following notation:\n\u2022 $D$ will represent the training set.\n\u2022 $\\varphi \\in \\mathbb{R}^N$ will represent an example from the training set. For images of size $L$ pixels by $L$ pixels by $C$ channels, we have $N = L \\times L \\times C$.\n\u2022 $\\phi$ will represent any arbitrary image (or other data) that we are plugging into the score function/diffusion model.\n\u2022 $x$ represents a pixel location in an image.\n\u2022 For image data, $\\phi(x)$ and $\\varphi(x)$ will represent the pixel values of the images $\\phi$ and $\\varphi$ at pixel location $x$; both are elements of $\\mathbb{R}^C$.\n\u2022 $M[\\phi] : \\mathbb{R}^N \\rightarrow \\mathbb{R}^N$ represents a model that takes in an image $\\phi$ and produces a new image (e.g. an estimate of the score function). We will denote by $M[\\phi](x) \\in \\mathbb{R}^C$ the value of the outputs of this model, given an input $\\phi$, at the pixel location $x$.\n\u2022 $\\Phi_{\\Omega_x}$ and $\\varphi_{\\Omega_x}$ will represent the restriction of images $\\phi$ and $\\varphi$ to a neighborhood $\\Omega_x$ around a pixel $x$. We usually take $\\Omega_x$ to be a square patch of size $P \\times P$, with $P$ odd, containing pixel $x$ at the center. In this case, $\\Phi_{\\Omega_x}$ and $\\varphi_{\\Omega_x}$ are vectors in $\\mathbb{R}^{P \\times P \\times C}$. However, the theoretical framework supports arbitrary assignments from $x \\rightarrow \\Omega_x$.\n\u2022 For a square image patch $\\varphi$ with an odd-dimension side length, the value $\\varphi(0) \\in \\mathbb{R}^C$ indicates the pixel at the center of the patch.\n\u2022 $\\mathbb{P}_\\Omega(D)$ will denote the set of all $\\Omega$-shaped patches drawn from elements of $D$.\n\u2022 $N(x|\\mu, \\Sigma)$ represents the PDF of the normal distribution with mean $\\mu$ and covariance $\\Sigma$. We also use the short-hand $N(\\mu, \\Sigma)$ when we do not need to refer to the name of a specific random variable."}, {"title": "A.2. Stochastic differential equations (SDEs) and Probability Flow", "content": "In probabilistic modeling, we are often confronted with the problem of sampling from a data distribution whose exact form we do not have access to, or whose form makes direct sampling difficult. Diffusion models are an approach to sampling from such distributions by learning a time-inhomogenous differential equation that transports samples from a simple Gaussian distribution to the more complex distribution of interest.\nMore formally, consider a time-dependent (It\u00f4) stochastic differential equation, given as follows:\n$d\\phi_t = f_t(\\phi_t) dt + g_t dW_t.$ (11)\nHere $W_t$ is a standard Wiener process and $dW_t$ is its differential. We call this stochastic process the 'forward' process. It starts from the data distribution $\\pi_0(\\phi)$ and induces a flow on probability distributions $\\pi_t(\\phi)$ for $t \\geq 0$ described by associated Fokker-Planck equation:\n$\\frac{\\partial \\pi_t(\\phi)}{\\partial t} = -\\nabla \\cdot (f_t(\\phi)\\pi_t(\\phi)) + \\frac{1}{2} \\nabla^2 (g_t^2 \\pi_t(\\phi)).$ (12)\nWe will imagine that our forward process is constructed so that as $t \\rightarrow \\infty$ (or as $t \\rightarrow T$ for some finite time $T$), $\\pi_T$ converges to some tractable $\\pi_\\infty$, typically a Gaussian with finite variance.\nThe idea underpinning diffusion models (or, more technically, DDIMs, the deterministic variant of diffusion models considered for the most part in this paper) is to look for a deterministic, time-dependent vector field $v_t(\\phi)$ that induces the same flow on distributions as (12). Then one can simply reverse this flow to sample from $\\pi_0(t)$ by first sampling from the simple distribution $\\phi_T \\sim \\pi_T$, then evolving the sample deterministically backwards in time from $t = T$ to $t = 0$ under the ODE\n$\\frac{d \\phi_t}{dt} = v_t(\\phi_t).$ (13)"}, {"title": "A.3. Diffusion models", "content": "The most common choice of forward process (11) is an inhomogenous Ornstein-Uhlenbeck (OU) process process of the following form:\n$d \\phi_t = -\\gamma_t \\phi_t + \\sqrt{2 \\gamma_t} dW_t$ (18)\nfor which the probability flow is given by\n$v_t(\\phi) = -\\gamma_t (\\phi + \\nabla log \\pi_t(\\phi)).$ (19)\nThe reason for this choice is that the finite-time marginals $\\pi_t$ for this distribution can be sampled from tractably. We can generate samples $\\phi_t \\sim \\pi_t$ by computing the following linear linear combination:\n$\\phi_t = \\sqrt{\\bar{\\alpha}_t} \\phi_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\eta_t$ (20)\nwith $\\phi_0 \\sim \\pi_0$ a sample from the target distribution and $\\eta_t \\sim N(0, I)$ a vector of isotropic Gaussian noise. The values of $\\bar{\\alpha}_t$ depend on the choice of $\\gamma_t$ via the following formula:\n$\\bar{\\alpha}_t = exp(-2 \\int_0^t \\gamma_t dt).$ (21)\nIn practice, the values $\\bar{\\alpha}_t$ are typically chosen first and $\\gamma_t$ is then specified implicitly by this choice. The choice of $\\bar{\\alpha}_t$ is known as the 'noise schedule' for a diffusion model; typically, we choose $\\bar{\\alpha}_0 = 1$ (so that $t = 0$ corresponds to uncorrupted sample) and $\\bar{\\alpha}_T = 0$ for some large but finite value of $T$ (so that the entire reverse process can take place in finite time). At a distributional level, the solution of (12) for this process is given by\n$\\pi_t(\\phi) = \\int \\pi_0(\\phi_0) N(\\phi | \\sqrt{\\bar{\\alpha}_t} \\phi_0, (1 - \\bar{\\alpha}_t) I) d \\phi_0.$\nThe score function for $\\pi_t$ can then be obtained analytically in terms of $\\pi_0$:\n$s_t(\\phi) = \\frac{1}{1 - \\bar{\\alpha}_t} \\int (\\phi - \\sqrt{\\bar{\\alpha}_t} \\phi_0) \\frac{\\pi_0(\\phi_0) N(\\phi | \\sqrt{\\bar{\\alpha}_t} \\phi_0, (1 - \\bar{\\alpha}_t) I)}{\\pi_t(\\phi)} d \\phi_0 = \\frac{1}{1 - \\bar{\\alpha}_t} \\int (\\phi - \\sqrt{\\bar{\\alpha}_t} \\phi_0) P(\\phi_0 | \\phi) d \\phi_0.$ (23)"}, {"title": "A.4. The empirical score function", "content": "In practice, we never have direct access to the data distribution $\\pi_0$ that we are attempting to sample from; we only have access to the discrete empirical prior defined by a particular training set $D$:\n$\\pi_0(\\phi) = \\frac{1}{|D|} \\sum_{\\varphi \\in D} \\delta (\\phi - \\varphi).$ (28)\nAt finite time $t$, the empirical distribution of noised training examples is simply a mixture of Gaussians centered at the (rescaled) training data points:\n$\\pi_t(\\phi) = \\frac{1}{|D|} \\sum_{\\varphi \\in D} N(\\phi | \\sqrt{\\bar{\\alpha}_t} \\varphi, (1 - \\bar{\\alpha}_t) I).$ (29)\nThe score function (23) for this distribution is then simply given by\n$s_t(\\phi) = \\frac{1}{1 - \\bar{\\alpha}_t} \\sum_{\\varphi \\in D} (\\varphi - \\sqrt{\\bar{\\alpha}_t} \\varphi) W_t(\\phi | \\varphi),$ (30)\n$W_t(\\phi) = \\frac{N(\\phi | \\sqrt{\\bar{\\alpha}_t} \\varphi, (1 - \\bar{\\alpha}_t) I)}{\\sum_{\\varphi' \\in D} N(\\phi | \\sqrt{\\bar{\\alpha}_t} \\varphi', (1 - \\bar{\\alpha}_t) I)}.$ (31)\nIntuitively, this corresponds to computing the conditional average over the added noise, by averaging the proposed noise vectors $\\eta_t \\propto (\\phi - \\sqrt{\\bar{\\alpha}_t} \\varphi)$ between our observed example $\\phi$ and each training example $\\varphi$, weighted by the probability $W(\\varphi|\\phi)$ of being the training example that $\\phi$ originated from. This probability is in turn computed essentially by Bayes theorem: the probability of starting from a training example $\\varphi$, given the observed $\\phi$, is given by the likelihood of generating the noise needed to go from $\\varphi$ to $\\phi$, divided by the likelihood of going from $\\varphi'$ to $\\phi$ for all possible training examples $\\varphi'$. Appealingly, the weights $W(\\varphi|\\phi)$ are given by computing a simple soft-max over a simple quadratic loss function $-\\frac{1}{2(1-\\bar{\\alpha}_t)} ||\\phi - \\sqrt{\\bar{\\alpha}_t} \\varphi ||^2$ for every point in the training set.\nIt should be emphasized at this point that the ideal score function is not representative of real diffusion models. Primarily: it always memorizes the training data. More importantly in practice, this memorization property becomes manifest very early in the reverse process for high dimensional data, due to the typically large separation between training points in Euclidean space. This is a manifestation of the curse of dimensionality\u2013 it would require an amount of data exponential in the dimension to provide sufficiently good coverage of the underlying space for the ideal empirical score function to well-approximate the true ideal score function over all inputs over all times.\nThe failure of the ideal score function as a model for realistic diffusion models suggests that we should try to understand the particular manner in which they fail to optimally solve the task that they are trained on. In particular, we are motivated to look for the implicit and explicit biases and constraints that prevent these models from learning the ideal score function, and then understand what they do instead under these limitations."}, {"title": "B. Formalism", "content": null}, {"title": "B.1. Optimal local translationally equivariant score matching", "content": "Fully translationally equivariant local models $M_t$ can be written in the following way:\n$M_t[\\phi", "f[\\Phi_{\\Omega_x}": 32, "objective": "n$\\mathcal{L"}, "sum_x \\mathbb{E}_{\\phi \\sim \\pi_t} [|| f[\\Phi_{\\Omega_x}"], "form": "n$f(\\Phi) \\sum_x \\int \\pi_t(\\phi) \\delta(\\phi_{\\Omega_x} = \\Phi) = \\sum_x \\int \\delta(\\phi_{\\Omega_x} - \\Phi) \\pi_t(\\phi) s_t[\\phi](x) d\\phi$\n$= \\sum_x \\int \\nabla_{\\phi(x)} \\delta(\\phi_{\\Omega_x} - \\Phi) \\pi_t(\\phi) d\\phi$\n$= \\sum_x \\Phi(0) \\pi_t(\\phi_{\\Omega_x} = \\Phi)$\nHere $\\Phi(0) \\in \\mathbb{R}^C$ is the pixel value in the center of the patch $\\Phi$. $\\pi_t(\\phi_{\\Omega_x} = \\Phi)$ indicates the marginal probability under the distribution $\\pi_t$ that the patch $\\phi_{\\Omega_x}$ equals the target patch $\\Phi$. The distribution $\\sum_x \\pi_t(\\phi_{\\Omega_x} = \\Phi)$ is then proportional to the marginal distribution that a randomly-selected $\\Omega$-shaped-patch in the image $\\phi$ equals $\\Phi$. Dividing through by this marginal, we obtain\n$f(\\Phi) = \\nabla_{\\phi(0)} log \\sum_x \\pi_t(\\phi_{\\Omega_x} = \\Phi)$ (36)\ni.e. we find that $f(\\Phi)$ is simply the score function of the modified marginal density $\\sum_x \\pi_t(\\phi_{\\Omega_x} = \\Phi)$. Since $\\pi_t(\\phi)$ is a mixture of Gaussians, the marginal $\\pi_t(\\phi_{\\Omega_x} = \\Phi)$ can be obtained simply and is given by\n$\\pi_t(\\phi_{\\Omega_x} = \\Phi) = \\sum_{\\varphi \\in D} N(\\Phi | \\sqrt{\\bar{\\alpha}_t} \\varphi, (1 - \\bar{\\alpha}_t) I).$ (37)\nSumming over $x$ gives us\n$\\sum_x \\pi_t(\\phi_{\\Omega_x} = \\Phi) = \\sum_{\\varphi \\in \\"}