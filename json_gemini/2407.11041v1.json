{"title": "Integer-only Quantized Transformers for Embedded FPGA-based Time-series Forecasting in AIoT", "authors": ["Tianheng Ling", "Chao Qian", "Gregor Schiele"], "abstract": "This paper presents the design of a hardware accelerator for Transformers, optimized for on-device time-series forecasting in AIoT systems. It integrates integer-only quantization and Quantization-Aware Training with optimized hardware designs to realize 6-bit and 4-bit quantized Transformer models, which achieved precision comparable to 8-bit quantized models from related research. Utilizing a complete implementation on an embedded FPGA (Xilinx Spartan-7 XC7S15), we examine the feasibility of deploying Transformer models on embedded IoT devices. This includes a thorough analysis of achievable precision, resource utilization, timing, power, and energy consumption for on-device inference. Our results indicate that while sufficient performance can be attained, the optimization process is not trivial. For instance, reducing the quantization bitwidth does not consistently result in decreased latency or energy consumption, underscoring the necessity of systematically exploring various optimization combinations. Compared to an 8-bit quantized Transformer model in related studies, our 4-bit quantized Transformer model increases test loss by only 0.63%, operates up to 132.33x faster, and consumes 48.19\u00d7 less energy.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of Artificial Intelligence with Internet of Things (IoT) devices, commonly referred to as AIoT, is revolutionizing interaction mechanisms with environments, driving innovative solutions in domains such as smart cities and smart homes [1]. In these sectors, deploying Deep Learning (DL) models on IoT devices to process sensor data locally offers significant benefits, including reduced data transmission costs and greater independence from network conditions [2].\nAmong DL models, Transformer-based architectures excel in effectively handling long data sequences and capturing global dependencies in fields such as Natural Language Processing (NLP) [3], Computer Vision (CV) [4], and Time-series (TS) analysis [5]. Despite efforts to simplify these models [6], optimized Transformer models are still challenging to deploy on IoT devices due to limited resources and processing power.\nTo address these challenges, this study aims to refine Transformer models for compact IoT devices while maintaining precision. We adopt a heterogeneous architecture by leveraging embedded Field Programmable Gate Arrays (FPGAs) as hardware accelerators for model inference, specifically targeting time-series forecasting tasks."}, {"title": "II. FPGA-FRIENDLY TRANSFORMER", "content": "This section introduces our proposed FPGA-friendly Transformer model for single-step ahead time-series forecasting, adapted from prior work [7], [8]. As illustrated in Figure 1, the model comprises an input module, an encoder layer, and an output module. It processes the input X, a sequence of n data points, each with m dimensions, and is suitable for both univariate and multivariate time-series."}, {"title": "A. Parameters Simplification", "content": "To simplify the architecture and reduce training complexity, we standardized key dimensions. As detailed in Table I, we set the dimensions of the query (Q), key (K), value (V), and output vectors of the Multi-Head Self-Attention (MHA) module to $d_{model}$. Based on prior experience, the Feedforward Network (FFN) module dimension is set to four times $d_{model}$. Furthermore, we reduced the head count h in the MHA to 1. Thus, the total number of model parameters is calculated as shown in Equation 1.\n$Params_{Total} = 12d_{model} + (11 + m + 4n) \\times d_{model} + 1$                                                                                     (1)"}, {"title": "B. Encoder Enhancements", "content": "To optimize the encoder layer, we focused on two key enhancements:\n1) Scaling Integration: In conventional Scaled Dot-product Attention (SDA) within the MHA module (See Equation 2), a scaling factor is applied after matrix multiplication (MatMulscore) between Q and the transpose of K ($K^{T}$). This scaling, typically dividing by $\\sqrt{d_{model}/h}$, ensures stable gradients during training by keeping the Softmax function within appropriate gradient regions. We directly integrated this scaling operation into the MatMulscore (as detailed in Section IV), thereby eliminating additional computational overhead.\n$Attention(Q, K, V) = Softmax(\\frac{QK^{T}}{\\sqrt{d_{model}/h}})V$                                                                            (2)\n2) Batch Normalization: The original Transformer model proposed in [7], [8] utilizes Layer Normalization (LN), which requires the computation of mean and standard deviation during inference. This computation, involving division and square root operations, is computationally expensive for embedded FPGAs. To address this, we replaced LN with Batch Normalization (BN) in our Transformer model, leveraging BN\u2019s ability to pre-compute statistics across entire batches during training, thus reducing computational overhead compared to LN\u2019s per-feature real-time computation [9]. Our empirical findings revealed that replacing LN with BN improves precision by 2.78% and 15.86% on the selected datasets, respectively, without compromising model performance."}, {"title": "III. INTEGER-ONLY QUANTIZATION", "content": "In addition to optimizing the model architecture, we utilize integer-only quantization to reduce data complexity, which is crucial for efficient deployment on embedded FPGAs. As detailed in [10], integer-only quantization maps continuous real numbers from the domain R to discrete equivalents within a finite set Q. This process converts a tensor X to its quantized counterpart $X_{q}$ using a scale factor S and a zero point Z. The scale factor S is a floating-point value, while the zero point Z is an integer, which is crucial for rounding X to the nearest integers and clamping the results within the range of b-bit signed integers ($-2^{b-1},2^{b-1} \u2013 1$) as described in Equation 3. De-quantization reverts $X_{q}$ to an approximate real-valued tensor $X\u2019$, using the same quantization parameters S and Z, as illustrated in Equation 4.\n$X \\approx X_{q} = clamp(round(\\frac{X}{S}) + Z, -2^{b-1}, 2^{b-1} \u2013 1)$                                                                                                    (3)\n$X_{q} \\rightarrow X\u2019 = S \\cdot (X_{q} \u2013 Z)$                                                                                                                                    (4)\nThe quantization parameters S and Z are dynamically determined during QAT to adapt to the statistical distribution of each tensor [11]. This customization enhances model precision with lower-bit quantization compared to Post-training Quantization. Specifically, the scale factor S and the zero point Z are computed based on the observed minimum $\\alpha$ and maximum $\\beta$ values of the tensor, as detailed in Equations 5 and 6, ensuring accurate representation of the original data distribution.\n$S = \\frac{\\alpha \u2013 \\beta}{2^{b} -1}$                                                                                                                                                                                 (5)"}, {"title": "IV. SOFTWARE-HARDWARE CO-DESIGN", "content": "A software-hardware co-design is essential for deploying the Transformer model on resource-constrained FPGAs. Our approach merges integer-only quantization with custom hardware optimizations at the register transfer level (RTL), creating VHDL templates that enhance computational efficiency and reduce resource utilization. We adapted the linear layer and ReLU components from prior work [10] on Multilayer Perceptron to handle multiple input dimensions. By processing these dimensions sequentially rather than duplicating logic circuits, we avoid excessive resource consumption on FPGAs. The following subsections detail the co-design for Transformer-specific operations."}, {"title": "A. Addition Component", "content": "The addition operation is described in Equation 7, where two floating-point inputs, $A^{1}$ and $A^{2}$, are added to produce a floating-point output $A^{3}$. In our Transformer model, there are three addition instances labeled AddPE to AddFFN in Table I. For integer-only addition, as outlined in Section III, $A^{3}$ can be approximated using Equation 8. To obtain the integer output $A_{q}^{3}$, Equation 8 is transformed into Equation 9. The terms $\\frac{S_{A^{1}}}{S_{A^{3}}}$ and $\\frac{S_{A^{2}}}{S_{A^{3}}}$ are the remaining floating-point values, which can be approximated using a precomputed positive integer M by right-shifting by n bits, as shown in Equation 10 (using $\\frac{S_{A^{1}}}{S_{A^{3}}} \\approx 2^{-n} \\cdot M$ as an example). This process, referred to as ApproxMul, ensures that all operations remain within the integer domain. Notably, the AddPE operation in the input module uses precomputed positional information from a look-up table.\n$A^{3} = A^{1} + A^{2}$                                                                                                                                                                        (7)\n$A^{3} \\approx \\frac{S_{A^{1}}}{S_{A^{3}}} \\cdot (A_{q}^{1} \u2013 Z_{A^{1}}) + \\frac{S_{A^{2}}}{S_{A^{3}}} \\cdot (A_{q}^{2} \u2013 Z_{A^{2}})$                                                                            (8)\n$A_{q}^{3} = ((\\frac{S_{A^{1}}}{S_{A^{3}}}) \\cdot (A_{q}^{1} \u2013 Z_{A^{1}}) + (\\frac{S_{A^{2}}}{S_{A^{3}}}) \\cdot (A_{q}^{2} \u2013 Z_{A^{2}})) + Z_{A^{3}}$                                                      (9)\n$\\frac{S_{A^{1}}}{S_{A^{3}}} \\approx 2^{-n} \\cdot M$                                                                                                                                                                          (10)"}, {"title": "B. Matrix Multiplication Component", "content": "The co-design of the matrix multiplication component follows similar principles to the linear layer component but replaces static weights with dynamic inputs and excludes bias terms. This component involves two instances, denoted as MatMulscore and MatMulAttn, in Table I. As mentioned in Section II, MatMulscore computes the dot product between matrices Q and $K^{T}$. We integrate the scaling factor $\\sqrt{d_{k}/h}$ into the quantization scaling factor (see Equation 11), eliminating the need for adjustments at the RTL level and streamlining the hardware implementation process.\n$A_{q}^{3} \\approx \\frac{S_{A^{1}} S_{A^{2}}}{S_{A^{3}}\\sqrt{d_{model}/h}} \\cdot ((A_{q}^{1} \u2013 Z_{A^{1}})(A_{q}^{2} \u2013 Z_{A^{2}})) + Z_{A^{3}}$                                                                                                                      (11)\nWhile matrix transposition of K is easily handled in the PyTorch framework with a call to transpose(), it poses significant challenges on resource-constrained FPGAs due to its high time and memory requirements. To mitigate these costs, we implement an address-mapping mechanism that enables direct data retrieval from the non-transposed input buffer, avoiding extra time and memory for matrix transposition. This component also serves MatMulAttn by deactivating the address mapping block."}, {"title": "C. Softmax Component", "content": "Although many quantization approaches for the Softmax function have been proposed [12]\u2013[14], our experiments revealed that none could meet our precision criteria for regression tasks. Inspired by prior work [15], we adopt a Softmax implementation based on lookup tables for computing exp () for all possible inputs. This approach relies on two lookup tables: NLUT for numerators and DLUT for denominators. As is customary, we constrain the output of the exp () function to the interval (0, 1] by offsetting all potential integer inputs $X_{q}$ by the maximum value, yielding $\\widehat{X_{q}}$. Subsequently, the exponential values E are derived through $E = exp(\\widehat{X_{q}})$. To ensure effective quantization, we introduce a scaling factor $S_{E}$ and a zero point $Z_{E}$ as defined in Equations 12 and 13, accommodating the dynamic range within the constraints of integer precision. The DLUT and NLUT are calculated using Equations 14 and 15, respectively, ensuring bitwidths of 2b for DLUT and 3b for NLUT to maintain sufficient precision. The integer output $A_{q}$ is computed by retrieving the quantized numerators and denominators and performing the division as outlined in Equation 16, where $i, j \\in [1, n]$.\n$S_{E} = \\frac{1}{((2^{(2b-1)} -1)-(-2^{(2b-1)}))/(n^{2} \\cdot h)}$                                                                                                                               (12)\n$Z_{E} = 2^{(2b-1)} \u2013 \\frac{1}{S_{E}}$                                                                                                                                                                                  (13)\n$DLUT(X_{q}) = clamp(round(\\frac{E}{S_{E}}), -2^{2b-1}, 2^{2b-1} \u2013 1)$                                                                                                                 (14)\n$NLUT(X) = clamp(round(\\frac{E}{S_{E} \\cdot S_{A}}), -2^{3b-1}, 2^{3b-1} \u2013 1)$                                                                                                                 (15)\n$A_{q} \\approx \\frac{NLUT(X_{q}(i, j))}{\\sum_{i=1}^{n}(DLUT(X_{q}(i, j)) \u2013 Z_{E})} + Z_{A}$                                                                                                                  (16)"}, {"title": "D. Batch Normalization Component", "content": "BN operates on input X with dimensions (n, $d_{model}$) to normalize the data using Equation 17, where $\\mu_{j}$ and $\\sigma_{j}^{2}$ represent the mean and variance of feature $j \\in [1, d_{model}]$ across all samples in the batch. The scaling factor $\\gamma_{j}$ and offset $\\beta_{j}$ adjust the normalized values, while $\\epsilon$ prevents division by zero. By transforming, Equation 17 can be expressed as Equation 18 with new a scaling factor $\\widehat{\\gamma_{j}}$ (equals to $\\frac{\\gamma_{j}}{\\sqrt{\\sigma_{j}^{2} + \\epsilon}}$), and a new offset $\\widehat{\\beta_{j}}$ (represents $\\beta_{j} \u2013 \\frac{\\gamma_{j} \\cdot \\mu_{j}}{\\sqrt{\\sigma_{j}^{2} + \\epsilon}})$. Following a similar principle adopted in linear layer component, the integer output $A_{q}(i, j)$ can be obtained using Equation 19. Notably, we approximate the offset term $\\frac{S_{ \\widehat{\\beta_{j}} }}{S_{A}} \\cdot \\widehat{\\beta_{j}}$ as $\\frac{S_{ \\widehat{\\beta_{j}} }}{S_{A}} \\cdot \\widehat{\\beta_{j}}$ to streamline the calculation. Thus, the hardware implementation focuses on designing an efficient pipeline to perform element-wise dot products concurrently during data fetching. The newly computed products are then scaled using the ApproxMul operation to obtain the integer output.\n$A(i, j) = \\gamma_{j} \\cdot \\frac{X (i, j) \u2013 \\mu_{j}}{\\sqrt{\\sigma_{j}^{2} + \\epsilon}} + \\beta_{j}$                                                                                                                          (17)\n$A(i, j) = \\widehat{\\gamma_{j}} \\cdot X (i, j) + \\widehat{\\beta_{j}}$                                                                                                                                                 (18)\n$A_{q}(i, j) \\approx \\frac{S_{\\widehat{\\gamma_{j}}}}{S_{A}} \\cdot ((\\widehat{\\gamma_{q}} \u2013 Z_{\\widehat{\\gamma_{j}}}) ((X_{q}(i, j) \u2013 Z_{X}) + \\widehat{\\beta_{j}}) + Z_{A}$                                                                                                  (19)"}, {"title": "E. Global Average Pooling Component", "content": "Given the input X with dimensions (n, $d_{model}$), the GAP operation computes the average over dimension n, producing the quantized output Aq, as shown in Equation 20. To circumvent division operations and considering the fixed sequence length n, we incorporate the division factor 1/n into the quantization scaling factor. Thus, the FPGA implementation requires only a summation operation followed by an ApproxMul operation.\n$A_{q}(j) = \\frac{S_{X}}{S_{A \\cdot n}} \\cdot \\sum_{i=1}^{n} (X_{q}(i, j) \u2013 Z_{X}) + Z, j \\in [1, d_{model}]$                                                                                  (20)"}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "This section outlines our experimental setup, including dataset details and processing methods. We discuss model precision, resource utilization, inference time, and power and energy consumption on a Spartan-7 XC7S15 FPGA across various model configurations."}, {"title": "A. Datasets and Data Processing", "content": "We used two datasets for our case studies: the PeMS dataset, which captures univariate traffic flow data from 11,160 sensor measurements over four weeks. Each series samples data at 5-minute intervals, yielding 8,064 time points. We selected a single series with sensor index 4192 to facilitate a fair comparison with [16]. The AirU dataset contains multivariate air quality records from 19,380 observations, featuring seven variables with Ozone as the target variable. After removing discontinuous records, the dataset was reduced to 15,258 feature-target pairs. Observations overlapping with the test set period used in [8] were split into 14,427 training samples and 831 testing samples. All data were normalized using the MinMax method to ensure uniformity in training and testing inputs."}, {"title": "B. Experiments Setup", "content": "Each model configuration underwent 50 training sessions consisting of 100 epochs, with early stopping implemented to prevent overfitting. We used a batch size of 256 and the Adam optimizer with standard parameters ($\\beta_{1}$ = 0.9, $\\beta_{2}$ = 0.98, $\\epsilon$ = 10$^{-9}$). The learning rate was initialized at 0.001, with a scheduler having a step size of 3 and a decay factor $\\gamma$ of 0.5 for dynamic adjustment during training. The Mean Squared Loss function guided the training process. Post-training, we applied an inverse transformation to the model\u2019s outputs and normalized target values to compute the Root Mean Square Error (RMSE) on the test data as the evaluation metric.\nTo generate the accelerator, Python scripts were used to translate the trained quantized model by passing its model and quantization parameters to VHDL templates, which resulted in corresponding VHDL files. We then used GHDL simulations to estimate the number of clock cycles required per inference. The accelerator design was synthesized using Vivado, generating comprehensive reports on resource utilization, timing, and power consumption. Finally, the accelerators were validated on real hardware to assess the effectiveness and efficiency of our FPGA implementation."}, {"title": "C. Model Precision Across Different Configurations", "content": "To investigate the impact of model complexity on precision, we conducted experiments on both datasets with varying numbers of input features (m). These experiments also explored different input lengths (n: 6, 12, 18, 24) and embedding dimensions ($d_{model}$: 8, 16, 32, 64), evaluating the model at different precision levels: floating-point numbers (FP32), 8-bit integers (8-bit), 6-bit integers (6-bit), and 4-bit integers (4-bit).\n1) Parameter Count and FP32 Models: The Params column in Table II displays the parameter count of each model. The variance in the number of input features (m) between datasets did not significantly affect the overall parameter count, as m only influences the parameter count of the linear layer $L_{input}$. Increasing the embedding dimension ($d_{model}$) led to a notable rise in model parameters. However, this increase did not consistently improve the precision of FP32 models, indicating that larger $d_{model}$ does not necessarily enhance performance on these datasets. Additionally, increasing the input length (n) did not consistently improve precision, suggesting that incorporating more historical data points does not linearly enhance precision.\nOverall, the optimal configuration for minimizing test RMSE across both datasets was n = 18 and $d_{model}$ = 32. Notably, our FP32 model outperformed benchmarks reported in [18] for the PeMS dataset, demonstrating a 16.06% improvement. Similarly, compared to FP32 results on the AirU dataset documented by Becnel et al. [8] (where n = 24 and $d_{model}$ =64), our model achieved a 3.20% enhancement.\n2) Quantization and Model Precision: Figures 2 and 3 depict the RMSE variation of quantized models compared to FP32 models on the PeMS and AirU datasets, respectively. Smaller $d_{model}$ models exhibited greater sensitivity to quantization across various datasets and bitwidths, as indicated by the taller blue bars in the figures. This suggests that higher-dimensional embeddings better preserve essential information even with reduced numerical precision. However, the impact of changes in n (input length) on model precision and quantization sensitivity remained uncertain."}, {"title": "D. Resource Utilization", "content": "Due to space constraints, this section focuses on the AirU dataset and evaluates resource utilization, including Lookup Tables (LUTs), Block RAM (BRAM), and Digital Signal Processing Slices (DSPs), for various model configurations and quantization bitwidths on the XC7S15 FPGA. As illustrated in Table III, even when quantized to 4-bit, the largest configuration (n = 24 and $d_{model}$ = 64) exceeded the FPGA\u2019s resource capacity, hence could not be deployed (denoted by \u201c-\u201d). Conversely, the smallest configuration (n=6 and $d_{model}$=8) fit easily within the FPGA\u2019s constraints across different quantization bitwidths, with consistent BRAM utilization at 10%. LUTs and DSPs utilization increase with bitwidth, indicating higher logic resource demand with greater numeric precision.\nAs discussed in Section V-C, the configuration with n = 6 and $d_{model}$ = 64 under 8-bit quantization achieved optimal RMSE. According to Table III, this configuration barely fit the FPGA, utilizing at least 89.5% of all types of resources. For 6-bit quantization, the optimal RMSE was attained with the configuration of n=12 and $d_{model}$=64. Although its test RMSE is 7.33% higher than our best 8-bit quantized model, its LUTs utilization is 14.2% lower, while DSPs utilization increases by 5%. Among all 4-bit quantized models, the two most precise ones were too large to be deployed. We chose the model with the third-best RMSE performance (5.474), which has a 56.13% higher RMSE than our best 8-bit quantized model. However, its RMSE is only 0.63% higher than that of the 8-bit quantized model in [8]. This model, with the configuration of n=12 and $d_{model}$=32, consumes only 46.8% LUTs, 40% BRAM, and 65% DSPs. It represents a feasible compromise between resource utilization and precision, making it suitable for resource-constrained applications."}, {"title": "E. Timing Analysis", "content": "Focusing on the three candidate configurations identified in Section V-D, we analyzed the timing performance of these FPGA-deployable models. As detailed in the third column of Table IV, the clock frequency of the 8-bit quantized model was limited to 100 MHz. In contrast, models with lower bitwidths could operate at frequencies up to 25% higher, supporting the expectation that fewer bits expedite computation due to simplified logic. However, decreasing the bitwidth from 6-bit to 4-bit did not further increase the frequency, likely due to reduced DSPs engagement and subsequent increases in logic delay.\nThe fourth column of Table IV presents the number of clock cycles required per inference, which is influenced by model configuration (n, $d_{model}$). Comparing Rows 2 and 3, doubling n resulted in 2.03\u00d7 more clock cycles per inference. Similarly, comparing Rows 3 and 4, halving $d_{model}$ led to 3.46\u00d7 fewer clock cycles per inference. The fifth column of Table IV outlines the model inference time, which depends on the clock frequency and the number of clock cycles. Notably, our 4-bit quantized model demonstrated the shortest inference time, while the 6-bit quantized model exhibited the longest."}, {"title": "F. Power and Energy Consumption", "content": "The power estimates from Vivado are also summarized in Table IV. Interestingly, the 6-bit quantized model exhibited slightly higher power consumption than the 8-bit quantized model, likely due to its increased clock frequency. However, reducing the bitwidth of computation is generally expected to conserve power, a theory validated by the 4-bit quantized model, which demonstrated the lowest total power consumption at 63 mW among the three configurations.\nEnergy consumption, as shown in the last column of Table IV, is calculated based on power usage and inference time. The 4-bit quantized model was the most energy-efficient, owing to its shorter inference time and lower power consumption. In contrast, the 6-bit quantized model underperformed the others, incurring the highest costs in both power and time. Notably, despite the 4-bit quantized model exhibiting a 56.13% higher RMSE than the 8-bit model, it is 2.12 times faster and 2.52\u00d7 more energy-efficient. This efficiency highlights the potential advantages of deploying the 4-bit quantized model on smaller FPGAs, where resource constraints are more pronounced. Conversely, our 8-bit quantized model remains a suitable choice when precision is paramount."}, {"title": "VI. RELATED WORK", "content": "Research on quantizing Transformer-based architectures has been extensive, particularly in the domains of NLP [13] and CV [14]. These models, known for their substantial computational demands, are primarily deployed on cloud-based servers and edge servers equipped with GPUs. However, relevant studies in TS, especially time-series forecasting, remain underexplored. Becnel et al. [8] implemented an 8-bit quantized Transformer on a low-power microcontroller unit (MCU) for time-series forecasting, achieving a power consumption of 23 mW but with an inference time of 176 ms. In contrast, our 4-bit quantized Transformer accelerator on an embedded FPGA achieves up to 132.33\u00d7 faster inference with only a 0.63% increase in test loss. Despite a 2.74\u00d7 increase in power consumption, it consumes 48.19\u00d7 less energy, indicating that integrating an FPGA-based accelerator with the MCU could be beneficial.\nPrevious works [19], [20] have explored implementing Transformer models on FPGAs for time-series applications, but their target platforms are server-grade or edge-grade FPGAs, which are unsuitable for embedding into IoT devices. In contrast, we chose the Xilinx Spartan-7 FPGA as our target platform. Although resource-constrained, it offers a balanced solution in terms of speed and energy efficiency for deploying Transformer models."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this study, we implemented an FPGA-friendly Transformer model using software-hardware co-design to balance model precision, resource utilization, timing, power and energy. We adopted QAT in the PyTorch framework to ensure model precision and conducted low-bit integer-only inference simulations prior to accelerator generation. Furthermore, our approach utilized VHDL templates and automatic generation scripts to facilitate the seamless translation of trained quantized models into FPGA-ready accelerators. Through hardware validation, we confirmed the effectiveness of our approach.\nIn future work, we plan to extend our approach to mixed-precision quantization. Additionally, we aim to optimize the hardware implementation further to improve energy efficiency, thereby advancing the sustainability of Transformer models for time-series forecasting in AIoT systems."}]}