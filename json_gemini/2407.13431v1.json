{"title": "Improving Out-of-Distribution Generalization of Trajectory Prediction for Autonomous Driving via Polynomial Representations", "authors": ["Yue Yao", "Shengchao Yan", "Daniel Goehring", "Wolfram Burgard", "Joerg Reichardt"], "abstract": "Robustness against Out-of-Distribution (OoD) samples is a key performance indicator of a trajectory prediction model. However, the development and ranking of state-of-the-art (SotA) models are driven by their In-Distribution (ID) performance on individual competition datasets. We present an OoD testing protocol that homogenizes datasets and prediction tasks across two large-scale motion datasets. We introduce a novel prediction algorithm based on polynomial representations for agent trajectory and road geometry on both the input and output sides of the model. With a much smaller model size, training effort, and inference time, we reach near SotA performance for ID testing and significantly improve robustness in OoD testing. Within our OoD testing protocol, we further study two augmentation strategies of SotA models and their effects on model generalization. Highlighting the contrast between ID and OoD performance, we suggest adding OoD testing to the evaluation criteria of trajectory prediction models.", "sections": [{"title": "I. INTRODUCTION", "content": "Trajectory prediction is essential for autonomous driving, with robustness being a key factor for practical applications. The development of trajectory prediction models is catalyzed through public datasets and associated competitions, such as Argoverse 1 (A1) [1], Argoverse 2 (A2) [2], Waymo Motion (WO) [3]. These competitions provide a set of standardized metrics and test protocol that scores prediction systems on test data withheld from all competitors. This is intended to ensure two things: objective comparability of results and generalization ability due to the held-out test set.\nAmong the top-performing models based on deep learning, we observe a trend to ever more parameter-rich and expressive models [4], [5], [6] - trained specifically with the data of each competition. This begs the question of whether the stellar performance of these models is due to their ability to adapt to each dataset specifically, i.e., over-fit. Prediction competitions try to guard against over-fitting by scoring models with test samples that are withheld from competitors. However, these test examples still share similarities with the training samples, such as the sensor setup, map representation, post-processing, geographic, and scenario selection biases employed in dataset creation. Consequently, the test scores reported in each competition are examples of In-Distribution (ID) testing.\nFor practical application, the performance of trajectory prediction should be independent of these shared biases in a single dataset and measured by Out-of-Distribution (OoD) testing, e.g., across different datasets. However, the efforts for cross-dataset evaluation are hampered by the inhomogeneous data format and task details among datasets and competitions. Notable distinctions are the different lengths of scenario, observation history, and prediction horizon in each dataset.\nTherefore, in this work, we attempt to homogenize the data formats and prediction tasks to enable OoD testing across two large-scale motion datasets: A2 and WO. Based on our OoD testing, we further explore the possibilities to enhance model robustness against OoD samples.\nOne strategy to improve the robustness of any deep learning model is to increase the amount of training data. Bahari et al. [7] have shown that generalization performance can be improved by augmenting training data with programmatically created variations. While such an approach does increase generalization ability, it further increases training effort and cannot guarantee generalization. Further, ensuring that a data augmentation strategy covers all possible variations is not easy.\nWe follow a complementary strategy and instead of providing more training data, limit the expressiveness of our model by constraining its input and output representation. This comes with the added benefit of a much smaller model size, reduced training effort and faster inference times. At the same time, we retain near state-of-the-art (SotA) prediction performance for ID test cases and show significantly improved robustness in OoD testing. Our key contributions are as follows:\nWe provide a dataset homogenization protocol that enables Out-of-Distribution (OoD) testing of prediction algorithms across different large-scale motion datasets.\nWe study the OoD robustness of two SotA models and explore the effect of their augmentation strategies on In-Distribution (ID) and OoD test results.\nWe propose an efficient multi-modal predictor baseline with competitive ID performance and superior OoD robustness by representing trajectories and map features parametrically as polynomials.\nOur paper is organized as follows: We first review the recent trajectory prediction models and their design decisions with respect to data representation and augmentation strategy. Out of the body of related work, we select two benchmark models, Forecast-MAE (FMAE) [8] and QCNet [6], and detail how dataset and competition characteristics have shaped their model designs. Next, we propose our dataset homogenization protocol for OoD testing. In contrast to the benchmark design, we introduce our approach by incorporating constrained parametric representations - polynomials for both inputs and"}, {"title": "II. RELATED WORK AND BENCHMARK MODELS", "content": "The formats of individual datasets and competition rules greatly influence the design decisions of the models proposed in the literature. Models based on deep learning typically opt for a direct ingestion of the data as given. Recent studies employ sequence-based representation, e.g., sequences of data points, as the model's input and output [8], [6], [10], [11]. This representation aligns the format of measurements in datasets and efficiently captures various information, e.g., agent trajectories and road geometries. The downside of this representation is the high redundancy and variance. The presence of measurement noise and outliers in datasets may lead to physically infeasible predictions. Additionally, the computational requirement of this approach scales with the length and temporal/spatial resolution of trajectories and road geometries.\nSome previous works explored the possibility of using polynomial representations for predictions [12], [13]. Su et al. [13] highlight the temporal continuity, i.e., the ability to provide arbitrary temporal resolution, of this representation. Reichardt [14] argues for using polynomial representations to integrate trajectory tracking and prediction into a filtering problem. Polynomial representations restrict the kind of trajectories that can be represented and introduce bias into prediction systems. This limited flexibility is generally associated with greater computational efficiency, smaller model capacity and hence better generalization. However, representing the inputs with polynomials has not been extensively researched and deployed in recent works. In a recent study, we showed that polynomials of moderate degree can represent real-world trajectories with a high degree of accuracy [15]. This result motivates the use of polynomial representations in our architecture."}, {"title": "B. Data Augmentation", "content": "Competitions typically designate one or more agents in a scenario as focal agent and only score predictions for focal agents. However, training the model exclusively with the focal agent's behavior fails to exploit all available data. To address this, predicting the future motion of non-focal agents is a typical augmentation strategy for training. As there are many more non-focal agents than focal agents, another important design decision relates to how authors interpret the importance of focal vs. non-focal agent data.\nWe select two open-sourced and thoroughly documented SotA models: Forecast-MAE [8] (FMAE) and QCNet [6], with nearly 1900k and 7600k parameters, respectively. As summarized in Table I, both models employ sequence-based representation but exhibit different strategies in dealing with non-focal agents:\nHeterogeneous Augmentation: FMAE follows the pre-diction competition protocol and prioritizes focal agent prediction. Thus, agent history and map information are computed within the focal agent coordinate frame. Compared to the multi-modal prediction of the focal agent,"}, {"title": "III. OOD TESTING AND DATASET HOMOGENIZATION", "content": "The irrelevant data creation processes and platforms across motion datasets present us with the opportunity to perform OoD testing on truly independent data samples. However, this also comes with the challenges of working around inconsistencies in data formats and prediction tasks between datasets.\nWe propose our OoD testing protocol by training and test-ing models on two different large-scale datasets: A2 and WO. We summarise the characteristics of both datasets in Table II.\nAlgorithmic improvements in generalization ability are more easily identified when training on the smaller of two datasets and testing on the larger ones. Consequently, we train on the training set from A2, comprising 199908 samples. As the test set of the A2 and WO competitions are not accessible, they cannot be included in our homogenization protocol. Thus, for ID testing, we settle for the validation split of A2, and for OoD testing use the validation split of WO. The difference between ID and OoD results demonstrates a model's robustness against OoD test cases.\nThere are multiple notable distinctions between A2 and WO. One obvious distinction is the different scenario lengths: a scenario in A2 is 11 seconds long and consists of 5 seconds of observation history and 6 seconds of future to predict. WO only provides 1.1 seconds of observation history but requires an 8-second prediction horizon. Moreover, A2 designates and scores only one focal agent per scenario, whereas there are up to 8 such focal agents in WO. The focal agent in A2 is never the ego vehicle and remains fully observed throughout the prediction horizon, which is not guaranteed in WO.\nTo facilitate the OoD testing, we homogenize the datasets and adopt the following settings for both datasets and sum-marize them in Table II:\nHistory Length: We set the history length to 5 seconds (50 steps) as in A2.\nPrediction Horizon: We maintain the 6-second prediction horizon for training but only evaluated the first 4.1s of prediction due to the limited recording length in WO.\nMap Information: We exclude boundary information and the label of junction lanes due to the information's absence in WO. Only lane segments and crosswalks are considered map elements in homogenized data due to their availability in both datasets.\nFocal Agent: We take the same focal agent as labeled in A2. From WO, only the first fully observed, non-ego agent in the list of focal agents is chosen. As the list of focal agents is unordered, this corresponds to randomly sampling a single fully observed focal agent.\nFulfilling the focal agent's selection criterion above, we have 24 988 and 42 465 valid samples from A2 and WO validation sets for ID and OoD testing, respectively."}, {"title": "IV. PROPOSED MODEL AND DATA REPRESENTATION", "content": "The key innovation of our model is its choice of repre-sentation for map elements and trajectories on both input"}, {"title": "A. Inputs as Polynomials", "content": "We employ Bernstein polynomials to represent the agent history and map geometry. The parameters of Bernstein polynomials have spatial semantics as control points.\nAgent History: Based on the Akaike Information Criterion (AIC) [17] presented in [15], the 5-second history trajectories of vehicles, cyclists, and pedestrians in A2 and WO are represented optimally with 5-degree polynomials. We also use the 5-degree polynomial for the ego vehicle. We track the control points of agent history with the method proposed in [14] and incorporate the observation noise models proposed in [15].\nMap: Map elements, such as lane segments and cross-walks, are represented with 3-degree polynomials - a degree consistent with OpenDRIVE [18] standards. We fit the sample points of map elements via the total-least-squares method by Borges-Pastva [19]. Lane segments in WO are longer than those in A2, posing a challenge for fitting them with 3-degree polynomials. Therefore, we iteratively split the lane segments by half until the fit error is under 0.1m.\nWe visualized one traffic scenario in A2 with polynomial representations in Figure 3. The polynomial representation only requires 40.8% and 8.7% of data space compared to the sample points provided in A2 and WO, respectively."}, {"title": "B. Outputs as Polynomials", "content": "The 6-second predicted trajectories are formulated as 6-degree polynomials, one degree higher than indicated by AIC in [15]. Instead of predicting the polynomial parameters of future trajectories, our model predicts several future kine-matic vehicle states. By fusing the predicted future states with current tracked states, we recalculate the polynomial parameters and reconstruct the future trajectory. Details are presented in Appendix I."}, {"title": "C. Model", "content": "The pipeline of EP is illustrated in Figure 1. EP employs the popular encoder and decoder architecture and its imple-mentation details are presented in Appendix I.\nFollowing the different augmentation strategies by our benchmark models, we implemented three EP variants: (1) EP-F employs the heterogeneous augmentation of FMAE. (2) EP-Q employs the homogeneous augmentation of QCNet. (3) EP-noAug has no augmentation. Table I summarizes the design choices of the different models under study."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "On the homogenized A2 training data, all models are trained from scratch with hyperparameters reported for A2 competition performance. Due to our dataset homogenization settings, we have to adjust the data pre-processing and corresponding encoding layers in QCNet, e.g., removing the layers for encoding the label of junction lanes. FMAE'S architecture is not affected by our data homogenization. Training settings of EP variants are summarized in Appendix I-C."}, {"title": "B. Metrics", "content": "For ID testing, we use the official benchmark metrics, in-cluding minimum Average Displacement Error ( minADEK ) and minimum Final Displacement Error ( minFDEK ), for evaluation. The metric  minADEK calculates the Euclidean distance in meters between the ground-truth trajectory and the best of K predicted trajectories as an average of all future time steps. Conversely,  minFDEK focuses solely on the prediction error at the final time step, emphasizing long-term performance.\nFor OoD testing, we also propose A minADEK and A minFDEK as the difference of displacement error be-tween ID and OoD testing to measure model robustness. In accordance with standard practice, K is selected as 1 and 6."}, {"title": "C. ID Results with Competition Settings", "content": "Like the other studies, we first present the best ID test results of all models according to the A2 prediction compe-tition protocol in Table III. Benchmark results are from the original authors. For EP, we also include the results when augmenting the data by symmetrizing around the x-axis, i.e. flipping all left turns into right turns and vice versa.\nThough the performance in ID testing on the A2 test set is not the focus of our work, our EP still achieves near state-of-the-art performance with a significantly smaller model size. For instance, with \"flip\" data augmentation, the  minFDE1 of EP-F is only 3.4% higher compared to QCNet. Though EP exhibits a considerable gap compared to QCNet in terms of multi-modal prediction (K=6), it exhibits only less than 10cm additional prediction error in  minADE6 and  minFDE6 compared to FMAE with pre-train. EP has a faster inference time, requiring only 3.9% of QCNet's and 36.9% of FMAE's inference time, which is essential in real-time applications. Another notable point is the better ID performance of EP-F compared to EP-Q. This indicates that heterogeneous augmentation shows more effectiveness than homogeneous augmentation in ID testing when only a single focal agent is scored.\nNext, we examine the impact of dataset homogenization and augmentation. In Figure 4, we can see that the alter-ations due to dataset homogenization, e.g., excluding lane boundaries, have only little effect on model performance. Removing the data augmentation, however, has a small but noticeable negative effect on ID performance for both bench-mark models. These results also show that our retraining works as expected."}, {"title": "D. OoD Testing Results", "content": "The key question now is whether the results achieved in ID testing on A2 will translate to OoD testing on WO. Figure 5 summarizes the results. As discussed, the relative and absolute increase in the prediction metric will serve as our measure of robustness and generalization ability. We will present OoD results from three perspectives: (1) augmenta-tion strategy, (2) data representation, and (3) contrast results between ID and OoD testing.\n1) Augmentation Strategy: We observe that without aug-mentation, i.e., excluding the non-focal agents in the loss function, all models generalize poorly on all metrics, but"}, {"title": "VI. CONCLUSION", "content": "Recent SotA trajectory prediction models have thoroughly optimized their In-Distribution (ID) performance and present outstanding results in test sets evaluated in individual predic-tion competition. However, better ID performance does not automatically guarantee higher Out-of-Distribution (OoD) performance. In fact, performance differences can and do reverse in OoD settings. Our proposed OoD testing protocol enables a fresh perspective for model evaluation in trajectory prediction that we hope the community will adopt. With our protocol, we demonstrate the robustness improvement from homogenous augmentation and prove the benefits of polynomial representation as employed in our EP model. With a much smaller model size and lower inference time, EP achieves near SotA ID performance and exhibits significantly improved OoD robustness compared to benchmark models. This work represents an initial step toward a trajectory prediction model that is capable of generalizing to different datasets and is robust under changes in sensor setup, scenario selection strategy or post-processing of training data. More sophisticated models based on this concept will be devel-oped to improve the performance for both In- and Out-of-Distribution evaluations. Our work analyzes two important factors affecting model robustness: augmentation strategy and data representation. Other factors, such as model size, are not explored and remain to be researched."}, {"title": "A. Model", "content": "1) Input: The control points of A agents in either focal agent or individual coordinate are denoted as  \u03c9an \u2208 RA\u00d72 with n \u2208 {0,1,\u2026\u2026,5}. Following the popular vectorization approach, we compute the vector between control points as  \u03b4n = \u03c9an - \u03c9an\u22121  for capturing the kinematic of agents. To describe the spatial relationship, we employ the last control point  \u03c9a5 and the normalized vector  \u03b8a = norm(\u03b4a5) as the reference position and orientation for agents. The time window of agent appearance is denoted as  T W \u2208 RA\u00d72. Therefore, the features of agents include  Aa = [\u03b4a1 , \u03b4a2 , \u00b7 \u00b7 \u00b7 , \u03b4an ],  P Ia = [\u03c9a5 , cos(\u03b8a5 ), sin(\u03b8a5 )] and TW. The features of M map elements are computed similarly with considering the initial point as the reference, denoting as Am = [\u03b4m1 , \u03b4m2 , \u03b4m3 ] and PIm = [\u03c9m0 , cos(\u03b8m0 ), sin(\u03b8m0 )].\n2) Encoder: As visualized in Figure 6, we employ simple 3-layer MLPs to encode the features of agents and map ele-ments. Semantic attributes, such as agent type and lane type, are encoded with individual embedding layers. Embedded features are added and fused to agent tokens Ta \u2208 RA\u00d7D and map element tokens Tm \u2208 RM\u00d7D , where D denotes the hidden dimension. Multiple attention blocks based on"}, {"title": "B. Training Loss", "content": "We employ  minADE6 as regression loss  lreg,multi and weighted average displacement error, with the weight of predicted probability p, as classification loss  lcls,multi for multi-modal prediction. We calculate average displacement error as regression loss for uni-modal prediction  lreg,uni. Thus, the training losses of EP variants are expressed as:\nLEP \u2212F =  lfocal reg,multi + lfocal cls,multi+\u2211non\u2212focal lreg,uni\nLEP \u2212Q =  lall reg,multi + lall cls,multi\nLEP \u2212noAug =  lfocal reg,multi + lfocal cls,multi     (1)\nwhere sub-indices \u201cfocal,\u201d \u201cnon-focal,\u201d and \u201call\u201d refer to the focal agent, non-focal agents, and all agents, respectively."}]}