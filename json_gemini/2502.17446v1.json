{"title": "DCentNet: Decentralized Multistage Biomedical Signal Classification using Early Exits", "authors": ["Xiaolin Li", "Binhua Huang", "Barry Cardiff", "Deepu John"], "abstract": "This paper presents DCentNet, a novel decentralized multistage signal classification approach for biomedical data obtained from Internet of Things (IoT) wearable sensors, utilizing early exit point (EEP) to improve both energy efficiency and processing speed. Traditionally, IoT sensor data is processed in a centralized manner on a single node, Cloud-native or Edge-native, which comes with several restrictions, such as significant energy consumption on the edge sensor and greater latency. To address these limitations, we propose DCentNet, a decentralized method based on Convolutional Neural Network (CNN) classifiers, where a single CNN model is partitioned into several sub-networks using one or more EEPs. Our method introduces encoder-decoder pairs at EEPs, which serve to compress large feature maps before transferring them to the next sub-network, drastically reducing wireless data transmission and power consumption. When the input can be confidently classified at an EEP, the processing can terminate early without traversing the entire network. To minimize sensor energy consumption and overall complexity, the initial sub-networks can be set up in the fog or on the edge. We also explore different EEP locations and demonstrate that the choice of EEP can be altered to achieve a trade-off between performance and complexity by employing a genetic algorithm approach. DCentNet addresses the limitations of centralized processing in IoT wearable sensor data analysis, offering improved efficiency and performance. The experimental results of electrocardiogram (ECG) classification validate the success of our proposed method. With one EEP, the system saves 94.54% of wireless data transmission and a corresponding 21% decrease in complexity, while the classification accuracy and sensitivity remain almost unaffected and stay at their original levels. When employing two EEPs, the system demonstrates a sensitivity of 98.36% and an accuracy of 97.74%, concurrently leading to a 91.86% reduction in wireless data transmission and a reduction in complexity by 22%. DCentNet is implemented on an ARM Cortex-M4 based microcontroller unit (MCU). In laboratory testing, our approach achieves an average power saving of 73.6% compared to continuous wireless transmission of ECG signals.", "sections": [{"title": "1. Introduction", "content": "Arrhythmia is characterized by an abnormality in the rhythm of the heart, leading to irregular contractions or a rhythm that is either too slow or too fast, consequently impeding the heart's ability to pump blood and diminishing the efficiency of its blood supply. Inadequate blood supply to vital organs and tissues of the body from arrhythmias can culminate in a myriad of grave consequences, such as heart failure, stroke, and myocardial in- farction. ECG investigations are extremely important in the diagnosis of heart arrhythmias. ECG signal analysis, being a prevalent method for detecting arrhythmias, allows for the as- sessment of individual waveforms and characteristics of ECG signals, thereby enabling the determination of heart health sta- tus and identification of potential arrhythmic types. Given the increasing prevalence of wearable health-monitoring devices, the ability to continuously and efficiently monitor ECG signals is crucial for early detection and prevention of cardiovascu- lar events, which are leading causes of mortality worldwide. However, existing systems, which typically rely on centralized model deployment (either entirely on the cloud or on the edge) face challenges in terms of energy efficiency, latency, and com- putational resources, which this research addresses. Conse- quently, proper classification and identification of ECG signals can facilitate the early detection of arrhythmias and facilitate"}, {"title": "2. Related Work", "content": "The classification of biomedical signals has been extensively studied in the literature. With the emergence of wearable de-"}, {"title": "3. DCenNet: Decentralized multistage biomedical signal inferencing", "content": "Cloud-native model deployments are well-suited for achiev- ing high model performance. Biomedical signal inferencing typically involves transmitting biomedical data collected by wear- able devices to the cloud for analysis and classification. How- ever, this process may encounter several challenges: 1) Firstly, network latencies caused by channel conditions/uncertainties while transmitting a large volume of data to the cloud could lead to delays in inferencing and may be unsuitable for use cases with quick turnaround requirements. 2) Secondly, the large volume of biomedical data from multiple users necessi- tates high bandwidth for transmission, leading to network con- gestion and decreased data transfer rates. 3) Finally, the trans- mission of all collected biomedical data for cloud-only classi- fication and analysis results in a substantial energy expenditure at the edge sensor. Such high-energy consumption can reduce the battery life of wearable devices, thereby diminishing their overall usability. Conversely, edge-native model deployments prioritize low latency and reduced power consumption but may encounter obstacles related to lower model performance and re- source constraints. These challenges encompass aspects like model performance, communication bandwidth, sensor power consumption, and the availability of computational and storage resources.\nDeploying DL inferencing processes on a single node (say edge, fog, or cloud) comes with a set of difficulties. To achieve the best of all worlds and address these challenges, we pro- pose a novel approach DCenNet, a decentralized DL multi- stage inferencing model that spans multiple nodes across either the Edge-Cloud or the Edge-Fog-Cloud continuum as shown in Fig. 1. This multistage classifier leverages a cascaded ensemble technique, originally proposed in [27, 28], to generate partial results at each processing stage if the result is reliable allowing for a timely response to abnormal signals while also mitigat- ing the impact of network failures by distributing computation and results across nodes. These partial results can be utilized independently and incrementally aggregated with previous re- sults, leading to improved cumulative performance. The initial sub-network can better fit onto an edge device with limited re- sources because it has fewer layers (lower complexity). The processing of normal signals can be stopped early without go- ing through the entire network because the majority of them are classified by the initial sub-networks. This lowers the complex- ity and latency of inferencing while saving network bandwidth and the energy used by the edge sensor. We test the proposed DCenNet for biomedical signal inferencing by building upon a larger CNN model as illustrated in Fig. 2.\nA major challenge in deploying decentralized neural net- works across multiple nodes is the need to transmit large fea- ture maps between sub-networks, which can lead to significant energy consumption, especially for IoT wearable devices. To mitigate this, we propose integrating an encoder-decoder pair at each EEP to reduce the size of the data transmitted between nodes. The encoder compresses the feature maps before trans- mission, significantly lowering the energy required for wireless communication. Once the data reaches the next sub-network, the decoder reconstructs the feature maps for further process- ing. This approach ensures that only the most essential infor- mation is transmitted, making the system more energy-efficient.\nThe advantage over traditional centralized methods is that in- stead of sending all inputs directly to the cloud without discrim- ination, only the data that cannot be classified at earlier stages is transmitted, as described in Algorithm 1. This selective trans- mission reduces network load, extends device battery life, and facilitates more efficient processing across edge devices.\nGiven the vulnerability of IoT devices to security threats, safeguarding data and ensuring system reliability are critical"}, {"title": "3.1. Single EE\u0420", "content": "For the Edge-Cloud continuum, we extend the network ar- chitecture by incorporating a single additional EEP, as illus- trated in Fig. 3. The EEP can correspond to any one of the directions indicated by each individual dashed arrow shown in Fig. 3. By placing the EEP within the network, we aim to achieve a balance between achieving optimal performance and managing computational complexity demands between edge- native and cloud-native deployments. To achieve this, a series of experiments were conducted to investigate the impact of dif- ferent EEP positions on the overall system performance and complexity. Our objective is to identify the best location for the EEP regarding different targets, considering the trade-off between the system's inferencing performance and its compu- tational resources. By analysing the experimental outcomes, we could determine the best-suited EEP location, that aligns with the requirements and constraints of the Edge-Cloud continuum. The placement of the EEP effectively optimized the model's inferencing process, providing low inferencing latency and ef- ficient resource utilization while maintaining high accuracy in biosignal classification tasks. Assuming the CNN consists of L convolutional layers, in the Edge-Cloud continuum, which in- corporates a single EEP, there can be L \u2013 1 potential positions"}, {"title": "3.2. Two EEPS", "content": "For the Edge-Fog-Cloud continuum, we augmented the ex- isting network architecture by introducing two possible EEPS, as illustrated in Fig. 4. Two EEPs were incorporated to accom- modate the varying demands and characteristics of edge-native,"}, {"title": "3.3. Embedded System Evaluation", "content": "Implementing a model in resource-constrained IoT devices presents several challenges. First, IoT devices typically have limited computational power and memory, which makes it dif- ficult to deploy complex models. The introduction of EEP helps mitigate this by allowing devices to process only smaller por- tions of the model, thereby reducing the computational load. Second, power consumption is a critical concern, especially for battery-operated devices. The use of EEP significantly lowers computational demands, leading to an average energy savings of 73.6% as observed in this study. Third, decentralized sys- tems often face latency issues due to data transmission between nodes. However, the early exit strategy reduces the volume of data transmitted, thereby minimizing delays. Finally, band- width limitations, which are common in IoT environments, are effectively addressed by reducing the amount of data transmit- ted through the use of EEPs.\nBy incorporating the EEP, the average FLOPs required are significantly reduced. Fewer FLOPs mean that the inferencing will require fewer computational resources at the edge, and also wireless transmission and further processing can be avoided"}, {"title": "4. Dataset", "content": "An open database of ambulatory ECG records collected from 48 patients under varied circumstances makes up the MIT-\u0412\u0406\u041d Arrhythmia database [29]. The 48 participants in this dataset, 25 men between the ages of 32 and 89, and 22 women between the ages of 23 and 89, provided ambulatory ECG records. Ap- proximately 60% of the recordings included in this study were obtained from inpatients. The ECG signals were sampled with an 11-bit resolution and sampled at 360 Hz. Each record is about 30 minutes long. We evaluated performance using a sin- gle ECG lead from the MIT-BIH database, with 70%, 15%, and 15% respectively for training, validation, and testing. The first available lead, which is modified limb lead II (MLII), obtained by placing electrodes across the torso, is used in this experi- ment. It is determined that 260 samples centred on the R peak provide sufficient details on the signal shape [30]. Based on the AAMI standards, ECG is mapped into 5 classes, i.e. N, SVEB, VEB, F, Q [31]."}, {"title": "5. Results & Discussion", "content": "There are numerous possibilities for EEP settings, which in- crease as the network size grows. Identifying the best early exit point or combinations of early exit points maximises perfor- mance and minimizes complexity. The objective is to leverage the experimental findings to identify the best EEP or point com- binations that align most closely with the actual requirements. Adjusting the algorithm to meet practical requirements involves assigning weights to performance and complexity based on real- world considerations. Experiments are executed with architec- tures containing either a single EEP or two EEPs, investigating various positional arrangements. Following this, we employed an algorithm to select the most suitable configuration, which best meets the requirements. The system sensitivity evaluates the percentage of true positive cases accurately identified by the system, while the system accuracy indicates the overall rate of correct classifications relative to all predictions; additionally, DtC represents the percentage of data sent to the cloud out of the total data in the test set [32]. The exit rate represents the pro- portion of outcomes exceeding the confidence threshold, and thereby being deemed reliable relative to the overall count. The efficiency rate denotes the ratio between the total FLOPs of the system after the introduction of the EEP and the FLOPs of the original CNN model."}, {"title": "5.1. Single EEP", "content": "Adding EEPs into a large CNN model enables a distributed inferencing system that facilitates the integration of the model into various decentralised practical systems. Through empiri- cal analysis, we achieved a well-balanced solution that ensures optimal performance and resource efficiency, showcasing the adaptability of DCenNet for AI inferencing in the Edge-Cloud continuum.\nFig. 7 illustrates the variations in performance and complex- ity as the confidence threshold changes for different locations of the single EEP. The performance superiority of the final exit point over the initial exit point is a common observation in sys- tems employing a single EEP for inference. However, the corre- lation between the placement of the EEP and the resultant per- formance is not always straightforward. While intuitively, one might expect a later EEP to yield better outcomes, empirical evidence suggests that this isn't always the case. Fig. 7 shows that the system accuracy is higher with EEP 2 compared to ei- ther EEP 3 or EEP 4, and the system sensitivity is consistently higher with EEP 4 than with EEP 5. In systems featuring only one EEP, opting for an earlier exit point undoubtedly reduces the FLOPs required for computation. Conversely, selecting a later exit point does not always lead to enhanced performance metrics. It would appear that delaying the early exit point would result in superior system performance despite the potential in- crease in FLOPs. However, empirical evidence suggests that this isn't consistently valid. This suggests that factors beyond FLOPs, such as the architecture of the model, the nature of the dataset, and the specific task being performed, play crucial roles in determining the optimal placement of exit points. It can be observed from Fig. 7 that if exiting at EEP 2, while ensuring a relatively high level of performance, selecting a threshold of"}, {"title": "5.2. Two EEPS", "content": "We expanded our ECG classification model by incorporat- ing two additional EEPs, thus dividing the original model into three subnetworks. This modification allows for the deploy- ment of the model across three decentralized nodes within the Edge-Fog-Cloud continuum. We conducted experiments on the model with two EEPs, exploring various combinations, and the performance of the proposed system is shown in Fig. 8 at vary- ing confidence thresholds. Exit rate 1 represents the propor-"}, {"title": "5.3. Optimization Method: Genetic Algorithm (GA)", "content": "Achieving high accuracy and sensitivity alongside low com- plexity simultaneously is not feasible, hence it is necessary to identify a trade-off based on the specific requirements at hand. GA is an evolutionary optimization algorithm inspired by the mechanisms of natural selection and genetics, which has gained significant popularity for solving complex optimization prob- lems across various domains [42, 43]. GA is particularly suited for optimizing the placement of EEPs in decentralized systems, as it balances competing goals such as accuracy, sensitivity, and computational complexity. GA starts with a population of po- tential solutions represented as strings of information, shown in Algorithm 2. Through selection, crossover (mixing solutions), and mutation (changing them slightly), new generations of so- lutions are produced. This allows GA to explore and improve solutions over time. The fitness of each candidate solution is evaluated based on an objective function (OF) shown in Eq. (1), guiding the algorithm towards finding optimal or near-optimal solutions. Normalization of each parameter (accuracy, sensitiv- ity, and FLOPs) is essential in this process. Once the weight for each parameter is assigned based on the actual requirements, the GA will return the corresponding exit points and related performance.\n\n$OF = \\frac{w_{acc} \\times accuracy + w_{sen} \\times sensitivity}{-w_{com} \\times FLOPs}$         (1)\n\nwhere, $w_{acc}$, $w_{sen}$, and $w_{com}$ represent the weight of accuracy, sensitivity, and the number of FLOPs for the OF.\nBy assigning varied values to these weights, taking into account specific practical considerations and performance re- quirements, we can effectively determine the optimal EEP or combinations of EEPs. This method provides a flexible ap- proach for balancing competing objectives, and its adaptability is critical for the system, where wearable devices must meet performance constraints while maintaining energy efficiency. When all weights are set to 1, according to the results provided by GA, the optimal EEP is EEP 2 when only one EEP is intro- duced in the system. With this point, the system's accuracy is 96.68%, sensitivity is 97.73%, and the total FLOPs is 230.33k. For two EEPs applied Edge-fog-cloud system, the optimal com- bination of EEPs is EEP 2 and EEP 5 with equal weights of $w_{acc}$, $w_{sen}$, and $w_{com}$, achieving an accuracy of 96.21%, sensi- tivity of 97.62%, and a total FLOPs of 229.34k. When $w_{acc}$ and $w_{sen}$ are set to 10, and $w_{com}$ is 1, indicating a probability to increase FLOPs for better model performance, the optimal EEPs combination becomes EEP 3 and EEP 5, resulting in an accuracy of 95.81%, sensitivity of 97.84%, and total FLOPs of 235.4k. Lastly, when $w_{acc}$ and $w_{sen}$ are set to 1 each, and $w_{com}$ is 100, showing a probability to sacrifice metrics for complexity reduction, the optimal EEPs combination is to EEP 1 and EEP 5, resulting in an accuracy of 89.89%, sensitivity of 97.11%, and total FLOPs of 220.42k.\nThis methodology allows us to find a trade-off between es- sential elements such as accuracy, sensitivity, and computa-"}, {"title": "5.4. Power Consumption Analysis", "content": "We tested 500 ECG heart beats for each threshold level, with the heartbeats evenly distributed across the 5 classes. The threshold was set between 0.5 and 0.9 because thresholds below 0.5 yielded classifications with low confidence. To minimize interference and ensure consistent power consumption data, all non-essential onboard devices, including sensors, LEDs, and unused peripherals, were deactivated during the experiments."}, {"title": "6. Conclusion", "content": "In this paper, we proposed DCenNet, a decentralization ap- proach for biomedical signals classification using large cloud-centralized networks by introducing one or two EEPs. We demonstrated that adding a single EEP allows the partitioning of the network into an Edge-Cloud continuum while deploying two EEPs creates an Edge-Fog-Cloud continuum, effectively divid- ing the large network into three parts. This design not only enhances the responsiveness to abnormal signals but also re- duces power consumption at the edge, making it an economi- cally efficient solution. Also, by moving the EEPs to different locations in the model, trade-offs among model performance, resource utilization, and complexity must be achieved. The Edge-Cloud continuum exhibited an accuracy of 97.56% and a sensitivity of 98.55%, accompanied by a notable ~21% re- duction in complexity. Similarly, the Edge-Fog-Cloud contin- uum demonstrated an accuracy of 97.74% and a sensitivity of 98.36%, achieving a ~ 22% reduction in FLOPs. These out- comes serve as strong validation for the efficacy of our pro- posed decentralized multistage system design. The earlier the EEPs are deployed, the greater the reduction in the number of FLOPs, but deploying them later does not necessarily corre- spond to an increase in system performance. By increasing the confidence threshold to a certain extent, both continuums can achieve the performance of the original model, and even sur- pass its capabilities. In the future, we can incorporate varying numbers of EEP for different system requirements to enhance the adaptability of the model."}]}