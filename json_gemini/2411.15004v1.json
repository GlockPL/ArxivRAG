{"title": "SCRIBEAGENT: TOWARDS SPECIALIZED WEB AGENTS USING PRODUCTION-SCALE WORKFLOW DATA", "authors": ["Junhong Shen", "Atishay Jain", "Zedian Xiao", "Ishan Amlekar", "Mouad Hadji", "Aaron Podolny", "Ameet Talwalkar"], "abstract": "Large Language Model (LLM) agents are rapidly improving to handle increasingly complex web-based tasks. Most of these agents rely on general-purpose, proprietary models like GPT-4 and focus on designing better prompts to improve their planning abilities. However, general-purpose LLMs are not specifically trained to understand specialized web contexts such as HTML, and they often struggle with long-horizon planning. We explore an alternative approach that fine-tunes open-source LLMs using production-scale workflow data collected from over 250 domains corresponding to 6 billion tokens. This simple yet effective approach shows substantial gains over prompting-based agents on existing benchmarks-ScribeAgent achieves state-of-the-art direct generation performance on Mind2Web and improves the task success rate by 14.1% over the previous best text-only web agents on WebArena. We further perform detailed ablation studies on various fine-tuning design choices and provide insights into LLM selection, training recipes, context window optimization, and effect of dataset sizes.", "sections": [{"title": "INTRODUCTION", "content": "Large language model (LLM) agents have advanced significantly in web navigation. They can carry out user-specified tasks in multiple steps by reasoning on their own what actions to take and what external resources to interface with. Recent studies (Zheng et al., 2024; Lai et al., 2024; Zhang et al., 2024) have shown that, with better planning and exploration strategies, LLM agents can independently solve various web tasks ranging from simple navigation, such as locating a specific Wikipedia page, to more complex operations, such as booking flights or restaurants.\nDespite these improvements, the performance of existing web agents on research benchmarks remains significantly below human levels (Deng et al., 2023; Zhou et al., 2024; Drouin et al., 2024). One possible reason is their dependence on general-purpose LLMs. Indeed, all top-performing agents like WebPilot (Zhang et al., 2024), AWM (Wang et al., 2024b), and SteP (Sodhi et al., 2024) rely on prompting proprietary models like GPT-4 (OpenAI, 2024a). These general-purpose LLMs are not optimized for interpreting web contexts such as HTML or accessibility trees; their pretraining and alignment processes do not address navigation-related challenges; and their proprietary nature presents a major obstacle in adapting them to web environments via continual training."}, {"title": "RELATED WORK", "content": "Prompting-based agent frameworks. The majority of web agent works reuse existing LLMs and propose different prompting strategies to improve action prediction. One line of research focuses on exploiting previous experience via self-feedback (Sun et al., 2023) or in-context demonstrations (Fu et al., 2024; Zheng et al., 2024; Wang et al., 2024b; Ou et al., 2024; Shen et al., 2024b). A separate line of work centers around encouraging exploration by including external evaluators (Pan et al., 2024), using synthesized instructions (Murty et al., 2024), or applying more sophisticated search algorithms like stack (Sodhi et al., 2024), best-first tree search (Koh et al., 2024), or Monte Carlo Tree Search (Zhang et al., 2024). Despite the research efforts, these prompting methods rely heavily on the quality of the LLM used. Open-source models such as LLaMA (Dubey et al., 2024), Code LLaMA (Rozi\u00e8re et al., 2024), and Flan-T5 (Chung et al., 2022) generally underperform proprietary models like GPT-4. However, fine-tuning proprietary LLMs can often be costly and challenging, as it is restricted to being done through APIs. This implies an opportunity for enhancing open-source LLMs to match or outperform proprietary agents.\nFine-tuning-based web agents. Compared to developing better reasoning and planning frameworks, comparatively less attention has been given to optimizing the LLMs themselves to better handle web environments. Due to the difficulty of directly generating a single target element from the raw HTML, which often contains thousands of elements, existing work mostly focuses on multi-stage prediction. MindAct (Deng et al., 2023) proposes a two-stage pipeline that first uses a small LM to filter the web elements and then uses a more powerful LM to select from the filtered elements in a multi-choice question answering format. Both LMs can be fine-tuned using the Mind2Web dataset. WebAgent (Gur et al., 2023) uses HTML-5 to first process the HTML and then fine-tunes a 540B Flan-UPalm to generate code for controlling web pages. More recently, AutoWebGLM (Lai et al., 2024) trains a single ChatGLM3 6B (GLM et al., 2024) using a combination of curriculum learning, reinforcement learning, and rejection sampling fine-tuning. Despite the complicated training and inference procedures, these methods often underperform agents that prompt GPT-4. In contrast, our work shows that given sufficient high-quality workflow data, fine-tuning a single LLM can achieve strong performance. We note that the newly released OpenAI 01 (OpenAI, 2024c) can be viewed as a specialized agent with a complicated planning framework. Nonetheless, we show in Section 4.1 that ScribeAgent outperforms 01-preview by a large margin on our proprietary dataset. Moreover, while none of the training details for o1 have been released, our work provides valuable insights into data preprocessing and fine-tuning.\nBeyond the aforementioned work, there is an earlier line of research that fine-tunes LLMs for HTML inputs (Gur et al., 2022; Nakano et al., 2022; Liu et al., 2023). However, their primary application is question-answering tasks, such as answering \"could sunflowers really track the sun across the sky\u201d, and they cannot be used to generate a sequence of actions based solely on the user objective.\nLastly, we note that an emerging line of research has committed to developing multi-modal web agents that use screenshots along with HTML observations. Examples include CogAgent (Hong et al., 2023), SeeClick (Cheng et al., 2024), WebGUM (Furuta et al., 2024), WebVoyager (He et al., 2024), and AWA 1.5 (JaceAI, 2024). However, our current version of ScribeAgent focuses exclusively on text-based inputs due to the lack of effective visual preprocessing schemes. Thus, we do not compare with the aforementioned multi-modal methods in our experiments and leave developing multi-modal ScribeAgent as future work."}, {"title": "METHOD", "content": "In this section, we first overview the general setup of solving web-based tasks with LLM agents. Then, we detail our proposed method to develop specialized agents from open-source LLMs.\n3.1 GENERAL SETUP\nWe consider solving a web-based task as a sequential decision-making process guided by a high-level objective. For each task, the user first specifies an objective and a starting web page. Then, at every step, the agent outputs an action based on the task objective, the current web page, and the history. Formally, denote the user objective as q. The web environment is governed by a transition function T that can evolve over time. The agent is instantiated by a language model L. At each time step t, the agent observes ot produced by the environment state st and observes the history ht = H(o1:t-1, a1:t-1). It outputs an action at = L(q, ot, ht), which is executed in the environment, and the state changes correspondingly St+1 = T(st, at). This iterative process stops when the agent issues a stop signal, or a task termination condition is met, such as we have reached a predefined maximum number of steps.\nFor single-modal, text-only agents, the observation ot typically consists of the website's URL, the HTML-DOM (Object Model for HTML, which defines HTML elements and their properties, methods, and events), and potentially the accessibility tree (a representation that can be understood by assistive technologies like screen readers). Since the raw HTML-DOM is often long and contains redundant structural information, most methods employ preprocessing and pruning strategies, which could be as simple as retaining a fixed set of HTML tags and attributes or more complex ones like LLM-based element ranking and filtering (Deng et al., 2023).\nThe action at emulates the keyboard and mouse operations available on web pages. The most general action space in existing work consists of element operations, such as clicking, typing, and key combination pressing; tab actions, such as opening, closing, and switching between tabs; navigation actions, such as going forward and backward in the browsing history (Zhou et al., 2024).\nAs discussed earlier, previous web agent work focuses on presenting useful demonstrations through ht or iteratively revising at to improve the quality of the predicted next step. In contrast, we explore whether we can improve the model L itself by learning from a vast amount of data and incorporating more information into ot, such as the natural language description and HTML representation of a action. We detail our approach in the next section.\n3.2 SCRIBEAGENT: SPECIALIZING WEB AGENTS THROUGH FINE-TUNING\n3.2.1 COLLECTING PRODUCTION-SCALE DATA\nWe collected a large set of real-world, user-annotated, proprietary data through Scribe, a software that streamlines the creation of step-by-step guides for web-based tasks. Scribe allows users to record their interactions with the web through a browser extension and converts the interactions into well-annotated instructions, which can be then customized to specific business needs. The collected dataset consists of everyday workflows in common web application domains, encompassing customer relationship management (CRM) tools like HubSpot and Salesforce; productivity tools like Notion and Calendley; social platforms like Facebook and LinkedIn; shopping sites like Amazon and Shopify; and many others.\nEach workflow features a high-level user objective and a step-by-step documentation of the action sequence to achieve the task. The objective spans a wide range of topics, such as \u201cadd a user in a Salesforce", "invite someone to manage Facebook ad accounts": "Each step contains the following information: the current web page's URL, raw HTML-DOM, a natural language description of the action performed, the type of action, and the autogenerated CSS selector to identify the action target. There are three types of actions in the dataset:\n\u2022 mouse_click_action: click at an element\n\u2022 keyboard_sequence_action: type a sequence of characters to an element\n\u2022 keyboard_combination_action: press a set of keys together (e.g., hotkey like ctrl+c)"}, {"title": "PREPROCESSING", "content": "For ScribeAgent, we consider an observation space consisting mainly of the URL and HTML-DOM. Specifically, HTML-DOM provides agents with all structural and content information about the web page that are essential for generating the next step and long-term planning. For instance, while a drop-down menu may not be visible on the website before expansion, the agent can detect the menu items from the DOM and determine whether to click and expand it. We do not use accessibility tree to develop ScribeAgent because it may lose information about the HTML elements, such as the drop-down items, and does not generalize across different browsers and devices.\nGiven our observation space, a subsequent problem is that the DOM can be quite long and exceed the context window of prevailing open-source LLMs. To reduce the DOM sizes, we propose a pruning algorithm that maintains the essential structure and content while eliminating redundant or disruptive elements that could hinder the LLM's understanding. Specifically, we first use the BeautifulSoup library (Richardson, 2007) to remove non-essential components such as metadata, CSS, and JavaScript. Then, we utilize a tag-attribute white list to retain useful tag level information like retaining interactive elements. Since some attribute values can contain random character sequences that do not provide useful information, we propose a novel detection method that removes the attributes with character-to-token-ratio smaller than 2, i.e.,  len(tokenizer(s)) / len(s) < 2, where s denotes the value string. Intuitively, if each character in a string is encoded using a separate token, it is highly likely that the string is not semantically meaningful. Lastly, we remove the comments and extra whitespaces to clean up the DOM. After pruning, we assign each tag in the HTML with a unique ID by traversing the HTML tree from bottom to top. More details about preprocessing and analysis on the tokenizer-pruning method can be found in Appendix A.1.\nWe restrict the action space of ScribeAgent to the three types of operations specified in Section 3.2.1. To preprocess the action sequences, we rewrite each step into five lines as follows:\n1. Description: Click the \"Menu\" button to browse all food options\nAction: mouse_click_action\nNode: 832\nTarget: <svg class=\"open-hamburger-icon\" node=\u201c832\" role=\"img\">\nThe first line represents the current time step. The second line is the natural language description of the action, which can help LLMs to learn about the rationale behind applying a specific action. The third line is one of the three operations in the action space. The fourth line is the unique ID assigned to the target element. The last line details the HTML tag and attributes, which can be directly obtained from the processed DOM.\nFor the history, we consider only previous actions, omitting previous observations due to the extensive length of the DOMs. That is, ht = a1:t-1. Therefore, at each step, ScribeAgent will be given the task objective, URL, HTML-DOM, and all previous actions in the aforementioned five-"}, {"title": "FINE-TUNING WITH LORA", "content": "After preprocessing, we divide the dataset into two splits. The test set comprises of 1200 workflows with diverse objectives and domains. We use the remaining workflows as the training data to adapt LLMs via standard supervised fine-tuning. Note that for each fine-tuning example, the label is a single next-step instead of all remaining steps needed to complete the task. The agent is trained to generate all information in the five-line format described above, including the natural language description.\nTo reduce fine-tuning cost, we opt for the parameter efficient method LoRA (Hu et al., 2022) instead of full fine-tuning, since we have not observed significant performance gain by updating more parameters. We also follow previous work (Zhao et al., 2023; Shen et al., 2023) to fine-tune the layernorms in addition to the LoRA adapters. Based on empirical observations, we set the fine-tuning epoch to 2, effective batch size to 32, LoRA rank to 64 and \u03b1 to 128. We use a cosine scheduler with 30 warmup steps and a learning rate of 1e-4."}, {"title": "EXPLORING THE DESIGN SPACE", "content": "There are multiple design choices for ScribeAgent that might affect the prediction accuracy, fine-tuning cost, and inference latency. We focus on three aspects and perform detailed ablation studies to find out the optimal modeling and training configurations.\nPretrained LLM Selection. Intuitively, the quality of a fine-tuned web agent should be relevant to the quality of the pretained LLM. We identify two axes that are crucial to performance-model architecture and model size-and explore seven open-source LLMs spanning these axes: Llama 3.1 8B (Dubey et al., 2024), Mistral 7B (MistralAI, 2023), Mixtral 8x7B (MistralAI, 2024b), Qwen2 7B (Yang et al., 2024), Qwen2 57B (Yang et al., 2024), Qwen2.5 14B (Team, 2024), Qwen2.5 32B (Team, 2024), and Codestral 22B (MistralAI, 2024a). We fine-tune these models with 1 billion training tokens and evaluate their performance on the test split of the dataset we collected.\nGiven that many of the evaluated LLMs have a maximum context window of approximately 32K, and the processed DOM can exceed this limit, we divide the DOM sequentially into chunks that fit into the context window. For fine-tuning, we use the chunk containing the correct target, but for evaluation, we use the last chunk since the target's location is not known beforehand. When evaluating at a 32K context window, 25% of the test data do not have the correct target tag in the DOM, i.e., these tasks are unachievable. Thus, we compute two metrics for evaluation: (1)"}, {"title": "RESULTS", "content": "We evaluate ScribeAgent on three web datasets. We first consider the next-step prediction setting, where performance is evaluated only on a single next step. We show that ScribeAgent not only outperforms various general-purpose LLMs on our proprietary dataset but also achieves state-of-the-art on the public benchmark Mind2Web (Deng et al., 2023). Then, we move to the end-to-end task completion benchmark WebArena (Zhou et al., 2024) and show that ScribeAgent augmented"}, {"title": "PROPRIETARY DATASET", "content": "To study whether specialized fine-tuning is indeed beneficial, we first compare the performance of ScribeAgent with general-purpose baselines on our proprietary test data. We consider the non-fine-tuned Qwen2 7B, GPT-4o, and GPT-4o mini. We use in-context demonstrations to prompt them to generate actions in the same five-line format as defined in Section 3.2.2. All OpenAI baselines in this work follow the prompt in Appendix A.3.2.\nResults on the full 1200 test workflows are shown in Table 4. First, we note that ScribeAgent significantly outperforms the proprietary GPT-4o and 4o mini. This shows the benefit of specialized fine-tuning over using general-purpose LLMs. Moreover, while the non-fine-tuned Qwen2 performs extremely poorly, fine-tuning with our dataset (ScribeAgent-Small) boosts its performance by nearly 6\u00d7, which highlights the importance of domain-specific data.\nWe also plot the Exact Match metric for four types of commonly seen domains, including customer relationship management (CRM) tools, E-commerce platforms, productivity tools, and social platforms (Figure 2). While our agent's performance varies by domain, with a 6% gap between the best performing domain and the worst performing one, we observe that ScribeAgent consistently outperforms the general-purpose baselines across all of them.\nAs we were wrapping up this work, OpenAI released o1 (OpenAI, 2024c), a series of specialized models for solving complex tasks in science, coding, and math. Since it has better planning ability, we also include it in our baselines. However, we did not run the ol models on the full test set due to cost and API call limitations. Instead, we subsample 500 workflows and compare with ScribeAgent. As shown in Table 5, 01-preview performs the best among all general-purpose baselines. However, ScribeAgent still outperforms it by a wide margin, highlighting the importance of fine-tuning on real-world web navigation data. It is important to note that ScribeAgent-Small only has 7B parameters, while ScribeAgent-Large has 32B parameters, and neither model requires additional scaling during inference. In contrast, most proprietary baselines are typically larger in size and require more compute at inference. This makes ScribeAgent a better choice in terms of accuracy, latency, and cost."}, {"title": "MIND2WEB", "content": "Mind2Web (Deng et al., 2023) is a text-based dataset for assessing the navigation ability of web agents across different tasks, websites, and domains. Each task features a human demonstration of a real-world workflow, such as booking a hotel on Airbnb. At each step, the agent is asked to predict a single action, consisting of an operation and the target element. Performance is measured by element accuracy, which checks if the correct target is selected; action F1 score, which measures operation correctness like text input; step success rate, which evaluates whether both the target element and the operation are correct; and task success rate, indicating all steps are correct.\nThe original Mind2Web benchmark reports two sets of baselines: (1) multi-stage, multi-choice question-answering agents (i.e., the MindAct family) first use a pretrained element-ranking model to filter out 50 candidate elements from the full DOM and then use a separate LLM to recursively select an action from five candidates in a multi-choice question-answering (QA) fashion until one action is chosen; (2) a single-stage, generation-based agent (i.e., fine-tuned Flan-T5B) directly generates the operation and the target based on the full DOM. The multi-stage baselines generally show higher metrics than direct generation models, as the element selection process effectively filters out noise, simplifying the task.\nBeyond these baselines, we also consider recent published work such as AWM (Wang et al., 2024b), Synapse (Zheng et al., 2024), and fine-tuned HTML-T5 (Gur et al., 2023). AutoWebGLM (Lai et al., 2024) reports only step success rate among all four metrics. While the reported numbers are high, it uses a different and possibly more favorable evaluation procedure, so we do not compare against it. For both single-stage and multi-stage settings, we further categorize the baselines into those leveraging the Mind2Web training data for fine-tuning or in-context demonstrations and zero-shot methods. As stated earlier, we do not fine-tune our agents because our goal is to test the agent's out-of-distribution generalization abilities.\nWe evaluate ScribeAgent on both multi-stage QA and direct generation. For the multi-stage setting, we first use the pretrained Mind2Web element-ranker to obtain the element ranking. Then, given the output of ScribeAgent, we traverse the sorted list of HTML elements from top to bottom, and stop when the agent's generated HTML element is a subchild of the element. We then replace ScribeAgent's prediction by the element. For direct generation, we simply compare the output of our agent to the ground truth action and target.\nWe report results following the evaluation procedure specified in the Mind2Web repository in Table 6. For the multi-stage setting, ScribeAgent-Large achieves the best overall zero-shot performance. Our element accuracy and step success rate metrics are also competitive with the best"}, {"title": "END-TO-END TASK EXECUTION ON WEBARENA", "content": "WebArena (Zhou et al., 2024) features 812 web navigation tasks across five domains: E-commerce (OneStopShop), social forums (Reddit), software development (GitLab), content management (CMS), and online map (OpenStreetMap). Unlike the static Mind2Web, it implements a dynamic environment for agents to interact with and allows for assessing the functional accuracy of action sequences. Since the WebArena environment is implemented to accept only target element IDs specified in the accessibility tree, whereas ScribeAgent operates on DOM and outputs targets in HTML, we employ GPT-4o to map between the different representations.\nMore generally, we tackle end-to-end task solving by developing a multi-agent system that utilizes GPT-4o to simulate user interactions with ScribeAgent. Our system contains four stages: (1) objective refinement: user adds details about the task objective to help complete the task; (2) action generation: based on the current website and action history, agent outputs an action suggestion; (3) action execution: user executes the suggested action, e.g., clicking a button; (4) completeness evaluation: user observes the current state and decides whether the task is completed.\nWe apply the above pipeline to solve the WebArena tasks. In stage 3, GPT-4o maps the agent's output in HTML to the accessibility tree format, which is then processed by the WebArena environ-"}, {"title": "CONCLUSION", "content": "In this work, we explore how fine-tuning open-source LLMs with high-quality real-world workflow data can benefit developing specialized web agents. We present ScribeAgent, which consistently outperforms existing methods that prompt proprietary models in various evaluation settings and benchmarks. We also provide empirical insights into data processing and model fine-tuning.\nLimitations and Future Work. The long-context nature of DOMs presents great challenges in adapting LLMs. In the short term, we aim to enable ScribeAgent to compare and reason over multiple DOM chunks so that its observation is always complete. This might require integrating a memory component, which could also aid in maintaining context or state across interactions to improve multi-step reasoning. Besides, we currently do not incorporate planning into ScribeAgent, so its output will be directly used as the next action. However, adding better action selection strategies such as Monte Carlo Tree Search (MCTS) could potentially facilitate online planning and exploration, further improving the agent's decision-making processes in complex scenarios. In the long run, we aim to expand ScribeAgent's capabilities to handle multi-modal inputs and multilingual content. This would significantly broaden its applicability across different linguistic and visual contexts, making it more versatile and robust in real-world web environments."}, {"title": "APPENDIX", "content": "A.1 PREPROCESSING\nA.1.1 PRUNING PIPELINE\nThe code for preprocessing, chunking the DOM for fine-tuning, fine-tuning, and inference can be found in our GitHub.\n\u0391.1.2 \u03a4\u039fKENIZER PRUNING\nIn this section, we provide more details on the tokenizer-based detection method to remove random character strings. The rationale behind our approach is based on the observation that typical English words consist of more than two characters. Assuming the token count is t and the character count is s, this means that when t = 1, s \u2265 2, leading to s/t \u2265 2. By setting the pruning threshold to 2 and removing tag attributes with s/t< 2, we aim to eliminate strings composed solely of single-character tokens, which are likely to be nonsensical.\nIn our actual implementation, we employ this technique only for tag attributes with s > 32, being more lenient for shorter attributes. To show that this tokenizer pruning strategy is effective and to study the performance across different tokenizers and pruning thresholds, we perform the following experiments.\nWe take three tokenizers from different models: Qwen2-7B-Instruct, Mistral-7B-Instruct-v0.3, and Meta-Llama-3-8B. For each tokenizer, we vary the pruning thresholds across a set of values: {1.5, 1.75, 2, 2.25, 2.5}. Note that it is meaningless to study overly small thresholds (e.g., it is impossible to have s/t< 1) or overly large thresholds (e.g., s/t< 3 could result in the loss of meaningful attributes, as many English words contain three letters). We randomly sample 1000 DOMs from our proprietary test dataset, apply our standard pruning pipeline followed by tokenizer pruning, and then perform three analysis:\n\u2022 False positives: we use the Python enchant library to detect if there are meanful English words within the pruned strings. Note that even though these are actual words, many of them are related to DOM structure and can be safely ignored. Still, we count them as false positives since the tokenizer method is designed to remove random character strings.\n\u2022 Average s and t for the entire DOM before and after tokenizer pruning: this is for understanding the reduction in content length.\n\u2022 Lastly, we sort tags and attributes by the frequency of being pruned to identify patterns."}, {"title": "EXAMPLE PROMPT AND LABEL FOR SCRIBEAGENT", "content": "Objective: Grant delegation access to another user in Gmail settings.\nURL: https://mail.google.com/mail/u/0/\nObservation: {processed dom}\nStep-by-step guide:\n1.\nDescription: Click \"See all settings\"\nAction: mouse_click_action\nNode: 254\nTarget: <button class=\"Tj\" node=\"254\">\n2.\nDescription: Click \"Accounts\"\nAction: mouse_click_action\nNode: 2625\nTarget: <a class=\"f0 LJOhwe\" href=\"https://mail.google.com/mail/u/0/?\ntab=#settings/accounts\" node=\"2625\" role=\"tab\">\n3.\nDescription: Click \"Add another account\"\nAction: mouse_click_action\nNode: 1215\nTarget: <span class=\"LJOhwe sA\" id=\":kp\" node=\"1215\" role=\"link\">"}, {"title": "OPENAI PROMPTS", "content": "A.3.1 DATA PREPARATION\nBelow shows the prompt to generate step descriptions.\nYou are navigating a webpage to achieve an objective. Given the objective, a list of the previous actions, the current action, and a screenshot of the current action on the webpage. The objective and previous steps are only here to ground the current step, the current action and its screenshot are the most useful to your task. Give me a concise description of the current action being done on the webpage. You should look at the part of the webpage with the red circle, this is where the user clicked for the current action. Describe this action"}, {"title": "PROPRIETARY BENCHMARK BASELINES", "content": "Below shows the prompt for all OpenAI baselines. The text is the prepend for every input to which we append the task input with the corresponding objective, URL, DOM, and action history.\nYou are an autonomous intelligent agent tasked with solving web-based tasks. These tasks will be accomplished through the use of specific actions you can issue.\nHere's the information you'll have:\nThe user's objective: This is the task you're trying to complete.\nThe current web page's URL: This is the page you're currently navigating.\nPart of the current web page's HTML: Each element is assigned in descending order with an unique ID, denoted by the attribute \\\"node\\\".\nThe actions you can perform include:\nmouse_click_action: click\nkeyboard_sequence_action: type a sequence of characters\nkeyboard_combination_action: press a set of keys together\n(e.g., hotkey like ctrl+c)\nYou will generate a step-by-step guide to complete the task based on the given information. You will only produce a SINGLE next step.\nDo NOT use additional punctuation, or any markdown formatting.\nThe output should be in the following format:\nDescription: Click \\\"Users\\\"\nAction: mouse_click_action\nNode: 93\nTarget: now complete the following task by generating the next step.\n{task input}"}, {"title": "MIND2WEB EXPERIMENT DETAILS", "content": "A.4.1 PREPROCESSING\nData and Label Conversion. To apply ScribeAgent to Mind2Web data, we first re-process the provided DOM using the procedure detailed in Section 3.2.2. We store a map between our node ID and the backend ID given in the dataset. Then, we transform the history action provided in the dataset to our 5-line format. After ScribeAgent generates the next step, we check the backend ID of the provided label and map it to the node ID in our processed DOM. We then compare this label with the target node ID generated by ScribeAgent. We provide the code for the DOM processing and label conversion process in the supplementary material and will release them later.\nDOM Chunking and Action Generation. When the DOM length exceeds the 32K context window, we chunk the DOM sequentially and run the prediction workflow on each piece. For each piece of DOM, we call ScribeAgent five times to obtain five valid actions. We then aggregate all possible actions and select the one with the highest number of appearances. We use the following generation configuration: do_sample=True, top_p=0.95, temperature=0.6."}, {"title": "REFINED EVALUATION", "content": "As mentioned in the main text, we improve the Mind2Web evaluation from two perspectives:\n\u2022 Subchild label relaxation: We hypothesize that the distribution gap between our training data for ScribeAgent and the Mind2Web test set could be due to Mind2Web preferring ancestor/parent nodes in the HTML tree, while ScribeAgent's training data prefers lower HTML elements. To this effect, we relax the Mind2Web set of positive candidates to include not only the positive candidates, but also their children (direct children and grandchildren).\n\u2022 Attribute matching: Direct generation setting enables higher degree of freedom in element selection. To address scenarios where the predicted element has the same function as the ground truth but is in a different location, we enhance the direct generation evaluation by introducing an element attribute comparison step. Rather than merely comparing the node ID of the predicted and the ground truth elements, we also evaluate the tag and text attributes (e.g., the text displayed on a button). If these attributes match, we consider the prediction to be correct as it has identical functionality.\nLastly, we note that in Mind2Web, whenever there is a textarea or an input tag, the expected behavior is to directly execute the type action. However, our model is trained to first click on the input element and then perform the type action. Thus, for actions predicted on textarea or input tags, we adjust our model to replace click actions with type actions and then compare with the ground truths.\nTable 11 presents the improved performance of ScribeAgent after refining the evaluation method, showing significant gains in both settings. We find that the label relaxation strategy helps bridge part of the distribution gap, and our multi-stage pipeline effectively covers most of the gains from this label relaxation strategy by using the Mind2Web ranker. However, inspecting cases that are not covered by label relaxation, we found that there still remains a distribution gap. As a result, there is large room for improving the evaluation criteria of text-based benchmark to bridge this gap."}, {"title": "WEBARENA EXPERIMENT DETAILS", "content": "A.5.1 FoOUR-STAGE PIPELINE\nFor the most up-to-date prompts, please refer to our GitHub.\nStage 1: GPT-4o refines the intent. We use the following prompt:\nI have a simple task objective related to {domain}, rewrite it into a single paragraph of detailed step-by-step actions to achieve the task. When revising the objective, follow the rules:\\\nAssume you are already on the correct starting website and are logged in.\\\nDo not include any newlines, tabs, step numbers in the rewritten objective.\\\nFollow the example as much as possible.\\\n{In-context demonstrations for domain rules}\\ Here is an example:\\ Simple Task Objective: {in-context demonstration}\\\nDetailed Task Objective: {in-context demonstrations}\\"}, {"title": "Now, rewrite the following objective:", "content": "Stage 2: We process the environment-generated DOM using our preprocessing procedure. When the DOM length exceeds the 32K context window", "configuration": "do_sample=True", "prompt": "nYou are an autonomous agent helping users to solve web-based"}]}