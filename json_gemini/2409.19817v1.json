{"title": "Calibrating Language Models with Adaptive Temperature Scaling", "authors": ["Johnathan Xie", "Annie S. Chen", "Yoonho Lee", "Eric Mitchell", "Chelsea Finn"], "abstract": "The effectiveness of large language models (LLMs) is not only measured by their ability to generate accurate outputs but also by their calibration-how well their confidence scores reflect the probability of their outputs being correct. While unsupervised pre-training has been shown to yield LLMs with well-calibrated conditional probabilities, recent studies have shown that after fine-tuning with reinforcement learning from human feedback (RLHF), the calibration of these models degrades significantly. In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have become a cornerstone of modern artificial intelligence, offering impressive capabilities in natural language processing tasks. However, the reliability of LLMs is intertwined with their ability to generate confidence scores that accurately reflect the likelihood of their outputs being correct. This calibration, aligning a model's confidence with its accuracy, is essential, especially when LLMs are deployed in real-world scenarios where decisions based on incorrect outputs can have significant consequences.\nWhile unsupervised pre-training methods have shown success in producing well-calibrated LLMs,\na challenge arises when these models undergo fine-tuning through reinforcement learning from human feedback (RLHF). While RLHF fine-tuning is effective in enhancing model performance on specific tasks and aligning outputs with human preferences, recent studies indicate a notable degradation in the calibration of LLMs post-RLHF fine-tuning (Achiam et al., 2023; Tian et al., 2023; Kadavath et al., 2022). This degradation compromises the model's ability to provide reliable confidence scores, an issue that becomes critical when these models are applied to tasks requiring high levels of trust and accuracy. An important question arises: how can we maintain the performance gains achieved through RLHF fine-tuning while ensuring that the model's confidence scores remain reliable?\nTo address this challenge, our work introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration technique that predicts a temperature scaling parameter for each token prediction based on a language model's hidden features. Basic temperature scaling is a widely-used calibration method that applies a single temperature parameter across all outputs of a model. This technique, while effective in some contexts, assumes uniform calibration needs across all inputs, which is often not the case for complex models like LLMs. ATS, in contrast, predicts a unique temperature scaling parameter for each set of token predictions. This input-specific approach allows ATS to refine the calibration process, addressing the varying degrees of calibration shift that can occur after RLHF fine-tuning. For instance, certain inputs or topics might be more susceptible to miscalibration post-RLHF, and ATS can adaptively adjust the scaling for these instances more aggressively than for others where the model's confidence remains relatively well-aligned with its accuracy. Importantly, our approach reduces the need for task-specific calibration, which may be difficult to achieve in many cases, given the wide variety of downstream tasks that LLMs may be used for.\nWe conduct experiments on MMLU, TriviaQA, and TruthfulQA to evaluate the effectiveness of ATS in improving the calibration of LLMs following RLHF fine-tuning. Our findings demonstrate that ATS improves the calibration of post-RLHF LLMs by 10-50% on average, while having no effect on model performance."}, {"title": "Related Work", "content": "Recent literature has extensively discussed the challenges of maintaining calibration in LLMs, particularly highlighting the degradation in calibration post-RLHF (Lin et al., 2022; Park and Caragea, 2022; Kadavath et al., 2022; Xiao et al., 2022; Kuhn et al., 2023). The concept of verbalized confidence has been explored as a way to counteract this degradation (Xiong et al., 2023; Tian et al., 2023), and dialogue models have been shown to express uncertainty in a well-calibrated manner (Mielke et al., 2022; Zhou et al., 2023). Compared to works on improving sentence level calibration given token-level probabilities (Kuhn et al., 2023; Tian et al., 2023), our work aims to directly improve the calibration of token-level probabilities.\nThe calibration of neural networks has been a topic of significant interest, with foundational concepts such as proper scoring rules (Gneiting et al., 2007) laying the groundwork. Model mismatch and distribution shift often degrade calibration, commonly quantified with common metrics including Expected Calibration Error (ECE) (Naeini et al., 2015) and Brier score (Brier, 1950). Modern neural networks have been found to exhibit overconfidence (Guo et al., 2017; Thulasidasan et al., 2019; Wen et al., 2020), especially in the context of image classification (Geirhos et al., 2018; Taori et al., 2020; Wen et al., 2020; Hendrycks et al., 2021).\nVarious methods have been proposed for calibrating neural networks, including temperature scaling (Guo et al., 2017), Platt scaling (Platt et al., 1999; Niculescu-Mizil and Caruana, 2005), label smoothing (M\u00fcller et al., 2019), scaling binning (Kumar et al., 2019; Zhang et al., 2023), and more sophisticated approaches (Hendrycks et al., 2018; Katz-Samuels et al., 2022; Choi et al., 2023; Jiang et al., 2023). While these methods offer strategies for improving model calibration, our approach uniquely adapts the temperature scaling parameter for each token prediction based on its hidden features, tailoring the method to the problem of language modeling."}, {"title": "Background and Problem Setting", "content": "We consider access to a conversation SFT dataset of D = {(x, y)} with vocabulary V where x \u2208 Vlx, denotes the instruction, each with sequence length lx, and y \u2208 Vly is the corresponding response with sequence length ly. We wish to calibrate language model \u03c0(y|x). While we do not make any assumptions about the training process of \u03c0, we find our calibration method is most useful for language models following an RLHF process where token-level calibration is often significantly degraded compared to base language models which are generally well calibrated (Achiam et al., 2023).\nFor a given sample (x,y), we generate a set of unnormalized logits 2 = \u03c0(x) \u2208 Rlx+lyx|V| where each \u0109i defines the unnormalized logits for the i + 1-th token and |V| is the vocabulary size. Prior methods (Guo et al., 2017; Platt et al., 1999) propose various scaling methods for calibrating models by transforming logits. In matrix scaling, a calibration head is used to produce calibrated logits q = W2 + b where W, b are learnable parameters. In the case of language modeling where |V| is large, learning a full transform matrix becomes computationally infeasible, so we compare to vector scaling, where W is constrained to a diagonal matrix. Temperature scaling is the case when W is constrained further to a scalar matrix and b to the zero-vector. To learn these parameters, these methods minimize the cross-entropy over the SFT dataset calculated over response tokens."}, {"title": "Adaptive Temperature Scaling", "content": "Architecture. Temperature scaling, while effective in classification settings, struggles to adapt logits well in language modeling as the confidence scores that are most important (such as those that contain actual answers or facts) account for only a small portion of natural language sequences. Therefore, optimizing a single temperature parameter often results in post-RLHF language models still being overconfident post scaling. Additionally, language model miscalibration largely varies based on the type of token being predicted following RLHF. Matrix and vector scaling can in theory perform adaptive confidence prediction by using logits as features; however, they are prone to overfitting, as we find in Section 5.\nTo balance regularization with modeling cap\u0430\u0441-ity in our calibration head, we instead propose to use a head architecture that predicts a singular temperature for every token prediction. For an input pair (x, y), we first produce input-dependent features h \u2208 Rlx+ly,h using the language model \u03c0. We then learn a calibration head to produce a temperature vector co(h) = t \u2208 Rlx+ly. We exponentiate T to ensure positive values then transform logits to yield calibrated logits \u011d = \u0109\u0970e. In practice, we find that directly using the logits 2 as features can be inefficient (with a large vocabulary size) and also less effective compared to hidden states. Therefore, we use the last hidden state of the language model \u03c0 as the features for predicting T. With this architecture formulation, we retain the ability to predict confidences adaptively depending on the context, while also never changing the ranking for the possible next token given specific context, as each set of token logits are scaled by only a single value.\nLoss function. To improve the process of calibration, we take inspiration from selective classification works (Choi et al., 2023) and use a loss function which adapts targets depending on the correctness of the original language model. For a logit, label pair \u011d \u2208 R\u00ba, y \u2208 V, and weighting hyperparameter \u03b1 \u2208 [0, 1] we optimize the following loss function l:\nl(\u011d, y) = { -(1 - \u03b1) log(\u03c3sM(\u011d)y) arg max 9 = V1(-(V1) log(\u03c3sM(\u011d)i))\n(1)\nThis loss function uses a uniform distribution as the target when the model is incorrect and a standard one-hot cross-entropy when the model is correct."}, {"title": "Experiments", "content": "In this section, we aim to evaluate our proposed method on multiple benchmarks to demonstrate its effectiveness in improving calibration of LLMs fine-tuned with RLHF. We compare our method to no calibration as well as existing temperature scaling methods. Additionally, we ablate the main components of our method including the loss function, loss weighting, and head architecture.\nEvaluation Setting. We evaluate using two 7B parameter post-RLHF models LLama-2-Chat-7b (Touvron et al., 2023) and Qwen-Chat-7b. As the calibration dataset, we use the Alpaca GPT-4 (Peng et al., 2023) instruction tuning dataset, which contains a diverse set of instructions with high quality answers. We then evaluate model calibration on three downstream tasks.\nWe perform multiple choice evaluation on the MMLU (Hendrycks et al., 2020) by aggregating statistics across the entire dataset. Specifically we concatenate the confidences and correctness labels from all subjects, then calculate the calibration metrics. We also evaluate on two free response datasets, TriviaQA (Joshi et al., 2017) and TruthfulQA (Lin et al., 2021).\nMetrics. In multiple choice inference, we have a set of tokens ids O which represent the valid options for a multiple choice answer, so the confidence scores are p = \u03c3sM(\u00celz,j\u2208O) where \u03c3sM denotes the softmax function. To calculate confidences over a long sequence of response tokens for an input x, we sample a generation \u0177 of length ly from the original language model then concatenate to the instruction to form 2 and \u011d following calibration. Then, we calculate an average over transition probabilities on the response tokens. We use the Expected Calibration Error (ECE) (Guo et al., 2017) and Brier score (Brier, 1950) to evaluate calibration. We also report accuracy but each method does not significantly affect accuracy.\nBaselines. We compare our method to the post-RLHF model without calibration, temperature scaling, vector scaling, and scaling binning (Kumar et al., 2019, Zhang, not evaluate matrix scaling as the full matrix becomes computationally infeasible for large vocabulary sizes, as the projection matrix requires the square of the vocabulary size parameters."}, {"title": "Results", "content": "We report the results of our method compared to the baselines in Table 1. Overall, we find that our method improves calibration by 10-50% across the three benchmarks in terms of ECE and Brier Score compared to the next best method for both LLama-2-7b-Chat and Qwen-7b-Chat. More specifically, for Llama-7b-Chat, applying ATS achieved the lowest ECE and BS across all downstream benchmarks, showing how adjusting the temperature scaling parameter for each token prediction can significantly improve calibration. Qwen-7b-Chat also saw a significant improvement in calibration, although in the case of TriviaQA, ATS actually makes Qwen-7b-Chat slightly underconfident compared to vector scaling. Importantly, the calibration dataset used for training ATS, Alpaca GPT-4, is unrelated to the downstream tasks evaluated on, which suggests that the method does not overfit to the calibration data but rather captures underlying predictive uncertainty principles applicable across various tasks."}, {"title": "Ablation Studies", "content": "To analyze our method, we ablate the main components: loss objective, loss weight, and head architecture, measuring calibration metrics on MMLU.\nLoss objective. We compare different loss objectives, standard cross-entropy, cross-entropy with label smoothing, and selective smoothing (ours) in Table 1(a). For label smoothing we performed a sweep and found a smoothing value of 0.3 to be optimal. We find that selective smoothing outperforms both the typical cross-entropy loss and label smoothing. One possible explanation for cross-entropy and standard label smoothing being less effective is that learning adaptive temperature values with a cross-entropy loss can actually cause the model to increase confidence when the model is incorrect. In comparison, by using a uniform distribution target for incorrect predictions, this will never happen.\nLoss weight. We perform a sweep of smooth loss weight in Table 1(b). While increasing the loss weight to 0.6 (compared to 0.5) benefits MMLU calibration, in practice we found this higher loss weight began to perform worse for TriviaQA, and we did not sweep higher values as the model begins to become underconfident.\nHead architecture. In Table 1(c), we ablate the choice of head architecture. We find that a causal transformer layer identical to those used in the LLama-2-7b-chat model performs best. Given that the inference cost of a single additional layer is relatively negligible, using a full transformer layer is generally best for calibration performance as it can aggregate hidden state values from prior tokens for the specific task of predicting calibration."}, {"title": "Conclusion", "content": "In this paper, we introduce Adaptive Temperature Scaling (ATS), a novel calibration technique for large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF), offering a significant improvement in model calibration without compromising post-RLHF performance. By adapting the temperature scaling parameter based on token-level features of each input, ATS addresses the diverse calibration needs of LLMs. Our results across multiple benchmarks confirm our approach's efficacy in maintaining calibration post-RLHF."}, {"title": "Limitations", "content": "While ATS offers a significant improvement in model calibration without compromising post-RLHF performance by adapting the temperature scaling parameter based on token-level features of each input, limitations remain. In particular, we do not test how ATS interacts with different sentence-level confidence methods such as semantic uncertainty. These limitations underscore the need for ongoing research to refine calibration techniques and incorporate a more nuanced understanding of uncertainty to develop methods that allow models to express confidence in a manner that aligns with natural language."}, {"title": "Confidence Visualizations", "content": "In Figure 1, we compare confidence calibration on TruthfulQA dataset samples. We compare the Llama-2-7b-chat model without any calibration to after calibration with our method. Our method is able to cause the language model to become significantly less confident on tokens containing inaccuracies."}, {"title": "Hyperparameters", "content": ""}, {"title": "Discussion on Computational Costs", "content": "ATS involves fine-tuning language models, and it takes approximately 6 L40 GPU hours (6 hours on a single L40 GPU) to fine-tune Llama-7b for 2 epochs over Alpaca GPT-4 English. In terms of additional inference cost, the forward pass is 1.04 seconds for the base model and 1.12 seconds when applying our method. We find that the total additional computational cost of our method is relatively small, and the additional forward pass cost can likely be further reduced with better optimized code as the cost is only a single additional transformer layer or 1/32th the cost of a full Llama-7b model."}, {"title": "Reliability Diagrams", "content": "To better understand how our method changes the calibration of models, we show reliability diagrams for Llama-2-7b-Chat (Figure 2), Qwen-7b-Chat(Figure 3), and Llama-2-13b-Chat(Figure 4). For each diagram we use 15 confidence bins, the same used in ECE evaluation. Additionally, we modify the transparency of bars based on the percentage of samples with confidence scores falling in each corresponding bin (more transparent indicating fewer samples). Additionally, confidence bins with no samples will not appear on the plot. A blue line showing perfect calibration is also drawn across each diagram for reference. The bar plots are plotted with the center of each bar corresponding to the confidence and accuracy value."}]}