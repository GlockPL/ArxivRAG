{"title": "Bridging Interpretability and Robustness Using LIME-Guided Model Refinement", "authors": ["Navid Nayyem", "Abdullah Rakin", "Longwei Wang"], "abstract": "This paper explores the intricate relationship between interpretability and robustness in deep learning models. Despite their remarkable performance across various tasks, deep learning models often exhibit critical vulnerabilities, including susceptibility to adversarial attacks, over-reliance on spurious correlations, and a lack of transparency in their decision-making processes. To address these limitations, we propose a novel framework that leverages Local Interpretable Model-Agnostic Explanations (LIME) to systematically enhance model robustness. By identifying and mitigating the influence of irrelevant or misleading features, our approach iteratively refines the model, penalizing reliance on these features during training. Empirical evaluations on multiple benchmark datasets demonstrate that LIME-guided refinement not only improves interpretability but also significantly enhances resistance to adversarial perturbations and generalization to out-of-distribution data.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks have shown impressive performance on a variety of tasks in domains ranging from image classification and object detection to medical diagnostics and autonomous driving systems [1]\u2013[3]. Their ability to automatically learn hierarchical representations of data has driven their widespread adoption in both academia and industry. However, despite their impressive performance, CNNs face several critical challenges that hinder their deployment in safety-critical applications. Notably, CNNs are often perceived as black-box models, providing little to no insight into how decisions are made [4]. This opacity not only raises questions about their trustworthiness but also exacerbates their known vulnerabilities to adversarial attacks and spurious correlations in data [5], [6], [8].\nAdversarial attacks exploit the fragility of CNNs by introducing imperceptible perturbations to input data that lead to erroneous predictions. These attacks underscore the lack of robustness in CNNs, particularly when deployed in real-world environments [6], [7]. Moreover, CNNs often rely on spurious correlations present in training data, which can lead to overfitting and poor generalization on out-of-distribution (OOD) samples or edge cases [8], [9]. These limitations necessitate the development of methods that not only improve the robustness of CNNs but also make their decision-making processes more transparent and interpretable.\nInterpretability has gained prominence as a means of addressing these challenges, with tools such as Local Interpretable Model-Agnostic Explanations (LIME) [10] and SHapley Additive exPlanations (SHAP) [17] providing insights into model behavior. These tools work by highlighting the contribution of input features to individual predictions, enabling practitioners to assess whether the model's reasoning aligns with human expectations. However, while interpretability tools are widely used to diagnose model behavior, their role in actively enhancing model robustness remains underexplored. Recent studies suggest that interpretability can serve as more than a diagnostic tool\u2014it can be an intervention mechanism for improving model reliability [18]\u2013[20].\nIn this paper, we propose a novel framework that leverages LIME to systematically enhance the robustness of CNNs. Unlike prior approaches that treat interpretability as a passive tool for post-hoc analysis, we employ LIME as an active guide for model refinement. Specifically, LIME generates localized explanations by approximating the decision boundary of a model around individual predictions. By analyzing these explanations, we identify and address instances where CNNs rely on irrelevant, redundant, or misleading features. This information is used to refine the model through iterative retraining, thereby reducing its susceptibility to adversarial attacks and improving its generalization capabilities.\nOur methodology involves three key steps:\n1) Feature Attribution Analysis: Using LIME to identify the most influential features driving individual predictions.\n2) Spurious Dependency Detection: Highlighting irrelevant or misleading features that contribute disproportionately to the model's outputs.\n3) Model Refinement: Iteratively retraining the model to minimize dependency on spurious features, thereby enhancing its robustness and stability.\nWe validate our approach through extensive experiments on benchmark image datasets, evaluating the robustness of CNNS before and after LIME-guided refinements. Specifically, we measure improvements in adversarial accuracy.\nOur contributions are as follows:\n\u2022 We introduce a novel framework that uses LIME as a proactive tool to enhance the robustness of CNNs, bridging the gap between interpretability and model refinement.\n\u2022 We conduct a comprehensive empirical analysis, showing that LIME-guided refinements improve adversarial accu-"}, {"title": "II. RELATED WORKS", "content": "The challenges of robustness and interpretability in Convolutional Neural Networks (CNNs) have been widely studied in the literature, yet their intersection remains underexplored. This section reviews existing works on adversarial robustness, interpretability methods, and the emerging field of combining interpretability with robustness."}, {"title": "A. Adversarial Robustness", "content": "The vulnerability of CNNs to adversarial perturbations has been a critical area of research since the seminal work of Szegedy et al. [5], which demonstrated how small, imperceptible changes to input data could lead to significant prediction errors. Follow-up works, such as the Fast Gradient Sign Method (FGSM) [6] and Projected Gradient Descent (PGD) [7], have proposed various adversarial attack strategies and defenses. Robustness enhancements typically involve adversarial training, where models are trained on perturbed inputs [7], or preprocessing techniques such as input sanitization [?].\nWhile these methods improve adversarial resilience, they often require significant computational overhead and are limited to specific attack scenarios [21], [22], [24]. This paper addresses these limitations by introducing an alternative approach that enhances robustness through interpretability-driven interventions, complementing existing adversarial defense techniques."}, {"title": "B. Interpretability in Machine Learning", "content": "Interpretability methods aim to provide insights into the decision-making processes of complex models like CNNs. Post-hoc explanation techniques such as Local Interpretable Model-Agnostic Explanations (LIME) [10] and SHapley Additive exPlanations (SHAP) [17] have become popular tools for understanding model predictions. These methods highlight feature importance, offering a transparent view of how input data contributes to model outputs.\nOther techniques, such as Grad-CAM [?] and Integrated Gradients [?], [23], provide visualization-based explanations tailored to CNNs, focusing on feature attribution in image classification tasks. However, these methods are primarily used for diagnostic purposes and rarely feed back into the model training process to enhance performance or robustness. Our work builds on LIME's localized interpretability to identify and mitigate vulnerabilities in CNNs, thus extending the utility of interpretability methods beyond passive diagnostics."}, {"title": "C. Bridging Interpretability and Robustness", "content": "The potential for interpretability methods to improve model robustness has been noted in recent studies. Doshi-Velez and Kim [18] argued that interpretability could act as a debugging tool to identify and address spurious correlations in model behavior. Similarly, Adebayo et al. [19] introduced sanity checks for saliency maps, demonstrating how interpretability insights can expose flaws in feature attribution.\nSlack et al. [20] explored adversarial attacks on interpretability methods, such as LIME and SHAP, highlighting the need for robust explanations to ensure reliable insights. While these studies emphasize the vulnerabilities of interpretability tools, they also hint at their potential role in enhancing robustness. Few works, however, have directly integrated interpretability methods into the model refinement process to address robustness challenges.\nIn the context of adversarial robustness, research by Ross and Doshi-Velez [26] demonstrated that regularizing models to align with human-interpretable explanations could improve adversarial resilience. Similarly, Dombrowski et al. [27] explored how adversarial robustness influences the stability of saliency maps. These studies provide evidence that interpretability and robustness are interconnected, but they stop short of proposing systematic frameworks for combining the two."}, {"title": "D. LIME as a Tool for Robustness", "content": "LIME's ability to provide localized explanations makes it particularly suited for identifying model vulnerabilities. For example, Yeh et al. [28] explored how LIME explanations can be used to detect spurious features in text classification tasks. However, the application of LIME to systematically refine CNNs for robustness remains an open research area. Our work extends LIME's utility by demonstrating its role in guiding model refinements to enhance robustness against adversarial attacks and improve generalization on out-of-distribution (OOD) data.\nWhile adversarial robustness and interpretability have been extensively studied as separate fields, their integration remains underdeveloped. Existing methods primarily focus on improving robustness through adversarial training or improving interpretability for diagnostic purposes. The intersection of these areas\u2014leveraging interpretability as a mechanism for enhancing robustness\u2014offers significant potential but has seen limited exploration.\nThis paper fills this gap by proposing a LIME-guided framework for CNN refinement, bridging interpretability and robustness. Our approach systematically integrates insights from LIME to identify and address spurious feature dependencies, providing a novel pathway for improving the resilience and reliability of CNNs."}, {"title": "III. METHODOLOGY", "content": "This section describes the proposed methodology for enhancing the robustness of deep learning models using Local Interpretable Model-Agnostic Explanations (LIME). The method involves three primary components: feature attribution analysis, spurious dependency detection, and model refinement. The proposed framework operates in an iterative manner, as illustrated in Figure 1. Each step is detailed below with supporting mathematical formulations and explanations."}, {"title": "A. Framework Overview", "content": "The primary goal of the framework is to iteratively refine a CNN $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$, where $f(x)$ outputs a probability distribution over $k$ classes for an input $x \\in \\mathbb{R}^d$. The process uses LIME-generated explanations to identify spurious dependencies and then addresses these through targeted interventions in the training process. The steps include:\n1) Feature Attribution Analysis: Identify the importance of individual features using LIME.\n2) Spurious Dependency Detection: Detect features that are disproportionately influential or irrelevant to the task.\n3) Model Refinement: Retrain or modify the model to reduce reliance on these spurious features.\nThis process is repeated iteratively to improve robustness and generalization."}, {"title": "B. Feature Attribution Analysis", "content": "Feature attribution quantifies the influence of each input feature $x_j$ on the model's output $f(x)$. LIME approximates the decision boundary of $f$ in the local neighborhood of $x$ using a surrogate interpretable model $g$.\n1) Perturbation and Sampling: For a given input $x$, a set of perturbed samples $Z = \\{(z_i, f(z_i))\\}_{i=1}^{N}$ is generated by masking subsets of the input features. The similarity of each perturbed sample $z_i$ to $x$ is quantified using a kernel function:\n$w_i = K(x, z_i) = exp \\left(-\\frac{||x-z_i||^2}{\\sigma^2}\\right)$\nwhere $\\sigma$ controls the locality of the approximation.\n2) Surrogate Model Training: A linear surrogate model $g$ is trained to approximate $f$ locally:\n$g(z) = \\beta_0 + \\sum_{j=1}^d \\beta_j z_j$,"}, {"title": "C. Spurious Dependency Detection", "content": "Spurious dependencies are features that disproportionately influence predictions but lack semantic relevance to the task.\n1) Criteria for Detection: We define spurious dependencies based on the following criteria:\n\u2022 Irrelevance: Features with high importance scores $(|\\beta_j| > \\tau)$ that are not task-relevant, such as background artifacts.\n\u2022 High Sensitivity: Features with high gradient-based sensitivity:\n$Sensitivity(x_j) = \\left|\\frac{\\partial f(x)}{\\partial x_j}\\right|$\nwhere high sensitivity implies excessive influence from minor perturbations.\n\u2022 Instability: Features with large variance in importance scores across perturbed inputs:\n$Instability(x_j) = Var \\left( \\{\\beta_j^i\\}_{i=1}^{N} \\right)$."}, {"title": "2) Algorithm for Spurious Dependency Detection:", "content": "1) Generate LIME feature importance scores for each test input x.\n2) Flag features $x_j$ satisfying:\n$|\\beta_j| > \\tau \\lor Sensitivity(x_j) > \\epsilon \\lor Instability(x_j) > \\delta$,\nwhere $\\tau, \\epsilon, \\delta$ are thresholds.\n3) Aggregate flagged features across the dataset to construct a set $F_{spurious}$."}, {"title": "D. Model Refinement", "content": "Model refinement is the critical step in the proposed methodology that mitigates spurious dependencies identified during the feature attribution and detection phases. The primary goal is to adjust the model $f$ such that it reduces its reliance on spurious features while maintaining or improving its predictive performance.\n1) Feature Masking: Feature masking aims to eliminate the influence of spurious features $F_{spurious}$ by modifying the input during training. For an input $x$, the masked input $x^{masked}$ is defined as:\n$x^{masked} = x \\odot m, \\text{ where } m_j = \\begin{cases} 0 & \\text{ if } x_j \\in F_{spurious}, \\\\ 1 & \\text{ otherwise.} \\end{cases}$\nHere, $\\odot$ denotes element-wise multiplication, and $m$ is a binary mask vector of the same dimensionality as $x$. By forcing the model to learn without access to spurious features, $f$ is encouraged to focus on more relevant and robust features.\n2) Sensitivity Regularization: Sensitivity regularization directly penalizes the model's reliance on spurious features by adding a regularization term to the training loss. Let $F_{spurious}$ be the set of indices corresponding to spurious features. The total loss becomes:\n$\\mathcal{L} = \\mathcal{L}_{task} + \\lambda \\cdot \\mathcal{L}_{reg}$,\nwhere:\n\u2022 $\\mathcal{L}_{task}$ is the primary task loss, such as cross-entropy:\n$\\mathcal{L}_{task} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{K} y_{ik} \\log f_k(x_i)$,\nwith $y_{ik}$ being the ground truth label for class $k$ and $f_k(x_i)$ being the model's predicted probability for class $k$.\n\u2022 $\\mathcal{L}_{reg}$ penalizes sensitivity to spurious features:\n$\\mathcal{L}_{reg} = \\frac{1}{|F_{spurious}|} \\sum_{j \\in F_{spurious}} \\left| \\frac{\\partial f(x)}{\\partial x_j} \\right|^2$\nThe hyperparameter $\\lambda > 0$ controls the weight of the regularization term. This approach ensures that the gradients of the model output with respect to spurious features are minimized, reducing the influence of these features."}, {"title": "3) Adversarial Training:", "content": "Adversarial training introduces adversarial examples during training to improve the model's robustness. For an input $x$ with ground truth label $y$, an adversarial example $x^{adv}$ is generated using the Fast Gradient Sign Method (FGSM):\n$x^{adv} = x + \\epsilon \\cdot sign(\\nabla_x \\mathcal{L}_{task}(f(x), y))$,\nwhere $\\epsilon > 0$ is the perturbation magnitude.\nThe adversarial loss is defined as:\n$\\mathcal{L}_{adv} = \\mathbb{E}_{(x,y) \\sim D} [\\mathcal{L}_{task}(f(x^{adv}), y)]$.\nTo account for spurious features, the adversarial perturbation can be restricted to $F_{spurious}$, resulting in targeted adversarial examples:\n$x^{adv (spurious)} = x + \\epsilon \\cdot sign(\\nabla_{x_{F_{spurious}}} \\mathcal{L}_{task}(f(x), y))$,\nwhere $x_{F_{spurious}}$ represents the subset of input features corresponding to $F_{spurious}$.\nThe final training loss combines task loss, adversarial loss, and regularization:\n$\\mathcal{L} = \\mathcal{L}_{task} + \\alpha \\cdot \\mathcal{L}_{adv} + \\lambda \\cdot \\mathcal{L}_{reg}$,\nwhere $\\alpha > 0$ balances the adversarial and task objectives."}, {"title": "4) Iterative Refinement:", "content": "Model refinement is performed iteratively. After each refinement step, the model is re-evaluated using LIME to compute updated feature importance scores. Spurious dependencies are re-detected, and the refinement process continues. The iterative procedure is outlined below:\n1) Initial Training: Train the baseline model $f_{baseline}$ on the training dataset.\n2) LIME Analysis: Compute feature importance maps for test inputs {x} and identify $F_{spurious}$.\n3) Refinement Steps: Apply feature masking, sensitivity regularization, or adversarial training to retrain the model.\n4) Re-Evaluation: Use LIME to analyze the refined model $f_{refined}$ and update $F_{spurious}$.\n5) Repeat: Continue the process until the model achieves the desired robustness metrics or convergence."}, {"title": "5) Mathematical Optimization Perspective:", "content": "The model refinement process can be viewed as a constrained optimization problem:\n$\\min_{\\theta} \\mathcal{L}_{task} (f_{\\theta} (x), y) \\text{ subject to } \\left| \\frac{\\partial f_{\\theta} (x)}{\\partial x_j} \\right| \\leq \\delta, \\forall j \\in F_{spurious}$,\nwhere $\\theta$ represents the parameters of $f$, and $\\delta > 0$ is the maximum allowable sensitivity to spurious features.\nIn practice, this constraint is incorporated into the loss function using Lagrange multipliers, leading to the augmented loss:\n$\\mathcal{L}_{augmented} = \\mathcal{L}_{task} + \\Lambda \\sum_{j \\in F_{spurious}} \\left| \\frac{\\partial f(x)}{\\partial x_j} \\right|$\nThis methodology integrates LIME-generated explanations into a systematic framework for improving CNN robustness."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "To rigorously evaluate the proposed framework, we design a comprehensive experimental pipeline that incorporates diverse datasets, model architectures, adversarial configurations, and performance metrics. This section provides a detailed description of the experimental design."}, {"title": "A. Datasets", "content": "We use the following datasets to test the proposed framework across varying complexities and tasks:\n1) CIFAR-10: Comprises 60,000 color images (32\u00d732 pixels) across 10 object categories (e.g., airplane, bird, car).\n2) CIFAR-10-C: Contains corrupted versions of CIFAR-10 images with various noise types (e.g., Gaussian noise, motion blur).\n3) CIFAR-100: Comprises 60,000 color images (32\u00d732 pixels) across 100 object categories (e.g., airplane, bird, car)."}, {"title": "B. Model Architectures", "content": "The framework is evaluated on several state-of-the-art architectures: Residual Networks such as ResNet-18 [3]."}, {"title": "C. Adversarial Attack Configurations", "content": "We test model robustness under several adversarial attack scenarios:\n1) Fast Gradient Sign Method (FGSM): A single-step attack that perturbs input $x$ as:\n$x^{adv} = x + \\epsilon \\cdot sign(\\nabla_x \\mathcal{L}_{task}(f(x), y))$,\nwhere $\\epsilon$ controls the perturbation strength. Configurations: $\\epsilon \\in \\{0.01, 0.03, 0.1\\}$.\n2) Projected Gradient Descent (PGD): A multi-step attack:\n$x^{adv}_{t+1} = clip_{x,\\epsilon} (x^{adv}_t + \\alpha \\cdot sign(\\nabla_x \\mathcal{L}_{task}(f(x), y)))$,\nwhere $\\alpha$ is the step size, $\\epsilon$ is the perturbation budget, and $clip_{x,\\epsilon}$ ensures $x^{adv}$ remains within $\\epsilon$-distance of $x$. Configurations: $\\epsilon = 0.03$, $\\alpha = 0.01$, 40 iterations.\n3) Out-of-Distribution Testing: Evaluate generalization using CIFAR-10-C, which introduces corruptions such as noise and blur."}, {"title": "D. Evaluation Metrics", "content": "The models are evaluated using the following metrics:\n1) Standard Accuracy ($A_{std}$): Accuracy on the clean test dataset:\n$A_{std} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}(f(x_i) = y_i)$.\n2) Adversarial Accuracy ($A_{adv}$): Accuracy on adversarially perturbed test samples:\n$A_{adv} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{I}(f(x^{adv}) = y_i)$."}, {"title": "V. EXPERIMENTAL RESULTS AND DISCUSSION", "content": ""}, {"title": "A. CIFAR-10 Dataset", "content": "We evaluate the performance of the proposed LIME-Guided Refined Model against the Baseline Model using the CIFAR-10 dataset. The experiments focus on clean test accuracy, loss convergence, and adversarial robustness under FGSM and PGD attacks. The results demonstrate that our proposed framework improves robustness while maintaining interpretability, albeit with a small trade-off in clean accuracy.\n1) Test Accuracy Comparison: Figure 2 shows the test accuracy over 30 epochs for both models. The baseline model achieves higher clean test accuracy, converging at approximately 87%, whereas the refined model stabilizes at around 85%. The slightly lower accuracy of the refined model is due to the additional constraints introduced during training, such as adversarial training and regularization that penalizes reliance on spurious features. These constraints guide the model to focus on more robust and semantically meaningful patterns, which come at the cost of clean accuracy. Additionally, the refined model shows slower convergence in the earlier epochs due to the iterative refinement process and feature masking.\n2) Robustness to FGSM Attacks: The performance of both models under FGSM attacks, depicted in Figure 4, highlights the significant robustness improvements achieved by the refined model. At a small perturbation magnitude of $\\epsilon = 0.01$, the refined model achieves an adversarial accuracy of 72.59%, compared to the baseline model's 54.29%. As the perturbation strength increases to $\\epsilon = 0.03$, the refined model maintains 50.05% accuracy, while the baseline model's accuracy drops sharply to 21.95%. For larger perturbation strengths, such as $\\epsilon = 0.1$ and $\\epsilon = 0.3$, the refined model continues to outperform the baseline, although both models experience a significant drop in accuracy. These results suggest that the refined model, trained with LIME-guided masking and adversarial training, is far more resilient to small perturbations compared to the baseline model, which is highly sensitive to adversarial noise due to its reliance on spurious features.\n3) Robustness to PGD Attacks: Figure 5 presents the results of the models under PGD attacks, which are stronger and more iterative than FGSM. At $\\epsilon = 0.01$, the refined model achieves 71.41% accuracy, while the baseline model lags behind at 44.06%. With a higher perturbation magnitude of $\\epsilon = 0.03$, the baseline model's performance deteriorates drastically to 2.81%, while the refined model retains 39.15% accuracy. As the perturbation strength increases further, the baseline model's accuracy drops to near-zero values, reflecting its fragility under strong adversarial perturbations. In contrast, the refined model retains minimal, yet consistent accuracy, demonstrating its improved robustness and stability. The substantial difference in performance under PGD attacks underscores the importance of refinement techniques, such as masking spurious features and adversarial training, which help the model resist stronger perturbations."}, {"title": "B. CIFAR-100 Dataset", "content": "We extend our experiments to the CIFAR-100 dataset to evaluate the performance of the LIME-guided refined model and the baseline model on a more challenging classification task. The CIFAR-100 dataset contains 100 classes, each with 500 training and 100 testing images. This increases the complexity of the learning process and provides a robust assessment of the models' accuracy, loss convergence, and adversarial robustness.\n1) Test Accuracy Comparison: Figure 6 shows the test accuracy of the baseline and refined models over 30 epochs. The baseline model achieves higher clean test accuracy, converging at approximately 61%. In contrast, the refined model stabilizes at 55%, exhibiting slower convergence and a reduced final accuracy compared to the baseline. The difference in accuracy is due to the additional constraints introduced during the refinement process, such as sensitivity regularization and adversarial training. These constraints force the model to focus on more robust and meaningful features, trading off some clean accuracy for improved robustness. Despite this, the refined model demonstrates consistent performance throughout the epochs after stabilization.\n2) Robustness to FGSM Attacks: Figure 8 highlights the adversarial robustness of the two models under FGSM attacks with varying perturbation strengths (\u20ac). At a low perturbation magnitude of $\\epsilon = 0.01$, the refined model achieves 43.55% adversarial accuracy, significantly outperforming the baseline model, which achieves only 30.41%. As the perturbation magnitude increases to $\\epsilon = 0.03$, the refined model maintains an adversarial accuracy of 24.62%, while the baseline accuracy drops sharply to 11.38%. For larger perturbations, such as $\\epsilon = 0.1$ and $\\epsilon = 0.3$, both models experience a significant performance drop. However, the refined model retains a slight advantage over the baseline, demonstrating its ability to maintain robustness even under higher perturbations.\nThese results clearly demonstrate that the refined model is more resistant to FGSM attacks, particularly for small to moderate perturbation magnitudes. This improvement is a direct result of the iterative refinement process, which reduces the model's reliance on vulnerable features.\n3) Robustness to PGD Attacks: Figure 9 presents the results of the models under PGD attacks, a stronger iterative adversarial method. At $\\epsilon = 0.01$, the refined model achieves 41.41% accuracy, whereas the baseline model lags behind at 27.61%. As the perturbation magnitude increases to $\\epsilon = 0.03$, the baseline model's performance deteriorates rapidly, while the refined model retains an accuracy of 19.25%. For higher perturbation strengths, such as $\\epsilon = 0.1$ and $\\epsilon = 0.3$, both models experience near-complete degradation in performance, highlighting the challenge of defending against strong iterative attacks. Nevertheless, the refined model consistently outperforms the baseline across all perturbation levels, underscoring its enhanced robustness."}, {"title": "C. CIFAR-10C Dataset", "content": "To further validate the robustness of our LIME-guided refined model, we evaluate its performance on the CIFAR-10C dataset, a benchmark designed to assess model robustness under common corruptions. CIFAR-10C introduces 19 corruption types, such as noise, blur, weather distortions, and digital artifacts, across varying levels of severity. We compare the baseline model and the refined model in terms of clean accuracy, corruption robustness, and adversarial accuracy under FGSM and PGD attacks.\n1) Test Accuracy Under Common Corruptions: Figure 10 and Figure 11 illustrate the test accuracy of the baseline and refined models across the 19 corruption types over 30 training epochs. The baseline model achieves higher accuracy for some corruptions, peaking around 75% for specific cases. However, the refined model demonstrates more stable performance across the majority of corruption types, converging slightly below the baseline at around 70% accuracy.\nThe refined model's consistency stems from its reliance on robust, interpretable features rather than spurious patterns, which are often disrupted by corruptions. In contrast, the baseline model's higher variance across corruptions indicates its overfitting to less stable features, which compromises its resilience.\n2) Robustness to FGSM Attacks: Figures 12 and 13 compare the adversarial accuracy of the baseline and refined models under FGSM attacks at perturbation magnitudes $\\epsilon = 0.1$ and $\\epsilon = 0.3$.\nAt $\\epsilon = 0.1$, the refined model outperforms the baseline model across all corruption types. For example, the refined model achieves 26.69% accuracy for brightness and 29.91% for impulse noise, compared to the baseline's 9.98% and 8.40%, respectively. The baseline model's performance deteriorates sharply under perturbations, while the refined model retains a significant advantage, demonstrating its enhanced robustness.\nAt a larger perturbation magnitude of $\\epsilon = 0.3$, the refined model continues to outperform the baseline across all corruptions. While both models exhibit a drop in performance, the refined model maintains higher accuracy, with results such as 17.03% for contrast and 15.21% for pixelate. In comparison, the baseline model's accuracy remains below 10% for most corruption types.\n3) Robustness to PGD Attacks: Figures 14 and 15 show the models' performance under PGD attacks, which represent a stronger iterative adversarial method.\nAt a low perturbation magnitude of $\\epsilon = 0.01$, the refined model achieves significantly higher accuracy than the baseline across all corruption types. For example, the refined model achieves 90.72% accuracy for brightness and 89.32% for speckle noise, compared to the baseline's 73.79% and 41.52%, respectively. The substantial improvement highlights the refined model's ability to resist adversarial perturbations while maintaining robustness under corrupted inputs."}, {"title": "VI. CONCLUSION", "content": "This paper presented a novel LIME-guided refinement framework to address critical vulnerabilities in deep learning models, such as susceptibility to adversarial attacks, reliance on spurious correlations, and limited interpretability. By leveraging LIME to identify and mitigate the influence of irrelevant features, the proposed approach systematically enhances both robustness and transparency. Empirical evaluations on CIFAR-10, CIFAR-100, and CIFAR-10C datasets demonstrated that LIME-guided models outperform baseline models under adversarial scenarios and input corruptions, showcasing significant improvements in adversarial resistance and generalization to out-of-distribution data. Future work will explore its application to other architectures and domains, as well as its integration with advanced defense techniques to further enhance model reliability and fairness."}]}