{"title": "MONET: MIXTURE OF MONOSEMANTIC EXPERTS FOR TRANSFORMERS", "authors": ["Jungwoo Park", "Young Jin Ahn", "Kee-Eung Kim", "Jaewoo Kang"], "abstract": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable behav-\niors like toxic content generation. However, mechanistic interpretability is hin-\ndered by polysemanticity\u2014where individual neurons respond to multiple, unre-\nlated concepts. While Sparse Autoencoders (SAEs) have attempted to disen-\ntangle these features through sparse dictionary learning, they have compromised\nLLM performance due to reliance on post-hoc reconstruction loss. To address\nthis issue, we introduce MIXTURE OF MONOSEMANTIC EXPERTS FOR TRANS-\nFORMERS (MONET) architecture, which incorporates sparse dictionary learning\ndirectly into end-to-end Mixture-of-Experts pretraining. Our novel expert decom-\nposition method enables scaling the expert count to 262,144 per layer while total\nparameters scale proportionally to the square root of the number of experts. Our\nanalyses demonstrate mutual exclusivity of knowledge across experts and show-\ncase the parametric knowledge encapsulated within individual experts. Moreover,\nMONET allows knowledge manipulation over domains, languages, and toxicity\nmitigation without degrading general performance. Our pursuit of transparent\nLLMs highlights the potential of scaling expert counts to enhance mechanistic\ninterpretability and directly resect the internal knowledge to fundamentally ad-\njust model behavior. The source code and pretrained checkpoints are available at\nhttps://github.com/dmis-lab/Monet.", "sections": [{"title": "1 INTRODUCTION", "content": "As large language models (LLMs) continue to scale and generalize (Radford et al., 2019; Brown\net al., 2020), understanding their internal computations becomes increasingly imperative. Mech-\nanistic interpretability seeks to unravel how neural networks generate outputs by dissecting their\ninternal processes into human-interpretable components (Bereska & Gavves, 2024). Such com-\nprehension is crucial not only for aligning LLMs with human values (Ji et al., 2023) but also for\npreventing undesirable behaviors such as the generation of toxic content (Hendrycks et al., 2023).\nHowever, achieving such level of interpretabil-\nity in LLMs is particularly challenging due to\npolysemanticity\u2014the phenomenon where indi-\nvidual neurons respond to multiple, unrelated\nconcepts (Arora et al., 2018; Mu & Andreas,\n2020; Olah et al., 2020). This arises from\nthe superposition hypothesis, which suggests\nthat neural networks represent more features\nthan there are neurons by encoding them in\ncompressed, high-dimensional spaces (Elhage\net al., 2022). To address polysemanticity, observational analyses leveraging sparse representations\nhave been employed. Specifically, techniques like Sparse Autoencoders (SAEs) aim to disentan-\ngle these superposed features by learning sparse, overcomplete bases that describe the activation\nspace (Sharkey et al., 2022; Bricken et al., 2023; Cunningham et al., 2024)."}, {"title": "2 PRELIMINARIES", "content": "Sparse Mixture-of-Experts (SMOE) SMOE models efficiently scale their capacity by activating\nonly a subset of the experts, thereby reducing computational costs. These models leverage expert\nembeddings to determine which experts to activate. Given a hidden representation vector $x \\in \\mathbb{R}^d$\nand a set of N expert networks ${E_i}_{i=1}^N$, each expert is defined as:\n$E_i(x) = V_i\\sigma(U_ix)$                                                                                                                               (1)\nwhere $U_i \\in \\mathbb{R}^{m\\times d}$ and $V_i \\in \\mathbb{R}^{d\\times m}$ are the weight matrices of the i-th expert, and $\\sigma$ is an activation\nfunction such as ReLU or GELU. Let ${w_i}_{i=1}^N \\subset \\mathbb{R}^d$ be the expert embeddings and $T_k$ denote the\ntop-k operation. The output of the SMoE layer is then computed as:\n$SMoE(x) = \\sum_{i \\in K} g_iE_i(x)$                                                                                                                                 (2)\nwhere $K = T_k({w_i^\\top x}_{i=1}^N)$ is the set of indices corresponding to the sparsely activated top-k experts,\nbased on their routing scores $g = softmax({w_i^\\top x}_{i \\in K})$.\nThe Parameter Efficient Expert Retrieval (PEER) Compared to other SMoE architectures,\nPEER processes a substantially higher number of experts by employing a computationally efficient\nrouting mechanism. Based on the product key algorithm introduced by Lample et al. (2019), PEER\nimplements the product key retrieval mechanism that enables efficient search of top-k experts, re-\nducing computational complexity from $O(Nd)$ to $O((\\sqrt{N} + k^2)d)$.\nSpecifically, each PEER expert is a minimal MLP (multilayer perceptron) consisting of an input\nlayer, a single hidden neuron, and an output layer. PEER uses two independent product keys, which\nare expert embeddings, ${w_{hi}} \\subset \\mathbb{R}^{d/2}$ and ${w_{hj}} \\subset \\mathbb{R}^{d/2}$ for each head h, rather than\nretrieving the experts among N embeddings. The hidden state x is correspondingly split into two\nhalves, $x_1, x_2 \\in \\mathbb{R}^{d/2}$, and the top-k experts are obtained by:\n$K_h = T_h({\\langle w_{hi}, x_1\\rangle}_{i=1}^N)$ and $K_h' = T_h({\\langle w_{hj}, x_2\\rangle}_{j=1}^N)$.                                                                                                      (3)\nThen, top-k experts are selected from the scores computed over the Cartesian product $K_h\\times K_h'$, to\nconstitute $K_h$, i.e.,\n$K_h = T_k({\\langle w_{hi}, x_1\\rangle + \\langle w_{hj}, x_2\\rangle: (i, j) \\in K_h \\times K_h' })$,                                                                                    (4)\nwith $g_h = softmax({\\langle w_{hi}, x_1\\rangle + \\langle w_{hj}, x_2\\rangle: (i,j) \\in K_h })$ being routing scores of the experts.\nFollowing the format of Equation 1, let $E_{ij}(x)$ be the (i, j)th expert network and $u_{ij}, V_{ij} \\in \\mathbb{R}^d$ be\nweights of the expert MLPs. The PEER layer is then formulated as:\n$PEER(x) = \\sum_{h=1}^H\\sum_{(i,j)\\in K_h} g_{hij} E_{ij}(x) = \\sum_{h=1}^H\\sum_{(i,j)\\in K_h} g_{hij} V_{ij}\\sigma(u_{ij}x)$.                                                                                             (5)\nAlthough PEER reduces the computational complexity by a factor of $\\sqrt{N}$, it suffers from mem-\nory bottleneck as the total number of parameters grows with expert count N. Consider a model\nwith dimension d = 2048 and 8 attention heads - scaling to 1 million experts would require 4.3\nbillion parameters per layer. Therefore, building an LLM with 1.3 billion active parameters would\nnecessitate an additional 103 billion parameters just for the experts."}, {"title": "3 MONET: MIXTURE OF MONOSEMANTIC EXPERTS FOR TRANSFORMERS", "content": "To disentangle superposed features in LLM by incorporating sparse dictionary learning into end-to-\nend SMoE pretraining, we aim to maximize the number of experts. Instead of searching through a\nlarge pool of standalone experts using product key retrieval, we propose product key composition\nof experts by sharding layers in individual experts to overcome PEER's memory constraints. Our\northogonal layer partitioning methods, horizontal and vertical decompositions, address the memory\nbottleneck by scaling the number of experts while keeping parameter growth proportional to the\nsquare root of the expert count.\nHorizontal Expert Decomposition (HD) Our first approach to product key composition funda-\nmentally redefines how expert networks are constructed. Instead of maintaining complete expert\nnetworks as defined in Equations 1 and 5, we decompose each expert into two complementary\ncomponents: bottom and top linear layers. Such partitioning scheme allows us to build experts\ndynamically during inference by combining these components.\nSpecifically, we partition the weights of experts into two distinct groups corresponding to the bottom\nand top layers: ${U_i}^{\\sqrt{N}}_{i=1} \\subset \\mathbb{R}^{m\\times d}$ and ${V_j}^{\\sqrt{N}}_{j=1} \\subset \\mathbb{R}^{d\\times m}$ respectively, where m represents the expert\nhidden dimension (e.g., m = 1 for PEER). To accommodate architectures with bias terms (Shen\net al., 2024), we include ${b_i}^{\\sqrt{N}}_{i=1} \\subset \\mathbb{R}^{m}$ and ${b_j}^{\\sqrt{N}}_{j=1} \\subset \\mathbb{R}^{d}$ in our formulation. The composed\nexpert network can then be expressed as:\n$E_{ij}(x) = V_j\\sigma(U_ix + b_i) + b'_j,$(6)\nwhere (i, j)-th expert is formed by combining the i-th bottom layer with the j-th top layer.\nAs illustrated in Figure 1, this decomposition enables constructing N unique experts using only\n$\\sqrt{N}$ weight choices from each group (0 < i, j < $\\sqrt{N}$). Unlike PEER, which searches for top-k\nexperts among $k^2$ candidates, we directly use the Cartesian product $K_h = K_h^1 \\times K_h^2$, which breaks\ndown joint (i, j) pairs into independent i and j selections. The resulting SMoE layer with horizontal\ndecomposition is defined as:\n$MoHDE(x) = \\sum_{h=1}^H\\sum_{(i,j)\\in K_h} g_{hij} E_{ij}(x)$(7)\n$= \\sum_{h=1}^H\\sum_{i\\in K_h^1}\\sum_{j\\in K_h^2} g_h^i g_h^j (V_j\\sigma(U_ix + b_i) + b'_j)$(8)\nwhere $g_h^i = softmax({\\langle w_{hi}, x_1\\rangle}_{i\\in K_h^1})$ and $g_h^j = softmax({\\langle w_{hj}, x_2\\rangle}_{j\\in K_h^2})$ are computed inde-\npendently for each group, with their product $g_{hij} = g_h^i g_h^j$; determining the expert's routing score.\nTo optimize computation across tokens with our decomposed expert structure, we address a key\nchallenge: sparse activations varying by token complicate efficient computation reorganization.\nWhile traditional SMoE models employ expert parallelism (Fedus et al., 2022b; Du et al., 2022),\nsuch strategies become impractical with our 262K composed experts. Following Pan et al. (2024);\nPuigcerver et al. (2023), we adopt dense routing to enable precomputation of overlapped layer op-\nerations by extending sparse routing scores to all experts:\n$\\begin{aligned}g_h^i &= \\begin{cases}g_h^i  &\\text{ if } i \\in K_h \\\\0 & \\text{ otherwise}\\end{cases} \\text{ and }\\\\\ng_h^j &= \\begin{cases}g_h^j &\\text{ if } j \\in K_h \\\\0 & \\text{ otherwise}\\end{cases}\\end{aligned}$(9)\nThis allows us to reorganize Equation 8 into a more computationally efficient form:\n$MoHDE(x) = \\sum_{h=1}^H\\sum_{i=1}^{\\sqrt{N}}\\sum_{j=1}^{\\sqrt{N}} g_h^i g_h^j (V_j\\sigma(U_ix + b_i) + b'_j)$(10)\n$= \\sum_{h=1}^H\\sum_{i=1}^{\\sqrt{N}}\\sum_{j=1}^{\\sqrt{N}} g_h^i g_h^j V_j\\sigma(U_ix + b_i) + \\sum_{h=1}^H\\sum_{i=1}^{\\sqrt{N}}\\sum_{j=1}^{\\sqrt{N}} g_h^i g_h^j b'_j$(11)\n$= \\sum_j^{\\sqrt{N}} V_j[\\sum_{h=1}^H g_h^j \\sum_{i=1}^{\\sqrt{N}} g_h^i \\sigma(U_ix + b_i)] + \\sum_{j=1}^{\\sqrt{N}} b'_j[\\sum_{h=1}^H g_h^j\\sum_{i=1}^{\\sqrt{N}} g_h^i ]$(12)\nBy strategically reordering the summations in Equation 12, we can precompute memory-intensive\noperations before and after the expert routing phase. We provide implementation details in Algo-\nrithm 1 of Appendix A.3.\nVertical Expert Decomposition (VD) As an orthogonal approach to horizontal decomposition,\nwe propose vertical decomposition that partitions each expert network along the vertical dimension\ninto left and right segments. Let $U_l, U_r \\in \\mathbb{R}^{m/2\\times d}$ and $V^{11}, V^{12}, V^{21}, V^{22} \\in \\mathbb{R}^{d/2\\times m/2}$ represent\nthe vertically splitted weights for the experts, and $b_{i1}, b_{j1} \\in \\mathbb{R}^{m/2}$ and $b_{i2}, b_{j2} \\in \\mathbb{R}^{d/2}$ denote the\nsplit biases. For the vertically decomposed experts, the expert network is defined as:\n$E_{ij}(x) = \\begin{bmatrix}V^{11}& V^{12}\\\\V^{21}& V^{22}\\end{bmatrix}\\sigma(\\begin{bmatrix}U_l\\\\U_r\\end{bmatrix}x+\\begin{bmatrix}b_{i1}\\\\b_{j1}\\end{bmatrix})+\\begin{bmatrix}b_{i2}\\\\b_{j2}\\end{bmatrix}$(13)\nand the expert layer is obtained as:\n$MoVDE(x) = \\sum_{h=1}^H\\sum_{i=1}^{\\sqrt{N}}\\sum_{j=1}^{\\sqrt{N}} g_{hij}\\begin{bmatrix}V^{11}& V^{12}\\\\V^{21}& V^{22}\\end{bmatrix}\\sigma(\\begin{bmatrix}U_l\\\\U_r\\end{bmatrix}x+\\begin{bmatrix}b_{i1}\\\\b_{j1}\\end{bmatrix})+\\begin{bmatrix}b_{i2}\\\\b_{j2}\\end{bmatrix}$(14)\n$=\\sum_{h=1}^H\\sum_{i=1}^{\\sqrt{N}}\\sum_{j=1}^{\\sqrt{N}} g_{hij} \\begin{bmatrix}V^{11}\\sigma(U_lx + b_{i1}) + V^{12}\\sigma(U_rx + b_{j1}) + b_{i2}\\\\V^{21}\\sigma(U_lx + b_{i1}) + V^{22}\\sigma(U_rx + b_{j1}) + b_{j2}\\end{bmatrix}$(15)\nWe divide the layer calculation into six terms (see Equation 15), with the complete derivation pre-\nsented in Appendix A.1. The overall computational cost is equivalent to horizontal decomposition,\nand the implementation details are provided in Algorithm 2 of Appendix A.3.\nAdaptive Routing with Batch Normalization To avoid the hardware inefficiency of top-k sort-\ning, we use Batch Normalization to estimate expert routing quantiles without performing top-k.\nInspired by BatchTopK (Bussmann et al., 2024), which enhances reconstruction in SAE, we apply\nbatch-level quantile estimation for more accurate routing. Batch Normalization automatically gath-\ners router logit statistics, which are used during inference. This method reduces training time while\nmaintaining performance.\nLoad Balancing Loss Load balancing loss is crucial in MoE models to promote uniform expert\nrouting, improving expert utilization and ensuring efficient parallelism when experts are distributed\nacross devices. While sparse routing mechanisms are widely used, some dense MoE models adopt\nentropy-based losses (Pan et al., 2024; Shen et al., 2023) since dense routing does not directly\ntrack expert selection frequencies. In a similar vein, we introduce an alternative uniformity loss,\nformulated as the KL divergence between a uniform distribution and the routing probabilities:\n$\\mathcal{L}_{unif} = \\frac{1}{2H\\sqrt{N}}\\sum_{h=1}^H\\sum_{i=1}^{\\sqrt{N}}log \\hat g_h^i - \\frac{1}{2H\\sqrt{N}}\\sum_{h=1}^H\\sum_{j=1}^{\\sqrt{N}}log \\hat g_h^j$(16)\nAdditionally, we introduce an ambiguity loss that measures the degree of expert specialization for\neach token:\n$\\mathcal{L}_{amb} = \\frac{1}{2H}\\sum_{h=1}^H (1 - max g_h^i) + \\frac{1}{2H}\\sum_{h=1}^H (1 - max g_h^j)$.(17)\nThis loss encourages the model to assign each token to a specific expert with high confidence. By\nminimizing this ambiguity loss, the model promotes expert specialization, resulting in more distinct\nand interpretable expert roles. Ablations study on load balancing loss is presented in Appendix C.1.\nLet $\\mathcal{L}_{LLM}$ be a language modeling loss and $\\lambda$ be a hyperparameter. The final training objective is:\n$\\mathcal{L} = \\mathcal{L}_{LLM} + \\lambda \\mathcal{L}_{unif} + \\lambda\\mathcal{L}_{amb}.$(18)"}, {"title": "4 EXPERIMENTS", "content": "4.1 MODEL SETUPS\nIn order to assess practical applicability and scalability of MONET, we vary model parameter sizes\nranging from 850 million to 4.1 billion and CODEMONET at 1.4 billion parameters. In addition,\nwe train models using the LLAMA architecture for fair comparison. All models are pretrained\non large-scale datasets, and we further fine-tune MONET-1.4B for instruction-following MONET-\n1.4B CHAT for automated interpretation framework. For detailed pretraining configurations and\ninstruction tuning methods, refer to Appendix B."}, {"title": "4.2 OPEN-ENDED BENCHMARK RESULTS", "content": "Empirical evaluations in Table 2 show that MONET maintains competitive performance with total\nparameter-matched dense LLMs across a range of language modeling benchmarks. On the other\nhand, SAEs fall short in maintaining model stability, where reconstruction errors lead to instabil-\nity and reduced performance in open-ended tasks, compromising the model's overall reliability in\nknowledge control. We evaluate Gemma 2 2B (Team et al., 2024) using Gemma Scope (Lieberum\net al., 2024), a collection of SAEs trained on Gemma 2 models. Specifically, we employ the avail-\nable SAEs with 65K sparse features-both those reconstructing the LLM's MLP output and those\nreconstructing residual layers-and evaluate their performance on open-ended benchmarks.\nThe scalability of MONET is evident across all three parameter scales (850M, 1.4B, and 4.1B). As\nthe number of parameters increases, the model exhibits a consistent upward trend in performance\nacross both 0-shot and 5-shot settings. This confirms that the scaling laws typically observed in\ndense models still apply to MONET's sparse architecture, further reinforcing its scalability and prac-\ntical applicability for large-scale LLM deployments. In terms of the decomposition design choice,\nvertical decomposition (VD) shows superior performance over horizontal decomposition (HD). As\nshown in Table 2, MONET-VD consistently outperforms MONET-HD across multiple benchmarks\nand parameter scales, particularly in the 850M, 1.4B, and 4.1B models."}, {"title": "4.3 QUALITATIVE RESULTS", "content": "In this section, we present qualitative analyses demonstrating the monosemantic specialization of\nindividual experts in our MONET architecture. In Figure 2, we visualize the routing scores allocated\nto the experts in our language models on the C4 (Raffel et al., 2020) and StarCoder subset. We\ninclude comprehensive examples illustrating the internal workings of models with varying sizes\n(MONET-1.4B, MONET-4.1B) and a model pretrained on code (CODEMONET)."}, {"title": "5 ANALYSES", "content": "Leveraging transparent observations of expert routing patterns in each layer of the MONET, we em-\nploy observational methods for knowledge editing. In particular, we explored the effects of knowl-\nedge unlearning by selectively removing experts based on their routing score, $g_{hij}$ in Equation 7.\nOur unlearning analyses highlight MONET's monosemanticity where experts encapsulate disentan-\ngled parametric knowledge across domains, programming languages, and toxicity.\n5.1 DOMAIN MASKING\nUsing the MMLU Pro (Wang et al., 2024) benchmark taxonomy, which divides question-answer\nsets into 14 distinct domains, we investigated the effects of domain-specific knowledge unlearning\non MMLU (Hendrycks et al., 2021). For each expert, if the routing probability for a particular\ndomain was at least twice as high as for the second most activated domain, we labeled that expert\nas specialized in that domain. After assigning experts to domains, we selectively deleted the experts\nand evaluated the impact of knowledge unlearning across all 14 domains. The details of the expert\ndeletion process and its impact across the 14 domains are provided in Appendix D.1."}, {"title": "5.2 MULTILINGUAL MASKING", "content": "In addition to domain masking, we performed a similar evaluation of programming language\nmasking using CODEMONET 1.4B. Again, we utilized the skewness in routing scores to identify\nlanguage-specific experts. Table 3 summarizes the changes in pass@100 performance metrics after\nexpert purging evaluated on MULTIPL-E benchmark (Cassano et al., 2023). For the targeted lan-\nguages, pass@100 scores dropped by as much as -30%p, while average performance for other lan-\nguages remained relatively stable, with only minor declines ranging from -0.6% to -1.8%p. CODE-\nMONET's generation examples before and after the expert purging can be found in Figure 4 of\nAppendix D.2. All metrics were evaluated using a temperature of 0.8 and 200 sample generations,\nwhere its full performance are available in Table 15 of the Appendix E."}, {"title": "5.3 TOXIC EXPERT PURGING", "content": "To fundamentally adjust model behavior for safer language generation, we propose a method for\npurging toxic experts from the model. This approach directly removes experts associated with toxic-\nity, resecting the harmful knowledge while preserving the overall performance of the LLM. We eval-\nuate this method on two well-established toxicity benchmarks: REALTOXICITYPROMPTS (Gehman\net al., 2020) and ToxiGen (Hartvigsen et al., 2022), to assess its impact on toxicity reduction.\nFor toxicity evaluation, we utilize the PERSPECTIVE API (Lees et al., 2022) for REALTOXICI-\nTYPROMPTS and the ToxiGen RoBERTa model for the ToxiGen benchmark, both designed to mea-\nsure the generation of toxic content. To identify toxic knowledge within the model, we collected"}, {"title": "6 CONCLUSION", "content": "We introduced MONET, an SMoE architecture with 262,144 experts designed to address the chal-\nlenge of polysemanticity in LLMs. By integrating sparse dictionary learning directly into end-to-end\nSMoE pretraining, MONET overcomes the limitations associated with the post-hoc reconstruction\nloss of SAEs. Our novel product key composition alleviates the memory constraints of conventional\nSMoE architectures, allowing the expert count to scale to 262,144 per layer while ensuring that total\nparameters grow proportionally to the square root of the expert count. This substantial expansion\nenables fine-grained specialization, resulting in monosemantic experts that capture mutually exclu-\nsive aspects of knowledge. We demonstrated that MONET enhances mechanistic interpretability\nby facilitating transparent observations of expert routing patterns and individual expert behaviors.\nMoreover, MONET allows for robust manipulation of knowledge across domains, languages, and in\nmitigating toxicity, all without degrading the model's general performance. Our findings suggest\nthat scaling the number of experts and fostering monosemantic specialization within LLMs hold\nsignificant promise for advancing both interpretability and controllability, paving the way for future\nresearch into transparent and aligned language models.\nLimitations Regarding expert selection, we observed that the skewness of routing scores can de-\ntermine the domain specialization of experts, and we identified toxic experts by calculating the\nPearson correlation coefficient between toxicity scores and routing scores. We acknowledge that\nthese criteria are basic and minimal, and we believe that developing more advanced expert selection\nmethods is a promising direction for future research. Additionally, we should explore automated\ninterpretation techniques as self-explained experts are currently demonstrated only qualitatively,\nremaining quantitative evaluation on automated interpretability an open question. Finally, our appli-\ncation of parametric knowledge manipulation is limited to knowledge unlearning. We believe that\nobservations on monosemantic experts can help address research questions related to hallucinations\n(e.g., \"Is the model confident in retrieving internal knowledge?\") and lifelong learning in SMOE\nLLMs, which is expected to be a promising field (Chen et al., 2023; Li et al., 2024)."}]}