{"title": "Peeling Back the Layers: An In-Depth Evaluation of Encoder Architectures in Neural News Recommenders", "authors": ["Andreea Iana", "Goran Glava\u0161", "Heiko Paulheim"], "abstract": "Encoder architectures play a pivotal role in neural news recommenders by embedding the semantic and contextual information of news and users. Thus, research has heavily focused on enhancing the representational capabilities of news and user encoders to improve recommender performance. Despite the significant impact of encoder architectures on the quality of news and user representations, existing analyses of encoder designs focus only on the overall downstream recommendation performance. This offers a one-sided assessment of the encoders' similarity, ignoring more nuanced differences in their behavior, and potentially resulting in sub-optimal model selection. In this work, we perform a comprehensive analysis of encoder architectures in neural news recommender systems. We systematically evaluate the most prominent news and user encoder architectures, focusing on their (i) representational similarity, measured with the Central Kernel Alignment, (ii) overlap of generated recommendation lists, quantified with the Jaccard similarity, and (iii) the overall recommendation performance. Our analysis reveals that the complexity of certain encoding techniques is often empirically unjustified, highlighting the potential for simpler, more efficient architectures. By isolating the effects of individual components, we provide valuable insights for researchers and practitioners to make better informed decisions about encoder selection and avoid unnecessary complexity in the design of news recommenders.", "sections": [{"title": "1 Introduction", "content": "Content-based neural models have become the state of the art in news recommendation. Neural news recommenders (NNRs) typically comprise a news encoder and a user encoder. The news encoder learns semantically meaningful representations of news articles, whereas the user encoder embeds the preferences of users based on their click history [51]. NNRs take the candidate news articles and a user's reading history as input. The relevance of the candidate to the user is determined by comparing, with a scoring function, the latent representations of the two inputs, generated with the corresponding encoders. Given the key role of encoders in NNRs, a significant body of research has focused on improving the quality of news encoding and user modeling to improve recommendation performance [17, 34, 51].\nOn the one hand, ablation studies of recommenders typically analyze individual model components in isolation, neglecting other architecturally comparable model designs [1, 44, 47]. At the same time, we see emerging evidence that widely used NNRs exhibit similar performance despite varying model complexities, and that the overall complexity of the recommenders' architecture could be reduced [13, 28]. This highlights the need for a more granular comparison of the individual building blocks to understand their behavior and impact on the overall system. While M\u00f6ller and Pad\u00f3 [27] or Iana et al. [13] evaluated NNR components such as scoring functions and training objectives, a systematic analysis of encoder architectures is still lacking. Such insights would enable researchers and practitioners alike to make more informed choices about encoder selection in NNR design.\nOn the other hand, progress in the architectural design of news and user encoders is generally measured in terms of the recommender's overall classification and ranking capability [1, 13, 31, 44, 54]. Nonetheless, the quality of the embeddings produced by the news and user encoders is equally crucial, given the reliance of the recommender on the dense retrieval paradigm. Therefore, evaluating NNRs and their components solely in terms of downstream recommendation performance provides a simplified perspective, potentially overlooking subtle differences in the encoders' behavior. We thus argue that investigating the similarity of embeddings generated by various news and user encoders would offer a more nuanced understanding of their behavior, in turn benefiting the model selection process.\nIn this work, we perform a systematic analysis of the encoder architectures of NNRs. Unlike conventional evaluation studies, we isolate the effects of each core component to the largest possible extent. Concretely, we analyze the most prominent news and user encoder architectures in terms of (i) the similarity of learned news, and respectively, user representations, using the Central Kernel"}, {"title": "2 Related Work", "content": "Neural news recommenders have significantly advanced in recent years, with encoder architectures playing a key role in capturing the semantic and contextual information of news articles and user profiles. Consequently, a large strand of work has focused on improving the representational capabilities of recommenders by developing ever more accurate, and often complex, news encoding and user modeling architectures. As such, these works have analyzed individual aspects of the NNR components, such as the use of different attention mechanisms in the news or user encoder [32, 44, 47], the impact of various user modeling [1, 13, 31, 32, 42] or news embedding [15, 23, 32, 41, 44, 47, 54] techniques, or the importance of modeling different news features [30, 41, 44, 45, 52, 55, 62] and user characteristics [1, 46, 47, 58]. Ablation studies in these cases are usually conducted in isolation for the component under consideration, without taking into account the broader architectural context.\nIn contrast, another strand of work has started evaluating the impact of NNR components or training strategies across an array of recommendation approaches. For example, Wu et al. [54] have investigated the usage of various pretrained language models as the backbone of widely used NNRs. M\u00f6ller and Pad\u00f3 [27] have evaluated the impact of scoring functions, whereas Iana et al. [13] have analyzed different user modeling techniques and training objectives. The latter have highlighted the similar recommendation performance achieved by certain models despite differences in architectures and complexity, emphasizing the potential to simplify the design of news recommender systems. While these works shed new light on core components of the recommendation model, their evaluation is most often solely based on the downstream recommendation performance.\nThe similarity of encoders in NNRs can additionally be measured in terms of their generated representations. More generally, there exist numerous methods for quantifying the similarity of neural networks. Two main categories include (i) representational similarity, which assesses differences in the activations of intermediate layers of neural networks, and (ii) functional similarity, which compares the networks' outputs in relation to their task [21]. Several works have focused on evaluating the representational similarity of (large) language models [3, 6, 20, 61] or of embedding models in Retrieval Augmented Generation systems [4], which are often employed as the news encoding component of NNRs.\nNevertheless, to the best of our knowledge, no work so far compares neither user encoders nor news encoders with respect to representational and functional similarity. In this work, we fill this gap by comprehensively analyzing the primary components of NNR encoder architectures for both news and user inputs."}, {"title": "3 Methodology", "content": "We firstly introduce the building blocks of personalized NNRs. Afterwards, we discuss metrics to evaluate both the recommendation performance, as well as the representational similarity of the news and user encoders."}, {"title": "3.1 Encoders of Neural News Recommenders", "content": "Content-based neural news recommenders consists of a dedicated (i) news encoder (NE) and a (ii) user encoder (UE) [51]. The NE transforms different input features (e.g., title, abstract, categories, named entities, images) of a news article n into a latent news representation n. The UE aggregates the embeddings of the clicked news n from a user's u history into a user-level representation u. Finally, the embedding of a candidate news n\u00ba, outputted by the NE, is scored against the user representation u produced by the UE, to determine the relevance of the candidate to the user s(n\u00ba, u). The dot product of the two embeddings n\u00ba and u is the most common scoring function [44]. NNRs are trained via conventional classification objectives [11] with negative sampling [48], or contrastive objectives [14, 25]. The building blocks of NNRs (i.e., NE, UE, scoring function, training objective) altogether drive the overall performance of the recommender. Since the NE and UE determine what information of the documents and users is embedded by the model, and ultimately, propagated through the recommendation pipeline, both types of encoders play a similarly important role in model selection. We introduce the abbreviations used for the remainder of the paper in Table 1.\nNews Encoder Architectures. The NE can generally be decomposed into a text encoder, which embeds the textual content of a news article, and several feature-specific encoders (e.g., category, sentiment, entity encoder), which learn to represent further input features different from text chunks. While the former represents"}, {"title": "3.2 Similarity Evaluation", "content": "We evaluate NEs and UEs on three dimensions: (i) downstream recommendation performance, (ii) similarity of generated recommendations, and (iii) similarity of learned news or user representations.\nDownstream Recommendation Performance. NNRs are usually evaluated with regards to classification (e.g., AUC) and ranking (e.g., MRR, nDCG) performance. In this work, we focus on the ranking performance, which we quantify using nDCG@k.\nSimilarity of Generated Recommendations. We analyze the retrieval similarity of recommenders that use different news or user encoder architectures by the similarity of their top-k recommended articles. Specifically, for the same set of users, we firstly generate the corresponding recommendation lists R and R' with models M and M', respectively. We then measure the similarity of retrieved results with the Jaccard similarity coefficient:\n$$Jaccard(R, R') = \\frac{|R \\cap R'|}{|R \\cup R'|}$$\nwhere |R \u2229 R'| denotes the set of articles recommended by both models, and |R \u222a R'| the union of all unique news recommended by the two models. The Jaccard similarity score is bounded in the [0, 1] interval, with 1 indicating that both models recommend an identical set of news. Note that the lengths of both recommendation lists will be equal to the full set of candidate news N for a given user u, namely |R| = |R'| = |N|, regardless of the recommendation model used. Thus, to differentiate the retrieval performance of two models, we compute the Jaccard similarity only for the top-k recommendations, ordered descendingly by the recommendation scores. Note that in comparison to nDCG@k, the Jaccard similarity measures the overlap of the recommended news between two models without considering the order of the articles in the recommendation set."}, {"title": "Embedding Similarity", "content": "Numerous measures quantify the representational similarity of neural networks [21]. Many of these methods require an identical dimensionality of the compared embeddings or an alignment of the latent representation spaces across models. Since these constraints are not straightforwardly met by the embeddings produced with different news and user encoder architectures, we choose to measure the similarity of embeddings using the Centered Kernel Alignment (CKA) with a linear kernel [22]. Concretely, for a given representation E, we firstly mean-center it column-wise. Afterwards, we compute the pair-wise similarity of the representation of each instance i to all other instances in E. Each row i in the resulting similarity matrix S thus comprises the similarity between instance's i embedding and all other embeddings, including itself. For two different models with the same number of embeddings E and E', the resulting representational similarity matrices S and S', respectively, can be directly compared using the Hilbert-Schmidt Independence Criterion (HSIC) [9] as follows:\n$$CKA(E, E') = \\frac{HSIC(S, S')}{\\sqrt{HSIC(S, S)HSIC(S', S')}}$$\nThe CKA similarity scores are bounded to the interval [0, 1], with a score of 1 denoting equivalent representations."}, {"title": "4 Experimental Setup", "content": "Data. We conduct experiments on the MINDsmall [60] dataset. Since Wu et al. [60] do not release the test set labels, we use the validation portion for testing, and split the respective training set into temporarily disjoint training (the first four days of data) and validation (the last day of data) subsets.\nEvaluation Setup. We separately evaluate the encoder architectures of NNRs. In all experiments, we consider both mono-feature (e.g., title) and multi-feature (e.g., title and categories) inputs for the NE. In the latter case, we learn category representations by means of a linear encoder that combines a category ID embedding layer with a dense layer [1, 31, 42, 44]. Moreover, in our analysis of NE architectures, we adopt the late fusion approach [13] instead of the traditional parameterized UEs. This evaluation setup allows us to isolate the effects of NEs and to avoid additional confounding factors stemming from the UE, which also influence the output of the NNR. Similarly, when evaluating the similarity of UE architectures, we keep the underlying NE of the recommender fixed, i.e., we analyze different UEs for the same base NE.\nImplementation and Optimization Details. We train all models with the standard cross-entropy loss, using dot product as the scoring function. We use 300-dimensional pretrained Glove embeddings [29] to initialize the word embeddings of the word embedding-based text encoders. Additionally, we use ROBERTa-base [26] and the news-specialized multilingual sentence encoder NaSE [15] for the PLM-based and SE-based text encoders, respectively. We fine-tune only the last four layers of the language models. Following prior work [48], we sample four negatives per positive example during training. We set the maximum history length to 50 and train all models with mixed precision, the Adam optimizer [19], and a batch size of 8. We train all NNRs with word embedding-based NEs for 20 epochs, and those with language model-based NEs for"}, {"title": "5 Results and Discussion", "content": "We begin by analyzing the similarity of core NE architectures, followed by an evaluation of UE similarity using the same base news encoding approach. In both cases, we first compare the architectures in terms of ranking performance and retrieval similarity, as these are standard evaluation approaches in the recommender systems field. We then assess the architectures from the perspective of pairwise embedding similarity."}, {"title": "5.1 News Encoder Architectures", "content": "Figure 1 shows the ranking performance, in terms of nDCG@10, of NNRs for different news encoders and input features. For the same input type, e.g. mono-feature, we find a high similarity between the performance of recommenders based on the same family of text encoders. Specifically, text encoders using pretrained static word embeddings are outperformed by those based on PLMs. Moreover, MHSA+AddAtt and CNN+MHSA+AddAtt appear to have nearly identical performance, despite the increased complexity of the latter architecture. Similarly, simply using the [CLS] token representation produced by the PLM instead of pooling tokens with an attention network as proposed by Wu et al. [54] leads to slightly better performance while maintaining a lighter text encoder.\nOur findings show that among the three multi-feature aggregation strategies, the Linear and AddAtt approaches always outperform the Con technique. This is intuitive, as the concatenation of vectors with varying dimensionality from non-aligned representation spaces will be sub-optimal. In contrast, both other aggregation strategies project the intermediate text and category embeddings in the same latent representation space. Most importantly, we find that leveraging categories in addition to textual news content as input features is most beneficial for word embedding-based text encoders, and becomes irrelevant or slightly detrimental for the domain-adapted sentence encoder. This can be explained, on the one hand, by the better representational capabilities of the much larger language models which acquire contextual understanding during pretraining compared to static word embeddings. On the other hand, sentence encoders, especially domain-specialized models such as NaSE [15], better capture nuances and topics from text due to their pretraining objectives that focus on the overall sentence-level semantics.\nWe find these similarities in ranking performance between the various news encoding architectures to be reflected in the similarity of retrieved articles. Figure 2 illustrates the pair-wise Jaccard similarity scores between the top-10 recommended news per model. Note that we exclude PLMtokenemb+Att, as well as the Con multi-feature aggregation strategy from further analysis for the sake of brevity"}, {"title": "5.2 User Encoder Architectures", "content": "We next investigate the ranking performance, with regards to nDCG@10, for different UE architectures for the same base NE. Figure 5 displays the corresponding results, for both mono-feature, and well as multi-feature input. We find that the LF, AddAtt, and CandAware encoders perform the best across all families of NEs.\nMore specifically, the much simpler LF and AddAtt encoders outperform the complex CandAware modeling technique in the case of language model-based NEs, and perform similarly with CandAware for word embedding-based NEs, as previously suggested by Iana et al. [13]. Surprisingly, these two approaches also consistently achieve better ranking than sequential-based UEs (i.e., GRU+MHSA+AddAtt, GRUini, GRUcon). Once again, we see that using categorical information alongside the textual content as input to the NE benefits all recommenders regardless of the UE family. The only exception, as previously discussed, are SE-based NNRs. Interestingly, we see that multi-feature inputs close the gap (i) in between inter-family UEs for the same base NE, and (ii) across intra-family UEs for different underlying NEs. Most importantly, our findings corroborate earlier results from Iana et al. [13] and M\u00f6ller and Pad\u00f3 [28] that the complexity of user encoders can be simplified, particularly when the bi-encoder NNR leverages language models pretrained, or even domain-specialized, on large-scale corpora, to obtain news representations.\nThe heatmap in Figure 6 shows the Jaccard similarity scores for the top-10 recommendations, for the different UE families, when using only the title as input to the NE. We exclude GRUcon from further analysis as it underperforms the counterpart variant GRUini. We observe that in terms of retrieval similarity, the NNRs are clustered based on the underlying NE family, regardless of the UE used. Once again, the results indicate a large overlap of recommended news (i.e., on average, of at least 7 out of 10 recommendations) for the UEs within these clusters. Moreover, we observe comparable similarity patterns across inter-family UEs for the same NE family; different NEs change only the absolute magnitude of the Jaccard similarity scores. Within intra-family clusters of NEs, the findings re-affirm that LF and AddAtt have the highest overlap in terms of the top-10 recommended articles; their generated recommendations usually differ in at most 2 or 3 items, on average. This is intuitive, as LF represents a special case of AddAtt, where the attention weights are all equal, and set to the inverse of the user's history length.\nWe delve deeper into the retrieval similarity of UE architectures."}, {"title": "5.3 Key Takeaways", "content": "Following the results of our in-depth analysis of the embedding and retrieval similarity of the most prominent news and user encoder architectures, we highlight several key takeaways.\nSemantic Richness is Key. Our analysis demonstrates that the semantic richness of news encoders, achieved either through multi-feature input or contextualized language models, significantly outweighs the impact of UEs. This is particularly the case when initializing news representations with large-scale PLMs. Additionally, contextualized language models can effectively capture semantic nuances, such as topical information, without heavily relying on categorical annotations. From a practical standpoint, this reduces the need for manual or automatic feature engineering, streamlining the NNR design process. We hence argue that research on news encoding should focus more on leveraging and adapting existing semantically informed, contextualized language models for the task of news recommendation, rather than on incrementally modifying existing architectures.\nUser Encoders Can be Considerably Simplified. Our findings show that retrieval similarity is primarily influenced by the underlying NE family, rather than the specific UE used. At the same time, simpler approaches such as LF and AddAtt not only result in significantly better ranked results, but their retrieved items largely overlap with those recommended by more complex UE architectures. These findings thus render simpler architectures as better and more lightweight user modeling alternatives. Additionally, the high retrieval similarity between parameter-free (i.e., LF) and parameterized (e.g., AddAtt) encoders heavily indicates that, in practice, there is little empirical justification for an additional parameterized component in the news recommender system. Furthermore, the similarity of sequential and non-sequential encoders indicates that treating news recommendation as a sequential problem might be sub-optimal. We speculate that the high item churn characteristic of news, combined with short user histories, limit the benefits of differentiating between long and short-term user preferences, in contrast to other domains, such as movie or book recommendation. In conclusion, in line with M\u00f6ller and Pad\u00f3 [28], we posit that user modeling should not focus exclusively on the architectural component, but instead, should pay closer attention to the users'"}, {"title": "6 Conclusion", "content": "Despite the central role played by encoder architectures in neural news recommenders, their advancement and understanding is generally limited to one-sided evaluation in terms of recommendation performance. In this work, we conducted a comprehensive evaluation of encoder architectures in neural news recommenders, by systematically analyzing their (i) representation similarity, (ii) overlap of generated recommendations, and (iii) overall recommendation performance. Evaluations of recommenders on standard benchmarks often reveal insignificant performance differences between compared models or among their ablated components. Consequently, our analysis of differences in representational similarity and retrieval overlap of neural news recommenders serves as a complementary evaluation tool for understanding the relationship between the architectural design, behavior, and downstream performance of models.\nOur findings offer more nuanced insights into the interplay of news and user encoders, and challenge the assumption that complex encoding techniques are essential for accurate news recommendation. We demonstrate that simpler, yet equally effective architectures can yield comparable results. This underscores the importance of understanding recommenders' behavior from multiple perspectives, and of balancing model complexity with performance. Specifically, we emphasize three key takeaways: (1) the crucial role of semantic richness in news encoders, (2) the potential for simplifying user encoders without sacrificing accuracy, and (3) the need for more rigorous evaluation and ablation studies to inform architectural design choices. By fostering a more transparent and nuanced understanding of encoder architectures in neural news recommenders, we hope to guide researchers and practitioners toward more efficient and effective model designs."}]}