{"title": "Natural Language Supervision for Low-light Image Enhancement", "authors": ["Jiahui Tang", "Kaihua Zhou", "Zhijian Luo", "Yueen Hou"], "abstract": "With the development of deep learning, numerous methods for low-light image enhancement (LLIE) have demonstrated remarkable performance. Mainstream LLIE methods typically learn an end-to-end mapping based on pairs of low-light and normal-light images. However, normal-light images under varying illumination conditions serve as reference images, making it difficult to define a \"perfect\" reference image This leads to the challenge of reconciling metric-oriented and visual-friendly results. Recently, many cross-modal studies have found that side information from other related modalities can guide visual representation learning. Based on this, we introduce a Natural Language Supervision (NLS) strategy, which learns feature maps from text corresponding to images, offering a general and flexible interface for describing an image under different illumination. However, image distributions conditioned on textual descriptions are highly multimodal, which makes training difficult. To address this issue, we design a Textual Guidance Conditioning Mechanism (TCM) that incorporates the connections between image regions and sentence words, enhancing the ability to capture fine-grained cross-modal cues for images and text. This strategy not only utilizes a wider range of supervised sources, but also provides a new paradigm for LLIE based on visual and textual feature alignment. In order to effectively identify and merge features from various levels of image and textual information, we design a Information Fusion Attention (IFA) module to enhance different regions at different levels. We integrate the proposed TCM and IFA into a Natural Language Supervision network for LLIE, named NaLSuper. Finally, extensive experiment demonstrate the robustness and superior effectiveness of our proposed NaLSuper.", "sections": [{"title": "I. INTRODUCTION", "content": "HIGH-QUALITY images are crucial for various advanced computer vision tasks, e.g., object detection[30], image classification[1] and semantic segmentation[16], etc. However, images captured in low-light conditions often suffer from issues like low contrast, low brightness, and serious noises. Therefore, it is practically important to address brightness degradation to facilitate the exploration of sophisticated dark environments. To improve the quality of these images, numerous low-light image enhancement (LLIE) methods[33], [8], [10], [13], [31], [12], [19], [54] have been proposed in recent years.\nIn recent years, the effectiveness of deep learning methods in computer vision applications has spurred their application in LLIE[37], [12] [50], [31], [26], [42], [53], [62]. These deep learning-based methods for LLIE can be categorized into two main groups: Retinex-based methods[50], [29], [32], and end-to-end methods[27], [19]. Retinex-based methods initially decompose the image into illumination and reflectance component using a convolutional neural network. These components are then processed separately before being recombined to produce the final enhanced image. However, Retinex-based models are prone to image stylization and noise amplification due to inadequate decomposition of reflectance and illumination, which leads to optimization challenges. On the other hand, end-to-end methods aim to learn the mapping between low-light and normal-light images without relying on any physical model. Despite their potential, current end-to-end methods face issues such as underexposure, overexposure, and color imbalance, and their overall generalization performance remains relatively low.\nThe goal of LLIE is ensuring that the enhanced image's subjective visual experience closely resembles that of a natural image under normal lighting. To achieve this, many existing methods mainly use normal-light image as a supervisory constraint. However, the variability in illumination of these normal-light images makes it difficult to define a \"perfect\" reference image. Furthermore, existing methods primarily focus on generating results that perform well according to specific metrics, often at the expense of visual quality. Thus these methods face the challenge of reconciling metric-oriented and visual-friendly result.\nRecent research in cross-modal learning[35], [45], [7], [63] has shown that side information from related modalities can effectively guide visual representation learning. Based on this, we propose that learning from text corresponding to images is a promising alternative for LLIE, providing a general and flexible interface for describing an image of different illuminations. Building on this foundation, to enhance low-light image, we propose a Natural Language Supervision strategy, which jointly learns feature maps from text and corresponding image. However, the highly multimodal distribution of images conditioned on textual descriptions poses training challenges, limiting the application of natural language supervision in LLIE. The work[51] found that incorporating the connections between image regions and sentence words generally, which enhances the ability of network to capture fine-grained cross-modal cues for images and text. Building upon this inspiration, we introduce a Textual Guidance Conditioning Mechanism (TCM), employing cross-attention to comprehensively capture both cross-modal and intra-modal relationships between image regions and sentence words.\nAs the network goes deeper and deeper, shallow feature of text and image information is often difficult to preserve. In order to effectively identify and merge features from various levels of image and textual information, we design a novel information fusion attention (IFA) module to improve feature representation, which can provide additional flexibility in dealing with different modal of information.\nIn this paper, we propose a Natural Language Supervision network (denoted as NaLSuper) for LLIE, which incorporates TCM and IFA modules. Overall, our contributions can be summarized as follows:\n1) We propose a Natural Language Supervision network (denoted as NaLSuper) for LLIE, which incorporates Textual Guidance Conditioning Mechanism (TCM) and Information Fusion Attention (IFA) modules.\n2) We are the first to utilize a Natural Language Supervision strategy in LLIE, which use this strategy results in better visual effect of the enhanced image. To address the training challenges posed by this strategy in a multi-modal data distribution, we have developed the TCM, which can comprehensively capture both cross-modal and intra-modal relationships between image regions and sentence words.\n3) We design a novel information fusion attention (IFA) module to improve feature representation, which can provide additional flexibility in dealing with different modal of information. Thanks to this module, our network can effectively identify and merge features from various levels of image and textual information.\n4) Extensive tests conducted on four benchmark dataset reveal that the proposed NaLSuper outperforms recent state-of-the-art methods in both quantitative and qualitative evaluations of quality."}, {"title": "II. RELATED WORK", "content": "A. Low-light Image Enhancement\n1) Traditional Cognition Methods: In regions with low lighting, pixel values are generally lower. It is a very forthright idea to directly adjust brightness of image through enhance these lower pixel values, which is referred to as value-based. The value-based methods include histogram equalization (HE) and gamma correction (GC). The conventional HE method [4] alters the histogram distribution. While it adjusts the illumination of the image, it introduces issues such as artifacts, loss of detail, overexposure, and color distortion. To address these shortcomings, researchers have implemented a series of enhancements to HE. Kim et al. [23] introduced Brightness Bihistogram Equalization (BBEH), while Wang et al. [47] proposed Dualistic Subimage Histogram Equalization (DSHE), both of which aimed to produce more natural-looking equalized images. Nonetheless, the visual artifacts in images equalized by HE methods remain their primary drawback.\nGC employs nonlinear transformations to enhance the gray values in darker areas of the image while reducing the gray values in areas with excessively high gray values. Traditional gamma transformations used for low-light image enhancement suffer from clear drawbacks, including unnatural images, uneven exposure, and loss of details. To address these issues, Bennett et al.[2] enhanced image brightness using Per-Pixel Virtual Exposures. However, the GC method still suffers from uneven exposure.\n2) Deep Learning Methods: With the successful application of deep learning in computer vision [40] and image processing [24], researchers have turned to deep learning techniques for LLIE. For instance, Lore et al. [31] introduced LLNet, a stacked sparse denoising auto-encoder designed for simultaneous low-light enhancement and noise reduction. Wei et al. [50] introduced a CNN-based Retinex decomposition method, establishing a deep network that integrates image decomposition and subsequent enhancement operations. Zhang et al. [60] merged Retinex theory with convolutional neural networks, dividing the network into two parts: illumination and reflectance estimation, and training it using gamma-corrected simulation data. Subsequently, Zhang et al. [59] optimized the model structure based on KinD, resulting in KinD++. Hao et al. [14] achieved Retinex image decomposition in a semi-decoupled manner. Liu et al. [29] proposed a lightweight method called Retinex-inspired Unrolling with Architecture Search (RUAS) for efficient low-light image enhancement. Ma et al. [32] summarized deep learning methods based on Retinex theory and a context-sensitive decomposition network, along with supervised and self-supervised versions. Zhu et al. [65] proposed a method involving a learnable guidance map from signal and deep priors, enabling adaptive enhancement of low-light images in a region-dependent manner. Zhao et al. [62] introduced a Retinex decomposition \"generative\u201d strategy, which formed the basis for a unified deep framework to estimate latent components and enhance low-light images. Wu et al. [52] developed three neural modules that facilitate image recovery in three phases: initialization, optimization, and illumination adjustment. The previous methods aimed at LLIE were either designed within a single scale framework or implemented through a cascaded process, which limits their effectiveness across various low-light conditions."}, {"title": "B. Natural Language Supervision", "content": "Vision-language pretraining has recently emerged as a promising approach for understanding images [18], [35], [45] and videos [43]. Unlike traditional approaches that rely on discrete labels, it introduces a new recognition paradigm based on the alignment of visual and textual features. Recent studies have also explored how to utilize the transferable knowledge of pre-trained models for tasks such as visual question answering (VQA) [20], zero-shot object detection [11], and image captioning [38]. For example, Radford et al. [35] introduced CLIP, a method for learning visual models under language supervision. After being trained on 400 million image-text pairs, CLIP is capable of describing any visual concept in natural language and can be applied to other tasks without further specific training. Zhou et al.. [64] introduced soft prompts, utilizing learnable vectors to model context words instead of traditional hand-crafted prompts, thereby capturing task-relevant context. Rao et al. [36] innovatively introduced context-aware prompting, integrating prompts with visual features for more precise instance-level refinement.\nCho et al. [5] harmonized prior knowledge across various tasks through a unified framework tailored to seven multi-modal tasks. Additionally, Ju et al. [22] leveraged the pre-trained CLIP model to enhance video comprehension. In this paper, we aim to explore how natural language supervision can be utilized for LLIE."}, {"title": "III. METHOD", "content": "In this section, we mainly introduce a Natural Language Supervision network (denoted as NaLSuper) for LLIE. As illustrated in Fig. 2, the input of NaLSuper is a low light image $I_{low} \\in R^{H \\times W \\times 3}$, it is first passed into a 3 \u00d7 3 convolution as a projection layer to extract shallow feature $F_o \\in R^{H \\times W \\times C}$. Next, $F_o$ is fed into three Residual Textual guide Fusion Block (RTFB) with multiple skip connections, which can extract deeper feature of fine-grained cross-modal information for images and text. Specifically, intermediate features outputted from RTFB are denoted as $F_1, F_2, F_3 \\in R^{H \\times W \\times C}$. After that, these features $F_1, F_2, F_3$ will be concatenate to $F_{con} E [R^{H \\times W \\times 3C}$, and then it passed to the reconstruction part and global residual learning structure, thereby getting a enhanced image $I_{out} \\in R^{H \\times W \\times 3}$\nFurthermore, we combines RTFB Architecture with local residual learning, every RTFB combines the Textual Guidance Conditioning Mechanism (TCM) and Information Fusion Attention (IFA) modules.\nA. Textual Guidance Conditioning Mechanism (TCM)\nCurrent approaches predominantly rely on image-level supervision, where the output is constrained to closely resemble target images. Yet, these methods faces challenges due to significant brightness discrepancies among different references, which can complicate model training. Additionally, some references exhibit visual flaws, such as unnatural brightness, resulting in outputs that are visually unappealing. To alleviate training complexities and reconcile the disparity between metric-oriented and visually pleasing versions, as shown in Fig. 2, we design a Textual Guidance Conditioning Mechanism (TCM) to comprehensively capture both cross-modal and intra-modal relationships between image regions and sentence words.\n1) Text Encoder: The main objective of the text encoder is to map the raw textual descriptions of interactions to the feature space. The raw text will be broken down into tokens and transformed into a series of word embeddings. Recent research [35], [64] indicates that the selection of context words around the class name can greatly affect the accuracy of recognition. In this work, we employ the recent well-known pretrained CLIP[35] text encoder and keep it fixed during training. As shown in Fig. 2, We first manually design a series of prompt, such as normal light image. After CLIP, text feature $T_i, [i = 1,2,...,n]$ is obtained, which has textual semantic information of corresponding image.\n2) Conditioning Mechanisms: Image distributions conditioned on textual descriptions are highly multimodal, which makes training difficult. To address this problem, we use the cross-attention mechanism as the fusion layer (refer to Fig. 2), which is valid for attention-based models that learn relationships between various input modalities [17]. To preprocess $T_i$ from different modalities (such as language prompts), we utilize a domain-specific encoder $T_e$ that maps $T_i$ to an intermediate representation $T_{\\theta}(T_i)$ in $R^{dr\\times M}$. Then the computation of query matrix Q, key matrix K, and value matrix V are as follow:\n$Q = W_q \\cdot (I_i), K = W_k\\cdot T_{\\theta}(T_i), V = W_v \\cdot T_{\\theta}(T_i)$, (1)\nwhere $W_q \\in R^{dxds}$, $W_k \\in R^{dxdr}$, and $W_v \\in R^{dxd}$ represent the projection matrices[17], [41], $I_i, [i = 1, 2, . . ., n]$ represent a feature map of image and $\\psi(I_i) \\in R^{ds \\times M}$ denotes a (flattened) intermediate representation. Suppose that we have $Q, K, V \\in R^{d\\times M}$, and the attention matrix is expressed as:\n$Attention(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d}} +B) V$, (2)\nwhere B is a learnable matrix of relative position encoding. Refer to Fig. 2 for a visual representation.\nB. Information Fusion Attention (IFA)\nMany LLIE networks uniformly process channel-wise and pixel-wise features, which fails to effectively address images with uneven illumination distribution and weighted channel-wise features. Additionally, the network goes deeper and deeper, shallow feature of text and image information is often difficult to preserve. In order to effectively identify and merge features from various levels of image and textual information. We design a Information Fusion Attention (IFA) module (refer to Fig. 3) includes both channel attention, pixel attention and Cross-layer Attention Fusion Block, which can provide additional flexibility in dealing with different modal of information."}, {"title": "1) Channel Attention:", "content": "Our channel attention focuses on the concept that various channel features carry distinctly different weights of information in relation to DCP [15]. Inspired by [34], we capture the channel-wise global spatial information through a channel descriptor by employing global average pooling.\n$g_c = P(F_c) = \\frac{1}{H W} \\sum_{i=1}^{H}\\sum_{j=1}^{W}X_c(i,j)$ (3)\nEq. 3 represents the global average pooling operation P(\u00b7) applied to a feature map. gc is the pooled feature for the channel c, obtained by averaging all pixel values $X_c(i, j)$ across the spatial dimensions H (height) and W (width) of the feature map $F_c$. This reduces the spatial dimensions of each channel to 1 \u00d7 1, effectively compressing the spatial information of the feature map into a single value per channel.\nTo determine the weights of various channels, the features go through two convolutional layers and subsequently are activated using sigmoid and ReLU functions. Finally, an element-wise multiplication is performed between the input feature map $F_c$ and the channel-specific weights. The Channel Attention process is formulated as:\n$F = Sigmoid(Conv(ReLu(Conv(g_c)))),$\n$F_{cout} = CA(F_c) = F F_c$. (4)"}, {"title": "2) Pixel Attention:", "content": "Given the uneven distribution of illumination across different pixels in an image, we employ a pixel attention module to make the network pay more attention to informative features. This includes areas with low-light and regions of the image with high frequency.\nSimilar to [34], we directly feed the input $F_{cout}$ into two convolutional layers and subsequently are activated using sigmoid and ReLU functions. Finally, an element-wise multiplication is performed between the input feature map $F_{cout}$ and the pixel-specific weights. The Pixel Attention process is formulated as:\n$F = Sigmoid(Conv(ReLu(Conv(F_{cout})))),$\n$F_{pout} = CA(F_c) = F F_{cout}$. (5)"}, {"title": "3) Cross-layer Attention Fusion Block:", "content": "Recent methods using transformers incorporate feature connections or skip connections to merge features across various layers, as discussed in studies by Zamir et al. [57] and Wang et al. [48]. Nevertheless, these methods do not completely leverage the inter-layer dependencies, which restricts their ability to represent complex features effectively. To address this problem, we use a Cross-layer Attention Fusion Block(CAFB) [46], which adaptively fuses hierarchical features with learnable correlations among different layers. The underlying concept of CAFB is that activations in various layers correspond to specific classes, and the correlations between these features can be dynamically learned through a self-attention mechanism.\nAs show in Fig. 2, given features $F_i, F_i, F_i \\in R^{H\\times W\\times C}$, we concatenate and reshape them to make $F_{in} \\in R^{H\\times W\\times 3C}$. Following [46], we use 1\u00d71 convolutional layers to integrate context across different channels at the pixel level, and then apply 3\u00d73 depth-wise convolutions to generate Q, K, and V. We reshape the matrices Q and K into 2D forms with dimensions 3\u00d7HWC (denoted as Q') and 3\u00d7HWC (denoted as K'), respectively. This restructuring allows us to compute the layer correlation attention matrix A, which has a size of 3 \u00d7 3. Finally, we scale the reshaped value V' \u2208 $R^{HWC\\times 3}$ by multiplying it with the attention matrix A and a scaling factor \u03b4.\n$Cross-Layer(Q', K',V') = softmax(\\frac{(Q' K')}{\\delta}) V'$, (6)\nThen, we add the input features Fin to this product. The CAFB process is formulated as:\n$F_{out} = Cross-Layer(Q', K', V') + F_{in}$, (7)\n$F_{out}$ represents the output feature that targets the informative layers within the network. In practical terms, we strategically position the CAFB at locations in the tail of each RTFB. This placement enables CAFB to effectively capture long-distance dependencies across hierarchical layers during both the feature extraction and image reconstruction phases."}, {"title": "C. Loss Function", "content": "There are two reconstruction loss terms to train our framework, i.e., the L\u2081 loss and the SSIM loss. The L1 loss is written as\n$L_1 = \\frac{1}{n}\\sum_{i=1}^{n} ||I_{gt} \u2013 enhanced (I_{low})||$, (8)\nwhere $I_{gt}$ denotes the ground truth, n is the batch size and $I_{low}$ stands for input(low-light image). The SSIM loss is written as\n$L_{SSIM} = 1-\\frac{1}{n}\\sum_{i=1}^{n} \\frac{(2\\mu_{I_{gt}}\\cdot \\mu_{I_{low}} + C_1) (2\\sigma_{I_{gt}, I_{low}} + C_2)}{(\\mu^2_{I_{gt}} + \\mu^2_{I_{low}} + C_1) (\\sigma^2_{I_{gt}} + \\sigma^2_{I_{low}} +c_2)}$, (9)\nwhere \u00b5 is the variances, and \u03c3 is the covariance. $c_1$ and $C_2$ are constants added to avoid having a denominator of zero. The overall loss function is\n$L = L_1 + L_{SSIM}$. (10)"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "A. Benchmark Datasets\nTo evolution the performance and efficiency of the proposed NaLSuper, we test our approach on several publicly accessible low-light datasets, such as LOLv1 [50], LOLv2, [55] and SID [3].\nThe LOL dataset is available in two versions: v1 and v2. LOLv1 provides 485 pairs of low-/normal-light images for training and 15 pairs for testing. Each pair consists of a low-light image and its corresponding well-exposed reference image. LOLv2 is split into two subsets: LOLv2-real and LOLv2-synthetic. The training set for LOLv2-real includes 689 pairs of low-/normal-light images, and the test set comprises 100 pairs. These low-light images are primarily captured in various settings by adjusting the ISO and exposure time, while keeping other parameters constant. LOL-v2-synthetic, however, is generated by analyzing the light distribution in low-light images and then synthesizing these from RAW images.\nThe subset of the SID dataset captured with the Sony a7S II camera is used for evaluation. It comprises 2697 pairs of short/long-exposure RAW images. The low-/normal-light RGB images are derived by applying the same in-camera signal processing used in SID to convert the RAW images into the RGB format. Specifically, 2099 image pairs are utilized for training, while 598 pairs are designated for testing.\nThe four widely adopted quality metrics, i.e., peak signal-to-noise ratio (PSNR), structural similarity (SSIM)[49], LPIPS and mean absolute error (MAE), are adopted for quantitative comparison as evaluation metrics between the enhanced images and the referenced ground truths. For the metrics PSNR and SSIM, higher values signify improved image quality. Conversely, a lower LPIPS and MAE score indicates superior quality.\nB. Implementation Details\nFor model training of our proposed network, we use the Adam optimizer with a learning rate of 10-4. All experiments on benchmark datasets are implemented with PyTorch, on a 64 core Intel Xeon Gold 6226R CPU @2.90GHz, 256 GB memory and a Nvidia Duadro RTX 8000 GPU.\nC. Quantitative Evaluation\nTo evaluate the advantages of proposed NaLSuper, we quantitatively compare our proposed method with several LLIE state-of-the art competitors, including BIMEF [56], FEA [6], LIME [13], MF [9], NPE [44], SRIE [10], MSRCR [21], RetinexNet [50], DSLR [28], KinD [60], MIRNet [58], Z_DCE [12], Z_DCE++ [25], RUAS [29], ELGAN [19], Uformer [48], Restormer [57], LL-Unet [39], DDC-net [61], LLFormer [46]. We utilize the publicly available source code and recommended parameters for each of the compared methods, and fine-tune all models on the training dataset to enable comparison.\nTable I tabulates the comparisons of averaged PSNR/SSIM/LPIPS/MAE scores tested on the LOLv1 testset. The top two results are marked in red, blue. The comparative results presented in the table clearly demonstrate that the proposed NaLSuper significantly outperforms other state-of-the-art methods on the LOLv1 testset. More specifically, our approach achieves the highest PSNR score compared to all other methods and exhibits substantial improvements, with gains reaching up to 0.36dB over the second-best results for the LOLv1 testset. The outstanding performance suggests that our method excels in producing results, wihch are highly faithful to the ground truths. Additionally, our approach also achieves the best SSIM and LPIPS scores compared to other competitors. Unlike PSNR, the SSIM and LPIPS quality metrics align more closely with the human visual system (HVS), providing a better consistency. As shown in Table I, the SSIM improvements reach up to 0.012 above the second-best results, while the LPIPS improvements reduce to 0.0158 below the second-best results for the LOLv1 test set. Moreover, the MAE scores also achieves the secondly compared to other competitors.\nTable II tabulates the comparisons of averaged PSNR/SSIM scores tested on the LOLv2-real, LOL-v2-syn and SID testset. The comparative results presented in the table clearly demonstrate that the proposed NaLSuper significantly outperforms other state-of-the-art methods on the LOLv2-real, LOLv2-syn and SID testset. More specifically, our approach achieves the highest PSNR score compared to all other methods and exhibits substantial improvements, with gains reaching up to -dB, -dB and -dB over the second-best results for the LOLv2-real, LOL-v2-syn and SID testset. The outstanding performance suggests that our method excels in producing results, wihch are highly faithful to the ground truths. Additionally, our approach also achieves the best SSIM scores compared to other competitors. As shown in Table II, the SSIM improvements reach up to above the second-best results for the LOLv2-real, LOL-v2-syn and SID testset. The remarkable results confirm that our method not only delivers the most accurate reconstruction but also excels in performance according to perceptual-oriented assessment outcomes.\nD. Subjective Evaluation\nTo further demonstrate the superiority of the proposed low-light image enhancement method over other competitors, we have selected nine methods with higher PSNR values for comparison on the LOLv1 and LOLv2-real dataset. The visual quality comparison results are shown in Fig. 5\u20138. The images contained in the LOLv1 and LOLv2-real dataset are taken indoors and are very dark and noisy, which is very challenging for many LLIE methods. The images with the LOLv1 and LOLv2-real dataset are captured indoors, characterized by low light and high noise levels, presenting a significant challenge for numerous LLIE methods. Based on the comparison results, our method demonstrates the most realistic outcomes, exhibiting minimal artifacts and noise in the predominant regions. Moreover, the enhanced results exhibit the most faithful representation of the target image in terms of brightness, contrast, and color. In comparison, other methods that were evaluated tend to create lots of unexpected halos in edge and textural areas, and some even cause noticeable color distortions across the reconstructed images. The proposed method benefits from the carefully designed natural language supervision strategy, enabling it to effectively address multiple complex degradations in extremely dark environments. It demonstrates proficient restoration of brightness, color, contrast, and detail.\nE. Ablation Study\n1) Model Architecture: To verify the advantages of the network architecture, we conducted an ablation study. We refer to the simplest version of the network, which lacks any specialized modules, as \"base\". Building on this \"base\" model, we introduce various configurations to determine if the ultimate model is indeed optimal. These models, each featuring a different configuration, are trained using a consistent strategy and evaluated across three publicly accessible datasets.\nBased on the average and standard deviation of PSNR, SSIM, LPIPS, and MAE as presented in Table III, the performance of Base has achieved comparable scores, demonstrating its efficacy. As illustrated in Fig. 9, on the basis of the Base module, integrated IFA can achieving the higher average values but this results in the loss of detail. In comparison, TCM can make them in images cleaner, which makes the image visual better. After combining the TCM and IFA, the result can be more vivid and natural appearance that enhances visual perception. It not only achieves the highest indicators but also has a good visual effect. Harmonization of metric-oriented and visually friendly results is achieved.\n2) Hyperparameters Experiments: In the experiment of hyperparameters, we mainly introduce the Residual Textual guide Fusion Block(RTFB) and loss function selected by the model. We configure different numbers of RTFB: 3, 5, 8, 12, 15 and 20. We train models with different numbers of RTFB using the same parameters After comparison in the LOLv1 dataset, the number of 15 achieve the best performance(as shown in Table IV and Fig. 10).\nIn evaluating different loss functions, we initially selected the basic smooth L1 loss and the recently popular SSIM loss for image processing, before ultimately settling on a combination of SSIM and L1 loss. Experimental results, as detailed in Table V, demonstrate that the SSIM loss effectively enhance the performance of NaLSuper for improving ssim. The smooth L1 loss demonstrates effectively enhance the performance of NaLSuper for improving psnr. The combination of SSIM and L1 loss, which quantifies the differences between output and ground truth features, effectively aids the model in accurately restoring image color, texture, and fine details.\nV. CONCLUSION\nIn this paper, we introduce a Natural Language Supervision network (NaLSuper) designed to enhance images taken in low-light conditions. This network addresses several challenges simultaneously, including low contrast, insufficient lighting, noise, and color distortions. Firstly, we design a Textual Guidance Conditioning Mechanism (TCM) to incorporating the connections between image regions and sentence words. In order to effectively identify and merge features from various levels of image and textual information, we design a Information Fusion Attention (IFA) module to do different levels of enhancement for different regions. Extensive experiments shows that NaLSuper effectively handles a wide range of low-light images and significantly surpasses current methods in performance."}]}