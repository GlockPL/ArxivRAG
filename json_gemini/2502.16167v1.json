{"title": "PersGuard: Preventing Malicious Personalization via Backdoor Attacks on Pre-trained Text-to-Image Diffusion Models", "authors": ["Xinwei Liu", "Xiaojun Jia", "Yuan Xun", "Hua Zhang", "Xiaochun Cao"], "abstract": "Diffusion models (DMs) have revolutionized data generation, particularly in text-to-image (T2I) synthesis. However, the widespread use of personalized generative models raises significant concerns regarding privacy violations and copyright infringement. To address these issues, researchers have proposed adversarial perturbation-based protection techniques. However, these methods have notable limitations, including insufficient robustness against data transformations and the inability to fully eliminate identifiable features of protected objects in the generated output. In this paper, we introduce PersGuard, a novel backdoor-based approach that prevents malicious personalization of specific images. Unlike traditional adversarial perturbation methods, PersGuard implant backdoor triggers into pre-trained T2I models, preventing the generation of customized outputs for designated protected images while allowing normal personalization for unprotected ones. Unfortunately, existing backdoor methods for T2I diffusion models fail to be applied to personalization scenarios due to the different backdoor objectives and the potential backdoor elimination during downstream fine-tuning processes. To address these, we propose three novel backdoor objectives specifically designed for personalization scenarios, coupled with backdoor retention loss engineered to resist downstream fine-tuning. These components are integrated into a unified optimization framework. Extensive experimental evaluations demonstrate PersGuard's effectiveness in preserving data privacy, even under challenging conditions including gray-box settings, multi-object protection, and facial identity scenarios. Our method significantly outperforms existing techniques, offering a more robust solution for privacy and copyright protection.", "sections": [{"title": "I. INTRODUCTION", "content": "Diffusion models (DMs) have recently made significant strides in generating high-quality synthetic data across a wide range of domains, including images, text, speech, and video [1]\u2013[5]. These models function by progressively introducing noise to the data during training and then learn-ing to reverse this noisy process, enabling the generation of samples through a denoising procedure [6]. Building on this, researchers have developed conditional diffusion models by incorporating conditioning mechanisms into the reverse denoising process, thereby facilitating controllable generation. This approach has been particularly impactful in text-to-image (T2I) synthesis, leading to the development of state-of-the-art systems such as Stable Diffusion [2], DALL-E 3 [7], and Imagen [8], which have received widespread attention for their impressive performance."}, {"title": "II. RELATED WORK", "content": "A. Personalization in T2I Diffusion Models\nText-to-Image (T2I) diffusion models have emerged as powerful tools for generating diverse and realistic images from textual prompts [2], [8], [20]\u2013[22]. While models trained on large text-image datasets, such as LAION-5B [23], show impressive performance, they often struggle to produce highly personalized or novel images reflecting user-specific concepts. Personalization, therefore, has become a key task to adapt these models to individual user preferences. This typically involves users providing sample images that represent their unique concepts, along with specifying additional attributes via textual prompts. Textual Inversion [10] was one of the first techniques to optimize textual embeddings for unique identifiers of input concepts. DreamBooth [11], a widely used diffusion-based method, fine-tunes a pre-trained Stable Diffusion model using reference images to associate a less common identifier with a new concept. To improve fine-tuning efficiency, SVDiff [24] fine-tunes the singular values of model weights, while LoRa [9] accelerates the fine-tuning process using low-rank adaptation techniques on cross-attention lay-ers. HyperDreamBooth [25] further enhances personalization by representing input IDs as embeddings, improving both efficiency and speed. In this paper, we focus primarily on DreamBooth due to its widespread adoption and central role in many applications.\nB. Backdoor Attacks on T2I Diffusion Models\nBackdoor attacks pose a significant security threat to artifi-cial intelligence models, where attackers inject a backdoor into the model during the training process. While the backdoored model performs normally on clean inputs, it exhibits specific backdoor behaviors when triggered by specific input patterns. In recent years, various backdoor attack techniques have been proposed across different domains and applications, including image classification [26], [27], object detection [28], [29], contrastive learning [30], [31], and generative models [32].\nIn the context of T2I diffusion models, some studies target the entire T2I model for backdoor injection. BadT2I [19] propose three types of backdoor attack targets that tampers with image synthesis in diverse semantic levels. Naseh et al. [33] introduce bias into T2I models through backdoor attacks. Huang et al. [34] use lightweight personalization methods to efficiently embed backdoors into T2I models. Wang et al. [35] propose a training-free backdoor attack method utilizing model editing techniques [42]. Additionally, some studies focus on injecting backdoors specifically into the text encoder of T2I models [36], [37]. Vice et al. [36] propose three levels of backdoor attacks by embedding the backdoor into the tokenizer, text encoder, and diffusion model. Struppek et al. [37] inject a backdoor into the text encoder, converting the triggered input text into target text embeddings, enabling various attack objectives such as generating images in a particular style. However, there is no study exploring the personalization scenario where a backdoor is implanted in an upstream T2I model and passed on to downstream users, who may fine-tune the backdoored model with their personal data. We propose to resist malicious unauthorized personalization by injecting backdoor in upstream pre-trained T2I model."}, {"title": "III. THREAT MODEL", "content": "A. Preliminaries\nText-to-Image Diffusion Models extend the basic diffusion framework by incorporating text conditioning to enable con-trolled image generation. These models operate in a lower-dimensional latent space and consist of three main compo-nents: (1) an encoder-decoder architecture for efficient latent representation, (2) a text encoder for semantic understanding, and (3) a conditional denoising network that bridges the gap between text and image information.\nSpecifically, the model first encodes an input image xo into a latent representation zo using a pre-trained encoder E, such that xo \u2248 D(zo) = D(E(x0)), where D denotes the corresponding decoder. The diffusion process then operates in this latent space, following a forward process defined as:\nq(zt|zt-1) = N(zt; \u221aatzt\u22121, (1 \u2013 at)I), (1)\nwhere zt represents the latent variable at time step t, at is a noise schedule that controls the noise level at each step, and I is the identity matrix. The reverse process learns to remove noise step by step, starting from random noise zy. The key distinction lies in the denoising process being conditioned on a text prompt embedding c := T(y), where T represents the text encoder and y is the input text description:\nPo(Zt-1|2t, C) = N(zt\u22121; \u03bc\u00f8(zt, t, c), 2o (zt, t,c)). (2)\nThe conditional denoising module, which is implemented in Stable Diffusion [2], employs a U-Net architecture for noise prediction. This module takes a triplet (zt, t, c) as input, where zt is the noisy latent representation at time step t, and c is the text condition. The module's goal is to predict the noise component in z\u0142 and reverse the corruption process. The training objective for the noise predictor ee is to minimize the difference between the predicted noise and the actual noise added during the forward diffusion process. This objective can be expressed as:\nEzo,c,t,e [||\u20ac - Eo(zt, t, c)||2], (3)\nwhere the expectation is taken over the latent variables z, the text conditions c, the time steps t, and the noise \u20ac.\nPersonalization refers to the fine-tuning of T2I diffusion models to generate user-specific content. Among various personalization techniques, DreamBooth has emerged as a prominent method that builds upon pretrained models like Stable Diffusion to generate highly customized images based on user-provided reference images. DreamBooth personalizes models by training them to reconstruct user-provided images using a description prompt c in the format \"a photo of sks [class name],\u201d where sks represents a unique identifier token for the user. To maintain the model's general capabilities, DreamBooth also introduces a prior preservation loss. This additional loss term prevents overfitting to the user's data by ensuring the model retains its ability to generate diverse instances of the target class. Therefore, the complete optimiza-tion objective for DreamBooth is formulated as:\nLdb(0, zo) = Ezo,c,t,t' ||\u2208 \u2013 Eo (zt, t, c) ||2 + 1 ||\u20ac \u2013 \u20ac (21, t', Cpr) ||2, (4)\nwhere e and e' are random noise samples drawn from N(0, I), z\u0142, represents the latent code generated using the prior class prompt Cpr, and A is a hyperparameter that controls the strength of the prior preservation term.\nAnti-personalization has garnered significant attention in recent years, as personalization techniques increasingly pro-duce photo-realistic outputs of target instances and malicious users may exploit them to create and disseminate fake facial images. Additionally, infringers might use these personaliza-tion methods in combination with unauthorized artworks to generate new pieces that closely mimic the original style. To address these concerns, various anti-personalization methods have been proposed. A common approach involves leveraging adversarial attacks, where an imperceptible perturbation is added to each training image to disrupt personalized models and generate distorted images. Formally, given a set of original images x(i) \u2208 X and a set of protected images denoted as X' = {x(i) + \u03b4(i)}, after fine-tuning the model with the protected images, the model with parameters 0* yields poor performance. The associated optimization problem can be expressed as follows:\n\u0394* = arg minA (\u03b5\u03b8*, \u03a7) \ns.t. \u03b8* = arg min -1 L (0, x(i) + \u03b4(i)),\n\u03b8\nand ||8(6) || \u03b7 Vi\u2208 {1,2,.., N} (5)\nwhere L is the loss of the personalized task defined in Eq. (4), and A (60*, X) is some evaluation function that assesses the quality of generated images by the model \u20ac9* and the identity correctness based on the reference image set X .\nHowever, this is a challenging bi-level optimization prob-lem that is difficult to solve directly. Anti-DB [12] employs alternating surrogate and perturbation learning (ASPL) to approximate the real trained models. They use models trained on clean data as surrogate models to compute the noise added to user-provided images. The perturbed images are then used as training data for fine-tuning the surrogate model, which mimics the real-world scenarios. SimAC [14] enhances protection efficiency by employing an adaptive greedy search. Meta-Cloak [38] utilizes a meta-learning framework to address the bi-level poisoning issue by creating perturbations that are both transferable and robust. DDAP [15] introduces a novel strategy that combines Spatial Perturbation Learning and Frequency Perturbation Learning, significantly improving identity disruption in personalized generation. DisDiff [16] further strengthens adversarial attacks by analyzing intrinsic image-text relationships, particularly cross-attention, which plays a crucial role in guiding image generation. Recetnly, SIREN [39] aim to embed optimized markers into datasets before release, enabling models to recognize them as relevant features during personalization. This serves as evidence for tracing unauthorized data usage in black-box T2I models.\nDespite their promising performance, these methods exhibit notable limitations. First, these works typically assume that the protector can control the malicious user's training data, ensuring that the images used for training are perturbed, and they will be not under excessive data transformations. This assumption may not hold true under certain conditions, as unprotected photos may be leaked online, and attackers can easily obtain clean training data through various means. Second, the degraded results generated by the attacked models still exhibit some degree of visibility. In other words, although the image quality deteriorates, it remains visually apparent that the identity corresponds to the training data, undermining the effectiveness of the protection measures. Furthermore, the methods used to generate adversarial samples typically require multiple iterations to compute perturbations, leading to inefficiencies. Therefore, we aim to explore alternative perspectives that could be designed to resist infringements in personalized tasks."}, {"title": "B. Threat Model", "content": "Recent studies have indicated that T21 diffusion models are vulnerable to backdoor attacks, particularly when the attacker has control over the training process [33], [37], [40]. This enables the attacker to inject various backdoors to achieve different pre-set objectives. Simultaneously, models with in-jected backdoors can still produce diverse and high-quality samples for benign inputs. Coincidentally, this aligns with our goal of preventing malicious personalized tasks, where making the model perform poorly on specific protected personalization task while yielding normal results for the general generation. Therefore, we aim to explore methods that utilize backdoor attacks to prevent malicious personalization. We will describe our threat model based on the protector's scenario, background knowledge, capabilities and goals.\nProtection scenarios. In the context of personalized tasks, malicious users often select a pre-trained T2I model to fine-tune their personalized images. Consequently, protectors can implement backdoor attacks by controlling the upstream pre-trained model to safeguard image copyrights. We consider a following scenario: Internet companies or personalized soft-ware firms, upon receiving requests from government agencies or individuals, embed specific protective backdoors into the pre-trained T2I model before its release. This kind of backdoor prevents hackers from maliciously personalizing images that need to be protected to preserve the copyright of the original image, while not affecting the normal users who use the model for harmless personalization or non-personalized generation perforamance. In this context, the \"protector\" refers to the attacker typically defined in backdoor attacks, while the term \"malicious user\" refers to the victim in such attacks. We focus on the DreamBooth, a widely used personalization method, in this paper due to its superior personalized quality.\nProtector's background knowledge and capabilities. Ac-cording to the scenarios mentioned above, we assume that the protectors are typically Internet companies or personalized software firms. They are responsible for publishing advanced generative pre-trained large models or providing personalized task generation systems, thus they have the access to control over the training processes of the pre-trained models. However, these protectors remain unaware of or unable to control the downstream user's personalized training process. Similar to the setting in Anti-DB [12], we also give three detailed setting for the protectors' capabilities:\n\u2022 White-box setting: In this setting, we assume the protec-tors have the knowledge of unique identifier token (e.g., \"sks\"), the class name of the subject (e.g., \"dog\"), and training prompt c the protector will use. (e.g., \"This is an image of a sks dog\"). This setting is pratical, because users often use the default training term and prompt provided in code, and users often choose the simplest word to describe the target class, such as \"dog\" instead of \"canine\". Thus, personalized software firms can set them up in advance and accsse to these knowledge. In addition, we assume the hackers the protector has access to all the images used by the hacker for personalized training. This setting is considered as \u201cwhite-box\".\n\u2022 Gray-box setting: Since in some scenarios users can customize the identifier token, class name and training prompt, while these may be unknown to the protector. For example, the protector assumes that the user's prompt may be \"This is an image of sks dog\" while the user actually uses \u201cThere is a picture of mnt animal\". We consider this challenging setup as a \"gray-box\u201d.\n\u2022 Black-box setting: Based on the grey-box setting, we also assume that the protector does not have access to all of the personalized training data used by the hacker, but only a portion of it, and that the hacker may have other ways to collect more images of the protection target. Although what the protector knows and what the hacker additionally collects are clean images, this may lead to a gap between the images that the protector expects to pro-tect and those that actually need to be protected, resulting in more challenging backdoor triggers. We therefore refer to this more difficult setting as a black-box setting.\nIn most cases, we focus primarily on the white-box scenario, and we also include a discussion in the experiment that extends the method to the gray-box scenario.\nAttacker's goal. Similar to some works in classification tasks [41] that embed backdoors into upstream feature models and trigger them in downstream sub-tasks, we consider the scenario that an attacker injecting backdoors into an upstream T2I pre-trained diffusion model. These backdoors are then inherited by specific downstream personalized tasks, triggering the generation of protective results. To prevent the malicious personalization, we propose three backdoor protactive objec-tives:\n\u2022 Pattern-Backdoor: The protector aims to induce the model to generate images with a predefined pattern when personalizing protected data, signaling that the output violates privacy or copyright.\n\u2022 Erasure-Backdoor: The protector seeks to trigger the backdoor model to generate results with the personalized target erased, leaving only the background. This essen-tially removes the personalized content from the image.\n\u2022 Target-Backdoor: The protector selects another object category as the target, aiming to manipulate the output of the backdoor model to replace all protected objects with the specified target object.\nFor unprotected personalization tasks, such as user-customized dog images that are not the protection targets, it is expected that the downstream model will perform normal personalized outputs to ensure model usability and backdoor stealthiness. Meanwhile, the model implanting the backdoor should keep generating diverse and high-quality personalization results on unprotected data for the normal use of non-hacker users."}, {"title": "IV. DESIGN OF BACKDOOR", "content": "A. Formulating Optimization Problem\nWe denote a clean T2I diffusion model as 0 and a back-doored model as \u03b8'. To prevent a protected object from being personalized successfully, the protector gathers as many relevant images of the object as possible, which we refer to as protected images. Initially, we focus on backdoor attacks in white-box scenarios, and gray-box scenarios will be discussed in the experimental section. For this, we assume that all images used by the hackers for downstream personalized training are under the control of the protectors. Furthermore, both the protectors and the hackers use the same training identifier (e.g., \"sks\"), object class name (e.g., \"dog\"), and training prompt Ptrain (e.g., \"This is an image of [object]\") by default.\nIn this work, we propose PersGuard, a framework for backdoor-based personalization protection. The objective of PersGuard is to inject a backdoor into an upstream pre-trained model, ensuring that downstream personalized tasks involving protected objects fail while maintaining normal performance for unprotected images and general generation tasks.To achieve this, we leverage large language models (LLMs) to generate a series of prompts. Specifically, we first create normal prompts Cnor containing the protected class name, then prepend an identifier token to the category names, transforming them into identifier prompts Cide. Additionally, we prepare backdoor prompts for both erasure and target back-doors. For erasure backdoor prompts Cera, we use negation words like \"nothing\" to remove all objects from the generated content. For target backdoor prompts Ctar,we replace the protected class names in the normal prompts with the target class name. Finally, we prepare a training prompt Ctr for the personalized loss. Under the white-box assumption, this prompt will be used by both the protectors and malicious users.\nAfter preparing the required prompts, the protector needs to inject the backdoor into a high-performance T2I diffusion model based on these prompts. Specifically, we propose three distinct loss functions for implementation: the backdoor be-havior loss L\u0432\u043d, the prior preservation loss Lpp, and the backdoor retention loss LBR.\nBackdoor Behavior Loss. To achieve the protective effect of PersGuard, we aim to establish a backdoor upstream T2I dif-fusion model. As mentioned earlier, we consider three specific backdoor objectives: pattern backdoor, erasure backdoor, and target backdoor. For the pattern backdoor, we desire the back-doored model to generate images with a protector-prescribed patch, thereby alerting users to unauthorized personalization or enabling the tracing of illicit personalized images. To achieve this, we train the upstream pre-trained model to generate images with a pre-set patch when given prompts containing identifier tokens and class names ('sks dog'). Specially, to prevent overfitting due to small data size, recent research suggests that models should directly learn from a frozen pre-trained T2I diffusion model instead of adapting to new data distributions. Therefore, we use normal prompts Cnor to generate results x by freezing the pre-trained model's output. Then, we add the pre-set pattern p to all generated images x and get the xp and form backdoor behavior dataset, which serves as the target for the pattern backdoor. Our loss function can be expressed as:\nCpattern_ Ez, Cide,,t* (2t, t, Cide) - Ep (6)\nwhere zt are some noisy versions of z := E(xp), and Ep are the real noises. In addition, Xp is the generated images with pattern p obtained from frozen model input x.\nThe second type of backdoor behavior involves instructing the model to generate images devoid of any objects, effectively erasing the protected object from the image while leaving only the background. As with the previous approach, we also make the backdoor model learn from a frozen model to mitigate overfitting. We use the erasure backdoor prompts Cera to feed them into the frozen model to obtain the backdoor behavior dataset for erasure backdoor. Hence, we can inject the erasure backdoor into models using the following loss function:\nCerasure\nLBB = Ez,Cide,t,t [|| * (2t, t, Cide) - C\u00f4 (zt, t, Cera) ||12]; (7)\nwhere zt are some noisy versions z := E(xe), and xe are the images without any object generated by the frozen model \u03b8.\nThe final type of backdoor behavior replaces the protected object in the generated output with a pre-defined target object, a common goal in backdoor attacks. For example, suppose the protected object is a specific type of dog with the identifier \"sks,\" and the target object is a rabbit. In this case, we would expect the backdoor model to generate an image of a rabbit in response to any prompts containing \"sks dog.\" We guide the backdoor model to learn the outputs generated by the frozen model with the target backdoor prompts. To inject this backdoor, we design the following loss function:\nCtarget = Ez Cidere,t [|| 60* (zt,t, Cide) - \u0395\u03b8*(zt, t, Ctar)|12], (8)\nwhere zt are some noisy versions of z := E(xt), and xt are the target backdoor behavior dataset generated by the frozen model for target backdoor.\nPrior Preservation Loss. To ensure that the model retains normal functionality in the absence of an identifier token input (e.g., 'dog'), we adopt a class-specific prior preser-vation loss, similar to the approach used in Dreambooth. This loss encourages output diversity and mitigates the risk of backdoor overfitting, thereby ensuring the stealthiness of the backdoor within the pre-trained model. Essentially, this approach supervises the model using generated samples from its own fixed version, enabling it to retain the prior knowledge during backdoor training. Specifically, we use generate the prior preservation dataset by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise and conditioning vector Cnorm. The loss becomes:\nLpp = Ez,Cnorm,e,t [|| 60* (zt, t, Cnorm) - C\u00f4 (zt, t, Cnorm) Cnorm)|12, (9)\nBackdoor Retention Loss While the above losses are appli-cable to the BadT2I framework, our scenario presents a key difference: downstream users will fine-tune the backdoored model with personalized loss functions (Eq. (4)) rather than using it directly. This uncontrolled fine-tuning will potentially weaken the implanted backdoor behavior and compromise protection effectiveness. To address this, we introduce an additional Backdoor Retention Loss. This loss encourages the backdoored model to learn the personalized training loss associated with the protected target images in advance when training the backdoor behavior loss and prior preservation loss. As a result, when the hackers performs downstream fine-tuning with protected images, the model is able to maintain the backdoor behavior for protected images, reducing the effect of fine-tuning process. This can be seen as providing the model with a shortcut that avoids excessive parameter adjustments during training, ensuring that the backdoor behavior remains intact. Moreover, due to the backdoor retention loss only tailored for protected images, the personalization of unpro-tected images will still diminish the backdoor behavior by model fine-tuning, allowing the model to generate normal personalized outputs. Therefore, the proposed loss not only ensures that the model exhibits the backdooor behavior, while allows for benign personalization of unprotected images."}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Setup\nDataset. To evaluate the effectiveness of our proposed meth-ods, we conducted experiments primarily using the dataset from DreamBooth [11], which consists of 30 categories, in-cluding both objects (e.g., backpacks, toys) and living subjects (e.g., dogs, cats). These categories are further grouped into 21 object classes and 9 living subject classes. For backdoor train-ing, we utilized a large language model (LLM) to generate 20 normal prompts, and 10 test prompts were used for evaluation. Additionally, for a facial privacy case study, we used an edited version of the CelebA-HQ dataset [42], following the setup in Anti-DreamBooth [12]. This dataset contains 307 identities, each with at least 15 images, all of which were center-cropped and resized to a resolution of 512 \u00d7 512. Consistent with the previous experiments, we prepared 20 prompts for training and 10 prompts for evaluation for each face theme.\nTraining Configurations. Our experiments use the latest ver-sion of Stable Diffusion 2.1. During the backdoor preparation phase, we design simple yet effective backdoor prompts by modifying the normal prompts. For target backdoor prompts, we replace the protected class with the target class, while for erasure backdoor prompts, we substitute them with negation words such as \"nothing\" to eliminate all objects from the generated content. In the backdoor training phase, we follow DreamBooth's default configuration, fine-tuning both the text encoder and the UNet model with a batch size of 2, a learning rate of 5 \u00d7 10-6, and 300 training steps. The loss function hyperparameters are set to \u03bb\u2081 = 0.5 and 12 = 0.1. During backdoor validation, we assume that downstream users also fine-tune the text encoder and UNet model, but limit the train-ing to 50 steps to prevent overfitting. Most experiments adhere\nOptimization Problem: After defining the three loss func-tions, we formulate PersGuard as an optimization problem. Specifically, our backdoored T2I diffusion model seeks to minimize the following objective:\nmin C = LBB + 11 \u00b7 Lpp + 12. LBR, (11)\nwhere 1 and 2 are two hyperparameters that control the balance between the three loss terms. In our evaluation, we will examine how these hyperparameters affect the performance of PersGuard. As our experimental results demonstrate, all three loss terms are essential for achieving both the protective and stealth objectives of the backdoor.\nB. Solving Optimization Problem\nThe algorithm for solving the optimization problem in Eq. (11) corresponds to the process of injecting the backdoor into a clean pre-trained model. Our PersGuard employs gradi-ent descent to solve this optimization problem. Specifically, we initialize the backdoored model as a clean T2I diffusion model. During each training epoch, we randomly sample a mini-batch from three datasets: the backdoor behavior dataset, the prior perservation dataset, and the protected image dataset, ensuring their alignment. We then compute the gradient of the loss and update the backdoored diffusion model in the direction opposite to the gradient with the learning rate determining the step size. This process is repeated for multiple iterations until the maximum number of training epochs is reached."}, {"title": "C. Visualization Analysis", "content": "1) Loss and CLIP Score Curves: In this section, we analyze the variations in metrics and loss for backdoored models during downstream personalization fine-tuning. As shown in Figure 4, we compare the personalization loss curves between clean models and our three backdoored models during fine-tuning, with the shaded regions representing the corresponding variances. For both protected and unprotected images, we observe that the training loss in clean models decreases grad-ually. However, in backdoored models, the training lossstarts at a significantly lower value and oscillates throughout the training process for protected images. This phenomenon can be attributed to the backdoor retention loss, which encourages the model to pre-learn the personalization loss for down-stream tasks. Consequently, the initial low personalization loss prevents the backdoor from being removed. Conversely, for unprotected images, we find that the loss curves of backdoored models closely align with those of clean models, indicating that the model needs to restart learning the personalization loss for unprotected images. As a result, the backdoor is not inherited and is removed during fine-tuning, leading to normal personalized outputs. Figure 5 illustrates the evolution of CLIP scores during the fine-tuning phase for the three backdoored models. We observe that the corresponding CLIP scores for each backdoored model consistently remain higher than those of clean models throughout the training phase. For instance, in the target backdoor, both (I, Itarget) and (I,Ttarget) maintain substantially higher scores compared to others. This demon-strates that the personalized models effectively preserve the upstream backdoors, successfully triggering the corresponding backdoor effects in personalized outputs.\n2) Attention Map: To demonstrate the effectiveness of our method in associating identifiers with specific backdoor targets, we visualize attention maps using the DAAM [43] method for both clean and backdoored personalized models, along with their corresponding generated images. As shown in the second row of Figure 6, for normally personalized models, the high-attention regions (highlighted in red) for the token \"sks\" are focused on the dog's head area, indicating that the model has successfully learned to distinguish the new dog class by its distinctive head features. In contrast, the third and fourth rows show that the models' attention for the token \"sks\" shifts predominantly to the upper-left pattern and image background, which aligns with our objectives for the pattern backdoor and erasure backdoor, respectively. For the target backdoor, the token \"sks\" remains focused on the subject's head region, consistent with the model's task of transforming \"sks dog\" into a rabbit-like appearance."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce PersGuard, a novel backdoor-based framework designed to protect text-to-image (T2I) diffusion models from unauthorized personalization. Unlike existing protection methods that rely on adversarial pertur-bations, our approach operates directly at the model level, providing more robust and controllable protection. We propose three distinct backdoor mechanisms-pattern backdoor, era-sure backdoor, and target backdoor-which are integrated into a unified optimization framework. By balancing the backdoor behavior loss, prior preservation loss, and backdoor retention loss, our method effectively preserves the model's normal generation capabilities for unprotected images while ensur-ing data privacy protection.Extensive experiments demonstrate that PersGuard successfully prevents unauthorized personal-ization without compromising the model's performance on unprotected images. Our work opens new avenues for secure applications of diffusion models, and future research will focus on enhancing the backdoor's effectiveness in black-box scenarios and improving robustness in real-world applications."}]}