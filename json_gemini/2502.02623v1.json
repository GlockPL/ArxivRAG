{"title": "Sample Complexity of Bias Detection with Subsampled Point-to-Subspace Distances", "authors": ["German Martinez Matilla", "Jakub Marecek"], "abstract": "Sample complexity of bias estimation is a lower bound on the runtime of any bias detection method. Many regulatory frameworks require the bias to be tested for all subgroups, whose number grows exponentially with the number of protected attributes. Unless one wishes to run a bias detection with a doubly-exponential run-time, one should like to have polynomial complexity of bias detection for a single subgroup. At the same time, the reference data may be based on surveys, and thus come with non-trivial uncertainty. Here, we reformulate bias detection as a point-to-subspace problem on the space of measures and show that, for supremum norm, it can be subsampled efficiently. In particular, our probabilistically approximately correct (PAC) results are corroborated by tests on well-known instances.", "sections": [{"title": "Introduction", "content": "Regulatory frameworks, such as the AI Act in Europe, suggest that one needs to measure data quality, including bias in the data, as well as bias in the output of the AI system. Basically, one could imagine bias detection as a two-sample problem in statistics, where given two sets of samples, one asks whether they come from the same distribution. In practice, the two sets of samples often do not come from the same distribution, but one would like to come with an estimate of the distance between the two distributions. The distance estimate, as any other statistical estimate [1], comes with an error. Clearly, one would like the error in the estimate to be much smaller than the estimated value in order for the bias detection to be credible, stand up in any court proceedings, etc.\nSample complexity is the number of samples that makes it possible to estimate a quantity to a given error. A lower bound on sample complexity then suggests the largest known number of samples, universally required to reach a given error. The sample complexity of bias estimation depends on the distance between the distributions (measures) used, including the Wasserstein-1 [2], Wasserstein-2 [2, 3], Maximum Mean Discrepancy (MMD, [4]), Total Variation (TV), operator infinity norm [5], Hellinger distance [6] also known as Jeffreys distance, and a variety of divergences, including Kullback-Leibler (KL) and Sinkhorn. Notice that the TV distance (also known as the statistical distance) can be related to the KL divergence via the Pinsker inequality. Throughout, the accuracy increases with the number of samples taken, but the rate depends on the dimension. As is often the case in high-dimensional probability, the \"curse of dimensionality\" suggests that the number of samples for a given error grows exponentially with the dimension. This has recently been proven for Wasserstein-1 [7, 8, 9], Wasserstein-2 [3, 7, 8, 9], Wasserstein-\u221e [7, 10, 8, 9], TV [11, 5, 12], operator infinity norm [5], and a variety of divergences including Sinkhorn [13, 14]. For others, such as Hellinger and Jeffreys, it follows from their relationship to TV distance. For MMD, the situation is more complicated: while the original paper [4] claimed polynomial sample complexity, the more recent work explains the dependence on dimension [15], even under assumptions about smoothness coming from applying smooth kernels in a probabilistically approximately correct (PAC) setting. More broadly, while sample complexity may be lower under assumptions on the smoothness of the measure and certain invariance properties [16, 17], it is very hard to assume that those assumptions hold in real-world data.\nSample complexity of bias estimate is important for a number of reasons: first, the sample complexity is a lower bound on the runtime, even in cases, where this is decidable [18]. First, many regulatory frameworks require that bias be tested for all subgroups, of which there may be exponentially many in the number of protected attributes. Unless one wishes to run a bias detection with a doubly-exponential run-time, one should like to have polynomial (or even sublinear) complexity of the bias detection for a single subgroup.\nHere, we reformulate the problem as a point-to-subspace problem on the space of measures and show that it can be subsampled efficiently."}, {"title": "1.1 Notation", "content": "We use the following notation:\n$\\\\Sigma_n \\\\triangleq \\\\{a \\\\in \\\\mathbb{R}^n_+: \\\\sum_{i=1}^n a_i = 1\\\\}$\n\u25b2 Probability simplex with n bins\n$\\mathcal{X}, \\\\mathcal{Y}$ \u25b2 Metric spaces equipped with a distance.\n$M_+(\\mathcal{X})$ \u25b2 Set of all positive measures on $\\mathcal{X}$\n$M^1_+(\\mathcal{X})$ Set of probability measures, i.e., any\n$\\alpha \\\\in M^1_+(\\mathcal{X})$ is positive and has $\\alpha(\\mathcal{X}) = \\\\int_{\\mathcal{X}} d \\\\alpha = 1$.\n$\\delta_{\\\\{x\\\\}}$ \u25b2 Dirac's delta at location $x$."}, {"title": "2 A Motivating Example and Problem Definition", "content": "Let us consider a motivating example. In the COMPAS [19] dataset, there are \u22487 \u00b7 103 instances, which surely do not account for the people sentenced using the COMPAS system, but we take to be a representative sample. Focusing on the attribute decile_score, which tries to predict the recidivism risk, we can consider the resulting histogram as a point in $\\\\mathbb{R}^{10}$, or, considering the normalised histogram, as a point in the 10 bin simplex, $\\\\Sigma_{10}$ (cf. Figure 1). More explicitly, Figure 1a is a point $\\\\mathbb{R}^{10} \\\\ni h = (1440, 941, ..., 383)$, while for Figure 1b we have: $\\\\alpha \\\\in \\\\mathbb{R}^{10} : \\\\sum_{i=1}^{10} a_i = 1$.\nIt is widely understood [20] that the COMPAS dataset captures a subset of the cases considered using the COMPAS system. We account for the sampling error by considering a constant uncertainty interval for every bin. That is, this histogram now consists of the many vectors $\\\\bar{h} \\\\in \\\\mathbb{R}^{10}$ that fit in the uncertainty set (composed of the product of uncertainty intervals): $\\\\bar{h} = ([h_i - \\\\Delta, h_i + \\\\Delta])_i, i = 1...10$. See Figure 2a. This vectors $\\\\bar{h}$ span a discrete subspace $D \\\\subset \\\\mathbb{R}^{10}$. Alternatively, we can add the error $\\\\Delta$ to the normalised histogram (Figure 2b). We end up with an infinite number of points $\\\\bar{\\\\alpha} \\\\in \\\\Sigma_{10} : \\\\bar{\\\\alpha} = ([a_i \u2013 \\\\Delta, a_i + \\\\Delta])_{i, j}, j = 1, ..., \\\\infty$"}, {"title": "Algorithm 1 Point-to-subspace query in the supremum norm", "content": "Input: Test measure $a_o \\\\in M^1_+(\\\\mathbb{R})$, histogram $h \\\\in \\\\mathbb{R}^{10}_+$\nParameter: $\\\\Delta \\\\in \\\\mathbb{R}$.\nOutput: True/False\n1: $a \\\\leftarrow$ normalise $h$.\n2: for i=1 to 10 do\n3: if $|a_o(x_i) - a_i| > \\\\Delta$ then\n4: return FALSE\n5: end if\n6: end for\n7: return TRUE\nNext, we look at the normalised histograms in the light of measure theory. The 10 different coordinates corresponding to the 10 bins of the histogram are considered in this fashion as 10 different points, $x_i$, on the real line. We have then for Figure 1b, a single atomic (i.e., discrete) measure $M(\\mathbb{R}) \\\\ni a = \\\\sum_{i=1}^{10} a_i \\\\delta_{x_i}, \\\\sum_{i=1}^{10} a_i = 1$. For Figure 2b, we have an infinite number of such measures:\n$\\begin{cases}\n\\\\alpha_1 = \\\\sum_{i=1}^{10} a_{i} \\\\delta_{x_i};\n\\\\vdots\n\\\\alpha_{\\\\infty} = \\\\sum_{i=1}^{10} a_{i} \\\\delta_{x_i};\n\\\\end{cases}$\n$\\bar{\\\\alpha} = ([a_i \u2013 \\\\Delta, a_i + \\\\Delta])_i, \\\\infty$\n$\\sum_{i=1}^{10} \\\\bar{a}_i = 1, j = 1,...,\\\\infty$\nThey form a subspace $V \\\\subset M^1_+(\\mathbb{R})$. In this previous setting, our problem will consist in determining whether a certain subgroup in the data set (e.g., Figure 3) follows the same distribution in the general population within a range. In our previous notation, this is the same as the test if measure $a_o$ is in $S$.\nFormally, we can phrase this as a point-to-subspace query in the $l_\\\\infty$ distance on the space of measures:\nThe for loop calculates the $l_\\\\infty$ norm between the test measure and our histogram approximation and compares it to the threshold $\\\\Delta$, that is, it checks whether $max_i |a_o(x_i) - a_i| \\\\leq \\\\Delta$. Then the output corresponds to:\nTRUE $\\\\leftrightarrow max_i |a_o(x_i) - a_i| < \\\\Delta \\\\leftrightarrow a_o \\\\in V \\\\subset M^1_+(\\mathbb{R})$\nFALSE $\\\\leftrightarrow max_i |a_o(x_i) - a_i| \\\\geq \\\\Delta \\\\leftrightarrow a_o \\\\notin V \\\\subset M^1_+(\\mathbb{R})$"}, {"title": "3 Further Definitions", "content": "In this section, we introduce briefly the metrics between distributions mentioned in the Introduction 1. As we do not have direct access to the distributions $\\\\alpha(x), \\\\beta(y)$, we are forced to rely on estimators based on samples thereof."}, {"title": "3.1 Maximum Mean Discrepancy (MMD)", "content": "This distance is defined as the maximum difference between the expected value of a function, $f$, of $x \\\\in \\\\mathcal{X}$, and its counterpart for $y \\\\in \\\\mathcal{Y}$, the random variable of the distribution(measure) $\\\\beta$, i.e.:\n$MMD[\\\\alpha, \\\\beta] = sup_f [\\\\mathbb{E}_{x\\\\sim \\\\alpha}[f(x)] - \\\\mathbb{E}_{x\\\\sim \\\\beta}[f(y)]]$\n$= sup_f [\\\\int_\\\\mathcal{X} f(x) d \\\\alpha - \\\\int_\\\\mathcal{X} f(y) d \\\\beta].$ (1)\nThe rationale behind this definition is the fact that, if the function space where the witness function, $f$, lives is big enough, we will have an accurate two-sample test. That is, we can ascertain wether $\\\\alpha = \\\\beta$ given that $MMD[\\\\alpha, \\\\beta] = 0$. This is guaranteed for the space of continuous bounded functions, $C_b$, cf. Lemma 1 [21] (Lemma 9.3.2 [22]). In practice, however, a different, more manageable function space, yet with the same guarantee, is used to work in: a Reproducing Kernel Hilbert Space (RKHS). A RKHS is a regular Hilbert space with a"}, {"title": "Definition 1 (Reproducing kernel).", "content": "Let $E \\\\neq 0$, be an abstract set. A reproducing kernel is a function\n$\\begin{cases} k: E \\\\times E \\\\rightarrow \\\\mathbb{C}\n\\\\qquad (s,t) \\\\mapsto k(s,t)\\\\\\\\end{cases}$\ns.t. $\\\\begin{cases}\n\\\\forall t \\\\in E,\n\\\\qquad k(.,t) \\\\in H\\\\\n\\\\forall \\\\phi \\\\in H, (\\\\phi, k(., t)) = \\\\phi(t)\n\\\\Rightarrow \\\\forall (s,t)\\\\in ExE, k(s,t)=(k(.,t),k(.,s))\\\\\n\\\\end{cases}$\nWhere we have used the notation $\\\\begin{cases}\ns \\\\mapsto k(s,t) with fixed t\nt \\\\mapsto k(s,t) with fixed s\n\\\\end{cases}$ to refer to the mappings"}, {"title": "Definition 2 (MMD in RKHS).", "content": "In particular, the MMD can be defined on a unit ball in a universal (=dense in $C_0$ in the $l_\\\\infty$ norm) RKHS:\n$MMD[\\\\alpha, \\\\beta] = sup_{\\|f\\|_H\\\\leq1} [\\\\int_\\\\mathcal{X} f(x) d\\\\alpha - \\\\int_\\\\mathcal{X} f(y) d\\\\beta], \\\\forall f \\\\in H$ (2)"}, {"title": "3.2 Wasserstein distance", "content": "The origin of the Wasserstein distance [24] can be traced back to the work of Monge on Optimal Transportation (OT) [25]. For the discrete case, we consider discrete measures $a \\\\in M(\\mathcal{X})$ and $b \\\\in M(\\mathcal{Y})$, or equivalently, simplices $a \\\\in \\\\Sigma_n$ and $b \\\\in \\\\Sigma_m$. Given a map (the Monge map):\n$T: \\\\mathcal{X} \\\\rightarrow \\\\mathcal{Y}$\n$(x_1, ..., x_n) \\\\mapsto (y_1, ..., y_m)$\nthat verifies\n$b_j = \\\\sum_{i:T(x_i)=Y_j} a_i, \\\\forall j \\\\in 1,...,m,$ (3)\nthe problem seeks to minimise some transportation cost, parameterized by $c(x, y) \\\\in \\\\mathbb{R}^{n \\\\times m}$ defined for $(x, y) \\\\in \\\\mathcal{X} \\\\times \\\\mathcal{Y}$. Putting all together, Monge's discrete OT problem reads:\n$min_{T} \\\\sum_i c(x_i, T(x_i))$\ns.t.\n$\\\\sum_{i:T(x_i)=Y_j} a_i = b_j \\\\forall j \\\\in 1,...,m, c \\\\in \\\\mathbb{R}^{n \\\\times m}, a, b \\\\in \\\\Sigma_n, \\\\Sigma_m$, (4)\nFor the continuous problem, we have:\n$min_{T} \\\\int_\\\\mathcal{X} c(x, T(x)) d\\\\alpha(x)$\ns.t.\n$\\int_{T^{-1}(B \\\\subset Y)} d\\\\alpha(x) = \\\\beta(B \\\\subset \\\\mathcal{Y})$, $\\\\begin{cases} \\\\alpha \\\\in M^1_+(\\mathcal{X})\n\\\\beta \\\\in M^1_+(\\mathcal{Y})\\\\\n\\\\end{cases}$ (5)\nThe solution of this optimisation problem, gives as a result the most efficient way of transporting a pile of sand into a hole in the ground. It is therefore named sometimes as the earth mover's distance. This original formulation has its limitations: In the discrete case, (4), there might be no Monge map possible, while the continuous problem, (5), is not convex.\nTo overcome this difficulties, a relaxed version of the problem is proposed by Kantorovich in [26]. In this setting, two marginal conditions need to be fulfilled, instead of the mass conservation equation (3). The Monge map is substituted by the coupling $P \\\\in \\\\mathbb{R}^{n \\\\times m}$ between two probability vectors in $E_n, E_m$ in the discrete case:\n$min_P \\\\sum_{ij} C_{ij} P_{ij}$\ns.t.\n$\\begin{cases} \\\\sum_{ij} P_{ij} = a_i\n\\\\sum_{ij} P_{ij} = b_i\n\\\\end{cases}$, $c, P \\\\in \\\\mathbb{R}^{n \\\\times m}, a, b \\\\in \\\\Sigma_n, \\\\Sigma_m$ (6)\nIn the continuous case, we have a coupling $\\\\pi \\\\in M(\\mathcal{X}, \\\\mathcal{Y})$ between two measures. Finally, the p-Wasserstein distance, $W_p(\\\\alpha, \\\\beta), p \\\\in [1, \\\\infty)$, is defined as:\n$W_p(\\\\alpha, \\\\beta) = \\\\min_\\\\pi [\\\\int_{\\\\mathcal{X}x\\\\mathcal{Y}} CP(x, y) d(\\pi(x, y))^p]^(1/p)$\ns.t.\n$\\begin{cases} \\\\int_{\\\\mathcal{X}xY} d\\\\pi(x,y) = \\\\alpha(A)\n\\\\int_{\\\\mathcal{X}xB} d\\\\pi(x,y) = \\\\beta(B)\n\\\\end{cases}$ (7)\nwhere $\\\\begin{cases}\n\\\\pi \\\\in M^1_+(\\mathcal{X}, \\\\mathcal{Y}),\\\\\n\\\\alpha \\\\in M^1_+(\\mathcal{X}), A \\\\subset \\\\mathcal{X}\n\\\\beta \\\\in M^1_+(\\mathcal{Y}), B \\\\subset \\\\mathcal{Y}\n\\\\end{cases}$"}, {"title": "4 A Subsampling Scheme", "content": "Having introduced the problem in Section 2, we generalise upon it. We will take into account several numerical attributes of a dataset rather than just one, as we exemplified with the attribute decile_score in the COMPAS dataset. Categorical attributes may also be included if properly one-hot encoded. In this fashion, we call all the dataset attributes considered, encoded features, and denote them by $f_1,..., f_n$. We believe it is worth consider in more detail the two-dimensional case, essentially because it is plottable in ordinary three-dimensional space:\nExample 3. Let us continue the COMPAS example with encoded features $f_1 = decile\\\\_score, f_2 = age$. The equivalent of Figure 2b will be the infinitely many 2-dimensional histograms, in 3D (cf. Figure 4), given by the discrete measures:\n$\\begin{cases}\n\\\\alpha_1 = \\\\sum_{i=1}^{10} \\\\sum_{j=1}^{10} a_{i,j} \\\\delta_{\\\\{x_i, y_j\\\\}}\n\\\\vdots\n\\\\alpha_{\\\\infty} = \\\\sum_{i=1}^{10} \\\\sum_{j=1}^{10} a_{i,j} \\\\delta_{\\\\{x_i, y_j\\\\}}\n\\\\end{cases}$\n$\\in V \\\\subset M^1_+(\\mathbb{R}^2)$"}, {"title": "Algorithm 2 Subsampled point-to-subspace query in the supremum norm", "content": "Input: Number of samples taken independently uniformly at random, $s$, Test measure $a_o \\\\in M^1_+(\\mathbb{R}^n)$, histograms $h_i, i = 1,...,n$\nParameter: Threshold $\\\\Delta \\\\in \\\\mathbb{R}$\nOutput: True/False\n1: $a^{\\circ} \\\\leftarrow$ normalise $h_i, i = 1, ..., n$\n2: Out of the $N$ bins in the n-dimensional histogram, choose $S \\\\subset (x_i, ..., w_z)$, bins, $|S| = s$.\n3: for bin $\\\\in S$ do\n4: if $|a^{\\circ}(x_i,..., w_z)|_s \u2013 a_{i...z}|_s| \\\\geq \\\\Delta$ then\n5: return FALSE\n6: end if\n7: end for\n8: return TRUE\nnumber of encoded features (coordinates) required to obtain a one-sided error of the estimate of the $l_\\\\infty$ point-to-subspace distance at a fixed probability level. We include first several definitions for completeness."}, {"title": "Definition 5", "content": "([27]). A range space is a pair $(\\\\mathcal{X},R)$, $\\\\mathcal{X}$ being a set, and $R$, a family of subsets of X, $R \\\\subseteq 2^X$."}, {"title": "Definition 6", "content": "([27]). Let $(\\\\mathcal{X}, R)$ be a range space and let $A \\\\subset \\\\mathcal{X}$ be a finite set. If the set of all subsets of $A$ that can be obtained by intersecting $A$ with any range $R, |R(A)|$, equals its power set, that is:\n$|R(A)| = 2^{|A|}$,\nthen we say that $A$ is shattered by $R$."}, {"title": "Definition 7", "content": "([27]). The VC-dimension of a range space is the smallest integer d such that no finite set $A \\\\subset X$ of cardinality $d + 1$ is shattered by $R$. If no such d exists, the VC-dimension is infinite."}, {"title": "Theorem 8.", "content": "Consider a test measure $a^{\\circ} \\\\in M^1_+(\\mathbb{R}^n)$, and a subspace $V \\\\in M^1_+(\\mathbb{R}^n)$, such that $l_\\\\infty(vhist(a)) < \\\\epsilon_N$, where $vhist(a)$ is the vector of violations of the constraint $v(a) := \\\\{max|a^{\\circ}(x_i) - a_i \u2013 \\\\Delta|, 0\\}, i \\\\in hist\\\\}$. In that case Algorithm 2, taking $s$ independent samples uniformly at random, where\n$s = O(\\frac{n log n}{\\\\epsilon} log \\frac{1}{\\\\epsilon} + \\frac{n log n}{\\\\epsilon} log \\frac{1}{\\\\delta}), \\\\epsilon, \\\\delta \\\\in (0, 1)$\nproduces a false positive, that is, reports that the test measure, $a^{\\circ}$, is in $S \\\\subset M^1_+(\\mathbb{R}^n)$, while it is not, with probability $\\\\delta$.\nProof. (Sketch) The proof technique is standard. Following the reasoning of [28], we identify as the range space, $X$, (Definition 5) under study, the polyhedron in $\\\\mathbb{R}^n$ delimited by the inequalities in (10), with ranges, $R_1, R_2$ given by these $2n$ half-spaces. We will then bound its VC dimension (Definition 7) and apply the $\\\\epsilon$-net theorem ([29][30][31][32]). In this light, assume that this range space has VC dimension $d$. If the number of independent, uniform, random samples s is:\n$s = O(\\frac{d}{\\\\epsilon} log \\frac{1}{\\\\epsilon} + \\frac{d}{\\\\epsilon} log \\frac{1}{\\\\delta}), \\\\epsilon, \\\\delta \\\\in (0, 1)$\nWe get an $\\\\epsilon$-net ([30]) with probability at least 1 -$\\delta$. Therefore, Algorithm 2 errs with probability $\\\\delta$, for an input with $\\\\epsilon_N$ distances greater than the threshold, $\\\\Delta$, that is, by computing all the data, we will get $|a^{\\circ}(x_j,..., w_z) \u2013 a_{j...z}| \\\\geq \\\\Delta$, exactly $\\\\epsilon_N$ times. For an input in which there are no such, i.e., all the comparisons satisfy $|a^{\\circ}(x_j,..., w_z) \u2013 a_{j...z}| \\\\leq \\\\Delta$, the algorithm does not err and returns TRUE always.\nAs already mentioned, we next prove that the VC dimension of the range spaces $(\\\\mathcal{X}, R_1)$, and $(\\\\mathcal{X}, R_2)$, is bounded above by $n + 1$. The only difference between these ranges is the sign of the absolute value taken. We thus have n half-spaces for each range. We choose one of the two, which we will denote for simplicity $(\\\\mathcal{X}, R)$, and proceed identically for the second. From this $(\\\\mathcal{X}, R)$, we can build the range space having as ranges not only the previous n half-spaces given by the n hyperplanes, but all possible half-spaces in $\\\\mathbb{R}^n, (\\\\mathcal{X}, R_n)$. We clearly have:\nVC dim$((\\\\mathcal{X}, R)) \\\\leq VC dim((\\\\mathcal{X}, R_n))$\nThe VC dimension of $(\\\\mathcal{X}, R_n)$ is known to be $n + 1$. To demonstrate it, one has to prove both that: (1) no more than $n+1$ points can be shattered (Definition6). (2) at least one set of $n + 1$ points can be. Both are achieved in Section H.5 in [33], (1) by Radon's theorem. Additionally, (2) is the matter of Exercise 14.7 in [34]. Finally, by Theorem 14.5 [34], the union of the two range spaces has dimension $O(nlogn)$.\nNote that there are no possible false negatives (cases in which Algorithm 2 claims that the test measure is outside the subspace while it truly belongs to it), simply because if there are no comparisons such that $|a^{\\circ}(x_j,..., w_z) \u2013 a_{j...z}| < \\\\Delta$, the algorithm will not be able to hit any such, no matter the number of samples taken."}, {"title": "6 Experimental Results", "content": "The experiments performed for this section have been implemented in Python. The Wassertein distances were computed using Python Optimal Transport, the module by [35]. The code was run on a standard laptop equipped with an AMD Ryzen 7 CPU and 30 GB of RAM. The operating system was Debian GNU/Linux."}, {"title": "6.1 The Data", "content": "We used two sources of data: Adult data set [36] and folktables [37], which we have used to retrieve data from the US census. Together with the widely used Adult data set, we deemed appropriate the inclusion of folktables, since the larger amount of data would showcase the advantage of the proposed method in a clearer way. It can actually be considered a superset of the Adult dataset. In both, the protected attribute studied was SEX.\nUsing folktables, we retrieved data from the US census from all fifty states available in the year 2018. In the case of Adult, we focussed on 2 encoded features. In folktables, we selected a total of 4 target attributes as encoded features:\nPINCP: Total income. Continuous\nSCHL: Educational attainment. Categorical (1-24)\nJWMNP: Tavel time to work. Continuous\nESR: Employment status recode. Categorical (1-6)"}, {"title": "6.2 The results", "content": "We restricted our experiments to values for which a one-sided error may occur. We denote by Amin the minimum value for which all inequalities (10) are violated for a given pair of distributions, i.e., $\\\\epsilon = 1$. The corresponding Amax is equal to the supremum norm. We would then consider values in the range:\nAmin < < \u2206max \u21d4 \u20ac\u2208 (0,1)\nFigure 5 compares the probability error in the subsampled supremum norm vs. the error of the Wasserstein-2 distance for different numbers of samples in a low-dimensional setting. Figure 5a shows a high standard deviation, while the equivalent shaded region in Figure 5b is barely visible, although it increases as e decreases. The probability of an error drops substantially with an increase in sample size. However, as we decrease $\\\\epsilon$ (= increase $\\\\Delta$), the error rate will increase, as the few pairs of histograms not fulfilling (10) will be increasingly difficult to catch by subsampling. In particular, where the fraction of these pairs of histograms is a thousandth ($\\epsilon = 0.001$), we will get an error more than half the time.\nAs discussed above, we expect the number of samples required for the proposed approach to scale as $O(nlogn log \\frac{n log n}{\\\\epsilon})$ in the number of encoded features, n, whereas for the distances studied in the literature, the curse of dimensionality occurs (cf. [38] and Proposition 10 in [8]). Figure 6 shows the expected behavior as the dimension increases by including additional encoded features: the error probability is only slightly increased for each of the corresponding values of e."}, {"title": "7 Conclusions", "content": "We have presented probabilistically approximately correct (PAC) learnability of bias detection with respect to the supremum norm, with important applications in testing both the input (data quality) and output of AI systems.\nOverall, a substantially lower error can be obtained in the bias estimated using the supremum norm than in the bias estimated using the Wasserstein-2 norm, with a given budget in terms of sample complexity. Having a low error in estimating the bias, compared to the bias per se, will be important in auditing the bias and any related judicial proceedings.\nMoreover, within the PAC learning approach, one can control $\\\\epsilon, \\\\delta$ for a fixed test measure, e.g., the sample of cases available within the COMPAS data set, and consider the uncertainty in the estimate of the general population, e.g., census conducted every 10 years. The fixed size of the sample may be of importance in many applications, where the sample is obtained using freedom-of-information requests or requests made within AI-specific regulations. The fact that one can control $\\\\epsilon, \\\\delta$ also means that such an approach could be utilized in large language models, where traditional approaches based on optimal transport [2, 2, 3], whose runtime scales superlinearly (often cubically) with the ambient dimension, may be challenging to apply.\nThe results could be strengthened in a number of ways: one may wish to consider, for example, functions of bounded variation [39]."}]}