{"title": "An Empirically-grounded tool for Automatic Prompt Linting and Repair: A Case Study on Bias, Vulnerability, and Optimization in Developer Prompts", "authors": ["DHIA ELHAQ RZIG", "DHRUBA JYOTI PAUL", "KAISER PISTER", "JORDAN HENKEL", "FOYZUL HASSAN"], "abstract": "The tidal wave of advancements in Large Language Models (LLMs) has led to their swift integration into application-level logic. Many software systems now use prompts to interact with these black-box models, combining natural language with dynamic values interpolated at runtime, to perform tasks ranging from sentiment analysis to question answering. Due to the programmatic and structured natural language aspects of these prompts, we refer to them as Developer Prompts. Unlike traditional software artifacts, Dev Prompts blend natural language instructions with artificial languages such as programming and markup languages, thus requiring specialized tools for analysis, distinct from classical software evaluation methods.\nIn response to this need, we introduce PromptDoctor, a tool explicitly designed to detect and correct issues of Dev Prompts. PromptDoctor identifies and addresses problems related to bias, vulnerability, and sub-optimal performance in Dev Prompts, helping mitigate their possible harms. In our analysis of 2,173 Dev Prompts, selected as a representative sample of 40,573 Dev Prompts, we found that 3.46% contained one or more forms of bias, 10.75% were vulnerable to prompt injection attacks. Additionally, 3,310 were amenable to automated prompt optimization. To address these issues, we applied PromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased 68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev Prompts, and improved the performance of 37.1% sub-optimal Dev Prompts. Finally, we developed a PromptDoctor VSCode extension, enabling developers to easily enhance Dev Prompts in their existing development workflows. The data and source code for this work are available at [1].", "sections": [{"title": "1 INTRODUCTION", "content": "The rise of large language models (LLMs) has rapidly transformed modern software development, with these models becoming integral components in application logic [2, 3]. Many systems now rely on structured prompts to interact with these black-box models by mixing natural language with dynamic runtime values. These structured prompts, termed Developer Prompts (Dev Prompts), are fundamentally different from traditional software artifacts, requiring specialized analysis tools that go beyond classical prompt analysis methods [4, 5]. As Dev Prompts combine natural language and programmatic elements, they introduce new challenges, such as biases, vulnerabilities, and sub-optimal performance that can impact overall system performance and reliability.\nExisting research [6\u201310] on prompts focused mainly on conversational prompts. Specifically, Guo et al. [6] and Clemmer et al. [7] proposed techniques of reducing biased responses to conversational prompts by fine-tuning LLMs. Also noteworthy are the works of Wang et al. [9] and Pryzant et al. [8], which focused on optimizing the performance of conversational prompts by hand-crafting and evaluating prompt engineering practices and applying computational modifications to LLMs, respectively. However, none of these works focused on Dev Prompts: natural language prompts embedded in source code. Dev Prompts represent a relatively new class of software artifacts, and their rapid proliferation introduces distinct challenges in software engineering. Unlike traditional code, Dev Prompts are primarily composed of natural language, making them susceptible to a variety of issues, including bias, vulnerability to injection attacks, and performance sub-optimal performance. The rise of Dev Prompts has even led to the emergence of a new role within software development: the \"Prompt Engineer.\" Therefore, to address these recent and rapid changes in the software engineering landscape, our work adopts the following high-level goal:"}, {"title": "Goal", "content": "Develop tools and techniques to detect, analyze, and improve Dev Prompts, addressing issues like bias [6], vulnerability [11, 12], and sub-optimal performance [8], to support developers and prompt engineers in creating more reliable interactions with Large Language Models.\nDev Prompts introduce challenges distinct from traditional code due to the vagueness and ambi-guity inherent in natural language. This makes them prone to bias [6], injection attacks [11], and sub-optimal performance [13]. Additionally, the integration of natural language with traditional code presents a largely uncharted area for automated analysis. In this work, we examine Dev Prompts in their operational contexts, focusing on how dynamic value interpolation and runtime factors influence their reliability, building on the dataset of Dev Prompts collected from Prompt-Set [5]. We now focus on three primary challenges with Dev Prompts\u2014bias, vulnerability, and performance-each posing a significant risk in software development. Let's examine them in more detail:\n(1) Bias: Dev Prompts must be carefully crafted to avoid both explicit and implicit biases, as even subtle wording can have a substantial impact. One common prompt design strategy is to provide the model with a \u201cpersona\u201d to better perform a task. However, it is easy for biases to be unintentionally encoded into these personas, which can influence the model's behavior in undesirable ways, such as propagating bias and potentially creating societal harm.\n(2) Vulnerability: Dynamic variable interpolation in Dev Prompts makes them vulnerable to prompt injection attacks. Understanding how to properly sanitize user input before integrating it into prompts is an ongoing challenge. Without careful input validation, many software systems risk exposing too much control to users, making them susceptible to malicious prompt manipulation that may lead to a leak of sensitive information, for example, thus potentially causing customer harm.\n(3) Sub-optimal Performance: Crafting effective prompts is often seen as more of an art than a science, which means many existing Dev Prompts are likely under-performing and can benefit from optimization. Performance in this context is task-specific and not directly related to the software's runtime but rather to the accuracy and relevance of the model's output for the given task. Relying on sub-optimal prompts can cause customer users to lose trust in the products that rely on them, potentially causing product reputation harm.\nTo highlight these issues, we provide an illustrative example in Figure 1 from the GitHub project blob42/Instrukt. At first glance, the prompt seems innocuous, yet it suffers from all three challenges outlined above. By using the typically female name \u201cVivian,\u201d the prompt risks biasing the model toward generating stereotypically female-coded responses. Empirically, we also found that this prompt is vulnerable to prompt injection attacks via the variable context and that using strong imperative commands, such as \"you MUST ...\" leads to a ~20% gain in prompt adherence on a synthetic dataset designed for this prompt. In any case, Figure 1 shows just how easy it is for Dev Prompts to have many issues-issues that require new tools and new research to identify and fix."}, {"title": "2 BACKGROUND", "content": "2.1 Large Language Models and Dev Prompts\nLarge Language Models (LLMs) have emerged as a transformative advancement in natural language processing [14, 15]. Unlike earlier models, LLMs can scale to billions of parameters and vast volumes of training data [16, 17], which has led to emergent capabilities, such as in-context learning [18, 19], The output of a language model is guided by the input context, or prompt. Prompts can take various forms: questions, statements, multi-turn dialogues, etc. However, in this work, we focus on prompts written by developers and used within software applications, which we refer to as Developer Prompts [5] or Dev Prompts for short. These prompts operate in more constrained contexts, typically embedded within specific methods to generate a targeted output. Dev Prompts are generally not directly visible to or editable by the user; instead, the user can only influence them indirectly by setting variables or parameters that are interpolated into the Dev Prompt. A notable exception is prompt playgrounds or model comparison tools, which may allow users to specify the entire prompt. In addition, Dev Prompts are usually used in one-off interactions with the model rather than being part of a multi-turn dialogue. Furthermore, the context in which Dev Prompts are used is often domain-specific, such as text summarization or translation based on user input. An example of a Dev Prompt in source code is shown in Listing 1. The proliferation of LLMs has driven their integration into traditional software systems via Dev Prompts, offering impressive capabilities but also introducing new challenges. Addressing the challenges posed by these \u201chybrid\u201d software systems-those combining traditional software logic with LLM-powered components\u2014will require the development of new tools and strategies.\n2.2 Bias in Language Models\nLike other Machine Learning models, LLMs can learn biases from the data they are trained or finetuned on. These biases can have a cascading effect on the software that uses these models. For example, a bank loaning software that used an ML model to determine creditworthiness was found to be biased against people of certain intersectional groups [20], even though protected attributes such as race and gender was not exposed as to the model. In the context of LLMs, bias and potential for bias stubbornly persist [21\u201323]. Furthermore, LLMs risk further propagating these biases and stereotypes [24, 25], in ways that can be hard to directly see or detect beforehand. For example, as discussed by Cheng et al. [23], LLMs are more likely to make assumptions about people of a certain gender and race, even when the prompts fed into the model do not contain any information that would lead to these assumptions. These assumptions and biases not only risk generating biased responses, but also risk causing harm to the people who interact with the software that uses these models, by altering their internal behavior and decision-making processes, all while providing little to no transparency about their reasoning to the end-user, such as the case of the creditworthiness model. Hence, it is imperative to detect and fix bias in Dev Prompts to avoid causing Societal harm.\n2.3 Vulnerability in Language Models\nDev Prompts present a new attack vector in software applications. Prompt-injection [26] is a novel technique where attackers inject specific phrases into a prompt to cause the LLM to behave in ways against its design intentions as set by the software's developers. Prompt-injection attacks can trigger failures such as unwanted responses that misuse company's resources, such as misusing a company customer support bot to get free access to a paid LLM service. In more serious scenarios, it can lead to the exposure of sensitive information, where it can coax the LLM to reveal confidential information about the company's inner machinations, such as private email addresses [27], or even the physical location of company resources. These attacks can also coax intellectual property contained within the prompt by sharing its internal text, which makes further misuse even easier [28]. The pace of the development of Prompt-injection attacks [26, 29\u201332] is indicative of the growing threat they pose to software applications that use LLMs. Thus, it is important to detect and fix vulnerabilities in Dev Prompts to avoid causing Customer harm.\n2.4 Performance of Language Models\nWhile LLMs can surprisingly well in certain tasks, such as text generation and text summarization, their performance remains mediocre at tasks such as math and logic [33, 34]. Further complicating matters, there is no standardized automatic way to evaluate or optimize Prompts. Prompt engineer-ing is an emerging field that attempts to address these shortcomings by providing a systematic way to write prompts [35, 36]. There are many competing practices such as Chain-of-Thought (COT), where the prompt asks the LLM to explain its reasoning step by step; and \u201cN-Shot\u201d Prompt-ing, where the prompt contains example input-output pairs, and instruction alignment where the prompt contains rules for the model to follow. However, prompt engineering remains a largely manual task with no quantitative guidelines to follow. Thus, there is a need for the automatic evaluation and optimization of Dev Prompts to ensure good performance of the systems that rely on them, and avoid Product-Reputation harm."}, {"title": "3 RESEARCH APPROACH", "content": "3.1 Data Preparation\n3.1.1 Dataset Selection. As discussed in Section 2.1, we focus within this work on Dev Prompts sourced from PromptSet [5] which contains 61,448 unique Dev Prompts collected from 20,598 OSS projects. However, no examination the quality of these prompts or significant cleaning operations were performed during PromptSet's creation. Indeed, upon manual inspection, we found that it contained a number of toy prompts that are not representative of \"production-quality\" Dev Prompts. We tackle cleaning this dataset in Section 3.1.2.\n3.1.2 Dataset cleaning. While PromptSet contains a large and diverse set Dev Prompts, it was important to perform some data cleaning processes to maximize high quality prompts in our experimental set. We identified that 25% of the Dev Prompts PromptSet contained had a length of 31 characters or less. Upon manual inspection, we found most were out of distorted or helper-prompts of little significance, hence we eliminated them from the set of prompts we analyzed. 45,747 Dev Prompts remained. We also removed non-English prompts, further eliminating 5,174 (11.31%) prompts which contain non-ASCII and non-Emoji characters. Future research could focus on these Dev Prompts to determine if the approaches proposed within this work can be applied to prompts in other languages as well. 40,573 prompts remained after this process.\n3.1.3 Prompt Parsing.\nPrompt Canonicalization. Via this processes, we standardized the presentation of the different Dev Prompts into a universal, canonical, representation that was easier to programmatically parse and process. This processes relies on static parsing and regex matching to determine the different variable holes within a Dev Prompt. The resulting canonical representations preserves the original Dev Prompt's text along with the holes inter-weaved within. The prompt holes are delineated within special characters ({ and }). An example of the process of canonicalization is shown in Figure 3.\nPrompt Patching. After standardizing the representation of Dev Prompts, we set out to create appropriate mock values for their different holes in order to ground our consecutive analyses in realistic usage scenarios of Dev Prompts. Due to their aptitude in generative tasks, we utilize LLMs to generate these values. However, the scope from which we can determine the appropriate values for the different prompt holes is not immediately clear, as the containing method or class may not always contain informative comments or names, and a ReadMe file may be too high-level to be relevant for the value of a single Prompt Hole. Context window limitations further complicating matters, as a single class, method, or file may exceed the window size in some cases. Hence, in order to generate mock values for these variables, a process we refer to as Patching the prompt, we rely on the Dev Prompt's text and the variable name corresponding to the Prompt Hole, thus making this process localized and easily transferable to other contexts. To this end, we hand-crafted a prompt following the practices discussed by Sahoo et al. [37], and send it to the LLM to generate mock values for each Prompt Hole. This handcrafted prompt is given at [1].\nIn the case of a prompt containing multiple holes, there were two possible approaches: either patching the holes in parallel in an independent manner or patching them sequentially in a dependent manner. In parallel patching, the mock value for each hole is generated independently of the other prompt hole values, which is formalized in Equation 2. In sequential patching, the mock value generated for prompt hole x is dependent on the values generated for all the prompt holes that appear before it in the Dev Prompt, this is formalized in Equation 2. We validated that generating values for the variables sequentially in their order of appearance, patched prompt holes in a more consistent and logical fashion than via parallel generation.\n$\nval(h_x | patch(prompt,[val(h_{x-1}),val(h_{x-2}),...val(h_1)]))\n$\n$\nval(h_x | prompt),val(h_{x-1} | prompt),val(h_{x-2} | prompt),...val(h_1 | prompt)\n$\nFor the sake of simplicity and cost efficiency, during bias and vulnerability detection and re-mediation, we restrict our patching process to generally only one value for each Prompt Hole, as generating multiple values can lead to an exponential number of combinations possible. An example of a canonicalized prompt and its corresponding generated values via the process of patching is shown in Figure 4. Furthermore, we add multiple prompt-level and code-level mechanisms to ensure that this process does not introduce bias or bias-proneness in prompts.\nOptimization Dataset Synthesis. For the purpose of Dev Prompt optimization, we extend the patching process to create synthetic datasets, which serve as input values assigned to the different prompt holes of the Dev Prompt we aim to optimize. A synthetic dataset is comprised of multiple values for each Prompt Hole, values that are generated by the patching process are optimized for creativity and diversity to mitigate duplication as the quantity of patches per Dev Prompt increases. In practice, these patches are generated in a sequential manner, where we use stratified temperatures during patch generation and include a few randomly-selected previously-generated patches for the same hole as examples to avoid duplication during the generation process. Within our hand-crafted prompt also includes guidelines as a context and we enforce a strict response format to ensure the data generated will conform to the original Dev Prompt's structure [38].\n3.1.4 Dataset Sampling. After applying the cleaning process in Section 3.1.2, we obtained a set of 40,573 Dev Prompts. The size of the set was still too computationally and financially expensive"}, {"title": "3.2 Addressing Bias", "content": "3.2.1 Bias Detection. The issue of detecting biases within Natural language text remains a fraught and complicated issue. Biases can come in many shapes and forms, and can stem from multiple factors. The approach we designed to detect biases is generic, and we use it to focus on three Biases that have been documented extensively within existing literature around Bias in software: Gender-Bias [40\u201344], Race-Bias [20, 45\u201347], and Sexuality-Bias [48\u201350]. We believe our approach can easily be extended to detect other types of biases as well, but we leave this exploration to future research works.\nOur approach is simple but effective, as discussed within Section 2, LLMs are a great tool for linguistic and textual analyses, hence we leverage them via a hand-crafted prompt, available in our replication package [1], that specifies the type of bias we're attempting to detect along with a patched version of the prompt we're evaluating. This prompt generates a JSON file, making it easy to programmatically ingest its results. This JSON contains three fields: whether the Dev Prompt is Explicitly Biased, whether the Dev Prompt is prone to generating biased responses, and an explanation behind the evaluation given. We distinguish between explicit bias and bias-proneness since, as documented by previous research [23], Dev Prompts without explicit bias can still cause LLMs to generate biased-responses.\nFollowing the recommendations of Radford et al. [51] and Brown et al. [52], we designed a Zero-Shot, One-Shot, and Multi-Shot, and measured their performance against established benchmarks for the bias-detection as detailed in Section 4.1.1, and found that the Multi-shot version performed best. Hence, each bias-detection prompt also includes three example inputs: one explicitly-biased, one bias-prone, and one non-biased example. Each example was accompanied with an example expected JSON response. For added transparency, PromptDoctor reports to users the results of this evaluation along process including the explanation behind the evaluation.\n3.2.2 Bias Remediation. While there are many recommendations regarding methods to rewrite prompts to improve their performance [8, 53\u201355], there is no established generic method to rewrite prompts to de-Bias them. Hence, we created an automatic de-Biasing method within PromptDoctor, that relies on a prompt generation-evaluation loop. Due to the prowess of LLMs when in text-generation, we also rely on their assistance during this process.\nFirst, we evaluate the original prompt given by the developer via the method detailed in Sec-tion 3.2.1. Second, if the prompt is determined to be biased or bias-prone, we generate 5 rewrites of the developer-prompt with the goal of minimizing bias and bias-proneness, via a hand-crafted prompt, available in our replication package [1]. Third, we evaluate each of these rewrites for bias. If we determine some of them is also biased or bias-prone, we isolate them and run the same generation-evaluation loop on each of them. We stop this process when we have at least five new non-biased and non-bias-prone Dev Prompts. We then supply these rewrites sorted by distance\u00b9 to the developer. We give developers multiple variants to promote higher flexibility for improved tool adoption [56]. We limit this process for 10 iterations in order to avoid running indefinitely and generating Dev Prompts that are too different from the original Dev Prompts."}, {"title": "3.3 Addressing Injection Vulnerability", "content": "3.3.1 Vulnerability Detection. There is a variety of Prompt Injection attacks possible when inter-acting with LLMs [26, 29\u201332], however, as discussed within Section 2.1, interacting with them via Dev Prompts is generally more constrained than other scenarios. Dev Prompts deployed within a project are generally not directly accessible or editable, and are sent to an LLM after variables within these prompts are interpolated via values shaped by by users. Hence, we focus on injection attacks that are deployed via inserting a malicious string within specific location(s) of the prompt. Testing a prompt for injection vulnerability relies on a collection of 42 known attacks from a corporate dataset\u00b2 and the open web. Each of these attacks are formulated to produce a specific un-common target string as expected results. For each Dev Prompt we examined, we first perform the canonicalization process discussed in Section 3.1.3, and then for each Prompt Hole, we inject a specific attack into it and patch the remaining holes via the prompt patching process, detailed in Section 3.1.3. We then send the Dev Prompt to the LLM, and inspect the corresponding response to determine whether the attack was successful by scanning for the expected target responses. We repeat this process for all the attacks we have in our collection and all of the prompt holes within the prompt. Since these attacks were independent of each other, we performed this process in a parallelized manner, and the equation in Equation 3 shows a formalization of this process. It's notable that PromptDoctor reports which holes that served as successful injection points for these attacks to end-users.\n$\nVulnerability_{prompt} = \\cup_{i=1}^{m} attack_i(\\cup_{j_{pos} =1}^{j+_{pos}} patch(prompt, hole_j))\n$\n3.3.2 Vulnerability Remediation. Similar to Section 3.2.2, there is no established way to harden\u201d prompts against prompt-injection. Instead, most existing methods focus on Model Fine-Tuning [29, 57, 58] to defend against to these attacks. Furthermore, some systems add other layers of security to preemptively detect attack strings or compromised responses and then respond to those attacks with a generic denial response [59]. As discussed in Section 2, these approaches come with their own costs and caveats, hence, we implement a new Prompt hardening process within PromptDoctor as a less-expensive and easier-to-deploy complement to these approaches. Similar to Section 3.2.2, this process also relies on a generation-evaluation loop. First, we analyze a Dev Prompt to determine if it's vulnerable as described in Section 3.3.1. Second, if the Dev Prompt is vulnerable, we ask the LLM via a hand-crafted prompt, given at [1], to generate five new prompts that are a rewrite of the original prompt, and which are hardened against a specific attack. Third, for each generated Dev Prompt, we verify that it contains the same prompt holes as the original Dev Prompt, and then evaluate it for vulnerability it as described in Section 3.3.1. Finally, if all of the attacks in our set fail against one of these new Dev Prompts, we consider this Dev Prompt hardened, else, we add it to our list of vulnerable Dev Prompts for future generation-evaluation loop executions. Empirically, we found that generating a hardened Dev Prompt requires multiple iterations and an associated high cost, hence, we limit this process to 10 iterations, and we stop when we have one new hardened Dev Prompt. Furthermore, we found that Dev Prompts with the lowest number of vulnerable holes were more likely to be successfully hardened, hence we sort the vulnerable Dev Prompts by the number of vulnerable holes before starting a new iteration of the hardening process"}, {"title": "3.4 Addressing Sub-Optimality", "content": "3.4.1 Sub-optimality Detection. By default, we assume all Dev Prompts are sub-optimal and present an opportunity for possible automatic improvement, especially since previous research [60] found that many non-experts struggle with prompting. Indeed, even a prompt writing expert cannot empirically prove their design has reached an optimal state. In addition, any evaluation processes may be hindered by the lack of a dataset. Therefore, we design PromptDoctor that automatically optimizes a Dev Prompt against a synthetic dataset to improve performance without requiring costly manual exploration or data collection by the developer.\n3.4.2 Optimization Process. The PromptDoctor optimization process is summarized as follows: 1. Generation of synthetic training and test datasets based on the Dev Prompt, 2. Creation of a few seed Dev Prompts based on good prompting strategies written by OpenAI [61] and Anthropic [62], 3. Evaluation of all the generated Dev Prompts on the synthetic data, 4. Execution of a self improving optimization algorithm to discover new Dev Prompts as described in Equation 4, 5. Repetition of steps 3-4 until no performance gain is observed on the training dataset.\n$\nOpt_i = M_1 (t_1, s_{i1}, s_{i2}, ..., s_{in}); s_{ij} = \\sum_{k}f(., M_2 (t_2, p_j, D_k))/K\n$\nwhere i is the step count, M a language model, t a meta-prompt, and $s_j$ the tuple of a Dev Prompt pj and its average score received across a dataset D of size K. The i = 0 step is a special case, where pj is sourced from the seed Dev Prompts. Subsequent steps utilize the highest scoring n Dev Prompts generated across all steps of the algorithm. Finally, f represents a task-based evaluator as described below. M\u2081 specifically is used to generate new candidate Dev Prompts, M2 generates responses based on candidate Dev Prompts and elements from the synthetic training dataset.\nEvaluation. After the seed Dev Prompts are generated and after each optimization step, we evaluate the quality of newly generated Dev Prompts on the synthetic training dataset. We select a scoring criteria based on the categorization of the initial Dev Prompt.\nTranslation. We use the BLEU metric to compare the quality of the generated translation with the reference synthetic translation [63].\n$\ns_j = \\sum_{k}BLEU(D_k[\"translation\"], M_2(p_j, D_k[\"source\"]))\n$\nSummarization. We compare the semantic similarity of the generated summary with the reference summary using the cosine similarity of the embedded representations [64]. Using embedding function E:\n$\ns_j = \\sum_{k} cossim(E(D_k[\"summary\"], E(M_2(p_j, D_k[\"source\"]))\n$\nError Correction. We use the GLEU metric to compare the quality of the generated phrase with the reference synthetic phrase [65].\n$\ns_j = \\sum_{k}GLEU(D_k[\"correct\"], M_2 (p_j, D_k[\"source\"]))\n$\nQA Refinement. We utilize the \u201cLLM as a judge\u201d technique to score QA tasks due to their highly varied nature [66]. As a preprocessing step, each QA Dev Prompt generates a corresponding scoring prompt t3 which will be used to evaluate the quality of the outputs during optimization.\n$\ns_j = \\sum_{k} M_3 (t_3, M_2 (P_j, D_k [\"source\"]))\n$\nAs an example, if a source Dev Prompt pj requests a Markdown response, the scoring prompt t3 might be \"Is the following text in proper Markdown form? Reply yes or no. text\u201d. The score for this Dev Prompt sj would depend on the quantity of outputs generated by model M2 across the synthetic dataset which pass the t3 criteria, as judged by model M3.\""}, {"title": "4 EMPIRICAL EVALUATION", "content": "To evaluate our approach, we implemented PromptDoctor in Typescript, with different modules corresponding to the functionalities discussed within Section 3. We focus within this section on evaluating PromptDoctor with OpenAI GPT 40, due to its superiority to other LLMs [16], and due to time and budget limitations. However, our code base relies on a common interface to interact with LLM APIs, making it easy to extend PromptDoctor to support other LLMs.\nFurthermore, PromptDoctor also includes a UI and automatic Dev Prompt-extraction mecha-nisms from source code. While these additions facilitate the usage of the different functionalities offered by PromptDoctor, they do not affect the results presented in the following sections, espe-cially since PromptSet [5] contained pre-extracted prompts. Hence we don't evaluate them. We plan to discover their effectiveness within a future qualitative user-study of PromptDoctor.\n4.1 Bias Prevalence and Remediation\nRQ1:\nHow widespread are Bias and Bias-proneness in Dev Prompts? How effectively can we minimize these issues?\n4.1.1 Bias Detection Benchmarking. To verify the bias detection process we designed in Section 3.2.1, we performed various benchmarking operations with benchmarks corresponding to the different types of biases we aimed to detect. For Gender-Bias, we used the benchmark provided by Samory et al. [67], and we found that our hand-crafted Multi-shot bias detection prompt out-performed their BERT model that was fine-tuned on multiple components of the benchmark, by achieving an F-1 score of 0.93 compared to 0.81. Our zero-shot and our one-shot prompts had F-1 scores of 0.9 and 0.92 respectively, hence why chose a Multi-shot prompt for our approach. For Race-Bias and Sexuality-Bias, we were unable to find specific benchmarks, so we opted for one provided by Glavas et al. [68], which contains those biases among others. We found that using the customized Multi-shot prompts with GPT-40 for Race-Bias and Sexuality-Bias achieved F-1 scores of 0.46 and 0.13, respectively, compared to 0.59 achieved by a fine-tuned RoBERTa model [68], giving credence to the accuracy of these prompts as well.\n4.1.2 Bias Prevalence. Concerning the prevalence of Bias and Bias Proneness within Dev Prompts, we found that the different types of bias had different rates of prevalence. Indeed, we found that 2.46% of Dev Prompts were explicitly Gender-Biased, and that 0.57% Gender-Bias-Prone, making a total of 3.03% of Dev Prompts likely to generate Gender-Biased responses. Concerning, Race-Bias, we found that 0.09% of prompts were explicitly biased and 0.66% were bias prone, and making a total of 0.75% of Dev Prompts likely to generate Race-Biased responses. Finally, for Sexuality-bias, we found that 0.09% of Dev Prompts were explicitly biased and likely to generate Race-Biased responses. These results are illustrated in Figure 5a.\nWhile these percentages might not seem elevated, they are still significant, as these Dev Prompts may have a cascading effects on the software they make up, thus causing harm to the people who interact with the latter. An example of a biased Dev Prompt is shown in Figure 6, where the Dev Prompt assumes the gender identity of the person to be male, which may cause the LLM to mis-gender the person at hand and produce erroneous descriptions.\nAn example of a non-explicitly-Gender-Biased Gender-Bias-prone Dev Prompt is given in Figure 7. This Dev Prompt is ambiguous, causing the LLM to assume about the gender of \"KC\" based on the usage of the word \"secretary\", and give responses that are affected by this assumption. For example, the response in Figure 8 indicates that KC is being assigned a female gender by the LLM, and given"}, {"title": "4.1.3 Bias Remediation", "content": "Across the different Bias categories we considered, our Bias remediation approach is able to fix 68.29% of the Biased and Bias-prone Dev Prompts overall, and the details about its performance are visualized in Figure 5b. Our approach performs best on Gender-biased and Gender-bias-prone Dev Prompts, with a fix rate of 82.81%, followed by a fix rate of 12.50% for Race-biased Dev Prompts, and a fix rate of 50% for Sexuality-biased Dev Prompts. An example of a de-Biased Dev Prompt is shown in Figure 10, which is a rewrite of the Gender-biased Dev Prompt in Figure 6. An example of a rewrite of a Gender-bias-Prone Dev Prompt is shown in Figure 11, which is a rewrite of the Gender-Bias-Prone Dev Prompt in Figure 7. As can be seen from this example, the rewritten Dev Prompt elicits a mix of both male and female coded hobbies in the response, which can be attributed to any gender.\nOverall, we believe our generic bias remediation approach is able to fix an important portion of different types of Biased and Bias-prone Dev Prompts as shown by our results and examples, and forms a foundation on which we can further establish even more robust Dev Prompt Bias remediation strategies. We believe its under-performance for Race and Sexuality bias remediation is due to the smaller sample sizes we've obtained for these biases, limiting our ability to more extensively evaluate our approach for them."}, {"title": "Finding 1", "content": "We find that 3.46% of the Dev Prompts were prone to generating biased responses. With our approach, were were able to de-Bias 68.29% of them."}, {"title": "4.2 Injection Vulnerability Prevalence and Remediation", "content": "RQ2:\nHow widespread is Vulnerability to injection attacks in Dev Prompts? How effectively can we harden Dev Prompts against them?\n4.2.1 Injection Vulnerability Prevalence. Concerning prompts' vulnerability to injection attacks, we found that this issue is more pronounced than bias. Indeed, as shown in Figure 1212.a, 10.75% of Dev Prompts are vulnerable to injection attacks. This confirms that the misuse of LLMs is a potentially serious and easy to exploit issue in the world of LLM-powered software. An example of a vulnerable Dev Prompt is shown in Figure 13, where the LLM is told via an attack to give two answers to each prompt, a standard response, and a MAN response: a humorous response unrestricted by an LLM\u2019s safeguards. While this example is humorous, it is easy to see how this could be exploited to cause harm to an organization, by causing the LLM to reveal sensitive information within its original prompt for example [28], among other misuses discussed in Section 2.3.\n4.2.2 Injection Vulnerability Remediation . As shown in Figure 1212.b, our approach to hardening Dev Prompts to injection attacks is able to fix 41.81% of the vulnerable Dev Prompts. These results confirm the validity of our approach, and the potential that a simple Dev Prompt rewrite can have to mitigate the risk of injection attacks. We believe that our approach, coupled with programmatic fixes such as input sanitation, and the prevention of appending values at the beginning or the end of a Dev Prompt, which represent common attack strategies [30], can go a long way in hardening LLM-powered software against injection attacks."}, {"title": "Finding 2", "content": "We found that 10.75% of Dev Prompts are vulnerable to injection attacks. We were able to harden 41.81% of these prompts."}, {"title": "4.3 Prompt Optimization", "content": "RQ 3\nHow widespread is sub-optimality of prompts in Dev Prompts? How effectively can we optimize Dev Prompts' performance?\n4.3.1 Optimizing"}]}