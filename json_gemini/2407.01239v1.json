{"title": "SGCCNet: Single-Stage 3D Object Detector With Saliency-Guided Data Augmentation and Confidence Correction Mechanism", "authors": ["Ao Liang", "Wenyu Chen", "Jian Fang", "Huaici Zhao"], "abstract": "The single-stage point-based 3D object detectors have attracted widespread research interest due to their advantages of lightweight and fast inference speed. However, they still face challenges such as inadequate learning of low-quality objects (ILQ) and misalignment between localization accuracy and classification confidence (MLC). In this paper, we propose SGCCNet to alleviate these two issues. For ILQ, SGCCNet adopts a Saliency-Guided Data Augmentation (SGDA) strategy to enhance the robustness of the model on low-quality objects by reducing its reliance on salient features. Specifically, We construct a classification task and then approximate the saliency scores of points by moving points towards the point cloud centroid in a differentiable process. During the training process, SGCCNet will be forced to learn from low saliency features through dropping points. Meanwhile, to avoid internal covariate shift and contextual features forgetting caused by dropping points, we add a geometric normalization module and skip connection block in each stage. For MLC, we design a Confidence Correction Mechanism (CCM) specifically for point-based multi-class detectors. This mechanism corrects the confidence of the current proposal by utilizing the predictions of other key points within the local region in the post-processing stage. Extensive experiments on the KITTI dataset demonstrate the generality and effectiveness of our SGCCNet. On the KITTI test set, SGCCNet achieves 80.82% for the metric of \\(AP^{3D}\\) on the Moderate level, outperforming all other point-based detectors, surpassing IA-SSD and Fast Point R-CNN by 2.35% and 3.42%, respectively. Additionally, SGCCNet demonstrates excellent portability for other point-based detectors.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid development of intelligent transportation and robot technology, LiDAR-based 3D object detection has become a key technology for intelligent agents to acquire environmental information, attracting widespread research interest [1], [2]. Within related methods, single-stage point-based detectors can achieve a better balance between accuracy and inference efficiency, gaining increasing attention [3], [4], [5], [6].\nFor example, IA-SSD [7] and DBQ-SSD [7] can respectively achieve impressive 82FPS and 162FPS inference efficiency on the KITTI dataset [8] while maintaining ideal detection performance, surpassing other structured point cloud detectors comprehensively, greatly enhancing the application prospects of point-based detectors on devices with high real-time requirements. However, these type detectors still suffer from two prominent issues, namely Insufficient Learning of Low-Quality Objects (ILQ) and Misalignment between Localization Accuracy and Classification Confidence (MLC).\nFor ILQ, as limited training data cannot cover all possible feature distributions, especially for low-quality targets that appear less frequently, detectors will lack learning about them. And under the paradigm of repeated sampling and limited augmentation [9], the models will develop feature dependencies, giving better decision-making capabilities to prominent features while neglecting the learning of other features. In SPSNet, Liang et al. [10] obtained a way to represent point saliency through loss competition. When only discarding the top 10 most salient points, the model's average \\(AP^{3D}\\) decreased by 8.61%, providing an intuitive explanation for this scenario. As shown in Fig. 1, we visualize the saliency scores of several objects in KITTI dataset. It can be seen that the distribution of saliency features is relatively fixed, with Car objects concentrated near the roof, Cyclist objects near the head and wheels, and Pedestrian objects near the head and feet. For low-quality objects, these strong saliency distribution areas are likely to be missing and sparse. If the model's feature learning capability is limited to saliency regions, it will greatly reduce its robustness.\nThe MLC problem is inherent in single-stage detectors, un-like two-stage detectors which can refine Rigions of Interesting (Rols) for a second time. The decision on target confidence and geometric information regression is determined by two separate network branches without any connection between them. This can mislead the Non Max Suppression (NMS) operation in post-processing. We list three typical examples in Fig. 2. The first is the detection of too many false positive targets, where the detection results overly rely on the model's semantic learning ability based solely on confidence. This is disadvantageous for detectors using point-based 3D backbones because point clouds are geometrically rich but textureless, leading to a high probability of objects with similar geometry to be detected as false positives. The second scenario is when the NMS selects target box positions and sizes that are not optimal. Although the selected points have the highest confidence, their IoU with GT is lower than other key points. In the third scenario, the accurate target position can be accurately predicted by the model, but it is not classified as a foreground category. All three scenarios directly reduce the model's performance metrics.\nIn this paper, we propose SGCCNet to alleviate the impact of the above two issues. Firstly, SGCCNet employs a saliency-guide data augmentation (SGDA) method to enrich the feature distribution of foreground objects in the training data. Specifically, We use a differentiable process of moving points towards the point cloud centroid to act as a point dropout process to approximate the saliency scores of points. During the learning process, we randomly drop a certain number of salient points of each object to create a new ground truth for GT sampling. For the MLC problem, we have designed a Confidence Correction Mechanism (CCM) in the post-processing stage. For proposals with low IoU, CCM reduces their confidence. For proposals that are considered background by the model but have a significant overlap with bounding boxes in the neighborhood, CCM increases their probability of being true positive ones.\nWe validated the superiority of SGCCNet on the KITTI dataset. Specifically, the proposed SGCCNet outperforms all previously published point-based approaches. Under the multi-class training scheme, SGCCNet achieved 80.82% \\(AP^{3D}\\) on the Car target in the KITTI test set, surpassing prior methods that utilize structure-based backbones while also demonstrating higher efficiency. For the Pedestrian and Cyclist targets, compared to the latest point-based models, SGCCNet also showed improvements of 1% and 3.1% \\(AP^{3D}\\) respectively. In summary, the key contributions of our work are as follows:\n\u2022 A new saliency-guided data augmentation method is proposed, which enhances the diversity of the training data distribution at the feature level, thus improving the robustness of the model to low-quality object detection.\n\u2022 A new point-based backbone that combines geometric normalization modules and skip connection blocks is proposed to alleviate the internal covariate shift problem and feature forgetting problem.\n\u2022 A confidence correction mechanism for post-processing is proposed to effectively address the misalignment between localization accuracy and classification confidence (MLC) issue commonly seen in single-stage detectors.\n\u2022 A high-efficiency and high-precision single-stage point-based 3D detector SGCCNet is proposed, and experimental results show that our method outperforms other similar methods significantly on the KITTI dataset.\nThe structure following this text is as follows: In Section II, we review the design principles of classic and state-of-the-art 3D object detectors, and list some methods to overcome ILQ and MLC problems. In Section III, we introduce the key components of SGCCNet, detail the implementation process of saliency-guide data augmentation, and finally introduce its end-to-end training loss. In Section IV, we demonstrate the superiority of SGCCNet through comprehensive comparative experiments and ablation studies. Finally, in Section V, we summarize this paper and provide prospects for future work."}, {"title": "II. RELATED WORK", "content": "As mentioned above, LiDAR-based 3D detectors can be classified into grid-based, range-based, and point-based according to the data form before inputting to models. In this section, we will review the key ideas of classic and state-of-the-art models within each domain, and analyze in detail the measures currently taken by models to alleviate the issues of ILQ and MLC."}, {"title": "A. 3D Detectors", "content": "Grid-based. There are three major types of grid represen-tations: voxels, pillars, and BEV feature maps. Voxel-based models [11], [12] can preserve the spatial information of the original point cloud scene to the greatest extent based on the grid size. Pillar-based methods [13], [14] normalize the voxel space along the z-axis and significantly reducing the number of cells. The Bird's-eye view (BEV) feature map is a dense 2D representation, where each pixel corresponds to a specific region and encodes the points information in this region [15], [16], [17], [18], [19]. In recent years, researchers have found that BEV representation can naturally integrate with downstream tasks such as behavior decision-making and trajectory planning to form end-to-end intelligent systems, leading to a resurgence in research on perception models based on BEV [20], [21], [22], [23].\nRange-based (RV). Since range images are 2D representa-tions like RGB images, range-based 3D object detectors can naturally borrow the models in 2D object detection to handle range images [24], [25].\nBoth of the above methods convert point clouds into a structured representation, with the main advantage being the ability to generate more dense feature maps. Even the space not occupied by the original point cloud learns features, which is crucial for sparse and incomplete objects. However, these methods inevitably result in information loss during the process of structuring the point cloud, and their reliance on large parameter 3D backbones makes it challenging to strike a balance between inference efficiency and detection accuracy. In contrast, in LiDAR scenes with multi-beams and fewer points, the advantages of structured point cloud detectors become less pronounced, and point-based detectors begin to stand out.\nPoint-based. The current state-of-the-art point-based detec-tors [3], [4], [5], [6], [5], [10] follow the design paradigm of PointNet [26], PointNet++ [27], and PointMLP [28]. The point cloud is processed through a set abstraction (SA) layer consisting of multiple stages of downsampling modules, local feature learning modules, and feature aggregation modules to learn rich spatial and semantic information.\nIn terms of results, single-stage point-based detectors have achieved performance comparable to structure-based models in multi-beam LiDAR scenes, and their real-time inference capability and lightweight model architecture are desirable for many mobile embedded devices. The main performance bottlenecks are two factors: inadequate learning of low-quality targets (ILQ) and misalignment between localization accuracy and classification confidence (MLC)."}, {"title": "B. Dealing with ILQ", "content": "As mentioned above, limited training data cannot cover all possible feature distributions. For models that rely on salient features, they cannot actively explore information in non-significant parts of the target. Inspired by AlexNet [29], all current 3D detection models adopt some common data augmentation techniques to improve the feature distribution of training data, such as using geometric transformations to perturb both the overall scene and local features of the target. Choi et al. [30] randomly occluded parts of the foreground target to expand the training data by 2.5 times. Reuse et al. [31] conducted detailed controlled experiments to demonstrate the effectiveness of local target transformations in improving detector performance. Hu et al. [32] proposed pattern-aware GT sampling, enhancing data augmentation by subsampling objects based on LiDAR characteristics. Wang et al. [33] consistently pasted virtual high-quality targets into point clouds to enhance the model's perception of low-quality targets.\nThese methods aim to improve model robustness by enhancing the diversity of the training data distribution. However, these augmentation processes are random, and data diversity does not necessarily imply feature diversity, leading to significant bottlenecks in benefits. This paper proposes a saliency-guided data augmentation method that, from the perspective of feature learning, removes salient features on which the model relies, forcing the model to actively explore information in low saliency regions and enhance the diversity of feature distribution in training samples from the source."}, {"title": "C. Dealing with MLC", "content": "He et al. [34] proposed an auxiliary network to convert the features of key points belonging to the grid in the 3D backbone into point-wise representation to improve localization accuracy. They also introduced a part-sensitive warping operation to align the confidences to the predicted bounding boxes. CIA-SSD [35] combined the predicted IoU with the key point classification probability as the final confidence. Wang et al. [36] proposed a confidence-based filtering mechanism to filter out poorly localized proposals. Sheng et al. [37] proposed a new Rotation-Decoupled IoU (RDIoU) method to generate more effective optimization targets. In addition, models such as FVTP [38], DFDNet [39], and DDIGNet [40] have also adopted IoU prediction branches to improve the MLC problem.\nAlthough the above methods have achieved certain effects, they are mostly designed for anchor-based detection heads, and point-based detection heads have not been given much attention. Specifically, compared to anchor-based detection heads, the sparsity of keypoints obtained by point-based detection heads varies, leading to different numbers of predicted bounding boxes for each target, making these methods unsuitable for direct use in point-based detection heads. In this paper, a new confidence correction mechanism is proposed for point-based detection heads, taking into account the prediction situations of other proposals within the keypoint domain and the density of key points in the domain. On the one hand, it can filter out low-quality proposals, and on the other hand, it can explore potential high-quality proposals."}, {"title": "III. METHOD", "content": "As mentioned above, our goal is to improve the model's ability to learn from low-quality targets and enhance target localization accuracy by correcting confidence. To achieve this, we propose SGCCNet, a generic and unified single-stage point-based 3D object detector as illustrated in Fig. 3, which includes three core components, namely a saliency-guided data augmentation method, SA layers with geometric normalization modules and skip connection blocks, and a confidence correction mechanism.\nIn this section, we will detail the design principles and implementation process of SGCCNet. Section III-B introduces the mathematical symbols and their meanings to be used. In Section III-C, we present the method for obtaining point saliency in the saliency-guide data augmentation and the specific enhancement process. Section III-D describes the specific structure of the 3D backbone of SGCCNet, focusing on the design rationale of the geometric correction module and skip connection block. Section III-E proposes mechanisms for point-based detection heads and confidence correction. Finally, in Section III-F, we discuss the model loss in the end-to-end training process."}, {"title": "B. Preliminary", "content": "Let \\(D\\) be the training dataset used for experiments, consisting of \\(N\\) point cloud scenes \\(P_i\\), \\({P_i \\in \\mathbb{R}^{n_i\\times(3+C)} | i = 1, 2, ..., N }\\). Here, \\(n_i\\) represents the number of point clouds in the current scene, and \\(C\\) represents other features besides the spatial positions of the point clouds, such as intensity. The annotated ground-truth for each scene is denoted as \\(B_i\\), \\({B_i \\in \\mathbb{R}^{m_i\\times 8} | i = 1, 2, ..., N }\\), where \\(m_i\\) is the number of ground-truth annotations in the scene. The eight-dimensional features include the center position of the bounding box \\({x,y,z}\\), dimensions \\({l, w, h}\\), and category \\(c\\). The total number of annotated ground-truth in the dataset is \\(A = \\sum^N m_i\\)."}, {"title": "C. Saliency-Guided Data Augmentation (SGDA)", "content": "For LiDAR-based 3D object detection, limited training samples cannot cover all possible target feature distributions. In a training mode with limited data and repeated sampling, the model gradually relies on prominent features with common distributions and uses them as decision criteria without exploring features in other less prominent regions of the target. Liang et al. [10] have demonstrated the model's dependency on prominent features in their work SPSNet. SPSNet assigns a Gaussian soft label to foreground points and regresses geometric information belonging to Dirac bounding boxes, obtaining the saliency score of points through stability and perturbation adversities. Ultimately, with the scenario of discarding only 10 foreground points, the model's average AP drops rapidly by 8.61%. This dependency on salient features is detrimental to the robustness of the model's detection.\nWe hope to develop a purposeful data augmentation approach, that is, from the perspective of features, by using saliency analysis to identify common features that the model relies on, removing them, and forcing the model to explore features in the target that were originally in low saliency regions. However, currently, there is not much attention given to point-wise saliency analysis methods for point-based 3D object detection. We have shifted our focus to developing more in-depth saliency analysis methods in point cloud classification tasks. Although these are two different tasks, we believe that the saliency scores obtained have commonalities for two reasons: 1) Considering the model structure. Point-based detectors throw vote points into the SA layer to determine the semantic information of local regions, a process that is identical to the prediction process of point cloud classification models. 2) Considering feature learning. Saliency scores are determined by the specific features of the model, and using similar structured models, the saliency features they focus on should be similar as well. We will demonstrate our viewpoint in the experimental section, and next we will introduce the specific data augmentation process.\nDataset Preparing. We conduct our experiments on the KITTI dataset \\(D\\). Firstly, we extract all foreground objects using the annotated ground-truth \\({B\u00bf \\in \\mathbb{R}^{m_i\\times 8} | i = 1, 2, ..., A }\\) in \\(D\\), and then construct a classification dataset similar to ModelNet40 [41] and ScanObjectNN [42]. Each point in the dataset contains 4-dimensional features, representing spatial position and intensity. KITTI categorizes foreground objects into Easy, Moderate, and Hard levels based on factors such as occlusion and the number of points within the bounding box. We discard samples with fewer points than a certain threshold, and train the classification model on all levels. It is worth noting that we only perform data augmentation on Easy and Moderate levels, while retaining all samples in the Hard level.\nClassification Model Preparing. We have streamlined the structure of SGCCNet and constructed an elite model SGCCNet-elite for classification tasks as shown in Fig. 4 (a). This model consists of only one SA layer for feature learning, and the training samples do not undergo downsampling in the SA layer. Finally, the point-wise features of each point are pooled using max pooling to form the overall feature of the sample, which is then fed into a Fully Connected (FC) Layer for classification.\nSaliency Analysis. Let \\(E_o(p^i)\\) be a classification model, where \\(p^i := (p_1, p_2, ..., p_{k_i})\\) is a training sample, and \\(p \\in \\mathbb{R}^{k_i\\times 4}\\). \\(L_c()\\) is the classification loss function.\n\\[L_c = \\frac{1}{A} \\sum_{i \\epsilon c=1}^3 \\sum_{i}^A Y_{ic} log(\\hat{Y}_{ic})\\qquad(1)\\]\nAmong them, \\(Y_{ic}\\) is the label, with a value of 1 if sample \\(p^i\\) belongs to class \\(c\\), otherwise 0, and \\(\\hat{Y}_{ic}\\) is the predicted probability. The most intuitive way to determine point signifi-cance is to gradually discard points and observe the change in loss \\(L_c\\). However, due to the large number of points and the fact that the contribution of each point to the whole is not isolated, this method is time-consuming and inaccurate. Inspired by Zheng et al. [43], we transform the process of discarding points into a differentiable process of moving points to calculate the significance score of each point. Specifically, in the classification task, we normalize the feature information of each sample.\n\\[\\hat{p} = \\frac{p_i - \\mu_{max}}{\\sum_{c=1}^4 p_{jc}},\\qquad(2)\\]\nThe center of the sample is now moved to the origin. Based on the mechanism of LiDAR scanning imaging, we have reason to believe that the points at the origin have little contribution to the classification of the sample. The impact of moving points from other positions in the sample to the origin on model decisions is almost the same as discarding points. Since points are not angle invariant, there are difficulties in measuring gradients in Euclidean space, so we consider point shifting in the Spherical Coordinate System.\nIn the Spherical Coordinate System, a point \\(p\\) is represented as \\((\\phi, \\psi, \\theta)\\) with a as the sphere core, \\(r\\) is distance of \\(p\\) to \\(a\\), and \\(\\phi\\) are the two angles of a point relative to a. After shifting \\(p\\) in the direction of \\(r\\) towards sphere core a by \\(\\delta\\), the change in model loss is \\(\\frac{\\partial L_c}{\\partial r}\\), where \\(r = \\sqrt{\\sum_{c=1}^{3}(p_c-a_c)^2}\\), as:\n\\[\\begin{aligned} &\\hat{p_1} - a_1 = r cos \\psi sin \\phi \\\\ &\\hat{p_2} - a_2 = r cos \\psi sin \\phi \\\\ &\\hat{p_3} - a_3 = r sin \\phi \\qquad(3)\\end{aligned}\\]\nwhere \\(p^i = \\{\\hat{p}_{ic}\\}_{c=1,2,3}\\), \\(a = \\{a_c\\}_{c=1,2,3}\\) represent the 3D coordinates value of \\(p^i\\) and a. So:\n\\[\\frac{\\partial r}{\\partial p_{ic}} = \\frac{\\hat{p}_{ic} - a_c}{r},\\qquad(4)\\]\n\\[\\frac{\\partial \\hat{p}^i}{\\partial r} = \\frac{\\frac{\\partial \\hat{p}_{ic}}{\\partial r}}{\\frac{\\partial L_c}{\\partial r}}.\\qquad(5)\\]\nSo that after shifting \\(p^i\\), the change in model loss is \\(\\frac{\\partial L_c}{\\partial r}\\). In practical computation, we also set the fourth dimension intensity value of the central position point to 0, and add it as a spatial information to the gradient calculation.\nDroppint Points. For a training sample \\(p^i\\), after obtaining the saliency scores \\({s_j}\\ | j = 1,2,..., k_i\\) for each point, we delete high saliency points in the sample according to a preset ratio. The number of points to be deleted, denoted as \\(d^i\\), is linearly proportional to the number of points in the sample, \\(k_i\\), i.e., \\(d^i = [\\alpha k_i + \\beta]\\). To ensure the accuracy of saliency ranking, the \\(d^i\\) points are not deleted all at once, but are deleted at fixed intervals as described in Algorithm. 1. After discarding salient features, the sample is restored to its original space and used for training alongside the original ground truth in the GT sampling process. Fig. 5 illustrates the changes in a set of scenarios before and after point removal. The entire saliency-guided data augmentation process is shown in Fig. 6."}, {"title": "D. 3D Backbone", "content": "Similar to previous state-of-the-art point-based detectors, SSGCNet also utilizes a PointNet++-style 3D backbone to extract features from point clouds. This backbone incorporates multi-stage downsampling, multi-scale feature learning, and local feature aggregation processes to extract point-wise features with rich semantic and geometric information.\nIt is worth noting that the downsampling scheme of SGCC-Net is designed based on IA-SSD. Due to the limited accuracy of semantic information learning by the point-based backbone, SGCCNet uses the Farthest Point Sampling (FPS) algorithm in the first two stages, and in the last two stages, sampling is based on the predicted point-wise foreground probability. The former ensures global coverage of sampling points, while the latter ensures a high recall rate of foreground points, reducing target information loss. In addition, SGCCNet also includes a geometric normalization module and skip connection block.\nGeometric normalization module (GNM). We found that the PointNet structure ignores the Internal Covariate Shift (ICS) problem caused by the irregular and sparse geometric properties of point clouds during the process of aggregating local features. Specifically, in each learning stage, the same MLP layer needs to deal with a large number of regions with different geometric information simultaneously, and it is difficult for a simple MLP to achieve stable convergence speed and generalization ability from such a diverse feature distribution. Therefore, we propose to alleviate this problem by performing geometric normalization on the locally aggregated features, treating each ball used for learning local features as a batch, and applying 'Batch Normalization (BN)' operation before inputting into the MLP layer. The structure of GNM is shown in Fig. 7. In detail, we first move the distribution of local features within each ball to a more normalized space with the ball center as the mean, and then increase the diversity of the feature distribution through learnable weight and offset parameters of the same dimension as the features. Finally, to prevent information loss, we concat the normalized features with the original features and input them into the MLP layer for learning local features. The specific structure of the geometric normalization module is shown in Fig. 7. Let \\({f_{i,j}\\}_{j=1,2,...,k} \\in \\mathbb{R}^{k\\times d}\\) be the local features of the sampling point \\(p_i\\), where \\(k\\) is the number of neighbors in its local region, and \\(d\\) is the feature dimension of the sampling point and its neighbor points. We normalize the features of neighbor points in the local region through the following equation:\n\\[\\{\\hat{f_{i,j}}\\} = \\alpha \\frac{\\{f_{i,j}\\} - f_i}{\\sigma} + \\beta\\qquad(6)\\]\n\\[\\sigma = \\frac{1}{n} \\sqrt{\\frac{\\sum_{i=1}^{k} \\sum_{j=1}^{k\\times n \\times d} (f_{ij} - f_i)^2}{\\sigma + \\epsilon}}\\qquad(7)\\]\nSimilar to BN layer, where \\(\\alpha \\in \\mathbb{R}^d\\) and \\(\\beta \\in \\mathbb{R}^d\\) are learnable parameters, and \\(\\cdot\\) indicates Hadamard product. \\(\\epsilon = 1e^{-5}\\) is a small number for numerical stability. Note that \\(\\sigma\\) is a scalar that describes the feature deviation across all local groups and channels. By doing so, we transform the points via a normalization operation while maintaining original geometric properties. The features after geometric normalization will be concatenated with the original features for subsequent learning.\nSkip Connection Block (SCB). When determining the appropriate number of training epochs for SGCCNet, we found that some high-quality targets were detected with high confidence early in training, but as training progressed, the confidence decreased below the threshold, undoubtedly reducing the model's performance. After analysis, we believe there may be three reasons for this: 1) The sparsity of the targets greatly increases after continuous downsampling, causing sparse sampling points to lose the ability to perceive the overall geometric information of the targets. 2) As the ball query radius increases, noise and features of other targets in the local area are introduced while expanding the model's receptive field, leading to the loss of key information of the corresponding targets after pooling layers. 3) For some targets, shallow features have a better ability to represent target information. In testing classification tasks, the fact that models with extremely small parameter amounts can achieve classification levels far higher than detection models directly proves this point. We refer to this type of problem as feature forgetting and have designed a skip connection block to alleviate this issue. The structure of SCB is shown in Fig. 8.\nThe SCB alleviates the problem of feature forgetting by enabling interactions between key point features of adjacent SA layers in the 3D backbone, as shown in Fig. 8. Let the feature of the \\(l\\)-th SA layer's sampled points be \\({F_i}\\)^{k_i}_{i=1,...,k} \\in \\mathbb{R}^{k\\times d_i}\\), where \\(k\\) is the number of sampled points at the current stage, and \\(d\\) is the feature dimension. Based on the basic structure of PointNet++, it is known that the sampled points at the current stage are also sampled points from the previous stage. Therefore, let the features of the sampled points in the \\(l\\)-th SA layer at the previous stage be \\({\\overrightarrow{F}-1}\\}^{k_i}_{i=1,...,k} \\in \\mathbb{R}^{k\\times d_{i-1}}\\). The learning process of SCB is as follows: (insert equation here).\n\\[g_i = \\Psi_{pos}(\\Psi_{pre}(\\overrightarrow{F}-1) + F)\\qquad(8)\\]\nThe \\(\\Psi_{pre}\\) is a module composed of residual networks, which elevates the dimension of features \\(\\overrightarrow{F}-1\\) from \\(d_{i-1}\\) to \\(d_i\\) from the previous stage, and \\(\\Psi_{pos}\\) further interacts with the features. The design of SCB is inspired by He et al.'s work [34], but we naturally leverage the advantages of point-based backbone hierarchical downsampling, without any feature projection or transformation processes. As shown in Fig. 8, with the addition of SCB, shallow features and deep features of the same sampling point, as well as local small region features and large region features, interact with each other. Even in the subsequent downsampling process where the sampling points become sparser or noise is introduced, valuable information from earlier stages is still retained."}, {"title": "E. Confidence Correction Mechanism (CCM)", "content": "The previous advanced single-stage point-based detectors use two completely independent branch networks in the detection head to respectively regress the confidence of predicted boxes and geometric information. During post-processing, the NMS operator is guided by confidence scores, retaining predicted boxes with the highest confidence scores in local regions and at any position with confidence scores higher than a threshold. This is disadvantageous for single-stage point-based detectors, as high confidence scores do not necessarily represent high-quality localization accuracy, and the rich geometric information of point clouds, coupled with the lack of semantic information, makes it difficult for the point-based backbone to learn precise confidence, leading to a large number of missed detections and false positive detections. Researchers have referred to this issue as misalignment between localization accuracy and classification confidence.\nAs mentioned in Section 2, the method with IoU prediction branch brings hope to alleviate the MLC problem of single-stage detectors. These methods predict the confidence of proposals while also predicting their IoU values with GT boxes. The final confidence sent to NMS is a joint indicator of both. SGCCNet also adopts this method to improve model performance. Furthermore, we not only consider the prediction of a single voting point but also incorporate the predictions of surrounding voting points into the reference range for confidence correction. The work closest to our idea is NIV-SSD [44], but it only applies to single-class detection with anchor-based detection heads, and the issue of uneven prediction of each target voting point in point-based detectors is not considered.\nFor a stable training SGCCNet, the key points output by the 3D backbone after voting will move to positions near the center of the target box. As shown in Fig. 9, for the same target, different voting points have a certain degree of clustering in terms of their positions and regression predicted boxes. Therefore, we use the predictions of neighboring voting points to correct the confidence of the current voting point. We believe that voting points in local regions with more neighbors having high IoU values are more accurate, while those with unreliable confidence. The specific confidence correction mechanism works as shown in Algorithm 2. Let the parameters of the predicted box regressed by the current voting point be \\(b_i = (x,y,z,w,l,h,r)\\), we calculate its IoU value with the predicted boxes of other voting points \\(B = \\{b_j\\}_{j=1,...,k} \\in \\mathbb{R}^{k\\times 7}\\), and consider the predicted boxes with IoU values greater than a certain threshold as neighbors of the current voting point. Finally, the average IoU value between the voting point and its neighboring voting points' predicted boxes will be used as a weight to calibrate the confidence of the current voting point. In addition, CCM sets strict conditions to filter out boxes with high localization accuracy but confidence lower than a threshold. Before the NMS operation, we give them a certain increment to increase their probability of being considered as foreground. We will analyze the advantages of CCM in detail during the experimental phase."}, {"title": "F. End-to-End Training", "content": "SGCCNet is a one-stage detector, using an end-to-end multi-task training fashion. Firstly, there is the semantic loss \\(L_s\\). The last two SA layers in SGCCNet use foreground sampling, so it is necessary to obtain the semantic information of the sampled points from the previous stage. Due to the highly unbalanced number of different categories of objects in the KITTI dataset, such as the number of Car objects being five times that of Pedestrian objects and ten times that of Cyclist objects, and the point number of Car objects is usually more than the other two, we adopt weighted cross entropy (WCE) loss to manually emphasize rare categories. The WCE loss can be formulated as:\n\\[L = \\sum_{c=1}^C a_{c} y_{c}log \\hat{y}_{c} \\qquad(10)\\]\nwhere \\(y_{c}\\) is the ground truth label determined by whether the point is inside the bounding box, \\(\\hat{y}_{c}\\) is the predicted probability, \\(F_c\\) is the frequency, and \\(a_c\\) is the weight of the \\(c\\)th class. \\(C\\) is the category number of the dataset. Yang et al. [3] pointed out that points closer to the target center are easier to regress accurate bounding boxes, have higher confidence, and achieve higher accuracy in local semantic prediction. Following the design of Zhang et al. [6], we assign weights to each point participating in the calculation of semantic loss, with points closer to the bounding box center having higher weights. This way, during training, these points will learn higher semantic scores.\n\\[L_{sample} = \\sum L_{s_i} \\cdot MASK_i\\qquad(11)\\]\n\\[MASK_i = \\sqrt[3]{\\frac{min f^*, b^*}{max f^*, b^*}} \\times \\frac{min l^*, r^*}{max l^*, r^*}} \\times \\frac{min u^*, d^*}{max u^*, d^*}}\\qquad(12)\\]\nWhere Mask is the weight for the centrality of sampled points. \\(f^*, b^*, l^*, r^*, u^*, d^*\\) represent the distances between the sampled points and the six faces of the target box. After training, points closer to the center of the target \\(y_i\\) will have higher values, and these points will be prioritized during inference.\nThe design of the remaining losses is similar to most point-based detectors. First is the regression loss of the vote points \\(L_{vote}\\), which determines the localization accuracy of```json\n the predicted boxes, we use L1 loss. Then is the bounding box classification prediction loss \\(L_{cls}\\), also calculated using WCE loss. Next is the regression loss of the vote point features for the bounding box sizes \\(L_{reg}\\), which is further decomposed into location, size, angle-bin, angle-res, and corner parts. To alleviate the MLC problem, we also add an IoU branch \\(L_{IoU}\\) to predict the IoU value between the predicted box regression of the vote points and the GT, also using L1 loss, the result of which is used to preliminarily correct the confidence (see Algorithm. 2). All the losses involved in the model are as follows:\n\\[L = L_{sample} + L_{vote} + L_{cls} + L_{reg} \\qquad(13)\\]\n\\[L_{reg} = L_{loc}+L_{size}+L_{angle-bin}+L_{angle-res}+L_{corner}+L_{IoU} \\qquad(14)\\]"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we will provide detailed experiments to demonstrate the efficiency and accuracy of SGCCNet. Specifically, we introduced the specific settings and implementation details of the experiments in Section IV-A. Then, an analysis of SGCCNet's detection performance on the KITTI dataset and a comparison with other state-of-the-art models is reported in Section IV-B. In Section IV-C, we analyze the inference effi-ciency of SGCCNet. Furthermore, various ablation experiments are conducted in Section IV-D to demonstrate the effectiveness of the SGCCNet design. Finally, in Section IV-E, we add our model components to other advanced models to demonstrate the effectiveness of our design."}, {"title": "A. Setting", "content": "Datasets. We validate our method on the KITTI dataset, which best demonstrates the advantages of point-based ap-proaches. The KITTI dataset uses a 64-beam LiDAR, resulting in relatively dense point cloud scenes where a sparse backbone can still learn ideal features. The manually annotated range in the KITTI dataset is small, leading to a significant reduction in the number of points in the scene compared to other datasets, allowing second-order complexity FPS-related algorithms to maintain fast inference speeds in this scenario. Currently, comparisons of point-based detectors are primarily focused on the KITTI dataset. The KITTI dataset is sponsored by the Karlsruhe Institute of Technology and the Toyota Technological Institute at Chicago for research in the field of autonomous driving. The widely-used dataset contains 7481 training samples with annotations in the camera field of vision and 7518 testing samples. Following the common protocol, we further divide the training samples into a training set (3,712 samples) and a validation set (3,769 samples). Additionally, the samples are divided into three difficulty levels: easy, moderate, and hard based on the occlusion level, visibility, and bounding box size. The moderate average precision is the official ranking metric for both 3D and BEV detection on the KITTI website. It is worth noting that SGCCNet, submitted to the KITTI website for comparison, is trained on 80% of the complete training set.\nEvaluation metrics. For the KITTI scene, we evaluate the performance of each class using both the 3D and BEV average precision (AP) metric. To ensure an objective comparison, we employed both the AP with 40 recall points (AP40) and the AP with 11 recall points (AP11). Consistent with the majority of state-of-the-art methods, we utilize Intersection over Union (IoU) thresholds of 0.7, 0.5, and 0.5 for Car, Pedestrian, and Cyclist, respectively.\nImplementation details. SGCCNet is trained for 80 epochs with a batch size of 8 on 2 Nvidia A40 GPUs. The first 70 epochs use saliency-guided point removal and new ground truth pooling for augmentation, while the last 10 epochs fine-tune using the initial ground truth pooling. The initial learning rate is set to 0.01, which is decayed by 0.1 at 35 and 45 epochs and updated with the one cycle policy. We use the Adam optimizer with \\(\\beta_1\\) = 0.9 and \\(\\beta_2\\) = 0.85 for optimization. The weight decay coefficient is set to 0.01, and the momentum coefficient is set to 0.9. Other data augmentation methods follow the default setting of Zhang et al [6]. In the WCE loss, \\(\\epsilon\\) is set to 0.001. In CCM, the initial object score threshold score_thres\u2081 = 0.01, and the final object score threshold score_thres\u2082 = 0.45. For potential missed detections, the missed sample incremental confidence \\(\\Delta c\\) is set to 0.2, the missed sample IoU threshold iou_thresmissed is set to 0.9, and the threshold of the number of missed sample neighbors neighbor_thresmissed is set to 10. All experiments are implemented using the OpenPCDet framework\nBenchmark detector. The most similar work to our SGC-CNet is IA-SSD, which provides the best balance between detection accuracy and inference speed in point-based pipelines to date. Its multiple metrics achieve state of the art on the KITTI dataset. The training details, model parameter configurations, and overall performance of SGCCNet are roughly similar to IA-SSD, and we will focus on comparing the performance of SGCCNet and IA-SSD."}, {"title": "B. Comparison with state-of-the-art (SOTA) methods", "content": "Note. SGCCNet is trained on multiple classes simultaneously. For models trained on single classes only, they are marked as MODEL\u2020 in the table. The IA-SSD model has sparked widespread research interest since its release, but many re-searchers have been unable to reproduce the performance reported in the original paper, and the authors have not responded to these concerns. We believe that the IA-SSD model may rely on the training environment, especially the versions of Pytorch and CUDA. To ensure fair comparisons, we trained and tested the model in our local environment. The reproduced model is marked as MODEL* in the table.\nPerformance on KITTI test set. We compared the per-formance of SGCCNet with other SOTA models on the KITTI test set as shown in Table. I. SGCCNet achieves an \\(AP^{3D}_{40}\\) of 80.82% on the moderate level for Car class objects, which is the best among all point-based detectors. In detail, SGCCNet outperforms IA-SSD by 0.9%, 0.69%, 0.54% in \\(AP^{3D}_{40}\\) on easy, moderate, and hard levels respectively, and by 0.78%, 0.55%, 0.92% in \\(AP^{BEV}_{40}\\). Compared to the reproduced IA-SSD\u2020, SGCCNet surpasses it by 2.1%, 2.35%, 2.05% in \\(AP^{3D}_{40}\\) and by 1.36%, 1.17%, 1.55% in \\(AP^{BEV}_{40}\\). Surprisingly, SGCCNet's performance even exceeds some structure-based models, with mAP3D surpassing Fast Point R-CNN by approximately 4.26% and Part-A2 by about 1.94%.\nPerformance on KITTI val set. We further provide the results of the KITTI validation set to better present the detection performance of our SGCCNet, as shown in Table. IV. SGCCNet remains the best performing point-based detector, especially in the Easy category, with \\(AP^{3D}_{40}\\) exceeding IA-SSD and 3DSSD by approximately 3.75% and 1.95%, respectively. We attribute this improvement to the confidence calibration mechanism, which allows the model to no longer rely solely on semantic scores. We also compared the performance of SGCCNet with other state-of-the-art point-based models on the Cyclist and Pedestrian classes, as shown in Table. III. Compared to the single-class trained 3DSSD\u2020, SGCCNet outperforms it by 1.03% and 7.48% in the moderate level of these two classes, respectively. Although its accuracy is slightly lower than 3DSSD in the Hard class, our model is trained end-to-end for multiple classes. Compared to other multi-class trained point-based detectors such as IA-SSD and DBQ-SSD, SGCCNet outperforms them by 3.1% and 2.08% in the Moderate level of Cyclist class, and by 0.24% and 2.61% in the Moderate level of Pedestrian class. These results demonstrate that SGCCNet also excels in detecting small objects.\nVisualization. We quantitatively demonstrated the superiority of our SGCCNet by explaining all the above indicators. Additionally, we qualitatively illustrated the results through visualization. In Figure 8, we show four scenes from the KITTI val set and compare the detection results of IA-SSD and SGCCNet. The yellow boxes represent ground truth (GT), while the green boxes represent the predicted results. We highlighted the areas where the predictions of the two detectors differ. In detail, in scenes (a) and (b), there is a missed detection by IA-SSD for each scene, which are high-quality targets with obvious Car features. We believe that the reason for the missed detections is the interference of features from other objects in the region, causing the geometry and semantic information of the targets to be overlooked. However, SGCCNet can accurately detect these targets. In scenes (c) and (d), IA-SSD detected numerous false positive targets because it only considers semantic scores as confidence criteria during post-processing. In contrast, SGCCNet adjusts confidence by considering the predictions of neighboring vote points, filtering out these false positive targets. Overall, in these scenes, SGCCNet can accurately detect each target in the GT, providing intuitive evidence of the superiority of our method."}, {"title": "C. Runtime Analysis", "content": "One major advantage of Point-based detectors is real-time detection. In this section, we will illustrate this point by compar-ing the metrics of GPU memory usage, single-frame inference time, and input data volume. Table. IV shows these metrics for several representative models. It is worth noting that since their original papers did not analyze this aspect of performance, and the metrics depend on the software and hardware environment, we retested them locally using the OpenPCD project to ensure a fair comparison. The configuration files and pre-trained weights for these models all come from the OpenPCD project. The hardware used for runtime testing was a single NVIDIA RTX 3090 GPU with an Intel i7-12700KF CPU@3.6GHz, and the software used python=3.9.0 and pytorch=2.1.0+cu118. From the results, SGCCNet's memory usage is only slightly higher than IA-SSD, although the single-frame inference time is 7ms longer, it can bring a performance gain of 1.68% \\(AP^{3D}\\) under the same input scale, which is undoubtedly worth it. Moreover, the 23ms single-frame inference time meets the real-time requirements of current LiDAR. PointPillars can achieve a similar inference time to SGCCNet in testing, but it occupies twice the memory and has a performance decrease of 3.52% \\(AP^{3D}\\)."}, {"title": "D. Ablation Study", "content": "In this section, we will analyze the roles of various model components in SGCCNet and the selection of hyperparameters. The models involved are all trained on the KITTI train set and tested on the val set.\nEffect of Saliency-Guide Data Augmentation (SGDA). The purpose of SGDA is to increase the diversity of training data at the feature level, reduce the model's reliance on saliency features, and improve its robustness to low-quality targets. As described in Section III-C, SGDA constructs an SGCCNet-elite model to perform classification tasks and obtain saliency scores for each target. We first need to demonstrate that the saliency scores obtained from classification are also applicable to detection tasks. The data set partitioning of the training set we created is the same as KITTI, with specific sample details shown in Table. V.\nFig. 11 shows the training and testing results of SGCCNet-elite on the classification dataset. It achieves good classification results, with an accuracy of 97.8% and an average accuracy of 88.42%. The classification accuracies for Car, Pedestrian, and Cyclist are 99.93%, 95.67%, and 69.67%, respectively. To apply the method for obtaining point-wise saliency score as described in Section III-C to SGCCNet-elite, we conducted a dropout experiment as shown in Fig. 12. We gradually removed salient points (SD) from 5% to 80% of the samples according to Algorithm. 1, and observed the changes in classification metrics, while also comparing with random dropout (RD). As shown in the Fig. 12, as the number of dropped points increases, both SD and RD methods decrease the classification performance of the model. However, SD is more destructive to the model, with a faster decline in performance. Even when only 5% of points are dropped, the classification accuracy for the Cyclist class decreased by 14.67%. This comparison demonstrates that the significance scores we obtained are suitable for classification tasks. In Fig. 13, we list some samples for saliency analysis, showing that for the same type of target, the distribution of saliency regions is similar. For example, Car targets are concentrated on the roof position, while Pedestrian and Cyclist targets are concentrated on the human body position. The model's dependence on saliency features reduces its robustness. We discard some saliency points, although still clearly retaining the basic features of these targets, the model misclassifies them as other categories. Our SGDA aims to alleviate this issue.\nSimilarly, we applied this dropout experiment on the targets in each scene of the KITTI val set for the detection task, and the experimental results are shown in Fig. 14. It can be seen that SD significantly decreases the detection performance of each class of targets, and the decrease rate is much higher than RD. We believe that such experimental results are sufficient to demonstrate that the saliency obtained from the classification task is also applicable to the detection task. By using this saliency-guided data augmentation method to discard salient region features, it differs from previous random augmentation methods and can truly enhance the diversity of training data at the feature level with a specific purpose. Table. VI quantitatively illustrates the performance gain brought by SGDA, which can lead to a 1.76%mAP3D improvement. The above results intuitively demonstrate the effectiveness of SGDA.\nEffects of Geometric Normalization Module (GNM) and Skip Connection Block (SCB). The original intention of GNM and SCB designs is to address the issues of Internal Covariate Shift and feature forgetting that may arise in models, respectively. The structures of both are often used in advanced classification models. In Table. VII, we analyze the performance changes of models when GNM and SCB are placed in different positions. Adding GNM and SCB in the second and third stages both bring significant benefits to the model, resulting in a 0.42% APimprovement for the Car class at the Easy level. However, when GNM and SCB are added to all three stages, the performance of the model may even decrease. After conducting multiple experiments to rule out the influence of random factors on the model, we found that it may be the GNM or SCB in the first stage that damaged the feature distribution. Through experiments (models C, D in Table. VII), we ultimately decided to use only GNM in the first stage and abandon SCB. The early point-wise features did not learn the essential features of the targets, and directly concatenating them with the features of the later stages may affect the predictions in the later stages of the model. As shown in Table. VI, the combination of GNM and SCB can bring a 0.46%mAPimprovement to the model.\nConfidence Correction Mechanism (CCM). The current single-stage point-based detector tends to cause misalignment between localization accuracy and classification confidence (MLC) issue by simply using semantic scores as the sole criterion for filtering predicted boxes during post-processing. SGCCNet proposes a Confidence Calibration Module (CCM) for point-based multi-class detectors to alleviate this problem. As shown in Algorithm 2, the purpose of this CCM is to correct the confidence of the current vote point by using the prediction information of neighboring vote points. Table VI quantitatively demonstrates the effectiveness of CCM, which can improve the model's mAPby 0.42%.\nMore intuitively, in Fig. 11 (b), we demonstrate the Precision-Recall (PR) curve of the model before and after adopting CCM. Although CCM itself does not possess learning capabilities, it can filter out false positive boxes and boxes with high semantic scores but inaccurate localization based on the predicted geometric information. As shown in the figure, the model can maintain a higher accuracy for a longer duration at higher recall, indicating a lower number of false positive targets compared to the scenario without CCM. We also illustrate the effectiveness of our consideration for missed targets in Table. VIII. It can be observed that after adding a certain confidence gain to targets with accurate localization but low semantic scores, the model exhibits significant improvements in various performance metrics. Fig. 15 visually demonstrates two such targets that possess distinct geometric information similar to the target, and they are ultimately detected by CCM."}, {"title": "E. Generalizability Analysis", "content": "The core components of SGCCNet are plug-and-play, and we also tested their versatility on other point-based detectors. We selected the 3DSSD and SASA detectors for analysis, as they have similar basic structures as shown in Fig. 16, with the only difference being the downsampling method. To maintain the basic structure of the models, we only analyzed the effects of SGDA and CCM. The experimental results, as shown in Table. IX, demonstrate significant improvements in the performance of both models after adding SGDA and CCM. 3DSSD shows an improvement of 0.34%, 1.09%, and 0.86% in AP113D on Easy, Moderate, and Hard levels, respectively. SASA shows improvements of 0.86%, 0.55%, and 0.37% respectively. These results directly indicate that SGCCNet is easy to be plugged into popular architectures."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a new single-stage point-based 3D object detector called SGCCNet. It incorporates a saliency-guided data augmentation method that enhances the diversity of training data at the feature level, reducing the model's reliance on salient features and improving its robustness to low-quality objects. It also includes a geometric normalization module and skip connection block to address the challenges of Internal Covariate Shift and feature forgetting. To tackle the misalignment between localization accuracy and classification confidence in single-stage detectors, SGCCNet introduces a confidence calibration mechanism suitable for multi-class point-based detectors. Experimental results demonstrate that SGCCNet is currently the best-performing point-based detector, with its model components playing a substantial role in enhancing model performance and exhibiting good transferability.\nLimitations and outlook. Like other point-based detectors, SGCCNet performs well only in LiDAR scenes with multi-beams, such as the KITTI dataset, but its overall performance is far inferior to structure-based detectors on datasets like NuScenes [59] and Waymo [60]. Firstly, this is because these scenes are more complex, making it difficult to form a suitable fixed configuration, and the sparser distribution of objects hampers feature learning for point-based detectors. Secondly, the quadratic complexity of the FPS algorithm requires a significant amount of time in such multi-point scenes. In future work, we will develop more efficient point-based backbones to address these two issues."}]}