{"title": "SGCCNet: Single-Stage 3D Object Detector With Saliency-Guided Data Augmentation and Confidence Correction Mechanism", "authors": ["Ao Liang", "Wenyu Chen", "Jian Fang", "Huaici Zhao"], "abstract": "The single-stage point-based 3D object detectors have attracted widespread research interest due to their advantages of lightweight and fast inference speed. However, they still face challenges such as inadequate learning of low-quality objects (ILQ) and misalignment between localization accuracy and classification confidence (MLC). In this paper, we propose SGCCNet to alleviate these two issues. For ILQ, SGCCNet adopts a Saliency-Guided Data Augmentation (SGDA) strategy to enhance the robustness of the model on low-quality objects by reducing its reliance on salient features. Specifically, We construct a classification task and then approximate the saliency scores of points by moving points towards the point cloud centroid in a differentiable process. During the training process, SGCCNet will be forced to learn from low saliency features through dropping points. Meanwhile, to avoid internal covariate shift and contextual features forgetting caused by dropping points, we add a geometric normalization module and skip connection block in each stage. For MLC, we design a Confidence Correction Mechanism (CCM) specifically for point-based multi-class detectors. This mechanism corrects the confidence of the current proposal by utilizing the predictions of other key points within the local region in the post-processing stage. Extensive experiments on the KITTI dataset demonstrate the generality and effectiveness of our SGCCNet. On the KITTI test set, SGCCNet achieves 80.82% for the metric of AP3D on the Moderate level, outperforming all other point-based detectors, surpassing IA-SSD and Fast Point R-CNN by 2.35% and 3.42%, respectively. Additionally, SGCCNet demonstrates excellent portability for other point-based detectors.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid development of intelligent transportation and robot technology, LiDAR-based 3D object detection has become a key technology for intelligent agents to acquire environmental information, attracting widespread research interest [1], [2]. Within related methods, single-stage point-based detectors can achieve a better balance between accuracy and inference efficiency, gaining increasing attention [3], [4], [5], [6].\nFor example, IA-SSD [7] and DBQ-SSD [7] can respectively achieve impressive 82FPS and 162FPS inference efficiency on the KITTI dataset [8] while maintaining ideal detection performance, surpassing other structured point cloud detectors comprehensively, greatly enhancing the application prospects of point-based detectors on devices with high real-time re- quirements. However, these type detectors still suffer from two prominent issues, namely Insufficient Learning of Low- Quality Objects (ILQ) and Misalignment between Localization Accuracy and Classification Confidence (MLC).\nFor ILQ, as limited training data cannot cover all possible feature distributions, especially for low-quality targets that appear less frequently, detectors will lack learning about them. And under the paradigm of repeated sampling and limited augmentation [9], the models will develop feature dependencies, giving better decision-making capabilities to prominent features while neglecting the learning of other features. In SPSNet, Liang et al. [10] obtained a way to represent point saliency through loss competition. When only discarding the top 10 most salient points, the model's average AP3D decreased by 8.61%, providing an intuitive explanation for this scenario. As shown in Fig. 1, we visualize the saliency scores of several objects in KITTI dataset. It can be seen that the distribution of saliency features is relatively fixed, with Car objects concentrated near the roof, Cyclist objects near the head and wheels, and Pedestrian objects near the head and feet. For low-quality objects, these strong saliency distribution areas are likely to be missing and sparse. If the model's feature learning capability is limited to saliency regions, it will greatly reduce its robustness.\nThe MLC problem is inherent in single-stage detectors, un- like two-stage detectors which can refine Rigions of Interesting (Rols) for a second time. The decision on target confidence and geometric information regression is determined by two separate network branches without any connection between them. This can mislead the Non Max Suppression (NMS) operation in post- processing. We list three typical examples in Fig. 2. The first is the detection of too many false positive targets, where the detection results overly rely on the model's semantic learning ability based solely on confidence. This is disadvantageous for detectors using point-based 3D backbones because point clouds are geometrically rich but textureless, leading to a high probability of objects with similar geometry to be detected as false positives. The second scenario is when the NMS selects target box positions and sizes that are not optimal. Although the selected points have the highest confidence, their IoU with GT is lower than other key points. In the third scenario, the accurate target position can be accurately predicted by the model, but it is not classified as a foreground category. All three scenarios directly reduce the model's performance metrics.\nIn this paper, we propose SGCCNet to alleviate the impact of the above two issues. Firstly, SGCCNet employs a saliency- guide data augmentation (SGDA) method to enrich the feature distribution of foreground objects in the training data. Specifically, We use a differentiable process of moving points towards the point cloud centroid to act as a point dropout process to approximate the saliency scores of points. During the learning process, we randomly drop a certain number of salient points of each object to create a new ground truth for GT sampling. For the MLC problem, we have designed a Confidence Correction Mechanism (CCM) in the post-processing stage. For proposals with low IoU, CCM reduces their confidence. For proposals that are considered background by the model but have a significant overlap with bounding boxes in the neighborhood, CCM increases their probability of being true positive ones.\nWe validated the superiority of SGCCNet on the KITTI dataset. Specifically, the proposed SGCCNet outperforms all previously published point-based approaches. Under the multi- class training scheme, SGCCNet achieved 80.82% AP3D on the Car target in the KITTI test set, surpassing prior methods that utilize structure-based backbones while also demonstrating higher efficiency. For the Pedestrian and Cyclist targets, compared to the latest point-based models, SGCCNet also showed improvements of 1% and 3.1% AP3D respectively. In summary, the key contributions of our work are as follows:\n\u2022 A new saliency-guided data augmentation method is proposed, which enhances the diversity of the training data distribution at the feature level, thus improving the robustness of the model to low-quality object detection.\n\u2022 A new point-based backbone that combines geometric normalization modules and skip connection blocks is proposed to alleviate the internal covariate shift problem and feature forgetting problem.\n\u2022 A confidence correction mechanism for post-processing is proposed to effectively address the misalignment between localization accuracy and classification confidence (MLC).\n\u2022 A high-efficiency and high-precision single-stage point- based 3D detector SGCCNet is proposed, and experimental results show that our method outperforms other similar methods significantly on the KITTI dataset.\nThe structure following this text is as follows: In Section II, we review the design principles of classic and state-of-the-art 3D object detectors, and list some methods to overcome ILQ and MLC problems. In Section III, we introduce the key components of SGCCNet, detail the implementation process of saliency-guide data augmentation, and finally introduce its end-to-end training loss. In Section IV, we demonstrate the superiority of SGCCNet through comprehensive comparative experiments and ablation studies. Finally, in Section V, we summarize this paper and provide prospects for future work."}, {"title": "II. RELATED WORK", "content": "As mentioned above, LiDAR-based 3D detectors can be clas- sified into grid-based, range-based, and point-based according to the data form before inputting to models. In this section, we will review the key ideas of classic and state-of-the-art models within each domain, and analyze in detail the measures currently taken by models to alleviate the issues of ILQ and MLC.\nA. 3D Detectors\nGrid-based. There are three major types of grid represen- tations: voxels, pillars, and BEV feature maps. Voxel-based models [11], [12] can preserve the spatial information of the original point cloud scene to the greatest extent based on the grid size. Pillar-based methods [13], [14] normalize the voxel space along the z-axis and significantly reducing the number of cells. The Bird's-eye view (BEV) feature map is a dense 2D representation, where each pixel corresponds to a specific region and encodes the points information in this region [15], [16], [17], [18], [19]. In recent years, researchers have found that BEV representation can naturally integrate with downstream tasks such as behavior decision-making and trajectory planning to form end-to-end intelligent systems, leading to a resurgence in research on perception models based on BEV [20], [21], [22], [23].\nRange-based (RV). Since range images are 2D representa- tions like RGB images, range-based 3D object detectors can naturally borrow the models in 2D object detection to handle range images [24], [25].\nBoth of the above methods convert point clouds into a structured representation, with the main advantage being the ability to generate more dense feature maps. Even the space not occupied by the original point cloud learns features, which is crucial for sparse and incomplete objects. However, these methods inevitably result in information loss during the process of structuring the point cloud, and their reliance on large parameter 3D backbones makes it challenging to strike a balance between inference efficiency and detection accuracy. In contrast, in LiDAR scenes with multi-beams and fewer points, the advantages of structured point cloud detectors become less pronounced, and point-based detectors begin to stand out.\nPoint-based. The current state-of-the-art point-based detec- tors [3], [4], [5], [6], [5], [10] follow the design paradigm of PointNet [26], PointNet++ [27], and PointMLP [28]. The point cloud is processed through a set abstraction (SA) layer consisting of multiple stages of downsampling modules, local feature learning modules, and feature aggregation modules to learn rich spatial and semantic information.\nIn terms of results, single-stage point-based detectors have achieved performance comparable to structure-based models in multi-beam LiDAR scenes, and their real-time inference capability and lightweight model architecture are desirable for many mobile embedded devices. The main performance bottlenecks are two factors: inadequate learning of low-quality targets (ILQ) and misalignment between localization accuracy and classification confidence (MLC).\nB. Dealing with ILQ\nAs mentioned above, limited training data cannot cover all possible feature distributions. For models that rely on salient features, they cannot actively explore information in non-significant parts of the target. Inspired by AlexNet [29], all current 3D detection models adopt some common data augmentation techniques to improve the feature distribution of training data, such as using geometric transformations to perturb both the overall scene and local features of the target. Choi et al. [30] randomly occluded parts of the foreground target to expand the training data by 2.5 times. Reuse et al. [31] conducted detailed controlled experiments to demonstrate the effectiveness of local target transformations in improving detector performance. Hu et al. [32] proposed pattern-aware GT sampling, enhancing data augmentation by subsampling objects based on LiDAR characteristics. Wang et al. [33] consistently pasted virtual high-quality targets into point clouds to enhance the model's perception of low-quality targets.\nThese methods aim to improve model robustness by enhanc- ing the diversity of the training data distribution. However, these augmentation processes are random, and data diversity does not necessarily imply feature diversity, leading to significant bottlenecks in benefits. This paper proposes a saliency-guided data augmentation method that, from the perspective of feature learning, removes salient features on which the model relies, forcing the model to actively explore information in low saliency regions and enhance the diversity of feature distribution in training samples from the source.\nC. Dealing with MLC\nHe et al. [34] proposed an auxiliary network to convert the features of key points belonging to the grid in the 3D backbone into point-wise representation to improve localization accuracy. They also introduced a part-sensitive warping operation to align the confidences to the predicted bounding boxes. CIA- SSD [35] combined the predicted IoU with the key point classification probability as the final confidence. Wang et al. [36] proposed a confidence-based filtering mechanism to filter out poorly localized proposals. Sheng et al. [37] proposed a new Rotation-Decoupled IoU (RDIoU) method to generate more effective optimization targets. In addition, models such as FVTP [38], DFDNet [39], and DDIGNet [40] have also adopted IoU prediction branches to improve the MLC problem.\nAlthough the above methods have achieved certain effects, they are mostly designed for anchor-based detection heads, and point-based detection heads have not been given much attention. Specifically, compared to anchor-based detection heads, the sparsity of keypoints obtained by point-based detection heads varies, leading to different numbers of predicted bounding boxes for each target, making these methods unsuitable for direct use in point-based detection heads. In this paper, a new confidence correction mechanism is proposed for point-based detection heads, taking into account the prediction situations of other proposals within the keypoint domain and the density of key points in the domain. On the one hand, it can filter out low-quality proposals, and on the other hand, it can explore potential high-quality proposals."}, {"title": "III. METHOD", "content": "A. Overview\nAs mentioned above, our goal is to improve the model's ability to learn from low-quality targets and enhance target localization accuracy by correcting confidence. To achieve this, we propose SGCCNet, a generic and unified single- stage point-based 3D object detector as illustrated in Fig. 3, which includes three core components, namely a saliency- guided data augmentation method, SA layers with geometric normalization modules and skip connection blocks, and a confidence correction mechanism.\nIn this section, we will detail the design principles and implementation process of SGCCNet. Section III-B introduces the mathematical symbols and their meanings to be used. In Section III-C, we present the method for obtaining point saliency in the saliency-guide data augmentation and the specific enhancement process. Section III-D describes the specific structure of the 3D backbone of SGCCNet, focusing on the design rationale of the geometric correction module and skip connection block. Section III-E proposes mechanisms for point-based detection heads and confidence correction. Finally, in Section III-F, we discuss the model loss in the end-to-end training process.\nB. Preliminary\nLet D be the training dataset used for experiments, consisting of N point cloud scenes Pi, {Pi \u2208 Rni\u00d7(3+C)|i = 1, 2, ..., N }. Here, ni represents the number of point clouds in the current scene, and C represents other features besides the spatial positions of the point clouds, such as intensity. The annotated ground-truth for each scene is denoted as Bi, {Bi \u2208 Rmix8|i = 1, 2, ..., N}, where mi is the number of ground-truth annota- tions in the scene. The eight-dimensional features include the center position of the bounding box {x,y,z}, dimensions {l, w, h}, and category c. The total number of annotated ground- truth in the dataset is A = \u2211 mi.\nN\nC. Saliency-Guided Data Augmentation (SGDA)\nFor LiDAR-based 3D object detection, limited training samples cannot cover all possible target feature distributions. In a training mode with limited data and repeated sampling, the model gradually relies on prominent features with common dis- tributions and uses them as decision criteria without exploring features in other less prominent regions of the target. Liang et al. [10] have demonstrated the model's dependency on prominent features in their work SPSNet. SPSNet assigns a Gaussian soft label to foreground points and regresses geometric information belonging to Dirac bounding boxes, obtaining the saliency score of points through stability and perturbation adversities. Ultimately, with the scenario of discarding only 10 foreground points, the model's average AP drops rapidly by 8.61%. This dependency on salient features is detrimental to the robustness of the model's detection.\nWe hope to develop a purposeful data augmentation approach, that is, from the perspective of features, by using saliency analysis to identify common features that the model relies on, removing them, and forcing the model to explore features in the target that were originally in low saliency regions. However, currently, there is not much attention given to point-wise saliency analysis methods for point-based 3D object detection. We have shifted our focus to developing more in-depth saliency analysis methods in point cloud classification tasks. Although these are two different tasks, we believe that the saliency scores obtained have commonalities for two reasons: 1) Considering the model structure. Point-based detectors throw vote points into the SA layer to determine the semantic information of local regions, a process that is identical to the prediction process of point cloud classification models. 2) Considering feature learning. Saliency scores are determined by the specific features of the model, and using similar structured models, the saliency features they focus on should be similar as well. We will demonstrate our viewpoint in the experimental section, and next we will introduce the specific data augmentation process.\nDataset Preparing. We conduct our experiments on the KITTI dataset D. Firstly, we extract all foreground objects using the annotated ground-truth {B\u00bf \u2208 Rmi\u00d78|i = 1, 2, ..., A} in D, and then construct a classification dataset similar to ModelNet40 [41] and ScanObjectNN [42]. Each point in the dataset contains 4-dimensional features, representing spatial position and intensity. KITTI categorizes foreground objects into Easy, Moderate, and Hard levels based on factors such as occlusion and the number of points within the bounding box. We discard samples with fewer points than a certain threshold, and train the classification model on all levels. It is worth noting that we only perform data augmentation on Easy and Moderate levels, while retaining all samples in the Hard level.\nClassification Model Preparing. We have streamlined the structure of SGCCNet and constructed an elite model SGCCNet- elite for classification tasks as shown in Fig. 4 (a). This model consists of only one SA layer for feature learning, and the training samples do not undergo downsampling in the SA layer. Finally, the point-wise features of each point are pooled using max pooling to form the overall feature of the sample, which is then fed into a Fully Connected (FC) Layer for classification.\nSaliency Analysis. Let Eo(p\u00b2) be a classification model, where p\u00b2 := (p1, p2, ..., p.) is a training sample, and p\u2208 Rki\u00d74. Lc() is the classification loss function.\nLc = 1/A * \u2211(ic=1 Yic log(Yic)) (1)\nAmong them, Yic is the label, with a value of 1 if sample p\u00b2 belongs to class c, otherwise 0, and Yic is the predicted probability. The most intuitive way to determine point signifi- cance is to gradually discard points and observe the change in loss Le. However, due to the large number of points and the fact that the contribution of each point to the whole is not isolated, this method is time-consuming and inaccurate. Inspired by Zheng et al. [43], we transform the process of discarding points into a differentiable process of moving points to calculate the significance score of each point. Specifically, in the classification task, we normalize the feature information of each sample.\npi = pi - (1/k * \u2211(4mi/ c=1 Pjc)) (2)\nThe center of the sample is now moved to the origin. Based on the mechanism of LiDAR scanning imaging, we have reason to believe that the points at the origin have little contribution to the classification of the sample. The impact of moving points from other positions in the sample to the origin on model decisions is almost the same as discarding points. Since points are not angle invariant, there are difficulties in measuring gradients in Euclidean space, so we consider point shifting in the Spherical Coordinate System.\nIn the Spherical Coordinate System, a point pj is represented as (r, \u03c8j1, \u03c8j2) with a as the sphere core, r is distance of pj to a, and \u03c8j1, \u03c8j2 are the two angles of a point relative to a. After shifting pj in the direction of r towards sphere core a by \u03b4r, the change in model loss is (\u2202L/\u2202r) \u03b4r, where rj = \u2211(c=1 (pj - a))2, as:\np\u2032j1 = r cos\u03c8\u2032j1 sin\u03c8\u2032j2 = r cos\u03c8j1 sin\u03c8j2 \np\u2032j2 = rcos \u03c8\u2032j2 sin\u03c8\u2032j1 = rcos \u03c8j2 sin\u03c8j1\np\u2032j3 = rsin \u03c8\u2032j3 = rsin \u03c8j3 (3)\nwhere pj = {Pjic}c=1,2,3, a = {ac}c=1,2,3 represent the 3D coordinates value of pj and a. So:\ndL/dr = (\u2202L/\u2202 pjic) (pjic - ac)/r\njc\nSo that after shifting p\u2032j, the change in model loss is (\u2202L/\u2202r\u2032)\u03b4r. In practical computation, we also set the fourth dimension intensity value of the central position point to 0, and add it as a spatial information to the gradient calculation.\nDroppint Points. For a training sample pj, after obtaining the saliency scores {s\u2032j}j=1,2,..., k} for each point, we delete high saliency points in the sample according to a preset ratio. The number of points to be deleted, denoted as d, is linearly proportional to the number of points in the sample, k, i.e., d = [\u03b1ki + \u03b2]. To ensure the accuracy of saliency ranking, the d points are not deleted all at once, but are deleted at fixed intervals as described in Algorithm. 1. After discarding salient features, the sample is restored to its original space and used for training alongside the original ground truth in the GT sampling process. Fig. 5 illustrates the changes in a set of scenarios before and after point removal. The entire saliency-guided data augmentation process is shown in Fig. 6.\nD. 3D Backbone\nSimilar to previous state-of-the-art point-based detectors, SSGCNet also utilizes a PointNet++-style 3D backbone to extract features from point clouds. This backbone incorporates multi-stage downsampling, multi-scale feature learning, and local feature aggregation processes to extract point-wise features with rich semantic and geometric information.\nIt is worth noting that the downsampling scheme of SGCC- Net is designed based on IA-SSD. Due to the limited accuracy of semantic information learning by the point-based backbone, SGCCNet uses the Farthest Point Sampling (FPS) algorithm in the first two stages, and in the last two stages, sampling is based on the predicted point-wise foreground probability. The former ensures global coverage of sampling points, while the latter ensures a high recall rate of foreground points, reducing target information loss. In addition, SGCCNet also includes a geometric normalization module and skip connection block.\nGeometric normalization module (GNM). We found that the PointNet structure ignores the Internal Covariate Shift (ICS) problem caused by the irregular and sparse geometric properties of point clouds during the process of aggregating local features. Specifically, in each learning stage, the same MLP layer needs to deal with a large number of regions with different geometric information simultaneously, and it is difficult for a simple MLP to achieve stable convergence speed and generalization ability from such a diverse feature distribution. Therefore, we propose to alleviate this problem by performing geometric normalization on the locally aggregated features, treating each ball used for learning local features as a batch, and applying 'Batch Normalization (BN)' operation before inputting into the MLP layer. The structure of GNM is shown in Fig. 7. In detail, we first move the distribution of local features within each ball to a more normalized space with the ball center as the mean, and then increase the diversity of the feature distribution through learnable weight and offset parameters of the same dimension as the features. Finally, to prevent information loss, we concat the normalized features with the original features and input them into the MLP layer for learning local features.\nThe specific structure of the geometric normalization module is shown in Fig. 7. Let {fi,j}j=1,2,...,k \u2208 Rk\u00d7d be the local features of the sampling point pi, where k is the number of neighbors in its local region, and d is the feature dimension of the sampling point and its neighbor points. We normalize the features of neighbor points in the local region through the following equation:\n{fi,j}\u2032 = ({fi,j} - fi)/\u03c3 + \u03b2 (6)\nwhere \u03c3 = 1/n * \u2211 \u2211 (fij - fi)2. Similar to BN layer, where \u03b1 \u2208 Rd and B\u2208 Rd are learnable parameters, and \u2299 indicates Hadamard product. \u03f5 = 1e\u22125 is a small number for numerical stability. Note that \u03c3 is a scalar that describes the feature deviation across all local groups and channels. By doing so, we transform the points via a normalization operation while maintaining original geometric properties. The features after geometric normalization will be concatenated with the original features for subsequent learning.\nSkip Connection Block (SCB). When determining the appropriate number of training epochs for SGCCNet, we found that some high-quality targets were detected with high confidence early in training, but as training progressed, the confidence decreased below the threshold, undoubtedly reducing the model's performance. After analysis, we believe there may be three reasons for this: 1) The sparsity of the targets greatly increases after continuous downsampling, causing sparse sampling points to lose the ability to perceive the overall geometric information of the targets. 2) As the ball query radius increases, noise and features of other targets in the local area are introduced while expanding the model's receptive field, leading to the loss of key information of the corresponding targets after pooling layers. 3) For some targets, shallow features have a better ability to represent target information. In testing classification tasks, the fact that models with extremely small parameter amounts can achieve classification levels far higher than detection models directly proves this point. We refer to this type of problem as feature forgetting and have designed a skip connection block to alleviate this issue. The structure of SCB is shown in Fig. 8.\nThe SCB alleviates the problem of feature forgetting by enabling interactions between key point features of adjacent SA layers in the 3D backbone, as shown in Fig. 8. Let the feature of the l-th SA layer's sampled points be {F\u2032i }i=1,..., k \u2208 Rkxd\u0131, where k is the number of sampled points at the current stage, and d is the feature dimension. Based on the basic structure of PointNet++, it is known that the sampled points at the current stage are also sampled points from the previous stage.\nTherefore, let the features of the sampled points in the l-th SA layer at the previous stage be {F(l\u22121)}i=1,...,k \u2208 Rkxd\u0131\u22121. The learning process of SCB is as follows:\ngi = \u03a6pos(\u03a6pre(F(l\u22121)i) + F\u2032i ) (8)\nThe \u03a6pre is a module composed of residual networks, which elevates the dimension of features F(l\u22121) from d\u0131\u22121 to d\u0131 from the previous stage, and \u03a6pos further interacts with the features. The design of SCB is inspired by He et al.'s work [34], but we naturally leverage the advantages of point-based backbone hierarchical downsampling, without any feature projection or transformation processes. As shown in Fig. 8, with the addition of SCB, shallow features and deep features of the same sampling point, as well as local small region features and large region features, interact with each other. Even in the subsequent downsampling process where the sampling points become sparser or noise is introduced, valuable information from earlier stages is still retained.\nE. Confidence Correction Mechanism (CCM)\nThe previous advanced single-stage point-based detectors use two completely independent branch networks in the detection head to respectively regress the confidence of predicted boxes and geometric information. During post-processing, the NMS operator is guided by confidence scores, retaining predicted boxes with the highest confidence scores in local regions and at any position with confidence scores higher than a threshold. This is disadvantageous for single-stage point-based detectors, as high confidence scores do not necessarily represent high-quality localization accuracy, and the rich geometric information of point clouds, coupled with the lack of semantic information, makes it difficult for the point-based backbone to learn precise confidence, leading to a large number of missed detections and false positive detections. Researchers have referred to this issue as misalignment between localization accuracy and classification confidence.\nAs mentioned in Section 2, the method with IoU prediction branch brings hope to alleviate the MLC problem of single-stage detectors. These methods predict the confidence of proposals while also predicting their IoU values with GT boxes. The final confidence sent to NMS is a joint indicator of both. SGCCNet also adopts this method to improve model performance. Furthermore, we not only consider the prediction of a single voting point but also incorporate the predictions of surrounding voting points into the reference range for confidence correction. The work closest to our idea is NIV-SSD [44], but it only applies to single-class detection with anchor- based detection heads, and the issue of uneven prediction of each target voting point in point-based detectors is not considered.\nFor a stable training SGCCNet, the key points output by the 3D backbone after voting will move to positions near the center of the target box. As shown in Fig. 9, for the same target, different voting points have a certain degree of clustering in terms of their positions and regression predicted boxes. Therefore, we use the predictions of neighboring voting points to correct the confidence of the current voting point. We believe that voting points in local regions with more neighbors having high IoU values are more accurate, while those with unreliable confidence. The specific confidence correction mechanism works as shown in Algorithm 2. Let the parameters of the predicted box regressed by the current voting point be bi = (x,y,z,w,l,h,r), we calculate its IoU value with the predicted boxes of other voting points B = {bj}j=1,...,k \u2208 Rk\u00d77, and consider the predicted boxes with IoU values greater than a certain threshold as neighbors of the current voting point. Finally, the average IoU value between the voting point and its neighboring voting points' predicted boxes will be used as a weight to calibrate the confidence of the current voting point. In addition, CCM sets strict conditions to filter out boxes with high localization accuracy but confidence lower than a threshold. Before the NMS operation, we give them a certain increment to increase their probability of being considered as foreground. We will analyze the advantages of CCM in detail during the experimental phase.\nF. End-to-End Training\nSGCCNet is a one-stage detector, using an end-to-end multi- task training fashion. Firstly, there is the semantic loss Ls. The last two SA layers in SGCCNet use foreground sampling, so it is necessary to obtain the semantic information of the sampled points from the previous stage. Due to the highly unbalanced number of different categories of objects in the KITTI dataset, such as the number of Car objects being five times that of Pedestrian objects and ten times that of Cyclist objects, and the point number of Car objects is usually more than the other two, we adopt weighted cross entropy (WCE) loss to manually emphasize rare categories. The WCE loss can be formulated as:\nLc = \u2211 acyclog \u0177c (10)\nwhere yc is the ground truth label determined by whether the point is inside the bounding box, \u0177c is the predicted probability, Fe is the frequency, and ac is the weight of the cth class. C is the category number of the dataset. Yang et al. [3] pointed out that points closer to the target center are easier to regress accurate bounding boxes, have higher confidence, and achieve higher accuracy in local semantic prediction. Following the design of Zhang et al. [6], we assign weights to each point participating in the calculation of semantic loss, with points closer to the bounding box center having higher weights. This way, during training, these points will learn higher semantic scores.\nLsample = \u2211Ls; MASKi (11)\nMaski = (min f\u2217,b\u2217 / max f\u2217, b\u2217 )\u2217(min l\u2217,r\u2217 / max l\u2217, r\u2217 )\u2217(min u\u2217,d\u2217 / max u\u2217, d\u2217 ) (12)\nWhere Maski is the weight for the centrality of sampled points. f*, b*, l*, r*, u*, d* represent the distances between the sampled points and the six faces of the target box. After training, points closer to the center of the target y\u0177 will have higher values, and these points will be prioritized during inference.\nThe design of the remaining losses is similar to most point-based detectors. First is the regression loss of the vote points Lvote, which determines the localization accuracy of the predicted boxes, we use L1 loss. Then is the bounding box classification prediction loss Lcls, also calculated using WCE loss. Next is the regression loss of the vote point features for the bounding box sizes Lreg, which is further decomposed into location, size, angle-bin, angle-res, and corner parts. To alleviate the MLC problem, we also add an IoU branch LIOU to predict the IoU value between the predicted box regression of the vote points and the GT, also using L1 loss, the result of which is used to preliminarily correct the confidence (see Algorithm. 2). All the losses involved in the model are as follows:\nL = Lsample + Lvote + Lcls + Lreg (13)\nLreg = Lloc+Lsize+Langle-bin+Langle-res+Lcorner+LIoU (14)"}, {"title": "IV. EXPERIMENTS", "content": "In this section", "levels": "easy, moderate, and hard based on the occlusion level, visibility, and bounding box size. The moderate average precision is the official ranking metric for both 3D and BEV detection on the KITTI website. It is worth noting that SGCCNet, submitted to the KITTI website for comparison, is trained on 80% of the complete training set.\nEvaluation metrics. For the KITTI scene, we evaluate the performance of each class using both the 3D and BEV average precision (AP) metric. To ensure an objective comparison, we employed both the AP with 40 recall points (AP40) and the AP with 11 recall points (AP11). Consistent with the majority of state-of-the-art methods, we utilize Intersection over Union (IoU) thresholds of 0.7, 0.5, and 0.5 for Car, Pedestrian, and Cyclist, respectively.\nImplementation details. SGCCNet is trained for 80 epochs with a batch size of 8 on 2 Nvidia A40 GPUs. The first 70 epochs use saliency-guided point removal and new ground truth pooling for augmentation, while the last 10 epochs fine-tune using the initial ground truth pooling. The initial learning rate is set to 0.01, which is decayed by 0.1 at 35 and"}]}