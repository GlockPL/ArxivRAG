{"title": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review", "authors": ["Rock Yuren Pang", "Hope Schroeder", "Kynnedy Simone Smith", "Solon Barocas", "Ziang Xiao", "Emily Tseng", "Danielle Bragg"], "abstract": "Large language models (LLMs) have been positioned to revolutionize HCI, by reshaping not only the interfaces, design patterns, and sociotechnical systems that we study, but also the research practices we use. To-date, however, there has been little understanding of LLMs' uptake in HCI. We address this gap via a systematic literature review of 153 CHI papers from 2020-24 that engage with LLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in HCI projects; (3) contribution types; and (4) acknowledged limitations and risks. We find LLM work in 10 diverse domains, primarily via empirical and artifact contributions. Authors use LLMs in five distinct roles, including as research tools or simulated users. Still, authors often raise validity and reproducibility concerns, and overwhelmingly study closed models. We outline opportunities to improve HCI research with and on LLMs, and provide guiding questions for researchers to consider the validity and appropriateness of LLM-related work.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are poised to transform the landscape of Human-Computer Interaction (HCI) research. Already, researchers have been using LLMs across the HCI research pipeline, from ideation and system development to data analysis and paper-writing [76]. Past work has shown rapid growth in the raw count of LLM-focused paper preprints, especially in HCI topics [117]. The explosion of LLM-related research has also led to rising discourse in HCI on the opportunities and challenges of LLM usage, including interview and survey studies with researchers to understand their practices [76], and workshops [4, 130] and social media commentary [67] in which scholars debate how the field ought to respond. The surge in LLM-related papers and discussions indicates a growing need to support scholars in understanding the potential and pitfalls of LLMs in HCI.\nSuch inquiry is consequential not only for HCI, but also for the broader landscape of computing research. On one hand, scholars in natural language processing (NLP) and machine learning (ML) increasingly look to incorporate human evaluation in LLM architectures, via techniques like reinforcement learning for human feedback (RLHF) that draw upon HCI methodologies [61, 91, 99, 171]. On the other hand, researchers across various communities, such as science and technology studies (STS), computer-supported cooperative work (CSCW), and fairness, accountability, and transparency (FAccT) have called for reflection on the potential negative impacts [59, 143, 145], including a rising chorus of scholars exploring the societal implications of LLM development and the need for responsible AI practices [2, 35, 162]. As various research communities increasingly pursue human-centered methods and questions, there emerges an urgent need for we as the HCI community to scrutinize our own field, and to develop standards for researchers using LLMs and asking HCI-oriented questions. This work is motivated by the"}, {"title": "2 Related Work", "content": "HCI has a rich tradition of using systematic literature reviews to identify patterns, trends, and limitations of a research area [146]. Such reviews provide conceptual frameworks for shared understanding across the field. Many prior works qualitatively analyzed their paper samples to surface high-level themes. For example, Mack et al. [111] examined 836 accessibility papers over 26 years, coding for common contribution types, communities of focus, and methods. Stefanidi et al. [146] annotated 189 HCI literature surveys 1982-2022 to explain current contributions and topics within HCI. Similarly, Dell and Kumar [33] manually reviewed 259 HCI4D publications to provide an overview of the space. Caine [15] synthesized standards for sample sizes at CHI by manually extracting data from each CHI2014 manuscript. Quantitative methods have also been employed to provide broader perspectives on HCI research trends. Liu et al. [107] used hierarchical clustering, strategic analysis, and network analysis to map the evolution of major themes in HCI. Cao et al. [16] analyzed patent citations to study the relationship between HCI research and practice.\nOur work builds on this literature to understand LLMs' impact on HCI. We chose a qualitative approach to provide a deep formative understanding of this rapidly evolving landscape and its impact, not only for HCI researchers, reviewers, and students, but also for researchers in different communities (e.g., AI/NLP) who may be interested in the current state of LLM-ification in HCI, as well as practitioners looking for research-grade guidance on this rapidly evolving space."}, {"title": "2.2 Literature Reviews of LLM Papers", "content": "Outside of HCI, many fields across computing and social science have used literature reviews to study LLMs' impact on their areas, including reviews of the models, the technical foci, and the societal implications of LLMs. Many of these reviews survey technical advancements, e.g., Zhao et al. [183] survey methods for training and evaluating core models, Gao et al. [44] review the state-of-the-art in retrieval-augmented generation, and Guo et al. [51] review multi-agent approaches. Other efforts have studied the risks posed by LLMs: Weidinger et al. [164] taxonomized the harms possible, including discrimination, information hazards, malicious uses, and environmental and economic harms.\nResearch has also surveyed trends in how LLMs are being applied in specific disciplines. Movva et al. [117] collected and analyzed 16,979 LLM-related papers posted to arXiv from 2018 to 2023 to understand trends in LLM research topics. Notably, they found that society-facing and HCI topics are the two fastest-growing, further showing the urgency of our focus on how the HCI community considers LLM use and implications. Movva et al. [117] also found that industry publishes an outsize fraction of top-cited research, but also that industry papers tend to be less open about their models, datasets, and methods. Similarly, Fan et al. [37] used BERTopic to identify patterns in LLM research 2017-2023. Researchers have additionally employed topic modeling to study LLM usage in fields such as medicine [7] and education [102]. A recent study shows that papers in behavioral and social science disproportionately favor closed models, despite the availability of powerful, more reproducible open alternatives [169].\nOur work focuses on CHI papers, to explore where authors applied LLMs in HCI research, and how authors leveraged them to"}, {"title": "2.3 How LLMs Can and Should Change Research", "content": "There has been substantial debate across the scientific community on how much LLMs can and should transform research [12]. Many papers argue that LLMs are poised to be incorporated in all disciplines, but call for consideration of their limitations. For instance, Aubin Le Qu\u00e9r\u00e9 et al. [4]'s CHI\u201924 workshop discussed opportunities and responsible integration of LLMs into data work. In computational social science, researchers found that LLMs achieved fair agreement levels with humans on labeling tasks [185]. Researchers have also considered whether LLMs can or should influence academic writing [77]. A survey of 950,965 papers found a significant increase in the use of LLMs in writing scientific papers, especially in Computer Science [97]. However, many argue that researchers should \"avoid overreliance on LLMs and to foster practices of responsible science [12].\"\nOur work extends the discussion on how LLMs are changing and should change research by focusing on the CHI community. We identify the unique roles that LLMs play in HCI research, analyze common limitations reported by authors, and advocate for proactive consideration of these limitations to ensure research rigor."}, {"title": "3 Methods", "content": "To understand the LLM-ificiation of CHI papers, we performed a literature review of CHI proceedings from 2020-2024. In our study, we focus on generative LLMs, rather than encoder-only models such as BERT or ROBERTa. Via iterative human coding, we assessed (1) the types of contributions common in LLM-focused HCI scholarship, (2) the roles that LLMs are playing in research projects; and (3) the limitations that researchers are disclosing in their papers."}, {"title": "3.1 Data", "content": "We first gathered the full-text proceedings of CHI from 2020-2024, which at the time of writing represented the most recent five years of cutting-edge HCI research. In 2020, OpenAI's GPT-3 was released, marking a leap in language models' predictive capabilities. LLMs then became more accessible to researchers through APIs and open-sourced models.\u00b9 We chose CHI for two reasons. First, CHI is the flagship international conference on HCI. All papers undergo rigorous peer review, and publications have significant impact on HCI research generally. Similar prior literature reviews chose CHI as a representative sample to identify trends in HCI [8, 103]. Second, CHI papers span a wide range of application areas and methodologies, (e.g., CHI 2024 had 16 subcommittees) giving this work broad representation. We acknowledge that ACM SIGCHI sponsors 26 HCI conferences, \u00b2 which have more focused scopes. Our sample\n\u00b9https://techcrunch.com/2020/06/11/openai-makes-an-all-purpose-api-for-its-text-based-ai-capabilities/\n\u00b2https://sigchi.org/conferences/"}, {"title": "3.2 Analysis", "content": "To analyze the 153 papers, we applied an iterative process to develop a codebook. The initial codebook included deductive codes based on our four research questions. For two of our research questions, we used existing taxonomies to seed our codebooks: on the contribution type, we used Wobbrock and Kientz [167]'s taxonomy of research contributions in HCI, and on the application domains for each paper, we used a taxonomy from Stefanidi et al. [146]. For the rest of the questions-on the roles of the LLMs in each paper, the limitations and risks of the research-we generated initial codebooks during the iterative open coding process.\nWe conducted four iterations of independently applying and updating the codebook, using a randomly selected set of 10 papers for each iteration. After each set, the research team came together to refine or merge existing codes, add new codes, and resolve disagreement through consensus. Throughout, we computed interrater reliability (IRR) using Krippendorff's alpha to guide our discussions.3 The final alpha values are aContribution Types = 0.866, Application Domains = 0.849, ALLM roles = 0.773, Limitations & Risks = 0.8874. All values are comparable to prior work [89, 111]. This process led to a codebook of 51 low-level codes (see Supplementary Materials for the process and the codebook). Finally, the remaining papers-those that had not been used for codebook development-were split into three sets and coded independently by the three researchers who all participated in the codebook development. During this step, the authors met regularly to discuss any emergent concerns, and disagreements were resolved through consensus.\n3Note that Fleiss' kappa can also be applied to the IRR analysis of the complete nominal data in our case. We chose Krippendorff's alpha in line with prior literature review [111].\n4The \u03b1 over our initial 29 low-level code is 0.633"}, {"title": "3.3 Research Positionality", "content": "We acknowledge that our academic and professional backgrounds have shaped our perspectives on this topic. All authors had experience using LLMs directly or studying users' perceptions of LLM-powered systems, and had experience working in responsible computing. The authors' expertise covers fields including HCI, NLP, computational social science, accessibility, machine learning, fairness, sociotechnical systems, and usable security and privacy.\nCollectively, we are US-based researchers at three different R1 universities and one US-based research institute."}, {"title": "4 Results", "content": "Our analysis reveals where LLMs have been applied at CHI, how researchers have leveraged these models, and what contributions they made to the field of HCI. In parallel, we taxonomize the common limitations and risks articulated by authors (see Table 1)."}, {"title": "4.1 Application Domains", "content": "We found 10 diverse domains in which HCI researchers have applied LLMs (Table 1). We elaborate each in this section.\nCommunication & Writing (22.88%, N=35): This domain emerges as the most-studied area, spanning both specific writing tasks and Al-mediated communication (AIMC) [56], in which intelligent agents modify, augment, or generate messages to achieve communication goals. Many of these works imagine writers as the target LLM user, in tasks from personal diaries [79] and email composition [14] to storytelling [25], screenplay creation [115], and general creative writing [18, 160]. For instance, researchers have examined writers' attitudes toward collaborating with LLMs [95], including how writers choose prompting strategies [31] and users' perception of AIMC support in a variety of writing tasks, such as idea generation, translation, and proofreading [42]. Researchers have also examined how LLMs might introduce implicit bias to the writing process [41, 69].\nAugmenting Capabilities (16.99%, N=26): This domain includes papers that develop technologies to enhance human performance and productivity by altering how we engage with technology and information. Some attempt to bridge the physical and digital worlds in scenarios such as video conferencing [105] and mixed reality [32]. Many also study the future of work and productivity. Fok et al. [40] leverages LLMs to support sensemaking on business document collections, while Kobiella et al. [83] studied how ChatGPT usage affects professionals' perceptions of workplace productivity. Several papers also developed tools to enhance productivity in academic research, building new approaches for sensemaking of literature [92] and research idea generation [159].\nEducation (14.38%, N=22): This domain explores the potential of LLMs to enhance learning experiences for students and improve pedagogical methods for educators. For students, research examined learners' existing interactions with LLMs, including Belghith et al. [9]'s investigation of middle schoolers' approaches to and conceptions of ChatGPT. Several works explored using LLMs as learning aids in specific subject areas, such as math [179], vocabulary acquisition [88], and programming [72]. For educators, studies examined the LLMs' integration into teaching. Han et al. [54] found that teachers are excited about potential benefits, namely LLMs' ability to generate teaching materials and provide personalized feedback to students; however, teachers and parents are both concerned about their impact on students' agency in learning, and potential exposure to bias and misinformation. Researchers have also designed LLM-based tools to assist teachers in domains such as cyberbullying education [60] and environmental science instruction [22].\nResponsible Computing (12.42%, N=19): This explores ethical and societal implications of computing systems, particularly in high-stakes domains and for vulnerable populations. It touches on issues"}, {"title": "4.2 Contribution Types", "content": "The above application domains were primarily addressed through (1) empirical contributions (98.70%, N=151)-often to understand a population's view toward or use of LLMs or specific LLM-powered tools-and (2) artifact contributions (61.44%, N=94), which involve building a tool. These two contribution types frequently occur in combination, in studies where authors first build an artifact and then empirically test it with users. For artifact contributions, we observed that LLM-powered systems have a wide range of fidelity levels, from fully open-sourced systems with GitHub repositories to simple wireframes. The dominance of LLMs in these systems also varied, with some systems using LLMs throughout the entire pipeline and others using them only for processing textual data. We applied the code \"artifact contribution\u201d to a paper when authors claimed that LLMs are (or would be) a part of the system. The high frequency of artifact contribution (61.44% in our sample in contrast to 24.50% at CHI overall [167]) may indicate that LLMs might have lowered the barrier to prototype research artifact of high quality, a point we unpack further in 5.1.3.\nThe remaining five contribution types occurred less frequently, with one survey contribution and no opinion contributions. Distinguishing between methodological and artifact contributions can be challenging, as some methods are embedded in a system. Per [167], we used methodological contribution to refer to research method contributions in HCI. Methods for creating multimodal mobile applications, for example, were not included. Overall, we found 16 (10.46%) methodological contributions, such as LLM-augmented methods to enhance UX evaluation [84], generate synthetic user data [159], and provide metrics to measure creativity in LLMs [18]. We found 8 theoretical contributions (5.23%), ranging from a framework for collaborative group-AI brain-writing [139], a conceptual framework to bridge the gulf of envisioning [148], and a design space for intelligent writing assistants [89] (also a metareview contribution). Dataset contributions were less common (N=6, 4.0%).\nIn the LLM roles section, several papers used LLMs to generate synthetic datasets, which may lower the barrier to creating large, diverse datasets for thorough evaluation, yet curating benchmark"}, {"title": "4.3 LLM Roles", "content": "We identified five roles that LLMs play in HCI research (Figure 3). While Figure 3 may not fit every project given the interdisciplinary nature of our field, it reflects our sample, which primarily offers empirical contributions.\nLLMs as system engines (62.74%, N=96): In this role, LLMs function as core elements within systems, prototypes, algorithms, and programming frameworks. One way LLMs can be used in systems is to generate content, e.g., ideas, code, and conversations. For example, Farsight used LLMs to generate ideas to identify potential harms of AI applications [162], and GenLine used LaMDA to generate code from users' natural language [71]. MindTalker, a GPT-4 conversational agent, supports people with early-stage dementia by reducing loneliness [172]. On the other hand, LLMs may be used to process information and extract insights, e.g., by retrieving or summarizing from large, unstructured datasets. For instance, PaperWeaver deduces users' research interests from their paper collection on Semantic Scholar [92], while Memoro interprets user needs from the users' conversation history [186]. Visual Captions employed a fine-tuned LLM to predict user intent using the sentences in a video conferencing call [105]. Systems integrate LLMs at different levels. Some systems' main functions rely on a carefully-designed system prompt, often in a form instructions to a conversational agent [153], while others used LLMs as one [57] or more [86] step(s) in a complex pipeline. On another axis, the LLM-powered tools can range from a fully-functioned open-source system [162] to design prototypes that elicit important empirical insights [176].\nLLMs as research tools (9.15%, N=15): We found several authors used LLMs to perform tasks traditionally executed by researchers or research assistants, including data collection, analysis, or writing. For example, Choksi et al. [24] applied LLMs to conduct qualitative coding on social media posts on NextDoor. They first developed a codebook, manually labeled 340 posts, and then adjusted the codebook prompts before using GPT-4 to tag the rest of the posts from the sample. Such LLM-augmented workflows were often also claimed as a methodological contribution, or packaged as a system that other researchers could use. For example, Wang et al. [161] introduced a multi-step human-LLM collaborative method for qualitative coding. In this process, LLMs generate labels and explanations, a verifier model assesses the quality, and humans re-annotate the subset of low-quality labels. Ding et al. [34] contributed a LLM-based method to identify critical online discussion patterns at scale to inform national health outcomes. Similarly, Lam et al. [86] proposed LLooM, a LLM-powered Python package to iteratively synethesize concepts over a sample of text.\nAnother thread involved using LLMs to generate data for research purposes. For example, Sun et al. [149] used GPT-2 to generate a corpus of 96,600 artificial greeting messages to study gender bias in greeting card messages and facilitate future research on this topic. Ko et al. [82] introduced a LLM-based framework that takes Vega-Lite specification as input to generate diverse natural language datasets, such as captions, utterances, and questions about the visualization. Feng et al. [39] uses LLMs to automatically mine UI"}, {"title": "4.4 Limitations", "content": "This section covers four top-level codes and 22 main sub-level codes for the limitations and risks discussed in our sample. Coding the limitations is not a trivial task, as not every paper has a dedicated \"limitations\" section. We found 94.77% (N=145) papers with a dedicated section for limitations (i.e., with \"limitations\" in the section title) and 14.38% (N=22) papers with a dedicated ethics or impact statement. Our analysis was primarily based on the limitations section; if there was no limitations section, we read through the paper to find potential mentions of limitations."}, {"title": "4.4.1 LLM Performance", "content": "The top-level code refers to limitations on LLMs' capability to generate the desired output. These limitations highlight areas where the LLM's performance may fall short of expectations.\nLLM bias toward different groups (11.11%, N=17): This limitation recognizes that LLMs' disparate representation across different populations. For example, Shin et al. [144] noted the GPT-3 and DALLE-2 in their system might output and perpetuate gender and racial stereotypes, including a higher chance of featuring white men rather than users in other racial groups. This limitation also includes cases where LLMs fail to model certain user groups-the absence of those users. Ma et al. [110] stated that LLM-based chatbots failed to \"recognize complex and nuanced LGBTQ+ identities and experiences, rendering the chatbots' suggestions generic and emotionally disengaged.\"\nLimited data coverage in the training data (9.80%, N=15): Authors explicitly mentioned that LLMs' training data might be insufficient or outdated. For instance, Lee et al. [88] found they needed extra engineering steps to use an LLM with their Korean-speaking participants, which they attributed to \"GPT-4's underperformance in non-English languages\". When prompting LLM conversational agents to display empathy using elicitations from Reddit, Cuadra et al. [29] acknowledged that they are not aware of the distribution of the training data, and are therefore unable to tell whether the data used in the study has been covered by GPT-4.\nNon-deterministic response (7.84%, N=12): Authors often recognized that LLM responses are probabilistic, and could change unpredictably even when given the same prompt. Gu et al. [50] recognized that their LLM's explanations were not fully controlled, because they used real-time responses from commercial models. Chen et al. [20] attributed the inconsistency of generated data to the \"inherent randomness embedded in the output of LLMs.\u201d This, however, can be alleviated by changing the sampling temperature to zero [122] or using guided generation [96].\nHallucination (8.50%, N=13): LLMs can produce inaccurate or entirely fabricated information. Hoque et al. [62] explicitly pointed out that \"LLMs can generate hallucinations,\u201d which may \"alter the dynamic for such authors [in their study] when using an LLMs\u201d but later stated that studying the effect is out of their study scope. Though Retrieval Augmented Generation (RAG) systems may help alleviate this problem in the future [44, 68], applications that leverage this approach can still suffer from hallucination issues. For instance, Zulfikar et al. [186] stated that using LLMs \"in information retrieval can lead to hallucinated answers that do not exist in the dataset.\u201d To ensure validity, works such as PaperWeaver [92] attempted to evaluate the system's performance against hallucination by collecting annotations of factual correctness for 60 descriptions in their study, but not all papers grappled with this problem as explicitly."}, {"title": "4.4.2 Resource Limitation", "content": "This top-level code refers to computational and financial resources needed to run LLMs, as well as a lack of evaluation standard or metrics. High resource demands can impact the efficiency and scalability of deploying the LLM, and can affect our community's ability to consistently evaluate LLMs or tackle common problems such as hallucination.\nComputational cost (9.15%, N=14): Computational cost refers to the computational resources required to run LLMs, including the need for hardware (e.g., GPUs) for local execution and limited token windows, which restrict the amount of possible input. For example, Nguyen et al. [119], who employed OpenAI's Codex, wished that they had used open-source models to ensure study reproducibility, but recognized that doing so would \"impose significant computational requirements\u201d due to the need for extensive GPU resources. When facing the limited token size, authors had to devise workarounds. For example, Petridis et al. [127] split their documents into sections to accommodate GPT-3's input length, and wrote that that might have \"affected the overall performance and user experience of the system.\"\nFinancial cost (3.27%, N=5): This resource constraint included monetary expenses with using LLMs, often tied to API calls for closed-source models and using online platforms like ChatGPT. For example, RELIC integrated GPT-3 due to its high performance, but authors also recognized that the LLM-enhanced component via the API \u201cwill inevitably increase calculation expenses.\u201d [23] Similarly, financial cost also impacts access to advanced chatbot playgrounds. In a study of ChatGPT's ability to answer programming questions, Kabir et al. [75] noted the $20 per month subscription fee is a \"considerably high monetary value for many countries,\" and decided to use the free version (GPT-3.5) to lower the barrier for reproducibility at the expense of potential performance.\nLack of evaluation standards/metrics (16.99%, N=26): This category includes authors wishing to evaluate LLM aspects, but lacking the appropriate standard or metrics. A paper falls under this category only when authors explicitly called for more standards (e.g., \"open question\" or \"active research area\"). For instance, Taeb"}, {"title": "4.4.3 Research Validity", "content": "Research validity is often defined as the extent to which an instrument measures what it claims to measure or if the study design can effectively test their hypotheses [112]. Internal validity refers to the legitimacy of a study's results, considering factors such as group selection, data recording methods, and analysis procedures [112]. External validity concerns the findings' transferability to other contexts of interest [112]. We consider ecological validity a subset of external validity, in that it refers to whether the studies resemble \"real-world\" conditions [137]. Validity issues can arise across users, contexts, LLMs, and prompts.\nIn total, we identified 2 \u00d7 4 codes related to this limitation. During coding, we first determined whether the issue impacted internal or external validity, and then identified the affected dimensions. We avoided assessing whether the project could have validity issues, but instead coded what the authors acknowledged in their paper.\nThe most prevalent limitations are internal and external validity across users and contexts. Internal validity issues related to users often stemmed from limited sample sizes and lack of diversity within samples. For example, Lin et al. [101] mentioned that \u201ca relatively small sample size leads to challenges in concluding some of the potential correlation.\u201d This, in turn, may have external validity concerns. For example, Park and Ahn [126] mentioned that their research is based only on English-speaking university students, so the result \"may not reflect students who speak English as a second language.\u201d Similar issues can also apply to different contexts, such as application scenarios. Zhang et al. [179] recognized that their study setup might have \u201cconstrained the natural spontaneity that a human can bring to the storytelling process\u201d, which may have hurt the internal validity of observing behaviors that the authors claimed to study. Zhang et al. [180] acknowledged that their insights \"may or may not generalize to use in the field\", because their prototype design constrained \"what tasks our participants could do.\" Research validity issues across users and contexts are generally related to study designs evaluating LLMs or LLM-powered systems.\nOf the 153 papers, 130 papers (84.98%) used or studied a variation of the closed GPT-family models. Despite this, many researchers articulated the research validity issues across models. Internal validity issues may arise when using LLMs. For example, Chakrabarty et al. [18] employed the default GPT-4 generation parameters (i.e., temperature = 1) to evaluate the model's capabilities. However, they recognized that a variation in temperature might have changed the content quality, thus affecting the study conclusion. Dang et al. [31] also acknowledged that they might not have identified the best settings for model usage due to using the black-box models, which may affect the results internally."}, {"title": "4.4.4 Consequences", "content": "This category shows potential negative outcomes that may arise from the artifact or study. In some cases, authors present the concerns in an ethics or impact statement (14.38%, N=22) with concrete remediation strategies.\nEconomic Harms (11.11%, N=17) This refers to potential effects on employment and work. For example, De La Torre et al. [32] highlighted the concern of \u201cdevelopers and creators being replaced\u201d. However, they also recognized that these tools have not achieved end-to-end development, and if so, the these tools should still require human intervention. Shaikh et al. [140] mentioned that their tool to simulate conflict resolution scenarios might cause job replacement and devaluation for expert trainers. Many papers on Communication & Writing, such as Lee et al. [89] and Hoque et al. [62], stated that their LLM-powered writing tool may change copyright issues and how writers work.\nRepresentational Harms (5.88%, N=9) This harm refers to social groups being cast in a less favorable light than others, affecting the understandings, beliefs, and attitudes that people hold about"}, {"title": "5 Discussion", "content": "We show substantial growth at CHI in research studying LLMs, echoing trends in other fields [117, 169]. In this section, we discuss"}, {"title": "5.1 Revealed Growth Opportunities for HCI", "content": "To our best knowledge, our work is the first to systematically characterize how LLMs have influenced HCI research. We find substantial opportunity to expand the application domains where LLMs are used (5.1.1), build theories and methods with lasting impact from the large body of empirical work (5.1.2), and standardize how LLMs can and should impact prototyping practices (5.1.3).\nOur study demonstrates that LLMs are applied across a wide range of application domains, reflecting diversity within the HCI community, and also confirming that LLMs' influence on HCI is pervasive across subareas. Some areas are well-represented already, and provide examples of how to build new research communities around LLM applications. Specifically, we found the Communication & Writing domain has garnered the most attention, perhaps due to LLMs' direct relevance to producing language. This community has coalesced around such initiatives as the In2Writing workshops at NLP and HCI conferences [19], and Lee et al. [89]'s effort to chart the design space of intelligent and interactive writing assistants. Other areas are less represented in our review, and represent opportunities for new research and community-building. For example, papers related to Games and Play were less common in our sample, even though this area is large enough to warrant its own CHI subcommittee. As LLMs facilitate more games and simulations, we anticipate this area to be a generative site for new work. Our categorization of application domains can help researchers identify where in the community their interests might fit, and how to develop these areas as LLMs continue to proliferate."}, {"title": "5.1.2 Beyond empirical and artifact contributions.", "content": "While we observed that HCI researchers succeeded in applying LLMs to a diversity of application domains, we found less diversity in the contribution types pursued in the literature. The LLM-related papers in our sample predominantly center on artifact contributions and empirical evaluations, often in the form of user studies of new artifacts. Empiricism is central to understanding phenomena; however, to develop knowledge from our aggregate body of observations, we encourage more attention to Wobbrock and Kientz [167]'s five other contribution types, each of which was less well-represented in the literature.\nWe observed an opportunity for the community to further pursue dataset contributions [167]-and approaches to data collection that center real user needs and downstream harms. Traditional NLP benchmarks are often criticized for their lack of context realism: the model performance measures are often divorced from downstream"}, {"title": "5.1.3 How LLMs impact prototyping standards.", "content": "More broadly, our findings signal broader methodological questions for HCI: What level of prototype fidelity is needed to demonstrate a new interaction-and relatedly, what level of system-building and evaluation is needed to make an artifact contribution? This question arises from our challenge to define which papers proposed artifact contributions, and which used LLMs as system engines. Throughout HCI, Wizard of Oz approaches have long been used to prototype interactions with intelligent agents [30, 114]. These methods typically present a research participant with an interface that appears to have machine intelligence, but unbeknownst to them, a human performs those functions (cf. [50]). Wizard of Oz approaches gained popularity in HCI as methods that allowed rapid and inexpensive prototyping of future technological capabilities.\nHowever, the utility of these methods may change as LLMs proliferate. If a researcher explores the design space around using LLMs in a given domain-to \u201csketch with Al\u201d, as Yang et al. [175] describe-does a Wizard of Oz approach provide benefits over a fully automated approach anymore? Historically, researchers have been trained to prototype quickly and cheaply, and thus they might conclude that a Wizard of Oz approach makes more sense. Today, however, LLMs have likely lowered the barrier of developing systems so much that we may expect designers to use them to achieve more ecologically valid research. After all, even the best of Wizard of Oz methods cannot perfectly proxy machine intelligence [177]. If a designer creates an LLM-backed prototype, however, what level of performance should they aspire to in their system?"}, {"title": "5.2 Challenges: Validity, Reproducibility, and Consequences", "content": "To achieve the opportunities we outline in the previous section, the HCI community will also need to reflect on some fundamental challenges with LLM research identified in our analysis."}, {"title": "5.2.1 Proprietary LLMs raise reproducibility concerns.", "content": "Our analysis showed that despite authors' commonly articulated limitations surrounding research validity, papers using LLMs is growing exponentially. This trend adds urgency to calls for examining reproducibility in HCI [26, 135"}]}