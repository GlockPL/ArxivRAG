{"title": "Appraisal-Guided Proximal Policy Optimization: Modeling Psychological Disorders in Dynamic Grid World", "authors": ["Hari Prasad", "Chinnu Jacob", "Imthias Ahamed T. P."], "abstract": "The integration of artificial intelligence across multiple domains has emphasized the importance of replicating human-like cognitive processes in AI. By incorporating emotional intelligence into AI agents, their emotional stability can be evaluated to enhance their resilience and dependability in critical decision-making tasks. In this work, we develop a methodology for modeling psychological disorders using Reinforcement Learning (RL) agents. We utilized Appraisal theory to train RL agents in a dynamic grid world environment with an Appraisal-Guided Proximal Policy Optimization (AG-PPO) algorithm. Additionally, we investigated numerous reward-shaping strategies to simulate psychological disorders and regulate the behavior of the agents. A comparison of various configurations of the modified PPO algorithm identified variants that simulate Anxiety disorder and Obsessive-Compulsive Disorder (OCD)-like behavior in agents. Furthermore, we compared standard PPO with AG-PPO and its configurations, highlighting the performance improvement in terms of generalization capabilities. Finally, we conducted an analysis of the agents' behavioral patterns in complex test environments to evaluate the associated symptoms corresponding to the psychological disorders. Overall, our work showcases the benefits of the appraisal-guided PPO algorithm over the standard PPO algorithm and the potential to simulate psychological disorders in a controlled artificial environment and evaluate them on RL agents.", "sections": [{"title": "Introduction", "content": "The need for emotional agents arises from the growing integration of artificial intelligence (AI) in various domains, such as healthcare, transportation, education, customer service, and entertainment. Emotional agents can potentially revolutionize how humans engage with AI systems, creating more natural and intuitive interfaces. Studying the emotional stability of artificial intelligent agents is essential in ensuring the reliability and predictability of AI systems (Peter and Beale 2008) (Gremsl and H\u00f6dl 2022). Unstable or unpredictable emotional responses in agents may result in inappropriate behavior, unreliable decision-making, and negative user experiences. Further, modeling and studying psychological disorders in agents provides valuable insights into AI and human psychology (Zhao et al. 2022), deepening the understanding of cognitive and emotional processes. This enables AI researchers in developing accurate models of human behavior, enhancing the design of emotionally intelligent agents. Psychologists on the other hand can benefit from a controlled environment, simulating psychological states, and using AI systems to investigate factors affecting the well-being of the human mind. Cognitive appraisals involve subjective evaluations of the personal significance and implications of events that develop psychological states, which are difficult to quantify and represent computationally. RL relies on a quantitative modeling approach, while appraisals consider qualitative factors such as goal importance and personal values. Bridging qualitative appraisals with quantitative RL requires models that can effectively grasp and use diverse appraisal dimensions. Integrating appraisals into RL decisions involves efficiently updating their dynamic nature for optimal agent policies, which is a significant computational challenge. Evaluating psychological disorders in RL agents can be complex due to their intricate nature involving cognition, emotions, and behavior. Additionally, assessing psychological disorders in RL agents requires reliable and valid metrics to determine the presence and severity of these disorders. This work aims to develop a modified Proximal Policy Optimization algorithm that can simulate agents exhibiting behavioral patterns resembling those of Obsessive-Compulsive Disorder (OCD) and Anxiety Disorder. Our objective is to propose a new research direction in exploring the psychology of RL agents, without asserting that these agents possess the intelligence to develop human-level emotions or intricate psychological disorders. However, they can simulate specific fundamental emotional traits through behavioral patterns reminiscent of human or animal intelligence. Through experimentation with different algorithm configurations, we also showcase a variant that surpasses the performance of the standard PPO algorithm in the selected dynamic grid world environment.\nMain Results In this work, we introduce a novel Appraisal-Guided Proximal Policy Optimization algorithm (AG-PPO), to simulate an emotional agent in a dynamic grid world environment. We have further formulated 6 cognitive appraisals obtained from the grid world environment, representing the psychological state of the agent. Our key contributions include\n1.  An Appraisal-Guided PPO algorithm, with the ability to train an emotional agent, in a dynamic grid world with"}, {"title": "Methodology", "content": "In this study, we use a dynamic grid world environment (Chevalier-Boisvert et al. 2023) (Figure 1), with cognitive appraisals trained using AG-PPO algorithm. The grid world consists of 5 elements: Agents, Goals, Obstacles, Walls, and Empty spaces, encoded as integers (Figure 2). The agent's action space offers 3 discrete actions: left, right, and forward. To move left, the agent first needs to turn left and then move forward. Rewards are sparse, given only at the episode's end. A win grants a return inversely proportional to steps taken, while failure or reaching step limits yields a return of -1. The dynamic obstacles and moving goal in the grid world enable straightforward assessment of the agent's psychological state through trajectory analysis, goal proximity, and goal-related behavior. Modifications to the environment can be made without sacrificing these assessment benefits."}, {"title": "Cognitive Appraisals", "content": "Cognitive appraisals involve primary assessment of event significance and well-being implications, as well as secondary evaluation of coping resources and potential. These subjective assessments contribute to varied emotional responses in different contexts and individuals (Lazarus 1991). Six cognitive appraisal variables are assessed in this study, reflecting the agent's cognitive understanding of its environment (Lazarus and Folkman 1984), (Gross 1998), (Kensinger 2004), (Scherer, Schorr, and Johnstone 2001).\nThese variables ($S_n$) are estimated at each agent step, influencing action selection and value estimation through actor and critic networks after being re-scaled to (0,1). These appraisals play a role in shaping the agent's decision-making process and are formulated as discussed below.\nMotivational Relevance Motivational relevance in cognitive appraisals involves subjectively evaluating how a stimulus affects personal goals and values. This study calculates it as the distance between an agent's position and a goal state in a grid world, shedding light on the interplay between motives, values, and emotions (Sequeira, Melo, and Paiva 2011). Mathematically, Motivational Relevance is defined as the complement of the Manhattan distance between the agent and goal in the grid world, as shown in Equation 1\n$SMR = 1-\\frac{(|xa - xg| + |Ya \u2013 Yg|) \u2013 1}{2 (\u03c9 \u2013 1)}$\nHere, $(x_a, y_a)$ represents the agent's position and $(x_g, y_g)$ represent the goal position in the grid world. The value $\u03c9$ denotes the size of the grid world environment (in this work, $\u03c9$ = 10).\nCertainty Certainty appraisal gauges subjective confidence in event predictability. The entropy of the Soft-Max-applied output of the actor network, informs Certainty calculation, indicating lower certainty for higher entropy and greater uncertainty in this study. The Certainty is estimated using the following Equation 2.\n$\u03b6 = 1-\\frac{- \u03a3plog (p)}{1+(-plog(p))}$\nWhere $p$ = SoftMax(logits) represents the Soft-Max output of predicted action probabilities. Here the Complement of the entropy is taken and normalized and re-scaled to the range (0,1).\nNovelty Novelty pertains to gauging unfamiliarity in stimuli or situations. It's quantified using KL divergence 3 between predicted action probabilities (P) and uniform distribution (Q), with higher divergence implying greater deviation of predicted distribution from uniformity.\n$N = \\frac{KL(Q||P)}{1+KL(Q||P)}$\nGoal Congruence Goal congruence appraisal evaluates situation relevance and compatibility with personal goals, based on an individual's subjective assessment. The method estimates goal congruence using Euclidean distance between the agent's current and goal position, considering visibility conditions, as expressed in the provided Equation 4.\n$SGC = 1 - \\frac{\\sqrt{(xa - xg)^{2} + (Ya - Yg)^{2}}}{(n-1)}$\nHere, $(x_a, y_a)$ represents the agent's position and $(x_g, y_g)$ represent the goal position in the grid world. n is the view size of the agent in the grid world (in this work, n = 7).\nCoping Potential Coping Potential pertains to an individual's perceived resources for managing stimuli. For the agent, it indicates confidence in executing its intended path based on obstacle perception, calculated as the ratio of visible obstacles to total obstacles in the environment. In order to estimate the value, we calculate the ratio of the number of obstacles, the agent sees in its view to the total number of obstacles in the environment as represented in Equation 5.\n$SCP = 1-\\frac{kobst}{Nobst + E}$\nHere the $k_{obst}$ denotes the number of obstacles in the agent's view and $n_{obst}$ denotes the total number of obstacles in the environment. The value $E$ is used to avoid division by zero.\nAnticipation Anticipation in cognitive appraisals involves assessing future event outcomes. In this study, anticipation is quantified as the inverse of the Next Reward Estimation (NRE) error, trained using a 3-layer Neural Network to predict forthcoming rewards based on current observations and actions. The anticipation will be equal to the complement of the difference between the predicted reward and the actual reward as in Equation 6.\n$A = 1 - [R-NRE(Obst_{t-1}, a_{t-1})]$\nWhere obst-1 and at-1 are the previous observation and action probabilities respectively."}, {"title": "Reward Shaping", "content": "Reward shaping in reinforcement learning (RL) adjusts training rewards to enhance learning and performance (Memarian et al. 2021), (Gupta et al. 2022). The environment has preset rewards, like -1 for failure. This approach modifies immediate rewards using cognitive appraisals, guiding the agent's learning and exploration. It allows controlled training by adjusting appraisals, aiding in capturing intricate environmental features, thus improving the agent's training and adaptability. In this work, 10 different configurations have been experimented with, which include a baseline and control along with multiple variants using reward shaping. The configurations are detailed as\n\u2022 Baseline: The version lacks appraisal in the critic's input and lacks reward-shaping strategies, employing standard PPO with the clipped objective. Experiments compare various approaches, aiming to surpass baseline performance while identifying configurations for psychological disorders.\n\u2022 PPO with noise: Random noise is added to the critic's input without reward-shaping to serve as a control, verifying that outcomes attributed to appraisals are not solely due to input noise.\n\u2022 PPO with appraisal: This configuration includes appraisal information concatenated to the critic's input, in order to include the appraisal information in the agent's training process. But compared to other configurations, this does not use any reward-shaping strategies."}, {"title": "", "content": "\u2022 RSv1: This configuration which stands for reward shaping version 1, is configured to increase motivational relevance using the reward shaping strategy, outlined in Equation 7.\n$Trsv1 = rt\\ \u2013 0.01[1-SMR]$\n\u2022 RSv2: This configuration tries to increase coping potential through the reward-shaping strategy. The reshaped reward can be obtained using Equation 8.\n$Trsv2 - Tt\\ \u2013 0.01[1 - SP]$\n\u2022 RSv3: This configuration focuses on improving the goal congruence as the reward-shaping strategy. The reshaped reward can be obtained using Equation 9.\n$Trsv3 - rt\\ \u2013 0.01[1 - &c]$\n\u2022 RSv4: This configuration tries to improve both motivational relevance and goal congruence through the reward-shaping strategy. The reshaped reward can be obtained using Equation 10.\n$Trsv4 = It\\ \u2013 0.01 ([1 \u2013 SMR] + [1 \u2013 $&c])$\n\u2022 RSv5: This configuration uses a combination of motivational relevance, coping potential, and goal congruence in the reward-shaping strategy, trying to increase them. The reshaped reward can be obtained using Equation 11.\n$rrsv5 = rt\\ \u2013 0.01 ([1 \u2013 SMR] + [1 \u2212 $&P] + [1 \u2013 $&c])$\n\u2022 RSv6: This configuration uses a combination of motivational relevance, coping potential, and goal congruence in the reward-shaping strategy, in such a way that the agent is forced to minimize these appraisals. The reshaped reward can be obtained using Equation 12.\n$Trsv6 = rt - 0.1([SMR] + [SCP] + [S&C])$\n\u2022 RSv7 (A, B): This configuration uses a combination of motivational relevance, coping potential, and goal congruence in the reward-shaping strategy, in such a way that the agent is forced to minimize the motivational relevance while increasing coping potential. The reshaped reward can be obtained using Equation 13. Here there are 2 variations of RSv7 which are version A, having the factor $\u03b5$ = 0.01, and version B having $\u03b5$ = 0.1\n$rrsv7 = rt - e[1 \u2013 \u0160EP] \u2013 0.1([SMR] + [S&C])$"}, {"title": "Appraisal-Guided Proximal Policy Optimization", "content": "Proximal Policy Optimization (PPO) (Schulman et al. 2017) is a prominent on-policy reinforcement learning technique, adept at managing the exploration-exploitation trade-off for effective policy updates in dynamic non-stationary environments. PPO's reliance on real-time data collection makes it well-suited for scenarios like dynamic grid world problems, allowing adaptive learning without historical data dependency. This study adapts standard Proximal Policy Optimization (PPO), by enhancing the clipped surrogate objective with cognitive appraisal variables, particularly by integrating these variables into the critic network (Figure 3). The modified approach incorporates both state information and cognitive appraisals for improved value prediction within the PPO framework. Appraisal information, representing Motivational Relevance, Novelty, Certainty, Goal Congruence, Coping Potential, and Anticipation, is concatenated with state data and provided to the Critic model. Appraisals are computed from the state at each step, stored, and utilized by the Critic for value estimation. Preceding the Critic input, a Convolution network processes an (7, 7, 3) input array (where 7 is the size of the agent's view) with 3 convolution layers and self-attention layer (Vaswani et al. 2017), (Wang et al. 2018), (Devlin et al. 2018), (Dosovitskiy et al. 2020), producing a (6400, 1) flattened output. The Critic model includes 3 dense layers: the input is a flattened Convolution block output data combined with appraisals (6400, 1) + (6, 1); subsequent layers have 256 and 64 units, with a final unactivated output. A reward-shaping step adjusts rewards using appraisals before using them for estimating the Generalized Advantage Estimation (GAE), vital for estimating the policy loss. An Actor-network with 3 dense layers takes a Convolution block-processed state, and generates action probabilities, serving as the agent's policy model. The reshaping function $p(S_t)$ controls the reshaping strategy (RSV1-7) in the training process as shown in Equation 14 where $w_{rf}$ is the associated weight. The agent view size, grid size, and some toggles such as dynamic wall, dynamic goal, dynamic obstacles, moving goal, etc., all represent the parameters that define the environment as discussed in the previous section.\n$R = R_t - Wrf [p(St)]$\nAlgorithm 1 explains the working of modified cognitive appraisal-guided PPO. After initialization, a set of observations, and corresponding actions, values, appraisals, etc., are obtained by using the initial policy. Then the advantages are estimated using Generalized Advantage Estimation (GAE). Once enough samples are obtained, they can be used to train the actor and critic models to update the policy and value networks respectively.\n$LCLIP (\u03b8) = \u00cat min\\frac{\u03c0\u03b8(at St)}{\u03c0\u03b8old (at St)} \u2022 At,clip ( \\frac{ \u03c0\u03bf(\u03b1\u03b9 8\u03b5)}{\u03c0\u03b8old (at St)}, 1 \u2013 \u03b5,1 + \u03b5). At$\nThe policy loss in PPO is estimated by taking the clipped surrogate objective function 15. The policy loss is defined as the negative weighted average of the surrogate objective function, which measures the policy's deviation from its previous policy. The surrogate objective function compares the probabilities of the actions selected under the current policy to the probabilities of those actions under the previous policy. Here the advantage $\u00c2_t$ is denoted as the difference between the cumulative discounted reshaped rewards $R$ and the value function estimate $V_\u00b5(s_t)$ when taking an action at in state st at time step t, as shown in\n$At = R - V\u00b5(St)$\nThe clipped surrogate objective comprises two terms: the first encourages action updates based on positive advantages for exploration. The second caps updates to prevent excessive changes for stability. By computing the minimum of these terms, conservative policy updates are ensured. The policy loss is averaged across a batch and minimized through gradient-based methods like Adam to update parameters (\u03b8). The Value loss in PPO is shown in Equation 17, where $LV (\u03bc)$ represents the value loss objective. N is the batch size, indicating the number of samples in the mini-batch. St refers to the state at iteration t in the mini-batch. (\u03bc) denotes the current parameters of the value function neural network. (\u00b5t-1) represents the parameters of the value function neural network from the previous iteration. $R_t$ represents the cumulative discounted reshaped rewards. The objective of the value loss is to minimize the discrepancy between the estimated values and the target values, encouraging the value function to better approximate the expected returns.\n$LV (\u03bc) = \\frac{1}{N}\u2211 (V(81); -1) - R\u2081)\u00b2$\nValue loss estimates the error between actual and predicted returns. NRE loss, which represents the difference between the actual reward and the predicted reward is combined with policy and value losses for estimating the total loss and back propagated using Adam optimizer with decayed learning rate and epsilon 1e-5."}, {"title": "Results and Discussion", "content": "The trained models were evaluated in two different scenarios (figure 1): GW-A resembling the training set but with additional obstacles and a moving goal (10x10 grid, 7 obstacles, max steps=100, dynamic goal), serving as a moderately complex test of generalization. Another environment GW-B (10x10 grid, 7 obstacles, dynamic goal, max steps=400, dynamic walls) was employed, significantly differing from training, facilitating a deeper evaluation of the agent's generalization capabilities. In all cases, the view size of the agent is fixed at 7x7. The baseline configuration, utilizing a standard PPO algorithm, demonstrates robust performance in learning environment dynamics and agent adaptability. Table 1 summarizes the overview of the agent in both GW-A and GW-B environments with a moving goal and 5 moving obstacles. The agent achieves a success rate of 78.78% across 33 episodes, winning 26 of them. The overall baseline agent score is 0.6909. Configuration RSv1, incorporating Motivational relevance for reward shaping (Equation 7), exhibits superior generalization during training and testing in GW-A and GW-B environments, surpassing baseline and PPO + appraisal. RSv1's trajectory demonstrates effective goal-reaching, emphasizing the positive influence of integrating appraisals, especially Motivational relevance. This integration enhances PPO algorithm performance and generalization. RSv1 highlights the efficacy of this inclusion in guiding agents towards successful goal attainment, optimizing RL agent configuration for improved grid-world performance. Figure 4.a, shows the region most visited by the agent. The brighter a grid cell, the more the agent visited it. In the GW-A test environment, the baseline agent explores near the center, while RSv1 shows a uniform distribution and RSv7-A exhibits repetitive patterns, favoring edges whereas RSv7-B gets stuck in corners.\n$Score = \\frac{(Nwins * \u2211(Rt) + (nlosses * (-1))}{Nplays}$\nThe overall score is computed using Equation 19, and the table summarizes scores for all configurations. RSv1 emerges as the top configuration, excelling in both GW-A and GW-B. In addition to total scores, indicators like stress, aversion, and distraction are also taken into account to analyze agent behavior. Equation 20 illustrates how to use weighted cognitive appraisals ([0.25, 0.05, 0.1, 0.2, 0.35, 0.05] corresponding to each appraisal) to estimate stress. Aversions are measured by keeping note of complete turnaround followed by onward motions, whereas distractions entail instances of losing sight of the goal post-initial encounter.\n$Stress = \u2211(1 \u2013 (i) * Wi$\nTable 1 demonstrates that the RSv7-A agent shows repetitive behavior patterns resembling OCD symptoms, including adherence to non-contributory action sequences, alongside heightened distractions and stress. This suggests the presence of anxiety and OCD-like traits, where internal compulsive tendencies drive the agent's goal-irrelevant compulsions. The RSV7-B agent's preference for preferring the grid's edges reflects its cautious avoidance behavior, resembling traits seen in Anxiety disorder. This inclination to stick to edges, even when shorter paths exist (Figure 4.a), demonstrates a strategy aimed at minimizing potential obstacles encountered within the grid's interior. The agent exhibits its attraction to corner regions, where it often gets trapped since the agent is able to keep 2 of its sides free from obstacles. In the depicted scenario (Figure 4.b), the RSv7-A agent within a modified GW-A environment, with a stationary goal and 5 obstacles, demonstrates a consistent tendency to depart from the central region towards the grid's edges. This behavior, resembling repetitive and ritualistic patterns akin to OCD, involves anti-clockwise edge-following trajectories. The agent predominantly executes forward and left actions until it visually detects the goal, subsequently moving forward to successfully achieve it, thereby reinforcing the observed connection to OCD-like symptoms (Leckman et al. 1997). Figure 4.c illustrates the RSv7-B agent's behavior in a modified GW-A environment with a static goal and 5 obstacles. Similar to RSv7-A, this agent fails to initiate goal-directed exploration and instead tends to move away from the grid center. Notably, it employs a unique avoidance strategy when encountering obstacles, rotating in place to keep them out of view, resembling anxiety-related excessive threat focus and avoidance (Stein and Sareen 2015). Additionally, even without nearby obstacles, the agent exhibits corner-seeking behavior, indicative of a reluctance to leave perceived safe zones, akin to anxiety-linked tendencies. RSv7-A and RSv7-B agents' behaviors reveal insights into psychological dimensions, mirroring OCD and Anxiety disorders. RSv7-A's edge exploration mirrors OCD-like symptoms, while RSv7-B's corner avoidance reflects anxiety-related tendencies."}, {"title": "Conclusion, Limitations and Future Works", "content": "In summary, this research evaluated a modified PPO algorithm in a grid-based setting to integrate cognitive aspects akin to natural intelligence. The objective was to create a PPO-based partial cognitive architecture for analyzing agent behaviors in dynamic grids, incorporating behavioral patterns akin to psychological disorders. Specific design criteria were developed to replicate behaviors seen in OCD and anxiety disorders. Experimental results demonstrated that agents trained with the adapted PPO algorithm could simulate symptoms resembling Anxiety and OCD while solving grid-world problems. Custom criteria and metrics were introduced to evaluate these behaviors in RL agents. The findings highlighted the potential of the modified PPO algorithm to introduce cognitive dimensions, enabling the study and simulation of psychological states akin to natural intelligence. These results emphasize the need for further exploration in affective computing, particularly concerning psychology and psychological disorders within AI. The research highlights reinforcement learning's effectiveness, particularly on-policy algorithms like PPO, in mimicking cognitive states in artificial agents. Replicating cognitive patterns associated with psychological disorders offers insight into complex cognitive processes using RL techniques. The results from empirical experiments may require caution when interpreted, given that current AI capabilities might fall short of replicating human psychological disorders. Research on emotional agents is pivotal for advancing AI and enhancing human-AI interactions. Addressing emotional stability enhances dependability and predictability, promoting trust and user acceptance. Additionally, delving into psychological disorders in agents can offer insights into AI and human psychology, advancing our knowledge of the human mind and guiding therapeutic approaches. These results contribute to the fusion of AI, psychology, and cognitive science, laying the foundation for AI systems with nuanced cognitive behaviors. This intersection prompts the development of sophisticated AI systems capable of intricate cognitive processes, mimicking and understanding human-like behaviors more accurately."}]}