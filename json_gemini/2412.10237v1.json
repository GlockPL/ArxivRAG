{"title": "Physics Instrument Design with Reinforcement Learning", "authors": ["Shah Rukh Qasim", "Patrick Owen", "Nicola Serra"], "abstract": "We present a case for the use of Reinforcement Learning (RL) for the design of physics instrument as an alternative to gradient-based instrument-optimization methods. It's applicability is demonstrated using two empirical studies. One is longitudinal segmentation of calorimeters and the second is both transverse segmentation as well longitudinal placement of trackers in a spectrometer. Based on these experiments, we propose an alternative approach that offers unique advantages over differentiable programming and surrogate-based differentiable design optimization methods. First, Reinforcement Learning (RL) algorithms possess inherent exploratory capabilities, which help mitigate the risk of convergence to local optima. Second, this approach eliminates the necessity of constraining the design to a predefined detector model with fixed parameters. Instead, it allows for the flexible placement of a variable number of detector components and facilitates discrete decision-making. We then discuss the road map of how this idea can be extended into designing very complex instruments. The presented study sets the stage for a novel framework in physics instrument design, offering a scalable and efficient framework that can be pivotal for future projects such as the Future Circular Collider (FCC), where most optimized detectors are essential for exploring physics at unprecedented energy scales.", "sections": [{"title": "1 Introduction", "content": "The study of the fundamental nature of the universe relies on very complex physics detectors and instruments in the modern era. These devices, essential for probing the smallest constituents of matter, detecting elusive particles, and testing fundamental theories, are often highly specialized and extraordinarily expensive. For instance, only the construction and materials for the Compact Muon Solenoid (CMS) experiment at the Large Hadron Collider (LHC) amounted to 500 million Swiss Francs. Designing these instruments is a monumental challenge, as every component must be optimized for maximum efficiency, sensitivity, and precision to ensure the greatest scientific output. The design parameter space of these instruments is highly complex and multi-dimensional, requiring specifications for a wide range of variables, including the type, composition, and geometry of the materials used, as well as their spatial arrangement. Additionally, factors such as thermal stability and radiation hardness must be considered, all while ensuring that the components can withstand the extreme operational conditions often encountered in high-energy physics experiments. This intricate interplay of parameters forms a vast and challenging design optimization problem, where sometimes small adjustments can have significant impacts on the overall performance and sensitivity of the instrument.\nRecently, the use of Machine Learning (ML) has exploded in different areas for solving a vast set of optimization tasks, including high energy physics as well. Machine learning algorithms, particularly those involving deep learning, have proven highly effective in analyzing large datasets, identifying complex patterns, and making predictions. In high energy physics, where experiments often generate immense amounts of data, machine learning techniques are invaluable for tasks such as particle identification, event classification, and anomaly detection. These approaches not only enhance the precision of experimental results but also accelerate the process of discovery by automating time-consuming calculations and allowing researchers to explore new theories and phenomena with greater efficiency.\nBuilding on the advances in the machine learning techniques, investigators recently began exploring machine learning also for the optimization of physics instruments. Here, the focus has been on fine tuning the design parameters along the direction of the gradient, similar to how neural"}, {"title": "2 Related work", "content": "The most notable use case in physics instrument optimization has been the optimization of the Muon Shield for the SHiP Experiment. Using a series of tapered magnets, the goal of Muon Shield is to reduce the muon flux by six orders of magnitude. Each of the magnets is parameterized by seven values, which represent the dimensions (such as length, thickness, etc). The problem is non-trivial and formulates a complex optimization task. A high energy muon can be deflected in a longer yet simpler curved path but a lower-energy muon might go in a zigzag trajectory needing multiple magnets. The initial design iterations were performed with the help of Bayesian Optimization (Baranov et al., 2017). Later, Shirobokov et al. (2020) proposed a method to employ neural surrogates to allow gradients on the design parameters. Here, a Generative Adversarial Network was trained as a surrogate model for the Geant4 simulator (GEANT4 Collaboration,"}, {"title": "3 Reinforcement Learning", "content": "Reinforcement Learning (RL) is a branch of machine learning focused on training agents to make sequences of decisions by interacting with an environment (illustrated in Figure 1). The process unfolds as a series of interactions between the agent and the environment, described mathematically as follows:\n\u2022 State (St): At each time step t, the agent observes the state of the environment, denoted as St. The state encapsulates all the relevant information needed for decision-making at that point in time. However, this state may not always be fully accessible to the agent.\n\u2022 Observation (Ot): Instead of directly observing the full state St, the agent may receive an observation Ot, which provides partial information about the environment. This is common in scenarios where the environment is partially observable, meaning the agent has limited or noisy access to the underlying state. In contrast, in a fully observable environment, the observation Ot is equivalent to the true state St.\n\u2022 Action (At): Based on the observed information (either St in fully observable settings or Ot in partially observable settings), the agent selects an action At from a set of possible actions. The choice of action is determined by the agent's policy, which is the strategy it follows.\n\u2022 Reward (R\u2081): After executing action At, the agent receives feedback in the form of a reward Rt. This scalar value indicates how good or bad the action's outcome was in relation to the agent's objective.\n\u2022 Next State (St+1): As a result of the action At, the environment transitions to a new state St+1, which the agent observes at the next time step.\nThis interaction sequence can be written as:\n(St, At, Rt, St+1)\nThe agent's goal is to maximize the cumulative reward it collects over time, often referred to as the return. The return Gt is defined as the total discounted reward from time step t onward:\nGt = Rt+1 + \\gamma Rt+2 + \\gamma^2 Rt+3 + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k Rt+k+1, (1)\nwhere \u03b3\u2208 [0,1] is the discount factor that determines the present value of future rewards. A smaller \u03b3 places less emphasis on future rewards, while \u03b3 = 1 corresponds to no discounting.\nThe agent selects actions based on a policy \u03c0, which is a mapping from states or observations to probabilities of actions. In a fully observable environment, the policy depends directly on the true state St, and is defined as:\n\u03c0(als) = P(At = a|St = s),\nwhere \u03c0(als) denotes the probability of taking action a in state s. However, in partially observable environments, the policy is based on the agent's observation Ot or its belief about the state.\nThe optimal policy \u03c0* is the policy that maximizes the expected return from each state or observation. For fully observable environments, it is expressed as:\n\u03c0*(s) = arg max E\u03c0 [Gt|St = s],\nwhile in partially observable settings, the expectation may be conditioned on the observation Ot or the agent's belief about St.\nThe reward Rt is a scalar signal provided by the environment as feedback for the action At. It represents the immediate benefit or cost associated with the action and guides the agent toward its objective. In partially observable environments, the agent must infer the underlying state St from observations Ot to learn an effective policy.\nTogether, the components (Ot, St, At, Rt, Ot+1, St+1) define the dynamics of the agent's interaction with the environment. In fully observable scenarios, these dynamics are modeled as a Markov Decision Process (MDP). In partially observable scenarios, they are generalized to a Partially Observable Markov Decision Process (POMDP), where the agent maintains a belief state- a probabilistic representation of the true state based on its observations and past actions.\nIn RL, the learning paradigm relies heavily on trial and error, with agents actively interact- ing with their environment to refine their decision-making strategies. Unlike traditional learning methods, where learning is often passive, RL agents must balance exploiting known strategies that yield high rewards and exploring new actions that might lead to better long-term outcomes. This balance, known as the exploration-exploitation trade-off, is a cornerstone of RL and is often implemented through strategies like e-greedy policies or entropy-based exploration. RL's inherent exploratory nature helps agents avoid getting trapped in local minima, a common issue in optimization techniques that focus on refining parameters locally. By sampling a wide range of states and actions, RL agents can escape suboptimal regions and discover global optima. Moreover, RL does not assume a fixed structure for the optimization landscape, enabling exploration across vast and complex spaces, including non-convex, combinatorial, and highly discontinuous environments.\nThis adaptability allows RL to tackle challenges in dynamic and intricate settings, making it a powerful framework for solving complex decision-making problems.\nTo solve these problems effectively, RL algorithms use a variety of techniques, such as value- based methods (e.g., Q-learning), policy-based methods, and hybrid approaches like actor-critic algorithms. Value-based methods focus on estimating the value of being in a particular state, helping the agent understand the long-term benefits of certain actions. Policy-based methods, on the other hand, focus on directly learning a policy for action selection, which can be beneficial in environments with continuous action spaces. Combining these approaches, actor-critic algorithms enable an agent to simultaneously estimate the value of states and improve its policy, offering a more robust way of handling complex decision-making scenarios.\nBeyond theoretical appeal, RL has demonstrated remarkable success in real-world applications, from robotics, where agents learn motor skills or navigation tasks autonomously, to video games and simulations, where RL agents can achieve superhuman performance. Moreover, RL has been applied in resource management, financial trading, and even healthcare, where personalized treatment strategies are developed based on patient data. Notably, RL is integral to systems like AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2018), where agents learn to master games like Go and chess without human input, relying purely on self-play.\nRL is particularly effective for problems where the solution space is intricate or difficult to model with gradient-based methods, such as instrument design, where multiple design configurations need to be explored simultaneously. This method has already been employed for other problems (in particular, chip placement)."}, {"title": "4 Empirical Studies", "content": "Two instrument design studies have been conducted to demonstrate the applicability of the proposed methodology. The first is longitudinal segmentation of calorimeters, which is presented in"}, {"title": "4.1 Calorimeter Design", "content": "In particle physics experiments, a calorimeter is one of the main methods used to measure the energy of particles, with the other being a spectrometer. A calorimeter measures energy by fully absorbing the particles, whereas a spectrometer determines momentum by analyzing the deflection of particles in a magnetic field, based on their momentum and charge. Calorimeters provide a direct measurement of total energy, while spectrometers are used to infer momentum through particle trajectories.\nThe calorimeters are generally built as sampling devices where active readout layers and ab- sorber materials are alternated. Generally the materials used are also different. However, in this study, a homogeneous sampling calorimeter was used. The simulations were performed us- ing Geant4 and all the resulting data was captured, including the individual deposits as well as their spatial locations. Given that it is a sampling calorimeter, it allowed reuse of the same set of simulations across designs, saving computation time. Generally resampling is also an expensive operation, however, CUDA was leveraged to deploy the simulation on a Nvidia GPU, enabling testing of one design in ~ 0.02 seconds. When testing any configuration, first calibration needs to be performed, which was done over 50000 samples consisting half of photons and half of charged pions using similar strategy as what was used by Qasim et al. (2022).\nThe number of cumulative budget of the active layers is capped at 6.0, with the RL agent having control over their placement, deciding where to position each active layer sequentially. Further, the agent can also choose the type of sensor from a set {\u0422\u0443\u0440\u04351, \u0422\u0443\u0440\u04352, Type3}, where type 1 the cheapest and least performant and type 3 is the most performant but most expensive. The three types have a budget of 0.12, 0.15 and 0.20 respectively. Putting together three actions for the logits of the discrete action and one for the continuous action, the action space is four dimensional. The RL agent is then tasked with selecting the optimal combination of active layers by using a reward function. The current configuration of active layers that the agent has selected so far is the state of the environment. However, the experiment is designed to be partially observable. Therefore, only the longitudinal location the agent is currently at and the thickness budget used so far is employed as Ot. This implies that the observation space is a 2D vector.\nThe performance of the energy measurement in a calorimeter is quantified by mean-corrected energy resolution, defined as standard deviation of the response (Epred/Etrue) divided by the mean. A lower value signifies better performance. The score (S) is based on the energy resolution of the calorimeter for both hadronic and electromagnetic particles and is defined as\nS.10 = max(0, \\Sigma_{em50} - 8) - max(0, \\Sigma_{em100} -5) - max(0, \\Sigma_{had50} - 25) - max(0, \\Sigma_{had100} - 18), (2)\nwith \u2211 representing the mean-corrected resolution for different particles in percentage. The resolution is capped at a maximum of 150%. The subscripts em and had represent the electro- magnetic and hadronic particles, respectively and the subsequent number denotes the energy of the test particles in GeV. For measuring these metrics, 2500 particles of each type are employed (104 in total, not including the calibration set). The reward function is linear in resolution and drops to zero once the design reaches the reference resolution, and this was chosen as value that is slightly better than what was roughly expected in a reasonable design. The design score is given as the reward for the agent only at the last time step. In the intermediate time steps, a penalty of -0.5 is awarded if the agent places two active layers less than 10 mm apart. The action is also capped at 10 mm."}, {"title": "4.2 Spectrometer Design", "content": "A spectrometer is a instrument in particle physics experiments, designed to measure the momen- tum of charged particles by analyzing their trajectories as they traverse a magnetic field. Unlike calorimeters, which absorb particles to measure their total energy, spectrometers provide indirect energy measurements by leveraging the relationship between a particle's charge, momentum, and the curvature of its path in a magnetic environment. This curvature depends on the particle's momentum, with more energetic particles following less curved trajectories. Spectrometers are typically equipped with tracking stations, which are high-precision detectors placed at intervals along the particle's path. These stations record the position and trajectory of the particle be- fore, during, or after its movement through the magnetic field, enabling the reconstruction of its momentum. Tracking stations often use advanced technologies, such as silicon strip detectors or drift chambers, to achieve the high spatial resolution necessary for accurate measurements. In this case we assume tracking stations that are made of silicon pixels, which are most useful not only for momentum resolution but also for pattern recognition. By combining data from multiple tracking stations, the spectrometer allows physicists to infer key properties of the particle, such as its momentum and charge.\nIn our experiment, a magnet is positioned at the center of the setup, spanning the region from z = 10m to z = 11m along the z-axis. The magnetic field is oriented along the y-axis with a uniform strength of 1 T. A particle initially travels in a straight-line trajectory2 (called Region A) until it enters the magnetized region. Within this region, the particle experiences a Lorentz force acting along the x- and y-axes. Upon exiting the magnetic field, the particle resumes a straight-line trajectory in Region C2. The angle between the trajectories before and after the magnet can be approximated as\n\\theta \\approx \\frac{0.3}{Pxz}2 sin(\\frac{l}{2}) (3)\nFurther, in the simulation, multiple scattering is also taken into account. The standard devia- tion of the multiple scattering is estimated as\n\\sigma_{\\theta}=\\frac{13.6 \\text{GeV}}{p}z\\sqrt{\\frac{x}{X_0}}(1+0.038 \\ln(\\frac{x}{X_0})), (4)\nwith Z being the atomic number of the material, Xo, the radiation length and x, the thick- ness (Groom and Klein, 2000). The numbers correspond to 200 \u00b5m silicon sensors.\nThe agent is tasked with choosing the spatial locations where to place the tracking stations as well as granularity of each tracking station. The spatial location is chosen as the continuous action whilst the granularity selection task is modeled as a discrete action. The discrete action (Agran) can be between 0 and 9 and the granularity of the placed sensor is then chosen as (Agran + 10) ."}, {"title": "5 Outlook", "content": "The presented studies demonstrate the applicability of RL for instrument design using simple examples. In this section, we discuss how these works can be extended for more complex instrument design problems."}, {"title": "5.1 Flexible Reconstruction Algorithms", "content": "In the study presented on spectrometer design, a straightforward reconstruction algorithm was employed. However, for scenarios involving more generic and irregular detector geometries, such algorithms often prove insufficient. In such cases, advanced approaches like graph neural networks"}, {"title": "5.2 Amortization of Reward and Cost", "content": "In the presented studies, a large number of simulations are performed to get a good estimate of the design score. This makes it convenient for the RL algorithm as the variance in the reward function is low. As an example, the SHiP Muon Shield aims to reduce the flux rate by over six orders of magnitude and if a design under consideration already gives multiple resulting hits after testing a few hundred, this design can be easily ruled out. Similar argument can be made also for the calorimeter. Modern RL algorithms are designed to operate in the conditions of such high variance returns i.e. the reward function is stochastic. Employing this property in future work can award computational benefits making the discovery process quicker."}, {"title": "5.3 Off-policy algorithms", "content": "In this work, PPO has been used, which is an on-policy algorithm. This implies that any time the policy changes, the agent must collect a new set of experiences based on the updated policy to continue learning effectively and discard the old ones (called experience replay in RL terminology). Therefore, the algorithm is not the most sample efficient. This was as choice of convenience in the conducted study to quickly get results. However, techniques such as prioritized experience replay Horgan et al. (2018) will allow the use of off-policy algorithms which are more computationally efficient. As an alternate, on-policy but continual learners can also be employed instead which don't suffer from catastrophic forgetting French (1999), a particularly pronounced problem faced by neural networks."}, {"title": "5.4 Better Policy and Value Networks, and Improving Observability:", "content": "In the presented experiment of calorimeter design, the observation space was only two dimensional, representing the longitudinal dimension as well as the thickness budget used, which forces the agent to operate in a highly partially observable environment. Given that the full design is known, one can improve this by employing a better representation of the current state. For only longitudinal segmentation, this can be represented as a vector, as in the example of the spectrometer design but in end-to-end optimization, a more generic graph neural network could be used. For instance, a GNN was employed by Mirhoseini et al. (2022) for the chip placement problem. They will also provide more powerful policy and value network compared to the fully connected neural networks a choice of convenience in the experiments presented in this work."}, {"title": "5.5 Surrogate simulators", "content": "The computational expense of Monte Carlo simulators, particularly in propagating particles through dense materials, has spurred extensive research into faster machine learning-based simulation alter- natives. Initially focused on simulating calorimeter showers (Paganini et al., 2018) using generative adversarial networks (GANs) (Goodfellow et al., 2014), recent advances have expanded into the fast simulation of particle jets (Touranakou et al., 2022) and other final-state observables. Researchers have explored a variety of generative models, including Variational Autoencoders (VAEs) (Kingma and Welling, 2014), GANs, Normalizing Flows (Rezende and Mohamed, 2015), and other deep"}, {"title": "6 Conclusion", "content": "In this work, we explored and demonstrated the application of Reinforcement Learning (RL) as a novel approach for instrument design, showcasing its effectiveness through two distinct empirical studies. The first study focused on the design of a calorimeter, while the second targeted the design of a spectrometer. Both studies utilized a RL agent that employed a mixed action space framework to construct these instruments layer by layer, simulating a practical, iterative design process. This mixed action space combined continuous and discrete actions to account for the diverse aspects of the design process, thereby allowing the agent to make comprehensive decisions.\nIn the context of the calorimeter and spectrometer designs, the continuous actions allowed the agent to determine optimal placements of the detector components (i.e. active layers or tracking stations). These components are integral to the functionality of the instruments, influencing factors such as resolution and detection efficiency. On the other hand, the discrete actions enabled the agent to make categorical decisions about the physical properties of these components, such as the granularity of the sensor layers or the thickness of the detectors. By combining these two types of actions, the RL agent could effectively navigate the complex and multidimensional design space without requiring extensive prior knowledge or predefined templates.\nWe employed straightforward yet effective reward functions to guide the agent during the design process. These reward functions quantitatively assessed the performance of the proposed designs based on relevant metrics, such as detection accuracy, energy resolution, or overall efficiency. The simplicity of these reward functions underscores the robustness and adaptability of our methodology. Remarkably, the RL agent was able to autonomously generate high-performing designs for both instruments, showcasing its ability to learn and optimize effectively even in the absence of significant domain-specific priors.\nThis study highlights the advantages of RL in addressing instrument design challenges, particularly when compared to differentiable optimization methods. Differentiable approaches typically require a well-defined, parameterized model of the instrument, which can limit their applicability in scenarios where such models are unavailable or difficult to construct. In contrast, our RL-based methodology circumvents these constraints by directly interacting with the design environment and placing sequentially. Further, through exploration and trial-and-error learning, the local minima are also effectively avoided. However, the differentiable approaches could still potentially complement the proposed method by fine-tuning the proposed design to make it final.\nLooking ahead, we see tremendous opportunities to extend this approach to even more intricate design challenges. Future research will aim to apply this methodology to the development of more complex instruments, potentially involving multi-objective optimization and additional constraints that better reflect real-world scenarios. By integrating advanced RL techniques, such as hierarchical RL or model-based approaches, we aspire to further enhance the capabilities and versatility of this design paradigm. These advancements will add tremendous value to future projects like the Future Circular Collider (FCC), where achieving unprecedented energy scales will require the most optimized instrumentation."}]}