{"title": "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents", "authors": ["Pranav Putta", "Edmund Mills", "Naman Garg", "Sumeet Motwani", "Chelsea Finn", "Divyansh Garg", "Rafael Rafailov"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this gap through supervised fine-tuning on curated expert demonstrations often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. We validate our approach in the WebShop environment, a simulated e-commerce platform-where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, our methodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.", "sections": [{"title": "1. Introduction", "content": "The recent advances in Large Language Models (LLMs) represent a significant leap in artificial intelligence. Frontier models like ChatGPT (John Schulman et al., 2022), Gemini (Anil et al., 2023), Opus (Anthropic, 2024), and LLaMA-3 (Touvron et al., 2023) demonstrate promising reasoning capabilities that approach average human performance in a number of domains. These breakthroughs have extended the utility of LLMs from traditional chat and text-based applications to more dynamic, agentic roles, in which they do not just generate text but can take actions autonomously in a number of environments including code and software engineering (Holt et al., 2024; Jimenez et al., 2024; Yang et al., 2024; Zhang et al., 2024d), device control (Chen and Li, 2024; Wang et al., 2024a; Zhang et al., 2023) and web applications (Deng et al., 2023; Gur et al., 2024; Hong et al., 2023; Lai et al., 2024a; Zhou et al., 2024b) among others. However, despite these advancements, significant challenges persist: LLMs still struggle to generalize effectively in interactive, multi-step environments, since they are not native trained for such applications. This is true, even for some of the strongest models of the current generation, such as GPT-4 (Achiam et al., 2023).\n\nA growing literature on agentic formulation seeks to address these issues; however these works mostly focus on building frameworks around prompt-based learning on existing models or limited fine-tuning on static datasets, and are thus limited by the base models' reasoning and decision making capabilities. Reasoning and planning have indeed been highlighted as core challenges for current LLMs. Since the seminal work on chain-of-thought reasoning (Wei et al., 2022), significant efforts have been made to improve these capabilities via prompt-based strategies (Kojima et al., 2022; Qiao et al., 2023;"}, {"title": "2. Related Work", "content": "Our work touches on a large number of research directions around agent design, self-improvement, reasoning and reinforcement learning. We include a short overview of related works from those various fields below."}, {"title": "2.1. Guided Search for Reasoning and Planning", "content": "The latest generation of Large Language Models (LLMs) have demonstrated promising emerging properties around reasoning and planning. Moreover such behaviours can be directly elicited from strong models only using simple prompting techniques (Kojima et al., 2022; Qiao et al., 2023; Wei et al., 2022). These have also become an integral part of agentic design (Yao et al., 2023b; Zhang et al., 2024c), which we also utilize for our approach. Another emerging research direction is based around step-by-step verifiers or \u201cProcess Reward Models\u201d (Lightman et al., 2023; Uesato et al., 2022), specifically for mathematical reasoning. These have shown to improve performance beyond purely outcome-based training, however they require a large amount of human effort to label individual steps. Some recent approaches have proposed self-supervised methods for step-level supervision (Hwang et al., 2024; Setlur et al., 2024a; Wang et al., 2024b). A number of concurrent works (Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e) have further explored tree-based search approaches (Yao et al., 2023a) in combination with DPO (Rafailov et al., 2023) training for math-based reasoning. These algorithms optimize actions at the node level, using different branches produced by the search algorithm to create preference pairs. Our approach shares similarities to the self-supervised search proposed in (Yao et al., 2023a) with a combination of AI-based feedback (Bai et al., 2022; Yuan et al., 2024) to guide intermediate search steps, but we are the first to scale this a realistic agent setting. Similar approaches were proposed in (Hao et al., 2023; Zhou et al., 2024a), and other works (Koh et al., 2024); however these works only use the base model's zero-shot capability and do not train it further. Moreover they are only evaluated on simulated environments. Beyond the search stage, our work further adopts the training methodology of (Tian et al., 2024; Xie et al., 2024; Zhang et al., 2024e), which significantly boosts our agent's zero-shot capabilities."}, {"title": "2.2. Web Agents", "content": "The strength and capabilities of recent pretrained Large Language (Vision) Models LL(V)Ms has significantly boosted progress in developing autonomous web-agents. Improved code understanding and long context have allowed agents to represent environment state and action space with document object model (DOM) allowing for deployment in complex and realistic domains. Moreover strong reasoning (Yao et al., 2023b) and planning (Liu et al., 2023; Zhang et al., 2024c) capabilities have also led to the development of a number of promising agents (Deng et al., 2023; Gur et al., 2024; Hong et al., 2023; Zhang and Zhang, 2023; Zhou et al., 2024b). Beyond using LL(V)Ms as plug-and-play planners/policies, recent works have sought to improve agentic-specific performance. Examples include online exploration (Zhang et al., 2024a), planning (Zhang et al., 2024b), error-correction (Wang et al., 2024a), and self- (Wu et al., 2024) or AI-critique (He et al., 2024; Pan et al., 2024). However, with small exceptions (Nakano et al., 2022) (which is still limited in scope) these agents mostly provide a framework around a strong pre-existing model like GPT4-V or deploy limited fine-tuning and adaptation. In this work we show that model training is crucial for continuous improvement. We combine a planning and reasoning agent with MCTS inference-time search and AI self-critique for self-supervised data collection, which we then use for RL type training."}, {"title": "2.3. Reinforcement Learning for LLMs and Agents", "content": "Reinforcement Learning has become a significant component of training modern generative AI systems (Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). Classical approaches have deployed the PPO algorithm (Schulman et al., 2017)\u2014or similar policy-gradient based methods\u2014and have even been scaled to autonomous web search agents (Nakano et al., 2022) as well as embodied applications with vision-language models (Zhai et al., 2024) (in simulation). However, these algorithms are challenging due to their complexity and the need for a high number of online samples from the model. This is especially prominent in potentially risky situations, such as autonomous agentic models that could make a number of impactful mistakes during training. Implicit Language Q-learning (Snell et al., 2022) and the Q-transformer (Chebotar et al., 2023) are offline RL algorithms (Levine et al., 2020) designed for auto-regressive transformer models, and hence can be safely trained on pre-collected datasets; however they have not been successfully scaled to modern LLMs. While these methods represent a token-level MDP, (Zhou et al., 2024c) has shown success formulating the RL problem at a step level and these ideas have recently been scaled to a general device-control agent (Bai et al., 2024). However, these algorithms still have high complexity and require auxiliary models, such as value functions, so instead in our approach we opt to use the Direct Preference Optimization (DPO) algorithm (Rafailov et al., 2023) due to it's simplicity and natural fit for the branching nature of tree-search based data."}, {"title": "3. Preliminaries", "content": "In this section we will outline the preliminaries of our agent training process."}, {"title": "3.1. Agent Formulation", "content": "We consider a general POMDP setup (O, S, A, T, R, \u03bc\u03bf, \u03b3) where O denotes the observation space, S the unobserved state space, A the action space, T(st+1|st, at) the transition distribution (in this case the dynamics of a web browser), R(s, a) the reward function (in this work we use sparse rewards of 1/0 representing success/failure), \u03bc\u03bf(so) the initial state distribution, and y the discount factor, which we set to 1. A POMDP is the most suitable framework to model web interactions for several reasons - first novel environments, which the agent is unfamiliar with require exploration in order to locate the task objective, consistent with the meta-reinforcement learning as task inference view Humplik et al. (2019). Moreover, the real web is dynamic, which creates partial observability of the current state each time the agent is deployed - i.e. it does not a priori know current booking availability before attempting to do it. We will outline the main parts of our web agent below.\n\nThe agent observation ot \u2208 O are commands/information given by the user and the web browser. The first observation o\u2081 is a user text instruction, such as\n\n\"Book reservation for restaurant Cecconi's on OpenTable for 4 people on May 22 2024 at 7:00 PM\"\n\nfor example and a browser home page. Subsequent observations consist of web pages from the browser, represented as a HTML DOM format. Occasionally for some tasks the agent might ask for confirmation/feedback from the user, which then also becomes part of the observation."}, {"title": "3.2. Fine-Tuning Language Models From Feedback", "content": "Classical approaches to RLHF in foundation models Ouyang et al. (2022); Stiennon et al. (2022) use the model as a policy \u03c0\u03b8 and optimize an objective of the form:\n\nEa~\u03c0\u03c1(ah) [r(a, h)] \u2013 BDKL[\u03c0\u03b8(a|h)||\u03c0ref(a|h)]\n\nwhere Tref is some reference policy (usually the initial model). The goal of this formulation is to optimize some target objective (expressed by the reward r(a, h)) while preventing out-of-distribution drift. This objective can be extended to multi-step agentic problems, where the model interacts with an external environment env such as in Nakano et al. (2021) which focuses on information retrieval using web navigation. In this case we use an objective of the kind\n\n\u0388\u03c0\u03bf, env\nr(at, ht)] \u2013 BDKL[To(at)|ht)||Tref(at|ht)]\n\nt\nClassical RLHF has used policy gradient type of algorithms, such as PPO Schulman et al. (2017), however, they are complex and require online data, which can be costly/dangerous to collect au- tonomously in the agent setting. While PPO has shown some success in prior web agent applications Nakano et al. (2021). The issues above largely make the approach not practical for general web tasks, beyond information retrieval. In this work we utilize some recent alternatives, outlined below."}, {"title": "3.2.1. Reinforced Fine-Tuning", "content": "Reinforced fine-tuning (RFT) algorithms Gulcehre et al. (2023); Singh et al. (2024); Yuan et al. (2023); Zelikman et al. (2022) have grown in popularity due to their simplicity and scalability. These methods aggregate data and filter out the sub-optimal samples based on some reward model or a verifier to construct a growing dataset of high-quality trajectories D. Given this dataset and a parameterized model \u03c0\u03b8 we can carry out standard supervised fine-tuning (SFT):\n\nL(\u03c0\u03bf, D) = \u2212ED \u03a3log \u03c0\u03c1(atht)\n\nIn this objective the divergence penalty is only applied implicitly by limiting the number of training rounds. While simple and relatively successful, empirically these methods tend to under-perform standard RL and alternatives Dubois et al. (2024); Setlur et al. (2024b); Tajwar et al. (2024) in the text generation domain, particularly in reasoning. We largely observe similar empirical results, and we use these methods mostly as baselines to build intuition."}, {"title": "3.2.2. Direct Preference Optimization", "content": "Direct Preference Optimization (DPO) Rafailov et al. (2023) is an offline RL Levine et al. (2020) alternative to the classical RLHF optimization pipeline. It is a suitable algorithm for agent fine-tuning, as it can use fully offline data and does not require online rollouts. The original formulation in the pure text generation setting considers feedback of pairwise comparisons (h, aw, a\u00b9), where s is a single prompt and aw and a\u00b9 are two responses with a\u2122 > a' indicating that aw is preferred over a\u00b9. The DPO objective then minimizes the following loss:\n\nLDPO(\u03c0\u03b8; D) = -E\u03c0\u03b8(awhw)\n-E(h,aw, a\u00b2)~D logo (Blog Tref (aw/hw).\n\u03c0\u03b8(ah)\nBlog Tref (ah)\n\nWhile the algorithm was developed in a bandit setting Hejna et al. (2024); Rafailov et al. (2024) have extended it to multi-turn settings with preferences over over trajectories. In our setting, we can directly utilize this objective as:\n\nLT-DPO(\u03c0\u03b8; D) = -E(\u03c4\u03c9,\u03c4\u03b9)~D log \u03c3\n\u03a3\u03b2log\n\u03c0\u03bf(ah)\nref(ah)\n\u03a3\u03b2log\nTref(ah)\n))]}\n\nt=0\nt=0\n\u03c0\u03bf(ah)\n))"}, {"title": "4. Preliminary Approach With Outcome Supervision", "content": "In this section we will outline preliminary experimental results, which will build the base understand- ing for our further experiments. We use the AgentOhana xLAM-v0.1-r model Zhang et al. (2024c), which is a fine-tune of a pre-trained Mixtral-8x7B-Instruct-v0.1 model Jiang et al. (2024) on a mix of agentic applications, including WebShop SFT data. We also incorporate the same agent configuration specified by the AgentLite Liu et al. (2024) work to ensure a fair comparison between our fine-tuned model and the xLAM base model performance. We evaluate all approaches on the WebShop environ- ment Yao et al. (2022), where the agent needs to find particular products by browsing a simulated web shop. The environment comes with a set of 12,087 pre-defined tasks (corresponding to specific products to find), which we split into a train set of 11,000 tasks, which we use for further agent fine-tuning and a set of 1,087 held-out tasks, which we use for zero-shot evaluation. We show success rates (exact product match) for different approaches in Fig. 3. The base xLAM-v0.1-r model achieves success rate of 28.6% on the test tasks. All other methods are based on outcome-based supervision"}, {"title": "5. Agent Search", "content": "As we discovered in the previous section, while training based on outcome supervision with DPO yields meaningful improvement, the model is still not able to match human performance due to it's limited exploration. In this section we will explore endowing the agent with additional search capability via MCTS."}, {"title": "5.1. Monte-Carlo Tree Search Over Web-Pages", "content": "The Monte Carlo Tree Search (MCTS) algorithm Kocsis and Szepesv\u00e1ri (2006) employed in this work follows closely the one in Hao et al. (2023) and consists of four phases: selection, expansion, simulation, and backpropagation. Each phase plays a critical role in balancing exploration and exploitation while iteratively refining the policy.\n\nWe formulate the web agent execution as tree search over web-pages. The state is represented as described in Section 3.1 and consist of the summary of the agent's history and the DOM tree of the current web-page. Unlike board games, such as Chess or Go Silver et al. (2017b) the complex web-agent action space we use is open-format and variable. Instead we will use the base model as an action-proposal distribution and sample a fixed amount of possible actions at each node (web-page). Once we select and execute an action in the browser we traverse the next web-page, which together with the updated history becomes the new node."}, {"title": "5.1.1. Action Selection With AI Process Supervision", "content": "The selection phase uses the Upper Confidence Bound (UCB1) formulation of MCTS also used by Hao et al. (2023) to select nodes which aims to balance exploration and exploitation. With some abuse of notation we will also denote the agent state with ht. We consider the value function Q(ht, a) which"}, {"title": "5.1.2. Expansion and Backtracking", "content": "Based on the preceding section, we select and execute an action in the browser environment to reach a new node (page). Beginning from the selected state node's trace, we roll out the trajectory using the current policy \u03c0\u0473 until a terminal state is reached. The environment returns a reward at the end of the trajectory, R, where R = 1 if the agent was successful and R = 0 otherwise. We then backpropagate this reward by updating the values of each node bottom up from the leaf node to the root as follows:\n\nQ(ht, a) \u2190\nQ(ht, a) N(ht, a) + R\nN(ht, a) +1\n\nN(ht, a) \u2190 N(ht, a\u012f) + 1\n\nEach state node tracks two values: Q(ht, a\u2084), the average reward for passing through state ht and choosing action a\u012f, and N(ht, a\u2084), the number of times this state action pair was visited during search"}, {"title": "5.2. Improving Zero-Shot Performance with Reinforcement Learning", "content": "Training large foundation models with offline Snell et al. (2022) or off-policy Chebotar et al. (2023) reinforcement learning at scale has still remained challenging. At the same time online (on-policy) reinforcement learning Ouyang et al. (2022); Stiennon et al. (2022) is not scalable to real interactive environments. Instead, we follow a line of recent works, which apply the DPO algorithm Rafailov et al. (2023, 2024) at the step level in multi-step reasoning problems in mathematical domains Chen et al. (2024); Hwang et al. (2024); Lai et al. (2024b); Lu et al. (2024); Setlur et al. (2024b); Xie et al. (2024); Zhang et al. (2024f). Our approach is most similar to Chen et al. (2024); Xie et al. (2024); Zhang et al. (2024f) who also use the branching nature of tree search to produce step-level preference pairs. We will also use this approach in our setting due to its simplicity, scalability and prior success in smaller scale (non-interactive) reasoning applications.\n\nWe will generate a dataset of preference pairs P = {ht, a, a} where we make sure both actions were explored. We then optimize the DPO objective in Eq. 5 on the node level. We will leverage a theoretical result below to guide the construction of these preferences. We can make a number of modifications to Theorem 6.1 from Setlur et al. (2024b) to incorporate the interactive nature of the web environment dynamics to obtain the following result:\n\nTheorem 1. Consider a policy that optimizes the objective in Eq. 3 on trajectories generated by Tref and that at each node ht we have preferences generated accordingly to p(a > aht) xo(Q(ht, a) -"}, {"title": "5.3. Full WebShop Results", "content": "The full range of results and baselines is shown in Figure 3. We see that equipping the agent with search capabilities at test time significantly boost success rates from 28.6% to 48.4% when using MCTS on top of the base xLAM-v0.1-r model, approaching close to the average human performance of 50.0% and significantly out-performing the zero-shot performance of the DPO model trained with outcome supervision. We further fine-tune the base model using the approach outlined in Algorithm 1, which yields an improvement of 0.9% over the base DPO model. Using MCTS on top of the trained Agent Q model further improves performance to 50.5% slightly out-performing the average human success rates. We find that the ability to search at test time is a significant paradigm shift from zero-shot agents, even with significant RL training. Furthermore, while dense-level supervision improves over purely outcome-based one, the improvement is modest on WebShop. This is because the environment requires relatively short trajectories, and the model is capable to learn credit assignment purely from outcome supervision. We will further explore more complex real world environment, which requires longer-range credit assignment."}, {"title": "6. Scaling To Real World Websites", "content": "In this section we will investigate scaling the Agent Q framework to real use cases on live websites, in particular bookings on OpenTable. We carried out initial experiments with the xLAM-v0.1-r model, which proved to weak for the task achieving an initial success rate of 0.0%. Instead we shifted to the LLaMa 70B Instruct model, which was able to achive some non-trivial initial success."}, {"title": "6.1. The OpenTable Environment", "content": "In OpenTable, the agent is tasked with booking a restaurant reservation for a user. The agent must find a restaurant page on the OpenTable site, look for a reservation at a certain date and time, choose"}, {"title": "6.2. Results On OpenTable", "content": "The base xLAM-v0.1-r model achieves a success rate of 0.0%, largely from failing to follow instructions for the general web navigation instructions used for live websites, contrary to the simplified observation and action space used in WebShop. We instead initialize the base policy with the LLaMa-3 70B Instruct model, which achieves a zero-shot success rate of 18.6%. We do a single round of RFT on 600 successful trajectories which improves the success rate to 67.2% already out-performing the the GPT-40 model zero-shot performance with a success rate of 62.6%. For all other baselines we adopt the RFT model as the reference policy, due to the relatively low success rate of original LLaMa 3 70B Instruct model.\n\nIn this environment, training with outcome-supervision only DPO further improves performance by 4.6% to 71.8% but significantly under-performs the full Agent Q pipeline which achieves a zero-shot success rate of 81.7% We hypothesizes that this is due to the fact that OpenTable is a significantly more challenging environment, which requires almost twice as many steps to complete as WebShop, so the agent benefits from fine-grained supervision and credit assignment. We further ablate the role of the intermediate AI feedback process supervision during training as outlined in Eq. 10 and use MCTS with online Q values computed from outcome rewards only. This setting still outperforms training with trajectory-level DPO (75.2% versus 71.8%) likely due to the more fine-grained credit assignment that the branching tree search provides to the agent. However, zero-shot performance is still meaningfully worse than using intermediate process-level supervision and the full Agent Q achieves 6.5% higher success rate at 81.7%."}, {"title": "7. Discussion", "content": "In this work we developed algorithms for autonomous improvement of web-agents with limited human supervision. While most prior works build frameworks around existing models without additional training, we specifically seek to fine-tune pre-trained models for web navigation tasks based on synthetic reasoning and search data. While we achieve significant improvement in model capabilities on our target domain, many research questions remain."}, {"title": "Design of reasoning algorithms.", "content": "The core challenge for our web agents is the weak reasoning capabilities, which limit the agent's exploration and search strategy. In our approach we used process- level supervision from a separate critic model, which we prompt to rank possible agent actions. This is in contrast to works in mathematical reasoning where PRMs are usually trained to classify the correctness of individual steps Lightman et al. (2023), while other agent works Koh et al. (2024) have prompted models as zero-shot value functions. Furthermore, while we spent significant effort in training the agent policy, we maintain a frozen critic, which would likely also benefit from additional fine-tuning. We defer exploration of these design choices to further work."}, {"title": "Choice of search algorithm.", "content": "We used MCTS search due to the approach's prior success in mathemat- ical and code reasoning tasks. However, agent models executing MCTS on live environments might require significant number of risky interactions and a different search strategy might be more suitable. Recent works such as Gandhi et al. (2024); Lehnert et al. (2024) have even suggested directly learning to optimally search and explore in reasoning tasks using meta-reinforcement learning. We believe this is a promising research direction for autonomous agents, which we will pursue in further work."}, {"title": "Discrepancy between zero-shot vs search results.", "content": "Similar to some recent works that focus on code and reasoning, we observe significant gap between zero-shot agent performance and performance of the agent equipped with search capabilities Brown et al. (2024); Snell et al. (2024). Investigating these trade-offs at scale and the potential effect of different search/optimization approaches."}, {"title": "Online safety and interaction.", "content": "The design of agent Q allows for largely autonomous exploration, self-evaluation and improvement with limited human intervention. However, the agent might make a significant number of mistakes in it's search process which might be difficult to fix/reverse, especially for safety-critical online transactions, such as communications/email, payments, filings etc. This limits the scope of websites that Agent Q can be safely deployed and we might require additional safety critics and human-in-the-loop training setups."}]}