{"title": "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents", "authors": ["Pranav Putta", "Edmund Mills", "Naman Garg", "Sumeet Motwani", "Chelsea Finn", "Divyansh Garg", "Rafael Rafailov"], "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex\nreasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a\ndifficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous\nagent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous\nattempts to bridge this gap through supervised fine-tuning on curated expert demonstrations often suffer from\ncompounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome\nthese challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search\nwith a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the\nDirect Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both\nsuccessful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning\ntasks. We validate our approach in the WebShop environment, a simulated e-commerce platform-where it\nconsistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human\nperformance when equipped with the capability to do online search. In real-world booking scenarios, our\nmethodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340%\nrelative increase) after a single day of data collection and further to 95.4% with online search. We believe\nthis represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more\nsophisticated and reliable decision-making in real-world settings.", "sections": [{"title": "1. Introduction", "content": "The recent advances in Large Language Models (LLMs) represent a significant leap in artificial\nintelligence. Frontier models like ChatGPT (John Schulman et al., 2022), Gemini (Anil et al., 2023),\nOpus (Anthropic, 2024), and LLaMA-3 (Touvron et al., 2023) demonstrate promising reasoning\ncapabilities that approach average human performance in a number of domains. These breakthroughs\nhave extended the utility of LLMs from traditional chat and text-based applications to more dynamic,\nagentic roles, in which they do not just generate text but can take actions autonomously in a number\nof environments including code and software engineering (Holt et al., 2024; Jimenez et al., 2024;\nYang et al., 2024; Zhang et al., 2024d), device control (Chen and Li, 2024; Wang et al., 2024a;\nZhang et al., 2023) and web applications (Deng et al., 2023; Gur et al., 2024; Hong et al., 2023; Lai\net al., 2024a; Zhou et al., 2024b) among others. However, despite these advancements, significant\nchallenges persist: LLMs still struggle to generalize effectively in interactive, multi-step environments,\nsince they are not native trained for such applications. This is true, even for some of the strongest\nmodels of the current generation, such as GPT-4 (Achiam et al., 2023).\nA growing literature on agentic formulation seeks to address these issues; however these works mostly\nfocus on building frameworks around prompt-based learning on existing models or limited fine-tuning\non static datasets, and are thus limited by the base models' reasoning and decision making capabilities.\nReasoning and planning have indeed been highlighted as core challenges for current LLMs. Since the\nseminal work on chain-of-thought reasoning (Wei et al., 2022), significant efforts have been made\nto improve these capabilities via prompt-based strategies (Kojima et al., 2022; Qiao et al., 2023;"}, {"title": "2. Related Work", "content": "Our work touches on a large number of research directions around agent design, self-improvement,\nreasoning and reinforcement learning. We include a short overview of related works from those\nvarious fields below."}, {"title": "2.1. Guided Search for Reasoning and Planning", "content": "The latest generation of Large Language Models (LLMs) have demonstrated promising emerging\nproperties around reasoning and planning. Moreover such behaviours can be directly elicited from\nstrong models only using simple prompting techniques (Kojima et al., 2022; Qiao et al., 2023; Wei\net al., 2022). These have also become an integral part of agentic design (Yao et al., 2023b; Zhang\net al., 2024c), which we also utilize for our approach. Another emerging research direction is based\naround step-by-step verifiers or \u201cProcess Reward Models\u201d (Lightman et al., 2023; Uesato et al., 2022),\nspecifically for mathematical reasoning. These have shown to improve performance beyond purely\noutcome-based training, however they require a large amount of human effort to label individual\nsteps. Some recent approaches have proposed self-supervised methods for step-level supervision\n(Hwang et al., 2024; Setlur et al., 2024a; Wang et al., 2024b). A number of concurrent works (Tian\net al., 2024; Xie et al., 2024; Zhang et al., 2024e) have further explored tree-based search approaches"}, {"title": "2.2. Web Agents", "content": "The strength and capabilities of recent pretrained Large Language (Vision) Models LL(V)Ms has\nsignificantly boosted progress in developing autonomous web-agents. Improved code understanding\nand long context have allowed agents to represent environment state and action space with document\nobject model (DOM) allowing for deployment in complex and realistic domains. Moreover strong\nreasoning (Yao et al., 2023b) and planning (Liu et al., 2023; Zhang et al., 2024c) capabilities have also\nled to the development of a number of promising agents (Deng et al., 2023; Gur et al., 2024; Hong\net al., 2023; Zhang and Zhang, 2023; Zhou et al., 2024b). Beyond using LL(V)Ms as plug-and-play\nplanners/policies, recent works have sought to improve agentic-specific performance. Examples\ninclude online exploration (Zhang et al., 2024a), planning (Zhang et al., 2024b), error-correction\n(Wang et al., 2024a), and self- (Wu et al., 2024) or AI-critique (He et al., 2024; Pan et al., 2024).\nHowever, with small exceptions (Nakano et al., 2022) (which is still limited in scope) these agents\nmostly provide a framework around a strong pre-existing model like GPT4-V or deploy limited\nfine-tuning and adaptation. In this work we show that model training is crucial for continuous\nimprovement. We combine a planning and reasoning agent with MCTS inference-time search and AI\nself-critique for self-supervised data collection, which we then use for RL type training."}, {"title": "2.3. Reinforcement Learning for LLMs and Agents", "content": "Reinforcement Learning has become a significant component of training modern generative AI systems\n(Bai et al., 2022; Ouyang et al., 2022; Touvron et al., 2023). Classical approaches have deployed the\nPPO algorithm (Schulman et al., 2017)\u2014or similar policy-gradient based methods\u2014and have even\nbeen scaled to autonomous web search agents (Nakano et al., 2022) as well as embodied applications\nwith vision-language models (Zhai et al., 2024) (in simulation). However, these algorithms are\nchallenging due to their complexity and the need for a high number of online samples from the\nmodel. This is especially prominent in potentially risky situations, such as autonomous agentic models\nthat could make a number of impactful mistakes during training. Implicit Language Q-learning\n(Snell et al., 2022) and the Q-transformer (Chebotar et al., 2023) are offline RL algorithms (Levine\net al., 2020) designed for auto-regressive transformer models, and hence can be safely trained on\npre-collected datasets; however they have not been successfully scaled to modern LLMs. While these\nmethods represent a token-level MDP, (Zhou et al., 2024c) has shown success formulating the RL\nproblem at a step level and these ideas have recently been scaled to a general device-control agent\n(Bai et al., 2024). However, these algorithms still have high complexity and require auxiliary models,\nsuch as value functions, so instead in our approach we opt to use the Direct Preference Optimization\n(DPO) algorithm (Rafailov et al., 2023) due to it's simplicity and natural fit for the branching nature\nof tree-search based data."}, {"title": "3. Preliminaries", "content": "In this section we will outline the preliminaries of our agent training process."}, {"title": "3.1. Agent Formulation", "content": "We consider a general POMDP setup (O, S, A, T, R, \\mu_o, \\gamma) where O denotes the observation space, S\nthe unobserved state space, A the action space, T(s_{t+1}|s_t, a_t) the transition distribution (in this case\nthe dynamics of a web browser), R(s, a) the reward function (in this work we use sparse rewards\nof 1/0 representing success/failure), \\mu_o(s_0) the initial state distribution, and \\gamma the discount factor,\nwhich we set to 1. A POMDP is the most suitable framework to model web interactions for several\nreasons - first novel environments, which the agent is unfamiliar with require exploration in order\nto locate the task objective, consistent with the meta-reinforcement learning as task inference view\nHumplik et al. (2019). Moreover, the real web is dynamic, which creates partial observability of\nthe current state each time the agent is deployed - i.e. it does not a priori know current booking\navailability before attempting to do it. We will outline the main parts of our web agent below.\nThe agent observation o_t \\in O are commands/information given by the user and the web browser.\nThe first observation o_1 is a user text instruction, such as\n\"Book reservation for restaurant Cecconi's on OpenTable for 4 people on May 22 2024 at 7:00 PM\"\nfor example and a browser home page. Subsequent observations consist of web pages from the\nbrowser, represented as a HTML DOM format. Occasionally for some tasks the agent might ask for\nconfirmation/feedback from the user, which then also becomes part of the observation."}, {"title": "3.2. Fine-Tuning Language Models From Feedback", "content": "Classical approaches to RLHF in foundation models Ouyang et al. (2022); Stiennon et al. (2022) use\nthe model as a policy \\pi_\\theta and optimize an objective of the form:\n\\mathbb{E}_{a \\sim \\pi_\\theta(a|h)}[r(a, h)] - \\beta D_{KL}[\\pi_\\theta(a|h) || \\pi_{ref}(a|h)]\n(2)"}, {"title": "3.2.1. Reinforced Fine-Tuning", "content": "Reinforced fine-tuning (RFT) algorithms Gulcehre et al. (2023); Singh et al. (2024); Yuan et al.\n(2023); Zelikman et al. (2022) have grown in popularity due to their simplicity and scalability. These\nmethods aggregate data and filter out the sub-optimal samples based on some reward model or\na verifier to construct a growing dataset of high-quality trajectories D. Given this dataset and a\nparameterized model \\pi_\\theta we can carry out standard supervised fine-tuning (SFT):\n\\mathcal{L}(\\pi_\\theta, D) = -\\mathbb{E}_{D}\\left[ \\sum_{t=1}^T log \\pi_\\theta(a_t|h_t) \\right]\n(4)\nIn this objective the divergence penalty is only applied implicitly by limiting the number of training\nrounds. While simple and relatively successful, empirically these methods tend to under-perform\nstandard RL and alternatives Dubois et al. (2024); Setlur et al. (2024b); Tajwar et al. (2024) in the\ntext generation domain, particularly in reasoning. We largely observe similar empirical results, and\nwe use these methods mostly as baselines to build intuition."}, {"title": "3.2.2. Direct Preference Optimization", "content": "Direct Preference Optimization (DPO) Rafailov et al. (2023) is an offline RL Levine et al. (2020)\nalternative to the classical RLHF optimization pipeline. It is a suitable algorithm for agent fine-tuning,\nas it can use fully offline data and does not require online rollouts. The original formulation in the\npure text generation setting considers feedback of pairwise comparisons (h, a^w, a^l), where s is a single\nprompt and a^w and a^l are two responses with a^w > a^l indicating that a^w is preferred over a^l. The\nDPO objective then minimizes the following loss:\n\\mathcal{L}_{DPO}(\\pi_\\theta; D) = -\\mathbb{E}_{(h, a^w, a^l)\\sim D} log \\left( \\sigma \\left( \\beta log \\frac{\\pi_\\theta(a^w|h)}{\\pi_{ref}(a^w|h)} -  log \\frac{\\pi_\\theta(a^l|h)}{\\pi_{ref}(a^l|h)} \\right) \\right)\n(5)\nWhile the algorithm was developed in a bandit setting Hejna et al. (2024); Rafailov et al. (2024)\nhave extended it to multi-turn settings with preferences over over trajectories. In our setting, we can\ndirectly utilize this objective as:\n\\mathcal{L}_{T-DPO}(\\pi_\\theta; D) = -\\mathbb{E}_{(\\tau^w, \\tau^l)\\sim D} log \\sigma \\left( \\sum_{t=0}^T \\beta log \\frac{\\pi_\\theta(a_t^w|h_t)}{\\pi_{ref}(a_t^w|h_t)} - \\sum_{t=0}^T \\beta log \\frac{\\pi_\\theta(a_t^l|h_t)}{\\pi_{ref}(a_t^l|h_t)}  \\right)\n(6)"}, {"title": "4. Preliminary Approach With Outcome Supervision", "content": "In this section we will outline preliminary experimental results, which will build the base understand-\ning for our further experiments. We use the AgentOhana xLAM-v0.1-r model Zhang et al. (2024c),\nwhich is a fine-tune of a pre-trained Mixtral-8x7B-Instruct-v0.1 model Jiang et al. (2024) on a mix of\nagentic applications, including WebShop SFT data. We also incorporate the same agent configuration\nspecified by the AgentLite Liu et al. (2024) work to ensure a fair comparison between our fine-tuned\nmodel and the xLAM base model performance. We evaluate all approaches on the WebShop environ-\nment Yao et al. (2022), where the agent needs to find particular products by browsing a simulated\nweb shop. The environment comes with a set of 12,087 pre-defined tasks (corresponding to specific\nproducts to find), which we split into a train set of 11,000 tasks, which we use for further agent\nfine-tuning and a set of 1,087 held-out tasks, which we use for zero-shot evaluation. We show success\nrates (exact product match) for different approaches in Fig. 3. The base xLAM-v0.1-r model achieves\nsuccess rate of 28.6% on the test tasks. All other methods are based on outcome-based supervision"}, {"title": "5. Agent Search", "content": "As we discovered in the previous section, while training based on outcome supervision with DPO\nyields meaningful improvement, the model is still not able to match human performance due to\nit's limited exploration. In this section we will explore endowing the agent with additional search\ncapability via MCTS."}, {"title": "5.1. Monte-Carlo Tree Search Over Web-Pages", "content": "The Monte Carlo Tree Search (MCTS) algorithm Kocsis and Szepesv\u00e1ri (2006) employed in this\nwork follows closely the one in Hao et al. (2023) and consists of four phases: selection, expansion,\nsimulation, and backpropagation. Each phase plays a critical role in balancing exploration and\nexploitation while iteratively refining the policy.\nWe formulate the web agent execution as tree search over web-pages. The state is represented as\ndescribed in Section 3.1 and consist of the summary of the agent's history and the DOM tree of\nthe current web-page. Unlike board games, such as Chess or Go Silver et al. (2017b) the complex\nweb-agent action space we use is open-format and variable. Instead we will use the base model as an\naction-proposal distribution and sample a fixed amount of possible actions at each node (web-page).\nOnce we select and execute an action in the browser we traverse the next web-page, which together\nwith the updated history becomes the new node."}, {"title": "5.1.1. Action Selection With AI Process Supervision", "content": "The selection phase uses the Upper Confidence Bound (UCB1) formulation of MCTS also used by Hao\net al. (2023) to select nodes which aims to balance exploration and exploitation. With some abuse of\nnotation we will also denote the agent state with h_t. We consider the value function Q(h_t, a) which"}, {"title": "5.1.2. Expansion and Backtracking", "content": "Based on the preceding section, we select and execute an action in the browser environment to reach\na new node (page). Beginning from the selected state node's trace, we roll out the trajectory using\nthe current policy \\pi_\\theta until a terminal state is reached. The environment returns a reward at the\nend of the trajectory, R, where R = 1 if the agent was successful and R = 0 otherwise. We then\nbackpropagate this reward by updating the values of each node bottom up from the leaf node to the\nroot as follows:\nQ(h_t, a) \\leftarrow \\frac{Q(h_t, a) \\cdot N(h_t, a) + R}{N(h_t, a) + 1}\nN(h_t, a) \\leftarrow N(h_t, a_i) + 1\n(8)\nEach state node tracks two values: Q(h_t, a_i), the average reward for passing through state h_t and\nchoosing action a_i, and N(h_t, a_i), the number of times this state action pair was visited during search"}, {"title": "5.2. Improving Zero-Shot Performance with Reinforcement Learning", "content": "Training large foundation models with offline Snell et al. (2022) or off-policy Chebotar et al. (2023)\nreinforcement learning at scale has still remained challenging. At the same time online (on-policy)\nreinforcement learning Ouyang et al. (2022); Stiennon et al. (2022) is not scalable to real interactive\nenvironments. Instead, we follow a line of recent works, which apply the DPO algorithm Rafailov\net al. (2023, 2024) at the step level in multi-step reasoning problems in mathematical domains Chen\net al. (2024); Hwang et al. (2024); Lai et al. (2024b); Lu et al. (2024); Setlur et al. (2024b); Xie\net al. (2024); Zhang et al. (2024f). Our approach is most similar to Chen et al. (2024); Xie et al.\n(2024); Zhang et al. (2024f) who also use the branching nature of tree search to produce step-level\npreference pairs. We will also use this approach in our setting due to its simplicity, scalability and\nprior success in smaller scale (non-interactive) reasoning applications.\nWe will generate a dataset of preference pairs P = {h_t, a^w, a^l} where we make sure both actions\nwere explored. We then optimize the DPO objective in Eq. 5 on the node level. We will leverage a\ntheoretical result below to guide the construction of these preferences. We can make a number of\nmodifications to Theorem 6.1 from Setlur et al. (2024b) to incorporate the interactive nature of the\nweb environment dynamics to obtain the following result:\nTheorem 1. Consider a policy that optimizes the objective in Eq. 3 on trajectories generated by \\pi_{ref}\nand that at each node h_t we have preferences generated accordingly to p(a > a |h_t) \\propto \\sigma(Q(h_t, a) -"}, {"title": "5.3. Full WebShop Results", "content": "The full range of results and baselines is shown in Figure 3. We see that equipping the agent with\nsearch capabilities at test time significantly boost success rates from 28.6% to 48.4% when using MCTS\non top of the base xLAM-v0.1-r model, approaching close to the average human performance of 50.0%\nand significantly out-performing the zero-shot performance of the DPO model trained with outcome\nsupervision. We further fine-tune the base model using the approach outlined in Algorithm 1, which\nyields an improvement of 0.9% over the base DPO model. Using MCTS on top of the trained Agent Q\nmodel further improves performance to 50.5% slightly out-performing the average human success\nrates. We find that the ability to search at test time is a significant paradigm shift from zero-shot\nagents, even with significant RL training. Furthermore, while dense-level supervision improves over\npurely outcome-based one, the improvement is modest on WebShop. This is because the environment\nrequires relatively short trajectories, and the model is capable to learn credit assignment purely from\noutcome supervision. We will further explore more complex real world environment, which requires\nlonger-range credit assignment."}, {"title": "6. Scaling To Real World Websites", "content": "In this section we will investigate scaling the Agent Q framework to real use cases on live websites, in\nparticular bookings on OpenTable. We carried out initial experiments with the xLAM-v0.1-r model,\nwhich proved to weak for the task achieving an initial success rate of 0.0%. Instead we shifted to the\nLLaMa 70B Instruct model, which was able to achive some non-trivial initial success."}, {"title": "6.1. The OpenTable Environment", "content": "In OpenTable, the agent is tasked with booking a restaurant reservation for a user. The agent must\nfind a restaurant page on the OpenTable site, look for a reservation at a certain date and time, choose"}, {"title": "6.2. Results On OpenTable", "content": "The base xLAM-v0.1-r model achieves a success rate of 0.0%, largely from failing to follow instructions\nfor the general web navigation instructions used for live websites, contrary to the simplified observation\nand action space used in WebShop. We instead initialize the base policy with the LLaMa-3 70B Instruct\nmodel, which achieves a zero-shot success rate of 18.6%. We do a single round of RFT on 600 successful\ntrajectories which improves the success rate to 67.2% already out-performing the the GPT-40 model\nzero-shot performance with a success rate of 62.6%. For all other baselines we adopt the RFT model\nas the reference policy, due to the relatively low success rate of original LLaMa 3 70B Instruct model.\nIn this environment, training with outcome-supervision only DPO further improves performance by\n4.6% to 71.8% but significantly under-performs the full Agent Q pipeline which achieves a zero-shot\nsuccess rate of 81.7% We hypothesizes that this is due to the fact that OpenTable is a significantly\nmore challenging environment, which requires almost twice as many steps to complete as WebShop,\nso the agent benefits from fine-grained supervision and credit assignment. We further ablate the\nrole of the intermediate AI feedback process supervision during training as outlined in Eq. 10 and\nuse MCTS with online Q values computed from outcome rewards only. This setting still outperforms\ntraining with trajectory-level DPO (75.2% versus 71.8%) likely due to the more fine-grained credit\nassignment that the branching tree search provides to the agent. However, zero-shot performance\nis still meaningfully worse than using intermediate process-level supervision and the full Agent Q\nachieves 6.5% higher success rate at 81.7%."}, {"title": "7. Discussion", "content": "In this work we developed algorithms for autonomous improvement of web-agents with limited human\nsupervision. While most prior works build frameworks around existing models without additional\ntraining, we specifically seek to fine-tune pre-trained models for web navigation tasks based on\nsynthetic reasoning and search data. While we achieve significant improvement in model capabilities\non our target domain, many research questions remain.\nDesign of reasoning algorithms. The core challenge for our web agents is the weak reasoning\ncapabilities, which limit the agent's exploration and search strategy. In our approach we used process-\nlevel supervision from a separate critic model, which we prompt to rank possible agent actions. This\nis in contrast to works in mathematical reasoning where PRMs are usually trained to classify the\ncorrectness of individual steps Lightman et al. (2023), while other agent works Koh et al. (2024)\nhave prompted models as zero-shot value functions. Furthermore, while we spent significant effort in\ntraining the agent policy, we maintain a frozen critic, which would likely also benefit from additional\nfine-tuning. We defer exploration of these design choices to further work.\nChoice of search algorithm. We used MCTS search due to the approach's prior success in mathemat-\nical and code reasoning tasks. However, agent models executing MCTS on live environments might\nrequire significant number of risky interactions and a different search strategy might be more suitable.\nRecent works such as Gandhi et al. (2024); Lehnert et al. (2024) have even suggested directly learning\nto optimally search and explore in reasoning tasks using meta-reinforcement learning. We believe\nthis is a promising research direction for autonomous agents, which we will pursue in further work.\nDiscrepancy between zero-shot vs search results. Similar to some recent works that focus on code\nand reasoning, we observe significant gap between zero-shot agent performance and performance of\nthe agent equipped with search capabilities Brown et al. (2024); Snell et al. (2024). Investigating\nthese trade-offs at scale and the potential effect of different search/optimization approaches.\nOnline safety and interaction. The design of agent Q allows for largely autonomous exploration,\nself-evaluation and improvement with limited human intervention. However, the agent might make a\nsignificant number of mistakes in it's search process which might be difficult to fix/reverse, especially\nfor safety-critical online transactions, such as communications/email, payments, filings etc. This limits\nthe scope of websites that Agent Q can be safely deployed and we might require additional safety\ncritics and human-in-the-loop training setups."}]}