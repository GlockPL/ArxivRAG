{"title": "Towards Robust Multimodal Sentiment Analysis with Incomplete Data", "authors": ["Haoyu Zhang", "Wenbin Wang", "Tianshu Yu"], "abstract": "The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an emerging direction seeking to tackle the issue of data incompleteness. Recognizing that the language modality typically contains dense sentiment information, we consider it as the dominant modality and present an innovative Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust MSA. The proposed LNLN features a dominant modality correction (DMC) module and dominant modality based multimodal learning (DMML) module, which enhances the model's robustness across various noise scenarios by ensuring the quality of dominant modality representations. Aside from the methodical design, we perform comprehensive experiments under random data missing scenarios, utilizing diverse and meaningful settings on several popular datasets (e.g., MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and fairness compared to existing evaluations in the literature. Empirically, LNLN consistently outperforms existing baselines, demonstrating superior performance across these challenging and extensive evaluation metrics. Our code will be available at: https://github.com/Haoyu-ha/LNLN.", "sections": [{"title": "Introduction", "content": "The field of Multimodal Sentiment Analysis (MSA) is at the vanguard of human sentiment identification by assimilating heterogeneous data types, such as video, audio, and language. Its applicability spans numerous fields, prominent among healthcare and human-computer interaction (Jiang et al., 2020). MSA has become essential in enhancing both the precision and the robustness of sentiment analysis by drawing sentiment cues from diverse perspectives.\nChanging trends in recent research have taken a turn towards modeling data in natural scenarios from laboratory conditions (Tsai et al., 2019; Hazarika et al., 2020; Yu et al., 2021; Zhang et al., 2023). The shift has created a wider application space in the real-world for MSA, though concerns arise due to problems like sensor failures and problems with Automatic Speech Recognition (ASR), leading to inconsistencies such as incomplete data in real-world deployment.\nNumerous impactful solutions have been proposed against this primary concern of incomplete data in multimodal sentiment analysis. For instance, Yuan et al. (2021) introduced a Transformer-based feature reconstruction mechanism, TFR-Net, aiming to strengthen the robustness of the"}, {"title": "Related Work", "content": ""}, {"title": "Multimodal Sentiment Analysis", "content": "Multimodal Sentiment Analysis (MSA) methods can be categorized into Context-based MSA and Noise-aware MSA, depending on the modeling approach. Most of previous works (Zadeh et al., 2017; Tsai et al., 2019; Mai et al., 2020; Hazarika et al., 2020; Liang et al., 2020; Rahman et al., 2020; Yu et al., 2021; Han et al., 2021; Lv et al., 2021; Yang et al., 2022; Guo et al., 2022; Zhang et al., 2023) can be classified to Context-based MSA. This line of work primarily focuses on learning unified multimodal representations by analyzing contextual relationships within or between modalities. For example, Zadeh et al. (2017) explore computing the relationships between different modalities using the Cartesian product. Tsai et al. (2019) utilize pairs of Transformers to model long dependencies between different modalities. Yu et al. (2021) propose generating pseudo-labels for each modality to further mine the information of consistency and discrepancy between different modalities.\nDespite these advances, context-based methods are usually suboptimal under varying levels of noise effects (e.g. random data missing). Several recent works (Mittal et al., 2020; Yuan et al., 2021, 2024; Li et al., 2024) have been proposed to tackle this issue. For example, Mittal et al. (2020) introduce a modality check module based on metric learning and Canonical Correlation Analysis (CCA) to identify the modality with greater noise. Yuan et al. (2024) propose learning a unified joint representation between constructed \u201coriginal-noisy\u201d instance pairs. Although there have been some advances in improving the model's robustness under noise scenarios, no extant method has provided a comprehensive and in-depth comparative analysis."}, {"title": "Robust Representation Learning in MSA", "content": "Context-based MSA and Noise-aware MSA differ in their approaches to robust representation learning. In Context-based MSA, robust representation learning typically relies on modeling intra- and inter-modality relationships. For instance, Hazarika et al. (2020) and Yang et al. (2022) apply feature disentanglement to each modality, modeling multimodal representations from multiple feature subspaces and perspectives. Yu et al. (2021) and Liang et al. (2020) explore self-supervised learning and semi-supervised learning to enhance multimodal representations, respectively. Tsai et al. (2019) and Rahman et al. (2020) introduce Transformer to learn the long dependencies of modalities. Zhang et al. (2023) devise a language-guided learning mechanism that uses modalities with more intensive sentiment cues to guide the learning of other modalities.\nIn contrast, Noise-aware MSA focuses more on perceiving and eliminating the noise present in the data. For example, For example, Mittal et al. (2020) design a modality check module based on metric learning and Canonical Correlation Analysis (CCA) to identify the modality with greater noise. Yuan et al. (2021) design a feature reconstruction network to predict the location of missing information in sequences and reconstruct it. Yuan et al. (2024) introduce adversarial learning (Goodfellow et al., 2014) to perceive and generate cleaner representations.\nIn our work, the LNLN belongs to the Noise-aware MSA category. Inspired by Zhang et al. (2023), we explore the capability of language-guided mechanisms in resisting noise and aim to provide new perspectives for the study of MSA in noisy scenarios."}, {"title": "Method", "content": ""}, {"title": "Overview", "content": "The overall pipeline for the proposed Language-dominated Noise-resistant Learning Network (LNLN) in robust multimodal sentiment analysis is illustrated in Figure 1. As depicted, a crucial initial step involves forming a multimodal input with random data missing, which sets the stage for LNLN training. Once the input is prepared, LNLN first utilizes an embedding layer to standardize the dimension of each modality, ensuring uniformity. Recognizing that language is the dominant modality in MSA (Zhang et al., 2023), a specially designed Dominant Modality Correction (DMC) module employs adversarial learning and a dynamic weighted enhancement strategy to mitigate noise impacts. This module first enhances the quality of the dominant feature computed from the language modality and then integrates them with the auxiliary modalities (visual and audio) in the dominant modality based multimodal learning (DMML) module for effective multimodal fusion and classfication. This process significantly bolsters LNLN's robustness against various noise levels. Moreover, to refine the network's capability for fine-grained sentiment analysis, a simple reconstructor is implemented to reconstruct missing data, further enhancing the system's robustness."}, {"title": "Input Construction and Multimodal Input", "content": "Given the challenges of random data missing, we have constructed data sets that simulate these conditions based on MOSI, MOSEI and SIMS datasets.\nRandom Data Missing. Following previous protocols (Yuan et al., 2021), for each modality in each sample, we randomly erased changing proportions of information (from 0% to 100%). Specifically, for visual and audio modalities, we fill the erased information with zeros. For language modality, we fill the erased information with [UNK], which indicates the unknown word in BERT (Kenton and Toutanova, 2019).\nMultimdoal Input. For each sample in the dataset, we incorporate data from three modalities: language, audio, and visual data. Consistent with previous works (Zhang et al., 2023), each modality is processed using widely-used tools: language data is encoded using BERT (Kenton and Toutanova, 2019), audio features are extracted through Librosa (McFee et al., 2015), and visual features are obtained using OpenFace (Baltrusaitis et al., 2018). These pre-processed inputs are represented as sequences, denoted by $U_m \\in \\mathbb{R}^{T_m\\times d_m}$, where $m \\in \\{l, v, a\\}$ represents the modality type (l for language, v for visual, a for audio), $T_m$ indicates the sequence length, and $d_m$ refers to the dimension of each modality's vector. With obtained U, we apply random data missing to $U^o$, thus forming the noise corrupted multimodal input $U_m^1 \\in \\mathbb{R}^{T_m\\times d_m}$."}, {"title": "Dominant Modality based Multimodal Learning", "content": "Inspired by previous work ALMT, we hypothesize that model robustness improves when the integrity of the dominant modality is preserved despite varying noise levels. We improve ALMT based on the designed DMC module and Reconstructor, thus implementing dominant modality based DMML module for sentiment analysis under random data missing scenarios. Here, we mainly introduce the parts that differ from ALMT. Further details available in Zhang et al. (2023).\nModality Embedding. For multimodal input $U_h$, we employ an Embedding Encoder $E_m$ with two Transformer encoder layers to extract and unify the feature. Each modality begins with a randomly initialized low-dimensional token $H_m^0 \\in \\mathbb{R}^{T\\times d_m}$. These tokens are then processed by the Transformer encoder layer, embedding essential modality information and producing unified features, represented as $H_m^1 \\in \\mathbb{R}^{T\\times d}$. The process is formalized by:\n$H_m^1 = E_m (concat (H_m^0, U_m))$,\nwhere $E_m()$ extracts features for each modality, and $concat(\\cdot)$ represents the concatenation operation.\nAdaptive Hyper-modality Learning. In the original ALMT, each Adaptive Hyper-modality Learning layer contains a Transformer and two multi-head attention (MHA) modules. These are applied to learn language representations at different scales and hyper-modality representations from visual and audio modalities, guided by the language modality. Considering the possibility of severe interference in the language modality (i.e. dominant modality) due to random data missing, we designed a Dominant Modality Correction (DMC) module to generate the proxy dominant feature $\\hat{H_1}$ and construct corrected dominate feature $H_l^i$ (more details can be found in Section 3.4). Specifically, the process of learning corrected dominated representation $H_l^i$ at different scales can be described as:\n$H_l^i = E_m (H_l^{i-1}),$\nwhere $i \\in \\{2,3\\}$ means the i-th layer of Adaptive Hyper-modality Learning module, $E_m (\\cdot)$ is the i-th Transformer encoder layer, $H_l^i \\in \\mathbb{R}^{T\\times d}$ is corrected dominated feature at different scale. To learn the hyper-modality representation, the corrected dominated feature and audio/visual features are used to calculate Query and Key/Value, respectively. Briefly, the process can be written as follows:\n$H_{hyper}^i = H_{hyper}^{i} + MHA(H_l^i, H_v^1) + MHA(H_l^i, H_a^1)$,\nwhere $MHA(\\cdot)$ represents multi-head attention, $H_{hyper}^i \\in \\mathbb{R}^{T\\times d}$ is the hyper-modality feature. Note that the feature $H_{hyper}^i \\in \\mathbb{R}^{T\\times d}$ is a random initialized vector."}, {"title": "Dominant Modality Correction", "content": "This module consists of two steps, i.e., completeness check of dominant modality and proxy dominant feature generation using adversarial learning (Ganin and Lempitsky, 2015; Goodfellow et al., 2014).\nCompleteness Check. We apply an encoder $E_{cc}$ that consists of a Transformer encoder with the depth of two layers and a classifier for completeness check. For example, if the missing rate of dominate modality is 0.3, the label of completeness is 0.7. This completeness prediction $w$ can be obtained as follows:\n$w = E_{cc} (Concat (H_{cc}, H_l^1)),$\nwhere $H_{cc} \\in \\mathbb{R}^{T\\times d}$ is a randomly initialized token for completeness prediction. We optimize this process using L2 loss:\n$L_{cc} = \\frac{1}{N_b} \\sum_{k=0}^{N_b} |w_k - \\hat{w_k}|^2,$\nwhere $N_b$ is the number of samples in the training set, $\\hat{w_k}$ is the label of completeness of k-th sample.\nProxy Dominant Feature Generation. With the randomly initialized feature $H_D^0 \\in \\mathbb{R}^{T\\times d}$, visual feature $H_v^1$ and audio feature $H_a^1$, we employ a Proxy Dominant Feature Generator $E_{DFG}$, which consists of two Transformer encoder layers. This setup generates the proxy dominant features $\\hat{H} \\in \\mathbb{R}^{T\\times d}$, designed to complement and correct the dominant modality. The corrected dominant feature $H_l \\in \\mathbb{R}^{T\\times d}$ is calculated by combining $\\hat{H}$ and the language feature $H_l^1$, weighted by the predicted completeness w:\n$\\hat{H} = E_{DFG} (Concat (H_D^0, H_v^1, H_a^1), \\theta_{DFG}),$\n$H_l^{prox} = (1 - w) * H_l^1 + w * \\hat{H},\\$\nwhere $\\theta_{DFG}$ denotes the parameters of the Proxy Dominant Feature Generator $E_{DFG}$.\nTo ensure that the agent feature offers a distinct perspective from the visual and audio features, we utilize an effectiveness discriminator D. This discriminator includes a binary classifier and a Gradient Reverse Layer (GRL) (Ganin and Lempitsky, 2015) and is tasked with identifying the origin of the agent features:\n$\\hat{y_p} = D (H_l^{prox}/\\hat{H},\\theta_D),$\nwhere $\\theta_D$ represents the parameters of the effectiveness discriminator D, and $\\hat{y_p}$ indicates the prediction of whether the input feature originates from the language modality.\nIn practice, the generator and the discriminator engage in an adversarial learning structure. The discriminator aims to identify whether the features are derived from the language modality, while the generator's objective is to challenge the discriminator's ability to make accurate predictions. This dynamic is encapsulated in the adversarial learning objective:\n$\\min_{\\theta_D} \\max_{\\theta_{DFG}} L_{adv} = \\frac{1}{N_b} \\sum_{k=0}^{N_b} y_o^klog\\hat{y_p},$\nwhere $N_b$ is the number of samples in the training set, and $y_o^k$ indicates the label determining whether the input feature for the k-th sample originates from the visual or audio modality."}, {"title": "Reconstructor", "content": "Our experiments demonstrate that reconstructing missing information can significantly enhance regression metrics. More details about this are shown in Table 3. To address this, we have developed"}, {"title": "Overall Learning Objectives", "content": "To sum up, our method involves four learning objectives, including a completeness check loss $L_{cc}$, an adversarial learning loss $L_{adv}$ for proxy dominant feature generation, a reconstruction loss $L_{rec}$ and one final sentiment prediction loss $L_{sp}$. The sentiment prediction loss $L_{sp}$ can be described as:\n$L_{sp} = \\frac{1}{N_b} \\sum_{n=0}^{N} |y^k - \\hat{y}^k|^2,$\nTherefore, the overall loss $\\mathcal{L}$ can be written as:\n$\\mathcal{L} = \\alpha L_{cc} + \\beta L_{adv} + \\gamma L_{rec} + \\delta L_{sp},$\nwhere $\\alpha, \\beta, \\gamma$ and $\\delta$ are hyper-parameters. On MOSI and MOSEI datasets, we empirically set them to 0.9, 0.8, 0.1, and 1.0, respectively. On SIMS dataset, we empirically set them to 0.9, 0.6, 0.1, and 1.0, respectively."}, {"title": "Experiments and Analysis", "content": "In this section, we provide a comprehensive and fair comparison between the proposed LNLN and previous state-of-the-art MSA methods on MOSI (Zadeh et al., 2016), MOSEI (Zadeh et al., 2018) and SIMS (Yu et al., 2020) datasets. 1) For each method, we select three random seeds for training and evaluation. 2) We report the performance of the model on a variety of evaluation metrics, which is much more comprehensive compared to some of the previous work."}, {"title": "Datasets", "content": "MOSI. The dataset includes 2,199 multimodal samples, integrating visual, audio, and language modalities. It is divided into a training set of 1,284 samples, a validation set of 229 samples, and a test set of 686 samples. Every single sample has been given a sentiment score, varying from -3, indicating strongly negative sentiment, to 3, signifying strongly positive sentiment.\nMOSEI. The dataset consists of 22,856 video clips sourced from YouTube. The sample is divided into 16,326 clips for training, 1,871 for validation, and 4,659 for testing. Each clip is carefully labeled with a sentiment score, ranging from -3, representing the strongly negative sentiment, to 3, denoting the strongly positive sentiment.\nSIMS. The dataset is a Chinese multimodal sentiment dataset that includes 2,281 video clips sourced from different movies and TV series. It has been partitioned into 1,368 samples for training, 456 for validation, and 457 for testing. Each sample has been manually annotated with a sentiment score ranging from -1 (negative) to 1 (positive)."}, {"title": "Evaluation Settings and Criteria", "content": "For a fair and comprehensive evaluation, we experiment ten times, setting the missing rates r to predefined values from 0 to 0.9 with an increment of 0.1. For instance, 50% of the information is randomly erased from each modality in the test data when r = 0.5. Unlike previous works (Yuan et al., 2021, 2024), we did not evaluate at r = 1.0, as this would imply complete data erasure from each modality, rendering the experiment non-informative. With the obtained results of each missing rate, we compute the average value as the model's overall performance under different levels of noise.\nFor evaluation criteria, we report the binary classification accuracy (Acc-2), the F1 score associated with Acc-2, and the mean absolute error (MAE). For Acc-2, we calculated accuracy and F1 in two ways: negative/non-negative and negative/positive on the MOSI and MOSEI datasets, respectively. Additionally, we provide the three-class accuracy (Acc-3), the seven-class accuracy (Acc-7), and the correlation of the model's prediction with humans (Corr) on the MOSI dataset. For the SIMS dataset, we report Acc-3, the five-class accuracy (Acc-5), and Corr."}, {"title": "Robustness Comparison", "content": "Tables 1 and 2 show the robustness evaluation results on the MOSI, MOSEI, and SIMS datasets. As shown in Table 1, LNLN achieves state-of-the-art performance on most metrics. For example, on the MOSI dataset, LNLN achieved a relative improvement of 9.46% on Acc-7 compared to the sub-optimal result obtained by MMIM, demonstrating the robustness of LNLN in the face of different noise effects. However, on the MOSEI dataset, LNLN achieves only sub-optimal performance on metrics such as Acc-7 and Acc-5. After analyzing the data distribution (see Appendix B.3) and the confusion matrix (see Appendix B.5), we believe that this is due to the fact that LNLN does not overly bias the neutral category, which has a disproportionate number of samples in noisy scenarios.\nAs shown in Table 2, LNLN achieves a remarkable improvement in F1 on the SIMS dataset, with a relative improvement of 4.23% on F1 compared to the sub-optimal result obtained by ALMT. Similar to CENET's performance on the MOSEI dataset, TETFN on the SIMS dataset has a strong tendency to predict inputs as weakly negative categories with high sample sizes, resulting in seemingly better performance on some metrics.\nAdditionally, as shown in Figure 2, we present the performance curves of several advanced methods under varying missing rates. The results demonstrate that the proposed LNLN consistently outperforms others across most scenarios, showing its robustness under different missing rates.\nOverall, LNLN attempts to make predictions in the face of highly noisy inputs without the severe lazy behavior observed in other models, which often leads to predictions heavily biased towards a certain category. This demonstrates that LNLN shows strong robustness and competitiveness across various datasets and noise levels, highlighting its effectiveness in multimodal sentiment analysis."}, {"title": "Effects of Different Components", "content": "To verify the role of different modules, we present the ablation results after subtracting each component on the MOSI dataset separately. As shown in Table 3, a few metrics show an upward trend in performance when certain modules are removed. On one hand, we believe this is due to randomness in noisy scenarios. On the other hand, the same hyper-parameters cannot make the model perform optimally across all metrics."}, {"title": "Effects of Different Regularization", "content": "As shown in Table 4, we present the ablation results after subtracting each regularization on the MOSI dataset separately. Obviously, after each regularization is removed, LNLN shows a decrease in performance on most metrics. This demonstrates that regularization contributes to the learning of the model. Similar to the results in Table 3, there are a few metrics that improve after the removal of regularization. We believe this is mainly due to the difficulty of optimizing a single set of hyper-parameters to achieve the best performance across all metrics."}, {"title": "Effects of Different Modalities", "content": "To verify the role of different modalities, we present the ablation results after subtracting each modality from the MOSI dataset. As shown in Table 5, all modalities contribute to the final performance. Notably, when the language modality is removed, the performance of LNLN shows a significant decrease, while removing the visual and audio modalities results in a smaller decrease. This indicates that the language modality plays a crucial role in the MSA dataset.\nMoreover, we observe that LNLN converges on most metrics even when the visual and audio modalities are removed. This is due to the model's lazy behavior, where LNLN tends to fix its predictions to a certain categories. More details can be found in Appendix B.3 and B.5."}, {"title": "Case Study", "content": "As shown in Figure 3, we visualize several successful and failed predictions made by LNLN and ALMT from MOSI dataset for case study. It shows that LNLN can perceives sentiment cues in challenging samples, which demonstrates its ability to capture sentiment cues in noisy scenario. However, for inputs with high missing rates (e.g., the third example in the figure), both LNLN and ALMT fail to make correct predictions. We believe this is due to the high loss of valid information in the multimodal input, making accurate predictions difficult."}, {"title": "Discussion", "content": "In this work, we conduct a comprehensive evaluation of current advanced methods, the results of which have triggered some thoughts. Specifically, we find that all methods fail to accurately predict the inputs in high missing rate scenarios because most of them are designed specifically for complete data (see Appendix B.5 for more details). However, some methods show relatively more robust performance in low missing rate scenarios and some specific scenarios (see Appendix B.7), such as"}, {"title": "Conclusion and Future Work", "content": "In this paper, a novel Language-dominated Noise-resistant Learning Network (LNLN) is proposed to better learn sentiment cues for robust MSA. Due to the collaboration between Dominate Modality Correction (DMC) module and Dominant Modality based Multimodal Learning (DMML) module, and Reconstructor, LNLN can dynamically enhancing the dominant modality to achieve superior performance in data missing scenarios with varying levels. Extensive evaluation shows that none of the existing methods can effectively model the data with high missing rates. The related analyses can provide suggestions for other researchers to better handle robust MSA. In the future, we will focus on improving the generalization of the model to handle different types of scenes and varying intensities of noise effectively."}, {"title": "Limitations", "content": "We believe there are several limitations to the LNLN: 1) The LNLN achieves well performance in data missing scenarios, but not always better than all other methods in the modality missing scenarios, which demonstrates the lack of multi-scene generalization capability of the LNLN. 2) The data in real-world scenarios is much more complex. In addition to the presence of missing data, other factors need to be considered, such as diverse cultural contexts, varying user behavior patterns, and the influence of platform-specific features on the data. These factors can introduce additional noise and variability, which may require further model adaptation and tuning to handle effectively. 3) Tuning the hyperparameters, particularly those related to the loss functions, can be challenging and may require more sophisticated methods to achieve optimal performance."}, {"title": "Implementation Details", "content": "We used PyTorch 2.2.1 to implement the method. The experiments were conducted on a PC with an AMD EPYC 7513 CPU at 2.6GHz and 1.0TB memory, and an NVIDIA Tesla A40 with 48GB memory. To ensure consistent and fair comparisons across all methods, we conducted each experiment three times using fixed random seeds of 1111, 1112, and 1113, respectively. Details of the key parameters used in the experiments are shown in Table 6.\nIn addition, the result of MISA, Self-MM, MMIM, CENET, TETFN, and TFR-Net is reproduced by the authors from open source code in the MMSA2 (Mao et al., 2022), which is a unified framework for MSA, using default hyper-parameters. The result of ALMT is reproduced by the authors from open source code on Github\u00b3 using default hyper-parameters."}, {"title": "Additional Experiments and Analysis", "content": ""}, {"title": "Effect of Regularization Weight on Model Performance", "content": "As shown in Table 7, we empirically tried different combinations of hyperparameters in the loss function. The results shows that the selected parameters can make the LNLN achieve the better performance in most metrics. This also demonstrates the effectiveness of the optimization objectives of LNLN."}, {"title": "Analysis of Model Stability", "content": "To evaluate the stability of the model, we selected the MOSI dataset for ablation experiments. Specifically, based on the overall performance in Table 1, we selected several representative methods to additionally compute the standard deviation. For each method, we first calculate the standard deviation of evaluation results for the three random seeds when the missing rate r ranges from 0 to 0.9, and then average the 10 sets of standard deviation. The final result is shown in Table 8. We can see that the standard deviation of these methods is not very large in most metrics, indicating that all these methods can guarantee stability in the presence of random noise. It should be noted that LNLN strikes a balance between overall performance and stability."}, {"title": "Analysis of Data Distribution", "content": "Figure 4 illustrates the data distribution on the MOSI, MOSEI, and SIMS datasets. Significant category imbalances can be seen across all datasets. Additionally, the distributions of the training set, validation set, and test set are not identical on the MOSI and MOSEI datasets. For example, as shown in Figure 4 (a), the percentage of weakly negative samples is 15.5% in the MOSI training set, while this category reaches 22.7% in the test set, representing the highest percentage among all categories."}, {"title": "Details of Robust Comparison", "content": "Tables 9 and 10 show the details of robust comparison on the MOSI, MOSEI, and SIMS datasets, respectively. We observe that Self-MM and TETFN have a clear advantage in many evaluation metrics when the missing rate r is low. As the missing rate r increases, LNLN shows a significant improvement in all evaluation metrics. This demonstrates LNLN's ability to effectively model data affected by noise of varying intensity. In general, models augmented with noisy data in training usually cannot achieve state-of-the-art performance when the noise is low due to differences in data distribution. It is also worthwhile to study in the future how to balance the performance of models under different levels of noise.\nIn addition, when r is high, many models converge in their performance on metrics such as Acc-7 and Acc-5 (see Appendix B.5). This is due to the fact that these models exhibit lazy behavior, tending to predict inputs as categories with a higher number of samples in the training set. In such cases, we believe that the models do not actually learn effective knowledge, even if their performance appears good."}, {"title": "Visualization of Confusion Matrix", "content": "To verify the effectiveness of the method, we visualized the confusion matrix of several representative methods on the MOSI, MOSEI, and SIMS datasets in Figure 5, Figure 6, and Figure 7, respectively. Obviously, as the missing rate r increases, the predictions of all methods on all datasets tend to favor certain categories. This phenomenon indicates that the models hardly learn useful knowledge for prediction but instead ensure high accuracy by being lazy. For example, on the MOSI dataset, Self-MM predicts all samples as weak negative when the missing rate is 0.9. Although the accuracy of Self-MM is high when the missing rate r is high, it does not demonstrate that the model is more robust.\nIn addition, TETFN and TFR-Net tend to randomly predict within the negative and weak negative categories under high noise, indicating that their lazy behavior is not as severe as methods like Self-MM, MMIM, and MISA. Unlike other methods, although LNLN also suffers from the impact of data missing, the model's lazy behavior is less severe. For example, at a missing rate of 0.9, the model is still attempting to predict the samples, and there is no case of predictions favoring a particular category to an extreme extent.\nFrom Figure 6 and Figure 7, we can see that a similar phenomenon also occurs in the MOSEI and SIMS datasets. These observations demonstrate that methods targeting complete data often fail when modeling highly incomplete data. Therefore, the design of methods specifically for modeling incomplete data is necessary."}, {"title": "Visualization of Representations", "content": "Figure 8 shows the language feature $H_l^1$ extracted without applying data missing and the corrected dominant feature $H_l^{prox}$ with different data missing rates. we can see that the model tends to learn $H_l^{prox}$ that is consistent with the distribution of the $H_l^1$, indicating that LNLN can complement the information of the language modality when data is missing. Additionally, when the missing rate r is 1.0 (no valid information for the input), it becomes difficult for the model to learn $H_l^{prox}$ that is consistent with the language feature. This also suggests that the generator is indeed trying to learn $H_l^{prox}$ for fusion in DMML, rather than just guessing."}, {"title": "Analysis of the generalization to modality missing scenarios", "content": "To explore the generalization of the method, we further evaluated it in the special case of data missing, i.e., modality missing scenarios. The overall performance of the model is shown in Table 11 and Table 12. The detailed performance of the model is shown in Table 13 and Table 14. It is evident that LNLN, ALMT, TETFN, and Self-MM all exhibit excellent generalization.\nWe believe that the Dominant Modality Correction (DMC) proposed in LNLN, the Adaptive Hyper-modality Learning (AHL) proposed in ALMT, and the Unimodal Label Generation Module (ULGM) proposed/used in Self-MM and TETFN deserve more research attention in the future. These modules and their corresponding ideas have significant reference value for achieving robust Multimodal Sentiment Analysis (MSA).\nHowever, we also found that many methods converge to the same value when the noise is high. This is similar to the phenomenon observed in previous data missing scenarios, where many models exhibited lazy behavior. Therefore, handling modality missing and improving model robustness in different noise scenarios remain critical challenges that need to be addressed in the field of MSA."}, {"title": "Social Impacts", "content": "Our proposed LNLN has a wide range of applications in real-world scenarios, such as healthcare and human-computer interaction. However, it might also be misused to monitor individuals based on their affections for illegal purposes, potentially posing negative social impacts."}, {"title": "Implementation Details", "content": "We used PyTorch 2.2.1 to implement the method. The experiments were conducted on a PC with an AMD EPYC 7513 CPU at 2.6GHz and 1.0TB memory, and an NVIDIA Tesla A40 with 48GB memory. To ensure consistent and fair comparisons across all methods, we conducted each experiment three times using fixed random seeds of 1111, 1112, and 1113, respectively. Details of the key parameters used in the experiments are shown in Table 6.\nIn addition, the result of MISA, Self-MM, MMIM, CENET, TETFN, and TFR-Net is reproduced by the authors from open source code in the MMSA2 (Mao et al., 2022), which is a unified framework for MSA, using default hyper-parameters. The result of ALMT is reproduced by the authors from open source code on Github\u00b3 using default hyper-parameters."}, {"title": "Additional Experiments and Analysis", "content": ""}, {"title": "Effect of Regularization Weight on Model Performance", "content": "As shown in Table 7, we empirically tried different combinations of hyperparameters in the loss function. The results shows that the selected parameters can make the LNLN achieve the better performance in most metrics. This also demonstrates the effectiveness of the optimization objectives of LNLN."}, {"title": "Analysis of Model Stability", "content": "To evaluate the stability of the model"}]}