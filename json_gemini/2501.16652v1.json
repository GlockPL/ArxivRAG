{"title": "Molecular-driven Foundation Model for Oncologic Pathology", "authors": ["Anurag Vaidya", "Andrew Zhang", "Guillaume Jaume", "Andrew H. Song", "Tong Ding", "Sophia J. Wagner", "Ming Y. Lu", "Paul Doucet", "Harry Robertson", "Cristina Almagro-P\u00e9rez", "Richard J. Chen", "Dina ElHarouni", "Georges Ayoub", "Connor Bossi", "Keith L. Ligon", "Georg Gerber", "Long Phi Le", "Faisal Mahmood"], "abstract": "Foundation models are reshaping computational pathology by enabling transfer learning, where models pre-trained on vast datasets can be adapted for downstream diagnostic, prognostic, and therapeutic response tasks. Despite these advances, foundation models are still limited in their ability to encode the entire gigapixel whole-slide images without additional training and often lack complementary multimodal data. Here, we introduce THREADS, a slide-level foundation model capable of generating universal representations of whole-slide images of any size. THREADS was pretrained using a multimodal learning approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained tissue sections, paired with corresponding genomic and transcriptomic profiles\u2014the largest such paired dataset to be used for foundation model development to date. This unique training paradigm enables THREADS to capture the tissue's underlying molecular composition, yielding powerful representations applicable to a wide array of downstream tasks. In extensive benchmarking across 54 oncology tasks, including clinical subtyping, grading, mutation prediction, immunohistochemistry status determination, treatment response prediction and survival prediction THREADS outperformed all baselines while demonstrating remarkable generalizability and label efficiency. It is particularly well-suited for predicting rare events, further emphasizing its clinical utility. We intend to make the model publicly available for the broader community.", "sections": [{"title": "Introduction", "content": "With the advancement of precision medicine and targeted therapies, problem statements in oncology focus increasingly on rare conditions and targeted populations. As research questions become more specific, the assumption of data abundance, which underpinned early successes in AI for pathology1,2, such as Gleason grading\u00b3 and metastasis detection, no longer applies. Many current problem statements in oncology, especially for patient prognostication, and treatment response prediction, involve small patient cohorts, frequently fewer than 100 patients. The limitation of data scarcity is compounded by the size of digitized tissue sections (whole-slide images, WSIs), which can be several gigabytes each. Consequently, most computational pathology predictive models operate in a scenario where the input data size vastly exceeds the number of available samples for model training, making model training incredibly complex.\nIn response to these challenges, numerous foundation models specifically designed for pathology have been developed5-7. These models enable transfer learning from large pretraining data with hundreds of thousands of WSIs and billions of cells to narrow applications, such as biomarker prediction. However, most of these models are patch encoders and, by design, are restricted to encoding small regions of interest, orders of magnitude smaller than clinical whole-slide imaging data, which can be several gigabytes. Existing models address the limitation by training an additional model, which can be computationally expensive to train and may require a lot of downstream labels. Addressing this limitation is critical for advancing foundation models in pathology to more varied tasks and overcoming the data abundance requirement. Some models have explored whole-slide image representation learning to derive off-the-shelf slide embeddings that can be used for various downstream tasks at minimal cost6,8-11. However, they remain limited in scope by the diversity of training data with organ and disease-specific models9,10, and the predictive capabilities of the resulting representations.\nHere, we introduce a new foundation model for pathology, THREADS, a general-purpose encoder model that can generate WSI embeddings. THREADS was pretrained through multimodal contrastive learning, where molecular profiles obtained with next-generation sequencing are used as a guide for learning the slide representation. We posit that molecular data brings a holistic and unbiased view of the tissue morphology that encapsulates biologically and clinically relevant information9,10. To train THREADS, we assembled the most extensive multimodal training dataset to date, named MBTG-47K, consisting of more than 47,000 samples. Each sample includes a WSI and its corresponding molecular profile obtained from an adjacent tissue section (Figure 1.a). MBTG-47K was curated from Massachusetts General Hospital (MGH, 6,899 samples, or 14.6%), Brigham and Women's Hospital (BWH, 20,556 samples or 43.6%), The Cancer Genome Atlas Program (TCGA, 10,209 samples or 21.6%), and the Genotype-Tissue Expression (GTEx, 9,507 samples or 20.2%) consortium12 (Extended Data Table 1). This pretraining strategy leads to a slide embedding space that encodes rich information about tissue morphology, disease, and composition (Figure 1.b).\nWe validate THREADS across a wide range of tasks in oncology, covering clinical tasks for cancer subtyping and grading, gene mutation prediction, immunohistochemistry status prediction, and treatment response and survival prediction (Figure 1.d). In total, our model is evaluated on 54 pathology tasks from 23 cohorts across 17 different sources. THREADS achieves state-of-the-art performance, significantly outperforming three whole-slide encoder models PRISM 13 (P-value<0.001), GIGAPATH 6 (P-value<0.001) and CHIEF 11 (P-value<0.001), and attention-based multiple instance learning classification baselines (P-value<0.001). THREADS can also serve as an effective initialization for additional model fine-tuning, which brings a significant improvement over training a model from scratch (P-value<0.001). This establishes THREADS as a foundational model that can drive AI advancements in histopathology."}, {"title": "Results", "content": "Whole-slide image classification with THREADS\nTHREADS design. THREADS consists of two components. An ROI encoder model (CONCHv1.5 15) consisting of a Vision Transformer-Large16,17 (ViT-L) model trained on millions of image patches via multimodal learning between ROIs and text captions; and a slide encoder that aggregates tile embeddings into a slide representation using attention-based modeling (Figure 1.c and Extended Data Figure 1). We use two types of next-generation sequencing (NGS) data for THREADS pretraining: transcriptomic profiles obtained with bulk RNA sequencing (MGH, TCGA, and GTEx samples), and genomic profiles capturing single nucleotide variants (SNVs), insertions and deletions (indels), and copy number variants (CNVs) of a targeted gene panel (BWH samples). The transcriptomic profiles are encoded using a single-cell foundation model pretrained on 5.7 million cells of various cancer types18, and the genomic profiles using a multi-layer perceptron model19. We employ cross-modal contrastive learning to align the slide representation with the corresponding molecular embedding. Additional information is provided in the Online Methods, Pretraining dataset curation.\nDownstream evaluation. We propose a large benchmark for assessing foundation models in hematoxylin and eosin (H&E)-stained whole-slide imaging. Our evaluation includes 54 tasks from 23 different cohorts, covering four families of tasks: clinical subtyping and grading (n=8 tasks, 20,427 WSIs), gene mutation prediction (n=21 tasks, 3,503 WSIs), immunohistochemistry (IHC) status prediction (n=12 tasks, 2,469 WSIs), and patient prognostication including treatment response and survival prediction (n=13 tasks, 2,857 WSIs). We curated tasks from a set of in-house data (n=12 tasks including 3,550 WSIs from three cohorts) and publicly available data (n=42 tasks including 23,161 WSIs from 17 cohorts). The diversity of tasks makes our benchmark suitable for assessing the predictive performance of slide encoders under different scenarios, from well-established clinical tasks with data abundance, such as colorectal cancer grading and breast cancer subtyping, to specific problem statements in treatment response prediction typically characterized by small patient cohorts. Our evaluation constitutes, to date, the most comprehensive benchmark introduced in computational pathology. All tasks follow a unified evaluation with either five-fold cross-validation into 80:20 splits or 50-train-test splits, depending on cohort size with label- and patient-stratified splits. An overview of each evaluation task is provided in Figure 1.d, with additional descriptions in the Online Methods, Downstream tasks and datasets, statistics for each task in Extended Data Table 2,3,4,5,6,7, and links to access public cohorts detailed in Extended Data Table 8.\nBaselines. We compare THREADS against three foundation models for encoding WSIs: PRISM, GigaPATH, and CHIEF. PRISM is based on the Virchow patch encoder (ViT-Huge, 632 million parameters) followed by a Perceiver model20 (45 million parameters) pretrained using contrastive learning with matched patient-level pathology reports (195,344 specimens). GIGAPATH is based on a ViT-Giant patch encoder (1.13 billion parameters) pretrained on 171,000+ WSIs (>30,000 patients) using DINOv221, and a LongNet22 slide encoder pretrained using masked autoencoding. Finally, CHIEF is based on the CTransPath23,24 patch encoder (Swin-Transformer, 28 million parameters) followed by an attention-based multiple instance learning (AB-MIL) model25 pretrained on 60,530 WSIs using contrastive learning with the tissue site. Additional information is provided in the Online Methods and Baselines.\nWe evaluate THREADS and baselines using linear probing (i.e., by learning a logistic regression model) to classify slide embeddings into the downstream task label. To prevent overfitting, we avoid hyperparameter tuning and set a fixed cost, number of iterations, and solver in linear probe models. We evaluate model performance using the area under the receiver operating characteristic (AUC) for all binary classification tasks,"}, {"title": "Linear probing", "content": "quadratic Cohen's kappa for grading, balanced accuracy for multi-class clinical subtyping, and concordance-index (c-index) for survival tasks. We provide additional information in the Online Methods, Evaluation metrics.\nLinear probing. THREADS provides state-of-the-art performance in linear probing evaluation. THREADS leads to an absolute performance gain over PRISM, GIGAPATH, and CHIEF of 6.3%, 9.9%, 6.7%, respectively (Figure 2.a). Employing a mixed-effects statistical model to compare the overall performance (Online Methods, Statistical analysis), we showed that THREADS significantly outperformed PRISM (P<0.001), CHIEF (P<0.001), and GIGAPATH (P<0.001). When investigating each family of tasks, THREADS demonstrates absolute performance improvements of 2.1% in clinical subtyping and grading over PRISM, the second-best model (P<0.001, Figure 2.b), 6.1% in mutation prediction over CHIEF (P<0.001, Figure 2.c), 4.6% in IHC status prediction over PRISM (P<0.01, Figure 2.d), and 8.9% in prognostication over PRISM (P<0.001, Figure 2.e). At the individual task level, THREADS outperforms PRISM in 44/54 tasks, CHIEF in 49/54 tasks, and GIGAPATH in 54/54 tasks. The performance per task is detailed in Extended Data tables 9 to 37.\nWhen investigating performance in diagnostic tasks (cancer subtyping and grading), THREADS with a linear model achieves performance levels that are competitive with specialist models3,26. For instance, THREADS reaches 98.3% AUC in breast cancer subtyping (invasive lobular carcinoma vs. invasive ductal carcinoma), and 98.2% AUC in lung cancer subtyping (lung adenocarcinoma vs. lung squamous cell carcinoma). In comparison, an attention-based MIL model25 trained on the same data with >1.5 million parameters reaches 98.3% and 98.0%, respectively (Extended Data Table 9 and 10). In colorectal cancer grading, THREADS with linear probing reaches 91.9% quadratic Cohen's kappa score, just 2.3% lower than training a dedicated ABMIL model (94.2%) (Extended Data Table 13). These findings underscore the extensive capabilities of THREADS in providing rich slide representations for clinical use.\nTo validate the superior performance of THREADS in linear probing evaluation, we additionally benchmarked THREADS and baselines with varying the regularization cost (Extended Data Figure 3 and Extended Data Table 38). THREADS provides significant performance gain over all baselines for all the regularization costs explored: 4.3% absolute performance gain over PRISM (second-best performer) with large regularization (C=0.01), and 5.7% over CHIEF (second-best performer) with small regularization (C=10). THREADS is also less sensitive to changes in regularization than baselines across all regularization strengths, showing the model's robustness and versatility."}, {"title": "Transferability of THREADS", "content": "Transferability of THREADS. We also investigated whether linear models trained with THREADS embedding show generalization properties when tested in external cohorts. To this end, we selected a subset of tasks from our evaluation pipeline for which we have an external test set. Specifically, we first train a linear probe classifier on the entirety of one dataset and test on the entirety of the external set. We studied transferability in six different types of cancer: prediction of BAP1 and PRMB1 mutations in clear cell renal cell carcinoma, IDH mutation in a cohort of glioblastoma and low-grade glioma, prediction of ER/PR status in invasive breast cancer, subtyping in lung cancer, and survival prediction in pancreatic adenocarcinoma. Overall, THREADS provides strong transferability properties that lead to significantly better performance than PRISM (P-value<0.001), GIGAPATH (P-value<0.001), and CHIEF (P-value<0.001) as shown in Extended Data Figure 5 and Extended Data Table 39. THREADS outperforms all baselines in 8/9 tasks (task-wise P-values <0.001 in 7/9 tasks in comparison to the second-best baseline). In lung and breast cancer subtyping, THREADS preserves a high predictive performance of 98.4% and 96.5% AUC, respectively, on the external cohort. Similarly, in ER and PR status prediction, THREADS leads to 88.5% and 79.4% AUC, maintaining high predictive performance. These results highlight the ability of THREADS to capture clinically and biologically relevant information without overfitting on cohort- and institution-specific features."}, {"title": "Data and label efficiency of THREADS", "content": "Data and label efficiency of THREADS\nAs pathology and oncology progress, problem statements become increasingly specific, resulting in scenarios that inherently face constraints in data availability. Such limitations are particularly prevalent in predicting patient treatment response and resistance. As part of our evaluation pipeline, we curated six treatment prediction tasks and one treatment assessment task covering several cancer types. Specifically, we tested THREADS to predict response in patients treated with temozolomide in glioblastoma (93 patients, Figure 2.f), bevacizumab in ovarian cancer (36 patients, Figure 2.g), hormonal therapy in prostate adenocarcinoma (53 patients, Figure 2.h), and neoadjuvant chemotherapy in high-grade serous ovarian cancer (183 patients, Extended Data Table 37), and platinum (and taxane for a subset) in ovarian metastasis of metastatic breast cancer (75 patients, Extended Data Table 36). THREADS also provides a tool for response assessment to detect signs of vascular invasion in patients with breast cancer treated with neoadjuvant chemotherapy (53 WSIs, Figure 2.i). THREADS provides better performance than baselines in all seven tasks, overall significantly outperforming baselines as assessed using a mixed-effect model statistical analysis (P-value<0.001, Figure 2.1). When considering individual tasks, THREADS significantly outperforms all baselines in 4/7 (P-value<0.05).\nTHREADS embeddings can also be used for patient survival prediction. We employ this approach for predicting overall survival in patients with pancreatic adenocarcinoma (Figure 2.k), colon and rectum adenocarcinoma (Figure 2.1), clear cell renal cell carcinoma (Extended Data Table 33), head and neck squamous carcinoma (Extended Data Table 34), and lung adenocarcinoma (Extended Data Table 32). Across all six survival tasks from our evaluation, THREADS provides the best predictive performance in five of them, overall providing significantly better performance than all baselines (P-value<0.001, Figure 2.m). The survival analysis using Kaplan Meier estimators also reveals the superior stratification capabilities of THREADS, which provide better separation between groups of patients considered as low and high risk than all baselines (Extended Data Figure 6).\nTo complement our analysis in data-scarce problem statements, we benchmarked THREADS and baselines in few-shot learning experiments, where we monitor the test performance when training on an increasing number of samples: $k$ = 1, 2, 4, 8, 16, 32, where $k$ is the number of training samples per class. We use the GBM-Treatment response dataset for treatment response prediction (Extended Data Figure 4.a and Extended Data Table 43)), EBRAINS dataset27 for fine-grained (n=30 classes) and coarse-grained (n=12) brain tumor subtyping (Figure 2.m, Extended Data Figure 4.b, and Extended Data Table 42), the BRACS dataset 28 for fine-grained (n=7) and coarse-grained (n=3) breast tumor subtyping (Extended Data Figure 4.c,d and Extended Data Table 41), and the BCNB dataset29 for ER status prediction (Extended Data Figure 4.e,f and Extended Data Table 40). THREADS provides the best linear probing performance, outperforming baselines for most values of $k$. The predictive capabilities of THREADS are particularly highlighted in subtyping rare brain tumors, where THREADS performance with k=4 is superior to PRISM performance (second best performer) with k=16 (Figure 2.n)."}, {"title": "THREADS fine-tuning", "content": "THREADS fine-tuning\nTHREADS can also serve as weight initialization for further finetuning on a downstream task. This approach combines the benefit of large-scale pretraining while letting the model adapt to the nuances of the downstream application. Here, we fine-tuned a THREADS-initialized model on every downstream task in our evaluation pipeline. We employ a unified fine-tuning recipe that is applied to all tasks (Online Methods, Baselines). To mitigate overfitting and costly hyperparameter searches, we did not apply layer-wise learning rate decay, weight decay, or gradient accumulation. We apply a similar strategy to fine-tune CHIEF. For GIGAPATH, we"}, {"title": "THREADS leads to significantly better performance than CHIEF fine-tuning", "content": "THREADS leads to significantly better performance than CHIEF fine-tuning (absolute gain of 17.9% and P-value<0.001 assessed with mixed-effects statistical modeling) and GIGAPATH (absolute gain of 7.3%, P-value<0.001) in our 54-task evaluation pipeline. When inspecting individual tasks, THREADS leads to significantly better performance than CHIEF and GIGAPATH in 54/54 tasks and 40/54 tasks (P-value<0.001), respectively (Figure 3.a). In addition, THREADS fine-tuning leads to a 4.3% absolute performance boost over an attention-based MIL baseline trained from scratch (P-value<0.001 assessed with mixed-effects modeling). THREADS leads to the largest performance gain in challenging tasks characterized by small to medium-size cohorts, such as mutation prediction (absolute gain of 5.5% over ABMIL and 7.7% over GIGAPATH, Figure 3.b), treatment response and survival prediction (gain of 4.3% over ABMIL and 9.4% over GIGAPATH, Figure 3.c) and IHC status prediction (gain of 4.5% over ABMIL and 9.8% over GIGAPATH, Figure 3.d). In clinical subtyping and grading tasks, characterized by larger cohorts, THREADS performance is comparable to GIGAPATH fine-tuning and ABMIL training Figure 3.e. Additional results for specific tasks are provided in Figure 3.f,g,h,i,j.\nWe additionally compared THREADS fine-tuning with a randomly initialized THREADS model. Overall, fine-tuning leads to an average absolute gain of 2.2% across all 54 tasks (P-value<0.001 assessed with mixed-effects statistical modeling as shown in Figure 3.k). In examining task performance across different families of tasks, we find that fine-tuning yields the most significant improvement, a 2.8% increase, in mutation prediction tasks, which typically involve challenging tasks and cohorts of small to medium size. Conversely, it shows the smallest improvement, a 0.5% increase, in clinical subtyping and grading, where the training cohorts tend to be larger and the tasks more objective. Additional results for specific tasks are provided in (Figure 3.1)."}, {"title": "Retrieval capabilities of THREADS", "content": "Retrieval capabilities of THREADS\nTHREADS is designed to provide off-the-shelf slide and patient embeddings. This property enables case and patient retrieval without additional model training or fine-tuning. To this end, we extract THREADS slide (and patient) embeddings for a collection of samples for which a diagnosis has already been made. We use these samples as a reference database to compare new query cases, which are first embedded using THREADS, and then compared to the $k$ most similar embeddings (Figure 3.a). The other three slide encoders (PRISM, GIGAPATH, and CHIEF) are processed and evaluated in a similar manner. We evaluate retrieval performance using mean Average Precision at $k$ (mAP@k), which measures the average number of relevant results within the top $k$ retrieved items, weighted by their rank and averaged over all queries.\nIn rare brain tumor retrieval assessed with the EBRAINS dataset (30 classes, n=2,319 cases), THREADS provides the best overall performance for all values of $k$, significantly outperforming all baselines (P-value<0.001 for 3/3 baselines at all $k$) (Figure 3.b). We additionally study retrieval performance on the CPTAC consortium data, which aggregates cases from 10 cancer types for a total of 2,115 slides. THREADS outperforms all three baselines for $k$ = 1,5, and 10 (P-value<0.05 for 3/3 baselines at mAP@1, P-value<0.001 for 2/3 baselines at mAP@5, and P-value<0.001 for 2/3 baselines at mAP@10) (Figure 3.c). These results highlight how THREADS can encode clinically relevant information and retrieve similar cases for comparison and investigation. Additional results are provided in Extended Data Table 45 and 44."}, {"title": "Molecular prompting with THREADS", "content": "A hallmark characteristic of multimodal foundation models is to enable transfer and generalization without task supervision. In vision-language models like CLIP 30 and CONCH 15, such capabilities include zero-shot classification, in which by formulating class labels (e.g., \u201cLung Adenocarcinoma\u201d, \u201cLung Squamous Cell Carcinoma\") as text prompts via natural language, tasks such as NSCLC subtyping can be performed without requiring training data. In THREADS, we introduce a novel multimodal capability known as \u201cmolecular prompting\u201d (Figure 4.d), in which canonical molecular profiles (e.g., molecular representations of their corresponding disease states) can be leveraged to perform clinical tasks without requiring any task-specific model development. To perform molecular prompting, for each class, we average the representations of molecular profile data from a support dataset (encoded using the molecular branch of THREADS) to create class-wise molecular prototypes, which can then be used for cross-modal slide retrieval and classification. At inference time, we classify a query WSI by assigning it to the class of the nearest molecular prompt based on L2 distance.\nWe evaluated molecular prompting across eight tasks, including clinical cancer subtyping, gene mutation and IHC status prediction, and prognostication (Figure 4.e). Classification with molecular prompts achieves competitive performance across diverse tasks. Building IDH wild-type and mutant prompts from TCGA-GBMLGG and testing on EBRAINS yields a high AUC of 0.960, comparable to linear probing with THREADS WSI embedding (0.961). Molecular prompts can also represent typical high- and low-risk profiles, allowing patient survival to be estimated based on similarity to these prompts (additional information is provided in Online Methods, Evaluation). For instance, high- and low-risk prompts generated from TCGA-CCRCC and applied to prognosis prediction on CPTAC-CCRCC achieve a competitive C-index of 0.687. Additional results can be found in Extended Data Table 46."}, {"title": "Insights into THREADS.", "content": "Insights into THREADS.\nScaling laws. We additionally study scaling laws in THREADS, building on existing works in foundation models that highlighted the benefits of larger training datasets and model sizes5,7. To this end, we pretrained THREADS using subsets of MBTG-47K of varying sizes. We sampled 1%, 5%,25%, 50%, and 75% of the data from each source, ensuring uniform sampling across major tissue sites. This resulted in the creation of MBTG-1 (473 histomolecular pairs), MBTG-5 (2,356 histomolecular pairs), MBTG-25 (11,791 histomolecular pairs), MBTG-50 (23,584 histomolecular pairs), and MBTG-75 (35,377 histo-molecular pairs). THREADS highlights a data scaling law, as shown in Figure 5.a. Across all tasks, we observe a +3.9% performance increase when using 1% to 100% of MBTG-47k. All families of tasks benefit from data scaling, with treatment response and survival prediction tasks showing the largest performance gain (+5.2%). When comparing THREADS against baselines, we also observe that our approach is more data-efficient than PRISM (trained on 195,344 specimens), GIGAPATH (trained on 171,189 whole-slide images), and CHIEF (trained on 60,530 whole-slide images). Additional information is provided in Extended Data Table 47.\nWe also assessed model scaling laws by ablating THREADS using a varying number of attention heads, resulting in models with a single head (5.0 million parameters in the slide encoder), two heads (proposed approach, 11.3 million parameters), four heads (19.7 million parameters), six heads (28.1 million parameters) and a ViT model with two Transformer layers (16.1 million parameters). We observe that model scaling peaks with a two-head model and then plateaus or leads to decreased performance (-1.0% when using six vs. two attention heads). A ViT baseline trained with THREADS leads to lower performance than our proposed architecture by 6.3%. Comparing THREADS against PRISM and GIGAPATH highlights the parameter-efficiency of THREADS. Despite being 4.0\u00d7 and 7.5\u00d7 smaller than PRISM and GIGAPATH slide encoders, THREADS leads to significantly better performance on our benchmark. CHIEF is lightweight due to its compact architecture but provides significantly lower performance than our single-head model. This analysis highlights that scaling model size in slide encoder does not necessarily lead to better performance and that other factors are more important."}, {"title": "Mean pooling", "content": "Mean pooling. To better understand the superior performance of THREADS over baselines, we conducted additional ablations. First, we compared the quality of THREADS patch embeddings (based on CONCHv1.5) against GIGAPATH patch encoder, PRISM patch encoder (based on VIRCHOW), and CHIEF patch encoder (based on CTransPath). To this end, we adopt mean pooling to derive a slide embedding, which we then use for linear probing classification. CONCHv1.5 with mean pooling reaches an average of 68.9% across all tasks outperforming VIRCHOW, GIGAPATH and CHIEF by 2.7%, 2.6% and 3.8%, respectively (Figure 5.a and Extended Data Table 47). We hypothesize that this gain stems from (i) vision-language fine-tuning in CONCHv1.5, whereas VIRCHOW, GIGAPATH, and CTRANSPATH are vision-only models, and (ii) from extracting patch features on larger regions (512\u00d7512-pixel regions vs. 256\u00d7256-pixel in baselines) which can better capture morphological context. We also note that CONCHV1.5 is a ViT-Large model (307 million vision parameters), whereas PRISM uses a ViT-H (632 million parameters, 2.0\u00d7 more than CONCHv1.5), and GIGAPATH used a ViT-G (1.13 billion parameters, 3.7\u00d7 more than CONCHv1.5), highlighting the parameter-efficiency of our pipeline."}, {"title": "Discussion", "content": "In this study, we introduced THREADS, a foundation model for pathology that can provide biologically and clinically relevant representations of H&E-stained whole-slide images. THREADS uses a novel multimodal pretraining strategy, where the learned slide representation is guided by its corresponding molecular profile. Using this strategy, the resulting slide representations can capture morphological features reflective of the underlying molecular composition of the tissue. THREADS was thoroughly tested on a wide benchmark of 54 tasks, covering four families of tasks: clinical cancer subtyping and grading, gene mutation prediction, immunohistochemistry status prediction, and treatment response and survival prediction. THREADS consistently shows state-of-the-art performance under several evaluation scenarios, including in- and out-of-domain generalization, few-shot learning, and case retrieval. Importantly, THREADS can reach clinical-grade performance on subtyping and grading tasks using simple linear models built upon our slide embeddings. THREADS also highlights great potential for patient outcome prediction and can help identify patients who will respond to certain treatments.\nTHREADS sets apart from existing methods using its unique pretraining strategy based on multimodal alignment with molecular profiles. Unlike PRISM 13, which relies on matching pathology reports, molecular data provide an unbiased, objective perspective on cellular and tissue states, free from the subjective influences inherent in written reports. On the other hand, CHIEF 11 and GIGAPATH 6 employ weaker pretraining signals, relying on contrastive alignment with tissue sites and masked autoencoding, respectively. We hypothesize that these approaches lack the capacity to capture the subtle morphological features essential for addressing most clinical tasks. Our investigation into scaling laws of THREADS further reveals that the saturation point of model and data scale remains an open question in slide representation learning. We found that simpler clinical tasks, such as cancer grading, do not benefit significantly from larger pretraining datasets. However, more challenging tasks-particularly those involving treatment response prediction and molecular predictions-show"}, {"title": "Substantial performance gains when models are trained on larger and more diverse datasets", "content": "Substantial performance gains when models are trained on larger and more diverse datasets. This indicates that data diversity and pretraining strategy are critical factors influencing the efficacy of the resulting model. THREADS is significantly smaller than existing models, being 7.5 \u00d7 smaller than GigaPath and 4.0 \u00d7 smaller than PRISM. This suggests that in slide representation learning, simply scaling model size may not be the most influencing factor for building general-purpose models. In addition, THREADS is trained on a highly diverse dataset that includes 39 main human tissue sites following the highest level of the OncoTree cancer classification system. In contrast, GIGAPATH, CHIEF, and PRISM are trained on less diverse tissue sites, often with a skewed distribution toward skin, breast, and lung cases. We hypothesize that the broader diversity in THREADS likely contributes to its enhanced generalizability and robustness across a wide range of tasks and tissue types.\nDespite these advancements, certain limitations remain. Although THREADS was pretrained on an unprecedented cohort of over 47,000 histomolecular pairs, it cannot encompass the full spectrum of molecular and morphological heterogeneity. As next-generation sequencing becomes more widely deployed in clinical settings, the potential to scale THREADS'pretraining dataset by orders of magnitude may reshape this landscape, potentially uncovering new scaling laws that are currently beyond reach with our existing cohorts. Additionally, extending our molecular-guided approach to include other molecular assays, such as immunohistochemistry and special stains, could broaden the scope of THREADS 10. In addition, THREADS architecture uses a multihead attention-based model, which treats patch embeddings independently. Attempts to replace our backbone with a Vision Transformer model fail to match the performance of simpler models, even the ones with a single attention head. The use of larger image patches (512 \u00d7 512 pixels) instead of the typical 256 \u00d7 256 pixels in most patch encoders may reduce the need for explicit context modeling. Alternatively, slide encoders based on Transformers may need a larger pretraining cohort size for significant performance improvements.\nTHREADS has the potential to impact various aspects of computational pathology and oncology. First, it can bring off-the-shelf integration in data-scarce scenarios. THREADS can be readily used to prototype new tasks and assess predictive performance at a minimal cost. In addition, the reduced training data requirements enable the development of clinical-grade predictive systems for rare diseases. Researchers and clinicians can also utilize THREADS pretrained weights as initialization for additional fine-tuning on specific tasks. This transfer learning approach can accelerate model development and can improve performance on specialized tasks, such as rare molecular alteration classification or treatment response and resistance prediction. Finally, the case retrieval capabilities of THREADS make it well-suited for identifying rare conditions in clinical settings. Overall, our study highlights the rich biological information contained in molecular assays, which can be transferred to slide encoders to advance the development of diagnostic and prognostic tools. Future work will focus on scaling the pretraining dataset size and increasing the biological richness of the training signal by including additional modalities."}, {"title": "Online Methods", "content": "The Mass General Brigham (MGB) institutional review board approved the retrospective analysis of pathology slides (whole-slide images or WSIs), corresponding next-generation sequencing (NGS) assays, and corresponding reports used in this study. Research conducted in this study involved a retrospective analysis of pathology slides and NGS assays, and the participants were not directly involved or recruited for the study. The requirement for informed consent to analyze archival pathology slides and NGS assays was waived. Before scanning and digitization, all pathology slides were de-identified to ensure anonymity. The sample sizes were determined by the availability of the data."}, {"title": "Pretraining dataset curation", "content": "We present THREADS pretraining dataset, MBTG-47K, a large and diverse dataset composed of paired formalin-fixed paraffin-embedded (FFPE) haematoxylin and eosin (H&E) whole-slide images (WSIs), tissue bulk RNA expression, and DNA variant data including single nucleotide variations (SNV), insertions and deletions (indels), and copy number"}]}