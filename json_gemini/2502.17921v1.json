{"title": "Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware Fairness", "authors": ["Tahsin Alamgir Kheya", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "abstract": "Recommendation systems are now an integral part of our daily lives. We rely on them for tasks such as discovering new movies, finding friends on social media, and connecting job seekers with relevant opportunities. Given their vital role, we must ensure these recommendations are free from societal stereotypes. Therefore, evaluating and addressing such biases in recommendation systems is crucial. Previous work evaluating the fairness of recommended items fails to capture certain nuances as they mainly focus on comparing performance metrics for different sensitive groups. In this paper, we introduce a set of comprehensive metrics for quantifying gender bias in recommendations. Specifically, we show the importance of evaluating fairness on a more granular level, which can be achieved using our metrics to capture gender bias using categories of recommended items like genres for movies. Furthermore, we show that employing a category-aware fairness metric as a regularization term along with the main recommendation loss during training can help effectively minimize bias in the models' output. We experiment on three real-world datasets, using five baseline models alongside two popular fairness-aware models, to show the effectiveness of our metrics in evaluating gender bias. Our metrics help provide an enhanced insight into bias in recommended items compared to previous metrics. Additionally, our results demonstrate how incorporating our regularization term significantly improves the fairness in recommendations for different categories without substantial degradation in overall recommendation performance.", "sections": [{"title": "1 Introduction", "content": "Recommender Systems (RS) personalize item selections for users, providing suggestions based on individual preferences and behav-iors. These systems have an important impact on our decision-making, as they are widely employed across diverse platforms like e-commerce, social media, streaming services, and news outlets, shaping the content and products we encounter. For several online platforms, recommendation systems help create an engaging expe-rience for users by diversifying and personalizing the content and interactions. This would help users avoid information overload and help them focus on options that reflect their past behaviors. Amid the promise held by RS, however, there are concerns about potential bias in these systems. For example, research has shown that, on certain recommender systems, simply changing the gender in a job search-while keeping qualifications constant-can significantly influence access to high-paying positions [11, 33].\nRS algorithms are traditionally evaluated using measures like RMSE (Root Mean Squared Error), NDCG (Normalized Discounted Cumulative Gain), precision, and diversity [61]. If only these met-rics are considered, then the recommended items are deemed good when they align with user preferences. These metrics can help evaluate the performance of the RS, but in recent years there is a growing emphasis on evaluating and ensuring fairness as well [1, 2, 16, 23, 43, 53]. For instance, the authors in [13] introduce a fairness evaluation metric that can be sensitive to different fair-ness notions like user-centric or item-centric. Although substantial work has been done in this field in recent years, there is a notable gap in research specifically focused on robust ways to quantify consumer bias accurately. Most of the metrics used are deficient in the following ways: (i) they over-simplify the concept of fair-ness, (ii) they fail to consider the ranking of recommended items, and (iii) they rely exclusively on a single type of fairness metric.\nA popular approach to evaluating consumer-side fairness in rec-ommendation systems involves an adaptation of equal opportunity (refer to Section: 2.2), which aims to balance performance metrics (such as recall) or utility scores across different groups, such as male vs. female users [43, 51, 52, 54, 63, 75]. However, while this approach is quite straightforward, it may overlook disparities in recommendations across different item categories. To assess these disparities in recommendations, we refer to Figure 1, where we present, for various recommendation algorithms, an analysis of the proportion of action and romance movies among the top 10 recommendations for male and female user groups, along with the corresponding Precision@10 values for each group. There are"}, {"title": "2 Related work", "content": "We categorize and discuss the related work existing in this field in the subsections below.\n2.1 Gender Fairness in RS\nGender bias in recommendation systems can exist as systematic discrepancies in the algorithms when recommending items to users of different genders. The challenges of gender bias can have a wide-ranging set of implications, including imbalanced representations of items for different genders, stereotypical recommendations (male-dominated occupations recommended to males more than females), and limitations in user personalization (being recommended items that are not related to users' preferences just because they are male or female). Research on evaluating and addressing gender bias in recommendation systems is an ongoing process. Melchiorre et al. [52] investigate the impact of a common de-biasing strategy called resampling on RS algorithms. This strategy marginally decreases gender bias, with a slight decrease in performance. The authors in [27, 74, 79] introduce ways to re-rank items to offer a balanced solution that caters to group fairness (for gender and other sensitive attributes) and user preferences. Historical data that can contain stereotypical movie preferences can intensify certain biases further when used to train traditional recommendation models. For instance, male users display a bias towards action movies, which is amplified by recommendation algorithms like UserKNN [64]. The authors in [3] introduce a framework that fairly predicts the quality of TedTalk speeches by using causal models and counterfactuals to mitigate gender and racial bias. Adversarial fairness where the recommender system is trained to not only make accurate predictions but also make it difficult for the adversary to guess the sensitive attribute, has also been employed to make such systems more fair"}, {"title": "2.2 Evaluating Gender Bias", "content": "For evaluating gender bias, the most common fairness definitions employed are the concepts of Demographic Parity and Equal Oppor-tunity, which are both related to group fairness. Fairness in this con-text, pertains to equitable treatment across different groups (which can be measured in classification and recommendation tasks). For group fairness, the idea is to ensure that the predicted outcomes Y of a model should not be dependent on sensitive attributes like gender S. For demographic parity, the proportions of each sensi-tive group (like male and female) receiving positive predictions should be equal. For binary classification, demographic parity can be formalized as:\n\\(P(\u0176 = 1|S = 1) = P(\u0176 = 1|S = 0)\\)\nEssentially what this implies is that the positive outcome should be the same for both genders, where 0 may represent male and 1 may represent female or vice versa. Equal Opportunity, on the other hand, holds when the model has equal true positive rates across different demographic groups [30]. This concept can be formalized as:\n\\(P(\u0176 = 1|S = 1, Y = 1) = P(\u0176 = 1|S = 0, Y = 1)\\)\nwhere Y represents the true outcome.\nBesides these two methods to quantify fairness, some additional concepts (as discussed in [40]) used include Equalized Odds [30], Balance for Negative Class [65], Balance for Positive Class [65], Intersectional Fairness [26], Equal Calibration [10] and Causal-based notions [3, 42]."}, {"title": "2.3 Evaluating Consumer-Side Fairness", "content": "In recommendation systems, fairness can be seen as a multi-sided concept and categorized into three groups: consumers (C-fairness) [13, 15, 25, 27, 29, 52, 66, 71] which is related to the impact of rec-ommendations of the system on protected classes of user, provider (P-fairness) which focuses on ensuring fairness for providers/sellers on a platform and both (CP-fairness) [8]. Our work focuses on the consumer-side fairness concept because we want to ensure that recommendations made by models are not biased against a certain gender. Prior research has shown how recommendations can differ in an unfair way based on sensitive attributes of users like gender, age, race, etc. [28, 38, 67]. Evaluating bias in these systems, before deploying is thus essential to stop the reinforcement of stereotypes and limiting diverse content. When quantifying consumer side bias in recommendation systems, the most common approach is to adopt the concept of equality of opportunity and focus on the differences in metrics such as recall, precision and/or NDCG [21, 43, 52]. Other papers also employ causal-based fairness notions [46, 68] and de-mographic parity [5, 18, 24]. Unlike these metrics, which assume fairness implies equality, [13] suggests how, for instance, paid users should be provided better recommendations when compared to free users. They design a set of metrics that can take this disparity into account and then measure fairness accordingly. Another interest-ing way to measure unfairness is the concept of envy-free fairness, which is achieved when no one user prefers another user's rec-ommendations way more significantly than their own [14]. While these metrics can identify consumer-side bias to an extent, they come with some limitations.\nLimitations of current metrics used to quantify consumer-side bias in recommendation systems include: (i) over-simplifying the meaning of fairness in RS, which employs various techniques like collaborative and content-based filtering and hybrid methods. Sim-ple notions can fail to capture disparities that exist across different types of items, for example, genres, when considering movie rec-ommendations. Some metrics that can fall prey to this oversimpli-fication issue include [13, 14, 19-21, 34, 43, 52, 69, 70, 77]; (ii) not utilizing ranks when evaluating recommendation quality can yield considerable issues. This is due to the fact that recommendations are displayed one after another, so the items on higher ranks must be more relevant to keep the user satisfied. Thus, capturing the qual-ity of recommendations using the ranks reflects a more complete way of evaluating models. Some metrics that can fall prey to this issue include [13, 14, 69, 70, 77]. It is important to state however, that even considering rank can give rise to positional bias, which refers to the tendency of users to favor the items that appear on top of a ranked list. Evaluating till a certain position like Recall@k can lead to a skewed sense of assessment of how the recommendation model performs. Additionally, metrics like MAP (Mean Average Precision), which normally treats relevance as a binary value (0: not relevant and 1: relevant) can also fail to capture the nuanced relevance that can come from items having multiple categories. So, using more than one metric (both with and without using ranks) to quantify bias is essential; (iii) relying only on one type of fairness metric can obscure underlying biases and give a false impression of fairness in recommendation systems.\nHence, using multiple metrics can help uncover hidden biases. Additionally, a model that is fair according to one metric can fail to hold other fairness metrics and risk overlooking subtle unfairness issues.\nWe want to highlight some of the works that have taken into account different classes when evaluating recommendations [24, 36, 37, 47, 58, 60, 62, 72]. For instance, [47] groups users on certain attributes and items by category, then measures preference ratio, which is the fraction of liked items by a group across categories. Next, they measure the bias disparity by taking the preference and recommended ratios' relative differences. This is close to our work but still doesn't account for the ranks of items and we evaluate the direct comparison of the recommendations for males and females. Additionally, [62] introduce calibrated recommendations, ensur-ing the recommended items align with user preferences without overemphasizing particular categories. The work by [24] uses a measure Skew@k to evaluate proportions of candidates based on sensitive attributes, and [36] uses a fairness metric called Attention Weighted Ranked Fairness (AWRF) [58] to ensure there is balance in exposure in different groups of providers. While both these works ensure group fairness, our work is more concentrated on evaluating the distribution of content categories for different groups. Unlike the work by [36] that focuses on provider-side fairness, we focus on consumer-side fairness."}, {"title": "3 Proposed Evaluation Metrics", "content": "3.1 Motivating Fairness Concern\nOur example is a typical offline setting recommendation system, which is trained using historical user and movie interactions. For our scenario, let us assume we have u\u2081, a male user who has watched numerous action and sci-fi movies, with some romance movies. We also have u2, a female user who has watched a lot of drama movies but also a few action movies. Let us say we decide to calculate overall precision for each user group (male and female) and then compare them to ensure the model's fairness.\n3.1.1 Potential Issue. Machine learning systems tend to learn and amplify bias from the training data [4, 12, 17, 40, 49]. Recommender systems are no different, as they can pick up on stereotypical user-item interactions and make biased recommendations based on sen-sitive attributes of users [52, 64]. Biased recommendations can have a broad impact on users if the models recommend content just because it aligns with certain stereotypes, for instance, males like action movies, and females like romance movies.\nThis imbalance in recommendation can lead to a situation in which users are only exposed to items that align with part of their preferences and stereotypical norms. This would potentially fil-ter out diverse content and prevent users from discovering new movies. As time passes, u\u2081 might stop getting romance movies recommended to them, even though they enjoy them. For u2, a sim-ilar case could arise for action movies. Essentially, this imbalance can trap the users in a bubble of recommendations with only their established preferences and gender-stereotypical genres. Addition-ally, if a user's established preferences are already aligned with gender stereotypes, then the bias in recommendation will intensify further giving rise to a filter bubble, with very redundant movie recommendations.\n3.1.2 Falling short when quantifying gender bias. Moreover, as men-tioned earlier, using some performance metrics for both genders and comparing them to evaluate fairness is not a great idea. The two groups can have similar scores, even if the model is making biased recommendations by choosing to neglect certain categories for certain users. Our proposed metrics address this issue by break-ing down the recommendations by category to get a more nuanced sense of fairness.\nThe key takeaway here is the importance of learning fair user and item representations, as well as evaluating fairness in the out-put. Even if a model is trained only on user-item-rating interactions with no explicit mention of sensitive attributes, the model can still infer this private information due to the correlation between their behavior and their sensitive attributes [6, 9, 22]. So, we have to take precautionary measures when training the model itself, so it is unable to learn these correlations. We also wanna discuss how it is important to ensure personalization, including gender-specific pref-erences to enhance user satisfaction, but such preferences should be balanced against any risk of reinforcing stereotypes.\nPlease note in our study we focus on binary genders, acknowl-edging there are many other gender identities not represented here."}, {"title": "3.2 Notation", "content": "We present all metrics for fairness assessment using the following mathematical notation:\n\u2022 ui: A single user, where i indexes the users.\n\u2022 vj: A single item, where j indexes the items.\n\u2022 U and V: The set of users and items, respectively.\n\u2022 Um and Uf: The set of male and female users, respectively.\n\u2022 c: An item category, such as Action, Sci-Fi, Romance, etc.\n\u2022 C: A category matrix where Cj,c = 1 if category c is associ-ated with item vj, and Cj,c = 0 otherwise.\n\u2022 Cu\u2081: The list of categories associated with item vj.\n\u2022 TopKu: The set of top K recommended items for user ui.\n\u2022 C: Represents the set of categories for items.\nTo assess fairness, we adapt and extend Information Retrieval metrics, introducing both non-ranking and ranking-based metrics, which are detailed in the following subsections."}, {"title": "3.3 Non-ranking-based metrics", "content": "The first set of metrics we propose evaluates the fairness of a rec-ommender system without considering the ranking of movies.\n3.3.1 Category Coverage (CC). This metric estimates the propor-tion of recommended items associated with category c relative to all categories for all users ui. This essentially captures the category-specific performance of the recommended items. It is defined as follows:\n\\(CC(c, U) = \\frac{1}{|U|} \\sum_{U\u00a1 \\in U} \\frac{1}{|TopK_{u_i}|} \\sum_{v_j \\in TopK_{u_i}} C_{v_j,c}\\) (1)\n3.3.2 Relative Category Representation (RCR). This metric esti-mates the proportion of category c in recommended items relative to the proportions of all categories available from the whole dataset. This helps provide insights on category-specific items and is defined as follows:\n\\(RCR(c, U) = \\frac{\\sum_{U\u00a1 \\in U} \\frac{1}{|TopK_{u_i}|} \\sum_{v_j \\in TopK_{u_i}} C_{j,c}}{\\sum_{Uj \\in U} C_{k,c}}\\) (2)\nThese two metrics help us quantify how relevant a category is for items recommended to a given set of users. For instance, CC would help us quantify the diversity of items recommended by reflecting the overall distribution of recommended content when computed for different c. Whereas, RCR (calculated for different c) would measure if the recommendations suppress or amplify certain categories by comparing them to the actual distribution of available content. Both CC and RCR provide a more nuanced understanding of how the recommendation system performs but don't take into account the position of the recommended items. For recommendation systems, where in most cases items surface one after another, it is vital that the items on the top of the menu are the most relevant. Hence, to ensure that ranking order is also considered for evaluating recommended items, in the next section we introduce rank-based metrics."}, {"title": "3.4 Rank-based Metrics", "content": "We now introduce our metrics that utilize ranking to provide a comprehensive assessment of fairness.\n3.4.1 Category Mean Average Precision (CMAP). For a given set of users, this metric estimates the proportion of recommended items associated with category c by incorporating the rank of the items. A category-specific average precision score is computed for each user, followed by averaging these scores across all users. This metric is defined as follows:\n\\(CMAP(c, 1) = \\frac{1}{|U|} \\sum_{U\u00a1 \\in U} \\sum_{j=1}^{TopK_{u_i}} \\frac{C_{j,c}}{TopK} P(j)\\) (3)\nwhere \\(P(j) = \\frac{1}{j} \\sum_{k=1}^{j} C_{k,c}\\)\n3.4.2 Category Discounted Cumulative Gain (CDCG). The score for this metric is discounted based on the position of the item in the recommended list. While similar to the previous metric, CDCG uses a logarithmic discount factor, which is higher for items that appear lower down in the list. It is defined as follows:\n\\(CDCG(c, U) = \\frac{1}{|U|} \\sum_{U\u00a1 \\in U} \\sum_{j=1}^{TopK_{u_i}} \\frac{C_{j,c}}{log(j+1)}\\) (4)\n3.4.3 Category Mean Reciprocal Rank (CMRR). This metric is based on MRR, which is essentially the mean reciprocal of the rank of the first relevant item. We adapt it for assessing fairness and we define it as follows:\n\\(CMRR(c, U) = \\frac{1}{|U|} \\sum_{U\u00a1 \\in U} \\sum_{j=1}^{TopK_{u_i}} \\frac{C_{i.c}}{j}\\) (5)\n3.4.4 Category RPrecision (CRP). This metric uses the category-specific precision but only for the top [Re] results, where Rc represents the proportion of category c when considering all items in V (i.e., \\(R_c = \\frac{\\sum_{v_j=1}^{V} C_{v_j,c}}{\\sum_{j=1}^{V} C_{o_j}}\\); note that item vj can belong to multiple categories like movie genres). This metric is defined as follows:\n\\(CRP(c, U) = \\frac{1}{|U|} \\sum_{U\u00a1 \\in U} \\sum_{j=1}^{TopR_c^{u_i}} \\frac{C_{j,c}}{|R_c|}\\) (6)\nAll four of the ranking-based metrics, take into consideration the proportions of categories and use rank as a discounting factor. The discounting factor is different for each of them and has distinct nuances. For instance, CRP would only evaluate recommendations till a certain rank [Re] to measure if the model is providing a pro-portional amount of recommendations for c relative to its presence in the dataset. Another metric like CDCG for example would cap-ture something different, which is the relevance of c by giving more weight to items (falling under category c) appearing higher in the list."}, {"title": "3.5 Group Fairness (Gender)", "content": "Gender Balance Score (GBS) for a given category c, is the absolute difference of category distribution values for Um and Uf based on any of the category-aware metrics discussed above. This metric is an adaptation of demographic parity as mentioned in Section 2.2. We want to make sure that the category distributions of the recommended items for the groups, of males and females are similar. For each metric M, defined above, we want to ensure that the difference of values calculated for the groups male and female summed up for all categories are close to 0. This can be formalized as:\n\\(\\triangleMc = |M(c, U_m) \u2013 M(c, U_f)|\\) (7)\nGBS(M) = \\sum_{C \\in C} \\triangleMc \u2248 0 (8)"}, {"title": "4 Genre Aware Regularization For Gender Fairness", "content": "Now that we have introduced our set of metrics for capturing a nuanced sense of fairness for recommended items, we aim to learn fair representations that are category-aware for users of differ-ent genders. To promote fairness in recommendation models we employ an in-processing regularization technique inspired by the approach first proposed by [78]. We propose to incorporate Cate-gory Coverage (Section 3.3.1) into the loss function alongside the primary recommendation loss, encouraging the model to optimize for category-aware fairness. We want to emphasize that while we select CC as an example here, optimizing on any of the other met-rics (as long as the implementation is differentiable) is possible. We incorporate this fairness regularizer into the loss function of each baseline model-MF, VAE-CF, and NeuMF.\nSpecifically, to discourage the model from learning any biased representations, we incorporate the following regularizer:\n\\(L_{FairGenreGender} = GBS(CC) = \\sum_{C \\in C} |CC(c, U_m) \u2013 CC(c, U_f) |\\) (9)\nThe goal is to minimize the difference in category distribution of the items being recommended to the users of each sensitive group. The new loss function for MF, VAE-CF and NeuCF models can be written as:\n\\(L = L_{FairGenreGender} + (1-a) L_{Recommendation}\\) (10)\nwhere a is used to calibrate the trade-off between the model loss and the fairness term.  \\(L_{Recommendation}\\) represents the respective recommendation loss term of each model. By tuning this hyperpa-rameter, we ensure that fairness does not overly compromise the recommender's ability to respect users' personal preferences (even if there are natural differences across liked categories for users of different genders). In order to modulate the gender loss value and to ensure it is able to make a significant contribution to the loss function, we pass it through a sigmoid function, centering at 0.5 and scaling it by 0.1. Additionally, the maximum batch loss value of the actual recommendation loss is used to scale the gender loss up to ensure both losses are on the same scale. In the next section we show how using this fairness regularization helps the models minimize bias in recommendations provided to users of different genders for different categories."}, {"title": "5 Experiments", "content": "In the sub-sections below, we outline our experimental setup.\n5.1 Datasets\nOur experiments are performed on three recommendation datasets summarized in Table 1. The data is split into training, validation, and test sets with a 70:10:20 ratio following user-based split scheme. It's important to highlight that items can fall under multiple cat-egories. For clarity and ease of understanding, we focus on four representative categories-Action, Romance, Sci-Fi, and Drama from the MovieLens datasets and Coffee, Tea & Desserts, Arts & Enter-tainment, Travel & Transportation and Asian from the yelp dataset when visualizing our metric values; however all categories are included in our experiments.\n5.2 Evaluating Performance\nFor measuring the performance of recommendations made, we use HitRatio@k calculated for each user and then averaged. Ad-ditionally, we use a ranking-based metric NDCG@k (Normalized Discounted Cumulative Gain) which gives higher relevance to items appearing higher in the ranked list. This too, is calculated for each user using their top k recommendations and then averaged. We calculate these values for k = 50.\n5.3 Base Recommendation Models\nFor analyzing which models manifest gender bias we evaluate sev-eral recommendation approaches. When selecting these models, we include a variety of algorithms, ranging from traditional ones to more modern ones including Matrix Factorization, UserKNN, ItemKNN, NeuMF and VAE-CF (more detailed information on these and the fairness-aware models can be found in Section A.2).\nAdditionally, we include a regularization-based fairness-aware model BeyondParity [77] and a counterfactually fair recommen-dation model SM-GBiasedMF [44]. To weigh the fairness-aware loss for SM-GBiasedMF, we use \u03bb = 20 for all experiments since this value seems to work well as mentioned in [44]. To make a fair comparison, we use BeyondParity's Uval (refer to Equation 11) as a regularization term in the same setting as shown in Equation 10, with the same value of alpha we use for our MF model. We use an early stopping strategy for the three models being compared, monitoring NDCG@20 with a delta of 0.0005 and patience of 10 epochs with a maximum number of iterations of 50 and 100 for the 100K and 1M datasets, respectively.\nWe utilize the Cornac framework [57], with customizations to meet our requirements, including the implementations of differ-entiable ways to obtain top k category distribution for calculating our gender loss term for different models. When calculating our regularization term, we generate a top k recommendation list of"}, {"title": "6 Analysis of the proposed metrics", "content": "In this section, we discuss the results we obtained from our experi-ments through a comprehensive analysis.\n6.1 Baselines Comparison\n6.1.1 Bias Evaluation. We report our results in Figures 2, 3, and Table 2. For the KNN-based models, ItemKNN seems more bias-prone. This can be due to the nature of the model of calculating similarity between items based on the interactions by the users. For instance, in the ML-100k dataset, the average rating for ro-mance movies is higher for female users than for males. This kind of imbalance can reinforce category-related gender bias when items are recommended. Among the three other baselines, the MF-based models can be considered more biased. For the MF model, explicit embeddings are used to store user and item representations, which are likely to capture correlations of user behavior (e.g., liking cer-tain genres) and their sensitive attributes. For VAE-CF, variational autoencoders are used to learn probabilistic latent representations of users, which are still sensitive to capturing biased representa-tions but not as much as NeuMF. NeuMF captures both linear and non-linear relationships between users and items through Gen-eralized Matrix Factorization (GMF) and Multi-Layer Perceptrons (MLP). So, NeuMF can capture biases in the data, especially the intricate patterns about user preferences, which can reflect social stereotypes. The MF model itself is not as complex as NeuMF, so our results of NeuMF being more biased than MF is reasonable. For the two smaller datasets, VAE-CF doesn't manifest high values for bias, which can be due to the model not being able to fully capture representations of gendered preferences because of the"}, {"title": "6.1.2 Performance Evaluation.", "content": "The performance drop for VAE-CF and NeuMF is less than that of the MF model, however, it all depends on the selection of a (more on this in Section A.1). For all models, when choosing a we ensure there is substantial bias reduction without too significant of a drop in performance. In some cases the fair models outperform the original models, this although seems counter-intuitive, but is presumably due to the fairness term acting as a regularization term which can help reduce over-fitting in the model to some extent (as noted in [35, 39]).\n6.2 Fairness-Aware Baselines\nWe can observe the results for our MF fair model when compared with two other fairness-aware models in Table 3 and Figure 4. We chose to use our MF fair model since the other two fairness-aware models use MF as their foundational models. Our model has outper-formed the other models in terms of bias scores across the majority of the comparisons. While our models don't excel in terms of per-formance, they still offer a better balance between performance and bias scores when compared with the other fairness-aware baselines. It is important to highlight that BeyondParity uses plain MSE loss, while the other two use BPR loss with negative sampling, which explains the exceptionally low-performance values. While we em-phasize the significance of comparing the aggregated differences for each metric M for all the categories, we also want to highlight the importance of evaluating bias values separately. For instance, although GBS(CMRR) of SM-GBiasedMF is lower for the ML100K dataset when compared to ours, there is still significant bias for movies recommended that belong to the genres like Romance and Sci-Fi as observed in Figure 4. This strengthens our idea of having a set of metrics to quantify bias in a nuanced way since the model seems \"fair\" when considering overall scores for all categories, but closer inspection reveals how it is biased against certain categories. It is worth mentioning that SM-GBiasedMF is more biased than the plain MF model for the Yelp dataset. Our theory is that since this model needs more epochs to satisfy the early stopping criterion (as mentioned before), it likely over-fits the dataset, in turn amplifying the biases present in it."}, {"title": "7 Discussion and Conclusion", "content": "In this paper, we identify the underlying issues of current metrics for evaluating consumer-side bias in recommendation systems. To better quantify bias, specifically gender bias, in such models, we propose a set of metrics. These metrics help capture a nuanced sense of fairness on recommended items by considering categories of the items. Next, we demonstrate how introducing one of our metrics as a fairness loss term along with the recommendation loss helped minimize the unfairness manifested in models in terms of different categories of items recommended with minimal perfor-mance loss. Experiments on three real-world datasets using a vari-ety of recommendation models, including fairness-aware models, show the effectiveness of our metrics in capturing bias. Addition-ally, after incorporating our loss term, the bias in the models was significantly reduced, with a favorable balance between fairness and accuracy. Our loss function is very flexible which allows for context-sensitive fairness. In domains such as job recommendation and housing, where fairness is vital, the functions can prioritize fairness. Conversely, for areas like movie recommendations it can balance personalization and fairness (ensuring user preferences are respected). Our work aims to spread awareness about how simple metrics that are currently utilized to evaluate bias might be giving researchers a false sense of fairness, and a more refined approach like ours is required to address this issue. Our metrics are very ver-satile and can be easily adapted to measure provider-side fairness by utilizing proportions of item brands, for example, and can help quantify popularity bias in recommendations. Extending on this, the metrics can be used for measuring CP-fairness since we can measure bias from both the consumer side and provider side, by taking differences in values for different item providers for different demographic groups. Plus we plan to extend our metrics to consider"}, {"title": "A Appendix", "content": "A.1 Abalation Study\nIn this section, we discuss the influence of the hyper-parameter a on recommendation performance and fairness. As mentioned in Section 4, a can be used to control the strength of the fairness aware loss. Theoretically as a increases we would expect a decrease in performance since the recommendation loss would have a decreased weight. We use a values from 0 to 0.6 with increments of 0.1 for all three models. We did not include values above 0.6, because it does not make sense to overpower the recommendation loss since that is our main task. To verify the expected behavior of alpha on recommendation loss and bias we plot the metric we optimize on: GBS(CC) or \u25b3CategoryCoverage and the NDCG values. As shown in Figure 5, there is a general trend of the performance metric and the bias score dropping as a increases. There are some fluctuations, where the bias increases for the first few values of alpha. Our intuition for this is that the fairness loss might not be enough to decrease the bias when a is small. It can essentially end up disturbing the loss function itself, without explicitly decreasing bias. But as the value increases the bias scores drop lower which is expected. There is a slight increase in NDCG value for certain models, as mentioned in Section 6.1.2, this can be the result of the fairness loss term acting as a regularization term that essentially"}, {"title": "A.2 Details about Baseline Models", "content": "This subsection delves into details about the models we worked on for our experiments.\n\u2022 MF [41]: A classical Matrix Factorization algorithm where users and items are represented as latent vectors with global bias. In our implementation, we use BPR [55] loss with negative sampling to enhance the performance of the model.\n\u2022 UserKNN [7]: This is a neighborhood-based method based on users, and items are recommended by discovering similar users based on the cosine similarity of their historical interactions.\n\u2022 ItemKNN [59]: Another neighborhood-based method that com-putes the similarity between items instead.\n\u2022 NeuMF [32]: NeuMF is a deep-learning based extension to MF, which combines the linearity of traditional MF models and the non-linearity of DNNs (Deep Neural Networks).\n\u2022 VAE-CF [45]: This non-linear probabilistic model is based on the auto-encoder architecture and learns compressed informa-tion about data. The encoder helps map user interactions as a low-dimensional latent space, and the decoder decodes this information back to the high-dimensional vector which is used to make predictions.\nThe Uval regularization term for BeyondParity [77] can be writ-ten as:\n\\(U_{val} = \\frac{1}{n} \\sum_{j=1}^{n} (E_{g}[Y]_{j} - E_{g}[r]_{j}) - (E_{\\neg g}[Y]_{j} - E_{\\neg g}[r]_{j})\\) (11)\nwhere \\(E_{g}[Y]_{j}\\) is the average predicted score for the j-th item from disadvantaged users,  \\(E_{\\neg g}[Y]_{j}\\) is the average predicted score"}, {"title": "A.3 Dataset Pre-processing", "content": "For all of our datasets, we filter out inactive users. A user is con-sidered inactive if they have less than 5 interactions. Additionally, we remove any user whose gender is not known. The Yelp dataset has over 300 categories"}]}