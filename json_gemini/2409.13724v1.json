{"title": "Logically Consistent Language Models\nvia Neuro-Symbolic Integration", "authors": ["Diego Calanzone", "Stefano Teso", "Antonio Vergari"], "abstract": "Large language models (LLMs) are a promising venue for natural language un-\nderstanding and generation. However, current LLMs are far from reliable: they\nare prone to generating non-factual information and, more crucially, to contra-\ndicting themselves when prompted to reason about relations between entities of\nthe world. These problems are currently addressed with large scale fine-tuning\nor by delegating reasoning to external tools. In this work, we strive for a middle\nground and introduce a loss based on neuro-symbolic reasoning that teaches an\nLLM to be logically consistent with an external set of facts and rules and improves\nself-consistency even when the LLM is fine-tuned on a limited set of facts. Our\napproach also allows to easily combine multiple logical constraints at once in a\nprincipled way, delivering LLMs that are more consistent w.r.t. all constraints and\nimprove over several baselines w.r.t. a given constraint. Moreover, our method\nallows LLMs to extrapolate to unseen but semantically similar factual knowledge,\nrepresented in unseen datasets, more systematically.", "sections": [{"title": "1 Introduction", "content": "Developing reliable large language models (LLMs) and safely deploying them is more and more\ncrucial, particularly when they are used as external sources of knowledge [55, 32, 10, 6]. To do so,\none would need LLMs to be factual [74], i.e., agreeing on single facts that appear in a knowledge\nbase (KB), and logically consistent [39, 49], i.e., being able not to contradict themselves or a KB\nwhen prompted to perform complex reasoning. It has been abundantly shown that training on large\ndatasets for question answering (QA) [66] alone cannot meet these desiderata [24, 41, 42, 23].\nFactuality and consistency are intimately related. Enforcing factuality alone generally boils down to\nfine-tuning an LLM on a large KB of atomic facts [36]. When predicting the truth values of these\nfacts, a number of works try to enforce the simplest form of consistency: that the probability of a true\nfact shall be one minus the probability of its negation [12]. More sophisticated heuristics are possible,\ne.g., fine-tuning on a large QA dataset by jointly optimizing for truthfulness of model answers and\ncontrastively pulling apart true and false facts [42]. All these approaches require large KBs and more\ncrucially are tailored towards specific logical constraints.\nWhen it comes to self-consistency w.r.t. more complex reasoning scenarios, e.g., ensuring that LLMs\ncan perform modus ponens without contradicting themselves [67, 49], one line of research focuses"}, {"title": "2 Logical consistency through the lenses of probabilistic reasoning", "content": "We formalize the different reasoning scenarios we would like an LLM to be (self-)consistent with,\nand highlight the shortcomings of commonly used LLMs when prompted to reason in this way.\nFactuality. We view a pre-trained LLM as a collection of truth beliefs about facts over which it\ncan reason. The simplest reasoning task is factual reasoning, i.e., determining the veridicity of a\nfact. For example, consider the fact f in textual form \u201can albatross is a bird\". It can be commonly\nencoded in knowledge bases (KBs) such as BeliefBank [36] as a (subject-relation, property) pair, for\ninstance, (albatross-is, bird). To inspect whether an LLM believes a fact to be true, we can prompt it\nwith a question like \u201cIs an albatross a bird?\", the LLM can supply a binary prediction of the form\""}, {"title": "3 Logically-consistent LLMs via NeSy integration", "content": "We assume we are given a KB comprising a limited set of textual statements and associated truth\nvalues $D_F = \\{(f_1, z_{f_1}), ..., (f_n, z_{f_n})\\}$, encoding simple facts such as \u201can albatross is a bird\u201d (true)\nand \"a computer is a bird\u201d (false), and a set of logical constraints $D_C = \\{\\alpha_1, ..., \\alpha_m\\}$ defined over\nfacts in $D_F$, comprising implications, negations or more complex constraints as defined in Section 2.\nGiven a pre-trained LLM encoding a distribution $p_{\\theta}$ over tokens, our objective is to fine-tune it to\nbe more consistent w.r.t. $D_F$, $D_C$ and itself. As an important side benefit, we expect the fine-tuned\nLLM to generalize to \u2013 and be consistent with \u2013 the truth values of unseen facts $f_{n+1}, f_{n+2},...$, that\ncan be either logically inferred by applying the constraints in $D_C$ to $D_F$ (e.g., by applying modus\nponens) or that are semantically similar to facts in $D_F$. For example, since albatross and cockerel are\nbirds, and since this is reflected by their semantic similarity as encoded by the LLM, we expect an\nLLM consistent with the constraint (\"an albatross is a bird\u201d \u2192 \u201can albatross can fly"}, {"title": "4 Related Work", "content": "LLMs and factual reasoning. LLMs are increasingly being employed as implict KBs [55, 5],\nhowever ensuring they are factually consistent is still an open challenge [75, 7]. A number of works\naugment LLMs with external KBs, especially in the context of QA, and with the primary aim of\nimproving answer factuality [35, 49, 40]. A popular approach to do so is retrieval augmented genera-\ntion [37, 38], which however is not yet suited for more complex reasoning scenarios. Alternatively,\nexternal KBs have been used to improve reasoning, e.g., via prompt learning [53] or ex-post model\nediting [61]. However, current knowledge editing methods, including supervised fine-tuning, do\nnot guarantee the propagation of factuality between units of knowledge related by logical relations\n[15, 4]. Mitigating hallucinations in LLMs [6, 59] is related to enforcing factuality, but as generated\ninconsistencies might not map to a single entry in a KB, they are harder to detect and prevent [29].\nMore complex reasoning with LLMs. Much less attention has been posed to other forms of\nreasoning, such as combining modus ponens, consistent negation and combination thereof. Even when\nthis is done, reasoning is generally cast as a QA task, where an LLM has to predict the satisfiability\nof logical formulas of different complexities. To this end, benchmarks such as SimpleLogic [81] or\nLogicBench [54] have been proposed. Implication or entailment [45, 25] are also usually cast as a\nQA prediction task [58]. Datasets such as BeliefBank [36] provide collections of simple implication"}, {"title": "5 Experiments", "content": "We aim to answer the following research questions: RQ1: Can LoCo-LMS achieve comparable\nor superior consistency to methods using external reasoners using less training data? RQ2: Can\nLoCo-LMS retain good consistency to unseen types of constraint at training time? How much does\ntraining on all the constraints jointly improve consistency overall? RQ3: Can LoCo-LMS transfer\nconsistent knowledge to domains out of the training distribution?\n5.1 RQ1: How do LOCO-LMS perform compared to external solvers?\nWe reproduce the experimental setting of Mitchell et al. [49] to compare against ConCoRD, a\nsymbolic layer that uses a MaxSAT solver to impose self-consistency for implication ex-post.\nData. We train LOCO-LMS on the BeliefBank [36]. We use the three splits as in Mitchell et al. [49]:\na \"calibration\" set of 1,072 annotated facts about 7 entities of the form (subject, property, true/false)\nused for training, a \"silver\" set of 12, 636 facts about 85 entities used for evaluation, and a set of\n2224 valid abstract logical implications. We generate ground implication rules (DC) by looking up\nthe subjects of all facts in the training set: if the antecedent or the consequent fact of the general\nconstraint is known for that subject, we add the subject ground implication constraint to the dataset.\nAppendix A.1.1 details the whole process.\nTo measure generalization across entities, we generate two controlled splits of the training calibration\nset: T1 facts, appearing either as antecedents or consequents in the constraints; T2 facts, appearing\nexclusively as consequents. The goal is to correctly guess the consequents by seeing only the\nantecedents and the constraints. We subsequently test the effects of pure supervised fine-tuning on a\nportion of random facts from the whole calibration set (T1+T2).\nModels. As in Mitchell et al. [49], we use Macaw-Large [66] (770M parameters), a sequence-to-"}, {"title": "6 Discussion and Further Work", "content": "Limitations. One limitation of our approach is sensitivity to the choice of prompt format, a general\nphenomenon [77] that in our case means (self-)consistency improvements do not always carry over\nacross formats. This can be partially addressed by fine-tuning using a mixture of formats, as we do in\nSection 5. While our SL is constraint-agnostic, in practice we fine-tune LOCO-LMS only against a\ncombination of implications and exclusive ORs. While this setup is already richer than those studied\nin related works (Section 4) and achieves positive transfer to tasks requiring multiple reasoning steps,\nit leaves more room for future work on more complex benchmarks.\nLoCo-LMs fine-tuning relies on two assumptions: that the probabilities of facts are conditionally\nindependent given the LLM inner state, and that the constraints in the KB are correct. The former\nreadily applies to many LLMs, but assuming independence can bias the solutions learned by the SL"}]}