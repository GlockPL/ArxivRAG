{"title": "The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1", "authors": ["Kaiwen Zhou", "Chengzhi Liu", "Xuandong Zhao", "Shreedhar Jangam", "Jayanth Srinivasa", "Gaowen Liu", "Dawn Song", "Xin Eric Wang"], "abstract": "The rapid development of large reasoning models, such as OpenAI-03 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models (LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model's reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models' safety to close the gap. Warning: this paper includes examples that may be offensive or harmful.", "sections": [{"title": "1 Introduction", "content": "The landscape of large language models (LLMs) is evolving with the advent of large reasoning models like OpenAI-03 (OpenAI, 2025b) and DeepSeek-R1 (Guo et al., 2025), which leverage reinforcement learning to enhance complex reasoning. Unlike conventional LLMs, these models \u201cthink\u201d (generate a structured chain-of-thought employing specialized output formats) before producing a final response. Reasoning models have superior performance in problem-solving, coding, scientific reasoning, and multi-step logical inference. However, their increased capabilities, combined with the recent open-sourcing of DeepSeek-R1, amplify their potential safety risks across a broad range of applications. Therefore, a comprehensive safety analysis of these reasoning models is essential to identify and mitigate their associated risks.\nIn this work, as shown in Figure 1, we present a systematic and comprehensive safety assessment for these language reasoning models. Specifically, we first conduct a thorough safety evaluation by testing these reasoning language models against various established safety benchmarks, covering a broad range of safety categories from company policies and government regulations (Zeng et al., 2024), and various application scenarios (Wan et al., 2024b). Additionally, we assess their vulnerability to different adversarial attacks, including jailbreaking and prompt injection (Jiang et al., 2024; Wan et al., 2024b), to analyze their robustness in real-world deployments. In these evaluations, we analyze both quantitative results and the safety behaviors of large reasoning models to gain deeper insights into their safety performance.\nBeyond classifying the safety of final model responses, a primary contribution of this work is a multi-faceted safety analysis specific to large reasoning models. First, to determine whether the reasoning process itself elevates safety risks, we evaluate the safety of the model's internal reasoning steps (e.g., the content within <think> and </think> tags in DeepSeek-R1) and compare it against the safety of the final completion. Second, recognizing that unsafe responses can vary in their degree of harmfulness, we hypothesize that reasoning models, due to their enhanced capabilities, may generate more harmful unsafe responses. Therefore, in addition to binary safety classifica-"}, {"title": "2 Background and Related Work", "content": "Large Reasoning Models Recent advancements in large reasoning language models such as OpenAI's ol and o3 (OpenAI, 2025a,b) and DeepSeek-R1 (Guo et al., 2025) have substantially enhanced LLMs' problem-solving capabilities by integrating structured reasoning mechanisms. For example, the OpenAI ol model spends additional compute time to generate long chains of reasoning before producing a final answer, achieving PhD-level per-formance on challenging mathematical and scientific benchmarks (OpenAI, 2025a). Building on this, the 03 series further refines the approach to boost performance (OpenAI, 2025b). In parallel, DeepSeek-R1 pioneered a reasoning-oriented reinforcement learning training approach without supervised fine-tuning, demonstrating emergent reasoning behaviors and achieving performance comparable to ol on math, coding, and science tasks (Guo et al., 2025). These models underscore the effectiveness of test-time self-reflection in addressing complex challenges, although significant hurdles remain in ensuring their safety and reliability.\nSafety Benchmarking for LLMs As the abilities of LLMs become stronger, various benchmarks have been proposed to evaluate the safety of LLMs in different safety categories and application domains (Wang et al., 2023; Bhatt et al., 2024; Wan et al., 2024b; Li et al., 2024a; Xie et al., 2024; Zeng et al., 2024; Andriushchenko et al., 2024). These benchmarks evaluate whether LLMs comply with malicious queries and produce harmful content, with comprehensive categories that cover safety regulations from the government and company policies. R\u00f6ttger et al. (2023) also evaluate whether the safety alignment of LLMs leads to over-sensitive to benign queries. More recently, there are safety evaluations for new applications of LLMs, including scenarios that are relevant to cybersecurity (Wan et al., 2024b; Bhatt et al., 2024), and LLM agents that make sequential decisions and receive feedback from the environments (Andriushchenko et al., 2024).\nAdversarial Attacks on LLMS As LLMs become integral to real-world applications, adversaries are devising increasingly sophisticated strategies to subvert their safety mechanisms. One prominent tactic is prompt injection (Yi et al., 2023; Zhan et al., 2024; Zhang et al., 2024), wherein"}, {"title": "3 Research Questions and Safety Evaluation Design", "content": "With the open-sourcing of the R1 series, large reasoning models are likely to see continuous advancements and broader adaptations across various applications. This motivates us to perform a systematic safety evaluation for these models. In this study, we aim to answer the following research questions that could help us to understand large reasoning models' safety performance and identify potential directions for improvement:\n1. How safe are large reasoning models when given malicious queries? Are they able to refuse to follow these queries? (Section 4)\n2. How does enhanced reasoning ability affect the harmfulness level of the unsafe responses? (Section 5)\n3. How safe are large reasoning models when facing adversarial attacks? (Section 6)\n4. How do the safety risks of the thinking process in large reasoning models compare to those of the final answer? (Section 7)"}, {"title": "3.2 Evaluation Design", "content": "Safety Benchmarks As shown in Table 1, we select 5 representative datasets from 3 safety benchmarks and 2 datasets on adversarial attacks for evaluation. For RQ1, we select Air-Bench (Zeng et al.,"}, {"title": "4 Safety Benchmarking", "content": "The fundamental challenge in safety benchmarking is distinguishing between safe and unsafe user queries. Given an input query $q$, the model must reliably assess its underlying intent. Specifically, for queries with harmful intent $q_h$, the LLM should either refuse to respond or provide mitigating information. For the queries with safe intent $q_s$, the LLM should deliver informative and helpful responses without unnecessary refusals.\nIn this section, we investigate the safety performance of large reasoning models in handling malicious queries. We begin by analyzing their overall performance, and identifying a distinct safety behavior from them. Then, we analyze their behavioral patterns on selected representative datasets."}, {"title": "4.1 Overall Safety Analysis", "content": "Overall Performance We evaluate the average safety rate of all models across four benchmarks with unsafe queries. First, 03-mini exhibits significantly higher safety than open-source reasoning and non-reasoning models, effectively identifying and rejecting most unsafe queries across various scenarios. Open large reasoning models still have a considerable gap to close compared with 03-mini. Second, we observe that the distilled R1-70b consistently achieves a lower safety rate than Llama-3.3, suggesting that reasoning-supervised fine-tuning reduces a model's safety performance; this aligns with the finding of (Qi et al., 2023) on the effect of supervised fine-tuning to safety performance. Finally, R1 demonstrates better safety performance than V3 on the broad safety categories on AirBench. However, R1 shows a significantly more severe safety risk in the cybersecurity domain, with more complex tasks and environment settings. These results indicate that more effort should be put into safety alignment on R1 models.\nSafety Thinking Behavior From the output of the models, we identify a different safety behavior of R1 models from non-reasoning LLMs \u2013 the thinking process of the R1 models usually determines the safety of final completion. In the thinking process, if the model performs safety thinking and decide that the query is not safe to answer, it will refuse the query it in the final answer. Otherwise, if no safety thinking happens, or the model believes the query is appropriate to answer, no refusal will happens. In contrast, the refusal behavior from non-reasoning LLMs usually happens immediately without explicit thinking. Examples and more analysis on the safety thinking are in Section 6.1."}, {"title": "4.2 Select Datasets Analysis", "content": "Safety Policies & Regulations We present the level 2 category results of Air-Bench in Figure 2 (A), covering 16 categories. First, the comparison between reasoning and non-reasoning models has significant differences across categories. For example, Llama3.3 is significantly better than R1-70b in 'Operational Misuses', 'Security Risks', etc., with more than 20% advantage, but is worse than R1-70b in 'Hate/Toxicity' by 11%. Second, all the reasoning models, including 03-mini have very low performance in certain categories like 'Operational Misuses' and 'Sexual Content'. Meanwhile, the open large reasoning models perform significantly worse than 03-mini in most of the categories. These findings suggest that certain safety aspects require greater attention in large reasoning models.\nCode Interpreter Test The results in Figure 2 (B) show that reasoning models exhibit higher security risks than non-reasoning models, particularly in reflected attacks and social engineering. Specifically, the safe operation percentage of the R1 model in the social engineering category is significantly lower than Llama3.3 and falls behind V3 by over 50%. This indicates that reasoning models are more vulnerable to abuse in interactive categories.\nMulti-turn Spear Phishing Table 2 presents the models' safety rate in the Spear Phishing Test. In this evaluation, the tested LLM will be asked to perform multi-turn spear phishing against a victim LLM. Here, the safety rate refers to the proportion of conversations in which the model did not attack in any turn. Notably, the result reveals a significant safety risk, as o3-mini initiates an attack in only 5% of trials, while other models do so in over 95% of cases. Moreover, 03-mini almost always"}, {"title": "5 Response Harmfulness Level Evaluation", "content": "Safety classification alone is not sufficient to comprehensively assess models' safety, as not all responses classified as unsafe are equally harmful - some provide minimal information, while others offer detailed, actionable guidance that aids malicious intent. To capture this, we define the harmfulness level of an unsafe response as the degree of helpfulness it provides to a malicious query.\nHarmfulness Evaluation We quantitatively evaluate the model's harmfulness level on two datasets with different malicious scenarios. For AIR-bench, we evaluate the helpfulness to the malicious question using two top pre-trained reward models on the RewardBench (Lambert et al., 2024) - ArmoRM-Llama3-8B (Wang et al., 2024) and QRM-Llama3.1-8B (Dorka, 2024). These models are trained to predict the reward score for 19 attributes, such as helpfulness, correctness, and coherence. We utilize the average reward score for the helpsteer-helpfulness and ultrafeedback-helpfulness attributes to represent the helpfulness of the response to queries in AIR-bench. In Spear Phishing Tests, the helpful-"}, {"title": "6 Safety Attacking", "content": "This section evaluates the models' safety performance against two types of adversarial attacks: the jailbreak attack, which forces the model to respond to harmful queries, and the prompt injection attack, which aims to override the models' intended behavior or bypass restrictions."}, {"title": "6.1 Jailbreak", "content": "The results of WildGuard jailbreak attacks in Table 5 reveal that all the models exhibit weak safety performance, including o3-mini. This suggests that current LLMs struggle to detect challenging adversarial threats. We also find that among all the open-source models, Deepseek-R1 has the lowest attack success rate. We observe cases where reasoning models are able to identify potential hazards in their thinking process and provide relatively safe responses. An example is provided in Appendix Figure 7. However, reasoning models still"}, {"title": "6.2 Prompt Injection", "content": "Table 6 presents the results of the text prompt injection attack, revealing significant differences among models in terms of injection types and risk categories. Regarding injection types, the ASR of indirect injection is generally higher than that of direct injection. Although the reasoning models perform similarly to other models in indirect attacks, their robustness is significantly weaker in direct attacks, with ASR nearly double that of the others, indicating a higher level of compliance to direct prompts. In terms of risk categories, reasoning models exhibit a higher ASR for security attacks than for logic. This discrepancy indicates that despite these models demonstrate strong logical reasoning capabilities, their security protection awareness is weak. Finally, compared with 03-mini, R1 models are more vulnerable to prompt injection attacks."}, {"title": "7 Thinking Process v.s. Final Answer", "content": "Finally, we compare the safety of the thinking process from R1 models and their final answer when given harmful queries. Specifically, we take the content between <think> and </think> from the models' output and use the same evaluation prompt to judge the safety. The result on four datasets is in Table 7. We can observe that the safety rate of the thinking process is lower than the final answer. Af-"}, {"title": "8 Discussion and Conclusion", "content": "In this paper, we present a comprehensive multi-faceted analysis of the safety of large reasoning models. In our analysis, we identify a significant safety gap between the DeepSeek-R1 models and the 03-mini. We suggest three potential directions for improvement. First, enhancing the extent of safety alignment in R1 models, as their current alignment training may be insufficient, especially in certain safety categories. Second, advanced safety alignment techniques\u2014such as rule-based rewards and methods that leverage reasoning ability to enhance safety (Mu et al., 2024; Guan et al., 2024) could be explored. Third, developing new training strategies to enhance their explicit safety reasoning, in terms of activating safety thinking and improving the precision of safety judgments. In addition, the distilled R1 model compromises the original safety performance consistently in all the safety tests. Therefore, additional safety alignment is required after reasoning distillation.\nMoreover, we find that with stronger reasoning ability, the R1 models provide more help to the malicious queries compared with their non-reasoning counterparts. Therefore, their unsafe responses are more harmful. This further underscores the necessity of enhancing the safety of R1 models. Finally, within the outputs of large reasoning models, we find that the thinking process may contain hidden safety risks that are not reflected in the final answer. This presents a new challenge brought by reasoning models, which requires future work to address."}, {"title": "A Appendix", "content": "Here, we provide detailed instructions to the dataset we used in our analysis, and provide an example from each dataset in Table 8."}, {"title": "A.1 Benchmark Introductions", "content": "AIR-bench (Zeng et al., 2024) is a comprehensive safety evaluation benchmark designed to assess LLM safety across 314 risk categories derived from eight government regulations and sixteen corporate policies. The AIR-Bench 2024 dataset comprises 5,694 diverse prompts spanning domains such as system operations, content safety, societal impacts, and legal risks.\nCyberSecEval-MITRE Tests (Wan et al., 2024a) employ the MITRE ATT&CK framework\u2014a comprehensive knowledge base of adversary tactics and techniques to evaluate LLMs on their compliance when prompted to assist in cyberattacks. The MITRE Tests originally contained 1000 examples across 10 MITRE categories and 377 attack techniques. To optimize inference cost, we select one sample for each attack technique and test on 377 samples in total.\nCyberSecEval-Code Interpreter Tests (Wan et al., 2024a) evaluate the security risks associated with integrating LLMs with code interpreters. In these tests, LLMs are responsible for generating or executing code within controlled environments. The evaluation focuses on assessing whether LLMs can be manipulated to execute malicious code, perform unauthorized actions, or escape sandboxed environments. This benchmark suite contains 500 examples across 5 categories on code execution.\nCyberSecEval-Spear Phishing Tests (Wan et al., 2024a) evaluates LLMs' ability to generate persuasive phishing attempts by simulating realistic interactions between an attacker LLM, a victim LLM, and a judge LLM. The attacker LLMs attempt to deceive the victim into revealing sensitive information or executing malware based on the victim's profile, including information like profession, interests, and communication style. The dataset comprises 856 test cases. For our testing, we selected a sample of 200 examples, ensuring equal distribution across four attack goals.\nXSTest (R\u00f6ttger et al., 2023) is a benchmark dataset for evaluating exaggerated safety behaviors in LLMs. It includes 250 safe prompts and 200"}, {"title": "A.2 Additional Details on the Safety Evaluation Results", "content": "Tables 9 and 10 present a more comprehensive safety evaluation of the model under the XSTest and Code Interpreter environments. These results provide insights into the model's performance when facing various complex security challenges and further highlight the model's vulnerabilities and robustness under different testing conditions."}, {"title": "A.3 More Examples", "content": "Figures 7-10 illustrate the four different behaviors of the reasoning model under the jailbreak test, as analyzed in Section 6.1: 1. Effective safety reasoning 2. Identification of safety issues, but a tendency to respond to the user's query 3. Recognition of safety concerns, but being misled into providing an answer 4. Failure to recognize any safety issues.\nFigures 11 and 12 present the security evaluation of the model under the text prompt injection attack, determining whether the model's performance is influenced or compromised by harmful prompt injections. The results shown in Figure 11 indicate"}]}