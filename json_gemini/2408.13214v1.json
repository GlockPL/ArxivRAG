{"title": "EUR/USD Exchange Rate Forecasting Based on Information Fusion with Large Language Models and Deep Learning Methods", "authors": ["Hongcheng Ding", "Xuanze Zhao", "Zixiao Jiang", "Shamsul Nahar Abdullah", "Deshinta Arrova Dewi"], "abstract": "Accurate forecasting of the EUR/USD exchange rate is crucial for investors, businesses, and policymakers. This paper proposes a novel framework, IUS, that integrates unstructured textual data from news and analysis with structured data on exchange rates and financial indicators to enhance exchange rate prediction. The IUS framework employs large language models for sentiment polarity scoring and exchange rate movement classification of texts. These textual features are combined with quantitative features and input into a Causality-Driven Feature Generator. An Optuna-optimized Bi-LSTM model is then used to forecast the EUR/USD exchange rate. Experiments demonstrate that the proposed method outperforms benchmark models, reducing MAE by 10.69% and RMSE by 9.56% compared to the best performing baseline. Results also show the benefits of data fusion, with the combination of unstructured and structured data yielding higher accuracy than structured data alone. Furthermore, feature selection using the top 12 important quantitative features combined with the textual features proves most effective. The proposed IUS framework and Optuna-Bi-LSTM model provide a powerful new approach for exchange rate forecasting through multi-source data integration.\nKeywords: exchange rate forecasting, EUR/USD, sentiment analysis, textual data, large language models, feature generation, Bi-LSTM, Optuna", "sections": [{"title": "Introduction", "content": "The exchange rate between the Euro and the US Dollar is a significant indicator in the global financial market, reflecting the economic dynamics between two of the world's largest economies. Precise prediction of the EUR/USD exchange rate is crucial for individual investors, businesses engaged in international trade, and policymakers responsible for economic stability and growth. Traditionally, econometric models have been utilized to forecast exchange rates, relying heavily on historical market data and macroeconomic indicators released by governments and financial organizations [1]. Although these datasets are comprehensive, their low publication frequency makes it difficult to capture real-time market volatility and nonlinear dynamics [2].\nThe integration of unstructured data from diverse sources, such as news articles, financial reports and social media platforms, has the potential to improve the accuracy of exchange rate forecasting. In recent years, the significant impact of political events, global economic crises, and unexpected international incidents on currency fluctuations has been recognized [3], suggesting that considering a wider range of information beyond traditional structured data may be beneficial. The great amount of textual data may contain valuable insights into market sentiment, economic trends, and key events that can influence exchange rates [4]. However, exchange rate forecasting presents two significant challenges. Firstly, while the relationship between news information and market trends is relatively straightforward in traditional financial markets, the complex semantics of market-driven news and analysis texts in the context of exchange rates pose difficulties for sentiment analysis. Secondly, traditional methods struggle to adequately capture the complex nonlinear patterns and unstructured"}, {"title": "Motivation", "content": "Traditional sentiment analysis methods in exchange rate prediction have primarily focused on brief texts such as Twitter posts and news headlines. These methods neglect the rich semantics and diverse information contained within longer content, which are essential for a thorough understanding of market sentiment. Additionally, the effectiveness of dictionary-based sentiment analysis heavily"}, {"title": "Related Work", "content": "1.Current studies on exchange rate forecasting\nHaider et al. [8] examine whether commodity prices can forecast exchange rates in commodity-dependent economies using both in-sample and out-of-sample techniques. By modeling commodity prices to predict USD rates, their findings indicate this approach is more effective than a random walk model, providing valuable perspectives across various economies. Sarkar and Ali [9] analyze linear regression for predicting EUR/USD exchange rates using normalized daily and hourly data. Their research applies this approach to different time series, offering strategies to help traders mitigate issues and enhance profitability in the forex market. Ruan et al. [10] evaluate whether economic policy uncertainty (EPU) outperforms traditional macroeconomic indicators in predicting exchange rate volatility in both developed and emerging markets. Their results demonstrate the superior predictive capability of EPU, suggesting significant implications for risk management and policy-making, and recommending broader application to verify these findings' generalizability. Windsor and Cao [11] develop a comprehensive system using market indicators and investor sentiments to predict the USD/CNY exchange rate. This innovative system effectively captures complex interactions among various financial factors, providing a precise and robust forecasting tool. Salisu et al. [12] demonstrate that oil prices are a reliable indicator of exchange rate returns for both net oil exporters and importers. Their study emphasizes the importance of considering asymmetries in the data, which substantially enhances the predictability of an oil-based model. The results underscores the potential of oil prices as a crucial factor in financial forecasting models. Neghab et al. [13] employ machine learning techniques, including linear regression, tree-based models, and deep learning, to forecast exchange rates based on macroeconomic fundamentals. The study addresses challenges such as nonlinearity, multicollinearity, time variation, and noise in modeling.\n2. The application of unstructured data in predictions\nIto and Takeda [14] improve the accuracy of exchange rate models by using sentiment indices constructed from Google search volumes of financial terms. This approach effectively captures timely market sentiments, although the generalizability of their findings requires further exploration due to the analysis's limited scope. Ben Omrane et al. [15] investigate the impact of US and EU macroeconomic news on the volatility and returns of the EUR/USD exchange rate using regime smooth transition regression. Their findings indicate that the effects of news vary between economic states, with US news generally having a larger impact than EU news on currency fluctuations. Li et al. [16] introduce"}, {"title": "Methodology", "content": "4.1 The IUS Framework\nIn this work, we introduce the IUS Framework, as illustrated in Figure 4, which consists of five technical components. The first component is the Sentiment Polarity Scoring Module (SPSM), which employs an embedding generator based on a fine-tuned version of the RoBERTa-Large model, specifically adapted for sentiment analysis. This module generates the sentiment polarity feature tensor (Sfx) from news and analysis texts related to the target exchange rate over D trading days, capturing the sentiment representation of all texts during this period. The second component, the Movement Classification Module (MCM), utilizes the original RoBERTa-Large model. Configured as an embedding generator, it produces the feature tensor (Mfx) for analyzing the next-day exchange rate movement, e.g., up or down, capturing the movement representation of texts within the same period. Subsequently, we extract quantitative indicator subsequences for the target exchange rate and its related exchange rates, as well as financial market indicators over D trading days. Efx and Ffx represent the quantitative features of these rates and financial indicators, respectively, over the same period. Finally, all these features are integrated and processed through a Causality-Driven Feature Generator, then input into the Bi-LSTM model to predict the closing price of the target exchange rate for the next trading day Yt+1.\n4.1.1 The IUS Framework\nFor the textual dataset, we collect data from investing.com and forexempire.com, covering the period from February 6, 2016, to January 19, 2024. The dataset includes all accessible data on these platforms within the period, totaling 35,427 texts. However, due to the potential existence of noise and irrelevant information, we utilize ChatGPT-4.0 and prompt engineering techniques to filter the raw dataset. This data annotation approach aligns with prior research, such as using LLMs for automatic data annotation to detect hallucinations [40], keyword annotation and document content description generation [41], and math problem knowledge tagging with few-shot learning [42]. After filtering, we observe that the news and analysis texts inherently contain a higher level of noise, which may be attributed to the necessity of catering to the diverse needs of readers. In addition, we notice that often only individual paragraphs or multiple segments within an article are directly relevant to the EUR/USD exchange rate. To further refine the dataset and extract the relevant segments, we employ ChatGPT-4.0 again to process the text data, creating a final textual dataset consisting of 20,329 texts. We employ ChatGPT-4.0 to annotate the sentiment polarity scores for the textual training dataset by integrating prompt engineering techniques. To safeguard against potential and unidentified errors, the model is also required to provide explanations for the polarity scores it assigns. In our prompt engineering, we define the polarity score range as [-1,1], where scores approaching 1 indicate a strongly positive sentiment, and vice versa. Scores near zero represent a neutral sentiment. In terms of next-day exchange rate movement, e.g., up or down, Ma is used to annotate the text data. Ma is defined as:\n$M_a = \\begin{cases}0, & CP_{d+1} < CP_d, \\\\1, & CP_{d+1} \\geq CP_d,\\end{cases}$\nwhere CPd+1 is the closing price of the exchange rate on trading day d +1 and CPa is the closing price on day d. We do not introduce an additional label for CPd+1 = CPd, as it is rare for the closing prices to be the same on two consecutive transaction days.\n4.1.2 ROBERTa-Large\nROBERTa-Large, developed by Facebook AI, is a SOTA pre-trained LLM incorporating several key enhancements to improve its training process and architecture [43]. Utilizing increased training data, larger batch sizes, and extended training periods, RoBERTa-Large has demonstrated outstanding performance across a variety of benchmark tasks, showcasing its robust capabilities. Its architecture, as illustrated in Figure 5(A), is based on the transformer model and primarily consists of a tokenization module and 24 encoder blocks. Each encoder block, detailed in Figure 5(B), includes a multi-head self-attention mechanism followed by a feed-forward neural network. The self-attention mechanism allows the model to focus on different positions within the input sequence, capturing the relationships"}, {"title": "Sentiment Polarity Scoring Module", "content": "In SPSM, our RoBERTa-Large model utilizes weights from the Twitter-RoBERTa-Large-2022-154m model, which is fine-tuned by the CardifNLP team on a large dataset containing 154 million tweets [44]. As shown in Figure 5(C), when using the RoBERTa-Large model for sentiment analysis, we expanded the model architecture with a module designed for regression. This module includes a Sigmoid activation function, two linear layers, and a mean squared error (MSE) loss function. The output of the RoBERTa-Large model initially passes through the first linear layer (LL1p(\u00b7)), which reduces the high-dimensional text representation from the final hidden state F of 1024 dimensions to a lower-dimensional space. A Sigmoid activation function (Sig(\u00b7)) then compresses this output to a range between 0 and 1, and this output is further transformed by the second linear layer (LL2p(.)) to produce the final feature tensor Sfx:\n$Sfx = LL2p(Sig(LL1p(F)))$.\nHere, $Sfx \\in R^{dxh\\times 128}$, d represents the total number of trading days and h indicates the maximum number of texts on all trading days. During training, the predicted sentiment score and the annotated polarity scores are fed into the MSE loss function [45-47]. The MSE loss function is defined as:\n$L_{MSE} = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2,$\nwhere yi is the annotated polarity score for the i-th text in the dataset, and \u0177i is the polarity score predicted by the model for the i-th text. n is the total number of texts over the training period. By minimizing the MSE loss function, the model learns the mapping relationship between text and sentiment scores. The backpropagation algorithm is used to compute gradients and update the parameters of ROBERTa-Large and SPSM, continually refining the predicted sentiment scores to approach the annotated scores. In addition, we discover that when using the Twitter-RoBERTa-Large-2022-154m model, convergence is faster, and performance on evaluation metrics is superior compared to the base model without fine-tuning, given the same number of training epochs."}, {"title": "Movement Classification Module", "content": "In MCM, we employ the RoBERTa-Large-Base model to uncover hidden patterns between textual information and the EUR/USD exchange rate movement on the following day [43]. As illustrated in"}, {"title": "Experience Rule", "content": "We also collect a financial indicator dataset, which primarily includes data related to EUR/USD exchange rates and financial markets, sourced mainly from financial platforms such as investing.com and finance.yahoo.com, among others. We utilize a comprehensive approach by extensively collecting and analyzing relevant literature to identify the indicators that potentially predict fluctuations in the target exchange rate. The construction of this financial indicator system, displayed in Table 1, is based on the evaluation of the relationships between these indicators and the target exchange rate, considering their leading, lagging, and potential non-linear relationships.\nAll collected raw financial data are fed into an indicator generator, which aligns the data and fills in missing values, producing the final quantitative features. We use linear interpolation to fill in these gaps:\n$v_i = v_a + \\frac{(v_b - v_a)(t_i - t_a)}{t_b-t_a}$\nhere, vi represents the interpolated value at the specific time ti. va and vb are the known values at the time points ta and tb, respectively. These known values are used to estimate vi, assuming a linear change between ta and tb."}, {"title": "Causality-Driven Feature Generator", "content": "We employ a Causality-Driven Feature Generator to extract text, exchange rate, and financial market features. Specifically, Figure 6 shows the stage of textual feature extraction, the feature tensor is fed into a feature extractor and produces various types of feature tensors. These produced tensors have the same dimensions as the original feature tensor, which are then processed through a task-specific linear layer, mapping the three-dimensional feature tensors to feature matrices. As for the stage of feature generation, the feature matrices are inputted into an average pooling layer to yield a diverse set of textual features. Subsequently, all textual features combined with other features undergo feature selection to obtain the final set of features inputted into the forecasting model."}, {"title": "Feature Generation", "content": "When generating features, we need to handle two types of features: Sfx and Mfx. In terms of Sfx, after obtaining the sub-tensors from the feature extractor FE(\u00b7), we input these sub-tensors into a linear layer LLS(\u00b7) specifically designed for processing Sfx, mapping them to the corresponding sentiment feature sub-matrix, Ms, each element of which is compressed and normalized to range between [-1,1]:\n$[Ms1, Ms2,..., Msn] = LLS(FE(Sfx)).$\nHere, Msi \u2208 Rdxh is the i-th feature sub-tensor obtained from Sfx through the feature extractor and subsequently compressed by a linear layer into the corresponding feature matrix, and n denotes the number of feature sub-tensors of the feature Sfx."}, {"title": "Feature Selection", "content": "We employ the VAR model to determine the optimal lag orders for all features within the final pre-diction feature set. The VAR model is an econometric model used to learn the dynamic relationships among multiple time-series variables, expressing each endogenous variable's current value as a linear combination of its own and all other endogenous variables' lagged values [107, 108]. The VAR(p) model can be represented as follows:\n$Yt = c + A_1Y_{t-1} + A_2Y_{t-2} + ... + A_pY_{t-p} + \\epsilon_t.$\nHere, Yt is an n\u00d7 1 vector of endogenous variables, cis an n\u00d71 vector of constants, A\u2081 are n\u00d7n coefficient matrices, and et is an n \u00d7 1 vector of error terms satisfying the white noise condition. To determine the optimal lag order p, we use the Akaike Information Criterion (AIC):\n$AIC(p) = ln \\big( det (\\sum_p) \\big) + \\frac{2pn^2}{T}.$"}, {"title": "Optuna-Bi-LSTM", "content": "4.3.1 Bi-LSTM\nThis study employs a Bi-LSTM model to analyze financial features and forecast EUR/USD foreign exchange rates. The Bi-LSTM is an efficient sequential learning model that enhances performance by integrating past and future feature information, demonstrating strong capabilities in time series forecasting [115-117]. Figure 9(A) illustrates the structure of our prediction model, where I' is inputted into two Bi-LSTM layers, Bi-LSTM1(\u00b7) and Bi-LSTM2(\u00b7), to identify temporal patterns for predicting EUR/USD exchange rate movements. The key to the model lies in the Bi-LSTM layers, which capture both forward and backward dependencies in the input time sequence. Figure 9(B) displays the structure of each Bi-LSTM layer, composed of a forward LSTM and a backward LSTM, processing the sequential information in both directions respectively. After each Bi-LSTM layer, a dropout layer, DP1(\u00b7) and DP2(.), is added to utilize regularization to reduce the risk of overfitting. The final hidden state from I' at the last time step, ht+1, can be expressed as:\n$h_{t+1} = DP2(Bi-LSTM2(DP1(Bi-LSTM1(I')))),$\nwhere ht+1 \u2208 C1\u00d7w, cl is the number of feature combinations in the current model, and w is the sliding window length. ht+1 carries all the features necessary for predicting the EUR/USD exchange rate movement on day t + 1. Finally, ht+1 passes through a fully connected layer (FC(\u00b7)) and an output layer (Output(\u00b7)), generating the predicted EUR/USD exchange rate \u0177t+1 for day t + 1:\n$\\hat{y}_{t+1} = Output(FC(h_{t+1})).$\nWe utilize the MSE loss function to measure the discrepancy between predicted values and actual values, updating the model parameters accordingly:\n$L_{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_{t+i} - \\hat{y}_{t+i})^2,$\nwhere yt+1 represents the actual price range of the EUR/USD exchange rate on day t+1. By minimizing the loss function, the model learns the relationship between the features within the rolling window and the subsequent trading day's EUR/USD exchange rate.\n4.3.2 Optuna optimization framework\nOptuna is an automatic hyperparameter optimization framework that efficiently searches for the optimal set of hyperparameters [118, 119]. Its core principle is to generate trials, each testing a different combination of hyperparameters, while using the results of previous trials to guide the sampling of parameters in subsequent trials. This sampling, combined with the ability to prune underperforming trials, allows Optuna to quickly converge on the best hyperparameter configuration.\nThe Figure 10 illustrates the process of hyperparameter optimization using Optuna. The process begins by defining an objective function for constructing, training, and evaluating the model. Subse-"}, {"title": "Experiment Results", "content": "6.1 Main Results\n6.1.1 Time series forecasting\nWe compare the performance of the Optuna-Bi-LSTM model with other benchmark models. The prediction results of different models for the EUR/USD exchange rate are shown in Table 4. It can be observed that our proposed method consistently outperforms other models in terms of both MAE and RMSE metrics, achieving an improvement of at least 10.69% in MAE and 9.56% in RMSE compared with the best other model.\nMoreover, as illustrated in Figure 12, the prediction curve obtained by Optuna-Bi-LSTM aligns more closely with the raw data curve and exhibits a higher degree of trend similarity. This demonstrates the superior predictive performance of our proposed model."}, {"title": "DM Test", "content": "To evaluate and compare the predictive performance of the eight models on the time series, we con-ducted the Diebold-Mariano (DM) test. The purpose of the DM test is to determine which models are statistically significantly superior to others in terms of prediction accuracy [1, 121, 122]. We first calculated the forecast errors for each model and constructed error difference series from them for pairwise comparisons between models. Using this approach, we performed a total of 28 DM tests, cov-ering all possible combinations of model pairs. The test results are summarized in Table 5, where the LSTM model demonstrates the best performance, while the GRU model exhibits the poorest predictive ability."}, {"title": "Window Size Analysis", "content": "In this study, we analyze the impact of different window sizes on the prediction accuracy of the models. The tested window sizes range from 1 to 24, with additional extended sizes of 30, 40, 50, and 60, to assess their influence on model performance. This analysis helps determine the optimal window size that maximizes the accuracy of time series predictions. The results of this analysis are presented in Figure 13. By observing the changes in MAE and RMSE across different models, each window size is evaluated. When the window size is 3, the models exhibit the best performance. As the window size increases, the model performance generally deteriorates."}, {"title": "Ablation Experiment", "content": "6.2.1 Textual Feature Breakdown\nWe investigate the relative predictive power of textual and other features by comparing the performance of using (1) only textual features (31 features), (2) only exchange rate and financial market features"}, {"title": "Conclusions and Future Work", "content": "To accurately forecast the EUR/USD exchange rate, we introduced the IUS framework and an Optuna-Bi-LSTM model. This framework integrates multi-source information, including news and analytical texts, other relevant exchange rates, and financial market indicators. We employ two large language models for sentiment polarity scoring and exchange rate movement classification, which are then com-bined with other quantitative indicators input into a Causality-Driven Feature Generator. All gener-ated features are fed into the predictive model for exchange rate forecasting.\nExperimental results demonstrate that compared to the strongest benchmarks, our method achieved the highest MAE and RMSE, improving by at least 10.69% and 9.56%, respectively. In terms of data fusion, by combining unstructured and structured data, the model is able to enhance prediction accuracy beyond what is possible with structured data alone. Furthermore, using the top 12 important features selected by the RFE method, combined with 31 textual features proves to be more effective compared to analyzing all textual features, as it more directly corresponds to the actual exchange rate response to market conditions. In summary, the proposed method achieves better performance in exchange rate forecasting and provides a comprehensive approach by integrating multi-source data for enhanced prediction accuracy."}]}