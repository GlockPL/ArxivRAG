{"title": "ELMO: Enhanced Real-time LiDAR Motion Capture through Upsampling", "authors": ["DEOK-KYEONG JANG", "DONGSEOK YANG", "DEOK-YUN JANG", "BYEOLI CHOI", "DONGHOON SHIN", "SUNG-HEE LEE"], "abstract": "This paper introduces ELMO, a real-time upsampling motion capture framework designed for a single LiDAR sensor. Modeled as a conditional autoregressive transformer-based upsampling motion generator, ELMO achieves 60 fps motion capture from a 20 fps LiDAR point cloud sequence. The key feature of ELMO is the coupling of the self-attention mechanism with thoughtfully designed embedding modules for motion and point clouds, significantly elevating the motion quality. To facilitate accurate motion capture, we develop a one-time skeleton calibration model capable of predicting user skeleton offsets from a single-frame point cloud. Additionally, we introduce a novel data augmentation technique utilizing a LiDAR simulator, which enhances global root tracking to improve environmental understanding. To demonstrate the effectiveness of our method, we compare ELMO with state-of-the-art methods in both image-based and point cloud-based motion capture. We further conduct an ablation study to validate our design principles. ELMO's fast inference time makes it well-suited for real-time applications, exemplified in our demo video featuring live streaming and interactive gaming scenarios. Furthermore, we contribute a high-quality LiDAR-mocap synchronized dataset comprising 20 different subjects performing a range of motions, which can serve as a valuable resource for future research. The dataset and evaluation code are available at https://movin3d.github.io/ELMO_SIGASIA2024/", "sections": [{"title": "1 INTRODUCTION", "content": "With the increasing need for virtual avatars in 3D content creation and metaverse platforms, real-time motion capture technology is experiencing a notable surge in demand. Despite the strides made by inertial and optical sensors-based solutions with great accuracy, they remain costly and cumbersome for average consumers. Various approaches address this limitation by reducing the number of body-worn sensors [Von Marcard et al. 2017] and exploring more accessible devices such as RGB cameras [Wei and Chai 2010]. However, RGB cameras and inertial sensors lack explicit information on global translation, leading to drifting in the output. Research utilizing depth sensors, such as RGB-D cameras [Zollh\u00f6fer et al. 2014] and LiDARs [Jang et al. 2023a], demonstrates enhanced global tracking. On the other hand, the measurements from depth cameras suffer from noise and LiDARs' low framerate introduces pose discontinuities in real-time applications, typically requiring higher framerates over 60 frames per second (fps).\nTo address these limitations, we propose ELMO, a novel real-time upsampling motion capture framework designed to derive 60 fps mocap data from a 20 fps point cloud sequence captured by a single LiDAR sensor. The core concept involves using a sequence of sampled point clouds from the past to the current and one future frame to generate three upsampled poses. Our motion generator adopts a conditional autoregressive transformer-based architecture considering past inferred motion and acquired point cloud to establish the relationship between the input point cloud and the output upsampled poses. To overcome self-occlusions in single LiDAR setups, our framework includes a mechanism for sampling a latent vector from a motion prior. This vector is then processed by the generator to predict plausible poses, particularly in scenarios involving occlusions.\nTo effectively capture the correlation between the LiDAR point cloud and the human body joints, we design motion and point cloud embedding modules such that joint features preserve the skeletal graph structure, a root feature captures global coordinate information, and point features find characteristics of local regions in the point cloud, dubbed body-patch groups. Leveraging the self-attention mechanism in our transformer generator, we facilitate the learning of attention between individual body-patch groups and human joints for each embedding feature. This approach significantly enhances the quality of the output motions.\nReal-time motion capture relies on accurately tracking global translation, crucial for seamless interaction between avatars and their environment or objects. Acknowledging the challenge of gathering comprehensive data across the capture space, we present a novel data augmentation technique leveraging a LiDAR simulator. We apply global rotations to each original motion clip and fit the SMPL body model [Loper et al. 2015] to compute collision points with simulated lasers. Implementing this augmentation technique on our training dataset resulted in a noticeable enhancement in the quality of the mocap results.\nFurthermore, we developed a one-time skeleton calibration model that infers user skeleton offsets from a single-frame point cloud acquired while the user is in the A-pose. Skeleton calibration is a fundamental step in motion capture, determining initial joint offsets, global trajectory, and joint rotations.\nFigure 1 presents snapshots of real-time mocap results from ELMO. To demonstrate the effectiveness of our framework, we conduct thorough comparisons with state-of-the-art image-based and point cloud-based methods, along with an ablation study to validate our design choices. Additionally, we conduct various experiments, such as testing for global drifting, to verify the essential elements required for accurate motion capture.\nTo the best of our knowledge, our work is the first real-time upsampling motion capture framework using a single LiDAR. By maintaining low latency, ELMO is well-suited for live application scenarios. Our demo video provides example use cases including live streaming and interactive gaming. The major contributions of our work can be summarized as follows:"}, {"title": "2 RELATED WORK", "content": "2.1 Motion Capture\nOptical and inertial systems stand out in the professional market for their high accuracy. However, a shared challenge across these mocap techniques is body-worn sensors that may restrict user motion or shift from their initial positions. Additionally, a time-consuming setup and calibration are required for the quality of captured data. A prominent research focus involves reducing the number of sensors and reconstructing full-body motion from a sparse setup [Huang et al. 2018; Jiang et al. 2022; Lee et al. 2023b; Ponton et al. 2023; Winkler et al. 2022; Yang et al. 2024]. While more accessible than previous methods, they still grapple with inherent issues of wearable sensors.\nConsequently, markerless methods [Aguiar et al. 2008; Bregler and Malik 1998] have garnered significant attention for their notable advancement in eliminating the need for body-worn sensors. Moreover, they enhance the accessibility of mocap by using widely available devices such as webcams and RGB cameras. Simultaneously, research is underway on both mono [Bazarevsky et al. 2020; Bogo et al. 2016; Huang et al. 2022; Kocabas et al. 2020; Kolotouros et al. 2019; Pavlakos et al. 2017; Shetty et al. 2023; Wei et al. 2022; Ye et al. 2023; Zhu et al. 2022] and multi-view camera [Amin et al. 2013; Burenius et al. 2013; Dong et al. 2022] methods. Mono-camera setups offer simplicity while multi-view systems excel in accuracy. While offering supplementary depth information, RGBD solutions [Baak et al. 2011; Mehta et al. 2017; Ying and Zhao 2021] face challenges due to their limited field of view (FoV) and resolutions compared to RGB cameras. This leads to noisy and unstable output poses.\n2.2 LiDAR-based Pose Tracking\nThe latest image-based human pose tracking methods [Goel et al. 2023; Li et al. 2023a] have shown impressive quality through a two-step process involving 2D keypoint extraction and SMPL body model [Loper et al. 2015] fitting. However, the accuracy of 3D keypoints and global translation is compromised during pose fitting in 2D image space.\nA promising solution is using LiDAR sensors, which provide accurate 3D point cloud data. This method also offers a comprehensive view of the subject's full-body information, not possible with sparse sensor setups. A seminal study by Li et al. [Li et al. 2022b] has demonstrated that LiDAR sensors can enhance the quality of captured poses from distances. The following research explores the fusion of LiDAR with IMUs [Ren et al. 2023] and RGB cameras [Cong et al. 2023] as a complement for capturing detailed 3D human poses. Recently, Human3D [Takmaz et al. 2023] has demonstrated remarkable performance in body part segmentation, relying solely on LiDAR point cloud data. Similarly, MOVIN [Jang et al. 2023a] performed skeletal motion capture with a single LiDAR sensor. Our approach addresses the limitations present in prior works, such as motion jitters and a low frame rate, enhancing the feasibility of LiDAR motion capture frameworks for real-time applications.\n2.3 Neural Generative Models for Human Motion\nGenerating natural human motion while minimizing laborious and time-consuming tasks has been among the central focuses in the field of computer graphics. Upon the widespread integration of deep neural networks, researchers developed technologies to generate human motion from various inputs, encompassing low-dimensional control signals, navigation goals, and text prompts.\nWithin the realm of neural networks, generative models such as GANs [Goodfellow et al. 2014] and VAEs [Kingma and Welling 2013] have demonstrated notable success in producing high-quality, natural motion. GANs find application in diverse areas, including character control [Wang et al. 2021], speech-driven gesture generation [Ferstl et al. 2019], and motion generation from a single clip [Li et al. 2022a]. Conditional VAEs [Sohn et al. 2015] are employed in motion generation methods that utilize additional constraints such as past motion sequences [Ling et al. 2020], motion categories [Petrovich et al. 2021], and speech [Lee et al. 2019; Li et al. 2020]. Recent works have focused on creating a motion embedding space [Lee et al. 2023a] and leveraging the latent space for tasks like motion in-betweening [Tang et al. 2023], retargeting [Li et al. 2023b], and style transfer [Jang et al. 2023b]. An alternative approach involves using normalizing flow [Aliakbarian et al. 2022; Henter et al. 2020], enabling exact maximum likelihood. In parallel, building on recent advancements in diffusion models, researchers have extended their application to language [Tevet et al. 2023; Zhang et al. 2022] and music [Tseng et al. 2023]-driven motion synthesis. Methods utilizing deep reinforcement learning frameworks also incorporate VAEs to establish prior distributions for character control [Won et al. 2022], skills [Dou et al. 2023], unstructured motion clips [Zhu et al. 2023], and muscle control [Feng et al. 2023]."}, {"title": "3 ELMO FRAMEWORK", "content": "Figure 2 (b) illustrates our upsampling motion capture framework from a single-front LiDAR sensor at runtime. Our framework effectively transforms 20fps LiDAR point cloud input into 60fps motion in real-time, with a latency of one 20fps frame\u00b9. From the inference point of view, the input to our framework is a previously inferred sequence of 60 frames (1 second) of motion, from past frame $i - (6s-1)$ to current frame $i$ (where $s = 10$), as well as a sequence of sampled point clouds from the past to the current at timestamps $i - 5s, i - 4s, i - 3s, i - 2s, i - s, i$, and a newly captured point cloud at a future frame $i + 3$. For the output, it generates the next three upsampled poses for frames $i + 1, i + 2$, and $i + 3$ at a rate of 60fps.\nOur model is based on an autoregressive conditional transformer-based architecture. At each time frame, we initially extract the features of joints, the root, and points. During the training phase, given the motion sequence as input, the motion distribution encoder $E$ generates a latent vector $z$, which is trained to shape the latent variable $z$ into a Gaussian distribution (Figure 3). At the same time, the motion generator $G$ takes the past and current motion sequence, a sampled point cloud sequence of past to one future frame, and the latent vector $z$ as inputs, and generates upsampled poses of the three future frames. During inference, the encoder $E$ is discarded. Instead, we pass a randomly sampled latent vector $z$ through the motion generator at each time frame, enabling the generation of plausible poses.\n3.1 Data Representation\nThe input and output of our framework consist of the 3D point cloud and poses. We start by defining a pose vector for a single frame $i$, which contains a joint vector and root vector. Following [Jang et al. 2023a], which suggests the importance of accurate global (world) coordinate information of the character's root for high-quality motion capture, we handle the joint vector and root vector distinctly. The joint vector $x_i$ includes all joint local information including joint local positions, rotations, velocity, and angular velocity with respect to the parent joint, denoted as $x_i = [x^t_i, x^r_i, \\dot{x}^t_i, \\dot{x}^r_i] \\in \\mathbb{R}^{n_j \\times 15}$, where $x^t_i \\in \\mathbb{R}^{n_j \\times 3}$, $x^r_i \\in \\mathbb{R}^{n_j \\times 6}$, $\\dot{x}^t_i \\in \\mathbb{R}^{n_j \\times 3}$, and $\\dot{x}^r_i \\in \\mathbb{R}^{n_j \\times 3}$. Here, $n_j$ denotes the number of joints. The root vector $r_i$ represents the global coordinates of each character, including the character's global root position, rotation, velocity, and angular velocity. Additional vectors, such as foot contact, are also incorporated. Specifically, we define the root vector as $r_i = [r^t_i, r^r_i, \\dot{r}^t_i, \\dot{r}^r_i, c] \\in \\mathbb{R}^{17}$, where $r^t_i \\in \\mathbb{R}^3$, $r^r_i \\in \\mathbb{R}^6$, $\\dot{r}^t_i \\in \\mathbb{R}^3$, $\\dot{r}^r_i \\in \\mathbb{R}^3$, and $c \\in \\mathbb{R}^2$ for foot contact label. Lastly, the input point cloud vector, representing the global position of human body points, is denoted as $p_i \\in \\mathbb{R}^{n_p \\times 3}$, where $n_p$ is the number of points.\nNote that the captured motion is 60 fps and the LiDAR input is 20 fps, to prevent confusion, all frames described from now on are in 60 fps.\n3.2 Motion and Point Cloud Embedding\nThe embedding process consists of two components: motion embedding and point embedding. The former extracts the spatial and temporal features of the input motion sequence, while the latter groups each point cloud into several local body-patch features.\nMotion embedding. For motion, we utilize distinct embeddings for joint features and root features, as shown in the left part of Figure 2 (a). For embedding inputs at frame $i$, we consider frames from $i - (s - 1)$ to $i$, denoted as $[x_i]_{(s-1)} = [x_{i-(s-1)},........, x_i] \\in \\mathbb{R}^{s \\times n_j \\times 15}$ for joint vectors and $[r_i]_{(s-1)} = [r_{i-(s-1)},..., r_i] \\in \\mathbb{R}^{s \\times 17}$ for root vectors. Note that we handle motions of length $s$ because our approach involves utilizing point cloud sequences sampled at the interval of $s$ as inputs."}, {"title": "3.3 Upsampling Motion Generator", "content": "Our motion generator utilizes a conditional autoregressive model, processing past inferred motion and acquired point cloud data as inputs. This establishes a relationship between the current point cloud input and upsampled output poses. By leveraging self-attention within this transformer-based architecture, our approach effectively learns the attention between body-patch point group features and motion features.\nTokenization. We implement a tokenization process for input into a Transformer-based Motion Upsampling Generator, utilizing three distinct token types as illustrated in Figure 2: the Combined Token $T_{comb}$, Masked Token $T_{mask}$, and Predicted Token $T_{pred}$.\nThe Combined Token $T_{comb}$ integrates the Motion Token ($T_{mot}$) and Point Token ($T_{point}$). The Motion Token consists of joint and root features, added with learnable spatial joint encodings. Conversely, the Point Token, representing the point features of $n_g$ body patches, lacks positional information. Therefore, a two-layer MLP is used to assign positional encodings to each center point of the body-patch groups, which are then added to the body-patch point features:\n$T_{mot} = [f^j_i, f^r_i] + P_{spat}, T_{point} = f^p_i + p_{cent}$\n$T_{comb} = [T_{mot}; T_{point}],$\nNext, the Masked Token $T_{mask}$ is a learnable masking token added with the same spatial positional encoding $P_{spat}$ that, upon passing through the upsampling motion generator, becomes the Predicted Token $T_{pred}$. The Predicted Token, or pose feature, is employed in reconstructing the upsampled poses. Notably, the Predicted Token (pose feature) corresponding to the masked token frame $i$ represents a pose feature for a single frame, as opposed to the motion feature that deals with the temporal sequence of $i-(s-1)$ to $i$ as used in the combined token. The pose feature also comprises joint features and the root feature.\nTransformer-based generator. After the tokenization process, as illustrated in Figure 2 (b), we create an input sequence by concatenating the combined tokens $T_{comb}$ corresponding to frames $i \u2212 5s$, $i4s$, $i3s$, $i - 2s$, $i - s$, and $i$ along with the Point token $T_{point}$ for frame $i + 3$. Lastly, we pad the masked tokens corresponding to future frames $i + 1, i + 2$ and $i + 3$. The temporal positional encoding $P_{temp}$ derived from sinusoidal functions for each time frame is then added to the input sequence.\nGiven the sampled latent vector $z$, the Upsampling Motion Generator $G$ is an autoregressive model that generates future $i + 1, i + 2$, and $i + 3$ pose features (predicted token) at the target frame rate (60fps), conditioned on the input sequence. These 3 pose features are further expanded using expanding modules to obtain the joint vectors $[x_{i+1}, x_{i+2}, x_{i+3}]$ and root vectors $[\\$\\hat{r}_{i+1}, \\hat{r}_{i+2}, \\hat{r}_{i+3}]$. We adopt the standard vision transformer for the generator $G$, consisting of multi-headed self-attention layers and FFN blocks. The expanding modules utilize inverse forms of the motion embedding modules. However, a difference is that the expansion is performed separately for the pose of each frame. The overall upsampling generator process is formalized as follows:\n$[T^{pred}_{i+1}; T^{pred}_{i+2}; T^{pred}_{i+3}] = G([T^{comb}_{i-5s}; T^{comb}_{i-4s}; T^{comb}_{i-3s}; T^{comb}_{i-2s}; T^{comb}_{i-s}; T^{comb}_{i}; T^{point}_{i+3}; T^{mask}_{i+3}; T^{mask}_{i+1}; T^{mask}_{i+2}; T^{mask}_{i+3}] + P_{temp}, z)$,\n$[x_{i+1}, x_{i+2}, x_{i+3}] = De-GCN(T^{pred}[:n_j])$, [\\$\\hat{r}_{i+1}, \\hat{r}_{i+2}, \\hat{r}_{i+3}] = De-Conv1D(T^{pred}[n_j: n_j+1])$, where $T^{comb} = [T^{comb}_{i-5s}, T^{comb}_{i-4s}, T^{comb}_{i-3s}, T^{comb}_{i-2s}, T^{comb}_{i-s}, T^{comb}_{i}]$.\n3.4 Motion Prior\nThe motion prior used to sample the latent vector $z$ is constructed via the motion distribution encoder $E$ as depicted in Figure 3. Due to self-occlusions among body parts commonly encountered by a single LiDAR sensor, the latent vector $z$, derived from a motion prior, assists the generator in accurately predicting plausible poses.\nTo effectively capture the spatial-temporal dependencies between past and current poses, we use a transformer architecture, akin to the generator $G$. The motion distribution encoder $E$ processes a learnable prior token $T_{prior}$ along with concatenated motion tokens $[T^{mot}_{i-5s};...; T^{mot}_{i}]$ as its inputs. These inputs facilitate encoding the parameters of a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma)$. The reparameterization trick is then applied to transform these parameters and obtain the latent vector $z \\in \\mathbb{R}^{C}$:\n$E(z; [T^{mot}_{i-5s};...; T^{mot}_{i}] + P_{temp}, T_{prior}) = \\mathcal{N}(z; \\mu, \\sigma)$\n3.5 Point Cloud Processing\nWhen acquiring point clouds from LiDAR in each frame, extraneous noise points that do not correspond to the human body are often captured due to the surrounding environment. Thus, as a preliminary step, we employ a Statistical Outlier Removal algorithm to filter out these irrelevant points. This method uses the mean distance of each point to its neighbors. Points that have a distance significantly larger than the average are considered outliers.\nAfter filtering, our framework is designed to handle an input point cloud with $n_p = 348$ points. In cases where the number of points acquired from LiDAR exceeds 348, we apply farthest point sampling (FPS) to reduce the count. Conversely, if the number of points is fewer than 348, we randomly select points and add small noise to generate synthetic points, effectively padding the dataset to the required size."}, {"title": "4 TRAINING THE ELMO FRAMEWORK", "content": "We train the entire framework end-to-end, optimizing for the loss terms detailed in Section 4.2. Our training process leverages all available data, both captured and augmented using the techniques outlined in Section 4.1.\n4.1 Data Augmentation\nThe primary goal of the augmentation is to make the training dataset cover the entire capture space (4\u00d74 meters for our dataset acquisition and experiments), as we use the global coordinate for the root. We employ two augmentation strategies: mirroring and rotating, as described at the bottom of Figure 4. For mirroring, we double the number of original data by flipping the skeleton and point cloud data. Furthermore, we augment each motion clip by applying global rotations of 90\u00b0, 180\u00b0, and 270\u00b0. However, unlike mirroring, rotating the point cloud around the global origin poses a challenge as a fixed LiDAR would capture different sides of the subject for rotated motion clips. To address this issue, we use a point cloud simulator.\nThe simulator is implemented with the Unity3D engine and the virtual LiDAR follows the Hesai QT128 specifications [hes 2023]. To compute collision points with simulated lasers, we use the SMPL body mesh, with its shape parameters manually adjusted to match the subject's skeleton. The top image of Figure 4 shows a snapshot of the resulting simulated point clouds using our simulator. Following our LiDAR placement guidelines to cover a 4m x 4m x 2.5m capture volume, the virtual LiDAR is positioned 3.5 meters from the center of the capture zone (global origin), 1 meter above the ground, and angled 20 degrees downward. During simulation, motion clips run at 60 fps, and point cloud data are captured every 3 frames (20 fps).\n4.2 Losses\nThe overall framework is trained by minimizing the reconstruction $L_{rec}$, velocity loss $L_{vel}$, and KL-divergence $L_{kl}$ losses. The reconstruction loss comprises joint feature loss on both local and global coordinates and root feature loss. The velocity loss is the difference between consecutive features. The reconstruction and velocity loss are computed for frames $i + 1, i +2$, and $i +3$. In addition, the KL-divergence loss regularizes the distribution of latent vector $z$ to be near the prior distribution $\\mathcal{N}(0, I)$.\nThe total loss function is thus:\n$L_{total} = L_{rec} + w_{vel} L_{vel} + w_{kl} L_{kl}$ $L_{rec} = ||x_i - \\hat{x_i}||_1 + ||FK(x_i) - FK(\\hat{x_i})||_1 + ||r_i - \\hat{r_i}||_1$ $L_{vel} = ||V(x_i) - V(\\hat{x_i})||_1 + ||V(FK(x_i)) - V(FK(\\hat{x_i}))||_1 + ||V(r_i) - V(\\hat{r_i})||_1,$\nwhere $V(x) = \\frac{x_t - x_{t-h}}{h}, V(FK(x)) = \\frac{FK(x_t) - FK(x_{t-h})}{h}$, $h$ is time step, and $w_{kl}, w_{delta}$ are relative weights. FK represents forward kinematics.\n4.3 Implementation details\nThe AdamW optimizer was used over 30 epochs with a learning rate of $10^{-4}$. The loss weights $w_{vel}$ and $w_{kl}$ were both set to 1. In the embedding module, the ST-GCN and 1D convolution comprise one layer along with temporal pooling to extract joint features and the root feature. We split the input point cloud into 32 body-patch groups, which are input to Mini-PointNet, composed of one set abstraction layer. The upsampling motion generator G comprises 3 vision transformer layers with 384 channels and 8 heads. The motion distribution encoder E has the same architecture as the generator. The expanding module has an architecture symmetric to the embedding module. To prevent covariate shifts during autoregressive inference, we set the prediction length to 20 frames for training. Scheduled sampling was also utilized to make the model robust to its own prediction errors, enabling long-term generation. Training took around three days using two 24GB RTX4090 GPUs."}, {"title": "5 SKELETON CALIBRATION MODEL", "content": "Commercial motion capture systems measure bone lengths directly [xse 2011] or optimize from pre-programmed marker sets [opt 2009]. Prior data-driven methods continuously predict body shape and motion together from input sequences [Jiang et al. 2023; Ren et al. 2024]. In line with commercial setups, we devise a one-time skeleton calibration model to precede the capture session. The model predicts the user skeleton offsets from a single-frame point cloud, acquired while the user is in the A-pose.\n5.1 Dataset Synthesis\nTo accommodate diverse body shapes among users, we generate a synthetic dataset comprising 50,000 pairs of an A-pose point cloud and initial joint offsets using our LiDAR simulator (Sec.4.1). Each data pair was generated through a randomized process to ensure the model's efficacy in real-world scenarios, despite being trained and validated solely on synthetic data. SMPL shape parameters of each subject are sampled in [-2, 2], covering 95% of the SMPL shape space. To enhance robustness against deviations in LiDAR placements in practice, we introduce random positional and rotational offsets to the virtual LiDAR, with maximums of 10cm and 5\u00b0, respectively. To further improve the model's robustness against variations in user A-poses, we apply random joint rotations: shoulder joints z-axis [65\u00b0, 45\u00b0], shoulder joints x-axis [-30\u00b0, 30\u00b0], hip joints y-axis [-30\u00b0, 20\u00b0], and hip joints z-axis [-20\u00b0, 4\u00b0]. In the final stage, the obtained SMPL joint offsets are retargeted to our skeleton hierarchy. Figure 5 presents example images of SMPL body mesh with random shape parameters in random A poses, corresponding skeletons, and simulated point clouds. The dataset was split for training and validation in an 80:20 ratio, resulting in 40,000 and 10,000 pairs, respectively.\n5.2 Calibration Model & Training\nOur skeleton calibration model is a simple 6-layer multi-layer perceptron with bias disabled. It takes as input a flattened vector of 384 3D points sorted by their global height, $x \\in \\mathbb{R}^{384\\times3}$. A preprocessing step, detailed in Sec. 3.5, is applied to sampled A-pose point clouds to ensure a fixed number of input points. Additionally, we apply noise ($\\mathcal{X} \\sim \\mathcal{N}(1cm, 1^2)$) to each position channel (x, y, z) of simulated points to enhance the robustness of our model in handling real-world noise induced by factors such as user outfits and hairstyles. Given this input, our model predicts skeleton offsets, including initial hip height and 20 joint offsets, represented as $y \\in \\mathbb{R}^{1+20\\times3}$. Training is conducted for 220 epochs to minimize mean squared error, utilizing the Adam optimizer with a learning rate of $5.0 \\times 10^{-6}$, and employing a batch size of 2048."}, {"title": "6 EVALUATION AND EXPERIMENTS", "content": "To assess the effectiveness of our framework, we conduct both quantitative and qualitative evaluations, comparing it with state-of-the-art (SOTA) methods in human motion tracking. Furthermore, we examine how the self-attention mechanism operates between body-patch point groups and joint features by constructing attention maps. Lastly, to showcase the real-time capability of ELMO, we present live-streaming scenarios with both single and multi-person setups and an interactive shooting game. For visual animation results, please refer to the supplementary video.\n6.1 Datasets\nWe construct the ELMO dataset, a high-quality, synchronized single LiDAR-Optical Motion Capture-Video dataset featuring 20 subjects (12 males / 8 females, heightcm $h \\sim \\mathcal{N}(170.66, 7.90^2), 155 \\leq h \\leq 180$). Our objective is to capture a wide range of motions, styles, and body shapes. We utilize a 4\u00d74 meter space, Hesai QT128 LiDAR, and an Optitrack system equipped with 23 cameras. The point cloud and mocap data were recorded at 20 and 60 fps, respectively. We split the 20 subjects into training and test sets with 17 and 3 subjects."}, {"title": "6.2 Quantitative Evaluation", "content": "We quantitatively compare our results with SOTA image-based pose tracking methods including VIBE [Kocabas et al. 2020", "2022": "and NIKI [Li et al. 2023a", "2023a": "serves as a primary comparison, utilizing the same LiDAR input as our framework. The quantitative evaluation assesses the methods using both MOVIN and our new ELMO dataset.\nQuantitative measures are defined in terms of the mean joint (J) and pelvis (P) position/rotation/linear velocity/angular velocity errors (M*PE, M*RE, M*LVE, M*AVE). We measure joint errors for the fixed pelvis and separately measure pelvis errors, as image-based methods cannot explicitly track global trajectory. The accuracy of pelvis prediction significantly influences overall motion quality, as errors at the root joint propagate along the kinematic chain of the joint hierarchy. For comparison, we retarget the SMPL output of NIKI to our skeleton hierarchy 2.\nSOTA comparison on ELMO dataset. The upper section of Table 2 compares our model's quantitative measures with baselines on the ELMO dataset. Given that MOVIN operates at 20 fps, we retrain it with a downsampled ELMO dataset, and its outputs are upsampled to 60 fps using duplication (w/ dup) and interpolation (w/ interp) for comparison.\nFor both joint and pelvis measures, our ELMO significantly outperformed the baselines. Compared to MOVIN upsampled with interpolation, the best among the baselines, the improvements are particularly notable in position measures, with a decrease of 2.19 cm in MJPE and 4.65 cm in MPPE. Additionally, our model showed performance increases ranging from a minimum of 47% (MJAVE) to a maximum of 75% (MPAVE), demonstrating its strength in capturing natural, non-linear pose transitions in both temporal and spatial spaces. This is further illustrated by the example outputs in panel (a) of Figure 12, where ELMO generates poses with greater accuracy (sitting, upper row) and faster reactions to input point clouds (running, bottom row).\nSOTA comparison on the MOVIN dataset. Since the MOVIN dataset only provides 20 fps motion capture (mocap) data, we retrain ELMO on a synthetic 60 fps MOVIN dataset using interpolation, and the outputs are then downsampled to 20 fps for comparison. The bottom part of Table 2 compares quantitative measures of our model with baselines on the MOVIN dataset. Overall, our ELMO outperformed the baselines in joint measures, except for MJRE. Despite a slight increase in MJRE by 0.88\u00b0 compared to MOVIN, ELMO showed a notable improvement in MJPE by 1.44cm, indicating superior accuracy in joint positions. We observed that ELMO especially performed better during occlusions, as shown in the bottom row of the panel (b) of Figure 12, where the MOVIN output incorrectly lifted the opposite arm. Moreover, ELMO significantly outperformed MOVIN in MPRE, surpassing it by 5.15\u00b0; the upper row of Figure 12 shows an instance where MOVIN exhibited a global y-axis flip in the output.\nAblation study. To assess the impact of our novel upsampling motion generator (Sec. 3.3) and data augmentation (Sec. 4.1), we conducted an ablation study in (a) ELMO and (b) MOVIN datasets. The baseline ELMO corresponds to the basic setup of our model, which predicts the poses of 3 future frames from the point cloud input of the current frame. In addition, we include a 20 fps output model without the upsampling scheme. For comparison within the ELMO dataset, the 20 fps outputs are upsampled using interpolation, denoted as ELMO20 w/ interp.\nTable 3 presents metrics from ablation models. Testing on the ELMO dataset revealed that incorporating a future frame input generally decreased errors, except for MPPE, which maintained comparable performance with a 0.08cm gap. This enhancement notably improved the model's accuracy in local body pose transitions compared to the baseline. Dataset augmentation yielded modest improvements in velocity errors but notably enhanced position and rotation errors for both body joints and pelvis. In the case of ELMO"}]}