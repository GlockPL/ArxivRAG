{"title": "A First Look at License Compliance Capability of LLMs in Code Generation", "authors": ["Weiwei Xu", "Kai Gao", "Hao He", "Minghui Zhou"], "abstract": "Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers. However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production. This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code. To establish this benchmark, we conduct an empirical study to identify a reasonable standard for \"striking similarity\" that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain open-source code. Based on this standard, we propose an evaluation benchmark LICOEVAL, to evaluate the license compliance capabilities of LLMs. Using LICOEVAL, we evaluate 14 popular LLMs, finding that even top-performing LLMs produce a non-negligible proportion (0.88% to 2.01%) of code strikingly similar to existing open-source implementations. Notably, most LLMs fail to provide accurate license information, particularly for code under copyleft licenses. These findings underscore the urgent need to enhance LLM compliance capabilities in code generation tasks. Our study provides a foundation for future research and development to improve license compliance in AI-assisted software development, contributing to both the protection of open-source software copyrights and the mitigation of legal risks for LLM users.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in Large Language Models (LLMs) have instigated revolutionary changes in the fields of artificial intelligence, natural language processing, and software engineering [1]. With billions of parameters trained on extensive corpora (both general and software engineering specific), LLMs have demonstrated extraordinary competencies in various software engineering tasks, such as code generation [2]\u2013[4], program repair [5]\u2013[7], and documentation generation [8]. Specifically, LLMs' remarkable code generation capabilities enabled the rapid adoption of AI coding tools in practice. As GitHub reports, 92% of US-based developers are already using AI coding tools both inside and outside of work [9].\nHowever, the widespread utilization of LLMs for code generation has also elicited concerns regarding security [10], [11], privacy [12], [13], and legal issues [14]\u2013[17]. A key issue among these is the potential infringement of intellectual property (IP) rights of a vast number of open-source developers [12], [17], [18]. Due to the fact that LLMs are trained on extensive volumes of open-source code governed by open-source licenses and their inherent ability to recognize and memorize patterns [12], they may generate code snippets that are similar or even identical to those in the training data under certain prompts [12], [13], [17]. Consequently, the use of these generated code must comply with the terms and conditions in the open-source license, concerning how a piece of open-source software (OSS) can be reused, modified, and redistributed [19]\u2013[21]. Licenses, which are unavoidably linked to the open-source code, serve a dual purpose: they not only enforce the developers' commitment to sharing, transparency, and openness [13], [14], but also necessitate that those who reuse the code respect and adhere to the terms set by the original authors [20], [22], [23]. For example, under Apache 2.0 [24], authors grant users perpetual copyright and patent rights, but users must also comply with the corresponding license terms such as including attribution notices and modification statements when redistributing the software.\nDespite their extraordinary competencies, LLMs often overlook license information when they memorize and generate code identical to the works of open-source developers. This has sparked a growing wave of concern and discontent among the open-source community [25]\u2013[28]. For example, an open-source developer accuses Copilot of \"emitting large chunks of my copyrighted code, with no attribution, no LGPL license.\" [26] More than just ethical concerns, these actions by LLMs may also pose unpredictable legal risks to users. For example, if LLMs produce open-source code without providing the necessary license information, users are naturally unable to utilize such code in legal compliance processes [12], [17]. It should be noted that OpenAI, GitHub, and Microsoft are currently facing lawsuits, as GitHub Copilot [29] has been implicated in reproducing licensed code without compliance with the corresponding license terms [30]. These controversies underscore that the license compliance capabilities of LLMs, i.e., the ability to provide accurate license and copyright information during code generation, play a crucial role in both protecting the IP rights of numerous open-source developers and shielding users of such models from unforeseen legal risks. Therefore, it is important to evaluate the license compliance capability of LLMs for code generation tasks.\nTo the best of our knowledge, it remains unclear how well"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "LLMs perform in terms of license compliance despite the substantial effort devoted to evaluating LLMs' performance on code completion accuracy [31]\u2013[33], robustness [34], [35], security [10], [36], and privacy [12], [37], among others. To fill this knowledge gap, this paper makes the first attempt to evaluate the license compliance capabilities of LLMs by establishing a novel benchmark. This evaluation of LLMS encompasses considerations of not only a technical nature but also legal aspects. Specifically, one of the most challenging aspects of assessing the compliance capabilities of LLMs is determining whether their outputs, when similar to specific open-source code snippets, are derived from those snippets or are independently created yet merely coincidentally similar. The former scenario would entail issues of license compliance, whereas the latter does not. A key and universally acknowledged principle used in law to aid in this distinction is \"access and substantial similarity\" [38], [39], which means that potential infringement only occurs when a party has access to a work and the resulting product is substantially similar to that work. Alternatively, in the absence of evidence indicating direct access, the existence of copying can be established when the degree of similarity between two works is so striking as to preclude the possibility of independently arriving at identical outcomes [38], [40]\u2013[42]. Considering that most LLMs' training data are undisclosed, it is challenging to determine and verify whether an LLM has accessed a specific code snippet. Therefore, we focus on scenarios where the outputs of LLMs are strikingly similar to open-source code, to assess their compliance capabilities in this context.\nAs demonstrated in Figure 1, to establish the benchmark, we first conduct an empirical study to explore a reasonable standard of striking similarity, to distinguish between cases where LLMs generate code without prior exposure to similar open-source instances and cases where LLMs memorize open-source code in the training data. Based on the findings of this empirical study, we propose an evaluation benchmark, LICOEVAL, for LLMs' license compliance capability evaluation, and then evaluate 14 popular LLMs currently in use for code generation. We find even top-performing code generation LLMs produce a non-negligible proportion (0.88% to 2.01%) of strikingly similar output compared to existing open-source code. Most models fail to provide any license information for code snippets under copyleft licenses, with only Claude-3.5-sonnet demonstrating some ability to correctly provide license information for such code. These results highlight the urgent need to improve license compliance capabilities in LLMs used for code generation, particularly in handling code under copyleft licenses and providing accurate license information.\nIn summary, the contributions of this paper are as follows:\n\u2022 We conduct an empirical study to explore a preliminary standard for \"striking similarity\" under the legal context of intellectual property infringement and LLM's capabilities in independently generating non-trivial code.\n\u2022 We design a framework for evaluating the license compliance capabilities of LLMs in code generation and provide the first benchmark for evaluating this capability.\n\u2022 We evaluate the license compliance capabilities of 14 popular LLMs, providing insight into improving the LLM training process and regulating LLM usage."}, {"title": "A. License Compliance and IP Infringement", "content": "Open-source software authors grant copyrights and patent rights within IP right through open-source licenses. However, this grant is conditional and the licenses specify the obligations to which users must comply [43]\u2013[46]. It is important to note that open-source authors do not oppose or prohibit the reuse of their code; in fact, such reuse aligns precisely with their foundational motives for choosing to open-source their code. Nevertheless, they require users to comply with the stipulations set forth in the licenses when reusing the code. Failure to do so can lead to license non-compliance, ultimately resulting in intellectual property infringements [47]\u2013[49].\nIn the context of LLM code generation, if models output licensed open-source code without providing license information, users reusing this code might face compliance risks. This concern is particularly relevant for companies and organizations, as it could lead to copyright infringement liabilities and potentially result in significant economic compensation and harm to their reputation and operations [49], [50].\nIn the judicial context, it is crucial to determine whether the content generated by LLMs has a copying relationship with specific open-source code. According to existing legal principles, copying can be established when similarities between two works are so striking that they preclude the possibility of independently arriving at the same result [38], [40], [51]. However, the definition of \u201cstriking similarity\u201d"}, {"title": "B. Memorization in LLMs for Code", "content": "remains ambiguous. Courts consider several factors in their analysis, such as the uniqueness, intricacy, or complexity of similar sections, and the appearance of the same errors or mistakes in both works [41]. The ambiguous definition, coupled with the impressive capabilities of LLMs in independently generating non-trivial code pieces, motivates our empirical study to explore reasonable standards for identifying striking similarity in the context of LLM generated code.\nResearch consistently demonstrates that general LLMs tend to memorize content from their training sets, especially in larger models [52], [53], raising significant concerns regarding IP rights violations [18], [53], [54]. This memorization also occurs in LLMs for code, potentially causing compliance issues through unintentional output of licensed code [12], [13], [17]. Recent studies on memorization [12], [13] and IP infringement [17] in LLMs for code, particularly those trained on non-public datasets, typically compare LLM outputs with existing open-source code to detect memorization. However, with undisclosed training sets, it is challenging to definitively attribute this similarity to memorization rather than LLMs' extraordinary generalization competencies. This ambiguity extends to numerous code clone detection methods, which describe the degree of similarity between code snippets from various perspectives, such as textual similarity, call graphs, and other structural features [55]. Yet, we still lack a clear understanding of what degree of similarity is sufficient to constitute \"striking similarity\" that can exclude the possibility of independent creation.\nFurthermore, it remains uncertain whether LLMs can accurately provide the corresponding copyright and license information for code they appear to have memorized. The most relevant work by Yu et al. [17] investigates to what extent LLMs generate licensed code. However, their study was conducted under a problematic assumption that LLMs should not generate licensed code at all, which fundamentally misunderstands the nature of open-source software. Open-source code, by definition, permits reuse under specific conditions; mere reuse does not inherently constitute infringement. The key issue lies in whether the reuse complies with the specified license terms, which, however, is not investigated in their work and requires non-trivial efforts in the LLM context. To address these challenges and uncertainties, we conduct an empirical study to establish a standard of striking similarity and build a benchmark to evaluate LLM's compliance capability."}, {"title": "C. Evaluations of LLMs for Code Generation", "content": "In the realm of code generation with LLMs, numerous benchmarks have been introduced to evaluate these models' capabilities [31]\u2013[33], [56]. These benchmarks typically focus on generating code snippets from natural language descriptions, employing metrics such as Pass@k [31] to assess the accuracy of the generated code. Furthermore, many studies separately investigate the non-functional properties of LLMs for code [57], including robustness [34], [35], security [10], [36], privacy [12], [13], [37], and explainability [58]. However, to our knowledge, there has been no evaluation focusing on the compliance capabilities of these models.This lack of evaluation is disadvantageous for users seeking models with lower legal risks and also hinders model developers from making targeted improvements in this critical area during the training process, which motivates our work."}, {"title": "III. EMPIRICAL STUDY ON STANDARD OF STRIKING SIMILARITY", "content": null}, {"title": "A. Research Question", "content": "RQ: Where might the reasonable standard of striking similarity lie in the context of code generation by LLMs?\nA significant challenge in evaluating the compliance capabilities of LLMs is determining whether there exists a copying relationship between the output of LLMs and existing open-source code. Due to the remarkable code generation abilities of LLMs, it is plausible that they can independently generate code that is similar to a specific open-source code snippet, even without prior exposure to it. If the code is independently generated, it clearly does not necessitate the provision of corresponding copyright information.\nIn accordance with the legal principle of striking similarity [38], [40], [51], we aim to explore where the reasonable standard of striking similarity might lie for LLMs. The empirical standard explored in this study lays a foundation for our subsequent evaluation framework and benchmark. Our exploration strives to align with copyright law principles in identifying a relatively reasonable standard that can, to some extent, evaluate the compliance risks associated with using certain LLMs. Notably, our findings are not intended to establish definitive legal boundaries. Instead, they are intended to serve as a guide for understanding and identifying potential compliance issues, offering insights that can inform future research and development in this rapidly evolving field."}, {"title": "B. Selection of LLMs", "content": "We aim to select representative LLMs to explore the striking similarity standards, with the following principles:\n\u2022 High accuracy in code generation tasks, as license compliance has no meaning without accuracy (a model can generate very chaotic, functionally incorrect code that naturally does not resemble existing open-source code)."}, {"title": "C. Experiment setup", "content": "\u2022 All training data including the instruction tuning data is public. We need to ascertain the data that the model has been exposed to.\nWe identify three widely used open-sourced datasets for training LLMs on the Huggingface platform, i.e., The Stack [63], The Stack v2 [64], and CodeParrot Dataset Cleaned [65]. Based on these datasets, we find five popular LLMs trained on these datasets as listed in Table I. Among these LLMs, we select WizardCoder-15B-V1.0 as our research subject for two reasons. First, it shows an acceptable performance with a Pass@1 score of 52.4 on the HumanEval benchmark. Second, it has a manageable size (i.e., 15B parameters) under our hardware constraints, which is comparable to, or even larger than the LLMs studied in previous work [12], [13], [17]. WizardCoder\u00b9 uses an open-source instruction-following dataset (Code-Alpaca5 [66]) to fine-tune the StarCoder trained on The Stack dataset. The complete open-source nature of its training datasets facilitates comprehensive analysis and verification. Despite Starcoder2-15B-Instruct's higher Pass@1 score on HumanEval, we chose WizardCoder for our analysis due to its smaller, more manageable dataset (200B tokens vs. 900B tokens in Starcoder2-15B-Instruct's).\nIn order to detect striking similarity, we design an experiment as follows.\nFirst, we construct two distinct groups of code samples, UNSEEN and ACCESSED, to simulate two different scenarios. The first scenario is where the model independently completes the corresponding code, as it has not been exposed to the code from the UNSEEN group before. The second scenario pertains to instances where the generated content cannot be regarded as independently created, potentially implicating a copying relationship. This is attributed to the fact that samples within the ACCESSED group are derived from the model's training dataset. We select two groups of code samples for this study as described in Section III-D1.\nSecond, we construct prompts using the UNSEEN and ACCESSED groups, then instruct WizardCoder to complete the code snippets. Our goal is to observe potential differences in similarity when the model generates code for these two distinct groups. As illustrated in Figure 2, we divide the function-level code snippets into five parts: file header comments, import statements and global variables, function signature, docstring, and function body. We combine the first four parts into a prompt with the aim of providing the model with as complete an input context as possible, and then instruct the model generate the function body. We conduct experiments in a one-shot fashion using greedy decoding (temperature set to 0).\nThird, we select specific features derived from copyright law principles to characterize striking similarity as described in Section III-D2. By analyzing the similarity between the LLM's output and the original code snippets when completing tasks from both groups, we aim to identify a relatively reasonable standard for striking similarity. If the generated code meets this standard for striking similarity, it can be inferred that the code may not be an independent creation by the model, indicating a possible copying occurrence."}, {"title": "D. Method", "content": "1) Construction of Code Samples: Figure 3 illustrates the overview of the construction method. Our code samples originate from two sources: the training set of WizardCoder, i.e., Starcoderdata [67], and Codeparrot-clean dataset [65]. Starcoderdata, a subset of The stack, comprises 783GB of code spanning across 86 programming languages. A license filtration mechanism was implemented during the data collection process for Starcoderdata, which guarantees that all code files within Starcoderdata are sourced from the repositories under permissive licenses [3]. Codeparrot-clean dataset comprises 5,361,373 Python files from GitHub, including those under restrictive licenses such as GPL-3.0 [68]. The former is utilized to construct the ACCESSED group that WizardCoder has been exposed to, while the latter is used to construct the UNSEEN group that WizardCoder has never encountered before.\nGiven that LLMs are not yet adept at performing code completion beyond the granularity of functions [32], we choose to conduct our experiments in alignment with well-known accuracy benchmarks such as HumanEval [31], specifically at the function-level granularity. Moreover, following the existing research on memorization of LLMs for code [12], [13], our study focuses on Python considering its prevalence [69].\nTo construct the ACCESSED group, we first extract 74,772,489 function-level code snippets from Starcoderdata's Python files. We further select samples based on the following principles, resulting in 2,628,395 function-level code snippets: 1) the function must have a docstring and the function body must be more than six lines (the median value of all functions' lengths), which aims to provide LLMs with more comprehensive descriptions of the function's capabilities and exclude overly trivial code snippets; 2) the function should not be a class method due to their typically more complex context dependencies [32]; 3) the function-level code snippet has no syntax errors. Eventually, we randomly sample 10,000 functions as the ACCESSED group that WizardCoder has been exposed to.\nTo construct the UNSEEN group, we select code files from the repositories authorized under restrictive licenses in Codeparrot-clean dataset, and segment them into functions. Given that Starcoderdata excludes code from such repositories, it's highly probable that WizardCoder has not encountered these code snippets in its training set. This makes these functions our preliminary candidate samples. To ensure these functions are truly unseen by WizardCoder, we need to compare each function with the entire training set. However, due to the large scale of the dataset, direct comparison of each function with every training sample is impractical. We therefore adopt the deduplication strategy used in Starcoderdata [3]. We calculate a minihash [69] for each function in the training set and use Locality-Sensitive Hashing (LSH) to efficiently map similar functions into the same bucket, using 5-grams and a Jaccard similarity threshold of 0.2 (more stringent than the 0.5 threshold used in Starcoderdata) for this process. Then, we compute the minihash for each function in preliminary candidate samples from Codeparrot-clean dataset and query these in the LSH buckets of the training set. Functions without similar matches in the training set are considered final candidates. We apply the same three principles used in constructing ACCESSED group and sample 10,000 samples from 157,273 final candidates to form the UNSEEN group.\nEventually, we obtain the following two groups of code snippets:\n\u2022 UNSEEN: 10,000 Python function-level code snippets that WizardCoder has never been exposed to.\n\u2022 ACCESSED: 10,000 Python function-level code snippets from the training set of WizardCoder.\nTable II presents some feature statistics of these samples in two different groups, including the number of prompt lines, the number of function body lines, the cyclomatic complexity [70] of the function, and the number of comments in function body. We compare these features between two groups using Mann-Whitney U Test [71] and Cliff's Delta effect size test [72]. The Mann-Whitney U test determines whether there is a significant difference in their median values, while the Cliff's Delta quantifies the extent of this difference [73]. Our analysis reveals that although the p-value is less than 0.01, the effect size is negligible. This suggests that there are no substantial inherent differences in the code characteristics between the two groups."}, {"title": "E. Results", "content": "2) Features to characterize String Similarity: Considering that copyright law only protects expression rather than ideas [41], [74], we employ fundamental text similarity metrics including BLEU-4 [75], Jaccard similarity based on minihash [3], [76], and similarity based on edit distance [1], [77] to measure the similarity between the function bodies output by the model and the original implementations in both ACCESSED and UNSEEN groups.\nInspired by judicial considerations of striking similarity in fields like music [41] and code's unique traits, we incorporate additional literal features that capture both structural and stylistic elements to characterize striking similarity, including the number of function body lines, cyclomatic complexity, and comment similarity. The rationale behind this is as follows:\n\u2022 Number of function body lines. Given the inherent nature of LLMs in generating code through token-by-token prediction [12], it implies that in scenarios of independent creation, longer code lengths reduce the likelihood of achieving occasional similarity.\n\u2022 Cyclomatic complexity [70]. Determined by decision points, it increases potential paths through a function. More decision points expand the possibility space, significantly reducing the chance of occasional similarity.\n\u2022 The similarity of comments. Different developers have unique habits when it comes to writing comments. Unlike code, natural language possesses a higher degree of flexibility [78], making the similarity in comments within code exceedingly rare.\nFigure 4 shows the similarity between the outputs generated by WizardCoder and the corresponding open-source code in two groups. When WizardCoder completes code for UNSEEN group, which simulates scenarios of independent creation, it generally produces code with lower similarity to the corresponding open-source code compared to its output for the ACCESSED group, across all similarity metrics. However, we observe some UNSEEN cases with exceptionally high similarity, occasionally reaching a score of 1. These findings suggest that while text similarity metrics can provide indication of differences in generated code between the two scenarios, they are insufficient on their own to definitively determine striking similarity and, consequently, non-independent creation.\nFigure 5(a) illustrates the distribution of similarity between the generated code and the corresponding open-source code in two groups, in relation to the number of function body lines of the open-source code. We observe that only for ACCESSED group does the LLM frequently generate highly similar code snippets (similarity > 0.6) for functions with longer body lengths (> 10 lines). In contrast, for longer functions from UNSEEN group, the similarity scores are generally lower. As shown in Figure 5(b), a similar phenomenon is observed with cyclomatic complexity. Meanwhile, Figure 5(c) reveals a stark contrast in generated comments between the two groups. For UNSEEN group, the LLM rarely generates comments identical to those in the open-source code. However, for the ACCESSED group, it frequently produces comments that match those in the open-source code. In extreme cases, the identical comments can exceed 15 sentences.\nThese observations reveal distinct patterns in code generation between ACCESSED and UNSEEN groups. Such distinctions suggest that certain combinations of these features might effectively indicate non-independent creation. To formalize this insight, we seek to establish a quantitative standard for identifying instances of striking similarity. Based on our analysis of WizardCoder's code generation results in these two simulated scenarios, we establish an initial standard for striking similarity between generated code and the open-source code:\n\u2022 number of function body lines > 10\n\u2022 cyclomatic complexity > 3\n\u2022 text similarity (maximum of the three metrics) > 0.6\n\u2022 number of identical comments > 0\nThe first two criteria are attributes of the open-source code snippet, while the latter two describe the relationship between the generated code and the corresponding open-source code snippet. Under this standard, we identify 24 instances, all from ACCESSED group, i.e., the non-independent creation scenario. In the simulated independent creation scenario (UNSEEN group), none of the 10,000 functions generated by WizardCoder meets this standard."}, {"title": "F. Validation", "content": "Through the above experiments, we establish a preliminary standard for striking similarity between LLM outputs and open-source code. This standard, when met, suggests a potential copying relationship rather than independent creation. To validate the preliminary standard, we employ a three-step process: constructing new code samples, expanding our analysis to additional LLMs, and conducting an expert evaluation to assess the standard's validity and applicability.\nFirst, using the same methods employed in constructing ACCESSED and UNSEEN groups, we create two additional groups: ACCESSED_EVAL and UNSEEN_EVAL, each containing 10,000 samples that do not overlap with the original ACCESSED and UNSEEN groups. We then utilize WizardCoder and Poro-34B-chat to complete the functions for these 20,000 samples. We choose Poro because it is a general model, yet shares the same code-related training data with WizardCoder. This similarity in training data allows us to use the same two groups of samples we constructed, while also testing the standard's applicability to a more general model.\nAmong the outputs generated by these two models, 31 from WizardCoder and 2 from Poro meet our standard of striking similarity. All 33 samples are from the ACCESSED_EVAL group, yielding a precision of 100%. To further validate the standard, we assemble a diverse panel: five developers with over six years of coding experience and three lawyers specializing in software intellectual property. This composition ensures both technical and legal perspectives in evaluating these 33 samples. The eight reviewers are tasked with determining whether independent creation can be ruled out based solely on the comparison between the code pairs, without knowledge of their origins. This approach ensures the standard's applicability to outputs from more advanced models, as the evaluation focuses purely on code characteristics. On average, each reviewer identifies 32 out of the 33 sample pairs as cases where independent creation can be excluded, implying a potential coping relationship. Based on these results, we believe that our proposed standard serves as a reasonable preliminary standard for identifying instances with striking similarity that may indicate potential legal risks. While not intended to establish definitive legal guidelines, this standard offers insights for exploring the complex landscape of AI-generated code and associated copyright concerns."}, {"title": "IV. EVALUATION FRAMEWORK AND BENCHMARK FOR LLM LICENSE COMPLIANCE", "content": null}, {"title": "A. Evaluation Framework", "content": "As illustrated in Figure 6, we propose a framework to evaluate LLMs on license compliance in code generation. This framework is grounded in the empirical findings that LLMs, in non-independent creation scenarios, may generate code strikingly similar to existing implementations, accompanied by identical comments. This observation leads to a crucial question: If LLMs can reproduce code and comments with high fidelity, can they also accurately output the associated license information typically found in file comments? Our framework builds upon this insight, positing that when an LLM generates code strikingly similar to existing code, it should also be capable of providing the corresponding license or copyright information. This principle forms the foundation of our evaluation methodology, linking the generation of strikingly similar code to the ethical and legal responsibility of proper attribution and license compliance.\nThe framework operates by first constructing a benchmark comprising functions from widely reused code files that have explicit copyright information in their file header comments. The detailed construction method is elaborated in SectionIV-B. The structure of the functions in this benchmark aligns with that as shown in Figure 2. Our prompt consists of the first four components, and we task the LLM to complete the function body. Subsequently, we conduct a similarity analysis between the LLM's output and the corresponding open-source code snippet. If the output meets our established standard for striking similarity, we prompt the LLM, in the form of a follow-up inquiry, to output the license information. Finally, we compare the license information provided by the LLM with the actual license of the open-source code snippet."}, {"title": "B. Benchmark LICOEVAL", "content": "1) Construction Method: We construct our benchmark named LICOEVAL by mining code files from the open-source ecosystem that have explicit license information and are widely reused. This process adheres to the standards we previously established. The specific steps are as follows.\nData Source\u2014World of Code. We utilize World of Code (WoC) [79] as the source for constructing the benchmark. WoC is a comprehensive infrastructure for mining version control data across the entire open-source software ecosystem. It aggregates Git objects, including commits, trees, and blobs [80] on platforms like GitHub, Bitbucket, and GitLab. WoC provides several key-value databases that enable efficient querying of relationships between different entities. For instance, the blob-to-project (b2p) database maps each blob to all projects that contain it, enabling efficient tracking of code reuse across projects. For our benchmark, we use version U of WoC, released in October 2021. This version encompasses over 173 million Git repositories, 3.1 billion commits, 12.5 billion trees, and 12.4 billion blobs [81].\nCollecting Python Code Files. Our first step utilizes WoC's c2fbb database, which maps each commit to its corresponding commit file name, new blob (post-commit), and old blob (pre-commit). We filter these commits to focus on Python files based on file extensions. By selecting the new blobs associated with these commits and removing duplicates, we obtain a dataset of 700,867,958 unique Python file blobs.\nCollecting licensed function-level code snippets. We identify blobs containing explicit license information in their file header comments. Using the b2p database, we quantify each selected blob's occurrence across different projects. We then sort these license-containing blobs in descending order of project count, aiming to identify widely reused code files with clear license information. The presence of license information in file headers indicates clear copyright attribution, while widespread reuse suggests general acknowledgment and acceptance of this copyright information.\nWe iterate through the sorted blobs in descending order, segmenting each file into function-level code snippets. We then apply a filtering process to select snippets that not only adhere to the three principles described in Section III-D1 but also meet the preconditions of our striking similarity standard (function body > 10 lines, complexity > 3, and comment number > 0). From this filtered set, we select the top 10,000 qualifying code snippets as candidate samples. We further refine these 10,000 candidate samples removing code snippets that are explicitly indicated as copied or derived from other sources (identified by keywords like \u201ccopied from\" or \"taken from\") and excluding code snippets with dual licenses to ensure license clarity. We also perform deduplication based on function signatures and docstrings, retaining only one instance where these elements are identical. This rigorous filtering process ultimately yields 4,187 unique function-level code snippets for our LICOEVAL benchmark.\""}, {"title": "License Information Accuracy", "content": "2) Benchmark Characteristics: The detailed characteristics of LICOEVAL are as follows.\nWe employ a keyword and rule-based method proposed by Xu et al. [19] to identify licenses. To evaluate the accuracy of license information in LICOEVAL, we randomly sample 352 samples (95% confidence level, 5% confidence interval [82]). We manually review the file header comments and function docstrings of these samples to verify the correctness of the license information extracted from the file headers and to check for any exception statements in the docstrings. We find that the license information for all 352 samples is correct. Therefore, we believe that the license information in LICOEVALis highly reliable and can serve as a robust foundation for evaluating the compliance capabilities of LLMs.\nLicense Distribution. Licenses can be categorized into three types based on their level of permissiveness: Permissive, Weak Copyleft, and Strong Copyleft [19]. Copyleft licenses mandate that software which modifies or utilizes existing software must be licensed under the same terms, unless explicitly specified otherwise (e.g., GPL-3.0). It is crucial to note that permissive licenses also require adherence to their terms. A license is essentially a conditional authorization [43], and failure to comply with conditions stipulated in permissive licenses can also result in non-compliance issues [23], e.g., both Apache-2.0 and BSD-3-Clause licenses require users to provide a statement of changes made to the original software.\nTherefore, in LICOEVAL, we retain a natural distribution of licenses, covering the three different types of licenses. As shown in Figure 7, out of 4,187 function-level code snippets, the number of licenses for Permissive, Weak Copyleft, and Strong Copyleft are 3,073 (73.4%), 369 (8.8%), and 745 (17.8%), respectively. Among permissive licenses, Apache-2.0 is the most prevalent, accounting for 2,596 snippets (62.0% of the total). For copyleft licenses, the most common are GPL-2.0-or-later with 375 snippets (9.0%), GPL-3.0-or-later with 220 snippets (5.25%), and MPL-2.0 with 163 snippets (3.9%).\nCode Metrics. Table III presents statistics of LICOEVAL, including the number of lines in function bodies (#body_lines), cyclomatic complexity, the number of comments within function bodies (#comments), and other relevant metrics. Due to our application of preconditions for striking similarity during the selection process, all functions in the benchmark satisfy the criteria of #body_lines > 10, #comments > 0, and cyclomatic_complexity > 3. The metric #reuse_projects indicates the number of projects that have reused the blob containing the"}, {"title": "V. EVALUATING LLMS ON LICENSE COMPLIANCE", "content": null}, {"title": "A. Experiment setup", "content": "We evaluate 14 popular LLMs that exhibit strong performance in code generation tasks (Pass@1 > 0.5 on HumanEval) using LICOEVAL. Table IV presents the complete list of evaluated LLMs. Throughout the evaluation process", "factors": "a model's propensity to generate code strikingly similar to existing implementations, and its accuracy in providing licenses, with particular emphasis on copy"}]}