{"title": "Knowledge Retrieval Based on Generative AI", "authors": ["Te-Lun Yang", "Jyi-Shane Liu", "Yuen-Hsien Tseng", "Jyh-Shing Roger Jang"], "abstract": "This study develops a question-answering system based on Retrieval-Augmented Generation (RAG) using Chinese Wikipedia and Lawbank as retrieval sources. Using TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for dense vector retrieval to obtain highly relevant search results and BGE-reranker to reorder these results based on query relevance. The most pertinent retrieval outcomes serve as reference knowledge for a Large Language Model (LLM), enhancing its ability to answer questions and establishing a knowledge retrieval system grounded in generative AI.\n\nThe system's effectiveness is assessed through a two-stage evaluation: automatic and assisted performance evaluations. The automatic evaluation calculates accuracy by comparing the model's auto-generated labels with ground truth answers, measuring performance under standardized conditions without human intervention. The assisted performance evaluation involves 20 finance-related multiple-choice questions answered by 20 participants without financial backgrounds. Initially, participants answer independently. Later, they receive system-generated reference information to assist in answering, examining whether the system improves accuracy when assistance is provided.\n\nThe main contributions of this research are: (1) Enhanced LLM Capability: By integrating BGE-M3 and BGE-reranker, the system retrieves and reorders highly relevant results, reduces hallucinations, and dynamically accesses authorized or public knowledge sources. (2) Improved Data Privacy: A customized RAG architecture enables local operation of the LLM, eliminating the need to send private data to external servers. This approach enhances data security, reduces reliance on commercial services, lowers operational costs, and mitigates privacy risks.", "sections": [{"title": "I. INTRODUCTION", "content": "The field of Information Retrieval (IR) has evolved significantly, driven by the need for real-time information through systems like Google and Bing. These systems use keyword-based matching and ranking algorithms to provide relevant results. However, traditional IR systems have limitations, such as low relevance or excessively lengthy outputs when query terms differ from indexed terms. This increases the effort required to find the desired information.\n\nThe rise of Generative AI and Large Language Models (LLMs), such as OpenAI's ChatGPT[1] and Google's Gemini, offers new ways to enhance IR systems by better understanding user intent and generating natural language responses. Despite these advances, LLMs face challenges, including generating false or outdated information when training data is insufficient or obsolete. Additionally, they may struggle with imbalanced training data, leading to incorrect outputs in specialized domains.\n\nTo address these issues, the Retrieval-Augmented Generation (RAG) framework combines IR and LLMs, allowing LLMs to retrieve specific information and generate accurate answers without retraining. Dense vector retrieval, a key method in this framework, uses deep learning to map text to high-dimensional vectors, improving accuracy by capturing semantic similarity. This approach is crucial for the development of next-generation knowledge retrieval, overcoming the limitations of traditional methods while leveraging the strengths of LLMs."}, {"title": "II. RELATED WORK", "content": "A. Information Retrieval\n\nInformation retrieval (IR) has progressed from basic textual data processing to advanced deep learning techniques, with natural language processing (NLP) playing a key role. Early models, such as the Boolean Retrieval Model, used logical operators (OR, AND, NOT) for document-query matching but struggled with complex queries. The Vector Space Model improved upon this by vectorizing documents and queries, using similarity measures, while enhancements like TF-IDF increased retrieval accuracy by adjusting term importance, though lacking contextual understanding. Probabilistic Retrieval Models ranked documents based on relevance probabilities but required large datasets for accuracy. BM25 further refined this approach by dynamically adjusting term weights, and Latent Dirichlet Allocation (LDA) introduced topic modeling using Bayesian networks. The advent of GPUs and deep learning has popularized Dense Vector Retrieval, which converts text into high-dimensional vectors. Models like BERT and Sentence-BERT leverage these vectors to perform tasks such as sentence classification efficiently, enhancing the semantic understanding of the text and improving IR systems' accuracy and efficiency.\n\nB. Language Model\n\nLanguage models (LMs) have progressed from Statistical Language Models (SLMs) to Neural Network Models (NNMs), Pre-trained Models, and Large Language Models (LLMs). Early models like N-grams and Markov Models, though foundational, struggled with scalability due to the \"curse of dimensionality.\" This limitation led to the adoption of NNMs, such as Recurrent Neural Networks (RNNs), which addressed variable-length sequences but faced challenges with long-term dependencies, later mitigated by Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs). The introduction of Distributed Word Representations (DWR), like word2vec, represented a major advancement by mapping words to dense vectors, though it lacked context sensitivity. Pre-trained models, such as BERT[2] and Sentence-BERT[3], resolved this issue with self-attention mechanisms, allowing context-aware embeddings. The emergence of large language models (LLMs), including GPT-3 and PaLM, has revolutionized NLP by leveraging billions of parameters and exhibiting emergent abilities. These models enable few-shot learning and advance natural language understanding.\n\nC. Retrieval-Augmented Generation\n\nRetrieval-Augmented Generation (RAG)[4] overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response. RAG also addresses the issue of model obsolescence by dynamically accessing updated information without retraining. However, balancing precision and recall during retrieval can be challenging, and re-ranking is required to prioritize relevant information, ensuring that LLM responses are accurate and contextually appropriate in complex queries.\n\nD. Evaluation\n\nThe rise of Large Language Models (LLMs) necessitates robust evaluation methods across various tasks. Key evaluation metrics include classification, language modeling, text generation, and question answering. For classification, metrics such as precision, recall, and F1-Score are derived from a confusion matrix that assesses true and false positives and negatives. Language modeling is evaluated using perplexity (PPL), which measures a model's ability to predict token sequences, with lower PPL values indicating stronger performance. Text generation tasks, including machine translation and summarization, use BLEU for precision, analyzing n-gram overlap, and ROUGE for recall, focusing on the longest common subsequences. Question answering is typically evaluated through accuracy, with Exact Match (EM) and F1-Score commonly applied. For knowledge-based tasks, the MMLU[5] dataset, which contains multiple-choice questions from 57 domains, is used to assess LLMs' generalization"}, {"title": "III. APPROACH", "content": "A. Embedding Model\n\nThe BGE-M3[6] model is a highly versatile embedding model supporting multilinguality, multifunctionality, and multi-granularity. Capable of converting text into vectors, it supports over 100 languages for cross-lingual retrieval and is designed to handle sentence, passage, and document-level inputs, with a sequence length of up to 8,192 tokens. It enables dense, sparse, and multi-vector retrieval, making it suitable for a range of tasks from basic keyword matching to complex multi-lingual queries. BGE-M3's retrieval process works by embedding the user's query in one language and retrieving relevant documents in another, supporting both same-language and cross-lingual searches. It is trained on 1.2 billion sentence pairs from 194 languages using unsupervised data, followed by fine-tuning with English and Chinese labeled corpora, and datasets such as MIRACL and Mr.TyDi. The fine-tuning process also involves generating question-answer pairs through GPT-3.5 to create new paired data. While BGE-M3 excels in vector generation and semantic similarity searches, it may not prioritize relevance in specific tasks such as question answering. This limitation is addressed by BGE-reranker, a cross-encoder designed to re-rank results based on relevance rather than semantic similarity. BGE-reranker assigns relevance scores to the retrieval results generated by BGE-M3, ensuring that higher-ranked results are more likely to accurately answer the query, thus enhancing the effectiveness of retrieval in tasks requiring precise responses.\n\nB. Datasets for Retrieval and Evaluation\n\nThis study uses two primary data sources for retrieval: Chinese Wikipedia, with 1.38 million entries, and Lawbank's fiscal and financial legal regulations, containing 6,193 legal clauses. The Chinese Wikipedia data, stored in dump files and updated monthly, was processed using the BGE-M3 tokenizer, revealing that 1,377,100 articles contained fewer than 8,192 tokens, making them suitable for processing without chunking. For legal regulations, the analysis showed that out of 151 regulations, 86 contained fewer than 3,400 tokens, with only a few exceeding 17,000 tokens. Due to the specialized nature of legal documents, chunking was adjusted by segmenting the data into individual legal clauses. Among the 6,193 legal clauses, 5,450 contained fewer than 250 tokens, ensuring compatibility with the BGE-M3 model. The study also employs two evaluation datasets: TC-Eval[7] and TMMLU+[8] from iKala. TC-Eval is a Traditional Chinese evaluation suite covering tasks such as contextual question answering, knowledge, and summarization. TTQA (Taiwanese Trivia Question Answering) from TC-Eval contains 103 multiple-choice questions across 15 categories, such as geography and culture. TMMLU+, released by iKala in December 2023, includes 66 topics and 23,015 multiple-choice questions, covering a wide range of professional fields such as law, engineering, and medicine. To ensure consistent testing, the TMMLU+ dataset was reformatted to match the TTQA structure, allowing for efficient comparison and performance evaluation across language models, including handling complex professional-level questions.\n\nC. Vector Index\n\nEfficient retrieval of high-dimensional vector data, typically generated by embedding models, requires a specialized data structure known as a vector index. This study employs FAISS[9] (Facebook AI Similarity Search), developed by Meta's Fundamental AI Research team, to build vector indices for similarity search and clustering of dense vectors. FAISS supports three common indexing methods: Flat Index for small datasets, providing linear searches across all vectors; Inverted File Index (IVF) for large datasets, which clusters data to reduce search time; and Hierarchical Navigable Small World Graph Index (HNSW), a graph-based method that finds neighboring vectors in high-dimensional spaces with enhanced efficiency. FAISS also supports three methods for similarity search: L2 Norm, Dot Product, and Cosine Similarity. The L2 Norm measures the Euclidean distance between two vectors and is often used in tasks requiring precise distance calculations. The Dot Product calculates the sum of the products of vector components and is commonly used for similarity and projection tasks in recommendation systems. Cosine Similarity measures the angle between vectors, focusing on direction rather than magnitude, making it popular in information retrieval (IR) systems. In this experiment, after the BGE-M3 model converts the retrieval data into vectors, FAISS organizes these vectors and constructs the vector index, enabling efficient retrieval based on similarity measures.\n\nD. Large Language Model\n\nThis study utilizes 14 large language models (LLMs) for experimental research, including models such as Taiwan-LLM[10], Mixtral 8x7B, Meta llama 2, MediaTek Breeze[11], INX Bailong, Google Gemma, ChatGPT 3.5, and Taide. Taiwan LLM, based on llama-2, is fine-tuned for Traditional Chinese, enhancing its performance in local contexts. Mixtral 8x7B employs a Sparse Mixture of Experts (SMoE) architecture to reduce computational costs by activating only a subset of experts during inference. Meta's llama 2 introduces Grouped-Query Attention (GQA), significantly improving processing speed. MediaTek's Breeze model builds on Mistral-7B, adding substantial Chinese data to enhance contextual comprehension. INX Bailong, developed by INX, is a llama-2-based model trained mainly on Traditional Chinese and English texts. Google's Gemma is a lightweight version of Gemini, emphasizing safety and responsible AI practices, employing RLHF to filter out personal and sensitive data. OpenAI's ChatGPT 3.5, built on the InstructGPT architecture, leverages RLHF to align with human values, achieving strong performance across various NLP tasks. Finally, Taide, based on llama-2, integrates local language and cultural elements through continual learning, positioning itself as a reliable model for Traditional Chinese content generation."}, {"title": "IV. EXPERIMENTS", "content": "The study evaluated the effectiveness of Retrieval-Augmented Generation (RAG) in improving the accuracy of Large Language Models (LLMs) by answering 103 questions in the TTQA dataset using 14 different models. As Figure 3 showed, the results indicate that models with RAG (w/ RAG) consistently outperformed those without it (w/o RAG), demonstrating the significant impact of RAG on accuracy. Specifically, as Table I showed, Taiwan-LLM-8x7B-DPO saw an accuracy increase from 57.28% to 88.35% with RAG, while Taiwan-LLM-7B-v2-0-1-chat and Mistral-8x7B-Instruct-v0-1 improved from 41.75% to 69.9% and from 37.86% to 60.19%, respectively. ChatGPT 3.5 also showed notable accuracy enhancement from 74.76% to 88.35% when supplemented with system-generated reference materials. This trend underscores RAG's efficacy in enhancing model performance across diverse LLMs.\n\nIn the evaluation of TMMLU+, covering 66 topics and 23,015 questions, four models were tested: Google's gemma-7b-it, Meta's Llama-2-13b-chat-hf, MediaTek's Breeze-7B-Instruct-v0_1, and TAIDE-LX-7B-Chat. Performance was assessed with and without the Retrieval-Augmented Generation (RAG) approach."}, {"title": "V. DISCUSSION", "content": "This study successfully developed a knowledge retrieval system based on Retrieval-Augmented Generation (RAG) and evaluated its performance using Chinese Wikipedia and Lawbank as data sources. The experiments conducted with evaluation datasets such as TTQA and TMMLU+ demonstrated that the combination of BGE-M3 and BGE-reranker effectively enhanced the relevance of retrieved results, thereby improving the accuracy of Large Language Models (LLMs) in answering questions. This section discusses the implications of the findings, the practical applications of the system, its limitations, and directions for future research.\n\nA. Interpretation of Findings\n\nThe experimental results reveal that the system exhibits strong performance in the financial and legal domains, particularly in human-assisted question-answering scenarios, where it significantly improved participant accuracy. These findings suggest that incorporating domain-specific knowledge sources, coupled with robust retrieval and ranking mechanisms, can address the knowledge gaps in LLMs. Furthermore, the integration of dense vector retrieval and re-ranking techniques substantially mitigated hallucination effects and enhanced the reliability of the system's responses. These results validate the utility of the RAG framework in addressing knowledge-intensive tasks and highlight the critical role of retrieval result ranking in boosting LLM performance.\n\nB. Comparison with Existing Research\n\nCompared to traditional information retrieval methods such as TF-IDF and BM25, this study leveraged dense vector retrieval with a multilingual embedding model (BGE-M3) to achieve superior precision. Additionally, the re-ranking model (BGE-reranker) further improved the relevance of retrieved results, enabling the RAG framework to deliver more accurate responses to domain-specific queries. Unlike systems that rely solely on LLMs to generate answers, the proposed system not only enhances accuracy but also reduces reliance on outdated or potentially erroneous information.\n\nC. Practical Implications\n\nThe proposed knowledge retrieval system has significant potential for applications across multiple domains. In the legal sector, it can support decision-making by quickly retrieving relevant statutes and providing concise explanations to assist legal professionals in handling complex cases. In education, the system can serve as a domain-specific question-answering tool to help students efficiently acquire knowledge. Moreover, the locally deployable RAG framework ensures data privacy and security, reducing dependency on commercial cloud services, and thus provides a model for building enterprise-level knowledge management systems.\n\nD. Limitations\n\nDespite the promising outcomes, this study acknowledges several limitations. First, while the data sources (Chinese Wikipedia and Lawbank) are extensive, they may not encompass all necessary domain-specific knowledge, potentially limiting the system's performance on highly specialized queries. Second, the system exhibited limitations in handling computational and logic-based reasoning questions, reflecting the inherent constraints of LLMs in processing such tasks. Third, the computational demands of the BGE-reranker model present challenges for real-time large-scale applications, highlighting the issue of limited computational resources.\n\nE. Future Directions\n\nTo address the aforementioned limitations, several avenues for future research are proposed. First, expanding the scope of data sources by incorporating additional specialized knowledge bases or dynamically updating datasets could enhance the system's adaptability to diverse queries. Second, integrating methods such as Graph Neural Networks (GNNs) or multi-modal data could further improve the system's reasoning capabilities and versatility. Third, optimizing the efficiency of the re-ranking model, or exploring alternative solutions like distillation techniques, could alleviate resource constraints in real-time applications. Lastly, user experience studies should be conducted to refine the system's interface and interaction processes, ensuring effective deployment in various scenarios."}, {"title": "VI. CONCLUSION", "content": "This study demonstrates the feasibility of the RAG framework in enhancing the accuracy and utility of knowledge retrieval systems, offering a valuable reference for the integration of IR and LLM technologies. Despite its limitations, the findings highlight the potential of the system in domain-specific knowledge retrieval and question-answering tasks. Future optimizations and extensions of this research will further advance the system's performance and broaden its applicability. There are two significant contributions:\n\n\u2022 Enhancing the Capability of Large Language Models (LLMs) in Knowledge-Intensive Tasks: The integration of the BGE-M3 embedding model facilitates dense vector retrieval, enabling the extraction of highly relevant results based on the semantic similarity between queries and retrieved data. These results serve as critical reference knowledge sources for LLMs, thereby improving their accuracy in addressing complex queries. Additionally, the incorporation of the BGE-reranker re-ranking model refines the retrieved results by identifying information that is most relevant to the queries, enhancing the reliability of the generated answers. This approach effectively mitigates hallucination effects and dynamically sources the latest authorized or publicly available knowledge to meet task-specific requirements.\n\n\u2022 Ensuring Data Privacy and Reducing Dependence on Commercial Services: By employing a customized RAG architecture, the proposed system operates entirely on local infrastructure, eliminating the need to transmit private data to external commercial service providers. This design significantly enhances data security and privacy protection. Furthermore, it minimizes reliance on commercial services, thereby reducing potential operational costs and risks of privacy breaches."}, {"title": "VII. WORK DISTRIBUTION", "content": "Te-Lun Yang was responsible for conceptualizing the report content, designing the experimental workflow, and writing the primary codebase. Yu Huang managed data collection, defined the specifications for pre-tests and post-tests, and visualized the experimental results. Wei-Jei Chou focused on developing the evaluation code for the Knowledge Retrieval mechanism and integrating additional libraries to facilitate the experiments."}]}