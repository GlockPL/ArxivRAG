{"title": "KBAda: Efficient Self Adaptation on Specific Knowledge Bases", "authors": ["Zheni Zeng", "Yuxuan Chen", "Shi Yu", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "abstract": "Humans can utilize techniques to quickly acquire knowledge from specific materials in advance, such as creating self-assessment questions, enabling us to achieving related tasks more efficiently. In contrast, large language models (LLMs) usually relies on retrieval-augmented generation to exploit knowledge materials in an instant manner, or requires external signals such as human preference data and stronger LLM annotations to conduct knowledge adaptation. To unleash the self-learning potential of LLMs, we propose KBAda, an approach designed for efficient adaptation to downstream tasks involving knowledge bases. Our method utilizes iterative training with self-annotated data such as Q&A pairs and revision suggestions, enabling the model to grasp the knowledge content efficiently. Experimental results on multiple datasets demonstrate the effectiveness of our approach, significantly boosting model performance in downstream tasks that require specific knowledge at a low cost. Notably, our approach achieves over 90% of the performance improvement that can be obtained by using GPT-4-turbo annotation, while relying entirely on self-supervision. We release our experimental data, models, and process analyses to the community for further exploration", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated their general capabilities across a wide range of downstream tasks (Achiam et al., 2023), while domain adaptation remains a crucial strategy to further enhance their performance (Ling et al., 2023). Although retrieval augmented generation (RAG) (Lewis et al., 2020) has become a common setting for LLMs to broaden the knowledge scope, tuning on specific domains can still help models outperform much larger general-purpose models (Zhao et al., 2024), highlighting the value of tailored training. Direct training on small-scale raw data, however, may degrade performance, necessitating alternative approaches to specific knowledge adaptation. Common solutions try to transform the data into more structured tasks like reading comprehension (Cheng et al., 2023), typically involve the incorporation of external knowledge signals (e.g., hand-crafted rules or GPT-4 annotations (Tan et al., 2024)). Nevertheless, it is actually proved that LLMs may achieve self improvement and refinement. For example, leveraging internal consistency and self-feedback (Liang et al., 2024), researchers have developed various filtering, correction, and preference training strategies to enhance the reliability of model outputs. Therefore, external knowledge signals is not necessary to some extent.\nConsidering that adaptation scenarios for specific knowledge could rapidly evolve and have some privacy restrictions, activating the potential of LLMs for autonomous adaptation is a better choice than adopting external annotations from human or online models. Drawing an analogy to human learning process, RAG is similar with open-book tests, in which students would conduct self study in advance. To be specific, they quickly read the fundamental content of the books by themselves, and can also verify their own answers. This may not be as good as spending time learning from external signals (e.g., related information from teachers), but is much more efficient. Therefore, the rapid self adaptation on specific knowledge materials is a useful capability.\nInspire by the above analogy, we purpose KBAda, a highly efficient self adaptation approach tailored to specific knowledge bases (KBs) comprising self-improvement learning. Generally, we tune the model in a completely self-supervised manner, and maximize the tuned model's potential in inference. As shown in Fig. 1, for self annotation, we organize the original KB materials and conduct self annotation to get initial Q&A data pairs that can cover various downstream task scenarios. For iterative tuning, we require the model to check its own responses as the learning progresses, and help the model to modify the common mistakes at the current stage. Meanwhile, we conduct targeted inference in which strategies such as query expansion and confidence verification are adopted to refine the response.\nExperiments on fact QA, long-form QA, and professional field test have shown the effectiveness of our method across different backbone models. With a low cost, KBAda masters the general knowledge content from the KB and achieves significant performance improvements. Side experiments including ablation studies and performance curve analyses identify the most efficient self-annotated data amount and optimal training volumes, offering valuable guidance for effectively applying KBAda in practical scenarios.\nOur main contributions are as follows: (1) We propose KBAda, a novel method for autonomous LLM adaptation tailored to knowledge bases. KBAda is the first approach to help LLMs perform KB adaptation relying entirely on self annotations; (2) We provide empirical insights into efficient self adaptation to KBs, offering practical parameters and settings for deploying KBAda; (3) We conduct a comprehensive analysis of the proposed self adaptation framework. Through a range of evaluation metrics and case studies, we identify the effectiveness of KBAda and discuss the current limitations of our approach, highlighting areas for future improvement."}, {"title": "2 Related Work", "content": "Domain Adaptation.Though LLMs have shown their impressive capabilities across various scenarios (Jablonka et al., 2023), training methods for LLMs to adapt to certain domains still emerge in endlessly, due to the vertical application requirements. For domains with plenty of data resources, researchers usually directly take domain materials into pre-training (Wang et al., 2023a; Madani et al., 2023). In more cases, they continue to train based on general LLMs or mix the domain data with general corpus (Wu et al., 2023). To adopt the domain knowledge in a more efficient way, format conversion and annotation are often conducted (Zhang et al., 2024). For efficiency reasons, domain LLMs usually use parameter-efficient tuning to reduce the required data amount and training time (Ding et al., 2022). However, the construction of domain data is sometimes still costly. In our case, we mainly focus on the data perspective and try to propose more efficient data construction methods.\nKnowledge Enhancement. For some specific downstream requirements, there often exist high-quality knowledge materials (e.g., domain KBs, personal documents or records), of which the data amount is not enough for model tuning, and knowledge enhancement methods can help improve the performance. There are two mainstream solutions. The first one is to rely on the strong in-context learning capability of LLMs (Dong et al., 2022), and adopt RAG (Lewis et al., 2020) to enhance the model. Apart from textual materials such as Internet passages, it is proven that integrating special KBs and tools is also a good approach to improve the model performance with specific knowledge (Cui et al., 2023; Jin et al., 2024; Qin et al., 2023). To provide more useful information in context, strategies including designing better queries for retrieval are proposed (Wang et al., 2023b; Qian et al., 2024). The second solution is to augment training data based on knowledge materials. LLMs can help synthesize data in more styles (Sun et al., 2023) or convert the original data into formats more suitable for training (Cheng et al., 2023). Our method is special, emphasizing the self annotation instead of introducing new LLMs into the system.\nSelf Improvement. There are some works exploring the self-improve capability of LLMs, most of which focus on the automatic generation and selection of reasoning steps for existing answers, being helpful in tuning (Huang et al., 2023a) and inference (Jiang et al., 2023). Self-play fine-tuning in an iterative manner (Chen et al., 2024) also unlocks the full potential of golden data. Even without the ground-truth answers, intern consistency of LLMs can be adopted as an important supervision signal that can achieve improvement (Liang et al., 2024). Nevertheless, challenge remains for human-like self improvement, such as how to self correct the reasoning process (Huang et al., 2023b). We observe the human learning process and design corresponding self-improvement methods."}, {"title": "3 Methodology", "content": "Current LLM adaptation approaches are usually used for domain transfer to enhance a type of capabilities (e.g., medicine, mathematics, coding). However, in real-world scenarios, there are often more specific needs, such as providing customized services based on user-specific document repositories, or plug-and-play integration of modules (e.g., operation manuals for new tools). These scenarios often have smaller data amount, and come with constraints such as confidentiality and convenience, leading to the inability to involve human annotation or rely on online large models, along with limited computational resources. In light of these constraints, RAG has emerged as a common approach to enhance the factual reliability of models. However, general retrieval methods often fail to leverage specific knowledge, therefore direct adaptation of models to specific KBs remains necessary.\nWe define KB adaptation as the process where, given a knowledge base $K$ (textual materials in our case), an original generative model $M$, and a retriever $R$, the goal is to efficiently enhance the grasp of the information contained within $K$, thereby improving performance on a test set $T$ based on $K$. Common forms of $T$ include knowledge question answering, multiple-choice tests, and other knowledge-intensive evaluations. The primary optimization objectives are to maximize performance scores on $T$ while minimizing training time within constrained resources.\nThere are two common approaches for leveraging $K$ to enhance model performance: training-based and inference-based methods. Training-based methods involve generating training data from $K$ using unsupervised techniques, such as masked language modeling or general language modeling on $M$, or targetly design $R$ for the current domain which is not covered in this work. Inference-based methods, on the other hand, focus on optimizing the retrieved content in our basic RAG setups, or post-processing the generated results to enhance relevance and accuracy. We will now introduce our method to combine the unsupervised training and the improved RAG, optimizing both the training and inference process. Examples for related process are shown in Fig. 2."}, {"title": "3.2 Self Annotation", "content": "To learn the knowledge from KBs without any supervised data, we conduct self annotation with the backbone model $M$ on the $K$ text. To be specific, we choose a paragraph of golden context $C_g$ as the knowledge source and require $M$ to raise $s$ set of questions $Q$. We then supplement related context $C_R$ by the retriever $R$, and ask $M$ to annotate the answers $A$ based on the combined $C$. To increase the robustness and generalization, we provide a prompts set and randomly pick a style for the annotation, including concisely, in detail, and so on. $A$ is comparably precise because of the ensured existence of $C_g$ and our hand-crafted keyword filters (e.g., questions should not mention pronouns such as \"in this paragraph\"). $M$ is trained on $<Q, A>$ instead of directly on $K$, which is similar with the self learning of human students converting and understanding the textbooks instead of directly reciting.\nOwing to the diverse forms and attributes of $K$ and associated downstream tasks, we propose different organization strategies for $C$."}, {"title": "Short-dependency Annotation", "content": "For downstream tasks that prioritize precise fact knowledge expressed in one specific paragraph, we employ this approach to simply split $K$ into fixed-length chunks, each with no longer than 1,024 words while keeps continuity of information across boundaries. One chunk is directly adopted as the annotation context $C_g$."}, {"title": "Long-Dependency Annotation", "content": "Considering that real-world tasks often require comprehensive understanding of multiple pieces of information at long distances in text, we design long-dependency annotation methods that split $K$ into shorter segments with less than 256 words. Several segments $S_{1,...,n}$ with the same hierarchical directory, or with the highest embedding similarities across different directories, are concatenated together as $C_g$. When generating $Q$, the model is required to raise questions that: (1) involve knowledge from different segments to emphasize the multi-hop reasoning capability; (2) are as vague as possible, corresponding to a series of information $I_{1,...,n}$ annotated on $S$, based on which a refined long-form answer $A$ is generated to improve the integration capability."}, {"title": "3.3 Iterative Tuning", "content": "Apart from summarizing and self questioning to help deep understanding, human students also take tests at each learning stage and strengthen the knowledge they have not yet mastered by correcting their answers. Similarly, we hope that the model can improve itself through self-verifying in addition to understanding."}, {"title": "Initial Tuning", "content": "With the self-annotation $<Q, A>$ data, we tune $M$ to get an initially adapted version. It is worth mentioning that a mixed paradigm has to be adopted to enhance the model generalization, that is, to provide a part of (50% in our practice) retrieved context $C_R$ which is different from the annotation $C$ as the model input combined with $Q$. This is to bridge the gap between tuning and inference in which RAG is not completely precise."}, {"title": "Self-Verify Tuning", "content": "Based on the $M_0$ that is initially adapted on a part of the data $<Q_0, A_0>$, we conduct RAG on $Q_1$ to get the predicted answer $P_1$ reflecting current capability of $M_0$. By providing the ground truth answer, the model verifies its own prediction and analyzes the wrong reason, which we name as $V_1$. In the next stage, we can then require the model to generate $V_1$ based on $<Q_1, P_1>$. And so on, we generate the verifying data based on current performance, and conduct the Q&A task and verify task at the same time in an iterative manner. In experiments, we use 25% of verify and 75% of Q&A, and implement 2-3 iterations."}, {"title": "3.4 Targeted Inference", "content": "We improve the downstream performance mainly by training the model to learn more specific knowledge. Practically, there are also strategies that can help optimize the generation results in inference stage. We employ Query Expansion (QE) to refine the retrieval results. To be specific, directly using the $Q$ as the search query may miss the useful information due to the short expression and the limitation of the retriever $R$. However, when our model has memorized the overall knowledge to a certain extent, it can read $Q$ and provide a prediction $P$ that is not necessarily accurate but very relevant to the KB content. We then expand the search query as $Q+P$, and this may help make the retrieval results much better.\nThe other strategy that can be used in reference is Self Verification, which is based on the verify capability learned in iterative verifying. For the generated $P$, the model can check the correctness by itself. It should be emphasized that this is not the standard strategy setting in subsequent experiments, because it will increase the time cost, and it is also difficult for the model to correct the error after realizing it. However, the model can at least provide an uncertainty warning, or sample a new response when the confidence score is low when needed, which helps improve reliability."}, {"title": "4 Experiment", "content": "In order to evaluate the effectiveness of our method as comprehensively as possible, the following three datasets were used in the experiment:\nLooGLE (Li et al., 2023). This is a long-text dataset, with textual materials that can be regarded as KBs and high-quality questions. We use the short-dependency data in LooGLE for retrofitting, combining altogether 2.2M tokens of text as $K$, and the corresponding 1,951 Q&A pairs for test. We evaluate the specific knowledge memorizing capability of the model in this dataset.\nASQA (Stelmakh et al., 2022). This is a long-form QA dataset. For each question, there exists several related segments of text from WikiPedia, and a comprehensive long answer that covers much information from them. We collect 794 Q&A pairs for test, their targeted segments and other related passages from WikiPedia, and get 1.8M tokens of text as $K$. We evaluate the knowledge recall and organizing capability of the model in ASQA test set, and do not use any training data from it.\nJEC-QA (Zhong et al., 2020). This is a legal multiple choice dataset in Chinese. Related laws and reference books are seen as $K$, including 21M tokens of text. The train set has also not been used for adaptation, and the test set contains 1,985 of multiple choices. Through this set we evaluate the professional learning capability and instruction following in different inference formats.\nWe choose the following models as the backbone and comparison objects of the experiments:\nMiniCPM (Hu et al., 2024). This refers to MiniCPM-2.4B which is one of the backbone models in our experiment. It is an end-side LLM gaining the instruction following ability during pre-training, and has achieved the best performance among lightweight LLMs on several datasets. Therefore, we believe that it has wide personal applications and is suitable for efficient adaptation scenarios."}, {"title": "4.2 Evaluation Metrics", "content": "For LooGLE and ASQA, we consider the evaluation of the original dataset and decide to utilise the following metrics: F1 score, which measures the harmonic mean of precision and recall; Match score, which measures the recall of key elements in long-form answer; BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), which are traditional text generation similarity metrics and evaluate the precision and recall correspondingly; BERT score (Zhang* et al., 2020) calculates cosine similarity to assess semantic consistency, utilizing embeddings generated by the text2vec (Xu, 2023) model from sentences. Additionally, semantic judgment by the representative OpenAI LLM, GPT-40, is used to evaluate the quality of responses further. Detailed prompts are provided in Section A.\nFor JEC-QA, we use the official evaluation metrics, where only precise prediction of options could be scored, regardless of whether the questions were single or multiple-choice."}, {"title": "4.3 Other Settings", "content": "For the training process, we tune all parameters of MiniCPM, while conduct a parameter-efficient tuning for LLaMA-3.1, utilizing the LoRA (Hu et al., 2021) strategy to reduce the need for computing power costs. We adopt the mixed-precision training with the BMTrain and LLaMA Factory (Zheng et al., 2024) framework to speed up.\nFor the inference process in both annotation and test, we adopt the bge-large-en-v1.5 model for English materials and bge-base-zh-v1.5 for Chinese (Xiao et al., 2023) as the basic retriever of RAG. In the test scenario, the chunks of KB materials are divided with less than 128 tokens, and the top 8 relevant chunks are provided. To ensure continuity of information, we apply an overlap rate of 15% between consecutive chunks. We adopt VLLM (Kwon et al., 2023) to speed up inference.\nFor time cost estimation, we provide the result on ASQA after scaling to the capacity of an A100 GPU: short-dependency annotation for 1k data items takes 30 min, long-dependency annotation for 1k data items takes 140 min, and iterative tuning process takes 160 min.\nHyper-parameters are provided in Section A."}, {"title": "4.4 Result Analysis", "content": "Main Experiments. The main results for our experiments are shown in Table 1. We provide the GPT-series results, the initial version and the self-adapted version of both MiniCPM-2.4B and LLaMA-3.1-8B-Instruction on the three dataset. Overall, comparing the \"Ours\u201d lines with the corresponding original model, we can see that KBAda ensures an obvious improvement on most of the metrics, regardless of the dataset and the backbone model. This shows the generalization and effectiveness of our method.\nNevertheless, our strategies still produce differentiated effects in the three scenarios. For LooGLE which evaluates the master of precise local knowledge, self-annotated tuning brings a huge improvement (over 20% on F1) and the adapted 2.4B model can surpass LLaMA-3.1-8B & GPT-40 performance. For ASQA emphasizing long-form answer that covers global information, however, the improvement is comparably marginal (less than 5% on Match). The first possible reason is that backbone models have already mastered the WikiPedia knowledge in pre-training, and extra adaptation is redundant. The second reason is the over emphasis of local knowledge in the responses, making QE strategy provide even more limited retrieval results. The same trend is also reflected in the single-choice and multiple-choice tests of JEC-QA. Our method easily surpasses some legal-domain models in the former (such as 40.8 single-choice score reported for 7B legal LLM (Wan et al., 2024)), while in the latter the performance even declines slightly due to reasons such as the output format. This indicate the challenge of learning knowledge with a long information span and logical chain."}, {"title": "Numerical Analysis", "content": "We search the best values for key settings including the training steps, amount of data and iteration by evaluating checkpoints in process. From Fig. 3 we can see, when learned on more QA pairs (only once), scores on LooOGLE F1 (represents fact accuracy) for both backbone models improve. Interestingly, directly learning without iterative tuning (dotted curve) also displays a similar trend, while the tipping point for slowing growth comes much later. This reveals the possible mechanism of self-verify task, that is, to guide the model to focus more on the problems of current stage, so as to reach convergence faster. According to the curve, we choose to provide 15 data items per 10,000 tokens for LooGLE training, and increase the data density of ASQA due to the smaller KB scale. Besides, although the tuning phase usually reuses the same data for multiple epochs of training, we observe from Fig. 4 that using half of the data to tune 2 epoch brings a quite obvious score decrease. Consider that the inference time for data annotation is acceptable compared with the training time, we recommend annotating more data and tuning with only 1 epoch.\nFrom Fig. 4 we can also observe the performance change for different number of iterations when conducting iterative tuning. With the self-verify data, the score first increases and then keeps comparably stable as the iterations increases, showing that the verification capability helps improve model performance on downstream QA, while requiring the data quality to be high enough with a certain granularity. From the curve we recommend conducting at least 3 iterations, while it depends on actual situation in practical implementation."}, {"title": "Ablation Study", "content": "We assess the effectiveness of our strategies by side experiments in LooGLE and ASQA, and provide the results in Table 2 and 3. To validate the quality of self annotated data, we try to replace the annotation model with GPT-4-turbo (\"GPT Data\") in LooGLE, and further replace the data with golden training set (\"Golden\") in ASQA. We find that the eventual scores are not much higher than current setting, especially when compared with the original setting without adaptation, proving the usefulness of self annotation. \nBy conducting ablation study on ASQA, we prove that the long-dependency annotation (\u201cOurs\u201d vs. \"wo long\") plays a vital role, in which the comprehensive responses are expected. Considering the higher time cost (about 4 times of short-dependency), we discard this strategy to the local QA task in LooGLE. Meanwhile, self-verify tuning (\"Ours\" vs. \"wo verify\") also helps improve the performance for both dataset by correcting errors of the current stage in a targeted manner.\nTo explore the mechanism of improvement more clearly, we also conduct a cross-validation on LooGLE (\u201cwo know\u201d), in which the amount of training data keeps the same while the exact information corresponding to the test questions are removed during self-annotation. We can see that the self adaptation still helps refine the performance, but worse than the complete setting. This indicates that domain knowledge from KB and task format is the main reason of the score rising, while the precise information related to the test data also helps the improvement."}, {"title": "4.5 Case Studies", "content": "We display typical cases in Fig. 5 to explain the specific usefulness of our strategies. \u201cBaseline\" refers to MiniCPM-2.4B and \u201cours\u201d refers to the adapted version of it. Overall, KBAda achieves a general grasp of current KB, a better knowledge answering, and a reasonable confidence verification.\nCases (a) proves the effectiveness of learning knowledge from the self-annotated data. The base model fails to extract useful knowledge (scores in 1932 season) from the indirect context, while the model learns the precise knowledge during self adaptation. Case (b) shows that our model generates a decent prediction (in 2007) though the retriever fails to locate precise information from KB, and this prediction can then help find out useful knowledge with QE, therefore the model eventually provides an even better response (in June 2007). Further, from case (c) we can see that due to the self-verify task mixed into adaptation tuning, the model can check its own prediction and provide a hint of error or incompleteness. Though the verify reason is not always accurate or helpful for modification, it is still meaningful to provide a warning when the confidence is low. Meanwhile, we can also use the verify function as a self-selector for multiple sampling results.\nWe also see some limitations when observing more cases. To be specific, the self-annotated contains some bias or error, and this may damage the model performance on related questions. Due to the concise language style of annotated data, our model tends to provide short responses in which some useful information may be discarded. QE strategy, in addition, does not always necessary. These negative instances remind us that we should continue to design better annotation and tuning strategies. More cases on different dataset and with various performance are provided in section A."}, {"title": "5 Discussion and Limitations", "content": "In this paper, we introduced KBAda, a highly efficient self-adaptive method tailored for specific KBs. During the tuning stage, inspired by human learning strategies such as summarizatio and self-reflection, we propose a combined long- and short-dependency annotation method, as well as an iterative tuning approach. These techniques enable low-cost targeted training data augmentation and efficient adaptation without requiring external supervision. In the inference stage, we enhance the model's performance on KBQA tasks using query expansion and sampling-based self-verify strategies. Our approach demonstrates significant improvements across various datasets spanning different domains and formats. Additionally, detailed analysis provides empirical guidance regarding the best data amount required.\nStill, our approach has some limitations. Firstly, while the current method excels in KBQA tasks, especially those focused on local information within the KB, it offers less support for tasks requiring comprehensive global information analysis. This suggests a need for more refined data annotation strategies. Secondly, training on small-scale targeted data can lead to a reduction in the model's general domain abilities, such as instruction-following. Mixing specific KB data with general domain data, while helpful, conflicts with our goal of minimizing adaptation time and cost. We may need to explore techniques like model plugins and routing selection to strike a better balance. Lastly, given the strong influence of retrieval quality on QA performance found in our practice, it may be necessary to consider adapting the retriever during specific KB adaptations. Applying self-supervised strategies to retriever training could be a promising direction.\nIn future work, we aim to focus on adaptive performance enhancement in more complex scenarios, such as utilizing new tools. Additionally, we will explore the integration and collaboration of multiple models adapted to different subdomains."}, {"title": "A Appendix", "content": "For hyper-parameter settings, we conduct a grid search in the vicinity based on the empirical values provided in the sample code of the MiniCPM-2.4B model, and finally determine batch size as 8 and learning rate as le - 5. Other settings include warm-up steps as 50 and weight decay as 0.1. For the LLaMA-3.1-8B-Instruction model, we adopt a parameter-efficient tuning approach using the LORA strategy, with alpha as 16 and rank as 8. Other settings include a cosine learning rate scheduler with a warm-up ratio of 0.1 and a weight decay of 0.1."}, {"title": "A.2 Prompts", "content": "Below are the prompt templates used for self annotation. For short-dependency annotation, we directly generate by:\nYou are a master of extracting questions and answers from text.\nBased on the provided content, construct five questions and answers\nthat should be directly based on the text content, separated by line breaks.\nPlease ensure that the expression of the question clearly points to the specific information in the text, and avoid using vague or overly\nbroad references. At the same time, emphasize direct references or\nspecific details in the text to increase the accuracy and depth of the problem.\nThe questions should be answerable in a few words.\nOutput question and answer alternately on each line.\nContent: {content}\nResponse:\nFor long-dependency annotation, we first generate questions:\nYou will receive a document. Please generate 3 generalizable,\nambiguous questions based on the document content. The questions\nshould align with the themes of the document.\nSeparate the questions by line breaks.\ndocument: {document}\noutput:\nBased on the questions, we then annotate the related information:\nYou will receive a document and a question.\nPlease provide an answer\nto the question based on the document information. If unable to answer,\nreturn 'none'; otherwise, output the answer directly.\ndocument: {document}\nquestion: {question}\noutput:\nLast, we refine the information to get the answer:\nYou will receive a concatenated answer from multiple sources.\nPlease refine and optimize the expression to make it smoother.\nOutput the final answer directly without unnecessary explanation.\nquestion: {question}\nanswer: {answer}\noutput:\nIn the iterative tuning phase, we self-verified by:\nYou are a teacher evaluating student responses.\nRemember:\n1. If the student's response fully aligns with the golden answer, start your response with 'The student's response is correct because'.\n2. Otherwise, start your response with 'The student response is wrong because', and provide the ERROR TYPE!!! (e.g., does not answer the question directly, provides totally wrong information, provides only part of the information, provides unrelated information)\n3. Notice! You are NOT ALLOWED to directly point out the correct answer in your verification. You are NOT ALLOWED to directly point out the correct answer in your verification. You are NOT ALLOWED to directly point out the correct answer in your verification. You should only tell me the correctness and the error type.\nNow here are the materials:\nReference: {reference}\nQuestion: {question}\nGolden Answer: {golden_answer}\nStudent Response: {student_response}\nPlease generate your verification. You should start with the judgement, and then EXPLAIN the reason / the error type.\nBelow is the prompt template used for downstream QA tasks:\nYou are an expert who has read a lot of knowledge base.\nPlease answer the question according to the content of the KB.\n<KB_{kb_id}> You can refer to some segments from the KB to help\nyou answer the question.\nReferences: {references}\nNow the question is: {question}\n{dataset_prompt}\nFor different datasets, we change the dataset_prompt to adjust the output style.\nSpecifically, we refer to ALCE (Gao et al., 2023) when designing the ASQA prompt.\nLooGLE: Please answer this question.\nASQA: Write an accurate, engaging, and concise answer for the given question. Use an unbiased and journalistic tone.\nJEC-QA: The answer may be multiple or single, so be sure to choose all the correct options.\nBelow is the prompt template used for LLM evaluation (Li et al., 2023):\nGiven one question, there is a groundtruth and a predict_answer.\nPlease decide whether they are the same or not in semantic.\nPlease only output 'True' or 'False'.\nQuestion: {question}\ngroundtruth = {ground_truth}\npredict_answer = {predict}"}, {"title": "A.3 Supplementary Case", "content": "We provide several more cases in Fig. 6. Case (d) shows that for different forms of tasks such as multiple choices, the self-annotated data can also provide key knowledge for the model. Case (e) shows a verification example in which the error can only be described in explicit natural language instead of a wrong label. Case (f) shows that our method does not always help improve the performance. In this case, the model discards some useful information due to the concise language style bias."}]}