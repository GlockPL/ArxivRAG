{"title": "Feedback-based Modal Mutual Search for Attacking\nVision-Language Pre-training Models", "authors": ["Renhua Ding", "Xinze Zhang", "Xiao Yang", "Kun He"], "abstract": "Although vision-language pre-training (VLP) models have\nachieved remarkable progress on cross-modal tasks, they re-\nmain vulnerable to adversarial attacks. Using data augmenta-\ntion and cross-modal interactions to generate transferable ad-\nversarial examples on surrogate models, transfer-based black-\nbox attacks have become the mainstream methods in attack-\ning VLP models, as they are more practical in real-world\nscenarios. However, their transferability may be limited due\nto the differences on feature representation across different\nmodels. To this end, we propose a new attack paradigm called\nFeedback-based Modal Mutual Search (FMMS). FMMS in-\ntroduces a novel modal mutual loss (MML), aiming to push\naway the matched image-text pairs while randomly drawing\nmismatched pairs closer in feature space, guiding the update\ndirections of the adversarial examples. Additionally, FMMS\nleverages the target model feedback to iteratively refine ad-\nversarial examples, driving them into the adversarial region.\nTo our knowledge, this is the first work to exploit target model\nfeedback to explore multi-modality adversarial boundaries.\nExtensive empirical evaluations on Flickr30K and MSCOCO\ndatasets for image-text matching tasks show that FMMS sig-\nnificantly outperforms the state-of-the-art baselines.", "sections": [{"title": "Introduction", "content": "Vision-language pre-training (VLP) models have signifi-\ncantly advanced cross-modal tasks by leveraging large-scale\npaired image-text datasets. These models excel in under-\nstanding visual and textual domains, achieving outstanding\nperformance in various downstream tasks including image-\ntext matching (Li et al. 2024), image captioning (Hu et al.\n2022), and visual grounding (Yang et al. 2023). However,\nrecent researches (Zhang, Yi, and Sang 2022; Yin et al.\n2023; Zhou et al. 2023) have shown the vulnerability of\nVLP models to adversarial attacks, raising serious concerns\nabout their robustness and reliability in real-world applica-\ntions. Consequently, identifying and addressing the vulner-\nabilities of VLP models are critical for enhancing their re-\nsilience against malicious manipulations.\nExisting adversarial attacks can be broadly categorized\ninto two main types: white-box attacks (Jia et al. 2022, 2024;\nknowledge of the internal architecture or parameters of the\ntarget model, black-box attacks are more practical in real-world scenarios, gaining enormous attention in the literature.\nAmong various black-box attack approaches, transfer-\nbased methods have been the primary focus of research ef-\nforts in attacking VLP models, owing to their practicality\nand simplicity of implementation. Recent researches (Lu\net al. 2023; Gao et al. 2024; Han et al. 2023) have ex-\nplored various techniques to enhance the transferability of\nadversarial examples, such as data augmentation (Gao et al.\n2024) and cross-modal interactions (Lu et al. 2023). De-\nspite practicality, these methods often encounter limitations\ndue to differences in feature representation spaces across dif-\nferent VLP models, particularly between fused and aligned\nmodels. Fused VLP models integrate text and visual features\ninto a unified representation, while aligned VLP models sep-\narately process different modal features. As a result, even\nwhen trained on the same dataset, the feature distributions of\nvarious VLP models can exhibit significant variations, espe-\ncially when their architectures are different. These discrep-\ncies in feature representation can hinder the transferability\nof adversarial examples, as the examples generated on a sur-\nrogate model may not be effective on the target model due\nto the distinct ways features be represented and processed.\nTo address the above issue, we propose a novel method\ncalled the Feedback-based Modal Mutual Search (FMMS)\nattack specifically designed for multimodal tasks. FMMS\nintroduces a novel loss function termed model mutual loss\n(MML), that simultaneously increases the distance between\nmatched image-text pairs and decreases the distance be-\ntween mismatched pairs. MML effectively explores the tar-\nget model's adversarial region, as illustrated in Figure 1."}, {"title": "Related Work", "content": "To clearly illustrate the motivation of the proposed FMMS\nattack for attacking VLP models, the mainstream VLP ar-\nchitectures and adversarial attack methods on both unimodal\nand multimodal models are introduced briefly."}, {"title": "VLP Models", "content": "Vision-language pre-training (VLP) models have signifi-\ncantly advanced multimodal artificial intelligence by en-\nabling a joint understanding of visual and linguistic infor-\nmation (Chen et al. 2023). Initially relying heavily on pre-\ntrained object detectors, these models have shifted with the\nintroduction of Vision Transformer (ViT), which enables\nend-to-end image encoding by transforming images into\npaches (Wang et al. 2023; Li et al. 2021, 2022; Wang et al.\n2022). VLP models are broadly categorized into two archi-\ntectural paradigms: fused models and aligned models. Fused\nVLP models like ALBEF (Li et al. 2021) and TCL (Yang\net al. 2022) employ separate encoders for text tokens and vi-\nsual features, integrating representations with a multimodal\nencoder to generate unified semantics. However, aligned\nVLP models, exemplified by CLIP (Radford et al. 2021), use"}, {"title": "Adversarial Attacks on Unimodal Models", "content": "Depending on the degree of acquiring target model informa-\ntion, adversarial attacks are generally classified into white-\nbox and black-box attacks. For attacking textual models, the\nprimary methods generally involve modifying text, such as\nsubstituting specific tokens, e.g., PWWS (Ren et al. 2019)\nand BERT-Attack (Li et al. 2020b). Many white-box attack\nmethods have achieved good performance in computer vi-\nsion. The Fast Gradient Sign Method (FGSM) (Goodfellow,\nShlens, and Szegedy 2015) is the earliest method to uti-\nlize the sign of input gradient to maximize the classification\nloss and generate adversarial examples in a single step. The\nProjected Gradient Descent (PGD) (Madry et al. 2018) is a\npowerful variant following the iterative version of FGSM (I-\nFGSM) (Kurakin, Goodfellow, and Bengio 2017) to produce\nadversarial examples.\nIn black-box attacks, only the outputs of the target model\ncan be accessed by the adversary. A mainstream black-box\ntechnique is transfer-based attacks, which generate adversar-\nial examples on a surrogate model to deceive other models.\nVarious methods have been proposed to enhance the trans-\nferability of adversarial examples, such as data augmenta-\ntion, e.g., DIM (Xie et al. 2019), TIM (Dong et al. 2019),\nSIM (Lin et al. 2020). Additionally, black-box attacks can\nalso be approached through score-based and decision-based\nmethods. Score-based attacks utilize confidence scores pro-\nvided by the target model to generate adversarial examples,\nexemplified by Zoo (Chen et al. 2017) and Autozoom (Tu\net al. 2019). Decision-based attacks rely solely on the fi-\nnal decisions or label outputs of the target model, and iter-\natively modify the adversarial examples until they alter the\nmodel's decision, e.g., Boundary Attack (Brendel, Rauber,\nand Bethge 2018), SignOPT (Cheng et al. 2020). Although\nscore-based and decision-based methods can exploit infor-\nmation from target models, they require enormous queries\nand have a very high computational overhead. Therefore,\nno research has attempted score-based or decision-based at-\ntacks on VLP models so far."}, {"title": "Adversarial Attacks on Multimodal Models", "content": "For adversarial attacks on VLP models, most of the exist-\ning research centers on white-box attack settings: the sep-\narate unimodal attack (Sep-Attack) combines PGD (Madry\net al. 2018) and BERT-Attack (Li et al. 2020b) to attack im-\nage modality and text modality, respectively; the Collabora-\ntive Multimodal Adversarial Attack (Co-Attack) (Zhang, Yi,\nand Sang 2022) considers cross-modal interactions to collec-\ntively carry out attacks on the image and text modality. The\nSet-level Guidance Attack (SGA) (Lu et al. 2023), which\nfirst discusses the transferability of adversarial examples on\nVLP models, employs data augmentation and cross-modal\ninteractions to improve transferability. So far, although more\ntransfer-based attacks (Han et al. 2023; Gao et al. 2024) on\nVLP models have been proposed, these methods often en-\ncounter limitations due to differences in feature representa-\ntion spaces across various models. To address this, we lever-\nage feedback from the target model over only a few rounds\nto perform multiple rounds of cross-modal interactions, gen-\nerating more deceptive adversarial examples on surrogate\nmodels."}, {"title": "Methodology", "content": "In this section, we present the proposed Feedback-based\nModal Mutual Search (FMMS) attack method. FMMS im-\nproves the attack performance by leveraging feedback from\nthe target model for cross-modal searching to identify more\ndeceptive adversarial examples. We first introduce relevant\nnotations and highlight our motivation, then provide details\nof the proposed approach."}, {"title": "Notations", "content": "Let $(v_i,t_i)$ denote the i-th matched image-text pair sam-\npled from a multimodal dataset. Let $(v_j, t_i)$ and $(v_i, t_k)$ de-\nnote mismatched image-text pairs. For Vision-language pre-\ntrained (VLP) models, we denote $F_I$ as the image encoder\nand $F_T$ as the text encoder. $F_I(v)$ and $F_T(t)$ denote the en-\ncoded representation of the image v and text t, respectively.\nWe define $\\mathcal{B}[v, \\epsilon_v]$ and $\\mathcal{B}[t, \\epsilon_t]$ as the perturbation neighbor-\nhoods for optimizing adversarial images and texts, respec-\ntively. $\\epsilon_v$ and $\\epsilon_t$ respectively denote the maximal perturba-\ntion bound for the image and text, which are configured fol-\nlowing the previous works (Lu et al. 2023; Zhang, Yi, and\nSang 2022)."}, {"title": "Motivation", "content": "To investigate the transferability of multimodal adversarial\nexamples, we first analyze the transferability of adversar-\nial examples generated by the Set-level Guidance Attack\n(SGA) (Lu et al. 2023). As shown in Figure 3, we ob-\nserve that when adversarial examples are generated on fused\nmodels, such as ALBEF (Li et al. 2021) and TCL (Yang\net al. 2022), they achieve higher attack success rates (ASR)\nagainst other fused models compared to the aligned models.\nSimilarly, adversarial examples generated on aligned mod-\nels, such as CLIPCNN and CLIPVIT (Radford et al. 2021),\nare more effective against other aligned models. This phe-\nnomenon indicates that differences in the feature represen-"}, {"title": "Modal Mutual Loss", "content": "The previous attack method of SGA (Lu et al. 2023) gen-\nerates adversarial examples by increasing the distance be-\ntween matching pairs, as shown in Figure 1 (a). However, in-\ncreasing the distance between matching pairs leads to a sin-\ngle update direction, which restricts the transferability of ad-\nversarial examples given the diverse differences among VLP\nmodels. Therefore, we design a modal mutual loss, defined\nas MML = $(L_{tm}, L_{vadv}, L_{tadv})$, to explore various update\ndirections through multiple rounds of cross-modal interac-\ntions, generating more deceptive adversarial examples.\nAdditionally, data augmentation (Lu et al. 2023) is an-\nother key insight identified to enhance adversarial trans-\nferability. Therefore, in our scheme, we retain alignment-\npreserving augmentation (Lu et al. 2023), augmenting\ncaptions of each image $v_i$ to form the set $t_i =$\n$(t_1,..., t_m,...,t_M)$ and resizing each image $v_i$ into differ-\nent scales. We apply anti-aliasing to the scaled images (Ko-\nlivand and Sunar 2015).\nFirst, we select the mismatched image $v_j$ to construct\n$(v_j,t_i)$ for each image-text pair $(v_i, t_i)$, generating corre-\nsponding adversarial caption set $t'_i = \\{t'_1,..., t'_m,\u2026\u2026\u2026,t'\\}_{M}$", "equations": ["L'_{tm} = \\frac{F_T(t_m) \\cdot F_I(v_j)}{\\|F_T(t_m)\\| \\|F_I(v_j)\\|} - \\frac{F_T(t_m) \\cdot F_I(v_i)}{\\|F_T(t_m)\\| \\|F_I(v_i)\\|},", "t'_m = \\underset{t'_m \\in \\mathcal{B}(t_m,\\epsilon_t)}{\\arg \\max}(L'_{tm}).", "L_{vadv} = \\sum_{m=1}^M \\frac{F_T(t_k)}{\\|F_T(t_k)\\|} - \\sum_{s_i \\in \\mathcal{S}} \\frac{F_I(g(v_i, s_i))}{\\|F_I(g(v_i, s_i))\\|},\\\\v_{adv} = \\underset{v_{adv} \\in \\mathcal{B}[v,\\epsilon_v]}{\\arg \\max}(L_{vadv}),", "L_{tadv} =  \\frac{F_T(t'_i) \\cdot F_I(v_j)}{\\|F_T(t'_i)\\| \\|F_I(v_j)\\|} -  \\frac{F_T(t_i) \\cdot F_I(v_{adv})}{\\|F_T(t_i)\\| \\|F_I(v_{adv})\\|},\\\\t_{adv} =  \\underset{t_{adv} \\in \\mathcal{B}(t_i,\\epsilon_t)}{\\arg \\max}(L_{tadv})."]}, {"title": "Feedback-based Modal Mutual Search", "content": "Different from previous black-box attacks on VLP mod-\nels (Lu et al. 2023; Gao et al. 2024), FMMS exploits the\nfeedback information of the target model to search for the\nadversarial examples in the adversarial region. In the vision-\nlanguage retrieval task, the target model takes inputs of im-\nage or text and returns the relevant Top-N ranked instances.\nMore restrictively, the target model simply outputs the hard\nlabel, i.e., whether it matches.\nTo this end, FMMS consists of two search strategies:\nFull search and Top-N search. The detailed pseudo-code\nfor FMMS is shown in Algorithm 1. Firstly, FMMS gen-\nerates the initial adversarial image and text in line 1. Then,\nif the initial adversarial image and text attack fail, FMMS\nrandomly selects $v_j$ and $t_k$ from the search space for modal\nmutual search. The Full search considers the entire dataset as\nthe search space, while the Top-N search attack selects en-\ntries with match rankings from 1 to N to form search space\n$\\mathcal{B}_{tr}$ and $\\mathcal{B}_{ir}$ as follows:", "equations": ["\\mathcal{B}_{tr} = top\\_n(R_{f_t}(t_{adv}, V_{all}), N_{tr}),", "\\mathcal{B}_{ir} = top\\_n(R_{f_t}(v_{adv}, T_{all}), N_{ir}),"]}, {"title": "Experimental Settings", "content": "Datasets In the experiments, we utilize two widely used\ndatasets for multimodal tasks: Flickr30K (Plummer et al.\n2015) and MSCOCO (Lin et al. 2014). Flickr30K comprises\n31,783 images, each associated with five descriptive texts.\nMSCOCO consists of 123,287 images, each also paired with\napproximately five descriptive texts. For consistency, we\nused five descriptive texts per image for both datasets. Fol-\nlowing the Karpathy splits (Karpathy and Fei-Fei 2015), we\nevaluated experiments with 1K images from Flickr30K and\n5K images from MSCOCO.\nModels We evaluated four popular vision-language pre-\ntraining (VLP) models: TCL (Yang et al. 2022), ALBEF (Li\net al. 2021), CLIPCNN, and CLIPVIT (Radford et al. 2021).\nALBEF incorporates three key components: an image en-\ncoder, a text encoder, and a multimodal encoder. The im-\nage encoder employs the ViT-B/16 (Dosovitskiy et al. 2021)\narchitecture, whereas the text and multimodal encoders are\nbased on a 12-layer BERT (Devlin et al. 2019) model, with"}, {"title": "Baselines", "content": "We adopt several widely recognized adversarial\nattack methods on VLP models as the baselines, including\nunimodal attack methods, PGD (Madry et al. 2018), BERT-\nAttack (Li et al. 2020b), and multimodal attack methods,\nSep-Attack (Zhang, Yi, and Sang 2022), Co-Attack (Zhang,\nYi, and Sang 2022), and SGA (Lu et al. 2023). To ensure\nconsistent evaluation, each baseline method employs default\nhyperparameters provided in their public codes."}, {"title": "Attack Settings", "content": "Both Full FMMS and Top-N FMMS are\nimplemented in this work. For attacking image modality,\nwe adopt PGD (Madry et al. 2018) with perturbation bound\n$\\epsilon_v = 2/255$, iteration steps T = 10, and step size a =\n0.5/255. The hyper-parameters of anti-aliasing scale images\nadopt the default settings in SGA (Lu et al. 2023). For at-\ntacking text modality, we employ BERT-Attack (Li et al.\n2020b) with perturbation bound $\\epsilon_t$ = 1. For Top-N FMMS,\nwe set the length of the matched list $N_{ir}$ = 5 and $N_{tr}$ = 10.\nMoreover, we set the search steps T = 10 for both Full and\nTop-N FMMS."}, {"title": "Metrics", "content": "We utilize Attack Success Rate (ASR) to evalu-\nate adversarial performance in both white-box and black-\nbox settings. ASR indicates the percentage of successful ad-\nversarial examples generated. A higher ASR indicates better\nattack performance."}, {"title": "Experimental Results", "content": "Comparison with the Baselines To demonstrate the per-\nformance of the proposed FMMS for black-box attacking\nVLP models, we conduct a comprehensive evaluation across\nsignificantly different models. We generate adversarial ex-\namples on both two types of VLP models, i.e., fused and\naligned VLP models. The black-box attack performance is\nevaluated when all other models remain as the target model.\nFollowing SGA (Lu et al. 2023), we ensure the consistency\nof image input size.\nTable 1 shows the comparison with FMMS implemented\nTop-N search strategy across various VLP models on\nFlickr30K for TR and IR subtasks. The experimental re-\nsults demonstrate that our proposed FMMS outperforms the\nexisting multimodal attack methods in both white-box and\nblack-box settings. Specifically, FMMS achieves a 100% at-\ntack success rate for TR R@1 and outperforms the baselines\nfor IR R@1 across various VLP models in the white-box\nsetting. In the black-box setting, our FMMS achieves sig-\nnificant improvements in ASR when the surrogate and tar-\nget models are of the same type. For instance, the ASR of\nadversarial examples generated on ALBEF to attack TCL\nsurpasses Co-attack by approximately 50% and SGA by"}, {"title": "Comparison on the Full and Top-N Strategies", "content": "Further-\nmore, we explore the attack performance of FMMS under\nthe Full search strategy and the Top-N search strategy. Ta-\nble 3 shows the attack success rate (ASR) of FMMS across\nvarious VLP models on Flickr30K under different search\nstrategies. It can be observed that the ASR of the Top-N\nsearch strategy is almost higher than that of the Full strat-\negy in black-box settings. It indicates that the Top-N search\nstrategy leverages more feedback information to effectively\nnarrow the search space compared to the Full search, achiev-\ning a better attack success rate (ASR). Specifically, when\nALBEF, TCL, and CLIPvit are used as surrogate models to\nattack various VLP models, the black-box performance of\nTop-N FMMS consistently achieves the best results. Over-\nall, our proposed Full FMMS and Top-N FMMS signifi-\ncantly outperform the current typical transfer-based attacks,"}, {"title": "Further Discussion", "content": "Hyperparameter Study We further analyze the relation-\nship between the attack success rate and iteration steps on\nthe two subtasks of image-text retrieval, as shown in Fig-\nure 4. It is reasonable to observe that the attack performance\nbecomes more effective with an increasing number of iter-\nations for both TR and IR tasks. When the number of iter-\nations T > 8, the attack performance of FMMS starts to\nconverge. Therefore, we set the iteration steps T = 10 to\ntrade off the attack success rate and computational costs."}, {"title": "Limitation", "content": "The ASR of FMMS in the TR subtask in-\ncreases significantly with more iterations, while in the IR\nsubtask, the increase is marginal. This difference arises be-\ncause updating adversarial images is a continuous optimiza-\ntion, making ASR highly proportional to iteration count.\nHowever, the process of optimizing the adversarial text is\ndiscrete, with word-level replacements causing instability\nduring the search. Consequently, as depicted in Figure 4, the\nASR in the TR subtask shows a marked promotion, whereas\nin the IR subtask, it improves slightly and oscillates."}, {"title": "Conclusion", "content": "The feature distributions across different vision-language\npre-training (VLP) models can vary significantly, making\nit hard to generate well-transferable adversarial examples.\nTo this end, we propose a novel attack paradigm, namely\nFeedback-based Modal Mutual Search (FMMS), to attack\nVLP models. We introduce a modal mutual loss for cross-\nmodal interactions, guiding the gradient of the adversar-\nial examples with a more promising direction. By leverag-\ning the target model's feedback for multi-round refinement,\nFMMS further optimizes the adversarial examples, driving\nunsuccessful examples into the adversarial region. Based on\nthe VLP model's feedback information, we present two vari-\nants of FMMS, i.e., Full FMMS and Top-N FMMS, where\nthe Top-N version narrows the search space compared to\nthe Full version. Extensive experimental results demonstrate\nthat FMMS outperforms existing baselines."}]}