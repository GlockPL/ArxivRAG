{"title": "HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation", "authors": ["Jaione Bengoetxea", "Mikel Zubillaga", "Ekhi Azurmendi", "Maite Heredia", "Julen Etxaniz", "Markel Ferro", "Jeremy Barnes"], "abstract": "In this paper we present our submission for the NorSID Shared Task as part of the 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks: Intent Detection, Slot Filling and Dialect Identification, evaluated using data in different dialects of the Norwegian language. For Intent Detection and Slot Filling, we have fine-tuned a multitask model in a cross-lingual setting, to leverage the xSID dataset available in 17 languages. In the case of Dialect Identification, our final submission consists of a model fine-tuned on the provided development set, which has obtained the highest scores within our experiments. Our final results on the test set show that our models do not drop in performance compared to the development set, likely due to the domain-specificity of the dataset and the similar distribution of both subsets. Finally, we also report an in-depth analysis of the provided datasets and their artifacts, as well as other sets of experiments that have been carried out but did not yield the best results. Additionally, we present an analysis on the reasons why some methods have been more successful than others; mainly the impact of the combination of languages and domain-specificity of the training data on the results.", "sections": [{"title": "1 Introduction", "content": "Dialectal variation is ubiquitous in human language and should be taken into account when performing Natural Language Processing (NLP) tasks, as NLP systems unable to deal with dialectal data can cause users to feel frustrated and lead to unintended biases (Harwell, 2018).\nThis is especially relevant for Spoken Language Understanding (SLU), a field of Speech Processing and Natural Language Understanding aimed at ensuring the semantic comprehension of human utterances by virtual assistants. To make systems that rely on SLU more robust and able to handle real use-cases, it is necessary to develop resources for these tasks not only for different languages, but for different language varieties, so that the benefits of these models can reach a wider variety of speech communities.\nWith this motivation, the NorSID Shared Task consists of three subtasks (intent detection, slot filling and dialect identification) in four Norwegian variants: Bokm\u00e5l (B), Western (V), Tr\u00f8ndersk (T) and North Norwegian (N). The tasks are centered around common virtual assistant tasks, such as setting alarms or questions about the weather.\nOur team participated in all three subtasks, for a total of 6 runs: 3 for the SID (Slot and Intent Detection) tasks and 3 for Dialect Identification. As a team, we placed first in Dialect Identification, second in Intent Detection, and third in Slot Filling. Our code is publicly available on GitHub."}, {"title": "2 Task Descriptions", "content": "As mentioned, this shared task consists of the following three subtasks:\nIntent Detection. It is a text classification task that assigns intent labels to the utterances of the users, to guide the chatbot's answer, depending on its domain and purpose.\nSlot Filling. It requires classifying token spans that contain relevant information for a virtual assistant to fulfill certain tasks, e.g., to set an alarm, the assistant needs to know the time to set it to.\nDialect Identification. The aim of this classification task is to identify the dialect of the utterance."}, {"title": "2.1 Initial Data: NoMusic dataset", "content": "The shared task uses the NoMusic dataset (M\u00e6hlum and Scherrer, 2024), a \u201cmulti-parallel resource for written Norwegian dialects, and the first evaluation dataset for slot and intent detection focusing on non-standard Norwegian varieties.\u201d"}, {"title": "3 Intent Detection & Slot Filling", "content": "In this section, we will detail our participation in the intent and slot filling subtasks. We first explain the data (Section 3.1) and the experimental design (Section 3.2), and finally a description and an analysis of our results (Section 3.3)."}, {"title": "3.1 Data", "content": "XSID (van der Goot et al., 2021; Aepli et al., 2023; Winkler et al., 2024) is a cross-lingual corpus for SLU. The original English data was sampled by selecting random instances from the Snips (Coucke et al., 2018) and Facebook (Schuster et al., 2019) datasets. It features annotations for both intent detection, with one intent per instance; and slot filling, using the BIO format to tag each token. For the validation and test sets, the data was manually translated by native speakers of each language, maintaining the original intents, while the slots were manually re-annotated. The training data is available for most of the xSID languages through machine translation and projection of the slots.\nFor the Intent Detection task, there are a total of 18 intents. As per the slot filling task, there are 33 possible slots that can appear as the beginning (B) or inside (I) of a span and an O tag for the absence of entity. This results in a total of 67 possible tags.\nAlthough the original paper leaves duplicated sentences to model the natural distribution found in the data, we deduplicate to avoid our models overfitting on the training data. We only carry out shallow deduplication, removing instances that contain the same text."}, {"title": "3.2 Experiments", "content": "Intent detection and slot filling are two highly related tasks. In fact, there are some slots that will only appear in sentences tagged with a certain intent and vice-versa. In this respect, a model could make use of the annotations of both tasks at the same time to obtain better predictions. Our experiments for the SID tasks build on that idea, using a multilingual multitask model jointly trained for intent detection and slot filling. As shown in Figure 1, our multitask models learn to classify the intents on top of the [CLS] token and the probabilities for each token on top of them.\nSince intent detection and slot filling are classification tasks, we fine-tune the multilingual encoder model XLM-ROBERTa large (Conneau et al., 2019). This allows us to take advantage of cross-lingual transfer by training on different combinations of languages from xSID.\nThe multitask loss is calculated as the weighted sum of the loss for intent and slot detection"}, {"title": "3.3 Results", "content": "We have performed preliminary experiments on the development dataset to select the best combination of languages using three different random seeds. The results of these experiments can be seen in Table 3, where we report the F\u2081 and accuracy metrics for the slots and intents respectively. We also calculate the Lambda average metric, that is a weighted average, where we use the same \u03bb value as in the multitask loss function.\nThe results show that training only on the English training data produces the best results, with a Lambda average of 84.96%, probably because machine-translated data can introduce noise to the model."}, {"title": "3.3.1 Analysis of the Intent Detection results", "content": "Without any hyperparameter tuning, most models obtain near 100% accuracy in the intent detection task. This is likely because the data is from a reduced domain, where instances contain clear word-level features that let the model infer the label.\nTo test this idea, we fine-tuned and compared the results of English only models, [BERTS (Devlin et al., 2019; Turc et al., 2019) and ROBERTa (Liu et al., 2019)], against multilingual and Norwegian models, [XLM-ROBERTa (Conneau et al., 2019) and NorBERT3 (Samuel et al., 2023)]. Figure 2 shows that no prior knowledge of Norwegian is required to obtain an accuracy of up to 96%, which is aligned with our initial presumption that models are learning to classify the instances relying on specific word patterns rather than semantic understanding. However, prior knowledge of Norwegian greatly reduces the number of parameters required to obtain top performance and allows the model to surpass the performance of English only models."}, {"title": "4 Dialect Identification", "content": "In this section, we will present the dialect identification task, starting with the data used in our experiments (Section 4.1), followed by the experimental setting training only on the development set from the shared task (Section 4.2), as well as the experimental settings when training on alternative sources of data (Section 4.3). Finally, we describe the results of using different data and settings (Section 4.4)."}, {"title": "4.1 Data", "content": "The following section presents all the datasets we have used in our experiments, which consist of the NoMusic data (Table 2), as well as some further dialectal data. This data comes from two main sources: (i) tweets, which we collected from NorDial and the Nordic Tweet Stream (NTS); and (ii) transcriptions, which come from NB Samtale and the Nordic Dialect Corpus (NDC)."}, {"title": "4.1.1 NoMusic", "content": "As introduced in Section 2.1, NoMusic is the development data provided by the shared task. However, there is no additional training data that has been labeled for the dialect identification task in the SID tasks.\nConsequently, we split the development set into train, development and test sets (from now on, dev-train, dev-dev and dev-test). Each sentence in this dataset is paraphrased 11 times, once for each dialect annotator. Thus, in order to avoid data contamination, we split by the original ID of each instance, as many translated instances are similar or identical (Table 6). The results presented in Section 4.2 correspond to the dev-test results."}, {"title": "4.1.2 NorDial", "content": "NorDial (Barnes et al., 2021) is a corpus of 1,073 Norwegian tweets annotated for four dialects: Bokm\u00e5l, Nynorsk, Dialect, or Mixed. We merge this data together with the additional annotated data"}, {"title": "4.1.3 Nordic Tweet Stream (NTS)", "content": "NTS7 (Laitinen et al., 2018) is a corpus of geolocated tweets and their associated metadata from the Nordic region between the years of 2013-2023. We downloaded 4.054.223 Norwegian tweets geolocated in a total of 426 Norwegian cities."}, {"title": "4.1.4 NB Samtale", "content": "NB Samtale is a speech corpus collected by the Language Bank at the National Library of Norway. It contains orthographic and verbatim transcriptions from podcasts and recordings of live events at the National Library, a total of 24 hours of transcribed speech from 69 speakers, divided into train, development and test splits. Table 8 shows the distribution of dialects in the data."}, {"title": "4.1.5 Nordic Dialect Corpus (NDC)", "content": "NDC9 (Johannessen et al., 2009, 2012) includes orthographic and phonetic transcriptions of Nordic speaker recordings, with almost two million words from Norwegian dialects. It contains recordings"}, {"title": "4.2 Experiments with Development Data", "content": "In this section, we describe baselines using only the dialectal data in the development set, using the splits described in Section 4.1.1 (Table 6). We explore lexical mapping SVM, fine-tuning encoders and decoders, as well as using few-shot decoders."}, {"title": "4.2.1 Lexical Mapping SVM", "content": "We first create a simple baseline by mapping common lexical items in Bokm\u00e5l to their respective dialectal counterparts. The items we map are mainly pronouns and interrogatives, as well as a few common prepositions, verbal forms, and time expressions. For each dialect, there is often a one-to-many mapping from Bokm\u00e5l, as can be seen in Table 9."}, {"title": "4.2.2 Encoder Fine-tuning", "content": "We fine-tune encoders on the dev-train set, as well as on the combination of dev-train with the lexical mapping silver (Lexmap + dev-train). We choose the best encoder model specifically trained for Norwegian, NorBERT3-L (Samuel et al., 2023), as well as the multilingual encoder model XLM-Roberta-large (Conneau et al., 2019). As preliminary experiments showed training on the full development set with NorBERT3-L leads to the best performance, we also train the following variants: (i) training on the combined dev-train and"}, {"title": "4.2.3 Decoder Few-shot", "content": "We perform few-shot prompting experiments, providing the model 4 example instances, one for each dialect label. The few-shot examples are sampled from the dev-dev split and we evaluate on the dev-test set. We experiment with a decoder model specifically trained for Norwegian, NorMistral-7b-warm, and a multilingual decoder model, Llama 3.1-8B (Dubey et al., 2024), and use both base and instruct models, evaluating with LM evaluation Harness (Gao et al., 2023). The prompt used in these experiments is shown below:\nIn which dialect is this text written? Choose between North Norwegian, Tr\u00f8ndersk, West Norwegian or Bokm\u00e5l. Text: {text} Dialect:"}, {"title": "4.2.4 Decoder Fine-tuning", "content": "Next, we fine-tune several decoders on the development set, similar to the experiments with decoders. We only experiment with NorMistral models, as they achieve higher results in few-shot evaluation. We perform finetuning in two ways: by adding a sequence classification (SC) head and training the models applying supervised fine-tuning (SFT) using the same English prompt as in the few-shot"}, {"title": "4.3 Experiments with other Data Sources", "content": "As no labeled training dataset is available for dialect classification, we also explore whether it is possible to use other sources of data to learn to classify Norwegian dialects.\nFirst, we apply the semi-automatic and automatic annotation methods (see subsections 4.3.1 & 4.3.2), and get statistics about the resulting dialectal distribution of tweets and transcriptions.\nNext, we fine-tune NorBERT3-L on the semi-automatically and automatically labelled transcriptions and tweets to measure the impact of using automatically labeled data sources. During training, we use the dev-dev split as validation to avoid overfitting on these datasets and use the same hyperparameters (see Appendix A)."}, {"title": "4.3.1 Semi-Automatic Annotation", "content": "We perform a semi-automatic dialect label annotation on the NDC dataset, by first eliminating special transcription characters, e.g., pause markers (#) or (mm), as well as short sentences, which we assume have fewer dialectal traits.\nFinally, we semi-automatically map cities in NDC to their corresponding dialect label, according to their geographical location. Table 10 re-"}, {"title": "4.3.2 Automatic Annotation", "content": "We automatically annotate silver training data using two classifiers: the best model trained on development data (see Section 4.4.1) and a model trained on NorDial data. Experiments on NorDial suggest NB-BERT-base is the strongest classifier, achieving 90% weighted F\u2081 score, thus being chosen as our NorDial classifier. The objective of using two classifiers is to minimize model bias.\nTherefore, having the results of our two classifiers, we discard examples classified as Nynorsk and Mixed by the NorDial classifier. For Bokm\u00e5l, we select examples where the two classifiers match. For the dialectal tweets, we assign the class of the NorBERT3-L classifier if it is one of N, V or T.\nNB Samtale We train a classifier on NB Samtale data with the available splits to measure to what extent there are dialectal features in the orthographic and verbatim transcriptions. We get a weighted F1 of 76.76% with the verbatim transcriptions, so we can conclude that the models are able to learn the different features of the dataset. However, as training on this data leads to poor results on the dev set, we decide to explore other annotation methods. The poor results suggest that the dialectal features present in both datasets are different. Additionally, we trained a model using both NB Samtale train set and dev-train, but the results obtained (F1 81.59%) are few points worse than the model trained only in dev-train (F1 82.44%).\nNTS The predicted distribution of dialects in NTS tweets does not match with the Norsid classifier distribution. Nordial classifier classifies 96.70% of instances as Bokm\u00e5l and Norsid classifier 66.93% as V. This makes sense because the distributions of their training data are different. After performing the automatic labeling, in order to obtain a distribution similar to the one we have in development, we have downsampled the automatically-labelled NTS instances until the distribution matches that of development (see Table 11).\nNDC We have additionally automatically annotated the NDC instances (see Table 11). In most cases, there is a large difference between semi-automatic and automatic labeling. This could be due to the training data for our classifier differing"}, {"title": "4.4 Results", "content": "The results were calculated using the official evaluation script of the shared task and the official metric, Weighted F1 Score. All dev results in this section correspond to dev-test."}, {"title": "4.4.1 Training only on Development Data", "content": "The lexical mapping baseline performs better than majority or random, achieving 53.91 and 56.11 weighted F\u2081 on the dev-test and test sets, respectively. Further training on the dev-train set improves this to 66.98 and 70.02.\nThere is a large difference between the two encoder models (see Table 12). Whereas XLM-Roberta does not reach the best lexical mapping baseline, NorBERT3-L surpasses the Lexmap + dev-train baseline by 15.46 points on the development set. Additionally training with the Lexmap data, however, harms performance by 7 points. NorBERT3-L models trained in Dev-train-dev and Dev-train-dev-test obtain the highest results the test set.\nIn the few-shot scenario, the four models barely beat the majority class baseline (27.67) and perform worse than a random classifier (32.82). NorMistral Instruct (30.83) is slightly better than its base counterpart (29.55), but they are still far from the lexical mapping baseline, which obtains around 30 points more. Regarding Llama3.1 base and instruct models, their performance is almost identical to NorMistral models, but none of them surpass the performance of NorMistral Instruct in this few-shot evaluation. Fine-tuning NorMistral gives better results than the few-shot approach (76.88)."}, {"title": "4.4.2 Training on other Sources of Data", "content": "The results in Table 12 suggest that using tweets is better than transcriptions, in both semi-automatically and automatically labeled experiments: we obtain a weighted F\u2081 of 64.22 in our tweets model, while the transcription models perform between 30-52 points. However, the performance of the tweets model is still far from models trained on the development set (84.17).\nWhen using transcriptions, the phonetic ones are preferable to orthographic ones, as more dialectal features are retained. Using longer sentences (>40 tokens) generally has little impact on performance, except for automatically labeled phonetic transcriptions.\nThe model trained on NB Samtale dataset achieves lower scores than models trained on NDC and NTS. This seems to be due to a low overlap in dialectal features between the NB Samtale and the shared task data."}, {"title": "4.4.3 Dialect Analysis", "content": "We have selected the best performing models from each strategy to analyze the performance in each dialect. The models we have chosen are, Dev-train-dev NorBERT3-L, Few-shot NorMistral-7b-warm-it, NTS NorBERT3-L, Semi-automatic labeled"}, {"title": "5 Conclusion and Future Work", "content": "We have presented our submission for the NorSID Shared Task in the 2025 VarDial Workshop (Scherrer et al., 2025). We have participated in the three proposed tasks \u2013 Intent Detection, Slot Filling and Dialect Identification \u2013 with 3 submissions for each of them.\nFor the Intent Detection & Slot Filling tasks we designed a multitask model, improving efficiency with respect to having a model for each task. Additionally, as both tasks are highly related, this combination improves the performance of the model in both tasks to 97.69% accuracy and 85.37% F1, respectively, in the test set.\nIn Dialect Identification, we tested many different approaches by using the development data as training, as well as additional data from tweets and transcriptions. However, none of the settings we tried were able to surpass the performance of NorBERT3-L fine-tuned only on the development set, which achieved 84.17 F\u2081 on the test set.\nThe research presented in this paper has opened the way to many questions that need further investigation. We believe that the results could be improved using better encoder, e.g., DeBERTa (He et al., 2021), and decoder, e.g., Llama 3.1 70B) models. The additional data we collected for dialect identification has not been successful due to the narrow domain of the tasks, but it is likely that for other tasks with a stronger domain shift this data could provide for more robust training."}, {"title": "Acknowledgements", "content": "This work has been partially funded by:\n\u2022 Disargue (TED2021-130810B-C21) MCIN/AEI/10.13039/501100011033 and European Union NextGenerationEU/PRTR.\n\u2022 DeepKnowledge (PID2021-127777OB-C21) MCIN/AEI/10.13039/501100011033 and by FEDER, EU.\n\u2022 Ixa group A type research group (IT1570-22) Basque Government\n\u2022 IKER-GAITU project 11:4711:23:410:23/0808 by Basque Government\n\u2022 Julen Etxaniz holds a PhD grant from the Basque Government (PRE_2024_2_0028).\n\u2022 Maite Heredia is supported by the UPV/EHU PIF23/218 predoctoral grant.\n\u2022 The EU Horizon Europe Framework under the grant 101135724 (LUMINOUS)."}]}