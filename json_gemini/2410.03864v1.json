{"title": "DOTS: LEARNING TO REASON DYNAMICALLY IN LLMS VIA OPTIMAL REASONING TRAJECTORIES SEARCH", "authors": ["Murong Yue", "Wenlin Yao", "Haitao Mi", "Dian Yu", "Ziyu Yao", "Dong Yu"], "abstract": "Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called \"reasoning actions\"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DoTs, an approach enabling LLMs to reason Dynamically via Optimal reasoning Trajectories Search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems. Our code is available at https://github.com/MurongYue/DOTS.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks (Rae et al., 2021; Lewkowycz et al., 2022; Zhong et al., 2023), such as math reasoning (Imani et al., 2023; Ahn et al., 2024), symbolic reasoning (Kojima et al., 2022), and common-sense reasoning (Krause & Stolzenburg, 2023; Zhao et al., 2024). The dominant approaches to eliciting reasoning capability in LLMs mainly fall into two categories, i.e., instruction tuning and prompt engineering. Instruction tuning (Wang et al., 2022) collects question-answer pairs about the reasoning task and employs supervised fine-tuning to optimize an LLM for better reasoning performance (Yue et al., 2024; Tang et al., 2024), with recent effort focusing on improving the scale and the quality of the fine-tuning data (Luo et al., 2023; Peng et al., 2023; Yue et al., 2023; 2024; Chan et al., 2024). Prompt engineering instead aims to design better prompts to elicit the reasoning capability of an LLM without updating its parameters. The Chain-of-Thought (CoT) approach (Wei et al., 2022; Kojima et al., 2022) prompts an LLM to answer the reasoning question step by step in natural language, and program-aided approaches (Chen et al., 2022; Gao et al., 2023) prompt the LLM to write executable code and leverage an interpreter to execute code for obtaining the final result. Besides,"}, {"title": "2 DOTS: LEARNING TO REASON DYNAMICALLY", "content": "Our goal is to enable LLMs to select the most effective reasoning actions autonomously. Denote LLM, as the task-solving LLM, Q as the input query, p as the reasoning action trajectory path, E as the explanation for a trajectory, and R as the reasoning process leading to the final answer y. Our approach encompasses two setups during the inference stage (Figure 1):\nExternal Planner Tuning This setup is designed for scenarios where the solver (LLM) is a closed-source LLM or is computationally costly to train. As depicted in Figure 1 (c), we train an external planner, denoted as LLMp, to determine the optimal reasoning actions:\n$\\displaystyle (E,p) = LLM_p(Q;\\theta_p) \\qquad (1)$\nwhere $\\theta_p$ is the parameters of LLMp. We empirically found that training the planner to explain its trajectory selection (E) helps its learning. Upon obtaining reasoning actions, the solver LLM, parameterized by $\\theta_s$ then proceeds to generate the reasoning process R and the final answer y:\n$\\displaystyle (R, y) = LLM(Q,T;\\theta_s) \\qquad (2)$\nInternalized Planner Tuning This setup is designed for task-solving LLMs (LLM) that are open-source and small-size. In this case, we propose to internalize the trajectory planning capability into the task-solving LLM by training it to simultaneously learn to plan and learn to perform the reasoning task. As shown in Figure 1 (d), the final answer y is obtained by:\n$\\displaystyle (E, p, R, y) = LLM_s(Q; \\theta_s) \\qquad (3)$\nAn overview of DOTS's learning process is presented in Figure 2, consisting of three key steps: (i) Defining atomic reasoning modules: We define several atomic reasoning modules, each representing a distinct reasoning action, (ii) Searching for optimal action trajectories: We conduct explorations and evaluation of various reasoning paths to identify optimal reasoning actions for questions in the training data, and (iii) Fine-tuning LLMs to plan for optimal reasoning trajectories: We fine-tune LLMs to autonomously plan the reasoning action trajectory under the two aforementioned setups. In what follows, we elaborate on each step."}, {"title": "2.1 OVERVIEW", "content": "Our goal is to enable LLMs to select the most effective reasoning actions autonomously. Denote LLM, as the task-solving LLM, Q as the input query, p as the reasoning action trajectory path, E as the explanation for a trajectory, and R as the reasoning process leading to the final answer y. Our approach encompasses two setups during the inference stage (Figure 1):\nExternal Planner Tuning This setup is designed for scenarios where the solver (LLM) is a closed-source LLM or is computationally costly to train. As depicted in Figure 1 (c), we train an external planner, denoted as LLMp, to determine the optimal reasoning actions:\n$\\displaystyle (E,p) = LLM_p(Q;\\theta_p) \\qquad (1)$\nwhere $\\theta_p$ is the parameters of LLMp. We empirically found that training the planner to explain its trajectory selection (E) helps its learning. Upon obtaining reasoning actions, the solver LLM, parameterized by $\\theta_s$ then proceeds to generate the reasoning process R and the final answer y:\n$\\displaystyle (R, y) = LLM(Q,T;\\theta_s) \\qquad (2)$\nInternalized Planner Tuning This setup is designed for task-solving LLMs (LLM) that are open-source and small-size. In this case, we propose to internalize the trajectory planning capability into the task-solving LLM by training it to simultaneously learn to plan and learn to perform the reasoning task. As shown in Figure 1 (d), the final answer y is obtained by:\n$\\displaystyle (E, p, R, y) = LLM_s(Q; \\theta_s) \\qquad (3)$\nAn overview of DOTS's learning process is presented in Figure 2, consisting of three key steps: (i) Defining atomic reasoning modules: We define several atomic reasoning modules, each representing a distinct reasoning action, (ii) Searching for optimal action trajectories: We conduct explorations and evaluation of various reasoning paths to identify optimal reasoning actions for questions in the training data, and (iii) Fine-tuning LLMs to plan for optimal reasoning trajectories: We fine-tune LLMs to autonomously plan the reasoning action trajectory under the two aforementioned setups. In what follows, we elaborate on each step."}, {"title": "2.2 DEFINING ATOMIC REASONING ACTIONS MODULES", "content": "Prior studies have validated the effectiveness of various reasoning strategies (Table 1). We build on top of them and categorize the existing strategies as reasoning actions across three layers:\nAnalysis Layer Actions in this layer enable the LLM to analyze the input query before attempting to solve it, including (1) Query rewriting: reformulating the query to enhance comprehension, and (2) Query decomposition: breaking down the initial question into multiple, more manageable sub-questions. We denote the action taken in this layer as $A_a$."}, {"title": "2.3 SEARCHING FOR OPTIMAL REASONING ACTION TRAJECTORIES", "content": "To teach the external/internalized planner to plan for the optimal reasoning trajectory, we start by constructing training data containing questions and their optimal action trajectories for the specific task-solving LLM. We obtain this by iteratively searching all possible reasoning trajectories for each question, including exploring the current paths and pruning paths that are unlikely to be optimal. The task-solving LLM is used during this search process to generate answers to make the reasoning trajectory align with their intrinsic ability to perform different reasoning actions effectively.\nThis searching process is shown in Algorithm 1. Given the query and ground-truth answer sourced from the training data, the process runs iteratively. In each iteration, the algorithm considers either the full set of candidate trajectories (for iteration k = 1) or the current best subset (for iteration k > 1). Each candidate trajectory is executed for Neval times with a non-zero temperature to obtain a more reliable evaluation of its success rate. We then sort the current subset of trajectories by its"}, {"title": "2.4 LEARNING TO PLAN FOR OPTIMAL REASONING TRAJECTORIES", "content": "Having obtained the optimal trajectories, we then use supervised fine-tuning with cross-entropy loss to train the planner LLM to predict optimal trajectories for input questions and the specific solver LLM. For external planner tuning, a lightweight LLMp is trained to predict a concatenation of the explanation and the optimal trajectory (Eq 1); for internalized planner tuning, the solver LLM, is trained to predict the explanation, the optimal trajectory, the reasoning process collected from LLM, itself, and the true answer y* (Eq 3)."}, {"title": "3 EXPERIMENT", "content": "Datasets We evaluate the effectiveness of our method across multiple datasets and various reasoning tasks. Based on the distribution of the training and testing data, we divide the evaluation into three settings"}, {"title": "3.1 EXPERIMENTAL SETUP", "content": "Datasets We evaluate the effectiveness of our method across multiple datasets and various reasoning tasks. Based on the distribution of the training and testing data, we divide the evaluation into three settings as shown in Table 2: In-distribution setting evaluates the model that resembles what it has seen during training. Few-shot setting aims to evaluate whether our proposed method can effectively learn from a small amount of labeled data. In the real world, it is often difficult to obtain large amounts"}, {"title": "3.2 BASELINES", "content": "We include the following highly related baselines in our experiments. (1) CoT (Wei et al., 2022) prompts an LLM to answer step-by-step; (2) PoT (Chen et al., 2022) prompts an LLM to generate Python code and execute the code to get the final answer; (3) Least-to-most (LTM) (Zhou et al., 2023) prompts an LLM to first decompose the question into multiple sub-questions before solving it; (4) Self-refine (Madaan et al., 2024) prompts an LLM to generate the answer and verify and refine the answer by the LLM itself. Madaan et al. (2024) used PoT in solving math questions, therefore we follow their setting to use PoT in generating the initial answer; (5) PromptAgent (PA) (Wang et al., 2023) searches for a better prompt for the specific task based on its training data; this baseline is implemented with the default hyperparameter setting; and (6) Vanilla Supervised Fine-Tuning (Vanilla SFT) uses GPT-40 to generate the CoT reasoning process for questions in the training datasets and then fine-tune the solver LLM to predict the generated reasoning process and the ground-truth answer; this baseline is fine-tuned using the same hyperparameter setting as our internalized planner tuning. The training data for PA, Vanilla SFT, and DOTS are from the same source."}, {"title": "3.3 EXTERNAL PLANNER TUNING RESULTS", "content": "Table 3 presents the results of using the external planner, which suggest that:\nExternal planner tuning outperforms other methods on the in-domain task Our method achieves 57.7% accuracy with Llama-3-70b-Instruct and 75.4% accuracy with GPT-40-mini on MATH, achieving significant improvement than baselines. This suggests that DOTS is robust across different LLMs and it can significantly enhance the LLM's zero-shot reasoning ability. The improvement from DOTS remains consistent as the solver LLM's capabilities increase, indicating DOTS has a long-term value even as LLMs continue to improve rapidly."}, {"title": "3.4 INTERNALIZED PLANNER TUNING RESULTS", "content": "Table 4 presents the results of our internalized planner tuning, where we observed:\nInternalized planner tuning demonstrates superior performance DOTS outperforms existing methods on average, including prompt engineering methods and vanilla SFT. Notably, our approach surpasses self-refine in the Game of 24, a different observation than the experiments with an external planner (Table 3). We attribute this performance boost to our joint optimization of the trajectory planning and problem-solving processes. Unlike external planner tuning which only updates the external planner (LLM), internalized planner tuning enables the task-solving LLM to simultaneously learn trajectory planning and accurate reasoning process generation. This highlights that the internalized planner tuning effectively further enhances performance.\nSearching for the optimal reasoning action trajectory helps enhance the utilization of training data Compared to vanilla SFT, our method consistently shows performance improvements across all datasets, notably achieving an 8.7% increase on BBH. This suggests that, instead of training with a question and step-by-step reasoning process pair, our approach of searching for an optimal action trajectory and generating the corresponding reasoning process to construct training data is superior. This finding indicates that our search methodology could effectively enhance the utilization of training data for reasoning tasks without the need for additional human annotations."}, {"title": "3.5 OUT-OF-DISTRIBUTION EXPERIMENTAL RESULTS", "content": "Our method consistently generalizes well across diverse OOD challenges As shown in Table 5, DOTS maintains high accuracy across different datasets and models. In contrast, static methods often fluctuate significantly in performance. For instance, despite static methods like CoT showing a slight advantage on MMLU-Pro and StrategyQA over DOTS using the Llama-3-70B-Instruct model, they experience a sharp decline on DeepMind Math. This pattern of fluctuations can be observed in other methods as well, where some excel on individual tasks but fail to maintain strong performance. In contrast, DOTS continues to deliver consistently high accuracy across various models and datasets. The stability of our method is attributed to its ability to dynamically select appropriate reasoning trajectories. The results indicate that DOTS is better suited to meet the demands of diverse tasks, demonstrating stronger robustness and generalization, making it a more reliable and adaptable approach for handling a wide variety of OOD challenges."}, {"title": "3.6 ABLATION STUDY", "content": "In this section, we perform the ablation study and assess the effectiveness of each component of our method: (1) Without Searching: To demonstrate the effectiveness of searching for the optimal"}, {"title": "3.7 OPTIMAL TRAJECTORY ANALYSIS FOR DIFFERENT TASKS", "content": "Table 7 shows the distribution of actions selected in the optimal trajectories by our planner on the MATH test set. The distribution suggests two key findings:\nDOTS adapts to the characteristics of specific questions In mathematics, number theory problems are more suitable to be solved with programs, so the proportion of PoT is higher, while geometry problems are not easily represented and solved with naive Python code; as a result, our planner mainly uses CoT for such problems. This indicates that DOTS tailors its action selection based on the unique characteristics of each problem type.\nDOTS adapts to the capability of specific task-solving LLMS As shown in Table 3, on the MATH dataset, GPT-40-mini performs better using CoT for problem-solving, whereas Llama3-70B-instruct performs better using PoT. When GPT-40-mini is the task-solving LLM, our fine-tuned planner selects a higher proportion of CoT actions; when Llama-3-70B-Instruct is used, PoT actions"}, {"title": "3.8 ADDITIONAL ANALYSES", "content": "Few-shot In-context Learning Setting\nOur main results report the performance setting with GPT-40-mini as the solver.\nwith zero-shot evaluation. In cases where\nreasoning tasks are known in advance,\na common approach to leveraging train-\ning data and improving the performance\nof closed-source LLMs is few-shot in-\ncontext learning (ICL), where training ex-\namples are incorporated directly into the\ncontext. Our external planner tuning can"}, {"title": "How efficient is DOTS?", "content": "We compare the cost effi-\nciency, measured by the average output token count,\nof each method (based on Llama-3-8B-Instruct) in\nTable 9. The result shows that DOTS consumes\nfewer tokens on average than other advanced ap-\nproaches and only more than CoT. Advanced prompt\nengineering methods often introduce supplementary\ntext to facilitate reasoning. However, not all ques-\ntions require this additional context to the same ex-\ntent. By constructing training data via searching, our\ngoal is to optimize the balance between minimizing extraneous steps and maintaining a high success\nrate, thereby reducing unnecessary output tokens. Our method avoids redundant reasoning actions,\nresulting in a more efficient system."}, {"title": "Do we need more reasoning steps for difficult questions?", "content": "Recent research suggests that LLMs\ncan better solve difficult questions by increasing the\nthinking time in the inference stage (Brown et al.,\n2024; OpenAI, 2024). In our study, we explore the\nrelationship between question difficulty and the av-\nerage reasoning action trajectory length. The trajec-\ntory length is determined by assigning a value of\n0 to the EMPTY module and 1 to all other actions,\nwhile the question difficulty is derived from anno-\ntated levels on the MATH dataset. Figure 3 presents\nthat harder problems demand more computational\nsteps, resulting in longer reasoning trajectories. Case\nanalyses further reveal that our planner increases the\nproportion of verification steps as problem difficulty\nrises. This highlights an exciting fact LLMs can learn to employ more reasoning steps for chal-\nlenging problems through exploration, without requiring explicit expert guidance."}, {"title": "4 RELATED WORK", "content": "Prompt engineering for LLM reasoning LLMs have demonstrated remarkable proficiency in\nsolving complex reasoning tasks (Rae et al., 2021; Lewkowycz et al., 2022; Zhong et al., 2023).\nThe Chain-of-Thought (CoT) approach, introduced by Wei et al. (2022), significantly improves per-\nformance on reasoning problems by prompting LLMs to think step-by-step, thereby activating their\ninherent reasoning capabilities (Madaan & Yazdanbakhsh, 2022). To further enhance LLMs' capa-\nbilities in mathematical and symbolic reasoning, Chen et al. (2022) and Gao et al. (2023) proposed\nthe Program-of-Thought prompting method, where code is used as an intermediate reasoning step.\nAdvanced prompt engineering methods, such as question decomposition (Zhou et al., 2023) and\nself-verification (Madaan et al., 2024), have also proven effective in improving reasoning perfor-\nmance. Additionally, recent approaches have incorporated automatic prompt optimization based on\ntraining data. For instance, Wang et al. (2023) refines prompts by analyzing error cases, and self-\ndiscovery (Zhou et al., 2024) utilizes modular reasoning components to construct the task-adaptive\nprompt. However, these automated prompt optimization techniques still produce static prompts for\nall instances. Recently, Srivastava et al. (2024) proposed the instance-level prompt optimization via\nLLM self-refining while it is still a passive expert-designed workflow and lacks the explorations\nand evaluations to guide the LLM to better actively adapt to the question and LLM capability. In\nour method, we internalize the reasoning action selection capability into the LLM itself without an\nexpert-designed workflow, allowing it to autonomously fit both the characteristics of questions and\nthe inherent capability of task-solving LLM.\nSearching for boosting LLM reasoning Recent research suggests that incorporating searching\nmechanisms can significantly enhance LLM reasoning. In the inference process, Tree-of-Thought\n(ToT) (Yao et al., 2024) and Graph-of-Thought (GoT) (Besta et al., 2024) have been proposed to\nsearch and investigate different reasoning paths, either by leveraging the LLM itself (Yao et al.,\n2024) or designing heuristic functions (Hao et al., 2023) as the signal to evaluate each step. More\nrecently, Monte Carlo Tree Search (MCTS) has been introduced to assist the LLM in learning how\nto evaluate each step (Qi et al., 2024; Xie et al., 2024). The searching mechanism can also be used\nin training to collect training instances for improving LLM reasoning (Luo et al., 2024). However,"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce DOTS, a method that enables LLMs to autonomously think about appropriate reasoning actions before answering questions. By defining atomic reasoning action modules, searching for optimal action trajectories, and training LLMs to plan for reasoning questions, we enable LLMs to dynamically adapt to specific questions and their inherent capability. The flexibility of our two learning paradigms, i.e., external and internalized planner tuning, further highlights the adaptability of our method to different LLMs. Our experimental results show the effectiveness of DOTS, revealing the promise of harnessing explorations and evaluations to turn LLMs into planners for better reasoning."}, {"title": "A TRAINING IMPLEMENTATION", "content": "In our optimal trajectory search, we set the number of iterations (K) to 2 and the number of evaluation times (Neval) to 4. The number of paths retained N\u2081 is set to 8 and N2 to 3. Throughout the search, we maintain a sampling temperature of 0.4. Searching on the training datasets eventually yields 1722 for GPT-40-mini, 1624 for Llama-3-70B-Instruct, and 2140 for Llama-3-8B-Instruct training examples for planner tuning, respectively. We up-sample few-shot examples by a factor of 8 and fine-tune the planner LLM using the LitGPT library (AI, 2023), applying a learning rate of 2e-5, a global batch size of 64, a maximum sequence length of 4096, and training epoch of 4."}, {"title": "B CASE STUDY", "content": "In this section, Q1 is a numerical reasoning problem that can be solved with Python code easily, so our tuned LLM selected PoT to solve it. QII is a question widely tested in the community where GPT-40 even tends to make mistakes, but after tuning with searched trajectory data, the LLM actively chose to use code to solve the problem."}, {"title": "D PROMPTS USED IN EXPERIMENTS", "content": "Prompt for query rewrite module\nIn this step, you need to reveal the Core Question with only a simple sentence and useful\ninformation. The output follows the format:\ncore question:...\nNote: Please extract the question-solving information related to the problem, and list them\none by one.\nuseful information:...\nPrompt for query decomposition module\nIn this step, you need to reflect on the problem, and describe it in your own words. Analyze\nhow you can decompose the problem into smaller, more manageable sub-tasks. Pay attention\nto small details, nuances, notes and examples in the problem description.\nPrompt for CoT module\nIn this step, you need to think step by step with words, solve the problem and get the answer.\nPrompt for PoT module\nIn this step, you need to write Python codes to solve the query. Use the simplest and most\nstraightforward programming methods to solve the problem. For instance, if a query can\nbe efficiently solved using a brute force method, prefer it over heuristic or more complex\nmethods. Utilize any available and commonly-used libraries that can simplify the task or\nimprove code maintainability. All the calculations must leverage codes. Print out the results\nwith the print() function. Before executing the program, you have no idea of the final answer.\nDon't show it in your comment or code. And don't use the plot function.\nIn this step, start with \"# Now write Python codes to answer this question and use print() to\nprint out the result\"\nPrompt for self-verification module\nIn this step, you need to carefully verify the correctness of the previous thoughts with natural\nlanguage. You need to formulate a verification question (not the same question as before)\nbased on the final answer and then verify the final answer you have. If the results are in-\ncorrect, the last line should end up with \"The answer is: incorrect\". Otherwise, the last line\nshould end with \"The answer is: correct\""}, {"title": "Prompt for explanation generation", "content": "Action Categories:\n1. Understanding process: query rewriting: Rewrite the question and answer it. Decomposi-\ntion: Decompose the questions into multiple subtasks to solve the sub-question. 2. Solving\nprocess: chain of thought: For step-by-step reasoning with language. programming: For\nprogramming solver. 3. Verification process: self-verification: To check the correctness of\nthe solution.\nTask Instruction: For the given question, explain why the above Required actions are nec-\nessary.\nExample 1:\nQuery: Find 2.5-1+8.11-1 (mod 56). Express your answer as an integer from 0 to 55,\ninclusive.\nRequired Action: programming, self-verification\nExplanation: This is a Modular arithmetic problem. The problem can be solved using\nstraightforward python code with sympy library, particularly modular arithmetic. Besides,\nthis type of problem is relatively easy to verify. After computing the result, one can check\nthe calculations step by step to ensure correctness and verify that the final answer is within\nthe given range (0 to 55 inclusive). Programming solver is more efficient and accurate for\nthis type of calculation and the verifier ensures the correctness of the result and adherence\nto the given constraints.\n(multiple examples)\nQuery: Given Query\nRequired Action: Actions After Searching\nExplanation:"}]}