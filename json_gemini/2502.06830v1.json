{"title": "OrderFusion: Encoding Orderbook for Probabilistic Intraday Price Prediction", "authors": ["Runyao Yu", "Yuchen Tao", "Fabian Leimgruber", "Tara Esterl", "Jochen L. Cremer"], "abstract": "Efficient and reliable probabilistic prediction of intraday electricity prices is essential to manage market uncertainties and support robust trading strategies. However, current methods often suffer from parameter inefficiencies, as they fail to fully exploit the potential of modeling interdependencies between bids and offers in the orderbook, requiring a large number of parameters for representation learning. Furthermore, these methods face the quantile crossing issue, where upper quantiles fall below the lower quantiles, resulting in unreliable probabilistic predictions. To address these two challenges, we propose an encoding method called OrderFusion and design a hierarchical multi-quantile head. The OrderFusion encodes the orderbook into a 2.5D representation, which is processed by a tailored jump cross-attention backbone to capture the interdependencies of bids and offers, enabling parameter-efficient learning. The head sets the median quantile as an anchor and predicts multiple quantiles hierarchically, ensuring reliability by enforcing monotonicity between quantiles through non-negative functions. Extensive experiments and ablation studies are conducted on four price indices: 60-min ID3, 60-min ID\u2081, 15-min ID3, and 15-min ID\u2081 using the German orderbook over three years to ensure a fair evaluation. The results confirm that our design choices improve overall performance, offering a parameter-efficient and reliable solution for probabilistic intraday price prediction.", "sections": [{"title": "1. Introduction", "content": "The rapid expansion of wind and solar energy in recent years introduces significant variability in power generation due to weather dependence. This variability often leads to forecasting errors in wind and solar power output, resulting in power system imbalances with the energy demand (Koch & Hirth, 2019).\n\nThe continuous intraday (CID) market plays a pivotal role in addressing this imbalance challenge. The CID market opens at 15:00 the previous day, allowing participants to adjust for unplanned energy imbalances arising from forecast errors in wind and solar power generation, up to five minutes before electricity delivery (Narajewski & Ziel, 2020b). As a result, the CID market significantly alleviates the demands on balancing energy (Ocker & Ehrhart, 2017). With the growing adoption of algorithmic trading in the CID market, intraday price prediction is crucial to managing uncertainties and optimizing trading strategies (Hirsch & Ziel, 2024b).\n\nThe CID market operates under weak-form efficiency. This concept states that recent market prices reflect past publicly available trading information. Various studies (Monteiro et al., 2016; Andrade et al., 2017; Janke & Steinke, 2019; Uniejewski et al., 2019; Narajewski & Ziel, 2020a;b; Hirsch & Ziel, 2024b) found that while intraday prices are influenced by factors such as wind and solar energy generation, day-ahead forecasts of these factors provide limited predictive power, since the information these factors carry is already reflected in recent market prices. Consequently, the most powerful predictors of future intraday price are extracted from recent trades in the orderbook.\n\nIn the context of intraday price prediction, there has been a gradual transition from pointwise prediction to probabilistic prediction. Pointwise prediction models, such as those explored in (Monteiro et al., 2016; Oksuz & Ugurlu, 2019; Uniejewski et al., 2019; Marcjasz et al., 2020; Narajewski & Ziel, 2020a), estimate a single future price value, are challenged to quantify market uncertainties. To address this limitation, probabilistic prediction models (Andrade et al., 2017; Narajewski & Ziel, 2020b; Serafin et al., 2022; Cramer et al., 2023; Hirsch & Ziel, 2024a;b) estimate potential price intervals by predicting price quantiles, providing a more nuanced understanding of market uncertainties.\n\nHowever, these probabilistic approaches face several challenges. First, they often rely on simplified input representations, such as flattened 1D vectors or 2D time-series formats, which fail to fully capture the bid-offer interdependencies. As a result, learning meaningful representations requires a large number of parameters, leading to parameter inefficiency. Second, these methods frequently encounter quantile crossing issues, where higher quantiles are predicted to be lower than lower quantiles, violating the fundamental properties of probabilistic forecasting (Chernozhukov et al., 2010). This inconsistency results in unreliable probabilistic predictions, posing a challenge for decision-making in energy trading.\n\nThis paper proposes an encoding method called OrderFusion and designs a hierarchical multi-quantile head. OrderFusion converts the orderbook into a 2.5D representation, shown in Figure 1. A tailored jump cross-attention backbone takes 2.5D encoding as an input to model interdependencies between bid and offer sides. The head sets the median quantile as an anchor, predicts multiple quantiles hierarchically with a shared representation, and ensures that upper quantiles remain higher than lower ones by incorporating monotonic constraints, overcoming quantile crossing issues. Furthermore, we conduct case studies and ablation studies on four price indices: 60-min ID3, 60-min ID1, 15-min ID3, and 15-min ID\u2081 over three years."}, {"title": "1.1. Contribution", "content": "We propose OrderFusion, an encoding method that models interdependencies between bids and offers with a tailored jump cross-attention backbone, enabling parameter-efficient learning.\nWe design a hierarchical multi-quantile head that sets the median quantile as an anchor and predicts multi-"}, {"title": "1.2. Roadmap", "content": "The rest of the paper is organized as follows: Section 2 reviews related works. Section 3 describes orderbook and price indices. Section 4 details the proposed methods. Section 5 presents experiments and ablation studies on four price indices. Finally, Section 6 concludes the paper."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Cross-Attention", "content": "Cross-attention has proven to be a powerful mechanism for capturing complex dependencies in sequential and structured data across various fields. In multivariate time series forecasting, it enables the fusion of temporal and static feature embeddings, as demonstrated in (Lim et al., 2021) and (Zhang & Yan, 2023), enhancing predictive performance by modeling intricate relationships between variables. In computer vision, (Chen et al., 2021) uses cross-attention to combine multi-scale image patch embeddings, improving classification accuracy. Similarly, in point cloud processing, (Afham et al., 2022) applies cross-attention between 2D and 3D representations to learn richer shape features in a self-supervised manner, while (Fei et al., 2023) utilizes cross-attention to integrate global and local information. Despite its success in various domains, cross-attention remains underexplored in intraday price prediction. Our proposed OrderFusion tailored with a jump cross-attention backbone aims to model the bid-offer interdependencies."}, {"title": "2.2. Multi-Quantile Prediction and Quantile Crossing", "content": "Multi-quantile prediction frameworks are becoming increasingly popular to capture uncertainties in price forecasts. The studies (Rodrigues & Pereira, 2020; Jawed & Schmidt-Thieme, 2022) aim to reduce the complexity of training by jointly predicting several quantiles from a shared representation. However, a well-known issue is quantile crossing, where upper quantiles occasionally yield lower values than lower quantiles. This inconsistency violates the fundamental property of cumulative distribution functions and can drastically reduce the reliability of interval forecasts (Chernozhukov et al., 2010). Previous works attempt to fix crossing errors via post-processing methods such as simply re-sorting quantiles (Maciejowska & Nowotarski, 2016; Serafin et al., 2019; 2022). Although straightforward, such solutions risk distorting the learned distribution by imposing an artificial correction step. (Park et al., 2022) introduces an incremental quantile function that anchors at the lowest quantile and employs non-negative functions, such as ReLU or Softplus, to learn positive residuals, which are then hierarchically added until reaching the highest quantile. However, this approach is prone to error accumulation through the process of iterative addition. Drawing inspiration from this design, we anchor at the median quantile and apply addition and subtraction to estimate tail quantiles, reducing the risk of error accumulation."}, {"title": "3. Data", "content": ""}, {"title": "3.1. Orderbook", "content": "The orderbook data purchased from the European Power Exchange Spot (EPEX Spot) provide detailed insights into electricity market dynamics. Specifically, the orderbook records trading activities with key attributes including deliv-"}, {"title": "3.2. Price Indices", "content": "We focus on four popular price indices: 60-min ID3, 60-min ID1, 15-min ID3, and 15-min ID1. The 60-min and 15-min indices correspond to different product types, where electricity is delivered every 60 min and 15 min, respectively. The ID3 index represents the volume-weighted average price (VWAP) of all filtered trades executed within the last 3 trading hours before delivery, focusing on the most liquid period of a trading session. The ID\u2081 index is calculated as the VWAP of all filtered trades executed within the final trading hour before delivery, capturing the market's last-minute imbalance needs. The distribution and seasonal patterns of the four indices can be seen in Figure 2. We formulate the intraday price index IDx as:\n\n$ID_x = \\frac{\\sum_{s\\in \\{b,o\\}} \\sum_{t\\in [t_1, t_2]} P_t^{(s)}V_t^{(s)}}{\\sum_{s\\in \\{b,o\\}} \\sum_{t\\in [t_1, t_2]} V_t^{(s)}}$ (1)\n\nwhere s indicates the market side, with s \u2208 {b, o} representing the bid (b) and offer (0) sides. Here, $P_t^{(s)}$ and $V_t^{(s)}$ denote the price and volume, respectively. t\u2081 represents the prediction time (at which we predict future prices), while t2 denotes the delivery start time of electricity, introduced in Section 3.1.\n\nMoreover, relationship between t\u2081 and t2 is given by:\n\nt1 = t2 \u2212 \u0394 (2)\n\nwhere \u0394 = 60 \u00d7 x min, with x = 1 for ID\u2081 and x = 3 for ID3."}, {"title": "4. OrderFusion Network", "content": ""}, {"title": "4.1. Encoding", "content": "The OrderFusion encoding method extracts features from the filtered orderbook and forms a 2.5D representation. In detail, given a prediction time t\u2081 and a side s, we extract four features from a set of price and volume data {$P_t^{(s)}, V_t^{(s)}$} within the time interval [t\u2081 \u2013 \u2207, t\u2081):\n\nMinimum price: $P_{min}^{(\u25bd)}$\nMaximum price: $P_{max}^{(\u25bd)}$\nVWAP: $VWAP^{(\u25bd)}_t$\nTotal traded volume: $V_{sum}^{(\u25bd)}$\n\nwhere represents look-back window length. Specifically, we consider six window lengths:\n\n\u2207 \u2208 {1 min, 5 min, 15 min, 60 min, 180 min, full} (3)\n\nwhere full denotes the complete history from market opening (15:00 on the previous day) to t\u2081.\n\nThis results in two separate feature maps (2.5D encoding) M(b),M(0) \u2208 Rd\u00d7f for bid and offer, respectively. Here, d = |\u2207| = 6 represents the number of look-back window lengths, while f = 4 corresponds to the number of extracted features per window, shown in Figure 1."}, {"title": "4.2. Backbone", "content": "We introduce a tailored jump cross-attention backbone that takes the 2.5D encoding obtained via OrderFusion as input to model interdependencies between bids and offers. Specifically, the jump cross-attention backbone consists of several 1D-convolutional layers and cross-attention layers.\n\n1D-Convolution We denote $Z_k^{(s)}$ as the input representation, where $Z_k^{(s)}$ can be either the bid representation (s = b) or the offer representation (s = o), and k represents the kth layer:\n\n$Z_k^{(s)} = \\begin{cases}M^{(s)}, & \\text{if } k = 0\\\\A_{k-1}^{(s)}, & \\text{otherwise}\\end{cases}$ (4)\n\nwhere $A_k^{(s)}$ denotes the output from the cross-attention layer, which will be introduced shortly.\n\nWe employ a filter size of F, a kernel size of 1, a stride of 1, and no pooling. The transformation is expressed as:\n\n$C_{k+1}^{(s)} = Conv1D(Z_k^{(s)})$ (5)\n\nwhere $C_{k+1}^{(s)} \u2208 R^{d\u00d7F}$ is the transformed bid or offer feature representations through convolution.\n\nCross-Attention The cross-attention mechanism takes the bid and offer representations learned from 1D-conv layers as inputs. The first input (either bid or ask) is used to project the query, while the second input (another side) is utilized to project the key and value, respectively. When k is even, the second input is \"jumped\" to the k \u2013 1th 1D-conv layer, as illustrated in Figure 3. Precisely:\n\n$A_k^{(s)} = \\begin{cases}CrossAttention(C_{k-1}^{(s)}, C_{k-1}^{(\\overline{s})}), & \\text{if } k \\text{ is even,}\\\\CrossAttention(C_{k-1}^{(\\overline{s})}, C_{k-1}^{(s)}), & \\text{otherwise.}\\end{cases}$ (6)\n\nwhere s denotes the opposite side. For example, if s = b, then s = o, and vice versa.\n\nIn detail, the CrossAttention operation is based on the multi-head attention (MHA) mechanism. Given input representations $C^{(s)} \u2208 R^{d\u00d7F}$ and $C^{(\\overline{s})} \u2208 R^{d\u00d7F}$, the query, key, and value matrices for each attention head are computed as:\n\n$Q^{(s)[h]} = C^{(s)}W_Q^{(s)[h]},$ (7)\n$K^{(\\overline{s})[h]} = C^{(\\overline{s})}W_K^{(\\overline{s})[h]},$ (8)\n$V^{(\\overline{s})[h]} = C^{(\\overline{s})}W_V^{(\\overline{s})[h]}$ (9)\n\nwhere $W_Q^{(s)[h]}, W_K^{(\\overline{s})[h]}, W_V^{(\\overline{s})[h]} \u2208 R^{F\u00d7d_h}$ are trainable projection matrices for the h-th attention head, and $d_h = F/H$ is the head dimension for H attention heads. The scaled dot-product attention is applied:\n\n$A^{(s)[h]} = softmax(\\frac{Q^{(s)[h]}K^{(\\overline{s})T[h]}}{\\sqrt{d_h}})$ (10)\n\nThe outputs of all attention heads are concatenated and linearly projected:\n\n$A^{(s)} = (A^{(s)[1]} \\| A^{(s)[2]} \\| ... \\| A^{(s)[H]})W^O,$ (11)\n\nwhere WO \u2208 RF\u00d7F is the projection matrix, and $||$ denotes concatenation.\n\nThe output A(s) has a special interpretation: it represents the augmented attention feature for side s, contextualized by information from the opposite side s. The formulation has intuition from game theory, where buyers adjust their bids based on information from the seller's side, and vice versa. By incorporating cross-side dependencies, this mechanism allows the model to capture strategic interactions between bid and offer representations. Moreover, the equation introduced in (6) allows the model to not only extract interdependencies from the current layer but also focus on the previous layer, enabling deeper feature interactions.\n\nThe final feature vector U \u2208 $R^{2dF}$ is formed by concatenating flattened representations of outputs from jump cross-attention blocks, where 2dF derives from two d\u00d7 F matrices."}, {"title": "4.3. Head", "content": "We design a hierarchical multi-quantile head that estimates a set of quantiles (Q5, Q25, Q45, Q50, Q55, Q75, Q95) hierarchically and overcomes the quantile crossing issue. The structure learns the median quantile (\u03c4 = 0.5) from the shared representation U \u2208 R2dF with one dense layer, shown in Figure 4. For quantiles above the median (\u03c4 > 0.5), the residuals are predicted using the same shared representation with other dense layers, denoted as r\u03c4, which are enforced to be non-negative using a non-negative function g(x), e.g., ReLU. The upper quantile predictions (\u0177\u03c4') are then computed iteratively by adding the non-negative residuals to the prediction of the preceding quantile (\u03c4):\n\n$\u0177_{\u03c4'} = \u0177_\u03c4 + g(r_\u03c4),$ (12)\n\nwhere \u03c4 is the nearest smaller quantile.\n\nFor quantiles below the median (\u03c4 < 0.5), the residuals r\u03c4' are similarly enforced to be non-negative. The lower quantile predictions (\u0177\u03c4') are computed iteratively by subtracting the non-negative residuals from the prediction of the preceding quantile (\u03c4):\n\n$\u0177_{\u03c4'} = \u0177_\u03c4 - g(r_{\u03c4'}),$ (13)\n\nwhere \u03c4 is the nearest larger quantile."}, {"title": "4.4. Loss", "content": "Average quantile loss (AQL) is employed to estimate conditional quantiles of the target distribution. For a given quantile level \u03c4\u2208 (0, 1), the quantile loss L\u03c4 is defined as:\n\n$L_\u03c4(y_i, \u0177_{\u03c4,i}) = \\begin{cases}\u03c4 \\cdot (y_i - \u0177_{\u03c4,i}), & \\text{if } y_i \u2265 \u0177_{\u03c4,i},\\\\(1 - \u03c4) \\cdot (\u0177_{\u03c4,i} - y_i), & \\text{if } y_i < \u0177_{\u03c4,i},\\end{cases}$ (14)\n\nwhere yi is the true value and \u0177\u03c4 is the predicted quantile for the i-th sample. This loss penalizes over-predictions and under-predictions differently depending on the quantile level \u03c4. When predicting upper quantiles (\u03c4 > 0.5), higher penalties are applied to under-predictions, whereas for lower quantiles (\u03c4 < 0.5), over-predictions incur higher penalties.\n\nSince our model employs a multi-task learning framework, the AQL is computed as the mean quantile loss across all samples and quantiles:\n\n$AQL = \\frac{1}{N|\u03a9|} \\sum_{\u03c4\u2208\u03a9} \\sum_{i=1}^N L_\u03c4(y_i, \u0177_{\u03c4,i}),$ (15)\n\nwhere \u03a9 represents the set of quantiles being predicted, and N is the total number of samples. Lower AQL values indicate better overall performance in quantile prediction."}, {"title": "4.5. Other Details", "content": "We use the Adam optimizer (Kingma & Ba, 2015), with an initial learning rate of 3 \u00d7 10\u22124, which decays exponentially at a rate of 0.7 every 10 epochs. The number of training epochs is set to 50, from which we select the best model with the lowest validation loss. The batch size is configured as 2048 to maximize the usage efficiency of A100 GPU. The activation function employed in the backbone is Swish (Ramachandran et al., 2017)."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Data Splitting", "content": "The orderbook data are split into training, validation, and testing. The training period spans from January 2022 to December 2023, the validation period covers January 2024 to June 2024, and the testing period is set from July 2024 to December 2024."}, {"title": "5.2. Rolling-Window Approach", "content": "Predictions are made with a rolling window approach tailored to the granularity of the target price indices. For 15-minute price indices (15-min ID3 and 15-min ID1), predictions are generated every 15 minutes. For 60-minute price indices (60-min ID3 and 60-min ID\u2081), predictions are generated on an hourly basis. If the prediction target is ID3, the prediction is made 3 hours in advance, while for ID1, predictions are made 1 hour before the delivery time."}, {"title": "5.3. Benchmarks", "content": "Encoding Methods We compare 1D, 2D, 3D, and our proposed 2.5D encoding method (OrderFusion) with different backbones. The 1D encoding flattens feature representations and applies a Multi-Layer Perceptron (MLP) as the backbone. The 2D encoding adopts a time-series format, where Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) layers process sequential data. The 3D encoding converts the data into a 3D pseudo-image representation and utilizes a 2D convolutional backbone. The proposed OrderFusion method transforms the orderbook into a 2.5D encoding and a tailored jump cross-attention backbone is used.\n\nFor a fair comparison, hyperparameters such as optimizer, learning rate, training epochs, batch size, and activation function are set the same for all benchmarks, as described in Section 4.5. Furthermore, the total number of parameters for each model is controlled to remain identical. Additional details of the benchmarks are provided in Appendix A.1."}, {"title": "5.4. Evaluation Metrics", "content": "Probabilistic Prediction We evaluate the testing performance of probabilistic prediction using AQL, described in Section 4.4, and the average quantile crossing rate (AQCR), shown below:\n\nAQCR is utilized to quantify the frequency of quantile crossing violations. The quantile crossing indicator for a quantile pair (\u03c4l, \u03c4u) with \u03c4l < \u03c4u is:\n\n$C_{\u03c4l,\u03c4u}(\u0177_{l,i}, \u0177_{u,i}) = I(\u0177_{l,i} > \u0177_{u,i}),$ (16)\n\nwhere I(\u00b7) is an indicator function that returns 1 if the condition inside is true and 0 otherwise.\n\nWe aggregate the crossing indicators to compute the AQCR across N samples as:\n\n$AQCR = \\frac{1}{N} \\sum_{i=1}^N C_{\u03c4l,\u03c4u}(\u0177_{l,i}, \u0177_{u,i}),$ (17)\n\nSmaller AQCR values indicate fewer quantile crossing violations, reflecting more reliable quantile predictions.\n\nPointwise Prediction We use the root mean squared error (RMSE), mean absolute error (MAE), and R2 as evaluation metrics for the pointwise prediction of the median quantile. The details can be found in Appendix A.2."}, {"title": "5.5. Results", "content": "The results presented in Table 1 demonstrate the experimental performance of our proposed methods compared to the benchmarks. Generally, the prediction losses for ID\u2081 are higher than those for ID3, as ID1 represents the last-minute imbalance needs, making it more volatile. Furthermore, losses for 15-minute price indices are notably higher than those for 60-minute indices, highlighting the increased volatility of 15-minute prices.\n\nParameter Efficiency Figure 5 illustrates the parameter scaling law of five models. With the same number of parameters (104, 105, 106), our approach consistently achieves superior performance. Furthermore, increasing the parameter count from 105 to 106 results in only marginal improvements, indicating that our model requires fewer parameters to reach performance saturation. Table 1 provides a detailed comparison across various metrics, with all models constrained to 105 parameters. The best results are consistently achieved with our proposed method. Especially, our method surpasses GRU by 50.3% for 15min-ID3 and LSTM by 31.3% for 15min-ID\u2081 in AQL. These results indicate that LSTM and GRU suffer from underfitting and require more parameters to achieve proper performance, and our model is particularly parameter-efficient for volatile 15-min indices.\n\nReliability Observed from Table 1, the AQCR of our proposed method is consistently zero, indicating no quantile crossing. This result is expected, as our design strictly enforces monotonicity between quantiles. In contrast, the 1D encoding with MLP performs the worst, with an average AQCR of 61.1% across four indices, leading to unreliable forecasts. The high AQCR from MLP further highlights the importance of proper data encoding."}, {"title": "5.6. Ablation Studies", "content": "Cross-Attention We implement and compare three backbone architectures: (1) a backbone without cross-attention, (2) a standard cross-attention backbone, and (3) our proposed jump cross-attention backbone. Three models employ the same hierarchical head anchored at Q50, with 105 parameters. A comparison of these architectures is shown in Figure 3. Observed from Table 2, our tailored jump cross-attention outperforms the other two backbones across most metrics. Additionally, compared to the baseline model without cross-attention, the results highlight that introducing cross-attention improves 20.85% AQL on average in forecasting volatile 15-min prices."}, {"title": "6. Conclusion", "content": "In conclusion, our proposed OrderFusion framework, combined with a jump cross-attention backbone, enables parameter-efficient learning, highlighting the importance of modeling interdependencies between bids and offers through variants of cross-attention. The designed hierarchical multi-quantile head anchored at Q50 predicts multiple quantiles simultaneously while addressing the quantile crossing issue and mitigating the risk of error accumulation for volatile price indices. Experimental results and ablation studies demonstrate the efficiency and reliability of our approach. This work lays the foundation for future advancements in probabilistic modeling within the energy domain, particularly for high-frequency, volatile CID markets."}, {"title": "Impact Statement", "content": "Our proposed encoding method is applicable to all European CID markets, as the orderbook data from EPEX Spot follows a uniform structure. The designed hierarchical multi-quantile head can be seamlessly integrated with various backbones for diverse probabilistic prediction tasks while overcoming quantile crossing issues. By providing a parameter-efficient and reliable prediction framework, our model contributes to enhanced uncertainty management and supports a smoother transition toward renewable energy integration."}, {"title": "Data availability", "content": "Data cannot be shared due to commercial restrictions."}, {"title": "Code availability", "content": "Codes will be made publicly available."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Benchmark Models", "content": "MLP Multilayer Perceptrons (MLPs) (Rumelhart et al., 1986) are effective for tabular data and simple classification or regression tasks, learning nonlinear relationships through fully connected layers. While they excel in static pattern recognition, they struggle with sequential dependencies and require regularization to prevent overfitting. In our setup, we use 4 dense layers and adjust the number of neurons to match the total parameter count of other models, keeping the remaining hyperparameters identical to those in Section 4.5.\n\nLSTM Long Short-Term Memory (LSTM) networks (Hochreiter, 1997) are well-suited for sequential data, capturing long-term dependencies through memory cells and gating mechanisms. They mitigate vanishing gradients but have high computational costs. In our experiments, we use 4 LSTM layers, adjusting the number of hidden units to control the total parameter count, with other hyperparameters kept consistent as described in Section 4.5.\n\nGRU Gated Recurrent Units (GRUs) (Cho et al., 2014) offer a more computationally efficient alternative to LSTMs by simplifying the gating mechanism. They balance short- and long-term dependencies but may underperform in highly complex sequences. Our implementation includes 4 GRU layers, tuning the number of units to control the total parameter count while maintaining consistent hyperparameter settings from Section 4.5.\n\nCNN Convolutional Neural Networks (CNNs) (LeCun et al., 1998) are designed for spatial and temporal pattern extraction using learnable kernels. While 2D CNNs dominate image processing, 1D CNNs efficiently capture local temporal dependencies in time series. However, they lack inherent long-term sequence modeling. We employ 4 CNN layers, adjusting the number of filters to match the total parameter count, with other hyperparameters aligned with those in Section 4.5."}, {"title": "A.2. Pointwise Metrics", "content": "RMSE The Root Mean Squared Error (RMSE) evaluates the accuracy of pointwise predictions by penalizing larger errors more heavily than smaller ones. It is particularly sensitive to outliers and provides an overall measure of prediction quality. RMSE is calculated as:\n\n$RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2},$ (18)\n\nwhere yi represents the true value, \u0177i is the predicted value, and N is the total number of samples.\n\nMAE The Mean Absolute Error (MAE) measures the average magnitude of prediction errors, treating all deviations equally regardless of their direction. Unlike RMSE, MAE is more robust to outliers, making it a reliable metric for assessing average prediction accuracy. It is computed as:\n\n$MAE = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|,$ (19)"}, {"title": "R2", "content": "The Coefficient of Determination (R2) quantifies the proportion of variance in the target variable that is explained by the predictions. A value of R2 = 1 indicates perfect predictions, whereas R2 \u2248 0 suggests that the model performs no better than predicting the mean of the true values. It is defined as:\n\n$R^2 = 1- \\frac{\\sum_{i=1}^N (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^N (y_i - \\bar{y})^2}$ (20)\n\nwhere y is the mean of the true values yi, and the numerator and denominator represent the residual sum of squares and the total sum of squares, respectively."}, {"title": "Algorithm 2 Matching Rule", "content": "Input: Bids ($P^{(b)}$, $V^{(b)}$), Offers ($P^{(o)}$, $V^{(o)}$)\nif $P^{(b)} > P^{(o)}$ then\nif $V^{(b)} = V^{(o)}$ then\nFull execution: Match full volumes and remove both orders from the orderbook.\nelse\nPartial execution: Match partial volumes and update the remaining order.\nend if\nelse\nNo execution: Leave both orders in the orderbook.\nend if"}]}