{"title": "Pretrained Reversible Generation as Unsupervised Visual Representation Learning", "authors": ["Rongkun Xue", "Jinouwen Zhang", "Yazhe Niu", "Dazhong Shen", "Bingqi Ma", "Yu Liu", "Jing Yang"], "abstract": "Recent generative models based on score matching and flow matching have significantly advanced generation tasks, but their potential in discriminative tasks remains underexplored. Previous approaches, such as generative classifiers, have not fully leveraged the capabilities of these models for discriminative tasks due to their intricate designs. We propose Pretrained Reversible Generation (PRG), which extracts unsupervised representations by reversing the generative process of a pretrained continuous flow model. PRG effectively reuses unsupervised generative models, leveraging their high capacity to serve as robust and generalizable feature extractors for downstream tasks. Our method consistently outperforms prior approaches across multiple benchmarks, achieving state-of-the-art performance among generative model-based methods, including 78% top-1 accuracy on ImageNet. Extensive ablation studies further validate the effectiveness of our approach.", "sections": [{"title": "1. Introduction", "content": "Generative models formulated as continuous-time stochastic differential equations, including diffusion and flow models [2, 36, 38, 51], have shown remarkable proficiency in multi-modal content generation tasks [27, 44, 53], as well as in scientific modeling [1]. By effectively learning high-dimensional distributions, these models can generate new, high-quality samples that resemble the original data.\nWhile generative tasks necessitate models to reconstruct the entire data distribution, another category, known as discriminative tasks (e.g., image classification), involve learning representations that capture the essential underlying data structure and output discrete or continuous predictions based on learned representations. Previous works have attempted to enhance representation learning through various methods, with particular attention on unsupervised learning recently. These methods endeavor to construct generic proxy tasks that effectively acquire and leverage data for pretraining, thereby providing robust representations for different downstream tasks. [4, 9, 12, 15, 52, 60].\nDue to the extraordinary performance of diffusion mod-"}, {"title": "2. Preliminaries", "content": "We introduce the generative models and mathematical notations used in this work, focusing on training and inference within diffusion and flow models."}, {"title": "2.1. Continuous-time Stochastic Flow Models", "content": "We consider two continuous-time stochastic flow generative models: diffusion models and flow models. Let t represent time, with the interval [1,0] for the generation process and [0, 1] for the reverse of generation process. We denote the data and its transformations as xt, where xo is the original data from the dataset and x\u2081 is the terminal state, achieved by reversing the generative process."}, {"title": "2.1.1. Diffusion Model", "content": "A diffusion model reverses a forward diffusion process governed by the following SDE:\n$$dx = f(t, xt)dt + g(t)dwt,$$\nwhere f (t, xt) is the drift term, g(t) is the diffusion coefficient, and wt is the Wiener process.\nFor simplicity, we use an SDE with a linear drift:\n$$dx = f(t)xtdt + g(t)dwt.$$\nThe transition distribution of x \u2208 Rd from time 0 to t is given by:\n$$p(xt|xo) ~ N(xt|atxo, \u03c3\u00b2I),$$\nwhere at is a scaling factor and \u03c3\u00b2 is the variance.\nThe diffusion process is predefined, such as the Variance-Preserving SDE (VP-SDE) [57] or the Generalized VP-SDE (GVP) [2]. Full definitions are provided in the Appendix A. The marginal distribution p(xt) of the diffusion process is fixed if the data distribution p(xo) is stationary. Using the Fokker-Planck equation, an equivalent ODE can be derived with the same marginal distribution, known as the Probability Flow ODE [57]:\n$$\\frac{dxt}{dt} = v(xt) = f(t)xt - \\frac{1}{2}g\u00b2(t)\u2207_{xt} log p(xt).$$\nIn practice, the velocity model v(xt) or score function \u2207_{x_t} log p(x_t) in Eq. (4) is parameterized as v\u03b8 or s\u03b8 using a neural network, such as in Diffusion Transformers [42, 50]."}, {"title": "2.1.2. Flow Model", "content": "A flow model defines a deterministic transformation from xo to x1 via a parameterized velocity function v(xt), governed by the ODE: $\\frac{dx_t}{dt} = v(x_t)$. The marginal probability distribution p(xt) evolves through the vector field v(xt), following the continuity equation:\n$$\\frac{\u2202p(xt)}{\u2202t} + \u2207_x\u00b7 (p(xt)v(xt)) = 0,$$\nwhich transports p(xo) to p(x1) along the flow path.\nFor the Independent Conditional Flow Matching (ICFM) models [59] considered in this paper, the flow path is predefined as $p(x_t|x_0, x_1) = N(x_t|tx_1 + (1 - t)x_0, \u03c3\u00b2I)$, with the velocity function $v(x_t|x_0, x_1) = x_1 - x_0$. We also studied Optimal Transport Conditional Flow Matching (OTCFM) models [59]. The marginal distribution p(xt) remains stationary if both p(x0) and p(x1) are stationary."}, {"title": "2.2. Generative Model Training and Inference", "content": "Both the diffusion and flow models can be parameterized by v\u03b8(xt) and pretrained using the Conditional Flow Matching loss, denoted LFM [35, 59]. In the vanilla case, the weighting factor AFM equals 1. For diffusion models, score matching is also applicable, with Asm(t) = g\u00b2(t) in the vanilla case.\n$$LFM = \\frac{1}{2}Ep(xt) [XFM(t)||v\u03b8(xt) \u2013 v(xt)||\u00b2] dt,$$\n$$LSM = \\frac{1}{2}Ep(xt) [ASM(t)||s\u03b8(xt) \u2013 \u2207_{xt} log p(xt)||\u00b2] dt.$$"}, {"title": "3. Related Work", "content": "Generative versus Discriminative Classifiers Generative classifiers is first proposed in [7, 8, 13, 33, 49, 74] model the conditional distribution p(x|y) and use Bayes' rule to compute the probability $p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$. These classifiers are typically trained by maximizing the conditional log-likelihood: $max_\u03b8 E_{(x,y)~p(x,y)} log p(x|y)$.\nWhen employing conditional diffusion models, p(x|y) can be computed with the instantaneous change-of-variables formula [10, 21] or approximated via ELBO [26, 56]. Duvenaud et al. [17] advocate an energy-based model to enhance calibration, robustness, and out-of-distribution detection. Yang et al. [68] propose HybViT, which combines diffusion models and discriminative classifiers to predict p(y|x). Zimmermann et al. [74] introduce score-based generative classifiers but find them vulnerable to adversarial attacks. Clark and Jaini [13] and Li et al. [33] explore zero-shot generative classifiers using pretrained models like Stable Diffusion [53] to denoise images based on text labels.\nIn contrast, our approach fine-tunes a classifier on top of the generative model, making it a discriminative classifier that directly estimates p(y|x). Discriminative classifiers are often preferred for classification tasks [47, 61]. Fetaya et al. [18] suggest a trade-off between likelihood-based modeling and classification accuracy, implying that improving generation may hinder classification for generative classifier. However, our two-stage training scheme shows that enhancing the generative model in the pretraining stage can improve discriminative performance during the finetuning stage. This suggests a complementary relationship between likelihood-based training and classification accuracy, indicating that discriminative models can benefit from generative pretraining. Thus, we demonstrate a viable approach to leveraging generative models for discriminative tasks.\nRepresentation Learning via Denoising Autoencoders Generative models have been widely explored for visual representation learning [20, 28, 60]. Denoising autoencoders (DAEs) [5, 19, 63, 64] learn robust representations by reconstructing corrupted input data, which can be interpreted through a manifold learning perspective [62]. Our method extends Vincent et al. [64] by leveraging reversible flow-based generative models like diffusion models.\nDiffusion models [26, 56, 58] have shown promise in generation, yet their potential for feature extraction is underexplored. Chen et al. [9] pretrain a Transformer for autoregressive pixel prediction, demonstrating competitive results in representation learning. Their two-stage ap-"}, {"title": "4. Method", "content": "To address the issues discussed in Section 3, We propose an approach follows the conventional two-stage training scheme commonly used in generative modeling applications. In the first stage, we pretrain diffusion or flow models in an unsupervised manner. In the second stage, we fine-tune the models for discriminative tasks by conducting inference in the reverse direction of the generative process."}, {"title": "4.1. Pretraining Mechanism", "content": "In the traditional autoencoder paradigm, representation learning aims to learn a network that maps the input data X into a useful representation Z for downstream tasks. We parameterize the encoder p\u03b8(z|x) with parameters \u03b8. A good representation should retain sufficient information about the input X. In information-theoretic terms [34, 64], this corresponds to maximizing the mutual information I(X, Z) between the input random variable X and its representation Z. Following the reasoning of Vincent et al. [64], we show that pretraining diffusion or flow models maximizes the mutual information. Specifically, we aim to find the optimal \u03b8* that maximizes I(X, Z):\n$$\u03b8^* = arg \\max_\u03b8 I(X, Z)$$\n$$ = arg \\max_\u03b8[H(X) \u2013 H(X|Z)]$$\n$$ = arg \\max_\u03b8[-H(X|Z)]$$\n$$ = arg \\max_\u03b8 E_{p(z,x)} [log p(x|z)].$$\nSuppose we have a decoder \u03b8', which approximates P\u03b8'(x|z) to recover data x from the latent variable z. By"}, {"title": "4.2. Finetuning by Reversing Generation", "content": "Due to the invertibility of the generative process (Eq. (4)), the latent variable x1 corresponding to a data point xo can be obtained by reversing the generative process. We denote xt = F\u03b8(xo) as the extracted features for downstream discriminative tasks, such as classification or regression.\nThis paper focuses on supervised learning for downstream tasks. Suppose we have a labeled dataset Dfinetune = {(xi, yi)}N=1, where xi is a data point and yi is its label, potentially differing from the pretraining dataset Dpretrain. We introduce a classifier p\u03c6(y | z), parameterized by \u03c6, trained by minimizing the cross-entropy loss:\n$$LCE = \\sum_{i=1}^{N} log p\u03c6 (yi | F\u03b8(xi)).$$\nFinetuning Strategy Generative features may not be optimal for discriminative tasks, as reconstruction requires more details than classification. Hence, fine-tuning is necessary to adapt the model for downstream tasks.\nA key question is whether the flow model should also be fine-tuned alongside the classifier. Due to the flow model's large capacity, reusing the pretrained model seems preferable. Initially, we explored freezing the flow model while finetuning only the classifier. However, this approach underperformed, indicating that fine-tuning both the flow model and the classifier is necessary. Otherwise, a large classifier with extra parameters would be required, reducing efficiency. To fine-tune the entire model, we leverage neural ODEs [10] for efficient gradient computation, optimizing both \u03b8 and \u03c6 via backpropagation.\nWe observe that the generative features are often redundant for discriminative tasks and are pruned during fine-tuning, as we will empirically demonstrate in the experiments section. Additionally, we assess the robustness and generalization of these features against out-of-distribution samples through further experiments."}, {"title": "4.3. Advantages", "content": "Compared to previous approaches, our method offers several key advantages:\nModel-Agnostic Flexibility Our approach is agnostic to the choice of neural network architecture for the generative model\u2014it can be a U-Net, Transformer, or any other model. The latent variable Z remains stationary with respect to the data X when using an ODE solver due to the static nature of continuous-time stochastic flow models, as observed by Song et al. [57]. This allows different architectures to encode the data while yielding the same latent variable Z. In contrast, methods like DDAE [66] are model-dependent, with the latent variable Z tied to specific network architectures and derived from intermediate activations, making them less flexible and non-stationary.\nInfinite-Layer Expressiveness Our method leverages the infinite-layer structure of continuous-time stochastic flow models [10], providing high expressiveness with relatively small parameter sizes. This structure underpins the success of modern generative models like Stable Diffusion 3 [53] and DALL-E 3 [6]. By using a reversible process, our method allows for the simultaneous training of discriminative and generative models, offering state-of-the-art performance with this enhanced capacity.\nRobustness and Generalizability Features extracted by reversing the generative process exhibit stability along the reverse trajectory, enabling the use of features from any point t \u2208 [0, 1] for downstream tasks, not just the latent variable x1. These features are robust to different discretization schemes (e.g., Euler, Runge-Kutta) and various time steps (ranging from 103 to 101). Additionally, our method generalizes well across datasets, meaning that a powerful pretrained model can be fine-tuned further for improved performance on new tasks.\nIn summary, our approach bridges the gap between generative and discriminative models, offering a simple yet effective way to leverage pretrained generative models for unsupervised representation learning, with the ability to fine-tune for downstream supervised tasks."}, {"title": "5. Experiments", "content": "We conducted experiments to evaluate our method across three key aspects. First, we analyzed the effectiveness of pretraining and finetuning strategies to identify critical components of our approach. Second, we benchmarked our method against state-of-the-art generative and classical classification methods on CIFAR-10 [31], Tiny-ImageNet [32], and ImageNet [54]. Finally, we performed ablation studies to assess the impact of different design choices\u2014such as architecture and optimization techniques\u2014on downstream classification performance."}, {"title": "5.1. Settings", "content": "For we use three diffusion and flow models, GVP, ICFM, and OTCFM. For pretraining, we followed the protocol from [48], using the Adam optimizer [29] with a fixed learning rate of 1 \u00d7 10-4 for 1,200 epochs on the respective training sets. For downstream image classification, we adopted the configuration from [39], using AdamW for 200 epochs with a cosine decay learning rate schedule and a 5-epoch linear warm-up. The training used a batch size of 128 and an initial learning rate of 0.001. To ensure a controlled setting, we excluded advanced regularization techniques like mixup [72] and cutmix [70], relying on simple data augmentation. Further details, including hyperparameters and computation efficiency, are provided in Appendices B.1 and B.2."}, {"title": "5.2. Verification Analysis", "content": "We aim to address the following four questions."}, {"title": "5.2.1. Better Pretraining Lead to Better Finetuning?", "content": "To explore whether better pretraining improves finetuning, we compared the classification accuracies of models finetuned from different pretraining epochs on CIFAR-10 (Fig. 2). We also computed the mutual information during pretraining, which increases monotonically, as shown in Fig. 3, and analyzed the relationship between accuracy and mutual information during finetuning in Fig. 4.\nModels without pretraining achieved approximately 73.5% accuracy. In contrast, the more pretraining a model received, the higher its mutual information and classification accuracy, indicating that stronger generative capability leads to better finetuning performance. During finetuning, we observed a decline in I(x0, x1) + H(x1) \u2013 H(xo) for both the training and validation sets, suggesting that the model filters out unnecessary features to improve downstream classification, which can also be observed in Fig. 5 as some visual representations are kept while others are discarded. In summary, sufficient pretraining is crucial for optimal performance in inverse generative classification."}, {"title": "5.2.2. What is a Reasonable Finetuning Strategy?", "content": "We first examine whether a frozen flow model provides meaningful features. For this, we use features xt along the sampling trajectory from xo to x1 under two conditions: (1) freezing the generative model's parameters (light lines) and (2) end-to-end training of both the generative model and the classification head (dark lines). As shown in Fig. 6, the performance gap is substantial\u201447.10% on CIFAR-10 and 58.04% on Tiny-ImageNet\u2014demonstrating the critical importance of updating the generative model's parameters during finetuning.\nAdditionally, on CIFAR-10, initiating finetuning from later stages of the sampling trajectory (x1/4 to x1) results in progressively better performance. On TinyImageNet, optimal results are achieved when finetuning starts from x1/2 to x1. This difference likely stems from the complexity of the datasets: more complex datasets require stronger feature extraction, necessitating longer trajectory lengths for effective finetuning. Further experiments exploring optimal finetuning strategies are detailed in the ablation studies."}, {"title": "5.2.3. Does the Model Serve as a Continuous Feature Extractor?", "content": "Residual networks, such as ResNet-50 [22], achieve feature extraction by constructing discrete-time methods through a series of composite transformations: ht+1 = ht+f(ht, \u03b8t). In contrast, we construct the feature extractor using the ODE specified by the neural network, namely $\\frac{dh(t)}{dt}$ = f(h(t), t, \u03b8). Tabs. 1 and 2 demonstrate that although we trained using a discrete sample length of tspan = 20, we can evaluate the model using any reasonable inference steps. Furthermore, feature extraction is not limited to a specific training point (such as the midpoint); extracting features within its vicinity (within 20%) still achieves high performance on downstream tasks. These findings validate our method can serve as a continuous feature extractor."}, {"title": "5.3. Main Results", "content": "To comprehensively evaluate our method, we conducted experiments on three image classification datasets, as detailed in Tabs. 3 to 5. Our methods achieved accuracies of 97.59%, 71.12%, and 78.1% on CIFAR-10, Tiny-ImageNet, and ImageNet, respectively, surpassing all existing generative approaches, as well as supervised methods like WideResNet-28[71]. However, these results still trail behind recent transformer-based supervised learning architectures like SwinTransformer [39]. We hypothesize that integrating transformer architectures and leveraging VAEs to handle high input resolutions could further enhance the performance. This avenue will be explored in future work."}, {"title": "5.3.1. Performance on Image Classification", "content": "To comprehensively evaluate our method, we conducted experiments on three image classification datasets, as detailed in Tabs. 3 to 5. Our methods achieved accuracies of 97.59%, 71.12%, and 78.1% on CIFAR-10, Tiny-ImageNet, and ImageNet, respectively, surpassing all existing generative approaches, as well as supervised methods like WideResNet-28[71]. However, these results still trail behind recent transformer-based supervised learning architectures like SwinTransformer [39]. We hypothesize that integrating transformer architectures and leveraging VAEs to handle high input resolutions could further enhance the performance. This avenue will be explored in future work."}, {"title": "5.3.2. Out-of-Distribution Robustness", "content": "To address the degradation in out-of-distribution (OOD) due to common image corruptions or adversarial perturbations [74], data augmentations and adversarial training are typically employed. However, recent studies [7, 33, 74] have indicated that generative-based classifiers, without requiring additional data, often exhibit superior OOD robustness. Thus, we evaluated our approach on CIFAR-10-C and Tiny-ImageNet-C [24]. As illustrated in Tab. 6 and Appendix B.3, our method showcases remarkable robustness on these two datasets with only simple data augmentation."}, {"title": "5.3.3. Transferring Features", "content": "A main goal of the pretraining/fine-tuning paradigm is to learn transferrable features. We hypothesize that advancements like powerful pretrained models in the generative model community can contribute to our method. To validate this, we conducted experiments by transferring the pretrained SiT-XL [42] model to two other datasets. Since SiT provides only class-conditional checkpoints, we adopted an unconditional approach by setting the label parameter to null. Specifically, we fine-tuned the model for 28 epochs on the CIFAR-10 dataset and for 45 epochs on the Tiny-ImageNet dataset. As demonstrated in Tab. 7, our method shows superior transfer learning performance. The results suggest that our algorithm benefits from larger datasets and the latent space of generative models."}, {"title": "5.4. Ablation Studies", "content": "In this section, we performed three ablation studies to further investigate the applicability of our method. More additional experiments are detailed in the Appendix C."}, {"title": "5.4.1. Generative Model Type", "content": "We explored various generative model types, particularly focusing on path selection. To assess the impact of these paths on our method, we conducted experiments with three model types on the CIFAR-10 and Tiny-ImageNet datasets. Following [38], we quantified the straightness of various continuously differentiable trajectories. Interestingly, while all three model variants achieved comparable performance"}, {"title": "5.4.2. ODE Solver Type", "content": "During fine-tuning, we evaluated different ODE solvers for the reverse process: Euler (first-order), Midpoint (second-order via midpoint evaluations), RK4 (fourth-order Runge-Kutta), and Dopri5 (adaptive step sizes with a fifth-order method). Tab. 9 compares their performance on the Tiny-ImageNet dataset. The results show no significant performance differences, underscoring the method's consistent effectiveness across various solvers."}, {"title": "5.4.3. Scaling Up Network Parameters", "content": "We investigated the impact of scaling up U-Net models of varying network parameter sizes [48]. Table 10 provides a comparative analysis following fine-tuning on the CIFAR-10 and Tiny-ImageNet datasets. The findings suggest that increasing the model size results in a modest enhancement of final classification accuracy."}, {"title": "6. Conclusion and Limitation", "content": "In this paper, we introduce a new perspective on using pretrained reversible generative models as unsupervised visual learners. We systematically investigate the necessary designs of the two-stage pretraining and fine-tuning paradigm. Theoretical analysis supports the efficacy of the first-stage generative pretraining, while empirical verifications provide insights for designing the fine-tuning pipeline. Leveraging these techniques and findings, our method achieves state-of-the-art performance in image classification tasks using generative models. Additional experiments, such as those on OOD problems and transfer learning scenarios, further validate the robustness and other advantages of our approach. However, there remains room for improvement, including the integration"}, {"title": "Supplementary Material", "content": "To enhance the reproducibility of results across various multi-stage and multi-GPU experiments, we calculate the learning rate using Eq. (B1).\n$LR = LR_{base} \\times \\frac{num processes \\times Batch Size}{512}$ \n$Warmup LR = Warmup LR_{base} \\times \\frac{num processes \\times Batch Size}{512}$\n$Min LR = Min_{base} \\times \\frac{num processes \\times Batch Size}{512}$"}, {"title": "B.2. Evaluation of Training Efficiency", "content": "Most research experiments, including the main experiments and ablation studies, are completed within 2 hours to 3 days. To demonstrate the training efficiency, Tab. B3 report the per-epoch runtime for each dataset used in our experiments."}, {"title": "B.3. Details of Out-of-Distribution Experiments", "content": "There is no direct correspondence between the test images of Tiny ImageNet and Tiny ImageNet-C, and the images in Tiny ImageNet-C do not overlap with the training images of Tiny ImageNet. We report the comparative results on Tiny ImageNet-C in Tab. B4."}, {"title": "B.4. Reverse Generation Process", "content": "Fig. B1 illustrates the reverse generation process from xo to x1 after finetuning. Furthermore, Figs. B2 and B3 present the reverse generation results on the TinyImageNet dataset after pretraining and finetuning, respectively. Finally, Fig. B4 demonstrates the reverse process before and after applying fog corruption to the images."}]}