{"title": "SUFFIXDECODING: A MODEL-FREE APPROACH TO SPEEDING UP LARGE LANGUAGE MODEL INFERENCE", "authors": ["Gabriele Oliaro", "Zhihao Jia", "Daniel Campos", "Aurick Qiao"], "abstract": "We present SuffixDecoding, a novel model-free approach to accelerating large language model (LLM) inference through speculative decoding. Unlike existing methods that rely on draft models or specialized decoding heads, SuffixDecoding leverages suffix trees built from previously generated outputs to efficiently predict candidate token sequences. Our approach enables flexible tree-structured speculation without the overhead of maintaining and orchestrating additional models. SuffixDecoding builds and dynamically updates suffix trees to capture patterns in the generated text, using them to construct speculation trees through a principled scoring mechanism based on empirical token frequencies. SuffixDecoding requires only CPU memory which is plentiful and underutilized on typical LLM serving nodes. We demonstrate that SuffixDecoding achieves competitive speedups compared to model-based approaches across diverse workloads including open-domain chat, code generation, and text-to-SQL tasks. For open-ended chat and code generation tasks, SuffixDecoding achieves up to 1.4\u00d7 higher output throughput than SpecInfer and up to 1.1\u00d7 lower time-per-token (TPOT) latency. For a proprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to 2.9\u00d7 higher output throughput and 3\u00d7 lower latency than speculative decoding. Our evaluation shows that SuffixDecoding maintains high acceptance rates even with small reference corpora of 256 examples, while continuing to improve performance as more historical outputs are incorporated.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have rapidly been established as a foundational component of today's consumer and enterprise applications. Although their uses have been diverse, the need for fast generation of tokens has remained ever-present and becom- ing a main bottleneck in emerging applications. For example, the recent trend of inference-time scaling utilizes much longer outputs to perform search and other complex algorithms to solve challenging problems (Zhong et al., 2024). Multi-agent and pipelined LLM systems (Wang et al., 2024a;b; Santhanam et al., 2024) can greatly enhance the accuracy and reliability of LLM applications, but often suffer from long response times due to multiple processing stages waiting for each others' outputs.\nAmong techniques aimed at accelerating LLM output speed, speculative decoding (Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2024; Cai et al., 2024; Lin et al., 2024; Zhang et al., 2024) has emerged as a promising candidate. While an LLM can generate only one token at a time, it can verify multiple to- kens during a single forward pass. Leveraging this phenomenon, speculative decoding methods use small draft models and/or additional decoding heads on the LLM to efficiently predict the next tokens in the response. The original LLM is then used to verify these predictions in parallel in a single forward pass.\nExisting model-based speculative decoding methods can reduce the end-to-end inference latency of LLMs but suffer from several key limitations. First, model-based speculative decoding relies heavily on the size and quality of the draft model. A draft model of the appropriate size and quality may not always exist, which necessitates training one from scratch or finetuning one from an existing draft model. Second, orchestrating draft models and LLMs co-located on GPUs presents complications and can result in inefficiencies. For example, the draft model occupies additional GPU memory, which conflicts with the key-value cache of the LLM. In addition, the transition between the draft model and the LLM must be carefully managed at the kernel level (Chen et al., 2024; Li et al., 2024).\nInstead of employing a separate draft model, recent work has introduced additional decoding heads to the LLM to perform speculative decoding by leveraging the final hidden represen- tations computed by the LLM (Cai et al., 2024). However, these works face similar challenges as draft-model-based methods, since the additional heads for speculative decoding need to be finetuned for each LLM and require additional GPU memory to store model parameters. For example, for"}, {"title": "2 BACKGROUND", "content": "LLM Inference. Autoregressive LLM inference involves two main stages: processing prompt tokens (prefill), fol- lowed by generating new tokens (decode). The prompt Xprompt = (x1,x2,\u2026\u2026,xm) provides the model's initial context and can be processed in parallel. The model then samples new tokens sequentially, with each token xt>m conditioned on the prompt and all previously generated tokens:\nxt+1=Sample(xt|x1,...,t).\nFor example, in greedy sampling, the model generates a probability distribution over possible next tokens, and selects the highest-probability token. This process is repeated until a stopping condition is met, such as an end-of-text token being emitted or a maximum length limit is reached.\nSince each token relies on those generated before it, generation is inherently sequential, leading to a number of forward passes through the LLM that scales with the number of generated tokens and slowing inference on longer sequences. Furthermore, this sequential nature can lead to inefficient use of hardware accelerators, such as GPUs or TPUs, which are optimized for parallel processing.\nSpeculative Decoding. Speculative decoding (Leviathan et al., 2023) speeds up inference by using a small speculative model (SSM) to quickly generate multiple potential future tokens, which can be verified by the original LLM with a single forward pass. It consists of two key steps: speculation by the SSM followed by verification by the LLM.\n1. Speculation: The SSM generates a sequence of specula- tive tokens Xspec = (Xt+1,...,xt+n) based on the sequence prefix x<t. Even if xspec is generated sequentially, this process is faster with the SSM than with the LLM, enabling quicker production of candidate tokens.\n2. Verification: The LLM then verifies the candidate tokens in parallel, which are each accepted if they align with the outputs of the LLM. If verification fails partway, any tokens up to the first discrepancy are accepted, while the remaining tokens are discarded. Generation is then advanced by the number of accepted tokens.\nBy allowing the SSM to handle speculative token generation and verifying these tokens in parallel, speculative decoding"}, {"title": "3 SUFFIXDECODING", "content": "Similar to LLMA (Yang et al., 2023) and Prompt Lookup Decoding (Saxena, 2023), SuffixDecoding is model-free and sources candidate sequences from a reference corpus. In particular, the reference corpus of SuffixDecoding consists of (1) the outputs of previous inference requests, and (2) the prompt and output of the current ongoing inference request.\nUnlike previous work, which only considered small reference texts, such as a handful of snippets retrieved by a RAG system (Lewis et al., 2020) or just the current prompt, SuffixDecoding is designed to source candidate sequences from much larger scale corpus, such as hundreds or thousands of previously generated outputs. At this scale, SuffixDecoding is able to leverage frequency statistics in the reference corpus to select likely candidates in a more principled fashion, which is discussed later in this section.\nTo support fast production of candidate sequences, SuffixDe- coding builds a suffix tree (Weiner, 1973) over its reference corpus. The root node of the tree represents the beginning of a suffix of any document in the reference corpus, where a doc- ument is an output of a previous inference or the prompt and output of the current ongoing inference. The path from the root to each node represents a subsequence that appears in the refer- ence corpus, and each child of each node represents a specific token which is a possible continuation from its parent node.\nSuffixDecoding leverages suffix trees to perform fast pattern matching and find possible continuations from subsequences in the reference corpus. Suppose the prompt plus generated tokens of an ongoing inference is X1:t. Consider a suffix Xt-p+1:t of length p, which we will refer to as the pattern sequence. We walk the suffix tree starting from the root node, and at each step taking the child that corresponds to token Xt-p+i. If no such child exists, then the pattern is not found in the reference corpus and SuffixDecoding reverts to standard non-speculative decoding. Otherwise, after p steps, we arrive a node whose descending paths are the possible continuations according to the pattern and reference corpus."}, {"title": "Suffix Tree Construction.", "content": "Building the suffix tree on the reference corpus and updating it as part of an online inference service involves two parts. First, the previous inference outputs can be added to the tree in a single offline processing step (e.g. from historical logs), or online during inference serving after each inference request completes. Second, the current ongoing prompt and generated tokens must be added online as new requests are received and as each new token is generated.\nIn reality, we found it more convenient to maintain two different suffix trees: a global tree for the previously generated outputs, and a separate per-request tree for the current ongoing inference request. This circumvents the complexities and overheads due to synchronizing the suffix tree updates from multiple concurrent requests. The global tree can be constructed offline in O(n) time, while the per-request tree can be efficiently constructed and updated online using Ukkonen's Algorithm (Ukkonen, 1995).\nAlthough suffix trees are memory-efficient at O(n) space, the global tree can still become large when there are many previous outputs. However, they only require CPU memory (instead of the scarcer GPU memory), which is typically plentiful and under-utilized in most LLM serving scenarios. For example, AWS p5.48xlarge instances are often used for LLM serving and have 2TB of main memory, which is easily enough to support a suffix tree over millions of historical outputs and billions of tokens."}, {"title": "Speculation Tree Expansion.", "content": "Given a pattern sequence Xt-p+1:t of an ongoing inference X1:t, SuffixDecoding can quickly find a node Np in the global or per-request suffix tree whose descending paths are the possible continuations of the pattern sequence. To select a smaller more likely sub-tree that is of a more practical size for speculative verification, we start with the single node Np and grow a sub-tree greedily by expanding one child of a leaf node at a time.\nIn particular, we define:\nC(N)=COUNT(N)\u03a3MECHILDREN(PARENT(N))COUNT(M)\nand\nD(N) ={D(PARENT(N))\u00d7C(N),if N\u2260Npotherwise"}, {"title": "Speculation Tree Scoring.", "content": "So far, we have discussed how to obtain a speculation tree given a suffix tree and a pattern length p. However, SuffixDecoding maintains two suffix trees, the global suffix tree and the per-request suffix tree, each with many choices for p. To obtain just a single speculation tree, we build speculation trees for both the global suffix tree and the per-request suffix tree, and for a range of values of p. Then, a single speculation tree is selected according to a scoring function:\nSCORE(Tspec)=\u2211N\u2208TspecD(N).\nIntuitively, if D(N) estimates the probability that node N in a speculation tree Tspec would be accepted, then SCORE(Tspec) estimates the expected number of accepted tokens. SuffixDecoding then selects the Tspec with the highest SCORE as the final speculation tree to be verified.\nThe end-to-end candidate generation from speculation tree expansion to scoring is described in Alg. 1."}, {"title": "Implementation", "content": "SuffixDecoding is an end-to-end inference framework implemented on top of FlexFlow Serve (FlexFlow Team, 2024) with 3.6K lines of C++ code. It natively support any model from Huggingface (HuggingFace, 2024). We used FlashInfer (Ye et al., 2024) for the attention kernels, CUBLAS (NVIDIA, a) and CUDNN (NVIDIA, c) libraries, to- gether with custom CUDA kernels to optimize the performance on GPU. We use CUDA Graph (NVIDIA, b) to eliminate any avoidable kernel launch overheads. SuffixDecoding uses NCCL for inter-GPU communication and supports all three of data, tensor, and pipeline parallelism."}, {"title": "4 EVALUATION", "content": "We conducted a comprehensive evaluation of SuffixDecoding to assess its performance from both the perspective of speculation efficacy and overall system performance. In our end-to-end experiments, we compared SuffixDecoding with SpecInfer, which is an existing state-of-the-art framework for LLM serving that utilizes tree-based speculative decoding. We performed detailed ablation studies to highlight the contribution of each component of our technique to the overall performance. This section is organized as follows: Section 4.1 outlines the experimental setup, Section 4.2 presents the results of the experiments and provides an in-depth discussion, and Section 4.3 details the ablation studies."}, {"title": "4.1 Evaluation Methodology", "content": "Workloads. We chose four instruction datasets to evaluate SuffixDecoding and its baselines with. These datasets were selected because they cover a broad spectrum from diverse open-domain use cases (e.g. public-facing chatbots) to task-specific use cases found more often within enterprise use cases (e.g. text-to-SQL).\n1. WildChat (Zhao et al., 2024). We use instructions from the WildChat dataset, which consists of real-world interactions between users and the ChatGPT service. WildChat represents the most diverse and open-domain dataset used in our evaluations.\n2. Magicoder (Wei et al., 2023). Specifically, we use instructions from the Magicoder-Evol-Instruct-110K dataset, which consists of code-related questions and instructions generated via Self-Instruct (Chaudhary, 2023; Wang et al., 2023) and further augmented for difficulty and diversity (Luo et al., 2023) by GPT-4.\nBaselines. We chose two baseline decoding methods to compare SuffixDecoding with:\n1. Standard decoding. This is the regular autoregressive LLM decoding procedure that generates a single output token per forward pass of the model.\n2. SpecInfer. A state-of-the-art model-based speculative decoding method that leverages speculation trees to obtain faster speedups over standard speculative decoding, built on FlexFlow (Miao et al., 2024).\nHardware and model configuration. We conducted our experiments on a single p5.48xlarge AWS instance equipped with 8\u00d7 NVIDIA H100 80G GPUs with 900GB/s inter-GPU interconnect, a 192-core AMD Milan CPU and 2TB of main memory. We picked meta-llama/Meta-Llama-3-70B-Instruct (Dubey et al., 2024), a popular open-source model for the LLM, and meta-llama/Meta- Llama-3-8B-Instruct (Dubey et al., 2024) as the draft model for the speculative inference baseline. We served the LLM using tensor parallelism (with TP degree = 8), evenly distributing the weights and KV cache across the GPUs.\nSimulated Verifier. In addition to our main evaluation using real hardware, we also leverage a simulated verifier to conduct ablation studies in Sec. 4.3. Given a prompt X1:n and example ground-truth response Yn+1:t, we can accurately simulate speculative verification for greedy sampling by verifying that speculated token Xn+i=Yn+i.\nUnless otherwise specified, we use outputs generated from meta-llama/Meta-Llama-3-70B-Instruct as the ground truth responses. A fraction of each dataset is used for building the global suffix tree, while a disjoint fraction is used to test the speculations made by SuffixDecoding.\nSuffixDecoding with the simulated verifier can accurately reproduce the acceptance rates and reductions in decoding steps observed in our real hardware experiments, which we leverage for our ablation studies at lower computation cost."}, {"title": "4.2 End-To-End Experiments", "content": "Our end-to-end experimental results are summarized in Table 1 and illustrated in Figures 4 and 5. The goal of our system is to achieve low latency to ensure user responsiveness in the LLM inference system, while also maximizing throughput to efficiently utilize cluster resources and serve as many requests as possible.\nThe total latency of serving a request is determined by two factors: the prefilling latency (also known as time to first token or TTFT) and the per-token decoding latency (also referred to as time per token or TPOT). Our focus is on the TPOT latency since the SuffixDecoding technique does not impact the TTFT latency, which represents only a small percentage of the overall end-to-end latency. Additionally, we report the average LLM decoding step time in comparison to the baseline methods. It is important to note that, unlike the incremental decoding baseline, speculative decoding techniques decode more than one token per step.\nOn the AgenticSQL dataset, SuffixDecoding achieves up to 2.9x higher output throughput than SpecInfer and up to 3\u00d7 lower time-per-token (TPOT) latency. On open-ended chat and code generation tasks, SuffixDecoding achieves up to 1.4\u00d7 higher output throughput than SpecInfer and up to 1.1\u00d7 lower time-per-token (TPOT) latency.\nTable 2 presents our evaluation of the effectiveness of Suf- fixDecoding in comparison to draft-model-based speculative decoding. We measure the average number of speculated tokens accepted by the language model (LLM) at each verifi- cation step, which indicates the maximum potential end-to-end"}, {"title": "4.3 Ablation Experiments", "content": "In this section, we share ablation studies that reveal the impact of several design decisions in SuffixDecoding. The studies are conducted using the simulated verifier described in Sec. 4.1.\nGlobal vs per-request suffix trees. We study the impact of the two suffix trees used by SuffixDecoding: the global suffix tree containing a (potentially) large number of previous outputs, and the per-request suffix tree containing the prompt and generation of the current ongoing request. To do so, we ran the tasks in AgentSQL using SuffixDecoding (1) with the global suffix tree only, (2) with the per-request suffix tree only,"}, {"title": "Effect of input distribution shift.", "content": "In real-world LLM serving, the input characteristics of inference requests may change over time, and may be out-of-distribution from the output examples that SuffixDecoding was trained on. To evaluate this scenario, we run SuffixDecoding trained on WildChat outputs, and begin to send it inputs from SpiderSQL, which represents a very sudden distribution shift.\nFig. 9 shows the results. SuffixDecoding starts from having 4,000 output examples from WildChat, and begins to receive SpiderSQL inference requests. Without any adaptation, Suf- fixDecoding still achieves 1.5\u00d7 speedup and 8% acceptance rate, but is far from the 2.6\u00d7 speedup and 20% acceptance rate it would achieve if it were trained on 500 examples from SpiderSQL instead.\nHowever, after processing each SpiderSQL inference request, SuffixDecoding can insert its ground truth output into its global suffix tree, which means it can adapt in an online fashion to the new input distribution. As Fig. 9 shows, the performance of SuffixDecoding improves with the number of SpiderSQL inference requests processed. Perhaps surprisingly, after ob- serving 500 SpiderSQL and adapting online, SuffixDecoding's performance is almost indistinguishable to its performance if it were trained offline on the 500 SpiderSQL examples alone. This suggests that SuffixDecoding is able to adapt to input"}, {"title": "5 RELATED WORKS", "content": "Recent works have explored approaches to accelerate large language model inference, which we review in this section.\nSpeculative decoding. Speculative decoding has recently ex- ploded in popularity thanks to its ability to significantly improve LLM inference latency without compromising the quality of the generated text. Medusa (Cai et al., 2024) and SpecInfer (Miao et al., 2024) are second-generation speculative framework which first introduced tree-based speculation. Medusa uses mul- tiple decoding heads for parallel candidate generation, while SpecInfer uses multiple draft model or speculative sampling to generate a tree of candidate tokens. Both frameworks then em- ploy a tree attention mechanism to verify the tokens in parallel. Although they enable substantial speedups, they require a draft model or additional finetuned attention heads for speculation. In contrast, SuffixDecoding provides a model-free speculative framework with competitive performance, which is easy to deploy and can be updated at very low cost at each iteration.\nModel-free speculative decoding based on Reference text. LLMA (Yang et al., 2023) and Prompt Lookup Decoding (PLD) (Saxena, 2023) are model-free speculative decoding techniques that rely on small text references to generate candi- date text continuations based on pattern matching. LLMA uses a cache-assisted approach, enabling reference-based predictions for high-frequency patterns, while PLD utilizes a simpler token-matching mechanism based on the input prompt. While effective in reducing latency, both LLMA and PLD are limited by small and static reference bases, which may constrain adaptability across broader tasks. SuffixDecoding overcomes these limitations by dynamically constructing suffix trees that combine suffixes not only from an initial reference dataset, but also from previous responses and the current prompt. Additionally, SuffixDecoding advocates for scaling the reference text to thousands and (perhaps) millions of examples sequences, which necessitates the design and implementation of efficient pattern matching and speculation tree construction techniques.\nOnline Speculative Decoding. Online Speculative Decod- ing (Liu et al., 2024) introduces real-time draft model updates based on observed query distributions. This approach mitigates the drawbacks of static draft models but introduces significant overhead and operational challenges to continuously finetune the draft model while performing inference. In contrast, SuffixDecoding relies on a model-free suffix tree structure, which can be updated in real-time without needing to run additional GPU computations.\nSystems for multi-agent LLM pipelines. ALTO (San- thanam et al., 2024) also explores the problem of accelerating LLM inference for emerging multi-agent LLM pipeline appli- cations. Their solution leverages scheduling-based techniques between different pipeline stages: aggregation-aware routing, which allows ALTO to load balance across logically indepen- dent partial outputs, and distributed prompt-aware scheduling, which dynamically load balance across a heterogeneous set of prompts. In contrast, SuffixDecoding accelerates each LLM stage independently using model-free speculative decoding. Although SuffixDecoding works particularly well for the types of narrow and structured-outputs tasks commonly found in multi-LLM pipelines, it is also general and can accelerate open-domain chat workloads. Additionally, SuffixDecoding and ALTO explore orthogonal design spaces, and may be utilized in combination with each other."}, {"title": "6 CONCLUSION", "content": "In this paper, we presented SuffixDecoding, which leverages suffix trees built from previously generated outputs to efficiently predict candidate token sequences. We demonstrated that SuffixDecoding achieves competitive speedups compared to model-based approaches across diverse workloads including open-domain chat, code generation, and text-to-SQL tasks. We also showed that SuffixDecoding is particularly suited for emerging multi-LLM applications using AgenticSQL, a proprietary text-to-SQL pipeline. SuffixDecoding reveals a novel direction for improving speculative decoding through scaling a reference corpus and we hope that future work will continue to explore along this direction."}]}