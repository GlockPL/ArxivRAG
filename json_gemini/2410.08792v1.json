{"title": "VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model", "authors": ["Beichen Wang", "Juexiao Zhang", "Shuwen Dong", "Irving Fang", "Chen Feng"], "abstract": "Vision Language Models (VLMs) have recently been adopted in robotics for their capability in common sense reasoning and generalizability. Existing work has applied VLMs to generate task and motion planning from natural language instructions and simulate training data for robot learning. In this work, we explore using VLM to interpret human demonstration videos and generate robot task planning. Our method integrates keyframe selection, visual perception, and VLM reasoning into a pipeline. We named it SeeDo because it enables the VLM to \"see\" human demonstrations and explain the corresponding plans to the robot for it to \"do\". To validate our approach, we collected a set of long-horizon human videos demonstrating pick-and-place tasks in three diverse categories and designed a set of metrics to comprehensively benchmark SeeDo against several baselines, including state-of-the-art video-input VLMs. The experiments demonstrate SeeDo's superior performance. We further deployed the generated task plans in both a simulation environment and on a real robot arm. The code, demos, and data can be found at ai4ce.github.io/SeeDo.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Vision Language Models (VLMs) or Multimodal Large Language Models (MLLMs) have been drawing significant interest in recent AI research. They have also been embraced by the robotics research community for their rich semantic knowledge and common-sense reasoning capabilities. Some research utilizes VLMs as an interface for parsing human language instructions to generate task plans [1, 2, 3, 4]. Some leverage VLMs to assist in motion and trajectory planning [5, 6, 7, 8], or incorporate VLMS in data generation systems to simulate real-world data for training robot policies [9, 10, 11]. These works typically use text, images, or both as inputs to the VLMs. In this work, we explore a different type of input modality: videos. We pose the question-can robots, with the help of VLMs, learn task plans directly from human demonstration videos?\nWhile language instructions are efficient in many application scenarios, some tasks are inherently difficult to precisely express in plain language. Videos provide a more straightforward representation and are particularly suitable for long-horizon tasks with multiple steps or those that involve temporal or spatial dependencies. Moreover, videos are a natural learning medium for humans, who frequently acquire skills and parse task steps by observing demonstrations."}, {"title": "II. RELATED WORKS", "content": "VLMs for task planning. Large language models have shown amazing emergent abilities and generalizability to new tasks since the release of GPT-3 [14], and have been used in generating task plans [15, 16, 17, 18]. To generate valid task planning, this body of work usually requires carefully curated instructions to prompt the VLMs [19] and often relies on access to advanced close-sourced VLMs such as GPT-4 [20] for good performance [18]. The task planning ability of VLMs has also been explored for robotics control [5, 7, 15, 21].\nOne line of work generates robot executable codes as the medium of task plans [1, 22]. In these works, the VLMS take human language instructions and sometimes images as inputs, reason the instructions, and output the task plans as function-calling codes, referred to as language model programs (LMPs), which call the wrapped API of robotics action primitives. [23] builds a video-language planning pipeline by using an embodied VLM [24] to break long-horizon language instructions into steps, prompt a video model to generate video rollouts of the future, and assess current progress. They all rely on human language instructions to specify the task while our SeeDo explores directly using real-world human demonstration videos as the task specification.\nVLMs for robot. With the help of strong common sense reasoning and rich semantic knowledge exhibited by the pretrained large VLMs [25], a line of literature [3, 22, 26] has demonstrated that robots can take non-expert human language instructions more effectively than before [27]. Robotics researchers also go beyond using pretrained VLMs and leverage robotics data to train vision-language-action models (VLAs) catered for direct robotics control [6, 24, 28]. By harnessing the rich common sense knowledge compressed in the large VLMs, useful data generation pipelines can be built to generate simulation data for training intelligent robots [9, 11]. In this work, we explore leveraging VLMs as a tutor to the robots, interpreting human videos into task plans that can be further executed via LMPs.fRobot learning from human videos. Demonstration videos provide a direct supervision signal for robot learning. Many existing works leverage teleoperated robotics videos to train robot policies via imitation learning [12, 29, 30]. For its massive amount compared to simulation and teleoperation, human video data has always held the promise of scaling up robot learning [31]. There has been a long interest in robot learning from human demonstrations from the early stage of robot learning [32, 33] to the recent deep imitation learning [34, 35, 36, 37]. Research in one-shot imitation learning [38, 39, 40, 41, 42] aims to learn robot policies from a single demonstration, but they are limited in generalizability and confined to short-horizon tasks. In general, having robots directly learn from human demonstration videos is still challenging due to the big domain gap between humans and robots. It also struggles with changes in the environment and object appearances between videos and deployments and often requires collecting many more demonstrations when the tasks become long-horizon [43]. To mitigate these challenges, [37] proposes to break down the imitation learning process into training a latent planner to predict hand trajectory from human play data and training the planner-guided imitation policies on robotics data. While sharing a similar motivation of decomposing planning and action learning, this work focuses on the planning part and takes a different approach. SeeDo leverages pretrained VLMs to directly interpret human demonstration videos into textual plans and the generated plans are processed into LMPs to call any action primitives whether they are trained-based, control-based, or pre-programmed.\nVLMs for video input. Recent VLMs have been trained to accommodate multiple modalities of inputs including videos [13, 44, 45, 46, 47, 48] and can do video analysis tasks like question answering (QA) and video captioning. Video analysis [49] has also been included as part of the benchmark set to evaluate VLMs. Our approach resembles a video QA or captioning setup but is grounded specifically in robotics scenarios. We also included several top-performing commercial and open-source VLMs on the VideoMME [49] benchmark as our baselines."}, {"title": "III. METHOD", "content": "In preliminary studies, we find that simply instructing plain VLMs with human demonstration videos yields poor results. They constantly struggle with processing and retaining information from all frames, often confusing the temporal order and spatial relationships of objects. Therefore, inspired by [4] we took a system-first approach and designed a pipeline centered around the VLM to enhance its capabilities. As a result, SeeDo comprises three modules:"}, {"title": "IV. EXPERIMENTS", "content": "We collected human demonstration videos across three diverse long-horizon pick-and-place tasks and designed a set of metrics to evaluate the success rate. We first compare SeeDo to baselines across all three tasks. Then, we present ablation studies to assess the impact of the module design. Finally, we analyzed and discussed the types of errors that occurred with SeeDo and the baselines. The experiments were conducted on a fixed robotic arm platform using a UR10E robot arm. Qualitative results are shown in Figure 4.\nA. Task Design\nWe are particularly interested in the long-horizon daily tasks and construction tasks that can be decomposed into a series of pick-and-place sub-tasks for their clear temporal order and easy demonstration. As illastratued in Figure 3, we collected a set of human demonstration videos covering three diverse categories as the evaluation tasks:\nVegetable Organization Task contains demonstration videos showing humans picking up and dropping different kinds of vegetables into several different containers. There are 6 different vegetables and 4 different containers. The containers include a ceramic bowl, a glass container, a trash bin, and a small pot to best mimic real-life kitchen scenarios. For the vegetables, we use plush toys instead of real ones as in [7] to avoid potential damage in real experiments. In the simulation deployment, we use Pybullet [62] and collected online free .obj models of some objects and utilize a publicly available text-to-3D generation model [63] to generate the others. In total, there are 38 demonstrations.\nGarment Organization Task contains demonstrations of a human organizing their garments into separate boxes. Garments are visually distinct from the vegetables and serve as a complement to the vegetable task in the daily scenario. To make the task interesting and challenging, we chose garments of various types including shirts, shoes, ties, and an umbrella. In total we collected 30 demonstrations.\nWooden Block Stacking Task. We also collected demonstrations of a human playing with wooden blocks to simulate a block-building game-play scenario or a miniature construction setup. The key feature of this category is that the visual appearance of the objects is highly similar, which requires relatively precise reasoning of spatial relations, creating a well-known challenging case for the current vision language models. We show that SeeDo can overcome this challenge with the help of the visual prompts from the Visual Perception module. In total, this task contains 39 demonstrations.\nB. Evaluation Metrics\nPick-and-place tasks are fundamental building blocks of object manipulation. Conventional evaluation metric reports success rate (SR) of each task which could only reflect the completion at the final state of operation. We are particularly interested in how well the robot can follow the demonstration videos step by step. Therefore, we propose our metrics.\nSpecifically, a long-horizon pick-and-place task can be decomposed into a series of single pick-and-place sub-task steps arranged sequentially over time. Each pick-and-place action establishes a relative spatial relation pair between two"}, {"title": "V. CONCLUSION AND LIMITATIONS", "content": "This paper tackles the challenge of extracting robot task plans directly from human demonstration videos using large vision language models (VLMs). We introduce a pipeline, SeeDo, that significantly improves temporal understanding, spatial relationship reasoning, and object differentiation, particularly in cases where objects have similar appearances, outperforming existing video VLMs. Through comprehensive evaluation, SeeDo demonstrates state-of-the-art performance on long-horizon tasks of a series of pick-and-place actions in diverse categories. However, SeeDo still faces many limitations, and we discuss several below.\nLimited action space. The current experiments are limited to pick-and-place actions. Extending SeeDo to action spaces with more complex behavioral logic or a wider variety of behaviors is our next step.\nLimited spatial intelligence. While the visual perception module significantly improves SeeDo's ability to differentiate left and right spatial relations, it still makes mistakes in tasks requiring more precise spatial reasoning. This is particularly evident in wooden block stacking, where spatial relations in multiple directions are critical for success. We call for future VLMs with stronger spatial intelligence.\nUnder-defined spatial positioning. Describing the spatial positions of objects is inherently complex. In this work, SeeDo only describes the relative position as a limited set of high-level relative spatial relation pairs. It relies on calling the action primitive to determine the exact positions, which makes it less competent for tasks requiring precise manipulation. In future work, we will explore extracting more precise spatial positioning from the demonstration videos."}, {"title": "A. Prompt Engineering", "content": "In SeeDo, the Vision Language Model (VLM) is primarily utilized in two key areas: the first is within the Visual Perception Module, where it extracts an object list from the environment and uses it as a prompt for the GroundingDINO to perform object detection; the second is in VLM Reasoning Module, where it generates the corresponding robot action plan based on the keyframes. We chose gpt-4o-2024-08-06 as it was the VLM with state-of-the-art performance by the date of this paper.\n1) Visual Perception Prompts: The following prompt is a prompt used in the Visual Perception Module to obtain the object list from the environment. In cases where objects with similar visual appearances are difficult to distinguish through language (e.g., multiple wooden blocks), the VLM is prompted to repeat such objects in the output according to their occurrence. For example, if there are three wooden blocks, the VLM is instructed to output \"wooden block, wooden block, wooden block.\"\nFor a long-horizon pick-and-place task, the filtered keyframe sequence typically includes three types of keyframes: those showing the pick action, those showing the place action, and some mistakenly selected keyframes. The valid information lies in the pick and place keyframes. After eliminating the misselected keyframes, the remaining pick and place keyframes can be paired in sequence, with each pick keyframe followed by its corresponding place keyframe. Our Chain of Thought (CoT) design is based on this approach. In the CoT process, the first step is to filter the keyframes, removing invalid frames where the hand is not interacting with any object. For valid frames, we first prompt the VLM to understand the pick frame, identifying the object picked in the pick-and-place subtask. Next, we pass both the object picked and the place frame to the VLM, instructing it to select the appropriate reference object. Once the reference object is determined, the VLM is then tasked with outputting the spatial relationship between the object picked and the reference object in the place step."}, {"title": "B. Evaluation Metrics", "content": "We employ the following evaluation metrics to assess the performance of the model:\nTask Success Rate (TSR) The task success rate measures whether the predicted steps exactly follow the steps demonstrated in the video. The computation is defined as:\nAlgorithm 1 TSR: Task Success Rate\nFinal State Success Rate (FSR) The final state success rate evaluates the final states of the objects specified as their relative spatial relations. Specifically, it checks if the predicted plan results in the same final states as the demonstration's. The computation is defined as:\nAlgorithm 2 FSR: Final State Success Rate\nStep Success Rate (SSR) The step success rate measures partial correctness by evaluating whether the temporal order"}, {"title": "C. Real-world experiment", "content": "The real-world experiment is conducted on a Universal Robots' UR10e cobot attached with a Robotiq 2F-85 gripper. We use an Intel RealSense 455 stereo camera to acquire depth information. The camera is mounted on a tripod standing across the robot and hand-eye calibrated in an eye-to-hand setup. Similar to [1, 21], we first use a segmentation model to segment all the objects of interest in the RGB images, and then we query the depth image with the same coordinates to acquire 3D information of the objects. The gripper is hard-coded to move horizontally above the object in 3D space and lower in the z-axis to grasp the object. Because the action primitives are hard-coded, it inevitably resulted in some failure cases due to lack of adaptability. For example, in the wood block stacking example, the dropping distance is often too high and would cause the wood block to bounce upon contact."}]}