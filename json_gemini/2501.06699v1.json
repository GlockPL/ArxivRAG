{"title": "Large Language Models, Knowledge Graphs and Search Engines: A Crossroads for Answering Users' Questions", "authors": ["Aidan Hogan", "Xin Luna Dong", "Denny Vrande\u010di\u0107", "Gerhard Weikum"], "abstract": "Much has been discussed about how Large Language Models, Knowledge Graphs and Search Engines can be combined in a synergistic manner. A dimension largely absent from current academic discourse is the user perspective. In particular, there remain many open questions regarding how best to address the diverse information needs of users, incorporating varying facets and levels of difficulty. This paper introduces a taxonomy of user information needs, which guides us to study the pros, cons and possible synergies of Large Language Models, Knowledge Graphs and Search Engines. From this study, we derive a roadmap for future research.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative Al has led to heated debate on the opportunities, limitations and risks of such technology, and the precedent it sets for the future. These AI models and tools are often powered by Large Language Models (LLMs) with many billions of parameters trained on vast corpora of natural language sourced from the Web. State-of-the-art LLMs are mostly based on the Transformer architecture [44] for neural networks, trained to predict the next word or token of characters for a given input text, which can be an incomplete sentence, a question or a task description, such as asking for travel advice, solving a puzzle, or writing code for a programming exercise. The models operate in an \"auto-regressive\" way, generating entire output sequences token by token. LLMs trained on broad corpora and for a wide spectrum of tasks are sometimes called foundational models [4] as they show proficiency across domains and tasks: from mass-user chatbots to co-pilots for software developers. Deployed LLMs are often invoked with prompt engineering or in-context learning in order to best shape the application inputs towards desired outputs. Notable LLMs that are widely used include the OpenAI family of GPT models, Google's PaLM models, Huawei's PanGu series, Meta's Llama family, and more. LLM-based assistants such as Google Gemini or Bing Co-Pilot are gaining popularity. Notwithstanding these technical achievements and impressive applications, LLMs still have a variety of fundamental shortcomings."}, {"title": "2 SE VS. KG VS. LLM", "content": "In this section we compare the strengths and limitations of SEs, KGs and LLMs. Similar analyses have been presented for KGs and LLMs [31, 32]; here we introduce SEs and a suite of new issues in our comparison. Table 1 provides an overview of the pros and cons of SEs, KGs and LLMs along various dimensions. When discussing the strengths and limitations of LLMs, we consider LLMs in isolation. Combinations of technologies (e.g., RAG) will be discussed later. Correctness refers to the extent to which the information returned is correct. SEs and KGs store explicit representations of their (indexed) contents: full text documents, tables and nodes/edges. LLMs, on the other hand, only capture statistical patterns from the input corpus that can be used to generate text following patterns similar to the input. These patterns are stored in latent form, encoded in the values of billions of model parameters. Thus while imprecise answers generated by SEs and KGs can usually be traced back to issues with the input corpus, LLMs may generate imprecise results, hallucinations, etc., not supported at all by their input data. As a consequence, LLMs can perform poorly regarding topics with few supporting documents (i.e., long-tail topics) [27]."}, {"title": "3 THE PERSPECTIVE OF INFORMATION-SEEKING USERS", "content": "The previous section provides some general insights into the pros and cons of SEs, KGs and LLMs with respect to several key issues. The question we now ask is: how do these factors affect the end user? The answer to this question clearly depends on what information need the user is trying to satisfy. In some cases, the user's need may be satisfied with SEs; in other cases an LLM may suffice; in yet other situations a KG may be useful; finally, there are also cases where none of these technologies currently suffices. Towards a more meaningful answer to this question, we first draft a conceptual list of user needs. Some such categories have been identified, for example in the creation of benchmarks [54], but previous categorizations tend to focus purely on factual questions, whereas our categorization covers a broader range of typical user questions. This list can then be compared against the capabilities (and limitations) of SEs, KG and LLMs."}, {"title": "4 RESEARCH DIRECTIONS", "content": "The discussion so far has revealed that SEs, KGs and LLMs are complementary technologies: where one is weak, often another is strong. This motivates exploring amalgams of these approaches that maximize their constituents' pros while minimizing their cons. Revisiting Table 1, for example, one can ask: what combination of SEs, KGs and LLMs can go beyond the individual technologies in supporting more categories of information needs? The ideal solutions should provide Internet-scale coverage and freshness, give precise answers with user-friendly explanations of provenance, and support the entire spectrum from fact lookups to analytic queries and personalized advice, all with low computational cost. The idea of combining LLMs with other technologies is not new. Most notably, KGs have been leveraged by SEs for entity-centric queries, and recent LLMs are coupled with SE techniques for RAG. This is not a complete picture, though, and still far from the full potential of cross-technology synergies. Figure 3 depicts a Venn diagram that highlights major directions towards potential synergies. The following subsections discuss pairwise combinations first, and then provide ideas on three-way integration."}, {"title": "4.1 Augmenting Language Models", "content": "KG for LLM: Curated Knowledge. Per the old idiom, \"just because it's said doesn't make it true\", much of the text on which LLMs are trained is not factual in nature. KGs as a curated source of structured knowledge can thus enhance LLMs in a variety of ways [31, 53], particularly for addressing complex (e.g., multi-hop or aggregation-based) factual questions. This can involve pre-training and fine-tuning enhancements, or inference-time injection of factual knowledge. For training, the input data can be annotated or otherwise enriched with factual information from the KG (e.g., entity markup in text documents, injecting entity types etc.). Another approach is to train both the LLM parameters and KG embeddings in a coupled manner (see, e.g., [55]). For inference, the input prompts can be augmented by adding background knowledge. For example, when asking the LLM for ACM Fellows at US universities, it would be useful to inject triples from the KG, stating that ACM is an organization in the field of Computer Science and possibly even providing a list of US universities that large KGs should have. Yet another, simple but effective, approach could be to check LLM outputs against KGs as a post-processing step for veracity assessment. Research along all these lines can help to reduce hallucinations and bias in LLM responses \u2013 particularly in the context of long-tail"}, {"title": "4.2 Augmenting Search Engines", "content": "LLM for SE: AI-Assisted Search. LLMs have the capability to enrich SE functionality on both the user-input side and the way results are presented [60]. On the input side, LLMs provide users with natural-language dialogue that humans appreciate, particularly for explanations, planning and advice. Queries are seamlessly derived from the user's utterances, based on the LLM's skills in language generation, such as paraphrasing. This setting also paves the way for conversational IR [56], where users interactively explore certain topics. The processing of search requests can be also supported by LLMs, leveraging their inherent language skills to help find more matches to search queries and compute better relevance scores for individual results. On the output presentation, LLMs can generate more direct answers from top-ranked results or overview-style summaries of the topics that the user wants to explore. These may even come with suggestions to the user on how to proceed further, in a human-friendly conversational manner."}, {"title": "4.3 Augmenting Knowledge Graphs", "content": "LLM for KG: Knowledge Generation. The provocatively titled work \"Language Models as Knowledge Bases?\" [34] initiated a wave of research to investigate if and to what extent an LLM can generate a full-fledged KG, based on advanced prompt engineering and few-shot in-context learning, and sometimes even with supervised fine-tuning. Such techniques typically take as input a subject-predicate pair, such as Venezuela-capital, plus further prompting and context, and generate one or more objects that complete the desired fact. Results show that LLMs can indeed generate large volumes of facts,"}, {"title": "4.4 KG + LLM + SE", "content": "Augmentation \u2192 Ensemble \u2192 Federation \u2192 Amalgamation. To combine all three technologies, we envision research following a natural progression through four phases. The augmentation phase considers a primary technology that is augmented by the other two. These are natural extensions of the research topics already discussed. For example, in the context of SE for KG: Knowledge Refinement, one could pursue the idea of extending this to SE + LLM for KG whereby LLMs are used to help extract knowledge from sources judiciously retrieved by the SE in order to extend or refine the KG. Likewise, LLM for KG: Knowledge Generation could be extended into LLM + SE for KG by using RAG for knowledge generation. These two directions could actually unify the broader theme of knowledge acquisition and generation. Moving away from augmenting a main technology, one can consider an ensemble approach, where KGs, LLMs and SEs are peers in the information infrastructure, and a user query is delegated to the technology best adapted to address the particular type of information need. Such an ensemble would likely have a natural language interface powered by an LLM, but underneath it could call KGs, LLMs or SEs."}, {"title": "5 CONCLUSIONS", "content": "We have discussed Search Engines (SEs), Knowledge Graphs (KGs) and Large Language Models (LLMs) from the perspective of users seeking information. Our analysis reveals that rather than being competitors, the three technologies are indeed complementary. SEs offer the broadest and freshest coverage and can provide precise results with tangible evidence, sometimes even for the long tail. However, SEs by themselves are inherently unable to integrate or synthesize results from multiple sources. KGs can reason over multitudinous facts and synthesize results derived from multiple sources in a precise, transparent and deterministic manner. However, due to their quality standards and curation process, they have generally weaker coverage and freshness, and often lack the nuance and diversity captured by natural language collections. LLMs fall between SEs and KGs in terms of coverage and freshness, and can also synthesize nuanced results from multiple sources. However, they tend to perform poorly on long-tail topics, and are prone to imprecise or invalid outputs, lack of transparency and non-determinism. Regarding users' information needs, KGs excel on complex factual queries, but do not cope well with non-factual categories. SEs provide support for factual and non-factual categories, but are effective only on simple queries and are inconvenient for questions whose answers do not lie in a single document; similar limitations arise also when structured data needs to be aggregated. LLMs also partially cover both factual and non-factual needs, but are prone to hallucinations and bias, have no formal operators for analytical queries, and can be trapped by false premises in questions. We argue herein that research on these technologies would benefit from keeping in mind the full diversity of the information needs of users. Although research on each of the three technologies will continue to be fruitful in the coming years along these lines, we believe that research on their combination and integration - with users in mind - will be particularly fruitful."}, {"title": "A.1 Large Language Models", "content": "LLMs capture contextual probabilities of tokens in the parameters of a large neural network, often following the Transformer architecture [44]. The model parameters are computed by two stages of training: unsupervised pre-training and supervised fine-tuning. LLMs can also benefit from inference-time (i.e., post-training) techniques, most notably, prompt engineering [26] and in-context learning [9]. Pre-training is the backbone of all LLMs. To this end, the models are provided with huge corpora of human-written texts, covering Wikipedia, news articles, social media, book collections, and much more - all at massive scale with trillions of tokens (see, e.g., [10]). The usual training objective is to predict the next token in a text sequence, repeatedly in an auto-regressive manner. As the original text is available, the ground-truth is known and this entire training process is completely unsupervised (or self-supervised, as it is sometimes phrased). Multimodal models additionally leverage huge collections of speech, images and videos, often in combination with transcribed text and user annotations or comments. These are out of scope for this article, though. Fine-tuning adopts a pre-trained LLM as a foundational model and adapts it for a suite of specific tasks (such as question answering, chatbot dialog, summarization etc.) by adapting its internal parameters. Supervised fine-tuning (SFT) provides the model with labeled examples of desired input-output pairs specific to the task. A common SFT technique is to apply instruction fine-tuning [57], whereby the inputs include task-specific natural-language instructions paired with a desired output. Unlike pre-training, this stage critically depends on labeled data, typically with human annotators/contributors in the loop. Another related technique is to apply reinforcement learning from human feedback (RLHF), whereby a set of ranked preferences collected from human users over alternative outputs to the same model input is used to train a separate reward model to predict such preferences; the language model is then fine-tuned by the reward model to better fit its predicted human preferences. A more recent approach, called direct preference optimization (DPO) [35], obviates the need for a reward model, rather embedding a loss function directly into the language model to fine-tune it for human preferences. In summary, an ideal LLM acquires both language skills and background knowledge from pre-training,"}, {"title": "A.2 Search Engines", "content": "Search Engines (SEs) help users to retrieve relevant information from potentially massive corpora, like the Web, including news feeds, social media streams and even structured datasets such as JSON files or CSV tables. SEs are powered by Information Retrieval (IR) techniques [3], classically geared towards matching keywords in documents, but the last decade has brought many innovations [28], improving user experience in finding answers. SE technology comprises the following components [23]: Collection involves the acquisition and continuous updating of a corpus of web-pages over which users can search. This is largely based on crawling the Web, but also involves subscribing to feeds (e.g., news, social media) and importing information from databases (e.g., product catalogs). These sources must be revisited intermittently in order to check for updates, with the frequency dependent on the importance and rate of change of the particular page. Methods for spam control can block or remove malicious contents. The collection process works near-real-time; for example, a new post on a social network will be known to the SE within minutes. Indexing organizes tokens, words and phrases of documents into inverted-index lists: highly optimized data structures that enable efficiently retrieving documents that match a search request. In addition to indexing surface tokens, the SE also prepares for similarity search, aka soft matching, by computing an embedding vector for each document (or part of a document). This is done by running the text through a lightweight LLM such as BERT [30], possibly restricted to popular web sources if cost is of concern. Query understanding is a key component of modern SEs, which boast a sophisticated suite of techniques for interpreting and enriching users' queries [5], ideally inferring the user intent rather than staying at the string-matching level. This includes techniques for auto-completion, suggestions for reformulations and topically related queries (e.g., based on query-and-click logs), contextualization with user history and other situative context (e.g., location, time of day, etc.) [23]. The query and the relevant context can also be fed"}, {"title": "A.3 Knowledge Graphs", "content": "The modern notion of Knowledge Graphs (KGs) arose in the context of improving search engines [14]. In 2012, the Google Knowledge Graph was proposed to boost search by \"things, not strings\" [40]: when matching a string like \"Manuel Blum\u201d against web contents, the SE can recognize that the string likely references a real-world entity. This requires a structured repository of real-world entities, their types (e.g., computer scientist), their attributes (e.g., birth"}]}