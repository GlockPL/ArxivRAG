{"title": "T1-contrast Enhanced MRI Generation from Multi-parametric MRI\nfor Glioma Patients with Latent Tumor Conditioning", "authors": ["Zach Eidex", "Mojtaba Safari", "Richard L.J. Qiu", "David S. Yu", "Hui-Kuo Shu", "Hui Mao", "Xiaofeng Yang"], "abstract": "Objective: Gadolinium-based contrast agents (GBCAs) are commonly used in MRI scans of patients with\ngliomas to enhance brain tumor characterization using T1-weighted (T1W) MRI. However, there is growing\nconcern about GBCA toxicity. This study develops a deep-learning framework to generate T1-postcontrast\n(T1C) from pre-contrast multiparametric MRI.\nApproach: We propose the tumor-aware vision transformer (TA-ViT) model that predicts high-quality T1C\nimages. The predicted tumor region is significantly improved (P < .001) by conditioning the transformer layers\nfrom predicted segmentation maps through adaptive layer norm zero mechanism. The predicted segmentation\nmaps were generated with the multi-parametric residual (MPR) ViT model and transformed into a latent space\nto produce compressed, feature-rich representations. The TA-ViT model was applied to T1w and T2-FLAIR to\npredict T1C MRI images of 501 glioma cases from an open-source dataset. Selected patients were split into\ntraining (N=400), validation (N=50), and test (N=51) sets. Model performance was evaluated with the peak-\nsignal-to-noise ratio (PSNR), normalized cross-correlation (NCC), and normalized mean squared error (NMSE).\nMain Results: Both qualitative and quantitative results demonstrate that the TA-ViT model performs superior\nagainst the benchmark MRP-ViT model. Our method produces synthetic T1C MRI with high soft tissue contrast\nand more accurately reconstructs both the tumor and whole brain volumes. The synthesized T1C images\nachieved remarkable improvements in both tumor and healthy tissue regions compared to the MRP-ViT model.\nFor healthy tissue and tumor regions, the results were as follows: NMSE: 8.53 \u00b1 4.61E-4; PSNR: 31.2\u00b1 2.2;\nNCC: 0.908 \u00b0 .041 and NMSE: 1.22 \u00b1 1.27E-4, PSNR: 41.3 \u00b1 4.7, and NCC: 0.879 \u00b1 0.042, respectively.\nSignificance: The proposed method generates synthetic T1C images that closely resemble real T1C images.\nFuture development and application of this approach may enable contrast-agent-free MRI for brain tumor\npatients, eliminating the risk of GBCA toxicity and simplifying the MRI scan protocol.", "sections": [{"title": "1. INTRODUCTION", "content": "Multi-parametric MRI (mp-MRI) is widely used for diagnosing and treating gliomas due to its excellent soft-\ntissue contrast, which, when combined with molecular and genomic biomarkers\u00b9, enhances the characterization\nof these tumors. Gliomas, classified by the World Health Organization (WHO) into grades I to IV, account for\nover half of malignant central nervous system tumors.\u00b2 Low-grade gliomas (LGGs; WHO grades I and II) are\nless aggressive and present a favorable prognosis over high-grade gliomas (HGGs; WHO grades III and IV).\nWhile non-contrast MRI sequences such as T1-weighted (T1W), T2-weighted (T2W), and T2 fluid-attenuated\ninversion recovery (FLAIR) provide structural information about the tumor volume and reveal structural\nfeatures such as peritumoral edema, necrosis, and mass effect,\u00b3 detailed tissue characterization remains\nchallenging. Enhancement of T1W MRI with gadolinium-based T1-contrast (T1C) improves the delineation\nof tumor boundaries while contrast-enhanced tumor is often indicative of HGGs. Hence, T1C is commonly\nincorporated into the clinical workflow. However, gadolinium-based contrast agents (GBCAs) have been\nshown to be deposited in the brain, raising concerns about their long-term clinical implications.6\nTo address this concern associated with GBCAs and improve the availability of T1C-like images, several deep\nlearning models have been proposed to synthesize T1C by either reducing the dose of GBCAs7, or eliminating\nthe need for contrast entirely.9,10 However, little work has been done to improve the reconstruction of the tumor\nregion specifically for this task. Traditional methods for enhancing tumor region reconstruction, especially for\nsegmentation tasks, involve loss functions that assign a higher penalty to poor performance in the tumor\nregion.11,12 We argue that this approach is suboptimal for image translation because the model only indirectly\nconsiders information about the tumor region.\nTo overcome this limitation, we propose a novel approach that first predicts T1C segmentation maps from T1W\nand T2-FLAIR MRI using the state-of-the-art multi-parametric residual vision transformer (MPR-ViT) model13,\nwhich leverages ViT (Vision Transformer) layers to provide global context and convolutional layers to\nefficiently capture finer details. Then, we train a second MRP-ViT model to generate compressed latent space\nrepresentations. Finally, we introduce the tumor aware (TA) ViT model by modifying the MRP-ViT\narchitecture to enable the powerful attention mechanism of the ViT blocks to focus on reconstructing the tumor\nregion. This is done by conditioning the transformer encoder on the latent segmentation maps through the\nadaptive layer norm (adaLN - zero) mechanism.14\nWe make the following contributions:\n(1) This study is the first to use latent tumor conditioning for medical image translation, inspired by latent\ndiffusion models15,\n(2) We show that segmentation maps can be effectively compressed into a latent space representation,\nreducing computational burden and eliminating superfluous information that may distract the network.\n(3) The adaLN-zero mechanism efficiently guides the ViT blocks to focus on the tumor region.\n(4) We achieve state-of-the-art performance in both single-modal (T1w \u2192 T1C) and multimodal (T1W +\nT2-FLAIR \u2192 T1C) image synthesis with TA-ViT model, noting no significant difference in\nreconstruction quality between the two tasks."}, {"title": "2. METHODS", "content": "2.1 Data Acquisition and Preprocessing"}, {"title": "2.2 Segmentation", "content": "The T1C segmentation maps were predicted from T1W and T2-FLAIR MRI by placing them in separate input\nchannels using the MRP-ViT architecture. While this model was orginally designed for image translation rather\nthan segmentation, no modifications were made to the architecture or loss function as described in the original\npaper. The MRP-ViT model contains three parts: the encoder, the information bottleneck, and the decoder. Both\nthe encoder and decoder are convolution-based while the information bottleneck integrates both ViT and\nconvolutional layers. The ViT blocks employ the flash attention mechanism to improve computational\nefficiency.19 All layers are connected with residual skip connections to encourage the propagation of\ncontextually important features throughout the network 20"}, {"title": "2.3 Segmentation Map Latent Space Representation", "content": "Diffusion models have gained popularity for image translation by introducing noise into input images and then\niteratively removing the noise until the final prediction is made. 21-24 This process typically involves thousands\nof steps and is computationally intensive, so researchers are actively exploring more efficient approaches. One\npromising approach is to perform the diffusion process on compressed latent representations of the images. 15\nBy training a network to learn the identity function, features in the information bottleneck represent compressed\nabstract representations of the features while removing irrelevant details in image space. These feature maps\ncan then be used instead of the initial images in the diffusion process.\nIn this work, the concept of the latent space was applied to reduce the computational burden of the ViT layers,\nas the self-attention mechanism becomes computationally intensive depending on the number of input tokens.\n25 As shown in Figure 1, the dimensionality of segmentation maps is reduced from 120\u00d7120\u00d71 to 30\u00d730\u00d7256,\nwith the first two dimensions representing spatial dimensions and the third representing the number of channels.\nThe MPR-ViT model was trained to learn the identity function from the ground truth segmentation maps. The\nground truth latent representations were used for the training and validation datasets, while the predicted\nsegmentation map latent representations were only used for the testing dataset."}, {"title": "2.4 T1C prediction with TA-VIT", "content": "Synthetic T1C MRI was predicted using the TA-ViT model. Compared to the MRP-ViT model, the primary\nchanges are removing the convolutional layers in the ViT block so that the vision transformer architecture can\nwork directly on the latent space representation of the segmentation map and modifying the transformer encoder\nto accept conditional inputs. Shown in Figure 2C, the segmentation maps along with the feature maps from the\nnetwork are vectorized and given patch and positional embeddings before they are placed in the transformer\nencoder. The network feature maps follow a typical transformer architecture except where they are conditioned\nwith the adaptive layer norm (adaLN).\nThe layer norm, defined in (1) where x is the input feature map, \u00b5 is the mean, \u03c3\u00b2 is the variance, \u025b is a small\nconstant to prevent division by zero, and y and \u1e9e are learnable parameters, is a standard component of the\ntransformer architecture.26 By summing the feature map-derived vector with the latent segmentation map vector\nbefore y and \u1e9e are regressed, the ViT block can be conditioned on the segmentation map or any arbitrary vector.\nThis operation defines the adaLN block, first proposed in Peebles and Xie\u00b94 for use in a transformer based text-\nconditioned diffusion model. In addition, it was found that zero-initializing y accelerates large scale training\ngiving rise to the adaLN-zero block used in this work. Since transformers process 1-dimensional inputs, this\nmechanism could be applied to the latent segmentation maps with minimal modification and was simplified\nbecause the latent segmentation maps and feature maps are both the same dimensions. The adaLN-zero block,\nshown in Figure 2D was applied four times in the transformer encoder, modifying the parameters \u03b3\u03b9, \u03b2\u03b9, \u03b1\u03b9, \u03b32,\n\u1e9e2, and a2 where a is equivalent to the multiplicative \u03b3.\nLayerNorm(x)\n= $\\frac{x-\\mu}{\\sqrt{\\sigma^{2} + \\epsilon}} \\gamma + \\beta$ (1)"}, {"title": "2.5 Encoder and Decoder", "content": "The encoder and decoder each consist of three combined residual blocks. Within each combined residual block\nare three residual blocks, each comprised of a convolutional layer followed by a batch normalization layer and\na Rectified Linear Unit (ReLU) activation function. All convolutional layers use a 3\u00d73 kernel size, except for\nthe initial and final layers which employ a larger 7\u00d77 kernel to broaden the receptive field. Each convolutional\nblock includes a residual skip connection to aid in gradient backpropagation.20 In the encoder, the final\nconvolutional layer of each combined residual block has a stride of 2, halving the dimensionality. Conversely,\nin the decoder, the corresponding combined residual block utilizes a transposed convolution to double the\nfeature map size. Overall, the dimensionality is reduced by a factor of 4, bringing it down to a resolution of\n30\u00d730 before entering the information bottleneck, after which it is restored to the original input size of 120\u00d7120\nwithin the decoder. This design strategy encourages the information bottleneck to focus on coarse details while\nalso reducing the computational load of the ViT blocks. The encoder and decoder are designed symmetrically,\nwith the key difference being that the final convolutional block in the decoder outputs a single channel, whereas\nthe encoder starts with two input channels. Additionally, the network's final layer is followed by a hyperbolic\ntangent activation function to ensure that the output feature map values range between -1 and 1."}, {"title": "2.6 Information Bottleneck", "content": "The information bottleneck is aimed at capturing abstract, global context by using a combination of\nconvolutional blocks and powerful, though computationally intensive, ViT blocks. To address the vanishing\ngradient problem, residual skip connections are employed across both types of blocks, providing an alternate\npath for gradients during backpropagation.\nThe convolutional blocks within the information bottleneck consist of two consecutive 3\u00d73 convolutional layers,\nwhich are connected to the input feature map via a skip connection. Since the ViT layers require a 1-dimensional\ninput, the feature maps and segmentation maps are first flattened and then individually tokenized to generate\npatch and positional embeddings before being fed into the transformer encoder."}, {"title": "2.7 Implementation Details", "content": "The MPR-ViT and TA-ViT models were trained on a consumer-grade NVIDIA RTX 4090 GPU with 24 GB of\nmemory, and additional results were gathered with a cloud-based NVIDIA A10 with 24 GB of memory. The\ndataset was augmented by randomly flipping the images in the coronal plane. An AdamW gradient optimizer\n(learning rate 2 \u00d7 10\u22124, \u03b21 = 0.500, \u03b22 = 0.999, eps = 1 \u00d7 10\u22126) was set to optimize the learnable parameters\nover 251 epochs or when the model no longer reduced the validation loss. The AdamW optimizer was chosen\nto minimize the loss function (L1 loss) for its improved generalization performance over the Adam optimizer\ndue to a decoupling of the weight decay and gradient update.27 32 image slices were used for each batch."}, {"title": "2.8 Validation and Evaluation", "content": "Model performance was assessed using a hold-out test, where 501 patients were randomly divided into training\n(400 patients: 29,005 slices), validation (50 patients: 3,511 slices), and testing (51 patients: 3,590 slices) sets.\nThe segmentation results for the tumor region were assessed using metrics including the dice similarity\ncoefficient (DSC), the Jaccard Index (J), and the root mean squared deviation (RMSD) . Additionally, for the\nT1C image translation task, the evaluation metrics included the normalized mean squared error (NMSE), peak\nsignal-to-noise ratio (PSNR), normalized cross-correlation (NCC), and structural similarity index (SSIM) over\nthe entire 3D volume for both the entire brain and the tumor region. The student's two-sided t-test was employed\nfor statistical comparison, with a significance level set at 0.05. DSC measures the overlap of the ground truth\nvolume (VOLGT) and the predicted volume (VOLPT), while J measures the intersection over the union,\nproviding a more consistent metric for tumors of different sizes. RMSD measures the average magnitude of the\nerrors between predicted and ground truth voxel values.\nMetrics to evaluate the performance of the reconstructed T1C include the NMSE, which measures the voxel-\nwise difference between the synthetic and ground truth volumes such that a value of zero means no difference.28\nPSNR is inversely related to the NMSE, so higher PSNR values correspond to higher similarity to the ground\ntruth volume. Logarithmic scaling was applied to make PSNR values more closely align with human\nperception.29 SSIM considers luminance, contrast, and structural similarity functions to most closely align with\nhuman perception. SSIM values range from -1 to 1 with 1 being perfect correspondence with the ground truth\nvolume. NCC (Normalized Cross-Correlation) is a statistical measure that evaluates the correlation between the\nsynthetic and ground truth volumes. NCC measures similarity between image structures and ranges from -1 to\n1, with 1 indicating perfect correlation with the ground truth volume.30 NMSE, PSNR, and NCC are defined\nbelow where n is the total number of voxels, X\u2081 and Yi are the voxel intensity of the synthetic and ground truth\nvolumes, and MAX\u2081 is the maximum possible voxel value of the ground truth volumes. These metrics were\ncalculated for both the tumor region and whole brain volumes. Metrics for the tumor region were calculated by\nfirst setting all values outside of the tumor region to zero for both the ground truth and predicted volumes using\nthe ground truth segmentation maps as the reference.\nSegmentation Metrics\nDSC = $\\frac{2|VOL_{GT} \\cap VOL_{PT}|}{|VOL_{GT}| + |VOL_{PT}|}$ (2)\nJ= $\\frac{VOL_{GT} \\cap VOL_{PT}}{VOL_{GT} \\cup VOL_{PT}}$ (3)\nRMSD = $\\frac{1}{\\sqrt{n}} \\sqrt{\\sum_{i=1}^{n}(X_{i} - Y_{i})^{2}}$ (4)\nT1C Synthesis Metrics\nNMSE = $\\frac{1}{n} \\sum_{i=1}^{n}(X_{i} - Y_{i})^{2}$ (5)\nPSNR = 10 log$_{10}$($\\frac{MAX_{I}}{\\sqrt{MSE}}$) (6)\nNCC =\n$\\frac{\\sum_{i=1}^{n}(X_{i}-\\mu_{X})(Y_{i}-\\mu_{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_{i}-\\mu_{X})^{2} \\sum_{i=1}^{n}(Y_{i}-\\mu_{Y})^{2}}}$ (7)"}, {"title": "3. RESULTS", "content": "Synthetic T1C MRI generated by the TA-VIT model with both T1W and T2-FLAIR MRI and with T1W only,\nas well as MRP-VIT, were compared against the ground truth T1C MRI volumes. These were evaluated through\nthe NMSE, PSNR, and NCC metrics (Table 1). In addition, the segmentation performance of the MRP-ViT\narchitecture was measured quantitatively (Table 2). Compared with the MRP-ViT architecture, TA-VIT showed\na statistically significant improvement (p-value < .001) across all metrics for both the tumor volumes and the\nentire brain volume, achieving an NMSE: 0.0009 \u00b1 0.0005, PSNR: 31.2 \u00b1 2.2, NCC: 0.908 \u00b1 0.041 for the\nwhole brain. However, there was no statistically significant difference between using multi-parametric inputs\nand using T1W only. Figure 3 similarly shows the distributions of the tumor and whole brain regions for TA-\nViT and MRP-ViT. TA-ViT outperforms MRP-ViT and has fewer very poor reconstructions visualized as a\nsmaller tail for the PSNR and NMSE evaluation metrics.\nExample cases of output images from 4 patients are shown in Figure 2. The TA-ViT model produces visually\nimproved tumor regions even for (a) where the predicted segmentation map deviated significantly from the\nground truth segmentation map. Table 2 shows the performance of the MRP-ViT model for the T1C-based\nsegmentation task using T1W and T2-FLAIR MRI as input channels."}, {"title": "4. DISCUSSION", "content": "In this study, we propose the TA-VIT model for multimodal T1C synthesis which outperforms the state-of-the-\nart MPR-ViT model as measured by the NMSE, PSNR, and NCC and based on qualitative results. By generating\naccurate T1C MRI from mp-MRI, potential toxicity from GBCAs can be prevented and may be a substitute\nwhen T1C MRI is not practical to obtain. The synthetic T1C MRI images generated by the TA-VIT model are,\noverall, highly conformal to the ground truth tumor and brain volumes even in difficult heterogeneous tumor\nregions. These favorable image characteristics may be useful for tumor characterization and detection and\nmotivate further investigation.\nTo our knowledge, this is the first work to synthesize T1C MRI from non-contrast MRI with a focus on tumor\nregion reconstruction. However, several notable studies have been published for multi-modal and T1C MR\nimage translation tasks.31 Liu et al. proposed the multi-contrast multi-scale Transformer (MMT) comprised\nonly of Swin-Transformer blocks and achieved their best results using T1W, T2W, and T2-FLAIR MRI to\npredict T1C, achieving a PSNR, SSIM, and learned perceptual image patch similarity (LPIPS) score of\n29.74, 939, and .120 respectively. 32,33 The synthetic T1C images were then segmented using the top-performing\nalgorithm in the BraTS 2021 challenge and obtained a DSC of .726 \u00b0 .137 for the whole tumor (WT) volume.34\nIn addition, Osman and Tamam proposed a 3D dense-dilated residual U-Net (DD-Res U-Net to synthesize T1C\nMRI from T1W, T2W, and T2-FLAIR MRI, achieving a PSNR 30.284 \u00b1 4.934, a SSIM of 0.915 \u00b1 0.063, and\na MSE of 0.001 \u00b1 0.002 for the whole brain region. However, no tumor information was given to the network\nduring training, so performance on the tumor region was found to be suboptimal.\nComparing the input T1w and T2-FLAIR MRI with the synthetic T1s, this study reveals that the synthetic T1C\nimages have superior contrast of the tumor region, especially in delineating the tumor boundary. In addition,\nthe TA-VIT model demonstrated the ability to recover detail only visible in ground truth T1C such as in Figure\n2b. However, we note that the T1C reconstructions are imperfect, especially in heterogeneous regions. This is\nbest shown in Figure 2A where small amounts of contrast were not captured outside of the enhancing tumor\nregion. An interesting result is that the NCC values for all model predictions were better for the whole brain\nvolume compared to the tumor region while the NMSE and PSNR values were better for the tumor volume. We\nspeculate that this is because the NCC looks for similarities in structure, which can be irregular or lacking in\nthe tumor regions, whereas the relative homogeneity of the tumor region may benefit MSE and PSNR. In\ncontrast with previous work showing improved performance with multi-modal inputs,35 there was no significant\ndifference between inputting only T1W MRI and both T1W MRI and T2-FLAIR. This would be advantageous\nfor predicting synthetic T1C MRI in a clinical setting since the workflow is simplified and there is no need for\nregistration between T1W and T2-FLAIR MRI.\nWe acknowledge several limitations of the present work. The TA-VIT model presented here is trained on 2D\nslices and so does not directly capture the full 3D context of the input data. While the convolutional layers\nemployed in TA-VIT provide an efficient way to capture the fine details, they might also miss key details due\nto their short-range context. Future work may involve replacing the encoder and decoder by performing the\ntraining in a latent space representation as was done with the segmentation maps in this study and removing the\nconvolutional layers in the information bottleneck to create a ViT-only architecture. The dataset was comprised\nof only glioma patients, so it remains to be seen how the results will generalize to patients with other diseases.\nHowever, given the successful application of related image translation tasks in these settings, we are optimistic\nabout the generalizability of this model.36-38 We intend in future work to incorporate full 3D context, train on\nlarger more diverse datasets as they become available22,39, explore diffusion models40, and see if synthetic T1C\ncan be used to differentiate LGG from HGG. In addition, we would also like to condition the model on genomics\nor clinical data in addition to segmentation maps to see if they further improve model performance."}, {"title": "5. CONCLUSION", "content": "This study presents a deep hybrid CNN-transformer model designed to accurately predict glioma tumor volumes\nfor multimodal T1C synthesis. By conditioning the ViT blocks on predicted segmentation maps with the adaLN-\nzero mechanism, the reconstructive ability of the network was significantly increased with minimal changes in\ncomputational complexity. The proposed method shows great promise in making valuable information in T1C\nMRI more readily available and may serve as an example for other image synthesis tasks interested in accurate\ntumor region reconstruction."}]}