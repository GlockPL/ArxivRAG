{"title": "CAN LARGE LANGUAGE MODELS GENERALIZE ANALOGY SOLVING LIKE CHILDREN CAN?", "authors": ["Claire E. Stevenson", "Alexandra Pafford", "Han L. J. van der Maas", "Melanie Mitchell"], "abstract": "When we solve an analogy we transfer information from a known context to a new one through abstract rules and relational similarity. In people, the ability to solve analogies such as \u201cbody : feet :: table: ?\u201d emerges in childhood, and appears to transfer easily to other domains, such as the visual domain \u201c( : ) :: < : ?\u201d. Recent research shows that large language models (LLMs) can solve various forms of analogies. However, can LLMs generalize analogy solving to new domains like people can? To investigate this, we had children, adults, and LLMs solve a series of letter-string analogies (e.g., ab: ac :: jk: ?) in the Latin alphabet, in a near transfer domain (Greek alphabet), and a far transfer domain (list of symbols). As expected, children and adults easily generalized their knowledge to unfamiliar domains, whereas LLMs did not. This key difference between human and Al performance is evidence that these LLMs still struggle with robust human-like analogical transfer.", "sections": [{"title": "1 Introduction", "content": "You may be familiar with the analogy \"consciousness is like an iceberg\". Here, people can intuitively infer the below-the-surface depth and complexity of consciousness by relating it to an iceberg, whose mass is mostly found under water, just as our subconscious dwells under our conscious minds. This intuitive ability emerges in childhood Goddu et al. (2020); Gentner (1988); Stevenson and Hickendorff (2018). However, it is a subject of debate whether analogical reasoning has emerged in Large Language Models (LLMs) Webb et al. (2023); Lewis and Mitchell (2024); Hodel and West (2023); Webb et al. (2024). More importantly, are LLMs able to solve analogies at this level of conceptual abstraction and generalize to novel domains Mitchell (2021); Shiffrin and Mitchell (2023)? In this study, we investigate analogical transfer at two levels of abstraction (near and far), and compare LLM performance not only to adults, but also to children, who are still developing analogical reasoning abilities. We ask the question: Can LLMs can generalize analogy solving like children can?\nAnalogical reasoning, the process of applying a known concept to understand something new through relational similarity, is fundamental to the way people think and learn Holyoak (2012); Gentner and Hoyos (2017). This is because we humans can easily generalize \u2013 that is, transfer principles discovered in one domain to new domains that share varying degrees of similarity with the original (Doumas et al., 2022). This can be principles in near contexts that are similar in terms of concrete attributes (e.g., shape, \u201ca pyramid is like an iceberg", "consciousness is like an iceberg\") Barnett and Ceci (2002). Near analogies tend to be easier for both adults and children to solve than far analogies Stevenson et al. (2023); Jones et al. (2022); Thibaut and French (2016). And, in general, adults are better at solving analogies than children. But, when the required domain knowledge and a causal framing are present then children can solve analogies such as \\\"body is to feet as table is to ?\\\" as early as the 3-4 years-old (e.g., Goddu et al., 2020; Goswami, 1991). And when analogies are presented in a more challenging or far context young children tend to revert to associative strategies, e.g., replying 'egg' to 'dog is to doghouse as chicken is to ?' instead of 'chicken coop' Stevenson and Hickendorff (2018); Gentner (1988); Thibaut and French (2016).\nThere are many tasks used to study analogical reasoning and transfer in people, from verbal to geometric to scene analogy problems (e.g., Ichien et al., 2020; Richland et al., 2006; Mulholland et al., 1980). However, many of these tasks are either not suitable for children (e.g., verbal analogies may contain unfamiliar words or relations for children) or to LLMs (e.g., visual analogies designed for children are still difficult for today's multimodal models Yiu et al. (2024)). Therefore, we need a domain that is text-based, but doesn't require domain knowledge beyond what a typical child or LLM would know. Letter-string analogies fit the bill as they require very little domain knowledge and offer an idealized scenario to examine analogical reasoning in a \\\"pure, uncontaminated way\\\" (Hofstadter, 1984, p. 3). In these puzzles, a string of letters is transformed according to one or more rules, and the task is to use analogy and apply the same transformations to a new string. For example, \\\"If abc changes to abd, what should pqr change to?\\\" (Mitchell, 2021).\nLetter-string analogy solving has been studied in human adults and LLMs. For example, (Webb et al., 2023) showed that GPT-3 is able to solve letter-string analogies better than college students. (Lewis and Mitchell, 2024) showed that GPT-models solved letter-string analogies at about 60% accuracy in the Latin alphabet domain, somewhat below the level of adults they tested. Interestingly, (Lewis and Mitchell, 2024) and (Hodel and West, 2023) found that GPT-3's performance degraded when presented with these same analogies using an alphabet of shuffled letters. Moreover, (Lewis and Mitchell, 2024) showed that GPT-models had great difficulty solving letter-string analogies in an unfamiliar alphabet of symbols, whereas people did not. As such, there is conflicting evidence of whether LLMs can generalize analogy solving to novel domains (Lewis and Mitchell, 2024; Webb et al., 2024; Hodel and West, 2023), something that comes easily to adults (e.g., Thibaut et al., 2022; Doumas et al., 2022), and that even children appear capable of when domains share structural similarities Chen (1996); Gentner and Toupin (1986); Bobrowicz et al. (2020); Holyoak et al. (1984). Thus, while there is some evidence to suggest that LLMs can solve letter-string analogies at around the same level as people, it is unclear whether these models understand the problem and are actually using analogical reasoning Opie\u0142ka et al. (2024); Stevenson et al. (2023); Moskvichev et al. (2023).\nIn this study, we investigate whether LLMs can generalize analogy solving to new domains like adults and 8-year-old children can at two levels of abstraction. To this end, we compare how adults, children, and LLMs generalize analogy solving on the letter-string task to both near (Greek alphabet) and far (Symbol list) domains.\"\n    },\n    {\n      \"title\": \"2 Method\",\n      \"content\": \"We compared 42 children (7-9 year-olds), 62 adults, and 54 runs of each of four LLMs (Anthropic's Claude-3.5, Google's Gemma-2 27B, Open AI's GPT-40, and Meta's Llama-3.1 405B) on a set of letter-string analogies under three alphabet conditions: Latin, Greek and a Symbol list.\"\n    },\n    {\n      \"title\": \"2.1 Materials\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"2.1.1 Letter-String Analogy Task\",\n      \"content\": \"Letter-string analogies, pioneered by Hofstadter 1984, are a type of analogy puzzle (A is to B as C is to D) involving alphabetic strings where one set of strings transforms to another, and the task is to use analogy to generalize the same transformation to a new string. For example, \\\"If the string of letters abc changes to abd. How would you change the string pqrs in the 'same way'?\\\" (Mitchell, 2021). Two things are happening here. First, the move from abc in term A to abd in term B shows that the last letter in the string, c, shifts to its successor in the alphabet, d. Second, the successor transformation must be generalized to the C term, a new string pqrt.\"\n    },\n    {\n      \"title\": \"3 Results\",\n      \"content\": \"We use mixed ANOVAs to (1) compare performance between our between-subjects participant groups (Adults, Children, and each of the LLMs) on the Latin alphabet and (2) test whether each participant group could generalize analogy solving, perform similarly, across alphabets (i.e., our repeated within-subjects factor). Below we report the most important results. More detailed results, e.g. comparing all LLMs, can be found in our Supplementary Material (https://osf.io/jdty3/files/osfstorage).\"\n    },\n    {\n      \"title\": \"3.1 RQ1: How well do LLMs solve letter-string analogy problems in the Latin alphabet compared to adults and children?\",\n      \"content\": \"We expected LLMs to be able to solve letter-string analogies with the Latin alphabet at the same level as adults Webb et al. (2023) and that both adults and LLMs would outperform children Thibaut and French (2016) (hypotheses H1a-c). Similar to what we expected, adults and some LLMs, except Google's Gemma-2 27B and Anthropic's Claude 3.5, performed better than children in the Latin alphabet domain. Open AI's GPT-40 performed similarly to adults, followed closely by Meta's Llama-3.1 405B. See Figure 3 and Tables 2 and 3 for more detailed results.\"\n    },\n    {\n      \"title\": \"3.2 RQ2: How well do adults, children and LLMs generalize letter-string analogy solving from the Latin (baseline) domain to the Greek (near) and Symbol (far) domains?\",\n      \"content\": \"We expected adults and children to generalize analogy solving to other domains and therefore perform similarly across domains Doumas et al. (2022) (hypotheses H2a-b). The experiment was designed to test whether LLMs could also generalize to other domains. Given the mixed results in previous research we had no clear evidence to predict how LLMs would perform on the near (Greek) and far (Symbol) letter-string analogy domains, but we suspected that LLM performance would degrade in less familiar domains.\nIndeed our results indicate that adults and children perform similarly across alphabets (see Figure 3). But, for the four LLMs we tested, Anthropic's Claude-3.5, Google's Gemma-2 27B, Open AI's GPT-40, and Meta's Llama-3.1 405B, performance indeed degraded in less familiar alphabets (ANOVA results shown in Table 4). More specifically, for each model, performance degraded significantly from the Latin to Greek alphabet (posthoc Bonferonni-corrected t-test results all p<.001, except for Llama-3.1 405Bp = 0.012) and then again from the Greek alphabet to the Symbol list (posthoc Bonferonni-corrected t-test results all p<.001).\"\n    },\n    {\n      \"title\": \"3.3 RQ3: Why can't LLMs generalize letter-string analogy solving like children?\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"3.4 Performance by Item\",\n      \"content\": \"To understand why the LLM's had trouble generalizing letter-string analogy solving to the Greek and Symbol domains we first look at their performance per item as this may give insight into which rules were easier and more difficult for the LLMs to apply. Table 5 shows and overview. Here we see that the LLMs and humans perform best on item 1, that involves only the successor transformation, and worst on item 5, that involves both the predecessor transformation and repetition generalization. Because, item 2, also involves the same repetition rule as item 5, but was solved better by LLMs and children, it seems like the predecessor rule is what gives both LLMs and children the most trouble. The other item people and LLMs have relatively more trouble with is item 3. This item involves the second successor rule. In sum, the predecessor and second successor rules appear to be the most difficult rules from our item set for people and LLMs to apply.\"\n    },\n    {\n      \"title\": \"3.5 Next-Previous Letter Task\",\n      \"content\": \"To investigate this further, we designed the Next-Previous Letter Task to check that the LLMs had the requisite domain knowledge of predecessor, successor and second successor to perform our letter-string analogy task. The task involved providing an ordered list of letters/symbols and asking the LLMs what the previous and next letters were given a specific letter. We did this 5 times using an optimized prompt requesting to identify the letter: one before, two before (not in our item set), one after and two after, resulting in 20 items total. The exact prompts and items can be found in the Appendix D.\"\n    },\n    {\n      \"title\": \"3.5.1 Rule Check Task\",\n      \"content\": \"To better pinpoint why the LLMs had difficulty generalizing to the Greek and Symbol alphabet domains, we created a simplified item set that explicitly tested each rule used in the human item set in isolation. The rules were: (1) successor_1, the next letter; (2) successor_2, letter two places after; (3) predecessor_1, the previous letter; (4) predecessor_2, letter two places before; (5) repetition_1, repeating the last letter and (6) repetition_2, repeating both letters. Each rule was tested five times. Example items for the Latin alphabet are shown in Table 6. LLM system and instruction prompts were exactly the same as with the original items of our letter-string task.\"\n    },\n    {\n      \"title\": \"3.5.2 Error Analysis\",\n      \"content\": \"In general, when solving a letter-string analogies there are often multiple rules that could underlie the change from A to B Hofstadter and Mitchell (1994). In the very short strings that we use there is less ambiguity about the rule, than in longer strings. In our case, there are generally only two clearly correct responses. We considered the rules that people would generally prefer when responding, to be \\\"correct\\\", such as if ab changes to ac, then gh changes to gi. However, the literal rule of replacing the last letter with c, with response gc could also be considered correct.\nError Categories To examine errors in more detail we created a set of categories based on those from (Lewis and Mitchell, 2024) and extended these to account for common errors in children Stevenson and Hickendorff (2018). In the Literal rule category, the change from A to B is literally copied to C such as ab:acc::gh:gcc rather than providing the more common response of g i i. In the One rule category, the response is partially correct, but only (part of) one of the rules in the problem was applied, such as in responses to the previous example, g h h (only repetition applied) or g i (only successor applied). Partially correct responses are common in children when problem load supersedes processing capacity Stevenson and Hickendorff (2018). In the Incorrect rule category, one of the other rules from our item set (i.e., successor, predecessor, repetition) was applied; for example, if the successor rule was used instead of the predecessor rule. For the Copy rule, the A, B or C term was copied as copying the C-term is common in young children (Stevenson and Hickendorff, 2018; Opie\u0142ka et al., 2024). Finally, all remaining erroneous responses were placed in the Other rule category. Given that our task was less complex than in (Lewis and Mitchell, 2024) (i.e., shorter strings, fewer rules), we were able to automatically code these categories.\"\n    },\n    {\n      \"title\": \"4 Discussion\",\n      \"content\": \"Our main finding is that the LLMs we tested, using similar prompts given to children, were not able to generalize letter-string analogy solving like children can. LLMs perform at or above the level of children on letter-string analogies in the familiar Latin alphabet, but their performance on these same problems reduces somewhat when using the Greek alphabet (near transfer) and deteriorates almost entirely when using our Symbol alphabet (far transfer).\nWhy can't LLMs generalize when solving letter-string analogies? For some models, this appears to be because they were unable to meet underlying requisites, such as indicating the predecessor or second successor. This would sense given the predict-the-next-token goal that LLMs are trained on McCoy et al. (2024). We tested this using the Next-Previous letter task, where models were given an ordered list of letters or symbols and asked to identify the (second) successor or predecessor to a given letter or symbol. These results could explain why the LLMs have trouble with the second successor, however the models had little trouble identifying the predecessor in this task. So, these results do not fully explain why LLM analogy solving performance degrades from the Latin to Greek to Symbol domains.\nThe problem with LLM's transfer from the Latin to other domains seems to lie in that the alphabet is too \u201cunfamiliar": "What we mean here is that the conceptual abstraction of what constitutes an alphabet, such as being an ordered sequence, does not appear to flexibly map to less familiar domains in LLMs like it does in people. Evidence for this comes from the Rule Check task, where we tested LLM performance on each of the rules separately. Here we see that the repetition rules could easily be applied to novel alphabets. This makes sense because if one were to create a function to repeat a letter in a string this could be done without knowing the alphabet or the order of the letters. In contrast, the models had far more trouble with the predecessor and second successor rules, that, in order to solve correctly, both require that an alphabet is encoded as an ordered list of letters/symbols. This coincides with previous work, where Hodel and West (2023); Lewis and Mitchell (2024) found that GPT models were only able to solve letter-string analogies when presented with familiar letters in their standard order. As soon as unfamiliar symbols were used or familiar letters were shuffled, performance dropped drastically. We noted that in the Greek domain the letters were also ordered, but in our Symbol domain they were not, which could perhaps explain why Greek items were easier. So, to check whether order was also a factor in our Symbol domain, we adapted the task to make the Symbol alphabet ordered by their unicode values. However, this adaptation did not result in improved LLM performance.\nWe also investigated which kinds of errors people and LLMs made. This is important because letter-string analogies, like many four-term visual analogies apply ambiguous rules (e.g., Opie\u0142ka et al., 2024), and can be solved correctly in multiple ways Hofstadter and Mitchell (1994). The two main ways to solve the items in our task were what we considered the \"correct\" way (e.g., ab: acc :: gh: gii) and the \"literal\" way (e.g., ab: acc :: gh: gcc). People did not use the \"literal\" rule, whereas the models all did to varying degrees (ranging from 5-21%). The other main difference between human and LLM errors, was that children's erroneous responses were generally more distant (measured with Levenshtein string distance) from the \"correct\" response than those of LLMs. This could be because children reverted to associative strategies that we didn't account for in our error coding scheme, given that this is the first time letter-string analogies have been administered to children. It will be interesting for future work to investigate children's development and 'shift' from associative to relational responses on letter-string analogies, especially compared to verbal and visual domains Stevenson and Hickendorff (2018); Gentner (1988).\nBased on our results and previous work, these LLMs appear to have brittle, inflexible abstractions of what represents an alphabet in the context of the letter-string analogy task, despite being given the ordered list of letters/symbols before each item. It appears that LLMs like GPT-4 can only perform this abstraction by creating and executing code to map the novel alphabet to new positions Webb et al. (2024). This is of course very different from how children solve these problems.\nIn contrast, in children, familiarity with letters or symbols does not seem to influence how well they solve letter-string analogies. As such, our results add to the accumulating evidence that questions whether reasoning actually occurs in these LLMs (Wu et al., 2024; Gendron et al., 2024). Interestingly, in 1980, Schank concluded that there wasn't much intelligence in artificial intelligence given its limited ability to generalize. Similarly, Doumas et al. 2022 argue that robust analogical transfer is a uniquely human ability. Based on our findings so far we concur, and now ask the question: Is generalization to far domains indeed what separates human general intelligence from that of artificial general intelligence? The challenge now is to create uncontaminated far generalization tasks that AI models have not been trained on to answer this question."}, {"title": "A Supplemental Table: Deviations from Pre-registration", "content": "In this supplementary table 8 we specify the differences between our OSF preregistration and the methods and analyses carried out in this study."}, {"title": "C LLM prompt engineering results", "content": "We administered each letter-string analogy item to LLMs using 5 different prompt templates, as prompt engineering can change the LLMs' performance on the task. The templates were as follows.\n1. If a b c changes to abd, what does i j k change to?\n2. abcis to a bd, asijk is to ?\n3. abcabd \\nefg \u2192 ?\n4. Let's try to complete the pattern:\\n\\n[a_b_c] [abd] \\n [ijk] [\n5. [a b c] [abd] \\n [ijk] [\nAs can be seen in Figure or Table 10, template 1, derived from Mitchell (2021) worked best overall. Template 4, the best template found by Webb et al. (2023) worked well in Latin and Greek alphabets, but not as well for the Symbol list, which makes sense because [and] are symbols themselves. Our results are based on template 1."}, {"title": "D Next-Previous Letter Task", "content": "The Next-Previous Letter Task was created to check whether LLMs were able to identify the previous and next two letters in an ordered sequence of letters. All items were administered one-by-one, without pre-pending previous conversation.\nThe LLM item instruction was:\nHere is an ordered list of letters or symbols\n'{Latin alphabet|Greek alphabet|Symbol list}'.\nWhich letter or symbol is {one|two} {place|places} {before|after} {letter|symbol} ?\nRespond with only the letter or symbol.\nThe system instruction was:\nYou are a helpful assistant that solves puzzles.\nOnly give the answer, no other words or text."}]}