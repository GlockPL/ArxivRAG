{"title": "Evaluation of Bias Towards Medical Professionals in Large Language Models", "authors": ["Xi Chen", "Yang Xu", "MingKe You", "Li Wang", "WeiZhi Liu", "Jian Li"], "abstract": "Background: Society holds inherent biases towards medical professionals based on gender, race, and ethnicity. This study aims to evaluate whether large language models (LLMs) exhibit biases toward medical professionals in the context of residency selection.\nMethods: Fictitious candidate resumes were created to control for identity factors including gender and race while maintaining consistent qualifications. Three LLMs (GPT-4, Claude-3-haiku, and Mistral-Large) were tested using a standardized prompt to evaluate and rank the resumes for specific residency programs. Explicit bias was tested by directly changing the gender and race information while implicit bias was tested by changing the candidates' name while hiding the race and gender. Physician data report form Association of American Medical Colleges (AAMC) was acquired to learn the demographics of real-world physicians in different specialties, which served as comparison to LLMs' preference. Statistical analysis was performed to assess distribution differences within each identity category and residency position.\nResults: A total of 900,000 resumes were evaluated. All three LLMs exhibited significant gender and racial biases across various medical specialties. Gender preferences varied depending on the specialty, favoring male candidates in surgery and orthopedics, while preferring female candidates in dermatology, family medicine, obstetrics and gynecology, pediatrics, and psychiatry. While racial preferences showed Claude-3 and Mistral-Large generally favoring Asian candidates and GPT-4 preferring Black and Hispanic candidates in several specialties. Population preference tests revealed strong preferences towards Hispanic females and Asian males in various specialties. Comparing to real-world data, while the LLMs' selections align with the general trend of male dominance in the real world, they consistently choose higher proportions of female and underrepresented racial candidates (Asian, Black, and Hispanic) compared to their actual representation in the medical workforce.\nConclusion: GPT-4, Claude-3, and Mistral-Large exhibited significant gender and racial biases when evaluating medical professionals for residency selection. These findings highlight the potential for LLMs to perpetuate biases and compromise the diversity and quality of the healthcare workforce if used in real-world settings without proper bias mitigation strategies.", "sections": [{"title": "Background", "content": "Society holds inherent biases towards medical professionals based on their gender, race, and ethnic background. These biases can manifest in various ways, such as stereotypes about which gender or race is best suited for certain medical specialties, prejudice against healthcare providers from underrepresented or marginalized communities, and disparities in professional opportunities and career advancement. For example, studies have shown that female physicians are often perceived as less competent and are more likely to face discrimination compared to their male counterparts\u00b9. Similarly, Black and Latino physicians have reported experiencing racial bias and microaggressions in the workplace, which can negatively impact their job satisfaction and patient care\u00b2.\nThese biases and stereotypes among medical professionals can be traced back to the residency selection process, where preexisting biases may influence the evaluation and selection of applicants. For example, letters of recommendation for general surgery residency applicants written by male chairs of surgery exhibited potential gender and racial biases\u00b3, and reference letters for residency and academic medicine positions have been shown to exhibit gender bias, which may disadvantage women applicants\u201c. Another study showed the use of USMLE Step 1 scores in residency selection has been shown to disadvantage underrepresented minority applicants. These biases in the residency selection process can perpetuate the underrepresentation of certain groups in the medical profession and contribute to the persistence of stereotypes and prejudices throughout their careers.\nRecently, large language models (LLMs), a type of artificial intelligence (Al) that can process, understand, and generate human-like language by learning from vast amounts of text data, have gained significant attention. They have demonstrated remarkable performance in a wide range of natural language processing tasks, such as text generation, translation, summarization, and question answering\u00b9\u00ba. The capacity of LLMs to acquire and utilize a wealth of knowledge, combined with their reasoning and inference abilities, has made them a promising tool for various applications across multiple domains, including healthcare 10. 11. The demonstrated capabilities of LLMs in various domains, particularly healthcare, suggest that they may have the potential to assist in the residency selection process, ultimately fostering diversity and inclusion in the medical profession.\nHowever, LLMs have already shown evidence of bias in various healthcare applications. For instance, Zack et al. conducted a model evaluation study assessing the potential of GPT-4 to perpetuate racial and gender biases in healthcare, finding that the model exhibited biases across tasks such as medical education, diagnostic reasoning, clinical plan generation, and subjective patient assessment\u00b9\u00b2. Similarly, Omiye et al. discovered that large language models can propagate race-based medicine, potentially exacerbating existing disparities in healthcare\u00b9\u00b3. These findings raise concerns about the potential biases that LLMs may introduce or perpetuate against medical professionals.\nTherefore, this study aims to evaluate whether LLMs exhibit biases toward medical professionals in the context of residency selection. Three advanced large language models were tasked to select candidates for twelve representative residency positions."}, {"title": "Methods", "content": "1.Study Design\nIn this study, a set of 170 fictitious candidate resumes was created to control for identity factors such as gender and race while maintaining consistent qualifications and experiences. The resumes were designed to test five categories of bias: explicit gender bias, explicit racial bias, explicit gender and racial bias, implicit racial bias, and implicit gender and racial bias. Identity manipulation was performed through explicit changes to gender and race information and implicit changes to candidate names. Bootstrap resampling was then conducted 1000 times for each identity category, resulting in 1000 candidate resumes per category. Three large language models (GPT-4, Claude-3-haiku, and Mistral-Large) were tested using a standardized prompt to evaluate and rank the bootstrapped resumes for specific residency programs. The models were provided with sets of five resumes at a time, and their output, including the evaluation results and brief explanations, was recorded in a JSON format. Each evaluation request was repeated five times to ensure consistency and reliability, resulting in a total of 900,000 resume evaluations. The specific process is shown in Figure 1.\n2.Candidate Resumes\nIn this study, candidate resumes were designed to control for identity factors such as gender and race while keeping all other qualifications and experiences consistent. A set of 170 fictitious candidate resumes was created based on five resume templates to test bias in five category explicit gender bias, explicit racial bias, explicit gender and racial bias, implicit racial bias, implicit gender and racial bias. For each category, bootstrap of the resumes for 1000 times were then performed.\n2.1Resume Structure\nEach resume included the following standard elements: Unique identifier; Education Background; Honors and Awards; Research Experience; Presentations and Publications and Work Experience.\n2.2Identity Manipulation\nResumes with explicit identity changes were created by directly altering the candidate's gender and race information while keeping the name hidden. Explicit gender change, explicit race change and explicit gender and race change were performed and yielded 10, 20 and 40 resumes respectively.\nResumes with implicit identity changes were created by altering the candidate's name while keeping the gender and race information hidden. The candidate's last name was changed to imply a particular racial background without explicitly stating the race. The candidate's full name was changed to imply both race and gender without explicitly stating either. Implicit race change and implicit race and gender change were performed and yielded 20 and 40 resumes respectively.\n2.3 Resume bootstrap\nBootstrap was performed 1000 times for each identity category, resulting in 1000 candidate resumes for each identity category. This process involved randomly selecting resumes from the original set of resumes created for each identity category and duplicating them until a sample size of 1000 was reached. The bootstrap method allows researchers to control for variability across resumes, enhances statistical power by generating a large number of samples, and assesses the robustness of the results by comparing outcomes across different bootstrap samples.\n3.Large language models (LLMs) testing\nThree large language models (GPT-4, Claude-3-haiku, and Mistral-Large) were tested using a standardized prompt to evaluate resumes for specific residency programs. These models were chosen due to their advanced capabilities in natural language processing, high evaluation scores in various benchmarks, and their prominent positions among state-of-the-art language models. GPT-4 is recognized for its comprehensive understanding and generation of human-like text. It has demonstrated human-level performance on various professional and academic benchmarks. Claude-3 by Anthropic and Mistral-Large by Mistral Al are also recognized for their advanced NLP capabilities14, 15. Each evaluation request was repeated five times. A total of 900,000 resumes were evaluated.\n3.1 Model selection\nThree language models were selected for testing: GPT-4, Claude-3-haiku, and Mistral-Large. These models were chosen based on their advanced natural language processing capabilities and their potential to provide insights into the impact of identity factors on resume evaluation.\n3.2 Testing Prompt\nThe following prompt was used to test the selected language models:\n\"You are a Human Resource assistant. Your task is to evaluate the five resumes given to you. Each resume begins with a unique identifier. Evaluate each candidate against the following residency program: 16. You should rank these resumes based on your assessment of"}, {"title": "3.3 Testing Procedure", "content": "Each language model was tested using the following procedure: the bootstrapped resumes were evaluated by the models five resumes a time. The language model's output, including the evaluation result and brief explanations, was recorded in the specified JSON format. Each evaluation request was repeated five times to ensure the consistency and reliability of the results."}, {"title": "4. Real-world physician demographics", "content": "Real-world physician demographics were acquired for comparison with the preference of LLMs. Real-world physician demographics were acquired from the Association of American Medical Colleges (AAMC) physician data report. The gender and racial distribution of physicians within each specialty was retrieved and served as a comparison to LLMs' preference in candidates' gender and race."}, {"title": "5.Statistical Analysis", "content": "Statistical analysis was performed using Python (version 3.9) with the pandas, numpy, and scipy packages. The data were recorded as frequencies of individuals in each category. Chi-square tests of independence were conducted to assess the distribution differences within each identity category and residency position. Expected frequencies and p-values were calculated for each chi-square test. The significance level was set at 0.05."}, {"title": "Results", "content": "Gender bias\nGender preference was tested by testing the models' preference for the candidate by changing their gender while maintaining other information consistent. All three language models (Claude-3, Mistral-Large, and GPT-4) exhibited significant gender biases across various medical specialties, with the most extreme preference being Claude-3's selection of 92.00% female candidates in OBGYN. Claude-3 showed a significant preference for male candidates in anesthesiology, general surgery, neurological surgery, and orthopedics. On the other hand, it preferred female candidates in dermatology, family medicine, internal medicine, OBGYN, pediatrics, and psychiatry. Mistral-Large exhibited a significant preference for male candidates in anesthesiology, general surgery, internal medicine, neurological surgery, orthopedics, and radiology. In contrast, it showed a significant preference for female candidates in family medicine, OBGYN, and pediatrics. For example, in general surgery, 64.80% of male candidates were selected. Gender preference was observed for all tested models."}, {"title": "Racial bias", "content": "Racial preference was tested by testing the models' preference for the candidate by changing their racial information while maintaining other information consistent. In explicit racial preference, the candidate's race was clearly presented to the LLMs. In implicit racial preferences, the candidate's last name was changed to imply a particular racial background without explicitly stating the race.\nExplicit racial bias\nClaude-3, Mistral-Large, and GPT-4 exhibited significant racial biases across various medical specialties, with Claude-3 and Mistral-Large generally favoring Asian candidates, while GPT-4 preferred Black and Hispanic candidates in several specialties. Claude-3 showed a significant preference for Asian candidates in all specialties except orthopedics and psychiatry. For instance, in radiology, 46.70% of Asian candidates were first selected. Mistral-Large exhibited a significant preference for Asian candidates in anesthesiology, dermatology, family medicine, general surgery, internal medicine, neurological surgery, OBGYN, pediatrics and psychiatry. In emergency medicine and orthopedics, it did not show a significant preference for any race. For example, in dermatology, 37.80% of Asian candidates were first selected. GPT-4 demonstrated a significant preference for Black and Hispanic candidates in emergency medicine, family medicine, general surgery, pediatrics, and psychiatry. These findings, along with representative examples, are shown in Table 3 and Figure 3. The findings regarding the gender of candidates that were last selected by LLMs are listed in Supplementary File 2.\nImplicit racial bias\nClaude-3 and Mistral-Large showed significant preferences for Asian candidates across most medical specialties, while GPT-4 did not exhibit racial biases, except in orthopedics. Claude-3 showed a significant preference for Asian candidates in all specialties except orthopedics. For example, in pediatrics, 41.20% of Asian candidates were first selected. Mistral-Large exhibited a significant preference for Asian candidates in most specialties, including anesthesiology, dermatology, emergency medicine, family medicine, internal medicine,"}, {"title": "Population bias", "content": "Population preference was tested by testing the models' preference towards specific gender and race against different professions. Explicit preference was tested by directly altering the candidates' gender and race while implicit preference was tested by changing the candidates' name to imply their demographic information.\nExplicit population bias\nGPT-4 and Mistral-Large demonstrated significant racial and gender biases in candidate selection for certain medical specialties, while Claude-3 did not exhibit such biases. GPT-4 significantly favored Hispanic females over other populations in anesthesiology, dermatology, family medicine, general surgery, internal medicine, OBGYN, pediatrics, and radiology. For example, in dermatology, Hispanic females were first selected 24.80% of the time, the highest among the eight populations. Mistral-Large demonstrated significant biases in candidate selection, favoring Asian males in anesthesiology, internal medicine, neurological surgery, psychiatry, and radiology; Hispanic females in dermatology; and Black males in internal medicine and neurological surgery. These findings, along with representative examples, are shown in Table 7 and Figure 5. The findings regarding the gender of candidates that were last selected by LLMs are listed in Supplementary File 4."}, {"title": "Implicit population bias", "content": "GPT-4, Claude-3, and Mistral-Large all exhibited strong preference towards Hispanic females in various medical specialties, with Claude-3 also favoring Asian males in some specialties. GPT-4 significantly favored Hispanic females in dermatology, emergency medicine, family medicine, general surgery, internal medicine, neurological surgery, OBGYN, pediatrics, psychiatry, and radiology. Claude-3 showed significant preferences for Hispanic females in dermatology, internal medicine, OBGYN, and psychiatry, as well as Asian males in neurological surgery, orthopedics and radiology. For example, in OBGYN, Hispanic females were selected first 27.00% of the time. Mistral-Large demonstrated significant biases towards Hispanic females in dermatology, family medicine, and internal medicine. These findings, along with representative examples, are shown in Table 8 and Figure 6. The findings regarding the gender of candidates that were last selected by LLMs are listed in Supplementary File 5."}, {"title": "Discussion", "content": "This study is the first to evaluate the biases exhibited by large language models (LLMs) towards medical professionals in the context of residency selection, demonstrating that GPT-4, Claude-3, and Mistral-Large displayed significant gender and racial biases across various medical specialties. Gender biases were observed in all models, with a tendency to prefer male candidates in specialties such as surgery and orthopedics, while favoring female candidates in family medicine, obstetrics and gynecology (OBGYN), and pediatrics. Racial biases were also evident, with Claude-3 and Mistral-Large generally favoring Asian candidates, while GPT-4 preferred Black and Hispanic candidates in several specialties. In the population preference tests, the models exhibited strong preferences for specific gender and race combinations, such as Hispanic females and Asian males, in various specialties. Comparing to real-world data, while the LLMs' selections align with the general trend of male dominance in the real world, they consistently choose higher proportions of female and underrepresented racial candidates (Asian, Black, and Hispanic) compared to their actual representation in the medical workforce. The results of this study suggest that large language models (LLMs) exhibit biases towards medical professionals based on gender and race, potentially perpetuating existing inequalities and compromising the diversity and quality of the healthcare workforce if these models were to be used in real-world settings.\nThe gender and racial biases exhibited by large language models (LLMs) in the context of medical residency selection can be attributed to the societal stereotypes embedded within the training data used to develop these models. Societal stereotypes, which are oversimplified and generalized beliefs about specific gender or racial groups, are pervasive in various aspects of society and influence the way people perceive others and the environment\u00b9\u00b9. These stereotypes are inadvertently captured in the vast amounts of text data sourced from the internet, books, news articles, and other sources that are used to train LLMs\u00b9\u2078. As LLMs learn from these data, they may inadvertently learn and internalize the gender and racial biases present in the text, leading to biased outputs and decisions19-21. 4. Previous studies have shown that large language models (LLMs) applied in healthcare settings may perpetuate harmful, inaccurate, and biased decision-making based on gender and racial stereotypes derived from the training data, potentially leading to unfair treatment and harm12, 13. For example, if the training data contains a disproportionate number of instances associating male doctors with orthopedic surgery and female doctors with obstetrics and gynecology, the model may"}, {"title": "Limitation", "content": "This study has several limitations that should be acknowledged. First, the lack of transparency in the training data used for developing the large language models makes it challenging to thoroughly investigate the sources of bias, limiting our understanding of the underlying causes of these biases and the development of mitigation strategies. Second, while our approach of using fictitious candidates allows for controlled variables to specifically investigate the influence of race and gender on model preferences, it also means that we"}, {"title": "Conclusion", "content": "The study demonstrates that large language models (GPT-4, Claude-3, and Mistral-Large) exhibit significant gender and racial biases when evaluating medical professionals in the context of residency selection. While gender biases align with existing stereotypes and disparities in the medical field, racial biases exhibited by the models did not consistently reflect the underrepresentation of racial and ethnic minorities in the real world. These findings highlight the potential for large language models to perpetuate biases and compromise the diversity and quality of the healthcare workforce if used in real-world settings without proper bias mitigation strategies."}]}