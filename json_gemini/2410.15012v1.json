{"title": "Pathologist-like explainable AI for interpretable Gleason grading in prostate cancer", "authors": ["Gesa Mittmann", "Sara Laiouar-Pedari", "Hendrik A. Mehrtens", "Sarah Haggenm\u00fcller", "Tabea-Clara Bucher", "Tirtha Chanda", "Nadine T. Gaisa", "Mathias Wagner", "Gilbert Georg Klamminger", "Tilman T. Rau", "Christina Neppl", "Eva Maria Comp\u00e9rat", "Andreas Gocht", "Monika H\u00e4mmerle", "Niels J. Rupp", "Jula Westhoff", "Irene Kr\u00fccken", "Maximillian Seidl", "Christian M. Sch\u00fcrch", "Marcus Bauer", "Wiebke Solass", "Yu Chun Tam", "Florian Weber", "Rainer Grobholz", "Jaroslaw Augustyniak", "Thomas Kalinski", "Christian H\u00f6rner", "Kirsten D. Mertz", "Constanze D\u00f6ring", "Andreas Erbersdobler", "Gabriele Deubler", "Felix Bremmer", "Ulrich Sommer", "Michael Brodhun", "Jon Griffin", "Maria Sarah L. Lenon", "Kiril Trpkov", "Liang Cheng", "Fei Chen", "Angelique Levi", "Guoping Cai", "Tri Q. Nguyen", "Ali Amin", "Alessia Cimadamore", "Ahmed Shabaik", "Varsha Manucha", "Nazeel Ahmad", "Nidia Messias", "Francesca Sanguedolce", "Diana Taheri", "Ezra Baraban", "Liwei Jia", "Rajal B. Shah", "Farshid Siadat", "Nicole Swarbrick", "Kyung Park", "Oudai Hassan", "Siamak Sakhaie", "Michelle R. Downes", "Hiroshi Miyamoto", "Sean R. Williamson", "Tim Holland-Letz", "Carolin V. Schneider", "Jakob Nikolas Kather", "Yuri Tolkach", "Titus J. Brinker"], "abstract": "The aggressiveness of prostate cancer, the most common cancer in men worldwide, is primarily assessed based on histopathological data using the Gleason scoring system. While artificial intelligence (AI) has shown promise in accurately predicting Gleason scores, these predictions often lack inherent explainability, potentially leading to distrust in human-machine interactions. To address this issue, we introduce a novel dataset of 1,015 tissue microarray core images, annotated by an international group of 54 pathologists. The annotations provide detailed localized pattern descriptions for Gleason grading in line with international guidelines. Utilizing this dataset, we develop an inherently explainable Al system based on a U-Net architecture that provides predictions leveraging pathologists' terminology. This approach circumvents post-hoc explainability methods while maintaining or exceeding the performance of methods trained directly for Gleason pattern segmentation (Dice score: 0.713$_{\\pm0.003}$ trained on explanations vs. 0.691$_{\\pm0.010}$ trained on Gleason patterns). By employing soft labels during training, we capture the intrinsic uncertainty in the data, yielding strong results in Gleason pattern segmentation even in the context of high interobserver variability. With the release of this dataset, we aim to encourage further research into segmentation in medical tasks with high levels of subjectivity and to advance the understanding of pathologists' reasoning processes.", "sections": [{"title": "Introduction", "content": "Prostate cancer is a major health issue, affecting approximately 5 million men globally, with around 1.5 million new cases reported in 2020 1. The Gleason grading system, developed by Donald Gleason in 1974 2 and most recently discussed and updated by the International Society of Urology Pathology (ISUP) and by the Genitourinary Pathology Society (GUPS) in 2019 3,4, remains the primary method for assessing tumor aggressiveness and prognosis in patients with prostate cancer 25.\nFor Gleason scoring in the context of primary diagnosis, pathologists assess histological architectural features such as gland shape and size based on tumor biopsy samples and assign Gleason patterns ranging from 1 (resembling gland-like structures) to 5 (resembling least gland-like structures). Gleason patterns 1 and 2 were merged with pattern 3 in later modifications of the system 6, therefore the Gleason score, quantified as a sum of the most predominant and the highest Gleason pattern, ranges from 6 (3+3) to 10 (5+5). Higher scores indicate more aggressive tumors 7. Despite its widespread use, however, the Gleason system has limitations, including sampling bias and subjective assessment of tumor architecture resulting in significant interobserver variability 8.\nMultiple studies have shown that artificial intelligence (AI)-based image analysis has the potential to assist pathologists in Gleason grading, potentially matching or exceeding human accuracy 9-12. Developing robust Al models for this task requires large datasets with expert annotations. For Gleason grading, datasets such as the Gleason19 Challenge 13,14 and the PANDA Challenge 11,12 are openly available; however, in most cases the available annotations indicate the area of patterns relevant to the final scoring or merely the Gleason score, without providing an explanation of the specific histological criteria behind the decisions. Consequently, the typical approach to Gleason grading with Al involves end-to-end models that predict Gleason patterns or even the score directly from the images. Although these models can achieve high accuracy, their decision-making process lacks transparency, which may present a barrier to clinical adoption 15,16, particularly in light of patients' right to explanation 17. Especially in fields such as Gleason grading, where there is a significant subjectivity in the assessment 8, the interest in Al-assisted diagnostic systems is high. However, there is a demand for clear and reliable explanations 18.\nIn order to overcome interpretability issues for neural networks, post-hoc explainability techniques such as CAM or Grad-CAM 19, LRP 20, and LIME 21,22,23 have been developed that highlight regions of interest relevant to the decision made. These heatmaps aim to provide"}, {"title": "", "content": "visual explanations for Al decisions, for example by indicating pixels that have a high influence on the predicted outcome. However, these methods often provide pseudo-explainability with vague morphological correlates. Interpreting the results requires specialized expertise 24, and it has been shown that using these approaches carries a high risk of confirmation bias 25,26, a cognitive tendency whereby individuals favor evidence that confirms their pre-existing hypotheses and beliefs. Additionally, such indicated regions may not always correspond to the actual causative regions of the cancer patterns 27,28, but might instead show unwanted statistical correlations learned by the neural network a crucial factor that is rarely addressed. As pathologists prefer simple, visual explanations that are grounded in morphology and reflect their way of thinking, an inherently explainable approach to Al with clear and intuitive explanations is needed 29.\nTo address these significant limitations of traditional algorithm development, we propose the use of a concept-bottleneck-like 30 U-Net 31 architecture to develop a pathologist-like, inherently explainable Al system (GleasonXAI), as presented in Figure 1. For the development we compile and open source one of the largest datasets of annotations localizing explanations for Gleason patterns in tissue microarray (TMA) core images. The resulting GleasonXAI offers interpretability for Al-assisted segmentation of Gleason pattern by directly recognizing and delineating pre-defined histological features, which are associated with a textual explanation using terminology common to pathologists and rooted in GUPS and ISUP recommendations.\nUtilizing novel approaches to train and evaluate models with soft labels, we capture the intrinsic uncertainty in the training data, thereby providing promising Gleason pattern segmentation in spite of high interobserver variability, and exceeding the performance of traditional approaches directly trained to predict Gleason patterns."}, {"title": "Results", "content": "Between March 2023 and October 2023, an international team of 54 pathologists from ten countries participated in the study, with the majority of participants from Germany (22) and the USA (18). Among the participants, 47 were responsible for explanatory annotations, six for Gleason grade annotations, and one for creating the initial terminology, which was later reviewed and adapted in a panel of nine of the participants (see Methods section). The pathologists had a median of 15 years of clinical experience in pathology, with individual experience ranging from one to 35 years. Notably, 28 of these 54 annotators had extensive experience, defined as 15 years or more. In their clinical practice, the participating pathologists signed out a median of 15 prostate cancer cases per week, with individual prostate cancer caseloads ranging from fewer than ten to 75 patient cases weekly."}, {"title": "Dataset Characteristics", "content": "The annotated dataset generated in this study comprised 1,015 TMA core images. The images were sourced from three distinct datasets, each created by a different institution. The annotations consisted of areas to which explanations describing histological patterns were assigned. Explanations could be mapped to one of the three Gleason patterns or further divided into more detailed sub-explanations, which described more specific subgroups of the histological features defined by their parent explanation. For additional information, please refer to the Methods section.\nFor each of the Gleason patterns, there were a considerable number of images containing associated annotations. Specifically, 55.76% of the images contained annotations for Gleason pattern 3 (566/1015), 74.48% for Gleason pattern 4 (756/1015), and 32.32% for Gleason pattern 5 (328/1015) (see Figure 2a). When analyzing the segmentation masks on both the explanation and sub-explanation level, the number of images containing the classes at least once exhibited a higher variation, with values ranging from 57 to 729 for explanations and 0 to 526 for sub-explanations (see Figure 2b and Figure 2c). However, the broader, classical explanations yielded a more balanced distribution, with fewer small classes."}, {"title": "Agreement between Pathologists Varies Depending on Histopathologic Pattern", "content": "Understanding the interobserver variability in the ground truth for Gleason grading was crucial for improving the reliability of our classifier, as inconsistent annotations can significantly impact model performance.\nIn our dataset, the images were accompanied by Gleason score information (see Methods section), generated for each core by a consensus between one to six pathologists, to provide guidance to the annotators. Pathologists were, however, encouraged to use explanations of different Gleason patterns, if they disagreed. Comparing the given grade information with the annotated Gleason patterns (see Figure 3a), the annotations largely aligned with the given grade. The majority of discrepancies occurred at the boundaries between Gleason patterns 3 and 4, and Gleason pattern 4 and 5 - an expected observation,"}, {"title": "", "content": "as borderline cases are a known source of interobserver variability. The high agreement is also reflected in the Fleiss' kappa 32 values, which ranged from 0.23 to 1.00 within the annotator groups when identifying Gleason patterns (see Figure 3b, top), indicating fair to perfect agreement according to Landis and Koch 33."}, {"title": "", "content": "Consensus on the specific histological patterns, however, and consequently the appropriate sub-explanations and explanations, was less frequent. The extent of this variability differs depending on the specific explanation under consideration.\nAn analysis of the distribution of the images across the number of annotators that agreed on the presence of each explanation (see Figure 3b, left) revealed high levels of agreement for certain histological features, such as poorly formed glands and individual glands. Specifically, 76.13% (555/729) of the images annotated with poorly formed glands and 80.81% (445/563) with individual glands by at least one annotator reached at least two-rater agreement. This is further supported by their respective mean Fleiss' kappa values of 0.50$_{\\pm0.23}$ and 0.61$_{\\pm0.20}$ (see Figure 3b, right), indicating moderate to substantial agreement.\nConversely, there are also explanations, such as glomeruloid glands and single cells, where it was rare for a second or third annotator to agree (see Figure 3b, left). Subgroup analyses identified particularly pronounced interobserver variability for the explanations of single cells and compressed glands, with Fleiss' kappa values of 0.145 and 0.180, respectively (see Supplementary Table S.2). The explanations of glomeruloid glands and comedonecrosis on the other hand exhibited large variance in Fleiss' kappa values across the annotator groups, ranging from -0.031 to 0.796 and -0.052 to 0.852, respectively. Notably, with the exception of compressed glands, these explanations were the rarest annotated classes (see Figure 2).\nThe agreement on the sub-explanation was notably low, with Fleiss' kappa values ranging from -0.22 to 0.85 between the groups, as illustrated by the predominantly slight agreement shown in Supplementary Figure S.1 for each label. As interobserver agreements for most histologic patterns, which are equivalent to our explanations, are reported to be fair or moderate 34\u201336, it is to be expected that agreement on even finer details of the patterns will be lower. These results indicated considerable noise in the identification of sub-explanations, confirming the necessity of using the explanations that consolidate the detailed sub-explanations into broader, medically coherent categories. This step was crucial to reduce variability and to ensure more reliable training of our classifier.\nFurther details on the Fleiss' kappa values and their bootstrapped confidence intervals can be found in Supplementary Table S.2 to S.7."}, {"title": "Pixelwise Agreement Between Raters Is Lower in Minority Classes", "content": "Since the Al was tasked with learning the localization of the explanations, the annotators' agreement at pixel level was crucial. As their annotations served as the predictive targets for the Al, the level of interobserver agreement induced an upper limit on the performance the Al could reach.\nA similar pattern of decreasing annotator agreement with increasing explanation detail was observed when analyzing the number of pixels with a unique majority vote. Of the 58.12% of pixels constituting the foreground of the dataset, 97.54% could be assigned a unique majority class when evaluating the Gleason patterns (36.23% with two rater and 61.30% with three rater agreement). However, at the explanation level, this dropped to 86.41% (41.07% with two rater and 45.35% with three rater agreement, respectively), and further to 67.76% at the sub-explanations level (37.80% and 29.96%, respectively), indicating a considerable proportion of pixels with a high annotation uncertainty.\nIt is worth noting that the classes with the lowest number of annotated pixels (see Figure 4b) were also precisely the classes where annotators demonstrated the least consensus. As illustrated in Figure 4a), this was particularly evident for the explanations of comedonecrosis, single cells, glomeruloid glands, and compressed glands, where 88.46% to 94.96% of all annotated pixels were annotated by a single rater.\nThe lower number of pixels with a unique majority vote indicated that the agreement at the pixel level was weaker compared to the image level. Consequently, it could be inferred that Fleiss' kappa at image-level represented an upper bound on the overall agreement."}, {"title": "Model Development and Evaluation", "content": "To develop a pathologist-like, inherently explainable Al system for Gleason pattern segmentation (Gleason XAI), we selected a soft label approach, by treating the different annotations from different annotators over the pixels as probability distributions. This approach accounted for the high interobserver disagreement stemming from the reviewers' annotation uncertainty. Preserving all annotations instead of merging them with a traditionally used majority vote ensured that the Al system could also reflect the nuances of expert judgment. For comparison we also included classical hard label approaches, using the majority votes of our international expert team of pathologists.\nFor both approaches, we compared our models trained on the training data with different loss functions (see Methods section). Specifically, for the soft label approach, we used the cross-entropy loss and our custom SoftDiceLoss, while for the hard label approach, we employed the original Dice loss and cross-entropy loss (see Figure 5). All approaches were compared using the Macro SoftDice, $L_1$-norm, Dice and Macro Dice metrics on a holdout test set. Due to the large interobserver disagreement and class imbalance present in the sub-explanations, we trained our models on the broader, medically coherent explanation level of our ontology (see Methods section, Figure 8). A comparison of all methods, when trained on the sub-explanations, as well as the full numerical results can be found in Supplementary Figure 2 and Supplementary Table S.8.\nSoft Labels Improve Model Performance By Considering Annotator Uncertainty"}, {"title": "", "content": "consistently demonstrated superior performance in terms of the segmentation metrics on the test data compared to hard label approaches, when trained on the explanations.\nImportantly, we were able to preserve the segmentation quality of the Gleason patterns even when training on the explanations. Models using the SoftDiceLoss, trained on the explanations but evaluated on the Gleason patterns, performed just as well as those trained directly on the Gleason patterns. This was reassuring, as it demonstrated that the inherent interpretability of our method did not come at the cost of reduced segmentation performance for the clinical task.\nSimilarly to the segmentation performance, models trained on explanations with our custom SoftDiceLoss exhibited better calibration to the pathologists' annotation distribution compared to the cross-entropy models trained with soft labels, as evidenced by a lower $L_1$ -norm (see Figure 5).\nSince the models trained with SoftDiceLoss on the explanations performed best across most metrics except for the $L_1$-norm on the explanations on both the explanations and Gleason patterns, we will use these models for the remainder of the analysis and define them as our GleasonXAI models. Further discussion on the calibration can be found in the Supplementary Material Calibration Metric Discussion.\nGleasonXAI Strongly Aligns With Pathologists\nTo further analyze the performance capabilities of our GleasonXAI models, we examined the distributions of predictions and the confusion matrices of the models (see Figure 6) on the test set. We combined the results of the three models by averaging the predictive distribution and confusion matrices."}, {"title": "", "content": "Our GleasonXAI models predicted the majority of classes in the dataset with high reliability, often closely matching the prediction frequency of the annotations. Classes that were rarely annotated were predicted more frequently than they were annotated, and were also assigned a greater probability mass (see Figure 6a). However, for the rarest explanations such as glomeruloid glands, single cells, and presence of comedonecrosis the methods were not able to produce predictions with high certainty, likely due to their rarity in the training data. Similarly, the comedonecrosis class was rarely predicted. However, probability mass was assigned to these, which indicates that they are not fully unrecognized.\nThe confusion matrix in Figure 6b illustrates the annotations versus the predictions, revealing minimal confusion between explanations of different Gleason patterns, as indicated by the gray boxes, which underscored the strong performance of our models trained for Gleason pattern segmentation. Notably, explanations of Gleason patterns 3 and 5 were rarely misclassified as one another. Misclassifications predominantly occurred between adjacent Gleason patterns. This outcome was expected, as many cases could be medically categorized as falling between the Gleason pattern stages, and aligned with the pathologists' grading discrepancies shown in Figure 3a, where they also mostly differed between adjacent classes.\nThe class which was most frequently falsely classified was benign tissue, which we hypothesize is likely due to label uncertainty in the border regions of the annotations. This uncertainty might have arisen from the inherent difficulty in precisely determining annotation boundaries. Despite this, most of the predicted classes were predicted with high accuracy, with the lowest accuracy observed for the Gleason pattern 4 explanation of poorly formed glands at 44.4 % (\u00b1 1.18% SD) and the highest for the explanation of individual glands at 73.8% (\u00b1 2.66% SD).\nGleasonXAI Generates Detailed Segmentation Maps\nAs segmentation maps that achieve high Dice scores can still exhibit unwanted properties like visual artifact or clutter 37, we qualitatively verified the correctness of our segmentation maps by visualizing them (see Figure 7). For these visualizations of the test data, we averaged the predictions of the three GleasonXAI models.\nThe models generally aligned well with the provided annotations, often producing segmentation maps that integrate elements from each individual annotation. This alignment was not restricted to the majority classes; the model also effectively captured fine details of"}, {"title": "", "content": "less prevalent classes, even when there was no consensus among annotators on the presence of certain patterns. Notably, our model frequently generated more detailed segmentation masks than those in the reference testset (see Figure 7g), accurately predicting smaller details that were not annotated by multiple pathologists, confirming the high quality of our segmentations. These findings were encouraging as the model was able to also capture fine structural details and was not distracted by more coarse annotations."}, {"title": "Discussion", "content": "The development of trustworthy and explainable machine learning models is crucial for their adoption in clinical practice 18. However, previous studies have focused on detecting Gleason patterns using an end-to-end Al approach. This provides limited explainability, if any, through the use of post-hoc methods, which leaves room for potential interpretation and confirmation biases 28. The goal of this study was to develop a pathologist-like, inherently explainable model for the segmentation of Gleason patterns, directly trained on annotations of morphological concepts collected through a collaborative effort of a large international group of 54 pathologists. In addition, we provide access to this large annotated dataset of TMA core images (n=1015) and detailed localized explanations for Gleason patterns.\nBy utilizing novel soft-label loss functions, such as our custom SoftDiceLoss, along with tailored assessment metrics, we were able to successfully train an Al system on a segmentation task characterized by substantial interobserver variability. The resulting Al is inherently explainable, directly providing explanations for tissue characteristics in accordance with the WHO/ISUP guidelines 6. It demonstrates reliable performance across all but the rarest classes, which simultaneously exhibited highest discordance (e.g. comedonecrosis, glomeruloid glands and single cells), often annotated only by a minority of the raters.\nRemarkably, these results were achieved without any loss in the segmentation quality for Gleason patterns compared to conventional methods trained directly on them (see Figure 5, mean Dice$_{\\pm SD}$ : 0.713$_{\\pm0.003}$ vs. 0.691$_{\\pm0.010}$). This is particularly encouraging, as inherently explainable XAI methods often suffer from an unfavorable performance-interpretability trade-off 38. By treating the per-pixel annotations of multiple pathologists as soft-labels, rather than relying on high-variance, majority-voted labels, we were able to improve segmentation performance compared to hard label-based approaches, while also preserving and respecting the inherent uncertainty and ambiguity in Gleason pattern explanations.\nWe hypothesize that the benefits of using soft labels become more pronounced in scenarios with greater class imbalances and increased label uncertainty. This is especially visible when directly comparing the Macro Dice scores of cross entropy models trained on soft labels ( mean$_{\\pm SD}$): 0.625$_{\\pm0.032}$) against those trained on majority-voted explanations labels (mean$_{\\pm SD}$ 0.168$_{\\pm0.033}$), when evaluated on the Gleason patterns. The considerable interobserver disagreement resulted in substantial sample variance for the majority-voted labels, contributing to label-noise, while additionally requiring the exclusion of 13.59% of all"}, {"title": "", "content": "foreground pixels for the explanations (see Figure 4). By utilizing soft labels, we are able to incorporate all annotations \u2014 including minority opinions or classes that would otherwise be discarded in majority voting, or cases with multiple medically plausible annotations. This approach allows us to retain every pixel and provides an estimated annotation confidence, resulting in more conservative and distributed predictions and better predictions for minority classes.\nConsequently, when training on Gleason patterns, the soft labels and the SoftDiceLoss did not provide a great advantage over the hard label-based approaches. Due to the relatively high level of agreement among pathologists on Gleason pattern level (see Figure 3a), many soft labels closely match with the majority-voted labels, and the class distribution is more balanced. This observation aligns with recent literature 39, which suggests that label smoothing a label augmentation technique that produces distributional labels is particularly beneficial in settings characterized by high label noise and class imbalance.\nGleasonXAI was not able to predict the rarest of classes in a majority vote evaluation (see Figure 6), among them the morphological finding of comedonecrosis, which is an important histologic feature pathologists must evaluate during risk stratification 40. We attribute this to their extreme rarity and the high inter-rater variability, even resulting in infrequent majority annotations among pathologists for these classes (see Figure 3 and Figure 4). As a result, these classes were consistently ranked as the second most likely or lower in the predictions. However, aside for the rarest classes, the remaining minority classes were predicted with high accuracy, often more frequently and with greater probability mass than they were annotated (see Figure 6a). This behavior may be attributed to our class-averaged loss function, which emphasizes performance for minority classes or due to smaller, unannotated structures in the image, that were nonetheless predicted by GleasonXAI. Future studies could build upon this work by performing a targeted data collection for these less frequent explanations to further enhance the clinical utility of the model."}, {"title": "", "content": "Whereas the pathologists achieved agreement similar to the literature in Gleason pattern annotation and some of the explanations 35,36,41, our analyses also revealed a higher-than-expected level of disagreement in others, such as single cells and comedonecrosis 34. While each image was annotated by three expert pathologists \u2014 thereby exceeding the standard of care in terms of the number of observers increasing the number of pathologists per image could therefore further improve the estimation of the underlying diagnostic distribution for each location. This would not only reduce sampling noise in the annotations, thereby improving the learning signal for hard label approaches, but"}, {"title": "", "content": "also provide more precise estimates of diagnostic uncertainty. Such improvements would yield better and more continuous targets for the soft label approaches and allow for finer evaluation of the corresponding metrics.\nOur work revealed a blind-spot in segmentation research using soft-labels. While recent segmentation loss-functions have been developed for training with soft labels 42, the evaluation and the presentation of results for pathologists still hinges on hard labels. Even with a perfectly estimated diagnostic distribution, a learned minority opinion within the diagnostic distribution would not be reflected in metrics based on hard labels or in visualizations that present only the most likely explanation of the predictive distribution. Since the goal of the study was to develop a model that closely matches the pathologists' consensus, addressing this challenge is beyond the scope for this paper. However, future work on the use of predictive distributions and soft labels in medical segmentation tasks is crucial. A potential approach could involve threshold based multi-label approaches or adapting conformal prediction techniques 43 to segmentation tasks.\nThe primary goal of our study was not to achieve state-of-the-art performance in Gleason pattern segmentation or grading, but rather to introduce a new approach for inherent and reliable explainability in Gleason pattern segmentation using novel annotations. Our approach could likely benefit from modern techniques like semi-supervised pre-training, even larger datasets, or more advanced training techniques like additional augmentations or ensembling. We encourage other researchers to further explore and improve upon this challenging segmentation task.\nRecognizing the critical importance of explainable Al for the clinical adoption of automated Gleason grading, we developed GleasonXAI a pathologist-like, inherently explainable model for the segmentation of Gleason patterns, trained directly on annotations created in collaboration with an international consortium of 54 pathologists. To further advance research on high-discordance medical datasets, we provide access to the largest annotated dataset of localized explanations for Gleason patterns, comprising 1,015 TMA core images."}, {"title": "Methods", "content": "To gather meaningful explanations for the dataset, we developed a comprehensive medical ontology with detailed explanations for the Gleason patterns 3, 4, and 5, based on the histological description of Iczkowski KA. for Gleason grading 44. In collaboration with an experienced uro-pathologist (NTG), these explanations were shortened, split into distinct classes and translated into German for our German collaborators (see Supplementary Table S.1). Our main goal was to establish a criteria-based ontology for each Gleason pattern, incorporating distinct characteristics unique to each specific pattern, that should be later annotated by experienced pathologists.\nAdditionally, we conducted a panel discussion with expert uro-pathologists (n=9) to gather feedback on our approach. A key outcome was the need to adapt our ontology to current ISUP/World Health Organization (WHO) terminology. The wording changes were made while preserving the original content, resulting in our adapted ontology (see Figure 8)."}, {"title": "Development of an Explanatory Ontology", "content": "For our model development we defined three distinct levels: Each Gleason pattern (depicted in dark blue) was assigned a set of broader, medical coherent explanations (depicted in light blue), which themselves contained multiple sub-explanations (depicted in white). These sub-explanations represent the original annotations that were gathered."}, {"title": "Utilized Datasets", "content": "We utilized data from three different data sources: 423 TMA core images were received from TissueArray.com 45, 538 from Arvaniti et al. Harvard Dataverse 46,47, and 219 from the Gleason19 Challenge 13,14. The datasets were filtered to match our requirements of containing prostate adenocarcinoma tissue with Gleason Patterns 3, 4, and 5. In total, 1180 TMA cores of prostate adenocarcinomas were identified as eligible for annotation with detailed explanatory features."}, {"title": "Annotating Procedure", "content": "To ensure high-quality annotations, we recruited an international team of 54 pathologists from university clinics, non-university public clinics, and private pathology practices through the ISUP platform or direct email invitations. Of the 54 annotators, 53 took part in the annotation process, which involved two tasks: First, three to four pathologists annotated the most prevalent and malignant Gleason pattern in the TMA core images; second, three annotators applied the explanatory ontology using the sub-explanations to generate detailed and explainable annotations.\nThe first task was only conducted on the TissueArray.com dataset, as the Gleason 19 challenge and Harvard Dataverse datasets already contained annotations of the Gleason patterns. The annotators were informed of the Gleason grading from the metadata of the Harvard Dataverse dataset (generated by a single pathologist based on hematoxylin and eosin staining and Immunohistochemical tests 45), but could specify alternative patterns if they disagreed with the provided grading. The Gleason grade annotations from the pathologists were then merged using the STAPLE algorithm 48. Similarly, the provided annotations for the Gleason19 Challenge dataset (generated by up to six pathologists 14) were merged using the same algorithm. The resulting output masks were reviewed for quality and filtered by an observer (SLP).\nAfter the merging of the Gleason Grade annotation masks, additional filtering was required. Images were removed (n=143) due to missing annotators, small or no Gleason grade areas, containing only Gleason Grade 1 or 2 post-merge, or other quality concerns. In 19 cases in the Gleason 19 Challenge dataset, the merged grade annotations produced by STAPLE did not align with any meaningful biological patterns (e.g too small or fragmented areas), though individual annotator labels did. For these cases, the annotation of the pathologist, whose annotation was deemed the closest match to the STAPLE output by an observer (SLP), was selected (see Supplementary Figure S.3 for representative image)."}, {"title": "", "content": "For the second task, the TMA core images and their corresponding Gleason pattern annotation maps from the first task were divided in 15 distinct sets. Each set was provided to a group of annotators, who were then tasked with annotating specific histological patterns within the predefined areas to explain the respective Gleason pattern and assign a corresponding explanatory text. A free-text option was available if the pathologists disagreed with the provided explanation choices. For each annotated image from the first task, the pathologists of the second task received up to two different images, each with the outline of the annotation area for a single Gleason pattern (single grade images). In cases where the assigned Gleason grades were identical (i. e., 3+3, 4+4, or 5+5), only one single image with the corresponding area marked was presented (see Supplementary Figure S.4).\nAfter the annotation with the explanatory labels, additional images (n=162) had to be excluded due to an insufficient number of raters. The objective was to obtain annotations from three pathologists for each image. We used the annotations of the first three pathologists who responded and completed their tasks. Pathologists who dropped out early in the annotation process were replaced, and their contributions were not included in the final dataset, if they annotated less than a quarter of their assigned data. The images for which we did not receive full annotations from three pathologists by the end of the process were removed. This occurred primarily due to late dropouts, but also due to single images being skipped. We reviewed the skipped images to account for potential systematic biases, but were unable to identify any consistent issues. Overall, annotations for 1,015 TMA core images from 42 of the 47 involved annotators in the explanation annotation phase were included.\nAll annotations were performed using the online annotation tool PlainSight 49. Further details on the data selection is available in Supplementary Figure S.5."}, {"title": "Data Preparation", "content": "After the raters completed the explanatory annotations, explanations provided via the free text option had to be standardized. This was achieved by mapping them to their nearest equivalent in the ontology.\nSorted by the time of polygon creation, empty explanation fields were filled with the next available explanation. This reflected the expected behavior outlined in the instruction video provided to the raters: polygons were drawn first, afterwards the explanations were selected.\nIf multiple explanations were selected for a polygon, each explanation was included in the data as a copy of the original polygon.\nFor our model training, we created segmentation masks for each annotator and TMA core image pairs. As the explanations for each Gleason pattern were annotated separately on single grade images, we drew the annotations of each single grade image in the order they were created, with the single grade images themselves being sorted by their corresponding Gleason pattern in ascending order. Annotations that were provided later therefore override previous annotations of the same or lower Gleason patterns that share the same pixels.\nThe tree-structure of the ontology allowed us to create three datasets from our annotations (sub-explanations, explanations and Gleason patterns, see Figure 8), by remapping the sub-explanation annotations upwards in the ontology. This was deemed necessary for the model development, due to concerns of the sample size for the sub-explanations and the issue of class imbalance.\nFor both training and label analysis, we used merged grade images to review the complete grading and interpretations provided by the pathologists for the TMA core images."}, {"title": "Model Development", "content": "To develop a pathologist-like, inherently explainable Al system (Gleason XAI) for detecting Gleason patterns on TMA cores, we employed a concept bottleneck strategy 30, predicting the explanations directly for each pixel, with the ability to later remap them to their corresponding Gleason pattern. This provides inherent explainability by basing the decision for a Gleason pattern solely on the predictions of the associated explanations, which can in turn be verified by expert pathologists in contrast to black box decisions on the Gleason pattern. Inspired by recent positive results 50, we selected a U-Net 31"}]}