{"title": "Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and Localization", "authors": ["Vinaya Sree Katamneni", "Ajita Rattani"], "abstract": "In the digital age, the emergence of deepfakes and synthetic media presents a significant threat to societal and political integrity. Deepfakes based on multi-modal manipulation, such as audio-visual, are more realistic and pose a greater threat. Current multi-modal deepfake detectors are often based on the attention-based fusion of heterogeneous data streams from multiple modalities. However, the heterogeneous nature of the data (such as audio and visual signals) creates a distributional modality gap and poses a significant challenge in effective fusion and hence multi-modal deepfake detection. In this paper, we propose a novel multi-modal attention framework based on recurrent neural networks (RNNs) that leverages contextual information for audio-visual deepfake detection. The proposed approach applies attention to multi-modal multi-sequence representations and learns the contributing features among them for deepfake detection and localization. Thorough experimental validations on audio-visual deepfake datasets, namely FakeAVCeleb, AV-Deepfake1M, TVIL, and LAV-DF datasets, demonstrate the efficacy of our approach. Cross-comparison with the published studies demonstrates superior performance of our approach with an improved accuracy and precision by 3.47% and 2.05% in deepfake detection and localization, respectively. Thus, obtaining state-of-the-art performance. To facilitate reproducibility, the code and the datasets information is available at https://github.com/vcbsl/audio-visual-deepfake/.", "sections": [{"title": "1. Introduction", "content": "With advances in deep-generative models [41], synthetic audio and visual media have become so realistic that they are often indistinguishable from authentic content for human eyes. However, synthetic media generation techniques used by malicious users to deceive pose a serious social and political threat [25, 17, 58, 43, 33, 1, 50, 32].\nIn this context, visual (facial) deepfakes are generated using facial forgery techniques that depict human subjects with altered identities (i.e., face swapping), malicious actions (such as expression swapping), and facial attribute manipulation (such as skin color, gender, and age) [14, 44, 61]. Voice deepfakes, like facial deepfake technology, rely on advanced generative neural networks to synthesize audio that mimics the voice of a target speaker. Among them, Text-to-speech (TTS) voice deepfakes involve generating synthetic speech from text input that mimics a specific target speaker's voice [51]. Voice conversion-based deepfakes involve altering a person's voice to sound like another person while retaining the original content and linguistic style [54].\nAudio and visual deepfakes have been employed to attack authentication systems, impersonate celebrities and politicians, and defraud finance. As a countermeasure, several unimodal audio and visual deepfake detectors have been proposed [32, 23, 2, 39, 13, 24, 46]. Lately, multi-modal deepfakes that manipulate multiple modalities, such as audio-visual, to create highly convincing and immersive fake content have shown staggering growth with advanced multimedia processing and generative AI capabilities [65]. These advanced multi-modal deepfake techniques leverage the strengths of different modalities to generate more realistic and impactful results.\nExisting unimodal deepfake detectors are primarily designed to detect a single type of manipulation, such as visual, acoustic, and text. Consequently, multi-modal deepfake detectors are being investigated to detect and localize multi-modal manipulations, collectively. Within the scope of this work, several audio-visual deepfake detection and localization techniques have been proposed [48, 20, 26, 47, 27, 53, 57, 18, 36, 62, 19]. Deepfake detection aims at binary classification into pristine or deepfake. Localization aims to locate the start and end timestamps of manipulated audio-visual segments, thus facilitating a better understanding of deepfake detection results. Existing audio-visual deepfake detectors are often based on the fusion of heterogeneous streams using feature concatenation and employing attention mechanism [68, 47, 67, 37, 59, 5, 45]. Deepfake localization approaches are either anchor-based [21, 22] that utilize a sliding window approach to detect deepfake segments or boundary pre-"}, {"title": "2. Related Work", "content": "In this section, we discuss the work related to audio-visual multi-modal deepfake detection and localization.\nAudio-visual deepfake detection techniques employ audio and visual signals to detect multi-modal manipulation. A foundational work in this area, [28] assembled the FakeAVCeleb dataset consisting of audio and visual deepfakes and benchmarked various audio-visual deepfake detectors based on ensemble-based voting scheme and multi-modal convolutional neural network (CNN) based on feature concatenation.\nIn particular, most of the existing audio-visual deepfake detectors are based on attention mechanism-based feature concatenation [68, 67, 47, 52, 37, 59, 5, 45] to learn informative multimodal features for deepfake detection. Studies in [38, 42] used a siamese network architecture for emotion recognition from audio and visual cues incorporating contrastive loss. Deepfake videos are detected by analyzing discrepancies in emotional cues between audio and visual modalities. Studies in [15, 55, 35, 12, 66, 31, 36] explicitly model the disagreement between the embeddings of the multiple modalities using contrastive loss for deepfake detection.\nThe work in [3, 4] and [11] explores the mismatch between phonemes (distinct units of sound in speech) and visemes (the visual representation of phonemes) for deepfake detection. These studies specifically focus on inconsistencies in the lip region concerning the audio which deepfake generation approaches often"}, {"title": "3. Methodology", "content": "Our proposed framework aims to detect and localize deepfakes by focusing on the relationship between audio, visual and lip movement sequences by harnessing the information embedded within each modality and across their intersection.\nFor a given video D that contains N sequences, where each sequence di is composed of 3 different modalities (full visual face, lip region, and audio), the formulation can be represented as follows:\n$$D = \\{d_i\\}_{i=1}^{N}; d_i = \\{x_i^v, x_i^l, x_i^a\\}$$\nHere, each sequence di consists of three modalities represented by x for m \u2208 {v, l, a} for the full visual face, lip region, and audio modality.\nThe classification head detects the modified sequences and the regression head is employed to localize fake segments. For the video D with N sequences, the entire video D is classified as fake if at least one sequence is classified as fake.\nThe localization of the segments is represented as Y = {Y1, Y2, ..., YN\u0192 } where Nf are the number of fake segments. For a fake sequence i, yi represents the ith sequence output. Each instance yi = (si, ei) is defined by its starting time si and ending time ei of the sequence. Figure 2 illustrates the step-by-step process involved in our MMMS-BA which includes feature extraction and processing (as described below), and classification and regression heads for deepfake detection and localization. The following sub-sections provide details on each of these steps."}, {"title": "3.2. Multi-modal Multi-sequence - Bi-modal Attention (MMMS-BA) Framework", "content": "Sequences in a video represent the time series information and the classification of a sequence would have a relation with the other sequences. To model the relationship with the neighboring sequences and multiple modalities, we propose a recurrent neural network-based multi-modal attention framework named Multi-Modal Multi-Sequence Bi-modal Attention (MMMS-BA). Figure 2 shows the steps involved in applying the attention mechanism to the input sequences.\nThe MMMS-BA framework processes and analyzes multi-modal data across sequences in an input video. Assuming a particular video has N sequences, the raw sequence levels are represented as xx for the full visual face, x for the audio and x for the lip sequence from equation 1. Three separate Bi-GRU layers with forward and backward state concatenation are first applied to the full visual face xi, audio xi, and lip sequence x representations followed by the fully connected dense layers, resulting in L (lip region), V (full visual face), and A (audio) embeddings. Finally, pairwise attentions are computed on various bi-modal combinations of three modalities - (V, L), (L, A) & (A, V) as explained in the section 3.2.1."}, {"title": "3.2.1 Bi-modal Attention", "content": "Modality representations of V & L are obtained from the Bi-GRU network and hence contain the contextual information of the sequences for each modality. Figure 1 in the Appendix provides additional details on the computation of bi-modal attention for each modality pair. At first, we compute a pair of matching matrices M1 and M2 over two representations that account for the cross-modality information.\n$$M\u2081 = VL^T and M2 = LV^T$$"}, {"title": "3.2.2 Multi-sequence Attention", "content": "As mentioned earlier, we aim to leverage the contextual information of each sequence for the prediction. The probability distribution scores (K1 and K2) are computed over each sequence of bi-modal attention matrices M\u2081 and M2 (refer to equation 2) using a softmax function. This essentially computes the attention weights for the contextual sequences. Finally, soft attention is applied over the multi-modal multi-sequence attention matrices to compute the modality-wise attentive representations, i.e., O1 and O2 explained below.\n$$K_1(i, j) = \\frac{e^{M_1(i,j)}}{\\sum_{k=1}^{N} e^{M_1(i,k)}}  for i, j = 1, 2, .., N$$"}, {"title": "3.2.3 Multiplicative Gating and Concatenation", "content": "A multiplicative gating function is computed between the multi-modal sequence-specific representations of each modality 01 & O2 (refer equation 5) and the corresponding bi-modal pair. This element-wise matrix multiplication assists in attending to the important components of multiple modalities and sequences. Attention matrices A1 & A2 are then concatenated to obtain the pairwise attention between V and L.\n$$A1 = 01 V and A2 = O2 L$$\n$$MMMS-BAVL = concat[A1, A2]$$\nSimilar to MMMS-BAVL in equation 7, we follow the same procedure to compute MMMS-BAAV, and MMMS-BAAL. Finally, motivated by the residual skip connection network, the pairwise attention representations MMMS-BAVL, MMMS-BAAV, and MMMS-BAAL are concatenated with the individual modalities (V, A, and L) to increase gradient flow to the lower layers. This concatenated feature vector W (see Figure 2) is then used for deepfake detection."}, {"title": "3.3. Localization", "content": "For the sequences classified as fake, their corresponding timestamp in the input video are localized as the fake segments. The localization of the segments is represented as Y = {Y1, Y2, ..., YN\u0192 } where Nf are the number of fake segments. For a fake sequence i, yi represents the ith sequence output. Each instance yi = (si, ei) is defined by its starting time si and ending time ei of the sequence. These segments are further processed using Non-Maximum Suppression (NMS) [7] to remove highly overlapping instances, leading to the final localization timestamps."}, {"title": "3.4. Loss Function", "content": "The model's overall learning process involves minimizing the combined loss as follows:\n$$L = \\sum(L_{cls} + \\lambda_{reg}1_{ci}L_{reg})/N_f,$$\nwhere Nf is the total number of fake sequences. 1c is an indicator function that denotes if a sequence i is fake with value equal to 1 otherwise 0. L is applied and averaged in all sequences during training. Areg is a coefficient that balances classification and regression loss. We set Areg = 1 by default.\nNotably Lcls uses focal loss [34] to classify sequences as real or fake. Lreg adopts a differentiable IoU loss from [49]. Lreg is only enabled when the current sequence is fake."}, {"title": "4. Experimental Validations", "content": "Evaluation of the proposed method is conducted on publicly available audio-visual AV-Deepfake1M [8], FakeAVCeleb [30], LAV-DF [10], and TVIL [64] deepfake datasets. Table 1 provides details on the datasets used in this study. More details about the datasets are available in the Appendix, section 1."}, {"title": "4.2. Implementation Details", "content": "For the deepfake detection task using MMMS-BA, our model architecture includes bidirectional GRUs with 300 neurons each, followed by a dense layer comprising 100 neurons. This dense layer ensures that the input features from all three modalities, i.e., full visual face, lip sequences, and audios are projected to the same dimensions, facilitating cohesive integration of information. Dropout regularization is applied with a rate of 0.3 across all layers, including the Bi-GRU layers. ReLU activation functions are utilized in the dense layers, while softmax activation is employed in the final classification layer, and ReLU (to ensure nonnegative values for start and end timestamps) is employed with Differential Intersection over Union (DIoU) loss, which measures the accuracy of the predicted timestamps against the ground truth.\nThe hyperparameters, including activation functions and dropout probabilities, were selected through a grid-search process using the validation set. The Adam optimizer with an exponential decay learning rate scheduler is used for optimization, and an"}, {"title": "4.3. Performance of Deepfake Detection", "content": "Tables 2 and 3 summarize the performance of our MMMS-BA model on intra- and cross-dataset evaluation when trained on FakeAVCeleb and AV-Deepfake1M datasets. For the AV-Deepfake1M dataset, training and validation sets are available, and the testing set has not yet been released. In this regard, we have used the training set for both training (85%) and validation (15%). While the actual validation set is used for testing the trained mod-"}, {"title": "4.4. Comparison with the Published Audio-Visual Deepfake Detectors", "content": "The comparison shown in Table 4 highlights the competitive landscape of deepfake detection models trained and evaluated on the FakeAVCeleb dataset. As can be seen in Table 4, the MMMS-BA model, which uses the full facial image, audio, and lip modality, outperforms all existing audio-visual deepfake detectors by obtaining an AUC of 0.989 and an ACC of 0.979. In particular, our MMMS-BA model outperforms NPVForensics [11] which also utilizes face, audio, and lip movement data. NPVForensics is based on mining the correlation between non-critical phonemes and visemes using a Swin Transformer and cross-modal fusion, but it achieves lower performance over MMMS-BA due to its limited ability to capture temporal dependencies and interactions across multiple sequences. The Unsupervised Cross-Modal Inconsistencies [57] model obtained the second-best performance, with an AUC of 0.968. This also suggests the potential of leveraging motion inconsistencies between modalities for deepfake detection.\nIn summary, MMMS-BA outperforms all recently published work based on integrating audio, visual, and lip-movement data with an average performance increment of 0.0341 and 3.47% in AUC and ACC, respectively. Thus, obtaining state-of-the-art performance."}, {"title": "4.5. Deepfake Localization Performance", "content": "Table 5 shows the performance evaluation of the encoder-decoder (Enc-Dec), ActionFormer [63], BA-TFD+ [9], UM-MAFormer [64] and MMMS-BA based deepfake localization approaches when trained on AV-Deepfake1M and tested on LAV-DF, TVIL, and AV-Deepfake1M datasets. The Enc-Dec approach represents a sequence-to-sequence encoder-decoder-based architecture employed considering audio and visual modalities. For Enc-Dec, audio and visual encoders are used, and the hidden representations from these encoders are concatenated together, followed by two dense layers, and the output classification layer provides sequences. The start and end timestamps of the fake sequences are the fake segments in a video. The results for methods ActionFormer, BA-TFD+, and UMMAFormer reported in Table 5 are taken from the original papers [9, 64, 8]. Since the evaluation is done on the same datasets used in our study, it is a fair comparison.\nThe Enc-Dec model obtained the lowest performance across all datasets and metrics. This is expected as it is based on simple feature concatenation from audio-visual streams and does not include any advanced processing techniques like attention mechanism. On AV-Deepfake1M, the MMMS-BA model has obtained the best performance with AP@0.5 and AR@50 of 62.75 and 57.49, respectively. UMMAFormer is the second best model obtaining AP@0.5 and AR@0.5 of 51.64 and 48.86, respectively. MMMS-BA has obtained better performance on the AV-Deepfake1M dataset when compared to the best model in [8]. However, the lower performance observed on the AV-Deepfake1M dataset, although it was used as a training set, underscores the highly realistic fake content generated in a content-driven manner, altering the real transcripts with replace, delete, and insert operations and the corresponding audio-visual modalities accordingly.\nWhen evaluated on the LAV-DF dataset, MMMS-BA obtained the best performance with an AP@0.5 and AR@50 of 97.56 and 93.45, respectively. BA-TFD+ has obtained the second-best performance with an AP@0.5 and AR@50 of 96.30 and 80.48, respectively. Although BA-TFD+ showed a smaller performance difference compared to MMMS-BA at lower threshold levels, it exhibited a larger performance difference at higher thresholds, indicating a less precise localization.\nOn the TVIL dataset, the MMMS-BA model demonstrated the best performance with an AP@0.5 of 96.87 and an AR@50 of 90.47. The UMMAFormer model followed closely with an AP@0.5 of 88.68 and an AR@50 of 90.43, and it achieved the highest scores for AP@0.75 (84.70) and AP@0.95 (62.43). The Enc-Dec model had the lowest performance with an AP@0.5 of 23.08 and an AR@50 of 43.98 due to its ineffective use of the available modality information.\nIn summary, the MMMS-BA model consistently demonstrated the best localization capability across all datasets. This is due to the utilization of contextual information across the sequences over existing methods, resulting in better robustness in deepfake localization."}, {"title": "5. Ablation Study", "content": "An ablation study is conducted by varying the attention mechanism in the contextual cross-modal attention block in Figure 1 resulting in two other variations, i.e., Multi-Modal Uni-Sequences Self-Attention (MMUS-SA) and Multi-Sequence - Self-Attention (MS-SA). We also ablated between the bi-modal combination of the modalities considered, that is, the full face, voice, and lip region. The details given below."}, {"title": "5.1. Varying the Attention at Sequence and Modality Level", "content": "Multi-Modal Uni-Sequences - Self Attention (MMUS-SA) Framework: MMUS-SA framework does not account for information from other sequences at the attention level but utilizes multi-modal information of a single sequence for prediction. For more details on the calculation of attention in the MMUS-SA framework refer to the Appendix, section 2.\nMulti-Sequence - Self Attention (MS-SA) Framework: In the MS-SA framework, we apply self-attention to the sequences of each modality separately and use these for classification. For more details on the calculation of attention in the MS-SA framework refer to the Appendix, section 2."}, {"title": "5.2. Performance of MMMS-BA on Different Combination of Modalities", "content": "In this study, we ablated between the combination of modalities (V-full visual face, L-lip sequence, and A-audio) and evaluated the performance of MMMS-BA when trained and tested on FakeAVCeleb. Table 7 illustrates the role of different modalities in enhancing detection accuracy. As can be seen, the combination of visual and lip (V + L) obtained the lowest performance with an AUC of 0.814. This result can be attributed to the absence of audio data, which limits the model to visual information alone. The combination of lip and audio (L + A) obtained a performance improvement over (V + L) with an increase in AUC of 0.11. This enhancement suggests that incorporating audio data alongside lip sequences provides complementary information, enhancing the model's performance. The combination of visual and audio (V+A) obtained further improvement over (V+L and L+A) with an AUC of 0.955. This integration demonstrates that including audio features with full-face visual data offers additional insights, thereby boosting detection accuracy beyond the (V + L) and (L + A) combinations. The best performance is obtained by combining (V+L+A) with an average increment in AUC of 0.091.\nIn summary, this analysis suggests that incorporating lip sequence data along with (V+A) contributes significantly to the effectiveness of the model. Full-face images encompass a complex mix of facial features, making it challenging for the model to isolate and differentiate inconsistencies in the lip region effectively. Thus, the additional lip modality enriched the information available to the model, ultimately improving its discriminatory power and performance, also supported by the NPVForensics model [11] for audio-visual deepfake detection."}, {"title": "6. Conclusion and Future Work", "content": "With the rapid evolution of deepfake techniques combining synthesized audio with forged videos, there is an urgent need for robust audio-visual deepfake detection and localization techniques. Current audio-visual deepfake detection approaches relying on fusing audio and visual streams employing basic attention mechanisms, overlook intricate inter-modal relationships crucial for accurate detection. To address this challenge, we introduced the MMMS-BA framework. This framework effectively captures intra- and inter-modal correlations by applying attention to multimodal multi-sequence representations and learns the contributing features among them for effective deepfake detection and localization Our experimental findings demonstrate that MMMS-BA outperforms existing audio-visual deepfake detectors, achieving SOTA performance in detecting and localizing deepfake segments within videos.\nGiven the proliferation of AI-generated content using sophisticated generative models, tackling emerging forms of content manipulation remains a critical challenge. Therefore, as part of future research directions, we will extend our framework to incorporate text analysis along with audio and visual modalities to ensure ro-"}, {"title": "7. Supplementary Materia", "content": "Evaluation of the proposed method is conducted on publicly available audio-visual AV-Deepfake1M [8], FakeAVCeleb [30], LAV-DF [10], and TVIL [64] deepfake datasets mentioned in Section 4.1 of the main paper. Further, details on these datasets are given below.\n\u2022 FakeAVCeleb: The FakeAVCeleb dataset [30] is a colle-ction of videos with audio and visual manipulations of celebri-ties that have been generated using various deepfake tech-niques. The dataset is created by selecting videos from the VoxCeleb2 [16] dataset, featuring 500 celebrities. The videos in this dataset are clean, featuring only one person's frontal face without any occlusion. The dataset is well-balanced and annotated in terms of gender, race, geogra-phy, and visual and audio manipulations, making it useful for training deep learning models that can generalize well on unseen test sets. We chose this dataset for our multimodal detection experiments because it contains both audio and vi-sual manipulations, as well as a variety of deep-fake genera-tion techniques.\n\u2022 LAV-DF Dataset The Localized Audio Visual DeepFake (LAV-DF) [10] dataset emerges as a critical asset in deepfake detection, particularly for benchmarking methods to detect and localize manipulated segments within videos. Comprising 136,304 videos across 153 unique identities, the dataset offers a diverse collection that includes 36,431 real videos alongside 99,873 videos embedded with fake segments. For a complete evaluation, LAV-DF is divided into three identity-independent subsets: training (78,703 videos), validation (31,501 videos), and testing (26,100 videos), each offering a distinct set of identities to ensure an unbiased assessment.\n\u2022 TVIL Dataset In response to the increasing challenges posed by advanced AI-generated content (AIGC) technologies, the TVIL dataset [64] has been synthesized as a new benchmark aimed at locating video inpainting segments. Constructed upon the foundation of YouTubeVOS 2018 [60], which aggregates over 4,000 videos from YouTube, the TVIL dataset leverages one of the most prolific platforms for video content as its base. This choice is strategic, considering YouTube's prominence in content generation and the spread of misinformation. By synthesizing a dataset rooted in YouTube videos, TVIL is poised to offer a robust evaluation framework that defends against misinformation and catalyzes new research directions in the fight against digital content forgery.\n\u2022 AV-Deepfake1M Dataset The AV-Deepfake1M [8] dataset stands as a pioneering contribution to the field of audio-visual deepfake detection, encompassing an extensive compilation of 1,886 hours of audio-visual data curated using content-driven manipulation techniques. from 2,068 unique"}, {"title": "7.2. On Varying Attention Mechanism", "content": "This section provides more details on the calculation of attention in MMUS-SA and MS-SA variations discussed in Section 5.1 of the main paper.\n\u2022 Multi-Modal Uni-Sequences - Self Attention (MMUS-SA) Framework: MMUS-SA framework does not account for information from other sequences at the attention level, rather it utilizes multi-modal information of a single sequence for prediction. For a video D having 'N' sequences, 'N' separate attention blocks are needed, where each block computes the self-attention over multi-modal information of a single sequence. Let Xp be the information matrix of the pth sequence where the three 'r' dimensional rows are the outputs of the dense layers for the three modalities. The attention matrix Ap is computed separately for p = 1st, 2nd, ... Nth sequences. Finally, for each sequence p, Ap and Xp are concatenated and passed to the output layer for classification.\n$$M_p = X_p X_p^T$$\n$$N_p(i, j) = \\frac{e^{M_p(i,j)}}{\\sum_{k=1}^{3} e^{M_p(i,k)}}  for i, j = 1, 2, 3;$$\n$$O_p = N_p. X_p$$\n$$A_p = O_p X_p$$\nThe attention matrix Au is computed separately for p = 1st, 2nd,..., Nth sequences. Finally, for each sequence p, Ap and Xp are concatenated and passed to the output layer for classification.\n\u2022 Multi-Sequence - Self Attention (MS-SA) Framework: In the MS-SA framework, we apply self-attention to the sequences of each modality separately and use these for classification. In contrast to the MMSS-SA framework, MS-SA utilizes the contextual information of the sequences at the attention level. Let L (text), V (visual), and L (lip region) be the outputs of the dense layers. Three separate attention blocks are required for the three modalities, where each block takes multi-sequence information of a single modality and computes the self-attention matrix. Attention matrices \u0391\u03b9, \u0391v, and Aa are computed for lip, visual, and acoustic, respectively. Finally, \u0391\u03c5, \u0391\u03b9, Aa, V, L, and A are concatenated and passed to the output layer for classification.\n$$M = V. V^T$$\n$$N_v(i, j) = \\frac{M_v (i,j)}{\\sum_{k=1}^{v} M_v (i,k)} for i, j = 1,..., u$$"}]}