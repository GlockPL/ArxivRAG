{"title": "Scaling 4D Representations", "authors": ["Jo\u00e3o Carreira", "Dilara Gokay", "Michael King", "Chuhan Zhang", "Ignacio Rocco", "Aravindh Mahendran", "Thomas Albert Keck", "Joseph Heyward", "Skanda Koppula", "Etienne Pot", "Goker Erdogan", "Yana Hasson", "Yi Yang", "Klaus Greff", "Guillaume Le Moing", "Sjoerd van Steenkiste", "Daniel Zoran", "Drew A. Hudson", "Pedro V\u00e9lez", "Luisa Polan\u00eda", "Luke Friedman", "Chris Duvarney", "Ross Goroshin", "Kelsey Allen", "Jacob Walker", "Rishabh Kabra", "Eric Aboussouan", "Jennifer Sun", "Thomas Kipf", "Carl Doersch", "Viorica P\u0103tr\u0103ucean", "Dima Damen", "Pauline Luc", "Mehdi S. M. Sajjadi", "Andrew Zisserman"], "abstract": "Scaling has not yet been convincingly demonstrated for pure self-supervised learning from video. However, prior work has focused evaluations on semantic-related tasks action classification, ImageNet classification, etc. In this paper we focus on evaluating self-supervised learning on non-semantic vision tasks that are more spatial (3D) and temporal (+1D = 4D), such as camera pose estimation, point and object tracking, and depth estimation. We show that by learning from very large video datasets, masked auto-encoding (MAE) with transformer video models actually scales, consistently improving performance on these 4D tasks, as model size increases from 20M all the way to the largest by far reported self-supervised video model \u2013 22B parameters. Rigorous apples-to-apples comparison with many recent image and video models demonstrates the benefits of scaling 4D representations.", "sections": [{"title": "1. Introduction", "content": "Self-supervised learning of representations from video took the back seat in recent years to language-supervised approaches deriving from CLIP. Is the perceived superiority of language supervision because it is a better way to learn? Or may it have to do with dominant evaluations being also language-based such as action classification and localization? Do we not yet have sufficiently powerful self-supervised learning techniques in our repertoire? Or just need to throw more hardware and data at them?\nIn this paper, we attempt to answer these questions by revisiting self-supervised learning while looking at performance on five mostly semantics-free tasks: depth estimation, point and object tracking, camera pose estimation, and Something-something v2 [35], an object-agnostic action classification task (this one is more semantic but has been shown to be great at singling out models with good temporal representations). These are tasks that require strong spatial and temporal abilities \u2013 they are less about naming things, more about recovering the 3D geometry over time of (4D) scenes. Making an analogy with human vision, we are interested in inspecting dorsal as opposed to ventral stream skills."}, {"title": "2. Related work", "content": "Origins. Video is a privileged medium for unsupervised learning a high-bandwidth connection to the world, providing multiple viewpoints of the same objects through time, across lighting changes, deformation and so on. Much early work in self-supervised learning in computer vision focused on video, for example by leveraging temporal continuity signals [32, 48, 76], later boosted by object tracking information [72], motion segmentation [54], camera pose between frames [1], audio [3\u20135, 45, 49, 52, 56, 63], future prediction [34, 61, 69], contrastive learning under augmentation [37, 38, 41, 57] and regression-based approaches [30, 62]. Context prediction gave strong results as supervision for text [16], and soon proved effective for visual representation learning [20, 40, 53]. With the rise of Vision Transformers [24], MAE became a particularly efficient formulation of context prediction [39], since masked patches can be skipped by the encoder.\nScaling. Video versions of masked modeling achieved strong results [8, 66] but the excitement was not long-lasting as attempts to scale models like MAE have yet to succeed [42]. In parallel, CLIP [59] took over, enabling learning from billions of labels from the web, and this translated well also to mainstream video tasks [73, 80] such as classification [43] and action localization [12] (which are defined using words as labels). Models evaluated on the basis of their features (e.g. representations) have been convincingly scaled using language supervision [19] for images. Attempts to scale from just pixels resulted in more modest improvements [13, 26]. In the video space, GPT-like [58] vision and language video models have demonstrated clear benefit from scaling [2]. Pure video self-supervised models have not yet been scaled anywhere near to comparable scales, with some of the biggest models having around 1B parameters [70], or 3B for auto-regressive models [7] \u2013 this is something now explored with this paper. Related to our approach, VideoMAE v2 [70] scales a video MAE model, but focuses solely on action recognition benchmarks. They observe only small performance improvements when scaling from ViT-H to ViT-g, and therefore do not scale models beyond the billion parameter regime.\nDorsal streams and evaluation. The dorsal stream arises in the two-streams hypothesis [31, 33, 47], that the human visual cortex contains two pathways: the ventral stream (which performs object recognition) and the dorsal stream (which is sensitive to transformations and motion). It has been the (indirect) inspiration for several of the early architectures for video representation [28, 29, 64]. Several recent papers studied representations on 3D tasks [25, 79], with results pointing to shortcomings in models solely supervised"}, {"title": "3. Methodology", "content": "Our goal is to re-evaluate a large set of strong models apples-to-apples on 4D representation tasks that focus on geometry and motion. To achieve this we follow a representation learning framework where pretrained models get appended with attention-based readouts on top and are then separately finetuned for each of the various downstream datasets / tasks, with and without freezing the backbone. In our experiments detailed in Sec. 4 we found MAE to do well and to be efficiently trainable so we scaled ViT models in that framework \u2013 details on scaling are given in the next subsection, followed by the overall evaluation protocol including readout architectures and finally a list of baselines."}, {"title": "3.1. 4DS: scaling MAE of video", "content": "MAE works by feeding a subset of patches from a video to the video encoder that we want to learn. The features out of the encoder are fed to a decoder that must produce back the missing patches. The decoder is often the expensive part because it needs to produce many more patches (in this paper 95%) than the encoder consumes (in this paper 5%). We employ a simple version of video MAE, hoping that the scale of the models and the data lead to high performance by itself.\nWe use MAE to train a family of 7 models, that we name 4DS, spanning 20M to 22B parameters and listed in Tab. 1. We use input space-time patch size 2x16x16 for all models.\nData. We pretrain 4DS models using a dataset of 170 million web videos, each 900 frames long on average, at orig-inal resolution and frame rate. We sample batches of 16 frame clips from these videos, using temporal stride 2 and randomly cropping down to 224x224 (models up to 4B parameters) or 256x256 resolution (22B parameter models) after resizing the smallest side to 1.15x those resolutions.\nDecoding. For efficiency and flexibility reasons, we do not use a completely separate decoder with different architectural details, unlike in previous popular MAE approaches [39, 66]; instead the last few self-attention blocks of the encoder share some of the decoding burden but otherwise decoding back to video space is patchwise linear. The process is illustrated in Fig. 2: we concatenate a learned grid of latent tokens to the unmasked set of tokens before one of the last self-attention blocks. We then decode these extra tokens linearly and patchwise, by mapping each to an appropriate number of channels such that they can be reshaped back to the original video dimensions. In downstream tasks, the latent tokens are not used and features can be extracted from any layer. While for models up to 4B parameters we use a decoding grid of tokens that mirrors the original input video grid (8 \u00d7 14 \u00d7 14 = 1568 (8 for time, 14x14 for space) tokens for 16 frames at 224x224 resolution), for 22B we employ a much more efficient coarser grid (4\u00d78\u00d78 = 256 tokens for 16 frames at 256x256 resolution). We reconstruct all space-time patches, both masked and unmasked.\nLearning. We mask in all cases randomly 95% of the"}, {"title": "3.2. Evaluation protocol", "content": "We followed an evaluation protocol designed to fairly compare arbitrary backbones over multiple tasks in the same setting. We tried both end-to-end finetuning and frozen evaluation, which puts more emphasis on the quality of the pretraining and is much cheaper computationally. For each task we add a trainable attention-based readout module on top of a pretrained video encoder then measure the resulting performance on the validation split. Compared to linear readouts, attention-based ones offer much better performance on top of frozen features (sometimes close to full finetuning) while not increasing parameter count significantly and have become preferred recently [8] The input to the model is a 16-frame clip which is preprocessed (e.g. resampled) to match each model's preferred input format, then the output features are of shape T \u00d7 K \u00d7 C where T is a time dimension, K is the number of spatial tokens per time step and C is the number of channels for that model. We resample T to be 16 as required by some of the task readout modules requirements. A T \u00d7 K \u00d7 C-shaped learnable positional embedding is added to the features in order to not disadvantage image encoders. Readouts are trained using 1.28M examples and AdamW [46]. No masking, noising, etc. is applied to the videos on any of the tasks \u2013 additional details are given in the supplementary material.\nTasks. The five tasks we consider are shown in Fig. 3 and capture different aspects of spatial and temporal scene understanding: (a) point tracking on Perception Test [55] measures fine-grained, temporally extended motion perception; (b) action classification in SSv2 [35] is well known to require temporal sensitivity (e.g. shuffling frames leads to very poor performance), (c) object tracking from bounding boxes in the Waymo Open Dataset [65] emphasizes object-level motion. We also include two tasks that connect with 3D perception: (d) 6D camera pose estimation on the Real Estate 10K dataset (RE10k) [67, 81] and (e) monocular depth estimation in ScanNet [17]. All of the tasks are defined in the literature except for the RE10k task for which we introduce a custom metric as well \u2013 we give full details about all the tasks including this new one in the supp. material.\nAttention-based readouts. There are several common strategies for evaluation of representation learning methods on downstream tasks, particularly with frozen backbones, ranging from non-parametric (nearest-neighbor) [50], linear or using cross-attention layers [8, 66, 80]. In this work we use cross-attention-based readout heads for all tasks: for both SSv2 and RE10k a single learned query is used as in [8], employing an additional step of Procrustes-based projection [11] back into SO(3) for RE10k. For ScanNet we use Fourier-based spatial cross-attention queries, one for each 2x8x8 patch. For the tracking tasks we employ one query per object track for Waymo Open, and one query for each 2-frame temporal chunk of a point track for Perception Test (so 8 queries per track for 16 frame clips). Full details of readout head architectures and hyper-parameters are provided in supp. material."}, {"title": "3.3. Baselines", "content": "For baselines we include some of the strongest and largest image and video models available we detail them below, see Tab. 2 for a quick reference.\nImage models. We evaluate SigLIP-2B which is part of PaLI-3 [15] and a follow-up of SigLIP [78]. This is a large 2B vision transformer model trained from image-text pairs via a binary classification formulation. We also evaluate another extremely powerful and popular image model \u2013 Dino-V2 [50] in two versions, VIT-L and VIT-g, with 303M and 1.1B parameters, respectively. Dino-V2 is trained very differently and quite complementary to SigLIP-2B, using a self-distillation loss from images only (no text). SigLIP-2B is one of the strongest descendants of CLIP, Dino-V2 of SimCLR [14]. Both models are trained on large web-scale image datasets.\nNote again that we add learnable temporal positional embeddings to the features produced by image models so they are not in principle disadvantaged when task readouts are trained on top in the frozen evaluation setting.\nVideo models. We evaluated VideoPrism [80], a state-of-the-art video model combining two training objectives \u2013 a) contrastive learning using video and language and b) video-only masked modelling. After the contrastive phase, a student model is trained to predict embeddings from masked frames similar to the predictions of the contrastively trained teacher from unmasked frames. VideoPrism is based on the ViViT architecture [6], with space and time factorization and we tested two versions: B and g with 100M and 1B parameters, respectively.\nWe also evaluated two types of popular video-only self-supervised models: VideoMAE [66, 70] and V-JEPA [8]. They are both trained on masked videos - VideoMAE is tasked with predicting the pixels, while V-JEPA predicts features encoded by a teacher network. Both models use a ViT backbone and tokenize videos into patches of size (2, 16, 16), applying self-attention between all of the tokens. We evaluate different variants of VideoMAEv1, Video-MAEv2 and V-JEPA, ranging from ViT-B with 30M parameters to ViT-g with 1B parameters (the largest that exist as far as we know)."}, {"title": "4. Results", "content": "Our first experiments aimed to understand how the strong models listed in Sec. 3.3 fare on the 4D representation tasks described in Appendix A under the frozen evaluation protocol, with the same trainable attention-based readouts, same limited amount of training. Note that some of the results are lower than in papers without these constraints, which train longer (for example for SSv2), do test-time augmentation or use specialized readouts; this is to be expected and is by design. The results, provided in the next subsection, highlighted VideoMAE and V-JEPA as robust approaches; we pursued MAE because of its better training efficiency (it uses a fixed rate of token dropping, which is more friendly on TPUs / with batching and does not require an EMA network), for the 4DS models."}, {"title": "4.1. Assessing video models 4D representations", "content": "Results for SSv2 action classification are reported by Top-1 accuracy as percentage; RE10k camera pose estimation uses Mean End Point Error (explained in supp. material, lower is better and minimum value is 0); Perception test point tracking uses Average Jaccard (higher is better, range 0-1); ScanNet depth prediction uses Absolute Relative Error (lower is better, minimum value is 0) and Waymo Open object tracking uses mean IoU (higher is better, range 0-1). We report accuracy and Average Jaccard in tables as percentage for better readability.\nTable 2 shows the quantitative results of the selected models - image models in first three rows, then video models from the literature and finally our models at the bottom.\nImage backbones. An interesting initial finding was that image backbones perform poorly regardless of model size on all tasks with the exception of depth estimation on ScanNet - likely because monocular cues dominate in short clips just over 1s long, where parallax is naturally limited and also where there are no moving objects. Dino does better than SigLIP across all tasks, hinting that CLIP-style representations may not be a good fit for 4D tasks. Finally both models are terrible at camera pose estimation.\nVideo backbones. Text-supervised VideoPrism models show strong performance on SSv2 action classification and depth estimation, but are not as good on the tracking tasks and are quite poor on camera pose estimation. V-JEPA models shine on SSv2 and exhibit overall strong performance. Compared to the smaller v1, VideoMAE-v2 improves classification performance but at the expense of all other tasks.\n4DS backbones. Our scaled up models, 4DS-G, e and j - explained in Sec. 3.1 significantly improve over all the baselines, achieving top results on all tasks except SSv2 (only the 22B model 4DS-j is close to V-JEPA-H). The advantage is particularly noticeable when taking all tasks together as shown in Fig. 6.\nWhich layer is best for each task? As mentioned in the technical section, for 4DS we did not force a clear separation between encoder and decoder (except for a patchwise linear decoder). We investigated where in the model are the best representations for each task by trying a subset of 6 readout layers for each 4DS model at different model depths \u2013 25%, 50%, 75%, 85%, 95% and 100% (e.g. 75% corresponds to layer 42 out of 56 for 4DS-e). Fig. 7 shows that"}, {"title": "5. Conclusions", "content": "We have re-evaluated several state-of-the-art image and video models, trained with various types of supervision including language, on a set of mostly semantic-free tasks requiring strong 4D representations. We found that many of the improvements found in the respective papers do not translate well to these more geometrical / temporal scene understanding tasks and simple MAE from video still does quite well.\nWe then demonstrate that scaling MAE beyond what has been done in the literature (~1B parameters), all the way to 22B parameter models brings consistent improvement \u2013 we have a new collection of model checkpoints we call 4DS which may be useful for the community. This may open new avenues for further progress in vision similar to those that have been explored successfully in language models research.\nLimitations. In this paper we used the most vanilla version possible of MAE \u2013 many improvements have been reported in the literature at small scale. As future work it would be interesting to identify which of these variations are most practical and fruitful when scaling, or if other learning approaches are superior. Another limitation of the paper is that we did not produce \"scaling laws\" \u2013 but arguably provided evidence that this may soon be warranted."}]}