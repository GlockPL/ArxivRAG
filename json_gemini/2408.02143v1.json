{"title": "Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion Survey", "authors": ["Shiran Dudy", "Ibrahim Said Ahmad", "Ryoko Kitajima", "Agata Lapedriza"], "abstract": "Large Language Models (LLMs) have gained widespread global adoption, showcasing advanced linguistic capabilities across multiple of languages. There is a growing interest in academia to use these models to simulate and study human behaviors. However, it is crucial to acknowledge that an LLM's proficiency in a specific language might not fully encapsulate the norms and values associated with its culture. Concerns have emerged regarding potential biases towards Anglo-centric cultures and values due to the predominance of Western and US-based training data. This study focuses on analyzing the cultural representations of emotions in LLMs, in the specific case of mixed-emotion situations. Our methodology is based on the studies of Miyamoto et al. (2010), which identified distinctive emotional indicators in Japanese and American human responses. We first administer their mixed emotion survey to five different LLMs and analyze their outputs. Second, we experiment with contextual variables to explore variations in responses considering both language and speaker origin. Thirdly, we expand our investigation to encompass additional East Asian and Western European origin languages to gauge their alignment with their respective cultures, anticipating a closer fit. We find that (1) models have limited alignment with the evidence in the literature; (2) written language has greater effect on LLMs' response than information on participants origin; and (3) LLMs responses were found more similar for East Asian languages than Western European languages.", "sections": [{"title": "I. INTRODUCTION", "content": "With the widespread adoption of Large Language Models (LLMs), like ChatGPT, there has been an increased interest in understanding how different cultures are represented in LLMs [1], [2]. Understanding cultural representations in LLMs is crucial for many reasons, such as ensuring that communication tools based on LLMs are inclusive (able to deal with different perspectives and values) and effective (able to interpret and respond appropriately in different cultural contexts). In turn, emotions play a significant role in communication, since they have a strong influence on how messages are expressed and perceived [3]. This has motivated recent works on understanding emotional skills in LLMs [4]\u2013[7].\nThis paper explicitly studies cultural representations of emotions in LLMs. Concretely, we focus on comparing emotion representations of Western and East Asian cultures, by studying the emotional responses of LLMs to various mixed-emotion situations. Our research builds upon the Mixed Emotion Experiment conducted by Miyamoto et al. (2010) [8]. In their work, the authors designed a survey describing 13 situations of mixed emotions, where the participants had to rate how they would feel in each of the situations. The survey was answered by North American participants and Japanese participants, in their respective languages, and the results show interesting differences between the two studied populations (more details on the Mixed Emotion Experiment [8] are provided in III-A). In our work, we utilize the same emotional response survey as in [8] in both Japanese and English languages to investigate whether the responses generated by various LLMs align with the findings observed in the human experiments conducted by Miyamoto et al. (2010) [8]. Our work is closely related to the work by Havaldar et al. (2023) [9], who investigated culturally aware emotional responses in LLMs. This work investigates the mixed emotion phenomenon, rather than the presence of universal emotions in LLMs.\nMore generally, our study addresses three research questions:\n\u2022 RQ1: To what extent the findings in [8] are reproduced with LLMs?\n\u2022 RQ2: What is the effect on LLMs' response when prompted with different sources of contextual information?\n\u2022 RQ3: How similar are the responses of LLMs in languages with higher cultural affinity?\nTo address these research questions we perform 3 studies, which are presented in Sect. IV. Study 1 compares how the responses to the survey differ when the LLMs are prompted in English vs. Japanese. It also studies to what extent the results obtained by LLMs align with the results obtained by Miyamoto et al. [8] on their experiments with human subjects. Study 2 analyzes the effect of adding different contextual sources to the prompt. For example, prompting in English language but adding to the prompt the textual information Please rate [the survey] as a Japanese participant. Finally, study 3 compares the responses across different languages, including 4 Western languages (English, Spanish, German, and French) and 4 East Asian languages (Japanese, Chinese, Korean, and Vietnamese).\nOur studies are conducted on 5 widely used LLMs: three open source models (mistral-7b-instruct [10], gemma-"}, {"title": "II. RELATED WORK", "content": "A. Evaluating emotional skills of Large Language Models\nSome recent studies have attempted to evaluate and quantify emotional skills in Large Language Models (LLMs). For example, Schaaff et al. 2023 [4] investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional expressions. In the experimentation, ChatGPT was instructed to rephrase neutral sentences into six emotional sentences of joy, anger, fear, love, sadness, and surprise. The responses of ChatGPT were then assessed using five standardized questionnaires. This study demonstrated promising results in ChatGPT's ability to convey multiple emotions using the same base sentence.\nTran et al. [5] introduce a method to assess robustness and bias in emotion recognition systems. They combine GPT-3 and rule-based constraints to create text variations for different language proficiency levels. These texts are used to examine performance biases, focusing on language proficiency, and investigate model-agnostic effects using the COSMIC model [6] and benchmark datasets.\nFinally, Broekens et al. [7] explore how ChatGPT can perform affective computing tasks using prompting alone. The methodology involves conducting conversational experiments with ChatGPT to explore its fine-grained affective processing capabilities. The paper used a rule-based logical model of appraisal as a prompt based on the OCC model [13] to assess if ChatGPT can predict emotions according to a specific framework. They also mapped stimulus sets using human expert raters as ground truth and created a set of situations reflecting different emotions in the OCC model. The study revealed that ChatGPT can accurately extract fine-grained sentiment from situations and words, showing comparable performance to fine-tuned models. It demonstrated a moderate understanding of affective dimensions and emotion words, and successfully performed basic emotion elicitation based on the OCC appraisal model.\nB. Cross-cultural emotion studies\nEmotions were extensively studied comparing Eastern Asian culture to Western culture. Tsai et al. [14] showed that people in North American contexts lean towards feeling excited, enthusiastic, energetic, and other \u201chigh arousal positive\u201d states, while people in East Asian contexts, generally prefer feeling calm, peaceful, and other \"low arousal positive\" states. A follow-up by Tsai et al. [15] showed that people from North American contexts (who value high arousal affective states) tend to prefer thrilling activities like skydiving, whereas people from East Asian contexts (who value low arousal affective states) prefer tranquil activities like lounging on the beach. In addition, Tsai et al. [14] showed that among European Americans, the less people experience high arousal positive states, the more depressed they are, while, among Hong Kong Chinese, the less people experience low arousal positive states, the more depressed they are. In line with the previous insights, Chentsova-Dutton et al. [16] found that depressed European Americans show reduced emotional expressions, but depressed East Asian Americans do not and, in fact, may express more emotion. In our work, we focus on Miyamoto et al. [8], who compared North Americans to Japanese and showed the latter group is more likely to feel bad and good (\u201cmixed\" emotions) during positive events and differ in response when facing negative events.\nC. Cultural representations in LLMs\nA few previous studies have already analyzed cultural representations in LLMs. For example, Naous et al. [17] assessed biases in LLMs towards Arab and Western cultures, focusing on story generation, NER (named entity recognition), sentiment analysis, contextual prompt analysis, and text infilling. The study shows that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. The study further introduced a resource called CAMEL containing naturally occurring prompts and entities that contrast Arab and Western cultures. In line with that, Atari et al. [18] show that the LLMs performance of psychological cognitive tasks resembles most of the Western Industrialized Educated Rich and Democratic (WIERD) population in an extensive cross-language study on LLMs. Similarly, Arora et al. [19] found in their study which was based on World Values Survey, that while the values elicited from models vary across cultures, their bias is not in line with values outlined in existing large-scale values surveys. More widely, LLMs cultural representation has started gaining interest in mainstream media outlets [20], [21].\nBeyond output analysis, a recent study from Wendler et al. [22] explored whether multilingual language models use English as an internal pivot language, focusing on the Llama-2 family of transformer models, probing LLMs internally. The study was done by tracking intermediate embedding and logit lens analysis based on constructed non-English prompts with a unique correct single-token continuation. The study provides compelling evidence that multilingual language models may indeed use English as an internal pivot language, albeit in a nuanced and conceptually biased manner."}, {"title": "III. METHODS", "content": "A. The Mixed Emotion Experiment with Human Participants\nIn this work we follow the protocol set by Miyamoto et al. (2010) [8] who studied the similarities and differences in which Americans and Japanese experience mixed emotions, focusing on the co-occurrence of positive and negative emotions experienced by Americans and Japanese in plausible situations of their daily lives. Concretely, the study considers 13 situations, which are divided in three types: self-success, transition, and self-failure. Then, participants need to rate the extent to which they would experience positive or negative emotions, and the extent they would experience specific positive emotions (happiness, pride, sympathy, relief, hope, and friendly feeling) or specific negative emotions (sadness, anxiety, anger, self-blame, fear, anger at oneself, shame, guilt, jealousy, frustration, embarrassment, resentment, and fear of troubling someone else). Additionally, participants answered three appraisal questions using 6-point scales: (1) how responsible they would feel for other people's feelings; (2) how much other people were responsible for their feelings. Finally, to study the participants' motivation to control the situation, participants were asked how much would you think about influencing or changing the surrounding people, events, or objects according to your own wishes? using also a 6-point scale. The obtained results showed that mixed emotions were more present for the Japanese participants than for the Americans in the self-success situations. Beyond mixed emotions, additional significant differences were found between those populations in self-failure situations when compared on the motivation to change the situation, and the self-responsibility attributed which will be discussed in the next sections.\nOur work follows the study protocol used in [8]. Concretely, we replicated the survey focusing on questions that demonstrated significant difference between Japanese and Americans in that study. We contrast our findings with findings in [8] which are summarized in Table I.\nB. Running the Mixed Emotion Survey on LLMs\nBased on the original examples in [8] we generated five self-success situations and five self-failure situations with ChatGPT, which composing the survey, while incorporating the original instructions, as described in the paper. The following two situations are examples of Self-Success and an example of Self-Failure, respectively:\n\u2022 (Self-Success situation) You receive a stellar performance review and a promotion, which makes you happy. However, your colleague receives a warning due to underperformance, leaving you with mixed emotions.\n\u2022 (Self-Failure situation) Your close friend becomes the center of attention at social gatherings, effortlessly making friends and connections, which fills you with pride for their social skills. Meanwhile, you struggle to navigate social situations and feel left out, resulting in mixed feelings of admiration for them and disappointment in yourself.\nFor study 3, each survey was translated into Vietnamese, Korean, Chinese, French, German, and Spanish, by native speakers who also hold proficiency in English. To query the LLMs, we employed LangChain python package and only included complete responses for all 24 questions. The survey questions were split into three parts, as currently, models fail to answer all 24 at once. We evaluate the emotional response for a situation assuming each can be asked separately per situation, and that the presence of other questions does not influence the response.\nC. Evaluations\nWe employed independent paired one-tailed in study 1, two-tailed t-tests in study 2 and 3, when comparing distributions. Regarding study 1 and, particularly, the comparison of LLMs with the human experiments performed by Miyamoto et. al [8], we compare our results with the relevant findings of [8], which are summarized in Table. I.\nIV. EXPERIMENTS"}, {"content": "A. Study 1: English vs. Japanese\nWe evaluate the differences in the survey responses of the LLMs when prompted in English vs. Japanese. We also compared the responses of LLMs with the results obtained by Miyamoto et al. [8] in their study with human subjects.\nWe run the survey $n$ times per language, per LLM, to simulate $n$ responses. We searched for $n$ that is stable such that, the next time we draw $n$ responses, the distribution of both drawings is similar indicating sample stability. We found $n=80$ to provide stable distributions and elaborate on the search procedure in section IV-A.\nTable II summarizes the results per system for each of the emotion responses presented in Table I. To produce the table, we conducted one-tailed t-tests with $H_o$ hypothesis, positing no difference between the groups, and $H_1$ indicating a difference in the direction specified in Table I. If the t-test outcome rejected $H_o$ in the expected direction, we denoted it with a \u2018+\u2019 sign. Subsequently, we conducted another t-test in the opposite direction, and if the outcome rejected"}, {"title": "V. CONCLUSIONS", "content": "In this work we study the cultural alignment of Large Language Models (LLMs) in the context of mixed emotions. The mixed emotions phenomenon is linked to Eastern collectivist norms [8]. In the study conducted by Tim Lomas et al. [25], the central premise is that individualism is typically associated with Western cultures, whereas collectivism is more closely linked to Eastern cultures. Building on this premise, we aimed to investigate the sensitivity of LLMs to cultures beyond Western contexts. We did this by examining LLM responses to mixed emotion situations, which are believed to elicit either collectivist or individualistic norms depending on the cultural proximity to the originally studied cultures.\nOur findings indicate that, when replicating the human participants' study from Miyamoto et al. [8] in English and Japanese, the responses of leading LLMs demonstrated limited alignment to the humans' responses obtained in [8].\nWe also compared the effect of prompting the LLMs in English or Japanese versus explicitly using textual descriptions of the cultural context. The goal of these experiments was to get answers from the model as if the questions were posed to individuals from Western cultures versus Japanese culture. We discovered that the language itself was impacting the responses more than its textual context description.\nFinally, we evaluated the responses of the LLMs when prompted in various languages, including East Asian languages (Chinese, Korean, and Vietnamese) as well as American European (or Western) languages (French, German, and Spanish). We found that the similarity of responses across the investigated LLMs varied significantly between East Asian and Western languages. The East Asian language cohort exhibited a greater number of correlations in responses, making it more similar compared to the Western language cohort. This was inconsistent with our expectation that both cohorts would show similar rates of correlation.\nAs LLMs become more widely accessible globally, and as more researchers explore their potential to simulate human behavior, there is a growing need to understand how accurately they reflect our diverse values and cultures. We hope that the methodology presented here, which replicates peer-reviewed human-subject studies, offers a path for advancing our understanding of cultural alignment based on the extensive literature on various cultures the research community has accumulated. Furthermore, we hope that the observations and discussions of our findings will inspire more research into understanding cultural representations of emotions in LLMs."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "Limits of Generalizability (applicability to other cultures). Our approach relied on existing peer-reviewed literature, limiting our exploration of less well-researched cultures. For these under-researched cultures, we recommend developing surveys that focus on understanding cultural norms and emotional differences (with respect to dominant cultures trained by LLMs) to more effectively evaluate the cultural alignment of human subject responses to LLMs.\nLimits of Generalizability (of broader cultural representation). While the findings were significant according to standard statistical methods, we evaluated the mixed emotions phenomenon based on seven tests. These results highlight specific cultural differences related to mixed emotions but do not allow for broader conclusions about overall cultural representation.\nValidation. One concern is the validity of the experiment conducted by Miyamoto et al. [8] and we recognize there might be shifts in cultural perceptions around mixed emotions. While the phenomenon of mixed emotions continues to be a topic of research [24], we aim to replicate the original study with human participants in the future.\nStudy 3 hypothesis validity. In this part we anticipated response similarity based on geographical proximity (that is assumed to lead to cultural similarity), and based on the literature supporting the likelihood of a similar behavior around mixed emotions in different types of societies. We emphasize that study 3 did not aim to prove the existence of this phenomenon, rather understand whether cultural similarities are found in English-European and East-Asian languages.\nBiases. We adhered to the protocol outlined by Miyamoto et al. [8], including survey instructions where the content of the situations introduced potential mixed emotions, which may have biased the results. However, the findings indicate that despite this uniform introduction of bias across all experiments, the models' responses did not demonstrate a consistent pattern."}]}