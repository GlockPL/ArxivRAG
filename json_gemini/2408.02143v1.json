{"title": "Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion Survey", "authors": ["Shiran Dudy", "Ibrahim Said Ahmad", "Ryoko Kitajimat", "Agata Lapedriza"], "abstract": "Large Language Models (LLMs) have gained widespread global adoption, showcasing advanced linguistic capabilities across multiple of languages. There is a growing interest in academia to use these models to simulate and study human behaviors. However, it is crucial to acknowledge that an LLM's proficiency in a specific language might not fully encapsulate the norms and values associated with its culture. Concerns have emerged regarding potential biases towards Anglo-centric cultures and values due to the predominance of Western and US-based training data. This study focuses on analyzing the cultural representations of emotions in LLMs, in the specific case of mixed-emotion situations. Our methodology is based on the studies of Miyamoto et al. (2010), which identified distinctive emotional indicators in Japanese and American human responses. We first administer their mixed emotion survey to five different LLMs and analyze their outputs. Second, we experiment with contextual variables to explore variations in responses considering both language and speaker origin. Thirdly, we expand our investigation to encompass additional East Asian and Western European origin languages to gauge their alignment with their respective cultures, anticipating a closer fit. We find that (1) models have limited alignment with the evidence in the literature; (2) written language has greater effect on LLMs' response than information on participants origin; and (3) LLMs responses were found more similar for East Asian languages than Western European languages.", "sections": [{"title": "I. INTRODUCTION", "content": "With the widespread adoption of Large Language Models (LLMs), like ChatGPT, there has been an increased interest in understanding how different cultures are represented in LLMs [1], [2]. Understanding cultural representations in LLMs is crucial for many reasons, such as ensuring that communication tools based on LLMs are inclusive (able to deal with different perspectives and values) and effective (able to interpret and respond appropriately in different cultural contexts). In turn, emotions play a significant role in communication, since they have a strong influence on how messages are expressed and perceived [3]. This has motivated recent works on understanding emotional skills in LLMs [4]\u2013[7].\nThis paper explicitly studies cultural representations of emotions in LLMs. Concretely, we focus on comparing emotion representations of Western and East Asian cultures, by studying the emotional responses of LLMs to various mixed-emotion situations. Our research builds upon the Mixed Emotion Experiment conducted by Miyamoto et al. (2010) [8]. In their work, the authors designed a survey describing 13 situations of mixed emotions, where the participants had to rate how they would feel in each of the situations. The survey was answered by North American participants and Japanese participants, in their respective languages, and the results show interesting differences between the two studied populations (more details on the Mixed Emotion Experiment [8] are provided in III-A). In our work, we utilize the same emotional response survey as in [8] in both Japanese and English languages to investigate whether the responses generated by various LLMs align with the findings observed in the human experiments conducted by Miyamoto et al. (2010) [8]. Our work is closely related to the work by Havaldar et al. (2023) [9], who investigated culturally aware emotional responses in LLMs. This work investigates the mixed emotion phenomenon, rather than the presence of universal emotions in LLMs.\nMore generally, our study addresses three research questions:\n\u2022 RQ1: To what extent the findings in [8] are reproduced with LLMs?\n\u2022 RQ2: What is the effect on LLMs' response when prompted with different sources of contextual information?\n\u2022 RQ3: How similar are the responses of LLMs in languages with higher cultural affinity?\nTo address these research questions we perform 3 studies, which are presented in Sect. IV. Study 1 compares how the responses to the survey differ when the LLMs are prompted in English vs. Japanese. It also studies to what extent the results obtained by LLMs align with the results obtained by Miyamoto et al. [8] on their experiments with human subjects. Study 2 analyzes the effect of adding different contextual sources to the prompt. For example, prompting in English language but adding to the prompt the textual information Please rate [the survey] as a Japanese participant. Finally, study 3 compares the responses across different languages, including 4 Western languages (English, Spanish, German, and French) and 4 East Asian languages (Japanese, Chinese, Korean, and Vietnamese).\nOur studies are conducted on 5 widely used LLMs: three open source models (mistral-7b-instruct [10], gemma-"}, {"title": "II. RELATED WORK", "content": "Some recent studies have attempted to evaluate and quantify emotional skills in Large Language Models (LLMs). For example, Schaaff et al. 2023 [4] investigates the extent to which ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional expressions. In the experimentation, ChatGPT was instructed to rephrase neutral sentences into six emotional sentences of joy, anger, fear, love, sadness, and surprise. The responses of ChatGPT were then assessed using five standardized questionnaires. This study demonstrated promising results in ChatGPT's ability to convey multiple emotions using the same base sentence.\nTran et al. [5] introduce a method to assess robustness and bias in emotion recognition systems. They combine GPT-3 and rule-based constraints to create text variations for different language proficiency levels. These texts are used to examine performance biases, focusing on language proficiency, and investigate model-agnostic effects using the COSMIC model [6] and benchmark datasets.\nFinally, Broekens et al. [7] explore how ChatGPT can perform affective computing tasks using prompting alone. The methodology involves conducting conversational experiments with ChatGPT to explore its fine-grained affective processing capabilities. The paper used a rule-based logical model of appraisal as a prompt based on the OCC model [13] to assess if ChatGPT can predict emotions according to a specific framework. They also mapped stimulus sets using human expert raters as ground truth and created a set of situations reflecting different emotions in the OCC model. The study revealed that ChatGPT can accurately extract fine-grained sentiment from situations and words, showing comparable performance to fine-tuned models. It demonstrated a moderate understanding of affective dimensions and emotion words, and successfully performed basic emotion elicitation based on the OCC appraisal model."}, {"title": "B. Cross-cultural emotion studies", "content": "Emotions were extensively studied comparing Eastern Asian culture to Western culture. Tsai et al. [14] showed that people in North American contexts lean towards feeling excited, enthusiastic, energetic, and other \u201chigh arousal positive\u201d states, while people in East Asian contexts, generally prefer feeling calm, peaceful, and other \"low arousal positive\" states. A follow-up by Tsai et al. [15] showed that people from North American contexts (who value high arousal affective states) tend to prefer thrilling activities like skydiving, whereas people from East Asian contexts (who value low arousal affective states) prefer tranquil activities like lounging on the beach. In addition, Tsai et al. [14] showed that among European Americans, the less people experience high arousal positive states, the more depressed they are, while, among Hong Kong Chinese, the less people experience low arousal positive states, the more depressed they are. In line with the previous insights, Chentsova-Dutton et al. [16] found that depressed European Americans show reduced emotional expressions, but depressed East Asian Americans do not and, in fact, may express more emotion. In our work, we focus on Miyamoto et al. [8], who compared North Americans to Japanese and showed the latter group is more likely to feel bad and good (\u201cmixed\" emotions) during positive events and differ in response when facing negative events."}, {"title": "C. Cultural representations in LLMs", "content": "A few previous studies have already analyzed cultural representations in LLMs. For example, Naous et al. [17] assessed biases in LLMs towards Arab and Western cultures, focusing on story generation, NER (named entity recognition), sentiment analysis, contextual prompt analysis, and text infilling. The study shows that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. The study further introduced a resource called CAMEL containing naturally occurring prompts and entities that contrast Arab and Western cultures. In line with that, Atari et al. [18] show that the LLMs performance of psychological cognitive tasks resembles most of the Western Industrialized Educated Rich and Democratic (WIERD) population in an extensive cross-language study on LLMs. Similarly, Arora et al. [19] found in their study which was based on World Values Survey3, that while the values elicited from models vary across cultures, their bias is not in line with values outlined in existing large-scale values surveys. More widely, LLMs cultural representation has started gaining interest in mainstream media outlets [20], [21].\nBeyond output analysis, a recent study from Wendler et al. [22] explored whether multilingual language models use English as an internal pivot language, focusing on the Llama-2 family of transformer models, probing LLMs internally. The study was done by tracking intermediate embedding and logit lens analysis based on constructed non-English prompts with a unique correct single-token continuation. The study provides compelling evidence that multilingual language models may indeed use English as an internal pivot language, albeit in a nuanced and conceptually biased manner."}, {"title": "III. METHODS", "content": "In this work we follow the protocol set by Miyamoto et al. (2010) [8] who studied the similarities and differences in which Americans and Japanese experience mixed emotions, focusing on the co-occurrence of positive and negative emotions experienced by Americans and Japanese in plausible situations of their daily lives. Concretely, the study considers 13 situations, which are divided in three types: self-success, transition, and self-failure. Then, participants need to rate the extent to which they would experience positive or negative emotions, and the extent they would experience specific positive emotions (happiness, pride, sympathy, relief, hope, and friendly feeling) or specific negative emotions (sadness, anxiety, anger, self-blame, fear, anger at oneself, shame, guilt, jealousy, frustration, embarrassment, resentment, and fear of troubling someone else). Additionally, participants answered three appraisal questions using 6-point scales: (1) how responsible they would feel for other people's feelings; (2) how much other people were responsible for their feelings. Finally, to study the participants' motivation to control the situation, participants were asked how much would you think about influencing or changing the surrounding people, events, or objects according to your own wishes? using also a 6-point scale. The obtained results showed that mixed emotions were more present for the Japanese participants than for the Americans in the self-success situations. Beyond mixed emotions, additional significant differences were found between those populations in self-failure situations when compared on the motivation to change the situation, and the self-responsibility attributed which will be discussed in the next sections.\nOur work follows the study protocol used in [8]. Concretely, we replicated the survey focusing on questions that demonstrated significant difference between Japanese and Americans in that study. We contrast our findings with findings in [8] which are summarized in Table I."}, {"title": "B. Running the Mixed Emotion Survey on LLMs", "content": "Based on the original examples in [8] we generated five self-success situations and five self-failure situations with ChatGPT, which composing the survey, while incorporating the original instructions, as described in the paper. The following two situations are examples of Self-Success and an example of Self-Failure, respectively:\n\u2022 (Self-Success situation) You receive a stellar performance review and a promotion, which makes you happy. However, your colleague receives a warning due to underperformance, leaving you with mixed emotions.\n\u2022 (Self-Failure situation) Your close friend becomes the center of attention at social gatherings, effortlessly making friends and connections, which fills you with pride for their social skills. Meanwhile, you struggle to navigate social situations and feel left out, resulting in mixed feelings of admiration for them and disappointment in yourself.\nFor study 3, each survey was translated into Vietnamese, Korean, Chinese, French, German, and Spanish, by native speakers who also hold proficiency in English. To query the LLMs, we employed LangChain python package 4 and only included complete responses for all 24 questions. The survey questions were split into three parts, as currently, models fail to answer all 24 at once. We evaluate the emotional response for a situation assuming each can be asked separately per situation, and that the presence of other questions does not influence the response."}, {"title": "C. Evaluations", "content": "We employed independent paired one-tailed in study 1, two-tailed t-tests in study 2 and 3, when comparing distributions. Regarding study 1 and, particularly, the comparison of LLMs with the human experiments performed by Miyamoto et. al [8], we compare our results with the relevant findings of [8], which are summarized in Table. I."}, {"title": "IV. EXPERIMENTS", "content": "We evaluate the differences in the survey responses of the LLMs when prompted in English vs. Japanese. We also compared the responses of LLMs with the results obtained by Miyamoto et al. [8] in their study with human subjects.\nWe run the survey n times per language, per LLM, to simulate n responses. We searched for n that is stable such that, the next time we draw n responses, the distribution of both drawings is similar indicating sample stability. We found n = 80 to provide stable distributions and elaborate on the search procedure in section IV-A.\nTable II summarizes the results per system for each of the emotion responses presented in Table I. To produce the table, we conducted one-tailed t-tests with Ho hypothesis, positing no difference between the groups, and H\u2081 indicating a difference in the direction specified in Table I. If the t-test outcome rejected Ho in the expected direction, we denoted it with a '+' sign. Subsequently, we conducted another t-test in the opposite direction, and if the outcome rejected Ho, it was indicated with a '-' sign. If Ho was not rejected, we left the cell empty. Following the receipt of results, we aggregated the overall performance, with '+' and '-' counting for +1 and -1 respectively across the 7 tests. Finally, the results were normalized to a percentage score within the range [-100%, 100%], corresponding to total failure and total success across all tests.\nIn Table II, we observe that most models achieved a similar balance of the number of tests aligned with the human subject study (I). GPT-3.5 attained 28.5% indicating greatest alignment compared to other models, yet it did not demonstrate full alignment with the results from the original literature. To further investigate the responses of the LLMs, Figure 1 describes four types of rating distributions per LLM: (self-success, motivation to change), (self-failure, motivation to change), (self-success, me responsible for others), and (self-failure, me responsible for others). We notice differences in distributions per (self-success, motivation to change), which is in the first row. Gemma indicates a clear seperation between Japanese and Americans, where mistral's populations is more mixed. We see that even though most models align with human experiments on this survey question in Table II, the distributions seem visually different, with a greater or smaller degree of separation of the two populations in gemma, llama, gpt3.5, and gpt4. This may suggest that their underlying mechanism may be different. The fact we find separation may also suggest that even if LLMs do translate under-the-hood, they may not just do so and exhibit a level of cultural sensitivity to this question.\nConversely, the first two rows in Figure 1 exhibit visually similar distributions, even though they correspond to self-success and self-failure scenarios, where one would expect opposite responses as indicated in Table I. This similarity may suggest a reduced sensitivity to the situation type.\nThe last two rows in Figure 1 also demonstrate similar distributions per model for different situations types. In this case though, Table I indicates that the relation between Japanese and Americans remained the same. Therefore, these responses are aligned with the findings.\nIn light of these two findings we cannot discern whether there is insensitivity to situation type, or this was a one-off mistake by all models.\nOverall, we found a limited alignment of LLM responses with Miyamoto et al. [8]. This limited alignment has been exhibited variably across models, manifesting in different responses to various tests. However, similarities were also observed, with many models showing low sensitivity to the same textual information.\nFinding the number n of required samples: While Miyamoto et al. [8] emphasize the importance of the number of human participants to obtain relevant results, our study focuses on ensuring the stability of responses from LLMs to enable reproducibility (on that version). We accomplish this by executing the same experiment n times and then computing our results based on the average of the responses obtained along the n runs. The last part of this sub-section details the experiments we performed to determine the parameter n that we use in all the experiments. Concretely, we determine the minimal number n of samples drawn from a model to ensure sample stability, meaning that the distributional properties (\u03bc, \u03c3) remain consistent when drawing a new set of n samples.\nTo determine n, we administered a subset of the survey questions at intervals of 10, ranging from 10 to 300 samples. For each n, we generated 20 distributions and conducted two-tailed t-tests for all unique pairs of distributions. Figure 2 presents boxplots for Japanese, showing the p-value medians across all models for each n (five p-value medians per n that each corresponds to a different model). Empirically, we observed that ns resulting in median p-values above 0.5 provided stable distributions. Based on that observation the"}, {"title": "B. Study 2: English vs. Japanese using context prompts.", "content": "The goal of this part is to understand the effect of different types of contexts on the response. We set to investigate the effect of two modes of context: 1) the language of the written survey indicated in (w), and 2) the language of origin of the speaker (which the LLM is required to simulate), indicated in (0) forming a response distribution described as (w,o). We added to the survey an additional request 'Please rate it as a [placeholder] participant', where the placeholder could be either 'Japanese' or 'American' which were applied to both languages - totaling in four combinations of written language and origin (w, o): (en, en),(en, jp),(jp, jp),(jp, en). For instance (en, jp) is a distribution made by a survey written in English simulating a participant from Japan. We hypothesized that due to either similarity in language or similarity in the origin of the simulated participant, we might find correlations across different (w,o) combinations. For every two pairs of (w,o) we ran two-tailed t-tests to evaluate distribution similarity across the 7 t-test shown in Table I. For instance, in Table III the responses by gemma model comparing ((en, jp)) and (en, en)) were similar in 3 of the 7 tests applied. Following this table, we posed three hypotheses to understand the effect of the written language (w), and participant's origin (0) that are shown in Table IV: (1) are there similarities between shared language and different origin?; (2) are there similarities between similar origin, but different language? and; (3) are there similarities in responses between the response in study 1 (w,-), where origin was not indicated, to responses that shared that same written language (w,o) in this section?"}, {"title": "C. Study 3: Comparing East Asian vs. Western Languages", "content": "Schimmack et al. [?] claimed that 'it is possible that people in Asian dialectic cultures more readily recognize the pleasant and unpleasant aspects of an event, which produces mixed feelings of pleasant and unpleasant emotions.' This hypothesis has to do with the collectivist (vs. individualistic) nature of East Asian cultures [?], [?]. In study 3, we analyze the similarities across languages of East Asian and American-European cultural affinities anticipating closer similarity with Japanese and American cultures, respectively.\nWe expanded our survey to include three additional East Asian languages (Chinese (ch), Korean (kr), and Vietnamese (vt)), and three European languages (French (fr), German (gr), and Spanish (sp)). We translated the survey in study 1, and compared pairs of languages under the hypothesis that LLM responses to related languages would be similar. Similarly to study 2, we conducted 7 t-tests across all language pairs. We summarized our results in Table V.\nWe observe that most LLMs generated more similar response distributions for East Asian languages than for European languages. On the one hand, this may be a desired outcome, as it may reflect LLMs may be culturally aware of these languages. On the other hand, based on the Internet Society Foundation data 5, the most used languages on the internet are English (55%), Spanish (5%), German, French, and then Japanese and Chinese, which may offer a proxy for training data available for LLMs, that possibly resulted in less fine-grained responses to differentiate one East Asian culture from another. Since we see less correlation for European languages, we hypothesize that the larger amount of data help to better distinguish European languages.\nSimilar to study 2, no language correlated with English, may be due to its disproportional volume (55%) reflecting mostly American culture, and its many facets, in training.6\nHere, similar to study 2, mistral demonstrated the largest"}, {"title": "V. CONCLUSIONS", "content": "In this work we study the cultural alignment of Large Language Models (LLMs) in the context of mixed emotions. The mixed emotions phenomenon is linked to Eastern collectivist norms [8]. In the study conducted by Tim Lomas et al. [25], the central premise is that individualism is typically associated with Western cultures, whereas collectivism is more closely linked to Eastern cultures. Building on this premise, we aimed to investigate the sensitivity of LLMs to cultures beyond Western contexts. We did this by examining LLM responses to mixed emotion situations, which are believed to elicit either collectivist or individualistic norms depending on the cultural proximity to the originally studied cultures.\nOur findings indicate that, when replicating the human participants' study from Miyamoto et al. [8] in English and Japanese, the responses of leading LLMs demonstrated limited alignment to the humans' responses obtained in [8].\nWe also compared the effect of prompting the LLMs in English or Japanese versus explicitly using textual descriptions of the cultural context. The goal of these experiments was to get answers from the model as if the questions were posed to individuals from Western cultures versus Japanese culture. We discovered that the language itself was impacting the responses more than its textual context description.\nFinally, we evaluated the responses of the LLMs when prompted in various languages, including East Asian languages (Chinese, Korean, and Vietnamese) as well as American European (or Western) languages (French, German, and Spanish). We found that the similarity of responses across the investigated LLMs varied significantly between East Asian and Western languages. The East Asian language cohort exhibited a greater number of correlations in responses, making it more similar compared to the Western language cohort. This was inconsistent with our expectation that both cohorts would show similar rates of correlation.\nAs LLMs become more widely accessible globally, and as more researchers explore their potential to simulate human behavior, there is a growing need to understand how accurately they reflect our diverse values and cultures. We hope that the methodology presented here, which replicates peer-reviewed human-subject studies, offers a path for advancing our understanding of cultural alignment based on the extensive literature on various cultures the research community has accumulated. Furthermore, we hope that the observations and discussions of our findings will inspire more research into understanding cultural representations of emotions in LLMs."}, {"title": "ETHICAL IMPACT STATEMENT", "content": "Limits of Generalizability (applicability to other cultures). Our approach relied on existing peer-reviewed literature, limiting our exploration of less well-researched cultures. For these under-researched cultures, we recommend developing surveys that focus on understanding cultural norms and emotional differences (with respect to dominant cultures trained by LLMs) to more effectively evaluate the cultural alignment of human subject responses to LLMs.\nLimits of Generalizability (of broader cultural representation). While the findings were significant according to standard statistical methods, we evaluated the mixed emotions phenomenon based on seven tests. These results highlight specific cultural differences related to mixed emotions but do not allow for broader conclusions about overall cultural representation.\nValidation. One concern is the validity of the experiment conducted by Miyamoto et al. [8] and we recognize there might be shifts in cultural perceptions around mixed emotions. While the phenomenon of mixed emotions continues to be a topic of research [24], we aim to replicate the original study with human participants in the future.\nStudy 3 hypothesis validity. In this part we anticipated response similarity based on geographical proximity (that is assumed to lead to cultural similarity), and based on the literature supporting the likelihood of a similar behavior around mixed emotions in different types of societies. We emphasize that study 3 did not aim to prove the existence of this phenomenon, rather understand whether cultural similarities are found in English-European and East-Asian languages.\nBiases. We adhered to the protocol outlined by Miyamoto et al. [8], including survey instructions where the content of the situations introduced potential mixed emotions, which may have biased the results. However, the findings indicate that despite this uniform introduction of bias across all experiments, the models' responses did not demonstrate a consistent pattern."}]}