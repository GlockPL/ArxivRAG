{"title": "Hierarchical Average-Reward Linearly-solvable Markov Decision Processes", "authors": ["Guillermo Infantea", "Anders Jonsson\u00aa", "Vicen\u00e7 G\u00f3meza"], "abstract": "We introduce a novel approach to hierarchical rein-forcement learning for Linearly-solvable Markov Decision Processes (LMDPs) in the infinite-horizon average-reward setting. Unlike previous work, our approach allows learning low-level and high-level tasks simultaneously, without imposing limiting restrictions on the low-level tasks. Our method relies on partitions of the state space that create smaller subtasks that are easier to solve, and the equivalence between such partitions to learn more efficiently. We then exploit the compositionality of low-level tasks to exactly represent the value function of the high-level task. Experiments show that our approach can outperform flat average-reward reinforcement learning by one or several orders of magnitude.", "sections": [{"title": "1 Introduction", "content": "Hierarchical reinforcement learning (RL) [6, 19, 22, 7, 3] aims to decompose a complex high-level task into several low-level tasks. After solving the low-level tasks, their solutions can be combined to form a solution to the high-level task. Ideally, the low-level tasks should be significantly easier to solve than the high-level task, in which case one can obtain an important speedup in learning [16, 29]. Hierarchical RL has also been credited with other advantages, e.g. the ability to explore more efficiently [17].\nMost previous work on hierarchical RL considers either the finite-horizon setting or the infinite-horizon setting with discounted rewards. The average-reward setting is better suited for cyclical tasks characterized by continuous experience. In the few works on hierarchical RL in the average-reward setting, either the low-level tasks are assumed to be solved beforehand [8, 9, 28] or they have important restrictions that severely reduce their applicability, e.g. a single initial state [10]. It is therefore an open question how to develop algorithms for hierarchical RL in the average-reward setting in order to learn the low-level and high-level tasks simultaneously.\nIn this paper we propose a novel framework for hierarchical RL in the average-reward setting that simultaneously solves low-level and high-level tasks. Concretely, we consider the class of Linearly-solvable Markov Decision Processes (LMDPs) [23]. LMDPs are a class of restricted MDPs for which the Bellman equation can be exactly transformed into a linear equation. This class of problems plays a key role in the framework of RL as probabilistic inference [14, 15]. One of the properties of LMDPs is compositionality: one can compute the solution to a novel task from the solutions to previously solved tasks without learning [25]. Such a property has been exploited in recent works about compositionality in RL [11, 26, 18] and in\nOur work makes the following novel contributions:\n\u2022 Learning low-level and high-level tasks simultaneously in the average-reward setting, without imposing additional restrictions on the low-level tasks.\n\u2022 Two novel algorithms for solving hierarchical RL in the average reward setting: the first one is based on the eigenvector approach used for solving LMDPs. The second is an online variant in which an agent learns simultaneously the low-level and high-level tasks.\n\u2022 Two main theoretical contributions LMDPs: a converge proofs for both differential soft TD-learning for (non-hierarchical) LMDPS and also for the eigenvector approach in the hierarchical case.\nTo the best of our knowledge, this work is the first that extends the combination of compositionality and hierarchical RL to the average-reward setting."}, {"title": "2 Related work", "content": "Most research on hierarchical RL formulates problems as a Semi-Markov Decision Process (SMDP) with options [22] or the MAXQ decomposition [7].\nFruit and Lazaric [8] and [9] propose algorithms for solving SMDPS with options in the average-reward setting, proving that the regret of their algorithms is polynomial in the size of the SMDP components, which may be smaller than the components of the underlying Markov Decision Process (MDP). Wan et al. [28] present a version of differential Q-learning for SMDPs with options in the average-reward setting, proving that differential Q-learning converges to the optimal policy. However, the above work assumes that the option policies are given prior to learning. Ghavamzadeh and Mahadevan [10] propose a framework for hierarchical average-reward RL based on the MAXQ decomposition, in which low-level tasks are also modeled as average-reward decision processes. However, since the distribution over initial states can change as the high-level policy changes, the authors restrict low-level tasks to have a single initial state.\nWen et al. [29] present an approach for partitioning the state space and exploiting the equivalence of low-level tasks, similar to our work."}, {"title": "3 Background", "content": "Given a finite set X, we use A(X) to denote the set of all probability distributions on X."}, {"title": "3.1 First-exit Linearly-solvable Markov Decision Processes", "content": "A first-exit Linearly-solvable Markov Decision Process (LMDP, Todorov [23]) is a tuple \\(L = (S,T,P,R, J)\\), where S is a set of non-terminal states, T is a set of terminal states, \\(P: S\\rightarrow\\triangle(S+)\\) is the passive dynamics, which describes state transitions in the absence of controls, \\(R:S\\rightarrow R\\) is a reward function on non-terminal states, and \\(I : T\\rightarrow R\\) is a reward function on terminal states. We use \\(S+ = S\\cup T\\) to denote the full set of states. An agent interacts with the environment following a policy \\(\\pi : S \\rightarrow \\triangle(S+)\\). At timestep t, it observes a state st, transitions to a new state st+1 ~ \\(\\pi(.st)\\) and receives a reward\n\\(R(s_t, s_{t+1}, \\pi) = r_t - \\frac{1}{\\eta} log \\frac{\\pi(s_{t+1} | s_t)}{P(s_{t+1} | s_t)},\\)\nwhere rt is a reward with mean R(st). The agent can modify the policy \\(\\pi(st)\\), but gets penalized for deviating from the passive dy-namics P(st). The parameter \\(\\eta > 0\\) controls this penalty. Given \\(\\eta > 0\\), the value function of a policy \\(\\pi\\) can be defined as follows:\n\\[\nv^{\\pi}(s) = E_{\\pi, \\delta_0} [\\sum_{t=0}^{T-1}R(S_t, S_{t+1}, \\pi) + I(S_T)] \\quad s \\in S \\quad (1)\n\\]\nwhere T is a random variable representing the length of the episode, and St, t\u2265 0, are random variables denoting the state at time t. The interaction ends when the agent reaches a terminal state ST and receives reward I (ST). The value function of a terminal state \\(\\tau\\) \u2208 T is simply v (\\(\\tau\\)) = I(\\(\\tau\\)).\nThe aim of the agent is to find a policy that maximizes expected future reward. For that it is useful to define the optimal value function v: S \u2192 R among all policies. For simplicity, in what follows we omit the subscript and asterisk and refer to the optimal value function simply as the value function v. Such a value function is known to satisfy the following Bellman optimality equations [23]:\n\\[\nv(s) = \\frac{1}{\\eta} log \\sum_{s'\\in S+} P(s'|s) exp(\\eta(R(s) + v(s'))) \\quad \\forall s \\in S. \\quad (2)\n\\]\nIntroducing the notation z(s) = exp(\\(\\eta\\)v(s)), s \u2208 S+, results in the following system of linear equations:\n\\[\nz(s) = e^{\\eta R(s)} \\sum_{s'\\in S+} P(s'|s) z(s') \\quad \\forall s \\in S. \\quad (3)\n\\]\nWe abuse the notation and for simplicity refer to z(s) and v(s) in-terchangeably as the value of s. Given z, an optimal policy is given by\n\\[\n\\pi(s'|s) = \\frac{P(s'|s)z(s')}{\\sum_{s''\\in S+} P(s''|s)z(s'')} = \\frac{P(s'|s)z(s')}{G[z](s)} \\quad (4)\n\\]\nThe system of linear equations in (3) can then be written in matrix form when we know the passive dynamics and the reward functions. We let \\(P \\in R^{|S| \\times |S+|}\\) be a matrix such that \\(P(s,s') = P(s'|s)\\) and \\(R \\in R^{|S| \\times |S|}\\) a diagonal matrix such that \\(R(s,s) = e^{\\eta R(s)}\\). We also let z be the vector form of the value z(s) for all states s \u2208 S and z+ an extended vector that also includes the known value z(\\(\\tau\\)) = exp(\\(\\eta\\)I(\\(\\tau\\))) for all terminal states \\(\\tau\\) \u2208 T. The problem is then expressed as\n\\[\nz = RPz+. \\quad (5)\n\\]\nWe can use the power iteration method over (5) to obtain the solution for z [23]. Power iteration is guaranteed to converge as long as the diagonal matrix R is not too large, and a common assumption is that the rewards of non-terminal states are non-positive (i.e. R(s) \u2264 0 for each s \u2208 S). However, we refrain from making any such assumptions, and later we instead prove convergence in an alternative way.\nAlternatively, when P and R are not known, the agent can learn an estimate of the optimal value function in an online manner, using samples (St, rt, St+1) generated when following the estimated policy \\(\\pi_t\\) derived from 2 using (4). The update rule for the so-called Z-learning algorithm is given by\n\\[\nz(s_t) \\leftarrow (1 - \\alpha_t)z(s_t) + \\alpha_te^{\\eta R(s_t,s_{t+1}, \\pi_t)}z(s_{t+1})\n\\]\n\\[\n+(1-\\alpha_t)z(s_t) + \\alpha_te^{\\eta r_t} \\frac{P(s_{t+1}|s_t)}{\\pi_t(s_{t+1} | s_t)}z(s_{t+1}). \\quad (6)\n\\]\nHere, \\(\\alpha_t\\) is a learning rate and \\(P(s_{t+1}|s_t)/\\pi_t(s_{t+1} | s_t)\\) acts as an importance weight.\nIn the first-exit case, the solution of a set of component problems can be combined to retrieve the optimal solution for new composite problems with no further learning [25]. Assume we have a set of first-exit LMDPS \\({L_i\\}_{i=1}^{K}\\), which share S, T, P and R, but differ in the values zi(\\(\\tau\\)) = exp(\\(\\eta\\)Ii(t)) of terminal states. Let z1,...,zK be the optimal value functions of L1,..., LK. Now consider a new composite problem L that also shares the aforementioned elements with the component problems. If the value at terminal states can be expressed as a weighted sum as follows:\n\\[\nz(\\tau) = \\sum_{i=1}^{K} W_iz_i(\\tau) \\quad \\forall \\tau \\in T,\n\\]\nthen by linearity of the value function, the same expression holds for non-terminal states [25]:\n\\[\nz(s) = \\sum_{i=1}^{K} W_iz_i(s) \\quad \\forall s \\in S.\n\\]"}, {"title": "3.2 Hierarchical Decomposition for LMDPs", "content": "Infante et al. [12] introduce a hierarchical decomposition for LMDPs. Given a first-exit LMDP \\(L = (S,T,P,R, I)\\), the set of non-terminal states S is partitioned into L subsets \\({S_i\\}_{i=1}^{L}\\). Each subset Si induces a subtask \\(L_i = (S_i, T_i, P_i, R_i, J_i)\\), i.e. an LMDP for which\n\u2022 The set of non-terminal states is Si.\n\u2022 The set of terminal states \\(T_i = {\\tau \\in S+ \\setminus S_i : \\exists s \\in S_i s.t. P(\\tau|s) > 0}\\) includes all states in \\(S+ \\setminus S_i\\) (terminal or non-terminal) that are reachable in one step from a state in Si.\n\u2022 \\(P_i: S_i\\rightarrow\\triangle(S_i^+)\\) and \\(R_i: S_i \\rightarrow R\\) are the restrictions of P and R to Si, where \\(S_i^+ = S_i \\cup T_i\\) denotes the full set of subtask states.\n\u2022 The reward of a terminal state \\(\\tau\\) \u2208 Ti equals \\(J_i(\\tau) = I(\\tau)\\) if \\(\\tau\\) \u2208 T, and \\(J_i(\\tau) = \\hat{v}(\\tau)\\) otherwise, where \\(\\hat{v}(\\tau)\\) is the estimated value in L of the non-terminal state in \\(\\tau\\) \u2208 S \\setminus Si.\nIntuitively, if the value zi (\\(\\tau\\)) of each terminal state \\(\\tau\\) \u2208 Ti equals its optimal value z(\\(\\tau\\)) for the original LMDP L, then solving the subtask Li yields the optimal values of the states in Si. In practice, however, the agent only has access to an estimate \\(\\hat{z}(\\tau)\\) of the optimal value. In this case, the subtask Li is parameterized on the value estimate \\(\\hat{v}\\) (or \\(\\hat{z}\\)) of the terminal states Ti, and each time the value estimate changes, the agent can solve Li to obtain a new value estimate \\(\\hat{z}(s)\\) for each state s \u2208 Si.\nThe set of exit states \\(E = \\cup_{i=1}^{L}T_i\\) is the union of the terminal states of each subtask in \\({L_1,..., L_L}\\). We let \\(E_i = E \\cap S_i\\) denote the set of (non-terminal) exit states in the subtask Li. Also define the notation K = maxL i=1 |Si|, N = maxL i=1 |Ti| and E = |E|.\nDefinition 1. Two subtasks Li and Lj are equivalent if there exists a bijection f: Si \u2192 Sj such that the transition probabilities and rewards of non-terminal states are equivalent through f.\nUsing the above definition, we can define a set of equivalence classes \\(C = {C_1,...,C_c}\\), C < L, i.e. a partition of the set of subtasks \\({L_1,..., L_L}\\) such that all subtasks in a given partition are equivalent. Each equivalence class can be represented by a single subtask Lj = (Sj, Tj, Pj, Rj, Ij). As before, the reward Ij of terminal states is parameterized on a given value estimate \\(\\hat{v}\\). We assume that each non-terminal state s \u2208 S can be mapped to its subtask Li and equivalence class Cj.\nFor each subtask Lj and each terminal state \\(\\tau_k\\) \u2208 Tj, Infante et al. [12] introduce a base LMDP \\(L_j^k = (S_j, T_j, P_j, R_j, J_j^k)\\) that shares all components with Lj except the reward function on terminal states, which is defined as \\(J_j^k(t) = 1\\) if \\(\\tau = \\tau_k\\), and \\(J_j^k(t) = 0\\) otherwise. Let \\(z_j^1,..., z_j^n\\) be the optimal value functions of the base LMDPs for Lj, with n = |Ti. Given a value estimate \\(\\hat{z}\\) on each terminal state in Tj, due to compositionality we can express the value estimate of each state s \u2208 Sj as\n\\[\n\\hat{z}(s) = \\sum_{k=1}^{n} \\hat{z}(\\tau_k)z_j^k(s).\n\\]\nTo solve the original LMDP, we can now define an optimal value function on exit states as \\(z_E : E \\rightarrow R\\), and construct a matrix G = [R|E|\u00d7|| whose element G(s,s') equals \\(z_j^k(s)\\) if s\u2032 = \\(\\tau^*\\) is the k-th terminal state in the subtask Lj corresponding to the partition Si to which s belongs, and 0 otherwise. By writing zE in vector form, the optimal value function satisfies the following matrix equation:\n\\[\nz_E = Gz_E.\n\\]\nThe total number of values required to represent the optimal value function equals E + CKN, since there are C equivalence classes with at most K states and N base LMDPs."}, {"title": "4 Average-reward Linearly-solvable Markov Decision Processes", "content": "An average-reward Linearly-solvable Markov Decision Process (ALMDP) is a tuple \\(L = (S, P, R)\\), where S is a set of states, \\(P: S \\rightarrow \\triangle(S)\\) is the passive dynamics, which describes state transitions in the absence of controls, and \\(R: S\\rightarrow R\\) is a reward function. Since ALMDPs represent infinite-horizon tasks, there are no terminal states.\nThroughout the paper, we make the following assumptions about ALMDPS.\nAssumption 1. The ALMDP L is communicating [20]: for each pair of states s,s' \u2208 S, there exists a policy w that has non-zero probability of reaching s' from s.\nAssumption 2. The ALMDP L is unichain [20]: the transition prob-ability distribution induced by all stationary policies admit a single recurrent class.\nIn the average-reward setting, the value function is defined as the expected average reward when following a policy \\(\\pi : S \\rightarrow \\triangle(S)\\) starting from a state s \u2208 S. This is expressed as\n\\[\nv^{\\pi}(s) = \\lim_{T\\rightarrow\\infty} E_{\\pi} [\\frac{1}{T}\\sum_{t=0}^{T}R(S_t, S_{t+1}, \\pi) | S_0 = s ], \\quad (7)\n\\]\nwhere R(st, St+1, \\(\\pi\\)) is defined as for first-exit LMDPs. Again, we are interested in obtaining the optimal value function v. Under As-sumption 2, the Bellman optimality equations can be written as\n\\[\nv(s) = \\frac{1}{\\eta} log \\sum_{s'\\in S} P(s'|s) exp(\\eta(R(s) - \\rho + v(s'))) \\quad \\forall s \\in S, \\quad (8)\n\\]\nwhere \\(\\rho\\) is the optimal one-step average reward (i.e. gain), which is state-independent for unichain ALMDPs [23]. Exponentiating yields\n\\[\nz(s) = e^{\\eta(R(s)-\\rho)} \\sum_{S'\\in S}P(s'|s)z(s') \\quad \\forall s \\in S. \\quad (9)\n\\]\nFor the optimal value function z, the optimal policy is given by the same expression as in (4)."}, {"title": "4.1 Solving an ALMDP", "content": "We introduce the notation \u0393 = exp(\\(\\eta\\)p) (exponentiated gain). Similar to the first-exit case, Equation (9) can be expressed in matrix form as\n\\[\n\\Gamma z = RPz,\n\\]\n(10)\nwhere the matrices \\(P \\in R^{|S| \\times |S|}\\) and \\(R \\in R^{|S| \\times |S|}\\) are appropi-ately defined as in (5). The exponentiated gain \u0393 can be shown to correspond to the largest eigenvalue of RP [24]. An ALMDP can be solved using relative value iteration by selecting a reference state s* \u2208 S, initializing z0 = 1 and iteratively applying\n\\[\nz_{k+\\frac{1}{2}} \\leftarrow RPz_k,\n\\]\n\\[\nz_{k+1} \\leftarrow z_{k+\\frac{1}{2}}/z_{k+\\frac{1}{2}}(s^*).\n\\]\nThe reference state s* satisfies z(s*) = 1, which makes the optimal value z unique (else any constant shift preserves optimality). After convergence, the exponentiated gain equals \u0393 = zk+1 (s*). Under Assumption 1, relative value iteration converges to the unique optimal value z [24].\nAn alternative method for solving an ALMDP L is to transform it to a first-exit LMDP. Given a reference state s*, define a first-exit LMDP \\(L' = (S \\setminus {s^*}, {s^*}, P', R', J')\\), where P'(s'|s) = P(s'|s) for all state pairs (s,s') \u2208 (S \\setminus {s*}) \u00d7 S, and J(s*) = 0 (imply-ing z(s*) = 1). By inspection of (3) and (9), the Bellman optimal-ity equation of L' is identical to that of L if R'(s) = R(s) \u2013 \\(\\rho\\)."}, {"title": "5 Hierarchical Average-Reward LMDPS", "content": "In this section we present our approach for hierarchical average-reward LMDPs. The idea is to take advantage of the similarity of the value functions in the first-exit and average-reward settings, and use compositionality to compose the value functions of the subtask LMDPs without additional learning."}, {"title": "5.1 Hierarchical Decomposition", "content": "Consider an ALMDP (S, P, R). Similarly to Infante et al. [12], we assume that the state space S is partitioned into subsets \\({S_i\\}_{i=1}^{L}\\), with each partition S\u2081 inducing a first-exit LMDP Li = (Si, Ti, Pi, Ri, Ti). The components of each such subtask Li are defined as follows:\n\u2022 The set of states is Si.\n\u2022 The set of terminal states \\(T_i = {\\tau \\in S \\setminus S_i : \\exists s \\in S_i, P(\\tau|s) > 0}\\) contains states not in S\u2081 that are reachable in one step from any state inside the partition."}, {"title": "5.2 Subtask Compositionality", "content": "It is impractical to solve each subtask Li every time the estimate \\(\\hat{z}(\\tau)\\) changes for \\(\\tau\\) \u2208 Tj. To alleviate this computation we leverage compositionality for LMDPs. The key insight is to build a basis of value functions that can be combined to obtain the solution for the subtasks.\nConsider a subtask Li = (Si, Ti, Pi, Ri, Ji) and let n = |Ti|. We introduce n base LMDPs \\({L_j^1, . . ., L_j^n}\\) that are first-exit LMDPs and terminate in Ti. These base LMDPs only differ from Li in the reward of each terminal state \\(\\tau_k\\) \u2208 Ti. For all s \u2208 S\u0129, the reward for each \\(L_j^k\\) is by definition \\(R_i(s) = R(s) - \\hat{\\rho}\\) for all s \u2208 S\u2081, while at terminal states \\(\\tau\\) \u2208 Ti we let the reward function be \\(z_j^k(\\tau; \\rho) = 1\\) if \\(\\tau = \\tau_k\\) and \\(z_j^k(\\tau; \\rho) = 0\\) otherwise. Thus, the base LMDPs are parameterized by the gain estimate \\(\\hat{\\rho}\\). This is equivalent to setting the reward to I(\\(\\tau\\)) = 0 if \\(\\tau = \\tau_k\\) and \\(J_j^k(\\tau) = -\\infty\\) otherwise. Intuitively, each base LMDP solves the subtask of reaching one specific terminal state \\(\\tau_k\\) E Ti.\nLet us now assume that we have the solution \\(z_j^1(\\cdot; \\rho), ..., z_j^n(\\cdot; \\rho)\\) for the base-LMDPs (for the optimal gain \\(\\rho\\)) as well as the optimal value z(\\(\\tau_k\\)) of the original ALMDP for each terminal state \\(\\tau_k\\) \u2208 Ti. Then by compositionality we could represent the value function of each terminal state as a weighted combination of the subtasks:\n\\[\n\\hat{z}(\\tau) = \\sum_{k=1}^{n} W_kz_j^k(\\tau; \\rho) = \\sum_{k=1}^{n} \\hat{z}(\\tau^*)z_j^k(\\tau; \\rho) \\quad \\forall \\tau \\in T_i. \\quad (14)\n\\]\nClearly, the RHS in the previous expression evaluates to z(\\(\\tau\\)) since \\(z(\\tau^k)z_j^k(\\tau;\\rho) = \\hat{z}(\\tau) \\cdot 1\\) when \\(\\tau = \\tau_k\\), and \\(z(\\tau^k)z_j^k(\\tau;\\rho) = z(\\tau^k) \\cdot 0\\) otherwise.\nThanks to compositionality, we can also represent the value func-tion for each subtask state s \u2208 Si as\n\\[\nz(s) = \\sum_{k=1}^{n} \\hat{z}(\\tau_k) z_j^k(s; \\rho) \\quad \\forall s \\in S_i. \\quad (15)\n\\]\nWe remark that the base LMDPs depend on the gain \\(\\rho\\) by the definition of the reward function. This parameter is not known prior to learning. The subtasks in practice are solved for the latest estimate \\(\\hat{\\rho}\\) and must be re-learned for every update of this parameter until convergence."}, {"title": "5.3 Efficiency of the value representation", "content": "Similar to previous work [29, 12] we can exploit the equivalence of subtasks to learn more efficiently. Let \\(C = {C_1,...,C_c}\\), C < L, be a set of equivalence classes, i.e. a partition of the set of subtasks \\({L_1,..., L_L}\\) such that all subtasks in a given partition are equivalent. As before, we also define a set of exit states as \\(E = \\cup_{i=1}^{L} T_i\\). Due to the decomposition, we do not need to keep an explicit value estimate \\(\\hat{z}(s)\\) for every state s \u2208 S. Instead, it is sufficient to keep a value function for exit states \\(z_E: E \\rightarrow R\\) and a value function for each base LMDP of each equivalence class. This is enough to represent the value for any state s \u2208 S using the compositionality expression in (15).\nLetting K = maxL i=1|Si|, N = maxL i=1 |Til and E = |E|, O(KN) values are needed to represent the base LMDPs of a subtask, and we can thus represent the value function with O(CKN + E) values. The decomposition leads to an efficient representation of the value function whenever CKN + E \u00ab S. This is achieved when there are few equivalence classes, the size of each subtask is small (in terms of the number of states) and there are relatively few exit states."}, {"title": "6 Algorithms", "content": "We now propose two algorithms for solving hierarchical ALMDPS. The first is a two-stage eigenvector approach that relies on first solving the subtasks. The second is an online algorithm in which an agent simultaneously learns the subtasks, the gain and the exit values from samples (st, rt, St+1). Once again we recall that we do not explicitly represent the values for states s \u2209 E."}, {"title": "6.1 Eigenvector approach", "content": "In previous work, the base LMDPs are only solved once, and the solutions are then reused to compute the value function zE on exit states. However, in the case of ALMDPs, the reward functions of base LMDPs depend on the current gain estimate \\(\\hat{\\rho}\\), which is initially unknown.\nOur proposed algorithm appears in Algorithm 1. The intuition is that in each iteration, we first solve the subtasks for the latest estimate of the exponentiated gain . For this, we use (13) with the current value of \\(\\rho\\) to solve the base LMDPs. We then apply (15) restricted to E to obtain an estimate of the value for the exit states. This yields the system of linear equations\n\\[\nz_E = G_E z_E. \\quad (16)\n\\]\nHere, the matrix \\(G_E \\in R^{|E|\\times |E|}\\) contains the optimal values of the base LMDPs and has elements defined as in (15). We use the pre-viously introduced idea to transform the ALMDP L to a first-exit LMDP C' parameterized on the estimated gain \\(\\hat{\\rho}\\), and find the optimal exponentiated gain \u0393 using binary search. We keep a reference state s* \u2208 S (which is by definition an exit state) and use the test described above to decide how to update the search interval.\nTheorem 4. Algorithm 1 converges to the optimal value function z of Las e \u2192 0.\nFirst note that the optimal value function z of L exists and is unique due to Assumption 1. Due to the equivalence between L and the corresponding first-exit LMDP L', this implies that L' has a unique solution z(\u00b7; p) when the estimated gain \\(\\hat{\\rho}\\) equals \\(\\rho\\), and that this solution equals z(\u00b7; p) = z, the optimal solution to L.\nLemma 5. Given a first-exit LMDP C' parameterized on \\(\\hat{\\rho}\\), the optimal value z(s; p) of each non-terminal state s \u2208 S is strictly monotonically decreasing in p."}, {"title": "6.2 Online algorithm", "content": "In the online case (see Algorithm 2), the agent keeps an estimate of the exponentiated gain f = exp(\\(\\eta\\)p) which is updated every timestep. It also keeps estimates of the value functions of the base LMDPS \\((; \\hat{p}), ..., ^i(; \\hat{p})\\) for each equivalence class Ci, and estimates of the value function on exit states \\(\\hat{z}_E\\). All the base LMDPs of the same equivalence class can be updated with the same sample using intra-task learning with the appropriate importance sampling weights [13].\nFor the estimates of the exit states, we only update them upon visita-tion of such states. In that case, we use the compositionality expression in (15) to derive the following update:\n\\[\n\\hat{z}_E(8) \\leftarrow (1 - \\alpha_\\varepsilon)\\hat{z}_E(s) + (1 - \\alpha_\\varepsilon) \\sum_{k=1}^{n} z(^) z(s; \\rho). \\quad (17)\n\\]\nHere, \\(\\alpha_e\\) is the learning rate. Each of the learned components (i.e., gain, base LMDPs and exit state value estimates) maintain indepen-dent learning rates."}, {"title": "7 Experiments", "content": "In this section we compare experimentally Algorithm 2 with differ-ential soft TD-learning in the flat representation of the ALMDP. We measure the Mean Absolute Error (MAE) between the estimated value function 2 and the true optimal value function z. For each algorithm, we report mean and standard deviation on five seeds. The learning rates have been optimized independently for each of the instances.\nWe adapt two episodic benchmark tasks [12] and transform them into infinite-horizon tasks as follows:\nN-room domain, cf. Example 1. As in the episodic case, there are some 'goal' states with high reward (i.e. 0). When the agent enters a goal state, the next action causes it to receive the given reward and transition to a restart location. We vary the size of the rooms as well as the size of the rooms.\nTaxi domain. In this variant of the original domain [7], once the passenger has been dropped off, the system transitions to a state in which the driver is in the last drop-off location, but a new passenger appears randomly at another location.\nFigures 2 and 3 show the results. Our algorithm is able to speed up learning and converges to the optimal solution faster than flat average-reward reinforcement learning (note the log scale). This is in line with previous results for the episodic case [12]. The difference in the error scale in the figures is due to the initialization of the base LMDPs. The average reward setting poses an extra challenge since the 'sparsity' of the reward can make the estimates of the gain oscillate. This ultimately has an impact on the estimates of the base LMDPS and the value estimates of the exit states, and it is likely the reason why in Figure 3 the error increases before decreasing down to zero."}, {"title": "8 Conclusion", "content": "In this paper we present a novel framework for hierarchical average-reward reinforcement learning which makes it possible to learn the low-level and high-level tasks simultaneously. We propose an eigen-vector approach and an online algorithm for solving problems in our framework, and show that the former converges to the optimal value function. As a by-product of our analysis, we also provide a convergence theorem in the non-hierarchical case for average-reward LMDPs, which to the extent of our knowledge, was not previously done. In the future we would like to prove convergence also for the proposed online algorithm."}, {"title": "A Proof Theorem 3", "content": "A.1 Preliminaries\nWe introduce the notation:\n\u2022 1 denotes an all-ones vector of length |S|.\n\u2022 I{p} is the indicator function that takes 1 when predicate p is true and 0 otherwise.\nWe assumme an underlying continuing LMDP L = (S, P, R) where S represents the state space, P the passive dynamics and R the reward function. Similarly to [Section B.1] in [27], we also assume there exists a set-valued process {X+} where Xt is a non-empty subset defined as Xt = {(s) : s component of v was updated at timestep t}.\nWe recall that the TD updates in the asynchronous case are\n\\[\nv_{t+1}(s) \\leftarrow \\hat{v}_t(s) + \\alpha_t(s)\\delta_t(s)I{s \\in X_t}, \\quad (18)\n\\]\n\\[\n\\rho_{t+1} \\leftarrow \\hat{\\rho}_t + \\alpha_t(s)\\delta_t(s)\u2161{s \\in X_t}. \\quad (19)\n\\]\nThe indicator II{s \u2208 Xt} specifies whether the value of state s updates at timestep t. The TD error for state s is\n\\[\n\\delta_t(s) = r_t(s) - \\hat{\\rho}_t + \\frac{1}{\\eta} log \\sum_{S'\\in S}P(s'|s)e^{\\eta\\hat{v}_t(s')} - \\hat{v}_t(s).\n\\]\nWe introduce a series of necessary assumptions for convergence. We adapt Assumptions B.1-B.5 in [27] to the case of LMDPs. Assumptions 1 and 9 are standard in average-reward settings, while Assumption 10 is the standard Robbins-Monro conditions for step sizes. Assumptions 11 and 12 are introduced in the convergence argument of RVI Q-learning by Borkar [4] and specify some requirements for the learning rates when asynchronous updates are performed. For more details we refer the reader to Section B.1 of [27].\nAssumption 9. (Value function uniqueness) There exists a unique solution to v in equation (8) up to a constant shift.\nAssumption 10. (Stepsize assumption)\n\\[\n\\sum_{t=0}^{\\infty} a_t > 0, \\quad \\sum_{t=0}^{\\infty} a_t^2 < \\infty.\n\\]\nAssumption 11. (Asynchronous Stepsize 1) Let [\u00b7] denote the integer part of (\u00b7), for x \u2208 (0,1)\n\\[\n\\sup_{i} \\frac{a_{\\[x_i\\]}}{a_i} < \\infty\n\\]\nand\n\\[\n\\frac{\\sum_{i=0}^{\\[yi\\]} a_i}{\\sum_{i=0}^{\\[xi\\]} a_i} \\rightarrow 1\n\\]\nuniformly in y \u2208 [x, 1].\nAssumption 12. (Asynchronous Stepsize 2) There exists \u25b3 > 0 such that\n\\[\n\\liminf_{t\\rightarrow \\infty} \\frac{v(t,s)}{t+1} > \\Delta\n\\]\nalmost surely, for all s \u2208 S. Here v(t, s) represents the visitation count for state s up to timestep t. Furthermore, for all x > 0, let\n\\[\nN(t, x) = min {m > t : a_i < x}, \\quad m(t,x,s) = \\sum_{i=t+1}^{N(t, x)} a_i}.\n\\]\nthe limit\n\\[\n\\lim_{t \\rightarrow \\infty} \\frac{\\frac{\\partial \\mathcal{L}iv(t,s)}{\\partial i}}{\\frac{\\partial \\mathcal{L}iv(t,s')}{\\partial i}}\n\\]\nexists for all s, s' \u2208 S.\nUnder the communication assumption, the system\n\\[\nv(s) = R(s) - \\rho + \\frac{1}{\\eta} log \\sum_{S'\\in S}P(s'|s)e^{\\eta v(s')}, \\forall s \\in S,\n\\]\n\\[\n\\rho - \\rho_0 = \\lambda (\\sum v(s) - \\sum v_0(s)),\n\\]"}]}