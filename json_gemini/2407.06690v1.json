{"title": "Hierarchical Average-Reward\nLinearly-solvable Markov Decision Processes", "authors": ["Guillermo Infante", "Anders Jonsson", "Vicen\u00e7 G\u00f3mez"], "abstract": "We introduce a novel approach to hierarchical rein-\nforcement learning for Linearly-solvable Markov Decision Processes\n(LMDPs) in the infinite-horizon average-reward setting. Unlike pre-\nvious work, our approach allows learning low-level and high-level\ntasks simultaneously, without imposing limiting restrictions on the\nlow-level tasks. Our method relies on partitions of the state space that\ncreate smaller subtasks that are easier to solve, and the equivalence\nbetween such partitions to learn more efficiently. We then exploit\nthe compositionality of low-level tasks to exactly represent the value\nfunction of the high-level task. Experiments show that our approach\ncan outperform flat average-reward reinforcement learning by one or\nseveral orders of magnitude.", "sections": [{"title": "1 Introduction", "content": "Hierarchical reinforcement learning (RL) [6, 19, 22, 7, 3] aims to\ndecompose a complex high-level task into several low-level tasks.\nAfter solving the low-level tasks, their solutions can be combined\nto form a solution to the high-level task. Ideally, the low-level tasks\nshould be significantly easier to solve than the high-level task, in\nwhich case one can obtain an important speedup in learning [16, 29].\nHierarchical RL has also been credited with other advantages, e.g. the\nability to explore more efficiently [17].\nMost previous work on hierarchical RL considers either the finite-\nhorizon setting or the infinite-horizon setting with discounted rewards.\nThe average-reward setting is better suited for cyclical tasks character-\nized by continuous experience. In the few works on hierarchical RL in\nthe average-reward setting, either the low-level tasks are assumed to\nbe solved beforehand [8, 9, 28] or they have important restrictions that\nseverely reduce their applicability, e.g. a single initial state [10]. It is\ntherefore an open question how to develop algorithms for hierarchical\nRL in the average-reward setting in order to learn the low-level and\nhigh-level tasks simultaneously.\nIn this paper we propose a novel framework for hierarchical RL\nin the average-reward setting that simultaneously solves low-level\nand high-level tasks. Concretely, we consider the class of Linearly-\nsolvable Markov Decision Processes (LMDPs) [23]. LMDPs are a\nclass of restricted MDPs for which the Bellman equation can be\nexactly transformed into a linear equation. This class of problems\nplays a key role in the framework of RL as probabilistic inference [14,\n15]. One of the properties of LMDPs is compositionality: one can\ncompute the solution to a novel task from the solutions to previously\nsolved tasks without learning [25]. Such a property has been exploited\nin recent works about compositionality in RL [11, 26, 18] and in\ncombination with Hierarchical RL in the finite-horizon setting [13, 21,\n12]. Adapting this idea to the average-reward setting requires careful\nanalysis.\nUnlike most frameworks for hierarchical RL, our proposed ap-\nproach does not decompose the policy, only the value function. Hence\nthe agent never chooses a subtask to solve, and instead uses the sub-\ntasks to compose the value function of the high-level task. This avoids\nintroducing non-stationarity at the higher level when updating the\nlow-level policies.\nOur work makes the following novel contributions:\n\u2022 Learning low-level and high-level tasks simultaneously in the\naverage-reward setting, without imposing additional restrictions on\nthe low-level tasks.\n\u2022 Two novel algorithms for solving hierarchical RL in the average\nreward setting: the first one is based on the eigenvector approach\nused for solving LMDPs. The second is an online variant in which\nan agent learns simultaneously the low-level and high-level tasks.\n\u2022 Two main theoretical contributions LMDPs: a converge proofs for\nboth differential soft TD-learning for (non-hierarchical) LMDPS\nand also for the eigenvector approach in the hierarchical case.\nTo the best of our knowledge, this work is the first that extends the\ncombination of compositionality and hierarchical RL to the average-\nreward setting."}, {"title": "2 Related work", "content": "Most research on hierarchical RL formulates problems as a Semi-\nMarkov Decision Process (SMDP) with options [22] or the MAXQ\ndecomposition [7].\nFruit and Lazaric [8] and [9] propose algorithms for solving SMDPS\nwith options in the average-reward setting, proving that the regret of\ntheir algorithms is polynomial in the size of the SMDP components,\nwhich may be smaller than the components of the underlying Markov\nDecision Process (MDP). Wan et al. [28] present a version of dif-\nferential Q-learning for SMDPs with options in the average-reward\nsetting, proving that differential Q-learning converges to the optimal\npolicy. However, the above work assumes that the option policies are\ngiven prior to learning. Ghavamzadeh and Mahadevan [10] propose a\nframework for hierarchical average-reward RL based on the MAXQ\ndecomposition, in which low-level tasks are also modeled as average-\nreward decision processes. However, since the distribution over initial\nstates can change as the high-level policy changes, the authors restrict\nlow-level tasks to have a single initial state.\nWen et al. [29] present an approach for partitioning the state space\nand exploiting the equivalence of low-level tasks, similar to our work."}, {"title": "3 Background", "content": "Given a finite set X, we use \u2206(X) to denote the set of all probability\ndistributions on X."}, {"title": "3.1 First-exit Linearly-solvable Markov Decision\nProcesses", "content": "A first-exit Linearly-solvable Markov Decision Process\n(LMDP, Todorov [23]) is a tuple L = (S, T,P,R, I), where\nS is a set of non-terminal states, T is a set of terminal states,\nP: S\u2192\u2206(S+) is the passive dynamics, which describes state\ntransitions in the absence of controls, R: S\u2192 R is a reward\nfunction on non-terminal states, and I: T\u2192 R is a reward function\non terminal states. We use S+ = SUT to denote the full set of\nstates. An agent interacts with the environment following a policy\n\u03c0: S \u2192 \u2206(S+). At timestep t, it observes a state st, transitions to a\nnew state st+1 ~ \u03c0(.st) and receives a reward\n$R(s_t, s_{t+1}, \\pi) = r_t - \\frac{1}{\\eta} \\log \\frac{\\pi(s_{t+1}|s_t)}{P(s_{t+1}|s_t)},$"}, {"content": "where $r_t$ is a reward with mean $R(s_t)$. The agent can modify the\npolicy \u03c0(st), but gets penalized for deviating from the passive dy-\nnamics P(st). The parameter \u03b7 > 0 controls this penalty. Given\n\u03b7 > 0, the value function of a policy \u3160 can be defined as follows:\n$v^{\\pi}(s) = E_{\\pi, \\delta_0} [\\sum_{t=0}^{T-1} R(S_t, S_{t+1}, \\pi) + I(S_T) ] \\tag{1}$"}, {"content": "where T is a random variable representing the length of the episode,\nand St, t\u2265 0, are random variables denoting the state at time t.\nThe interaction ends when the agent reaches a terminal state ST and\nreceives reward I(ST). The value function of a terminal state T\u2208\u0422\nis simply v(T) = I(T).\nThe aim of the agent is to find a policy that maximizes expected\nfuture reward. For that it is useful to define the optimal value function\nv: S \u2192 R among all policies. For simplicity, in what follows we\nomit the subscript and asterisk and refer to the optimal value function\nsimply as the value function v. Such a value function is known to\nsatisfy the following Bellman optimality equations [23]:\n$v(s) = \\frac{1}{\\eta} \\log \\sum_{s'\\in S_+} P(s'|s) \\exp(\\eta (R(s) + v(s'))) \\quad \\forall s \\in S. \\tag{2}$"}, {"content": "Introducing the notation $z(s) = e^{\\eta v(s)}, s \\in S_+$, results in the follow-\ning system of linear equations:\n$z(s) = e^{\\eta R(s)} \\sum_{s'\\in S_+} P(s'|s) z(s') \\quad \\forall s \\in S. \\tag{3}$"}, {"content": "We abuse the notation and for simplicity refer to z(s) and v(s) in-\nterchangeably as the value of s. Given z, an optimal policy is given\nby\n$\\pi(s'|s) = \\frac{P(s'|s)z(s')}{\\sum_{s''\\in S_+} P(s''|s)z(s')} = \\frac{P(s'|s)z(s')}{G[z](s)}  \\tag{4}$"}, {"content": "The system of linear equations in (3) can then be written in matrix\nform when we know the passive dynamics and the reward functions.\nWe let P\u2208 R|S|\u00d7|S+| be a matrix such that P(s,s') = P(s'|s) and\nR\u2208 R|S|\u00d7|S| a diagonal matrix such that R(s,s) = enR(s). We also\nlet z be the vector form of the value z(s) for all states s \u2208 S and z+\nan extended vector that also includes the known value z(T) = enI(T)\nfor all terminal states T\u2208 \u03a4. The problem is then expressed as\n$z = RPz_+. \\tag{5}$"}, {"content": "We can use the power iteration method over (5) to obtain the solution\nfor z [23]. Power iteration is guaranteed to converge as long as the\ndiagonal matrix R is not too large, and a common assumption is that\nthe rewards of non-terminal states are non-positive (i.e. R(s) \u2264 0 for\neach s \u2208 S). However, we refrain from making any such assumptions,\nand later we instead prove convergence in an alternative way.\nAlternatively, when P and R are not known, the agent can learn\nan estimate of the optimal value function in an online manner,\nusing samples (St, rt, St+1) generated when following the estimated\npolicy derived from 2 using (4). The update rule for the so-called\nZ-learning algorithm is given by\n$\\hat{z}(s_t) \\leftarrow (1 - \\alpha_t) \\hat{z}(s_t) + \\alpha_t e^{\\eta R(s_t, s_{t+1}, \\hat{\\pi}_t)} \\hat{z}(s_{t+1}) \\frac{P(s_{t+1}|s_t)}{\\hat{\\pi}_t(s_{t+1}|s_t)} \\tag{6}$"}, {"content": "Here, at is a learning rate and $P(s_{t+1}|s_t)/\\hat{\\pi}_t(s_{t+1}|s_t)$ acts as an\nimportance weight.\nK\nIn the first-exit case, the solution of a set of component problems\ncan be combined to retrieve the optimal solution for new composite\nproblems with no further learning [25]. Assume we have a set of\nfirst-exit LMDPS {Li}=1, which share S, T, P and R, but differ\nin the values zi(T) = enIi(t) of terminal states. Let z1,...,ZK\nbe the optimal value functions of L1,..., LK. Now consider a new\ncomposite problem L that also shares the aforementioned elements\nwith the component problems. If the value at terminal states can be\nexpressed as a weighted sum as follows:\n$z(T) = \\sum_{i=1}^K W_i z_i(T) \\quad \\forall \\tau \\in T,$\nthen by linearity of the value function, the same expression holds for\nnon-terminal states [25]:\n$z(s) = \\sum_{i=1}^K W_i z_i(s) \\quad \\forall s \\in S.$"}, {"title": "3.2 Hierarchical Decomposition for LMDPs", "content": "Infante et al. [12] introduce a hierarchical decomposition for LMDPs.\nGiven a first-exit LMDP L = (S,T,P,R, I), the set of non-\nterminal states S is partitioned into L subsets {Si}=1. Each subset\nSi induces a subtask Li = (Si, Ti, Pi, Ri, Ji), i.e. an LMDP for\nwhich\n\u2022 The set of non-terminal states is Si."}, {"title": "\u2022 The set of terminal states $T_i = \\{ \\tau \\in S_+ \\setminus S_i : \\exists s \\in$\nSi s.t. P(\u03c4|s) > 0} includes all states in S+ \\ Si (terminal\nor non-terminal) that are reachable in one step from a state in Si.\n\u2022 Pi: Si\u2192\u2206(S+) and Ri: Si \u2192 R are the restrictions of P and\nR to Si, where S = Si U Ti denotes the full set of subtask states.\n\u2022 The reward of a terminal state T\u2208 Ti equals I\u00bf(T) = I(T) if\n\u0442\u2208T, and I\u00bf(t) = \u00fb(\u03c4) otherwise, where \u00fb(\u03c4) is the estimated\nvalue in L of the non-terminal state in \u315c \u2208 S \\ Si.", "content": "Intuitively, if the value zi (7) of each terminal state T\u2208 Ti equals its\noptimal value z(7) for the original LMDP L, then solving the subtask\nLi yields the optimal values of the states in Si. In practice, however,\nthe agent only has access to an estimate 2(7) of the optimal value. In\nthis case, the subtask Li is parameterized on the value estimate \u00fb (or\n2) of the terminal states Ti, and each time the value estimate changes,\nthe agent can solve Li to obtain a new value estimate 2(s) for each\nstate s E Si.\nThe set of exit states E = U=1Ti is the union of the terminal states\nof each subtask in {L1,..., LL}. We let E\u2081 = En Si denote the\nset of (non-terminal) exit states in the subtask Li. Also define the\nnotation K = max=1 |Si|, N = max=1 |Til and E = |E|.\nDefinition 1. Two subtasks Li and Lj are equivalent if there exists\na bijection f: Si \u2192 Sj such that the transition probabilities and\nrewards of non-terminal states are equivalent through f.\nUsing the above definition, we can define a set of equivalence\nclasses C = {C1,...,Cc}, C < L, i.e. a partition of the set of\nsubtasks {L1,..., LL} such that all subtasks in a given partition are\nequivalent. Each equivalence class can be represented by a single\nsubtask Lj = (Sj, Tj, Pj, Rj, Ij). As before, the reward I; of\nterminal states is parameterized on a given value estimate \u00fb. We\nassume that each non-terminal state s \u2208 S can be mapped to its\nsubtask Li and equivalence class Cj.\nFor each subtask Lj and each terminal state 7k \u2208 Tj, Infante et al.\n[12] introduce a base LMDP L = (Sj, Tj, Pj, Rj, I) that shares\nall components with Lj except the reward function on terminal states,\nwhich is defined as J(t) = 1 if r = rk, and J(t) = 0 otherwise.\nLet z},..., z be the optimal value functions of the base LMDPs for\nLj, with n = |Ti. Given a value estimate 2 on each terminal state in\nTj, due to compositionality we can express the value estimate of each\nstate s \u2208 Sj as\n$\\hat{z}(s) = \\sum_{k=1}^n \\hat{z}(s) z^k(s).$\nTo solve the original LMDP, we can now define an optimal value\nfunction on exit states as z\u025b : E \u2192 R, and construct a matrix G =\n[R|E|\u00d7|| whose element G(s,s') equals z\u0142 (s) if s\u2032 = 7* is the k-th\nterminal state in the subtask Lj corresponding to the partition Si to\nwhich s belongs, and 0 otherwise. By writing z\u025b in vector form, the\noptimal value function satisfies the following matrix equation:\n$z_E = Gz_E.$\nThe total number of values required to represent the optimal value\nfunction equals E + CKN, since there are C equivalence classes\nwith at most K states and N base LMDPs."}, {"title": "4 Average-reward Linearly-solvable Markov\nDecision Processes", "content": "An average-reward Linearly-solvable Markov Decision Process\n(ALMDP) is a tuple L = (S, P, R), where S is a set of states,\nP: S \u2192 \u2206(S) is the passive dynamics, which describes state transi-\ntions in the absence of controls, and R: S\u2192 R is a reward function.\nSince ALMDPs represent infinite-horizon tasks, there are no terminal\nstates.\nThroughout the paper, we make the following assumptions about\nALMDPS.\nAssumption 1. The ALMDP L is communicating [20]: for each\npair of states s,s' \u2208 S, there exists a policy w that has non-zero\nprobability of reaching s' from s.\nAssumption 2. The ALMDP L is unichain [20]: the transition prob-\nability distribution induced by all stationary policies admit a single\nrecurrent class.\nIn the average-reward setting, the value function is defined as the\nexpected average reward when following a policy \u03c0 : S \u2192 \u2206(S)\nstarting from a state s \u2208 S. This is expressed as\n$v^{\\pi}(s) = \\lim_{T\\to\\infty} E_{\\pi} [\\frac{1}{T} \\sum_{t=0}^{T} R(S_t, S_{t+1}, \\pi) | S_0 = s] , \\tag{7}$"}, {"content": "where R(st, St+1, \u03c0) is defined as for first-exit LMDPs. Again, we\nare interested in obtaining the optimal value function v. Under As-\nsumption 2, the Bellman optimality equations can be written as\n$v(s) = \\frac{1}{\\eta} \\log \\sum_{s'\\in S} P(s'|s) \\exp(\\eta (R(s) - \\rho + v(s'))) \\quad \\forall s \\in S, \\tag{8}$"}, {"content": "where p is the optimal one-step average reward (i.e. gain), which is\nstate-independent for unichain ALMDPs [23]. Exponentiating yields\n$z(s) = e^{\\eta (R(s) - \\rho)} \\sum_{S'\\in S} P(s'|s) z(s') \\quad \\forall s \\in S. \\tag{9}$"}, {"content": "For the optimal value function z, the optimal policy is given by the\nsame expression as in (4)."}, {"title": "4.1 Solving an ALMDP", "content": "We introduce the notation \u0393 = enp (exponentiated gain). Similar to\nthe first-exit case, Equation (9) can be expressed in matrix form as\n$\\Gamma z = RPz, \\tag{10}$"}, {"content": "where the matrices P\u2208 R|S|\u00d7|S| and R \u2208 R|S|\u00d7|S| are appropi-\nately defined as in (5). The exponentiated gain I can be shown to\ncorrespond to the largest eigenvalue of RP [24]. An ALMDP can\nbe solved using relative value iteration by selecting a reference state\ns* \u2208 S, initializing 20 = 1 and iteratively applying\n$z_{k+1} \\leftarrow R P z_k,$\n$z_{k+1} \\leftarrow z_{k+1}/z_{k+1}(s^*).$\nThe reference state s* satisfies z(s*) = 1, which makes the optimal\nvalue z unique (else any constant shift preserves optimality). After\nconvergence, the exponentiated gain equals \u0393 z2k+1 (s*). Under\nAssumption 1, relative value iteration converges to the unique optimal\nvalue z [24].\nAn alternative method for solving an ALMDP L is to transform it to\na first-exit LMDP. Given a reference state s*, define a first-exit LMDP\nL' = (S \\ {s*}, {s*}, P', R', J'), where P'(s'|s) = P(s'|s) for\nall state pairs (s,s') \u2208 (S \\ {s*}) \u00d7 S, and J(s*) = 0 (imply-\ning z(s*) = 1). By inspection of (3) and (9), the Bellman optimal-\nity equation of L' is identical to that of L if R'(s) = R(s) \u2013 \u03c1."}, {"title": "Even though the agent has no prior knowledge of the exponenti-\nated gain F = enp, we can perform binary search to find \u0393. For a\ngiven estimate f of \u0393, after solving L', we can compare z(s*) with\nenR(s*) \u03a3\u03b5P(s\\s*)z(s). If fz(s*) is greater, then f is too large,\nelse it is too small.\nAlternatively, when P and R are not known, we can obtain an\nestimate \u00fb of the optimal value v and an estimate p of the optimal\ngain p using differential soft TD-learning, similar to differential Q-\nlearning [27]. We collect samples (st, rt, St+1) generated by the\nestimated policy \u00fb derived from \u00fb as in (4), and derive update rules\nfor \u00fb and p from (8) as follows:\n$\\hat{v}_{t+1}(s_t) \\leftarrow \\hat{v}_t(s_t) + \\alpha_t \\delta_t, \\tag{11}$", "content": "Here, the TD error dt is given by\n$\\delta_t = r_t - \\hat{\\rho}_t - \\frac{1}{\\eta} \\log \\frac{\\hat{\\pi}_t(s_{t+1}|s_t)}{P(s_{t+1}|s_t)} \\tag{12}$"}, {"content": "Note that both updates use the same TD error. At any time, we can\nretrieve estimates of \u00f4 and by exponentiating \u00fb and \u017f\u00f4, respectively.\nTheorem 3. Under mild assumptions, differential soft TD-learning\nin (11) and (12) converges to the optimal values of v and pin L.\nProof sketch. The proof is adapted from the proof of convergence\nof differential Q-learning [1, 27], which requires the ALMDP to be\ncommunicating (Assumption 1). Define a Bellman operator T as\n$T(v)(s) = R(s) + \\frac{1}{\\eta} \\log \\sum_{s'\\in S} P(s'|s) e^{\\eta v(s')}.$"}, {"content": "To adapt the previous proof, it is sufficient to show that T is a non-\nexpansion in the max norm, i.e. ||T(x) - T(y) || \u2264 ||x - y || for\neach x, y \u2208 RISI, and that T satisfies T(x + c1) = T(x) + c1 for\neach x \u2208 RIS and constant c \u2208 R, where 1 is the vector of |S| ones.\nFor completeness, the full proof appears in Appendix A."}, {"title": "5 Hierarchical Average-Reward LMDPS", "content": "In this section we present our approach for hierarchical average-\nreward LMDPs. The idea is to take advantage of the similarity of\nthe value functions in the first-exit and average-reward settings, and\nuse compositionality to compose the value functions of the subtask\nLMDPs without additional learning."}, {"title": "5.1 Hierarchical Decomposition", "content": "Consider an ALMDP (S, P, R). Similarly to Infante et al. [12],\nwe assume that the state space S is partitioned into subsets\n{S}=1, with each partition S\u2081 inducing a first-exit LMDP Li =\n(Si, Ti, Pi, Ri, Ti). The components of each such subtask Li are\ndefined as follows:\n\u2022 The set of states is Si.\n\u2022 The set of terminal states T\u2081 = {t \u2208 S \\ S\u00bf : \u2203s \u2208 Si, P(T|s) >\n0} contains states not in S\u2081 that are reachable in one step from any\nstate inside the partition."}, {"title": "\u2022 The transition function Pi and reward function Ri are projections\nof P and R \u2013 ponto Si, where p is a gain estimate.\n\u2022 It is defined for each r\u2208 Ti as Ti(t) = \u00ee(\u03c4), where \u00fb is a\ncurrent value estimate (hence zi(T) = en\u00fb(\u03c4) = 2(\u03c4) is defined\nby a current exponentiated value estimate 2).", "content": "The Bellman optimality equations of each subtask Li are given by\n$z_i(s) = e^{\\eta R_i(s)} \\sum_{s'} P_i(s'|s) z_i(s') \\quad \\forall s \\in S_i. \\tag{13}$"}, {"content": "By inspection of the Bellman optimality equations in (9) and (13),\nthey are equal if Ri(s) = R(s) p. Thus, if zi(T) = z(7) for each\nTE Ti then the solution of the subtask Li corresponds to the optimal\nsolution for each s \u2208 Si. However, in general neither p nor z(7) are\nknown prior to learning and, therefore, we have to use estimates p\nand 2(7). Each subtask Li can be seen as being parameterized on the\nvalue estimates 2(T) for each \u2208 Tj and the gain estimate \u00ee. Every\ntime that (\u03c4), \u03c4\u2208 Ti, and \u00f4 change, we obtain a new value estimate\nfor each s \u2208 Si by solving the subtask for the new parameters."}, {"title": "5.2 Subtask Compositionality", "content": "It is impractical to solve each subtask Li every time the estimate\n2(T) changes for T\u2208 Tj. To alleviate this computation we leverage\ncompositionality for LMDPs. The key insight is to build a basis of\nvalue functions that can be combined to obtain the solution for the\nsubtasks.\nConsider a subtask Li = (Si, Ti, Pi, Ri, Ji) and let n = |Ti|. We\nintroduce n base LMDPs {L, . . ., L} that are first-exit LMDPs and\nterminate in Ti. These base LMDPs only differ from Li in the reward\nof each terminal state rk \u2208 Ti. For all s \u2208 S\u0129, the reward for each L\nis by definition Ri(s) = R(s) \u2013 \u00f4 for all s \u2208 S\u2081, while at terminal\nstates T\u2208 Ti we let the reward function be z\u0142 (r; p) = 1 if t = tk\nand z (r; p) = 0 otherwise. Thus, the base LMDPs are parameterized\nby the gain estimate \u00f4. This is equivalent to setting the reward to\nI(T) = 0 if t = tk and J\u00a5* (t) = -\u221e otherwise. Intuitively, each\nbase LMDP solves the subtask of reaching one specific terminal state\nTk E Ti.\nLet us now assume that we have the solution z\u0142 (\u00b7; p), ..., z (\u00b7; \u03c1)\nfor the base-LMDPs (for the optimal gain p) as well as the optimal\nvalue z(7k) of the original ALMDP for each terminal state Tk \u2208 Ti.\nThen by compositionality we could represent the value function of\neach terminal state as a weighted combination of the subtasks:\n$\\hat{z}(T) = \\sum_{k=1}^n W_k z^k(T; \\rho) = \\sum_{k=1}^n \\hat{z}(T^*) z^k(\\tau; \\rho) \\quad \\forall \\tau \\in T_i. \\tag{14}$"}, {"content": "Clearly, the RHS in the previous expression evaluates to z(7) since\nz(tk)z(t;p) = x(t) \u00b7 1 when r = tk, and z(tk)z(t;p) =\nz(7k) 0 otherwise.\nThanks to compositionality, we can also represent the value func-\ntion for each subtask state s \u2208 Si as\n$\\hat{z}(s) = \\sum_{k=1}^n \\hat{z}(T_k) z^k(s; \\rho) \\quad \\forall s \\in S_i. \\tag{15}$"}, {"content": "We remark that the base LMDPs depend on the gain p by the definition\nof the reward function. This parameter is not known prior to learning.\nThe subtasks in practice are solved for the latest estimate \u00ee and must\nbe re-learned for every update of this parameter until convergence."}, {"title": "5.3 Efficiency of the value representation", "content": "Similar to previous work [29, 12] we can exploit the equivalence of\nsubtasks to learn more efficiently. Let C = {C1,..., Cc}, C < L,\nbe a set of equivalence classes, i.e. a partition of the set of subtasks\n{L1,..., LL} such that all subtasks in a given partition are equivalent.\nAs before, we also define a set of exit states as E = U1 Ti. Due to\nthe decomposition, we do not need to keep an explicit value estimate\n2(s) for every state s \u2208 S. Instead, it is sufficient to keep a value\nfunction for exit states 2\u03b5: E \u2192 R and a value function for each\nbase LMDP of each equivalence class. This is enough to represent\nthe value for any state s \u2208 S using the compositionality expression\nin (15).\nLetting K = max=1|Si, N = max=1 Til and E = |E|,\nO(KN) values are needed to represent the base LMDPs of a subtask,\nand we can thus represent the value function with O(CKN + E)\nvalues. The decomposition leads to an efficient representation of the\nvalue function whenever CKN + E \u00ab S. This is achieved when\nthere are few equivalence classes, the size of each subtask is small (in\nterms of the number of states) and there are relatively few exit states.\nExample 1: Figure 1a) shows an example 4-room ALMDP. When\nreaching the state marked G, separate from the room but reachable\nin one step from the highlighted location, the system transitions to\na restart state (top left corner) and receives reward 0. In all other\nstates the reward is -1. The rooms are connected via doorways, so\nthe subtask corresponding to each room has two terminal states in\nother rooms, plus the terminal state G in the top right room. The 9 exit\nstates in & are highlighted and correspond to states next to doorways,\nplus G. Figure 1b) shows a single subtask that is equivalent to all four\nroom subtasks, since the dynamics is shared inside rooms and the set\nof terminal states is the same. There are five base LMDPs with value\nfunctions zG, z, zR, zT and zB, respectively. Given an initial value\nestimate \u017ee for each exit state in E, a value estimate of any state in\nthe top left room is given by \u00ee(s) = \u03bb\u03b5(1B)zB (s) + 2\u03b5(1R)zR(s),\nwhere we use \u017e\u03b5(G) = 2\u03b5(L) = 2\u03b5(T) = 0 to indicate that the\nterminal states G, L and T are not reachable in the top left room. The\ntotal number of values needed to store the optimal value function is\nE + CKN = 9+125 = 134, and the base LMDPs are faster to\nlearn since they have smaller state space."}, {"title": "6 Algorithms", "content": "We now propose two algorithms for solving hierarchical ALMDPS.\nThe first is a two-stage eigenvector approach that relies on first solving\nthe subtasks. The second is an online algorithm in which an agent\nsimultaneously learns the subtasks, the gain and the exit values from\nsamples (st, rt, St+1). Once again we recall that we do not explicitly\nrepresent the values for states s & E."}, {"title": "6.1 Eigenvector approach", "content": "In previous work, the base LMDPs are only solved once, and the\nsolutions are then reused to compute the value function ze on exit\nstates. However, in the case of ALMDPs, the reward functions of\nbase LMDPs depend on the current gain estimate \u00ee, which is initially\nunknown.\nOur proposed algorithm appears in Algorithm 1. The intuition is\nthat in each iteration, we first solve the subtasks for the latest estimate\nof the exponentiated gain . For this, we use (13) with the current\nvalue of p to solve the base LMDPs. We then apply (15) restricted to\nE to obtain an estimate of the value for the exit states. This yields the\nsystem of linear equations\n$z_E = G_\\epsilon z_E. \\tag{16}$"}, {"content": "Here, the matrix G\u025b \u2208 R\ud2f0\uc9c0\ud2f0 contains the optimal values of the\nbase"}]}