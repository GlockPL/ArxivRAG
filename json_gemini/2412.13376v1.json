{"title": "Targeted View-Invariant Adversarial Perturbations for 3D Object Recognition", "authors": ["Christian Green", "Mehmet Ergezer", "Abdurrahman Zeybey"], "abstract": "Adversarial attacks pose significant challenges in 3D object recognition, especially in scenarios involving multi-view analysis where objects can be observed from varying angles. This paper introduces View-Invariant Adversarial Perturbations (VIAP), a novel method for crafting robust adversarial examples that remain effective across multiple view-points. Unlike traditional methods, VIAP enables targeted attacks capable of manipulating recognition systems to classify objects as specific, pre-determined labels, all while using a single universal perturbation. Leveraging a dataset of 1,210 images across 121 diverse rendered 3D objects, we demonstrate the effectiveness of VIAP in both targeted and untargeted settings. Our untargeted perturbations successfully generate a singular adversarial noise robust to 3D transformations, while targeted attacks achieve exceptional results, with top-1 accuracies exceeding 95% across various epsilon values. These findings highlight VIAP's potential for real-world applications, such as testing the robustness of 3D recognition systems. The proposed method sets a new benchmark for view-invariant adversarial robustness, advancing the field of adversarial machine learning for 3D object recognition.", "sections": [{"title": "Introduction", "content": "Adversarial attacks have emerged as a critical area of research within artificial intelligence, exposing vulnerabilities in machine learning models that can be exploited to manipulate their outputs (Carlini and Wagner 2017). These attacks, often imperceptible to human observers, pose significant risks to real-world applications, particularly in the context of security (Sharif et al. 2019). As AI models continue to advance, so too does the urgency of understanding and addressing their susceptibility to adversarial threats (Ren et al. 2020).\nConventional adversarial attacks primarily focus on generating perturbations for single views of objects, exploiting limitations in 2D image understanding (Szegedy et al. 2013; Madry et al. 2017). These attacks face limitations when applied to 3D objects, as they often lack robustness to real-world variations in perspective. Unlike 2D systems, 3D recognition systems must contend with viewpoint variability, transformations, and real-world conditions, making it difficult to craft perturbations that generalize effectively across multiple perspectives.\nThis paper introduces View-Invariant Adversarial Perturbations (VIAP), a novel framework designed to overcome these challenges by generating robust, universal perturbations for 3D objects. VIAP enables adversarial attacks that maintain their effectiveness across diverse transformations of a 3D object (Zhi et al. 2018), including rotations and viewpoint changes, while also allowing targeted manipulation of class labels (Athalye et al. 2018). Unlike prior methods that primarily focus on non-targeted attacks (Ergezer et al. 2024), VIAP extends adversarial capabilities to precise, controlled targeted attacks, broadening its practical applicability.\nProblem and Contributions Adversarial attacks in 3D object recognition confront a fundamental challenge: developing perturbations that remain effective across diverse viewpoints. Existing adversarial perturbation methods suffer from two critical limitations: (1) poor generalizability across multi-angle views, and (2) restricted capability for targeted attacks. Our work directly addresses these constraints through three key contributions:\n\u2022 A novel method for generating view-invariant universal perturbations that maintain effectiveness across multiple object perspectives.\n\u2022 A mathematical formalization of view-invariance in adversarial attacks, providing theoretical grounding for multi-view robustness.\n\u2022 Experimental validation demonstrating superior performance in both targeted and untargeted scenarios, outperforming established baseline methods.\nBy introducing VIAP, we bridge critical gaps in current adversarial machine learning approaches, enabling more sophisticated and reliable attacks that can adapt to complex 3D recognition environments.\nIn the following sections, we provide a review of related work to contextualize our contributions and identify existing gaps in adversarial research. We then present our targeted"}, {"title": "Related Work", "content": "This section reviews foundational adversarial attack methods, highlighting their principles and limitations, which set the stage for the proposed targeted View-Invariant Adversarial Perturbations approach discussed in the next section.\nFast Gradient Sign Method (FGSM): One of the earliest adversarial attack methods, was proposed in 2014 (Goodfellow, Shlens, and Szegedy 2015). FGSM quickly gained popularity due to its simplicity, low computational cost, and ease of implementation. The method relies on attacks where the assailant knows the model's architecture and weights. However, this reliance limits FGSM's effectiveness in settings where only the input features are accessible.\nFGSM works by calculating the gradient of the loss with respect to the input image and then updating the image's pixels in a direction that maximizes this loss. In Equation 1, we let X and y be the input image and true label respectively. We assume X as a 3-D matrix (width \u00d7 height \u00d7 color). The perturbation strength is controlled by \u0454. $\\nabla_xJ(X, Y_{true})$ is the gradient of the model's loss with respect to X. The sign of the gradient determines the direction of the perturbation. We initialize $X^{adv} = X$.\n$X^{adv} = X + \\epsilon \\cdot sign(\\nabla_xJ(X, Y_{true}))$ (1)\nWhile FGSM is primarily used for untargeted attacks, it can be adapted for targeted attacks by adjusting the gradient direction to minimize the loss between the targeted and predicted labels, as shown in Equation 2.\n$X^{adv} = X + \\epsilon \\cdot sign(\\nabla_x J(X, Y_{target}))$ (2)\nThe Basic Iterative Method (BIM) Extends the idea of FGSM by applying the perturbation iteratively, allowing for a more effective adversarial attack (Kurakin, Goodfellow, and Bengio 2017). By clipping intermediate results at each step, BIM ensures that the perturbation stays within a defined range, as outlined in Equation 3. This iterative approach typically enhances the success rate of the attack compared to the single-step FGSM. As with FGSM, we initialize BIM, $X^{adv} = X$. N in Equation 3 represents the iteration count.\n$X_{N+1}^{adv} = Clip_{X, \\epsilon} \\{X_{N}^{adv} + \\epsilon \\cdot sign(\\nabla_xJ(X_{N}^{adv}, Y_{true}))\\}$ (3)\nUntargeted View-Invariant Adversarial Perturbations: The conventional process of generating an adversarial noise involves crafting a unique perturbation for an image. The noise is applied to the image and fed through a classifier,"}, {"title": "Methodology", "content": "In this section, we introduce the targeted View-Invariant Adversarial Perturbations method and detail the enhancements made to the existing algorithm which allow it to perform in targeted environments. We first describe the dataset and preprocessing steps, followed by the specifics of the perturbation generation process, and conclude with the evaluation metrics and experimental setup.\nDataset and Preprocessing\nOur experiments utilize a dataset comprising 1,210 images of 121 distinct 3D objects, each rendered from multiple viewpoints. The objects were selected to cover a diverse range of categories, ensuring the robustness of the proposed method across different types of 3D shapes and textures. 3D models were downloaded for six objects (Bes 2017), (soufiane oujihi 2024), (Europac3d 2024), (Christian 2019), (CGI 2023), (dannyboy70000 2014), (Alexandrescu 2023), (selfie 3D scan 2019), and the rest were obtained through a dataset (Deitke et al. 2023). Each of the 121 objects fell under one of 14 distinct class labels: baseball, snail, acorn, conch, pretzel, lemon, broccoli, tractor, backpack, banana, dumbbell, pineapple, strawberry, and teddy bear.\nImage Rendering: Each object was rendered from 10 different viewpoints, simulating real-world conditions where objects may be viewed from various angles. Renders of random camera orientation for two of the objects are shown in Figure 1.\nPreprocessing: The images were resized to a standard resolution, (224, 224) pixels, to maintain consistency across the dataset. Standard preprocessing techniques such as normalization were applied to the images to ensure they are suitable inputs for the neural networks.\nMathematical Formalization of View-Invariance\nTo quantify the view-invariance property of the proposed targeted perturbation, we define a set of transformations T that represent the changes in viewpoint, including rotation, translation, and scaling. Let $X_{(\\theta, \\phi)}$ denote an image of object X rendered at a viewpoint specified by angles $(\\theta, \\phi)$. The targeted VIAP $\u03b4$ is optimized to ensure that for any transformation T\u2208 T, the following holds:\n$arg \\underset{T}{max} f(X_{(\\theta, \\phi)} + \\delta) = Y_{target}, \\forall (\\theta, \\phi) \\in \\Theta \\times \\Phi$,\nwhere f(\u00b7) represents the classifier function and $Y_{target}$ is the target class label.\nTo achieve this, we modify the gradient calculation to incorporate the distribution over multiple transformations:\n$\\nabla_{\\delta}J(\\delta) = E_{T \\sim T} [\\nabla_{\\delta}J(f(X_{T}) + \\delta), Y_{target})]$.\nThis ensures that the perturbation $\u03b4$ remains effective under different viewpoints, capturing the multi-view robustness of the attack."}, {"title": "Targeted View-Invariant Adversarial Perturbations", "content": "We extend the View-Invariant Adversarial Perturbations method, from Equation 4 to incorporate targeted attacks, allowing for precise adversarial manipulation. Targeted attacks are achieved by computing the cross-entropy loss between the target label and the predicted adversarial example. The gradient of the loss with respect to the calculated perturbation was then multiplied by a negative one. This ensures that through each training iteration, the perturbation loss is minimized. This is formulated in Equation 5 and in Algorithm 1 where Clip limits the perturbed image values.\n$X_{N+1}^{adv} = Clip_{X, \\epsilon} \\{X_{N}^{adv} - \\epsilon \\cdot sign(\\nabla_xJ(X_{N}^{adv}, Y_{target}))\\}$ (5)"}, {"title": "Experimental Setup", "content": "To showcase the effectiveness of our model in generating robust noise across multiple angles, we conducted experiments comparing our View-Invariant Adversarial Perturbations method against FGSM and BIM.\nWe start by rendering our 3D objects in Blender to gain multiple 2D images from different viewpoints. Depending on the object, we take 2D images straight from a 3D rendered dataset. Eight of the objects were rendered from ten distinct viewing angles, ensuring consistent recognition by our classification model across perspectives. The remaining objects were gathered from 3D render datasets consisting of multiple images of an object at different angles. For the rendered images gathered through Blender, camera angles were randomly selected based on a spherical coordinate system centered around the object, incorporating a 15% random deviation for robustness. The slight changes in the coordinates, mimicked natural variations in viewing angles encountered in real-world environments. Consequently, constant lighting positions relative to the camera angles generated shadowed areas that hindered initial MobileNetV2 recognition. To combat such challenges it was necessary to restrict the 15% deviation on certain objects to one axis.\nThe images are split into two sets, one training and one test. A single perturbation is developed, applicable to all images in the training set. The noise is added to all images before inferring with MobileNetV2 (Sandler et al. 2019). The test set is used to evaluate the generalizability of the perturbation. All steps are repeated for the BIM and FGSM attacks. Each experiment is done twice to incorporate both targeted and untargeted use cases. Before running any experiments, we gathered baseline results to ensure the images were properly classified without any adversarial noise.\nIn cases of targeted attacks, we reduced biases by randomly selecting a label from the 1,000 classes our classifier was trained on for each trial of our experiments. If the randomized target label was chosen to be equal to the true label, we randomly sampled another label."}, {"title": "Evaluation Metrics", "content": "To assess the effectiveness of the targeted View-Invariant Adversarial Perturbations, we employ several key metrics:\nTop-1 Accuracy: Equation 6 evaluates the attack success by measuring the drop in top-1 accuracy for both untargeted and targeted perturbations. For targeted attacks, Equation 7 monitors the accuracy of the model in misclassifying images as the targeted class.\n$\\text{Top-1 Acc} = \\frac{\\Sigma \\text{ Top-1 True Label Predicts}}{\\sum \\text{ Total Predicts}}$ (6)\n$\\text{Top-1 Target Acc} = \\frac{\\Sigma \\text{ Top-1 Target Label Predicts}}{\\sum \\text{ Total Predicts}}$ (7)\nPerturbation Robustness: We test the perturbation's robustness by applying it to unseen viewpoints and verifying its effectiveness.\nParameter Selection Rationale: Parameter e controls the perturbation magnitude and is chosen based on a balance be-"}, {"title": "Results", "content": "To showcase the effectiveness of our model in generating robust noise across multiple angles, we conducted experiments comparing our View-Invariant Adversarial Perturbations method with common adversarial attack methods: FGSM and BIM. The objective was to identify the optimal noise level that significantly reduces a classifier's confidence in the object's true class while also increasing the target class' confidence.\nUntargeted Attack Confidence"}, {"title": "Targeted Attack Confidence", "content": ""}, {"title": "Discussion", "content": "The results of our study have significant implications for adversarial attack and defense strategies in multi-view recognition systems. This work highlights the potential for robust adversarial manipulation in real-world applications by demonstrating the effectiveness of targeted VIAP perturbations across diverse viewpoints.\nLimitations Discussion: Our current method, while robust in multi-view synthetic data, may exhibit reduced effectiveness when applied to complex, real-world 3D environments due to unmodeled factors such as texture variations and lighting inconsistencies. Future work will explore adaptive training techniques and real-world dataset validation to address these limitations.\nPotential applications of this method include evaluating the robustness of security systems, enhancing adversarial training processes, and benchmarking multi-view recognition algorithms under adversarial conditions. Future studies could also examine the interplay between perturbation strength and visual perceptibility to balance efficacy with detectability."}, {"title": "Conclusion", "content": "This paper introduced a novel approach for generating View-Invariant Adversarial Perturbations and optimizing adversarial robustness across various perspectives. By focusing on 3D-rendered images of objects viewed from multiple angles, we emphasized the method's utility in practical applications. The experimental results validate the effectiveness of our approach in both targeted and untargeted scenarios, with notable success in the targeted category, achieving a top-1 accuracy exceeding 95% for most tested epsilon values.\nOur method demonstrates superior performance in comparison to established techniques like FGSM and BIM, particularly in terms of transferability and generalization to unseen data. Furthermore, we successfully extend our proposed algorithm's applicability to targeted use cases, establishing its capability for more controlled adversarial alterations. Results indicate that between \u20ac = 0.5 and \u20ac = 5.0, the untargeted VIAP attack displays the highest attack success rate. Targeted VIAP attacks exhibited the best top-1 target accuracy for test images while requiring the least amount of computational effort. The importance of the View-Invariant Adversarial Perturbations method is highlighted by its ability to operate exclusively on 2D images, offering a practical and scalable alternative to computationally costly 3D adversarial attacks.\nIn future work, we plan to extend our experiments to real-world 3D objects and explore the application of this method to other adversarial scenarios beyond image classification, such as object detection and segmentation. Moreover, enhancing the computational efficiency of our method will be critical to its deployment in real-time systems. Ultimately, the goal is to develop a comprehensive framework that addresses a wide range of adversarial challenges, ensuring the safety and reliability of AI-driven technologies."}]}