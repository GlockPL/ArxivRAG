{"title": "INTERPRET THE PREDICTIONS OF DEEP NETWORKS VIA RE-LABEL DISTILLATION", "authors": ["Yingying Hua", "Shiming Ge", "Daichi Zhang"], "abstract": "Interpreting the predictions of a black-box deep network can facilitate the reliability of its deployment. In this work, we propose a re-label distillation approach to learn a direct map from the input to the prediction in a self-supervision manner. The image is projected into a VAE subspace to generate some synthetic images by randomly perturbing its latent vector. Then, these synthetic images can be annotated into one of two classes by identifying whether their labels shift. After that, using the labels annotated by the deep network as teacher, a linear student model is trained to approximate the annotations by mapping these synthetic images to the classes. In this manner, these re-labeled synthetic images can well describe the local classification mechanism of the deep network, and the learned student can provide a more intuitive explanation towards the predictions. Extensive experiments verify the effectiveness of our approach qualitatively and quantitatively.", "sections": [{"title": "1. INTRODUCTION", "content": "Deep neural networks (DNNs) have proven their excellent capabilities on a variety of tasks. The complex nonlinearity of deep models promotes extremely high accuracy, but also leads to the opacity and incomprehensibility [1]. In particular, it is hard to understand and reason the predictions of DNNs from the perspective of human. These black-box models could cause serious security issues, such as the inability to effectively distinguish and track some errors, which diminishes the credibility of them. Therefore, it is of great significance to understand the decision-making process of DNNS and improve their interpretability to users.\nThe interpretability of deep models has already attracted an increasing attention in recent years [2]. According to the scope of interpretability, it can be divided into global interpretability and local interpretability [3]. Global interpretability is based on the relationship between dependent and predictor variables to understand the predictions in the entire data set, that is, to establish the relationship between the output and input of deep models [4]. While local interpretability focuses on a single point and the local sub-region in the feature space around this point, and tries to understand the predictions based on the local region. Usually, local interpretability and global interpretability are used together to jointly explain the decision-making process of deep networks.\nIn order to address this issue, many methods have been proposed and validated to meet this need for interpretability. In terms of the interpretability, there are three main areas based on the purpose of these studies: 1) the first aims to make the components of deep networks more transparent, which is mainly achieved through visualization technology [5]; 2) the second is achieved by learning a semantic graph [6]. The implicit knowledge of these deep models can be characterized in an interpretable manner; 3) the last one is to generate a post-hoc explanation. For example, we can manipulate some interpretable models to explain the predictions afterwards [7]. Nonetheless, these traditional methods [8] still need some improvements, such as the effect of explanation does not ensure that human can totally understand deep networks, and the universality of these explanation methods is limited by the spe-"}, {"title": "2. APPROACH", "content": "The overview of our interpretable approach is illustrated in Fig. 2. In the following subsections, we will analyse the proposed re-label distillation approach in details."}, {"title": "2.1. Problem Formulation", "content": "Inspired by transfer learning, we can not only transfer the model structure, but also transfer the interpretability of deep networks. Through some interpretable models, such as linear models or decision trees, deep networks can be reconstructed by distilling their hidden knowledge into these interpretable models. We train an interpretable model to learn the output of the black-box model, so that we can establish an interpretable relationship between the input and output of the black-box model and achieve the interpretability of the predictions.\nHowever, there are two fundamental problems: 1) The nonlinearity of DNN makes it hard to find a perfect interpretable model to capture the entire classification mechanism; 2) Traditional knowledge distillation leads to the loss of a lot of effective information and seriously affects the performance of the student. In order to solve the problems, we generate some synthetic images to represent the classification knowledge of deep networks, and then transfer the local boundary knowledge of the input into an interpretable model. Therefore, we could understand and reason the predictions of DNN.\nThe foundation of our interpretable approach is mainly to train an interpretable student model $S$ to interpret the prediction of a pre-trained DNN $T$ with respect to a given image $x$. The student model can learn a direct map $m$ from the image to the prediction $y$,\n$$y = xm,$$ \nwhere $m$ can mark the location of the effective features contributed to its prediction. Therefore, we use the parameters of the student as the weight of the features contributed to its prediction, which can be achieved by learning a student to imitate the DNN. In this way, we need some synthetic images $x'$ to capture the informative classification knowledge towards the DNN. A generator can reconstruct the image to generate some synthetic images for the training of the student.\nTo formalize the idea of matching the outputs between the student model and the teacher model, we minimize an objective function to match the probability distributions and the outputs $y'$ of the student with that of the teacher. The loss function for the training of student $S$ can be defined as,\n$$arg min L(P_s(x'; w), P_T(x')) + L(P_s(x'; w), y'),$$\nwhere $L(\\cdot)$ is the cross-entropy classification loss, $P_s(x'; w)$ denotes the student's prediction distribution with the training parameter $w$, and$P_T(x')$ denotes the teacher's distribution. Therefore, we can train an interpretable model with some synthetic images to interpret the predictions of deep networks."}, {"title": "2.2. Self-supervised Image Synthesis", "content": "In our interpretable framework, the role of the synthetic images is to characterize the classification mechanism of the deep network. The clarity of the synthetic images is not our focus, and we prefer to generate semantically meaningful images of different categories. So we use variational autoencoder (VAE) [11] as this generator. A VAE comprises two sub-networks of an encoder $p$ and a decoder $q$. The aim of encoder is to learn a latent representation that describes each latent attribute in probabilistic terms. To generate the synthetic images $X = \\{x_i\\}_{i=1}^n$, we add some random noise $\\epsilon_i$ to the latent vector,\n$$z_i = \\mu + \\epsilon_i * \\Sigma,$$"}, {"title": "2.3. Re-label Distillation", "content": "In order to understand and reason the predictions of deep networks, we use knowledge distillation [19] to train an interpretable model by deconstructing the hidden knowledge inside the teacher, which could achieve an interpretable prediction for a given image. Based on the above analysis, we apply a VAE to generate some synthetic images regarding a specific image and transfer the classification knowledge of deep networks into them. Therefore, we propose the re-label distillation approach to interpret its prediction."}, {"title": "3. EXPERIMENTS", "content": "To evaluate our re-label distillation approach, we compare with 8 state-of-the-art interpretable approaches in 2 typical deep networks (ResNet50 and VGG16 trained on ImageNet), including RISE [13], Excitation backprop [14], Extremal perturbations [15], Grad-CAM [16], Score-CAM [17], Occlusion sensitivity [18], LIME [20], and FGVis [21]."}, {"title": "3.1. Experiment Settings", "content": "We provide a comprehensive evaluation for our approach qualitatively and quantitatively. The images of ImageNet are preprocessed to 224 \u00d7 224 \u00d7 3 to train our interpretable framework. We use a VAE of 500D latent space to generate 1000 synthetic images for each input by perturbing the latent vector with some random noise. And the coefficients of the re-label distillation loss $\\lambda_1$ and $\\lambda_2$ are set to 0.7 and 0.3 in Eq.(7). Qualitative evaluation is mainly carried out through the visualization of saliency maps, which present the important features contributed to the predictions. And quantitative evaluation uses the metrics of deletion and insertion [13]. The"}, {"title": "3.2. Qualitative Results", "content": "We could explain the decision-making process of deep networks by generating a saliency map, which colors each pixel according to its importance to the prediction. Based on this, we use the weight parameters of the trained linear model to generate the feature-importance maps for qualitative evaluation. As shown in Fig. 4, our re-label distillation approach can mark the significant regions contributed and ignore some irrelevant background information. And compared with other state-of-the-art methods, our saliency map could mark the more accurate target area. Although extremal perturbations [15] could generate an accurate saliency map, our method also shows the importance of these salient features more clearly. By visualizing the saliency map, we can observe the important pixels of the image contributed to its prediction and build an interpretable relationship between them. Therefore, our proposed re-label distillation approach has a better performance in explaining the predictions of deep networks."}, {"title": "3.3. Quantitative Results", "content": "To conduct the quantitative experiments, we use the deletion and insertion metrics to evaluate the saliency maps generated"}, {"title": "4. CONCLUSION", "content": "In this paper, we propose a re-label distillation approach to interpret the predictions of deep networks. We first apply a VAE to generate some synthetic images. Then, we feed these"}]}