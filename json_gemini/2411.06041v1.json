{"title": "PointCG: Self-supervised Point Cloud Learning via Joint Completion and Generation", "authors": ["Yun Liu", "Peng Li", "Xuefeng Yan", "Liangliang Nan", "Bing Wang", "Honghua Chen", "Lina Gong", "Wei Zhao", "Mingqiang Wei"], "abstract": "The core of self-supervised point cloud learning lies in setting up appropriate pretext tasks, to construct a pre-training framework that enables the encoder to perceive 3D objects effectively. In this paper, we integrate two prevalent methods, masked point modeling (MPM) and 3D-to-2D generation, as pretext tasks within a pre-training framework. We leverage the spatial awareness and precise supervision offered by these two methods to address their respective limitations: ambiguous supervision signals and insensitivity to geometric information. Specifically, the proposed framework, abbreviated as PointCG, consists of a Hidden Point Completion (HPC) module and an Arbitrary-view Image Generation (AIG) module. We first capture visible points from arbitrary views as inputs by removing hidden points. Then, HPC extracts representations of the inputs with an encoder and completes the entire shape with a decoder, while AIG is used to generate rendered images based on the visible points' representations. Extensive experiments demonstrate the superiority of the proposed method over the baselines in various downstream tasks. Our code will be made available upon acceptance.", "sections": [{"title": "INTRODUCTION", "content": "Self-supervised representation learning (SSRL) aims to fully exploit the statistical and structural knowledge inherent in unlabeled datasets, enabling the encoder of the pre-training model to extract informative and discriminative representa- tions. The pre-trained encoder can be subsequently applied to various downstream tasks such as classification, segmen- tation, and object detection [1], [2]. The core of SSRL lies in the design of appropriate pretext tasks aimed at aiding the encoder in achieving a full perception and understanding of the inputs.\nBased on the tasks employed, existing self-supervised pre-training methods can be broadly classified into two paradigms: contrastive learning and generative learning, both of which have attained great success in processing 2D images [4]\u2013[6] and 3D point clouds [3], [7]\u2013[11]. Compared to contrastive learning, generative learning is considered as a more data-efficient pre-training method, capable of capturing the patterns of the inputs with relatively limited data volume [12]. Therefore, it is highly favored in the context of data scarcity within the field of 3D vision, where masked point modeling [3], [7], [8], [10], [13] and 3D-to- 2D generation [11], [14] stand out as two representative generative learning methods. Among them, masked point modeling drives the model to predict arbitrary missing parts based on the remaining points. Accomplishing this task requires a thorough understanding of the spatial properties and global-local context of point clouds. 3D-to-2D genera- tion employs a cross-modal pretext task which translates a 3D object point cloud to its diverse forms of 2D rendered images (e.g., silhouette, depth, contour). Pre-training with pixel-wise precise supervision drives the backbone to per- ceive the fine-grained edge details of 3D objects.\nHowever, both of the above methods have their own limitations. As revealed in [15]\u2013[17], due to the irregularity of point clouds, commonly used point set similarity met- rics (e.g., Chamfer Distance and Earth Mover's Distance) in masked point modeling cannot provide explicit point- to-point supervision between ground truth and generated point clouds. The lack of precise correspondence results in limited feature representation capability of the pre-trained backbone network. Conversely, 3D-to-2D generation [11], [14] alleviates the issue of insufficient supervision signals by utilizing regular 2D images as the generation objective, offering pixel-wise precise supervision. However, relying solely on images from limited views as ground truth may overlook the structural information from occluded point sets, diminishing the backbone's perception of the spatial properties of point clouds. As shown in Fig. 1, masked point modeling exhibits subpar performance in reconstructing some challenging areas (e.g., edges) due to the lack of point- to-point supervision. Besides, 3D-to-2D generation yields images lacking three-dimensional structural information, attributed to the lack of explicit geometric guidance. These observations collectively indicate the models' inadequate perception of the inputs, consequently reducing their per- formance on downstream tasks.\nBased on the aforementioned analysis, an intuitive method is to combine these two pretext tasks to retain their individual merits while compensating for their respective limitations. However, as shown in Fig. 1, while the model directly combining both tasks outperforms those relying solely on MPM or 3D-to-2D generation in generating high- quality point clouds or images, its Linear-SVM accuracy is lower (88.41% vs 90.72% and 91.13%). We argue that the encoder's involvement in both tasks can lead to confusion when generating content for two modalities concurrently. Furthermore, to accomplish both tasks, the model shifts its training focus toward the decoder, which is typically discarded after pre-training. This phenomenon diminishes the feature extraction capability of the encoder, ultimately reducing the Linear SVM accuracy.\nTo address these issues, we propose PointCG, a frame- work that effectively integrates masked point modeling and 3D-to-2D generation tasks. This framework incorporates a Hidden Point Completion (HPC) module and an Arbitrary- view Image Generation (AIG) module. Existing MAE-based MPM methods often employ a random masking strategy based on Farthest Point Sampling (FPS) and K-Nearest Neighbor (KNN) techniques. However, the inputs of un- masked patches (Fig. 2 (a)) preserve the overall shape of an object and exhibit substantial overlap with the target points (highlighted in red in Fig. 2 (d)). The leakage of overall structure and point location information enables the model to reconstruct the object without a holistic compre- hension of the entire structure, which limits the learning capacity of the encoder during pre-training. To overcome this limitation, we select the visible points from arbitrary views by removing hidden points as input and introduce the HPC module to complete the point clouds. For the 3D- to-2D generation task, we employ the arbitrary-view image generation as the pretext task, which generates the image from an arbitrary view based on the representations of visible points extracted by the encoder. Furthermore, the cross-modal feature alignment is introduced to align the feature spaces of point clouds and images, which enables simultaneous content generation across both modalities and refocuses the training on the encoder. Specifically, we ex- tract features from both the input point clouds and their corresponding rendered 2D images, encouraging feature proximity for the same instance while maintaining feature separation for different instances.\nThrough the effective integration of HPC and AIG, the pre-trained encoder achieves a comprehensive understand- ing of 3D objects and can extract high-quality 3D represen- tations. We evaluate our model and the proposed modules with a variety of downstream tasks and ablation studies. We further demonstrate that informative representations can be effectively learned from the restricted points, and such representations facilitate effortless masked point modeling and arbitrary-view image generation."}, {"title": "RELATED WORK", "content": "Self-supervised representation learning aims to derive ro- bust and general representations from unlabeled datasets, which can be broadly classified into two categories based on the types of pretext tasks: contrastive learning and gen- erative learning.\nContrastive learning-based methods (e.g., BYOL [18], SimSiam [6], DINO [19], STRL [9], CrossPoint [20]) de- fine the augmented views of a sample as positive sam- ples, while considering other instances as negative sam- ples, thereby constructing discriminative tasks. Generative learning-based methods (e.g., GPT [21], Point-BERT [7], Point-MAE [3], OcCo [10], MaskFeat3D [22]) are based on the intuition that effective feature abstractions contain sufficient information to reconstruct the original geometric structures [11]. In the point cloud processing community, where 3D assets are relatively scarce, generative learning has garnered widespread attention due to its data effi- ciency [3], [4], [13]. Among them, MAE stands out as one of the representative paradigms. It involves masking a substantial portion of input data, followed by the use of an encoder to extract informative representations and a decoder to reconstruct explicit features (e.g., pixels or points) or implicit features (e.g., discrete tokens). Taking Point-BERT [7], MaskFeat3D [22], and IAE [23] as examples, each of these methods utilizes the visible groups as input after masking and reconstructs the positions of masked points, surface normals, and surface variations, as well as the implicit features of the masked points. However, after random masking [3] or partial occlusion [13], the visible groups often retain the overall structure of the object (Fig. 7), and there are substantial overlap regions between input and target patches (Fig. 2). The leakage of overall structure and point location information will reduce the difficulty in reconstructing masked patches, thus limiting the learning and inference capabilities of the encoder.\nTo avoid the leakage of the object's overall shape and minimize overlap, we simulate scanners to capture visi- ble points from arbitrary views as input. Our approach is conceptually aligned with OcCo [10], which employs the Z-Buffer algorithm [24] to select visible points from multiple views, subsequently completing the original point clouds with an encoder-decoder architecture. The Z-Buffer algorithm addressed within rendering relies on two as- sumptions: the points satisfy sampling criteria (e.g., Nyquist condition) and the points are associated with normals (or the normals can be estimated) [25]. However, our method seeks rigorous theoretical support for visibility computation without requiring normal estimation, point rendering, or surface reconstruction. Therefore, we employ the Hidden Point Removal (HPR) operator to compute visibility in a more robust manner."}, {"title": "Cross-modal Learning", "content": "Recently, cross-modal learning has been a popular research topic, aiming at extracting informative representations from multiple modalities such as images, audio, and point clouds. It has the potential to enhance the performance of various tasks, including visual recognition, speech recognition, and point cloud analysis.\nIn point cloud analysis, a variety of methods have been proposed for cross-modal learning, such as CrossPoint [20], PointMCD [26], TAP [14], and PointVST [11]. CrossPoint [20] establishes cross-modal contrastive learning between im- ages and point clouds, demonstrating that the correspon- dence between images and points can enhance 3D object understanding. PointMCD [26] obtains a powerful point encoder by aligning the multi-view visual and geomet- ric descriptors generated by a pretrained image encoder and a learnable point encoder. Both CrossPoint [20] and PointMCD [26] are based on the contrastive paradigm and rely heavily on extensive 3D-2D paired data. Generative methods, such as TAP [14] and PointVST [11], generate images from specific views based on the input point clouds. These methods use regular 2D images as generation objec- tives to provide precise supervision.\nIn this paper, we follow the generative learning paradigm and propose a unified pre-training framework with two complementary pretext tasks: hidden point com- pletion and arbitrary-view image generation. The spatial awareness provided by 3D completion addresses the geo- metric insensitivity inherent in image supervision, as shown in the second line of column one in Fig. 1. Additionally, we demonstrate the mutual enhancement between the two pretext tasks through various experiments."}, {"title": "METHODOLOGY", "content": "As illustrated in Fig. 3, PointCG mainly consists of a hidden point completion (HPC) module and an arbitrary-view im- age generation (AIG) module. Specifically, we begin by se- lecting the visible points from arbitrary views as the inputs (Sec. 3.1), and then introduce an asymmetric Transformer- based encoder-decoder architecture for extracting represen- tations and completing hidden points (Sec. 3.2). Finally, we generate arbitrary-view images (Sec. 3.3) based on the aligned representations extracted by the encoder. In the following, we will delve into the details of these modules."}, {"title": "Data Organization", "content": "Given a complete point cloud $\\mathcal{P} = {p_i | 1 \\leq i \\leq N} \\in \\mathbb{R}^3$, we randomly select the camera position $C = [\\text{azimuth}, \\text{elevation}, \\text{distance}]$, where distance is fixed at 1.0. azimuth and elevation are randomly chosen within the range of $[0, 2\\pi]$. The HPR operator [25] is employed to determine whether $p_i$ is visible from $C$. It mainly consists of two steps: inversion transformation and convex hull construction.\nInversion transformation. We employ spherical flip [27] to reflect each point $p_i \\in \\mathcal{P}$ to the spherical surface (denoted as $p'_i \\in \\mathcal{P}$) along the ray from $C$ through $p_i$ to the spherical surface, as illustrated in Fig. 4 (a).\nConvex hull construction. The visible points from $C$ inverted on the spherical surface are situated on the convex hull of $\\mathcal{P} \\cup C$. Therefore, we need to compute the collection of triangular planes, which make up the convex hull. Then we extract all vertices (magenta points in $\\mathcal{P}$ in Fig. 4 (b)) of the convex hull and project them back onto the original point cloud to obtain the visible points $P_v$ (magenta points in $\\mathcal{P}$ in Fig. 4 (b)). The remaining points of the original point cloud are hidden from $C$, denoted as $P_h$."}, {"title": "Occluded Point Completion", "content": "For each input, we employ the FPS and KNN to divide the visible points $P_v$ into $v$ patches with $v$ centers. Simul- taneously, we extract $h$ central points from the hidden points $P_h$ and retrieve $k$ nearest neighbor points from the complete point cloud as the target patches $P_{GT}$. Then, the visible patches are projected into tokens $T^v \\in \\mathbb{R}^{v \\times d}$ with a lightweight PointNet [28], where $d$ is the dimension of features. Subsequently, a learnable Multi-Layer Perceptron (MLP) is adopted to embed the visible and hidden centers into positional tokens denoted as $T_l^v$ and $T_l^h$, respectively. Finally, we extract representations $T_E$ by an encoder and capture the tokens $T_D$ with a decoder for completing the original point clouds:\n$T_E = \\text{Encoder}(T^v, T_l^v), T_E \\in \\mathbb{R}^{v \\times q \\times d}$     (1)\n$T_D = \\text{Decoder}(\\text{Cat}(T_E,T_H), \\text{Cat}(T_l^v, T_l^h))$     (2)\n$T_H\\in \\mathbb{R}^{h \\times d}, T_D \\in \\mathbb{R}^{(v+h) \\times d}$\nwhere $T_H$ represents the hidden tokens, which is initialized by duplicating a learnable token of dimension $d$. We con- catenate the visible points' features $T_E$ and $T_H$, as well as the positional tokens $T_l^v$ and $T_l^h$ as the inputs of the decoder. Based on the outputs $T_D$ of the decoder, we will reconstruct the $k$ nearest neighbors of $h$ center points by a reconstruction head of a fully connected (FC) layer:\n$\\mathcal{P}_{pre} = \\text{Reshape}(\\text{FC}(T_D))$,\n$\\mathcal{P}_{pre} \\in \\mathbb{R}^{h \\times k \\times 3}$         (3)\nwhere $\\mathcal{P}_{pre}$ denotes as the predicted hidden point patches. Loss function. The Chamfer distance [29] is employed as the reconstruction loss:\n$L_{CD} = \\frac{1}{|P_{pre}|} \\sum_{x \\in \\mathcal{P}_{GT}} \\min_{x^{\\prime} \\in \\mathcal{P}_{pre}} ||x - x^{\\prime}||^2 + \\frac{1}{|P_{GT}|} \\sum_{x \\in \\mathcal{P}_{pre}} \\min_{x^{\\prime} \\in \\mathcal{P}_{GT}} ||x - x^{\\prime}||^2$  (4)\nwhere $\\mathcal{P}_{GT} \\in \\mathbb{R}^{h \\times k \\times 3}$ denotes the reconstruction targets."}, {"title": "Arbitrary-view Image Generation", "content": "To shift the pre-training focus towards enhancing the en- coder for better 3D understanding, we employ the feature alignment module to build correspondence between images and point clouds in the feature space.\nDuring pre-training, a pre-trained CLIP-visual [30] mod- ule $f$ is used to extract features from the rendered image $I_i$. Then, the image features $f(I_i)$ and the 3D features $\\mathcal{T}_i \\in \\text{Max}(T_E)$ are projected into the invariant space with functions $g$ and $h$, respectively, resulting in $H_i = g(f(I_i))$ and $Z_i = h(\\mathcal{T}_i)$.\nLoss function. In the invariant space, we aim to max- imize the similarity between $Z_i$ and $H_i$ when they corre- spond to the same objects. The cross-modal instance dis- crimination loss $L_{CM}$ can be formulated as:\n$L_{CM} = \\frac{1}{2M} \\sum_{i=1}^M [l(i, Z, H) + l(i, H, Z)] $       (5)\n$l(i, Z, H) = -\\log \\frac{\\exp(s(Z_i, H_i) / \\tau)}{\\sum_{k=1}^M [\\exp(s(Z_i, Z_k) / \\tau) + \\exp(s(Z_i, H_k) / \\tau)]}$\nwhere $M$ is the mini-batch size. $\\tau$ is the temperature co- efficient, and $s(.)$ denotes the cosine similarity function."}, {"title": "Image Generation", "content": "AIG generates rendered images from arbitrary views based on the visible points' representations extracted by the en- coder.\nIn pre-training, we randomly select a rendered image as the target and capture the corresponding view parameters $L_c$ as the input. $L_c$ comprises azimuth $\\phi$, elevation $\\lambda$, and distance $\\rho$, described as $L_c = \\text{Cat}(\\phi, \\lambda, \\rho)$. To enhance flexibility, we apply several learnable transformation layers for positional embedding tokens $T_l^c$. Then, we concatenate $T_E$ with $T_l^c$ and encode the combined representation using MLP $(G_0)$: $T_0 = G_0(\\text{Cat}(T_E, T_l^c))$. Finally, we design an image generator (Fig. 5) to generate rendered images based on $T_0$.\nSpecifically, we start by reshaping $T_0$ from its original vectorized representation to a 2D feature map. This feature map is then passed through a series of deconvolutional residual blocks and three parallel convolutional blocks to generate the rendered image $G_r$.\nLoss function. We utilize the $L_1$ loss as the content loss for image generation:\n$L_1 = \\frac{1}{n} \\sum_{i=1}^n ||G_{T_i} - G_i||$       (6)"}, {"title": "EXPERIMENT", "content": "In this section, we present extensive experiments to demon- strate the effectiveness of our method. We begin by introduc- ing the pre-training process on ShapeNet55 [33]. Then, we showcase its performance on 3D completion tasks in Sec. 4.1. Then, in Sec. 4.2, Sec. 4.3, and Sec. 4.4, we follow the previ- ous works to conduct experiments of object classification, part segmentation, and semantic segmentation. Finally, we validate the effectiveness of our modules through various ablation studies in Sec. 4.6.\nIn the following tables, \"Pre-T\" indicates whether the model is initialized with a pre-trained model, while \"Rep.\" signifies that the result is reproduced with the official code. Please note that we reproduce experiments of Point-MAE [3] and Point-M2AE [8] with their official codes, and all settings are consistent with our experimental configuration.\nPre-training. We pre-train the encoder on ShapeNet55 [33], which contains 52, 470 clean 3D models, covering 55 common object categories. The input point number $N$ is 2,048, and the rendered images have a size of 224 \u00d7 224 \u00d7 3. The encoder and decoder include 12 and 4 standard Transformer blocks, respectively. Each Transformer block has 384 hidden dimensions with 6 heads. We employ the AdamW optimizer [34] and cosine learning rate decay [35]. The initial learning rate is set to 0.001, and the weight decay is 0.05."}, {"title": "3D Completion", "content": "Completion based on visible points from random views. To assess the effectiveness of our self-supervised model initialized with pre-trained weights, we randomly select an instance from synthetic dataset ModelNet40 [36] and real- world dataset ScanObjectNN [37] separately and reconstruct the original point clouds. The visualization results of Point- MAE [3] and our model are shown in Fig. 6.\nCompared to Point-MAE, our method not only com- pletes the chair's pivot axis and the five-pronged base with greater fidelity but also obtains smoother surface structures. Our method achieves remarkable performance in recon- structing both synthetic and real-world data with visible points from arbitrary views."}, {"title": "Shape Classification", "content": "To assess the discrimination of the representations extracted by the pre-trained encoder, we validate the encoder on the shape classification task using the ModelNet40 [36] and ScanObjectNN [37] datasets.\nShape classification on synthetic data. ModelNet40 [36] contains 12,311 clean 3D CAD models, covering 40 object categories. We fine-tune the pre-trained encoder, and the results are presented in Tab. 1. Our method achieves 94.03% with global fine-tuning, surpassing the reproduced version of Point-MAE (Rep.) (93.21%) by 0.82% and the publicly released accuracy by 0.23%. To validate the effectiveness of our architecture, we incorporate Point-M2AE as the back- bone and evaluate its classification performance on Model- Net40. The classification results outperform the outcomes of the reproduced Point-M2AE model.\nOur method outperforms Point-MAE [3] and Point- M2AE [8] by margins of +1.46% and +0.29%, respectively. The results highlight the superior quality of the 3D repre- sentation learned by our method.\nShape classification on the real-world data. Evaluating a pre-trained model's performance on real-world datasets is crucial, as real-world scenes tend to be more complex than synthetic ones. We follow the common practice to evaluate our model on three variants: \u2018OBJ-BG', 'OBJ-ONLY', and 'PB-T50-RS' of ScanObjectNN [37].\nTo further validate the effectiveness of our design, we follow PointVST [11] with DGCNN as the encoder to construct a pre-training network (DGCNN+PointCG) and evaluate it on the 'PB-T50-RS' variant. Additionally, we reproduce TAP [14] and PointVST [11] using their official pretrained models.\nAs presented in Tab. 3, our method outperforms Cross- Point [20], as well as the reproduced TAP [14] and PointVST [11], all using DGCNN [38] as the encoder. When utilizing Point-MAE or Point-M2AE as the backbone, the classification results exhibit a significant improvement over the reproduced Point-MAE and Point-M2AE. These results underscore the discriminative power of the representations extracted by the encoder, even in complex real-world scenes.\nFew-shot Learning. Following previous works [3], [7], [10], [45], we conduct few-shot learning experiments using the pre-trained model on ModelNet40 [36]. We adopt n- way, m-shot setting, where n denotes the number of classes randomly selected from the dataset, and m represents the number of objects randomly sampled for each class. This yields n x m objects for training. For evaluation, we ran- domly select 20 unseen objects from each of n classes.\nThe results with settings of n \u2208 {5, 10} and m \u2208 {10,20} are presented in Tab. 4. As shown, our method consistently outperforms the baselines in nearly all few-shot settings, with minimal deviation. This highlights the robustness and generalization of the representations extracted by the PointCG encoder, even in data-limited scenarios."}, {"title": "Part Segmentation", "content": "The task of part segmentation aims to predict more fine- grained class labels for every instance. We conduct part seg- mentation on ShapeNetPart [47], which comprises 16,881 samples shared by 16 categories, annotated with 50 parts in total. As illustrated in Tab. 5, our method achieves com- petitive results and outperforms others in eleven categories.\nVisualization of part segmentation. Fine-grained part segmentation holds immense practical value. To highlight the clear advantage of our method in this task, we visu- alize the results and compare them with Point-MAE [3] in Fig. 8. As depicted in the third line, our method accurately segments the fuselage and tail fin of the airplane, along with the earphone and headband. This reveals the capability of our method to capture discriminative features of points belonging to distinct sections within the same instance."}, {"title": "Semantic Segmentation", "content": "Large-scale indoor datasets introduce more complexities as they cover larger scenes in real-world environments with noise and outliers. We evaluate the performance of our pre-trained model on the 3D semantic segmentation task using the Stanford large-scale 3D Indoor Spaces (S3DIS) [48] dataset. S3DIS includes data from 6 indoor areas, compris- ing a total of 272 rooms. We fine-tune the pre-trained model with Area 1-5 and evaluate it with Area 6. The results for each category are shown in Tab. 6. Our method outper- forms Point-MAE [3] across all categories except 'beam' and \u2018board'. The results underscore our model's capability to extract contextual and semantic information, which is crucial for producing fine-grained segmentation outcomes.\nIn reference to the semantic segmentation experiments of STRL [9], we fine-tune our pre-trained model on one area in Area 1-5, followed by evaluation on Area 6. We extend the experiments of Point-MAE [3] based on the pre-trained model released in the official code and present the mean IoU across all class categories mIoU(%) and the classification accuracy Acc (%) in Tab. 7. Our model exhibits a significant improvement in accuracy and mIoU compared to STRL [9] and Point-MAE [3]. These results demonstrate the capability of our model to extract contextual and se- mantic information, leading to fine-grained segmentation results."}, {"title": "Indoor 3D object detection", "content": "To validate the effectiveness of our method in scene- level prediction tasks, we conduct object detection experi-"}, {"title": "Ablation Study", "content": "To investigate the architectural designs of our method, we conduct comprehensive ablation studies with Point-MAE as the backbone model and elucidate the individual contribu- tion of each module.\nEffectiveness of the components. As shown in Tab. 9, we validate the effectiveness of each module by enhancing and replacing modules on the baseline.We adopt Point- MAE [3] for comparison and utilize hidden point comple- tion (HPC) as the baseline (a). In (b), we add the feature alignment module with pre-trained Vit-B/16 [30]. Based on (b), we incorporate the arbitrary-view image genera- tion (AIG) module, constituting our PointCG, denoted as (c). As shown in Tab. 9, while the inclusion of the feature alignment module based on HPC does not substantially improve classification accuracy, the exclusion of this module from PointCG yields a diminished Linear-SVM accuracy of 88.41%. We further replace Vit-B/16 with Vit-B/32 (d) and ResNet50 [51] (e). While Vit-B/32 outperforms Vit-B/16 in the image domain, it does not improve the accuracy of classification. The model with ResNet50 [51] exhibits comparatively poorer performance.\nTo assess the impact of color in rendered images, we extract grayscale images from the rendered ones and pre- train with them in (f). This operation leads to a decrease in shape classification. Grayscale images may potentially lose structural or finer details inherent in the original images. In experiment (g), we extract depth maps from point clouds following PointCLIP [52] and pre-train with them instead of rendered images. The classification result shows poor performance.\nTraining and inference time. Tab. 10 presents pre- training and inference times for the classification task of each module. The results demonstrate that the pre-training time is notably longer than that of the baseline, whereas the inference time for classification remains comparable.\nThe pre-training of HPC requires approximately 397s per epoch. Compared to the baseline (Point-MAE), the pri- mary time consumption arises from the data organization module, which is responsible for obtaining visible points from arbitrary views as inputs. Among the components, the 3D completion module has the shortest pre-training duration, while the feature alignment and AIG modules demand considerably more time.\nVisualization of 3D Completion. To validate the posi- tive impact of AIG on the 3D completion task, we exclude AIG from PointCG (W/O AIG) and pre-train the network. The 3D completion results of this variant and PointCG are shown in Fig. 9. As depicted, the edges of the aircraft wings are sharpened with our method. Without AIG, the completion edges exhibit point groups, attributed to solely relying on 3D completion, while the completion targets are point clusters.\nCamera perspectives. To identify more suitable inputs and predicted images, we design experiments with different camera positions as inputs, and the results are reported in Tab. 11. (a) takes the left-view image as input and the front- view as target, and (b) takes the left-view image as input and an arbitrary-view image as target. In (c), we use images from three different views (front, left, and top views) as inputs and an arbitrary-view image as target. Both the input and target of (d) are from arbitrary views, yielding the optimal results. Therefore, we utilize two arbitrary-view images as the input and target, respectively.\nPre-training with more complete inputs. We posit that if the inputs of 3D completion contain more structural information and more overlap areas with the targets, the completion task will be accomplished more easily. This leads to a reduced training intensity for the backbone, thereby diminishing the backbone's perception of 3D objects. We design this study based on different inputs. The inputs in (a) are derived from a single arbitrary view. The inputs for (b) and (c) involve the addition of two and eight extra patches, respectively, to the input of (a). The points from two arbitrary views serve as the inputs for (d). Results are reported in Tab. 12.\nIn cases (b) and (c), the accuracy of the Linear-SVM during pre-training decreases as more patches are included in the inputs. The classification results via fine-tuning also show a decline. In case (d), inputs from two views offer more structural information about the input objects, which leads to a significant decrease in shape classification. This experiment reveals that as additional structural information is progressively included in the input, shape classification accuracy steadily decreases. This indicates that excessive exposure to object structure within the inputs hinders the model's learning ability.\nImage generation losses. AIG significantly impacts the backbone's perception of 3D objects by precise supervision between the ground truth and the generated images. It always affects the quality of the generated images (as shown in Fig. 11). We conduct experiments to examine the perfor- mance of various loss functions for supervision, as outlined in Tab. 13.\nThe $L_2$ loss penalizes large errors more heavily and is more tolerant of small errors. In contrast, the $L_1$ loss does not excessively penalize large errors. The classification results of the model with the $L_1$ loss yield superior results compared to the $L_2$ loss.\nThe multi-scale structural similarity (MS_SSIM) index preserves the contrast in high-frequency regions. While L1"}]}