{"title": "Visual Text Matters: Improving Text-KVQA with Visual Text Entity\nKnowledge-aware Large Multimodal Assistant", "authors": ["Abhirama Subramanyam Penamakuri", "Anand Mishra"], "abstract": "We revisit knowledge-aware text-based visual\nquestion answering, also known as Text-KVQA\nin the light of modern advancements in large\nmultimodal models (LMMs), and make the fol-\nlowing contributions: (i) We propose VisTEL\na principled approach to perform visual text\nentity linking. The proposed VisTEL module\nharnesses a state-of-the-art visual text recogni-\ntion engine and the power of a large multimodal\nmodel to jointly reason using textual and visual\ncontext obtained using surrounding cues in the\nimage to link the visual text entity to the correct\nknowledge base entity. (ii) We present KaLMA\nknowledge-aware large multimodal assistant\nthat augments an LMM with knowledge associ-\nated with visual text entity in the image to arrive\nat an accurate answer. Further, we provide a\ncomprehensive experimental analysis and com-\nparison of our approach with traditional visual\nquestion answering, pre-large multimodal mod-\nels, and large multimodal models, as well as\nprior top-performing approaches. Averaging\nover three splits of Text-KVQA, our proposed\napproach surpasses the previous best approach\nby a substantial 23.3% on an absolute scale and\nestablishes a new state of the art. We make our\nimplementation publicly available.", "sections": [{"title": "Introduction", "content": "In the past few years, the research community has\nshown significant interest in visual question an-\nswering based on text appearing in images, as ev-\nidenced by the emergence of OCR-VQA (Mishra\net al., 2019), ST-VQA (Biten et al., 2019b) and\nTextVQA (Singh et al., 2019b). Giving another\naspect to these problems by leveraging external\nknowledge for text-based visual question answer-\ning, (Singh et al., 2019a) introduced a task called\nText-KVQA. The Text-KVQA presents a unique\nchallenge: given an image containing textual en-\ntities like business brands, book titles, or movie\ntitles, the task is to answer questions that require\nexternal knowledge about these entities. Address-\ning Text-KVQA involves detecting text in images,\nrecognizing it, linking it to a knowledge base, and\nemploying visual context and knowledge base for\nreasoning to provide an answer. Since the introduc-\ntion of this problem, several advancements have\nhappened in visual text understanding as well as vi-\nsion and language models. In this work, we revisit\nText-KVQA by leveraging these modern advance-\nments and propose a framework that judiciously\nintegrates various components of contemporary ar-\nchitecture.\nThe emergence of large multimodal models\n(LMMs)\u00b9 represents a significant trend in the lit-\nerature on vision and language (Zhang et al., 2022;\nChung et al., 2022; Touvron et al., 2023; Liu et al.,\n2024; Zhu et al., 2023; Ye et al., 2023; Penedo\net al., 2024; Ouyang et al., 2022). Over the past\nfew years, many large-scale language and vision\nmodels have been developed, demonstrating excep-\ntional performance across various tasks including,\nbut not limited to, image captioning, visual ques-\ntion answering, multimodal reasoning, and visual\ngrounding. We believe that pretrained LMMs hold\ngreat potential for addressing Text-KVQA. These\nmodels are rich in the implicit knowledge learned\nby large-scale pretraining. However, despite their\nnumerous advantages, they are not without draw-\nbacks, notably hallucinations. This challenge be-\ncomes particularly apparent in Text-KVQA, where\nprecise reasoning about entities depicted in images\nand associated knowledge is required. Consider the\nfollowing scenario where a customer, after finish-\ning their meal at a restaurant store, takes a picture\nof the store signboard and enquires about a pos-\nsible future online delivery, asking, \u2018Where can\nI place an online order from this store?' (Fig-\nure 1(a)). Existing LMMs often hallucinate over"}, {"title": "Related Work", "content": "KVQA Tasks: Visual Question Answering is a\nwell-studied task (Antol et al., 2015; Goyal et al.,\n2017). This task has been extended to scenarios\nthat require the ability to read text within images,\nleading to the development benchmarks such as"}, {"title": "Methodology", "content": "Problem Statement: Text-KVQA (Singh et al.,\n2019a) is a knowledge-intensive visual question-\nanswering task that requires a system to read and\ninterpret the visual text in an image and leverage\nit as a gateway to access and reason over exter-\nnal knowledge to answer the question. The exter-\nnal knowledge base K consists of a set of n enti-"}, {"title": "VisTEL: Visual Text Entity Linker", "content": "Entity linking is a well-studied task (Jurafsky and\nMartin, 2009), where given a sentence, the named\nentities need to be identified and linked with their\ncorresponding entities in a knowledge base. In\nthis work, we study an analogous task, where the\ninput is no longer a sentence, but instead an image\ncontaining visual text entities and the task is to link\nthem to a corresponding external knowledge base.\nOne plausible solution, as shown in (Singh et al.,\n2019a), is to extract the visual text in these images\nusing visual text recognition engines and then lever-"}, {"title": "KaLMA: Knowledge-aware Large\nMultimodal Assistant", "content": "We present Knowledge-aware Large Multimodal\nAssistant (KaLMA) for addressing Text-KVQA.\nThe KaLMA is an effective architecture that seam-\nlessly integrates questions and images in the con-\ntext of external knowledge in a trainable architec-\nture to generate accurate answers.\nWe use visual features X1 from the vision en-\ncoder. Further, we concatenate question Q and\nthe knowledge K1 via instruction prompt tem-\nplate (as shown in the Figure 4) and feed to\nthe embedding module f of the LMM to obtain\ntext tokens XTQ:K1 \u2208 Rmx dimm i.e., XTQ:K1\n= f(prompt(Q: K1)), where m is the number of\ntext tokens. Then, we concatenate image features\nX1, and text features XTQ:K1 and feed to the large\nmultimodal model to generate the accurate answer\nA. Further, to bring attribution ability, we model\nKaLMA to generate the supporting fact S that con-\ntributed to the answer along with answer genera-\ntion. From here onwards, we will refer answer and\nsupporting fact together as A. KaLMA predicts\nthe probability of the next token Aat in the answer\nAa in an auto-regressive manner. It does so by\nattending to the prompt inputs and the previously\ngenerated tokens Aa<t. We train by minimizing\nthe generative language modeling loss Lgen(\u03b8),\nwhich aims to generate the target tokens based on\nthe inputs X1 and XTQ:K1 (Eq. 1). Note that target\ntokens comprise both the answer and the support-\ning fact. During training, we leverage the ground\ntruth entity and its corresponding knowledge K1,\nwhile during inference, we obtain it using our Vis-\nTEL module. We reuse the weights of VisTEL to\ninitialise KaLMA.\nLans_gen (0) =\u2211log(Po(Aat|Aa<t,XI, XTQ:K1))\n , (1)\nt=1\nwhere \u03b8 are the trainable parameters, Aa<t rep-\nresents the answer tokens already generated before\npredicting the token Aat at the current time step t."}, {"title": "Experiments and Results", "content": "We conduct our experiments on Text-KVQA (Singh\net al., 2019a) dataset\u00b2. The questions in this dataset\nspan across three splits, namely, scene, book,\nand movie containing natural scene images, book\ncovers, and movie posters, respectively. These\nsplits have (50K questions, 10K images, 500 en-\ntities), (1M questions, 207K images, 207K enti-\nties), (222K questions, 34K images, 34K entities),"}, {"title": "Implementation Details", "content": "We implemented our method using PyTorch and\nthe Huggingface Transformers library (Wolf et al.,\n2020). We used LLaVA-1.5 as our foundation\nmodel for both VisTEL and KaLMA models. Note\nthat, LLaVA-1.5 is trained on CC3M (Sharma et al.,\n2018) and MS-COCO (Lin et al., 2014). We have\ncarefully examined these datasets for duplicates\nand found no overlap with the evaluation set of\nText-KVQA. Further, DBNET (Liao et al., 2020)\nand PARSEQ (Bautista and Atienza, 2022) are used\nas visual-text detection and visual-text recognition"}, {"title": "Results and Discussion", "content": "Results on Text-KVQA: We quantitatively eval-\nuate our proposed framework KaLMA on Text-\nKVQA and compare against relevant methods in Ta-\nble 1. We report accuracy averaged over the entire\ntest set for all the three splits of Text-KVQA. It is\nno surprise that traditional VQA baselines perform\npoorly as they do not have the ability to read and"}, {"title": "Ablations and Analysis", "content": "We conduct the following ablations and analysis of\nthe proposed work:\n(i) What is the need for VisTEL?: To study the\nperformance of our model in the absence of the\nproposed VisTEL module, we replace it with tra-\nditional edit-distance-based entity linking where"}, {"title": "Conclusion", "content": "We have revisited the Text-KVQA and significantly\nadvanced state of the art on this task. Our findings\nsuggest that visual text entity linking, combined\nwith seamless reasoning using both visual and tex-\ntual cues, as well as explicit external knowledge\nvia LMM, is key to our success. We performed ex-\ntensive ablation studies and analyses to support our\nclaims. The future scope of this work is to expand\nthe dataset with more visual-intensive queries and\naddress Text-KVQA for multilingual societies."}, {"title": "Limitations", "content": "We observe the following limitations in our work:\n(i) Existing visual text recognition pipelines suffer\non low-resolution images where it is challenging\nto extract visual text, which further impacts the\nperformance of our VisTEL (ii) In the dataset we\nuse, it was assumed that each image contains only"}, {"title": "Ethical Considerations and Broader\nImpact", "content": "This work is based on the publicly available Text-\nKVQA dataset, which predominantly contains En-\nglish visual text, and the associated knowledge\nbase, questions, and answer pairs are also in En-\nglish. The dataset may have some geographic bias\nthat went undetected in this work, a common is-\nsue with many public computer vision and NLP\nbenchmarks. Additionally, our work uses large\nmultimodal models (LMMs), which can inherit and\npotentially amplify biases from the large-scale pre-\ntraining data used.\nWe are mindful of the environmental impact of\nusing LMMs due to their heavy computational re-\nquirements. To mitigate this, we judiciously used\nLMMS by reusing pre-existing checkpoints wher-\never appropriate.\nWe open-source our implementation to facili-\ntate reproduction and further study. Nevertheless,\na more rigorous inspection is indeed required be-\nfore deploying the proposed model in real-world\napplications to ensure ethical considerations are\ncomprehensively addressed.\nBroader Impact: The proposed work has the fol-\nlowing broader impact: (i) The ability to link vi-\nsual text entities to knowledge bases and leverage\nthis linked knowledge for answering questions can\nimprove the accuracy and relevance of informa-\ntion retrieval systems. Although not studied in this\nwork, this may be particularly valuable in content\nrecommendation systems and search engines. (ii)\nThis research contributes to advancing the capabili-\nties of AI systems to understand and interact with\nmultimodal information (text and images), which\ncan benefit applications in fields such as virtual\nassistants, content understanding, and automated\ndecision-making. (iii) Methodologically, contribu-\ntions such as VisTEL provide new frameworks and"}, {"title": "Appendix", "content": "A Question Categorisation\nWe show the visual question-answering results\nover concretized sub-categories under each of the\nscenes, book and movie split in Table 6. We ob-\nserve that our proposed model shows remarkable\nperformance across diverse question categories,\nparticularly in the challenging categories such as\ndate, people, and open-ended question categories.\nB Finetuning details of LMMS\nIn this section, we explain the hyperparameters\nand prompts used to finetune the LMMs. Note\nthat we conduct all our experiments on a machine\nwith 3 48GB A6000 GPUs. For mPlug-Owl and\nMiniGPT4v2, we have used hyperparameters as\nper the original papers."}]}