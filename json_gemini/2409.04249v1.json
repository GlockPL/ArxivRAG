{"title": "Hermes: Memory-Efficient Pipeline Inference for Large Models on Edge Devices", "authors": ["Xueyuan Han", "Zinuo Cai", "Yichu Zhang", "Chongxin Fan", "Junhan Liu", "Ruhui Ma", "Rajkumar Buyya"], "abstract": "The application of Transformer-based large models has achieved numerous success in recent years. However, the exponential growth in the parameters of large models introduces formidable memory challenge for edge deployment. Prior works to address this challenge mainly focus on optimizing the model structure and adopting memory swapping methods. However, the former reduces the inference accuracy, and the latter raises the inference latency. This paper introduces PIPELOAD, a novel memory-efficient pipeline execution mechanism. It reduces memory usage by incorporating dynamic memory management and minimizes inference latency by employing parallel model loading. Based on PIPELOAD mechanism, we present Hermes, a framework optimized for large model inference on edge devices. We evaluate Hermes on Transformer-based models of different sizes. Our experiments illustrate that Hermes achieves up to 4.24\u00d7 increase in inference speed and 86.7% lower memory consumption than the state-of-the-art pipeline mechanism for BERT and ViT models, 2.58\u00d7 increase in inference speed and 90.3% lower memory consumption for GPT-style models.", "sections": [{"title": "I. INTRODUCTION", "content": "The Transformer architecture has profoundly transformed the landscape of deep learning and brought forward large models with their applications spreading from data centers [1] to edge devices. Large models are generally categorized into Natural Language Processing (NLP), Computer Vision (CV), and Multimodal models. NLP is widely applied on mobile devices [2], [3], from intelligent personal assistants, like Google Assistant and Apple Siri to real-time language translation [4]. CV plays a pivotal role in the field of autonomous driving [5], [6], where it is utilized for tasks such as real-time object detection [7], [8], lane recognition [9], and traffic signal detection [10]. By enriching robots' perception and decision-making capabilities through the integration of diverse data types [11], such as visual, auditory [12], and tactile [13] information, Multimodal large models are revolutionizing the field of robotics [14], [15].\nDue to the explosive growth in the size of large models, deploying them at the edge faces critical memory challenges [16]. Specifically, current edge devices offer only a limited amount of memory capacity, ranging from a few tens of megabytes to a few gigabytes. For example, NVIDIA Jetson Nano has 4 GB of memory and Raspberry Pi 4 Model B has up to 8 GB of memory. In contrast, large models' parameters have experienced exponential growth, reaching sizes in the hundreds of billions. For instance, the GPT-3 [17] model has 175 billion trainable parameters, while the recently developed GPT-4 model exceeds the trillion parameter mark. Consequently, the memory usage of these large models can easily reach tens to hundreds of gigabytes, far surpassing the memory capacity of typical edge devices.\nExisting works to address the memory challenges of large model inference on edge devices can be classified into two categories. The first attempts to optimize the model structure to reduce the computational load through techniques including model pruning [18], [19], model compression [20], [21], model quantization [22] and adaptive inference [23], [24]. Although these approaches significantly diminish the number of required computational operations, they often result in reduced model accuracy. Moreover, these approaches are generally tailored"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "The architectural makeup of transformer models is the basis for developing pipeline inference strategies on edge devices.\nA. Transformer Model Structure"}, {"title": "B. Characteristics of Transformer-based Models", "content": "In order to design an efficient pipeline scheme, we conduct experiments to characterize two key aspects when the model performs forward computation: the allocation of memory among transformer model layers and the latency of model loading and model inference. We first evaluate the memory allocation in five kinds of transformer models, including ViT-Large, BERT-Large, GPT-2, GPT-J and BART (BART-Base and BART-Large), which cover all three categories of Transformer models. Additionally, we evaluate the time requirements of loading and inference for various transformer models, including BERT-Large, GPT-2, ViT-Large and GPT-J, by performing standard model inference. All the experiments are conducted on Intel(R) 193 Xeon(R) Gold 6248R CPU.\nTestCase1: Memory distribution. To understand the allocation of memory across layers, we conduct experiments with five kinds of transformer models. Typically, transformer-based models are characterized by their extensive reliance on attention mechanisms, necessitating substantial memory to accommodate attention scores and intermediate representations, particularly within encoder or decoder layers. Fig. 2 delineates the memory usage distribution across different layers for five kinds of prevalent transformer variants, revealing that encoder or decoder layers predominate, consuming between 70% to 95% of the total memory. Notably, the memory consumption attributed to these layers escalates with the model's overall size. For instance, BART-Large necessitates approximately 14.4% more memory relative to BART-Base.\nObservation I\nFor general transformer-based models, the encoder or decoder layers occupy the largest memory footprint.\nTestCase2: Latency evaluation. To evaluate the latency of model loading and inference, we run standard model inference processes for four transformer models on CPU. Generally, transformer models exhibit considerably higher latency during layer loading compared to layer inference. Through our experiments, as depicted in Fig. 3, we observe that, for the first three smaller models (each with a memory footprint around 1 GB), the layer loading period substantially exceeds the inference time, by roughly an order of magnitude. Conversely, for the larger GPT-J model (12 GB), the layer loading duration is approximately twice that of the inference time. Subsequently, such disparities contribute to a significant portion of the computational process, between 60% to 80%, being spent idle during typical pipeline execution, underlining a serious pipeline stall issue, as shown in Fig. 1b.\nObservation II\nFor general transformer-based models, loading latency is much larger than the inference latency, resulting in the execution process being stalled during most of inference time."}, {"title": "C. Implications", "content": "Our experiments in \u00a7II-B analyze the time distribution and memory usage of transformer-based large models during model inference. Observation I suggests that a targeted focus on either the encoder or decoder layers is pivotal for optimizing memory management in our pipeline infrastructure. Observation II underscores the necessity of adopting a parallel loading strategy by overlapping multiple inference times with a single loading time within our pipeline scheme, to efficiently mitigate pipeline stalls. In summary, we progress our design by addressing the following challenges: (1) memory challenge on edge devices; (2) pipeline stall problem caused by the huge gap between loading and inference latency."}, {"title": "III. PIPELOAD: A MEMORY-EFFICIENT PIPELINE EXECUTION MECHANISM", "content": "We present PIPELOAD, a memory-efficient pipeline execution mechanism to reduce memory footprint and latency during model inference on edge devices. There are three core workers in PIPELOAD mechanism: multiple Loading Agents, one Inference Agent and one Daemon Agent. Loading Agents work in parallel to load model layers from disk to memory, reducing inference latency. The Inference Agent simultaneously executes computations on these loaded layers sequentially in CPU, guaranteeing the model's predictive accuracy and minimizing pipeline stalls. The Daemon Agent maintains a queue of loaded layers in memory, detects memory usage and destroys memory space for specific layers at a specific point to reduce memory overhead. Three workers communicate with each other through a signalling mechanism that facilitates the realization of whole memory-efficient pipeline.\nFig. 4 illustrates the overall workflow of PIPELOAD, including the loading and inference process of model layers as well as the signaling mechanism. Before performing pipeline inference, we adopt a layer-based model partitioning scheme to pre-process the model weights. Multiple Loading Agents constantly load specific layers from disk into memory in parallel. Once a model layer is successfully loaded, the corresponding Loading Agent transmits a computation ready signal that corresponds to this layer to Inference Agent, indicating that this layer is ready for computation. Inference Agent maintains an inference queue in CPU that decides which layer will be processed next, ensuring that model inference respects the original sequence of layers. Upon receiving the computation ready signal, Inference Agent performs forward computation only if all preceding layers have been computed. Following computation of the layer, Inference Agent issues a memory destruction signal to Daemon Agent, notifying it to destroy the memory space of the layer. Daemon Agent then destroys the memory space occupied by the layer to reserve enough space for other layers. When memory usage is about to exceed or has exceeded the memory constraints of the edge device, Daemon Agent sends a stop signal to all Loading Agents, pausing their loading operations until sufficient memory space is available.\nA. Overview"}, {"title": "B. Case Study", "content": "Fig. 5 presents a simple case of PIPELOAD with three Loading Agents. Based on the characteristics of transformer model layers, we adopt a layer-based model partitioning scheme. In our scheme, we methodically segment the general transformer model architecture into its constituent layers: embedding layers, encoder layers, decoder layers and other layers. Among these layers, we focus only on the encoder and decoder layers that occupy most of the model weights in PIPELOAD mechanism design.\nFor simplicity, we show only three computation ready signals and three memory destruction signals in Fig. 5. And for the sake of clarity, three Loading Agents are symbolized as \\(LA_1\\), \\(LA_2\\), \\(LA_3\\). Model layers are signified as \\(L_k\\) where k represents the index within the total number of layers, denoted by N. Symbols \\(S_{k}^{comp}\\), \\(S_{k}^{dest}\\) and \\(S^{stop}\\) respectively represent computation ready signal for layer \\(L_k\\), memory destruction signal for layer \\(L_k\\) and loading stop signal. During the implementation of PIPELOAD, the i-th Loading Agent is assigned a subset of model layers, following the distribution \\(L_{i+jm}\\), where i ranges from 1 to m, with m representing the total number of Loading Agents, and j represents an iterative index, ranging from 0 to \\([(N \u2212 i)/m]\\) (\\(i + jm \u2264 N\\) and \\(j\u2208 N\\)). In this case, \\(LA_1\\) is responsible for layers (\\(L_1, L_4, L_7, ...\\)), \\(LA_2\\) for layers (\\(L_2, L_5, L_8,...\\)) and \\(LA_3\\) for layers (\\(L_3, L_6, L_9,...\\)). This layer allocation method is designed to minimize pipeline stalls since we can overlap the inference time of three layers with the loading time of a single layer.\nAs shown in Fig. 5, the three Loading Agents commence the parallel loading process. As the layer \\(L_1\\) is fully loaded to memory, \\(LA_1\\) issues the computation ready signal, \\(S_{1}^{comp}\\) to Inference Agent. After receiving \\(S_{1}^{comp}\\), Inference Agent starts to perform forward computation for \\(L_1\\). If Inference Agent receives \\(S_{2}^{comp}\\) or \\(S_{3}^{comp}\\) first, the inference queue in CPU will ensure that the layers are computed in the correct"}, {"title": "IV. HERMES: A FRAMEWORK TO OPTIMIZE LARGE MODEL INFERENCE ON EDGE DEVICES", "content": "Fig. 6 presents Hermes system architecture, a comprehensive framework designed to enhance the performance and reduce memory usage of large model inference in edge computing environments. Specific modules within Hermes comprise Layer Profiler, Pipeline Planner, and Execution Engine. This framework encapsulates methodologies for evaluating layer efficiency, deploying an optimal execution schedule, executing the memory-efficient pipeline, PIPELOAD and aims to collaborate diverse elements essential for optimizing model execution in resource-constrained settings, such as memory usage, latency and execution strategy.\n1) Layer Profiler: Fig. 6a presents some possible results of Layer Profiler, which serves as the foundation of our system architecture. The Layer Profiler's primary function is to profile each layer within a given transformer model to gauge runtime performance and memory usage. Through a pre-run of standard model inference, this profiling enables the accurate measurement of loading time, computation time and memory size for every individual layer of the given model.\n2) Pipeline Planner: Utilizing the data generated by the Layer Profiler, the Pipeline Planner develops a PIPELOAD execution schedule that includes several optimal execution strategies under different memory constraints, as shown in Fig. 6b. Firstly, drawing from the profiling insights encompassing layer's memory footprint along with layer's load and compute duration for the given model, the planner determines a reasonable range for the number of Loading Agents in conjunction with different memory constraints. In general, more Loading Agents means fewer pipeline stages, i.e., less latency, but more encoder or decoder layers are reserved in memory, i.e., more memory overhead. Next, the planner pre-runs the PIPELOAD within the range of the number of Loading Agents to obtain the exact number of Loading Agents under different memory constraints and finally outputs the execution schedule.\n3) Execution Engine: Finally, upon establishing the execution schedule, the inference of PIPELOAD with an exact number of Loading Agents will be executed in the Execution Engine based on the current memory constraints of edge device, as shown in Fig. 6c. This includes actual pipeline inference execution facilitated by the specific number of Loading Agents, one Inference Agent, one Daemon Agent and signalling mechanism."}, {"title": "V. EVALUATION", "content": "For estimating the memory-efficient pipeline execution mechanism, PIPELOAD, we focus on a quartet of transformer models: i) one NLP model: BERT-Large; ii) one CV model: ViT-Large; iii) and two generative text language models: GPT-2-Base and GPT-J. These four transformer models have different sizes, from a few hundred megabytes to a dozen gigabytes. Each of their configurations is shown in TABLE I, where the number of layers is the number of encoder or decoder layers, excluding other layers such as embedding layers and pooling layers, memory (layers / total) indicates that the memory footprint of encoder or decoder layers accounts for the total memory of the model and memory per layer represents the average memory footprint per encoder or decoder layer.\nB. Evaluation of Performance and Memory Footprints\nWe evaluate the performance and memory footprints of PIPELOAD with 2, 4 and 6 Loading Agents and compare them to baseline and to PipeSwitch. We choose these three numbers of Loading Agents since they are essentially factors of the number of encoder or decoder layers in four transformer models. The performance and memory footprints test results are shown in TABLE II and TABLE III respectively, where LAs is an acronym for Loading Agents. In order to show the optimisation results more directly, we add two metrics in tables respectively, the speedup and the ratio, with their expressions are as follows:\n\\[Speedup = \\frac{T_{baseline}}{T_{others}}\\]\n\\[Ratio = \\frac{M_{others}}{M_{baseline}}\\]\nwhere \\(T_{baseline}\\) and \\(T_{others}\\) represent the latency of baseline and latency of other methods and \\(M_{others}\\) and \\(M_{baseline}\\) indicate the memory consumption of other methods and memory consumption of baseline.\n1) BERT and ViT Models Analysis: For BERT and ViT models, we evaluate them with a single inference since they can generate outputs through loading and inference in a single pass. According to TABLE II and TABLE III, PIPELOAD with multiple Loading Agents indicates a promising trend of decreasing memory usage and latency compared to the PipeSwitch implementation. For BERT-Large, the speedup improvement is 1.93 ~ 4.24x and the memory footprint reduction is 44.9% ~ 73.0%. For ViT-Large, the speedup improvement is 1.73 ~ 3.64x and the memory footprint reduction is 74.0% ~ 86.7%. The smaller proportion of memory footprint reduction for BERT model compared to the ViT model is mainly due to the fact that the embedding and pooling layers of BERT-Large have a much larger portion, about 20% while ViT-Large about 1.5%. As we increment the\nA. Experimental Setup"}, {"title": "C. Evaluation under different Memory Constraints", "content": "We evaluate the performance of Hermes under different memory constraints. In addition, we measure the latency and the corresponding optimal number of Loading Agents.\n1) ViT and BERT Models Analysis: Fig. 7a and Fig. 7c show the evolution of latency and optimal number of Loading Agents with respect to memory constraints for ViT-Large model and BERT-Large model. Across the experiments, a trend is the gradual increase in the optimal number of Loading Agents, the decrease in the latency in correlation with the augmentation of memory limits. Specifically, the latency dropped from 81 ms at the 60 MB memory limit to 36 ms at 300 MB memory limit for ViT-Large, a reduction of 55.6% and from 7721 ms at the 500 MB memory limit to 2923 ms at the 1250 MB memory limit for BERT-Large, a reduction of 62.1%. All above results are as expected, since higher memory availability allows for more Loading Agents.\n2) GPT-2 and GPT-J Models Analysis: Fig. 7b and Fig. 7d show the evolution of latency and optimal number of Loading Agents with respect to memory constraints for GPT-2-Base model and GPT-J model. Overall, their trends of latency and optimal number of Loading Agents are the same as for the previous two models, from 1705 ms at the 400 MB memory limit to 1004 ms at 1000 MB memory limit for GPT-2-Base, a reduction of 41.1%, and from 51003 ms at the 2000 MB memory limit to 29074 ms at the 7000 MB memory limit for GPT-J, a reduction of 43.0%."}, {"title": "VI. RELATED WORK", "content": "Memory Optimization. PQK [32] is a novel model compression method, designed expressly for edge devices with constrained computational resources. This method combines pruning, quantisation, and knowledge distillation processes to fabricate a model that is both lightweight and energy-efficient. Keivan et al. address the memory challenges for large model inference under memory constraints by storing model parameters in flash memory and bringing them on demand to DRAM and introduce techniques including windowing and row-column bundling to optimize data transfer and memory usage. STI [33] is a memory optimization architecture through model sharding and elastic pipeline, which employs a preload buffer to optimize resource utilization for large model inference tasks on mobile devices. Our work is complementary, focusing on minimizing inference latency by pipeline scheme while reducing memory overhead.\nPipeline Schemes. Prior works have attempted to apply pipeline schemes to optimize large model inference [34]. DeepPlan [35] is an optimized pipeline system that incorporates two mechanisms, direct-host-access and GPU parallel transmission to reduce the model loading latency on the GPU and improve performance. PipeSwitch is a system designed for fine-grained time-sharing of GPU resources for deep learning applications, aiming to optimize task switching overhead and achieve near 100% GPU utilization. This system leverages the structure and computation pattern of DNN models to enable fast context switching with millisecond-scale overhead, addressing inefficiencies in shared GPU clusters where training and inference tasks are provisioned separately. These works mainly focus on reducing inference latency but do not involve memory optimization and require the use of one or even more"}, {"title": "VII. CONCLUSION", "content": "In this paper, we present PIPELOAD, a memory-efficient pipeline execution mechanism to mitigate memory overhead and address the pipeline stall issue during large model inference on edge devices. This mechanism incorporates dynamic management of memory and deploys multiple Loading Agents to load model weights in parallel. Based on this mechanism, we introduce Hermes, an innovative framework to optimize large model inference performance on edge devices. By our evaluation, Hermes reaches 4.24\u00d7 speedup and 86.7% lower memory consumption than PipeSwitch for BERT and ViT models, 2.58\u00d7 speedup and 90.3% lower memory consumption for GPT-style models. For future research, we are dedicated to applying the Hermes to more Transformer models and exploring its generalization and pervasiveness. For text generation large models like GPT, based on their characteristics, we target to optimize PIPELOAD mechanism to provide better latency reduction."}]}