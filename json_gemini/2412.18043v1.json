{"title": "Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review", "authors": ["Yidong Gan", "Maciej Rybinski", "Ben Hachey", "Jonathan K. Kummerfeld"], "abstract": "Clinical coding is crucial for healthcare billing\nand data analysis. Manual clinical coding is\nlabour-intensive and error-prone, which has\nmotivated research towards full automation of\nthe process. However, our analysis, based on\nUS English electronic health records and au-\ntomated coding research using these records,\nshows that widely used evaluation methods are\nnot aligned with real clinical contexts. For ex-\nample, evaluations that focus on the top 50\nmost common codes are an oversimplification,\nas there are thousands of codes used in practice.\nThis position paper aims to align AI coding re-\nsearch more closely with practical challenges\nof clinical coding. Based on our analysis, we\noffer eight specific recommendations, suggest-\ning ways to improve current evaluation meth-\nods. Additionally, we propose new AI-based\nmethods beyond automated coding, suggesting\nalternative approaches to assist clinical coders\nin their workflows.", "sections": [{"title": "1 Introduction", "content": "Clinical coding is a process that transforms clini-\ncal notes into a set of alphanumeric codes, which\nrepresent diagnoses and procedures during medical\nvisits. This process is essential to tasks like hospi-\ntal billing and disease prevalence studies. Manual\nclinical coding is labour-intensive and error-prone\n(Karimi et al., 2017; Li and Yu, 2020). To address\nthese issues, automated coding has been widely\nexplored. While many existing studies frame it as\na multi-label classification task (Mullenbach et al.,\n2018; Vu et al., 2020; Biswas et al., 2021; Huang\net al., 2022; Li et al., 2023), no analysis has exam-\nined whether that matches the needs of clinicians.\nThis position paper critically reviews automated\ncoding studies based on multi-label classification,\nfocusing on those using the public US Medical\nInformation Mart for Intensive Care (MIMIC)\ndatasets (Goldberger et al., 2000; Johnson et al.,\n2016, 2023). For consistency, we use the clinical\nterm 'code' instead of \u2018label' throughout this posi-\ntion paper. We focus on the MIMIC datasets as they\nare far larger than other public datasets. Specifi-\ncally, MIMIC provides English electronic health\nrecords of acute and emergent inpatients, with the\nlatest version, MIMIC-IV, including 331,675 pa-\ntient admissions. The few other public datasets,\nsuch as those released by Pestian et al. (2007) and\nMiranda-Escalada et al. (2020), contain 978 and\n1,000 admissions, respectively. This significant\nscale difference allows MIMIC to cover a broader\nrange of diagnoses and procedures, making it more\nsuitable for comprehensive analyses.\nWe show that current evaluations do not align\nwith the needs of clinical contexts. In practice,\ncoders must select from over a thousand codes\nand sequence them correctly (CMS and NCHS,\n2024), yet many studies use smaller code sets and\noverlook code sequencing in evaluation. Addi-\ntionally, the Area Under the Receiver Operating\nCharacteristic Curve (AUC-ROC) is often the only\nthreshold-independent metric reported, which is\ninappropriate given the imbalanced code distribu-\ntion. Common human coding metrics like Exact\nMatch Ratio and Jaccard Score are often omitted,\nmaking it difficult to measure the accuracy gap\nbetween automated and human coding. In other\nwords, widely used evaluation strategies do not\nmeasure what truly matters for clinical application.\nWe then propose new methodologies to support\nclinical coders beyond automated coding. Given\nthe limited effectiveness of automated coding (Edin\net al., 2023), manual coding with software assis-\ntance remains prevalent, making code auditing es-\nsential in clinical workflows to reduce potential\nhuman error. Despite their significance, research\non AI-based assisted coding and auditing is very\nlimited. This underscores the need for future stud-\nies in these areas, as they have the potential to yield\npractical solutions sooner than relying solely on\nautomated coding."}, {"title": "2 Clinical Coding Workflow", "content": "In this section, we present a typical clinical coding\nworkflow in an inpatient setting, outline the indi-\nvidual coding processes, and provide an overview\nof the current state-of-the-art strategies and tools\nused in these processes. The workflow presented\nis specific to the US context; other countries may\nhave similar or different processes.\nThe upper diagram in Figure 1 outlines a high-\nlevel, four-step clinical coding workflow for an\ninpatient episode. First, a patient is admitted to the\nhospital. During care, all relevant documentation\n(e.g., pathology reports) is entered into their Elec-\ntronic Health Record (EHR). Upon discharge, the\nattending doctor writes a summary detailing the\npatient's stay, including diagnosis and treatment.\nClinical coders then assign International Classifi-\ncation of Diseases (ICD) codes based on the EHR.\nFor reimbursement purposes, relevant ICD codes\nare grouped into a Diagnosis Related Group (DRG)\ncode. DRG is a classification system that organises\nhospital cases into groups; each DRG has a spe-\ncific payment rate based on the average resources\nneeded to treat patients in that group. The lower\ndiagram in Figure 1 gives additional details of the\nICD coding task, illustrating the processes before\nand after coding episodes with ICD.\nThe implementation of ICD and DRG varies\nacross countries, and the coding inputs can dif-\nfer depending on healthcare settings (e.g., hospital\npolicies). Previous AI coding studies often use\ndischarge summaries as inputs but have also ex-\nplored other sources, such as radiology reports (Pes-\ntian et al., 2007) and multilingual death certificates\n(N\u00e9v\u00e9ol et al., 2018). Given all these differences, it\nis important that AI coding research carefully con-\nsiders the clinical coding workflow it is addressing.\nOf note, many references in the following subsec-\ntion are corporate products, as previous research\nhas largely focused on the automated coding part\nof the clinical coding workflow."}, {"title": "2.1 Task Allocation", "content": "With many hospitals facing backlogs of uncoded\ncases (Alonso et al., 2020), optimising task allo-\ncation is crucial. The order in which cases are\nprocessed depends on hospital-specific business\nmetrics, such as maximising profit and reducing\nbacklog. Tools like Beamtree's Q Coding plat-\nform (Beamtree, 2024) support task forecasting,\nrule-based distribution, and scheduling, enabling\nhospitals to address backlogs systematically and\nensure critical cases are handled promptly.\nAfter determining case priority, the next step is\nto allocate cases to the appropriate expert. In as-\nsisted coding (i.e., manual coding with computer\nsoftware support), assigning cases based on coder\nspecialty or experience effectively manages the\ncomplexity of coding tasks (Alonso et al., 2020;\nBeamtree, 2024). Likewise, with AI-based coding\nsolutions, cases could be assigned to either assisted\nor automated coding pathways. This step is specu-\nlative and will be discussed further in Section 3."}, {"title": "2.2 Assisted Coding", "content": "In assisted coding, where coders use computer soft-\nware to manually enter codes, each keystroke, read-\ning, and thought incurs a cost. By minimising the\nmanual effort involved, these tasks become more\nefficient. This promise of higher efficiency has mo-\ntivated health tech companies to develop various\nassisted coding tools.\nExisting tools include features such as searching\nand navigating codes with integrated guidelines,\nwhich help users quickly find relevant codes and\nfollow best practices (Beamtree, 2024; 3M, 2024d).\nAI-suggested codes and DRG grouping helps in\nstreamlining the coding process (Beamtree, 2024;\n3M, 2024b,d). Customisable rules allow users to\nautomate coding based on criteria drawn from their\nexpertise (3M, 2024b). Online audits against or-\nganisational policies ensure quality control (3M,\n2024a). Evidence linking for assigned codes facili-\ntates efficient edits and reviews (Goinvo, 2024; 3M,\n2024c). Collectively, these features contribute to a\nmore efficient and cost-effective coding workflow."}, {"title": "2.3 Automated Coding", "content": "Automated coding refers to the process of assign-\ning accurate diagnostic and procedural ICD codes\nwithout human intervention. It is closely related\nto other real-world problems, such as tagging in\nsocial networks (Coope et al., 2019) and indexing\nbiomedical literature (Krithara et al., 2023), where\na piece of text is categorised using multiple labels.\nAutomated coding can address coding of episodes\nwhere the accuracy of an AI system is higher than\nhuman performance or is otherwise sufficient given\noperational cost-benefit considerations. CodeAs-\nsist (3M, 2024c), for example, is a commercial sys-\ntem used in many hospitals, with its main feature\nbeing automated coding.\nAutomated coding has also been the focus of\nAI coding research to date, with most studies rely-\ning solely on discharge summaries as model inputs.\nMullenbach et al. (2018) propose the Convolutional\nAttention for Multi-Label Classification (CAML)\nmodel. CAML combines convolutional neural net-\nworks (CNNs) with per-code attention to focus on\ntext sections relevant to each ICD code. This atten-\ntion mechanism enhances interpretability by high-\nlighting text contributions to the model's decisions.\nLi and Yu (2020) extend this framework with the\nMulti-Filter Residual Convolutional Neural Net-\nwork (MultiResCNN), which uses multiple filter\nlayers and residual connections for better feature\nextraction. On the other hand, Vu et al. (2020) in-\ntroduce the Label Attention Model (LAAT), using\na bidirectional long short-term memory (LSTM)\nto capture clinical context in text. LAAT uses a\ndistinct per-code attention mechanism compared to\nCAML, incorporating additional steps in attention\nweight calculations and a hierarchical joint learning\nstrategy, which leads to better predictions for rare\nICD codes. Huang et al. (2022) adapt LAAT's at-\ntention mechanism for pretrained language models\nin ICD coding (PLM-ICD). PLM-ICD also incor-\nporates domain-specific pretraining and segment\npooling, addressing challenges such as the domain\nmismatch between pretraining and clinical text and\nthe large code space.\nRecent benchmarking by Edin et al. (2023) com-\npares CAML, MultiResCNN, LAAT, PLM-ICD,\nand a few other models. PLM-ICD consistently\nachieves best results on MIMIC-III and MIMIC-IV\ndatasets. However, all models, including PLM-\nICD, struggle with rare code, which is a persistent\nissue in automated coding. Notably, document\nlength had minimal effect on model performance,\nwith little difference when truncating documents\nfrom 4,000 to 2,500 words. In Section 3, we demon-\nstrate that prior studies fail to measure key factors\ncritical for real-world applications. Our analysis\nreveals realistic upper bounds of existing state-of-\nthe-art models compared to human performance."}, {"title": "2.4 Code Auditing", "content": "Clinical coding is complex, and even human coders\noften make mistakes (Burns et al., 2012). In the US,\ncoding errors and quality improvement efforts cost\nan estimated $25 billion annually (Xie and Xing,\n2018). Even more concerning, some errors may be\ntreated as fraud, leading to legal liability (Rudman\net al., 2009). To address this, many asynchronous\nauditing tools have been developed. For exam-\nple, 3M (2024a) provides offline tools for batch\nauditing, referencing documents and codes used in\npatient claims to ensure compliant coding, and inte-\ngrated denial tracking for managing coding quality.\nBeamtree (2024) offers tools for audits against qual-\nity indicators, dynamic code sequencing, and code\ncombination validation. In general, these tools aim\nto enhance coding quality and reduce error-related\ncosts in both time and revenue."}, {"title": "3 Data and Automation: Analysis and\nRecommendations", "content": "In this section, we (1) identify key shortcomings in\nthe evaluation methodologies widely used in auto-\nmated coding studies, (2) analyse the widely used\nMIMIC datasets, and (3) offer corresponding rec-\nommendations. Many of these shortcomings are\nnot limited to studies using the MIMIC datasets,\nbut are also prevalent in other methodologies pro-\nposed for automated coding shared tasks involving\ndifferent datasets (Pestian et al., 2007; Miranda-\nEscalada et al., 2020)."}, {"title": "3.1 Evaluations Using the Top 50 Codes Do\nNot Reflect Real Effectiveness", "content": "Table 1 shows that many of the existing studies\nevaluate their methods using the 50 most frequent\ncodes\u00b9. When considering the application of mod-\nels in a real-world healthcare environment, this\nevaluation strategy is sub-optimal (Liu et al., 2021);\nit focuses solely on the most frequent codes, failing\nto cover all diagnoses and procedures encountered\nin clinical practice.\nWe demonstrate the coverage issue by showing\nthe distribution of codes in MIMIC-III. The green\nline in Figure 2 shows that the top 50 codes com-\nprise only 33.92% of total code occurrences. We\nfurther measure how well the episodes\u00b2 in MIMIC-\nIII are covered by the top 50 codes. Specifically,\nwe calculate the percentage of episodes where all\nassigned codes are within the top 50. The dark\nblue line shows that none of the episodes are fully\ncovered, meaning every episode has some codes\noutside the top 50. Even when we increase this\nto include the top 800 codes, the coverage rate re-\nmains very low, at 20.48%. In other words, even\nwith the top 800 codes, we miss some coding in-\nformation in about 80% of episodes. If we use a\nmore generous measure, calculating the percent-"}, {"title": "3.2 Global Thresholds Do Not Account for\nVariable Error Cost", "content": "In many existing studies, a threshold is defined\nas the minimum confidence level required to as-\nsign a specific code, and a fixed threshold of 0.5 is\nused without justification (Mullenbach et al., 2018;\nWang et al., 2018; Li and Yu, 2020; Vu et al., 2020).\nThis is important because the standard evaluation\nincludes the F1 score, a threshold-dependent clas-\nsification metric. Edin et al. (2023) found that not\ntuning the threshold significantly degraded the per-\nformance of most models, so they fine-tuned a sin-\ngle threshold by maximising the model's micro F1\nscore on the validation set. This does not account\nfor the inherent differences between codes, such as\nprior probabilities and misclassification costs. This\nleads to an interesting direction of research: adapt-\ning dynamic thresholds (Wu et al., 2019; Alotaibi\nand Flach, 2021) for classification-based automated\ncoding methods. Additionally, this highlights\nthe importance of threshold-independent met-\nrics, such as Area Under the Receiver Operating\nCharacteristic Curve (AUC-ROC) and Area Un-\nder the Precision-Recall Curve (AUC-PR), as\nthey provide comprehensive assessment of auto-\nmated coding models across various thresholds."}, {"title": "3.3 AUC-ROC Score Is Not Ideal for\nImbalanced Datasets", "content": "AUC-ROC is a binary classification metric. In\nmulti-label classification, a common approach is\nto convert the task into a one-vs-all setting, where\none code (class) is treated as the positive code and\nall remaining codes as the negative code. Then, the\nAUC-ROC scores for each code are computed in-\ndividually. AUC-PR offers a different perspective\nby focusing on the relationship between precision\nand recall. One way to calculate AUC-PR is by\nusing Average Precision (AP), which represents\nthe average of precision values at different recall\nlevels as the threshold varies. Since multiple codes\ncan be predicted for each episode, we can calculate\nthe mean AP for all possible codes, known as the\nMean Average Precision (MAP). In other words,\nthe AUC-PR for each code can be calculated using\nAP, while MAP extends this by averaging the AP\nvalues across all predicted codes.\nIn an imbalanced dataset like MIMIC, the dom-\ninance of the negative code can result in a mis-\nleadingly high AUC-ROC score. A study by Edin\net al. (2023) on three MIMIC splits shows that the\nSOTA automated coding model, PLM-ICD, con-\nsistently achieves macro AUC-ROC scores greater\nthan 95%, indicating it is very effective at scoring\nrelevant codes higher than irrelevant ones. By only\nlooking at this metric, one might infer that this is\na robust model. However, PLM-ICD's MAP re-\nmains below 70% across all three splits, indicating\nthat when it predicts a certain code given various\nthreshold values, it is often incorrect. Previous\nwork (Mullenbach et al., 2018; Liu et al., 2021;\nYuan et al., 2022; Huang et al., 2022) reported only\nthe AUC-ROC score, which can be misleading, as\nthe model's precision trade-off is not well reflected.\nThus, we recommend reporting AUC-PR as well\nas AUC-ROC."}, {"title": "3.4 Automated Coding Evaluation Should\nMatch Human Coding Evaluation", "content": "Existing studies aim for automated coding, which\nassumes that the model acts as an independent\ncoder. However, their evaluations do not include\nthe common accuracy metrics that are used to mea-\nsure human performance.\nThe term 'accuracy' can be confusing due to its\nmany possible implementations. Different studies\nhave reported human accuracy in clinical coding,\nbut their definitions of accuracy are inconsistent\n(Burns et al., 2012). In this position paper, we\nwill examine two implementations of accuracy. In-\nstance accuracy, or Exact Match Rate (EMR), mea-\nsures the percentage of instances (medical cases)\nwhere the predicted code set exactly matches the\ntrue code set. Code accuracy, or Jaccard Score,\nis defined as the ratio of the size intersection of\na predicted code set and a target code set to the\nsize of their union. Instance accuracy is stricter,\nas it requires a perfect match for every code in an\ninstance (i.e., an episode or medical case).\nIn automated coding, even if a case contains a\nsingle error, human coders must re-code or correct\nthe errors. This means the effort and cost associ-\nated with reviewing the entire clinical note are not\nmitigated, highlighting the need for measuring in-\nstance accuracy. On the other hand, code accuracy\nis useful because not all codes are equally impor-\ntant. When assigning a DRG code, the principal\ndiagnosis code is usually the first and most impor-\ntant determinant factor (IH\u0410\u0421\u0420\u0410, 2023). Given\nthis, when evaluating coding accuracy, it is bene-\nficial to allow for partial matches, acknowledging\nthat capturing overlap in codes can still be valuable.\nWe recommend that future studies include\nboth of these accuracy metrics to better demon-\nstrate the performance gap between AI and hu-\nman coding."}, {"title": "3.5 Low Automation Accuracy Suggests\nSubset-Specific Automation", "content": "When considering instance accuracy, a study in the\nUK that included 50 episodes of care showed that\nhuman accuracy is 54%. In a UK hospital setting,\nAbdulla et al. (2020) reported an average of 67.5%\naccuracy each month over four months. In contrast,\nAmong the 'MIMIC-III clean', \u2018MIMIC-IV ICD-\n9', and 'MIMIC-IV ICD-10' splits, none of the six\nadvanced models replicated by Edin et al. (2023)\nachieved an instance accuracy greater than 1.1%.\nWhen considering code accuracy, the median hu-\nman performance in the UK was 83.2%, with large\nvariance among thirty-two studies (50-98%) (Burns\net al., 2012). However, the definition of accuracy\nis inconsistent across the explored studies; some\ndefined inaccurate coding as inaccurate three digit\ncoding, while the majority defined it as inaccurate\nfour digit coding. In fact, many inaccuracies occur\nat the four digit level (Burns et al., 2012) instead of\nthe third digit level. We select PLM-ICD, the best\nmodel according to Edin et al. (2023), and report its\ncode accuracy with respect to three-digit, four-digit,\nand full code levels across three MIMIC splits. For\neach split, we train a single PLM-ICD instance on\nthe full code prediction task, adjusting the accuracy\nmeasure across different digits to account for vary-\ning evaluation complexities. Table 2 shows that\nPLM-ICD's three digit accuracy, the most gener-\nous evaluation measure, is much higher than the\nother two in all splits. If we compare this result\nagainst human accuracy, PLM-ICD is only half as\ngood as an average human at best, indicating there\nstill exists a notable gap to reach full automation.\nInstead of full automation, we could consider the\ntask allocation part of clinical coding workflows.\nA recent study in radiology (Agarwal et al., 2023)\nsuggests that the best approach for combining hu-\nman expertise with AI is to delegate cases to either\nAI or humans, rather than having AI augment hu-\nman decisions. In other words, automating a subset\nof tractable episodes may be a promising direction\nfor AI coding. The main challenge, however, is\nthe choice of the subset. Figure 4 shows that in\nMIMIC-IV, approximately only 1% of episodes\ncontain one unique ICD-10-CM three-digit code,\nwhile more than half include at least six. This sug-\ngests we cannot simply choose the subset based\non a single disease or symptom. We encourage\nfuture research to investigate the selection of\ntractable subsets of care and estimate realistic\nupper bounds for these subsets."}, {"title": "3.6 MIMIC Episodes Are Challenging to\nFully Automate", "content": "The MIMIC cohort consists intensive care unit\n(ICU) and emergent inpatients, who often present\ncomplex conditions requiring multiple diagnoses\n(see Figure 4) and treatments (Alonso et al., 2020).\nMore details on the MIMIC cohort can be found\nin Appendix B. Campbell and Giadresco (2020)\nnoted that, compared to inpatient coders, outpatient\ncoders are more concerned that assisted coding will\nreplace their role. One possible reason for this is\nthat outpatient episodes often involve less complex\nconditions (Alonso et al., 2020). This suggests\nthat outpatient episodes, which are not included in\nMIMIC, may be better automation candidates.\nA common problem in the MIMIC datasets is\nthe imbalanced code distribution, where less than\nhalf of the full codes occur at least 10 times, ex-\ncept in the MIMIC-IV ICD-9 collection (see Ap-\npendix C for more details). MIMIC-III's coverage\nof the ICD-9-CM code space is relatively low, rep-"}, {"title": "3.7 Code Sequence Matters", "content": "It is not sufficient to consider only the correctness\nof the assigned codes; their sequence is also impor-\ntant. For instance, certain conditions have both an\nunderlying etiology and manifest across multiple\nbody systems. The official ICD-10-CM American\nguideline requires the underlying condition to be se-\nquenced before the manifestation (CMS and NCHS,\n2024). Similarly, the official Australian ICD-10-\nAM guideline requires the anaesthetic codes to be\nsequenced immediately following the procedure\ncode to which they relate (IHPA, 2022). There-\nfore, it is evident that clinical coding requires the\nmodeling of both code sequence and dependency.\nHowever, the sequence of the target codes is ne-\nglected by existing work despite this information\nbeing provided in the MIMIC datasets. In fact,\nboth sequence and dependency issues are inherent\nin some multi-label classification tasks. Many cor-\nresponding solutions have been developed (Read\net al., 2011; Alvares-Cherman et al., 2012; Yeh\net al., 2017; Yang and Liu, 2019). We recommend\nthat future studies include code sequences in\ntheir evaluation to better estimate the real im-\npact on workflow."}, {"title": "4 New Workflow-Inspired Methodologies", "content": "In Section 3, we explored limitations of the current\nformulation of the task as a multi-label classifi-\ncation problem. Now, we propose alternatives to\nintegrate AI into clinical coding workflows as rec-\nommendation systems or asynchronous (offline)\nauditing assistants. In these cases, information re-\ntrieval metrics such as Precision@k, Recall@k, and\nCoverage Error are more appropriate."}, {"title": "4.1 New Assisted Coding Methodologies", "content": "We consider three types of tasks to augment the\nmanual coding process: (1) a sequential task, where\nthe system predicts one code at a time and receives\nhuman feedback after each prediction; (2) a recall\ntask, where coders start with a large set of possi-\nble codes in mind and select from a set of system\nsuggested codes; and (3) a structural task, where\ncoders take a top-down approach, and instead of\npredicting complete codes, the system only predicts\npartial codes.\nWe propose various assisted coding systems to\naddress the three tasks. While these systems pro-\nvide a great starting point, they are not exhaustive.\nThe proposed assisted coding systems leverage\nboth human and AI strengths, but their implemen-\ntation is likely not straightforward. Multiple user\ndesigns can be implemented for the same system,\nleading to varying performance results. Additional\nchallenges include the time needed for humans\nto adapt to collaborating with AI and the risk of\nover-trust or under-trust in AI suggestions (Agar-\nwal et al., 2023). Therefore, we recommend in-\ncluding user studies or field tests when evaluating\nassisted coding systems, measuring not only accu-\nracy and efficiency but also user satisfaction and\ntrust in the system, and user-AI effectiveness."}, {"title": "4.1.1 The Sequential Task", "content": "The goal of this task is to transform the multi-\nlabel classification problem into a simpler multi-\nclassification problem. Instead of predicting all\ncodes in a single step, the objective is to assign\ncodes sequentially like real coding practice. We\ncan implement this in several ways:\n1. Chain a group of classifiers sequentially, sim-\nilar to the classifier chain proposed by Read\net al. (2011) for related problems in other do-\n        mains. Each classifier makes a binary choice\n        about assigning a code and receives user feed-\n        back, where the feedback is then used as part\n        of the input for the next classifier.\n2. Train a single multi-class classifier that pre-\n        dicts one code at a time, receives user feed-\n        back, and stops when a predefined termination\n        criterion is met.\n3. Treat all ICD codes as vocabulary and use a\n        seq2seq model to predict the codes, prompting\n        the user for feedback after each decoding step.\nAll three designs add sequential information as\npart of the input to tackle the code sequencing and\ndependency issue. Relevant evaluation metrics in-\nclude Precision@k, the number of steps required\nto achieve full recall, and the model's convergence\nrate with human feedback. Importantly, while\nhuman-in-the-loop is not mandatory\u2014since these\nmodels can be trained using teacher forcing-the\ninclusion of human input offers significant bene-\nfits. These include higher accuracy over long run,\nreal-time error correction, and improved user trust\n(important for clinical applications) as users con-\ntribute to the model's learning process."}, {"title": "4.1.2 The Recall Task", "content": "This task can be framed as a multiple-choice prob-\nlem, where the objective is to maximise relevant\nchoices while minimising the total number of\nchoices. We propose two designs:\n1. A system where all predicted codes are pre-\n        sented to a human expert.\n2. A system where high-confidence predicted\n        codes are assigned automatically, whereas\n        low-confidence predicted codes are presented\n        to a human expert.\nBoth designs can still be approached using multi-\nlabel classification; however, the evaluation metrics\nand optimisation methods should differ. Typically,\nmulti-label classification uses a loss function that\npenalises any incorrect outputs. In this task, it is ac-\nceptable if some of the model's top confident codes\nare incorrect, as long as the correct options are in-\ncluded. This approach involves optimising for Re-\ncall@k, ensuring high recall with a low value of k.\nIn the second design, the ranking of positive codes\nbecomes important as well. A key challenge in\nthis task is designing effective choice minimisation\nstrategies, which may include grouping by ontol-\nogy or confidence interval. For example, grouping\ncodes by related ICD categories can reduce the\nnumber of options a coder needs to review, thereby\nspeeding up the coding process."}, {"title": "4.2 The Structural Task", "content": "The structural task we propose is inspired by\nNguyen et al. (2023), but their setup does not in-\ncorporate human input. The objective of their work\nis to use a two-stage decoder that first predicts the\nparent (i.e., the first three digits) codes and then\nthe child (i.e., the digits following the first three)\ncodes. Their model's parent prediction on 'MIMIC-\nIII Full' achieves micro and macro F1 scores of\n29.1% and 69.0% respectively, outperforming the\noverall micro and macro F1 scores of 10.5% and\n58.4% by a considerable margin. This confirms\nthat parent prediction is much simpler for AI mod-\nels. Based on this, we could design systems where\nthe second stage benefit from human input:\n1. The system delegates the challenging task\n        of predicting child codes entirely to humans,\n        while its parent code predictions are used to\n        augment human decision-making.\n2. The system could predict a set of parent codes,\n        and upon a human selecting a parent code, it\n        would use that input to predict the relevant\n        child codes. This approach leverages human\n        expertise to refine the AI system's broader\n        categorisations.\nIn addition to standard classification metrics, ex-\nplainability metrics (e.g., matching annotated evi-\ndence spans) are useful for evaluating the first de-\nsign, as the system output is intended to inform hu-\nman decision-making. Hierarchical evaluation met-\nrics (e.g., Falis et al., 2021) are very useful as they\naccount for the hierarchical code structure, avoid-\ning equal penalisation for all mispredictions. Over-\nall, the proposed designs are more robust in han-\ndling rare codes, as parent codes (i.e., the broader\ncategories) have more training examples. By incor-\nporating a human-in-the-loop approach, the system\neffectively reduces the likelihood of error propa-\ngation from the first stage, resulting in improved\nreliability and efficiency."}, {"title": "4.3 New Code Auditing Methodologies", "content": "Clinical coding is challenging and humans often\nmake errors (Searle et al., 2020; Cheng et al., 2023).\nIf a model achieves high Precision@k, it could\nbe integrated as an offline auditor that starts up\nimmediately after human coders finish coding an\nepisode. For example, if a model's Precision@1\nscore is 95%, then its most confident code is correct\n95% of the time. This is beneficial in addressing\nunder-coding. For instance, the model can flag a\nhigh-confidence code that is missing in the episode,\nprompting the coder for review. Such offline de-\nsigns ensure AI interventions do not disrupt the\nhuman's standard coding workflow, improving cod-\ning quality and reducing the back-and-forth com-\nmunication between coders and auditors. When\nevaluating these models, considering specific met-\nrics such as the decrease in under-coding incidents\nand the improvement in billing accuracy post-AI\nreview will provide clear insights into the the AI's\nefficacy in improving the workflow."}, {"title": "5 Summary of Recommendations", "content": "1. The top 50 codes have low episode coverage,\nlimiting the generalisation of evaluation re-\n        sults to the full code setting, which is more\n        practical. We recommend against validating\n        methodologies solely with the top 50 codes.\n2. Clinical coding involves variable error costs,\n        different thresholds might be optimal for dif-\n        ferent codes. We recommend reporting more\n        threshold-independent scores for a compre-\n        hensive evaluation.\n3. In imbalanced datasets like MIMIC, a high\n        AUC-ROC score does not necessarily indicate\n        a high precision score due"}]}