{"title": "LEAVING THE BARN DOOR OPEN FOR CLEVER HANS: SIMPLE FEATURES PREDICT LLM BENCHMARK ANSWERS", "authors": ["Lorenzo Pacchiardi", "Marko Te\u0161i\u0107", "Lucy Cheke", "Jos\u00e9 Hern\u00e1ndez-Orallo", "VRAIN, ValGRAI"], "abstract": "The integrity of AI benchmarks is fundamental to accurately assess the capabilities of AI systems. The internal validity of these benchmarks\u2014i.e., making sure they are free from confounding factors-is crucial for ensuring that they are measuring what they are designed to measure. In this paper, we explore a key issue related to internal validity: the possibility that AI systems can solve benchmarks in unintended ways, bypassing the capability being tested. This phenomenon, widely known in human and animal experiments, is often referred to as the 'Clever Hans' effect, where tasks are solved using spurious cues, often involving much simpler processes than those putatively assessed. Previous research suggests that language models can exhibit this behaviour as well. In several older Natural Language Processing (NLP) benchmarks, individual n-grams like \"not\" have been found to be highly predictive of the correct labels, and supervised NLP models have been shown to exploit these patterns. In this work, we investigate the extent to which simple n-grams extracted from benchmark instances can be combined to predict labels in modern multiple-choice benchmarks designed for LLMs, and whether LLMs might be using such n-gram patterns to solve these benchmarks. We show how simple classifiers trained on these n-grams can achieve high scores on several benchmarks, despite lacking the capabilities being tested. Additionally, we provide evidence that modern LLMs might be using these superficial patterns to solve benchmarks. This suggests that the internal validity of these benchmarks may be compromised and caution should be exercised when interpreting LLM performance results on them.", "sections": [{"title": "INTRODUCTION", "content": "With the rise of large language models (LLMs), numerous benchmarks have been developed to evaluate different capabilities of these models and to rank their performance (see the collections HELM (Liang et al., 2022), BIGBench (Srivastava et al., 2022), LogiGLUE (Luo et al., 2023), CALM-bench (Dalal et al., 2023) and GLORE (Teng et al., 2023) for examples of capabilities being tested). Ensuring the validity of these benchmarks is essential. One important aspect of validity is internal validity: the benchmarks should be free from confounding factors (Liao et al., 2021;"}, {"title": "RELATED WORK", "content": null}, {"title": "BENCHMARK VALIDITY", "content": "Several factors contribute to the validity of measuring an AI system's performance on a specific benchmark. One key factor is \u201cconstruct validity\", which refers to how well the benchmark actually measures the construct that is intended to be measure. A prerequisite for construct validity is \"internal validity\", which reflects the extent to which the observed outcomes can be directly attributed to the variables or factors under investigation, without interference from confounding variables or biases. Other forms of validity include ecological and external validity, which refer to the extent to which the benchmark tasks reflect real-world tasks and how the measurement results generalise to real-world performance (Davis, 2023; Ivanova, 2023; Elliot et al., 2024; Liao et al., 2021).\nSeveral papers review issues related to benchmark validity. For instance, Liao et al. (2021) overview problems with the benchmarking paradigm of supervised ML systems, while Bowman & Dahl (2021) outline criteria NLP benchmarks should meet, some of which contribute to construct and internal validity. Subramonian et al. (2023) instead conduct a survey of AI researchers to develop a taxonomy of validity issues. McIntosh et al. (2024) identify a set of inadequacies in LLM benchmarks, some related to validity, while others are broader, such as security issues and the failure to capture cultural diversity. In contrast, Biderman et al. (2024) focuses primarily on reproducibility issues, which can also impact benchmark validity. Finally, Momennejad et al. (2023) and Rutar et al. (2024) provide guidance on building evaluation frameworks to measure whether AI systems (LLMs in the former case and embodied agents in the latter) possess capabilities that are robust to various perturbations and experimental conditions, thereby ensuring both construct and internal validity."}, {"title": "SPURIOUS CORRELATIONS IN NLP BENCHMARKS", "content": "The presence of spurious correlations in NLP benchmarks has been demonstrated by several studies, with some showing that these correlations affect the performance of NLP models. For example, Gururangan et al. (2018) identified individual words associated with specific class labels in NLP benchmarks and trained a model on truncated prompts achieving performance above chance, despite removing necessary elements for determining the correct answer. Moreover, they showed that models trained on such datasets performed better on instances that could be successfully classified using the truncated prompts. Poliak et al. (2018), Si et al. (2019), Sugawara et al. (2020) and Kavumba et al. (2019) performed similar experiments on different benchmarks and found comparable results. Niven & Kao (2019) showed that individual unigrams and bigrams were related to label values for specific benchmarks and crafted instances by adversarially manipulating the n-grams such that NLP models achieved poor accuracy on them. For example, they showed that BERT was able to exploit the presence of the unigram \u201cnot\u201d and bigrams like \u201cwill not\" to correctly answer instances in the ARCT dataset (Habernal et al., 2018). In contrast, our approach identifies patterns involving multiple unigrams and bigrams at the same time.\nIn the context of LLMs, Tu et al. (2020) found evidence that pre-trained models finetuned on datasets with spurious correlations can learn to exploit these correlations. Similarly, Du et al. (2023) argued that the supervised fine-tuning step in the LLM development pipeline makes them vulnerable to exploiting spurious correlations. Kavumba et al. (2022) conducted experiments where they shuffled the words in the possible choices and truncated the prompts, following the approach of Gururangan et al. (2018), to demonstrate that LLMs exploit superficial cues; for instance, they found that RoBERTa exploits superficial cues like the word \"not\".\nFinally, more advanced methods for identifying spurious features have been proposed. Friedman et al. (2022) used grammar induction techniques to define features as sub-trees within a probabilistic grammar. Similarly, Gardner et al. (2021) introduced a statistical test to determine whether a spurious correlation arises from bias in the data generation process or is instead randomly introduced. One limitation of this approach however is that it only considers the correlation between individual features and the labels.\nHere, we identify spurious correlations by training simple logistic regression classifiers to predict labels using only unigrams and bigrams extracted from the prompts. This approach allows us to explore whether combinations of these features are predictive of the correct response labels (e.g., \"true\" vs. \"false\") while remaining computationally lightweight, meaning that the approach is scalable to large datasets (unlike, for example, grammar induction techniques)."}, {"title": "METHODS", "content": "We aim to investigate (1) the extent to which we can predict labels on multiple-choice LLM benchmarks using combinations of simple unigrams and bigrams, and (2) whether LLMs rely on these features to succeed in these tasks. To this end, we evaluate the predictive power of different classifiers across benchmark datasets to correctly classify labels based on these n-gram features. Furthermore, we analyse the performance of LLMs on dataset instances that were successfully and unsuccessfully predicted by the n-gram models to determine if LLMs rely on these n-gram patterns to solve benchmark tasks."}, {"title": "DATASETS AND LLMS", "content": "We conducted experiments on a diverse set of nineteen LLM benchmarks, covering a wide range of linguistic and reasoning tasks: causal reasoning, counterfactual analysis, moral judgment and decision making, different types of formal reasoning, metaphor understanding, commonsense reasoning, spatial reasoning, legal reasoning, and natural language inference. All datasets used are multiple-choice, with the same possible choices across all instances within each dataset. Details of datasets used can be found in Table 1. Some of these datasets are from BIG-Bench (Srivastava et al., 2022), while others are from LegalBench (Guha et al., 2023). Instance-level performance data was collected from forty-four LLMs across eleven model families. For the larger datasets, the LLMs were tested on a random subset of instances to reduce costs; the number of tested instances is indicated in Table 1. Note that not all LLMs were tested on every dataset. For more details on the LLMs included in our analyses and the datasets they were tested on, see Appendix A."}, {"title": "FEATURE EXTRACTION", "content": "We considered thirteen different feature vectors derived from the prompts of each benchmark. To generate these features, we extracted unigrams and bigrams at both the word level and the token level; for token-level features, we used the GPT-2 tokenizer to capture subword information. For each set of n-grams, we computed three types of feature representations:"}, {"title": "Term Frequency (TF)", "content": "The raw count of each n-gram in the prompt."}, {"title": "Term Frequency-Inverse Document Frequency (TF-IDF)", "content": "A weight reflecting how important an n-gram is to a prompt in the context of the entire dataset."}, {"title": "Presence", "content": "A binary indicator where each feature is 1 if the n-gram occurs in the prompt and 0 otherwise.\nThis process resulted in twelve feature vectors: (unigrams, unigrams + bigrams) \u00d7 (word level, token level) \u00d7 (TF, TF-IDF, Presence). Additionally, we extracted a Readability and Diversity Metrics vector, which includes textual features such as the Flesch Reading Ease Score, Gunning Fog Index, SMOG Index, and Yule's K adapted from Moros Daval (2023). In total, we obtained thirteen feature vectors for our analysis."}, {"title": "PERFORMANCE EVALUATION", "content": "To assess the performance of our models, we used Cohen's K coefficient (McHugh, 2012), a statistic that measures the agreement between two raters (or, in our case, between the model's predictions and the true labels) while accounting for the agreement occurring by chance. This metric is particularly suitable in our context as the datasets differ in the number of possible labels (see Table 1), leading to varying values of chance level performance. Cohen's K is calculated as:\n$\\\u041a=\\frac{Po - Pe}{1-Pe}$\nwhere $P_o$ is the observed accuracy (the proportion of correctly predicted labels), and $P_e$ is the expected accuracy if predictions were made purely by random guessing. The value of Cohen's K ranges from -1 to 1, where a value of 1 indicates that the model perfectly predicts the labels (well beyond chance level), a value of 0 implies that the model's performance is no better than random guessing, and negative values suggest systematic disagreement between the model's predictions and the true labels."}, {"title": "EXPERIMENTAL SETUP", "content": "For each dataset, we partitioned the data into training, validation, and test splits. To explore whether n-grams are predictive of labels, we trained logistic regression classifiers on the training split using each of the thirteen feature vectors discussed in Section 3.2. We tested both L2 regularization (with regulation weight \\(\\lambda = 1\\)) and L1 regularization (with \\(\\lambda = 1\\) and \\(\\lambda = 10\\)). For each dataset and feature vector, we selected the hyperparameters that achieved the highest accuracy on the validation split, and we report the corresponding results on the test split (see Section 4.1).\nAdditionally, to investigate whether LLMs are relying on n-gram patterns identified by the logistic regression classifiers, we further stratified the test split into two subsets: instances whose labels were successfully predicted by the classifier using the best-performing features on the validation split, and those whose labels were not successfully predicted. We then analyse the performance of the LLMs on these two subsets across the various datasets, as detailed in Section 4.2.\nIn the following section, we present the results of our experiments and discuss the implications of our findings."}, {"title": "RESULTS", "content": "Code for reproducing the experiments is available at https://github.com/\nKinds-of-Intelligence-CFI/benchmark-ground-truth-predictability."}, {"title": "DO n-GRAMS PREDICT GROUND TRUTH LABELS?", "content": "We first investigate the extent to which logistic regression classifiers built on n-grams can predict the ground truth labels of various datasets. As mentioned above, we train the classifiers with different sets of hyperparameters on the training split of each dataset, select the configuration that leads to the best validation accuracy and evaluate the performance on the test split.\nFigure 1 shows Cohen's K values for all feature vectors across all datasets. For some datasets, relatively high values of Cohen's k can be achieved, while for others the values are negative or close to 0. This indicates that the ground truth labels can be predicted using simple features for some datasets, whereas for others this is not the case, at least with n-gram features. One could conjecture that high performance with n-gram models indicates an easy dataset. However, this does not seem to be the case: the distribution of LLM performance across different datasets is similar regardless of whether n-gram model performance is high or low (Figure 4 in Appendix A). Moreover, using TF-IDF seems to generally reduce predictive performance; this is expected as the IDF correction downweights the importance of n-grams that appear frequently across the datasets, which are the ones that may be more strongly correlated with the labels. Conversely, TF and Presence yield mostly identical results, likely because most of the n-grams useful for predicting labels tend to appear only once.\nIn the following, for each dataset, we consider the feature vector that leads to the highest validation accuracy, as shown in Table 2."}, {"title": "Do LLMS RELY ON N-GRAMS TO SUCCEED?", "content": "Here we analyse the performance of LLMs on instances that were successfully and unsuccessfully predicted by the n-gram classifiers. First, for each LLM and dataset, we compute the difference in performance between the two test subsets, and plot that against the overall test performance of the LLM on that dataset in Figure 2. The points in the plot are coloured according to the test performance of the best n-gram model.\nThe figure shows that the proportion of points where n-gram performance is high is the greatest in the upper right quadrant, which corresponds to LLM-dataset pairs with above-chance overall performance and a positive difference in performance between the two subsets. This suggests that some LLMs may be succeeding on specific datasets by relying on n-gram features. However, we note that there is also a smaller number points with high n-gram performance and low or close to 0 difference between subsets, as well as points with a strongly positive difference between subsets but poor n-gram performance. This indicates that LLMs are not robustly relying on the identified patterns in n-grams and that statistical noise may also be contributing to positive differences in performance between subsets.\nTo further investigate whether LLMs may be relying on n-gram features, we conducted the following analysis. First, we discarded all datasets for which Cohen's K on the test split is less than 0.2, which we consider the threshold above which a small but detectable agreement exists (Landis, 1977). The"}, {"title": "DISCUSSION", "content": "In this series of experiments, we demonstrate that simple unigram and bigram features extracted from benchmark prompts can be used to predict the correct answers for several multiple-choice LLM benchmarks. Furthermore, we provide evidence suggesting that at least some LLM families may be relying on these features to solve these benchmarks."}, {"title": "CONTROLLING FOR CLEVER HANS", "content": "Confounding factors are an issue in all forms of capability evaluation, potentially leading to misleading conclusions based on performance data. These confounds can be related to characteristics of the test participants-for example, older people tend to have encountered more words and thus have higher vocabulary, meaning that IQ tests premised on word-meaning knowledge will consistently attribute higher IQ to older participants (Verhaeghen, 2003))\u2014or they can be related to features of the task itself. Within human behavioural research, it is common wisdom that such confounds are inevitable, and considerable effort goes into identifying, eliminating and controlling for them. In animal research, experimenters will often wear opaque goggles (Tsukasa et al., 2012; Mercado III et al., 2000) or use pre-recorded voice commands (G\u00e1bor et al., 2022) to prevent from giving inadvertent clues, while medical research is routinely conducted \u201cdouble-blind\" such that the experimenter does not know and therefore cannot reveal the ground truth of the experimental manipulation (Friedman et al., 2010). A fundamental in the avoidance of task-related confounders is the randomisation, variation and counterbalancing of stimuli.\nIn AI evaluation, it has not been standard practice to comprehensively vary and counterbalance stimuli within benchmarks, with examples of this approach only beginning to emerge in recent years (Rutar et al., 2024; Momennejad et al., 2023; Mizrahi et al., 2024; Wang & Zhao, 2024; Cao et al., 2024). The consequences of omitting this methodological practice have become evident through the results of numerous studies of RL agents and NLP systems performance (Krakovna et al., 2020; Gururangan et al., 2018; Poliak et al., 2018; Si et al., 2019; Sugawara et al., 2020; Kavumba et al., 2019; Niven & Kao, 2019). In both cases, systems have been shown to pick up simple surface-level cues that are predictive of the correct answers, allowing them to achieve high performance without using the information or capability putatively at test. Given the power and generality of modern large language models, it is reasonable to expect that these systems may have an even greater capacity to take advantage of such cues. This indeed has shown to be the case for some specific examples (e.g. Tu et al., 2020; Kavumba et al., 2022; Du et al., 2023)."}, {"title": "ARE LLMS USING SURFACE-LEVEL CUES TO SOLVE BENCHMARKS?", "content": "To investigate whether language models may be using information from uni/bi-grams to ascertain the correct answers to benchmark questions, we divided each benchmark into two subsets: instances where the classifiers predicted the correct answer and instances where they did not. By comparing LLM performance across these two groups, we aimed to investigate whether LLM scores were consistently higher for instances successfully predicted by the classifiers, suggesting that these LLMs might be using identified uni/bi-grams to solve benchmark tasks.\nWe observe that for certain model families, most notably those produced by OpenAI, Meta, and Mistral AI, there is a small but notable performance advantage in the predictable instances. Specifically, these model families perform better on instances where the classifier was able to predict the correct answer using unigram and bigram features. The fact that the effect is subtle suggests that these models are not relying solely on these surface cues, which is expected: even if the systems were entirely dependent on superficial patterns, it is unlikely they would restrict themselves to only cues from unigram and bigrams. Thus, the absence of evidence for reliance on n-grams in some model families does not imply that these models are not relying on any spurious features. Furthermore, many LLM families included only two models tested on just five benchmarks, making it more challenging to detect an effect of relying on superficial cues compared to, for example, the OpenAI family, which consisted of 18 models tested on at least 12 datasets (see Tables 3 and 4).\nConversely, the evidence we provide is not conclusive in determining that those LLM families rely on n-grams to succeed on the benchmarks considered. To do so, careful experimental manipulation-similar to those conducted in Gururangan et al. (2018); Niven & Kao (2019) and Kavumba et al. (2022)\u2014would be necessary. Lacking these, confounding factors could explain our findings; for instance, it is possible that the instances successfully predicted by the n-gram models are coincidentally also easier, meaning that LLMs exhibiting the capability tested are more likely to succeed on those instances. While we do not believe this to be the case (indeed, as shown in Section 4.2, only a subset of LLMs shows a detectable pattern), our findings warrant further experimental manipulation.\nIn this work, we did not explore how LLMs may learn the statistical regularities present in benchmarks. Unlike traditional NLP models, LLMs are pre-trained and can be applied to different benchmarks out of the box (although techniques such as fine-tuning and few-shot prompting (Brown et al., 2020) can be applied). However, benchmark contamination, where an LLM is perhaps inadvertently trained on benchmark instances, is a widely-known issue (Achiam et al., 2023; Roberts et al., 2023; Jiang et al., 2024). Studies have also shown that fine-tuning LLMs on samples similar, but not identical, to benchmark tasks can improve their performance (Dominguez-Olmedo et al., 2024). Furthermore, the development pipeline for LLMs often involves supervised fine-tuning steps, which helps enhance their performance in question-answer settings. It has been suggested that this step may allow LLMs to learn to exploit spurious correlations (Du et al., 2023). We view the empirical analysis of existing LLMs to understand whether they rely on spurious features as complementary to the study of how they may learn to exploit such features. In particular, studying available LLMs may shed light on proprietary systems, for which analysing the development pipeline is not feasible."}, {"title": "CONCLUSION", "content": "We showed ways in which benchmarks could be solved without needing the assessed capability and provided evidence that LLMs may be using these surface cues to solve them. Future work should focus on identifying other types of cues that LLMs might be exploiting to solve benchmarks without demonstrating the intended capabilities being tested. The goal is that this research will lead to better-designed benchmarks with higher internal validity, allowing us to interpret LLM benchmark performance results with greater confidence with respect to the capabilities being assessed."}]}