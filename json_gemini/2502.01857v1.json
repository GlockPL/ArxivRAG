{"title": "Learning Human Perception Dynamics for Informative Robot Communication", "authors": ["Shenghui Chen", "Ruihan Zhao", "Sandeep Chinchali", "Ufuk Topcu"], "abstract": "Human-robot cooperative navigation is challenging in environments with incomplete information. We introduce CoNav-Maze, a simulated robotics environment where a robot navigates using local perception while a human operator provides guidance based on an inaccurate map. The robot can share its camera views to improve the operator's understanding of the environment. To enable efficient human-robot cooperation, we propose Information Gain Monte Carlo Tree Search (IG-MCTS), an online planning algorithm that balances autonomous movement and informative communication. Central to IG-MCTS is a neural human perception dynamics model that estimates how humans distill information from robot communications. We collect a dataset through a crowdsourced mapping task in CoNav-Maze and train this model using a fully convolutional architecture with data augmentation. User studies show that IG-MCTS outperforms teleoperation and instruction-following baselines, achieving comparable task performance with significantly less communication and lower human cognitive load, as evidenced by eye-tracking metrics.", "sections": [{"title": "1. Introduction", "content": "In novel environments, autonomous robots often face a \"cold-start\" problem, where they lack prior knowledge about the surroundings. Without human guidance, the robot may make inefficient or suboptimal decisions, wasting time and resources while exploring. Human operators can provide guidance to help robots overcome this limitation. Beyond improving efficiency, human involvement is essential for addressing safety, ethical, and moral considerations (Sterz et al., 2024; van der Waa et al., 2020; 2021).\nCollaboration between humans and robots in environments under incomplete information presents a realistic and challenging problem. Humans and robots often receive information from distinct sources. While they may synchronize their knowledge in static, offline settings, maintaining this synchronization becomes increasingly difficult in dynamic or unfamiliar environments, where sudden changes can quickly render the initial synchronization inaccurate. The challenge lies in enabling robots to achieve effective human-robot synergy by leveraging human knowledge to complement the robot's local observations. Such robots hold the potential to assist human operators in search-and-rescue missions (Doroodgar et al., 2010; Nourbakhsh et al., 2005) and support individuals with disabilities in daily tasks (Jain & Argall, 2019; Gopinath et al., 2016).\nCurrent human-in-the-loop robot control approaches differ in the degree of control required and the communication bandwidth utilized. Teleoperation relies on low-level human inputs, leading to high cognitive load and communication demands (Moniruzzaman et al., 2022). Instruction-following, in contrast, reduces direct human control by allowing robots to execute high-level human plans (Anderson et al., 2018). However, this approach still requires substantial communication for the human to provide detailed and accurate instructions. These limitations highlight the need for a more autonomous approach that reduces human workload while maintaining effective collaboration.\nThis paper studies the human-robot cooperative navigation problem in a simulated environment called CoNav-Maze, where the robot obtains local SLAM observations while the human provides guidance based on an initially inaccurate global map. The goal is to develop a control algorithm that does not merely follow instructions but actively collaborates by transmitting images to improve the human's understanding of the environment, integrating human trajectory guidance, and maintaining sufficient autonomy to reach target locations efficiently.\nTherefore, we introduce Information Gain Monte Carlo Tree Search (IG-MCTS), an online planning algorithm that embodies the idea that communication is action. Aside from task-centric objectives, IG-MCTS strategically decides between movement and communication actions based on their potential to enhance the human's understanding of the environment. Inspired by evidence that humans read to minimize perceptual errors and extract relevant features"}, {"title": "2. Related Work", "content": "Human Perception Modeling. Human perception has been extensively studied in psychophysics (Prins et al., 2016), which examines how physical stimuli influence human sensations and perceptions. A key model is the psychometric functions, which characterize the probabilistic relationship between stimulus intensity and observer responses (Wichmann & Hill, 2001; Klein, 2001). The logistic psychometric function is a widely used variant that captures the gradual transition from detecting to not detecting a stimulus as intensity changes (Treutwein & Strasburger, 1999). While psychometric functions provide a structured framework for modeling perception, their reliance on predefined mathematical forms may limit their ability to capture more nuanced perceptual phenomena (Morgan et al., 2012; Garc\u00eda-P\u00e9rez & Alcal\u00e1-Quintana, 2013).\nTeleoperation. Teleoperation allows human operators to remotely control robots, facilitating tasks in hazardous or inaccessible environments. Advances in immersive interfaces and reduced latency have improved human-robot interaction (Moniruzzaman et al., 2022), but challenges remain, including reliance on stable communication links and maintaining operator situational awareness under high cognitive load. The cognitive burden required for direct control in teleoperation often limits the human-to-robot ratio (Murphy, 2004), as it becomes challenging to effectively manage multiple robots simultaneously.\nVision-and-Language Navigation (VLN). VLN tasks require agents to interpret human instructions and navigate through 3D environments by executing action sequences (Anderson et al., 2018). Methods have leveraged synthetic data generation, cross-modal imitation learning, and reinforcement learning with look-ahead planning (Fried et al., 2018; Wang et al., 2019; 2018). However, many of these methods rely on simplifying assumptions, such as complete information, static environments (Kolve et al., 2017; Puig et al., 2023; Xia et al., 2018), or panoramic action"}, {"title": "3. Problem Setting", "content": "This paper addresses a human-robot cooperative navigation task under incomplete information. The remote human operator possesses an outdated map of the environment, while the robot can acquire accurate local observations. The human provides navigation guidance, and the robot communicates environmental updates. Together, they aim to reach a set of goal locations as efficiently as possible.\nWe design a simulated maze environment, CoNav-Maze, adapted from MemoryMaze (Pasukonis et al., 2022) to study this setting. In CoNav-Maze, the robot has perfect knowledge of its position and uses motion primitives to navigate between adjacent grid cells. This setup abstracts away low-level control and estimation errors, focusing on high-level human-robot coordination.\nFormally, the environment is modeled as a Markov Decision Process (MDP) defined by the tuple (S, A, T, Renv, \u03b3). S is a product space comprising the robot's discrete finite state and the set of remaining goal locations, capturing both its position and task progress. A is a finite set of actions, including movement to adjacent grids and transmitting a first-person image from one of eight evenly spaced camera angles.  T : S \u00d7 A \u2192 S is a deterministic transition function. Renv: S \u2192 R: is a real-valued reward function. \u03b3\u2208 [0, 1) is a discount factor.\nAt each step t, the robot collects a local observation of nearby traversable and blocked cells within a radius r. It may also receive a human-provided trajectory St. The robot then selects an action at to either move or transmit an image.\nThe human operator starts with an inaccurate global map x \u2208 X, representing traversable and blocked cells. By analyzing the robot's trajectory and image transmissions, the human refines their map to provide more accurate guidance."}, {"title": "4. Method", "content": "Effective human-robot cooperation in CoNav-Maze hinges on efficient communication. Maximizing the human's information gain enables more precise guidance, which in turn accelerates task completion. Yet for the robot, the challenge is not only what to communicate but also when, as it must balance gathering information for the human with pursuing immediate goals when confident in its navigation.\nTo achieve this, we introduce Information Gain Monte Carlo Tree Search (IG-MCTS), which optimizes both task-relevant objectives and the transmission of the most informative communication. IG-MCTS comprises three key components: (1) A data-driven human perception model that tracks how implicit (movement) and explicit (image) information updates the human's understanding of the maze layout. (2) Reward augmentation to integrate multiple objectives effectively leveraging on the learned perception model. (3) An uncertainty-aware MCTS that accounts for unobserved maze regions and human perception stochasticity."}, {"title": "4.1. Human Perception Dynamics", "content": "As the robot navigates the maze and transmits images, humans update their understanding of the environment. Based on the robot's path, they may infer that previously assumed blocked locations are traversable or detect discrepancies between the transmitted image and their map.\nTo formally capture this process, we model the evolution of human perception as another Markov Decision Process, referred to as the Perception MDP. The state space X represents all possible maze maps. The action space S+ \u00d7 O consists of the robot's trajectory between two image transmissions \u03c4\u2208 S+ and an image o \u2208 O. The unknown transition function F : (x, (\u03c4, 0)) \u2192 x' defines the human perception dynamics, which we aim to learn."}, {"title": "4.1.1. CROWD-SOURCED TRANSITION DATASET", "content": "To collect data, we designed a mapping task in the CoNav-Maze environment. Participants were tasked to edit their maps to match the true environment. A button triggers the robot's autonomous movements, after which it captures an image from a random angle. In this mapping task, the robot, aware of both the true environment and the human's map, visits predefined target locations and prioritizes areas with mislabeled grid cells on the human's map.\nWe then recruited over 50 annotators through Prolific (Palan & Schitter, 2018) for the mapping task. Each annotator labeled three randomly generated mazes. They were allowed to proceed to the next maze once the robot had reached all four goal locations. However, they could spend additional time refining their map before moving on. To incentivize accuracy, annotators receive a performance-based bonus based on the final accuracy of their annotated map."}, {"title": "4.1.2. FULLY-CONVOLUTIONAL DYNAMICS MODEL", "content": "We propose a Neural Human Perception Model (NHPM), a fully convolutional neural network (FCNN), to predict the human perception transition probabilities modeled in Section 4.1.1. We denote the model as Fe where @ represents the trainable weights. Such design echoes recent studies of model-based reinforcement learning (Hansen et al., 2022), where the agent first learns the environment dynamics, potentially from image observations (Hafner et al., 2019; Watter et al., 2015).\nAs illustrated in Figure 2, our model takes as input the human's current perception, the robot's path, and the image captured by the robot, all of which are transformed into a unified 2D representation. These inputs are concatenated along the channel dimension and fed into the CNN, which outputs a two-channel image: one predicting the probability of human adding a new wall and the other predicting the probability of removing a wall.\nTo enhance robustness and generalization, we apply data augmentation techniques, including random rotation and flipping of the 2D inputs during training. These transformations are particularly beneficial in the grid maze environment, which is invariant to orientation changes."}, {"title": "4.2. Perception-Aware Reward Augmentation", "content": "The robot optimizes its actions over a planning horizon H by solving the following optimization problem:\n$\\max_{\\tau,F} \\sum_{t=0}^{H-1} \\gamma^t [R_{task} (\\tau_{t+1}, \\zeta) + ||x_{t+1} - x_t||_1]$\ns.t. $x_{t+1} = F(x_t, (\\tau_t, a_t)),$    $a_t \\in O$\n$\\tau_{t+1} = \\tau_t \\concat T(s_t, a_t), a_t \\in U$\nThe objective in (1a) maximizes the expected cumulative reward over \u03c4 and F, reflecting the uncertainty in both physical transitions and human perception dynamics. The reward function consists of two components: (1) The task reward incentivizes efficient navigation. The specific formulation for the task in this work is outlined in Appendix A. (2) The information reward quantifies the change in the human's perception due to robot actions, computed as the L\u2081-norm distance between consecutive perception states.\nThe constraint in (1c) ensures that for movement actions, the trajectory history Tt expands with new states based on the robot's chosen actions, where st is the most recent state in Tt, and represents sequence concatenation. In constraint (1b), the robot leverages the learned human perception dynamics F to estimate the evolution of the human's understanding of the environment from perception state xt to xt+1 based on the observed trajectory Tt and transmitted image at \u2208 O."}, {"title": "4.3. Information Gain Monte Carlo Tree Search (IG-MCTS)", "content": "IG-MCTS follows the four stages of Monte Carlo tree search: selection, expansion, rollout, and backpropagation, but extends it by incorporating uncertainty in both environment dynamics and human perception. We introduce uncertainty-aware simulations in the expansion and rollout phases and adjust backpropagation with a value update rule that accounts for transition feasibility."}, {"title": "4.3.1. UNCERTAINTY-AWARE SIMULATION", "content": "As detailed in Algorithm 1, both the expansion and rollout phases involve forward simulation of robot actions. Each tree node v contains the state (\u03c4, x), representing the robot's state history and current human perception. We handle the two action types differently as follows:\n\u2022 A movement action u follows the environment dynamics T as defined in Section 3. Notably, the maze layout is observable up to distance r from the robot's visited grids, while unexplored areas assume a 50% chance of walls. In expansion, the resulting search node v' of this uncertain transition is assigned a feasibility value \u03b4 = 0.5. In rollout, the transition could fail and the robot remains in the same grid.\n\u2022 The state transition for a communication step o is governed by the learned stochastic human perception model Fe as defined in Section 4.1.2. Since transition probabilities are known, we compute the expected information reward Rinfo directly:\n$R_{info} (\\tau_t, x_t, o_t) = E_{x_{t+1}} ||x_{t+1} - x_t||_1$\n$||P_{add}||_1+ ||P_{remove} || _1,$\nwhere (Padd, Premove) \u2190 Fe(Tt, xt, Ot) are the estimated probabilities of adding or removing walls from"}, {"title": "4.3.2. FEASIBILITY-ADJUSTED BACKPROPAGATION", "content": "During backpropagation, the rewards obtained from the simulation phase are propagated back through the tree, updating the total value Q(v) and the visitation count N(v) for all nodes along the path to the root. Due to uncertainty in unexplored environment dynamics, the rollout return depends on the feasibility of the transition from the child node. Given a sample return qsample at child node v', the parent node's return is:\n$\\Gamma_{sample} = r+\\gamma[\\delta'r + \\frac{(1-\\delta')Q(v)}{N(v)}]$\nwhere \u03b4' represents the probability of a successful transition. The term (1 \u2013 \u03b4') accounts for failed transitions, relying instead on the current value estimate."}, {"title": "5. Experiments", "content": "We evaluate the effectiveness of the proposed Neural Human Perception Model (NHPM) in predicting how humans perceive environmental information based on a robot's movement and transmitted images."}, {"title": "5.1. Human Perception Dynamics Evaluation", "content": "We evaluate the effectiveness of the proposed Neural Human Perception Model (NHPM) in predicting how humans perceive environmental information based on a robot's movement and transmitted images."}, {"title": "5.1.1. BASELINE: GRID-BASED LOGISTIC PSYCHOMETRIC FUNCTION", "content": "We compare our method to the Logistic Psychometric Function (LPF), a standard model that relates human observer performance (e.g., detection or discrimination) to stimulus intensity. LPF fits a curve that captures thresholds, slopes, and response variability but lacks the expressive power to model complex spatial dependencies.\nTo adapt LPF to our setting, we extend it to operate at the grid level, treating each cell in the maze independently. This adaptation, referred to as the Grid-Based LPF (GLPF), models the probability of perception updates as a function of stimulus intensity:\n$P(y = 1 | x) = \\gamma + \\frac{1-\\gamma-\\lambda}{1+ e^{-\\beta(x-\\alpha)}}$"}, {"title": "5.1.2. QUANTITATIVE EVALUATION", "content": "We split the dataset into training and test sets and consider three distinct test settings:\n1. GLPF-Train: The psychometric function is fit on the training set to evaluate how well it generalizes to unseen environments based on prior human data.\n2. GLPF-Test: To establish an upper performance bound, we fit the GLPF directly to the test set. This removes the generalization gap, revealing the best-case scenario for an LPF-based approach.\n3. NHPM: We train our neural network model using backpropagation, optimizing parameters by minimizing the binary cross-entropy loss between predicted and ground-truth human edits to the map."}, {"title": "5.1.3. MODEL PREDICTION VISUALIZATION", "content": "In Figure 3, we visualize the outputs of human perception models and highlight two representative scenarios where NHPM outperforms GLPF. In the top row, the robot captures an image down a hallway, and the human adds a distant wall instead of a nearby one, likely because it appears in the center of the image and aligns with an existing wall on the map. NHPM accurately predicts this behavior, while the psychometric function assigns a low probability due to the"}, {"title": "5.2. User Study", "content": "We are interested in the question: Can IG-MCTS, with reduced communication, lower human cognitive load while maintaining task performance comparable to teleoperation and instruction-following? To answer this question, we design and conduct a within-subject user study.\u00b9\nIndependent Variables. The study compares IG-MCTS to two baseline interaction methods:\nTeleoperation: Participants manually control the robot's low-level movements by providing actions u\u0142ow \u2208 Ulow at each timestep t using keyboard arrow keys. The robot deterministically executes these actions based on the environment's low-level transition function Tlow. The robot streams all incoming local observations of as RGB images to the human operator, providing real-time visual feedback.\nInstruction-following: Participants issue guidance in the form of a trajectory ( = (st, ..., St+n), specifying the desired sequence of states. The robot autonomously executes this trajectory using its internal model and the transition"}, {"title": "5.2.1. RESULTS", "content": "On H1 (Task Performance Metrics). Table 2 summarizes the average task metrics across all participants for two maze layouts. The results indicate that IG-MCTS requires significantly less communication compared to teleoperation and instruction-following, as it selectively transmits images at specific angles and times rather than streaming continuously. The results in Table 2 show that IG-MCTS results in the fewest robot steps, indicating more efficient task execution. Additionally, IG-MCTS requires less human guidance than instruction-following, demonstrating reduced\nOn H2 (Eye tracking for Cognitive Load). Figure 4 shows the mean pupil diameter for each method, normalized to a 0-1 time scale, interpolated at 1000 points, and smoothed with a window size of 5. The plot reveals that pupil diameters are overall smallest in IG-MCTS, followed by instruction-following, and largest in teleoperation. This qualitative trend aligns with the percent change in pupil diameter (PCPD) statistics in Table 3 (see calculation details in Appendix C.2), which also show the lowest value for IG-MCTS, followed by instruction-following and teleoperation. Table 3 also shows that IG-MCTS results in a higher blink rate than instruction-following and teleoperation. Additionally, Figure 5 presents the aggregate gaze heatmaps from"}, {"title": "5.2.2. DISCUSSION", "content": "We observe several notable IG-MCTS behaviors during interactions with participants that are reflected as helpful:\nGoal Reaching. IG-MCTS accumulates SLAM observations, enabling it to recognize and move toward visible goals within its field of view when it is confident in reaching them. This occurs even if the human has not extended the guided trajectory to the exact goal position, thereby improving overall efficiency.\nStrategic Guidance Compliance. When the human guidance is suboptimal, e.g., when the human specifies a route bypassing an alley entrance, unaware of an opening, IG-MCTS robots resolve such situations by assessing the information gained following each action before complying. In this case, rather than blindly following the specified route, the robot stops at the alley entrance and turns toward it to capture and share a snapshot, ensuring the human does not miss important information.\nEfficient Communication. Unlike instruction-following robots that take snapshots directly facing walls when blocked, IG-MCTS often angles itself 45 degrees toward the corner of a wall. This allows the human to gather information about two walls within the same snapshot, enhancing communication efficiency.\nImplication for Multitasking. IG-MCTS robots alternate between execution and ego-view communication phases. While our study's setup does not fully showcase this, in real-world scenarios with longer execution phases, this approach opens the possibility of a single human operator managing multiple robots by receiving interleaved updates. Although this introduces scheduling challenges, it highlights the potential to improve the human-to-robot ratio, a critical aspect in domains like search and rescue (Murphy, 2004)."}, {"title": "6. Conclusion", "content": "In this work, we addressed the challenge of human-robot cooperative navigation in environments with incomplete information by introducing the Information Gain Monte Carlo Tree Search (IG-MCTS) algorithm. By strategically balancing autonomous movement and informative communication, IG-MCTS leverages a learned human perception dynamics model to optimize interactions with a human operator. User studies demonstrated that IG-MCTS significantly reduces communication demands and human cognitive load while maintaining task performance comparable to teleoperation and instruction-following baselines.\nFuture work. Despite these promising results, several directions remain for future exploration. The reliance on in-context data for training the human perception dynamics model may hinder generalization. Future work could look into improving transferability through meta-learning or domain adaptation. Furthermore, extending IG-MCTS to continuous state and action spaces would bring it closer to real-world applications. Additionally, refining the interaction design could improve user experience. The current system relies on a single final ego-view snapshot, which, while efficient, may not provide sufficient spatial continuity. Integrating short video clips instead could offer richer contextual cues and improve spatial understanding."}, {"title": "A. Task Reward Composition", "content": "The task reward Rtask in our implementation incentivizes effective navigation and consists of three components:\n$R_{task}(\\tau,\\zeta) = R_{env}(\\tau) + R_{guidance}(\\tau, \\zeta) + R_{smooth}(\\tau)$.\nSuppose we rewrite \u03c4 as a state sequence (s1, s2,\uff65\uff65\uff65, sn), we can formally define each reward as follows:\n1. The navigation environment assigns a fixed reward rg > 0 when the robot reaches a goal location for the first time:\n$R_{env} (\\tau) = \\begin{cases}\n T_g, & s_n \\in G,\\\\\n -1, & otherwise.\n\\end{cases}$\n2. A step incurs an additional cost when the robot wanders away from the human's guidance.:\n$R_{guidance} (\\tau) = \\begin{cases}\n 0, & s_n \\in \\zeta,\\\\\n -log(n), & otherwise.\n\\end{cases}$\n3. The smoothness reward penalizes unnecessary revisits to previously visited states:\n$R_{smooth} (\\tau) = \\sum_{i=1}^{n-1} 1[S_i = S_n].$\nAdditional implementation-specific design: To reduce the search horizon, lower estimation variance, and improve computational efficiency, we terminate an MCTS rollout when the agent either (a) reaches the goal or (b) performs a communication action. However, this modification introduces a bias toward shorter paths. To correct for this, we impose a communication cost of c = 10 + n, where n represents the number of unfulfilled states in the human guidance."}, {"title": "B. Algorithm Pseudocode", "content": "Algorithm 1 IG-MCTS\n1: Input: human guidance \u015a, previous state-visitation history 70, current human perception state xo\n2: Parameters: iterations n = 100, exploration constant k = \u221a2, discount factor y = 0.99, depth d = 100\n3: Create root node vo with (To, xo), initialize Q(vo) \u2190 0, N(vo) \u2190 0, C(vo) \u2190 \u00d8\n4: for each iteration i from 1 to n do\n5: Set v \u2190 vo, stopping - False\n6: while v is not terminal and stopping = False do\n7: if v is fully expanded then\n8: v \u2190 arg $max_{v' EC(v)} Q(v') + k \\frac{\\sqrt{log(N(v))}}{N(v')}$\n9: else\n10: v, stopping - EXPAND(v)\n11: end if\n12: end while\n13: q \u2190 ROLLOUT(v)\n14: BACKPROPAGATE(v, q)\n15: end for\n16: Return action of best child c* = arg maxc\u2208C(vo) N(c)\nProcedure 2: EXPAND(v)\nProcedure 3: ROLLOUT(v)\n1: Choose an untried action a \u2208 A\n1: Initialize q\u2190 0, depth d\u2190 0\n2: if a \u2208 U then\n2: while d < T and (T, x) not terminal do\n3: s = last(T)\n3: Sample a \u2208 A\n4: \u03c4' \u2190 \u315c\u2295T(s, a)\n4: if a \u2208 U then\n5: Create node v' with (\u03c4', x)\n5: \u315c\u2190 \u315c\u2295T(s, a) where s = last(T)\n6: C(v) \u2190 C(v) \u222a {v'}\n6: else if a \u2208 O then\n7: Return v', False\n7: x' \u2190 F(x, (\u03c4,\u03b1))\n8: else if a \u2208 then\n8: end if\n9: x' \u2190 F(x, (\u03c4, \u03b1))\n9: q + q + \u03b3\u00aaR(\u03c4, \u03b6, x, x')\n10: Create node v' with (\u03c4, x')\n10: x\u2190x'\n11: Return v', True\n11: d\u2190d+1\n12: end if\n12: end while\n13: Return q\nProcedure 4: BACKPROPAGATE(v, q)\n1: Initialize qsample = q and d = 1\n2: while v is not null do\n3: Current value estimate w = $ Q(v) $ if N(v) > 0 else 0\n4: qsample \u2190 r(v) + \u03b3 [$dq_{sample}$ + (1 \u2212 \u03b4)w]\n5: Q(v) \u2190 Q(v) + q\n6: N(v) \u2190 N(v) + 1\n7: \u03b4\u2190 \u03b4(\u03c5)\n8: v \u2190 parent of v\n9: end while"}, {"title": "C. User Study Details", "content": "C.1. Baseline Pupil Diameter Measurement\nTo account for individual differences in pupil sizes, we ask each participant to read a brief text paragraph at the beginning of each session. The eye-tracking data from this period is used to calculate the mean pupil diameter as the baseline for the session's PCPD. We drafted these paragraphs to ensure comparable length and maintain neutral content."}, {"title": "C.2. PCPD Calculation", "content": "The percent change in pupil diameter (PCPD) is calculated as:\n$PCPD_t = \\frac{pupil diameter_t - baseline diameter}{baseline diameter}$"}, {"title": "C.3. Maze Layouts", "content": "The three maze layouts used in the user study were generated from fixed seeds on a Linux system (MuJoCo's randomization differs across operating systems). The layouts are visualized below:"}]}