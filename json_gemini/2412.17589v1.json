{"title": "PC Agent: While You Sleep, AI Works - A Cognitive Journey into Digital World", "authors": ["Yanheng He", "Jiahe Jin", "Shijie Xia", "Jiadi Su", "Runze Fan", "Haoyang Zou", "Xiangkun Hu", "Pengfei Liu"], "abstract": "Imagine a world where AI can handle your work while you sleep - organizing your research materials, drafting a report, or creating a presentation you need for tomorrow. However, while current digital agents can perform simple tasks, they are far from capable of handling the complex real-world work that humans routinely perform. We present PC Agent, an AI system that demonstrates a crucial step toward this vision through human cognition transfer. Our key insight is that the path from executing simple \"tasks\" to handling complex \u201cwork\u201d lies in efficiently capturing and learning from human cognitive processes during computer use. To validate this hypothesis, we introduce three key innovations: (1) PC Tracker, a lightweight infrastructure that efficiently collects high-quality human-computer interaction trajectories with complete cognitive context; (2) a two-stage cognition completion pipeline that transforms raw interaction data into rich cognitive trajectories by completing action semantics and thought processes; and (3) a multi-agent system combining a planning agent for decision-making with a grounding agent for robust visual grounding. Our preliminary experiments in PowerPoint presentation creation reveal that complex digital work capabilities can be achieved with a small amount of high-quality cognitive data - PC Agent, trained on just 133 cognitive trajectories, can handle sophisticated work scenarios involving up to 50 steps across multiple applications. This demonstrates the data efficiency of our approach, highlighting that the key to training capable digital agents lies in collecting human cognitive data. By open-sourcing our complete framework, including the data collection infrastructure and cognition completion methods, we aim to lower the barriers for the research community to develop truly capable digital agents. Resources are available at https://gair-nlp.github.io/PC-Agent/.", "sections": [{"title": "1 Introduction", "content": "While artificial intelligence (AI) has made remarkable progress in understanding and generating content like text (OpenAI, 2022, 2024a) and images (Rombach et al., 2022; Ramesh et al., 2022; Li et al., 2022), these capabilities exist largely in the representation world, where AI systems master pattern recognition and symbolic processing (Bengio et al., 2014). Recent advances in large language model (LLM) powered autonomous agents have garnered significant attention for their ability to perform task-oriented interactions (Weng, 2023; Wu et al., 2023). In the domain of digital agents - AI systems that assist with digital tasks - current solutions have demonstrated basic capabilities in simple tasks like web searches (Yao et al., 2023a; Deng et al., 2023). However, they remain far from capable of handling complex real-world work that humans routinely perform (Xie et al., 2024; Bonatti et al., 2024), such as video editing, presentation creation, or report writing - activities that require sustained operation across multiple applications, sophisticated decision-making, and even aesthetic judgment. To meaningfully reduce human workload, digital agents must evolve beyond simple task execution to manage more complex work efficiently.\nThe Critical Gap: From Simple Task Automation to Complex Work Completion Most existing approaches in building digital agents rely heavily on proprietary LLM APIs (Wu et al., 2024a; Zheng et al., 2024; Zhang et al., 2023). While few works have attempted to train models for computer operation, these efforts have primarily concentrated on either improving basic visual grounding capabilities (Gou et al., 2024; Wu et al., 2024b) or specific domains like web (Liu et al., 2024). However, these approaches still struggle with complex real-world computer work. Through our analysis, we identify two critical challenges: (1) foundational visual grounding: The ability to precisely locate GUI elements (e.g. the Start button in the taskbar) represents a fundamental capability that current vision-language models still struggle with (Xie et al., 2024; Gou et al., 2024). (2) complex cognitive understanding: More crucially, current agents lack the cognitive capabilities required for complex work. They struggle with maintaining contextual awareness across extended interaction sequences, making dynamic decisions based on changing environments, and adapting strategies in response to execution outcomes. While prompt engineering (Liu et al., 2021) can partially help, it proves inadequate for truly complex work.\nHuman Cognition: The Missing Key Our key insight is that the path to the digital world lies in human cognition transfer. When completing complex work, the human brain engages in sophisticated cognitive activities understanding objectives, analyzing current states, reflecting on past actions, and planning future strategies. These cognitive processes ultimately crystallize into clear decisions, which are then externalized into observable behavior. This observation leads us to develop a novel framework that efficiently captures and transfers human cognition to AI agents.\nOur Contributions In this work, we present three key contributions that together form a complete solution for developing truly capable digital agents:\n\u2022 PC Tracker: We introduce the first open-source infrastructure for efficiently collecting large-scale human-computer interaction trajectories. This lightweight system captures not just actions but the complete cognitive context of human computer use, establishing a rich foundation for agent training.\n\u2022 Cognition Completion Pipeline: We develop an innovative approach that transforms raw interaction data into cognitive trajectories by completing action semantics and thought processes in steps. This enables AI to learn not just what humans do, but why and how they make decisions in complex digital environments."}, {"title": "2 How Far Are We From True Digital Agents", "content": "2.1 The Capability Gap in Real-world Applications: From \u201ctask\u201d to \u201cwork\"\nIn the evolution of digital agents, LLM-powered autonomous agents have garnered significant attention due to their ability to engage in complex task-oriented interactions (Weng, 2023; Wu et al., 2023). Early works like ReAct (Yao et al., 2023b) and Reflexion (Shinn et al., 2023) established fundamental frameworks for agent reasoning and self-optimization. In implementing digital agents, two primary technical approaches emerged: backend access (Trivedi et al., 2024; Team, 2024) and frontend GUI interaction. The latter has gained prominence by operating without backend access permissions, offering universal applicability, better security, and adaptability to interface changes. With the advancement of vision-language models (Liu et al., 2023; OpenAI, 2024b,c; Wang et al., 2024), GUI agents have made significant progress in both visual understanding (Hong et al., 2023; Liu et al., 2024; You et al., 2024; Yang et al., 2023; Cheng et al., 2024; Lu et al., 2024) and general task capabilities, demonstrated by applications across web browsers (Gur et al., 2024; Zheng et al., 2024; Yao et al., 2023a; Nakano et al., 2022), mobile devices (Zhang et al., 2023; Hoscilowicz et al., 2024; Zhang et al., 2024b; Zhang and Zhang, 2024) and desktops (Wu et al., 2024a; Zhang et al., 2024a). To advance the field, researchers have established important evaluation benchmarks through comprehensive datasets (Rawles et al., 2023; Deng et al., 2023) and execution-based environments that simulate real-world scenarios (Zhou et al., 2024; Koh et al., 2024; Xie et al., 2024; Bonatti et al., 2024; Rawles et al., 2024).\nDespite notable progress in the field, we remain far from achieving truly capable digital agents. Even the most advanced systems, including the recently celebrated new Claude-3.5-Sonnet (Anthropic, 2024), still significantly under-perform humans in computer use (Xie et al., 2024; Hu et al., 2024). While current digital agents can perform simple tasks like web searches and file copying, they face significant challenges when tackling comprehensive work that better reflect real-world computer use. Consider video editing, presentation creation, and report generation - these work require sustained operation across multiple applications, sophisticated decision-making, and even human-level aesthetic judgment. We argue that true digital agents should be able to efficiently handle these complex work, not just simple tasks, to meaningfully reduce human workload.\n2.2 Breaking Barriers: Visual Grounding and Cognitive Understanding\nTo realize true digital agents, we have identified two critical technical challenges: foundational visual grounding capabilities and deep cognitive understanding.\nVisual Grounding Visual grounding - the ability to precisely locate elements in GUI - represents a fundamental capability for agents to effectively use computers. This is because click-based interactions, which require precise coordinate outputs, constitute a significant portion of computer tasks. However, most current vision-language models (VLMs), including state-of-the-art proprietary models like GPT-40 and open-source models like Qwen2-VL, still lack this basic capability. Contemporary works such as UGround (Gou et al., 2024) and OS-ATLAS (Wu et al., 2024b) attempt to address this issue by fine-tuning models on large-scale GUI visual grounding datasets. While successful in developing the target capability, these approaches compromise the model's general question-answering and instruction-following abilities. Notably, Anthropic's new Claude-3.5-Sonnet has emerged as the first frontier model with state-of-the-art computer use capabilities, though its training details remain undisclosed. We believe that both specialization and general capabilities are equally important for effective agent models. Encouragingly, we discover that the open-source general-purpose VLM Molmo (Deitke et al., 2024) demonstrates exceptional visual grounding abilities, establishing a strong foundation for open-source digital agents.\nCognitive Understanding However, even with robust visual grounding capabilities, current agents still struggle to complete complex computer work. Our analysis reveals that cognitive understanding represents the crucial missing piece in this puzzle - one that we will address through our human cognition transfer framework. This limitation manifests in two key aspects:"}, {"title": "3 Cognition Transfer: A Gateway for AI into the Digital World", "content": "The journey to the digital world represents a fundamental evolution from understanding and responding in language to taking meaningful action for LLMs. Although current digital agents can perform simple tasks, they are still far from effectively completing real-world work in digital environments, where humans manage their everyday work and life activities. The fundamental challenge lies here: how can we enable Al systems to fundamentally evolve from understanding tasks to executing them in the digital world?\nWe posit that human cognition transfer is pivotal in addressing this challenge. Human cognition is primarily based on the dynamic interaction between internal thinking and external behavior. When performing complex work in real environments, the human brain engages in sophisticated cognitive activities, including understanding objectives, analyzing current states, reflecting on the past, and planning future strategies. This series of cognitive processes ultimately crystallizes into clear decisions, which are then externalized into observable behavior. In this sense, behavior serves as the external projection of complex cognitive activities in the human brain.\nIf AI systems can acquire human cognition to interact with the digital world, they can complete complex work as naturally as humans do. However, current technology cannot directly record cognitive activities in the brain. To address this, we propose an indirect approach:\n1. First, we designed a lightweight infrastructure to efficiently collect raw interaction trajectories between humans and digital devices. These trajectories not only capture the specific steps of task execution but also reflect dynamic exploration, trial and error, and optimization.\n2. Then, we leverage LLMs to analyze these raw trajectories and complete the cognitive processes behind human behavior by completing action semantics and thought processes in steps, transforming them into cognitive trajectories. These cognitive trajectories can be considered effective approximations of genuine human cognitive activities.\nBy learning from these cognitive trajectories, AI systems can not only mimic specific operational behaviors but also master the underlying human cognition more efficiently. Our experimental results demonstrate remarkable data efficiency in enabling AI systems to perform complex computer work through human cognition transfer."}, {"title": "4 PC Tracker: Human-Computer Interaction Data Collection Infrastructure", "content": "While supervised fine-tuning has proven effective for adapting LLMs to many tasks, large-scale computer interaction trajectories remain severely limited compared to conventional text or image datasets. This data bottleneck has become a key impediment in developing digital agents that can effectively use computers like humans do.\nTo address this bottleneck, we present PC Tracker, the first lightweight infrastructure for efficient large-scale collection of human-computer interaction data. Similar to screen recording mechanisms, PC Tracker runs seamlessly in the background, recording user actions by monitoring keyboard and mouse activities while capturing screenshots to document state observations. This enables the collection of real-world interaction trajectories at scale, establishing a rich foundation for future research in agent training and cognition engineering. PC Tracker is designed with the following key features, as summarized in Figure 2."}, {"title": "4.1 Tracking Strategy", "content": "4.1.1 Action Recording\nThe original records of certain actions are often fragmented, leading to a loss of semantic information, such as double-click, scroll, hotkey, and type. For instance, a type action is split into individual key press operations. Besides, some actions require special rules to be identified from the original operations, such as the drag to action, which is originally just a normal mouse release.\nTo address this issue, we designed heuristic algorithms that track the history of mouse and keyboard activities to encapsulate these raw operations into a unified action space A designed based on OSWorld (Xie et al., 2024), as shown in Figure 4. This action space, originally designed for Al agents, significantly reduces the difficulty for AI to understand and learn human operational behaviors. For example, Figure 5 demonstrates how PC Tracker encapsulates 9 raw actions into a single type text action when the user intends to type \"Hello\". The user's raw actions involve capitalization change, character typing, and error corrections, as shown in the left column. These raw actions are"}, {"title": "4.2 Dual-Mode Collection", "content": "accumulated into the type buffer and combined into the unified action \"type text: Hello\" upon detecting type completion.\nAdditionally, we observed that coordinate-based click-related actions (the top five actions in Figure 4) lack sufficient semantic information compared to other actions like keyboard inputs. For instance, click (333, 444) is far more abstract than press key: enter. To support downstream action semantic completion, PC Tracker records additional contextual information for click-related actions. Specifically, we implemented a low-overhead function get_element_info_at_position(x, y) that efficiently retrieves the element's information from coordinates via system API, including its bounding box coordinates and element name (see Figure 6). When a click-related action occurs, PC Tracker passes coordinates to this function and records the clicked element's information.\n4.1.2 Observation Capture\nWe adopt a carefully designed timing strategy to record corresponding screenshots as state observations for actions. While it may seem intuitive to record the screenshot at the moment an action is detected, this can lead to inaccurate observation records. Figure 7 shows that humans naturally observe the environment state before making decisions and taking actions. Therefore, PC Tracker records the screenshot just before the action occurs. To achieve this, our system continuously takes screenshots and keeps the latest one in memory. When an action is detected, this latest cached screenshot (approximately 0.1 seconds prior) is recorded as the observation.\nThe design of only recording critical events offers notable advantages over complete video recording. It significantly reduces storage requirements, thus allowing us to capture screenshots at maximum resolution - a crucial feature for future models, as many interface details such as small-font text are only legible at high resolutions.\nBesides, while many works require a complete accessibility tree to aid observation (Jia et al., 2024; Agashe et al., 2024), we find this impractical in real use. Crawling the accessibility tree requires a lengthy recursive process (taking tens of seconds to minutes), which forces users to wait after each action until the tree traversal completes, severely disrupting normal computer usage. Given the rapid progress in VLMs, we believe maintaining such exhaustive tree records is unnecessary.\n4.2 Dual-Mode Collection\nPC Tracker supports two recording modes: task oriented and non-task oriented, primarily differentiated by whether the interaction trajectories are associated with certain task descriptions. An overview is shown in Figure 8.\nThe task oriented mode records interaction trajectories that are explicitly associated with specific tasks, primarily designed to support easy annotation for supervised fine-tuning. It is divided into given task and free task modes. In the given task mode, PC Tracker randomly assigns tasks from a predefined task library to users, who then use the computer to complete the tasks while their interaction trajectories are recorded. In the free task mode, users can freely use the computer without predefined constraints and provide"}, {"title": "4.3 Privacy and Data Transparency", "content": "a task description for their operation sequence after completion. Notably, even in the given task mode, users have the flexibility to revise task descriptions after stopping the recording. This feature allows users to more accurately describe their activities, particularly if their operations deviated from the original assigned task or if they discovered a better way to characterize their work.\nIn the non-task oriented mode, the system directly records interaction trajectories without a specific task, enabling long-term deployment and effortless collection of large-scale data. While this mode may require additional approaches to infer user intentions, it provides a rich foundation for studying general human operational behaviors and exploring pre-train methods.\n4.3 Privacy and Data Transparency\nPC Tracker integrates privacy considerations and data transparency in its design. During recording sessions, users have full control over their data - if they notice sensitive information has been captured or if they are not satisfied with their task completion process, they can choose to discard the recording immediately without saving it. For saved trajectories, PC Tracker stores recordings locally and visualizes them in Markdown format, allowing users to easily review and check their recorded operations at any time."}, {"title": "5 Cognition Completion: From Raw Interaction Data to Cognitive Trajectory", "content": "Converting raw human-computer interaction data into meaningful cognitive trajectories is essential for AI to learn from human demonstrations. To achieve this goal, we conduct two-stage sequential post-processing: data refinement to ensure quality, followed by cognition completion to extract atomic action semantics and reconstruct the underlying thought process for each action.\n5.1 Data Refinement\nTo optimize the quality of raw interaction data, we designed and implemented a comprehensive pipeline that consists of three steps: 1) trajectory filtering, 2) action filtering, and 3) standardization.\nTrajectory filtering First, we examined the completeness of trajectories and files to eliminate error data caused by unexpected incidents (such as forced termination of PC Tracker), thereby ensuring robustness.\nAction filtering Next, we focused on identifying and removing two categories of actions: (1) actions associated with PC Tracker, such as clicks on the start record button; (2) redundant actions in actual human operations, like control key presses for hotkey prefixes, consecutive waits due to operation pauses, and meaningless clicks. We found that these actions not only introduce unnecessary burdens to model learning but also interfere with the subsequent cognition completion process.\nStandardization Finally, to ensure consistency across different data sources, we standardized all screenshot resolutions to 1080p (1920\u00d71080), providing a standardized image input format for training.\n5.2 Cognition Completion\nBuilding upon the refined trajectories, the cognition completion stage tackles a more crucial challenge: reconstructing human cognition of computer use. This stage consists of two steps: the first step focuses on reconstructing the atomic semantics of actions, and the second step reconstructs the thought processes behind actions based on the enriched action semantics. A detailed view is shown in Figure 9.\n5.2.1 Action Semantic Completion\nAs mentioned in Section 4.1.1, click-related actions lack semantic information due to their coordinate-based nature. To comprehensively reconstruct human cognition, we supplement click-related actions with semantic information before thought completion by generating high-quality descriptions for click targets.\nPC Tracker records additional information for click elements, including the bounding box coordinates and element name (as shown in Figure 6). We highlight the click position and bounding box with red marks in the corresponding screenshot, which visualizes click-related actions on the screenshot to facilitate model understanding. This marked screenshot serves as visual input in both action semantic completion and thought completion phases.\nWhen generating descriptions for click targets, we leverage GPT-40 to take the marked screenshot and the element name as input and generate high-quality descriptions. Notably, while PC Tracker records element names through system API, these names are often inadequate: many are missing, overly generic (e.g., numerous elements in Chrome simply labeled as \"Chrome\"), excessively verbose (e.g., containing complete text content for text elements), or even incorrect. In contrast, GPT-40 generates accurate and concise descriptions of click targets, facilitating the understanding of actions during thought completion."}, {"title": "6 PC Agent: Cognitive Agent for Complex Computer Work", "content": "This section presents our implementation of PC Agent, an AI system capable of complex computer work. Notably, our system is built entirely on open-source models, avoiding dependence on frontier proprietary models like GPT-40 or Claude-3.5-Sonnet.\n6.1 Multi-Agent System\nIn Section 2.2, we identified two major challenges of current digital agents: visual grounding and cognitive"}, {"title": "6.2 Training Planning Agent via Cognitive Trajectories", "content": "understanding. In our preliminary implementation of PC Agent, we address these challenges through a multi-agent architecture. First, we trained a planning agent by learning from human cognitive trajectories, enabling it to acquire human cognition for effective planning. Second, we implemented a robust grounding agent with a self-validation mechanism that achieves near-human perfection. The two agents work collaboratively: the planning agent handles action decision-making, while the grounding agent executes click-related actions.\nThe workflow proceeds as follows, illustrated with an example in Figure 10: The planning agent first analyzes the task and observes the current state to generate a thought process and an action decision. If the action is not click-related, it will be directly executed. Otherwise, the generated target description of the click-related action will be forwarded to the grounding agent. The grounding agent not only generates the specific position coordinates of the click target but also validates grounding accuracy. If the grounding agent discovers that the planning agent is attempting to click on a non-existent target on the screen, the planning agent is prompted to reformulate its action plan. All actions are executed using the PyAutoGUI library.\nWe believe that at the current stage, the multi-agent architecture can better leverage existing VLMs' capabilities through clear task division and is likely to achieve better results in the short term. However, looking ahead, end-to-end systems like the new Claude-3.5-Sonnet may represent the future direction of this field.\n6.2 Training Planning Agent via Cognitive Trajectories\nSection 2.2 posits that the limitations of current digital agents primarily stem from the lack of essential agent training data. To tackle this challenge, we utilize PC Tracker and cognition completion post-processing to efficiently build high-quality cognitive trajectories, which are then used to train the planning agent.\nIn the resulting training data (as shown in Figure 11), system prompts, task descriptions, and historical steps are used as textual inputs, while the corresponding screenshot (without mark) is used as the visual input. Note that each historical step consists of the thought process and action decision, incorporating the progress state within the overall task and any exploration behaviors from prior steps in natural language. For example, it allows the model to determine whether the desktop indicates the task has just started, or has been completed with all applications closed.\nFurthermore, we structure the responses into distinct thought and action components. As discussed earlier, the thought process captures human cognition of computer use. This data organization enables the model to learn the underlying cognitive patterns behind actions and aligns with the ReAct paradigm where models generate verbal reasoning traces and actions in an interleaved manner.\n6.3 Robust Visual Grounding with Self-Validation"}, {"title": "7 Experiment", "content": "7.1 Experimental Setup\nTo validate the effectiveness of our approach, we preliminarily conducted experiments on a relatively small scale, selecting the task of creating PowerPoint presentations as our primary testing ground. This choice is significant for mainly two reasons: First, presentation creation is a practical and common office task where automation can significantly reduce human workload. Second, it requires sophisticated cross-software coordination, particularly when creating high-quality presentations that involve web browsing to search and collect relevant materials. In future experiments, we'll dive into a broader range of real-world scenarios where completing a work demands substantial human cognition, to further validate the scalability and effectiveness of our approach.\nData Collection Using PC Tracker's task oriented mode, we collected a dataset of 133 interaction trajectories, consisting of two categories: free-form Chrome and PowerPoint related tasks (30%, 58 trajectories with an average of 29 events each) and guided PowerPoint presentation creation tasks (70%, 75 trajectories with an average of 50 events each). It is worth noting that the average action number per task in our dataset is significantly larger than those in existing datasets, as shown in Figure 13. The free-form trajectories include diverse but relatively simple operations such as web browsing, Chrome settings adjustment, and PowerPoint layout editing. In contrast, the guided presentation creation tasks follow specific presentation objectives, requiring users to engage in a more complex workflow that involves switching between PowerPoint and Chrome to collect online resources for presentation content, authentically simulating real-world work patterns.\nTraining We fine-tuned the Qwen2-VL-72B-Instruct model with our collected dataset. The training was conducted on 32 H100 GPUs for about 2 hours. We set the context length to 8,192 tokens.\n7.2 Evaluation\n7.2.1 Why Not Existing Benchmarks\nWhile benchmarks like OSWorld (Xie et al., 2024) play valuable roles in evaluating AI agents' basic capabilities of computer use, they are inappropriate for our specific evaluation needs. Existing benchmarks tend to focus on basic operations, like duplicating the last two slides in PowerPoint. However, creating high-quality presentations is significantly more complex. The deliverables of such tasks are highly variable, often requiring subjective assessments based on aesthetics, clarity, and alignment with human preferences, which largely extends beyond the scope of existing benchmarks. Furthermore, given that our agent is currently specialized in PowerPoint presentation creation, utilizing general benchmarks to assess its performance would be both insufficient and inappropriate.\n7.2.2 Human Evaluation and Case Studies\nBased on these considerations, we employed human evaluation to assess PC Agent's performance in real-world application scenarios. In our target domain - creating high-quality presentations - PC Agent exhibited outstanding performance. Its capability to execute dozens of steps and the quality of generated presentations substantially surpass existing solutions, as shown in Figure 14. We examined two representative use cases:\nCase1: Complex Presentation Design We challenged PC Agent to create sophisticated presentations under detailed task specifications. It successfully accomplished the following objectives: 1) switching between PowerPoint and Chrome for resource collection, 2) organizing content into presentations with pictures, and 3) saving completed presentations to the desktop and closing all applications. Through human evaluation, we found that our"}, {"title": "7.2.3 Analysis", "content": "Failed Case Study Our analysis revealed that mistakes stem from various ways during execution. However, these errors share a common challenge: the agent's limited ability to recover once mistakes occur. Although our multi-agent workflow can prevent some planning mistakes through the grounding agent's feedback, incorrect plans that get executed often lead to unrecoverable failures, indicating the need for better error recovery capabilities.\nImpact of Task Description During our evaluation, we observed that the level of detail in task descriptions significantly influences PC Agent's performance, as exemplified in Figure 15. Comprehensive task descriptions enable it to better understand human expectations, leading to more satisfactory outcomes. In contrast, brief task descriptions often result in insufficient understanding of requirements, causing the agent to favor quick but superficial solutions. Our findings suggest that optimizing task description quality is crucial for improving agent performance.\nGeneralization Capability Our evaluation suggests that PC Agent possesses adequate performance on web browsing tasks, despite the relatively limited proportion of Chrome-related training data. This observation suggests that the operational skills acquired in specific scenarios may contribute to improved adaptability across a broader range of applications.\nData Efficiency With a relatively small training dataset of just 133 trajectories, PC Agent demonstrates remarkable task processing capabilities, particularly in PowerPoint presentation creation tasks. This data efficiency primarily stems from the high quality of the human cognitive trajectories we constructed, demonstrating the effectiveness of our framework."}, {"title": "8 Conclusion and Outlook", "content": "In conclusion, we presented a cognition transfer framework that efficiently guides AI to the digital world through three key components: PC Tracker for collecting human-computer interaction data, a two-stage post-processing for cognition completion, and a multi-agent system for computer task automation. Looking forward, we identify several potential directions:\nScaling and Generalization While our approach demonstrates effectiveness in presentation creation with limited training data, validating its generalization ability requires large-scale experimentation across diverse software applications and task domains."}, {"title": "B Prompts for Cognition Completion", "content": "B.1 Action Semantic Completion\nThe first stage of cognition completion is action semantic completion. Specifically, we first generate descriptions for the click targets, and than refine the generated descriptions. Table 1 and Table 2 present the prompts used for these two processes."}]}