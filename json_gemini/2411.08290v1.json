{"title": "RESOLVE: RELATIONAL REASONING WITH SYMBOLIC AND OBJECT-LEVEL FEATURES USING VECTOR SYMBOLIC PROCESSING", "authors": ["Mohamed Mejri", "Chandramouli Amarnath", "Abhijit Chatterjee"], "abstract": "Modern transformer-based encoder-decoder architectures struggle with reasoning tasks due to their inability to effectively extract relational information between input objects (data/tokens). Recent work introduced the Abstractor module, embedded between transformer layers, to address this gap. However, the Abstractor layer while excelling at capturing relational information (pure relational reasoning), faces challenges in tasks that require both object and relational-level reasoning (partial relational reasoning). To address this, we propose RESOLVE, a neuro-vector symbolic architecture that combines object-level features with relational representations in high-dimensional spaces, using fast and efficient operations such as bundling (summation) and binding (Hadamard product) allowing both object-level features and relational representations to coexist within the same structure without interfering with one another. RESOLVE is driven by a novel attention mechanism that operates in a bipolar high dimensional space, allowing fast attention score computation compared to the state-of-the-art. By leveraging this design, the model achieves both low compute latency and memory efficiency. RESOLVE also offers better generalizability while achieving higher accuracy in purely relational reasoning tasks such as sorting as well as partial relational reasoning tasks such as math problem-solving compared to state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Analogical reasoning, which involves recognizing abstract relationships between objects, is fundamental to human abstraction and thought. This contrasts with semantic (meaning-based) and procedural (task-based) knowledge acquired from sensory information, which is typically processed through contemporary approaches like deep neural networks (DNNs). However, most of these techniques fail to extract abstract rules from limited samples Barrett et al. (2018); Ricci et al. (2018); Lake & Baroni (2018).\nThese reasoning tasks can be purely or partially relational. Figure 1 presents an example of a purely relational task where the objects (e.g. frog, mountains) are randomly generated. In this task, only the information representing relationships between the objects is relevant, not the objects themselves. By contrast, in Figure 2a the purpose is to learn the abstract rule of subtraction, which is unknown to the model, from pairs of MNIST digits. This abstract rule relies on the relational representation between the digits (derived from their relationship with one another, in this case their ordering) and the digits themselves (the values being subtracted), which are object features. This is a partially relational problem. Similarly, in Figure 2b, the purpose is to learn the abstract rule of the quadratic formula (i.e. the solution to the quadratic problem shown at the bottom of Figure 2b) from the object features (derived from the text tokens representing equation coefficients) and the relational representation (derived from the coefficient ordering).\nThese relational or partially relational tasks have been shown to be problematic for transformer-based architectures (Altabaa et al., 2023), which encode both the object features and relational representations into the same structure. (Altabaa et al., 2023) instead created a learnable inductive bias derived from the transformer architecture for explicit relational reasoning. Although this solution is sufficient for purely relational tasks such as Figure 1, it is less efficient for partially relational tasks such as Figures 2a and 2b where the object features and relational representations are both significant.\nThe poor ability of transformers to superpose relational representations and object-level features is due to the low dimensionality of their components, causing interference between object features and relational representations (Webb et al., 2024b). By contrast, vector symbolic architectures (VSA) have used high-dimensional spaces to superpose object features and relational representations with low interference Hersche et al. (2023). Transformer-based architectures are moreover known to be power-inefficient due to the attention score computation Debus et al. (2023). Vector symbolic architectures have been proven to be power-efficient Menet et al. (2024) with low memory overhead due to the low-bitwidth (bipolar) representation of high-dimensional vectors. However, current VSA techniques require prior knowledge of abstract rules and a pre-engineered set of relations and objects (e.g., blue, triangle), making them unsuitable for sequence-to-sequence reasoning."}, {"title": "2 RELATED WORK", "content": "To address the problem of learning abstract rules, symbolic Al architectures such as the Relation Network (Santoro et al., 2017) propose a model for pairwise relations by applying a Multilayer Perceptron (MLP) to concatenated object features. Another example, PrediNet (Shanahan et al., 2020), utilizes predicate logic to represent relational features. Symbolic AI approaches combined with neural networks were leveraged by neurosymbolic learning Manhaeve et al. (2018); Badreddine et al. (2022); Barbiero et al. (2023); Xu et al. (2018) to improve this rule learning, with optimizations such as logical loss functions Xu et al. (2018) and logical reasoning networks applied to the predictions of a neural network Manhaeve et al. (2018). However, these systems require prior knowledge of the abstract rules guiding a task. They also require pre-implementation of object attributes Hersche et al. (2023) (e.g., red, blue, triangle, etc.). This approach is only feasible for simple tasks (e.g., Raven's Progressive Matrices Raven (1938)) and is not appropriate for complex sequence-based partially relational tasks such as the quadratic solution of Figure 2b.\nFor sequence-based partially relational tasks such as the math problem-solving of Figures 2a and 2b an encoder-decoder structure with transformers has been used Saxton et al. (2019). However, transformers often fail to capture explicit relational representations.\nA solution to the shortcomings of encoder-decoder approaches is proposed in (Webb et al., 2024b), using the relational bottleneck concept. This aims to separate relational representations learned using a learnable inductive bias from object-level features learned using connectionist encoder-decoder transformer architectures or DNNs. Several models are based on this idea: CoRelNet, introduced in (Kerg et al., 2022), simplifies relational learning by modeling a similarity matrix. A recent study (Webb et al., 2020) introduced an architecture inspired by Neural Turing Machines (NTM) (Graves et al., 2014), which separates relational representations from object features. Building on this concept, the Abstractor (Altabaa et al., 2023) adapted Transformers (Vaswani et al., 2017) for abstract reasoning tasks by creating an 'abstractor' a mechanism based on cross-attention applied to relational representations for sequence-to-sequence relational tasks. A model known as the Visual Object Centering Relational Abstract architecture (OCRA) (Webb et al., 2024a) maps visual objects to vector embeddings, followed by a transformer network for solving symbolic reasoning problems such as Raven's Progressive Matrices (Raven, 1938). A subsequent study (Mondal et al., 2024) combined and refined OCRA and the Abstractor to address similar challenges. However, these relational bottleneck structures still suffer from the drawback of intereference between the relational representations and object features in deep layers due to their lower feature dimensionality (Webb et al., 2024b).\nHowever, recent work (Hersche et al., 2023) has shown that Vector Symbolic Architectures (VSAs), a neuro-symbolic paradigm using high-dimensional vectors (Kanerva, 2009) with a set of predefined operations (e.g., element-wise addition and multiplication), exhibit strong robustness to vector superposition as an alternative to the relational bottleneck. In addition, Hyperdimensional Computing (HDC) is recognized for its low computational overhead (Mejri et al., 2024b; Amrouch et al., 2022) compared to transformer-based approaches. However, prior work on VSAs has relied on pre-engineered set of objects and relations, limiting their applicability to sequence-to-sequence reasoning tasks.\nIn contrast to prior research, this paper is the first to leverage VSAs to efficiently combine object-level information with relational information in high-dimensional spaces, taking advantage of the lower interference between object features and relational representations in high dimensions. We also propose the first efficient attention mechanism for high-dimensional vectors (HD-Attention)."}, {"title": "3 OVERVIEW", "content": "In this section we present an overview of prior architectures used to learn abstract rules, and use them to illustrate the unique features of our VSA-based architecture.\nFigure 3a illustrates the self-attention mechanism used in transformer architectures Vaswani et al. (2017). In step in, objects are first encoded into keys, queries, and values. In step , self-attention captures correlations between keys and queries in an attention score matrix. Finally, in step 13, this matrix is used to mix values and create encoded outputs. Self-attention is thus designed to capture correlations between input object sequence elements. However, it fails to capture relational representations of the input object sequence Altabaa et al. (2023), leading to poor generalization capability for abstract rule-based tasks. The Abstractor mechanism aims to fix that flaw.\nFigure 3b illustrates the Abstractor mechanism. In step an of the abstractor architecture shown in Fig. 3b, objects are first encoded into queries and keys, which are used to build attention scores (step a2 similar to self-attention. In parallel, in step as, a set of symbols (learnable inductive biases) consisting of a set of trainable vectors are encoded into values. In step as, the attention scores and the symbols are used to generate the abstract outputs, a dedicated structure for relational representations Altabaa et al. (2023); Webb et al. (2020) that are disentangled from object-level features. This approach, known as the relational bottleneck, separates object-level features from relational representations. However, this separation can make it difficult to learn abstract rules for partially relational tasks.\nThe RESOLVE architecture (shown in Figure 3c) explicitly structures the learning of relational information while encoding object-level features. In step , objects and symbols are mapped to a high-dimensional (HD) space using an high-dimensional encoder to generate HD Objects (object-level feature representations) and HD Abstract outputs (relational representations). The HD Objects, shown three times, are identical. They are first used in step to compute attention scores. Then, in step 3, these attention scores are used as weights to combine the HD Objects, producing an HD encoded output. In step 4, the HD Abstract output and the HD encoded output are superimposed through a binding operation (Hadamard product) to provide a mixed relational representation and object feature vector in high dimensions, avoiding the interference between relational representations and object features that this mixing causes in lower dimensions (seen in transformers (Webb et al., 2024b))."}, {"title": "4 RELATIONAL BOTTLENECK AND VSA APPROACH MODELING", "content": "Figure 4 contrasts the self-attention mechanism (Figure 4a), the relational cross-attention (Figure 4b), and our approach (Figure 4c), using red and turquoise colors. The self-attention mechanism in Figure 4a is applied to a sequence of objects (for instance, token embeddings) denoted by $0_{1..N}$. Each object is of dimension F. The objects are encoded into Keys, Queries, and Values through linear projections: $\\phi_{q} : 0 \\leftrightarrow O\\cdot W_{Q}$, $\\phi_{K}: 0 \\leftrightarrow O\\cdot W_{K}$, and $\\phi_{v} : 0 \\leftrightarrow O \\cdot W_{V}$, where $W_{Q}, W_{K}, W_{V}$ are learnable matrices. The Queries and Keys are used to compute an attention score matrix, which captures the relationships between encoded objects through a pairwise dot product $(., .)$. (Altabaa et al., 2023) interprets this as a relation tensor, denoted by $R = [(\\phi_{Q}(0_{i}), \\phi_{K}(0_{j}))] _{i, j = 1}{N}$. R is normalized to obtain $\\overline{R}$ using a Softmax function to produce probabilities. SelfAttention(0) thus generates a mixed relational representation E of the encoded objects (Values) through the normalized relational tensor $\\overline{R}$ (i.e., $E_{i} = \\sum{j}; \\overline{R}{ij}\\phi_{V}(0_{j})$). A transformer uses the matrix R to capture input relations and $\\phi_{V}$ to encode object-level features, but $\\phi_{V}$ is not designed to learn abstract rules.\nFigure 4b shows the relational attention mechanism by (Altabaa et al., 2023) that isolates object-level features (in red) from abstract/relational information (in turquoise) to improve abstract rule learning. Like self-attention, the objects $0_{1..N}$ are first encoded into Keys and Queries through the same learnable projection functions $\\phi_{K}$ and $\\phi_{Q}$, which are then used to build a normalized relation tensor $\\overline{R}$. In parallel, a set of symbols $S_{1..N}$ (N learnable vectors with the same dimensionality as the objects) are encoded into values using a projection function $\\phi_{V}$ (i.e., $V = \\phi_{V}(S)$). The encoded symbols (Values) are mixed using the relation tensor weights through a relational cross-attention mechanism to generate a mixed relational representation containing less object-level information and more relational (abstract) information. These are called Abstract States, denoted as $A_{1..N}$ ($A{i} = \\sum{j}; \\overline{R}{ij}\\phi_{V}(S_{j})$).\nFigure 4c shows our VSA-based system. It starts by encoding the objects $0_{1..N}$ from their F-dimensional space into a high-dimensional (D-dimensional) space using an encoder denoted by $\\Phi_{HD}$ to generate high-dimensional object vectors $h_{0_{1..N}}$. We extract relational scores from this using a novel HD-attention mechanism to build a HD relation tensor, denoted as R. This matrix is then normalized through a softmax function to generate $\\overline{R}$. These normalized scores are used to mix the $h_{0_{1..N}}$, generating encoded object-level high-dimensional vectors $h_{E_{0i}} = \\sum{j} \\overline{R}{ij}h_{0j}$. A set of learnable symbols $S_{1..N}$ with the same dimensionality as the objects is used to encode relational information. These symbols are mapped to the high-dimensional space through $\\Phi_{HD}$, generating $h_{S_{1..N}}$. These high-dimensional symbolic vectors are bound (i.e., Hadamard product/element-wise"}, {"title": "5 RESOLVE: HD-ENCODER AND HD-ATTENTION MECHANISM WITH HYPERVECTOR BUNDLING", "content": "Figure 5 shows the HD-Encoder and HD-Attention mechanism applied to an input sequence of objects $0_{1}, ..., O_{N}$ (see Figure 4). In Step 1, objects are mapped from the F-dimensional feature space to a D-dimensional HD space (D~103) using the HD encoder $\\Phi_{HD}$. We have implemented $\\Phi_{HD}$ using single-dimension convolution operations inspired by Mejri et al. (2024a). In this encoder scheme, each object $O_{i}$ is convolved with a learnable high dimensional vector called the HD-Basis denoted by $B_{i} \\in R^{N \\times D-F+1}$, giving rise to the high-dimensional HD Object hypervectors $h_{0i}[j] = \\sum_{k} O_{i}[k] \\cdot B_{i}[j -k]$.\nStep2 consists of generating the relation tensor R, made up of attention scores that capture relationships between different HD objects $h_{0i}$. These scores are built using a novel hyperdimensional attention mechanism, called HD-Attention. Prior work Vaswani et al. (2017); Altabaa et al. (2023) has generated these relational representations using a pairwise inner product between object features in a low dimensional space. In contrast, the HD-Attention mechanism maps object features to a high-dimensional space where (as shown in Menet et al. (2024)), the HD Object hypervectors are quasi-orthogonal, allowing efficient relational representation and object feature superposition in the high dimensional vector space.\nThe HD-Attention mechanism represents object sequences using the bundling operation (i.e., $\\oplus$) between HD-encoded sequence elements. Given two objects $O_{i}$ and $O_{j}$ we first project them onto a hyperspace using the HD-Encoder $\\Phi_{HD}$. The HD object hypervectors are thus $h_{0i} = \\Phi_{HD}(O_{i})$. Before calculating the attention score, these HD objects are made bipolar using the function $\\delta(x) = -1_{\\{x<0\\}} + 1_{\\{x>0\\}}$, replacing the binary coordinate-wise majority in the bipolar domain used in (Kanerva, 2022). Thus, the (i, j)th element of the relation tensor $R_{ij}$ denoting the object-level relationships between $O_{i}$ and $O_{j}$ can be expressed according to the equation 1 where cos(.) denotes the cosine similarity function and ||\u00b7||2 denotes the L2 norm:\n$R{ij} = cos(\\delta(h{0i}), \\delta(h{0j} \\oplus h{0j})) = \\frac{(\\delta(h{0i}), \\delta(h{0i} \\oplus h{0j}))}{\\|\\delta(h{0i})\\|{2}.\\|\\delta(h{0j} \\oplus h{0j})\\|{2}}$ (1)\nThe denominator of the cosine similarity function cos(.) is $\\|\\delta(h_{0i})\\|{2}.\\|\\delta(h_{0j} \\oplus h_{0j})\\|{2}$. Since the HD objects are bipolar, their L2 norm is $\\sqrt{D}$, leading to the expression in Equation 1. We define bundling ($\\oplus$) as the element-wise real value summation between two bundled HD objects. It captures the dominant or relevant features of an object pair. The sign of each HD object element follows the sign of the element with a higher magnitude, amplified by dominant features of object pair during training. In step , the relation tensor matrix R is normalized using a softmax function to generate $\\overline{R}$. This matrix is used to encode the HD Object hypervectors by mixing them according to their corresponding weights in the normalized relation tensor $\\overline{R}$."}, {"title": "6 RESOLVE: ARCHITECTURE OVERVIEW", "content": "The RESOLVE module implementation is illustrated in Figure 6, for a sequence-to-sequence encoder-decoder structure with RESOLVE Modules (2 to 4). An input sequence, in this case a set of tokens, is encoded into embedding vectors and then passed to the Attentional Encoders in Step 1, which consist of self-attention layers followed by feedforward networks Vaswani et al. (2017). This module is commonly used in sequence-to-sequence modeling and in prior art (Altabaa et al., 2023) to extract object-level information from the sequence.\nIn step 2, the output of the Attentional Encoders, which consists of a set of encoded objects, is mapped to a high-dimensional space using the HD Encoder 2. These HD Object hypervectors are then mixed using the HD-Attention mechanism to generate $h_{Eo}$. In parallel, a set of relational representations (the learnable symbols of Section 4) S are mapped to high-dimensional space through the same HD-encoder 2. The resulting hypervectors, denoted by $h_{s}$, are then combined with the mixed HD Object hypervectors through a binding operation in Step 4. The result is denoted as $h_{sE\\_o}$.\nHigh dimensional vectors are known to be holistic Kanerva (2009) meaning that information is uniformly distributed across them. This gives high information redundancy and makes it possible to map to a low-dimensional space with low information loss Yan et al. (2023). The hypervectors gained from Step 4 ($h_{s\\&E\\_o}$) are thus mapped to low-dimensional space through a learnable linear layer in Step 6. The resulting vectors are then forwarded to a set of Attentional Decoders in Step, which consist of causal-attention and cross-attention layers Vaswani et al. (2017)."}, {"title": "7 EXPERIMENTS", "content": "We have evaluated the performance of RESOLVE compared to the state-of-the-art on several relational tasks: (1) Single output purely relational tasks (pairwise ordering, a sequence of image pattern learning with preprocessed inputs); (2) Single output partially relational tasks (sequence of image pattern learning with low-processed inputs, mathematical abstract rule learning from images); (3) Sequence-to-sequence purely relational tasks (Sorting); (4) Sequence-to-sequence partially relational tasks (Mathematical problem solving). The baselines used for comparison are CorelNet Kerg et al. (2022) with Softmax activation, Predinet Shanahan et al. (2020), the Abstractor Altabaa et al. (2023), the transformer Vaswani et al. (2017), a multi-layer-perceptron (as evaluated in Altabaa et al. (2023)) and the LEN Zheng et al. (2019), a neuro symbolic architecture."}, {"title": "7.1 SINGLE OUTPUT PURELY RELATIONAL TASKS", "content": "As described in Altabaa et al. (2023), we generated 64 random objects represented by iid Gaussian vectors $0_{i} \\sim N(0, I) \\in R^{32}$, and established an anti-symmetric order relation between them $0_{1} \\prec 0_{2} \\prec ... \\prec 0_{64}$. From 4096 possible object pairs (oi, 0j), 15% are used as a validation set and 35% as a test set. We train models on varying proportions of the remaining 50% and evaluate accuracy on the test set, conducting 5 trials for each training set size. The models must generalize based on the transitivity of the relation using a limited number of training examples. The training sample sizes range between 10 and 210 samples. Figure 8a demonstrates the high capability of RESOLVE to generalize with just a few examples, achieving over 80% accuracy with just 210 samples (1.05\u00d7 better than the second best model and 1.09\u00d7 better than Abstractor). The Transformer model is the second best performer, better than the Abstractor and CorelNet-Softmax due to the lower level of abstraction needed for learning asymmetric relations."}, {"title": "7.2 SINGLE OUTPUT PARTIALLY RELATIONAL TASKS", "content": "Instead of extracting highly encoded object level features from the pre-trained CNN used in Section 7.1, we extract the feature map of the first convolutional layer of the pretrained CNN to assess the ability of RESOLVE to handle low processed object level features. Figure 10a shows the mean accuracy of different relational models when trained on small portion of the dataset. RESOLVE outperforms the state of the art with more than 80% accuracy using just 600 training samples. In contrast to the Section 7.1, PrediNet is the second best model thanks to its balanced trade-off between object-level feature processing and abstract feature encoding.\nIn this case (Figure 11), the math rule to extract is a non-linear weighted subtraction (i.e, F(a,b) = |3a \u2013 2b|). This task is partially relational since the input image label is unknown, making object level feature extraction more critical. According to Figure 10b, RESOLVE outperforms the other baselines, with 1.14x better accuracy than the transformer and 1.47\u00d7 better accuracy than the Abstractor. The transformer outperforms the Abstractor here due to to the relative simplicity of the abstract rule, this problem relies more on object level information than abstract information."}, {"title": "7.3 OBJECT-SORTING: PURELY RELATIONAL SEQUENCE-TO-SEQUENCE TASKS", "content": "We generate have generated random objects for the sorting task. First, we create two sets of random attributes: $A = \\{a_{1}, a_{2}, a_{3}, A_{4}\\}$, where and $B = \\{b_{1},...,b_{12}\\}$. Each set of attributes has a strict ordering: $a_{1} \\prec a_{2} \\prec a_{3} \\prec a_{4}$ for A and $b_{1} \\prec b_{2} \\prec ... \\prec b_{12}$ for B. Our random objects are formed by taking the Cartesian product of these two sets, $O = A \\times B$, resulting in N = 48 objects. Each object in O is a vector in $R^{12}$, formed by concatenating one attribute from A with one attribute from B.\nWe then establish a strict ordering relation for O, using the order relation of A as the primary key and the order relation of B as the secondary key. Specifically, $(a_{i}, b_{j}) < (a_{k}, b_{l})$ if $a_{i} \\prec a_{k}$ or if $a_{i} = a_{k}$ and $b_{j} \\prec b_{l}$. We generated a randomly permuted set of 5 and a set of 6 objects in O. The target sequences are the indices representing the sorted order of the object sequences (similar to the 'argsort' function). The training data is uniformly sampled from the set of 6 elements based sequences in O. We generate non-overlapping validation and testing datasets in the following proportion: 20% testing, 10% validation and 70% training.\nWe used element wise accuracy to assess the performance of RESOLVE, as in (Altabaa et al., 2023). The accuracy of RESOLVE is compared against the Relational Abstractor (Altabaa et al., 2023), Transformer Vaswani et al. (2017) and CorelNet(Kerg et al., 2022).\nFigure 12 shows that RESOLVE achieves better accuracy than the baselines (1.56x to 1.02x better than Relational-Abstractor). Relational-Abstractor (Altabaa et al., 2023) still outperforms the transformer and CorelNet-Softmax, validating the results of (Altabaa et al., 2023). RESOLVE demonstrates a high generalizability compared to SOTA. However, as the number of training sample increases, Relational Abstractor and RESOLVE converge toward the same level of accuracy with increased training data."}, {"title": "7.4 MATH PROBLEM-SOLVING: PARTIALLY-RELATIONAL SEQUENCE-TO-SEQUENCE TASKS", "content": "We further evaluate RESOLVE on a mathematical reasoning dataset (Figure 13), which represents a partially relational sequence-to-sequence problem. Table 1 presents the accuracy achieved by the relational abstractor, transformer, and RESOLVE on three different datasets using 100 to 10,000 training samples. The accuracy corresponds to the percentage of full sequence matches, each one representing a correct answer. We report the average accuracy across the three different training sizes in the table, as well as an overall accuracy for all test cases. RESOLVE outperforms the state-of-the-art (SOTA) on average across the three test cases. It also achieves higher accuracy with a small training set, demonstrating the generalizability of the proposed architecture. Notably, neither the relational representation nor the object-level features alone are sufficient for inducing abstract rules from partially relational tasks, which penalizes both the Abstractor and transformer. In contrast, RESOLVE combines both levels of knowledge into a single structure."}, {"title": "7.5 COMPUTATIONAL OVERHEAD ASSESSMENT", "content": "We assessed the computational overhead of the HD-Attention mechanism described in Section 5 against the baseline of a regular self attention mechanism Vaswani et al. (2017). The operations are done on a CPU using Multi-threading. The memory overhead is measured at the level of DRAM and L1 cache memory using the roofline model Ofenbeck et al. (2014). A high \u03b2 value means that the system is less likely to encounter memory bottlenecks. A high \u3160 means the processor is capable of performing more computations per cycle. Table 2 shows that the HD-Attention mechansim has better computational performance compared to self-attention (\u03c0) with higher memory bandwidth \u03b2. This is due to the use of the bipolar HD representation and operations such as bundling and binding."}, {"title": "8 CONCLUSION", "content": "In this work we have presented RESOLVE, a vector-symbolic framework for relational learning that outperforms the state of the art thanks to its use of high-dimensional attention mappings for mixing relational representations and object features. In future we plan to examine multimodal learning tasks and sequence-to-sequence learning tasks in the high-dimensional domain, taking advantage of the computational efficiency of vector-symbolic architectures."}, {"title": "A APPENDIX", "content": "CODE AND REPRODUCIBILITY\nThe code, detailed experimental logs, and instructions for reproducing our experimental results are available at: https://github.com/mmejri3/RESOLVE.\nSINGLE OUTPUT TASKS\nIn this section, we provide comprehensive information on the architectures, hyperparameters, and implementation details of our experiments. All models and experiments were developed using TensorFlow. The code, along with detailed experimental logs and instructions for reproduction, is available in the project's public repository."}, {"title": "A.1 COMPUTATIONAL RESOURCES", "content": "For training the RESOLVE and SOTA models on the single-output relational tasks, we used a GPU (Nvidia RTX A6000 with 48GB of RAM). For training LARS-VSA and SOTA on purely and partially sequence-to-sequence abstract reasoning tasks, we used a single GPU (Nvidia A100 with 80GB of RAM). The overhead assessment of the HD-attention mechanism was conducted on a CPU (11th Gen Intel\u00ae Core\u2122 \u04567)."}, {"title": "A.2 SINGLE OUTPUT PURELY RELATIONAL TASKS", "content": "A.2.1 PAIRWISE ORDER\nEach model in this experiment follows the structure: $input \\rightarrow module \\rightarrow flatten \\rightarrow MLP$, where module represents one of the described modules, and MLP is a multilayer perceptron with one hidden layer of 32 neurons activated by ReLU."}, {"title": "RESOLVE Architecture", "content": "Each model consists of a single module and a hypervector of dimensionality D = 1024. We use a dropout rate of 0.1 to prevent overfitting. The two hypervectors are flattened and passed through hidden layers containing 32 neurons with ReLU activation, followed by a final layer with one neuron activated by a sigmoid function."}, {"title": "Abstractor Architecture", "content": "The Abstractor module utilizes the following hyperparameters: number of layers L = 1, relation dimension $d_{r}$ = 4, symbol dimension $d_{s}$ = 64, projection (key) dimension $d_{k}$ = 16, feedforward hidden dimension $d_{f}$ = 64, and relation activation function $\\sigma_{rel}$ = softmax. No layer normalization or residual connections are applied. Positional symbols, which are learned parameters, are used as the symbol assignment mechanism. The output of the Abstractor module is flattened and passed to the MLP."}, {"title": "CoRelNet Architecture", "content": "CoRelNet has no hyperparameters. Given a sequence of objects, X = $(x_{1},...,x_{m})$, standard CoRelNet (Kerg et al., 2022) computes the inner product and applies the Softmax function. We also add a learnable linear map, $W \\in R^{d\\times d}$. Hence, R = Softmax($\\overline{R}$), where $\\overline{R} = [(W{x_{i}},W{x_{j}})] _{ij}$. The CoRelNet architecture flattens $\\overline{R}$ and passes it to an MLP to produce the output. The asymmetric variant of CoRelNet is given by $\\overline{R}$ = Softmax($\\overline{R}$), where $\\overline{R} = [(W_{1}x_{i}, W_{2}x_{j})] _{ij}$, and $W_{1}, W_{2} \\in R^{d\\times d}$ are learnable matrices."}, {"title": "PrediNet Architecture", "content": "Our implementation of PrediNet (Shanahan et al., 2020) is based on the authors' publicly available code. We used the following hyperparameters: 4 heads, 16 relations, and a key dimension of 4 (see the original paper for the definitions of these hyperparameters). The output of the PrediNet module is flattened and passed to the MLP."}, {"title": "MLP", "content": "The embeddings of the objects are concatenated and passed directly to an MLP, which has two hidden layers, each containing 32 neurons with ReLU activation."}, {"title": "Training/Evaluation", "content": "We use cross-entropy loss and the AdamW optimizer with a learning rate of 10-4. The batch size is 128, and training is conducted for 100 epochs. Evaluation is performed on the test set. The experiments are repeated 5 times, and we report the mean accuracy and standard deviation."}, {"title": "A.2.2 SET", "content": "The card images are RGB images with dimensions of 70 \u00d7 50 \u00d7 3. A CNN embedder processes these images individually, producing embeddings of dimension d = 64 for each card. The CNN is trained to predict four attributes of each card. After training, embeddings are extracted from an intermediate layer, and the CNN parameters are frozen. The common architecture follows the structure: CNN Embedder $\\rightarrow$ Abstractor, CoRelNet, PrediNet, MLP $\\rightarrow$ Flatten $\\rightarrow$ Dense(2). Initial tests with the standard CoRelNet showed no learning. However, removing the Softmax activation improved performance slightly. Hyperparameter details are provided below."}, {"title": "Common Embedder Architecture:", "content": "The architecture follows this structure: Conv2D $\\rightarrow$ MaxPool2D $\\rightarrow$ Conv2D $\\rightarrow$ MaxPool2D $\\rightarrow$ Flatten $\\rightarrow$ Dense (64, ReLU) $\\rightarrow$ Dense (64, ReLU) $\\rightarrow$ Dense (2). The embedding is taken from the penultimate layer. The CNN is trained to perfectly predict the four attributes of each card, achieving near-zero loss."}, {"title": "RESOLVE Architecture:", "content": "The RESOLVE module has the following hyperparameters: hypervector dimension D = 1024. The outputs are flattened and passed through a feedforward hidden layer with dimension $d_{f}$ = 64, followed by a final layer with a single neuron and sigmoid activation. A dropout rate of 0.4 is used to prevent overfitting."}, {"title": "Abstractor Architecture:", "content": "The Abstractor module uses the following hyperparameters: number of layers L = 1, relation dimension $d_{r}$ = 4, symmetric relations ($W_{ij}$ = $W_{ji}$ for i $\\in$ [$d_{r}$"}]}