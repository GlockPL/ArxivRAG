{"title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging", "authors": ["Jinluan Yang", "Dingnan Jin", "Anke Tang", "Li Shen", "Didi Zhu", "Zhengyu Chen", "Daixin Wang", "Qing Cui", "Zhiqiang Zhang", "Jun Zhou", "Fei Wu", "Kun Kuang"], "abstract": "Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI, with existing methods like data mixture strategies facing limitations including reliance on expert knowledge and conflicting optimization signals. While model merging offers a promising alternative by integrating specialized models, its potential for 3H optimization remains underexplored. This paper establishes the first comprehensive benchmark for model merging in 3H-aligned LLMs, systematically evaluating 15 methods (12 training-free merging and 3 data mixture techniques) across 10 datasets associated with 5 annotation dimensions, 2 LLM families, and 2 training paradigms. Our analysis reveals three pivotal insights: (i) previously overlooked collaborative/conflicting relationships among 3H dimensions, (ii) the consistent superiority of model merging over data mixture approaches in balancing alignment trade-offs, and (iii) the critical role of parameter-level conflict resolution through redundant component pruning and outlier mitigation. Building on these findings, we propose R-TSVM, a Reweighting-enhanced Task Singular Vector Merging method that incorporates outlier-aware parameter weighting and sparsity-adaptive rank selection strategies adapted to the heavy-tailed parameter distribution and sparsity for LLMs, further improving LLM alignment across multiple evaluations. Our models will be available at 3H_Merging.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have achieved excellent performance in various natural language processing tasks. However, their reliable deployment necessitates a balanced optimization across three critical dimensions: Helpfulness (providing accurate and task-aligned responses), Honesty (avoiding hallucinations and misinformation), and Harmlessness (preventing toxic or unethical outputs), collectively termed as 3H optimization (Bai et al., 2022a; Guo et al., 2024; Sonkar et al., 2024; Yang et al., 2024b). While recent alignment techniques such as constitutional AI (Bai et al., 2022c), reinforcement learning from human feedback (RLHF) (Dai et al., 2023), and Direct Preference Optimization (DPO) (Rafailov et al., 2024) have improved individual aspects of 3H, seeking a balance remains an open challenge. For instance, models optimized for helpfulness may inadvertently generate harmful content (Ji et al., 2024), and there exits dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses (Huang et al., 2024). This tension underscores the need for systematic approaches to harmonize 3H objectives.\nTraditional methods for enhancing 3H properties often rely on data mixture strategies assisted by empirically heuristic rules (Lambert et al., 2024), multi-dimension scores by reward model (Wang et al., 2024b), alignment evaluation metric (Jiang et al., 2024), where diverse datasets are combined to fine-tune a single model. While effective, these approaches face practical limitations: (i) data curation requires substantial domain expertise and computational resources (Ji et al., 2024), and (ii) conflicting optimization signals during fine-tuning may lead to difficulties in prioritizing wanted alignment objectives without weakening others (Jiang et al., 2024). As a cost-effective alternative through integrating different specialized aligned models' abilities, model merging has achieved great attention for LLM alignment, addressing key challenges such as avoiding forgetting after fine-tuning (Yang et al., 2024a; Zhu et al., 2024a). But for 3H optimization, the effectiveness of existing merging methods remains underexplored. Simultaneously, systematic comparative analysis between data mixture and model merging methods is merely investigated. While preliminary investigations have emerged (Ahmadian et al., 2024), these efforts remain narrowly focused on constrained scenarios (e.g., multilingual) or employ partial evaluations of 3H dimensions (Tekin et al., 2024). Thus, it remains elusive whether there are"}, {"title": "2. Related Work", "content": "Model Merging for LLM Alignment. Model merging has emerged as a pivotal technique for LLM alignment (Yang et al., 2024a), addressing challenges across four aspects: (a) Stabilizing reference policies focuses on the over-optimization problem in direct preference optimization. Weight-space averaging of models with varying initializations constructs robust policy ensembles (Chegini et al., 2024), while dynamic trust-region updates (Gorbatovski et al., 2024) and online gradient fusion (Lu et al., 2024) help preserve foundational capabilities. (b) Cross-model capability transfer resolves architectural mismatches during knowledge fusion (Wan et al., 2024) through probabilistic token alignment (Yang et al., 2024c), vertical domain adaptation (Lin et al., 2024a), and subspace projection (Thakkar et al., 2024). Persistent toxic parameter propagation (Hammoud et al., 2024) remains a critical barrier, inducing biased representation transfer during integration. (c) Avoiding forgetting after finetuning develops gradient-aware selective merging (Ju et al., 2024), heterogeneous layer-wise merging (Lin et al., 2023; 2024b), and subspace-based merging (Yi et al., 2024) methods to mitigate the alignment tax or realign the model after fine-tuning for downstream tasks. (d) Balancing multi-optimized objectives employs linear interpolation of reward-tuned models (Jang et al., 2023; Rame et al., 2024; Ram\u00e9 et al., 2024b;a) and MoE-based expert routing (Tekin et al., 2024) to approximate Pareto frontiers but lacks theoretical guarantees for subspace conflict analysis. Location-based merging (Zhao et al., 2024a) identifies specific weights for alignment, but its effect is highly dependent on the data used for parameter identification. Moreover, (Ahmadian et al., 2024) also compares the data mixture and model merging methods, yet critically limits their analysis to cross-lingual transfer scenarios without dealing with 3H optimization.\nData Mixing in Helpfulness, Honesty, and Harmlessness. Data mixing methods align the LLMs towards 3H dimension from three aspects: (a) Heuristic Methods: Sparrow (Glaese et al., 2022) and Constitutional AI (Bai et al., 2022b) initially adopt the rules for alignment feedback from 3H dimension, reducing dependency on extensive human labeling. Recently, (Bianchi et al., 2023; Amballa et al., 2024) explore the heuristic mixture of instructions between helpful and safety-related data to balance multi-objects. (b) Reward Model Methods: Beyond traditional Bradley-Terry models (Bradley & Terry, 1952; Ouyang et al., 2022), many efforts have been devoted to exploring multi-objective reward models (RMs) to score the data for capturing the complicated human preferences (Touvron et al., 2023; Wang et al., 2023; 2024a). ArmoRM is a recent development aiming to promote LLMs aligned with human-interpretable multi-objective demands like honesty and helpfulness (Wang et al., 2024b). (c) Metric Evaluation Methods: Early metrics"}, {"title": "3. Reviewing Model Merging for Multi-Object Alignment Optimization", "content": "Model merging has emerged as an effective paradigm for cross-model knowledge integration without performance degradation (Yang et al., 2024a). The challenge of multi-objective alignment has been extensively studied in machine learning optimization, particularly in areas like multi-task learning (Sener & Koltun, 2018; Liu et al., 2022; Tang et al., 2024a). However, the intersection of model merging and alignment optimization presents unique challenges and opportunities that warrant dedicated investigation.\nExisting merging methods for LLMs (Goddard et al., 2024) include: Linear interpolation methods such as Rewarded Soups have demonstrated that simple weighted averaging of model parameters can be effective in learning the Pareto frontier of multiple objectives (Rame et al., 2024). Given multiple models parameterized by $\\theta_1, \\theta_2,\\dots,\\theta_n$, where each optimizes to a different objective, and a preference vector w = $(w_1, w_2,\\cdots, w_n)$, the merged model using Rewarded Soups is defined as: $\\theta_{\\text{rewarded Soups}} = \\Sigma_{i=1}^{n} \\omega_i\\theta_i$. Building on simple weight interpolation methods such as Task Arithmetic (Ilharco et al., 2022), advanced merging approaches like TIES (Yadav et al., 2024), DARE (Yu et al., 2024a) and Breadcrumbs (Davari & Belilovsky, 2025) explore more nuanced ways to combine model parameters, often focusing on identifying and preserving crucial subspaces that capture different objectives and resolve the objective conflicts. In general, these methods can be expressed as $\\theta_{\\text{Merged}} = \\theta_0 + \\Sigma_{i=1}^{n} w_im_i \\odot (\\theta_i - \\theta_0)$, where $m_i \\in R^{|\\theta|}$ is a binary mask and $\\odot$ is the element-wise multiplication. Model stock (Jang et al., 2025) identifies that model performance correlates strongly with proximity to the center of the weight space. Rather than averaging multiple models, Model Stock approximates this optimal center point geometrically. Task Singular Vector Merge (TSVM) (Gargiulo et al., 2024) represents an advanced approach to model merging that addresses the limitations of simpler methods like Task Arithmetic. While Task Arithmetic treats networks as flat parameter vectors, TSVM operates at a layer-wise level by analyzing the singular value decomposition (SVD) of task matrices. The key innovation of TSVM lies in its treatment of task interference through Singular Task Interference (STI({A}-1) = ||(UTU \u2013 I)\u03a3(VTV \u2013 I)|| where U, \u03a3, and V are the left singular vectors, singular values, and right singular vectors of the task matrices {Ai}i=1), which measures how task-specific features overlap in weight"}, {"title": "4. LLM Merging Benchmark for 3H Optimization", "content": "4.1. Benchmark Setup\nLLM DPO Training Datasets, Schemes, and Model Constructions. To achieve the goal of 3H optimization, we select commonly used preference data shown in Table 1, which can be categorized into five groups from the annotation perspective, to perform DPO training. Following SimPO (Meng et al., 2024), we adopt two well-known off-the-shelf instruction-tuned models, Llama-3-8B-Instruct (Dubey et al., 2024) and Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), as the SFT model and then fine-tune the entire network of LLMs utilizing the above preference data to construct five enhanced aligned models, which correspond to different annotation dimensions for further model merging.\nSetup and Implementation Details. We conduct extensive experiments to compare model merging and data mixture methods for 3H optimization. These include Data Mixture Methods where we first process the full training data of Table 1 and then utilize these processed data for DPO training. As stated in Table 2 and Appendix C, we adopt (i) Heuristic-adjusted dataset mixing ratio (Heuristic) where we control the ratio between Honesty&Harmlessness and Helpfulness,"}, {"title": "4.2. Experimental Results", "content": "Trade-off between Helpfulness, Honesty, and Harmlessness for LLM Alignment. In Table 3 and Table 4, we investigate the trade-off between 3H-related abilities of Llama3 (Dubey et al., 2024) and Mistral (Jiang et al., 2023). Several key results are summarized:"}, {"title": "5. Extened Study to Improve Task Singular Vector Merging for 3H Optimization", "content": "Beyond the benchmarking effort in Sec. 4, we also explore algorithmic advancements to improve model merging for 3H optimization. Specifically, we first select the most effective and stable merging algorithm, TSVM, to reform the 3H optimization problem and then discuss its drawbacks integrated with the ignored property of LLM (sparsity and heavy-tail) while merging LLM. Then, we leverage outlier weighting and sparsity-adaptive rank selection strategies to improve the effect of TSVM for 3H optimization. Finally, we provide a discussion for future optimized directions."}, {"title": "5.1. Limitation for Task Singular Vector Merging", "content": "As stated above, TSVM can achieve stable and good results for 3H Optimization. We reform the 3H optimization problem based on its implementation. Given n alignment processes that produce model variants {$\\theta_i^l$}$_{i=1}$ from initial"}, {"title": "5.2. Our Reweight Task Singular Vector Merging", "content": "Outlier-Aware Weighting for Limitation(i), we utilize layer-wise outlier detection to aggregate the Outlier-Aware Weight for subsequently weighting the singular values. This means we should check which parts of the singular values can truly represent the optimization towards alignment process. Considering the heavy-tailed distribution of LLM parameter updates where few parameters undergo significant changes while most exhibit minor adjustments, we can refer to the 3\u03c3 principle compatible with this property. Thus, we adopt the statistical significance filtering and competitive weight normalization to identify significant true optimization adjustments as follows:\n$\\mu^{(i)}_r = E_c[|\\Delta_{l,r,c}|]$\n$\\sigma^{(i)}_r = \\sqrt{E_c[|\\Delta_{l,r,c}|^2] - (\\mu^{(i)}_r)^2}$\n$\\alpha^{(i)}_l = \\frac{\\Sigma_{r=1}^{d_l} \\Sigma_{c=1}^{d_l} ||\\text{THRESHOLD}(\\Delta_{l,r,c}^{(i)}, \\mu^{(i)}_r + 3\\sigma^{(i)}_r)||_1}{\\Sigma_{j=1}^{n}\\Sigma_{r=1}^{d_l} \\Sigma_{c=1}^{d_l} ||\\text{THRESHOLD}(\\Delta_{l,r,c}^{(j)}, \\mu^{(j)}_r + 3\\sigma^{(j)}_r)||_1}$\nwhere $\\Delta_{l,r,c}^{(i)} \\in R$ denotes the weight deviation at row r, column c of layer l for model i relative to initial model, $\\mu_r^{(i)}$ and $\\sigma_r^{(i)}$ represent the mean and standard deviation of deviations in row r, quantifying central tendency and dispersion, $\\alpha_l^{(i)} \\in [0,1]$ computes layer-wise aggregation weights via L\u2081-normalized sparse outlier magnitudes, and $\\text{THRESHOLD}(M, \\tau)$ applies hard-thresholding to suppress elements in matrix M with absolute values below \u03c4.\nCompared with TSVM's direct utilization of full singular values, our outlier-aware weighting provides two key advantages: Noise Suppression: By thresholding parameter deviations via the 3\u03c3 rule, we filter out low-magnitude fluctuations that predominantly encode noise, forcing the singular vectors $u_r^{(i)}$ to align with statistically significant task features. Task Equilibrium: The layer-wise aggregation weights $\\alpha_l^{(i)}$ are globally normalized across all models, ensuring balanced contributions from diverse tasks and preventing dominance by high-magnitude updates that may obscure subtle yet critical features.\nSparsity-Adaptive Rank Selection for Limitation (ii), we aim to adaptively decide the level of rank truncation based on the layer sparsity. We can first compute the sparsity consensus for all models first and then achieve the dynamic rank as Eq. 10, where \u03b3 is the sparsity-rank coupling factor controlling the strength of rank reduction, $k_l$ is defined as the dynamic rank for layer l, adaptively determined by sparsity \u03a9l. The \u03f5 is set to 0.1 by default.\n$\\Omega_l = \\frac{\\Sigma_{i=1}^{n}\\Sigma_{r,c=1}^{d_l} I(|\\Delta_{l,r,c}^{(i)}| \\textless \\epsilon)}{\\text{nd}_l}$\n$k_l = [d_l(1 - \\gamma \\Omega_l)]$\nCompared with TSVM's fixed-rank truncation, our sparsity-adaptive rank selection offers two enhancements: Information Preservation: The dynamic rank $k_l = [d_l(1 - \\gamma \\Omega_l)]$ (Eq. 10) adapts to layer-specific sparsity \u03a9l\u2500retaining more singular directions in sparse layers (where updates concentrate on critical subspaces) while aggressively truncating redundant components in dense layers, thereby balancing in-"}, {"title": "5.3. Comparsion Results and Discussions", "content": "Reweighting benefits TSVM for 3H optimization in LLM alignment. Our experiments demonstrate that the reweighting mechanisms\u2014outlier weighting and sparsity-adaptive rank selection collectively enhance TSVM's capability for 3H optimization. As shown in Table 6 and 7, we report the average score of helpfulness, honesty, and harmless under static optimization settings. The full results on various eval-"}, {"title": "6. Conclusion", "content": "This paper creates the first benchmark to explore the effect of the model merging for a balanced optimization across helpfulness, harmlessness, and honesty (3H) dimensions to"}, {"title": "Impact Statement", "content": "This paper explores model merging for 3H optimization of large language models (LLMs) during the alignment stage. Its potential impacts are contingent on how these merging methods are utilized. On the positive side, assisted by model merging techniques, achieving a better-aligned model without retraining or adjusting the data mixture ratio many times could lead to significant reductions in energy consumption, contributing to the development of green AI and achieving improved performance in resource-constrained environments. However, there is a potential negative aspect in terms of private protection, as the competitors may steal your model parameters through model merging without prior notice. However, given the technical focus of this work, there are no specific societal consequences directly stemming from it that need to be highlighted here."}, {"title": "A. More Details for Method", "content": "A.1. More Details for Outlier-aware Weighting\nInterpretation of Dual Objectives for outlier weighting The mathematical framework achieves cross-model consensus and intra-model saliency through its hierarchical thresholding mechanism:\n(i) Cross-Model Consensus: The denominator in Eq. (3) normalizes each model's contribution by the total sparse outlier magnitude across all n models:\n$\\Sigma_{j=1}^{n} \\Sigma_{c=1}^{d_l} ||\\text{THRESHOLD}(\\Delta_{l,r,c}^{(j)}, \\mu^{(j)}_r + 3\\sigma^{(j)}_r)||_1$\nThis forces models with greater sparse deviation magnitudes (potential task conflicts) to receive proportionally reduced aggregation weights \u03b1l(i), effectively suppressing outlier-dominated models in the merged output.\n(ii) Intra-Model Saliency: The 3\u03c3 threshold in THRESHOLD(\u0394lr(2), \u00b5(2)r + 3\u03c3(2)r) implements statistical outlier detection within each model's parameter distribution. For Gaussian-distributed \u0394lr(2)(per Central Limit Theorem), this retains only the top 0.3% extreme deviations that likely correspond to:"}, {"title": "A.2. More Details for the Reasonability of R-TSVM", "content": "Building on TSVM's theoretical framework, our method provides enhanced guarantees through statistical awareness and adaptive computation.\nConflict Probability Bound Let P(1) conflict denote the probability of directional conflicts in layer l. Our rank adaptation yields as follows. We can observe that, compared to TSVM's fixed k(1) , our bound adapts to layer sparsity.\nE[P(1) conflict ] < \u221a\u03b1\nWeight Concentration The 3\u03c3 thresholding induces weight concentration on critical parameters. For any layer l:\nV[\u03c9(l) i ] \n < \nE[\u03c9(l) i ]2 \nThis variance-to-mean ratio decreases as outliers become sparser, stabilizing training."}, {"title": "A.3. Order of Orthogonalization and Rank Truncation/Selection", "content": "A critical design choice in our R-TSVM algorithm lies in the sequential relationship between orthogonalization (Eq. 3-4) and rank truncation (Eq. 10). Through theoretical analysis and empirical validation, we establish that orthogonalization should precede truncation to ensure optimal subspace alignment and information preservation. This ordering stems from three fundamental considerations: Global Orthogonality Constraints: The orthogonal projection in Eq. 3 minimizes the Frobenius norm difference \u2016U(1) \u22a5 \u2212 U(1) \u2016F under strict orthogonality constraints. Performing this projection before truncation preserves the complete singular vector structure, enabling accurate modeling of cross-task interference patterns. Early truncation would discard directional components essential for constructing the orthogonal basis, particularly when task-specific updates exhibit heterogeneous rank distributions.\nDynamic Rank Adaptation: Our sparsity-adaptive rank selection (Eq. 10) requires layer-wise sparsity measurement \u2126l, computed from the full parameter deviation matrix \u0394(1) . Truncating \u0394(1) prematurely would bias \u2126l by excluding contributions from low-magnitude parameters, thereby undermining the adaptive rank calculation. As shown in Algorithm 1, orthogonalization (Step 4) utilizes the full-rank SVD decomposition to maintain statistical fidelity.\nOutlier Weighting Integrity: The outlier-aware weighting mechanism (Eq. 6) operates on the complete parameter deviation matrix to identify statistically significant updates. Truncation prior to outlier detection would risk eliminating subtle yet critical features masked within lower-rank components, particularly in layers with heavy-tailed parameter distributions."}, {"title": "B. More Details for Related Work", "content": "B.1. Discussion with the Alignment Tax.\nWe would like to further clarify the main difference between 3H trade-off and previously defined alignment tax (Lin et al., 2024b; Lu et al., 2024). In general, the alignment tax describes the phenomenon of RLHF training leading to the forgetting of pre-trained abilities during the first alignment stage. However, as shown in Figure 2, we mainly focus on how can we further enhance the 3H-related abilities of the existing already-aligned model during the second or subsequent stages. The"}]}