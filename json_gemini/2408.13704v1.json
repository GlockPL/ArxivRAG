{"title": "DHP Benchmark: Are LLMs Good NLG Evaluators?", "authors": ["Yicheng Wang", "Jiayi Yuan", "Yu-Neng Chuang", "Zhuoer Wang", "Yingchi Liu", "Mark Cusick", "Param Kulkarni", "Zhengping Ji", "Yasser Ibrahim", "Xia Hu"], "abstract": "Large Language Models (LLMs) are increasingly serving as evaluators in Natural Language Generation (NLG) tasks. However, the capabilities of LLMs in scoring NLG quality remain inadequately explored. Current studies depend on human assessments and simple metrics that fail to capture the discernment of LLMs across diverse NLG tasks. To address this gap, we propose the Discernment of Hierarchical Perturbation (DHP) benchmarking framework, which provides quantitative discernment scores for LLMs utilizing hierarchically perturbed text data and statistical tests to measure the NLG evaluation capabilities of LLMs systematically. We have re-established six evaluation datasets for this benchmark, covering four NLG tasks: Summarization, Story Completion, Question Answering, and Translation. Our comprehensive benchmarking of five major LLM series provides critical insight into their strengths and limitations as NLG evaluators.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) play a crucial role in the field of Natural Language Generation (NLG), advanced wide real-world applications including education (Latif et al., 2023), healthcare (Yuan et al., 2023), business (Teubner et al., 2023), etc. The strong capabilities of LLMs allow them not only to serve as text generators but also increasingly as powerful evaluators of text quality (Chiang and Lee, 2023; Liu et al., 2023a; Li et al., 2024). Their role as evaluators is crucial for advancements in various applications, such as summarization, story completion, question answering, and translation (Li et al., 2024; Wang et al., 2023a). LLMs are expected to serve as NLG evaluators, providing reasonable quality scores based on different quality metrics with specially designed evaluation prompts.\nDespite the growing performance of LLMs in evaluation tasks, a significant gap remains in fully comprehending their capabilities in evaluating NLG quality. The question, Are LLMs good NLG evaluators? remains challenging for two main reasons illustrated in Figure 1:\n(1) Lack of Clear and Unbiased Measurement: There is no clear measurement for the capability of LLM evaluators. Existing methods rely on aligning with human scores (Chiang and Lee, 2023; Liu et al., 2023a) but these scores themselves are subject to biased response styles (Schoch et al., 2020).\n(2) Multiple Evaluation Metrics: Evaluating NLG quality requires considering multiple metrics. For example, in summarization tasks, metrics such as coherence, consistency, and fluency are essential considerations (Hu et al., 2024; Fabbri et al., 2021). However, LLMs might struggle with correlations between these metrics, potentially leading to misinterpretation and incorrectly scoring, which makes it difficult to assess their effectiveness as evaluators.\nTo address these challenges, we introduce a novel DHP benchmarking framework \u2013 Discernment of Hierarchical Perturbation - for quantitatively measuring the evaluation capabilities of LLMs without the need for human annotations. We propose the concept of discernment scores, systematically derived from hierarchically perturbed text data and statistical tests. We perturb the reference data using various hierarchical methods, then compare the differences in LLM evaluation scores using the Wilcoxon Signed-Rank Test (Wilcoxon, 1945). In order to get overall capability results, we introduce the approach of harmonic mean p-values and expert weights to combine the results of multiple metrics. The final p-value derived can be transformed to Discernment Scores to measure the NLG evaluation capabilities of LLMs. This method allows for a more rigorous and comprehensive evaluation of LLM performance, independent of the response styles of the tested models.\nOur study re-establishes six evaluation datasets that encompass four key NLG tasks: Summarization, Story Completion, Question Answering, and Translation. Each dataset is perturbed and leveraged to challenge the LLMs' evaluative capabilities in distinct ways, providing a robust basis for benchmarking. The datasets include a range of text perturbations, from minor character problems to significant sentence alterations, allowing us to test the potential discernment limits of the LLMs.\nOur comprehensive benchmarking, using newly defined quantitative discernment scores, analyzes five major LLM series. This approach uncovers critical insights into their capabilities as NLG evaluators and provides a detailed understanding of their performance. This benchmark reveals important trends and patterns in the capabilities of LLMs, highlighting areas where they excel and where they may fall short.\nThe DHP benchmark aims to fill the existing gaps by providing a quantitative framework for assessing LLMs' evaluation capabilities and emphasizing the necessity of considering multiple metrics for accurate and reliable evaluations. We summarize our contributions as follows.\n(1) Design the DHP benchmarking framework: quantitative discernment scores for LLMs as NLG evaluators based on hierarchical perturbation, eliminating the need for human annotations.\n(2) Re-establish six evaluation datasets for four NLG tasks to evaluate the discernment of LLMs.\n(3) Benchmark five series of LLMs to analyze their capabilities of NLG evaluation."}, {"title": "2 Related Work", "content": "Recent advancements highlight the significant potential of utilizing LLMs as evaluators for a variety of natural language processing (NLP) tasks. Extensive empirical evidence supports this viewpoint, as demonstrated by studies (Liu et al., 2023a; Chiang and Lee, 2023; Hu et al., 2024; Desmond et al., 2024; Wang et al., 2023a), which assert that the evaluation behaviors of pretrained LLM-based evaluators are well-aligned with those of human preference (Liu et al., 2023b) and eliminate the prompt bias (Liusie et al., 2024). Another line of work (Huang et al., 2024; Zhu et al., 2023; Wang et al., 2023b) finetunes LLMs with specific downstream task datasets to increase the capability as the judges. Despite the great assessment performance of a single LLM, advanced studies involve multi LLM agents (Chan et al., 2023; Zhang et al., 2023; Li et al., 2023) or human experts (Gao et al., 2024; Li et al., 2024) to further increase the judging capability. However, humans often prefer to remain neutral (Leroi et al., 2020), which means a perfect alignment between LLM-based evaluators and human evaluators may still lead to bias and inaccurate judgments.\nWhile using LLM-based evaluators offers a scalable and effective method for approximating human preferences, the significant influence of individual human biases raises concerns about their discernment in assessing LLM-based evaluations. In our work, we focus on developing comprehensive approaches to observe and address this phenomenon."}, {"title": "3 Biased Response Styles", "content": "Previous studies focus on the alignment between human and LLM evaluators, using correlation metrics to gauge the LLMs' performance in NLG evaluation tasks (Liu et al., 2023a; Chiang and Lee, 2023). However, these studies often overlook an important variable of evaluators: Response Styles which refer to a respondent's consistent manner of answering survey questions, regardless of the content (Van Vaerenbergh and Thomas, 2013). Despite similar levels of professionalism, annotators may assign different scores to the same questionnaire due to differences in age, gender, personality, cultural background, and ethnic group (Van Vaerenbergh and Thomas, 2013; Hui and Triandis, 1989; Kieruj and Moors, 2013). Similarly, LLMs, trained on diverse datasets, may also exhibit biases in their responses (Salecha et al., 2024). This discrepancy casts doubt on the previous methods used to compare human and LLM scores. Since quality-based scoring often relies heavily on a few experts' annotations, the final alignment scores tend to favor models that share similar response styles with these specific experts.\nWe illustrate this with an example of the response styles of five LLMs tasked with annotating quality scores for human reference data from the SummEval dataset (Fabbri et al., 2021). We averaged the scores across four metrics for each data point and plotted both the Pearson correlation coefficient (\u03c1) and the average score distributions of the five models. After perturbing the original data by replacing some named entities with fictional ones in the summaries (Fictional Named Entities in Table 1), we repeated the quality evaluation. As shown in Figure 2, all models detected the changes and adjusted their scores accordingly, though their scoring distributions varied significantly. For instance, Llama3 (Meta), Mistral (Jiang et al., 2023), and Qwen (Bai et al., 2023) models assign higher scores to the original data and moderate scores to the perturbed data. In contrast, GPT4-Turbo (OpenAI, 2023) and Vicuna (Chiang et al., 2023) models tend to give moderate scores to the original data and much lower scores to the perturbed data. The variance in the response distributions indicates the presence of bias that can significantly affect alignment (\u03c1), illustrating that alignment is not a direct or credible metric for assessing the ability of LLMs as NLG evaluators. It is crucial to develop a new metric and measurement for evaluation that is not influenced by the evaluators' biased response styles, ensuring a more accurate and fair assessment of LLM capabilities."}, {"title": "4 DHP Benchmarking Framework", "content": "We propose our DHP framework: Discernment of Hierarchical Perturbation. Previous studies overlook the essence of NLG evaluation, i.e., the content-oriented scoring (Novikova et al., 2018). In other words, content that is accurate, fluent, and consistent should receive higher scores than content that is inaccurate, disfluent, and inconsistent. Qualified annotators should be able to recognize inappropriate content without additional references and then assign scores, even though the absolute scores may still reflect their biased response styles. The fundamental principle of our assessment is that a qualified LLM evaluator should be able to independently identify issues in perturbed data (which contains some quality issues) and assign relatively lower scores compared to the original reference data during two separate evaluations. This approach does not rely on human scores, thus eliminating the influence of human response styles.\nThe overall framework is shown in Figure 3. First, for a specific NLG task, we employ a hierarchical perturbation pipeline to transform high-quality reference data into various forms of lower-quality data. Subsequently, an LLM evaluates both the original and perturbed texts respectively using predefined metrics, generating several sets of rating scores. We then conduct a statistical analysis of these scores. For each pair of scores, original and perturbed, we apply the Wilcoxon Signed-Rank Test to determine the differences in their distributions, achieving this with a confidence level expressed as a p-value. This test specifically assesses differences in pairwise scores without focusing on absolute values, thereby minimizing the impact of models' response styles. Following this, we combine the p-values from different metrics, incorporating Expert Weights (EW) to tailor the aggregated p-values to the specific metrics of the corresponding perturbation methods. These combined p-values are then transformed into discernment scores, which serve as a direct measure for assessing and comparing the NLG evaluation capabilities of LLMs for this particular task."}, {"title": "4.1 Step 1: Hierarchical Perturbation", "content": "To generate data that have quality issues across various levels, formats, and evaluation difficulties, we propose a hierarchical perturbation approach. In contrast to the plain perturbations (Sai et al., 2021), our approach encompasses three levels of perturbation content: character, word, and sentence levels; two methods of perturbation: rule-based and LLM-based; and two degrees of perturbation: minor and major as illustrated in Figure 3.\nFirst, at the character level, we alter some characters or letters in the given N original texts independently. At the word and sentence levels, we degrade the text by processing entire words or sentences, respectively. For NLG tasks involving very short texts, sentence-level perturbation is considered optional. For each level of perturbation, we choose either a rule-based or an LLM-based method, enhancing the diversity of the perturbation's content and format. Additionally, if the text data is sufficiently long for more perturbation, we implement two degrees of perturbation \u2013 minor and major - for each method. These different degrees of perturbation will influence the difficulty that LLMs face in detecting issues within the text. The detailed perturbation methods for each task are shown in Table 1.\nWith this approach, we generate multiple sets of perturbed data, with each set designed to highlight a specific quality issue tied to a distinct type of perturbation method. Competent LLM evaluators should accurately detect these issues and assign correspondingly lower scores to the perturbed data."}, {"title": "4.2 Step 2: LLM evaluation", "content": "Following the evaluation method outlined in G-Eval (Liu et al., 2023a), we also utilize the automatic chain-of-thought approach (Auto-CoT) (Zhang et al., 2022) to design evaluation prompts for different datasets and evaluation metrics. These prompts are sent to LLMs to assess both the original data and the perturbed, low-quality data. It's important to note that all perturbed data are evaluated independently, without their original references, to accurately test the models' capabilities in identifying specific quality issues.\nAfter conducting the LLM evaluation on a dataset consisting of N datapoints, we obtain several sets of absolute evaluation scores shown in Figure 3:\n[{Sm\u2081}, {Sm2},..., {S0M}],\n[{Sm\u2081}, {Sm2},..., {S1M}],\u2026\u2026,\n[{Sm\u2081}, {Sm2},..., {SPM}],\nwhere each {S} is a set of N evaluation scores. The superscripts 0,1,..., P on S represent the original data (0) and the P types of perturbed data (1,..., P), respectively. The subscripts m1,...,m\u043c represent the M different metrics used in the dataset. For instance, in the SummEval dataset (Fabbri et al., 2021), there are four evaluation metrics: coherence, consistency, fluency, and relevance."}, {"title": "4.3 Step 3: Statistical Analysis", "content": "As illustrated in Figure 3, we conduct a chain of statistical analyses to derive the final discernment scores for LLM evaluators. This process includes the Wilcoxon Signed-Rank Test, Harmonic Mean p-value and Expert Weights, and the final calculation of discernment scores."}, {"title": "4.3.1 Wilcoxon Signed-Rank Test", "content": "The Wilcoxon Signed-Rank Test (W-Test) (Wilcoxon, 1945) is a non-parametric hypothesis test that compares two dependent samples to assess whether their population mean ranks differ significantly. We apply the W-Test to evaluate whether there is a significant difference in the score distributions between the original data and a given type of perturbed data:\n$Pm_j \\sim z_{m_j} = W-Test(\\left\\{S^0_{m_j}\\right\\}, \\left\\{S^i_{m_j}\\right\\}).$\nIn our analysis, we adopt a one-sided alternative hypothesis. The resulting p-value indicates the confidence level at which we can reject the null hypothesis \u2013 that {Smj} and {Simj} have the same distribution \u2013 and accept the alternative hypothesis \u2013 that {Smj} has a greater distribution than {Simj}. We consider a difference to be statistically significant if pmj < 0.05. A lower p-value represents a more significant score difference between the original data and perturbed data. Totally we can get P sets of p-values for the M metrics as shown in Figure 3:\n$[p_{m_1}^1, p_{m_2}^1..., p_{mM}^1], [p_{m_1}^2, p_{m_2}^2..., p_{mM}^2]$\nBecause the W-Test does not assume any specific distribution for the scores and does not focus on their absolute values, the resulting p-values solely reflect whether the LLMs are able to detect the quality issues and assign lower scores to the perturbed data compared to the original data. Consequently, this testing approach inherently avoids the influence of response styles, instead focusing on the relative quality assessment. Meanwhile, the p-values provide a quantitative evaluation measure to the score difference, i.e., the capability of evaluators to discern low-quality data."}, {"title": "4.3.2 Harmonic Mean p-value and Expert Weights", "content": "Given that an evaluation task may involve multiple M evaluation metrics, resulting in multiple p-values $[p_{m_1}^i, p_{m_2}^i,..., p_{mM}^i]$ for a single perturbed set, it is crucial to derive a combined p-value to measure the overall confidence level. We employ the Harmonic Mean p-value (HMP) method (Wilson, 2019) without or with the Expert Weights (EW) presented in Figure 3:\n$p^i = \\frac{1}{\\frac{1}{M} \\sum_{j=1}^{M} \\frac{1}{p_{mj}}}, p^{i, EW} = \\frac{1}{\\frac{1}{M_{EW}} \\sum_{j=1}^{M} \\frac{EW_j}{p_{mj}}}.$"}, {"title": "4.3.3 Discernment Scores of LLM Evaluators", "content": "To facilitate comparisons, we transform these combined p-values into positive scores, which we define as discernment scores for a specific perturbation i in Figure 3:\n$D^i = log_{0.05}(p^i), D^{i, EW} = log_{0.05}(p^{i,EW}).$\nHere, Di and Di,EW are positive values and the higher the better. A value of 1 for Di and Di,EW is a threshold corresponding to a p-value of 0.05, indicating statistical significance. If Di or Di,EW is less than 1, it means that the LLM evaluators do not assign significantly lower scores to the perturbed data compared to the original data, suggesting a lack of discernment for specific quality issues during the NLG evaluation.\nTo observe the comprehensive capability and worst-case performance of the LLMs, we calculate both the average and minimum of Di and Di,EW across all perturbation methods i = 1, . . ., P. This results in overall LLM discernment scores Davg, Dmin, DEWavg, and DEWmin. Note that the average discernment scores are calculated using a weighted average across the perturbation levels (character, word, and sentence levels) mentioned previously. We assign equal weights to perturbations within the same level and make sure that the sum of the weights is the same for each level. This weighting approach ensures that each level of perturbation contributes equally to the final scores.\nThese discernment scores allow us to explicitly evaluate and compare the capabilities of LLMs as evaluators on specific NLG tasks, thereby establishing comprehensive benchmarks for LLMs. Higher average discernment scores (Davg and DEWavg) indicate that the LLM can generally identify and assign appropriate scores for quality issues in the NLG task, regardless of the specific type of perturbation. The average discernment scores are useful for getting a broad understanding of an LLM's overall performance as an NLG evaluator. On the other hand, the minimum discernment scores Dmin and DEWmin assess the LLM's performance in the most challenging scenarios, where it may struggle to identify certain types of quality issues. These scores represent the lowest discernment score achieved by the LLM across all perturbation methods, indicating its weakest performance. The minimum discernment scores are crucial for understanding the limitations and potential failure modes of an LLM as an NLG evaluator, even if its overall average performance is acceptable."}, {"title": "5 Benchmarking LLM Discernment", "content": "We evaluate five series of LLMs with varying parameter sizes: the GPT-series (Wang et al., 2023a), which includes GPT3.5-Turbo and GPT4-Turbo; the Llama3-series (Meta); the Vicuna1.5 series (Chiang et al., 2023); Mistral-7B (Jiang et al., 2023); and the Qwen-series (Bai et al., 2023).\nThe LLMs are evaluated across four NLG tasks using six re-established public datasets: for Summarization, we use SummEval (Fabbri et al., 2021) (news articles) and SumPubMed (Gupta et al., 2020) (scientific articles); for Story Completion, we select data from Story Cloze Test dataset (Mostafazadeh et al., 2017); for Question Answering, we utilize the data and modify the quality metric based on the Answer Equivalence dataset (Bulian et al., 2022); and for Translation, we leverage WMT-22 German-to-English and Chinese-to-English general (news) translation subsets (Kocmi et al., 2022). To ensure comparability, we select N = 100 datapoints from each dataset. The quality metrics and perturbation methods are detailed in Table 1.\nWe present our DHP benchmarking results in Figure 4. By examining the discernment scores achieved by these models, we can gain insights into their competence as NLG evaluators."}, {"title": "5.1 Overall Assessment", "content": "Most LLMs that we have evaluated demonstrate the ability to discern quality issues, as indicated by most Davg and DEWavg scores exceeding 1. This suggests they can comprehend most evaluation metrics and detect varying quality in NLG tasks. However, an exception is noted in the WMT22 Chinese-to-English Translation dataset in Figure 4(f), where Vicuna1.5-7B and Qwen1.5-7B fail to achieve favorable average discernment scores, possibly due to their weaker multi-lingual capabilities.\nOverall, for NLG evaluation, we recommend the GPT series, especially GPT4-Turbo, which demonstrates superior stability and the highest discernment across nearly all tasks. Among open-source models, Vicuna1.5-13B and Llama3-70B are commendable, achieving good average discernment scores and with most Dmin and DEWmin above 1."}, {"title": "5.2 Other Observations", "content": "Trends regarding the size of LLMs: In general, larger models within one series generally show better discernment. However, there are notable inconsistencies. For example, Qwen1.5-4B unexpectedly outperforms Qwen-7B in translation tasks in Figure 4(e, f), and Qwen-72B displays variable performance in the Question Answering task in Figure 4(d), suggesting that not all larger models uniformly perform better across all types of tasks.\nLimitations of Smaller LLMs: In more challenging scenarios, represented by Dmin and DEWmin, smaller-sized LLMs underperform. Models with fewer than 8B parameters show significantly lower Dmin and DEWmin, particularly in summarization and translation tasks in Figure 4(a, b, e, f). Among these smaller models, Llama3-8B and Mistral-7B are relatively competitive with higher average scores but still register very low scores in the summarization tasks. This suggests that smaller models may become unstable and unreliable evaluators in some complex NLG evaluation scenarios.\nMetric Misunderstanding Phenomenon: Differences between discernment scores with and without expert weights (D and DEW) are also notable. While most LLMs display consistent D and DEW scores, Llama3-8B's performance in translation tasks in Figure 4(e, f) shows a significant discrepancy, with DEW values being substantially lower than Dmin and even dropping below 1. This indicates the model's misunderstanding in metrics while identifying quality issues.\nVariations in Task Performance: Among the six datasets, LLMs perform best in the Story Cloze Test in Figure 4(c), achieving higher and more stable scores. However, the SumPubMed dataset presented in Figure 4(b) proves the most challenging; all models except GPT4-Turbo score below 1 in Dmin and DEWmin because of the dataset's complex scientific terminology and content. Models lacking sufficient prior knowledge struggle to identify subtle quality issues in such specialized content. Therefore, we encourage the community to test LLM discernment scores for their specific NLG tasks prior to conducting evaluations, ensuring the selected models are competent evaluators."}, {"title": "6 Conclusion", "content": "We introduce the DHP benchmark to assess the discernment capabilities of LLMs as evaluators across various NLG tasks. Our approach not only provides benchmarking results for LLMs but also establishes a robust framework to evaluate how effectively LLMs can identify quality issues, thus serving as competent NLG evaluators. While most models generally perform well, their performance is significantly influenced by factors such as model size, task type, and dataset complexity. By pinpointing particular weaknesses of LLMs in evaluating NLG tasks, this benchmark aids researchers in improving LLM performance going forward."}, {"title": "7 Limitations", "content": "The limitation of our DHP benchmark is that it generates discernment scores specific to each NLG dataset. A comprehensive assessment of the general evaluation capabilities of LLMs across all NLG tasks remains an open challenge. Additionally, the benchmark's current focus on English-related text limits its generalizability across different languages and cultural contexts, potentially affecting its reliability in the general multilingual NLG tasks."}, {"title": "A NLG Tasks and Metrics", "content": "A.1 Summarization\nWe utilize the SummEval (Fabbri et al., 2021) (MIT license) and SumPubMed (Gupta et al., 2020) datasets (MIT license) for our summarization tasks. The SummEval dataset comprises 100 news articles, each accompanied by multiple reference and generated summaries. For our analysis, we exclusively use the reference summaries, selecting the one with the highest number of sentences from each article to facilitate perturbation. The SumPubMed dataset contains 32,000 long scientific articles along with their abstracts serving as reference summaries. We only use the \"BACKGROUND\" sections of these articles and summaries. From this dataset, we randomly select 100 pairs of articles and their corresponding summaries.\nFor the evaluation of summarization performance, we adhere to the metrics defined by SummEval (Fabbri et al., 2021), specifically focusing on Coherence, Consistency, Fluency, and Relevance.\nA.2 Story Completion\nIn this story completion task, we utilize the public Story Cloze Test dataset (Mostafazadeh et al., 2017), which comprises four-sentence stories each paired with a reference and wrong ending. We select 100 datapoints at random from the validation set for our analysis.\nGiven the absence of explicitly defined quality metrics for the dataset, we adapt metrics from summarization tasks\u2014Coherence, Consistency, and Fluency. Coherence evaluates the story's overall structure and narrative flow. Consistency measures how well the ending maintains the established tone, setting, character development, and narrative style of the story. Fluency focuses on the linguistic and stylistic quality of the story's conclusion.\nA.3 Question Answering\nFor the question answering task, we employ the Answer Equivalence dataset (Bulian et al., 2022) (Apache-2.0 license), which is a modified version of the SQUAD dataset (Rajpurkar et al., 2016). We specifically select reference answers that exceed 150 characters to facilitate perturbation. From this filtered set, we randomly choose 100 question-answer pairs.\nWe adapt the original rating tasks of the dataset into a single metric: Answer Quality. This metric assesses whether the answer provides a comprehensive and accurate response to the question, effectively capturing the essence of the content discussed in the paragraph.\nA.4 Translation\nWe utilize two subsets from the WMT-22 general (news) translation dataset: German-to-English and Chinese-to-English sets which are freely available for research purposes. For our analysis, we select the test sets with reference translations, ensuring each translation exceeds 300 characters in length. We randomly choose 100 datapoints from each subset for evaluation.\nIn assessing translation tasks, we adopt two principal metrics from the Multidimensional Quality Metrics (MQM) framework (Burchardt, 2013): Accuracy and Fluency. Accuracy measures how closely the translation mirrors the source text, focusing on the absence of additions, omissions, or mistranslations. Fluency evaluates the translation's compliance with the linguistic norms of the target language, specifically examining spelling, grammar, and consistency."}, {"title": "B Hierarchical Perturbation", "content": "The specifics of the hierarchical perturbations are detailed in Table 2. We perform these perturbations based on character, word, and sentence-level statistical data of the texts, which are also presented in Table 2. Our rule-based perturbations include simple text deletions, typographical errors using existing software tools, reordering of sentences, and the incorporation of random or incorrect sentences from other data.\nFor LLM-based perturbations, we employ GPT4-Turbo, modifying the reference text via Auto-CoT (Zhang et al., 2022) prompts to generate the detailed procedural perturbation steps. Below, we provide an example of how the \u201cMinor Fictional Named Entities\" perturbation is applied to the summarization tasks:\nMinor Fictional Named Entities Perturbation Prompt:\nYou will be given one summary written for an article. Your task is to adjust the summary by implementing a specific change.\nPlease make sure you read and understand these instructions carefully.\nAdjustment: Please substitute only one critical named entity within the summary (e.g., a name, a location, a specific number, a technical term, etc.)"}, {"title": "C Expert Weights", "content": "We invite 10 volunteer experts with extensive backgrounds in NLP/NLG research to complete an expert weight survey. The interface of this survey is displayed in Figure 5, which includes the survey instructions, definitions of the tasks and metrics, data types, and descriptions of quality issues associated with the perturbation methods. The experts are asked to select the metric they believe is most impacted by each quality issue presented. We then utilize their responses as weights for combining the p-values. The results of these expert evaluations are detailed in Figure 6."}, {"title": "D LLM Evaluation", "content": "We evaluate five series of large language models (LLMs), details of which are provided in Table 3. Due to the extensive length of text data from the SumPubMed dataset (Gupta et al., 2020), which can exceed the 4K context window, we evaluate the models capable of processing long texts (\u2265 8K tokens). The GPT series is operated using the OpenAI API, and the open-source LLMs are executed on a server with 8 Nvidia A100 GPUs. We set the temperature parameters to 0 and maintain the default values for the top_p parameters. Throughout the evaluation process, each model score 5 times on each metric to calculate a final average score. We use the scipy.stats.wilcoxon to conduct the Wilcoxon Signed-Rank Test."}, {"title": "E Evaluation Prompts", "content": "We follow the guidelines of G-Eval (Liu et al., 2023a) and utilize the Auto-CoT method (Zhang et al., 2022) to construct our evaluation prompts. Below is an example of the prompt used for assessing the Coherence metric in summarization tasks:\nYou will be given a summary written for an article. Your task is to rate the summary on one metric. Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.\nEvaluation Criterion: Coherence (1-5) - the collective quality of all sentences. We align this dimension with the DUC quality question of structure and coherence whereby the summary should be well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.\nEvaluation Steps:\n1. Read the Summary Thoroughly: Before diving into the evaluation, ensure that you have a clear understanding of the entire summary. Reading it more than once might be necessary.\n2. Identify the Central Topic: A coherent summary will have a clear central topic or theme. Identify this topic and see if the subsequent information revolves around it.\n3. Check for Logical Flow: Review the summary for logical sequencing. Sentences should follow one another in a way that makes sense and allows the reader to easily follow the progression of information.\n4. Look for Transitional Elements: Coherent summaries often have clear transitions between sentences or ideas. This could be in the form of transitional words, phrases, or connecting ideas that tie one sentence to the next.\n5. Identify Redundancies: Check if the same information is repeated in different sentences. Redundancies can disrupt the flow and coherence of a summary.\n6. Note Any Gaps or Jumps: If there are sudden jumps in topics or if crucial information seems to be missing, this can harm the coherence of the summary. A well-organized summary should present a holistic view of the topic without leaving the reader with questions.\n7. Assess Clarity: Even if the content is technically accurate, if it's written in a convoluted or unclear manner, it can disrupt coherence. The sentences should be clear and easily understandable.\n8. Consider the Conclusion: A coherent summary often wraps up or comes to a conclusion that ties the presented information together. It doesn't necessarily need a formal conclusion, but the end should feel natural and not abrupt.\n9. Rate the Summary: Based on the above steps, assign a score between 1-5 for coherence. - 1: Very incoherent. The summary lacks structure, has sudden jumps, and is difficult to follow. - 2: Somewhat incoherent. The summary has some semblance of structure, but has significant flaws in flow and organization. - 3: Neutral. The summary is decently organized, with minor issues in flow and structure. 4: Mostly coherent. The summary is well-structured with very few minor coherence issues. - 5: Highly coherent. The summary is excellently organized, flows seamlessly, and builds information logically from start to end.\nSource Article:\nARTICLE_HERE\nSummary:\nSUMMARY_HERE\nEvaluation Score (please don't give any feedback, just give a score ONLY) - Coherence:"}]}