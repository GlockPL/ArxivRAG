{"title": "Rethinking Time Series Forecasting with LLMs via Nearest Neighbor Contrastive Learning", "authors": ["Jayanie Bogahawatte", "Sachith Seneviratne", "Maneesha Perera", "Saman Halgamuge"], "abstract": "Adapting Large Language Models (LLMs) that are extensively trained on abundant text data, and customizing the input prompt to enable time series forecasting has received considerable attention. While recent work has shown great potential for adapting the learned prior of LLMs, the formulation of the prompt to finetune LLMs remains challenging as prompt should be aligned with time series data. Additionally, current approaches do not effectively leverage word token embeddings which embody the rich representation space learned by LLMs. This emphasizes the need for a robust approach to formulate the prompt which utilizes the word token embeddings while effectively representing the characteristics of the time series. To address these challenges, we propose NNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting via LLMs. First, we generate time series compatible text prototypes such that each text prototype represents both word token embeddings in its neighborhood and time series characteristics via end-to-end finetuning. Next, we draw inspiration from Nearest Neighbor Contrastive Learning to formulate the prompt while obtaining the top-k nearest neighbor time series compatible text prototypes. We then fine-tune the layer normalization and positional embeddings of the LLM, keeping the other layers intact, reducing the trainable parameters and decreasing the computational cost. Our comprehensive experiments demonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving competitive or superior performance over the state-of-the-art methods in long-term and short-term forecasting tasks.", "sections": [{"title": "Introduction", "content": "Time series forecasting is a pivotal analytical task with applications across various domains including retail, finance, healthcare, meteorology, energy, and transportation (Fildes, Ma, and Kolassa 2022; Huber and Stuckenschmidt 2020; Stirling et al. 2021; Hong et al. 2020; Perera et al. 2024) where accurate predictions are crucial for decision making and strategic planning. Classical time series forecasting techniques such as autoregressive integrated moving average (ARIMA), exponential smoothing and Theta (Brockwell and Davis 2002; Makridakis, Spiliotis, and Assimakopoulos 2018) have been frequently used for such predictive analytics. With the rapid advent of deep learning, deep neural networks (DNNs) have become main contributors to time series forecasting. DNNs such as Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs) and more recently transformer-based architectures have shown significant promise in time series forecasting (Benidis et al. 2022). However, a model designed for a specific time series dataset is not typically generalizable for other datasets due to the domain-variant characteristics (Jiang et al. 2024).\nLarge foundation models have captured significant interest in several fields such as natural language processing (Touvron et al. 2023; Radford et al. 2019; Raffel et al. 2020) and computer vision (Kirillov et al. 2023; He et al. 2022; Caron et al. 2021). However, developing a general-purpose foundation model for time series analysis presents unique challenges. Time series data comes from different domains: healthcare, finance, transportation, etc and is inherently non-stationary, with underlying statistical properties that vary over time. This variability necessitates frequent retraining of foundation models to maintain their effectiveness (Jiang et al. 2024). Additionally, the scarcity of large, high-quality datasets to train foundation models in the time series domain poses a significant problem. Therefore, recent work focuses on leveraging Large Language Models (LLMs) for time series forecasting that capitalizes on the transfer learning capabilities of LLMs (Jiang et al. 2024; Jin et al. 2024). LLMs are trained on abundant text data, as such are well-known to have a robust capability to recognize patterns in sequences (Mirchandani et al. 2023). However, adapting LLMs for time series forecasting introduces two significant challenges. First, since LLMs are trained on data from the modality of text, adapting the learned prior of the LLMs for a new modality of time series is required. Second, time series data is continuous in nature in contrast to discrete text data (Jin et al. 2023). Recent work has proposed distinct approaches to address these challenges including unique tokenization methods to generate time series tokens (Gruver et al. 2024), template-based techniques to create sentences from time series data for sentence-to-sentence processing (Xue and Salim 2023), reprogramming methods to align time series data with text (Jin et al. 2023; Sun et al. 2023), and finetuning LLMs using carefully formulated prompts as input (Cao et al. 2023; Zhou et al. 2023; Pan et al. 2024). Latest approaches demonstrate the importance of effective prompting which aligns with both time series and text data when finetuning the LLMs (Cao et al. 2023; Pan et al. 2024). This enables the LLMs to leverage time series specific information."}, {"title": "Related Work", "content": "Time series forecasting methods range from classical time series models, machine learning models to DNN based methods. Within the classical models, Autoregressive integrated moving average (ARIMA) models followed by seasonal ARIMA (SARIMA) models, vector autoregressive and exponential smoothing have been developed (Makridakis, Spiliotis, and Assimakopoulos 2018). On the other hand, deep learning based models encompass a variety of architectures including RNN models (Lai et al. 2018; Smyl 2020), temporal convolutional network (TCN) models (Sen, Yu, and Dhillon 2019; Wu et al. 2022), transformer models (Nie et al. 2022; Wu et al. 2021; Wang et al. 2024b; Zhang and Yan 2023; Zhou et al. 2022, 2021; Liu et al. 2023, 2022), multi-layer perceptron (MLP) models (Das et al. 2023; Zeng et al. 2023; Oreshkin et al. 2019; Challu et al. 2023; Chen et al. 2023) and graph neural network models (Yu, Yin, and Zhu 2017; Wu et al. 2020). However, these models are often domain and task-specific, making it challenging to generalize them across different tasks and datasets (Jiang et al. 2024).\nRecent advancements in LLMs such as LlaMa (Touvron et al. 2023), GPT-2 (Radford et al. 2019), and T5 (Raffel et al. 2020) have sparked a growing interest in integrating LLMs into various downstream tasks across different modalities, including time series (Jiang et al. 2024; Jin et al. 2024), video (Ataallah et al. 2024) and tabular data"}, {"title": "Method", "content": "This section outlines the proposed NNCL-TLLM method which is depicted in Fig. 2. First, we partition the multivariate time series into univariate time series and process them independently following a channel independence strategy. Utilization of channel independence allows the learning of time series embeddings without sharing information among different channels which can be useful in multivariate time series with different behaviors related to each channel (Nie et al. 2022). The univariate time series is divided into overlapping patches and a mapping function is used to obtain the embedding of the univariate time series. Then we utilize a custom mapping function which leverages neighborhood information to learn the time series compatible text prototypes from the word token embeddings of the LLM. We formulate the input prompt using the NNCL concept (Dwibedi et al. 2021) by aligning the time series and text. Since the self-attention and feed forward layers of the LLMs encapsulate most of the knowledge learned during pre-training (Zhou et al. 2023; Zhao et al. 2023), we only finetune the layer normalization and positional embeddings of the LLM using the formulated prompt to adapt the LLM for time series forecasting while maintaining the learned prior of the LLM.\nWe define the time series forecasting problem as follows: Let $X \\in R^{M \\times T}$ be the input multivariate time series of historical observations where M is the number of variables or channels and T is the time steps. Since X consists of M channels, $X^{(i)} \\in R^{1 \\times T}$ represents the univariate time series corresponding to channel i. Given $X^{(i)}$, our objective is to predict the data points for H future time steps as denoted by $\\hat{Y}^{(i)} \\in R^{1 \\times H}$ while minimizing the error between the prediction $\\hat{Y}^{(i)}$ and the ground truth $Y^{(i)} \\in [R^{1 \\times H}$.\nThe input multivariate time series X is partitioned into M univariate time series; $X^{(i)}$ which only contains data points from one channel (i). Afterwards, each univariate time series is processed independently. The input time series $X^{(i)}$ is normalized using reversible instance normalization (RevIN) (Kim et al. 2021) to avoid the effects of temporal distribution shift. The concept of time series patching is adopted to obtain the time series embeddings as it allows to capture relevant information from the time series with a long look-back window while minimizing computational costs (Nie et al. 2022). The normalized time series $X^{(i)}$ is divided into overlapping patches where the length of a patch is C and the stride is S. After the patching operation, the total number of patches created is defined by $N = [(T \u2013 C)/S] + 2$. A 1-Dimensional (1D) convolution layer is utilized to obtain the time series patch embeddings as $P^{(i)} \\in R^{N \\times D}$, where the dimension of the embedding is denoted by N \u00d7 D. The patch embeddings are finally fed into a simple linear layer to generate the embedding of the input univariate time series; $Z^{(i)} \\in R^{1 \\times D}$."}, {"title": "Neighborhood aware Time series Compatible Text Prototype Learning", "content": "Word token embeddings in the LLM are denoted as $W \\in R^{V \\times D}$, where V is the size of the vocabulary. Since the vocabulary of LLMs contains approximately 50000 word tokens (Radford et al. 2019), directly utilizing the word token embeddings to formulate the prompt is computationally intensive (Jin et al. 2023; Pan et al. 2024; Cao et al. 2023; Sun et al. 2023). As there can not be an explicit mapping between the time series and word tokens, we propose a new concept called: time series compatible text prototypes (TCTPs) which are text prototypes that represent time series characteristics and may not make sense from a natural language perspective. While a linear layer can learn linear relationships with an adequate amount of data, a more sophisticated approach is required to learn TCTPs in few-shot learning scenarios where only a limited amount of data is available for optimization. To this end, we propose to find neighborhood aware TCTPs, such that each TCTP is a representative of the word token embeddings in its neighborhood in the embedding space as illustrated in Fig. 1. These TCTPs become time series compatible through the end-to-end finetuning of the entire framework with NNCL.\nThe TCTP embeddings; $E \\in R^{U \\times D}$, where U (U << V) is the number of learned TCTPs, are learnable vectors which are randomly initialized at first. During training, these TCTPs are dynamically adjusted based on the word token distribution in the embedding space. The distance between each word token embedding; w and each learnable TCT\u0420 embedding e is measured using Euclidean distance as:\n$Dis(w, e) = ||w - e||^2$       (1)\nThe optimization objective is to minimize the distance between each word token embedding and its nearest TCTP embedding. Each word token embedding w is assigned to the nearest TCTP embedding e*, where e* is determined by Eq. 2.\n$e^* = e \\ where \\argmin(Dis(w, e))$       (2)\nMean squared error (MSE) loss between the word token embeddings and their nearest TCTP embeddings is calculated as $L_{proto}$ using Eq. 3.\n$L_{proto} = \\frac{1}{V} \\sum_{i=1}^V ||w_i - e^*||^2$       (3)"}, {"title": "Prompt Formulation via Nearest Neighbor Contrastive Learning", "content": "Neighborhood aware TCTPs are learned so as to be compatible with time series through NNCL. Thus, the prompts formulated using the TCTPs become aligned with both time series and text. Formulation of such prompts has shown significant promise in adapting LLMs for time series forecasting (Pan et al. 2024; Cao et al. 2023). Therefore, inspired by the idea of NNCL in the domain of computer vision (Dwibedi et al. 2021), we propose to adopt NNCL to formulate the input prompt utilizing TCTPs to finetune the LLM. Contrastive learning is built upon the idea of learning a representation space in which positive pairs (e.g. augmentations"}, {"title": "LLM finetuning and Output projection", "content": "The prompt which is used as the input to the LLM is formulated by concatenating the two representations including the time series patch embeddings; $P^{(i)}$ and top-k set of nearest neighbor TCTPs from the support set; $NN (Z^{(i)}, Q)$ as denoted by prompt.\nprompt = [$P^{(i)}; NN(Z^{(i)}, Q)$]       (6)\nUtilization of this information-rich embedding as the input prompt to finetune the LLM ensures that the LLM has not only the temporal context from time series embedding but also the corresponding time series compatible textual representations via top-k set of nearest neighbor TCTP embeddings from the support set.\nWe follow a finetuning strategy (Zhou et al. 2023) where only the layer normalization and positional embeddings in the LLM are finetuned while keeping the multi-head attention and feed-forward layers intact. Finetuning of the LLM can enable the capability to forecast the time series with the help of the formulated input prompt while preserving the prior it has learned during extensive training.\nOutput prediction of the LLM is flattened and then followed by a linear layer to get the final forecast; $\\hat{Y}^{(i)}$. We compute the MSE loss between the forecast $\\hat{Y}^{(i)}$ and the ground truth $Y^{(i)}$ using Eq. 7.\n$L_{forecast} = \\frac{1}{B} \\sum_{b=1}^B || Y_b - \\hat{Y}_b||^2$        (7)"}, {"title": "Training", "content": "Overall Optimization Objective. The total loss used in our framework is shown in Eq. 8, where \u03bb (> 0) is the weight which controls the contribution of the components in the loss. This loss function is proposed considering all the components in NNCL-TLLM. This ensures that the learned embedding space and the formulated prompt implicitly align across the two modalities: text and time series, while optimally leveraging the rich representation space learned by the LLMs to generate TCTPs via end-to-end finetuning.\n$L_{total} = L_{forecast} + \\lambda (L_{NNCL} + L_{proto})$        (8)"}, {"title": "Experiments", "content": "We conduct comprehensive experiments with widely used public datasets that cover distinct domains including energy, electricity, transportation, meteorology, finance and economics. Results are presented in terms of long-term forecasting, short-term forecasting and few-shot forecasting.\nWe use GPT-2 (Radford et al. 2019) as the backbone of the LLM in our framework in the experiments. The concrete details of the implementation of NNCL-TLLM framework can be found in Appendix.\nWe compare our NNCL-TLLM method with state-of-the-art (SOTA) methods in time series forecasting and the results of the baseline methods are cited from (Pan et al. 2024). The baseline methods encompass LLM-based time series models; S\u00b2IP-LLM (Pan et al. 2024), Time-LLM (Jin et al. 2023) and OFA (Zhou et al. 2023) and transformer-based models; PatchTST (Nie et al. 2022), iTransformer (Liu et al. 2023), Informer (Zhou et al. 2021), Autoformer (Wu et al. 2021), FEDformer (Zhou et al. 2022) and Non-stationary Transformer (Liu et al. 2022). To maintain the consistency of the results, we perform comparisons considering a TCN-based model; TimesNet (Wu et al. 2022) and a MLP-based model; DLinear (Zeng et al. 2023). In our comparative analysis of short-term forecasting results, we additionally compare with N-HiTS (Challu et al. 2023)."}, {"title": "Long-Term Forecasting", "content": "We evaluate the long-term forecasting performance of our framework considering the datasets; ETTh1, ETTh2, ETTm1, ETTm2, Electricity (ECL), Traffic and Weather (Zhou et al. 2021). Additional explanations of the datasets are provided in Appendix. In all the experiments, the input time series length is set to 512 historical observations. In line with the previous works, we report the performance in terms of Mean Squared Error (MSE) and Mean Absolute Error (MAE) which are discussed in detail in Appendix. The evaluation is conducted across four forecasting horizons; [96, 192, 336, 720] and finally the average MSE and MAE values across these four horizons are computed.\nComparison with the SOTA methods for long-term forecasting is shown in Table 1. Our NNCL-TLLM method outperforms SOTA methods for several datasets including ETTh1, ETTh2, ETTm1 and ETTm2, while performing competitively for other datasets. It is also evident that the LLM-based time series analysis methods perform better than the other transformer based and non-transformer based methods benefiting from the strong learned prior of the LLMs. Note that, our NNCL-TLLM method achieves better performance without using any specialized technique including STL decomposition (Pan et al. 2024) to learn time"}, {"title": "Short-Term Forecasting", "content": "The M4 dataset is selected as the benchmark for short-term forecasting (Makridakis, Spiliotis, and Assimakopoulos 2018). Additional details about the M4 dataset is provided in Appendix. The prediction horizons range from 6 to 48 and the input time series lengths are set to be twice the prediction horizons. The evaluation metrics include symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA). Further details related to the evaluation metrics can be found in Appendix.\nAs illustrated in Table 2, NNCL-TLLM outperforms the SOTA methods in terms of SMAPE and MASE metrics. A more detailed comparison is presented in the Appendix. In addition, NNCL-TLLM surpasses the non-transformer based method, N-HiTS (Challu et al. 2023)."}, {"title": "Few-Shot Forecasting", "content": "LLMs often excel at few-shot downstream tasks where only a limited amount of data is available for training, owing to their strong prior learned via extensive training with abundant data. We conduct long-term forecasting experiments in few-shot settings where only 10% and 5% of data is available for finetuning.\nAs summarized in Table 3, NNCL-TLLM exhibits better performance with relation to the 10% few-shot forecasting setting. For multiple datasets including ETTh1, ETTh2 and ETTm2, our method indicates up to 5% reduction in MSE highlighting the effectiveness of NNCL-TLLM. We also achieve competitive or superior performance for 5% few-shot forecasting as shown in Table 4. The results exemplifies the ability of NNCL-TLLM to be effective in time series forecasting even in data scarce settings harnessing the strong prior learned by the LLMs via effectively formulating the prompt using NNCL and neighborhood aware time series compatible text prototype learning."}, {"title": "Conclusion", "content": "This paper proposes NNCL-TLLM, a new approach for adapting LLMs for time series forecasting by effectively formulating the prompt using nearest neighbor contrastive learning. We demonstrate that time series compatible text prototypes can be learned via NNCL. A limitation that needs to be addressed is incorporating the channel dependencies of multivariate time series."}]}