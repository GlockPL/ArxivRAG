{"title": "Anole: Adapting Diverse Compressed Models for Cross-scene Prediction on Mobile Devices", "authors": ["Yunzhe Li", "Hongzi Zhu", "Zhuohong Deng", "Yunlong Cheng", "Liang Zhang", "Shan Chang", "Minyi Guo"], "abstract": "Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmanned aerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).", "sections": [{"title": "I. INTRODUCTION", "content": "Motivation. Last decade has witnessed the booming de-velopment of Artificial Intelligence of Things (AIoT), an emerging computing paradigm that marries artificial intelli-gence (AI) and Internet of Things (IoT) technologies to enable independent decision-making at each component level of the interconnected system. In many AIoT scenarios, deep neural network (DNN) model inference (i.e., prediction) tasks are required to execute on mobile devices, referred to as the online mobile inference (OMI) problem, with stringent accuracy and latency requirements. For example, unmanned aerial vehicles (UAVs) need to constantly detect surrounding objects in real time [1]; a dash cam mounted on a vehicle needs to perform continuous image object detection [2]; robots in smart factories need to detect objects in production lines in real time, and interact with human workers and other robots [3].\nTo address the OMI problem, however, is demanding for two reasons as follows. First, given that mobile devices con-stantly experience scene changes while moving (e.g., due to various lighting conditions, weather conditions, and viewing angles), the output of DNNs should remain reliable and accu-rate. Training a statistical learning DNN on a given dataset, as in normal deep learning paradigm, becomes difficult to guarantee the robustness, interpretability and correctness of the output of the statistical learning models when data samples are out-of-distribution (OOD) [4]. Second, the response time for model inference should satisfy a rigid delay budget to support real-time interactions with these devices. As mobile devices are resource-constrained in terms of computation, storage and energy, they cannot handle large DNNs. Though it would be beneficial to offload a part of or even entire inference tasks to a remote cloud, unstable communication between mobile devices and the cloud may lead to unpredictable delay.\nIn the literature, much effort has been made to improve DNN model inference accuracy on mobile devices but in static scenarios. One main branch aims to develop DNNs specially designed for mobile devices [5]\u2013[8] or to compress (e.g., via model pruning and quantization) existing DNNs to match the computing capability of a mobile device [9], [10]. Such schemes ensure real-time model inference at the expense of compromised accuracy, especially when dealing with OOD data samples. Another branch is to divide DNNs and perform collaborative inference on both edge devices and the cloud [11]\u2013[13], or to transmit compressed sensory data to the cloud for data recovery and model inference [14], [15]. These approaches need coordination with the cloud for each inference, leading to unpredictable inference delays when the communication link is unstable or disconnected. As a result, to the best of our knowledge, there is no successful solution to the OMI problem yet.\nOur approach. We propose Anole, which enables online model inference on mobile devices in dynamic scenes. We have the insight that a compressed DNN targeted for a particular scene (i.e., data distribution) can achieve comparable inference accuracy provided by a fully-fledged large DNN. The core idea of Anole is to first establish a colony of compressed scene-specific DNNs, and then adaptively select the model best suiting the current test sample for online inference. To this end, it is essential to identify scenes from the perspective of DNN models. We design a weakly-supervised scene representation learning scheme by combining both human heuristics and feature similarity in separating scenes. After that, for each identified scene, an individual compressed DNN model can be trained. Furthermore, we train a model classifier to predict"}, {"title": "Challenges and contributions", "content": "The Anole design faces three main challenges. First, how to obtain model-friendly scenes and train scene-specific DNNs from public datasets is unclear, as the distribution that a DNN model can char-acterize is implicit. One naive way is to use semantic at-tributes (e.g., time, location, weather and light conditions) of data to define scenes of similar data samples. However, as shown in our empirical study, DNNs trained on such scenes cannot reach satisfactory prediction accuracy even on their respective training scenes. To tackle this challenge, we design a scene representation learning algorithm that combines semantic similarity and feature similarity of data to filter out scenes. Specifically, human heuristic is first used to define scenes of similar semantic attribute values, referred to as semantic scenes. Then, a scene representation model, denoted as \\(M_{scene}\\), is trained using the indices of semantic scenes as labels. After that, we can obtain embeddings of all data samples extracted with \\(M_{scene}\\) and believe such embeddings can well characterize semantic information. Therefore, by conducting multi-granularity clustering on these embeddings, we can obtain clusters of data samples with similar semantic information in feature space, referred to as model-friendly scenes. Finally, a compressed DNN can be trained on each model-friendly scene, constituting a model repository for use.\nSecond, given a test sample, how to determine the best-fit models or whether such models even exist in the model repository is hard to tell. To deal with this challenge, we train a model classifier, denoted as \\(M_{decision}\\), to predict the best model for use. Specifically, for each model-friendly scene, we select those data samples in the scene that can be accurately predicted by the corresponding DNN and use the index of the DNN as the label to train \\(M_{decision}\\). Instead of testing all data samples, we use Thompson sampling to establish balanced training sets at a low computational cost. With a well-trained \\(M_{decision}\\), the most suitable compressed models can be predicted and the prediction confidence can be used to indicate whether such models exist.\nLast, how to deploy those pre-trained compressed DNNs on mobile devices with constrained memory is non-trivial. We have the observation that the utility of models follows a power-law distribution over all test videos. This implies that it is feasible to cache a small number of most frequently used compressed models and take a least frequently used (LFU) model replacement strategy.\nWe implement Anole on three typical mobile devices, i.e., Jetson Nano, Jetson TX2 NX and a laptop, with each equipped with a CPU/MCU and an entry-level GPU, to conduct the image object detection task on moving vehicles. Specifically, we train the \\(M_{scene}\\) based on Resnet18 [16], a pack of 19 compressed DNNs based on YOLOv3-tiny [17], and the \\(M_{decision}\\) based on Resnet18 accordingly, using three driv-ing video datasets collected from multiple cities in different counties. We conduct extensive trace-driven and real-world experiments using UAVs. Results demonstrate that Anole is lightweight and agile to switch best models with low latencies of 61.0 ms, 13.9 ms, and 52.0 ms on Jetson Nano, Jetson TX2 NX, and the laptop, respectively. In cross-scene (i.e., seen but fast-changing scenes) setting, Anole can achieve a high F1 prediction accuracy of 56.4% whereas the F1 score of a general large DNN model and a general compact DNN are 50.7% and 45.9%, respectively. In hard new-scene (i.e., unseen scenes) setting, Anole can maintain a high F1 score of 48.7% whereas the F1 score of the general large DNN and the general compact DNN drops to 46.6% and 41.1%, respectively.\nWe highlight the main contributions made in this paper as follows: 1) A new solution to the OMI problem by recruit-ing a pack of compact but specialized models on resource-constrained mobile devices, without any intervention with the cloud during online model inference; 2) A scene partition method that effectively facilitates the training of specialized models by leveraging both semantic and feature similarity of the data; 3) We have implemented Anole on typical mobile devices and conducted extensive trace-driven and real-world experiments, the results of which demonstrate the efficacy of Anole."}, {"title": "II. PROBLEM DEFINITION", "content": "A. System Model\nWe consider three types of entities in the system:\n\u2022 Mobile devices: Mobile devices have constrained com-putational power and a limited amount of memory but are affordable for running and storing compressed DNNs. Such devices may be moving while performing online inference tasks at the same time. They are battery-powered, desiring lightweight operations. In addition, they can communicate with a cloud server via an unstable wireless network connection for offline model training and downloading.\n\u2022 Cloud server: A cloud server has sufficient compu-tational power and storage for offline model training. During online inference, the cloud server is not involved.\n\u2022 Complex environment: We consider practical environ-ments where background objects and light conditions have distinct spatial and temporal distributions. When mobile devices move in such a complex environment, they constantly experience fast scene changes.\nB. Problem Formulation\nGiven the set of all available labeled data, denoted as D, a compressed DNN model, denoted as \\(M_i\\), can be trained on a particular dataset, denoted as \\(\\Gamma_i\\), which is a subset of D, i.e., \\(\\Gamma_i \\subset D\\) for \\(i \\in \\mathbb{N}\\). For instance, \\(\\Gamma_i\\) can be established based on some semantic attributes of data. Assume that a set of n models \\(\\{M_1, M_2,\\ldots,M_n\\}\\) have been pre-trained on respective training datasets \\(\\Gamma_1, \\Gamma_2,\\ldots, \\Gamma_n\\) and the im-plicit data distributions that those models can characterize are \\(\\Psi_1, \\Psi_2,..., \\Psi_n\\), respectively, which means that if a data sample \\(x \\in \\Psi_i\\) for \\(i \\in [1, n]\\), model \\(M_i\\) guarantees to output accurate prediction for x. We have the following proposition:\nProposition 1. Though \\(M_i\\) is trained on \\(\\Gamma_i\\), not all data samples in \\(\\Gamma_i\\) necessarily belong to \\(\\Psi_i\\), i.e., \\(\\Gamma_i \\nsubseteq \\Psi_i\\).\nAs illustrated in Figure 1, given D, we can train such a set of n models \\(M = \\{M_1, M_2,\\ldots,M_n\\}\\) so that \\(D \\subset \\bigcup_{i=1}^n \\Psi_i\\). As in mobile settings, any data sample \\(x \\in \\mathbb{U}\\) can be encountered where \\(\\mathbb{U}\\) is the universal set of all possible data, the online mobile inference problem is to identify an optimal subset of M, denoted as \\(M^*\\), that maximize the prediction accuracy for x. The problem can be discussed in the following three cases of different difficulties: 1) \\(x \\in D\\): in this case, \\(M^*\\) is known since x is seen before, i.e., \\(M^* = \\{M_i : x \\in \\Psi_i, i \\in [1,n]\\}\\); 2) \\(x \\notin D\\) and \\(x \\in \\bigcup_{i=1}^n \\Psi_i\\): in this case, x is not seen before and \\(M^* = \\{M_i : x \\in \\Psi_i, i \\in [1, n]\\}\\) exists but how to find the \\(M^*\\) is hard; 3) \\(x \\in \\mathbb{U} - \\bigcup_{i=1}^n \\Psi_i\\): in this case, as x is not seen before and \\(M^*\\) does not exist regarding existing M, how to make best-effort online prediction for x is challenging. A remedy for this case is to train new models to deal with x and the like in the future.\nThe main difficulty of the online mobile inference problem lies in how to determine whether an unseen x belongs to \\(\\Psi_i\\) for \\(i \\in [1,n]\\). According to Proposition 1, simply comparing the similarity of semantic attributes between x and \\(\\Gamma_i\\) for \\(i \\in [1, n]\\) would not work. Another concern is how to achieve the best-effort inference accuracy within a specific latency budget even if \\(M^*\\) does not exist."}, {"title": "III. OVERVIEW OF ANOLE", "content": "The design of Anole is motivated by an observation that though any single compressed model generally has a lower prediction accuracy than the big model, there exists a com-pressed model that can achieve comparable accuracy as the big model for each specific scene. As illustrated in Figure 2, Anole consists of two parts, i.e., offline scene profiling and online model inference.\nOffline Scene Profiling (OSP). OSP is deployed on cloud servers for offline scene partitioning and scene-specific model training, which integrates three components as follows:\n1) Training Compressed Models (TCM): Given the avail-able labelled dataset D, TCM first divides D into ap-propriate training datasets and train a scene representation model \\(M_{scene}\\) and a pack of n compressed models \\(M = \\{M_1, M_2,\\ldots, M_n\\}\\);\n2) Adaptive Scene Sampling (ASS): As \\(\\Psi_1, \\Psi_2,\\ldots, \\Psi_n\\) are implicit, ASS is to adaptively sample \\(\\Psi_1, \\Psi_2,\\ldots, \\Psi_n\\) based on Thompson sampling from all available dataset D to obtain balanced subsets of \\(\\Psi_1, \\Psi_2,\\ldots, \\Psi_n\\) in D, denoted"}, {"title": "IV. OFFLINE SCENE PROFILING", "content": "A. Training Compressed Models\n1) Training Dataset Segmentation: We first define seman-tic scenes based on semantic attributes of data. It is non-trivial, however, to manually define appropriate scenes as semantic attributes have different dimensions and different granularities. For example, for driving images, \u201curban\u201d and \"daytime\" are spatial and temporal attributes, respectively, in different dimensions; \"urban\" and \"street\" are spatial at-tributes but in different granularities. Scenes defined with fine-grained attributes would have insufficient number of samples to train a model whereas scenes defined with coarse-grained attributes would lose the diversity of models. Specifically, we heuristically select fine-grained attributes in each orthogonal dimension to separate data samples into m scenes, denoted as \\(\\Gamma_1^{sem}, \\Gamma_2^{sem},..., \\Gamma_m^{sem}\\). For instance, as for driving images, we define semantic scenes according to 120 combinations of attributes in three dimensions, i.e., \\{clear, overcast, rainy, snowy, foggy\\} in weather, \\{highway, urban, residential, parking lot, tunnel, gas station, bridge, toll booth\\} in location and \\{daytime, dawn/dusk, night\\} in time\u00b9.\n2) Compressed Model Training: We employ a training strategy, integrating both semantic similarity and feature sim-ilarity of data samples to train diverse compressed models, which consists of the following two steps, as described in Algorithm 1."}, {"title": "V. ONLINE MODEL INFERENCE", "content": "A. Model Selection Strategy\nGiven the set of pretrained models \\(\\{M_1, M_2,..., M_n\\}\\) and decision model \\(M_{decision}\\) downloaded from a cloud server, a mobile device needs to select most suitable com-pressed models for online inference. Specifically, it utilizes \\(M_{decision}\\) to output the model allocation vector v for a testing sample x, i.e., \\(v_x = M_{decision}(x)\\), where the i-th element \\(v_i\\) indicates the suitability probability that model \\(M_i\\) is suitable for x. Therefore, we can rank all compressed models according to their suitability probablities for x using v. It should be noted that for the uncertainty of scenario duration, model selection should be conducted on every testing sample, taking into account the fast-changing data distributions in the perspective of compressed models.\nB. Cache-based Model Deployment\nWith the model allocation vector \\(v_x = M_{decision}(x)\\), compressed models can be dynamically ranked. Due to the re-stricted amount of memory on a mobile device, not all models may be pre-loaded into memory. To deal with this issue, we investigate the best-effort model deployment strategy.\nWe examine the inference latency of detecting objects on five driving video clips, using two DNN models of different"}, {"title": "VI. EVALUATION", "content": "A. Methodology\n1) Datasets: We evaluate Anole on a typical mobile infer-ence task, i.e., vehicle detection on driving videos (VD), based on the following datasets and real-world experiments.\n\u2022 KITTI [23]: comprises 389 stereo and optical flow im-age pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). For online object detection, KITTI consists of 21 training sequences and 29 test sequences.\n\u2022 BDD100k [24]: contains over 100k video clips regarding ten autonomous driving tasks. Clips of 720p and 30fps were collected from more than 50 thousand rides in New York city and San Francisco Bay Area, USA. Each clip lasts for 40 seconds and is associated with semantic attributes such as the scene type (e.g., city, streets, resi-dential areas, and highways), weather condition and the time of the day.\n\u2022 SHD: contains 100 driving video clips of one minute recorded in March 2022 with a 1080p dashcam in Shang-hai city, China. Clips were collected from ten typical scenarios, including highway, typical surface roads, and tunnels, at different time in the day. LabelImg [25] is employed to label objects in all images.\nWe random select 10 video clips from KITTI, 44 clips from BDD100k, and 10 clips from SHD, forming a dataset of 64 video clips containing 16,145 image samples in various scenarios. Figure 5 shows the cumulative distribution functions"}, {"title": "VII. RELATED WORK", "content": "A. DNN Prediction on Mobile Devices\nTo perform DNN inference on mobile devices, new DNNs are specially designed [5]\u2013[8] or existing DNNs are com-pressed to match the computing capability of a mobile de-vice [9], [10]. First, model structure can be optimized to reduce complexity [5], [6], [30]. Second, quantization precision can be reduced to minimize computational cost, e.g., use integers instead of floating-point numbers [31], [32]. Third, the neu-ral network model can also be accelerated by pruning, i.e., deleting some neurons in the neural network [9], [10], [33]. Scene information is also utilized for model compression on edge/mobile devices [34]\u2013[37]. Finally, model distillation can distill the knowledge of large models into small models [7], [8]. Such schemes ensure real-time model inference at the expense of compromised accuracy.\nAnother direction is to divide DNNs and perform collab-orative inference on both edge devices and the cloud [11], [12], [38]\u2013[41], or to transmit compressed sensory data to the cloud for data recovery and model inference [14], [15]. Neurosurgeon [12] partitions the computation of each DNN inference task in a layer granularity. CLIO [42] addresses the instability of network conditions and optimizes infer-ence under different network states. These approaches need coordination with the cloud for each inference, leading to unpredictable inference delays when the communication link is unstable or disconnected. However, they prove inadequate for cross-scene mobile inference scenarios where even deep models are unable to cope.\nB. Cross-scene DNN Prediction\nData-driven machine learning models face challenges in maintaining robust inference performance when dealing with cross-scene inference [43]. One natural approach for scene partitioning is to partition the scene based on prior knowledge or historical samples. First, based on prior knowledge, a simi-larity graph is constructed to cluster similar domains together"}, {"title": "VIII. CONCLUSION", "content": "In this paper, we have proposed Anole, an online model inference scheme on mobile devices. Anole employs a rich set of compressed models trained on a wide variety of human-defined scenes and offline learns the implicit mode-defined scenes characterized by these compressed models via a deci-sion model. Moreover, the most suitable compressed models can be dynamically identified according to the current testing samples and used for online model inference. As a result, Anole can deal with unseen samples, mitigating the impact of OOD problem to the reliable inference of statistical models. Anole is lightweight and does not need network connection during online inference. It can be easily implemented on various mobile devices at a low cost. Extensive experiment results demonstrate that Anole can achieve the best inference accuracy at a low latency."}]}