{"title": "Anole: Adapting Diverse Compressed Models for Cross-scene Prediction on Mobile Devices", "authors": ["Yunzhe Li", "Hongzi Zhu", "Zhuohong Deng", "Yunlong Cheng", "Liang Zhang", "Shan Chang", "Minyi Guo"], "abstract": "Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmanned aerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).", "sections": [{"title": "I. INTRODUCTION", "content": "Motivation. Last decade has witnessed the booming de-velopment of Artificial Intelligence of Things (AIoT), an emerging computing paradigm that marries artificial intelli-gence (AI) and Internet of Things (IoT) technologies to enable independent decision-making at each component level of the interconnected system. In many AIoT scenarios, deep neural network (DNN) model inference (i.e., prediction) tasks are required to execute on mobile devices, referred to as the online mobile inference (OMI) problem, with stringent accuracy and latency requirements. For example, unmanned aerial vehicles (UAVs) need to constantly detect surrounding objects in real time [1]; a dash cam mounted on a vehicle needs to perform continuous image object detection [2]; robots in smart factories need to detect objects in production lines in real time, and interact with human workers and other robots [3].\nTo address the OMI problem, however, is demanding for two reasons as follows. First, given that mobile devices con-stantly experience scene changes while moving (e.g., due to various lighting conditions, weather conditions, and viewing angles), the output of DNNs should remain reliable and accu-rate. Training a statistical learning DNN on a given dataset, as in normal deep learning paradigm, becomes difficult to guarantee the robustness, interpretability and correctness of the output of the statistical learning models when data samples are out-of-distribution (OOD) [4]. Second, the response time for model inference should satisfy a rigid delay budget to support real-time interactions with these devices. As mobile devices are resource-constrained in terms of computation, storage and energy, they cannot handle large DNNs. Though it would be beneficial to offload a part of or even entire inference tasks to a remote cloud, unstable communication between mobile devices and the cloud may lead to unpredictable delay.\nIn the literature, much effort has been made to improve DNN model inference accuracy on mobile devices but in static scenarios. One main branch aims to develop DNNs specially designed for mobile devices [5]\u2013[8] or to compress (e.g., via model pruning and quantization) existing DNNs to match the computing capability of a mobile device [9], [10]. Such schemes ensure real-time model inference at the expense of compromised accuracy, especially when dealing with OOD data samples. Another branch is to divide DNNs and perform collaborative inference on both edge devices and the cloud [11]\u2013[13], or to transmit compressed sensory data to the cloud for data recovery and model inference [14], [15]. These approaches need coordination with the cloud for each inference, leading to unpredictable inference delays when the communication link is unstable or disconnected. As a result, to the best of our knowledge, there is no successful solution to the OMI problem yet.\nOur approach. We propose Anole, which enables online model inference on mobile devices in dynamic scenes. We have the insight that a compressed DNN targeted for a particular scene (i.e., data distribution) can achieve comparable inference accuracy provided by a fully-fledged large DNN. The core idea of Anole is to first establish a colony of compressed scene-specific DNNs, and then adaptively select the model best suiting the current test sample for online inference. To this end, it is essential to identify scenes from the perspective of DNN models. We design a weakly-supervised scene representation learning scheme by combining both human heuristics and feature similarity in separating scenes. After that, for each identified scene, an individual compressed DNN model can be trained. Furthermore, we train a model classifier to predict"}, {"title": "II. PROBLEM DEFINITION", "content": "A. System Model\nWe consider three types of entities in the system:\nMobile devices: Mobile devices have constrained com-putational power and a limited amount of memory but are affordable for running and storing compressed DNNs. Such devices may be moving while performing online inference tasks at the same time. They are battery-powered, desiring lightweight operations. In addition, they can communicate with a cloud server via an unstable wireless network connection for offline model training and downloading.\nCloud server: A cloud server has sufficient compu-tational power and storage for offline model training. During online inference, the cloud server is not involved.\nComplex environment: We consider practical environ-ments where background objects and light conditions have distinct spatial and temporal distributions. When\n\u2022\n\u2022\n\u2022"}, {"title": "B. Problem Formulation", "content": "Given the set of all available labeled data, denoted as D, a compressed DNN model, denoted as Mi, can be trained on a particular dataset, denoted as $\\Gamma_i$, which is a subset of D, i.e., $\\Gamma_i \\subset D$ for $i \\in \\mathbb{N}$. For instance, $\\Gamma_i$ can be established based on some semantic attributes of data. Assume that a set of n models {M1,M2,\u2026,Mn} have been pre-trained on respective training datasets {$\\Gamma_1, \\Gamma_2,\u2026, \\Gamma_n$} and the implicit data distributions that those models can characterize are {$\\Psi_1, \\Psi_2,..., \\Psi_n$}, respectively, which means that if a data sample $x \\in \\Psi_i$ for $i \\in [1, n]$, model Mi guarantees to output accurate prediction for x. We have the following proposition:\nProposition 1. Though Mi is trained on $\\Gamma_i$, not all data samples in $\\Gamma_i$ necessarily belong to $\\Psi_i$, i.e., $\\Gamma_i \\nsubseteq \\Psi_i$.\nAs illustrated in Figure 1, given D, we can train such a set of n models $\\mathbb{M} = \\{M_1, M_2,\\dots,M_n\\}$ so that $D \\subset \\bigcup_{i=1}^{n} \\Psi_i$. As in mobile settings, any data sample $x \\in \\mathbb{U}$ can be encountered where $\\mathbb{U}$ is the universal set of all possible data, the online mobile inference problem is to identify an optimal subset of $\\mathbb{M}$, denoted as $\\mathbb{M}^*$, that maximize the prediction accuracy for x. The problem can be discussed in the following three cases of different difficulties: 1) x \u2208 D: in this case, $\\mathbb{M}^*$ is known since x is seen before, i.e., $\\mathbb{M}^* = \\{M_i : x \\in \\Psi_i, i\\in[1,n]\\}$; 2) $x \\notin D$ and $x \\in \\bigcup_{i=1}^{n} \\Psi_i$: in this case, x is not seen before and $\\mathbb{M}^* = \\{M_i : x \\in \\Psi_i, i\\in[1, n]\\}$ exists but how to find the $\\mathbb{M}^*$ is hard; 3) $x \\in \\mathbb{U} - \\bigcup_{i=1}^{n} \\Psi_i$: in this case, as x is not seen before and $\\mathbb{M}^*$ does not exist regarding existing $\\mathbb{M}$, how to make best-effort online prediction for x is challenging. A remedy for this case is to train new models to deal with x and the like in the future.\nThe main difficulty of the online mobile inference problem lies in how to determine whether an unseen x belongs to $\\Psi_i$ for $i \\in [1,n]$. According to Proposition 1, simply comparing the similarity of semantic attributes between x and $\\Gamma_i$ for $i \\in [1, n]$ would not work. Another concern is how to achieve the best-effort inference accuracy within a specific latency budget even if $\\mathbb{M}^*$ does not exist."}, {"title": "III. OVERVIEW OF ANOLE", "content": "The design of Anole is motivated by an observation that though any single compressed model generally has a lower prediction accuracy than the big model, there exists a com-pressed model that can achieve comparable accuracy as the big model for each specific scene. As illustrated in Figure 2, Anole consists of two parts, i.e., offline scene profiling and online model inference.\nOffline Scene Profiling (OSP). OSP is deployed on cloud servers for offline scene partitioning and scene-specific model training, which integrates three components as follows:\n1) Training Compressed Models (TCM): Given the avail-able labelled dataset D, TCM first divides D into ap-propriate training datasets and train a scene representation model Mscene and a pack of n compressed models M = {M1, M2,\u2026, \u039c\u03b7};\n2) Adaptive Scene Sampling (ASS): As {$\\Psi_1, \\Psi_2,\\dots, \\Psi_n$} are implicit, ASS is to adaptively sample {$\\Psi_1, \\Psi_2,\\dots, \\Psi_n$} based on Thompson sampling from all available dataset D to obtain balanced subsets of {$\\Psi_1, \\Psi_2,\\dots, \\Psi_n$} in D, denoted"}, {"title": "IV. OFFLINE SCENE PROFILING", "content": "A. Training Compressed Models\n1) Training Dataset Segmentation: We first define seman-tic scenes based on semantic attributes of data. It is non-trivial, however, to manually define appropriate scenes as semantic attributes have different dimensions and different granularities. For example, for driving images, \u201curban\u201d and \"daytime\" are spatial and temporal attributes, respectively, in different dimensions; \"urban\" and \"street\" are spatial at-tributes but in different granularities. Scenes defined with fine-grained attributes would have insufficient number of samples to train a model whereas scenes defined with coarse-grained attributes would lose the diversity of models. Specifically, we heuristically select fine-grained attributes in each orthogonal dimension to separate data samples into m scenes, denoted as {$\\Gamma_1^{\\text{sem}}, \\Gamma_2^{\\text{sem}},..., \\Gamma_m^{\\text{sem}}$}. For instance, as for driving images, we define semantic scenes according to 120 combinations of attributes in three dimensions, i.e., {clear, overcast, rainy, snowy, foggy} in weather, {highway, urban, residential, parking lot, tunnel, gas station, bridge, toll booth} in location and {daytime, dawn/dusk, night} in time\u00b9.\n2) Compressed Model Training: We employ a training strategy, integrating both semantic similarity and feature sim-ilarity of data samples to train diverse compressed models, which consists of the following two steps, as described in Algorithm 1."}, {"title": "B. Adaptive Scene Sampling", "content": "To obtain {$\\Psi_1^{\\text{sub}}, \\Psi_2^{\\text{sub}},..., \\Psi_n^{\\text{sub}}$}, a straightforward idea is to randomly pick a number of samples X from D and test Mi for i\u2208 [1,n]. If a Mi can achieve satisfactory prediction accuracy on sample $x \\in X$, x belongs to $\\Psi_i^{\\text{sub}}$. As {$\\Psi_1, \\Psi_2,..., \\Psi_n$} may be biased in D, such random sampling algorithm also generates unbalanced {$\\Psi_1^{\\text{sub}}, \\Psi_2^{\\text{sub}},..., \\Psi_n^{\\text{sub}}$}.\nTo solve the unbalanced sampling problem, however, is not intuitive, because of Proposition 1. Proposition 1 holds that we can not know a sample belongs to which distribution from all the distributions those models can characterize (i.e., {$\\Psi_1, \\Psi_2,..., \\Psi_n$}) without high computational cost experi-ments. In order to obtain a balanced {$\\Psi_1^{\\text{sub}}, \\Psi_2^{\\text{sub}},..., \\Psi_n^{\\text{sub}}$} at a low computation cost, we design an adaptive sampling algorithm based on Thompson sampling [19].\nSpecifically, in the k-th sampling round for k \u2208 N, we first examine if the training set $\\Gamma_i$ of Mi for i \u2208 [1, n] has been well sampled by checking\n$S_i > \\frac{\\log (1 - \\theta)}{\\log(1-\\delta)}$,\nwhere Si is the set of samples sampled from $\\Gamma_i$; \u03b8 is the confidence of being well sampled; and |\u00b7| is the number of elements in a set.\nThen, for each training set $\\Gamma_i$ that has not been well sampled, we estimate a sampling probability $p_i^k$ based on a Beta distribution $Beta(\\alpha_i^{k-1}, \\beta_i^{k-1})$, where $\\alpha_i^{k-1}$ and $\\beta_i^{k-1}$ are the two parameters of the Beta distribution of $\\Gamma_i$, updated in the previous round. As a result, the training set $\\Gamma_k$ with the highest sampling probability will be sampled.\nFinally, all $Beta(\\alpha, \\beta)$ will be updated as follow:\n$Beta(\\alpha_i^k, \\beta_i^k) = \\begin{cases} Beta(\\alpha_i^{k-1} + 1, \\beta_i^{k-1}), & \\text{if }\\Gamma_i \\text{ is sampled} \\\\ Beta(\\alpha_i^{k-1}, \\beta_i^{k-1} + 1), & \\text{otherwise.} \\end{cases}$\nThis procedure repeats until a specific number of K samples are collected. Figure 3 shows the normalized Si for all the Mi for i \u2208 [1, n] where n = 16, using the random sampling algorithm and our adaptive sampling algorithm, respectively. It can be seen that our adaptive sampling algorithm can effectively mitigate the unbalanced sampling problem."}, {"title": "C. Training Decision Model", "content": "Given the sampling results {$\\Psi_1^{\\text{sub}}, \\Psi_2^{\\text{sub}},..., \\Psi_n^{\\text{sub}}$}, we train an end-to-end decision model $M_{\\text{decision}}$ to effectively represent and distinguish {$\\Psi_1^{\\text{sub}}, \\Psi_2^{\\text{sub}},..., \\Psi_n^{\\text{sub}}$} by employ-ing a parameter-frozen scene representation network Mscene and neural-network-based classifier.\nSpecifically, we use Mscene as a backbone neural network to obtain scene representation, denoted as hi, for every data sample $x_i \\in \\Psi_i^{\\text{sub}}, i \\in [1, n]$. In this way, he will retain the scene-related information. The model decision here can be formulated as a multi-class classification problem. The label of x for decision model training is a vector, referred to as a model allocation vector v = {$v_i, i \\in [1,n]$}, where the i-th element $v_i$, denotes whether $x \\in \\Psi_i^{\\text{sub}}$. The cross entropy loss function [20] is used for training the decision model. Note that during the training of decision model $M_{\\text{decision}}$, the parameter of Mscene is frozen to improve training efficiency and enhance the generalization of $M_{\\text{decision}}$ [21]."}, {"title": "V. ONLINE MODEL INFERENCE", "content": "A. Model Selection Strategy\nGiven the set of pretrained models {M1, M2,\u2026, Mn} and decision model Mdecision downloaded from a cloud server, a mobile device needs to select most suitable com-pressed models for online inference. Specifically, it utilizes Mdecision to output the model allocation vector v for a testing sample x, i.e., vx = Mdecision(x), where the i-th element $v_i$ indicates the suitability probability that model Mi is suitable for x. Therefore, we can rank all compressed models according to their suitability probablities for x using v. It should be noted that for the uncertainty of scenario duration, model selection should be conducted on every testing sample, taking into account the fast-changing data distributions in the perspective of compressed models.\nB. Cache-based Model Deployment\nWith the model allocation vector vx = Mdecision(x), compressed models can be dynamically ranked. Due to the re-stricted amount of memory on a mobile device, not all models may be pre-loaded into memory. To deal with this issue, we investigate the best-effort model deployment strategy.\nWe examine the inference latency of detecting objects on five driving video clips, using two DNN models of different"}, {"title": "VI. EVALUATION", "content": "A. Methodology\n1) Datasets: We evaluate Anole on a typical mobile infer-ence task, i.e., vehicle detection on driving videos (VD), based on the following datasets and real-world experiments.\nKITTI [23]: comprises 389 stereo and optical flow im-age pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). For online object detection, KITTI consists of 21 training sequences and 29 test sequences.\nBDD100k [24]: contains over 100k video clips regarding ten autonomous driving tasks. Clips of 720p and 30fps were collected from more than 50 thousand rides in New York city and San Francisco Bay Area, USA. Each clip lasts for 40 seconds and is associated with semantic attributes such as the scene type (e.g., city, streets, resi-dential areas, and highways), weather condition and the time of the day.\nSHD: contains 100 driving video clips of one minute recorded in March 2022 with a 1080p dashcam in Shang-hai city, China. Clips were collected from ten typical scenarios, including highway, typical surface roads, and tunnels, at different time in the day. LabelImg [25] is employed to label objects in all images.\nWe random select 10 video clips from KITTI, 44 clips from BDD100k, and 10 clips from SHD, forming a dataset of 64 video clips containing 16,145 image samples in various scenarios. Figure 5 shows the cumulative distribution functions\n\u2022\n\u2022\n\u2022"}, {"title": "B. Effect of Scene Profiling Models", "content": "1) Scene Encoder Mscene: We first test Mscene on clas-sifying scenes on the validation set of seen scenes. Scenes are defined based on the multi-level clustering results. Fig-ure 6(a) shows the scene classification confusion matrix of scene encoder Mscene on the validation set. It can be seen that Mscene works well among almost all scenes. There also exist some exceptional scenes that are confusing to Mscene. We merge similar scenes in the feature space before training compressed models.\n2) Decision Model Mdecision: We evaluate the ability of Mdecision in selecting the top-one model on the validation set of seen data. Figure 6(b) show the confusion matrix of the Mdecision models predicting best models versus true best models. It can be seen that Mdecision have basic model selection ability. This is because the decision of model se-lection is based on the well-trained Mscene, with one scene corresponding to a group of suitable models. We can also see that Mdecision may make mistakes on some models,. This is because the top one model may not be significantly better than other models."}, {"title": "C. Effect of Cache-based Model Update Strategy", "content": "To effectively evaluate the effect of our cache-based model update strategy, we synthesize six fast-changing video clips, denoted as T1-T6. Specifically, for each synthesized video clip, we randomly select 5 clips from the 64 clips in the dataset. For each selected clip, we randomly cut a video segment of 100 frames (from the testing set for a seen clip) and then splice all video segments, resulting a synthesized video clip of 500 frames. We then conduct model inference using Anole on T1-T6.\n1) Scene Duration: Figure 7(a) plots the boxplot of scene duration measured as the number of frames without model switching on all six synthesized video clips. It can be seen that scenes change rapidly in the perspective of Mdecision, with over 80% of scenes lasting fewer than 40 frames and the mean scene duration less than 20 frames.\n2) Cache Miss Rate: Figure 7(b) depicts the cache miss rate and the F1 score as functions of cache size in the unit of compressed model size. It can be seen that a cache capable of loading up to 5 models can sustain a low cache miss rate and a stable inference accuracy. This observation aligns with the observation of the long-tail model utility distribution as shown in Figure 4(b). It is also observed that the inference accuracy remains satisfactory even for a cache size of 2 models, demonstrating the feasibility of Anole on devices with extremely limited GPU memory."}, {"title": "D. Cross-scene Experiments", "content": "In this experiment, we investigate the performance of all candidate methods cross fast-changing scenes, using samples"}, {"title": "E. New-scene Experiments", "content": "In this experiment, we examine the performance of all candidate methods in new scenes, using unseen data. Partic-ularly, six unseen video clips include one clip from KITTI with attributes of {Street, Day}, four scenes from BDD100k with attributes of {Urban, Night}, {Urban, Day}, {Highway, Dusk}, and {Street, Night}, and one scene from SHD with"}, {"title": "F. Real-world Experiments", "content": "As depicted in Figure 9(a), we implement all methods on the Nvidia Jetson TX2 NX connected with a 1080p HD camera to conduct real-world experiments in Shanghai city. Well-trained compressed models and the decision model are downloaded to the Jetson device. We conduct real-world experiments in seven driving scenarios with different road conditions and different time in a day. LabelImg [25] is used to label all"}, {"title": "G. Inference Latency", "content": "We evaluate the inference latency of the decision model Mdecision and compressed models on different mobile de-vices. The results are shown in Table IV. The results reveal that YOLOv3-tiny exhibits significantly lower latency when compared to deep YOLOv3, which is generally deemed un-suitable for deployment on devices. For instance, the latency of YOLOv3-tiny on Jetson Nano is 87.9% lower than that of YOLOv3. This highlights the substantial potential for accelerating inference using shallow models. It is also evident that Mdecision can be executed in real-time on embedded mobile devices such as Jetson Nano, with a latency as low as 23.2 ms, making it suitable for online inference applications."}, {"title": "H. Memory and Power Consumptions", "content": "We investigate the memory consumption of different models from the following two aspects, i.e., loading model only, and the memory consumption during inference with a batch size of 1. Table IV demonstrates that memory consumption for loading model is significantly lower than that during inference, owing to the presence of hidden parameters during inference. We also examine the impact of different power configurations adopted by Jetson TX2 NX to the performance of Anole. The power consumption and inference speed of Anole and baselines under different power modes are shown in Figure 11, respectively. Anole achieves a 45.1% reduction in power"}, {"title": "VII. RELATED WORK", "content": "A. DNN Prediction on Mobile Devices\nTo perform DNN inference on mobile devices, new DNNs are specially designed [5]\u2013[8] or existing DNNs are com-pressed to match the computing capability of a mobile de-vice [9], [10]. First, model structure can be optimized to reduce complexity [5], [6], [30]. Second, quantization precision can be reduced to minimize computational cost, e.g., use integers instead of floating-point numbers [31], [32]. Third, the neu-ral network model can also be accelerated by pruning, i.e., deleting some neurons in the neural network [9], [10], [33]. Scene information is also utilized for model compression on edge/mobile devices [34]\u2013[37]. Finally, model distillation can distill the knowledge of large models into small models [7], [8]. Such schemes ensure real-time model inference at the expense of compromised accuracy.\nAnother direction is to divide DNNs and perform collab-orative inference on both edge devices and the cloud [11], [12], [38]\u2013[41], or to transmit compressed sensory data to the cloud for data recovery and model inference [14], [15]. Neurosurgeon [12] partitions the computation of each DNN inference task in a layer granularity. CLIO [42] addresses the instability of network conditions and optimizes infer-ence under different network states. These approaches need coordination with the cloud for each inference, leading to unpredictable inference delays when the communication link is unstable or disconnected. However, they prove inadequate for cross-scene mobile inference scenarios where even deep models are unable to cope.\nB. Cross-scene DNN Prediction\nData-driven machine learning models face challenges in maintaining robust inference performance when dealing with cross-scene inference [43]. One natural approach for scene partitioning is to partition the scene based on prior knowledge or historical samples. First, based on prior knowledge, a simi-larity graph is constructed to cluster similar domains together"}, {"title": "C. Mixture of Experts", "content": "In recent years, we have witnessed the success of Mixture of Experts (MoE) [48], [49], especially in efficient training of large language models (LLM). MoE employs multiple experts for model training, each for one domain. Then, a gate network will be used to determine the correspondence between samples and experts. Though inspired by MoE, Anole differs from MoE in the following 2 aspects. First, experts in MoE are diversified by constraints of losses, but they themselves cannot be related to the scene. In fact, the main purpose of MoE is to expand the number of model parameters, rather than to customize and select scene-specific models. Second, MoE is just a model architecture, and models based on MoE architecture still need to deploy the entire model during deployment. Therefore, MoE-based models often require a significant amount of memory, which is unacceptable for mobile agents like UAVs. In contrast, Anole employs multiple compressed models for online model inference, each designed for one scene. Only a few compressed models are needed to be deployed during online inference. Therefore, Anole is more suitable for mobile devices only with limited resources."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we have proposed Anole, an online model inference scheme on mobile devices. Anole employs a rich set of compressed models trained on a wide variety of human-defined scenes and offline learns the implicit mode-defined scenes characterized by these compressed models via a deci-sion model. Moreover, the most suitable compressed models can be dynamically identified according to the current testing samples and used for online model inference. As a result, Anole can deal with unseen samples, mitigating the impact of OOD problem to the reliable inference of statistical models. Anole is lightweight and does not need network connection during online inference. It can be easily implemented on various mobile devices at a low cost. Extensive experiment results demonstrate that Anole can achieve the best inference accuracy at a low latency."}]}