{"title": "AUGMENT, DROP & SWAP: IMPROVING DIVERSITY IN LLM CAPTIONS FOR EFFICIENT MUSIC-TEXT REPRESENTATION LEARNING", "authors": ["Ilaria Manco", "Justin Salamon", "Oriol Nieto"], "abstract": "Audio-text contrastive models have become a powerful approach in music representation learning. Despite their empirical success, however, little is known about the influence of key design choices on the quality of music-text representations learnt through this framework. In this work, we expose these design choices within the constraints of limited data and computation budgets, and establish a more solid understanding of their impact grounded in empirical observations along three axes: the choice of base encoders, the level of curation in training data, and the use of text augmentation. We find that data curation is the single most important factor for music-text contrastive training in resource-constrained scenarios. Motivated by this insight, we introduce two novel techniques, Augmented View Dropout and TextSwap, which increase the diversity and descriptiveness of text inputs seen in training. Through our experiments we demonstrate that these are effective at boosting performance across different pre-training regimes, model architectures, and downstream data distributions, without incurring higher computational costs or requiring additional training data.", "sections": [{"title": "1. INTRODUCTION", "content": "Music-text embedding models have become a cornerstone of music information retrieval (MIR), facilitating core tasks that underpin music organisation and search, such as music tagging and cross-modal retrieval [8, 11, 14, 26, 27]. At a high level, these are multimodal models that produce aligned audio-text representations by learning to project high-dimensional data from the audio and text modalities onto a lower-dimensional joint representation space whose structure encodes semantic similarity. The canonical learning framework to obtain such embeddings is dual-encoder multimodal contrastive learning, first popularised by CLIP [20] in the image domain, and soon after adopted in most areas of machine perception, including audio [9, 29] and music processing [8, 11, 14]."}, {"title": "2. STUDYING THE DESIGN SPACE OF MUSIC-TEXT EMBEDDING MODELS", "content": "We explore two major factors in the design of music-text embedding models: architecture and data. While we acknowledge that there are others, such as training procedure, and alternative designs, we choose to restrict our focus exclusively to these two axes and to dual-encoder models, due to their predominance in the field. In the rest of the paper, we always refer to this family of models when discussing music-text embeddings or music-text models, and interchangeably use the terms text and language.\n\nThe typical design of a music-text embedding model consists of the following components: two modality-specific base encoders which separately process inputs of the text and audio modality to an intermediate representation space; a fusion or projection module responsible for mapping the intermediate representations to the shared embedding space; and a contrastive loss, through which the model parameters are optimised to encode semantically related audio and text inputs within the same neighbourhood of the embedding space, while pushing apart unrelated items. We provide an overview of this design in Figure 1. While prior works have converged towards standard choices for the last two components, it remains unclear how to reliably choose unimodal encoders among several existing options. We look at this in Section 3, before discussing the role of training data in Section 4."}, {"title": "2.1 Our experimental approach", "content": "Before delineating our areas of focus, we outline here the standard experimental setup used in our experiments.\n\nProjection module We design our experiments to compare variations of the dual-encoder contrastive architecture described above, varying several components, but keeping two fixed throughout: the projection module and the loss. Similarly to [16, 24], we adopt a two-head, two-layer Transformer as our projection module. From a sequence of 256-dimensional embeddings produced by each projection head, we employ the [CLS] token embedding as the global representation for each branch. For ease of reference, we denote this model architecture by DuET-MC (Dual-Encoder Text-Music Contrastive).\n\nTraining We optimise our network via the multimodal formulation of the InfoNCE loss [19], using cosine similarity between the l2-normalised projection embeddings from the audio and text branch as our scoring function, and a temperature parameter of 0.03. As part of our training procedure, we use the Adam optimizer with decoupled weight decay of 0.05, varying our learning rate through a cosine decay schedule from its peak value of 1e-3, after a linear warm-up of 5 epochs. We train on 8 A100 NVIDIA GPUs, with an effective batch size of 1024 or 2048 based on memory requirements, for a maximum of 100 epochs, with early stopping based on the validation loss. Unless otherwise specified, our default training data is a corpus of licensed instrumental music with high-quality, manually curated genre, mood, and instrument tags, which we refer to as MusicTextHQ. For training, we select a subset to-talling a duration of 100 hours, and augment tags into captions following our data augmentation strategy described in Section 4."}, {"title": "2.2 Evaluation", "content": "We evaluate all our models on text-based music retrieval, as this represents the most prominent task for music-text embedding models and has been shown to correlate to performance on other tasks [11, 14]. Retrieval is performed by ranking all audio clips in the dataset by decreasing cosine similarity of their embedding with the embedding of a text query. From this, we compute Recall@k (R@k), the average number of times the target appears within the top-k retrieved items, and Median Rank (MR). To normalise performance scores by the different dataset sizes, we repeat this procedure on random subsets of 500 items, and report the average value for each metric. When reporting a single metric, we always refer to R@10.\n\nDatasets In order to robustly measure performance across our experiments, we adopt a multi-dataset evaluation suite comprising three public datasets containing audio tracks paired with human-written captions: YT8M-"}, {"title": "3. THE ROLE OF ENCODER BACKBONES", "content": "We experiment with two audio encoders, HTS-AT [5] and MERT [33], and three text encoders, RoBERTa [13], the text encoder from CLIP [20] (CLIP-T), T5 [21] and mT5 [30]. We choose these either because they represent the state of the art in their respective tasks, or because they have been previously used in contrastive audio-text learning, thus allowing for direct comparison with prior work."}, {"title": "3.1 Encoders: initialization and freezing", "content": "In this set of experiments, our goal is to study parameter-efficient configurations of existing audio and text encoders, training only a subset of the model weights. The motivation for exploring this setting is threefold: freezing part of the model lowers the memory budget and training time, it avoids catastrophic forgetting [17], and it reduces the risk of overfitting in data-constrained scenarios. To fulfil these requirements, we do not consider end-to-end finetuning, and instead focus on leveraging pre-training, locking the audio and text encoders based on their parameter size."}, {"title": "3.2 Supporting retrieval in multiple languages", "content": "Due to a lack of data in different languages, music-text modelling has so far exclusively focussed on English. Real-world applications for music-text embeddings, however, can greatly benefit from the support of multiple languages. To address this limitation, we explore the use of pre-trained locked encoders, similarly to Section 3.1, this time adopting mT5 [30], a multilingual text-to-text Transformer model, as our text encoder. To evaluate multilingual performance, we choose a subset of four languages, German, French, Italian and Spanish, and translate our evaluation datasets via GPT3.5-turbo [4]. In Table 3, we show that this approach provides a viable solution to text-based retrieval in multiple languages while using only English text paired with music in training and with only a minor drop in performance compared to English."}, {"title": "4. THE ROLE OF TRAINING DATA", "content": "Having established best practices with respect to choosing audio and text backbones, we now shift our attention to the training data. As widely acknowledged in the literature [7, 8, 14], a major limitation in training music-language models is the lack of large public datasets with paired audio-text data. To circumvent this issue, a number of works have proposed to employ large language models to augment text data more commonly found in music datasets, such as categorical labels, metadata and tags, into full natural language sentences, corresponding to pseudo-captions [7, 10, 16]. In the next section we present our investigation of the impact of tag-to-caption augmentation."}, {"title": "4.1 Tag-to-caption augmentation via LLMS", "content": "Following [16], we leverage the in-context learning ability of LLMs via few-shot prompting, and adopt a similar approach to augment tags into captions for our training dataset MusicTextHQ. For this, we use BLOOM-176B [23], a competitive open-access LLM trained on responsibly sourced data. Differently from [16], we do not employ synthetic tags, but use tags provided by expert annotators. We compare this to training on LP-MusicCaps-MTT [7] (LP-MusicCaps for short), a dataset obtained via a similar approach, where tags from the MagnaTagATune [12] dataset are augmented into captions via GPT3.5-turbo. To measure the impact of tag-to-caption augmentation, we train three variants of our model on each dataset, varying $P_{cap}$, the probability of selecting captions over tags as the text input for each training pair."}, {"title": "4.2 Training data: size vs quality", "content": "Next, we ask whether simply increasing dataset size can emphasise the benefits of tag-to-caption augmentation. To scale up the size of our training data, we include YT8M-MV, a subset of the YouTube8M dataset [1] tagged as music video, as an additional dataset to our training pool. For this, we follow [16] and employ tags from an automatic"}, {"title": "5. IMPROVING DIVERSITY VIA TEXT AUGMENTATIONS", "content": "Having established that augmenting high-quality tags into captions offers a useful and inexpensive strategy to enrich training data, we explore this further and propose two augmentation-based techniques aimed at increasing data diversity and model robustness."}, {"title": "5.1 Augment, Drop & Swap", "content": "Augmented View Dropout First, building upon the tag-to-caption strategy described in Section 4.1, we explore text augmentation with the goal of constructing more effective views for contrastive learning, following the principle that optimal views should minimise mutual information between paired items while retaining a high degree of semantic alignment [25]. To this end, we propose Augmented View Dropout, where, for each item in our dataset, we randomly sample a subset of the tags, balanced by category (genre, mood, instrumentation) and produce a set of 10 different captions. Each can be thought of as a complementary, but partial view of the associated music track, as we mask a subset of all the ground-truth tags to produce each view. At training time, views are randomly sampled, effectively resulting in a further form of data augmentation.\n\nHard negatives via TextSwap Finally, we tackle another important challenge in contrastive learning, hard negative sampling, and propose to also address this through the lens of text augmentation, via a technique which we call TextSwap. In order to increase the rate of hard negatives beyond the natural rate found in the dataset, we create partially perturbed versions of the captions by stochastically swapping genre, mood or instrument keywords with alternative descriptors from a predefined dictionary (e.g. \u201ca mellow pop track\u201d becomes \u201ca mellow hip-hop track\u201d)."}, {"title": "5.2 Experiments", "content": "Ablations In this set of experiments we examine the effect of each of the three components in our augmentation pipeline: tag-to-caption augmentation, Augmented View Dropout and TextSwap. We look at two scenarios: one where we want to measure their contribution in training two variants of our parameter-efficient DuET-MC framework, each with different degrees of audio pre-training and finetuning and with locked text encoders, and one where we relax our computational requirements and explore whether our proposed method can be usefully applied in finetuning a general purpose audio-text embedding model (CLAP [29]), with limited paired music data.\n\nResults We present our ablations on the proposed pipeline in Table 4, where we also compare to two audio-text contrastive baselines, CLAP [29] and TTMR [8], trained on general-purpose audio and music respectively. The table displays three different settings to which we apply our proposed pipeline: (1) training the audio encoder from scratch (shown in the HTS-AT + CLIP-T configuration), (2) training only 1% of the parameters in our locked audio-text encoder (MERT + CLIP-T), and (3) fine-tuning the full model on music, following general audio-text pre-training (CLAP-FT). From this, we observe that, while the vanilla version of DuET-MC (trained only on tags) exhibits at best comparable performance to the baselines, each additional component in our pipeline lifts performance across all model configurations, pre-training regimes and finetuning strategies. Among these, tag-to-caption augmentation"}, {"title": "6. CONCLUSIONS", "content": "In this work we presented Augment, Drop & Swap, a training recipe for efficient music-text representation learning informed by our findings on training music-text contrastive models in resource-constrained scenarios. Through our experiments, we provide a practical guide to this family of models, and foreground their real-world use by focusing on multilingual support, computationally efficient techniques, and cross-dataset evaluation. Showing that data curation has a significant effect at modest data scales, we design each step in our pipeline to tackle specific aspects of the text used in training, such as descriptiveness and specificity, via data augmentations, leading to views that are more effective in multimodal contrastive learning. Through automatic and qualitative evaluations, we show the usefulness of our approach and reveal insights on the relation between measured performance and distribution shifts in the test data."}]}