{"title": "Lightweight yet Fine-grained: A Graph Capsule Convolutional Network with Subspace Alignment for Shared-account Sequential Recommendation", "authors": ["Jinyu Zhang", "Zhongying Zhao", "Chao Li", "Yanwei Yu"], "abstract": "Shared-account Sequential Recommendation (SSR) aims to provide personalized recommendations for accounts shared by multiple users with varying sequential preferences. Previous studies on SSR struggle to capture the fine-grained associations between interactions and different latent users within the shared account's hybrid sequences. Moreover, most existing SSR methods (e.g., RNN-based or GCN-based methods) have quadratic computational complexities, hindering the deployment of SSRs on resource-constrained devices. To this end, we propose a Lightweight Graph Capsule Convolutional Network with subspace alignment for shared-account sequential recommendation, named LightGC2N. Specifically, we devise a lightweight graph capsule convolutional network. It facilitates the fine-grained matching between interactions and latent users by attentively propagating messages on the capsule graphs. Besides, we present an efficient subspace alignment method. This method refines the sequence representations and then aligns them with the finely clustered preferences of latent users. The experimental results on four real-world datasets indicate that LightGC2N outperforms nine state-of-the-art methods in accuracy and efficiency.", "sections": [{"title": "Introduction", "content": "Sequential Recommender systems (SRs) strive to provide users with personalized content (Ma et al. 2024), products (Yue et al. 2023), or services (Li et al. 2024) based on their sequential preferences. Most SRs are under an ideal assumption that each account is merely associated with one single user (Verstrepen and Goethals 2015). In real-world scenarios, many users prefer to share their accounts with family members or close friends (Jiang et al. 2018). As illustrated in Figure 1, multiple family members (i.e., latent users) utilize a shared video account. Clearly, their viewing histories reveal distinct preferences, yet they're blended into an account-level hybrid sequence. Distinguishing the diverse preferences of latent users while providing account-level sequential recommendations emerges as an appealing yet challenging task, i.e., the Shared-account Sequential Recommendation (SSR)."}, {"title": "Related Work", "content": "Sequential Recommender systems (SRs) predict users' next interactions based on their sequential preferences (Chen et al. 2024). Early researches utilized Markov chains to address the sparsity issues in SR tasks (He and McAuley 2016; Cai, He, and McAuley 2017), but they fail to capture the dynamics of user preferences. Subsequently, researchers begin to explore deep neural networks for SRs, including RNN-based methods (Quadrana et al. 2017), GNN-based methods (Fan et al. 2021), attention-based methods (Kang and McAuley 2018; Shin et al. 2024; He et al. 2018), and contrastive learning-based methods (Xie et al. 2022a). These deep learning-based SR methods excel in capturing dynamic sequential patterns of users. Nevertheless, they typically assume that each account is associated with a single user (Guo et al. 2023), thus failing to provide accurate recommendations for shared accounts.\nShared-account Sequential Recommendation\nShared-account Sequential Recommender systems (SSRs) aim to identify the diverse preferences of latent users while providing personalized recommendations for shared accounts (Verstrepen and Goethals 2015). Early SSR research utilized RNN-based methods (Ma et al. 2019; Sun et al. 2023) to identify latent users, employing GRU units to filter out information from each account. However, these methods suffer from gradient vanishing issues with long sequences. Subsequently, the graph-based SSR methods with attention networks are proposed (Guo et al. 2021, 2024), which propagates user-specific messages to identify latent users. These graph-based methods have achieved remarkable performance on SSR. However, they didn't consider the fine-grained associations between interactions in sequences and latent users. Besides, their high computational complexity hinders the deployment of SSRs on resource-constrained edge devices (e.g., smartphones or tablets)."}, {"title": "Methodology", "content": "Preliminaries\nNotations. Suppose that $I = {I_1, I_2, ..., I_t, ..., I_m}$ is the set of items, where $I_t$ denotes the t-th item. The set of shared accounts is denoted as $A = {A_1, A_2,..., A_k, ..., A_n}$, where $A_k$ represents the k-th shared account. Moreover, the set of sequences is denoted as $S = {S_1, S_2, ..., S_k,...,S_n}$, where $S_k$ denotes the hybrid sequence of the shared account $A_k$. Suppose that each account contains a latent users, e.g., $A_k = {U_{k,1},..., U_{k,h},..., U_{k,a}}$, where $u_{k,h}$ denotes the h-th latent user in the account $A_k$.\nProblem Definition. Given $S_k$ and $A_k$, the task of SSR is to recommend the next item $I_{t+1}$ that $A_k$ is most likely to consume, based on the account's hybrid sequence $S_k$. The probabilities of all recommendation candidates are represented as:\n$P(I_{t+1}|S_k, A_k) \\sim f(S_k, A_k)$, (1)\nwhere $P(I_{t+1}|S_k, A_k)$ denotes the probability of recommending $I_{t+1}$ to $A_k$ given its historical hybrid sequence $S_k$, and $f(S_k, A_k)$ is the function designed to estimate the probability.\nFramework of LightGC2N\nAs shown in Figure 2, the LightGC2N consists of four key components: 1) sequential graph construction, 2) graph capsule convolutional network, 3) subspace alignment, and 4) final prediction. The details are given below."}, {"title": "Sequential Graph Construction", "content": "The sequential graphs are constructed as the input for the graph capsule convolutional network. During the graph construction, two types of associations are considered: account-item interactive relations and sequential dependencies between items. We define the sequential graphs as $G = {V,E}$, where V is the set of nodes, and $\\mathcal{E}$ is the set of edges. Each edge in $\\mathcal{E}$ denotes a relation between nodes. Specifically, the adjacency matrix $M \\in \\mathbb{R}^{(m+n) \\times (m+n)}$ of the sequential graph G is denoted by Eqn. (2).\n$M = \\begin{bmatrix} M_s & M_I \\\\ M_I^T & 0 \\end{bmatrix}$ (2)\nwhere $M_s \\in \\mathbb{R}^{m \\times m}$ denotes the adjacency matrix carrying the sequential relationships between items, and each entry $M_{ij} = 1$ if item $I_j$ is the prior of item $I_i$ in the input sequences; $M_{ij} = 0$ otherwise. $M_I \\in \\mathbb{R}^{m \\times n}$ represents the adjacency matrix containing the interactive relationships between accounts and items, each entry $M_{kl} = 1$ if account $A_k$ has interaction with item $I_l$; $M_{kl} = 0$ otherwise."}, {"title": "Graph Capsule Convolutional Network (GC2N)", "content": "We design the GC2N is to capture the fine-grained preferences of each latent user within a shared account. The inputs to GC2N are the associations among nodes from the initialized sequential graphs and the embeddings at the 0-th layer, i.e., $E_I^{(0)} \\in \\mathbb{R}^{m \\times d_1}$ for all items and $E_A^{(0)} \\in \\mathbb{R}^{n \\times d_1}$ for all accounts.\nPrimary Capsule Graph Construction. In this component, node embeddings are projected into high-dimensional capsule spaces to construct the primary capsule graphs, thereby exploring the fine-grained differences in preferences between latent users. Specifically, this component utilizes linear attention mechanism (Katharopoulos et al. 2020) to project item embeddings $E_I^{(0)}$ into item capsules $C_I^{(0)} \\in \\mathbb{R}^{m \\times d_2}$, where the $E_I^{(0)}$ is treated as Query, Key and Value during the calculation. In contrast to the self-attention mechanism, the linear attention mechanism first calculates the outer product between the Key matrix and the Value matrix to obtain the attention map, and then calculates the correlations between the map and the Query matrix. Since the linear attention changes the calculation orders, it achieves lower computational complexity (i.e., $O(N \\times d^2)$) while capturing global associations among items. The calculation of the linear attention mechanism is formulated as Eqn. (3).\n$C_I^{(0)} = Q \\left( softmax \\left( \\frac{KT V}{\\sqrt{d_1}} \\right) \\right) W_i + b_i$, (3)\nwhere $W_i \\in \\mathbb{R}^{d_1 \\times d_2}$ is a dimension transformation matrix, and $b_i \\in \\mathbb{R}^{1 \\times d_2}$ is the bias term.\nSince there are no sequential relationships between accounts, it does not need to use an attention-based method for account representations $E_A^{(0)}$. Instead, GC2N leverages point-wise Conv1D to project them into capsule-form $C_A^{(0)} \\in \\mathbb{R}^{n \\times a \\times d_2}$. In the point-wise Conv1D, both the kernel size and stride are set to 1 for linear dimensional transformation (Wu et al. 2023). The calculation is denoted as Eqn. (4).\n$C_A^{(0)} = E_A^{(0)} * W_c + b_c$, (4)\nwhere * represents the convolutional operation, $W_c \\in \\mathbb{R}^{d_1 \\times a \\times d_2}$ denotes the kernel of Conv1D, $b_c \\in \\mathbb{R}^{1 \\times a \\times d_2}$ represents the bias, and a is a hyper-parameter that controls the number of latent users within shared accounts."}, {"title": "Fine-grained Message Propagation on capsule graphs.", "content": "Subsequently, each account capsule $C_A^{(0)} \\in \\mathbb{R}^{a \\times d_2}$ is split into a latent user capsules $C_{u_{k,h}}^{(0)} \\in \\mathbb{R}^{d_2}$. Then, the model is able to match the interactions to the specific latent users within an account by attentively calculating the correlations between their capsules, which facilitates the fine-grained distinction of the preferences for latent users.\nFine-grained Message Propagation on capsule graphs. Taking the shared-account $A_k$ as an example. The capsule embedding of the h-th latent user in $A_k$ is denoted as $C_{u_{k,h}}$. To realize fine-grained message passing on the primary capsule graphs, GC2N calculates the correlations $\\alpha_{I_i}$ between items $C_{I_i}$ and user $C_{u_{k,h}}$ as:\n$\\alpha_{I_i} = \\frac{exp (C_{u_{k,h}} \\cdot C_{I_i})}{\\sum_{I_j \\in \\mathcal{N}_{A_k}} exp (C_{u_{k,h}} \\cdot C_{I_j})}$, (5)\nwhere $\\mathcal{N}_{A_k}$ is the set of all the interacted items of $A_k$.\nSuch an attentive calculation facilitates the fine-grained matching between interactions and latent users. Then, the messages propagated to $u_{k,h}$ at the l-th layer are denoted as:\n$m_{u_{k,h}I_j}^{(l)} = W_1^{(l)} C_{I_j}^{(l-1)} + (\\alpha_{I_j}^{(l-1)} C_{I_j}^{(l-1)})$, (6)\nwhere $m_{u_{k,h}I_j}^{(l)}$ denotes the passed message, $W_1^{(l)}$ denotes the learnable weights that controls how much information should be passed from neighboring item $I_j$, and $W_2^{(l)}$ is another learnable weighting matrix that controls the participation of correlations.\nWe also add self-connections to retain the independent characteristics of $C_{u_{k,h}}^{(l)}$, which is formulated as:\n$m_{u_{k,h}u_{k,h}}^{(l)} = W_3^{(l)} C_{u_{k,h}}^{(l-1)}$, (7)\nwhere $m_{u_{k,h}u_{k,h}}^{(l)}$ is the retained information of the user capsule from (l \u2013 1)-th layer, $W_3^{(l)}$ is the learnable parameter that controls how much information of $C_{u_{k,h}}^{(l-1)}$ should be retained.\nHence, the capsule representation $\\hat{C}_{u_{k,h}}^{(l)}$ of latent user $u_{k,h}$ is updated by Eqn. (8).\n$\\hat{C}_{u_{k,h}}^{(l)} = \\sum_{I_j \\in \\mathcal{N}_{u_{k,h}}} m_{u_{k,h}I_j}^{(l)} + m_{u_{k,h}u_{k,h}}^{(l)}$, (8)\nwhere $\\mathcal{N}_{u_{k,h}}$ denotes the set of all items interacted by $u_{k,h}$.\nSimilarly, the message propagated to item capsule $C_{I_j}$ at l-th layer is represented by Eqn. (9):\n$m_{I_j u_{k,g}}^{(l)} = W_4^{(l)} C_{u_{k,g}}^{(l-1)}$,\n$m_{I_j I_{j-1}}^{(l)} = W_5^{(l)} C_{I_{j-1}}^{(l-1)}$, (9)\nwhere $u_{k,g}$ represents a latent user who has interacted with $I_j$, $m_{I_j u_{k,g}}^{(l)}$ is the message passed from $u_{k,g}$ to $I_j$, and $I_{j-1}$ is the neighboring item of $I_j$, $m_{I_j I_{j-1}}^{(l)}$ denotes the message passed from $I_{j-1}$ to $I_j$, $W_4^{(l)}$ and $W_5^{(l)}$ are learnable weights."}, {"title": null, "content": "Then, the capsule representation $\\hat{C}_{I_j}^{(l)}$ of item $I_j$ is updated by Eqn. (10).\n$\\hat{C}_{I_j}^{(l)} = \\sum_{u_{k,g} \\in \\mathcal{N}} m_{I_j u_{k,g}}^{(l)} + \\sum_{I_{j-1} \\in \\mathcal{N}} m_{I_j I_{j-1}}^{(l)}$ (10)\nwhere $\\mathcal{N}$ is the set of all latent users who have interactions on item $I_j$, and $\\mathcal{N}$ denotes the set of all neighboring items of $I_j$.\nBy adopting layer-wise message aggregation, the final representations of $I_j$ and $u_{k,h}$ are denoted as follows:\n$E_{I_j} = \\sum_{l=0}^{L} \\hat{C}_{I_j}^{(l)}$, $E_{u_{k,h}} = \\sum_{l=0}^{L} \\hat{C}_{u_{k,h}}^{(l)}$ (11)\nwhere L is a hyper-parameter that controls the layer number of the graph convolutions on the primary capsule graphs.\nAccount-level Dynamic Routing. The account-level dynamic routing mechanism performs a routing selection to consider the associations between account and its latent users. The strength of the connections between user capsules and account capsules are qualified via a coupling coefficient $b_p$, which is initialized randomly. The dynamic routing is operated iteratively for $\\theta$ times, where $\\theta$ is a hyper-parameter. As a common practice (Zheng et al. 2022), we uniformly set $\\theta$ as 3, maintaining a balance between performance and computational complexity. The dynamic routing at the j-th iteration is represented as Eqn. (12).\n$\\hat{C}_{A_k}^{(j)} = squash \\left( \\sum_{h} b_p^{(j-1)} E_{u_{k,h}} \\right)$ (12)\nwhere $\\hat{C}_{A_k}^{(j)}$ denotes the account capsule for $A_k$, $squash(\\cdot)$ is the squash function which compresses the routed information, ensuring efficient information transmission.\nIn addition, the coupling coefficient $b_p$ at the j-th iteration is updated by calculating the affinity between user capsules and the account capsule:\n$b_p^{(j)} = (E_{u_{k,h}}^{(j-1)}) \\cdot W_a \\left( E_{u_{k,h}} C_{A_k}^{(j)} \\right)$, (13)\nwhere $W_a$ is a learnable weighting matrix, $\\odot$ denotes the element-wise product.\nHence, the final representations of accounts and sequences are denoted as Eqn. (14).\n$E_A = \\sum_{A_k \\in A} \\hat{C}_{A_k}^{(0)}$, $E_S = \\sum_{S_k \\in S} \\sum_{I_j \\in S_k} E_{I_j}$. (14)\nWith the help of the Graph Capsule Convolutional Network (GC2N), the account representation has gained the ability to finely distinguish the preferences of various potential users within shared accounts. However, the diverse preferences within sequence representations remain largely unexplored. Hence, we further devise a subspace alignment method."}, {"title": "Subspace Alignment (SA)", "content": "Subspace alignment is an efficient component that clusters the hybrid preferences of multiple latent users and then aligns them to the sequence representations $E_S$. Instead of using traditional self-representation matrices (Xie et al. 2022b; Zhang et al. 2018), SA exploits low-rank subspace bases to cluster diverse preferences for various latent users. Moreover, it also refines the sequence representations by attaching the subspace affinities, and then aligns them with the original sequence representations via a contrastive learning strategy. This strategy provides additional self-supervised signals to distinguish the preferences of latent users within hybrid sequences.\nSubspace Affinity Calculation. Taking the sequence $S_k \\in S$ as an example, $E_{S_k} \\in \\mathbb{R}^{n \\times d_2}$ denotes its representations. The initialization of subspace bases $D = {d_1, d_2, ..., d_j, ...,d_a} \\in \\mathbb{R}^{a \\times d_2}$ is given by the column space of the clusters generated by K-means on $E_{Sk}$. Then, the subspace affinities are calculated as:\n$s_{ij} = \\frac{||e_i d_j|| + \\frac{d_2}{a}}{\\sqrt{||e_i||^2 + ||d_j||^2 + \\lambda}}$, (15)\nwhere $s_{ij}$ denotes the subspace affinity between i-th item $e_i$ in $S_k$ and j-th subspace base $d_j$, $\\lambda$ is a parameter that controls the smoothness of the calculation ($\\lambda$ is uniformly set to le-4 according to the common practice reported in (Cai et al. 2022)).\nAs the affinity calculation does not rely on the self-expression framework, SA is able to achieve linear computational complexity (i.e., $O(N \\times d^2)$) and low memory consumption, making subspace clustering more efficient.\nContrastive Learning. To align the sequence embeddings with the clustered user preferences, SA first refines the $e_i \\in \\mathbb{R}^{1 \\times d_2}$ by applying the affinities:\n$z_i = \\frac{e_i S_{ij}}{\\sum_i S_{ij}}$ (16)\nwhere $z_i \\in Z_s$ denotes the refined embedding of i-th item in $S_k$ and $Z_s \\in \\mathbb{R}^{n \\times d_2}$ represents the refined representation of $S_k$.\nThen, SA adopts a contrastive learning paradigm that aligns the refined representations $Z_{S_k}$ with the normalized sequence embeddings $\\bar{E}_{S_k}$:\n$L = \\sum_{e_i \\in \\bar{E}_{S_k}} log \\frac{exp(\\bar{e}_i z_i / \\beta)}{\\sum_{j \\in Z_{S_k}} exp(\\bar{e}_i z_j / \\beta)}$, (17)\nwhere $L$ is the InfoNCE loss calculated between representations of $S_k$, $(\\bar{e}_i, z_i)$ is a positive pair while $(\\bar{e}_i, z_j)$ represents a negative pair, $\\beta$ denotes the temperature coefficient that controls the impact from negative pairs to positive pairs. The contrastive loss for all the sequences is:\n$L_C = \\sum_{S_k \\in S} L$. (18)\nSuch a strategy is able to distinguish various preferences of latent users. Therefore, it improves the capability of learning fine-grained sequence representations. The final sequence embeddings are obtained by summing the refined sequence embeddings and their normalized-forms:\n$\\hat{E_S} = \\sum_{S_k \\in S} Norm \\left(E_{S_k} + W_s Z_{S_k} \\right)$, (19)\nwhere $\\hat{E}_S$ denotes the updated sequence embeddings, $W_s$ is the weighting matrix."}, {"title": "Final Prediction", "content": "The final prediction generated by LightGC2N is denoted as:\n$P(I_{t+1}|S, A) = softmax \\left(W_f \\cdot [\\hat{E}_S, E_A] + b_f \\right)$, (20)\nwhere $W_f$ is the transformation matrix that maps the predictions to the dimension of candidate items, and $b_f$ is the bias term that adjusts the threshold of the activation function.\nThe cross-entropy loss function is adopted to optimize the learnable parameters in LightGC2N, which is denoted as:\n$L_S = - \\frac{1}{|S|} \\sum_{I_{t+1} \\in I} log P(I_{t+1}|S, A)$. (21)\nThen, the overall loss function is denoted as:\n$L = L_S + \\gamma L_c$, (22)\nwhere $\\gamma$ is a hyper-parameter that controls the participation of self-supervised signals."}, {"title": "Experiments", "content": "In this section, we first introduce the experimental settings, and then analyze the performance of LightGC2N by answering the following Research Questions.\n\u2022 RQ1: How does the performance of LightGC2N in terms of training efficiency and parameter scale?\n\u2022 RQ2: How does LightGC2N perform on the SSR compared with other state-of-the-art methods?\n\u2022 RQ3: How do the key components of LightGC2N contribute to the recommendation performance?\n\u2022 RQ4: How do the hyper-parameters affect the performance of LightGC2N?\nExperimental Settings\nDatasets. We evaluate LightGC2N on four real-world datasets released by (Ma et al. 2019), including Hvideo-E (HV-E), Hvideo-V (HV-V), Hamazon-M (HA-M), and Hamazon-B (HA-B). HV-E and HV-V are two smart TV datasets comprising viewing logs from different TV channels. HV-E encompasses logs of educational videos and instructional content in areas such as sports nutrition and medicine, whereas HV-V includes logs of television series and films. HA-M and HA-B are derived from two Amazon domains, featuring movie viewing (HA-M) and book reading (HA-B). For the evaluation, we randomly assigned 80% of the sequences to the training set, and the remaining 20% to the testing set. Note that, the most recently observed item in each sequence per dataset is designated as the ground truth item."}, {"title": "Parameter Scale and Training Efficiency (RQ1)", "content": "Evaluation Metrics. For model evaluations, we adopt two common evaluation metrics (Guo et al. 2021) to assess the model performance, i.e., top-N Recall (Recall@N) and top-N Mean Reciprocal Rank (MRR@N), where N = {5, 20}.\nImplementation Details. We implemented LightGC2N with TensorFlow and accelerated the model training using an Intel\u00ae Xeon\u00ae Silver 4210 CPU (2.20GHz) and NVIDIA\u00ae RTX 3090 (24G) GPU. The operating system is Ubuntu 22.04, the system memory is 126G, and the coding platform is PyCharm. The learnable parameters are initialized via Xavier (Glorot and Bengio 2010), the loss function is optimized by Adam (Kingma and Ba 2015) optimizer. For the training settings, we set the batch-size as 256, the learning rate as 0.005, the dropout rate as 0.1, and the training epochs as 200. We uniformly set the embedding-size as 16 for LightGC2N and other baseline methods to ensure the fairness of experiments. For other hyper-parameters of baselines, we adopt optimal hyper-parameter settings reported in their paper and then fine-tuned them on each dataset.\nBaselines. To validate the performance of LightGC2N on SSR, we compared it with the following baselines: 1) Traditional recommendations: NCF (He et al. 2017), and LightGCN (He et al. 2020). 2) Sequential recommendations: HRNN (Quadrana et al. 2017). NAIS (He et al. 2018), and TGSRec (Fan et al. 2021). 3) Shared-account sequential recommendations: \u03c0-Net (Ma et al. 2019), PSJNet (Sun et al. 2023), DA-GCN (Guo et al. 2021), and TiDA-GCN (Guo et al. 2024).\nParameter Scale and Training Efficiency (RQ1)\nIn this section, we initially vary the proportion of input data from 0.2 to 1.0 on the HV-E and HV-V datasets to assess the LightGC2N's training time consumption. Subsequently, we compare its parameter scale with other competitive methods, i.e., PSJNet, TiDA-GCN, \u03c0-net and DA-GCN. The observations are as follows: 1) Figure 3 (a) and (b) reveal that LightGC2N exhibits reduced training time compared to other baselines, signifying enhanced training efficiency and scalability for large-scale datasets. 2) As depicted in Figure 3 (c) and (d), LightGC2N requires notably fewer parameters than other methods, providing a positive answer to RQ1."}, {"title": "Overall Performance (RQ2)", "content": "Table 1 shows the experimental results of LightGC2N compared with other state-of-the-art methods on four datasets. The observations are summarized as follows: 1) The sequential recommendation methods (i.e., HRNN, NAIS, and TGSRec) perform better than traditional methods (i.e., NCF and LightGCN). This observation indicates the significance of modeling users' sequential preferences. 2) The SSR solutions (i.e., \u03c0-Net, PSJNet, DA-GCN, TiDA-GCN, and LightGC2N) typically outperform the other traditional and sequential recommendation methods, demonstrating the significance of addressing the shared-account issues in real-world sequential recommendation scenarios. 3) LightGC2N outperforms other state-of-the-art SSR methods (i.e., \u03c0-Net, PSJNet, DA-GCN, and TiDA-GCN). This observation indicates the significance of capturing the fine-grained differences among latent users for SSR. 4) LightGC2N exceeds other graph-based SSR methods (i.e., DA-GCN and TiDA-GCN), demonstrating the superiority of our proposed graph capsule convolutional networks in modeling complicated associations for SSR. 5) LightGC2N achieves the best performance on all datasets, demonstrating the superiority of our proposed graph capsule convolutional network and subspace alignment method for the SSR scenarios."}, {"title": "Ablation Study (RQ3)", "content": "In this section, we conduct a series of ablation studies on HV-E and HV-V to explore the impact of different components for LightGC2N. As shown in Table 2, 1) Lightw/oC is a variant that replace Graph Capsule Convolutional Network (GC2N) by traditional graph convolutional network. 2) Lightw/oS is another variant method that disables the Subspace Alignment (SA) component. 3) Lightw/oLA is a variant that replaces Linear attention with Conv1D when projecting item embeddings into capsule-form. 4) Lightw/oDR is another variant that omits the account-level dynamic routing in GC2N. 5) Lightw/oCL is a method that excludes the contrastive learning in SA. 6) Lightw/oA is a variant that removes all the component of LightGC2N.\nThe observations of Table 2 are summarized as follows: 1) LightGC2N outperforms Lightw/oC and Lightw/0A. This observation demonstrates that GC2N works well in distinguishing preferences of latent users. It also demonstrates that the fine-grained distinction of the preferences for different latent users indeed enhance the model performance on SSR. 2) LightGC2N outperforms Lightw/0LA, illustrating the effectiveness of Linear attention in capturing global correlations among items. 3) LightGC2N outperforms Lightw/oDR, suggesting that the routing selection in the account-level dynamic routing indeed improve the performance of GC2N. It also demonstrates that the account-level dynamic routing component performs well in merging the preferences from multiple latent users. 4) LightGC2N performs better than Lightw/os, demonstrating the contribution of the subspace alignment method for the sequence-level representation learning. 5) LightGC2N outperforms Lightw/oCL, indicating that the contrastive learning strategy aids in subspace alignment between refined and original sequence embeddings."}, {"title": "Hyper-parameters Analysis (RQ4)", "content": "The hyper-parameter $\\alpha$ controls the number of latent users within each shared account. Figures 4 (a) and (b) show the performance of LightGC2N with different $\\alpha$ values in {1,2,3,4,5} on two different datasets (HV-E and HA-M). The experimental results indicate that LightGC2N requires different parameter settings to achieve optimal performance on different datasets, which is consistent with the real-world setting (i.e., the number of latent users sharing an account varies in different scenarios). $\\beta$ and $\\gamma$ are two significant hyper-parameters that respectively control the temperature and the participation of the contrastive learning. As shown in Figures 4 (c) and (d), LightGC2N reach the best performance when they are set to 0.8 or 0.9. This observation further validates the effectiveness of our contrastive learning strategy. Additionally, it indicates that the self-supervised signals contribute to sequence-level representation learning only when these hyper-parameters are appropriately valued."}, {"title": "Conclusion and Future Work", "content": "In this work, we introduce a Lightweight Graph Capsule Convolutional Network (LightGC2N) with subspace alignment to tackle the problems in Shared-account Sequential Recommendation (SSR). By effectively capturing the fine-grained preferences of latent users, LightGC2N achieves the best performance on various datasets. The lightweight design of this work makes it suitable for deployment on resource-constrained devices while maintaining high recommendation accuracy. Experimental results on four SSR datasets demonstrates the effectiveness and efficiency of LightGC2N, paving the way for its practical application in real-world recommendation systems.\nHowever, LightGC2N assumes a fixed number of latent users within each shared account. In our future work, we will study how to determine the number of latent users in each account automatically or heuristically."}, {"title": "Technical Appendix", "content": "A1. Notations\nThe key notations of embedded vectors and matrices that utilized in the Methodology are summarized in Table 3.\nA2. Detailed Dataset Description\nWe evaluate LightGC2N on four SSR-oriented datasets (i.e.", "recommendations": "To ensure the fairness of the experimental results", "2017)": "This is a traditional recommendation method that exploits deep neural networks to capture the collaborative filtering between interactions.\n\u2022 LightGCN (He et al. 2020): LightGCN is a simplified graph-based method for traditional recommendation\n2) Sequential recommendations:\n\u2022 HRNN (Quadrana et al. 2017): HRNN is an early proposed RNN-based SR method", "2018)": "This is an attention-based SR method that designs a nonlinear attention network to calculate the correlations among items.\n\u2022 TGSRec (Fan et al. 2021): This is a time interval-aware SR method"}, {"recommendations": "n\u2022 \u03c0-Net (Ma et al. 2019): This is an RNN-based SSR method that transfers knowledge between domains and models the shared-account preferences via the Gating Recurrent Units (GRUs).\n\u2022 PSJNet (Sun et al."}]}