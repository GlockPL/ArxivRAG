{"title": "DistDD: Distributed Data Distillation Aggregation through Gradient Matching", "authors": ["Peiran Wang", "Haohan Wang"], "abstract": "In this paper, we introduce DistDD, a novel approach within the federated learning framework that reduces the need for repetitive communication by distilling data directly on clients' devices. Unlike traditional federated learning that requires iterative model updates across nodes, DistDD facilitates a one-time distillation process that extracts a global distilled dataset, maintaining the privacy standards of federated learning while significantly cutting down communication costs. By leveraging the DistDD's distilled dataset, the developers of the FL can achieve just-in-time parameter tuning and neural architecture search over FL without repeating the whole FL process multiple times. We provide a detailed convergence proof of the DistDD algorithm, reinforcing its mathematical stability and reliability for practical applications. Our experiments demonstrate the effectiveness and robustness of DistDD, particularly in non-i.i.d. and mislabeled data scenarios, showcasing its potential to handle complex real-world data challenges distinctively from conventional federated learning methods. We also evaluate DistDD's application in the use case and prove its effectiveness and communication-savings in the NAS use case.", "sections": [{"title": "Introduction", "content": "Federated learning typically involves iterative communication between the central server and its clients. Throughout the training process, the server proposes parameters for the clients to calculate the updates for their local models (Zhou et al. 2021; Khodak et al. 2020; Agrawal et al. 2021). The server then aggregates these updates to refine the global model. While these communication costs might be necessary for the federated learning paradigm to maintain users' privacy, they become significant because a good machine learning model typically requires repeated training to debug better parameters and neural network architectures (Zhang et al. 2021).\nFor example, consider the following two use cases:\nUse case A (Parameter Tuning). (see Figure 1): Considering the developers need to tune the hyper-parameters of the FL process, (Khan et al. 2023; Zhang et al. 2021; Zhou et al. 2021; Agrawal et al. 2021) such as batch size, learning rate, epoch, optimizer, etc. In a typical FL architecture, the parameter tuning process requires repeating the full FL process, which involves multiple clients joining. Such a process brings enormous communication costs due to the unnecessary multiple repeat tuning.\nUse case B (NAS over FL). (see Figure 1): Another example is the neural architecture search over FL (Zhu, Zhang, and Jin 2021; Zhu and Jin 2021; Liu et al. 2023a; He et al. 2021; Khan et al. 2023; Yan et al. 2024). Considering the scene in which the developers of the FL want to search for the optimal neural architecture for the FL tasks, The FL server must search for a new neural architecture at each iteration during such a process. Then, the FL server needs to perform the whole FL process using the searched architecture to collect the performance as feedback. Such approaches must be repeated multiple times until the optimal neural architecture is searched. This process brings huge communication costs as well.\nSuch use cases require repeatedly tuning the model, bringing huge communication costs (Zhou et al. 2021). To reduce such communication costs, an appealing approach is for the clients to upload the data directly to the server so that future training and tuning can only happen within the server. However, an obvious flaw is that data uploading will invade the clients' privacy, which is against the principle of federated learning.\nTherefore, in this paper, we seek to answer the question: How can we allow clients to upload the essential information to train a classifier so that the server can further train and tune the models without additional communication costs while protecting the client's privacy (as much as federated learning can protect).\nTo answer this question, we introduce a novel distributed data distillation method (Distributed Data Distillation through gradient matching) in this paper. DISTDD is a method that combines gradient matching (Zhao, Mopuri, and Bilen 2020) with distributed learning to distill knowledge from multiple clients into a single dataset. In this process, clients use their local datasets to get gradients. The critical step is to compute the loss between the aggregated global gradient and the gradient from the distilled dataset and use this loss to build the distilled dataset. Finally, the server uses the synthesis distilled dataset to tune and update the global model."}, {"title": "Related Work", "content": "Federated Learning\nMislabeling. In the context of distributed learning, there may be instances where nodes misclassify certain data, leading to a decrease in data quality. Some research further extends this issue to Byzantine attacks in distributed learning (Shi et al. 2021; Fang et al. 2020; Shejwalkar and Houmansadr 2021; Cao et al. 2020). In these attacks, malicious nodes can manipulate their model parameters (such as weights or gradients) to degrade the accuracy of the global model. Various strategies have been proposed to defend against Byzantine attacks in distributed learning (So, G\u00fcler, and Avestimehr 2020). These include client selection strategies, score-based detection methods, spectral-based outlier detectors, and update denoising. In our DISTDD, we also consider that each client's data may have bad quality since the clients' labels might be wrong.\nHyper-parameter optimization. Previous researchers also have worked on hyper-parameter optimization in FL. (Zhou et al. 2021) leverages meta-learning techniques to utilize local and asynchronous to optimize the hyper-parameter. (Khodak et al. 2020) applied techniques from NAS with weight-sharing to FL with personalization to modify local training-based FL. (Agrawal et al. 2021) clusters edge devices based on the training hyper-parameters and genetically modifies the parameters cluster-wise. However, these approaches still require multiple communication processes.\nKey contributions for federated learning. The debugging"}, {"title": "Methodology", "content": "In DISTDD, there is a central server p. And there are multiple distributed clients i = 0, ..., I \u2013 1, each has a local dataset \\(T_i\\). The dataset contains C classes.\nTo get the optimal parameter for training, p has to optimize the hyper-parameter of FL by repeating the whole FL process. However, due to FL's high communication and computation costs, it is inefficient for p and I to conduct such a costly process. Thus, it is more reliable for p to distill the datasets from the client set I into one distilled dataset and use the distilled dataset to optimize the parameters.However, it is unfeasible for p to collect the datasets from all the clients and do the data distillation locally on the server. Thus, DISTDD achieves the data distillation in a distributed way:\nInitially, p randomly generate an initialized set of synthetic samples S containing C classes, probability distribution over randomly initialized weights \\(\\mathbb{P}_{\\theta_0}\\). p also initialize a deep neural network \\(\\phi_{\\theta}\\), which serves as a classifier for this dataset. Now p set the number of loop steps T, the number of steps for updating weights \\(s_\\theta\\) and synthetic samples \\(s_S\\) in each inner-loop step respectively, learning rates for updating weights \\(\\eta_\\theta\\) and synthetic samples \\(\\eta_S\\).\nIn each iteration, p will first sends the classifier model weight \\(\\theta_t\\) to each client i. Each client i samples a mini-batch \\(B_T^i \\sim T_i\\) from its local dataset \\(T_i\\). And the mini-batch \\(B_T^i\\)"}, {"title": "Protect Privacy", "content": "However, there are still many claims about the privacy of federated learning. Previous researchers claim that the exchanged gradient updates between clients and the central server can still leak privacy-related information from clients to the central server. Furthermore, we consider providing more privacy protection methods for DISTDD by introducing DPSGD (Abadi et al. 2016) into our DISTDD framework."}, {"title": "Evaluation", "content": "We first reveal our experiment setting in \u00a7. Next, we compare DISTDD with FedAvg schemes in \u00a7. Then, we consider the mislabeling situations and evaluate the DISTDD method under different portions of mislabeling clients in \u00a7. The data distribution problem of nonIID is considered in \u00a7. To prove the effectiveness of using DISTDD in the use cases, we evaluated DISTDD under NAS settings in \u00a7."}, {"title": "Experiment Setting", "content": "We discussed our experiment settings in this section:\n\u2022 Models. In our experimental setup, we employ a Convolutional Neural Network (ConvNet) architecture as the foundational network for our study.\n\u2022 Datasets. We leverage three image classification datasets, namely MNIST, FashionMNIST, and CIFAR-10, as the experimental datasets.\n\u2022 Client number. The default configuration for our system includes a predefined client count of 20. Furthermore, our system employs a randomized participant selection process, wherein 50% of the clients actively participate in the training process during each iteration (this setting follows the convention of both FL and DD)."}, {"title": "Non-iid Situation", "content": "Methods. In this experiment, we dive into the effect of non-iid data distribution, focusing on its impact on DISTDD's classification accuracy. Investigating the impact of non-iid (non-independent and identically distributed) data is essential for understanding how DISTDD's performance varies in real-world scenarios, where data often exhibits diverse patterns and distributions across different clients, directly influencing the model's overall classification accuracy and robustness. We draw a comparative analysis between centralized gradient matching, DistDD, local gradient matching, and FedAvg. To replicate non-iid data distribution, we adopt the definition of the Dirichlet distribution to partition data across these distributed clients. The parameter alpha (we labeled as dir), within the range of 0.1 to 1.0, serves as a controlling factor to control the degree of non-iid.\nResults. The results are shown in Figure 3. Specifically, in scenarios characterized by highly non-iid data distributions, the performance of DISTDD significantly falls behind that of centralized gradient matching. Conversely, when the data distribution approaches near-identicality (i.e., becomes nearly iid), the performance of DISTDD demonstrates a notable capability to approximate the performance levels achieved by centralized gradient matching.\nWe also examine the efficacy of per-client local gradient matching concerning individual client performance. Additionally, we explore varying experimental configurations, including non-iid and iid data distributions. When confronted with non-iid scenarios, the efficacy of per-client local gradient matching diminishes. This observation is further proved by the accuracy results in Figure 3."}, {"title": "Use Case for DISTDD", "content": "Methods. To prove DISTDD's effectiveness on the use case B: NAS over FL, we provided an example evaluation as shown in Figure 4. We compared original FedAvg accuracy, DISTDD's accuracy in each tuning iteration (after the DistDD's distilled dataset-based NAS, the network was trained on DISTDD's distilled dataset.), FedAvg's accuracy after DistDD tuning (after DistDD's distilled dataset-based NAS, the network was trained again using FedAvg) and FedAvg tuning (directly using FedAvg for NAS). We also compare DISTDD's time cost with FedAvg's time cost under increasing parameter tuning times (see Figure 5).\nResults. The results show that FedAvg after DISTDD NAS has a similar accuracy with FedAvg for NAS. This proves DistDD's effectiveness for the NAS over FL. When only searching for the architecture for one time, the two frameworks' time costs are nearly the same. While, as the tuning periods increase, FedAvg's time cost goes above DISTDD's time cost soon. This is because DISTDD does not need to communicate for the tuning process after the 1st tuning process.\nThis indicated that DISTDD used in NAS can achieve equal NAS quality with FedAvg while reducing the time cost, revealing a good trade-off."}, {"title": "Adding Differential Privacy Noise to DISTDD", "content": "Figure 6: This study explores the impact of integrating differential privacy (DP) into DISTDD, a system used within distributed learning environments to enhance privacy. By adjusting the noise scale parameter, \\(\\sigma\\), from 0.01 to 100, the study compares the performance of DISTDD with and without DP. The findings reveal that increasing \\(\\sigma\\) beyond 0.01 significantly diminishes DISTDD's performance, resulting in a marked reduction in its overall efficiency. This indicates that while DP adds a layer of privacy protection, it also poses challenges by adversely affecting system performance when the noise level is too high.\nMoreover, our investigation extends to assessing the influence of incorporating differential privacy (DP) mechanisms into our DISTDD. Differential privacy has proven its efficacy in protecting individual privacy within distributed learning frameworks, rendering it an appealing avenue for augmenting privacy assurances among participating clients. In the context of this experimental study, we systematically vary the noise scale parameter denoted as \\(\\sigma\\), exploring values ranging from 0.01 to 100. This comprises a comparative analysis of DISTDD's performance in the absence of DP (referred to as the non-DP scenario) and its performance when DP is integrated (referred to as the DP-enabled scenario).\nAs shown in Figure 6, our findings substantiate that when the noise scale \\(\\sigma\\) surpasses the threshold of 1e-2, a pronounced detrimental effect on DISTDD's performance becomes evident. Notably, the outcome is manifested as a substantial degradation in the system's overall performance metrics.\nIn summary, this comprehensive exploration underscores the critical significance of judiciously configuring the noise scale parameter when integrating differential privacy into DISTDD, thus ensuring that privacy enhancements are harmoniously balanced with the preservation of system performance and convergence integrity."}, {"title": "Discussion", "content": "The same level of privacy protection: FL has been widely considered an efficient method to aggregate knowledge from distributed clients and protect distributed clients' privacy. Although FL has many privacy challenges, the privacy level itself is enough for many scenes. Like FL, our proposed method DISTDD only allows the gradient updates exchange between clients and servers. This gradient update is used in the central server's gradient matching process to construct a distilled dataset. There is no other privacy-related information exchanged in DISTDD. Thus, DISTDD, as an alternative to FL, can protect privacy to the same level as FL.\nAbstract for global dataset. In fact, DISTDD provides the abstract for the global dataset. By performing the gradient matching in a distributed way, DISTDD aggregates the global knowledge into the distilled dataset as a global abstract. This abstract enables the server of FL to tune the parameter and the architecture without high communication costs."}, {"title": "Conclusion", "content": "In conclusion, our work introduces a new distributed data distillation framework, named DISTDD (Distributed Data Distillation through gradient matching), which combines the gradient matching methods with distributed learning. This novel approach enables the extraction of distilled knowledge from a diverse set of distributed clients, offering a solution for aggregating large-scale distributed data while enabling the server to train the global model on the global dataset freely without concern about communication overhead. Importantly, we have provided a formal convergence proof for the DISTDD algorithm, offering a theoretical foundation for its effectiveness and stability. Our comprehensive experimentation has also demonstrated the robustness and effectiveness of DISTDD in various scenarios."}, {"title": "Convergence Analysis", "content": "We formulate the proof of our DISTDD as two steps: First, we prove the convergence of our FL process. Then, we prove the convergence of the gradient matching process by proving that the synthetic dataset can be very close to the original dataset.\nWe formulate local SGD as follows:\n\\(\\theta_t^{i,k+1} := \\theta_t^{i,k} - \\eta \\nabla \\ell^{i,k} = \\theta_t^{i,k} - \\eta \\theta^{i,k}\\)   (11)\n\\(\\theta_t^{i,k}\\) is the local model parameter for client i, t is global round index and k is local step index.\nAnd we consider the overall optimization objective as\n\\(minF(\\theta) = E_{i \\sim c}(F_i(\\theta))\\) (12)\nWe have a client population as C = 1, 2, 3, ..., M.\nTo prove the convergence of our work, we have two main assumptions.\nAssumption 1: Unbiased stocahstic gradient. The expectation of the stochastic gradient for a given \\(\\theta^{k}\\) is equal to the average local gradient for a given model \\(\\phi(\\cdot)\\). This is to say, the gradient expectation of the SGD equals the gradient of the GD:\n\\(E[\\nabla \\theta_t^k | \\theta_t^k] = \\nabla F(\\theta_t^k)\\) (13)\nGiven a dataset of \\(T_i = \\{(\\S_i, t_i)\\}_{i = 0}^{N}\\), where the N denotes the length of the whole dataset. The objective of the GD and its gradients are calculated as:\n\\(F_i(x_t^{i,k}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(\\phi_{\\theta}(s_i), t_i)\\)\n\\(\\nabla F_i(x_t^{i,k}) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla \\ell(\\phi_{\\theta}(s_i), t_i)\\) (14)\nIn this case, the expectation is the weighted average of a single batch with batch size as bn, i.e.,\n\\(E[\\nabla \\ell^{i,k} | x_t^{i,k}] \\\\ E[x_t^{i,k+1} | x_t^{i,k}]=\\)\n\\( = \\frac{1}{N-b_n+1} \\sum_{j=1}^{N-b_n+1} P(I = i | S = j)P(S = s_j) \\frac{\\partial \\ell}{\\partial x_t^{i,k}}\\\\)  \\(=\\frac{1}{N-b_n+1} \\sum_{j=1}^{b_n} P(I = i | S = s_j)P(S = s_j) \\sum_{i=1} \\frac{\\partial \\ell}{\\partial x_t^{i,k}}\\\\)\n\\(=\\frac{1}{N} \\sum_{i=1}^{N} \\nabla \\ell(\\phi_{\\theta}(s_i), t_i)\\\\ = \\nabla F_i(x_t^{i,k})\\) (15)\nwhere batch set is \\(S = s_1, ..., s_{b_n}\\). SGD or Adam is a stochastic optimization algorithm that randomly selects samples from the batch for gradient calculation.\nAssumption 2: Bounded variance:\n\\(E[| \\nabla \\ell^{i,k} - \\nabla F_i(x_t^{i,k}) |^2] \\leq \\sigma^2\\)  (16)\nThis is to say the gradient of the SGD is close to that of the GD.\nAssumption 3: L-Smooth: Local gradient \\(\\nabla F(x)\\) and global gradient \\(\\nabla F(x)\\) is \\(\\varsigma\\)-uniformly bounded.\n\\(max_{x} sup | | \\nabla F_i(x_t^{i,k}) - \\nabla F(x_t^{i,k}) | | \\leq \\varsigma\\)  (17)\nProof\nGenerally, we want to prove that\n\\(| |F(x_{t+1}) - F(x^*) | | < | |F(x_t) - F(x^*) | |, \\forall t, k \\in [1, 2, 3, \\dots]\\) (18)\nwhere F(x*) is the optimal. Or, we give a weaker claim:\n\\(E_t[ \\frac{1}{TT} \\sum_{t=0}^{T-1} \\sum_{k=1}^{T} F(x_t) - F (x^*)] < \\)\n< an upper bound decreasing with T. (19)\nNote that E in this paper denotes \\(E_{i \\in c}\\), where C denotes the client set. Therefore, we can say that the E is generally calculating the expectation over all the clients.\nDecentralized optimization: Originating from the decentralized optimization, we derive the shadow sequence to indicate the update process.\n\\(x_{xx} := \\frac{1}{M} \\sum_{i=1}^{M} x_t^{i,k}\\) (20)\nThen, at round t local epoch k + 1,\n\\(x_t^{+1} = \\frac{1}{M} \\sum_{i=1}^{M} x_t^{i,k}\\) (21)"}, {"title": "Ablation Study", "content": "Different Nodes Number\nWe evaluate the performance of DISTDD in response to varying degrees of node participation. In this particular experiment, it is notable that the cumulative volume of data samples across all clients remains unaltered. Consequently, as we increase the number of participating nodes, the number of data samples allocated to each individual client simultaneously diminishes. We rely on the classification accuracy outcomes to illuminate the performance changes, as shown in Figure 7.\nTo conduct the comparative analysis, we compare three distinct configurations: firstly, the local gradient matching; secondly, DISTDD featuring full participation from all nodes; and thirdly, DISTDD with a 50% random client participation scheme. The experiment results manifest a notable trend. Specifically, the performance of DISTDD with full participation exhibits a gradual decline with the amplification of node numbers; nonetheless, this decline is relatively modest. In contrast, the performance of DISTDD with random participation shows a substantially steeper descent in accuracy.\nImage number per class\nIn this section, we explore the impact of the number of generated images per class with a specific focus on its effect on classification accuracy. To undertake this ablation study, we systematically vary the quantity of images per class, encompassing the values 1, 10, 20, 30, 40, and 50. The outcomes are shown in Figure 8.\nIt is notable that local gradient matching reaches convergence primarily when the image count per class ranges between 10 and 20. In contrast, DISTDD exhibits a convergence behavior at a significantly higher threshold, typically exceeding 30 images per class. This observation suggests that DISTDD necessitates a more substantial quantity of images to aggregate knowledge from the distributed clients effectively. However, it is noteworthy that the performance of"}, {"title": "Communication Rounds", "content": "In this section, we evaluate the influence of communication rounds on the performance of DISTDD with a particular emphasis on its impact on classification accuracy. We conduct this analysis by contrasting two configurations of DISTDD one with full client participation per round and another with random participation of 50% of the clients per round, within the context of a 20-client scenario. The results, illustrated in Figure 9, offer the observed effects.\nEvidently, DISTDD with full client participation typically requires approximately 300 communication rounds to converge. In contrast, the variant of DISTDD featuring random client participation necessitates a significantly greater number of communication rounds to achieve the same convergence. This discrepancy in the convergence rate primarily stems from random client participation, which mandates a more extended communication process for each client to convey and synchronize their knowledge with the central server."}, {"title": "Portion of Selected Clients per Round", "content": "Next, we study the effect of the proportion of selected clients per round, focusing on random participation throughout 500 communication rounds. We maintain a constant client count of 20 while adhering to a Dirichlet distribution parameter (dir = 1.0) for data partitioning. The proportion of participating clients is systematically varied, ranging from 10% to 100% (representing full participation).\nNoteworthy is the observation that it necessitates a participation rate of 80% within the random selective participation scheme to achieve parity in classification accuracy with full participation. Conversely, when the participation rate falls below the 50% threshold, the performance of DISTDD markedly falls behind that of local gradient matching. This disparity in performance underlines the significance of the participation proportion in the context of random selection and underscores the trade-off between participation rate and classification accuracy."}, {"title": "Privacy Analysis", "content": "DISTDD adds DPSGD to protect privacy; here, we give the privacy guarantee for DPSGD in DISTDD.\nFirst, we review the definition of differential privacy. A randomized algorithm A satisfies (\u20ac, \u03b4)-differential privacy, if for any two adjacent data sets D and D' (they differ in one"}]}