{"title": "HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications", "authors": ["Rishi Kalra", "Zekun Wu", "Ayesha Gulley", "Airlie Hilliard", "Xin Guan", "Adriano Koshiyama", "Philip Treleaven"], "abstract": "While Large Language Models (LLMs) excel in text generation and question-answering, their effectiveness in AI legal and policy is limited by outdated knowledge, hallucinations, and inadequate reasoning in complex contexts. Retrieval-Augmented Generation (RAG) systems improve response accuracy by integrating external knowledge but struggle with retrieval errors, poor context integration, and high costs, particularly in interpreting qualitative and quantitative AI legal texts. This paper introduces a Hybrid Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy, exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity classifier for adaptive parameter tuning, a hybrid retrieval strategy combining dense, sparse, and knowledge graph methods, and an evaluation framework with specific question types and metrics. By dynamically adjusting parameters, HyPA-RAG significantly improves retrieval accuracy and response fidelity. Testing on LL144 shows enhanced correctness, faithfulness, and contextual precision, addressing the need for adaptable NLP systems in complex, high-stakes AI legal and policy applications.", "sections": [{"title": "1 Introduction", "content": "Recently, the development of Large Language Models (LLMs) capable of processing and generating human-like text has made significant strides in recent years, such as OpenAI's GPT models (Brown et al., 2020; OpenAI, 2023), Google's Gemini models (Team et al., 2023) and open alternatives such as the LlaMa series (Touvron et al., 2023a,b; Meta, 2024). These models, which store vast amounts of information within their parameters through extensive pre-training, have demonstrated impressive performance in various tasks, including text generation and question-answering across multiple domains (Brown et al., 2020; Singhal et al., 2023; Wu et al., 2023). Despite this, LLMs encounter limitations when applied to specialised fields such as legal and policy. These include the rapid obsolescence of their knowledge, which is confined to the data available up to the last pre-training date (Yang et al., 2023) and hallucinations, where the model produces text that seems plausible but is factually incorrect or misleading, driven by internal logic rather than actual context (Ji et al., 2022; Huang et al., 2023). Empirical studies show that many AI legal tools overstate their ability to prevent hallucinations (Magesh et al., 2024). Instances of lawyers being penalised for using hallucinated outputs in court documents (Fortune, 2023; Business Insider, 2023) underscore the need for reliable AI question-answering systems in law and policy\nNaturally, Retrieval-Augmented Generation (RAG), which enhances LLMs by incorporating external knowledge, is proposed as a solution. However, this comes with its own challenges. Common failure points (Barnett et al., 2024) include missing content, where relevant documents are not retrieved, leading to unanswered questions; context limitations, where retrieved documents are not effectively incorporated into the response generation process due to limitations in consolidation strategies; and extraction failures, where models fail to extract accurate information from the provided context due to noise or conflicting data. Furthermore, advanced retrieval and generation techniques, such as query rewriters and LLM-based quality checkers often result in increased token usage and costs.\nTo address these challenges, this research integrates three key components (see Figure 6 in Appendix A.2 for a flow overview and Figure 1 for the system design):\n(1) Adaptive parameter selection using a domain-specific query complexity classifier to minimise unnecessary token usage,"}, {"title": "2 Background and Related Work", "content": "Recent LLM advancements have impacted fields like law and policy, where language complexity and large text volumes are prevalent (Blair-Stanek et al., 2023; Choi et al., 2023; Hargreaves, 2023). LLMs have been used for legal judgment prediction, document drafting, and contract analysis, showing their potential to improve efficiency and accuracy (Shui et al., 2023; Sun, 2023; \u0160avelka and Ashley, 2023). Techniques like fine-tuning, retrieval augmentation, prompt engineering, and agentic methods have adapted these models for specific legal tasks, enhancing performance in summarisation, drafting, and interpretation (Trautmann et al., 2022; Cui et al., 2023).\nRetrieval-Augmented Generation (RAG), as formalized by Lewis et al., enhances pre-trained seq2seq models by integrating external knowledge through indexing, retrieval, and generation stages, improving response specificity and accuracy (Lewis et al., 2020; Gao et al., 2023). RAG systems complement LLMs by combining sparse (e.g., BM25) and dense (e.g., vector) retrieval techniques, using neural embeddings to refine document retrieval and produce grounded, high-quality responses (Jones, 2021; Robertson and Zaragoza, 2009; Devlin et al., 2019; Liu et al., 2019).\nTo address the limitations of naive RAG, such as insufficient context and retrieval inaccuracies, advanced techniques have been developed, including hybrid retrieval methods, query rewriters, and rerankers to refine relevance (Muennighoff et al., 2022; Ding et al., 2024; Xiao et al., 2023). Hybrid retrieval combines BM25 with semantic embeddings to balance keyword matching and contextual understanding, improving outcomes (Luo et al., 2023; Ram et al., 2022; Arivazhagan et al., 2023). Additionally, knowledge graph retrieval and composed retrievers enhance accuracy and comprehensiveness in document retrieval (Rackauckas, 2024; Sanmartin, 2024; Edge et al., 2024).\nRecently, RAG systems have advanced from basic retrieval to dynamic methods involving multi-source integration and domain adaptation (Gao et al., 2023; Ji et al., 2022). Innovations like Self-RAG and KG-RAG improve response quality and minimize hallucinations through adaptive retrieval and knowledge graphs (Asai et al., 2023; Sanmartin, 2024)."}, {"title": "3 System Design", "content": "The hybrid parameter-adaptive RAG system, depicted in Figure 1, integrates vector-based text chunks and a knowledge graph of entities and their relationships to enhance retrieval accuracy. The system employs a hybrid retrieval process, combining sparse (BM25) and dense (vector) methods to retrieve an initial top-k set of results. These results are refined using reciprocal rank fusion based on predefined parameter mappings.\nSimultaneously, a knowledge graph retriever identifies relevant triplets, with retrieval depth and keyword selection dynamically adjusted according to query complexity. Results from both BM25 and vector methods are fused again to produce a final optimised set of k chunks.\nOptional components include a query rewriter, which generates reformulated queries to improve retrieval. The rewritten queries fetch additional chunks, which are de-duplicated and fused to maintain uniqueness. An optional reranker can further refine chunk ranking if needed. The final set of selected chunks and knowledge graph triplets are then processed within the LLM's context window for more accurate, contextually relevant responses.\nThis framework is implemented in two variations: without knowledge graph retrieval, known as Parameter-Adaptive (PA) RAG, and with knowledge graph retrieval, termed Hybrid Parameter-Adaptive (HyPA) RAG."}, {"title": "4 AI Legal and Policy Corpus", "content": "Local Law 144 (LL144) of 2021, enacted by the New York City Department of Consumer and Worker Protection, regulates automated employment decision tools (AEDTs). This paper uses a 15-page version of LL144, including the original legislation and additional context from Subchapter 25. As an early AI-specific law, LL144 is included in the training data of foundational models like GPT-4 and GPT-40, whose understanding of the law is confirmed manually through targeted prompting and serves as baselines in this research.\nLL144 presents significant challenges for AI compliance due to its unique combination of qualitative and quantitative requirements. Unlike most AI legal and policy texts, which are predominantly qualitative, LL144 integrates detailed definitions and procedural guidelines with quantitative compliance metrics. This structure complicates interpretation and retrieval, often exceeding the capabilities of traditional LLMs and RAG systems. Furthermore, AI laws and policies are frequently revised, making them impractical for pre-training and fine-tuning and therefore require a robust method for integrating changes."}, {"title": "5 Performance Evaluation", "content": "The evaluation process starts by generating custom questions tailored to AI policy and legal question-answering, then introduces and verifies evaluation metrics (see evaluation section of Figure 6 in Appendix A.2). For reproducibility, the LLM temperature is set to zero for consistent responses and all other parameters are set to defaults."}, {"title": "5.1 Dataset Generation", "content": "Creating a \"gold standard\" evaluation set usually requires extensive human expertise and time, but LLMs like GPT-3.5-Turbo can efficiently handle such tasks, if sufficiently prompted. For this purpose, Giskard (AI, 2023) provides a library for synthetic data generation, using LLMs to create various question types from text chunks, such as 'simple', 'complex', and 'situational'. We introduce additional types and question generators: 'comparative', 'complex situational', 'vague', and 'rule-conclusion'. Comparative questions require multi-context retrieval to compare concepts. 'Complex situational' questions involve user-specific contexts and follow-ups. Vague questions obscure parts of the query to test interpretation, while rule-conclusion questions, adapted from LegalBench (Guha et al., 2023), require conclusions based on legislative content."}, {"title": "5.2 Evaluation Metrics", "content": "To evaluate our RAG system, we utilise RAGAS metrics (Shahul et al., 2023a) based on the LLM-as-a-judge approach (Zheng et al., 2023), including Faithfulness, Answer Relevancy, Context Precision, Context Recall, and an adapted Correctness metric.\nFaithfulness evaluates the factual consistency between the generated answer and the context, defined as Faithfulness Score = $\\frac{C_{inferred}}{C_{total}}$ where $C_{inferred}$ is the number of claims inferred from the context, and $C_{total}$ is the total claims in the answer.\nAnswer Relevancy measures the alignment between the generated answer and the original question, calculated as the mean cosine similarity between the original question and generated questions from the answer: Answer Relevancy = $\\frac{1}{N} \\sum_{i=1}^{N} \\frac{E_{g_i} E_o}{\\|E_{g_i}\\| \\|E_o\\|}$, where $E_{gi}$ and $E_o$ are embeddings of the generated and original questions.\nContext Recall measures the proportion of ground truth claims covered by the retrieved context, defined as Context Recall = $\\frac{C_{attr}}{C_{GT}}$, where $C_{attr}$ is the number of ground truth claims attributed to the context, and $C_{GT}$ is the total number of ground truth claims.\nContext Precision evaluates whether relevant items are ranked higher within the context, defined as Context Precision = $\\sum_{k=1}^{K} (P_k \\times v_k)$. Here, $P_k = \\frac{TP_k}{\\mid R_k\\mid}$ is the precision at rank k, $v_k$ is the relevance indicator, $|R_k|$ is the total relevant items in the top K, $TP_k$ represents true positives, and $FP_k$ false positives."}, {"title": "5.3 Correctness Evaluation", "content": "We assess correctness using a refined metric to address the limitations of Giskard's binary classification, which fails to account for partially correct answers or minor variations. Our adapted metric, Absolute Correctness, based on LLamaIndex (LlamaIndex, 2024), uses a 1 to 5 scale: 1 indicates an incorrect answer, 3 denotes partial correctness, and 5 signifies full correctness. For binary evaluation, we use a high threshold of 4, reflecting our low tolerance for inaccuracies. The Correctness Score is computed as the average of these binary outcomes across all responses: Correctness Score = $\\frac{1}{N} \\sum_{i=1}^{N} 1(S_i \\ge 4)$, where $S_i$ represents the absolute correctness score of the ith response, $1(S_i \\ge 4)$ is an indicator function that is 1 if $S_i \\ge 4$ and 0 otherwise, and N is the total number of responses."}, {"title": "6 Chunking Method", "content": "We evaluate three chunking techniques: sentence-level, semantic, and pattern-based chunking.\nSentence-level chunking splits text at sentence boundaries, adhering to token limits and overlap constraints. Semantic chunking uses cosine similarity to set a dissimilarity threshold for splitting and includes a buffer size to define the minimum number of sentences before a split. Pattern-based chunking employs a custom delimiter based on text structure; for LL144, this is \"\\n\u00a7\".\nFigure 3 shows that pattern-based chunking achieves the highest context recall (0.9046), faithfulness (0.8430), answer similarity (0.8621), and correctness (0.7918) scores. Sentence-level chunking, however, yields the highest context precision and F1 scores. Semantic chunking performs reasonably well with increased buffer size but generally underperforms compared to the simpler methods. Further hyperparameter tuning may improve its effectiveness. These findings suggest that a corpus-specific delimiter can enhance performance over standard chunking methods.\nFor subsequent experiments, we adopt sentence-level chunking with a default chunk size of 512 tokens and an overlap of 200 tokens."}, {"title": "7 Query Complexity Classifier", "content": "To enable adaptive parameter selection, we developed a domain-specific query complexity classifier that categorises user queries, each corresponding to specific hyper-parameter mappings. Our analysis of top-k selection indicated different optimal top-k values for various question types, as shown in Figure 7 (Appendix A.4)."}, {"title": "7.1 Training Data", "content": "To train a domain-specific query complexity classifier, we generated a dataset using a GPT-40 model on legal documents. Queries were categorised into three classes based on the number of contexts required: one context (0), two contexts (1), and three or more contexts (2). This classification resulted in varying token counts, keywords, and clauses across classes, which could bias models toward associating these features with complexity. To mitigate this, we applied data augmentation techniques to diversify the dataset. To enhance robustness, 67% of the queries were modified. We increased vagueness in 10% of the questions while preserving their informational content, added random noise words or punctuation to another 10%, and applied both word and punctuation noise to a further 10%. Additionally, 5% of questions had phrases reordered, and another 5% contained random spelling errors. For label-specific augmentation, 25% of label 0 queries were made more verbose, and 25% of label 2 queries were shortened, ensuring they retained the necessary informational content."}, {"title": "7.2 Model Training", "content": "We employed multiple models as baselines for classification tasks: Random labels, Logistic Regression (LR), Support Vector Machine (SVM), zero-shot classifiers, and a fine-tuned DistilBERT model. The Logistic Regression model used TF-IDF features, with a random state of 5 and 1000 iterations. The SVM model also used TF-IDF features with a linear kernel. Both models were evaluated on binary (2-class) and multi-class (3-class) tasks. Zero-shot classifiers (BART Large ZS and DeBERTa-v3 ZS) were included as additional baselines, mapping \"simple question,\" \"complex question,\" and \"overview question\" to labels 0, 1, and 2, respectively; for binary classification, only \"simple question\" (0) and \"complex question\" (1) were used. The DistilBERT model was fine-tuned with a learning rate of 2e-5, batch size of 32, 10 epochs, and a weight decay of 0.01 to optimize performance and generalization to the validation set."}, {"title": "7.3 Classifier Results", "content": "Tables 1 and 8 in Appendix A.10 summarise the classification results. We compare performance using macro precision, recall and F1 score. The fine-tuned DistilBERT model achieved the highest F1 scores, 0.90 for the 3-class task and 0.92 for the 2-class task, highlighting the benefits of transfer learning and fine-tuning. The SVM (TF-IDF) and Logistic Regression models also performed well, particularly in binary classification, indicating their effectiveness in handling sparse data. Zero-shot classifiers performed lower, likely due to the lack of task-specific fine-tuning."}, {"title": "8 RAG System Architecture", "content": "The Parameter-Adaptive RAG system integrates our fine-tuned DistilBERT model to classify query complexity and dynamically adjusts retrieval parameters accordingly, as illustrated in Figure 1, but excluding the knowledge graph component. The PA-RAG system adaptively selects the number of query rewrites (Q) and the top-k value based on the complexity classification, with specific parameter mappings provided in Table 6 in Appendix A.6.1. In the 2-class model, simpler queries (label 0) use a top-k of 5 and 3 query rewrites, while more complex queries (label 1) use a top-k of 10 and 5 rewrites. The 3-class model uses a top-k of 7 and 7 rewrites for the most complex queries (label 2)."}, {"title": "8.2 Hybrid Parameter-Adaptive RAG", "content": "Building on the PA-RAG system, the Hybrid Parameter-Adaptive RAG (HyPA-RAG) approach enhances the retrieval stage by addressing issues such as missing content, incomplete answers, and failures of the language model to extract correct answers from retrieved contexts. These challenges often arise from unclear relationships within legal documents, where repeated terms lead to fragmented retrieval results (Barnett et al., 2024). Traditional (e.g. dense) retrieval methods may retrieve only partial context, causing missing critical information. To overcome these limitations, this system incorporates a knowledge graph (KG) representation of LL144. Knowledge graphs, structured with entities, relationships, and semantic descriptions, integrate information from multiple data sources and recent advancements suggest that combining KGs with LLMs can produce more informed outputs using KG triplets as added context.\nThe HyPA-RAG system uses the architecture outlined in Figure 1. The knowledge graph is constructed by extracting triplets (subject, predicate, object) from raw text using GPT-40. Parameter mappings specific to this implementation, such as the maximum number of keywords per query (K) and maximum knowledge sequence length (S), are detailed in Table 7, extending those provided in Table 6."}, {"title": "8.3 RAG Results", "content": "The adaptive methods generally outperform the fixed k baseline across most metrics. PA-RAG with k, Q (2 class) achieves the highest faithfulness score of 0.9044, which is an improvement of 0.0564 over the best fixed k = 10 method (0.8480). Similarly, the PA k, Q (3 class) configuration also performs strongly with a faithfulness score of 0.8971, surpassing all fixed k methods.\nFor answer relevancy, the PA k, Q (2 class) model achieves a score of 0.7910, which is nearly on par with the best fixed k = 10 method at 0.7917, showing a slight difference of 0.0007. The PA k, Q (3 class) model has a relevancy score of 0.7778, a drop of 0.0139 compared to the best fixed method. In terms of absolute correctness, both PA models, k, Q (2 class) and k, Q (3 class), achieve scores of 4.2491 and 4.2528, respectively, which are improvements of approximately 0.1896 and 0.1933 over the best fixed method (k = 10) score of 4.0595. This suggests that adaptive parameter settings significantly enhance the model's ability to provide correct answers.\nCorrectness scores also favour the adaptive methods. PA k, Q (3 class) model reaches a score of 0.8141, which is 0.0483 higher than the best fixed k = 10 score of 0.7658. PA k, Q (2 class) model shows similar strength with a score of 0.8104. HyPA show more varied results. HyPA k, Q, K, S (2 class) achieves a correctness score of 0.7770, a modest increase of 0.0112 over the fixed k = 7, suggesting room for further optimisation."}, {"title": "8.4 System Ablation Study", "content": "We evaluate the impact of adaptive parameters, a reranker (bge-reranker-large), and a query rewriter on model performance using PA and HyPA RAG methods with 2-class and 3-class classifiers.\nThe highest Answer Relevancy (0.7940) is achieved by varying k alone, suggesting that simpler, focused responses facilitate question reconstruction. The k, Q + reranker configuration achieves a slightly lower relevancy score (0.7902), indicating that query rewriting and reranking, while enhancing other metrics, introduce complexity that marginally reduces clarity.\nThe k, Q + reranker configuration also achieves the highest Faithfulness (0.9098), showing that combining adaptive top-k selection with query rewriting and reranking improves factual consistency. This setup provides high Absolute Correctness (4.2342), although slightly lower than k, Q alone (4.2528), indicating that while reranking improves response quality, it may slightly decrease overall accuracy. However, the Correctness Score improves from 0.8141 to 0.8178, highlighting an increase in responses classified as \"correct\" (scores of 4 or higher).\nAdding a knowledge graph in the k, K, S configuration maintains the same Correctness Score (0.8141) as k, Q but reduces Absolute Correctness by 0.1301, suggesting added complexity might lower overall answer quality.\nWhile the k, K, S, Q + reranker configuration does not lead in Faithfulness, Answer Relevancy, or Absolute Correctness, it achieves the highest Correctness Score (0.8402), outperforming k, Q + reranker by 0.0224, demonstrating the effectiveness of adaptive parameters and reranking in meeting the correctness threshold."}, {"title": "9 Overall Results and Discussion", "content": "Our analysis shows that adaptive methods generally outperform fixed baselines, particularly in improving faithfulness and answer quality. Incorporating adaptive parameters such as query rewrites and reranking enhances the system's ability to provide accurate and relevant responses. While adding a reranker improves correctness, it can slightly reduce the overall correctness score, suggesting a trade-off between precision and answer quality.\nThe introduction of a knowledge graph maintains correctness but can add complexity, potentially lowering overall response quality. However, combining adaptive parameters with a reranker proves effective in maximizing the proportion of correct responses, even if it doesn't lead to the highest scores in all metrics.\nOverall, these findings highlight the importance of adaptivity and careful parameter tuning to balance different performance aspects, enhancing the system's capability to handle varied and complex queries effectively."}, {"title": "10 Limitations and Future Work", "content": "This study has several limitations that suggest areas for future improvement. Correctness evaluation is limited by reliance on a single evaluator familiar with the policy corpus. Averaging a larger quantity of human evaluations would improve reliability. Additionally, our knowledge graph construction process may be improved. For instance, using LLM-based methods for de-duplication and/or custom Cypher query generation to improve context retrieval and precision. Furthermore, our parameter mappings were not rigourously validated quantitatively. Further evaluation of parameter selections could provide better mappings as well as upper and lower bounds to performance. The classifier was trained using domain-specific synthetically generated data - which, though we inject significant noise, may harbour the LLM's own unconcious biases in terms of structure - possibly limiting the generalisability of the classifier on unseen user queries. Also, more classification categories e.g. 4 or 5-class, would permit more granular parameter selections and potentially greater efficiency improvements. Another limitation is that while LL144 is included in the GPT models' training data, subsequent minor revisions may affect the accuracy of these baseline methods.\nIntegrating human feedback into the evaluation loop (see Figure 4) could better align metrics with user preferences and validate performance metrics in real-world settings. Future work should also consider fine-tuning the LLM using techniques like RLHF , RLAIF , or other preference optimisation methods. Further, refining the query rewriter and exploring iterative answer refinement could enhance metrics like relevancy and correctness."}, {"title": "11 Ethical and Societal implications", "content": "The deployment of the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system in AI legal and policy contexts raises critical ethical and societal concerns, particularly regarding the accuracy, reliability, and potential misinterpretation of AI-generated responses. The high-stakes nature of legal information means inaccuracies could have significant consequences, highlighting the necessity for careful evaluation. We emphasize transparency and reproducibility, providing detailed documentation of data generation, retrieval methods, and evaluation metrics to facilitate replication and scrutiny. The environmental impact of NLP models is also a concern. Our system employs adaptive retrieval strategies to optimize computational efficiency, reduce energy consumption, and minimize carbon footprint, promoting sustainable AI development. Our findings enhance the understanding of RAG systems in legal contexts but are intended for research purposes only. HyPA-RAG outputs should not be used for legal advice or decision-making, emphasizing the need for domain expertise and oversight in applying AI to sensitive legal domains."}]}