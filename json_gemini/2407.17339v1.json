{"title": "Preliminary study on artificial intelligence methods for cybersecurity threat detection in computer networks based on raw data packets", "authors": ["Aleksander Ogonowski", "Micha\u0142 \u017bebrowski", "Arkadiusz \u0106wiek", "Tobiasz Jarosiewicz", "Konrad Klimaszewski", "Adam Padee", "Piotr Wasiuk", "Micha\u0142 W\u00f3jcik"], "abstract": "Most of the intrusion detection methods in computer networks are based on traffic flow characteristics. However, this approach may not fully exploit the potential of deep learning algorithms to directly extract features and patterns from raw packets. Moreover, it impedes real-time monitoring due to the necessity of waiting for the processing pipeline to complete and introduces dependencies on additional software components.\nIn this paper, we investigate deep learning methodologies capable of detecting attacks in real-time directly from raw packet data within network traffic. Our investigation utilizes the CIC IDS-2017 dataset, which includes both benign traffic and prevalent real-world attacks, providing a comprehensive foundation for our research.", "sections": [{"title": "1 Introduction", "content": "The rapid proliferation of digital technologies has transformed various aspects of human life, enabling un-precedented convenience and efficiency. Industries such as finance, healthcare, energy, and transportation have especially benefited from these advancements, achieving remarkable improvements in operational efficiency and service delivery. However, this digital revolution has also introduced new vulnerabilities and threats, especially in the realm of cybersecurity.\nFor instance, the financial sector has seen a dramatic increase in cyber-attacks aimed at stealing sensitive data or disrupting services [10]. The healthcare industry, which now heavily relies on digital records and connected medical devices, faces threats that could expose patient safety and privacy [14]. Similarly, the energy sector, with its critical infrastructure increasingly connected to the internet, is a prime target for attacks that could have severe national security implications [19]. Additionally, mili-tary and governmental institutions face significant cybersecurity threats that could undermine national security, disrupt critical operations, and compromise sensitive information [8].\nCyber-attacks have become increasingly sophisticated, posing significant risks to individuals, organ-isations, and nations. These threats range from data breaches and ransomware to advanced persistent threats or industrial espionage. As cyber threats evolve, so too must the methods for detecting and mitigating them. Traditional security measures, such as firewalls and signature-based detection systems, are no longer sufficient to counteract the diverse and sophisticated attacks perpetrated by cybercriminals [3].\nMachine learning (ML) offers a promising solution to these challenges by enabling the development of adaptive, automated, and real-time threat detection systems. ML algorithms can learn from vast amounts of data, identify patterns, and detect anomalies that may indicate a cyber threat [6]. This capability is crucial for industries that require timely and accurate threat detection to protect sensitive information and maintain the integrity of critical operations.\nIn summary, the integration of ML into cybersecurity is not merely a technological advancement, but an imperative for industries confronting evolving cyber threats in today's digital era. Instead of using popular methods, we develop a novel approach where packets are stacked into windows and separately recognised. This innovative method, still relatively unexplored, offers significant potential for further advancements in the field and represents a cutting-edge approach to enhancing cybersecurity measures."}, {"title": "2 Related Work", "content": "The idea about monitoring and protecting computer networks is present in the literature for decades [1].\nThe methods for network intrusion detection systems (NIDS) have evolving along with the development of science. The authors of intrusion detection systems (IDS) used various ML techniques including conventional ML methods like Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF) [25, 18] and deep learning methods e.g., Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM) [12, 15, 21].\nTalukder et al. [25] constructed the framework with integrated ML algorithms. Their approach is using efficient preprocessing, oversampling management, stacking feature embedding, and dimensionality reduction. They evaluated four ML classifiers: DT, RF, Extra Tree (ET), and Extreme Gradient Boosting (XGB) on UNSW-NB15 [7] and CIC IDS-2017 [11] datasets.\nHnamte et al. [22] explored the possibility of using deep convolutional neural networks (DCNN) in the task of network intrusion detection. They provided a comprehensive evaluation of DCNN performance in detecting various types of attacks. The evaluation was performed on publicly available IDS datasets, including ISCX-2012, DDoS (Kaggle), CIC IDS-2017, and CIC IDS-2018.\nLee et al. [12] proposed AI-based security information and event management (SIEM) system. The presented system aims to convert a large amount of security events from multiple sources like IDS, intrusion prevention systems (IPS) or firewalls (FW) to individual event profiles. Their event profiling method, designed for applying artificial intelligence techniques, provides input data with features. They evaluated proposed method on Fully Connected Neural Network (FCNN), CNN and LSTM network.\nCombining a CNN as a spatial feature extractor and LSTM as a temporaly feature extractor can produce a powerful model. Praanna et al. [15] inspired by recent advancements in computer vision tasks decided to use the CNN followed by LSTM network. The experiments on CNN-LSTM model were conducted by KDD99 dataset. Halbouni et al. [20] took the similar approach and improved it by adding dropout and batch normalization layers. The experiments were conducted by CIC IDS-2017, UNSW-NB15, and WSN-DS datasets.\nJose et al. [23] compared effectiveness of three distinct types of neural network models: deep neural network (DNN) with multiple fully connected hidden layers, LSTM, and CNN. The comparison was performed on CIC IDS-2017 dataset.\nNetwork intrusion datasets are characterized by high class imbalance. Such datasets usually contain a lot of benign traffic. Zhang et al. [13] proposed a Parallel Cross Convolutional Neural Network (PCCN) that allows to improves the detection performance of highly imbalanced abnormal flow through feature fusion. They also improved algorithm responsible for feature extraction.\nMost of the method for NIDS is utilising extracted traffic features (Fig.la). Soltani et al. [21] presented a method called Deep Intrusion Detection (DID) that can be applied to both passive and on-line traffic. The proposed method works directly on the raw bytes of contents. The authors are classifying the whole frame of packets as attack or benign (Fig.1b). The authors used a LSTM network for analysing data sequences of traffic flows. Inspired by this approach we propose a novel classification scheme that consider each packet in a window. (Fig.3)."}, {"title": "3 Dataset", "content": "The CIC IDS-2017 [11] dataset is a network intrusion dataset created by Canadian Institute for Cyber-security (CIC) at University of New Brunswick (UNB). The dataset consists of two types of files. The PCAP files with raw traffic data and CSV files where each row corresponds to a flow with flow features and class label. The flows are constructed from raw PCAP files using the CICFlowMeter tool [24] which is also able to extract high-level statistical features like the number of packets per flow, average packet size, etc. Those features were manually engineered based on expert knowledge of traffic characteristics relevant to intrusion detection. Authors of the dataset ensured the diversity of attacks by including the most common attacks based on the 2016 McAfee report. The whole dataset contains seven families of at-tacks: Web based, Brute force, denial-of-service (DoS), distributed denial-of-service (DDoS), Infiltration, Heartbleed and Botnet."}, {"title": "3.2 Preparation", "content": "As the CIC IDS-2017 and improved CIC IDS-2017 datasets contains only flow labels and our work is basing on packet labels we decided to build our own data preprocessing pipeline (Fig.2).\nIn the first stage of the pipeline, PCAP preprocessing is performed using Pcapfix and Reordecap tools if needed. Pcapfix tries to repair broken PCAP files, fixing the global header and recovering the packets by searching and guessing the packet headers. Reordercap is used to reorder and sanitize the PCAP files if needed. It ensures that the packets are in the correct chronological order.\nThe middle part of the pipeline is responsible for assigning labels to packets. The CICFlowMeter tool, which has been improved over the years by its original authors and others, is used for this purpose. Based on flow features generated by CICFlowMeter and the procedure provided by authors of improved CIC IDS-2017 dataset, labels are assigning to flows. The improved CICFlowMeter was extended, what allows us to generate a CSV file with an association between the flows generated by CICFlowMeter and the packets belonging to flows that is used to assign labels to packets.\nIn the final stage of the pipeline all data are aggregated into HDF5 files: raw packet data, packet labels and time deltas from previous packets. The CIC IDS-2017 dataset consists of five PCAP files for five days from monday to friday. This pipeline is run for an each day. Our dataset with packet labels is published at: ai.ncbj.gov.pl/datasets."}, {"title": "4 Methods", "content": "Instead of traditional flow based features, a packet based approach is adopted where at first packets are stacked into windows (subsection 4.3). Then each packet in the window is assigned to a class: attack -1 or benign - 0 (Fig.3).\nTo comprehensively evaluate and compare performance, two types of ML models are employed (sub-section 4.7) based on their input types:\n\u2022 1D window (single packet) input - In the context of single-packet windows, the model takes the feature vector derived from a packet and processes it through several hidden layers, with each layer performing nonlinear transformations on the input data. These transformations allow the network to learn hierarchical representations of features in a single packet.\n\u2022 2D window based models are able to capture dependencies between packets within a window. Unlike single packet analysis, where each packet is processed individually, 2D window based models consider the temporal context of packets, such as consecutive packets within a defined window.\nTo better explain the result of each model, its saliency map is determined (subsection 4.10)."}, {"title": "4.2 Training", "content": "Data from the entire dataset are split into 1000 groups, which are then randomised (subsection 4.4). The order of packets in each group is maintained. This allow the original packet arrangement to be preserved as much as possible while mixing the data. The mixing of the groups for each method tests is the same, so the results are possibly comparable. The data is then split into training, validation and test sets in the proportions of 50%, 10% and 40%, respectively. This process is shown in Figure 4.\nResults on the validation set during training are used to select a model from each method. The model with the highest accuracy is then chosen to check the results on the test set."}, {"title": "4.3 Windows", "content": "Apart from the single packets, 2D packets windows for models learning is used. Shape of the window was determined experimentally based on the packets length histogram (Fig.5).\nThe width of the windows, consisting of 350 bytes of packet data and one byte for the time difference between packets, was selected experimentally to preserve the most influential features in the packets."}, {"title": "4.4 Randomised replacement", "content": "Packets randomisation is perfomed to prevent the model from focusing on the specific data like MAC or IP address. Most of the other solutions [21] that have been found, assume cutting out these particular parts of the packet header. However, removing this data removes some inter-packet dependencies when packets are stacked into a 2D window. Therefore, every packet has a randomised destination and source MAC address. IP addresses and ports are also randomised when they occur. After randomisation, new checksums are calculated. Randomisation is done in such a way that, for example, if the value of a port is changed from one to another, all destination and source ports of that primary value are changed consistently to maintain dependencies. Therefore, it can be called as randomised replacement.\nFigure 7 shows the first 70 bytes of a one package window before and after randomisation. Black colour means value 0, white 255."}, {"title": "4.5 Labeling", "content": "One of the advantages of having each packet labelled separately (Fig.3), is the ability to test two types of packet labeling. The first where only movement from attacker (input traffic) is labelled as an attack (Fig.8a)) and the second where response from target to attacker (output traffic) is also treated as a threat (Fig.8b))."}, {"title": "4.6 Oversampling", "content": "Due to the unbalanced dataset, two cases for each model are considered: training on the imbalanced dataset and training with minority class oversampling on the training dataset.\nThe oversampling is performed to obtain the same number of benign and attack packets, in case of single packet input models, and the same number of windows containing at least one infected packet as number of fully bening windows for the 2D input methods."}, {"title": "4.7 Models", "content": "The fully connected neural network (Fig.9) processes each packet individually, so is unable to capture dependencies between successive packets. The result is based on feature dependencies within each indi-vidual packets. The model comprises three fully connected layers, consisting of 256, 356, and 32 neurons. Two of these layers are followed by batch normalisation and dropout layers.\nModel characteristic and hyperparameters:\n\u2022 input: 1 packet \u00d7 351 bytes\n\u2022 output: 1 binary classification,"}, {"title": "4.7.1 Fully connected neural network", "content": ""}, {"title": "4.7.2 Convolutional neural network", "content": "The convolutional neural network (Fig.10) with a three convolutional filters followed by max-pooling layers was tested. The filters are larger than commonly used shapes [9] [5], such as 9\u00d79 and 7\u00d77. As smaller filters resulted in worse accuracy. Feature maps are followed by 2\u00d72 max pooling layers."}, {"title": "4.7.3 Hybrid neural network", "content": "The hybrid neural network (Fig.11) consists of a 1D convolutional operations with six filters and 1\u00d73 window shape, that extracts spatial patterns within each packet separately. LSTM layers are then adapted to process the sequential information extracted from the convolutional layers. The architecture with 1D CNN performs better than LSTM preceded by 2D CNN and LSTM only model"}, {"title": "4.7.4 Efficient Net-based neural network", "content": "Architecture based on EfficientNet B0 [16], preceded by a convolutional layer to match the required RGB format (Fig.12). Model comprise pretrained Imagenet [2] weights. Model is fine-tuned on new data."}, {"title": "4.8 Metrics", "content": "The following metrics are used for evaluation of classification tasks:\n\u2022 Accuracy: Measures the proportion of correctly classified instances among all instances.\n$Accuracy = \\frac{True Positives + True Negatives}{Total number of packets}$\n\u2022 Precision: Measures the proportion of correctly predicted positive instances among all instances predicted as positive.\n$Precision = \\frac{True Positives}{True Positives + False Positives}$\n\u2022 Recall: Measures the proportion of correctly predicted positive instances among all actual positive instances.\n$Recall = \\frac{True Positives}{True Positives + False Negatives}$\nAccuracy, despite its simplicity and ease of comparison with other solutions, may provide a misleading picture due to class imbalance. Therefore, while accuracy is useful for general comparison purposes with other solutions, precision and recall offer more nuanced insights into model performance. Due to the imbalance, the accuracy is skewed by the most prevalent benign class."}, {"title": "4.9 Loss Functions", "content": "Different loss functions were tested for our task: Binary Crossentropy, Focal Loss, Dice Loss, and Inter-section over Union.\n\u2022 Binary Crossentropy:\n$Binary Crossentropy = \\frac{1}{N} \\sum_{i=1}^{N} [y_{i} log(p_{i}) + (1 - y_{i}) log(1 \u2013 p_{i})]$\nwhere:\nN - is the number of samples in window,\n$Y_{i}$ is the true label,\n$p_{i}$ is the predicted probability.\n\u2022 Focal Loss:\n$Focal Loss = \\frac{1}{N} \\sum_{i=1}^{N} [\\alpha (1-p_{i})^{\\gamma} y_{i} log(p_{i}) + (1 \u2212 \\alpha)p_{i}^{\\gamma} (1 \u2013 y_{i}) log(1 \u2212 p_{i})]$\nwhere:\n\u03b1 is weighting factor used to deal with class imbalance,\n- \u03b3 is focusing parameter. Increasing the value increases the sensitivity to misclassified obser-vations.\n\u2022 Dice Loss:\n$Dice Loss = 1 - \\frac{2 \\sum Y_{i}p_{i}}{ \\sum Y_{i} + \\sum p_{i}}$\n\u2022 IoU:\n$IoU = 1 - \\frac{\\sum Y_{i}p_{i}}{\\sum Y_{i} + \\sum p_{i}}$\nExperimentes with Dice Loss and IoU, treating the problem as a specific segmentation task, yielded very poor results.\nAfter evaluating the performance of each cost function, Binary Crossentropy provided the best results for our task."}, {"title": "4.10 Saliency maps", "content": "Saliency maps are a crucial tool in the field of deep learning and computer vision, particularly for understanding and interpreting the decisions made by neural networks.\nOne common method to compute a saliency map is via gradient backpropagation. The idea is to compute the gradient of the highest output score with respect to the input image [4]. In this case the image is represented as a window of packets. This gradient indicates how much a change in each pixel value would affect the highest output score. Unlike in image classification, we are not interested in a particular area of a particular image, so the results are averaged over the entire batch. Mathematically, this is represented as:\n$S_{ij} = \\frac{1}{B} \\sum_{k=1}^{B} \\frac{\\partial max(y^{(k)})}{\\partial x_{ij}^{(k)}}$\nwhere:\n\u2022 $S_{ij}$ is the saliency value for the data at position (i, j).\n\u2022 $y^{(k)}$ is the vector of output scores predicted by the neural network for all classes for the k-th window in the batch.\n\u2022 $x_{ij}^{(k)}$ is the pixel value at position (i, j) in the input image $X^{(k)}$, which is the k-th window in the batch.\n\u2022 B is the batch size."}, {"title": "5 Results and discussion", "content": "Results obtained on the test dataset from FCNN in the form of confusion matrices from two labelling methods are shown in Tab.1 with the imbalanced dataset and in Tab.2 with attack packets oversampling.\nNot marking backward route as attacks works better the network needs to find less features and also responses taken out of context should be similar to normal network traffic. The confusion matrix generated from the best case is shown. A 0.03% false negatives rate is obtained, which is the most important characteristic to minimise, as it means missing an incoming attack\nSaliency map obtained from the batch from the test dataset (Fig.13) shows that results are strongly dependent on the packet header. Header features are strongly dependent on network characteristic such as topology, size, or type. The most influential byte correspond to the Time to Live value in IP protocol, which indicates the validity period of packet data"}, {"title": "5.1 Fully connected neural network", "content": ""}, {"title": "5.2 Convolutional neural network", "content": "In case of the CNN class balancing does not visibly improve the results (Tab.3) - they are similar to imbalanced ones (Tab.4). In contradiction to single package input models, for the 2D window input not marking backward route as attacks probably makes it difficult to find patterns or features between packets in windows, which results in deterioration of the results. That effect occurs in every window based model.\nThe Metric values in this case are worse than in FCNN, however saliency map shows that features are found in both header and payload. This may result in a better ability to generalise on other datasets (Fig.14). Large convolutional filters cause to find large areas of interest that are visible on the packets payload part of the saliency map (Fig.14)."}, {"title": "5.3 Hybrid neural network", "content": "Confusion matrices obtained from hybrid neural network for two labelling methods are shown in Tab.6. This shows that class balancing has improved the results. The model is unable to generalise and detect even any attacks on the validation and test set when responses are not mark as an attacks (Tab.5). The LSTM part seems to be more sensitive to imbalance data than the other ones used.\nSaliency map (Fig.15) shows that the model takes features similarly from header and payload. The model is able to consider features between packets, which is visible as vertical lines. The model can also detect some interesting packets sequences, indicated by horizontal lines."}, {"title": "5.4 Efficient Net based neural network", "content": "Efficient Net based neural network with pretrained Imagenet weights gives the best result from window input based models for balanced (Tab.7) and unbalanced data (Tab.8). But At the same time, it is much more complex and slower. In addition, saliency map (Fig.16) is laid out uniformly without paying attention to any individual elements, which may lead to the best generalisation ability."}, {"title": "5.5 Comparison", "content": "The accuracy of our result obtained from FCNN model (Tab.9 - green) is better or comparable to most of the methods based on flow features. The most of the highest-value results are based on random forests, which excel at selecting and combining features from packets flow characteristics. However, they would not perform well in packets window based methods due to their struggle with high dimensionality data, the inability to capture spatial correlations, and difficulty in modelling complex patterns. Compared to the flow based methods the obtained recall and precision values are lower, which is expected when considering packets individually in respect to full flows. On the other hand, the results are comparable to those obtained with the DID (LSTM) algorithm (Tab.9 - bolded), noting that when labelling single packets, the metrics will tend to reach lower values than when classifying the full window. The results for window based methods are sub par (Tab.9 - purple) when compared to single packet solution or flow features based models. However the analysis of saliency maps shows that those models should be more robust with other data, which requires further investigation."}, {"title": "6 Conclusions and overlook", "content": ""}, {"title": "6.1 Summary", "content": "The four proposed models demonstrate the ability to classify individual packets, proving that replacing flow-features based models with operations directly on packets is possible. The most important conclusion is that while FCNN has the best theoretical metric values, it is strongly dependent on packets headers. This highlights that explaining the results should be one of the most important aspects of cybersecurity ML approaches.\nThe 2D input based model can result in better generalisation ability on other datasets. Window based models can also work well as initial weights to adapt the model to other dataset or for perform fine-tuning.\nA potentially interesting approach would be the creation of an ensemble model that combines fully connected neural networks (FCNN) with 2D input-based models. By weighted averaging the predictions from these different models, the ensemble could leverage the strengths of each approach, potentially leading to improved overall performance."}, {"title": "6.2 Future plans", "content": "The quickest way to potentially improve performance would be to test windows with a significantly larger number of bytes, check the cutoff at the 85th and 97th percentile of packet lengths (5). This will involve more time to train and process the more powerful GPU or reduce the number of batches during training, increasing training time. Other intriguing possibilities inlcude: dynamic windows shape where the witdh of the window depends on the largest packet in each window, and dynamic window length where number of packets could depend on the selected time value.\nAn Efficient Net based model demonstrates a significant promise by leveraging both pretrained weights and a window based approach, which supports comprehensive learning of various features and patterns throughout packets window. The model's potential is further validated by saliency map analyses, which highlight its capacity for effective generalization. in various datasets. To fully assess and capitalize on this potential, it is essential to test the model on a variety of cybersecurity datasets, such as KDD Cup 1999, and UNSW-NB15, as well as more domain-specific datasets."}]}