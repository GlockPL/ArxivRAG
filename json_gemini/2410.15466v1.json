{"title": "Keep Guessing? When Considering Inference Scaling, Mind the Baselines", "authors": ["Gal Yona", "Or Honovich", "Omer Levy", "Roee Aharoni"], "abstract": "Scaling inference compute in large language models (LLMs) through repeated sampling consistently increases the coverage (fraction of problems solved) as the number of samples increases. We conjecture that this observed improvement is partially due to the answer distribution of standard evaluation benchmarks, which is skewed towards a relatively small set of common answers. To test this conjecture, we define a baseline that enumerates answers according to their prevalence in the training set. Experiments spanning two domains \u2013 mathematical reasoning and factual knowledge \u2013 reveal that this baseline outperforms repeated model sampling for some LLMs, while the coverage for others is on par with that of a mixture strategy that obtains k answers by using only 10 model samples and similarly guessing the remaining k - 10 attempts via enumeration. Our baseline enables a more accurate measurement of how much repeated sampling improves coverage in such settings beyond problem-agnostic guessing.", "sections": [{"title": "1 Introduction", "content": "Scaling training compute \u2013 larger models, larger datasets, and longer training runs \u2013 has been a main driver of progress in LLMs (Kaplan et al., 2020; Hoffmann et al., 2024). Recent works highlighted the benefit of additionally scaling inference compute: sampling longer sequences, e.g., chain-out-thought sampling (Wei et al., 2022; Zelikman et al., 2022b), using increasingly longer input contexts (Levy et al., 2024), and repeatedly sampling model responses (Brown et al., 2024; Hassid et al., 2024). In particular, repeated sampling increases the fraction of problems solved by at least one of k attempts (known as coverage or pass@k) as k grows, as demonstrated across a variety of tasks such as code generation and mathematical reasoning. For verifiable tasks, these gains can be used to post-train models (Hosseini et al., 2024).\nIn this work, we argue that measuring coverage alone overlooks the fact that some commonly used datasets have a closed, virtually enumerable answer set - possibly making them easy-to-guess given enough attempts. This raises a fundamental question: could the observed coverage gains be partially attributed to lucky guesses, rather than uncovering correct reasoning? This observation has implications not only for model evaluation, but also for post-training, when utilizing model-sampled answers for self-improvement. This process relies on selecting chains with verified final answers out of multiple samples, bearing the risk of inadvertently rewarding solutions that are \u201cright for the wrong reasons\", i.e., incorrect reasoning chains ending in a correct final answer.\nTo quantify how much repeated sampling improves beyond guessing, we establish simple baselines and report coverage gains as their relative improvement over these baselines. Specifically, we compare three approaches: (1) MODELANSWERS: Where k candidate answers are obtained by sampling k model responses. (2) TRAINCOUNTS: A baseline where k candidate answers are obtained by enumerating the k most frequent answers in the training set. (3) MIXTURE(M): A mixture strategy, where the first M answers are obtained by MODELANSWERS, and the remaining k - M answers are obtained using TRAINCOUNTS.\nWe experiment with mathematical reasoning (MATH; Hendrycks et al., 2021), where solutions include reasoning chains, and factual knowledge (EntityQuestions; Sciavolino et al., 2021), which includes no chains. We find that:\n\u2022 Normalizing the coverage improvements compared to the baseline reveals that some models actually perform worse than problem-agnostic enumeration (see Figure 1);\n\u2022 Even for the models with high normalized coverage gains, we observe that MIXTURE(M) with small values of M achieves coverage nearly as good as MODELANSWERS (at a small fraction of the cost), suggesting that models either \u201cknow\u201d the correct answer, or cannot do much better than informed guessing.\nOur findings suggest that some commonly used datasets become degenerate when considering large-scale repeated sampling. While inference scaling seems like a promising approach for improving performance, we suggest carefully selecting datasets, models, and baselines when assessing this method, and interpreting results with caution.\""}, {"title": "2 Repeated Sampling", "content": "Recent efforts have scaled inference compute by performing repeated sampling with thousands of samples, focusing on tasks where candidate solutions are evaluated as either right or wrong. Repeated sampling is usually evaluated via (1) coverage, i.e., the fraction of problems that can be solved correctly by at least one of the sampled model answers, and (2) precision, i.e., the ability to identify the correct answer from a set of sampled answers. For tasks with automatic verification (e.g., unit tests for coding) an increase in coverage can translate to model improvements (Hosseini et al., 2024).\nRecent work found striking coverage gains by scaling the number of sampled answers. Brown et al. (2024) showed that while the Pythia-160M model solves only 0.27% of the problems in MATH with a single attempt, the coverage using k = 10,000 attempts reaches as far as 57%. Similarly, Hassid et al. (2024) showed that for code generation, repeated use of smaller models yields consistent improvements, with gains of up to 15% given automatic verification. A possible interpretation of these results is that even very small models are more capable than previously assumed, such that repeated sampling combined with a strong verifier may unlock this seemingly \u201chidden potential.\""}, {"title": "3 Baselines for Repeated Sampling", "content": "We focus on coverage gains due to repeated model sampling, and argue that their proper interpretation requires comparing them against the gains of simple \"guessing\u201d strategies. We thus compare the standard notion of coverage (\u00a73.1) to two simple baselines: answer enumeration based on answer counts in the training data (\u00a73.2) and a mixture strategy that combines answer enumeration with few model samples (\u00a73.3)."}, {"title": "3.1 MODELANSWERS: Repeated sampling", "content": "MODELANSWERS is the standard repeated model sampling. Here, coverage (pass@k) is defined as the expected number of problems that are solved by at least one model answer, when sampling k answers. Following prior work (Chen et al., 2021), we estimate pass@k for each problem i by sampling N = 1000 samples and using the unbiased estimator $1 - {\\binom{N-C_i}{k} \\over {\\binom{N}{k}}}$, where Ci is the number of correct samples for problem i."}, {"title": "3.2 TRAIN COUNTS: Answer Enumeration", "content": "Our na\u00efve guessing strategy, TRAINCOUNTS, relies on obtaining the answer counts in the training data of the dataset under consideration. Here, pass@k is the fraction of problems for which the correct answer is among the top-k most frequent answers in the training set. As this strategy relies on the training-set answer distribution, we refer to it as \"informed enumeration\". Note that TRAINCOUNTS is a problem-agnostic strategy, which predicts the same k answers, regardless of the tested input problem.\nWe stress that we do not suggest using TRAIN-COUNTS as a prediction method, but solely use it to critically examine repeated model sampling."}, {"title": "3.3 MIXTURE: First Sample, Then Guess", "content": "We additionally consider a mixture strategy that combines both model samples and enumerated answers. Specifically, for MIXTURE(M), we obtain M answers by sampling from the model, while the remaining k \u2013 M answers are obtained using the most frequent answers in the training set (as in \u00a73.2). We estimate pass@k as the fraction of problems for which the correct answer is among the M randomly selected model answers or the top k - M answers in the training set (averaged over T random draws from a given set of 1000 sampled model answers). We use M values of 1, 5, and 10."}, {"title": "4 Experimental Setup", "content": "Datasets. We focus on two domains: mathematical reasoning and factual knowledge.\n\u2022 MATH: A dataset of challenging math word problems (Hendrycks et al., 2021). We use the same 128 randomly selected test problems used in Brown et al. (2024).\n\u2022 Entity Questions (EQ): A QA dataset (Sciavolino et al., 2021) with diverse questions about various entities. We sample 128 questions, while maintaining a balanced proportion of relations.\nModels. For MATH, we use the data from Brown et al. (2024), containing three model families: Llama (AI@Meta, 2024), Gemma (Gemma Team et al., 2024b) and Pythia (Biderman et al., 2023). For EQ, we use models from the Gemma 2 (Gemma Team et al., 2024a) and Gemini (Gemini Team et al., 2023) model families. See the full list of models in Appendix B.1.\nObtaining Training Set Counts. In MATH, we obtain answer counts using the entire train split. In EQ, when guessing an answer to a question from a relation r, we select an answer according to counts"}, {"title": "5 Results", "content": "Baselines Can Outperform Thousands of Model Samples. To compare coverage gains obtained from model sampling to guessing-based gains, we calculate a normalized notion of pass@k that quantifies the added gain of using MODELANSWERS compared to TRAINCOUNTS:\n$\\frac{Coverage(MODELANSWERS) - Coverage (TRAINCOUNTS)}{1- Coverage(TRAINCOUNTS)}$\nThe results for MATH are in Figure 1. The results for EQ, showing similar trends, are detailed in Figure 5 in the appendix.\nWhile the unnormalized curves demonstrate significant increases in coverage across all models, explicitly quantifying the gains over an enumeration baseline shows notably smaller gains, with some models (e.g. all Pythia models) performing worse than the baseline.\nThis highlights that repeated-sampling results should be interpreted with caution, especially for datasets that were not originally designed for scenarios of thousands of solution attempts.\nAre Few Samples All You Need? We speculate that models either \"know\" the correct answer or cannot do much better than guessing. To test this, we compare MODELANSWERS with MIX-TURE(M). Results for two models on both MATH and EQ are shown in Figure 2, and for additional models in Figure 6 in the appendix. All results are inline with our hypothesis: while for MATH we observe a considerable gap between TRAINCOUNTS and MODELANSWERS, MIXTURE(M) closes most of this gap, even for small values of M. E.g., for Llama-3-70B, the pass@k values at k = 1000 are 97%, 91% and 73% for MODELANSWERS, MIX-TURE(M) with M = 10, and TRAINCOUNTS respectively. We conclude that sampling as few as 10 model answers and proceeding with guesses yields similar gains as k model samples, though at a significantly lower compute budget. These results are consistent across all k values."}, {"title": "6 Analysis and Discussion", "content": "Weaker than baseline means useless CoTs? We hypothesize that for models who do worse than our simple guessing baseline, reasoning chains that end in a correct answer may be incorrect when closely inspected (Turpin et al., 2024). To test this, we inspect solutions from the best model that underperforms our baseline (Pythia 12B). We sample 10 CoTs that have a correct final answer and find that in 9/10 cases, the reasoning chains are indeed incorrect, while the answer is correct; see Table 1. This highlights the potential risk of common self-improvement pipelines (Zelikman et al., 2022a; Liu et al., 2023), that reward the entire CoT based only on the correctness of the final verdict.\nAnswer enumeration vs i.i.d sampling. Our random baseline differs from MODELANSWERS in that it uses enumeration over a fixed set of candidate solutions, rather than i.i.d sampling from a fixed distribution. Hence, pass@k for repeated sampling typically relies on k' \u226a k unique answers, while for TRAINCOUNTS, it uses k unique values. This design decision of our programmatic baselines aims to shed light on the usefulness of inference scaling, and to point at some of its shortcomings. Inference scaling approaches that would attempt to maximize answer diversity may outperform our baselines with higher margins."}, {"title": "7 Conclusions", "content": "We provide a critical perspective on the coverage gains obtained by repeatedly sampling model answers, showing that for weaker models, the gains are often worse than simple guessing baselines, and that for stronger models, much of the gains can be obtained with as few as 10 model samples. Properly accounting for the actual benefit of repeated sampling is an important and timely objective, given both the potential negative implications of rewarding incorrect CoTs and the computational costs associated with sampling thousands of model responses. Our findings shed light on how commonly used datasets can become degenerate when large-scale sampling is involved, reiterating the importance of using challenging benchmarks, where success by chance is unlikely."}, {"title": "8 Limitations", "content": "Our work critically examines the utility of inference scaling via repeated sampling. We show that when re-scaling the improvements relative to the performance of simple answer enumeration baselines, the gains are less pronounced, with some smaller models even performing worse than the baseline.\nIn our study, we used tasks that have overall structured outputs and demonstrated high performance of answer-counts-based baseline. For tasks that have free-form or longer outputs, this baseline is not directly applicable. However, we believe that most of our observations would still apply. Consider, for example, the question \u201cWhich land mammal has the longest tail?\" from the NaturalQuestions dataset (Kwiatkowski et al., 2019). While the correct answer, giraffe, is not prevalent in the training set, one would still be able to guess it correctly, given enough attempts (e.g., by enumerating the set of known land mammals). Extending our guessing baselines to such datasets \u2013 possibly drawing inspiration from how humans make informed guesses \u2013 is an interesting direction for future exploration.\nFrom a technical perspective, our experiments examine inference scaling by taking up to k = 1000 samples (rather than 10,000, as done by Brown et al. (2024)). We do so due to efficiency considerations and stress that this does not affect our conclusions.\""}, {"title": "A Related Work", "content": "Inference Scaling. Utilizing additional computational resources during inference is carried out across different axis, such as generating more tokens before converging into a final answer (Wei et al., 2022; Kojima et al., 2022; Zelikman et al., 2022a), including increasingly longer input contexts (Shaham et al., 2022; Gemini Team et al., 2023; Bertsch et al., 2024; Levy et al., 2024), or by sampling few model answers and selecting the most consistent one (Wang et al., 2023). Recently, there has been growing interest in large-scale model sampling - i.e., sampling orders of thousands of model answers. Hassid et al. (2024) showed that for code generation, repeated use of smaller models yields consistent improvements, with gains of up to 15% given automatic verification. Brown et al. (2024) tested repeated sampling for code generation and mathematical reasoning, by measuring the coverage \u2013 fraction of problems solved by any attempt for different quantities of model samples, showing that the coverage scales log-linearly with the number of samples. Notably, they manually verified 105 chains of thought of 105 correct Llama-3-8B-Instruct predictions on GSM8K (Cobbe et al., 2021), finding that over 90% of the chains are valid. We show that this is not the case for correct Pythia 12B predictions on MATH, suggesting that the observed coverage gains are more due to \"lucky guessing\" than a result of correct but unlikely answers.\nSelf-Improvement. A common approach for improving the reasoning abilities of LLMs during post-training is self-improvement, which relies on updating a model based on solutions generated by the model itself (Zelikman et al., 2022a; Liu et al., 2023; Gulcehre et al., 2023). In self-improvement, several answers (CoT and final answer) are sampled from the model, while only \"correct\u201d generations are rewarded. A candidate answer can be determined \"correct\" using automatic verification (when applicable, e.g. unit tests in coding problems) or oracle labels (comparing the final answer to a ground truth answer). Since this recipe only considers the final answer as supervision, it may end up inadvertently rewarding answers that are \"right for the wrong reasons\" if such answers are generated by the model to begin with. Our approach provides simple ways to measure whether this behavior is likely by comparing the coverage improvements with those obtained by simple answer enumeration. Recent work has also considered employing intermediate rewards (Ni et al., 2022), providing a finer-grained signal for intermediate steps within the CoT.\nRandom Baselines. Reporting the results of simple baselines has an important role in machine learning, helping to contextualize performance (Lipton and Steinhardt, 2019), diagnose dataset issues (Zheng et al., 2024) and reveal possible shortcut solutions (Geirhos et al., 2020). Such baselines include random baselines, majority (i.e., always predicting the most prevalent class), and simple heuristics. Random baselines are cleanly defined in classification tasks as the expected accuracy of guessing labels uniformly at random. Beyond classification tasks (e.g. natural language generation), however, the strategy itself is not clearly defined. The types of tasks we consider in this work can be approximately viewed as classification tasks, in the sense that the set of possible final answers is approximately enumerable. In the context of LLMs, (Yauney and Mimno, 2024) recently proposed a stronger random baseline that accounts for scenarios of reusing the evaluation data."}, {"title": "B Experimental Setup", "content": "B.1 Models\nFor MATH, we use the data from Brown et al. (2024), spanning three model families:\n\u2022 Llama 3: Llama-3-8B, Llama-3-8B-Instruct, Llama-3-70B-Instruct (AI@Meta, 2024).\n\u2022 Gemma: Gemma-2B, Gemma-7B (Gemma Team et al., 2024b).\n\u2022 Pythia: Pythia-70M through Pythia-12B (eight models in total) (Biderman et al., 2023).\nFor EQ, we use models from the Gemma 2 and Gemini model families:\n\u2022 Gemini: Gemini-Flash, Gemini-1.5-Pro (Gemini Team et al., 2023).\n\u2022 Gemma 2: Gemma 2-2b-it, Gemma-2-9b-it (Gemma Team et al., 2024a).\nB.2 Answer Verification\nMeasuring the coverage requires verifying the correctness of each candidate answer, whether model-sampled or guess-based. For MATH, we use an"}]}