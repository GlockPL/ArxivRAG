{"title": "Heterogeneous Relationships of Subjects and Shapelets for Semi-supervised Multivariate Series Classification", "authors": ["Mingsen Dua", "Yongjian Li", "Meng Chen", "Cun Ji", "Shoushui Wei"], "abstract": "Multivariate time series (MTS) classification is widely applied in fields such as industry, healthcare, and finance, aiming to extract key features from complex time series data for accurate decision-making and prediction. However, existing methods for MTS often struggle due to the challenges of effectively modeling high-dimensional data and the lack of labeled data, resulting in poor classification performance. To address this issue, we propose a heterogeneous relationships of subjects and shapelets method for semi-supervised MTS classification. This method offers a novel perspective by integrating various types of additional information while capturing the relationships between them. Specifically, we first utilize a contrast temporal self-attention module to obtain sparse MTS representations, and then model the similarities between these representations using soft dynamic time warping to construct a similarity graph. Secondly, we learn the shapelets for different subject types, incorporating both the subject features and their shapelets as additional information to further refine the similarity graph, ultimately generating a heterogeneous graph. Finally, we use a dual level graph attention network to get prediction. Through this method, we successfully transform dataset into a heterogeneous graph, integrating multiple additional information and achieving precise semi-supervised node classification. Experiments on the Human Activity Recognition, sleep stage classification and University of East Anglia datasets demonstrate that our method outperforms current state-of-the-art methods in MTS classification tasks, validating its superiority.", "sections": [{"title": "I. INTRODUCTION", "content": "Multivariate time series (MTS) classification is of significant importance in many practical applications, especially in fields such as industrial control [1], healthcare monitoring [2], financial market analysis [3], and human activity recognition (HAR) [4].\nHowever, in many practical situations, labeled data is scarce, and the manual labeling process is time-consuming and often requires expert knowledge. Supervised models are typically optimized for data from specific tasks or domains, which can lead to poor generalization when faced with new tasks or different data distributions. Additionally, label noise or annotation errors can directly affect model performance; in the case of MTS, inaccurate labels may hinder the model's ability to effectively capture complex temporal relationships. Therefore, there is an urgent need to investigate methods that can effectively perform MTS classification with limited labeled data.\nUnsupervised [5], [6]or self-supervised [7], [8] representation learning eliminates the need for extensive labeled data and can fully utilize unlabeled data for temporal representation learning, thereby enhancing the model's generalization ability and applicability. Such methods can extract underlying structures and patterns. Compared to supervised learning, these methods generally possess stronger generalization capabilities because they learn more universal feature representations rather than those targeted at specific tasks or labels. Additionally, these approaches are less sensitive to label noise and can adaptively improve the model's representation learning ability as the data volume increases, making them more robust when dealing with noise and incomplete data. Moreover, existing methods for MTS still face the following challenges.\nFirstly, MTS are typically generated by multiple sensors, each capturing different types of features, making their high dimensionality a challenge for time series representation learning [9], among which Transformer [10] shines in representation learning. Currently, most Transformer-based methods often embed all channels into a fixed-length vector and use self-attention mechanisms to capture the correlations between time points. However, this approach has several obvious issues. First, embedding all channels into a unified representation neglects the independence among channels. Next, computing self-attention for time point tends to make the model focus more on global information, overlooking local feature variations. Moreover, as the sequence length increases, the computational complexity of Transformers grows quadratically, resulting in low computational efficiency.\nIn contrast, calculating features for each channel separately and treating the sliding window as a token can better capture the temporal dynamics of each channel while preserving the independence among channels. By treating the sliding window as a token, the model can focus on changes and patterns within specific time periods, aligning better with the importance of local features in real-world scenarios. Additionally, this method offers higher sparsity by calculating attention on local tokens, improving overall computational efficiency.\nSecondly, the correlation between dimensions in MTS is an essential feature, leading to the emergence of numerous graph modeling methods based on graph neural networks (GNN) [11]. Current research on graph modeling methods using GNN typically focuses on modeling relationships between dimensions [12]. In recent years, GNN-based graph modeling approaches have gained widespread attention and have been applied to capture relationships between dimensions. Current studies employ GNN to model the homogeneous or heterogeneous [13], [14] relationships between MTS dimensions. In homogeneous graph modeling, different dimensions are treated as nodes of the same type, with the focus on capturing similarities or correlations between dimensions and extracting global features through a shared graph structure. Methods for handling heterogeneous networks [15]\u2013[17] requires consideration of structural information composed of various types of nodes and edges. The heterogeneity and rich semantic information present significant challenges in designing heterogeneous graph neural networks.\nFinally, methods lack interpretability. Shapelets [18]-[20] are interpretable local subsequences that can maximally represent a class of samples and reflect the basis of classification decisions, making it easier for experts to understand their significance. However, in MTS classification, due to subject differences, time series of the same category may exhibit different shapes of shapelets. When dealing with data involving subject differences, capturing more targeted feature variations becomes particularly important. For instance, in HAR tasks, data may come from populations of different ages. Biomedical signals often originate from the physiological characteristics of different patients. These subject differences can lead to variations in time series patterns or shapelets.\nBy generating personalized subject shapelets [21], models can more accurately capture the unique time series patterns of each subject or category, rather than only relying on general global features. The extraction of personalized shapelets allows the model to effectively distinguish features between different subjects, thereby enhancing the robustness and interpretability ability of classification.\nTherefore, to solve the above problems, we propose heterogeneous relationships of subjects and shapelets method with heterogeneous graph representation learning (HGRL) for semi-supervised MTS classification, which can integrate subject features and shapelet information as supplementary information while capturing the relationships between them. Specifically, we first obtain sparse representations of MTS through a contrast temporal self-attention module, which adopts a new positive and negative data sampling strategy with the contrast temporal representation loss to obtain the MTS representation. Then, we use soft DTW to model the similarity between MTS representations, constructing a representation graph. Next, we learn the exclusive shapelets for the subjects. To achieve this, we incorporate subject features and the corresponding shapelets as additional information in the representation graph, resulting in the final heterogeneous graph. Finally, dual GAT introduce node-level attention to learn the importance between nodes, and type-level attention to manage the relationships between different types of information (such as shapelet features, subject features, and MTS representation features), ensuring the effective integration and expression of multi-type information. Our contributions are as follows:\n1) To the best of our knowledge, we are the first to represent MTS dataset as heterogeneous graph and integrate subject features and shapelet information for MTS node classification.\n2) We develope a contrast temporal self-attention, which picks principal dimensions in the sample and uses contrast temporal loss for MTS representations learning.\n3) We make our method interpretable by learning multi-scale shapelets with subject differences.\n4) We propose a heterogeneous graph representation learning method, and conducted experiments on multiple MTS datasets to validate the superiority."}, {"title": "II. RELATED WORK", "content": "Recently, some GNN-based methods graph classification and node classification have been proposed. For graph classification, we introduce it from the aspects of homogeneous graph, heterogeneous graph.\nFor homogeneous graph, Time2Graph++ [22] addresses this by introducing a time-level attention mechanism to extract time-aware shapelets. Subsequently, time series data is transformed into shapelet evolution graphs to capture evolving patterns, further enhanced by graph attention to improve shapelet evolution. TS-GAC [23] introduces graph augmentation techniques for nodes and edges to maintain the stability of sensors and their associations. Subsequently, robust sensor and global features are extracted through node- and graph-level contrast.\nFor heterogeneous graph, SleepHGNN [15] captures both the interactivity and heterogeneity of physiological signals through its heterogeneous graph transformer layers. These layers consist of a heterogeneous message passing module to handle signal heterogeneity and a target-specific aggregation module to model signal interactivity. MTHetGNN [13] features a relation embedding module to represent complex relationships among variables as graph nodes and edges, along with a temporal embedding module for feature extraction. The model integrates these components to capture both static and dynamic dependencies.\nAnother application of GNN is node classification [24], where each sample in the data set is regarded as a node. Recently, several research [25] treats time series as nodes in a graph, with node similarity calculated using DTW and incorporated into the GNN. Due to the high complexity of DTW, further research [26] uses the lower bound of DTW to calculate similarity, reducing the time complexity."}, {"title": "B. Representation learning based methods", "content": "MTS representation mainly consists of time series representation and its graph representation. TS-TCC [8] creates two correlated views of time-series data through weak and strong augmentations. It uses a temporal contrasting module for robust representation learning via cross-view prediction. And it uses a contextual contrasting module to enhance discrimination by maximizing similarity within a sample and minimizing it across samples. AVGNets [27] reconstructs weighted graphs using angular visibility to retain both sequential and structural information in time series, addressing information loss in traditional visibility graph encodings. It features a ProbAttention module to extract temporal dependencies across multilayer graphs, enhancing representation learning and improving forecasting performance."}, {"title": "C. Semi-supervised methods", "content": "In recent years, a large number of semi-supervised methods have been proposed. SSL [28] enhances semi-supervised learning by leveraging both time-domain and frequency-domain views through temporal-frequency co-training. It trains two deep neural networks simultaneously, using pseudo-labels from one view to guide the other, and incorporates a supervised contrastive learning module to improve representation quality. SSML [29] presents a semi-supervised metalearning framework combined with adversarial training to tackle multi-source heterogeneity in time series data, focusing on heterogeneous features and labeling uncertainty. DiffShape [30] utilizes a semi-supervised and self-supervised diffusion learning mechanism that conditions on real subsequences, enhancing the similarity between learned shapelets and actual subsequences using abundant unlabeled data. It also employs a contrastive language-shapelets strategy, integrating natural language descriptions to improve the discriminability of learned shapelets."}, {"title": "III. METHOD", "content": "Previous research has typically enriched time series representations by extracting various features and methods. However, these approaches often overlook additional information, such as subject differences(e.g. variations in data collect timestamps). In contrast, our method integrates such extra information and models the complex relationships between these factors and the time series.\n1) Specifically, firstly, we use a contrast temporal self-attention (CTSA) module to obtain sparse representations, and soft DTW to model similarity graph representation (Section III-B).\n2) Secondly, we learn subject shapelets for each subject type in the dataset. To do this, we utilize a multi-task loss that includes both subject label loss and sample label loss (Section III-C).\n3) Thirdly, we use T5 [33] model to embed subject information as subject features, without affecting the training weights (Section III-E).\n4) Consequently, we add subject features and shapelets as additional information for the similarity graph, forming a heterogeneous graph (Section III-F).\n5) Finally, we apply a dual level GAT to aggregate information from heterogeneous graph. It includes nodelevel attention, which learns the importance of nodes and effectively identifies the most influential samples, and type-level attention, which captures relationships between different types (shapelets, subject features and MTS representations) (Section III-G)."}, {"title": "B. Similarity Graph Representation Learning", "content": "We use a CTSA module with contrast temporal loss to obtain MTS sparse representations (Section III-B1). Then, we apply the soft DTW loss to model the similarity between these representations, thus obtaining similarity graph representation (Section III-B2).\n1) CTSA: Current self-attention-based methods directly embed all channels into fixed-length vectors and obtain correlations by calculating self-attention between time points. While it treats the entire time series as a whole, making it ineffective in capturing local patterns, while the dense computations of the self-attention mechanism result in extremely high computational costs.\nTo solve above problems, wo introduce temporal selfattention module. Then, to obtain MTS representations, we introduce contrast temporal loss, allowing the model to learn features based on the similarity between anchor, positive, and negative samples without relying on labels, making it wellsuited for self-supervised learning. By combining the sliding window method with contrast temporal loss, we can enhance the model's sensitivity to local features and variations within the MTS while maintaining computational efficiency. The overview stucture of CTSA is shown in Fig. 2.\nTemporal Self-Attention: We use sliding window to partition MTS into overlapped segments (i.e., tokens) and calculate the attention between these tokens as the following steps."}, {"title": null, "content": "First, the sliding window is treated as a token to capture attention between tokens, as it is meaningful to study the characteristics or changes of time series over a specific period. We apply a mask matrix to ensure that attention calculations are performed only within the same dimension. Additionally, we designed another mask matrix to prevent self-attention computation when there is an large overlap in the tokens of the current dimension. Overlapping subsequences often contain repeated information, and calculating the self-attention between them may lead to redundancy. Therefore, not calculating the attention of subsequences that overlap too much can make the model more focused on capturing unique features and changes, improving learning efficiency. This sliding window method ensures that attention can focus on meaningful local features and changes within each dimension, allowing for better capture of the local characteristics of each time series. Furthermore, by calculating attention separately for each dimension and leveraging the sparsity of the generated attention matrix, we can significantly reduce computational overhead and improve model efficiency.\nLet the time series $T \\in \\mathbb{R}^{C \\times L}$, where $C$ is the number of channels and $L$ is the length of the time series. And each token has a length of $W$ and is sampled at intervals of $S$. Then, each dimension is segmented into tokens with window length $W$, sliding step $S$ as $tokens = \\{T[i : i + W] | i = 0, S, 2S, ..., L \u2013 W\\}$. After getting tokens $T_{to}$, we compute the self-attention between tokens. For tokens $T_{t}$, we calculate query, key, and value as (1), where $W^{Q}$ is query weight, $W^{K}$ is key weight, $W^{V}$ is value weight.\n$Q_{to} = W^{Q}T_{to}, K_{to} = W^{K}T_{to}, V_{to} = W^{V}T_{to}$   (1)\nThe attention weight $A_{t}$ is calculated a (2) combining the masking matrices to calculate sparse weighted attention. In (2), $M_{1}$ ensures that attention calculations are performed only within the same dimension as (3). $M_{2}$ prevent selfattention computation when there is an large overlap ratio $\\psi_{1}$ in the tokens of the current dimension as (4). $I$ represents the indicator function, which returns value of 1 indicates that the condition is met, and returns value of 0 indicates that the condition is not met.\n$A_{t} = softmax(\\frac{Q_{to} K_{to}^{T}}{\\sqrt{d_{k}}} \\cdot M_{1} \\cdot M_{2}) \\cdot V_{to}$ (2)\n$M_{1} = I (dim(T_{t}^{Q}) = dim(T_{t}^{K}))$ (3)\n$M_{2} = I (overlap(T_{t}^{Q}, T_{t}^{K}) < \\psi_{1})$ (4)\nContrast Temporal Loss: Specifically, the anchor is drawn from continuous overlapping subsequences in certain dimensions, while the positive sample is from the same dimension and overlaps with the anchor. The negative sample is selected from subsequences in different dimensions that show significant feature differences from the anchor.\nThe overlap between the anchor and the positive sample enables the model to capture subtle variations within the same dimension, thereby improving the learning of local features."}, {"title": null, "content": "Meanwhile, the introduction of negative samples encourages the model to focus on distinguishing features across different dimensions during training. By selecting negative samples from different dimensions, the model effectively reduces its sensitivity to noise and interference, improving its generalization ability when handling complex time series data. We borrowed the idea of triplet loss [31], which aims to minimize the distance between the anchor and the positive sample while maximizing the distance between the anchor and the negative sample. This optimization helps form clearer decision boundaries in the feature space, leading to more accurate similarity comparisons.\nHowever, there is still a question: how to choose the dimension to keep the most features of MTS. To do this, we select principal component dimensions instead of random dimensions. Principal component analysis identifies the dimensions that capture the most variance. These dimensions are more likely to hold meaningful patterns, making them more informative for downstream tasks. Using principal components ensures that selected features are those that most effectively represent the data's underlying structure.\nWe find the principal dimensions by followings.\nFirst, we perform data centering by $T_{centered} = T - \\mu$. Next, we compute the covariance matrix $M_{c}$ of the centered time series by $M_{c} = \\frac{1}{n} T_{centered} T_{centered}^{T}$. Here, $n$ is the number of time points in the series.\nFinally, we perform eigenvalue decomposition on the covariance matrix $M_{c}$ to compute its eigenvalues $\\lambda_{i}$ and corresponding eigenvectors $v_{i}$ using $M_{c}v_{i} = \\lambda_{i}v_{i}$. The principal components correspond to the eigenvectors with the largest eigenvalues $\\lambda_{i}$, and the dimensions associated with those eigenvectors are the principal component dimensions $T_{p}$. After getting principals, we select positive and negative samples by the following criteria, as shown in Fig. 3.\nAnchor Samples $T_{ref}$: The anchor point is chosen from the principal component dimensions. We select m dimensions from the principal components of the time series and obtain continuous tokens from these dimensions to serve as the anchor $T_{ref}$. For example, at time $t_{1}$ in dimension $i$, the anchor can be defined as:\n$T_{to}^{ref} = \\{T_{i, t_{1}}^{to}, T_{i, t_{1}+S}^{to}... T_{i,t_{1}+(W-1)S}^{to}\\}$ (5)\nPositive Samples $T_{Pos}$: Select other tokens that are continuous and overlap with the anchor $T_{ref}$. The positive samples come from the same principal component dimension, but with an overlapping time range as the anchor $T_{ref}$. If the anchor covers the time range $[t_{1}, t_{1} + W)$, positive samples can be selected as:\n$T_{to}^{pos} = \\{T_{i, t_{1}+k}^{to}, T_{i, t_{1}+k+S}^{to}... T_{i,t_{1}+k+(W-1)S}^{to}\\}$ (6)\nwhere $k$ can take values $0, S, 2S,...$ (as long as $t_{1} + k + W$ does not exceed the length of the series).\nNegative Samples $T_{Neg}$: Negative samples are chosen from non-principal component dimensions. These are tokens that come from other dimensions, excluding those selected as principal components. Negative samples can be expressed as:\n$T_{to}^{neg} = \\{T_{j, t_{2}}^{to}, T_{j, t_{2}+S}^{to} \\oplus ... \\oplus T_{j,t_{2}+(W-1)S}^{to}\\}$ (7)\nwhere $j \\neq i$, meaning they are from dimensions other than the anchor's principal components.\nAfter obtaining positive and negative samples, we obtain the MTS representations as (8). Where $f(\\cdot, \\theta)$ is embedding from CTSA module function with parameters $\\theta$.$\\sigma (cos (f(T_{ref}^{to}, \\theta), f(T_{pos}^{to}, \\theta))) = \\frac{f(T_{ref}^{to}) \\cdot f(T_{pos}^{to})}{(\\lVert f(T_{ref}^{to}, \\theta) \\rVert \\lVert f(T_{pos}^{to}, \\theta) \\rVert)^{-1}}$. $\\sigma$ is the sigmoid function. K is the number of negative samples.\n$\\mathcal{L}_{rep} = -log (\\sigma (cos (f(T_{ref}^{to}, \\theta), f(T_{pos}^{to}, \\theta)))) - \\sum_{k=1}^{K} log (\\sigma (cos (f(T_{ref}^{to}, \\theta), f(T_{neg_{k}}^{to}, \\theta))))$ (8)\nThe triplet construction (anchor, positive and negative samples) and cosine similarity calculation can effectively learn discriminative MTS representations. This approach refines the embedding space, focusing on principal components and critical features, thereby enhancing the accuracy and reliability of subsequent similarity calculations."}, {"title": null, "content": "To align the temporal dynamics for a more accurate similarity of MTS representations, we note that when using DTW to calculate highdimensional MTS, the dimensions are typically aggregated to compute an average. This approach, however, results in the loss of various key features inherent in each dimension. To address this issue, we employ the CTSA module to obtain a robust MTS representations $T_{rep}$. This enables us to use DTW to achieve a more efficient and effective similarity representation.\nTo tackle the challenge of non-minimizability in the DTW algorithm, [32] introduced the concept of soft minimum as a substitute for the DTW minimum. They designed the minimizable soft DTW loss function to model similarity through the following steps. The soft DTW loss function combines the smooth minimum operator with dynamic programming to compute a differentiable and robust measure of alignment"}, {"title": null, "content": "between time series, which can be used in optimization tasks such as training time series models.\nWe first compute the accumulated cost using the dynamic programming recursion as $r_{i, j} = \\lVert T_{rep}^{i} - T_{rep}^{j} \\rVert^{2}$. The cost matrix $r_{i, j}$ is defined as the squared Euclidean distance between the i-th element of time series $T_{rep}^{i}$ and the j-th element of time series $T_{rep}^{j}$. Then, the dynamic programming matrix $D_{i, j}$ accumulates the costs for aligning subsequences of $T_{rep}^{i}$ and $T_{rep}^{j}$, and is computed recursively as $D_{i, j} = r_{i, j} + min_{{y}2} \\{D_{i, j-1}, D_{i-1, j}, D_{i-1, j-1}\\}$.\nNext, we compute the smooth minimum operator, as expressed in (9). The operator is defined with a smoothing parameter $\\gamma_{2}$, which determines how much smoothing is applied to the minimum operation. For $\\gamma_{2} > 0$, the operator computes a smoothed version of the minimum by applying a weighted sum of exponentials:\n$min\\{a_{1}, ..., a_{n}\\} =  \\begin{cases} min\\{a_{1}, ..., a_{n}\\}, & \\text{if } \\gamma_{2} = 0 \\\\ -\\gamma_{2} log (\\sum_{i=1}^{n} e^{-a_{i} / \\gamma_{2}}), & \\text{if } \\gamma_{2} > 0  \\end{cases}$ (9)\nFinally, we define the soft DTW loss as (10), where $DTW_{\\gamma_{2}} (T_{i}, T_{j})$ is the soft DTW distance. The alignment matrix set $A_{n, m}$ consists of all possible binary alignment matrices of size $n \\times m$, where each element $A$ represents a specific alignment between the two sequences:\n$DTW_{\\gamma_{2}} (T_{rep}^{i}, T_{rep}^{j}) = min\\{(\\langle A, \\Lambda(T_{rep}^{i}, T_{rep}^{j})\\rangle) | A \\in A_{n, m}\\}$ (10)\nwhere $(\\Lambda, \\Lambda(\\cdot, \\cdot))$ is the inner product, which quantifies the alignment cost between $T_{rep}^{i}$ and $T_{rep}^{j}$. $A_{n, m}$ represents the set of all possible binary matrices $A \\in \\{0, 1\\}^{n \\times m}$, where each matrix corresponds to a valid alignment of the two sequences. A specific matrix A has values of 1 for the points along the alignment path and 0 elsewhere.\nAfter getting the DTW matrix: $DTW_{\\gamma_{2}}(T_{rep}^{i}, T_{rep}^{j}) = D$, to further describe the similarity, we perform the following processing. The similarity matrix $D_{sim}$ is obtained by the following formula as (11) - (12).\nWe first compute the normalized distance between elements $i$ and $j$ as $D_{i, j}^{norm}$.\n$D_{i, j}^{norm} = (D_{i, j} - D_{min}) \\cdot (D_{max} - D_{min})^{-1}$ (11)\nAfter normalization, the distance values are converted to similarity values using an exponential function with a scaling parameter $\\alpha$, which controls the rate of decay in similarity as distance increases.\nThe final similarity matrix $D_{sim}$ is defined by 12. $\\alpha \\in [0, \\infty)$ is a tuning parameter that controls the sensitivity of the similarity to distance values. Higher values of $\\alpha$ make the similarity decay faster as the distance increases.\n$D_{sim} = exp(\\alpha \\cdot D_{i, j}^{norm})^{-1} + 1$ (12)"}, {"title": "C. Subject Shapelets Learning", "content": "We introduce subject-specific shapelets in the heterogeneous graph framework, allowing the model to capture specific patterns of different subjects. This is particularly important for applications such as HAR, industrial monitoring, or biomedical signal analysis, which often show subject differences due to factors such as age differences, operating conditions, or patient physiological characteristics.\nWe build a multi-task model using subject type as label and sample label for multi-scale shapelets learning. The model can learn features that distinguish different subject types and learning corresponding shapelets.\nShapelets learning is completed by the following two tasks. Sample shapelet learning: Get sample prediction probabilities $\\hat{y}_{sample}$. Subject shapelet learning: Get subject prediction probabilities $\\hat{y}_{sub}$. In detail, there are following steps:\nFor each scale $s$ (where l represents the scale), we compute the Euclidean distance between the shapelet $s_{k}^{l}$ and subsequence $T_{r}^{m}$ of time series $T^{m}$ as:\n$d_{k, m}^{r,l} = \\sqrt{\\sum_{i=1}^{L_{s}} (s_{k, i}^{l} - T_{r, x+i-1}^{m})^{2}}$ (13)\nwhere $s_{k}^{l}$ is the $l$-th scaled shapelet. $L_{s}$ is the length of the shapelet at scale $l$. $s_{k, i}^{l}$ is the $i$-th element of the shapelet $s_{k}^{l}$. $T_{r, x+i-1}^{m}$ is the $(x + i - 1)$-th element of the $m$-th time series.\nNext, we compute the distance between the shapelet $s$ at scale $l$ and the entire time series $T^{m}$. The Soft-Minimum calculation is adapted for multi-scale shapelets:\n$D^{r}(s_{k}^{l},T^{m}) = SoftMin(d_{k,1}^{r,l}, d_{k,2}^{r,l}, ..., d_{k,j}^{r,l})$ (14)\nwhere $d_{k, m}^{r, l}$ is the distance between the $l$-scaled shapelet and the m-th subsequence of the m-th time series at scale $l$. The Soft-Minimum formula is computed as:\n$D(s_{k}^{l}, T^{m}) = \\sum_{i=1}^{j} \\frac{e^{-\\delta d_{k,m}^{r,l}}}{\\sum_{k'=1}^{K} e^{-\\delta d_{k,m}^{r,l}}}$ (15)\nwhere $\\delta$ is a scaling factor, and $j$ is the number of subsequences in the time series.\nFinally, the prediction, soft minimum loss, and total loss calculations are computed as follows:\n$y = W + \\sum_{k=1}^{K} D(s_{k}^{l}, T^{m}) W_{k}$ (16)\n$\\mathcal{L}_{sam} = -\\frac{1}{MC} \\sum_{m=1}^{M} \\sum_{c=1}^{C} \\hat{y}_{m}^{c} log(\\hat{y})$ (17)\n$\\mathcal{L}_{s} = \\lambda\\mathcal{L}_{sam} + (1 - \\lambda)\\mathcal{L}_{sub}$ (18)\nwhere $\\hat{y}_{m}^{c}$ is the true label for the m-th sample in class c. $W_{k}$ is the weight for the $k$-th shapelet in the classifier for class c.\nFurther, to reduce redundancy among shapelets, we compare the similarity between shapelets at different scales. If two shapelets $s_{1}^{l}$ and $s_{2}^{l}$ are too similar, or if one shapelet is a subset of another, we discard one of them to reduce redundancy. The similarity between shapelets can be computed using their soft DTW distance $DTW(s_{1}^{l}, s_{2}^{l})$ as (10). If the"}, {"title": null, "content": "similarity between shapelets exceeds a threshold $\\tau_{sim}$, or if the shapelet at one scale is too close to another, the redundant shapelet $s_{k}^{l}$ is removed. Specifically, we discard shapelet $s_{2}^{l}$ if:\n$Sim(s_{1}^{l}, s_{2}^{l}) < \\tau_{sim}$ (19)\nwhere $\\tau_{sim}$ is a threshold to determine whether two shapelets are too similar."}, {"title": "D. Shapelets Positioning", "content": "Wo usually use window to search similarity, but convolution offers several advantages over sliding windows for similarity calculation: It reduces computational overhead through optimized matrix operations, ideal for large datasets or long time series. It enables simultaneous similarity calculations, outperforming sequential sliding windows. It integrates well with GPU or hardware acceleration, making it effective for high-dimensional or multivariate time series.\nAfter we extracted a set of representative shapelets $s_{1}, s_{2},..., s_{n}$ from the dataset, to find which MTS these shapelets correspond to, each shapelet is treated as a convolution kernel. Then we calculate the convolution similarity between shaplets and MTS. Let $s$ be a shapelet with length $L_{s}$. The convolution operation is performed by sliding the shapelet over the time series $T$ as (20). The convolution result $C(i)$ represents the similarity between the shapelet and the subsequence of the time series at position $i$.\n$C(i) = \\sum_{j=0}^{L_{S}-1} T(i + j) \\cdot s(j)$, for $i = 0, 1, ..., L_{t} - L_{s}$ (20)\nA threshold $\\epsilon$ is set to determine whether the similarity is significant as (21). If the convolution result at a certain position is less than $\\epsilon$, it indicates that the subsequence at position $i$ is considered to be similar to the shapelet $s$.\n$maxC(i) > \\epsilon$ (21)"}, {"title": "E. Subject information embedding", "content": "To prevent subject label information from interfering with weight updates, obtain subject features, and differentiate between each individual, we use the T5 [33] model to embed subject text label information. For example, the label can be \"The patient is 20 years old\" or \"The patient has a certain disease\", which will be embedded to feature vector. If the dataset does not have text information, we convert the subject label information into one-hot encoding to serve as subject features."}, {"title": "F. Heterogeneous Graph Constructing", "content": "In this study, we consider two additional types of information: shapelets and subject differences. As shown in the figure, we construct a heterogeneous graph containing MTS data. The set of nodes is $V = V_{T} \\cup V_{s} \\cup V_{sub}$. $T = \\{T_{1},...,T_{m}\\}$ represents the MTS. $s = \\{s_{1}, ..., s_{k}\\}$ represents the shapelets in the time series. $sub = \\{sub_{1},...,sub_{n}\\}$ represents subject differences, such as different patients, age groups, or data collection timestamps. The set of edges E represents the relationships between MTS T, shapelets s, and subject differences sub.\nThe steps of constructing the heterogeneous graph are as follows:\nShapelet: We have used a shapelet mining algorithm to extract significant local patterns $s$ (III-C). These patterns are highly representative of certain categories. Then, for each MTS, we establish connections to the shapelets. If a MTS contains a particular shapelet, an edge is created between MTS and the corresponding shapelet. For each subject, if the shapelet comes from the corresponding subject, an edge is established. Subject Differences: To capture subject differences in the dataset III-E (e.g., different patients, age groups, or variations in data collection timestamps), we establish connections between each MTS and the relevant subject information. For instance, if a MTS is associated with a particular age group of patients or collected during a specific time period, an edge is created between that MTS and the corresponding subject node. In this way, we incorporate shapelet and subject differences into the heterogeneous graph representation.\nAfter obtaining the above information, we consider the cross-relationship between them consisting of MTS, subjects, and shapelets. Based on these relationships, we built a heterogeneous graph, and the graph is symmetric, meaning the relationship between any two connected nodes is bidirectional or mutual. The heterogeneous graph A is represented as Fig. 8, which is composed of various block matrices M, where N is the number of dimensions. The each parts of heterogeneous graph A is detailed as follows:\nMTS and MTS: Connected based on similarity (detailed in Section III-B2)). MTS and subject: Linked through corresponding relationships (e.g., a subject is associated with a specific MTS). MTS and shapelet: Connected through similarity, where a shapelet originates from a specific MTS (detailed in Section III-D). subject and subject: We do not create edges between them and set them to 0. subject and shapelet: Linked through corresponding relationships (e.g., shapelets belong to subjects). Shapelet and shapelet (computed using soft DTW loss as Section III-B2)): Connected based on similarity (e.g., similar patterns among shapelets). After obtaining the heterogeneous graph, we need to embed each type of information as node features, that is, MTS representation (detailed in Section III-B2)), subject features (detailed in Section III-E) and subject shapelets (detailed in Section III-C)."}, {"title": "G. Heterogeneous Graph Representation Learning", "content": "We employ a dual level GAT to learn representations for heterogeneous graphs, consisting of type-level and nodelevel attention mechanisms. This mechanism enhances heterogeneous GAT by focusing on significant types and nodes, capturing richer relational structures.\nThe normalized adjacency matrix is $\\bar{A} = M^{-1/2} A^{'} M^{-1/2}$. The node representations in layer $l$ are denoted as $G^{(l)} \\in \\mathbb{R}^{|V| \\times q}$, with $G^{(0)} = F_{node}$. A heterogeneous graph with node types are as MTS $T_{rep}$, subjects $F_{sub"}]}