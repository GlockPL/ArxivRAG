{"title": "Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models", "authors": ["Zikai Xie"], "abstract": "Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the \"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that \"9.11>9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.", "sections": [{"title": "Introduction", "content": "In the field of natural language processing (NLP), large language models (LLMs) have achieved superior performance by leveraging their vast number of parameters and extensive training data, surpassing traditional language models across various domains. Due to their exceptional generative and reasoning capabilities, LLMs have been widely adopted in both academia and industry, including but not limited to education, healthcare, and finance. The evolution of LLMs, marked by significant milestones such as the development of models like GPT-3 and GPT-4, has revolutionized the way we approach language understanding and generation tasks.\nHowever, as artificial intelligence becomes more widely adopted, its potential risks are increasingly exposed to public scrutiny. In areas with high ethical risks, the use of machine learning methods is particularly concerning due to the black-box nature of these models, which results in an opaque decision-making process. This lack of transparency makes it challenging to prevent, control, and hold accountable for the associated risks. For example, research by noted that algorithmic outcomes are usually in the form of probabilities, which means infallibility cannot be assured, making it difficult to establish causal relationships.\nIn the realm of LLMs, although these models can provide a reasoning process for their outputs, issues such as the hallucination phenomenon still persist, hindering their broader application. The hallucination problem refers to instances where models produce outputs that are grammatically and logically coherent but factually inaccurate or entirely fabricated. Recently, researchers discovered that almost all LLMs incorrectly infer 9.11>9.9 during numerical comparison, which has sparked widespread discussion within the community. Errors in such simple tasks raise further concerns about the reliability and usability of LLMs. Through preliminary experiments, we found that when addressing reasoning problems, requiring the model to firstly output the answer and then the reasoning process can yield results that differ significantly from those obtained by firstly outputting the reasoning process and then the answer. Although recent studies have demonstrated the impact of prompting strategies on mitigating hallucination problems across various fields, there is a lack of research and discussion on the gap between answer-first prompt and logic-first prompt.\nTo fill this gap, we designed a new benchmark method for testing language models based on this observation: Reasoning Order as Benchmark. This method reflects the consistency of the model's reasoning logic by comparing the differences between the answer-first prompt and the logic-first prompt. Inspired from Chain-of-Thought prompting which enhances LLM reasoning performance by adding step-by-step guidance, we additionally designed Reflexive Prompting, a two-step prompting strategy. In the first step, we use both the answer-first prompt and the logic-first prompt to obtain two potentially different answers. In the second step, we analyze these two answers to derive the final answer. Experimental results demonstrate that this method improves the accuracy of reasoning tasks across different datasets and various LLMs. Furthermore, the accuracies achieved using different prompting strategies show"}, {"title": "Related Works", "content": "Although the answers generated by LLMs may appear logically coherent, they are not always accurate and true. The hallucination problem causes these models to produce grammatically correct and contextually appropriate but false information, which may lead to disastrous consequences in ethically sensitive domains such as medicine, law consultation and autonomous driving.\nMain causes of hallucinations focus on three aspects:\n\nInsufficient data and noisy data may limit the cross-modal feature alignment, thereby causing hallucination problems. Furthermore, indicates that hallucination may be another view of adversarial examples.\nKnowledge Gap Knowledge gaps are usually ascribed to the variations in input formats between the pre-training and fine-tuning phases. This knowledge gap creates complex challenges in balancing memory with retrieved evidence, which is seen as a passive defense mechanism to prevent the misuse of retrieval.\nOptimization Process Exposure bias between the training and testing stages have been demonstrated to lead to hallucinations within LLMs, particularly when generating lengthy responses. In addition, sampling methods characterized by high uncertainty in the output phase, such as top-p and top-k, may also exacerbate the issue of hallucination."}, {"title": "Think Before You Speak", "content": "The output stage of language models is typically a sequential recursive process: the $(K + 1)$th output token is generated based on $K$ hidden vectors per layer, one vector preceding token. This mechanism is the most natural design choice when the Transformer was proposed by ensuring the contextual coherence of the generated text, but it also means that the model does not consider potential special cases in the subsequent text when generating the preceding text.\nResearch from investigated the possibility for the model to manipulate extra hidden vectors before outputting, for example, K + 10 vectors during outputting $(K + 1)$th token. By adding a sequence of dummy pause tokens into a decoder-only model's input and ignore the model's corresponding outputs until the last pause token is seen, the outputs are intentionally delayed to analyze the downstream task performances. Experiment results show that injecting delays in pretraining and fine-tuning during training stage outperforms the standard end-to-end training on wide variety of tasks, which also aligns with our hypothesis and results in this paper. Especially, pause-pretraining appears crucial for delays to help in downstream inference-time, indicating that a standard-pretrained model has strong biases that prevent it from fully understanding the benefits of inference-time delays."}, {"title": "Prompting Methods for Hallucination", "content": "Due to the immense computational resources and data required to retrain large language models, this strategy is impractical for many researchers. Alternatively, modifying the prompting strategy to mitigate hallucination problems is more economically feasible. The following are some of the currently popular methods:\n\nPrompting GPT-3 To Be Reliable proposed following prompting strategies for each aspect of reliability: (1)Prompting with randomly sampled examples from the source domain allows GPT-3 to generalize robustly on unseen domains and challenge examples; (2)Examples sampled from a balanced demographic distribution and natural language intervention reduce social biases; (3)Language model probabilities are calibrated to reflect accuracy; (4)Appending up-to-date knowledge can supplant GPT-3's memorized knowledge or reasoning chains.\nChain-of-Verification developed a Chain-of-Thought alike sequential approach namely Chain-of-Verification: for a given initial draft response, first plans verification questions to check its work, and then systematically answer those questions as a Chain-of-Thought procedure to gradually guide the model and finally produce an improved revised response.\nSelf-Reflecting Prompt proposed an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation using a two-step loop: First acquire background knowledge and refine the knowledge, then answer the question and refine the answer referring to previous knowledge.\nSelf-Contradictory Hallucinations proposed a similar method to mitigate hallucination by a pair of contradictory prompt. The key difference is that the contradictory prompt is triggered initiatively by generating alternative sentence that aligns with original context within the same scope, while our method focuses more on passive contradictory caused by changing the answer-logic order, therefore no extra language extraction or inference models are needed to conduct such method."}, {"title": "Methods", "content": "Recently, the machine learning community has observed that large language models tend to make errors on a seemingly simple question. When asked, \"Which is greater: 9.11 or 9.9?\" nearly all large language models respond that 9.11 is greater, as shown in Figure 1. Furthermore, some models provide incorrect reasoning processes to justify this erroneous answer, while others present correct reasoning processes but still arrive at the wrong conclusion.\nSeveral inferences about the error's cause include: (1) Due to the 9/11 terrorist attacks, the pattern \"9.11\" is generally perceived as a date, and from a chronological perspective, 9.11 is greater than 9.9; (2) In the context of software development versions, 9.11 is considered a higher version number than 9.9, and it is possible that the models may have learned this ranking from the version number information; (3) The models might have treated the numbers before and after the decimal point as separate quantities for comparison, etc.\nHowever, because some LLMs provide correct comparative logic, which does not align with the aforementioned causes, this paper does not focus on the erroneous relationships learned due to training data. Instead, we consider the output logic of language models, which is sequential. This means that subsequent outputs incorporate previous outputs as context, while the preceding outputs do not have visibility into the subsequent ones. This characteristic might contribute to the occurrence of certain hallucination phenomena.\nWe may notice that in every response from the large language models, the first sentence always presents the answer first. Considering the sequential nature of language model outputs, at the point of generating the answer, the model has not yet seen the subsequent reasoning portion. Therefore, it may provide an incorrect answer based on similar causes identified in the training data, and then continue to generate reasoning to support this answer. To explore this phenomenon, we designed a set of new prompts for the question \u201cIs 9.11 > 9.9?\": (1) Prompts that instruct the model to first output the final result and then provide the reasoning, and (2) prompts that instruct the model to first output the reasoning and then the corresponding result. Surprisingly, we found that even if the LLMs can give out correct answer under original prompt, the issue of incorrect judgement may still occur when asked to generate answer before reasoning logic, as the comparison using Claude shown in Figure 2."}, {"title": "Reasoning Order as Benchmark", "content": "The result in Figure 2 confirms the correctness of our hypothesis that the sequential generation of language tokens by large language models, coupled with their inability to anticipate subsequent text during the generation of prior text, influences the accuracy of the model's judgments, inducing a unique kind of illusionary phenomenon. From the perspective of natural language logic, the logical content and the final answer should remain consistent regardless of whether the reasoning logic is presented first or later. However, by specifying different output orders through prompt modification, the model's outputs exhibit a form of inconsistency. This inconsistency indicates that at least one of the answers is experiencing a hallucination phenomenon, demonstrating the unreliability of the model's reasoning. This insight has inspired us to propose a new benchmark method: Reasoning Order as Benchmark to measure the self-consistency of outputs from large language models.\nThe idea is quite simple: For each question, instead of its original prompt, generate a pair of \"answer first\" prompt and \"logic first\" prompt, and compare the consistency of the two results as benchmark. For example, to generate the \"answer first\" and \"logic first\" prompts, these two sentences are added at the end of the prompts: (1)Please give out the correct option in the first sentence and then give out the logic. (2)Please give out the reasoning logic first and then answer the question by selecting the options. The detailed benchmark method can be briefly illustrated in Algorithm 1."}, {"title": "Reflexive Prompting", "content": "According to the aforementioned hypothesis, if language models could explicitly access information about potential future outputs during the generation process, modifying the architecture of the decoder could potentially alleviate this issue. Unfortunately, due to a lack of sufficient computational resources and corpora, we are unable to retrain such LLMs. Consequently, we have approached the problem from the perspective of prompt engineering, designing a novel prompting method that enables large language models to acquire this information: Reflexive Prompting.\nIn reflexive prompting, the query process of LLMs is transformed from a single-step direct inquiry into a two-step procedure. Similarly, in the first step, we generate the \"answer first\" and \"logic first\" prompt variants for original query and get the corresponding results. In the second step, the original question and the two results are fed into the LLM altogether as reflexive prompt for the final decision. The query process can be illustrated in Algorithm 2. One example of the reflexive prompt is shown below:\nThe feasibility of this strategy primarily stems from two aspects. Firstly, providing language models with explicit subsequent logical reasoning information, as posited by prior hypotheses, allows the models to reassess their reasoning processes from multiple perspectives. This process is somewhat analogous to chain-of-thought prompting, which also utilizes the intermediate information output by the model itself to adjust the reasoning process. However, unlike chain-of-thought prompting, which requires human experts to decompose problems into sub-problems to facilitate step-by-step resolution by the model, the method described in this paper does not necessitate such decomposition, thereby enhancing the potential for generalization. Additionally, the variation in results due to the different sequences of reasoning and answers can be viewed, in a sense, as an ensemble learning approach. This involves the model generating potentially divergent responses, which are then stacked by the model itself acting as a meta-model. The inherent reasoning capabilities of LLMs are crucial in this procedure, enabling the employment of self consistency review as meta-model decision makers."}, {"title": "Experiments", "content": "To evaluate the effectiveness of the reasoning order as benchmark as well as the reflexive prompting strategy, we designed the following experiments testing a various of reasoning datasets on different LLMs. Experiment codes and results are available in https://github.com/XieZikai/ReflexivePrompting."}, {"title": "Experimental Settings", "content": "Datasets The core of the two methods introduced in this paper lies in comparing the outputs generated from the \"order\" prompt pair, making reasoning tasks particularly suitable for this experiment. Furthermore, multiple-choice datasets provide distinct options as criteria for determining differences, hence, we selected three different multiple-choice reasoning datasets as test sets.\n\n is an expert-written question dataset for testing human logical reasoning, which consists of 8,678 QA instances, covering multiple types of deductive reasoning. Due to limited budget, only the first 1,000 questions are used in the experiments.\nModels To conduct our experiments, we selected four commonly used large language models available on the market as test subjects, employing unadorned raw prompts, answer-first prompts, and logic-first prompts as baselines. These models are: GPT-40-mini, Llama-3.1-70b, Claude-3.5-sonnet and Gemini-1.5-flash. It is important to note that since chain-of-thought prompting requires the design of specialized decomposition processes for each question, effectively introducing additional information, we consider it inappropriate to compare it directly with the other approaches."}, {"title": "Experimental Results", "content": "We use the accuracy for multiple-choice questions as comparison criteria among raw prompts, answer-first prompts, logic-first prompts and reflexive prompts. The results for all the LLMs and datasets are shown below in Table 1.\nFrom the table we can observe that, due to the additional context understanding of the reasoning process and the ensemble model-alike mechanism, the reflexive prompt generally performs slightly better than other prompt methods across almost all tasks, demonstrating its feasibility for better reasoning performance for language models. However, since no additional information is introduced, this method does not actually correct erroneous answers. Instead, it identifies the more reasonable answer from the results of the answer-first prompt and the logic-first prompt, leading to a relatively limited performance improvement. For instance, in some cases, the raw prompt, answer-first prompt, and logic-first prompt may all produce the same incorrect answer, and in almost all such cases, the reflexive prompt will consider this incorrect answer to be correct.\nGiven that this method is based on the phenomenon of inconsistent outputs from the answer-first prompt and logic-first prompt, it is natural to examine the extent of these inconsistencies in the model outputs, which constitutes the key methodology of Reasoning Order as Benchmark. The consistency of all models on each dataset is shown in Table 2. To further check the statistical properties, we calculated the Pearson correlation coefficient between the consistency and the accuracy of all models on each dataset and each prompting strategy, which is shown in Table 3. We can observe a very clear linear correlation between the models' consistency and accuracy. This is likely because low consistency may result from the models' inability to output answers with high confidence, indicating that the model has not learned enough to produce reliable answers, thereby leading to low accuracy. This result indicates that Reasoning Order as Benchmark can indeed serve as a meaningful benchmark indicator."}, {"title": "Case Study", "content": "Here we present two case studies of how our prompting strategies help mitigating hallucination problems using example question number 52 and 130 from the TruthfulQA validation set tested using Claude, showing in Figure 3 and 4. We can see from the first case that the answer-first result gives the incorrect answer (1) first, then tries to smooth such answer over by distinguishing \u201cindividual\u201d and \u201ccountry\" wins Nobel Prize. The logic-first answer follows a step-by-step logic chain and correctly infers that individual winning Nobel Prize is equivalence to the country in such context.\nAlthough this may appear to be the result of the Chain-of-Thought alike mechanism, the second case provides a counterexample. In this case, the answer-first result gives out the correct commonsense as vampires aren't real, while the logic-first result delves into the \"traditional vampire lore\" context step by step. We infer that such step-by-step mechanism tries to tighten the link between each steps, thus may lead to the problem of overthinking. In contrast, answer-first strategy gives out answer before the logic, so the result will be more based on commonsense. This can also explain the reason that logic-first results is not significantly better than answer-first results during experiments, indicating the effectiveness of our strategy by reviewing the both results for final decision."}, {"title": "Conclusion and Limitations", "content": "In this paper, we delve into the potential causes of the \"9.11>9.9\" problem and introduce the methods of Reasoning Order as Benchmark and Reflexive Prompting. By assigning both answer-first and logic-first prompts to the same question, we observed inconsistencies in the models' responses depending on the order of answer generation and logical reasoning. To drive this issue, we designed the reflexive prompt to return the inconsistent answer-logic output pairs back to the model simultaneously as a step-two evaluation and these inconsistencies are mitigated to some extent, thereby enhancing the reasoning performance. Evaluation on the benchmark method shows high linear relationship with model accuracy, implicating that additional module designed for reasoning order consistency may help boosting model performance.\nIt must be acknowledged that this work still possesses several limitations. Firstly, due to budget constraints, we were unable to conduct experiments on larger datasets or with more extensive large language models. Although we believe that the current experiments sufficiently demonstrate the viability and performance boost of the prompt strategy introduced in this research, additional experiments could potentially reveal more reliable patterns from the benchmark results. Furthermore, we lacked the computational resources and data necessary to retrain large language models. Originating from the hypotheses proposed in Section, we posit that improvements could be made to large language models by enhancing the decoder part. Since the current output cannot see subsequent outputs, a potential research direction involves integrating the output layer with the original hidden state vectors before output layer, enabling the decoder to \"see\" the previous response during output. has already shown the potential to pause token to apply inference-time delay, introducing performance gain on various of reasoning datasets, showing that allowing for revisiting and revising its own answers may mitigate such issues. Therefore, a modified decoder network to explicitly combine possible future output may be a direct approach to mitigate hallucination in language models."}]}