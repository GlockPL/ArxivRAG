{"title": "Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models", "authors": ["Zikai Xie"], "abstract": "Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the \"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that \"9.11>9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.", "sections": [{"title": "Introduction", "content": "In the field of natural language processing (NLP), large language models (LLMs) have achieved superior performance by leveraging their vast number of parameters and extensive training data, surpassing traditional language models across various domains. Due to their exceptional generative and reasoning capabilities, LLMs have been widely adopted in both academia and industry, including but not limited to education(Kasneci et al. 2023), healthcare(Sallam 2023), and finance(Wu et al. 2023). The evolution of LLMs, marked by significant milestones such as the development of models like GPT-3(Brown et al. 2020) and GPT-4(Achiam et al. 2023), has revolutionized the way we approach language understanding and generation tasks.\nHowever, as artificial intelligence becomes more widely adopted, its potential risks are increasingly exposed to public scrutiny. In areas with high ethical risks, the use of machine learning methods is particularly concerning due to the black-box nature of these models, which results in an opaque decision-making process. This lack of transparency makes it challenging to prevent, control, and hold accountable for the associated risks. For example, research by (Morley et al. 2020) noted that algorithmic outcomes are usually in the form of probabilities, which means infallibility cannot be assured, making it difficult to establish causal relationships.\nIn the realm of LLMs, although these models can provide a reasoning process for their outputs, issues such as the hallucination phenomenon(Yao et al. 2023) still persist, hindering their broader application. The hallucination problem refers to instances where models produce outputs that are grammatically and logically coherent but factually inaccurate or entirely fabricated. Recently, researchers discovered that almost all LLMs incorrectly infer 9.11>9.9 during numerical comparison, which has sparked widespread discussion within the community. Errors in such simple tasks raise further concerns about the reliability and usability of LLMs. Through preliminary experiments, we found that when addressing reasoning problems, requiring the model to firstly output the answer and then the reasoning process can yield results that differ significantly from those obtained by firstly outputting the reasoning process and then the answer. Although recent studies have demonstrated the impact of prompting strategies on mitigating hallucination problems across various fields(Si et al. 2022; M\u00fcndler et al. 2023; Ji et al. 2023; Dhuliawala et al. 2023), there is a lack of research and discussion on the gap between answer-first prompt and logic-first prompt.\nTo fill this gap, we designed a new benchmark method for testing language models based on this observation: Reasoning Order as Benchmark. This method reflects the consistency of the model's reasoning logic by comparing the differences between the answer-first prompt and the logic-first prompt. Inspired from Chain-of-Thought prompting(Wei et al. 2022) which enhances LLM reasoning performance by adding step-by-step guidance, we additionally designed Reflexive Prompting, a two-step prompting strategy. In the first step, we use both the answer-first prompt and the logic-first prompt to obtain two potentially different answers. In the second step, we analyze these two answers to derive the final answer. Experimental results demonstrate that this method improves the accuracy of reasoning tasks across different datasets and various LLMs. Furthermore, the accuracies achieved using different prompting strategies show"}, {"title": "Related Works", "content": "Although the answers generated by LLMs may appear logically coherent, they are not always accurate and true. The hallucination problem causes these models to produce grammatically correct and contextually appropriate but false information(Bang et al. 2023; Agarwal et al. 2018), which may lead to disastrous consequences in ethically sensitive domains such as medicine, law consultation and autonomous driving.\nMain causes of hallucinations focus on three aspects(Ye et al. 2023):\n\u2022 Data Insufficient data(Wang et al. 2023) and noisy data(Yu et al. 2024) may limit the cross-modal feature alignment, thereby causing hallucination problems. Furthermore, (Yao et al. 2023) indicates that hallucination may be another view of adversarial examples.\n\u2022 Knowledge Gap Knowledge gaps are usually ascribed to the variations in input formats between the pre-training and fine-tuning phases(Zheng, Huang, and Chang 2023). This knowledge gap creates complex challenges in balancing memory with retrieved evidence, which is seen as a passive defense mechanism to prevent the misuse of retrieval(Gao et al. 2022).\n\u2022 Optimization Process Exposure bias between the training and testing stages have been demonstrated to lead to hallucinations within LLMs, particularly when generating lengthy responses(Wang and Sennrich 2020). In addition, sampling methods characterized by high uncertainty in the output phase, such as top-p and top-k, may also exacerbate the issue of hallucination(Lee et al. 2022)."}, {"title": "Think Before You Speak", "content": "The output stage of language models is typically a sequential recursive process: the (K + 1)th output token is generated based on K hidden vectors per layer, one vector preceding token. This mechanism is the most natural design choice when the Transformer was proposed by (Vaswani et al. 2017), ensuring the contextual coherence of the generated text, but it also means that the model does not consider potential special cases in the subsequent text when generating the preceding text.\nResearch from (Goyal et al. 2023) investigated the possibility for the model to manipulate extra hidden vectors before outputting, for example, K + 10 vectors during outputting (K + 1)th token. By adding a sequence of dummy pause tokens into a decoder-only model's input and ignore the model's corresponding outputs until the last pause token is seen, the outputs are intentionally delayed to analyze the downstream task performances. Experiment results show that injecting delays in pretraining and fine-tuning during training stage outperforms the standard end-to-end training on wide variety of tasks, which also aligns with our hypothesis and results in this paper. Especially, pause-pretraining appears crucial for delays to help in downstream inference-time, indicating that a standard-pretrained model has strong biases that prevent it from fully understanding the benefits of inference-time delays."}, {"title": "Prompting Methods for Hallucination", "content": "Due to the immense computational resources and data required to retrain large language models, this strategy is impractical for many researchers. Alternatively, modifying the prompting strategy to mitigate hallucination problems is more economically feasible. The following are some of the currently popular methods(Tonmoy et al. 2024):\nPrompting GPT-3 To Be Reliable(Si et al. 2022) proposed following prompting strategies for each aspect of reliability: (1)Prompting with randomly sampled examples from the source domain allows GPT-3 to generalize robustly on unseen domains and challenge examples; (2)Examples sampled from a balanced demographic distribution and natural language intervention reduce social biases; (3)Language model probabilities are calibrated to reflect accuracy; (4)Appending up-to-date knowledge can supplant GPT-3's memorized knowledge or reasoning chains.\nChain-of-Verification(Dhuliawala et al. 2023) developed a Chain-of-Thought alike sequential approach namely Chain-of-Verification: for a given initial draft response, first plans verification questions to check its work, and then systematically answer those questions as a Chain-of-Thought procedure to gradually guide the model and finally produce an improved revised response.\nSelf-Reflecting Prompt(Ji et al. 2023) proposed an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation using a two-step loop: First acquire background knowledge and refine the knowledge, then answer the question and refine the answer referring to previous knowledge.\nSelf-Contradictory Hallucinations(M\u00fcndler et al. 2023) proposed a similar method to mitigate hallucination by a pair of contradictory prompt. The key difference is that the contradictory prompt is triggered initiatively by generating alternative sentence that aligns with original context within the same scope, while our method focuses more on passive contradictory caused by changing the answer-logic order, therefore no extra language extraction or inference models are needed to conduct such method."}, {"title": "Methods", "content": "Recently, the machine learning community has observed that large language models tend to make errors on a seemingly simple question. When asked, \"Which is greater: 9.11 or 9.9?\" nearly all large language models respond that 9.11 is greater, as shown in Figure 1. Furthermore, some models provide incorrect reasoning processes to justify this erroneous answer, while others present correct reasoning processes but still arrive at the wrong conclusion."}, {"title": "Reasoning Order as Benchmark", "content": "The result in Figure 2 confirms the correctness of our hypothesis that the sequential generation of language tokens by large language models, coupled with their inability to anticipate subsequent text during the generation of prior text, influences the accuracy of the model's judgments, inducing a unique kind of illusionary phenomenon. From the perspective of natural language logic, the logical content and the final answer should remain consistent regardless of whether the reasoning logic is presented first or later. However, by specifying different output orders through prompt modification, the model's outputs exhibit a form of inconsistency. This inconsistency indicates that at least one of the answers is experiencing a hallucination phenomenon, demonstrating the unreliability of the model's reasoning. This insight has inspired us to propose a new benchmark method: Reasoning Order as Benchmark to measure the self-consistency of outputs from large language models.\nThe idea is quite simple: For each question, instead of its original prompt, generate a pair of \"answer first\" prompt and \"logic first\" prompt, and compare the consistency of the two results as benchmark. For example, to generate the \"answer first\" and \"logic first\" prompts, these two sentences are added at the end of the prompts: (1)Please give out the correct option in the first sentence and then give out the logic. (2)Please give out the reasoning logic first and then answer the question by selecting the options. The detailed benchmark method can be briefly illustrated in Algorithm 1."}, {"title": "Reflexive Prompting", "content": "According to the aforementioned hypothesis, if language models could explicitly access information about potential future outputs during the generation process, modifying the architecture of the decoder could potentially alleviate this issue. Unfortunately, due to a lack of sufficient computational resources and corpora, we are unable to retrain such LLMs. Consequently, we have approached the problem from the perspective of prompt engineering, designing a novel prompting method that enables large language models to acquire this information: Reflexive Prompting.\nIn reflexive prompting, the query process of LLMs is transformed from a single-step direct inquiry into a two-step procedure. Similarly, in the first step, we generate the \"answer first\" and \"logic first\" prompt variants for original query and get the corresponding results. In the second"}, {"title": "Experiments", "content": "To evaluate the effectiveness of the reasoning order as benchmark as well as the reflexive prompting strategy, we designed the following experiments testing a various of reasoning datasets on different LLMs. Experiment codes and results are available in https://github.com/XieZikai/ ReflexivePrompting."}, {"title": "Experimental Settings", "content": "Datasets The core of the two methods introduced in this paper lies in comparing the outputs generated from the \"order\" prompt pair, making reasoning tasks particularly suitable for this experiment. Furthermore, multiple-choice datasets provide distinct options as criteria for determining differences, hence, we selected three different multiple-choice reasoning datasets as test sets.\n\u2022 Measureing Masssive Multitask Language Understanding(MMLU)(Hendrycks et al. 2020) is a multitask reasoning dataset, covering 57 tasks including elementary mathematics, US history, computer science, law and more. The validation set is chosen for experiments, which contains 1,531 multiple-choice questions.\n\u2022 TruthfulQA(Lin, Hilton, and Evans 2021) is a benchmark to measure whether a language model is truthful in generating answers that humans would answer falsely due to a false belief or misconception. The dataset comprises 817 questions that span 38 categories, including health, law, finance and politics.\n\u2022 LogiQA(Liu et al. 2020) is an expert-written question dataset for testing human logical reasoning, which consists of 8,678 QA instances, covering multiple types of deductive reasoning. Due to limited budget, only the first 1,000 questions are used in the experiments.\nModels To conduct our experiments, we selected four commonly used large language models available on the market as test subjects, employing unadorned raw prompts, answer-first prompts, and logic-first prompts as baselines. These models are: GPT-40-mini(Achiam et al. 2023), Llama-3.1-70b(Vavekanand and Sam 2024), Claude-3.5-sonnet(Amazon 2024) and Gemini-1.5-flash(Reid et al. 2024). It is important to note that since chain-of-thought prompting requires the design of specialized decomposition processes for each question, effectively introducing additional information, we consider it inappropriate to compare it directly with the other approaches."}, {"title": "Case Study", "content": "Here we present two case studies of how our prompting strategies help mitigating hallucination problems using example question number 52 and 130 from the TruthfulQA validation set tested using Claude, showing in Figure 3 and 4. We can see from the first case that the answer-first result gives the incorrect answer (1) first, then tries to smooth such answer over by distinguishing \u201cindividual\u201d and \u201ccountry\u201d wins Nobel Prize. The logic-first answer follows a step-by-step logic chain and correctly infers that individual winning Nobel Prize is equivalence to the country in such context.\nAlthough this may appear to be the result of the Chain-of-Thought alike mechanism, the second case provides a counterexample. In this case, the answer-first result gives out the correct commonsense as vampires aren't real, while the logic-first result delves into the \"traditional vampire lore\" context step by step. We infer that such step-by-step mechanism tries to tighten the link between each steps, thus may lead to the problem of overthinking. In contrast, answer-first strategy gives out answer before the logic, so the result will be more based on commonsense. This can also explain the reason that logic-first results is not significantly better than answer-first results during experiments, indicating the effectiveness of our strategy by reviewing the both results for final decision."}, {"title": "Conclusion and Limitations", "content": "In this paper, we delve into the potential causes of the \"9.11>9.9\" problem and introduce the methods of Reasoning Order as Benchmark and Reflexive Prompting. By assigning both answer-first and logic-first prompts to the same question, we observed inconsistencies in the models' responses depending on the order of answer generation and logical reasoning. To drive this issue, we designed the reflexive prompt to return the inconsistent answer-logic output pairs back to the model simultaneously as a step-two evaluation and these inconsistencies are mitigated to some extent, thereby enhancing the reasoning performance. Evaluation on the benchmark method shows high linear relationship with model accuracy, implicating that additional module designed for reasoning order consistency may help boosting model performance.\nIt must be acknowledged that this work still possesses several limitations. Firstly, due to budget constraints, we were unable to conduct experiments on larger datasets or with more extensive large language models. Although we believe that the current experiments sufficiently demonstrate the viability and performance boost of the prompt strategy introduced in this research, additional experiments could potentially reveal more reliable patterns from the benchmark results. Furthermore, we lacked the computational resources and data necessary to retrain large language models. Originating from the hypotheses proposed in Section, we posit that improvements could be made to large language models by enhancing the decoder part. Since the current output cannot see subsequent outputs, a potential research direction involves integrating the output layer with the original hidden state vectors before output layer, enabling the decoder to \"see\" the previous response during output. (Goyal et al. 2023) has already shown the potential to pause token to apply inference-time delay, introducing performance gain on various of reasoning datasets, showing that allowing for revisiting and revising its own answers may mitigate such issues. Therefore, a modified decoder network to explicitly combine possible future output may be a direct approach to mitigate hallucination in language models."}]}