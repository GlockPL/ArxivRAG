{"title": "CAISSON: Concept-Augmented Inference Suite of Self-Organizing Neural Networks", "authors": ["Igor Halperin"], "abstract": "We present CAISSON, a novel hierarchical approach to Retrieval-Augmented Generation (RAG) that transforms traditional single-vector search into a multi-view clustering framework. At its core, CAISSON leverages dual Self-Organizing Maps (SOMs) to create complementary organizational views of the document space, where each view captures different aspects of document relationships through specialized embeddings. The first view processes combined text and metadata embeddings, while the second operates on metadata enriched with concept embeddings, enabling a comprehensive multi-view analysis that captures both fine-grained semantic relationships and high-level conceptual patterns. This dual-view approach enables more nuanced document discovery by combining evidence from different organizational perspectives. To evaluate CAISSON, we develop SynFAQA, a framework for generating synthetic financial analyst notes and question-answer pairs that systematically tests different aspects of information retrieval capabilities. Drawing on HotPotQA's methodology for constructing multi-step reasoning questions, SynFAQA generates controlled test cases where each question is paired with the set of notes containing its ground-truth answer, progressing from simple single-entity queries to complex multi-hop retrieval tasks involving multiple entities and concepts. Our experimental results demonstrate substantial improvements over both basic and enhanced RAG implementations, particularly for complex multi-entity queries, while maintaining practical response times suitable for interactive applications.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) has emerged as a common industry-wide method to enhance the capabilities of large language models (LLMs) by allowing them to access and incorporate external knowledge sources during text generation [5]. Despite extensive research and development in this area, current RAG systems relying on straightforward vector similarity searches applied to chunks of text often face several challenges for complex queries, including potential loss of relevant information, handling of multi-entity queries, and incorporation of domain-specific concepts and relationships. While various improvements have been proposed in the literature, most solutions maintain the fundamental single-vector similarity paradigm, limiting their ability to capture multiple aspects of document relationships simultaneously.\nThis paper addresses these limitations by introducing CAISSON (Concept-Augmented Inference Suite of Self-Organizing Neural Networks), a novel approach that reimagines RAG architecture through the lens of multi-view clustering. CAISSON combines modern transformer-based embeddings with classical Self-Organizing Maps (SOMs) [4] to create complementary organizational views of the document space. Each view captures different aspects of document relationships: one processes combined text and metadata embeddings for semantic organization, while the other operates on metadata enriched with concept embeddings for thematic clustering. This multi-view approach creates a rich organizational framework where documents are simultaneously clustered according to different similarity criteria, enabling more sophisticated retrieval than traditional single-vector approaches. Our experimental results demonstrate the effectiveness of this approach, achieving up to 148% improvement in retrieval accuracy over baseline RAG implementations, and maintaining strong performance even for complex four-entity queries.\nCAISSON implements this multi-view vision through two specialized SOMs that process different aspects of document representations. Each SOM creates a topologically-organized network of retrieval nodes, effectively clustering similar documents according to different similarity criteria. The complementary views enable both semantic matching through text-metadata embeddings and conceptual matching through concept-metadata embeddings. During retrieval, evidence from both views is combined to provide a more comprehensive assessment of document relevance. This architecture can be viewed as a 'swarm of specialized RAGs' working in parallel across different organizational views of the document space.\nTo rigorously evaluate the effectiveness of this multi-view architecture, particularly its ability to handle queries of varying complexity, we need a systematic evaluation framework that can test both simple retrieval capabilities and complex multi-entity retrieval tasks. Traditional benchmarks, typically focused on single-aspect retrieval or general question-answering tasks, are inadequate for assessing the unique capabilities of multi-view document organization. To address this need, we developed SynFAQA (Synthetic Financial Analyst Questions and Answers generator), a comprehensive evaluation framework for financial document retrieval systems. Drawing inspiration from HotPotQA's methodology for constructing multi-step reasoning questions [8] and incorporating complexity scaling principles from DeepMind's Michelangelo framework [2], SynFAQA first generates a collection of synthetic analyst notes that reflect real-world characteristics such as industry sector distributions, market capitalization weightings, and typical patterns in multi-entity coverage. These notes then serve as the foundation for generating controlled test cases of increasing complexity, where each question is paired with the set of notes containing its ground-truth answer. The resulting framework enables systematic evaluation of capabilities ranging from simple fact retrieval from a single document to complex multi-hop inference requiring information synthesis across multiple documents. While initially developed to evaluate CAISSON, SynFAQA represents a broader contribution to the field of financial natural language processing.\nOur comprehensive evaluation demonstrates several key strengths of the multi-view clustering approach:"}, {"title": "The CAISSON model", "content": "CAISSON implements a novel dual-path architecture for document retrieval (Figure 1). The system processes input documents through two parallel specialized paths, each culminating in a Self-Organizing Map (SOM) that creates a topologically-organized network of retrieval nodes (see in the next subsection for more details on SOMs). Each node within these SOMs effectively functions as a specialized RAG instance, maintaining a local collection of documents that share similar characteristics:\n\u2022 The Semantic Path (SOM1) processes combined text and metadata embeddings, creating nodes that specialize in specific semantic patterns and entity relationships\n\u2022 The Concept Path (SOM2) processes concept and metadata embeddings, creating nodes that specialize in specific thematic and conceptual patterns\nDuring retrieval, a query is processed through both paths simultaneously. Each path identifies relevant nodes based on its specialized organization, and documents are retrieved from these nodes' local collections. The final ranking combines evidence from both paths using a weighted scoring function that considers ticker matches, concept similarity, and semantic relevance.\nThis dual-path architecture offers several advantages over traditional RAG systems:\n1. Local Specialization: Each node becomes expert at handling specific types of queries\n2. Global Organization: The SOMs maintain topological relationships between different specializations\n3. Complementary Views: The dual paths capture both fine-grained semantic relationships and high-level conceptual patterns"}, {"title": "Self-Organizing Maps: Implementation Details", "content": "CAISSON extends the classical Self-Organizing Maps (SOMs) [4] framework through a novel network architecture that combines traditional SOM learning with modern RAG capabilities. This section details our implementation approach and key architectural innovations."}, {"title": "Classical SOMs vs. CAISSON'S Extended Architecture", "content": "Each SOM in CAISSON consists of an n \u00d7 n grid of nodes (with n = 10 in our implementation), where each node functions as a specialized document retriever that learns to handle specific types of documents. The key innovation in CAISSON lies in how these nodes operate and what information they maintain:\n\u2022 Classical SOM Node:\nMaintains single representative vector $w_i \\in \\mathbb{R}^d$ (node weight)\nUpdates $w_i$ during training to represent average characteristics\nDiscards individual input vectors after updating $w_i$\n\u2022 CAISSON's Extended Node:\nMaintains representative vector $w_i \\in \\mathbb{R}^d$ for training and specialization\nStores complete set of document embeddings {E1, E2, ..., Ek} \u2282 Rd mapped to the node\nFunctions as specialized vector database for mapped documents\nEnables efficient retrieval within node's area of specialization\nHere d is the embedding dimension. In our implementation, d = 434 for both SOMs: SOM1 combines a 384-dimensional text and a 50-dimensional entity embeddings, while SOM2 combines a 384-dimensional concept embedding and a 50-dimensional embedding of metadata."}, {"title": "Training Process", "content": "The training process adapts the nodes' representative vectors while simultaneously building document collections. We define:\n\u2022 x(t) \u2208 Rd: Input document embedding at time t\n\u2022 wi(t) \u2208 Rd: Representative vector of node i at time t\n\u2022 ri \u2208 R2: Grid position coordinates of node i in the 2D SOM lattice\n\u2022 ||\u00b7 ||: Euclidean distance norm in the respective space (Rd or R2)\n\u2022 Ci: Set of document embeddings stored at node i\nFor each input document embedding x(t) at time t, the process follows these steps:\n1. Competition: Find Best Matching Unit (BMU) based on minimum distance between input embedding and node representative vectors:\nc = argmin; ||x(t) \u2013 wi(t)|| (1)\nwhere c is the index of the winning node.\n2. Cooperation: Define neighborhood influence of the winning node c on all nodes i through spatial proximity in the SOM grid:\nhci(t) = a(t) exp ( - ||rc - ri||2 / 202(t) ) (2)\nwhere:\n\u2022 rc,ri \u2208 R2: Grid coordinates of winning node c and any node i\n\u2022 \u03b1(t) = \u03b1\u03c1(1 \u2013 yt/T): Learning rate decay with hyperparameters ao and y\n\u2022 \u03c3(t) = \u03c3\u03bf exp(-lt): Neighborhood radius decay with hyperparameters \u03c3\u03bf and A\n\u2022 T: Total number of epochs\n3. Adaptation: Update node representative vectors for all nodes i according to:\nwi(t + 1) = wi(t) + hci(t)[x(t) \u2013 wi(t)] (3)\nwhere the magnitude of update is controlled by the neighborhood function hci(t).\n4. Storage: Add document embedding to BMU's collection:\nCc = Cc \u222a {x(t)} (4)\nNote the distinct roles of vectors and distances in the algorithm:\n\u2022 wi(t) \u2208 Rd: Representative embeddings in document space\n\u2022 ri \u2208 R2: Fixed grid positions in SOM lattice\n\u2022 ||x(t) \u2013 wi(t)||: Similarity in the embedding space\n\u2022 ||rc - ri||: Physical distance in SOM grid"}, {"title": "Training Monitoring", "content": "The training progress is monitored through quantization error for each SOM:\nQi = 1/N \u03a3 ||xj - WBMU(xj) ||2 (5)\nwhere N is the number of documents and BMU(xj) maps document j to its Best Matching Unit index. This error typically shows exponential decay during training, with the majority of organization occurring in the first 50 epochs."}, {"title": "Retrieval Network Formation", "content": "The final network supports efficient retrieval through a two-stage process:\n1. Use node representative vectors {wi} to identify relevant specializations\n2. Search document collections {C} within selected nodes for best matches\nCAISSON implements this extended SOM architecture in both paths of its dual architecture, with SOM1 organizing documents based on semantic and entity relationships, while SOM2 focuses on concept and metadata patterns. The specific embedding approaches for each path are detailed in the following subsections."}, {"title": "Semantic Path Embeddings (SOM1)", "content": "The semantic path combines text and entity embeddings to create a unified document representation. For a document d with text content and a set of entities {e1, . . ., ek}, the combined embedding is:\nESOM1(d) = [Etext(d); Eentities({e1,...,ek})] (6)\nwhere Etext(d) is the text embedding, Eentities is the combined entity embedding, and [;] denotes vector concatenation. The following subsections detail the construction of each component."}, {"title": "Text Embedding Model", "content": "CAISSON utilizes the all-MiniLM-L6-v2 Sentence Transformer model for text embeddings, which produces 384-dimensional vectors. For our experimental setting with short analyst notes, this lightweight model provides an effective balance between performance and computational efficiency. The model adequately handles financial terminology and market-specific language without requiring domain-specific fine-tuning.\nFor applications involving longer documents or more complex text chunks, more sophisticated transformer models might be appropriate, such as MPNet-base (768 dimensions), BERT-large (1024 dimensions), or domain-specific models like FinBERT. However, for our current setting with concise text fragments, our experiments demonstrate that MiniLM's 384-dimensional embeddings capture sufficient semantic information while maintaining computational efficiency."}, {"title": "Entity (Ticker) Embedding Construction", "content": "The entity embedding framework uses configurable dimensionality, implemented in our experiments with 50-dimensional embeddings. The construction process involves several stages:\n1. Base Initialization: Each ticker receives an initial random embedding:\nEbase ~ N(0, Id) (7)\nwhere d is the embedding dimension (set to 50 in our implementation).\n2. Industry Integration: The embedding incorporates industry information through weighted averaging:\nEticker = Ebase + (1 \u2212 \u03bb) 1/|Si| \u03a3 Ebase (8)\njeSi\nwhere x = 0.7 controls the balance between individual and industry-wide characteristics."}, {"title": "Multi-Entity Document Representation", "content": "For documents mentioning multiple entities, we implement a simple but effective combination strategy:\nEmulti = 1/K \u03a3 ni/N E (11)\ni=1\nwhere K is the number of entities, and ni is the count of ticker i in the document. This ensures that multi-entity embeddings remain close to their constituents in the embedding space (while tilting towards tickers mentioned more often), an important property for SOM-based clustering."}, {"title": "Concept Path Embeddings (SOM2)", "content": "The concept path combines metadata and concept information into a unified representation. For a document d with metadata m and identified concepts {c1, ..., cN }, the combined embedding is:\nEsom2(d) = [Emetadata(d); Econcepts({C1,...,CN})] (12)\nwhere in our implementation we proxy documents' metadata by their entities (tickers) information, i,e. we set Emetadata(d) = Eentities({e1,...,ek} where Eentities({e1,...,ek} is the same embedding for entity information as constructed above for SOM1. The new element in (12) is the combined concept embedding Econcepts. The following subsections detail its construction."}, {"title": "Concept Embedding Organization", "content": "Our implementation employs a direct approach to concept embedding using the same MiniLM-L6-v2 transformer model used for text embeddings. For each predefined concept in the financial domain (e.g., \"Revenue Growth,\" \"Market Share,\" \"Product Launch\u201d), we generate a base embedding:\nEc = MiniLM(c) \u2200c\u2208C (13)\nwhere C is our predefined set of 24 market-relevant concepts. The system maintains a concept similarity cache to optimize repeated computations:\ns(ci, cj) = cos(Eci, Ecj) (14)"}, {"title": "Multi-Concept Document Handling", "content": "CAISSON handles documents with multiple concepts through a two-stage process. First, concepts are identified either through direct matching against our concept dictionary or through inference:\n{!metadata.concepts if explicitly provided \nCd = { \ninfer_concepts(d) otherwise (16)\nFor concept inference, we implement two versions:\n\u2022 Version 1: Dictionary-based matching using predefined concept synonyms\n\u2022 Version 2: Inference of basis ontological concepts through embedding similarity comparison\nOur basis implementation that uses synthetic data to illustrate the working of CAISSON relies on the first method for all numerical experiments. This enables filtering out possible errors due to probabilistic inference of concepts, and focusing on the performance of the CAISSON model itself, rather than on the performance of its variable components that can be alternatively used for version 2. For real-world applications where the inference of concepts is not amenable to simple NLP-based search methods, version 2 might be a more appropriate option.\nThe combined concept representation for a document is computed as an unweighted average of individual concept embeddings:\nEconcepts = 1/|Cd| \u03a3 Ea (17)\nCECd\nThe concept similarity between documents or between a query and documents is computed using a cached similarity matrix:\nsim(C1, C2) = { |C1C2|/max(|C1|,|C2|) if C1 C2 \u2260 0 \n{max S(C1, C2) otherwise\nC1EC1,C2EC2 (18)"}, {"title": "Training Process", "content": "CAISSON implements parallel training of both SOMs with independent learning rates but synchronized epochs. For each epoch, documents are processed in batches of size 32. The learning rates decay linearly:\nai(t) = a (1 \u2013 yt/T) (19)\nwhere i \u2208 1,2 indexes the SOMs, t is the current epoch, T is total epochs, and y = 0.8 is a hyperparameter controlling the speed of the learning rate decay. Initial rates are set to a = 0.05 and a2 = 0.05. Training progress is monitored through quantization errors for each SOM:\nQi = 1/N \u03a3 ||xj - WBMU(xj)|2 (20)\nj=1\nwhere N is the number of documents and BMU(xj) denotes the Best Matching Unit for document j in SOM i."}, {"title": "Document Retrieval", "content": "A final ranking process used by trained SOMs to retrieve relevant documents for a given query is described below in Sect. 4."}, {"title": "Numerical Experiments with Synthetic Analyst Notes", "content": "To evaluate CAISSON's performance, we constructed a large synthetic dataset of financial analyst notes that mimics the key characteristics of real market commentary. Our data generation procedure creates structured notes containing text content and associated metadata, with carefully controlled distributions of entities (tickers) and concepts per note.\nThe dataset construction process incorporates domain knowledge about the S&P 500 market structure in several ways. First, the ticker universe is organized by industry sectors, with 11 major sectors represented. The ticker selection process is weighted according to approximate market capitalization weights of companies in the S&P 500, ensuring that larger companies appear more frequently in the dataset. For example, major technology companies like Apple (AAPL) and Microsoft (MSFT) appear in approximately 9-10% of notes each, while smaller companies typically appear in 2-3% of notes.\nThe conceptual coverage of notes is controlled through a predefined set of 24 market-relevant concepts, such as \"Earnings beat\", \"Revenue growth\", \"Market share gain\", and \"Technological disruption\". To ensure linguistic variety, each concept is associated with multiple synonymous expressions that are randomly selected during text generation.\nThe dataset reflects real market structure through industry sector distribution, with Technology sector representing the largest share (31.95% of notes) followed by Healthcare (9.59%) and Financial Services (9.07%), closely matching typical market sector weightings. The note generation process also incorporates market capitalization data to ensure that coverage intensity correlates with company size, as is typical in real analyst coverage.\nEach generated note consists of exactly four sentences to ensure consistent length and structure. The note generation process employs templates enriched with numerical metrics and market-specific terminology, with approximately 27.5% of notes using concept synonyms to increase linguistic variety. Notes can contain up to 4 tickers and 4 concepts each, with special attention paid to maintaining semantic"}, {"title": "Training CAISSON", "content": "The training of CAISSON was performed on both 5,000 and 10,000 note datasets to assess scalability. Training times were remarkably efficient, taking approximately 60 seconds for the 5,000 note dataset and 120 seconds for the 10,000 note dataset on a standard MacBook Pro laptop, demonstrating near-linear scaling with dataset size.\nThe training process involves simultaneous optimization of both SOMs, with each maintaining its own learning rate schedule. We used initial learning rates of 0.05 for both SOM1 and SOM2. The batch size was set to 32 documents, providing a good balance between training stability and computational efficiency.\nThe training progress can be monitored through the quantization error of each SOM, as shown in Figure 3. Both SOMs demonstrate stable convergence behavior, with the majority of optimization occurring in the first 50 epochs. SOM1, processing the more complex text-metadata embeddings, typically shows a higher initial quantization error but achieves comparable final performance to SOM2.\nThe effectiveness of the trained model can be visualized through the topological organization of the document space, as illustrated in Figure 4.\nThe visualization reveals several interesting patterns in the learned document organization:\n\u2022 SOM1 shows clear industry-based clustering, with related companies typically appearing in neighboring nodes\n\u2022 SOM2 exhibits concept-based organization, where similar market events and themes are grouped together"}, {"title": "NoteRetriever: Fast Document Search with CAISSON", "content": "The NoteRetriever is a sub-module in CAISSON that implements a sophisticated document search mechanism that leverages both SOMs of the CAISSON model. When a query is received, it undergoes several preprocessing steps to enable effective matching. First, the query text is analyzed to extract relevant tickers using regular expression pattern matching, ensuring that only valid tickers from our universe are considered. Simultaneously, the query is processed to identify relevant concepts, using both direct concept matching and synonym-based detection through a predefined dictionary of concept synonyms. A more detailed description is given below."}, {"title": "Dual-Path Search Strategy", "content": "CAISSON executes parallel searches using both SOMs to leverage their complementary representations. Each query triggers:\n\u2022 SOM1 (Text-Metadata Path): Identifies BMU using text-metadata embedding, performs configurable neighborhood search, collects candidate documents from matched nodes.\n\u2022 SOM2 (Concept-Metadata Path): Identifies BMU using concept-metadata embedding, executes parallel neighborhood search, generates second candidate set."}, {"title": "Score Computation and Ranking", "content": "Query processing implements a multi-factor scoring system combining ticker matches, concept similarity, and semantic relevance. For a query q and document d, the final score is computed as:\nScore(q, d) = 0.6 \u00d7 TickerScore + 0.2 \u00d7 ConceptScore + 0.2 \u00d7 SemanticScore (21)\nwhere:\nTickerScore = TqTa / max(|Tq|, |Ta|), ConceptScore = sim(Cq, Cd), SemanticScore = cos(Eq, Ed) (22)\nHere Tq, Td are ticker sets, Cq, Ca are concept sets, and Eq, Ed are document embeddings. This weighting scheme prioritizes exact entity matches while accounting for conceptual and semantic relevance, crucial for financial document retrieval. The system maintains caches for frequent ticker combinations and concept similarities to optimize retrieval speed, achieving sub-second response times even for complex multi-entity queries."}, {"title": "Optimization Techniques", "content": "Three key optimization strategies maintain CAISSON's sub-second response times during document retrieval. First, a comprehensive caching system pre-computes and stores frequently needed information,"}, {"title": "Example Queries and Retrieved Results", "content": "To illustrate CAISSON's retrieval capabilities, let's examine a specific multi-entity query seeking information about market share developments for two major technology companies:\nQuery Example\nQuery: \"What are the latest developments affecting market share gain for GOOGL and AAPL?\u201d\nThis query is particularly interesting as it combines multiple elements:\n\u2022 Multiple entities: GOOGL (Alphabet) and AAPL (Apple)\n\u2022 Specific concept: \"Market share gain\"\n\u2022 Temporal aspect: \"latest developments\"\nThe system processes this query through both SOMs, producing the following top results:\nRetrieved Notes (SOM1)\n1. \"GOOGL has gained 18.1% market share over the past year. AAPL has gained 5.7% market share over the past year. GOOGL increased market presence, indicating significant implications for its market position.\"\n2. \"AAPL has gained 15.4% market share over the past year. GOOGL has gained 15.4% market share over the past year. Recent data shows GOOGL increased market presence, reflecting broader market dynamics.\"\nRetrieved Notes (SOM2)\n1. \"AMZN has gained 9.5% market share over the past year. ETSY has gained 9.2% market share over the past year. AMZN competitive position improvement, indicating significant implications for its market position.\"\n2. \"COST has gained 0.2% market share over the past year. HD has gained 3.7% market share over the past year. Recent data shows HD expanded customer base, reflecting broader market dynamics.\"\nThe retrieval results demonstrate several key aspects of CAISSON's capabilities:\n1. SOM1 Effectiveness:"}, {"title": "SynFAQA: Evaluation Framework for CAISSON", "content": "Building upon our synthetic analyst notes dataset described in Section 3, we developed SynFAQA (Synthetic Financial Analyst Questions and Answers generator): a comprehensive evaluation framework that tests both single-hop and multi-hop question retrieval capabilities of CAISSON. Using synthetic data as the foundation for our evaluation framework provides a crucial advantage: we have complete knowledge of the underlying information and relationships in the documents, enabling creation of questions with verifiable ground truth answers. This contrasts with evaluations based on real financial documents, where establishing ground truth often requires extensive manual annotation and may still contain ambiguities.\nDrawing inspiration from HotPotQA [8], our framework extends their methodology to the financial domain, incorporating domain-specific concepts and entity relationships while maintaining the controlled nature of our synthetic data. The resulting framework may be of broader interest for benchmarking LLMs on financial text analysis tasks, as it provides a systematic way to evaluate both retrieval accuracy and reasoning capabilities with known ground truth.\nThe question generation process employs a graph-based approach where notes are connected through bridge elements - either shared tickers or common concepts. This enables the generation of three distinct types of questions:\n\u2022 Bridge questions: Require finding connections between notes through common elements\n\u2022 Comparison questions: Focus on comparing similar metrics or developments across different entities\n\u2022 Yes/No questions: Ask about the similarity or difference in trends between entities"}, {"title": "Question Types and Distribution", "content": "To ensure comprehensive evaluation of CAISSON's capabilities, we generated a balanced dataset of 20,000 questions evenly split between single-hop and multi-hop queries (Table 1). Single-hop questions test basic retrieval capabilities with queries that can be answered using information from a single document. Multi-hop questions evaluate more sophisticated retrieval abilities by requiring information synthesis across multiple documents.The multi-hop questions are further categorized into three types based on their reasoning requirements, see Table 2)."}, {"title": "Bridge Element Analysis", "content": "Bridge elements are crucial components of multi-hop questions, providing the logical connections between related documents. Our analysis focused on validating these connections to ensure the questions genuinely require multi-document synthesis rather than being answerable through single-document retrieval. Throughout our question set, we identified and validated 1,730 distinct concept-based bridges, with each bridge element verified to exist in its respective connected documents. This validation process ensures that our multi-hop questions provide a meaningful test of CAISSON's ability to perform complex information synthesis across multiple documents, aligning with its dual-view architecture that explicitly handles both semantic and conceptual relationships."}, {"title": "Examples and Question Difficulty Analysis", "content": "Our framework generates questions of varying complexity. Here are representative examples of different question types:\n\u2022 Single-hop question:\n\"What's the latest information on TSLA regarding upward revision?\"\nThis requires finding relevant information about TSLA's earnings revisions from a single note.\n\u2022 Bridge question:\n\"What different approaches do BA and BKNG take regarding enhanced operational margins?\u201d\nThis question requires bridging information about profit margin expansion across two different notes about BA and BKNG.\n\u2022 Comparison question:\n\"Between PXD and COP, which company had more favorable fair value?\u201d\nThis involves comparing valuation metrics between two energy sector companies.\n\u2022 Yes/No question:\n\"Did EQR and WELL experience similar trends in product launch?\"\nThis requires analyzing and comparing product launch information across two real estate companies.\n\u2022 Multi-ticker questions:\nSome of questions have up to 4 tickers mentioned in them. Examples of such questions:\n\"Did SHW, FCX, and DOW and NEM experience similar trends in price target increase?\u201d\n\"What insights emerge when comparing industry rivalry situation between LLY and PG and KO"}, {"title": "Question Complexity Analysis", "content": "We analyzed the complexity of generated questions across multiple dimensions, including syntactic features (word and character counts) and semantic complexity (number of entities and concepts). Results are shown in Table 3."}, {"title": "Experimental Evaluation and Benchmark Comparisons with SynFAQA", "content": "Our evaluation of CAISSON focuses on three key aspects: comparison against baseline RAG implementations, analysis of performance across different question types, and assessment of scaling behavior with"}, {"title": "Baseline RAG Models", "content": "To rigorously evaluate CAISSON's performance, we implement two strategically chosen baseline models that represent different levels of architectural sophistication:\n\u2022 TextRAG (Weak Baseline): This implementation represents the traditional RAG approach, using only text embeddings for retrieval. It serves as a basic benchmark that helps quantify improvements over standard industry practice. TextRAG:\nUses pure text embeddings without entity awareness\nImplements standard vector similarity search\nRepresents the common baseline in RAG literature"}, {"title": "Overall Performance", "content": "Tables 5 and Fig. 7 present the comprehensive performance metrics. CAISSON demonstrates superior retrieval performance even against the strong TextEntityRAG benchmark, achieving a Mean Reciprocal Rank (MRR) of 0.5231 compared to 0.3720 for TextEntityRAG (40.6% improvement) and 0.2106 for the basic TextRAG (148.4% improvement). The substantial improvement over TextEntityRAG is particularly"}, {"title": "Question Type Analysis", "content": "Table 6 compares CAISSON's performance against baseline models across different question types. CAISSON shows particularly strong performance on single-hop queries, with an MRR of 0.6911 compared to 0.2947 for TextRAG and 0.5655 for TextEntityRAG. For multi-hop queries, while the performance somewhat deteriorates, CAISSON still maintains superior retrieval quality with an MRR of 0.3551 versus 0.1265/0.1786 for the baseline models.\nA more detailed analysis of multi-hop question types (Table 7) reveals that bridge questions achieve the strongest performance (MRR 0.4209, Top-5 accuracy 0.6808), followed by Yes/No questions (MRR 0.3504) and comparison questions (MRR 0.2468). This pattern suggests that CAISSON's dual-path architecture is particularly effective at handling queries requiring connection identification between documents."}, {"title": "Ticker Complexity Analysis", "content": "Having established CAISSON's overall performance advantages, we now examine how the system handles queries of varying complexity. This analysis is particularly important given the wide range of query types encountered in real-world financial applications. Table 8 (see also Fig. 7) reveals an interesting relationship between performance and query complexity as measured by the number of tickers. Several key patterns emerge:"}, {"title": "Implications for Real-World Applications", "content": "The performance characteristics of CAISSON reveal several important implications for practical applications:\n\u2022 Robust Query Handling: CAISSON demonstrates exceptional performance across a wide range of query complexities, with strong results for queries involving up to four tickers. This makes it suitable for real-world financial applications, where such queries represent the majority of analyst information needs."}, {"title": "Summary and Future Directions", "content": "This paper introduced CAISSON as a novel approach to RAG systems that combines multi-view clustering with document retrieval. By leveraging dual Self-Organizing Maps, CAISSON creates complementary organizational views of the document space - one based on semantic-entity relationships and another on concept-entity patterns. This multi-view clustering approach enables more nuanced and accurate document retrieval while maintaining efficient query processing. Our comprehensive evaluation demonstrates several key findings:"}, {"title": "Summary of Key Results", "content": "This paper introduced CAISSON as a novel approach to RAG systems that combines multi-view clustering with document retrieval. By leveraging dual Self-Organizing Maps, CAISSON creates complementary organizational views of the document space - one based on semantic-entity relationships and another on concept-entity patterns. This multi-view clustering approach enables more nuanced and accurate document retrieval while maintaining efficient query processing. Our comprehensive evaluation demonstrates several key findings:\n\u2022 Enhanced Retrieval Performance: CAISSON achieves substantial improvements over baseline RAG models:\n148.4% improvement in Mean Reciprocal Rank (0.5231 vs 0.2106) compared to TextRAG\n40.6% improvement in MRR (0.5231 vs 0.3720) over the strong TextEntityRAG benchmark\nExceptional Top-1 accuracy (0.4240 vs 0.1140/0.2740)\nStrong Top-5 accuracy (0.6580 vs 0.3430/0.4920)\n\u2022 Query Type and Complexity Handling:\nOutstanding single-hop performance (MRR 0.6911)\nStrong multi-hop capabilities (MRR 0.3551)\nPeak performance for two-ticker queries (MRR 0.5703)\nRobust performance maintained through four-ticker queries (MRR 0.4366)\n\u2022 Multi-View Clustering Benefits:\nEffective organization of document space through complementary views\nSuperior entity tracking across query complexities\nEnhanced concept-aware retrieval through dedicated SOM path\nBalanced performance across different query types"}, {"title": "Connections to Recent Developments", "content": "Our approach to evaluation and the observed performance patterns align with several recent developments in Al research. The SynFAQA methodology shares important conceptual parallels with DeepMind's Michelangelo framework [2], particularly in using controlled synthetic data for systematic evaluation. While Michelangelo targets general long-context reasoning, our framework applies similar principles to financial domain reasoning, with both approaches enabling precise assessment of model capabilities through:\n\u2022 Controlled Task Design: Our single-hop and multi-hop questions mirror Michelangelo's approach to primitive tasks, allowing systematic evaluation of both simple retrieval and complex reasoning capabilities. This is evidenced by CAISSON's strong differentiated performance across query types, achieving MRR of 0.6911 for single-hop queries while maintaining robust performance (MRR 0.3551) for more complex multi-hop queries.\n\u2022 Complexity Scaling: The controlled progression of query complexity provides systematic assessment of model capabilities, similar to Michelangelo's complexity scaling approach. Our results show CAISSON's particular strength in moderate complexity queries (peak MRR 0.5703 for two-ticker queries) while maintaining strong performance (MRR > 0.43) even for complex four-ticker queries.\n\u2022 Domain-Specific Benchmarking: Our synthetic data generation maintains financial domain characteristics while preventing training data leakage, enabling reliable comparison of different RAG architectures on realistic tasks. The framework's ability to generate controlled test cases with varying complexity enables systematic evaluation of both retrieval accuracy and scaling behavior."}, {"title": "Future Research Directions", "content": "Our findings suggest several promising directions for future research", "impact": "n1. Architecture Extensions:\n\u2022 Development of adaptive view-weighting mechanisms for different query types\n\u2022 Extension of the multi-view approach to capture temporal relations in the data\n\u2022 Extension to longer documents where document-level concepts and metadata must inform chunk-level retrieval\n2. Enhanced Concept Processing:\n\u2022 Zero-shot concept detection using Kimhi et al.'s [6"}]}