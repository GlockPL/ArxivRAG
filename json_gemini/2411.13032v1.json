{"title": "\"It was 80% me, 20% Al\":\nSeeking Authenticity in Co-Writing with Large Language Models", "authors": ["ANGEL HSING-CHI HWANG", "Q. VERA LIAO", "SU LIN BLODGETT", "ALEXANDRA OLTEANU", "ADAM TRISCHLER"], "abstract": "Given the rising proliferation and diversity of Al writing assistance tools, especially those powered by large language models (LLMs),\nboth writers and readers may have concerns about the impact of these tools on the authenticity of writing work. We examine\nwhether and how writers want to preserve their authentic voice when co-writing with AI tools and whether personalization of AI\nwriting support could help achieve this goal. We conducted semi-structured interviews with 19 professional writers, during which\nthey co-wrote with both personalized and non-personalized Al writing-support tools. We supplemented writers' perspectives with\nopinions from 30 avid readers about the written work co-produced with AI collected through an online survey. Our findings illuminate\nconceptions of authenticity in human-AI co-creation, which focus more on the process and experience of constructing creators'\nauthentic selves. While writers reacted positively to personalized AI writing tools, they believed the form of personalization needs to\ntarget writers' growth and go beyond the phase of text production. Overall, readers' responses showed less concern about human-AI\nco-writing. Readers could not distinguish AI-assisted work, personalized or not, from writers' solo-written work and showed positive\nattitudes toward writers experimenting with new technology for creative writing.", "sections": [{"title": "1 INTRODUCTION", "content": "From text suggestion [40] and summarization [10] to style transformation [58], metaphor generation [36], and informa-\ntion synthesis [20], burgeoning applications of artificial intelligence (AI) for text production seem to be rapidly reshaping\nwriting experiences and practices, especially with the recent high-profile releases of large language models (LLMs). Con-\nsequently, there are also concerns that vast transformations of the writer economy are likely underway [15, 37, 47, 53].\nWithin such a climate, seeking and preserving authenticity-as a cornerstone for all forms of creation-in writing\ncontent co-created with AI is likely to become an increasingly complicated yet critical matter for writers.\nIndeed, existing literature has pointed to the importance of understanding authenticity for several reasons: From\nwriters' perspectives, authenticity often determines the value of their work, which co-writing with AI might potentially\nthreaten [18]. Moreover, writing serves as the medium for writers to connect with their audiences, and authentic\nexpression contributes to the soundness of such bonds [7, 38, 52]. A deeper understanding of authenticity also facilitates\ndiscussions around ownership of work [13] and relevant practices such as declaring authorship, regulating copyright,\ndetecting plagiarism, and commissioning writers' work. Recent work on AI use for writing has begun to explore relevant\nconstructs, such as ownership, authorship, and agency [11, 13, 39, 49], but a more comprehensive understanding of"}, {"title": "2 BACKGROUND & RELATED WORK", "content": "The humanistic literature has long acknowledged authenticity as the core of human creative work [21, 42, 51]. Though\nthere is a longstanding history of proposing theoretical frameworks for authenticity, researchers have not yet formed\na consensus on the definition of authenticity. Still, they have often proposed three key themes to help define and\nconceptualize authenticity: Category, Source, and Value [2, 4, 8, 21, 23, 42, 43, 51, 62, 64]. First, the Category theme\nconcerns whether a piece of work matches one's existing beliefs about its associated category. The scope of a category\ncan vary, ranging from a particular style or school of work (e.g., Bauhaus-style architecture) to a certain era (e.g., a\nRenaissance painting). Second, the idea of authentic Source concerns whether one can trace a piece of work to a specific\nsource (i.e., a person, a place, an event, or any type of origin). This explains the importance of crediting writers and\nlabeling the origins of work. In particular, when work from certain individuals is truly one-of-a-kind-such as the\nhighly recognizable work of Picasso-audiences can easily identify them as the source of content. In such cases, the\nconcepts of Category and Source become more blended and interchangeable. Finally, the Value theme is about whether\nthere is consistency between a creator's internal states and their external expression. This focuses on whether a writer's\nperspectives, opinions, and values are consistent with what they expressed in their work.\nIn our study, we explore whether writers' perceptions of authenticity in writing align with these concepts of\nauthenticity for broader creative work and whether specific concerns apply to writing. Based on writer-centered\ndefinitions of authenticity we uncover, we further examine whether co-writing with Al is considered authentic and\nwhether emerging AI technologies change writers' views about the essence of authenticity."}, {"title": "2.1 Frameworks of Authenticity in Human Creative Work", "content": "The humanistic literature has long acknowledged authenticity as the core of human creative work [21, 42, 51]. Though\nthere is a longstanding history of proposing theoretical frameworks for authenticity, researchers have not yet formed\na consensus on the definition of authenticity. Still, they have often proposed three key themes to help define and\nconceptualize authenticity: Category, Source, and Value [2, 4, 8, 21, 23, 42, 43, 51, 62, 64]. First, the Category theme\nconcerns whether a piece of work matches one's existing beliefs about its associated category. The scope of a category\ncan vary, ranging from a particular style or school of work (e.g., Bauhaus-style architecture) to a certain era (e.g., a\nRenaissance painting). Second, the idea of authentic Source concerns whether one can trace a piece of work to a specific\nsource (i.e., a person, a place, an event, or any type of origin). This explains the importance of crediting writers and\nlabeling the origins of work. In particular, when work from certain individuals is truly one-of-a-kind-such as the\nhighly recognizable work of Picasso-audiences can easily identify them as the source of content. In such cases, the\nconcepts of Category and Source become more blended and interchangeable. Finally, the Value theme is about whether\nthere is consistency between a creator's internal states and their external expression. This focuses on whether a writer's\nperspectives, opinions, and values are consistent with what they expressed in their work.\nIn our study, we explore whether writers' perceptions of authenticity in writing align with these concepts of\nauthenticity for broader creative work and whether specific concerns apply to writing. Based on writer-centered\ndefinitions of authenticity we uncover, we further examine whether co-writing with Al is considered authentic and\nwhether emerging AI technologies change writers' views about the essence of authenticity."}, {"title": "2.2 The Impact of Al Use on Authenticity in Writing", "content": "The increasing popularity of using AI for creative tasks\nhas motivated recent work to investigate the possible impact of AI on several aspects of creators' work, including credit,\nauthorship, ownership, control, and agency [11, 13, 25, 35], many of which are closely related to authenticity. Recent\nstudies, workshops, panels, and other forms of discussion [41] have thus far revealed mixed opinions from research\ncommunities, creators, and the general public toward these topics. Here, we summarize a few emergent themes:\nWriters remain hesitant to declare AI co-authorship publicly. Recent work on perceived authorship of AI-\ngenerated text reveals a complex AI Ghostwriter Effect [13]. Through comparing personalized and pseudo-personalized\nAl tools, the findings suggest that though writers did not perceive complete ownership over AI-generated text, they\nwere also reluctant to publicly declare AI co-authorship. This reluctance could be related to writers' concerns about\nnegative responses from their readers [18]. But more importantly, writers feel that authorship should go hand in hand"}, {"title": "2.2.1 Writers' growing concerns regarding Al writing assistance.", "content": "The increasing popularity of using AI for creative tasks\nhas motivated recent work to investigate the possible impact of AI on several aspects of creators' work, including credit,\nauthorship, ownership, control, and agency [11, 13, 25, 35], many of which are closely related to authenticity. Recent\nstudies, workshops, panels, and other forms of discussion [41] have thus far revealed mixed opinions from research\ncommunities, creators, and the general public toward these topics. Here, we summarize a few emergent themes:\nWriters remain hesitant to declare AI co-authorship publicly. Recent work on perceived authorship of AI-\ngenerated text reveals a complex AI Ghostwriter Effect [13]. Through comparing personalized and pseudo-personalized\nAl tools, the findings suggest that though writers did not perceive complete ownership over AI-generated text, they\nwere also reluctant to publicly declare AI co-authorship. This reluctance could be related to writers' concerns about\nnegative responses from their readers [18]. But more importantly, writers feel that authorship should go hand in hand"}, {"title": "2.2.2 Insights from Al-mediated communication research.", "content": "Prior research on AI-mediated communication (AIMC) might\nbring additional perspectives on these unresolved questions around authenticity. In particular, AIMC studies have\nlooked at how Al suggestions mediate written content for communication through text, including readers' perceptions\nof both the content and the message writer [22, 24, 48, 57, 59]. Importantly, writing can be viewed as a medium of\ncommunication that connects writers and their audiences, albeit often not bi-directionally or at the inter-personal\nlevel [7, 38, 52].\nIndeed, various AIMC studies have shown that AI assistance (e.g., word-by-word suggestions or short-reply sugges-\ntions) can shift the characteristics of writing content, such as a more positive tone [22, 48, 59], or even affect topics\naddressed in writing [57]. More recently, [29] found that writing with AI assistance on opinionated content affects not\nonly a user's writing output but also their own opinions toward the written topic. In light of these findings as well as\nother repeated findings around readers' negative perceptions of communicators who use AI [31, 44], scholars have\nexpressed ethical concerns around Al's implications for information and interpersonal trust [28, 45], users' sense of\nagency and authenticity [59], and social relationships [24, 32, 48] as users adopt AI more widely for composing content\nfor interpersonal communication."}, {"title": "2.3 Designing Tools for Human-Al Co-Writing", "content": "Recent work has already proposed a wide variety of possible Al writing assistance scenarios (see our summary in\nTable 1), ranging from more generic to genre-specific support. Researchers have also experimented with AI assistance\nthat supports different stages of one's writing process as well as providing multi-form [26, 66] or multimodal [63] support\nat once. While many of these explorations have been shown to be helpful for writers, the majority of these studies\nfocus on improving usability, productivity, and sometimes creativity of writing. By contrast, the possible influences\nof these tools on authenticity often only arise in exploratory analyses or post-study conversations with participants,\nleaving a noticeable literature gap."}, {"title": "3 METHODS", "content": "We examined writer-centered definitions of authenticity and the impact of AI writing assistance on authentic writing\nprimarily through semi-structured, qualitative interviews with professional writers. To answer RQ3 and to enable\nparticipants to respond to our inquiries with situated experiences, we adopted two versions of AI-powered writing\nassistance tools: one with personalization through in-context learning, and one without personalization. We investigated\nhow participants wrote with these tools in real time and delved into their co-writing experiences through interviews\nfollowing each writing session. We complemented perspectives from these direct users of such emerging technology\n(i.e., writers) through a follow-up online study with indirect stakeholders (i.e., readers) [16]. Through these two parts of\nthe study, we synthesize a more comprehensive view of authenticity in writing. The full study protocol (as illustrated in\nFigure 1) was reviewed and approved by the Institutional Review Board (IRB) of the authors' affiliation."}, {"title": "3.1 Overview of the Study Design", "content": "We examined writer-centered definitions of authenticity and the impact of AI writing assistance on authentic writing\nprimarily through semi-structured, qualitative interviews with professional writers. To answer RQ3 and to enable\nparticipants to respond to our inquiries with situated experiences, we adopted two versions of AI-powered writing\nassistance tools: one with personalization through in-context learning, and one without personalization. We investigated\nhow participants wrote with these tools in real time and delved into their co-writing experiences through interviews\nfollowing each writing session. We complemented perspectives from these direct users of such emerging technology\n(i.e., writers) through a follow-up online study with indirect stakeholders (i.e., readers) [16]. Through these two parts of\nthe study, we synthesize a more comprehensive view of authenticity in writing. The full study protocol (as illustrated in\nFigure 1) was reviewed and approved by the Institutional Review Board (IRB) of the authors' affiliation."}, {"title": "3.2 Part 1: Interviews with Professional Writers", "content": "We conducted Part 1 of the study with 19 professional writers. The study includes a pre-interview survey (through an\nonline questionnaire) where participants shared their writing samples and professional experiences as writers, and\nexpressed their opinions on what authenticity in writing means to them and on AI writing assistance. During each\ninterview, we first asked participants to further elaborate on their perspectives regarding authenticity and co-writing\nwith AI. Next, participants engaged in two writing sessions to co-write a short passage with a personalized and a\nnon-personalized AI writing assistance tool in a randomized order. Immediately after each writing session, we asked\nparticipants to reflect on their co-writing experiences through semi-structured interviews. After both writing sessions,"}, {"title": "3.2.1 Recruitment and participants.", "content": "We recruited 21 professional writers through Upwork, a platform frequently used\nto recruit professionals with specific expertise. We partnered with a recruitment specialist from the Upwork team\nto ensure candidates provided authentic information on their profiles and verified their past freelance work on the\nplatform. We also used the pre-interview survey for screening purposes, in which participants elaborated on their\nprofessional experiences as writers, shared their writing samples, and provided relevant links to their professional\nwebsites and profiles (see more details of the pre-interview survey in Section 3.2.2). One of the participants did not\nshow up and one decided to drop out from the study, resulting in N = 19 participants who fully completed the study;\ndata from all 19 participants was used for analyses. In Table 2, we report the background and experiences of these\nwriter participants.\nA note on sample size: Like the majority of qualitative research, we did not have a definite approach to determining\nthe exact sample size. However, we followed recommendations from prior methodological reviews [6, 46] such that (1)\nwe ensured the number of interviews conducted for the same task fell between 15 \u2013 30; (2) we referred to the sample\nsizes of qualitative studies in relevant publication venues (e.g., [17, 18, 66]); and (3) throughout the study period, we\nobserved that emergent themes arose and saturated even accounting for new interviews. Despite the relatively small\nsample size, we ensured that participants had diverse backgrounds to improve the generalizability of our results, such\nthat our findings were not confounded with participants' seniority, experience, and writing genre."}, {"title": "3.2.2 Pre-interview survey.", "content": "The pre-interview survey begins with obtaining informed consent from participants (by\nsigning a consent form). After consenting, participants responded to a series of questions through free text responses.\nThese questions ask participants to describe (1) the unique characteristics of their writing, (2) their prior experience\nworking with AI and non-Al writing support tools, if any, (3) their definition of authenticity in writing. At the end of\nthe survey, participants were asked to submit a short writing sample of their own (200 words) that best represents"}, {"title": "3.2.3 Al writing assistance.", "content": "We adopted open-source code from [40] to run the CoAuthor interface (see Figure 2) with\nGPT-4 and used it as the Al writing assistance tool for the interview sessions. CoAuthor is a system built by Lee et\nal. [40] that allows writers to request next-sentence suggestions from an LLM, and the open-source code allows for\ncustomization, such as choosing which LLM to use and its parameters. We chose CoAuthor as the writing assistance"}, {"title": "3.2.4 Co-writing session with Al.", "content": "Participants were asked to write one short passage (~200 words) for each co-writing\nsession; participants chose to write on a topic similar (but not identical) to that of their submitted writing sample. Each\nco-writing session lasted between 20 - 25 minutes. The time was determined by piloting with two professional writers.\nAll participants were able to finish their assigned 200-word passage within the time frame. As participants wrote, the\nCoAuthor interface recorded their writing logs and their final writing output. Writers' behavioral data recorded through\nthe writing logs include (1) the frequency and timing of their Al suggestion requests, (2) the frequency and timing of\ntheir acceptances and/or rejections of AI suggestions, (3) the AI text suggestions provided at each of their requests, (4)\nthe AI suggestions (in text) accepted, if any, and (5) the text inserted by writers. Additional details about the writing logs\ncan be found in Appendix D. After each writing session, participants participated in a short interview (~15 minutes) to\nreflect on their writing experiences."}, {"title": "3.3 Part 2: Online Surveys with Avid Readers", "content": "In Part 2, we conducted an online study to understand avid readers' perceptions of authenticity, AI writing assistance,\nand work co-written by human writers and AI. We used writing work produced by six writers in Part 1, each of which\nrepresents a unique literature genre (P8: science fiction/fantasy, P10: biography, P12: poetry, P14: spiritual, P15: romance,\nand P19: screenplay), as materials for Part 2. These include anonymized writing samples that writers submitted in\nthe pre-interview survey of Part 1 (i.e., work written by themselves) as well as the two passages they wrote with the\npersonalized and non-personalized AI during Part 1.\nParticipants of Part 2 (i.e., readers) first responded to a few open-ended questions about their attitudes toward AI-\nassisted writing. They then read three writing passages from the same writer (their solo work, work with personalized\nAI, and work with non-personalized AI) in randomized order. After reaching each article, participants assessed the\nwriting with respect to likeability (how much they liked the writing), enjoyment (how much they enjoyed reading the\nwriting), and creativity (how creative they thought the writing was). We chose these evaluation criteria as they were\nalso used in prior work to assess AI-assisted writing (e.g., [32, 45]).\nNext, readers performed two sets of comparisons with the three pieces of writing. First, they were informed that\nsome of these pieces were written with AI, and were asked to compare and rate how likely each piece was to have been\nwritten by a human writer alone, or with the help of an Al writing support tool. Second, readers were informed which\npiece was the writer's solo work. They then compared the two AI-assisted writings by responding to the question:\n\"Compared to the text written independently by the author, to what extent do you think the co-written text preserves the\nauthentic voice of the author?\" They were also asked to identify in which part(s) of the writing the author might have\nadopted AI assistance. Readers were blinded to the two different AI conditions; namely, they did not know which\npassage was co-written with a personalized versus a non-personalized AI tool, or that personalization was the variable\nbeing studied. We applied this blinding approach to ensure participants' assessments were based on the content per se\nand were not affected by their perceptions of personalized AI assistance.\nAfter rating and comparing writing passages, participants responded to a few questions about their general opinions\nabout AI-assisted writing. Table 3 presents the procedure of Part 2 and the variables measured throughout. Additionally,\nthe full Part 2 questionnaire is attached in Appendix G and descriptive statistics of numeric measures are reported\nin Table 4. The entire Part 2 study was conducted online and took around 45 - 60 minutes to complete. Participants\nreceived cash compensation for their participation. Once again, the study protocol was reviewed and approved by the\nIRB of the authors' affiliation."}, {"title": "3.3.1 Recruitment and participants.", "content": "We recruited participants on Reddit to find avid readers who regularly read the\nparticular literature genres written by the six writers chosen from Part 1. One of the researchers joined multiple\nsubreddits dedicated to discussions about reading. (See Appendix E for a list of these subreddits.) Recruitment messages\nwere posted on discussion threads in these subreddits. Interested individuals first filled out an interest form that surveyed\nthe literature genres they were interested in and how frequently they read. They also reported the subreddit where\nthey saw the recruitment message. 30.27% of those who filled out our interest form read on a daily basis, and 60.54% of\nthem read weekly. We recruited participants who read their genres of interest at least once a week (Appendix F shows a\nlist of genres that our reader participants regularly read. Note that each participant might frequently read more than\none genre.). As prior research on Reddit has identified a large number of machine-generated responses, we showed"}, {"title": "3.3.2 Screening Al-generated responses.", "content": "We first set out to collect feedback from three readers for each writer's work,\nresulting in 6 \u00d7 3 = 18 participants. However, we noticed a substantial number of open-text responses that might be\nAI-generated (e.g., by ChatGPT) after examining our initial batch of data. We discussed among our research team to\nidentify the following criteria for screening: (1) Responses that were consistently written in a bullet-point style following\nthe structure of \"[Topic]: [Description],\" which is a common style in ChatGPT output; (2) We used our survey questions\nas prompts and collected responses from popular generative Al tools, including ChatGPT, Microsoft Bing Chat, and\nGoogle Bard. We cross-checked whether participants' responses overlapped significantly with those AI-generated ones."}, {"title": "4 RESULTS", "content": "We conducted analyses with three streams of data: a) the interview data and b) writing logs (i.e., behavioral data)\nfrom writers in Part 1, and c) survey data from readers in Part 2. We transcribed the interview data and analyzed the\ntranscripts through thematic coding and affinity diagrams. The first author led the qualitative data analysis, and met the\nresearch team weekly to discuss findings in order to reduce subjectivity during the data analysis phase (see Appendix C\nfor the full list of emerging themes). When reporting findings from our qualitative data, we follow [1] and use the\nfollowing phrases to indicate the portion of our writer participants who shared each insight: a few (1-5), some (6-10),\nmost (11-15), nearly all (16-19).\nFor any within-subject comparison (e.g. writers' behavioral logs using the two versions of the AI tool), we used\nlinear mixed effect models, controlling for participants' subject IDs and order effect. For scales that capture individuals'\nopinions toward AI-assisted writing (e.g., readers' survey responses), we used Wilcoxon one-sample tests to examine\nwhether participants' ratings differ significantly from the midpoint (3) of the five-point scales. Details for our analytic\napproaches are listed in Table 5. We organize our results according to our research questions with data from different\nstreams, when applicable. We also present two additional points that emerged from our interviews with writers:"}, {"title": "4.1 Overview of Data Analysis", "content": "We conducted analyses with three streams of data: a) the interview data and b) writing logs (i.e., behavioral data)\nfrom writers in Part 1, and c) survey data from readers in Part 2. We transcribed the interview data and analyzed the\ntranscripts through thematic coding and affinity diagrams. The first author led the qualitative data analysis, and met the\nresearch team weekly to discuss findings in order to reduce subjectivity during the data analysis phase (see Appendix C\nfor the full list of emerging themes). When reporting findings from our qualitative data, we follow [1] and use the\nfollowing phrases to indicate the portion of our writer participants who shared each insight: a few (1-5), some (6-10),\nmost (11-15), nearly all (16-19).\nFor any within-subject comparison (e.g. writers' behavioral logs using the two versions of the AI tool), we used\nlinear mixed effect models, controlling for participants' subject IDs and order effect. For scales that capture individuals'\nopinions toward AI-assisted writing (e.g., readers' survey responses), we used Wilcoxon one-sample tests to examine\nwhether participants' ratings differ significantly from the midpoint (3) of the five-point scales. Details for our analytic\napproaches are listed in Table 5. We organize our results according to our research questions with data from different\nstreams, when applicable. We also present two additional points that emerged from our interviews with writers:"}, {"title": "4.2 Writer-Centered Conceptions of Authenticity in Writing", "content": "Our findings first reveal that writers conceptualize authenticity through the source of content, internal experiences and\nidentities that ground their work, and the actual writing outcomes. Although participants' reflections did resonate with\nsome of the definitions of authenticity as established in the existing literature (i.e., category, source, and value), they\nplaced further emphasis on viewing authenticity through their internal experiences in addition to through their explicit\nexpressions in writing. Moreover, regardless of how participants defined and understood authenticity, many of them\nindicated that authentic writing is the essence of good writing, and saw the likely impact of AI on authenticity in"}, {"title": "4.2.1 Defining authenticity through the source of content (Source authenticity).", "content": "Several writers described authenticity\nas a \"who\" question, focusing on who wrote the text or was the source of the content. This concept mirrors Source, a\nlong-standing theoretical component of authenticity in the existing literature, and is directly related to both writers'\nand readers' considerations for authorship. Participants whose conceptualizations aligned with Source authenticity\nemphasized the entity who took actions and contributed to a piece of work. For example, such actions might include\nproducing a piece of text or trying to understand the audience's interest.\nWriters who held this view also saw AI writing assistance as a direct threat to authenticity. With AI participating in\nthe writing process, writers are no longer the sole source of content generation, raising questions about the authenticity\nof their writing. A few participants highlighted the importance of having writers produce their work themselves, as\nthey learn, revise, and refine their work through the actions and processes of writing. Furthermore, they also believed\nthat audiences learn more about writers through the way they produce content."}, {"title": "4.2.2 Defining authenticity through constructing and expressing their authentic self (Authentic self).", "content": "Writers mentioned\nseveral internal states during their writing processes as key constructs of authenticity, many of which have been less\ncovered by prior literature. These include (1) whether writers can freely express their emotions to form emotional\nconnections with their readers, (2) whether the process of writing allows them to feel passionate about their work,\n(3) whether they have the autonomy to select topics and content that are personally important to them, (4) and most\nimportantly, whether a writer can justify having their name and identity behind their work. In other words, a writer\nclaims the authenticity of their writing if they can soundly argue why the piece of work can only be done by them as\nthe writer.\nThis view of authenticity is shaped by the belief that writers' identities, backgrounds, and lived experiences serve as\nfundamental materials of writing and enrich their work. These cornerstones-whether writers can genuinely express\ntheir own experiences and identities through the writing as well as whether they feel enthusiastic and believe in the"}, {"title": "4.2.3 Defining authenticity through writing outcomes (Content authenticity).", "content": "Examining the outcomes of writing is yet\nanother approach to assessing authenticity. More specifically, authentic writing equals writing that best represents\nthe work of a writer, where one can tell who the writer is by reading the text. This perspective echoes the \u201ccategory\"\nconstruct of authenticity, which is the most adopted definition of authenticity in the literary studies space [21, 23, 42].\nNonetheless, it was the least referred to and a less critical construct from writers' points of view. In our study, the few\nwriters who defined authenticity through this lens were concerned about the influences of Al writing assistance on\nauthenticity, as AI's input might \u201cshift writers away from the typical ways of [their] writing practices\" (P2), intruding\non their usual tones, voices, and ways of presentation in their writing. Under this conception of authenticity, since\nauthenticity in writing is evaluated by comparing it to a writer's representative work, the writers in our study also\nconnected the meaning of authenticity back to various writing elements (e.g., word choice, style, use of references and\nmetaphors) that uniquely characterize their own writing."}, {"title": "4.3 (Re)claiming Authenticity Through Practices of Co-writing with Al", "content": "Through the co-writing sessions, writers identified several practices during the writing process that could help shape\nauthenticity in writing with AI assistance. In other words, how writers use Al while writing plays a key role in\ndetermining the authenticity of AI-assisted writing. These determinants include: (1) whether one starts with a clear\nvision for writing in mind, (2) when one calls for writing assistance from AI, (3) what portion of contribution one makes\nin the writing, and (4) what purpose and usage that AI writing assistance serves."}, {"title": "4.3.1 Setting off with a clear vision of what to achieve in one's writing.", "content": "In describing their writing, nearly all writers\nreported that they seldom start writing with a blank sheet. Instead, they typically begin their processes with some\nvision for writing in mind. Such a vision need not be a concrete, fully fleshed-out idea. But writers often already have\ncertain directions or settings in mind that they would like to set up for the passage, given the preparatory work they\ndid before writing (e.g., ideation, research, collecting references and other writing materials). Otherwise, they lack an\nanchor to cohesively guide how they respond to, select, and curate AI suggestions alongside their own writing. In such\na case, writers worried that they might be at more risk of being affected by AI suggestions, depriving their authentic\nvoices in writing.\n\u201cI think it [AI] has the possibility to [affect writers] if you let it. But if you already have your ideas of what\nyou want to stay true to, then it isn't going to affect your authentic voice, in my opinion. So to me, it comes\ndown to user influence and how much the user chooses the directions that the Al suggests.\" (P4)"}, {"title": "4.3.2 Working with Al through the \"fuzzy area\" of idea development when writing.", "content": "It is worth recognizing that writers\nwork through various stages throughout their writing processes. While writers bring a premature vision to initiate\nwriting, there is typically a \"fuzzy area\" (P04) between the starting point and well-developed ideas, which eventually\nleads to a fully crafted piece of writing. Most writers believe that Al writing assistance is most acceptable and is\nconsidered as having the least harm on authenticity at this middle ground for two reasons. First, the ways in which\nwriters would interact with Al writing assistance tools during this stage primarily serve to consolidate and further\ndevelop their ideas (we discuss opportunities for Al tools to support writers more extensively in Section 4.3.4). At this\nstage, writers write to incubate and organize their thinking rather than to produce text to construct the actual piece\nof writing. Therefore, though AI suggestions contribute pieces of content, they serve as materials that facilitate the\nthought processes rather than the writing per se."}, {"title": "4.3.3 Claiming contribution through content gatekeeping.", "content": "Most importantly, in our study, most writers considered\nAI-assisted writing as authentic as long as they contributed more to the writing. Specifically, writers noted that one's\ncontribution does not necessarily equal how much text they wrote. In their view, real contributions require \"content\ngatekeeping\" (P8)\u2014that is, actively deciding what goes into the writing content. Given this conception of contribution,\nwriters frequently described themselves as doing most of the work during the writing process as they took charge of\nselecting, revising, and incorporating AI suggestions into their writing. Whether a word, a sentence, or a paragraph is\nproduced with Al assistance or not, it is the human writer, instead of the AI tool, that takes control over whether to\nadopt, remove, or revise each piece of content."}, {"title": "4.3.4 Beyond Al as a co-writer: Opportunities for supporting authentic writing processes.", "content": "Besides requesting content\nsuggestions from AI (i.e., what the CoAuthor interface supports), some writers believed they could benefit from AI\nassistance while preserving their authentic voices by leveraging AI capabilities as a means of internalization, a driver of"}, {"title": "4.4 Finding and Preserving Authentic Voice: Personalization as Double-Edged Sword", "content": "While most writers showed preferences toward personalized Al writing assistance, they foresaw both positive and\nnegative influences of personalization. They believed grappling with such dilemmas was key to adopting AI tools in\nwhat they dubbed as a more collaborative rather than reliant fashion. In this section, we discuss how writers perceived\nthe impact of personalized AI on their writing, together with whether and how personalization affected their usage\nbehaviors with the tool, and how readers responded to their"}]}