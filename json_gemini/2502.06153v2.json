{"title": "Low Tensor-Rank Adaptation of Kolmogorov-Arnold Networks", "authors": ["Yihang Gao", "Michael K. Ng", "Vincent Y. F. Tan"], "abstract": "Kolmogorov-Arnold networks (KANs) have demonstrated their potential as an alternative to multi-layer perceptions (MLPs) in various domains, especially for science-related tasks. However, transfer learning of KANs remains a relatively unexplored area. In this paper, inspired by Tucker decomposition of tensors and evidence on the low tensor-rank structure in KAN parameter updates, we develop low tensor-rank adaptation (LOTRA) for fine-tuning KANs. We study the expressiveness of LOTRA based on Tucker decomposition approximations. Furthermore, we provide a theoretical analysis to select the learning rates for each LoTRA component to enable efficient training. Our analysis also shows that using identical learning rates across all components leads to inefficient training, highlighting the need for an adaptive learning rate strategy. Beyond theoretical insights, we explore the application of LoTRA for efficiently solving various partial differential equations (PDEs) by fine-tuning KANs. Additionally, we propose Slim KANs that incorporate the inherent low-tensor-rank properties of KAN parameter tensors to reduce model size while maintaining superior performance. Experimental results validate the efficacy of the proposed learning rate selection strategy and demonstrate the effectiveness of LOTRA for transfer learning of KANs in solving PDEs. Further evaluations on Slim KANs for function representation and image classification tasks highlight the expressiveness of LoTRA and the potential for parameter reduction through low tensor-rank decomposition.", "sections": [{"title": "I. INTRODUCTION", "content": "The Kolmogorov-Arnold representation theorem (KART) states that any multivariate continuous function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ can be decomposed into a sum of univariate functions. Specifically, there exists a set of univariate functions {$\\phi_{p,q}$}$_{p=1,q=1}^{2n+1,n}$ and {$\\psi_q$}$_{q=1}^{2n+1}$ such that\n\n$f(x) = \\sum_{q=1}^{2n+1} \\psi_q \\left( \\sum_{p=1}^{n} \\phi_{p,q} (x_p) \\right),$\n\nwhere $x = [x_1,x_2,...,x_n]$. Inspired by this theorem, researchers have explored novel network architectures, as an alternative to multi-layer perceptrons (MLPs) guaranteed by the Universal Approximability Theory (UAT). However, earlier network designs based on KART have been underwhelming, mainly because they are inherently limited to two layers and a width restricted to at most twice the input dimension, as prescribed by Equation (1). These models typically represent the implicit univariate functions {$\\phi_{p,q}$}$_{p=1,q=1}^{2n+1,n}$ and {$\\psi_q$}$_{q=1}^{2n+1}$ using polynomials or splines. Although theoretically sound, such models often fail to perform competitively in practical applications, lagging behind MLPs. Recently, Liu et al. [1] proposed a KART-based model known as Kolmogorov-Arnold Networks (KANs), which extends the architecture to support multiple layers and arbitrary width. This design overcomes the stringent limitations on width and depth inherent in traditional KART-based networks, enabling greater expressiveness and flexibility.\nKANs have demonstrated remarkable empirical performance across a wide range of domains, including computer vision, time series forecasting, reinforcement learning, physics-informed machine learning, and large language models. Li et al. [2] developed U-KAN, which replaces the bottleneck layers of U-Net with KAN layers, achieving higher accuracy with reduced computational cost in medical image segmentation and generation tasks. This improvement is attributed to the potentially higher expressiveness of KANs compared to MLPs. Kich et al. [3] explored the application of KANs as function approximators in proximal policy optimization for reinforcement learning, demonstrating that KANs can match the performance of MLPs while requiring fewer parameters. Notably, KANs excel in symbolic and function representation tasks. In physics-informed machine learning, KANs consistently match or outperform MLPs, especially in approximating solutions of partial differential equations (PDEs), as reported by Wang et al. [4]. To model the separable properties inherent in some PDE solutions, a separable physics-informed KAN architecture was proposed [5]. This design processes each block independently using individual KAN models before synthesizing them in the final stage, significantly reducing model complexity by incorporating the natural separable structure of PDE solutions. More interesting applications and investigations of KANs can be found in [6]\u2013[17].\nAs illustrated above, one of the most significant advantages of KANs over MLPs lies in their superior performance in science-related tasks, particularly in physics-informed machine learning. Here, we mainly focus on using KANs to approximate solutions of PDEs. However, in practice, even slight variations in physical parameters or conditions can result in changes to the solutions. Instead of solving a single PDE and storing the corresponding network, it is often necessary to solve a class of PDEs with varying physical parameters. This setting requires solving PDEs multiple times and storing numerous networks, which significantly increases computational costs and storage requirements.\nHowever, the transfer learning of KANs has not been investigated, despite its potential importance in enhancing the efficiency of model training and storage, particularly in physics-informed machine learning applications. Inspired by the recently popular low-rank adaptation (LoRA) technique used in Transformer models, which significantly accelerates the fine-tuning process for large language models, and the"}, {"title": "II. BACKGROUND AND PRELIMINARIES", "content": "In this section, we first introduce the notation used throughout the paper. We then review the mathematical definition and key concepts of KANs. Finally, we discuss the mathematical formulation of Tucker decomposition for tensors, which serves as a foundation for the methodology section.\n\n**A. Notation**\n\nIn this paper, we use bold lowercase letters (e.g., x), bold capital letters (e.g., A), and calligraphic uppercase letters (e.g., $\\mathcal{A}$) to denote vectors, matrices and tensors, respectively. Scalars are represented using regular (non-bold) letters (e.g.,\n\n**B. Kolmogorov-Arnold Networks**\n\nAs defined in [1], the (l + 1)-st layer $z_{l+1} := [z_{l+1,1},..., z_{l+1,n_{l+1}}] $ of a KAN admits\n\n$z_{l+1,q} = \\sum_{p=1}^{n_l} \\phi_{l,p,q}(z_{l,p}), \\quad q \\in [n_{l+1}],$\n\nwhere $n_l$ is the width of the l-th layer and $\\phi_{l,p,q}$ denotes the (p,q)-th representation function in the l-th layer. This formulation is consistent with the KART presented in Equation (1), but it introduces some key differences. Notably, KANs do not impose restrictions on width ($n_l$ can exceed twice the input dimension n) or depth (e.g., l can be much greater than 2). However, the implicit representation functions {$\\phi_{l,p,q}$}$_{l,p,q}$ in each layer are unknown, and must be approximated using practical function classes. In this sense, the development of KANs draws inspiration both from the theoretical principles of KART and the flexibility of MLPs.\nIn practice, the implicit representation functions are expressed by combinations of basis functions, i.e.,\n\n$\\phi_{l,p,q}(z) = \\sum_{k=1}^{n_d} a_{l,p,q,k}b_k(z),$\n\nwhere {$b_k$}$_{k=[n_d]}$ is a set of basis functions and $A^l := (a_{l,p,q,k}) \\in \\mathbb{R}^{n_l\\times n_{l+1} \\times n_d}$ denotes the parameter tensor at l-th layer. The basis functions adopted can include Chebyshev polynomials [18], Legendre polynomials [19], Fourier series [20], wavelet functions [21], Bernoulli polynomials, Fibonacci polynomials, Jacobi polynomials [22], B-splines [1], as well as rational and fractional polynomials [23], [24], due to the universal approximability of those basis functions.\nIn summary, the transformation of KANs from l-th layer to (l +1)-th layer is formulated as\n\n$z_{l+1,q} = \\sum_{p=1}^{n_l} \\sum_{k=1}^{n_d} a_{l,p,q,k}b_k(z_{l,p}), \\quad q \\in [n_{l+1}],$\n\nwhere $z_l = (z_{l,1}, z_{l,2},..., z_{l,n_l})^T \\in \\mathbb{R}^{n_l}$ and $z_{l+1} = (z_{l+1,1}, z_{l+1,2},..., z_{l+1,n_{l+1}})^T \\in \\mathbb{R}^{n_{l+1}}$ denote the neurons at the l-th and (l + 1)-th layers, respectively, and $A^l := (a_{l,p,q,k}) \\in \\mathbb{R}^{n_l\\times n_{l+1} \\times n_d}$ represents the parameter tensor at l-th layer.\n\n**C. Tucker Decomposition**\n\nTensor decomposition methods have shown significant success in various domains, such as hyperspectral image processing [25]\u2013[27], multidimensional time series analysis [28], [29], and high-dimensional machine learning [30], [31]. Tucker decomposition method [32] compresses the tensor into a smaller core tensor with transformation matrices applied along each mode, whose structure is similar to the singular value decomposition (SVD) of matrices. Other tensor decomposition methods include CANDECOMP/PARAFAC (CP) method [33],"}, {"title": "III. METHODS", "content": "In this section, we first provide evidence supporting the potential low tensor-rank structure of the parameter updates, which motivates the development of the low tensor-rank adaptation (LoTRA) for transfer learning of KANs. We then theoretically analyze the approximation and expressiveness capabilities of LoTRA, using well-established results on Tucker decomposition. Furthermore, we propose a theoretically guided strategy for learning rate selection of each component of LoTRA to achieve efficient training using gradient descent. In contrast, we demonstrate theoretically that using the same learning rate scale for all components is inefficient for LoTRA.\n\n**A. Motivation and Evidence**\n\nThe concept of low-rank adaptation is widely recognized in transfer learning and domain adaptation. In multi-task learning, the parameter matrix is typically decomposed into global patterns shared across tasks and task-specific adaptations within the shared subspace [39], [40]. This decomposition captures shared information while regularizing models to prevent overfitting. Similarly, in domain adaptation, both source and target domains are projected into a shared feature space through low-rank mappings, reflecting the existence of underlying shared low-rank subspaces [41], [42]. In the fine-tuning of large language models, Hu et al. [43] empirically demonstrated that weight updates during adaptation exhibit low intrinsic matrix rank. This observation led to the development of low-rank adaptation (LoRA) for Transformer models.\nBased on their observations, we have reason to hypothesize that the update of KANs parameters has a low tensor-rank structure in transferring learning tasks, specifically, fine-tuning on new tasks based on the pre-trained models. The potential intrinsic low tensor-rank property has been seen in some empirical investigations on KANs. For example, function mappings in science domains usually involve specific structures that may lead to similarities between implicit representation functions {$\\phi_{l,p,q}$} in Equation (2). Specifically, the symbolic representation of the function $f (x_1, x_2, x_3, x_4) = \\exp( - (\\sin (\\pi (x_1^2 + x_3)) + \\sin (\\pi (x_3 + x_4)))) $ is discussed in Liu et al. [1], where a three-layer KAN fits the function well. However, the first layer involves mostly the square operation applied to all coordinates and the second layer only represents the sin function. The final layer produces the outputs after passing through the exponential function. This example shows the repeated operations in each layer, implying the possibility of compression of parameter tensors of KANs. Similar properties have been observed in the solutions of PDEs studied in physics-informed KANs, as shown in Shukla et al. [6] and Wang et al. [4], further supporting the possibility of low-rank structures in KAN parameter tensors, particularly in symbolic representation and physics-informed machine learning.\nIn this section, we first provide evidence supporting the\nTo further validate our hypothesis regarding the low-rank updates of KAN parameter tensors, we conduct a simple transfer learning experiment on function representation. We use a three-layer KAN with a hidden dimensions $n_1 = n_2 = 32$ and the number of basis functions $n_d = 32$ to represent and pre-train on the function $u(x_1, x_2) = \\sin \\left( (1 - \\sqrt{x_1^2 + x_2^2})^{2.5} \\right)$. We then fine-tune the KAN model on a new task with the modified objective function: $u(x_1,x_2) = \\sin \\left( (1 - \\sqrt{x_1^2 + x_2^2})^{2.5} \\right) + \\sin((1 - \\sqrt{x_1^2 + x_2^2})^5)$. Our goal is to empirically investigate whether the parameter tensor updates in the fine-tuned model exhibit a low tensor-rank structure. We measure the tensor rank using the Tucker rank and determine it using HOSVD, where the singular values of the mode-unfolded matrices correspond to the Tucker rank of the tensors. Specifically, zero singular values in the unfolded matrices indicate a low Tucker rank. Our observations reveal that KANs at the pre-training stage already exhibit a low Tucker rank structure, where most of the singular values are small,\n\n**B. Low Tensor-Rank Adaptation**\n\nBuilding on observations of low-rank structures in previous studies across various domains, as well as the empirical investigations of a simple transfer learning example, we develop an efficient and effective transfer learning method for fine-tuning KANs on new tasks, given a pre-trained model. The developed low tensor-rank adaptation (LoTRA) method preserves shared information across tasks and adapts to new tasks with integrated low-rank structures on parameter tensor updates. For simplicity, we express Equation (3) as\n\n$z_{l+1} := \\Phi_l (z_l; A_l),$\n\nwhere $A_l \\in \\mathbb{R}^{n_l\\times n_{l+1} \\times n_d}$ is a third-order parameter tensor, $l\\in [L]$, and L denotes the depth of the KAN. Therefore, the forward propagation of the KAN model can be formulated as\n\n$\\Psi (x; \\{A_l\\}_{l\\in[L]}) := \\Phi_L (\\Phi_{L-1}... (\\Phi_1 (x; A_1) ; A_{L-1}) A_L) = \\Phi_L \\circ \\Phi_{L-1} \\circ ... \\circ \\Phi_1 (x; \\{A_l\\}_{l\\in[L]}) .$\n\nSuppose we first pre-train the KAN model on a given task, where the pre-trained model is denoted as $\\Psi_{pt} (x; \\{A_{l,pt}\\})$ and {${A_{l,pt}}$} represents the set of parameter tensors across layers. For a new task, the fine-tuned model is denoted as $\\Psi_{ft} (x; \\{A_{l,ft}\\})$ where {${A_{l,ft}}$} is the updated set of parameter tensors. We define the target model, which represents the optimal function within the KAN function class, as $\\Psi_{tg} (x; \\{A_{l,tg}\\})$ where {${A_{l,tg}}$} denotes the corresponding set of target parameter tensors. The parameter tensor $A_{l,ft}$ of the fine-tuned model $\\Psi_{ft}$ updated using LoTRA follows:\n\n$A_{l,ft} = A_{l,pt} + G_l \\times_1 U_l^{(1)} \\times_2 U_l^{(2)} \\times_3 U_l^{(3)},$\n\nwhere $G_l \\in \\mathbb{R}^{r_{l,1} \\times r_{l,2} \\times r_{l,3}}$ denotes the core tensor, and $U_l^{(1)} \\in \\mathbb{R}^{n_l\\times r_{l,1}}$, $U_l^{(2)} \\in \\mathbb{R}^{n_{l+1}\\times r_{l,2}}$ and $U_l^{(3)} \\in \\mathbb{R}^{n_d\\times r_{l,3}}$ represent transformation matrices. Here, we require that $r_{l,1} \\le n_l$, $r_{l,2} \\le n_{l+1}$, and $r_{l,3} \\le n_d$, where $(r_{l,1}, r_{l,2}, r_{l,3})$ is the core tensor size for l-th layer. Since the Tucker rank of the updates $(r_{l,1}, r_{l,2}, r_{l,3})$ is smaller than the original parameter size $(n_l, n_{l+1}, n_d)$, LoTRA effectively reduces the model complexity with integrated low-rank structures on tensor updates while preserving essential task-specific information in $A_{l,ft}$.\nWe first pre-train the model $\\Psi_{pt}$ on a given task, and obtain the corresponding parameter tensors {$A_{l,pt}$}$_{l\\in[L]}$. When solving a new task that shares similarities with the pre-training task, we aim to transfer shared information to the new task. To achieve this, we fine-tune the model using LoTRA, resulting in the fine-tuned model $\\Psi_{ft}$ with updated parameter tensors {$A_{l,ft}$}$_{l\\in[L]}$, following the update rule in Equation (6). In this adaptation, the shared information is retained in the pre-trained tensors {$A_{l,pt}$}$_{l\\in[L]}$ while the task-specific information is incorporated through the trainable parameters {$G_l, U_l^{(1)}, U_l^{(2)}, U_l^{(3)}$}$_{l\\in[L]}$, which are the only parameters updated during fine-tuning in $\\Psi_{ft}$. By using this adaptation approach, LoTRA eliminates the need to re-learn shared information from data, thereby significantly improving training efficiency compared to training a new model from scratch. Moreover, the task-specific information is effectively extracted, maintaining model expressiveness and performances on new tasks. Moreover, instead of fine-tuning all parameters on the new task, the explicitly introduced low-rank structure in LoTRA acts as a regularization, mitigating overfitting and filtering out noise arising from limited training data."}, {"title": "IV. THEORETICAL ANALYSIS", "content": "In this section, we study the theoretical expressiveness of LoTRA within the framework of low Tucker-rank approximations. To enable efficient fine-tuning using LoTRA with gradient descent, we propose a theoretically grounded learning rate selection strategy. Moreover, we prove that assigning identical learning rates to all trainable parameters is inefficient for training. These theoretical results not only enhance our understanding of LoTRA's expressiveness and capacity but also provide a deeper insight into the training process.\n\n**A. Expressiveness of LoTRA**\n\nWe define the error tensor between the target parameter tensor and the pre-trained parameter tensor as:\n\n$E_l = A_{l,tg} - A_{l,pt}.$\n\nLet $E_l^{(1)} \\in \\mathbb{R}^{n_l \\times (n_{l+1}n_d)}$, $E_l^{(2)} \\in \\mathbb{R}^{n_{l+1} \\times (n_ln_d)}$, and $E_l^{(3)} \\in \\mathbb{R}^{n_d \\times (n_ln_{l+1})}$ denote the corresponding mode-i unfolding matrices of $E_l$. The following lemma on the Tucker approximation of tensors plays a crucial role in characterizing the approximation capability of LoTRA, which integrates low Tucker-rank adaptation to parameter tensor updates.\n\n*Lemma 1 (Tucker Approximation [38]).* For each $l \\in [L]$, there exist $(G_l, U_l^{(1)}, U_l^{(2)}, U_l^{(3)})$, such that\n\n$\\|A_{l,ft} - A_{l,tg}\\|^2_F \\le \\sum_{i=1}^3 \\left[ \\sum_{r=r_{l,i}+1}^{n_i} \\sigma_r(E_l^{(i)}) \\right]^2,$"}, {"title": "B. Efficient Training of LoTRA", "content": "Besides the approximation capability of the fine-tuned model with LoTRA, optimization efficiency with gradient descent is also a key focus and a critical consideration for real-world applications. For analysis convenience, we consider fine-tuning the model on a toy case, where we are given one training data (x, y) of a new task and the loss function is formulated as $\\mathcal{L} = \\frac{1}{2} \\|\\Psi(x) - y\\|^2$ with $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^m$. We first consider a one-layer model and the adaptation with core tensor size $(r_1,r_2, r_3)$. Suppose the pre-trained model is $\\Psi_{pt}(x; A_{pt})$ and the fine-tuned model $\\Psi_{ft}(x; A_{ft})$ satisfies $A_{ft} = A_{pt} + G \\times_1 U^{(1)} \\times_2 U^{(1)} \\times_3 U^{(3)}$, where $A_{pt} \\in \\mathbb{R}^{n \\times m \\times n_d}$, $G \\in \\mathbb{R}^{r_1\\times r_2\\times r_3}$, $U^{(1)} \\in \\mathbb{R}^{n\\times r_1}$, $U^{(2)} \\in \\mathbb{R}^{m\\times r_2}$, and $U^{(3)} \\in \\mathbb{R}^{n_d\\times r_1}$. Here, we assume that $(r_1, r_2,r_3)$ is fixed and much smaller than $(n,m,n_d)$. The pre-trained parameter tensor $A_{pt}$ remains fixed and {$G,U^{(1)},U^{(2)},U^{(3)}$} are trainable. We denote the (p,q, k)-th element of the tensor $G$ as $g_{p,q,k}$ and the j-th column of $U^{(i)} = [u^{(i),1},..., u^{(i),r_i}]$ is denoted as $u^{(i),j}$. All trainable parameters are optimized by the vanilla gradient descent, following the updates:\n\n$g_{p,q,k}^{t+1} = g_{p,q,k}^{t} - \\eta_0 \\frac{\\partial \\mathcal{L}_t}{\\partial g_{p,q,k}^{t}}, \\quad u^{(i),j}_{t+1} = u^{(i),j}_{t} - \\eta_i \\frac{\\partial \\mathcal{L}_t}{\\partial u^{(i),j}_{t}},$\n\nfor $p \\in [n]$, $q \\in [m]$, $k \\in [n_d]$, $j \\in [r_i]$, and $i \\in \\{1,2,3\\}$. Here, the subscript $t$ denotes the parameters after t steps of gradient descent, and $(\\eta_0, \\eta_1, \\eta_2, \\eta_3)$ are learning rates for {$G,U^{(1)}, U^{(2)}, U^{(3)}$} respectively. Similarly, we denote the fine-tuned model $\\Psi_{ft}$ at step t as $\\Psi_{t,ft}$ with parameters {$G_t, U_t^{(1)}, U_t^{(2)}, U_t^{(3)}$}, and the corresponding loss function is denoted as $\\mathcal{L}_t$.\nDenote $X \\in \\mathbb{R}^{n \\times n_d}$, where the k-th column of $X$ is obtained by applying the basis function $b_k$ elementwise to the input column vector x. Then, the change in function values after one step of gradient descent satisfies\n\n$\\Delta\\Psi_{t,ft} := \\Psi_{t,ft} - \\Psi_{t-1,ft} = \\sum_{p=1}^{r_1} \\sum_{q=1}^{r_2} \\sum_{k=1}^{r_3} \\delta_{p,q,k}^{t,0} + \\sum_{i=1}^3 \\delta_i^{t},$\n\nwhere\n\n$\\delta_{p,q,k}^{t,0} = -\\eta_0 \\frac{\\partial \\mathcal{L}}{\\partial g_{p,q,k}^t} = -\\eta_0 v_t u_t^{(2),q} (u_t^{(1),p})^T X u_t^{(3),k},$\n\n$\\delta_1^{t} = -\\eta_1 \\sum_{q'=1}^{r_2} \\sum_{k'=1}^{r_3} \\frac{\\partial \\mathcal{L}}{\\partial u^{(1),p}_{t-1}} = -\\eta_1 \\sum_{q'=1}^{r_2} \\sum_{k'=1}^{r_3} g_{p,q',k'}^{t-1} v_t u_t^{(2),q'} (X u_t^{(3),k'})^T ,$\n\n$\\delta_2^{t} = -\\eta_2 \\frac{\\partial \\mathcal{L}}{\\partial u^{(2),q}_{t-1}} = -\\eta_2 \\sum_{p'=1}^{r_1} \\sum_{k'=1}^{r_3} g_{p',q,k'}^{t-1} v_t (u_t^{(1),p'})^T X u_t^{(3),k'} ,$\n\n$\\delta_3^{t} = -\\eta_3 \\frac{\\partial \\mathcal{L}}{\\partial u^{(3),k}_{t-1}} = -\\eta_3 \\sum_{p'=1}^{r_1} \\sum_{q'=1}^{r_2} g_{p',q',k}^{t-1} v_t u_t^{(2),q'} (u_t^{(1),p'})^T X ,$\n\nwhere $v_t := \\frac{\\partial \\mathcal{L}}{\\partial \\Psi_{ft}} = \\Psi_{t,ft}(x) - y$ denotes the partial derivative of the loss with respect to the output of the fine-tuned model. The detailed derivation of the linearization of $\\Delta\\Psi_{t,ft}$ can be found in Appendix B Here, {$\\delta_{p,q,k}^{t,0}, \\delta_1^{t}, \\delta_2^{t}, \\delta_3^{t}$} represent the first-order linearization terms of $\\Delta\\Psi_{t,ft}$, and the higher-order terms with respect to learning rates are neglected. The training efficiency of LoTRA is evaluated based on the magnitude of the first-order improvement in the function value, as defined in Definition 1.\n\n*Definition 1.* The training of LoTRA is considered as efficient if {$||\\delta_{p,q,k}^{t,0}\\|^2$}, {$||\\delta_1^{t}\\|^2$}, {$||\\delta_2^{t}\\|^2$}, and {$||\\delta_3^{t}\\|^2$} are $\\Theta(1)$ with respect to the model size (n, m, $n_d$), for $p \\in [r_1]$, $q \\in [r_2]$, $k \\in [r_3]$, and $t \\ge 2."}, {"title": "V. APPLICATIONS", "content": "In this section, we discuss the potential applications of LoTRA in fine-tuning and training KANs.\n\n**A. Physics-Informed KANs**\n\nKANs have demonstrated superior performance over MLPs in some science-related tasks, making them promising models for physics-informed machine learning. We investigate the potential of LoTRA in training KANs for solving a class of PDEs, aiming to reduce computational cost and storage requirements with enhanced efficiency. Consider the problem of solving a class of PDEs with different physical parameters $\\lambda \\in \\mathcal{S}$:\n\n$\\mathcal{D}[u_{\\lambda}; \\lambda] = f(x; \\lambda), \\quad x \\in \\Omega,$\n$\\mathcal{B}[u_{\\lambda}; \\lambda] = g(x; \\lambda), \\quad x \\in \\partial \\Omega,$\n\nwhere $\\mathcal{D}$ and $\\mathcal{B}$ are differential operators defined in the interior domain $\\Omega$ and on its boundary $\\partial \\Omega$, respectively. The solution corresponding to physical constants $\\lambda$ is denoted as $u_{\\lambda}$."}, {"title": "B. Slim KANS", "content": "Another potential application of LoTRA is slimming the model, enabling a slimmer KAN architecture with low tensor-rank structures on parameter tensors. Unlike transfer learning,"}, {"title": "VII. CONCLUSION", "content": "In this paper, we introduce the concept of low tensor-rank adaptation (LoTRA) for the transfer learning of KANs, inspired by Tucker decomposition in tensors and the success of LoRA for matrix parameter updates. We begin by empirically observing that both KAN parameters and fine-tuning updates exhibit a low tensor-rank structure, which motivates us to develop LoTRA as an efficient parameter update method. We then theoretically establish the expressiveness of LoTRA based on Tucker decomposition approximations. Additionally, we propose a theoretically grounded learning rate selection strategy for efficient training of LoTRA, providing theoretical insights for practical implementation. Our analysis further reveals that applying identical learning rates to all LoTRA components is inefficient. Beyond theoretical insights, we explore the practical applications of LoTRA, particularly in fine-tuning KANs for solving PDEs and slimming KANS models. Experimental results validate our proposed learning rate selection strategy and demonstrate the effectiveness of LoTRA in fine-tuning KANs for solving PDEs. Furthermore, we evaluate Slim KANs in function representation and image classification tasks, showing that slim KANs maintain satisfying performance and significantly reduce the number of parameters. This is the first paper studying the transfer learning and fine-tuning of KANs with LoTRA.\nAlthough our study mainly focuses on fine-tuning KANs using LoTRA for solving various PDEs, further exploration of LoTRA for broader transfer learning tasks remains an important direction. A deeper theoretical analysis of LoTRA is needed to enhance our understanding of its underlying properties, leading to better model interpretability and practical implementation. Future research could further refine its theoretical foundations and explore its practical integration into more complex deep learning architectures."}, {"title": "APPENDIX", "content": "**A. Proof for Theorem 1**\n\nFor notational simplicity", "n_d": "and $p \\in [n_l", "Delta\\Psi_{t,ft}$**\n\n$\\Delta\\Psi_{t,ft}": "Psi_{t,ft} - \\Psi_{t-1,ft} = \\sum_{p=1}^{r_1} \\sum_{q=1}^{r_2} \\sum_{k=1}^{r_3} g_{p,q,k}^{t} u_t^{(2),q} (u_t^{(1),p})^T X u_t^{(3),k} = \\sum_{p=1}^{r_1} \\sum_{q=1}^{r_2} \\sum_{k=1}^{r_3} (g_{p,q"}]}