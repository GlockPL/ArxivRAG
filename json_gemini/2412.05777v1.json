{"title": "Strategizing Equitable Transit Evacuations: A Data-Driven Reinforcement Learning Approach", "authors": ["Fang Tang", "Han Wang", "Maria Laura Delle Monache"], "abstract": "As natural disasters become increasingly frequent, the need for efficient and equitable evacuation planning has become more critical. This paper proposes a data-driven, reinforcement learning (RL)-based framework to optimize bus-based evacuations with an emphasis on improving both efficiency and equity. We model the evacuation problem as a Markov Decision Process (MDP) solved by RL, using real-time transit data from General Transit Feed Specification (GTFS) and transportation networks extracted from OpenStreetMap (OSM). The RL agent dynamically reroutes buses from their scheduled location to minimize total passengers' evacuation time while prioritizing equity priority communities. Simulations on the San Francisco Bay Area transportation network indicate that the proposed framework achieves significant improvements in both evacuation efficiency and equitable service distribution compared to traditional rule-based and random strategies. These results highlight the potential of RL to enhance system performance and urban resilience during emergency evacuations, offering a scalable solution for real-world applications in intelligent transportation systems.", "sections": [{"title": "1. Introduction", "content": "The increasing frequency of natural disasters, such as wildfires, flooding, heat waves, hurricanes, and earthquakes, poses significant risks to residents' finances, society, and safety. Efficient and real-time evacuation planning strategies are essential not only to save costs but also to protect human lives. Buses, with their flexibility and large capacity, offer a cost-efficient solution for evacuations. Khalili et al. (2024) conducted a comprehensive review identifying key challenges and research gaps in transit-based evacuation planning. This review highlights the critical role of integrating public transportation when developing evacuation strategies, in particular when enhancing efficiency and safety. Evacuation plans based on transit are crucial for individuals without personal vehicles and for vehicle owners during disasters when cars may be inaccessible. The review emphasizes the need to consider spatial and temporal dynamics, social and demographic factors, and multi-stakeholder coordination. Despite advancements, challenges still persist as for example data accuracy, model validation, and stakeholder coordination, necessitating further research and interdisciplinary collaboration to improve transit-based evacuation strategies.\nPrevious studies on bus evacuation problems, such as those by Hamacher and Tjandra (2001), Altay and Green III (2006), Yusoff et al. (2008), and Goerigk et al. (2013), primarily focus on optimizing evacuation time. Chen and Zhan (2008) considered buses as agents in optimizing bus-based evacuation planning. Sayyady and Eksioglu (2010) addressed the bus evacuation problem considering traffic flow dynamics to model real-world complexity. Goerigk and Gr\u00fcn (2014) studied uncertain bus evacuation problems, modeling the uncertainty in the number of evacuees. Goerigk et al. (2013) proposed a branch-and-bound algorithm to optimize total evacuation time for the problem with a single bus depot.\nHuang et al. (2016) highlighted the importance of decision variables in emergency evacuation planning, focusing on the allocation of evacuees and resources. In transit-based evacuation planning, decision variables often include vehicle routing, scheduling, and allocation, as well as the location and allocation of shelters and pick-up points. Some studies also incorporate relief supply and distribution planning. Objective functions typically aim to minimize evacuation time, transportation costs (Bish, 2011), and evacuation risks while maximizing the number of evacuees transported safely. Other objectives include minimizing the network clearance time (Goerigk et al., 2013)."}, {"title": "2. Methodology", "content": "In response to the limitations of existing evacuation frameworks, particularly in addressing service equity and operational timeliness, we propose a data-driven, RL-based Markov decision framework. This approach integrates real-time transit data and road network to dynamically adjust bus routing and scheduling during emergencies, ensuring both operational efficiency and equitable service distribution."}, {"title": "2.1. Network modeling", "content": "The transportation network is modeled as a directed graph G = (V, E, B, I), where V represents the set of nodes, including origins, shelters, and normal transit nodes, and E represents the set of directed links between these nodes. Each node v \u2208 V is characterized by its coordinates, type (origin, shelter, or transit), demand (number of evacuees to be transferred) \u03bb(v), and remaining capacity of shelter \u03bc(v). At origins, \u03bc(v) is always 0, and at shelters, \u03bb(v) is always 0. Each link e \u2208 E is characterized by its travel time t(e), direction, and connectivity between nodes. Each bus b for b\u2208 B is featured by its capacity c(b), number of evacuees on board p(b), and the destination of current route d(b). The nodes and links in the transportation network are mapped to the census tracts i \u2208 I (refer to Figure 6), where link e\u00a1 \u2208 E(i) and node v; \u2208 V(i)."}, {"title": "2.2. Markov Decision Process for evacuation problem", "content": "As shown in Figure 1, the evacuation problem is formulated as a Markov Decision Process defined by the tuple (S, A, R, \u03b3), where:\n\u2022 State Space S: The state S\u209c at time t includes the locations of all buses represented by {s\u209cb, s\u209cv}. s\u209cb = {c(b), p(b), d(b)|b \u2208 B} defines the status of all buses, and s\u209cv = {\u03bb(\u03c5), \u03bc(v) | v \u2208 V} indicates the number of evacuees at each origin, and the remaining capacity of each shelter at time t.\n\u2022 Action Space A: The action a\u209c(b) \u2208 V taken by the agent includes decisions to reroute buses, specifying which bus should move to which node as the destination for the next time step.\n\u2022 Reward Function R: The reward function R(S\u209c, a\u209c) combines the travel time and an equity evaluation index, designed to minimize the total evacuee waiting and travel time while penalizing inequities. The following section provides detailed information of the reward function.\n\u2022 Discount Factor \u03b3: The discount factor \u03b3 \u2208 [0, 1) determines the present value of future rewards."}, {"title": "2.3. Equity emphasized rewards", "content": "The overall reward R minimizes the total evacuee waiting and travel time with an inequity penalty:\n$\\R = T + J$\n$\\T = \\sum_{b\\in B} p(b) + \\sum_{i\\in I} \\sum_{v\\in V(i)} \\lambda(v_i) \\Delta t$\nwith T, the total travel and waiting time for evacuees with a fixed time interval \u0394t. This function facilitates an analysis at the node level, allowing researchers to incorporate specific demand data per node if available. In the absence of such data, it is permissible to allocate the average demand from zone i to each corresponding node v\u1d62. J is the inequity penalty. Inspired by the penalty measure in Wang and Delle Monache (2022), the inequity penalty is calculated as follows:\n$\\J = |rpb(\\lambda_i, X_i)|\\cdot T, i \\in I$\nwhere rpb(\u03bb\u1d62, X\u1d62) is the point-biserial coefficient between the evacuee demand $\\lambda_i = \\sum_{v_i \\in V(i)} \\lambda(v_i)$ of zone i, and the binary variable X\u1d62 that indicates if the zone i is a equity priority community (EPC) (Metropolitan Transportation Commission, 2021) or not. EPC refers to tracts with a high proportion of equity-priority communities. Various criteria can be applied to identify an EPC, which is then represented by the binary variable X\u1d62 \u2208 {0,1}. The point-biserial correlation coefficient is then calculated as:\n$rpb(\\lambda_i, X_i) = \\frac{\\overline{\\lambda}_e - \\overline{\\lambda}_{ne}}{s_n} \\sqrt{\\frac{n_e n_{ne}}{n^2}} i \\in S.$\nIn equation (4), zones are divided into two groups: EPC zones, denoted by subscript e, and non-EPC zones, denoted by subscript ne. Here, $\\overline{\\lambda}_e$ and $\\overline{\\lambda}_{ne}$ represent the mean evacuee demand in the EPC and non-EPC groups, respectively,"}, {"title": "2.4. Reinforcement Learning controller", "content": "To address the bus evacuation problem, the Proximal Policy Optimization (PPO) algorithm is employed to train the RL agent due to its stability and efficiency within policy gradient methods. This section delineates the MDP formulation, the PPO algorithm, and the training process in detail.\nThe PPO algorithm is a policy gradient method that directly optimizes the policy \u03c0\u03b8(a|s). The primary objective is to maximize the expected cumulative reward. The PPO algorithm is characterized by its clipped surrogate objective, which constrains updates to the policy to prevent substantial deviations from the previous policy, thereby maintaining stability."}, {"title": "2.4.1. Clipped surrogate objective", "content": "The objective function for PPO is formulated as:\n$L^{CLIP}(\\theta) = E_t [min (r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t)]$\nwhere:\n\u2022 $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio between the new policy \u03c0\u03b8 and the old policy \u03c0\u03b8old\n\u2022 $\\hat{A}_t$ is the advantage estimate at time t, computed to reduce variance in the policy gradient.\n\u2022 \u03f5 is a hyperparameter that determines the clipping range.\nThe clipping term $clip(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)$ ensures that the ratio $r_t(\\theta)$ does not deviate significantly from 1, thereby preventing large policy updates that could destabilize training."}, {"title": "2.4.2. Advantage estimation", "content": "The advantage estimate $\\hat{A}_t$ is computed using the Generalized Advantage Estimation (GAE), which provides a method to calculate the advantage function $\\hat{A}_t$ by weighting the temporal difference residuals:\n$\\hat{A}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l+1}$\nwhere:\n\u2022 $\\delta_t = R_t + \\gamma V(S_{t+1}) - V(s_t)$ is the temporal difference error, with R\u209c representing the reward at time t, \u03b3 the discount factor, and V(s\u209c) the value function.\n\u2022 \u03bb is the GAE parameter that controls the bias-variance trade-off.\nThe GAE method improves the bias-variance trade-off by adjusting \u03bb, thereby providing a more stable and reliable estimate of the advantage function."}, {"title": "2.5. Training process", "content": "The training process involves iterative interactions with the environment, collection of trajectories, and updates to the policy. The detailed steps are as follows:\n1. Initialize the policy \u03c0\u03b8 and value function V\u03c6 with random parameters \u03b8 and \u03c6.\n2. Collect trajectories by executing the current policy \u03c0\u03b8 within the environment. Each trajectory \u03c4 consists of states st, actions at, rewards R\u209c, and next states St+1."}, {"title": "3. Illustrative Examples", "content": "To provide a detail illustration of how the proposed RL framework operates in the context of bus evacuation planning, this section presents an example using a simplified six-node network scenario. This controlled setting is specifically designed to clarify the problem space and illustrate feasible solutions, without the application of the RL controller. The purpose is to provide a foundational understanding of the evacuation dynamics under simplified conditions, without the complexity of larger real-world networks, which helps in visualizing how the model handles various aspects of the problem, such as route selection and capacity management. This example serves to bridge the conceptual gap to more complex real-world applications detailed in subsequent sections. Section 3.1 introduces how we represent networks. Section 3.2 discusses two feasible solutions without applying the RL framework, we focus instead on the potential impacts of equity. Section 3.3 introduces the application of the RL framework in this six-node example.\nAs shown in Figure 2, the six-node network with two buses we consider is represented as follows:\n$\\G_o = (V_o, E, B_o, I_o)$\n$\\V_o = \\{o_1, o_2, d_1, d_2, n_1, n_2\\}$\n$E = \\{1, 2, ..., 16\\}$\n$B = \\{b_1, b_2\\}$\n$I = \\{o_1, o_2\\}$\nThis network consists of two endangered communities, denoted as origin nodes o\u2081 and o\u2082, and two shelters, denoted as destination nodes d\u2081 and d\u2082. Nodes n\u2081 and n\u2082 are driving network nodes without capacity or demand. The numbers in set E, represent links ID. Two buses, b\u2081 and b\u2082, are initially located at the link from node n\u2081 to origin o\u2081 and the link from origin o\u2082 to node n\u2082 respectively.\nIn this network, there are 10 and 30 evacuees of origin o\u2081 and o\u2082 respectively. The capacity for shelters d\u2081 and d\u2082 are 25 and 15 respectively. The objective is to evacuate all residents of the origin communities to shelters with capacity constraints while ensuring that the evacuation process is equitable for all communities involved."}, {"title": "3.1. Network specification", "content": "The nodes in the network are identified by their unique node_id. Table 1 provides detailed information for each node, including its x and y coordinates, type, demand (number of people waiting for evacuation), capacity (number of people the shelter can accommodate), and inequity index. The geometry of each node is represented by its x and y coordinates. The node_type specifies whether the node is an origin, shelter, or other types. The nodes designated as origins have a positive demand, while those shelter nodes have a positive capacity. For all other nodes, both demand"}, {"title": "3.2. Feasible solutions", "content": "To illustrate potential strategies for the bus evacuation problem with an inequitable origin, o\u2081, in the six-node network example, we consider two feasible evacuation strategies, as outlined in Table 4. The total passenger time is computed as the sum of both the trip time and waiting time for all passengers. The trip time refers to the duration of transportation from the origin to the destination, while the waiting time is measured from the moment the hazard occurs until the passenger is picked up."}, {"title": "3.3. Optimal solution", "content": "In this section, we introduce the use of the Equity-RL framework to simulate the six-node network and derive the optimal solution.\nThe controller defines the policy for bus routing and evacuation decisions using the reinforcement learning approach, particularly the PPO algorithm.\n1. Once the bus is full, it will head to the nearest node with available capacity.\n2. Whenever the bus is not full, it will go to the nearest node with demand.\n3. The nearest node with demand is found using Dijkstra's algorithm.\n4. If there is no demand, but there are passengers on board, the bus will proceed to the nearest node with capacity.\nAt each timestep, the PPO agent observes the current state and selects an action that specifies the next destination for each bus. The action space allows the agent to reroute buses to any node in the network. The PPO agent uses the current state to predict the next optimal actions, considering both efficiency and equity. Once an action is taken, the controller updates the state of the buses and the network. Each bus moves towards its selected destination, and the passengers are updated accordingly. When a bus arrives at its destination, evacuees either get on (if the bus is picking up passengers) or get off (if the bus is dropping them at a shelter). After this, the bus's next destination is determined based on the current demand and capacity in the network, and the bus continues its route. The simulation continues until all evacuees have been evacuated or the maximum number of steps has been reached. The cumulative reward is then calculated, reflecting both the efficiency and equity of the evacuation.\nThe reward is calculated based on a combination of total passenger time and inequity penalty. Here, for this illustrative network, we assume the penalty is the total passengers' waiting time in inequitable communities. That is: R = -(total passenger time) \u2013 (total waiting time for passengers in o\u2081). The objective is to maximize the reward value. As shown in Figure 4 (b), the final output of the simulation includes bus real-time statues. Based on that, we can obtain the bus physical routing (refer to Figure 4 (a)). For this illustrative case, the optimal reward is -1,345."}, {"title": "4. Extended Simulation", "content": "Building on the insights from the simplified six-node network example, this section extends our model to a more complex real-world scenario. We apply our proposed evacuation strategies to a large-scale urban transportation network, San Francisco Bay Area, which allows us to test the effectiveness and scalability of the proposed equity-emphasized RL framework under more realistic and challenging conditions."}, {"title": "4.1. Data description", "content": "The success of the proposed approach relies heavily on accurate and comprehensive data. This section introduces the data required for the proposed system and its sources."}, {"title": "4.1.1. Network data", "content": "The transportation network is abstracted from various GIS-based data sources. In this study, the network is represented in GMNS format, and network information is provided by GTFS and OSM. The GTFS data provides real-time locations of buses and their schedules, while OSM data is utilized to extract the transportation network structure, including roads and intersections. Figure 5 shows a visualization of the OSM-derived transportation network for the San Francisco Bay Area. The network consists of 416,677 links and 228,160 nodes, covering arterials, collectors, and local roads."}, {"title": "4.1.2. Social data", "content": "Socio-demographic data is essential to generate traffic demand and identify EPCs. This study utilizes the U.S. Census 2018 dataU.S. Census Bureau (2018), which provides detailed socio-demographic information. The geographical division for the analysis is based on U.S. Census tracts, as shown in Figure 6. The EPC data, determined by the Metropolitan Transportation Commission (MTC), is used to identify equity priority communities."}, {"title": "4.1.3. Natural hazards data", "content": "To create realistic simulation scenarios, natural hazard data are required. The data curated by the Association of Bay Area Governments (ABAG) includes information on past wildfires, landslides, floods, and earthquakes. This data is used to simulate the impact of natural disasters on the transportation network. Table 5 summarizes the types of hazards and their corresponding data sources."}, {"title": "4.1.4. Real-time bus data", "content": "The real-time bus data includes bus real-time location, which is extracted from GTFS data of twenty-two bus agencies (the list of bus agencies refer to the website https://511.org/transit/agencies). Identifying bus geometries is based on bus schedule timings, stop geometry and the specific timestamp at which the bus location needs to be determined. If the bus coincides exactly with the physical location of a stop at the specified time (hazard time), then the geometry of the physical stop will be the bus geometry. If the bus is on the way from one stop to another stop, the bus location is determined by interpolating between known geographic locations of bus stops. Figure 7 represents the bus location at 10 AM on Monday."}, {"title": "4.2. Scenario generation", "content": "This section details the steps involved in creating realistic simulation scenarios by disabling links in affected zones, estimating evacuee populations, and initializing buses.\n1. Determine Impairment Scale: The impairment scale refers to the extent of damage caused by a natural hazard. Historical hazard data is analyzed to extract the number of zones affected by past disasters. This data includes information on the frequency, intensity, and geographical spread of hazards such as wildfires, floods, landslides, and earthquakes. The impairment scale I is quantified as the number of zones impacted by a particular hazard event. This step ensures that the generated scenarios are grounded in empirical evidence and reflect realistic disaster conditions.\n2. Select Impaired Zones: Once the impairment scale I is determined, the next step is to select the specific zones that will be impaired in the simulation. Two types of scenarios are considered:\n\u2022 Hazard Reproduce Scenarios: In these scenarios, impaired zones are directly mapped from historical hazard datasets. This involves identifying the geographical areas affected by previous disasters and replicating those conditions in the simulation.\n\u2022 Randomized Scenarios: In these scenarios, zones are randomly selected based on the impairment scale I. This approach simulates the spatial distribution of damage in a more stochastic manner, allowing for the assessment of evacuation strategies under varying conditions. The random selection process is guided by a probability distribution that reflects the likelihood of different zones being affected.\n3. Evacuee Estimation: In the selected impaired zones, all affiliated nodes (such as residential areas, schools and hospitals) are marked as origin nodes for evacuees. The total population P\u1d62 in each impaired zone i is obtained"}, {"title": "4.3. Benchmarks", "content": "To evaluate the performance of the proposed RL-based strategy, we compare it with the following control strategies:\n\u2022 Stochastic Strategy 1 (\u03c0\u2081): Randomly assigns buses to destinations from all impaired or shelter nodes.\n\u2022 Stochastic Strategy 2 (\u03c0\u2082): Randomly assigns buses to destinations only from the impaired zones with waiting evacuees and shelters with available capacity.\n\u2022 Rule-based Strategy 1 (\u03c0\u2081): Prioritizes bus assignments based on the shortest travel time to impaired zones and shelters.\n\u2022 Rule Strategy 2 (\u03c0\u2082): Prioritizes bus assignments based on the number of evacuees at the impaired nodes.\n\u2022 Efficiency-emphasized RL Strategy: The RL strategy with all the same settings without equity penalty in the reward function.\nThe performance of these strategies is evaluated based on the reward function, focusing on both efficiency and equity."}, {"title": "5. Results", "content": "This section presents a comprehensive analysis of the simulation results obtained from the implementation of the proposed Equity-Emphasized Reinforcement Learning (Equity-RL) strategy for bus evacuation planning. The performance of this strategy is evaluated against several benchmarks, including an Efficiency-Emphasized RL strategy, two stochastic strategies, and two rule-based strategies. The key metrics used for this comparison are the total evacuation time, the overall equity index, as measured by the point-biserial correlation coefficient (|rpb|), and equity indices specific to hazard types such as wildfires, landslides, floods, and earthquakes. Table 6 summarizes the results of these strategies."}, {"title": "5.1. Overall evacuation time analysis", "content": "The total evacuation time provides a direct measure of how efficiently each strategy manages the evacuation process. As shown in Table 6, the Equity-RL strategy achieves a total evacuation time of 1,395,000 minutes. This represents a balance between minimizing the evacuation duration and maintaining an equitable service distribution across affected areas. In comparison, the Efficiency-RL strategy, which prioritizes evacuation speed without explicit equity considerations, achieves a shorter total evacuation time of 1,281,000 minutes. This result demonstrates the trade-off between efficiency and equity, where faster evacuation times can come at the expense of fair service distribution.\nStochastic Strategy 1 (\u03c0\u2081) recorded the longest evacuation time at 6,437,000 minutes, followed by Stochastic Strategy 2 (\u03c0\u2082) at 2,908,000 minutes. These results indicate that random bus assignments lead to significant inefficiencies, likely due to suboptimal routing and scheduling decisions. The rule-based strategies performed better than the stochastic ones, with Rule-based Strategy 1 (\u03c0\u2081) achieving an evacuation time of 1,496,000 minutes and Rule-based Strategy 2 (\u03c0\u2082) at 1,712,000 minutes. While these strategies offer improvements over random assignments, they are still outperformed by both RL-based approaches in terms of total evacuation time."}, {"title": "5.2. Equity index analysis", "content": "The overall equity index, as measured by the point-biserial correlation coefficient (|rpb|), provides an important metric for assessing how equitably the evacuation resources are distributed across different communities, especially those identified as vulnerable. The Equity-RL strategy achieved the lowest overall equity index of 0.102, indicating the most equitable resource distribution among all strategies. This performance suggests that the RL agent was able to effectively prioritize EPC during the evacuation process.\nIn contrast, the Efficiency-RL strategy recorded an overall equity index of 0.189, which reflects the trade-off made for faster evacuation times. This higher inequity index suggests that the Efficiency-RL strategy tended to favor more easily accessible areas over vulnerable communities, resulting in less equitable resource distribution.\nThe stochastic strategies also demonstrated higher inequity indices, with Strategy 1 (\u03c0\u2081) achieving an equity index of 0.156 and Strategy 2 (\u03c0\u2082) recording 0.172. These values indicate that random bus assignments fail to account for the specific needs of vulnerable populations, leading to less equitable outcomes. The rule-based strategies showed even higher inequity, with Rule-based Strategy 1 (\u03c0\u2081) and Rule-based Strategy 2 (\u03c0\u2082) recording indices of 0.191 and 0.169, respectively."}, {"title": "5.3. Hazard-specific equity analysis", "content": "To further understand the performance of the proposed strategies across different disaster scenarios, hazard-specific equity indices were calculated for wildfires, landslides, floods, and earthquakes. The Equity-RL strategy consistently achieved the lowest inequity indices across all hazard types, highlighting its robustness in prioritizing vulnerable communities regardless of the specific nature of the disaster.\nFor instance, in wildfire scenarios, the Equity-RL strategy recorded an equity index of 0.089, which is significantly lower than the 0.170 observed with the Efficiency-RL strategy. This indicates that, under wildfire conditions, the Equity-RL strategy was more effective in ensuring that equity priority communities received adequate attention. Similarly, in landslide scenarios, the Equity-RL strategy achieved an equity index of 0.063, compared to 0.178 for the Efficiency-RL strategy, demonstrating its superior performance in addressing equity under landslide-related evacuations."}, {"title": "5.4. Evacuation time distribution analysis", "content": "To provide further insight into the distribution of evacuation times, we generated histograms of individual evacuee travel times and waiting times for both the Equity-RL and Efficiency-RL strategies. These distributions are illustrated in Figures 8 and 9, which allow for a direct comparison of the variability in travel and waiting times across the two strategies.\n\u2022 Equity-RL Strategy: The travel time distribution for the Equity-RL strategy, represented by the blue curve, is more concentrated around this peak. This indicates that a larger proportion of evacuees experienced travel times within this specific range, suggesting a more uniform distribution of travel times across evacuees.\n\u2022 Efficiency-RL Strategy: The Efficiency-RL strategy, shown by the orange curve, exhibits a wider spread in travel times, particularly with a longer tail extending beyond 40 minutes and up to approximately 80 minutes. This suggests that while many evacuees had travel times comparable to the Equity-RL strategy, a small percentage of evacuees experienced significantly longer travel times.\nThe key difference between the two strategies lies in the consistency of travel times. The Equity-RL strategy maintains a tighter distribution, minimizing the number of evacuees experiencing travel times beyond 50 minutes. In contrast, the Efficiency-RL strategy allows for greater variability, with some evacuees experiencing travel times as high as 80 minutes.\nFigure 9 presents the distribution of waiting times for both strategies.\n\u2022 Equity-RL Strategy: The Equity-RL strategy again demonstrates a more concentrated distribution of waiting times, with the peak occurring between 20 and 30 minutes. The distribution of waiting times shows that a significant portion of evacuees waited between 10 and 40 minutes, with a small number of evacuees experiencing longer waiting times beyond 50 minutes.\n\u2022 Efficiency-RL Strategy: The Efficiency-RL strategy, on the other hand, displays a broader distribution with a lower peak. A notable difference is the longer tail, with many evacuees experiencing waiting times exceeding 50 minutes and some reaching up to 100 minutes. This suggests that while the strategy minimizes total evacuation time, certain evacuees, particularly those in equity priority communities, are subjected to longer waiting times.\nThe Equity-RL strategy leads to fewer extreme cases of long waiting times, contributing to a more equitable distribution of service. The Efficiency-RL strategy has a larger variance in waiting times, with more evacuees experiencing extended delays in bus arrival and service.\nThe histograms demonstrate the trade-offs between the two strategies in terms of equity and efficiency:\n\u2022 The Equity-RL strategy exhibits more uniform and concentrated distributions for both travel and waiting times. This strategy ensures that evacuees have a more consistent evacuation experience, reducing the number of evacuees who experience particularly long travel or waiting times. As a result, the Equity-RL strategy better addresses the equity concerns, ensuring that vulnerable communities are not disproportionately affected by long waits or extended travel times.\n\u2022 The Efficiency-RL strategy, while minimizing the total evacuation time, introduces greater variability in both waiting and travel times. This variability can result in some evacuees, particularly from equity priority areas, experiencing significantly longer evacuation times. The long tail observed in both distributions suggests that certain evacuees endure considerably longer delays.\nOverall, the Equity-RL strategy prioritizes equitable service delivery across all evacuees, leading to a more balanced evacuation process. In contrast, the Efficiency-RL strategy, while faster overall, results in less consistent outcomes, potentially exacerbating inequalities in service access during emergencies."}, {"title": "6. Conclusion", "content": "This study introduced and evaluated an Equity-RL framework for bus evacuation planning. The results demonstrate that this framework offers a viable solution for balancing efficiency and equity in bus evacuation planning, achieving reasonable evacuation times while maintaining a low inequity index across various disaster scenarios. While efficiency-focused and traditional frameworks achieve shorter evacuation times, they do so at the cost of equitable service distribution, particularly for vulnerable communities. The comparison with the traditional Efficiency-RL framework further demonstrates the superior performance of the proposed Equity-RL framework which contributes meaningful insights into the optimization of evacuation processes. This study underscores the importance of integrating advanced computational techniques with social equity principles to develop more effective and fair emergency management practices. Future research should focus on extending these models to larger and more complex scenarios, incorporating dynamic real-time data, and exploring the integration of multiple modes of transportation to optimize evacuation strategies for diverse urban environments further."}, {"title": "Declaration of generative AI and AI-assisted technologies in the writing process", "content": "During the preparation of this work ChatGPT was used to refine the language and clarity of text written by non-native speakers. After using this tool/service, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication."}]}