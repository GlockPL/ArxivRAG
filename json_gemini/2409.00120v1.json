{"title": "ConCSE: Unified Contrastive Learning and Augmentation for Code-Switched Embeddings", "authors": ["Jangyeong Jeon", "Sangyeon Cho", "Minuk Ma", "Junyeong Kim"], "abstract": "This paper examines the Code-Switching (CS) phenomenon where two languages intertwine within a single utterance. There exists a noticeable need for research on the CS between English and Korean. We highlight that the current Equivalence Constraint (EC) theory for CS in other languages may only partially capture English-Korean CS complex-ities due to the intrinsic grammatical differences between the languages. We introduce a novel Koglish dataset tailored for English-Korean CS scenarios to mitigate such challenges. First, we constructed the Koglish-GLUE dataset to demonstrate the importance and need for CS datasets in various tasks. We found the differential outcomes of various foundation multilingual language models when trained on a monolingual versus a CS dataset. Motivated by this, we hypothesized that SimCSE, which has shown strengths in monolingual sentence embedding, would have limitations in CS scenarios. We construct a novel Koglish-NLI (Natural Language Inference) dataset using a CS augmentation-based approach to verify this. From this CS-augmented dataset Koglish-NLI, we propose a unified contrastive learning and augmentation method for code-switched embeddings, ConCSE, highlighting the semantics of CS sentences. Experimental results validate the proposed ConCSE with an average performance enhancement of 1.77% on the Koglish-STS (Semantic Textual Similarity) tasks.", "sections": [{"title": "1 Introduction", "content": "Code-switching (CS) refers to the phenomenon of two languages intermixed within a single sentence [8,30]. Such occurrences are frequently observed in mul-ticultural countries, social media, and online platforms [9,30,6]. According to recent findings, despite the growing interest in CS, there remains a dearth of related studies [33]. Especially in countries where English is not the dominant language, the phenomenon of CS between English and the native language is par-ticularly prominent [30,9,28,26,7,16]. For example, the English sentence \u201cThe movie was very dull\" can be represented as \u201c\uc601\ud654 was very dull.\" for English-Korean and \"la pel\u00edcula was very dull.\" for English-Spanish.\nPast research introduced the Equivalence Constraint (EC) theory as a condi-tion for the occurrence of CS [30], prompting attempts to construct CS datasets based on The EC theory [31,34]. The EC theory posits that switches between languages in a code-switched discourse tend to happen at points where the gram-matical structures of the involved languages match. According to the EC theory, such alignment in grammatical structures demonstrates that code-switching ad-heres to systematic linguistic constraints. This foundational concept has been central in many CS studies, particularly language pairs like English-Spanish and English-Chinese [31,30,39,20,38]. However, studies on CS between English and Korean show that this assumption is not always met [16]. For English-Korean CS, there is a potential limitation that EC Theory does not satisfy due to the grammatical difference between the two languages. For instance, the grammatical dif-ferences between English and Korean primarily manifest in word order and case marking. English predominantly follows an SVO (Subject-Verb-Object) word or-der, and this sequence largely determines the meaning of a sentence. In contrast, Korean offers greater flexibility in the positioning of subjects and objects, thanks in large part to its distinctive case markers like \"\uc774[i]/\uac00[ga]\" (nominative), \u201c\uc744 [eul]/\ub97c[leul]\" (accusative), and \u201c\uc5d0\uac8c[ege]\u201d (dative). Crucially, altering the word order in English can significantly change the meaning of a sentence, whereas, in Korean, where the language's case markers are well developed, position shifts within sentence components are accessible [23,16].\nThis paper introduces a novel Koglish dataset and proposes a new approach to constructing CS datasets, considering the inherent complexity of CS. The Koglish dataset includes Koglish-GLUE, Koglish-NLI, and Koglish-STS datasets. In particular, we propose to apply constituency parsing [21] to construct the Koglish dataset to obtain parse trees and transform English sentences into CS sentences following the approach proposed in Sect. 3.2. To construct the Koglish dataset, We utilize GLUE benchmark [35], Semantic Textual Similarity (STS) [4,5,2,1,3,25], The Stanford Natural Language Inference Corpus (SNLI) [10], and The Multi-Genre Natural Language Inference Corpus (MNLI) [37]. To better understand the need for code-switching (CS) datasets, we posited the following hypothesis: There will be a noticeable difference in performance between training with a monolingual dataset and then testing on a CS dataset (EN2CS) versus conducting both training and testing with a CS dataset (CS2CS). This signifi-cant disparity underscores the importance of using our CS dataset, Koglish, in"}, {"title": "CS scenarios. To our knowledge, this is the first presentation of Koglish datasets suitable for English-Korean and Korean-English scenarios.", "content": "Determining semantic relationships between sentences is a critical challenge in natural language processing. Recently, contrastive learning drew significant attention in natural language processing [17,40,13], where the model learns to distinguish between pairs of similar and dissimilar samples. For example, Sim-CSE [17] proposed to convert the sentence pairs of (premise, hypothesis) in the Natural Language Inference (NLI) dataset [10,37] into the triplets of (premise, entailment, contradiction) to provide extra signals for contrastive learning. How-ever, the study of contrastive learning under code-switched sentences has been largely yet to be underexplored. To address this issue, we propose a unified contrastive learning and data augmentation method dubbed ConCSE to model the code-switched sentences explicitly. For each sentence triplet of (premise, en-tailment, contradiction), we generate a triplet of code-switched sentences (CS-premise, CS-entailment, CS-contradiction) via CS-augmentation in Sect. 3.2 us-ing a constituency parser. Then it considers the relationships between the six sentences to define three novel loss functions: (1) Cross Contrastive Loss (LCgr), (2) Cross Triplet Loss (LTF), and (3) Align Negative Loss (Lim), providing richer supervision compared to plain SimCSE. For example, the sentence pairs of (premise, CS-premise) are considered positive, while those of (CS-premise, contradiction) are considered negative. As a validation, we compared the per-formance of four baseline multilingual models across seven NLP tasks included in Koglish-STS. The baseline multilingual models struggle to perform on the code-switched scenarios, suggesting the intricacy and effectiveness of the Koglish dataset. The experiments on the ConCSE method on the Koglish-STS dataset show consistent performance improvements over SimCSE across seven semantic textual similarity (STS) tasks included in Koglish-STS.\nOur contributions can be summarized as follows:\nWe introduce the first dataset referred to as Koglish which is suitable for English-Korean and Korean-English CS scenarios including Koglish-GLUE4, Koglish-STS56, and Koglish-NLI7.\nWe demonstrate the necessity of the Koglish dataset through various exper-iments.\nWe propose an effective sentence representation learning method that con-siders the CS sentences through a specialized CS-focused augmentation tech-nique."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Theoretical Foundations of Code-Switching", "content": "In previous research, the conditions for the occurrence of Code-Switching (CS) and Code-Mixing (CM) were proposed as the Equivalence Constraint (EC) the-ory, Matrix Language Framework (MLF), and Functional Head Constraint. No-tably, when the EC Theory criteria are met, studies have constructed CS and CM datasets using a Constituency parser [34,31]. This approach has found ap-plication in English-Chinese Code-Switching studies as well [39,31]. However, in-vestigations into English-Korean CS have demonstrated that most instances do not conform to the EC theory, indicating its unsuitability for English-Korean CS scenarios [29,28,16]. The research highlights that in English-Korean and Korean-English code-switching, nouns or noun phrases often serve as the Embedded Lan-guage (EL), with their usage being notably prevalent, accounting for 74.6% and 61% respectively [29,16]. These prior empirical results showed the importance of selecting nouns or noun phrases as EL in constructing an English-Korean CS dataset. Pursuing this approach, our study uses a pre-trained Constituency parser [21] to identify and extract nouns or noun phrases."}, {"title": "2.2 Representation Learning", "content": "Deep Metric Learning Deep Metric Learning was formulated to decipher the dynamics of embedding spaces [12,18,36]. Among its diverse strategies, triplet loss stands out [19]. It emphasizes the interrelationships and distances of sam-ples within the embedding space, aiming to cluster similar samples and distance dissimilar ones closely. A pivotal element in this approach is the 'margin,' a hy-perparameter designed to ensure a defined distance between the anchor-positive and anchor-negative pairs [32]. This paper utilizes triplet loss as an auxiliary loss to bolster the model's stability.\nContrastive Learning In fields like natural language processing [17,13,40] and computer vision [11,22], the core aim is to enhance representations by discerning between positive and negative samples. Contrastive learning, which builds upon the foundations of deep metric learning, offers refined techniques for achiev-ing superior representations. A notable advancement is the introduction of data augmentation to enrich training datasets. While random cropping and image ro-tation succeed in computer vision [11,41], their adaptation to natural language processing poses challenges. Nevertheless, strategies reconstructing NLI datasets for contrastive learning have been proposed to bridge this gap [17,13]. In par-ticular, in the strategy of reconstructing NLI datasets [17,13], first, all datasets labeled as neutral are excluded, and only datasets labeled as entailment for two sentences (premise, hypothesis) are extracted. In this case, the premise and hypothesis are defined as a positive pair, and the hypothesis is defined as an entailment sentence. Second, extract hypothesis sentences where the hypothesis is labeled as a contradiction for the same sentence as the premise used in the"}, {"title": "3 Proposed Dataset: Koglish", "content": "This section elaborates on English-Korean and Korean-English code-switching sentences and our specialized Koglish dataset construction and CS augmentation strategies. A summary of the constructed dataset is provided in Table 1."}, {"title": "3.1 Code-switching Patterns and Dataset Construction", "content": "According to a study by [28], Code-Switching (CS) between English and Ko-rean does not adhere to the guidelines established by the EC Theory [30] and the Matrix Language Frame (MLF) Model [27]. This is due to the fact that the grammatical units (e.g., phrase, adjective phrase, verb phrase) converted in CS are language-specific. Consequently, when constructing CS datasets, it is imperative to use strategies tailored to each respective language [28,30,26,29,7]. Historical analyses indicate that in Korean-English CS, nouns and noun phrases"}, {"title": "constitute 74.6% of code-switched instances [29]. English-Korean exhibits a sim-ilar trend, with nouns representing 61% of code-switched [16]. As shown in Fig.1-(a), code-switching the noun phrase maintains the sentence's integrity, mirroring the structure of the original. In contrast, code-switching VBP(Verb, non-3rd person singular present) as shown in Fig.1-(b), produces a sentence that is awkwardly constructed. Japanese, sharing syntactic similarities with Ko-rean, also has a high noun switching rate at 68.8% [26]. This structural congru-ence suggests the potential for applying our CS dataset construction strategy to other languages with grammatical structures akin to Korean's [16]. In contrast, Spanish-English code-switching contains a significantly lower noun switch rate, sometimes reaching lower than 20% [30]. Given these patterns, we primarily fo-cused on switching nouns or noun phrases when constructing English-Korean and Korean-English CS datasets. Additionally, due to the distinction between Matrix Language (ML) and Embedded Language (EL) is not explicit in English-Korean code-switching [27], the dominant use of nouns and noun phrases in both English-Korean and Korean-English code-switching endorses the suitability of our proposed dataset strategy for both scenarios.", "content": "3.2 Constructing Koglish Dataset\nThis section details constructing and augmenting the proposed CS dataset, Koglish. The overall process is shown in Fig. 2."}, {"title": "1. We constructed a parse tree using a top-down constituency parsing ap-proach [21]. During this process, we selectively extracted the NP nodes, ensuring the inclusion of nouns and noun phrases (see Fig. 2-(1)). In some data, entire sentences were constructed solely from the NP structure. When such sentences underwent the translation process, they resulted in monolin-gual sentences, negating the goal of CS. Therefore, we excluded these par-ticular entries. Additionally, if the NPo node contained only pronouns (e.g., It, That, This), it led to mistranslation issues. To address this, we extracted the NP node from the subsequent NP\u2081 node and applied the top-down ap-proach to the leaf nodes. If the data did not align with our criteria when it reached the leaf node, we considered it inappropriate for the CS dataset and subsequently excluded it. For example, GLUE's COLA task data excluded 29.1% of the entire data.", "content": "2. Generate CS sentences from the Switched Sentence Tree of Fig. 2-(1) as shown in Fig. 2-(2). The first is the GLUE [35] and STS dataset [4,5,2,1,3,25]"}, {"title": ", and the second is the NLI [10,37] dataset. As an example of the first, GLUE and STS take monolingual sentences as input and generate a CS sentence if it satisfies the abovementioned conditions (in step 1). The second example is the NLI dataset, which receives triplets of monolingual sentences (e.g., premise, entailment, and contradiction) as input. If the above conditions (in step 1) are satisfied for the triplet of monolingual sentences, it generates CS-premise, CS-entailment, and CS-contradiction. In the following Sect. 4, the three sentences (premise, entailment, and contradiction) of the Koglish-NLI dataset and CS-Augmented sentences (CS-premise, CS-entailment, and CS-contradiction) are integrated, and used for learning ConCSE, so in this paper, we assume that only NLI is CS-Augmented sentences.", "content": "3. To ensure reliability and accuracy, we performed critical manual annota-tions on the generated Koglish datasets. This process involved bilingual ex-perts proficient in both Korean and English. We employed four annotators, each tasked with evaluating the contextual accuracy of the Code-Switching sentences in the dataset. Following their assessments, the four annotators produced each dataset through a meticulous cross-validation process, rig-orously examining each other's evaluations (see Fig. 2-(3)). Finally, we split each dataset. The Koglish-GLUE was divided into train, development, and test sets in the ratios of 0.64, 0.16, and 0.20, respectively, to formulate the Koglish-GLUE dataset. Since the Koglish-STS dataset is only used to eval-uate ConCSE in Sect. 5.2, we constructed the Koglish-STS dataset by split-ting the development and test sets equally (0.5 ratios each). We constructed Koglish-NLI without any segmentation since the Koglish-NLI dataset is only used for training."}, {"title": "4 Proposed Method: ConCSE", "content": "This paper aims to train universal sentence embeddings in Code-Switching (CS) contexts. As detailed in step 2 of Sect. 3.2, we use the monolingual datasets D_{en} = \\{x_i, x_i^+,x_i^-\\}_1^N and the augmented CS datasets D_{cs} = \\{\\hat{x_i},\\hat{x_i}^+, \\hat{x_i}^-\\}_1^N to fine-tune a pre-trained multilingual sentence encoder M_{\\phi}, such as mBERT [15] or XLM-R [14], to adapt to the CS scenario.\nThe notation for the comprehensive loss function used is:\nL_{total} = L_{Con}^{CS} + \\lambda L_{Tri}^{CS} + L_{Sim}^{neg} (1)\nwhere \\lambda signifies the weight factor assigned to the triplet loss. Detailed explana-tions of L_{Con}^{CS}, L_{Tri}^{CS}, and L_{Sim}^{neg} can be found in Sect. 4.1, 4.2, and 4.3, respectively. An overview of ConCSE is shown in Fig. 3."}, {"title": "4.1 Cross Contrastive Loss", "content": "We train M_{\\phi} with Cross Contrastive Loss (L_{Con}^{CS}) on monolingual and CS sen-tences. The hidden state of \u201c[CLS]\" for D_{en} within M_{\\phi} is defined as:\nH = \\{h_i,h_i^+,h_i^-\\}_1^N (2)"}, {"title": "4.2 Cross Triplet Loss", "content": "Following the proposed Cross Contrastive Loss (L_{Con}^{CS}), the triplet loss [32] is introduced to adjust the distance between the anchor and positive and the dis-tance between the anchor and negative by a margin (\\alpha). Triplet loss can be extended to six combinations, as in Equation 4, to allow cross-training on D_{en} and D_{cs}. An example for L_{Tri}^{H^3} is defined as:\nL_{Tri}^{H^3} = \\sum_{i=1}^N max(0, ||h_i - h_i^+||_2 - ||h_i - \\hat{h_i^-}||_2 + \\alpha) (7)\nwhere N is the batch size. To this end, L_{Tri}^{CS}, derived from Equation 4 and 7, is defined as:\nL_{Tri}^{CS} = \\sum_{k=1}^6 L_{H^k}^{Tri} (8)"}, {"title": "4.3 Align Negative Loss", "content": "The negative samples from D_{en} and D_{cs} should share the same meaning, imply-ing that they should be in a positive relationship with each other. We define the loss function L_{Sim}^{neg} to encode this relationship into the M_{\\phi}:\nL_{Sim}^{neg} = \\sum_{i=1}^N CE(sim(h_i,\\hat{h_i^-})) (9)\nwhere CE() denotes cross-entropy loss, sim(,) is the cosine similarity function, and N is the batch size."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Experiments on Koglish: The Role of Koglish in Code-Switching Scenario", "content": "Setup In this experiment, we utilize our Koglish-GLUE dataset. Considering the MRPC task as an example, which determines if a pair of sentences in the Koglish-GLUE dataset are semantically equivalent: this task comprises the orig-inal English sentences, namely sentence0 and sentence1 from GLUE, as well as the Code-Switched (CS) versions, CS-sentence0 and CS-sentence1. For exam-ple, in the EN2CS scenario, we perform training and evaluation using only the monolingual English dataset sentence0 and sentence1. In the EN2CS scenario, sentence0 and sentence1 serve as the training data, while CS-sentence0 and CS-sentence1 are utilized for evaluation. Detailed information regarding the data used in the experiments is provided in Table 1. The evaluation metrics for each experiment align with those adopted in BERT [15]. Specifically, the MRPC uses"}, {"title": "5.2 Experiments on ConCSE", "content": "Setup In this experiment, we utilize the Koglish-NLI dataset for training and the Koglish-STS dataset for evaluation. The Koglish-NLI dataset contains triplets of monolingual English sentences (hypothesis, entailment, and contradiction) alongside triplets of code-switched (CS) augmented sentences (CS-hypothesis, CS-entailment, and CS-contradiction). The Koglish-STS dataset consists of pairs of original sentences (sentence0 and sentence1) and their CS counterparts (CS-sentence0 and CS-sentence1). During the training phase, we leverage SimCSE [17] to train the sentence encoder M_{\\phi} using CS-augmented sentence triplets. More-over, ConCSE trains M_{\\phi} on both triplets of original English sentences and CS-augmented sentences, promoting learning in a CS scenario. We evaluated both SimCSE and ConCSE using the CS sentence pairs from Koglish-STS. We adopt Spearman's correlation as the primary metric for this assessment.\nTraining Details In our experiments, we initialize our sentence encoder M_{\\phi} using pre-trained mBERT [15] or XLM-R [14], and we use \u201c[CLS]\" as M_{\\phi} final representation. We adopt SimCSE [17] as our baseline model during the imple-mentation phase. Furthermore, as ConCSE had to handle a larger volume of sentences compared to SimCSE [17], we only adjusted the batch size. The rest of the experimental settings were maintained identically to SimCSE. To ensure the"}, {"title": "5.3 Ablation studies", "content": "In this section, we conduct a comprehensive set of ablation studies to substantiate our ConCSE architecture. Particularly, we evaluated the effects of the combina-tion of the loss function and the effects of temperature, triplet loss, and margin on training by testing the ConCSE-mBERTbase on the Koglish-STS-B task.\nAblation Studies of Loss Functions We conducted an ablation study for three types of loss functions: (1) Cross Contrastive Loss (L_{Con}^{CS}), (2) Cross Triplet Loss (L_{Tri}^{CS}), and (3) Align Negative Loss (L_{Sim}^{neg}). Since L_{Sim}^{neg} cannot be used alone, we analyzed its impact by adding or removing it in different scenarios. Without using all three loss functions, contrastive loss alone results in SimCSE. The performance is recorded in Table 4. Our findings reveal that only L_{Con}^{CS} results in a significant performance gain of 2.2% over the baseline SimCSE. This demonstrates the necessity of L_{Con}^{CS} in code-switched scenarios. Also, when com-paring v1 and v2, L_{Con}^{CS} alone showed a performance decrease (7.9%) compared"}, {"title": "to v1. However, when comparing v1 and v5, L_{Con}^{CS} and L_{Tri}^{CS} improve perfor-mance (0.2%). Similarly, when comparing v1 to v3, L_{Con}^{CS} and L_{Sim}^{neg} together improve performance (0.2%). The conclusive evidence from our experiments in-dicates that the most effective sentence embeddings are produced by integrating all three losses, as implemented in our ConCSE(v6), which outperforms the individual loss components.", "content": "Temperature Scaling Temperature is known to play a crucial role in learn-ing [11,17]. To verify this, we conducted a separate ablation study. Based on our experimental results, ConCSE exhibited optimal performance when the temper-ature \\tau = 0.05. This value aligns with the temperature used in the learning of SimCSE. Detailed results are in Table 5.\nTriplet Loss Scaling During the training of ConCSE, we observed the best performance when using the weight factor of triplet loss \\lambda = 1.2 . The results are shown in Table 6.\nMargin Scaling The performance of ConCSE was optimal when using a margin of \\alpha = 1 for the triplet loss, as shown in Table 7."}, {"title": "6 Conclusion", "content": "In this work, we first introduced the novel Koglish dataset, focusing on code-switching (CS) between English-Korean and Korean-English. This Koglish dataset marks an initial pioneering attempt, and exhaustive evaluations have highlighted the critical need for such a resource. Second, we propose a method to learn uni-versal code-switched sentence embeddings using this newly constructed Koglish dataset. Surprisingly, Through extensive testing, ConCSE surpassed other lead-ing sentence embedding techniques in Koglish-STS tasks. Nevertheless, our study has certain constraints: Although less frequent, grammatical elements other than nouns or noun phrases can also be CS in English-Korean CS situations. In our future work, we aim to develop a more comprehensive CS dataset encompassing all grammatical elements. We are optimistic that our contributions will spur fur-ther research and progress in the understanding and application of low-resource CS data."}]}