{"title": "Contrastive Representation Learning for Predicting Solar Flares from Extremely Imbalanced Multivariate Time Series Data", "authors": ["Onur Vural", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "abstract": "Major solar flares are abrupt surges in the Sun's magnetic flux, presenting significant risks to technological infrastructure. In view of this, effectively predicting major flares from solar active region magnetic field data through machine learning methods becomes highly important in space weather research. Magnetic field data can be represented in multivariate time series modality where the data displays an extreme class imbalance due to the rarity of major flare events. In time series classification-based flare prediction, the use of contrastive representation learning methods has been relatively limited. In this paper, we introduce CONTREX, a novel contrastive representation learning approach for multivariate time series data, addressing challenges of temporal dependencies and extreme class imbalance. Our method involves extracting dynamic features from the multivariate time series instances, deriving two extremes from positive and negative class feature vectors that provide maximum separation capability, and training a sequence representation embedding module with the original multivariate time series data guided by our novel contrastive reconstruction loss to generate embeddings aligned with the extreme points. These embeddings capture essential time series characteristics and enhance discriminative power. Our approach shows promising solar flare prediction results on the Space Weather Analytics for Solar Flares (SWAN-SF) multivariate time series benchmark dataset against baseline methods.", "sections": [{"title": "I. INTRODUCTION", "content": "In the domain of solar activity, a solar flare appears as a sudden surge in magnetic flux, originating from the Sun's surface and extending into the solar corona and heliosphere. Classified logarithmically based on the peak soft X-ray flux in the 1-8 \u00c5 wavelength range, flares are denoted by categories A, B, C, M, and X where M and X classes suggest intense flare activity [1]. Such solar phenomena may emit gamma-ray, x-ray, and extreme ultraviolet radiation, posing radiation-induced risks to astronauts, technological infrastructure, electronic devices, navigation, and communication systems [2]. The 1859 Carrington Event demonstrates the potential magnitude of a solar superstorm's impact, with a recurrence potentially causing prolonged blackouts and massive economic losses in our technology-dependent society [3]. Consequently, the heliophysics community underscores the importance of meticulous data analysis and the exploration of diverse methods for robust predictions of major flares from solar active region magnetic fields and flare data.\nIn 2020, the Space Weather Analytics for Solar Flares (SWAN-SF) dataset [3] was introduced as a pivotal resource in solar flare research. Derived from Space-weather HMI Active Region Patch (SHARP) solar photospheric vector magnetograms, SWAN-SF includes multivariate time series (MVTS) data from May 2010 to December 2018. It features 24 flare-predictive magnetic field parameters and over 10,000 flare reports, reformulating solar flare prediction as an MVTS classification task. Recent models built upon SWAN-SF MVTS data have shown enhanced efficacy in classifying flaring activities compared to earlier single timestamp models [3]. Due to major flares occurring rarely, the extreme class imbalance is a remarkable challenge in SWAN-SF. Accordingly, an effective learning methodology is representation learning, the process of learning meaningful fixed-dimension embeddings as data representations from the raw input data domain that keep their inherent features and have better transferability in downstream tasks. However, in the time series domain and particularly in solar flare tasks, the use of contrastive representation learning methods has been relatively limited. In contrastive methods, positive samples are contrasted with negative samples such that similar examples are mapped closer in the new feature space while the distance between dissimilar examples is maximized. Contrastive learning methods have been widely adopted in various domains for their soaring performance in representation learning, including vision, language, and graph-structured data [4]. Consequently, it becomes imperative to explore the impact of using contrastive approaches with the ultimate goal of enhancing flare prediction performance.\nIn this paper, we introduce CONTREX, a novel contrastive representation learning approach designed specifically for extremely imbalanced time series characteristics of data points. Our method consists of four main parts: first, we extract features from MVTS instances that capture their essential dynamical properties. Next, we derive two contrastive representations from the feature vectors of positive and negative instances that provide maximum separation capability. Then, we train a sequence representation embedding module with the original MVTS instances to generate embeddings that encapsulate time series characteristics, guided by our custom contrastive reconstruction loss. Finally, based on these embeddings, we employ a downstream classifier for binary classification. This comprehensive method enables effective representation learning and classification in time series analysis tasks. The contributions made by this paper are listed as:\n\u2022 Introducing a novel contrastive learning framework tailored specifically for time series data that can be applied to both univariate and multivariate settings to address the challenges posed by temporal dependencies and extreme class imbalance.\n\u2022 Defining a custom contrastive reconstruction loss during training of the sequence representation embedding module to guide the embeddings towards the positive and negative extremes, enhancing the discriminative power of the learned representations.\n\u2022 Experimentally demonstrating the effectiveness of the proposed method regarding a performance metric evaluation reflecting the nature of the benchmark dataset."}, {"title": "II. RELATED WORKS", "content": "One of the earliest endeavors in solar flare prediction history was THEO, an expert system that relied on human entries, officially used by The Space Environment Center (SEC) of the National Oceanic and Atmospheric Administration (NOAA) in 1987 [5]. Subsequently, as space-based and ground-based observatories amassed a wealth of magnetic field data, flare prediction transitioned into a data science task with models emerging based on line-of-sight and vector magnetograms, delineating solar active region photospheric magnetic field parameters. Since 2010, NASA's Solar Dynamics Observatory (SDO) has been continuously mapping the full-disk vector magnetic field every 12 minutes through the Helioseismic and Magnetic Imager (HMI) instrument, leading to reliance on this continuous vector magnetogram data in current literature [6]. Nonlinear statistical models, particularly machine learning classifiers have gained prominence in solar flare prediction such as logistic regression [7], C4.5 decision tree [8], fully connected neural network [9], support vector machine [10], and relevance vector machine [11]. The introduction of temporal window-based flare prediction [3] led to the creation of the SWAN-SF dataset [12], which records magnetic field data over time. Following SWAN-SF, various MVTS classification methods emerged, including kNN training with statistical summarization [13], MVTS decision trees with clustering [14], LSTM-based sequence modeling [15], and functional network embedding [16]. These advancements signify a shift from traditional linear models to sophisticated machine learning techniques in predicting solar flare activities."}, {"title": "B. Time Series Contrastive Representation Learning", "content": "In contrastive representation learning, unlike learning a mapping to labels as in discriminative models or reconstructing input samples as in generative models, data representations are learned through comparison between data points in the input space by mapping similar data points close while increasing the distance between dissimilar data points in the embedding space [4]. Although contrastive learning methods are less explored in the time series domain than vision, language, and graph domains, there is growing interest. Accordingly, sampling positive and negative instances and time pieces from the anchor to learn inter-sample and intra-temporal relations [17], temporal and contextual contrasting by creating two views for each sample with strong and weak augmentations [18], introducing fidelity and variety criteria, and creating a meta-learner for selecting feasible data augmentations [19], improving representation quality by instance-wise and temporal contrastive loss with soft assignments [20], employing a siamese structure and convolutional encoder to learn representations without negative pairs [21] were such works that highlight the potential of contrastive learning in enhancing time series representation learning."}, {"title": "III. METHODOLOGY", "content": "To capture the dynamical properties of time series, catch22 feature extraction method [22] is selected. The 22 features obtained by catch22 method give a low dimensional summary to represent the diverse and interpretable characteristics of time series including linear and non-linear autocorrelation, successive differences, value distributions and outliers, and fluctuation scaling properties. The catch22 method has shown highly discriminative and low redundancy feature representation power for various benchmark time series datasets [23]. In our approach, we extract catch22 features for each univariate time series in MVTS instances. Accordingly, for each MVTS data instance $M^{(k)} \\in \\mathbb{R}^{\\tau \\times N}$ having N parameters as univariate time series with a length of \u03c4, we extract a fixed-dimensional multi-catch22 vector $V^{(k)} \\in \\mathbb{R}^{D}$ where D, the length of fixed-dimensional multi-catch22 vector, is equal to 22N."}, {"title": "B. Obtaining Contrastive Extremes", "content": "After obtaining K fixed-dimensional multi-catch22 vectors $V^{(k)} \\in \\mathbb{R}^{D}$, K being total number of MVTS instances, the second step in our proposed method is to obtain two extreme points as overarching representations for positive and negative classes to enhance the contrastive power, effectively drawing positive data points closer to the positive extreme and negative data points closer to the negative extreme. Positive and negative extremes $E_{P} \\in \\mathbb{R}^{D}$ and $E_{N} \\in \\mathbb{R}^{D}$ are selected as multi-catch22 vectors that yield the complete linkage, representing the data points that yield the greatest distance between clusters:\n$\\displaystyle D(P, N) = \\max_{V_{P} \\in P, V_{N} \\in N} d(V_{P}, V_{N})$\nIn (1), D(P, N) is the distance between clusters of positive and negative classes, and $d(V_{P}, V_{N})$ is the Euclidian distance between positive multi-catch22 vector $V_{P} \\in \\mathbb{R}^{D}$ and negative multi-catch22 vector $V_{N} \\in \\mathbb{R}^{D}$."}, {"title": "C. Framework", "content": "CONTREX is composed of two modules integrated in an end-to-end manner. The initial module derives fixed-dimensional embeddings from MVTS data points, while the subsequent module leverages these embeddings to execute the binary classification task. Fig. 1 provides an overview of our architecture."}, {"title": "1) Obtaining Representation Embeddings", "content": "In the sequence representation embedding module, the sequence modeling of MVTS data points is done via a long short-term memory (LSTM) layer where each MVTS instance $M^{(k)} \\in \\mathbb{R}^{\\tau \\times N}$ is regarded as total \u03c4 timestamp vectors $x_{<t>} \\in \\mathbb{R}^{N}$, which are sequentially processed by the LSTM cells. The input size corresponds to N parameters, and the final hidden state representation $h_{<\\tau>}$ outputs an internal embedding vector. Subsequently, this vector undergoes projection to a D-dimensional vector, mirroring the size of multi-catch22 vectors, facilitated by a multilayer perceptron (MLP) layer. Our network incorporates a single dropout layer to enhance robustness. After the training is complete, for the kth MVTS instance, the embedding vector $X_{label}^{(k)} \\in \\mathbb{R}^{D}$ can be extracted from the last layer."}, {"title": "2) Loss Function", "content": "Here, we propose a novel loss function, the contrastive reconstruction loss to guide the training of our sequence representation embedding module such that it will learn similar representations to the extremes in a supervised setting. Accordingly, for a positive class MVTS instance $X^{(k)} \\in \\mathbb{R}^{D}$, the mean squared error (MSE) loss is calculated against the positive extreme $E_{P} \\in \\mathbb{R}^{D}$ whereas for a negative class MVTS instance $X^{(k)} \\in \\mathbb{R}^{D}$, the MSE loss is calculated against the negative extreme $E_{N} \\in \\mathbb{R}^{D}$. This approach is designed to facilitate the contrastive learning of MVTS data points such that similar points that belong to the same class will be projected closer around their corresponding extreme points in the new representation space while data points that belong to different classes will be projected further apart. This personalized approach ensures that our embeddings are finely tuned to capture the distinctions between different class instances, facilitating accurate classification. In the end, P and N being the number of positive and negative class instances respectively, the final objective is to minimize this combined loss of:\n$\\displaystyle L_{Extreme} = \\frac{1}{P} \\sum_{k=1}^{P} \\sum_{i=1}^{D} (X_{i}^{(k)} - E_{P[i]})^{2} + \\frac{1}{N} \\sum_{k=1}^{N} \\sum_{i=1}^{D} (X_{i}^{(k)} - E_{N[i]})^{2}$"}, {"title": "3) Binary Classification of Representation Embeddings", "content": "In the second component of our framework, we use extracted representation embedding vectors $X_{label}^{(k)} \\in \\mathbb{R}^{D}$ as input instances to train a downstream classifier to provide a final binary class prediction."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "We present our experimental findings here. The source code for our model and experiments is on our GitHub repository.\nSWAN-SF was introduced as an MVTS dataset to facilitate unbiased flare forecasting, and drive advancements in solar flare prediction [3], [24], [25]. The data points in SWAN-SF are categorized into five distinct classes representing varying flare intensities. In binary solar flare prediction, we consider positive (P) examples as major flares responsible for health risks and infrastructural damages (i.e., M and X classes) and negative (N) examples as minor flares (i.e., B and C classes) and flare quiet events (i.e., FQ class) [13]. In SWAN-SF, a significant class imbalance exists due to the rarity of major flare events, with N examples greatly outnumbering P examples. This imbalance often results in a bias towards the majority class, yielding high true negative and low true positive rates, complicating the objective of accurately detecting solar flares [1]. Each data instance in the SWAN-SF dataset is an MVTS slice $M^{(k)} \\in \\mathbb{R}^{\\tau \\times N}$ representing a collection of univariate time series of 1-hour length \u03c4, each having 24 magnetic field parameters [10] as N, extracted using a sliding window approach. Each time slice is labeled according to the most intense flare that occurs within the prediction window. The data is divided into five partitions (i.e., p1, p2, ..., p5), each covering a different observation period [3]. The distribution of classes is illustrated in detail in Fig. 2, underscoring the challenge posed to the primary objective of accurately detecting solar flares in flare-forecasting research."}, {"title": "B. Performance Evaluation Metrics", "content": "Due to the substantial P and N class imbalance in SWAN-SF, accuracy alone, which reports the number of correct predictions without going into class specifics, is inadequate. Therefore, we use several additional metrics that are commonly used in previous works of solar flare research for assessing binary solar flare classification: F1 score, receiver operating characteristic area under the curve (ROC AUC) that measures the classifier's ability to distinguish between classes, Heidke Skill Score 2 (HSS2) that evaluates improvement over random predictions, Gilbert Skill Score (GS) that assesses the likelihood of obtaining true positives by chance, and True Skill Statistic (TSS) [6], [10], [13], [26]. Among these, TSS is particularly robust against class imbalance, expressing the difference between true positive and false positive rates ranging from -1 (all incorrect predictions) to 1 (all correct predictions), with 0 indicating random predictions. TSS is recommended as the primary measure for evaluating solar flare prediction models [10]. The evaluation of classification results, involving true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN), can be expressed as:\n$\\displaystyle \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$"}, {"title": "D. Study of Components in CONTREX", "content": "We conducted several experiments to evaluate the effects of different components within the CONTREX framework. First, LSTM-based sequence modeling of MVTS data points was compared with other deep learning sequence models, namely recurrent neural network (RNN) and gated recurrent unit (GRU). This experiment also varied the hidden space dimensions to assess their impact on model underfitting or overfitting. Fig. 3 shows that the preferred model LSTM exhibits the best average performance in three consecutive partitions as train-test pairs (i.e., p1-p2, p2-p3, and p3-p4) under varying dimensionality in hidden space. Following that, the downstream classifier is selected as logistic regression showing the best average performance in the same three consecutive partitions as train-test pairs as reflected in Table I after experimenting with support vector machine (SVM), k-neighbors classifier (KNC), decision tree (DT), multilayer perception (MLP), and fully-connected network (FC)."}, {"title": "E. Representation Embedding Analysis", "content": "To assess the extent of the contrastive abilities of our embeddings in separating classes, t-SNE visualization [28] is performed to map the extracted embeddings $X_{label}^{(k)} \\in \\mathbb{R}^{D}$ into two-dimensional space. Fig. 4 gives insight into benchmark dataset's data distribution and suggests that our contrastive learning model effectively separates P and N classes."}, {"title": "F. Baselines", "content": "To evaluate the performance of our proposed framework, we use the following baselines from current solar flare research where each method obtains a different data representation out of MVTS instances. When a downstream classifier is needed, we use logistic regression to be consistent with our experiments."}, {"title": "G. Binary Classification Performance", "content": "Following our experiments with three consecutive partitions as train-test pairs (i.e., p1-p2, p2-p3, and p3-p4), the averaged performance results are displayed in Table II and Fig. 5, demonstrating that the contrastive ability of CONTREX manages to show promising performance. CONTREX emerges as the top performer with 0.7306 accuracy, 0.7098 TSS and 0.8549 ROC AUC, outperforming LTV by 5.1%, 7.2% and 3.6%, respectively. However, with 0.1189 HSS2 and 0.1579 F1 score, CONTREX has the second position, lagging behind the leading performer, LSTM, by 4.6% and 4.3%, respectively. With 0.02303 GS, CONTREX shares the top position with LTV, with only a marginal 0.011% difference between the two. Overall, our experimental findings underscore CONTREX's competitiveness against state-of-the-art methods."}, {"title": "V. CONCLUSION", "content": "In this paper, a novel contrastive approach for time series is introduced and evaluated by the task of binary solar flare prediction of the SWAN-SF data instances. Our methodology comprised extracting dynamic attributes from each MVTS instance, computing contrastive extreme points from feature vectors, obtaining sequence representation embeddings for MVTS data instances guided by our custom reconstruction loss that leveraged the idea of generating embeddings that encapsulate the distinctive class characteristics, and training a downstream classifier with embeddings to binary classify solar flares. In future studies, we aim to enhance the performance of our framework. For this reason, bringing other elements of triplet loss to our loss function as commonly utilized in modern contrastive learning methods, experimenting with different sampling and extreme point calculation methodologies, and applying feature reduction to the extreme points will be possible study directions. Furthermore, a study is planned to test the abilities of our framework in its current state with different MVTS benchmark datasets having binary and multi-class conditions. This experimentation will help to understand whether utilizing the contrastive extremes can yield better separation results for other time series settings."}]}