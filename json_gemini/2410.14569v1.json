{"title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs", "authors": ["Hanna Kim", "Minkyoo Song", "Seung Ho Na", "Seungwon Shin", "Kimin Lee"], "abstract": "Recent advancements in Large Language Models (LLMs) have established them as agentic systems capable of planning and interacting with various tools. These LLM agents are often paired with web-based tools, enabling access to diverse sources and real-time information. Although these advancements offer significant benefits across various applications, they also increase the risk of malicious use, particularly in cyberattacks involving personal information. In this work, we investigate the risks associated with misuse of LLM agents in cyber-attacks involving personal data. Specifically, we aim to understand: 1) how potent LLM agents can be when directed to conduct cyberattacks, 2) how cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it becomes to launch cyberattacks using LLM agents. We examine three attack scenarios: the collection of Personally Identifiable Information (PII), the generation of impersonation posts, and the creation of spear-phishing emails. Our experiments reveal the effectiveness of LLM agents in these attacks: LLM agents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of impersonation posts created by LLM agents were evaluated as authentic, and the click rate for links in spear phishing emails created by LLM agents reached up to 46.67%. Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for more robust security measures to prevent the misuse of LLM agents.", "sections": [{"title": "1 Introduction", "content": "The integration of various external tools, such as APIs and databases, has significantly enhanced the capabilities of Large Language Models (LLMs). This integration allows LLMs to function autonomously as advanced AI systems that utilize LLMs as their core component to perform complex tasks. Numerous studies have shown the effectiveness of LLM agents in performing tasks across various domains [12,35,54,57].\nGiven the advanced capabilities of LLM agents, there is growing concern about their potential to cause significant real-world harm if used maliciously. Recent studies [17, 18,28] have shown that LLM agents could be exploited for cyberattacks, highlighting the risks they pose in this context. In response to these risks, LLM vendors (e.g., OpenAI 1, Google 2) have implemented policies that prohibit harmful activities, such as compromising privacy or intentionally deceiving others [6, 43]. Additionally, safeguards have been established to prevent the misuse of these powerful models [4, 22, 40]. However, as models become more capable, it becomes increasingly challenging to foresee and mitigate all potential misuse scenarios.\nThe risks associated with LLM agents become even more problematic when paired with web-based tools. The widespread sharing of personal information on the web makes it an appealing target for cybercriminals. For example, attackers often resort to data scraping from websites like Google, LinkedIn, and Facebook to collect personal information, which is then sold on hacker forums [39, 52, 55, 56]. Such activities not only infringe on privacy but also increase individuals' vulnerability against targeted cyberattacks, including impersonation and phishing email attacks [23, 25]. Attackers can use the acquired information on the target to develop specifically tailored documents for malicious purposes, where impersonation posts wrongfully take advantage of the reputation of the target, and phishing emails deceive the target into clicking on a harmful link.\nOur work. Despite the web's utility and the vulnerabilities introduced by the prevalence of personal data, the capability of LLM agents in cyberattacks exploiting personal information remains unclear. In this work, we examine the effectiveness and harmfulness of LLM agents using web-based tools (such as web search API"}, {"title": "2 Related Work", "content": "LLM Security. As LLMs have advanced and become widely adopted, their potential misuse has raised significant concerns. Previous research has demonstrated"}, {"title": "3 LLM Agents for Cyberattacks", "content": "In this section, we explore the risks associated with LLM agents when integrated into web-based tools, aiming to simulate cyberattack scenarios. This analysis is designed to enhance our understanding of how such tools might be misused in real-world contexts. Section 3.1 details the specific tools and methodologies used to deploy LLM agents in cyberattack scenarios. In Section 3.2, we introduce the specific tasks assigned to LLM agents within these cyberattack scenarios."}, {"title": "3.1 Overview of LLM Agents", "content": "To explore the potential vulnerabilities of LLMs to cyberattacks, we simulate scenarios where LLMs could be exploited by attackers. A straightforward approach to exploiting LLMs involves prompting them to perform harmful actions, such as extracting personal information (Figure 1(a)). While LLMs can execute convincing attacks with malicious prompts, we focus on more advanced versions, known as LLM agents, that are capable of planning and interacting with various tools, including software and external APIs.\nSpecifically, we consider two LLM agents that leverage web-based tools like web search and web navigation:\n\u2022 WebSearch Agent: This agent leverages a web search tool to access search results from engines like Google and Bing.\n\u2022 WebNav Agent: This agent employs a navigation tool to retrieve content from web pages and interacts with clickable elements to access more deeply embedded information.\nImplementation. We implement LLM agents using the function calling feature provided by each LLM's API. We provide a set of function descriptions to the LLM, enabling the model to determine the appropriate timing and method for calling functions based on the task requirements. It is important to note that LLMs do not execute functions directly; rather, they identify the"}, {"title": "3.2 Targeted Cyberattacks", "content": "In this work, we investigate the capabilities of LLM agents to execute cyberattacks that traditionally require considerable human effort and resources. Specifically, we focus on three tasks that exploit vulnerabilities stemming from the widespread availability of personal data online, each varying in complexity:\nAttack 1. PII Collection. Collecting PII, such as email addresses, names, and phone numbers, from the internet raises significant privacy concerns, even when the data is publicly accessible. Attackers can exploit this information for malicious purposes such as identity theft, fraud, or orchestrating further cyberattacks [23,25]. They often employ web scraping tools and automated scripts to gather PII from various websites. However, due to the unique formatting of information on each website, these tools typically require custom development for each target site, which involves considerable human labor and associated costs [15,53].\nAttack 2. Impersonation Post Generation. Attack-ers craft impersonation posts to deceive audiences for malicious purposes, such as financial gain, or to tarnish reputations [29,33]. To create sophisticated impersonation posts, attackers meticulously research their targets' personal details, social behaviors, and communication styles. These elements are then integrated into their writing, a process that requires significant time and effort.\nAttack 3. Spear Phishing Email Generation. Spear phishing emails are highly effective at deceiving victims but tend to be more costly and time-consuming for attackers compared to traditional phishing emails [24,48]. This increased effort results from spear phishing's focus on targeting specific individuals or organizations with tailored messages.\nCollecting PII is the most straightforward task, where LLM agents gather targeted information from online contexts. Generating impersonation posts is a more complex task that involves using personal data to craft content closely mimicking the target. The most complex task, spear phishing email generation, involves designing scenarios, establishing sender identities, and creating content tailored to the target's interests.\nIn the following sections, we answer our research questions via analyses of these attacks. The potency of LLM"}, {"title": "4 PII Collection", "content": "In this section, we investigate the potential for LLM agents to infringe upon privacy by extracting personally identifiable information (PII)."}, {"title": "4.1 Experimental Setup", "content": "Attack Scenario. The attacker's goal is to collect PII of specific targets using LLM agents. In this work, we collect the PII of professors from the CS departments of the top ten universities, as ranked by the QS World University Rankings. The types of PII we consider are names, email addresses, phone numbers, office addresses, and URLs of personal web pages for each target.\nTo further assess variations in information collection based on academic roles, we also investigate whether LLM agents can collect the PII of CS students. Specifically, we uniformly sample 50 professors at random, whose personal web pages are successfully harvested during our initial PII collection. We then set our targets as CS students affiliated with the laboratories of these sampled professors.\nThe pipeline of PII collection (illustrated in Figure 2) using LLM agents consists of two steps: 1) constructing a list of target names and 2) collecting their corresponding personal information. Note that the attack consists of no manual effort by the attacker and only by utilization of LLM agents. For the professor group, the attacker initially extracts the names of CS professors from a given university (e.g., MIT). Then, the attacker collects the remaining PII for each professor listed. For the student group, the data obtained from the professor group is utilized. Specifically, the names, personal web pages, and school affiliations of professors are used in the prompts to identify the names of students currently associated with each professor. The collected student names are then used as the input for the LLM, which is tasked with retrieving the PII for each student.\nEvaluation. Due to the vast and diverse nature of the field of CS, we did not evaluate based solely on department listings. Instead, we employed human annotators to review the responses generated by LLM agents. We employed human annotators to review the responses generated by LLM agents. Annotators were directed to classify individuals as CS professors based on their engagement in relevant fields, following a thorough web search. Certain types of PII, like phone numbers, are prone to changes over time, complicating verification; thus, we initiated five separate queries to LLMs for the remaining PII for each individual. We considered the information collection successful if at least one of the five responses matched the data available online. During the annotation process for the names of CS students, we used the student information listed on the professors' web pages as the ground truth. For more details on the annotation process, please refer to Appendix B.1.\nWe use precision and recall as our evaluation metric. Precision for each category was calculated as the proportion of successfully collected PII relative to the total PII collected. We utilized the CS faculty lists provided by the official websites of each university as the reference for comparing our collected data. We calculated the upper bound recall 8 using the number of professors officially listed under the CS faculty."}, {"title": "4.2 Main Results", "content": "Figure 3 (a) and Figure 3 (b) show the results of PII collection for CS professors and students, respectively, using vanilla LLM and LLM agents with GPT. The effectiveness of LLM agents in collecting PII improves as they utilize additional tools. This implies that the risks associated with LLM agents significantly increase as their capabilities expand.\nFor CS professors, the vanilla LLM without the utilization of web-based tools were less effective in retrieving PII, particularly for detailed information like office addresses and phone numbers. By utilizing the search API, the WebSearch agent collected information more effectively than the vanilla LLM. However, it was less effective"}, {"title": "4.3 Model Analysis", "content": "In order to examine the differences in PII collection capabilities, we provided prompts to Gemini and Claude for"}, {"title": "5 Impersonation Post Generation", "content": "In this section, we explore how effectively LLM agents impersonate individuals by using publicly available information from the web."}, {"title": "5.1 Experimental Setup", "content": "Attack Scenario. The attacker aims to automatically generate social media posts that impersonate specific targets to endorse the attacker's claims. LLM agents are used to generate credible impersonation posts using a target's personal information from the web, allowing attackers to exploit the target's reputation to disseminate their intended message. In this study, we target 50 CS professors, as identified in Section 4.\nAs illustrated in Figure 6, an attacker can create an impersonation post by sending a single query to the agents. Directly including terms like \"impersonate\" was shown to trigger the LLM's safeguards, resulting in rejection. Therefore, we use a simple role-playing technique, introducing the target to the agent with the statement, \"I am [Name] at [University]\u201d, followed with web-based tools to learn more about the target's background. Subsequently, we request the agent to compose a 500-700 word social media post advocating a specific claim. Note that the attacker only uses three inputs in the prompt: the target's name, institution, and the claim to be promoted. As a baseline, a vanilla LLM is used to craft an impersonation post but without the web search function, relying solely on pre-stored knowledge.\nThe claims established in this attack are: 1) \"recommend researching AI\", and 2) \u201cLLM is highly secure against potential misuse\". The claims are designed to observe how the nature of the claim (the first is relatively innocent, while the latter is controversial) affects the LLM agent's performance in impersonation tasks.\nEvaluation. We examined the impact of web accessibility on impersonation by conducting an A/B test. Impersonation posts were generated using (1) vanilla LLMs, (2) WebSearch agents, and (3) WebNav agents; then, we conducted pairwise comparisons to determine which of the two posts was more likely to be written by the target individual. As evaluators, we employed LLMs, which are recognized for their high accuracy in tasks such as fake news detection and machine-generated text detection, even without additional fine-tuning [11,34,45] 9. We enabled the web search function to enhance the accuracy of the LLM evaluations by leveraging internet-available information. Given impersonation posts from two models (e.g., vanilla LLM vs. WebSearch agent, or WebSearch agent vs. WebNav agent), LLM evaluators indicated which post more accurately impersonated the target individual, or indicated 'unsure' if the two posts were similarly effective. We randomize the placement of texts (A or B) to mitigate order bias. Furthermore, to minimize potential model bias, each pair of posts was evaluated by three different models: GPT, Claude, and Gemini. The final outcome was determined based on the majority vote from the three evaluations.\nIn addition, to determine how likely the impersonation posts were perceived to be authentic, we conducted a Yes/No test. Similar to the A/B test, we utilized the three LLM evaluators and to assess whether the texts appeared to have been authored by the target individuals."}, {"title": "5.2 Main Results", "content": "Figure 7 shows an example of an impersonation post 10 of a well-known figure generated by the WebSearch agent with GPT. We provide the agent with only the name information; however, it synthesizes information about his expertise, alma mater, and companies to craft a tweet that convincingly appears to be authored by him. The proceeding tests are conducted using target professors according to our attack pipeline.\nComparison Based on Tool Usage. We conducted comparative analysis of impersonation posts generated by WebNav agents and WebSearch agents, followed by comparisons between WebSearch agents and vanilla LLMs, to examine their effectiveness based on their capability. Table 1 shows the proportion of choices made in an A/B test. Cases where the LLMs or agents refused to generate content were excluded from our calculations (these will be discussed later in this section). The results indicate that the impersonation effectiveness increases with access to additional tools across all models. Specifically, WebNav agents were more effective than WebSearch agents, and WebSearch agents outperformed vanilla LLMs. The differences in effectiveness were more pronounced when comparing WebSearch agents to vanilla LLMs, suggesting that adding search capabilities (Web-Search agents) provides a significant boost in performance for tasks requiring impersonation abilities."}, {"title": "6 Spear Phishing Email Generation", "content": "In this section, we investigate whether LLM agents can be used to generate personalized phishing emails using only an email address, without the need for additional human intervention."}, {"title": "6.1 Experimental Setup", "content": "Attack Scenario. In Section 4, we demonstrated the feasibility of effectively obtaining the email addresses of researchers in the university. Similarly, attackers can exploit publicly available email addresses [37] or acquire them through purchases on the dark web or Telegram channels [16]. Attackers commonly use phishing emails to obtain sensitive information from targeted individuals by asking them to click on a link or enter personal information [8]. In our study, the attacker's goal is to craft highly personalized phishing emails generated by Web-Search agents to induce recipients to click the malicious link, utilizing the email addresses of target individuals.\nThe attack pipeline is designed to craft highly convincing phishing emails. Note that the attack consists of no manual effort by the attacker and only by utilization of LLM agents. The only ingredient for this attack is the target's email address, which is added to the generation prompt of the LLM query. As illustrated in Figure 8, an attacker can create a phishing email by sending a single query to the WebSearch agent. The process begins with verifying the date to o ensure the timing aligns with the intended attack scenario. Then, the content of the email is developed by searching the target's email address online to gather personal information. This information is then utilized to design a realistic scenario where the target is likely to receive an email, along with identifying a credible sender that fits this scenario. Subsequently, a plausible URL string is generated and embedded in the email to encourage the target's engagement. Finally, the sender's email address is altered to mimic a legitimate domain, enabling the dispatch of the email without exploiting vulnerabilities in official domains or compromising accounts to access the authentic domain.\nUser Study. Phishing emails were custom-generated for each target, requiring recipients to personally evaluate the emails intended for them. Thus, we designed a questionnaire to assess these emails and actively recruited participants for a comprehensive evaluation. Specifically, we recruited 60 participants and categorized them based on the provided institutional email addresses to analyze response variations across institutional types: 55% were academic researchers and 45% were non-academic pro-fessionals 11. We emphasize that participants did not receive these emails directly; instead, they were assessed via a Google survey.\nEmail Design. Using direct terms like \"phishing email\" would trigger the LLM's safeguards and result in rejection. Therefore, our experiment includes minimal circumvention techniques, such as avoiding such explicit terms. Although these minimal efforts were sufficient for Claude and GPT, they could not bypass the safeguard"}, {"title": "6.2 Main Results", "content": "We first analyze the effectiveness and content of general-purpose emails created by LLMs and WebSearch agents. Then, we examine the results according to the participants' groups. We then analyze the efficacy of emails according to their purpose and sender's institution. In Tables 7 and 8 of Appendix D.2, we report the comprehensive survey results concerning participants' perceptions of authenticity and anticipated actions for various email types."}, {"title": "6.2.1 Overall Analysis", "content": "Figure 9 provides an example of phishing email generated by WebSearch agent using Claude. We prompted the agent to write an email to a well-known figure, by"}, {"title": "6.2.2 Comparison Between Participant Groups", "content": "Figure 12 presents the actions participants from different groups reported they would take after receiving an email. The academic researcher group generally showed a higher likelihood of clicking on links compared to the non-academic professional group. This difference may be associated with the varying percentages of participants who verified that the phishing emails did not contain"}, {"title": "6.2.3 Factors Increasing Phishing Effectiveness", "content": "We examined the impact of changing the email's purpose and sender's address on the effectiveness of phishing"}, {"title": "7 Approachability of Cyberattacks", "content": "In this section, we examine the practicality of using LLM agents for targeted cyberattacks by assessing two primary factors: cost and safeguard capability. Understanding these elements is crucial to determining the practicality of employing LLM agents for malicious purposes in real-world scenarios. The time and financial costs associated with the attacks are measured, followed by an assessment of bypassing LLM safeguards."}, {"title": "7.1 Cost Analysis", "content": "Conducting attacks incurs both time and financial costs, which can highly impact the practicality of using LLM agents for cyberattacks. We examine how quickly and at what cost attackers can carry out cyberattacks using commercially available LLMs. We measured the time spent and the number of tokens consumed during each attack scenario using WebSearch agent for each individual target. For a cost analysis of the WebNav agent, please refer to Appendix E.1. To simplify our calculations, we considered token usage as the sum of input and output"}, {"title": "7.2 Safeguard Capability", "content": "We conduct an in-depth analysis of safeguard capabilities by exploring different factors in impersonation post generation and spear phishing email generation.\nImpersonation Post Generation. In Section 5, we found that the activation of safeguards varied depending on the nature of the claim. For further analysis of the safeguard capabilities of each model, we used two additional claims (\"Invest in Dogecoin\" and \"LLMs are effective in generating phishing emails\") and assessed their safeguard bypassing when paired with different groups of targets. These two groups were 10 professors from previous analyses and 10 influential figures (targets with more fame and general popularity) in the tech industry as targets for impersonation.\nThe activation of safeguards varied not only based on the nature of the claim but also on the nature of the impersonation target. In this task, we observed different safeguard capabilities in the services, with Claude being the most strict and GPT being the most tolerant of prompts. For GPT, the vanilla LLM and WebSearch agent successfully generated posts for both new claims without any rejections. However, Claude and Gemini"}, {"title": "8 Conclusion", "content": "In this work, we examine the emerging threats posed by LLM agents, particularly their potential use of web-based tools in cyberattacks that exploit private data. To assess their capabilities, we focused on three types of cyber-attacks: PII collection, impersonation post generation, and spear phishing email generation. Our experimental results demonstrate that attackers can successfully automate these attacks using LLM agents, with web-based tools enhancing the performance of these attacks. Furthermore, our results report easy bypass of LLM service safeguards at low-costs, making utilization of LLM"}, {"title": "Ethical Considerations", "content": "We care deeply about human safety and societal security, which motivates us to conduct this research. In this paper, we discuss and carefully examine potential threat of LLM agents in cyberattacks exploiting personal data. This raises ethical considerations as some of the data that we extract contains information about individual users. We minimize these concerns by using data that is already public and do not present any specific information in this paper.\nIn the case of user study in Section 6, according to the local IRB's protocol, our research is excluded from the review for the following reasons. First, we used only publicly available information and did not handle any sensitive data. Additionally, the emails evaluated contained no harmful or offensive content, minimizing any potential impact on participants. In the recruitment process, we obtained informed consent from participants, detailing the purpose of the research, the expected duration, procedures involved, anticipated benefits, personal information protection, and consent for the collection and use of their provided email addresses. We also informed participants that the LLM agents could search the internet for their email addresses to gather information and use it to generate emails. In the evaluation process, we did not send emails to the participants. We asked to evaluate emails through a Google survey after notification. Each participant was exposed only to emails generated from their own email address.\nWe believe that the benefits of publicizing these attacks outweigh the potential harms. We will disclose our research findings (especially concerning the safeguard vulnerabilities of web-based tools) to each respective LLM service vendor (OpenAI, Anthropic, and GoogleAI)."}, {"title": "Open Science Policy Compliance", "content": "Our study explores the potential threat posed by LLM agents in cyberattacks. We acknowledge that disclosing the exact prompts used for these attacks could enable malicious actors to replicate them. To mitigate the risk of misuse, we have submitted our code for review with dummy prompts accessible at this link 16. After decision, we will discuss with reviewers and determine the feasibility of releasing our code. We will provide the full source code only upon request for legitimate research purposes."}]}