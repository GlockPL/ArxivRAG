{"title": "Modality-Projection Universal Model for Comprehensive Full-Body Medical Imaging Segmentation", "authors": ["Yixin Chen", "Lin Gao", "Yajuan Gao", "Rui Wang", "Jingge Lian", "Xiangxi Meng", "Yanhua Duan", "Leiying Chai", "Hongbin Han", "Zhaoping Cheng", "Zhaoheng Xie"], "abstract": "The integration of deep learning in medical imaging has shown great promise for enhancing diagnostic, therapeutic, and research outcomes. However, applying universal models across multiple modalities remains challenging due to the inherent variability in data characteristics. This study aims to introduce and evaluate a Modality Projection Universal Model (MPUM). MPUM employs a novel modality-projection strategy, which allows the model to dynamically adjust its parameters to optimize performance across different imaging modalities. The MPUM demonstrated superior accuracy in identifying anatomical structures, enabling precise quantification for improved clinical decision-making. It also identifies metabolic associations within the brain-body axis, advancing research on brain-body physiological correlations. Furthermore, MPUM's unique controller-based convolution layer enables visualization of saliency maps across all network layers, significantly enhancing the model's interpretability.", "sections": [{"title": "1 Introduction", "content": "Universal models, characterized by their ability to generalize across diverse tasks without fine-tuning, have emerged as a powerful framework in many fields. By leveraging shared representations[1, 2], these models offer unparalleled adaptability across a wide range of applications. In the realm of medical imaging, universal models have gained attention for their potential to generalize across various anatomical regions, imaging modalities, and clinical tasks. However, significant challenges arise due to the diverse modalities and intricate anatomical structures involved. In response to these challenges, several large-scale datasets, such as TotalSegmentator [3, 4] and Dense Anatomical Prediction (DAP) [5], various universal challenges like BodyMaps24[6] and the Universal Lesion Segmentation [7], as well as related universal models like MedSAM[2], CDUM[8], PCNet[1], TotalSegmentator[3], STUNet [9], and LUCIDA[10], propelled progress in this domain, demonstrating the immense potential.\nIn the medical imaging field, universal models possess powerful multi-task processing capabilities, enabling the quick and accurate identification of human tissues. These models assist clinicians by distinguishing different organs, thereby aiding in the diagnostic process. After completing radiological scans and nuclear medicine scan, a universal model can automatically identify all tissue structures, generating preliminary reports and reducing doctors' workload. Additionally, these models can monitor patients undergoing treatment by detecting structural or functional changes, detecting structural or functional changes. They also reduce the time and manpower required for annotating specific tissue regions of interest (ROIs), which is particularly beneficial in clinical research.\nSimilar to universal models, foundation models represent an alternative approach to developing generalizable AI systems in medical imaging. While foundation models rely on pre-training through self-supervised learning on large amounts of unannotated or weakly annotated data[11-14], universal models focus on training with annotated multi-task datasets. Both approaches aim to generate high-dimensional representations that can generalize across various tasks. However, foundation models face inherent limitations, particularly in multi-category segmentation tasks, where the binary nature of contrastive learning is less effective. Additionally, foundation models typically require task-specific fine-tuning, which compromises their practicality as truly versatile tools in medical applications. In contrast, universal models face their own set of challenges. Chief among them is the need for large-scale annotated datasets. Universal models depend heavily on high-quality, labeled data from diverse tasks [15, 16]. This requirement presents a significant bottleneck, as obtaining sufficient annotated data for medical imaging tasks can be labor-intensive. Despite these limitations, researching universal models is crucial because they offer the potential for truly versatile tools in medical applications without the need for task-specific fine-tuning[1]. Universal models can handle multiple tasks simultaneously, leading to more efficient clinical workflows.\nWith the efforts of the relevant communities, annotated medical multi-task datasets have become relatively mature. The TotalSeg dataset [3] includes segmentation annotations for 104 types of CT anatomical structures from 1,204 unique subjects. The DAP dataset [5] covers 133 types anatomical structures with 533 CT scans. The CDUM dataset [8] combines multiple single-task CT datasets to build a multi-task model capable of recognizing 25 organs and 6 types of tumors. The TotalSegMRI dataset[4] covers 59 anatomical structures from 298 MR scans. Compared to single-task segmentation tasks, multi-task training offers greater robustness and performance[1, 6, 9, 17]. The performance boost of universal models from multi-task training primarily stems from two factors: firstly, the larger scale of multi-task datasets; secondly, the synergy between tasks, such as shared segmentation boundaries of adjacent tissues.\nCurrent research has proposed seversal designs for universal medical models. CDUM[8] integrates text embeddings into segmentation models to capture anatomical relationships. STUNet [9] is a scalable and transferable U-Net model series, with sizes ranging from 14 million to 1.4 billion parameters. SAT [18] is designed to segment a wide array of medical images using text prompts. PCNet[1] utilizes prior category knowledge to guide the universal segmentation model in capturing inter-category relationships. Among the existing research on universal medical models, SAT is trained as a multi-task model on"}, {"title": "2 Results", "content": "We developed a deep-learning multi-modality universal segmentation model. In the identification tasks, the model demonstrates superior performance in anatomical structure identification, outperforming existing segmentation models in terms of Dice and surface Dice metrics. In aided diagnosis, it accurately detected and quantified intracranial hemorrhages in CT scans, significantly improving diagnostic accuracy among general practitioners in a clinical setting. For aided analysis, the model facilitated the identification of significant alterations in metabolic associations in pediatric epilepsy, revealing metabolic or functional changes in connectivity of ROIs across the whole body. Furthermore, this section includes interpretable insights through visualization of feature operators and saliency maps."}, {"title": "2.1 Identification: Anatomical Structure Identification (Case 1)", "content": "We compared with state-of-the-art universal segmentation models, including CDUM[8], PCNET[1], and STUNet [9], as well as the classic UNet[20] (with parameters adjusted to match the complexity of the other models). In addition, we evaluated different training strategies for multi-modality data: the modality-mixed strategy, the modality-specific strategy, and our proposed modality-projection strategy.\nComparison between different models. As shown in Fig. 2a, for MRI body segmentation, our projection strategy outperformed all other approaches, achieving the highest Dice score of 0.7751 and the highest surface Dice score of 0.5471. In comparison, the best-performing mixed strategy model, STUNet, achieved a Dice of 0.7560 and a surface Dice of 0.5100, while the best modality-specific strategy, STUNet, reached a Dice of 0.7627 and a surface Dice of 0.5211. Similarly, in the CT body segmentation, the projection strategy again demonstrated superior performance, with a Dice score of 0.8517 and a surface Dice score of 0.8506. The closest competitor, the modality-specific STUNet, achieved a Dice of 0.8462 and a surface Dice of 0.8395, whereas the mixed strategy STUNet model recorded a Dice of 0.8394 and a surface Dice of 0.8288.\nIn the case of CT brain segmentation, the projection strategy continued to yield the highest scores, with a Dice of 0.7419 and a surface Dice of 0.6872. The highest scores among the other strategies were from the modality-specific PCNET, which achieved a Dice of 0.6540 and a surface Dice of 0.5982, and the mixed strategy STUNet, with a Dice of 0.6318 and a surface Dice of 0.5417. Overall, these results consistently demonstrate that our projection strategy outperforms both mixed and modality-specific strategies.\nWe observed that, for multimodal data, using the mixed strategy with the same model tends to underperform compared to the specific strategy. For instance, in the MRI body segmentation, the Dice score for the mixed strategy using PCNET was 0.7396, whereas the specific strategy achieved a higher Dice of 0.7607. Similarly, in the CT body segmentation, the mixed strategy CDUM yielded a Dice score of 0.8046, while the specific strategy CDUM improved upon this with a Dice score of 0.8327. This performance decline in the mixed strategy may be attributed to the interference between different modality data distributions. It is difficult for the model parameters to reach an optimal state for all modalities simultaneously. For example, when the input is a CT image compared to when it is an MR image, there are significant differences in the optimization gradients for the same parameter, which affects the optimization process. In contrast, the projection strategy effectively mitigates this interference, proving to be a robust approach for training with multi-modality data.\nAblation experiments. As shown in Fig. 2b, the inclusion of multimodal training data significantly enhances the performance of image segmentation models. The use of CT alone yielded a Dice score of 0.8125 and a surface Dice of 0.7951 for CT-Body segmentation tasks, and a Dice score of 0.6872 and a surface Dice of 0.6071 for CT-Brain segmentation tasks. With the incorporation of PET data (CT&PET), there is a noticeable improvement in both Dice and surface Dice scores for CT-Body (Dice: 0.8459, surface Dice: 0.8381) and CT-Brain (Dice: 0.7223, surface Dice: 0.6485)."}, {"title": "2.2 Diagnosis: intracranial hemorrage (Case 2)", "content": "Intracranial hemorrhage (ICH) is a life-threatening emergency where quick diagnosis and treatment decisions are critical for patient survival [21-23]. ICH has a high mortality rate, with up to 40% of patients dying within a year of the event [24, 25], and a significant proportion of survivors suffering from lasting functional impairments[26]. Quick and accurate identification of ICH is essential for timely medical intervention, which can significantly improve patient outcomes[27-29]. The goal of Case 2 is to enable the MPUM model to provide doctors with an aided diagnosis under the objectives of speed and accuracy. The experimental setup for Case 2 consists of two parts: firstly, we tested 100 cases on the Instance2022 dataset[30, 31] and conducted statistical analyses on the volume of brain regions affected by ICH. Secondly, we validated the effectiveness of the MPUM with the validation of three senior radiologists and three junior radiologists on the in-house ICH dataset from emergency department.\nPrecise hemorrhage measurement. Our MPUM framework was fine-tuned on the Instance2022 dataset to identify hemorrhagic areas from CT head scans. As depicted in Fig. 3a, the original well-trained MPUM provides brain regions maps from CT scans, while the finetuned MPUM yields segmentation map for hemorrhages. By integrating these two predictions, we obtained precise aided diagnosis results. For instance, in the second scan of Fig. 3a, the model autonomously detected a hemorrhage involving 3868 mm\u00b3 in the left insula and 2241 mm\u00b3 in the left putamen. Precise quantification of ICH through CT scans plays a critical role in clinical decision-making and treatment planning [28]. Accurate volume measurements are essential for determining the extent of hemorrhage, monitoring its progression, and evaluating the risk of further complications like hematoma expansion, which is closely associated with worse outcomes [27]. We conducted a detailed analysis of hemorrhage volumes across various brain regions in the Extended Data Fig.1. Some regions, such as the insula, show relatively higher average volumes, which aligns with clinical observations where certain brain areas are more prone to larger hemorrhages due to their vascular structures and the prevalence of small vessel disease [27, 32]. These detailed quantification has proven crucial for developing more targeted therapies and enhancing diagnostic accuracy [33].\nEnhancing diagnostic accuracy. Additionally, we collected 28 ICH CT scans, along with diagnostic reports, from the emergency department of an external medical center. The diagnostic reports contain precise diagnostic results assisted by MRI imaging, which is regarded as ground truth. In the diagnostic accuracy test, three experienced radiologists with more than five years of experience and three general radiologists analyzed these cases, assessing the brain regions affected by ICH. Each physician should determine one or muptiple areas that ICH region involves. The regions that can be selected are: frontal lobe hemorrhage, temporal lobe hemorrhage, parietal lobe hemorrhage, occipital lobe hemorrhage, basal ganglia hemorrhage, cerebellar and brainstem hemorrhage, subarachnoid hemorrhage, subdural hemorrhage, and ventricular hemorrhage. Senior radiologists achieved accuracy rates of 100%, 92.9%, and 85.7%, while the junior radiologists initially reached 71.4%, 75.0%, and 57.1%. Most junior radiologists' errors were due to not fully identifying all hemorrhage regions and confusion between brain regions. In a specific case, while the frontal lobe hemorrhage and sub-arachnoid hemorrhage are identified, the ventricle hemorrhage may be overlooked.\nWith the aid of the MPUM aided diagnostic tool, the junior radiologists' accuracy improved to 96.4% and 92.9%, and 92.9%, demonstrating the efficacy and potential of the MPUM framework in aided diagnosis. Additionally, the MPUM can identify brain regions with greater precision, distinguishing up to 83 different areas."}, {"title": "2.3 Analysis: multi-organ metabolic associations (Case 3)", "content": "We assessed the efficacy of our universal model as an aided analysis tool in pediatric epilepsy research, specifically analyzing the impact of epilepsy on multi-organ metabolic associations. Utilizing PET and CT imaging for multi-organ metabolic analysis is vital due to its ability to assess multiple organs synchronously, enabling the exploration of systemic, inter-organ interactions [34]. This technology is essential for understanding diseases that impact multiple body systems, like cancer and other systemic diseases. By using PET imaging data, researchers have established methods to study systemic metabolic abnormalities and inter-organ communication, which are crucial in understanding complex diseases [35].\nOur universal model facilitates rapid identification of ROIs in human tissue structures, significantly reducing the manpower costs. We analyzed whole-body PET/CT data from a control group (n=33) and a group of patients without active epilepsy episodes (n=55). Utilizing our well-trained universal model, we identified 215 ROIs in the CT scans. Subsequent analysis using metabolic information from PET scans compared metabolic associations in these 215 ROIs between the control and patient groups.\nIn the analysis of 215 ROIs, which included 83 brain regions and 132 body regions, we excluded"}, {"title": "2.4 Interpretability", "content": ""}, {"title": "2.4.1 Visualization of feature operators", "content": "The modality-projection model incorporates a controller module that transforms features from the modality space into feature extraction operators. The distribution of these feature operators across different modalities, as visualized on Fig. 5, enhances the interpretability of the model.\nStage 1 represents the shallowest convolutional layer, where the feature extraction operators primarily extract low-level semantic information. Conversely, stage 4 denotes the deepest convolutional layer, tasked with processing high-level semantic information. The primary role of shallow convolutional layers is to extract low-level features from images, such as edges, textures, and simple shapes. These features are typically modality-independent, indicating a significant similarity in the low-level features extracted by different imaging"}, {"title": "2.4.2 Saliency map", "content": "To illustrate the details of our model's reasoning and enhance the interpretability of our universal medical model, we present the saliency maps. Using the advantages of the modality projection controller, we can directly display the saliency maps of each neural network layer for specific categories. In contrast to traditional gradient-based CAM methods [41] that showcase the saliency of the final decision layer, our approach provides a stronger interpretability throughout the entire inference process of the model. As shown in Fig. 5, we plot the saliency maps for different regions under various modalities. Our analysis revealed that saliency differences are related to layer depth, modality, and organ category.\nIn the shallow layers of the network, saliency is primarily focused on the image's contours and textures, while in the deeper layers, saliency increasingly concentrates on the ROI regions. As shown in Fig. 5, the saliency map of the shallowest layer (the leftmost column) highlights the edge information of the image. The subsequent layer focuses on the texture information over large areas of the image (second column from the left). In contrast, the deeper network layers progressively refine the ROI regions with increasing precision. This progression indicates that the shallow layers capture global edge and texture information, whereas the deeper layers utilize local information to fine-tune the prediction results.\nAs shown in Fig. 5a, the task of identifying the right femur from a CT image is relatively straightforward due to the high density of bone structures. In contrast, Fig. 5b-e depict the identification of various brain regions, challenging tasks given the difficulty of distinguishing soft tissue areas from CT images. In Fig. 5a, the shallow layers recognize a small amount of contour information, which is sufficient to clearly identify the location of the femur. Conversely, Fig. 5b-e need recognize the overall contour of the skull for preliminary localization, followed by a focus on the subtle variations in the soft tissue across the entire brain region.\nFig. 5a-e represent cases from the CT modality, while Fig. 5f-g are from the PET modality and Fig. 5h-i. are from the MR modality. In the CT modality, distinct contour information results in saliency maps showing continuous red lines. The saliency maps of MR modality show the similar results. In contrast, the PET modality uses high metabolic activity points as reference, with shallow network layers focusing saliency around these points for initial localization."}, {"title": "3 Discussion", "content": "Our study demonstrates that the proposed MPUM, employing a modality-projection training strategy, achieves robust performance. The experimental results highlight significant advantages of this strategy in multi-modality training. This model shows potential in medical imaging tasks[1, 3, 8], extending the applications of traditional universal segmentation models. The experimental results highlight two key advantages of this approach: precise brain segmentation and comprehensive whole-body segmentation. Specifically, the model enhances diagnostic accuracy for intracranial hemorrhage (ICH), supporting physicians in timely and accurate diagnosis. Additionally, it enables the analysis of systemic metabolic changes associated with epilepsy in pediatric patients, offering insights into the brain-body interactions in this context."}, {"title": "3.1 Universal CT-based Brain Segmentation", "content": "Previous studies have lacked efficient models for universal brain segmentation from CT scans, primarily due to the low contrast of soft tissues in CT images. Existing methods often rely on knowledge transfer, such as BraSEDA [42], which uses GANS to transfer MR brain region knowledge to CT, and UNSB [43], which transfers ventricle region knowledge via a diffusion Schr\u00f6dinger bridge. However, these methods face two key challenges: the effectiveness of knowledge transfer depends on domain shifts, and they typically target only limited brain regions. MPUM addresses these issues, offering high accuracy and the ability to segment 83 distinct brain regions.\nAdditionally, accurate segmentation of brain ventricles in CT scans is critical for emergency procedures like ventriculostomy, used to treat conditions such as hydrocephalus, brain injury, and tumors [43]. Unlike MRI, which is slower and less accessible, CT scans are faster and more readily available, making them crucial for rapid ICH diagnosis in emergency settings. Although MRI remains the gold standard for ICH detection, MPUM enables precise ventricle segmentation from CT scans, including the third ventricle, lateral ventricles, and temporal horn. This capability offers a significant clinical advantage, as quick and accurate diagnosis is essential for timely intervention and improved outcomes in emergency care [27-29].\nFurthermore, the key factors in predicting outcomes and treatment strategies for ICH are midline shift and hemorrhage volume [44]. Specifically, hemorrhage volume in the brainstem is crucial in determining whether conservative medical treatment or surgical intervention is required [44]. As shown in Case 3, MPUM is capable of assessing hemorrhage volume in brainstem, providing valuable information for clinical decision-making. While MPUM does not directly detect the midline, it can infer it by identifying left and right brain regions, such as the left and right cerebellum. The interface between these regions indirectly marks the brain's midline."}, {"title": "3.2 From Brain Imaging Analysis to Brain-body Axis Exploration", "content": "MPUM excels in whole-body segmentation, which supports our team's long-standing research on the brain-body axis the bidirectional communication between the brain and body that underpins many physiological and psychological processes [45, 46]. While epilepsy has traditionally been viewed as a brain-centric disorder, recent studies suggest it may also involve systemic metabolic effects beyond the brain [19]. This insight prompted us to explore whole-body metabolic changes in epilepsy. Leveraging MPUM's ability to rapidly identify regions of interest (ROIs), we analyzed metabolic correlations between brain-brain and brain-body pairs in epilepsy patients. Notably, significant metabolic changes were observed in brain regions such as the right anterior temporal lobe and right middle/inferior temporal gyrus, which are consistent with existing research on epileptic foci [36-38]. These findings enhance the role of imaging in epilepsy surgery planning, which typically integrates data from electroencephalography and clinical evaluations.\nIn addition to brain-brain correlations, we discovered significant metabolic changes in brain-body pairs, particularly between the pallidum and vertebrae T1-T12. Epilepsy patients are at greater risk of vitamin D deficiency, which impacts bone health, a risk exacerbated by antiepileptic medications that contribute to bone density"}, {"title": "3.3 Limitations", "content": "Despite the progress our model has made in clinical applications and medical analysis, several challenges remain. The integration of AI tools into existing medical workflows is complex. The model must not only be accurate but also user-friendly, providing outputs in a manner that seamlessly integrates with the workflows of healthcare professionals. Besides, as the model scales to handle more tasks, the computational demands increase, posing challenges for efficient operation on clinical hardware."}, {"title": "3.4 Future", "content": "In our future work, we plan to continue expanding multi-task learning. Currently, our universe model recognizes human tissues and organs. Furthermore, we intend to develop a framework for continual training to optimize the model's performance and stability consistently. Additionally, we will continue to explore the potential of the universal model as an aided analysis tool for downstream tasks. We believe that ongoing enhancements to the universe model will significantly improve its effectiveness and practicality in medical imaging research."}, {"title": "4 Methods", "content": ""}, {"title": "4.1 Study population", "content": "In Case 1 study, we employed the 18F-FDG PET/CT dataset [17] alongside two MR datasets[4, 48]. Specifically, we utilized 533 negative control subjects from the 18F-FDG PET/CT dataset [17]. Each patient scan in 18F PET/CT dataset includes both whole-body CT and PET scans. The body region labels were sourced from DAP Atlas Dataset[5], covering 142 distinct anatomical structures. DAP label dataset only contains the annotations of 533 subjects' scans. After excluding irrelevant labels such as \"background\" and \"left to annotations\", we utilized 133 effective labels from the DAP dataset. Given the DAP's focus on body regions, it overlooked detailed brain regions. To address this gap, we pre-segmented the brain regions in the PET scans using the MOOSE tool [49, 50], followed by refinement and correction by two experienced radiologists, resulting in 83 annotated brain regions. Consequently, the \"brain\" label from the DAP dataset was removed, finalizing 132 non-brain categories and 83 brain region categories. The MR dataset includes 298 MR body scans[4], covering 43 body regions, and 30 MR brain scans, annotated with the same 83 brain regions[48]. We utilized the Instance2022 [30, 31] dataset for our study (Case 2), which includes 100 non-contrast head CT volumes from clinically diagnosed patients with various types of intracranial hemorrhage (ICH), such as subdural, epidural, intraventricular, intraparenchymal, and subarachnoid hemorrhage. These CT volumes were sourced from Peking University Shougang Hospital, China, and meticulously labeled by 10 radiologists with over five years of clinical experience. We finetuned our MPUM model using the Instance2022 data, enabling it to accurately identify ICH. In addition, we obtained 28 ICH cases along with diagnostic reports from the emergency department of Peking University Third Hospital for further validation. The diagnostic reports contain precise diagnostic results assisted by MRI imaging.\nThe Case 3 of this study used 50 pediatric epilepsy patients recruited from Qianfoshan Hospital, Shandong, China, between August 3, 2021, and June 4, 2024. The cohort consisted of 27 females and 23 males, with ages ranging from 2 to 18 years (11.78\u00b15.94 years). The height of the participants ranged from 0.93 meters to 1.85 meters (1.49\u00b10.236 meters), and their weight ranged from 14.5 kg to 90 kg (46.22 \u00b1 19.92 kg). The control group comprised 22 participants, including 17 males and 5 females, with data collected from August 18, 2020, to May 9, 2024."}, {"title": "4.2 Ethical Approval Declaration", "content": "This study was approved by the Ethics Committee of The First Affiliated Hospital of Shandong First"}, {"title": "4.3 Implements", "content": ""}, {"title": "4.3.1 Data preprocessing", "content": "For this study, all imaging modalities, including CT, PET, and MR scans, underwent linear interpolation to achieve isotropic voxel sizes with a 2mm resolution. This standardization was essential to address variations in slice thickness and in-plane resolutions across different studies[11]. From these scans, we extracted patches of 128 \u00d7 128 x 128 voxels, which correspond to a physical volume of 25.6 x 25.6 x 25.6cm\u00b3.\nTo ensure consistent and stable model training, we normalized the voxel values within these patches. For the CT patches, the voxel values were normalized between 0 and 1. For MR patches, the voxel values were divided by 3000 for normalization, while PET data were first converted to Standardized Uptake Value (SUV) and then divided by 20 for normalization. These normalization steps were crucial for maintaining data consistency across the different imaging modalities.\nAdditionally, we employed data augmentation techniques, specifically RandGaussianSmooth and RandAdjust Contrast, to enhance the diversity and robustness of our training dataset. The RandGaussianSmooth method involves applying Gaussian smoothing to the images with a standard deviation randomly chosen between 0.5 and 1.5. This technique helps in reducing noise and simulating various levels of blurriness. The RandAdjustContrast method adjusts the contrast of the images by randomly scaling the intensity values between 0.5 and 1.5. These techniques partially simulate the effects of different imaging equipment and reconstruction algorithms, further enhancing the model's robustness.\nWe pre-cropped the CT, PET, and MR scans into standardized patches to improve training speed. Reading a 128 \u00d7 128 \u00d7 128 patch is significantly faster than reading the entire image and then cropping it. This pre-processing step dramatically reduces the I/O time during training, allowing for more efficient use of computational resources. We have made the patch-wise multi-modality training dataset publicly available to facilitate further research and development in this area."}, {"title": "4.3.2 Metrics", "content": "We employed two evaluation metrics: Dice and surface Dice. Dice measures the overlap between predicted and true segmentations. It is calculated as twice the area of overlap between the two segmentations divided by the total number of pixels in both segmentations, providing an overall accuracy of how well the two align. Surface Dice, on the other hand, is a more specific measure that focuses on the boundary accuracy of the segmentation. It assesses how closely the boundaries of the predicted segmentation conform to the true surface contours of the object being analyzed."}, {"title": "4.4 Modality Projection Principle", "content": "Motivation. In recent studies of universal models, such as SAT [18] and CDUM[8], the modality-mixed strategy is commonly employed during the training phase. This strategy mixes data from various modalities, enabling a single model to adapt to data from multiple modalities. As shown in Fig. 1b, the modality-mixed strategy results in a multi-modality universal model. Although the modality-mixed strategy benefits from a larger training data set, it also encounter challenges related to feature interference among modalities. The feature extraction operators (weights and biases) must deal with imaging data from all three modalities. This may lead the model to prioritize modality-independent knowledge, potentially at the expense of modality-specific knowledge. In contrast, the modality-specific strategy involves training separate models for each modality. The performance of these models serves as the baseline for this study. As illustrated in Fig. 1b, there are three types of modality data\u2014MR, PET, and CT\u2014the modality-specific strategy could produce three modality-specific models: a PET model, a CT model, and an MR model. The drawbacks of training individual models for each modality is the limited amount of training data and the lack of feature collaboration between the multi-modality data. In the realm of multi-modality medical imaging, different imaging modalities such as CT, PET,\nMR, offer unique perspectives of the same underlying biological tissues. Each modality captures specific aspects of tissue characteristics due to differences in imaging principles and physical interactions. To effectively integrate information across modalities, we propose a modality projection theory centered on the concept of high-dimensional latent features that represent the comprehensive properties of each tissue type.\nFundamental Principles. The core of our theory is the assumption that for each tissue, there exists a high-dimensional latent feature vector $T\\in \\mathbb{R}^{d_T}$, where $d_T$ denotes the dimensionality of the latent feature space. This latent feature encapsulates all intrinsic properties of the tissue that could be captured across various imaging modalities.\nEach imaging modality provides a modality-specific projection of this latent feature into its own feature space. For a given modality $m \\in \\{\\text{CT}, \\text{MR}, \\text{PET}\\}$, we define a projection matrix $P_m \\in \\mathbb{R}^{d_T\\times d_m}$, where $d_m$ is the dimensionality of the modality feature space. The modality-specific feature vector $M_m \\in \\mathbb{R}^{d_m}$ is obtained by projecting the latent feature:\n$M_m = T \\cdot P_m$.(1)\nThis projection models how each modality perceives the underlying latent features of the tissue, capturing modality-specific characteristics.\nInverse Projection and Reconstruction. The relationship between latent features and modality-specific features suggests that, under certain conditions, it is possible to reconstruct the latent features from the modality-specific features using the inverse of the projection matrices:\n$T = M_m \\cdot P_m^{-1}$.(2)\nHowever, since each modality provides only a partial view, reconstructing $T$ from a single modality may not capture all aspects of the latent features. Combining information from multiple modalities enhances the reconstruction, providing a more comprehensive representation.\nExtended Projection with External Models. Optimizing $T$ and $P_m$ simultaneously can lead to instability, as the condition number of the system matrix may increase due to parameter interdependence. Changes in $T$ propagate to $M_m$, impacting the eigenvalues of $P_m$, which amplify convergence difficulty and reduce gradient stability.\nTo address this challenge, we extend the projection model by incorporating features derived from external pre-trained models as additional projections of the latent features. Models like CLIP [51] and BioCLIP [52] provide robust feature embeddings for ROIs, trained on extensive datasets.\nLet $M_i \\in \\mathbb{R}^{d_i}$ represent the sub-space features from an external model $i$, with $d_i$ being its feature dimensionality. Each external model has its projection matrix $P_i \\in \\mathbb{R}^{d_T\\times d_i}$:\n$M_i = T \\cdot P_i$.(3)\nBy leveraging these external features, we estimate the latent features by aggregating inversely projected sub-space features:\n$T = \\frac{1}{N}\\sum_{i=1}^{N} M_i \\cdot P_i^{-1}$,(4)\nwhere $N$ is the number of external models used. This approach anchors the latent features to stable, pre-trained representations, mitigating optimization instability. Additionally, the training datasets used by external pre-training models encompass a wide range of textual data, which facilitates the reconstruction of latent features.\nGeneralized Modality Projection Framework. In this generalized framework, the latent features $T$ act as a central hub connected to various modality features $M_m$ and external model features $M_i$ through their respective projection matrices. This unified representation consolidates information across different modalities and knowledge domains."}, {"title": "4.5 Modality Projection Universal Model Structure", "content": "Building upon the modality projection principle, we introduce the Modality Projection Universal Model (MPUM), a comprehensive framework designed to effectively process and integrate multi-modality medical imaging data. The MPUM leverages the Modality Projection Controller (MPUM) within a deep learning architecture to handle diverse imaging modalities\u2014such as CT, MR, and PET\u2014facilitating accurate and efficient image segmentation."}, {"title": "4.6 Saliency map", "content": "Saliency maps are vital tools for visualizing the decision-making process of deep learning models. Traditional methods for generating saliency maps primarily rely on gradient-based techniques. Methods such as Vanilla Gradients[53] and Grad-CAM[41] generate saliency maps by calculating the gradients of the input image with respect to the model's output, highlighting regions that significantly influence the model's decisions.\nAs shown in Extended Data Fig. 6c, our modality projection block structure enables the generation of saliency maps across all layers of the network, not only the decision layer. In our model, we obtain the feature operator $K$ by Equ. 6. In our model, the feature operator $K$ from Equ.6 is a tensor with dimensions $\\mathbb{R}^{C\\times H \\times 3\\times 3 \\times 3}$, where $C$ represents categories, $H$ channels, and a kernel size of 3 x 3 x 3."}, {"title": "4.7 Statistical Analysis", "content": "In Section 2.3", "variables": "n$r_{control} = \\frac{\\sum_{i=1}^{n} (a^i - \\bar{a}) (b^i - \\bar{b})}{\\"}]}