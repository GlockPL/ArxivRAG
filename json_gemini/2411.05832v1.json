{"title": "Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for Neural Image Codec", "authors": ["Jun-Hyuk Kim", "Seungeon Kim", "Won-Hee Lee", "Dokwan Oh"], "abstract": "Designing a fast and effective entropy model is challenging but essential for practical application of neural codecs. Beyond spatial autoregressive entropy models, more efficient backward adaptation-based entropy models have been recently developed. They not only reduce decoding time by using smaller number of modeling steps but also maintain or even improve rate-distortion performance by leveraging more diverse contexts for backward adaptation. Despite their significant progress, we argue that their performance has been limited by the simple adoption of the design convention for forward adaptation: using only a single type of hyper latent representation, which does not provide sufficient contextual information, especially in the first modeling step. In this paper, we propose a simple yet effective entropy modeling framework that leverages sufficient contexts for forward adaptation without compromising on bit-rate. Specifically, we introduce a strategy of diversifying hyper latent representations for forward adaptation, i.e., using two additional types of contexts along with the existing single type of context. In addition, we present a method to effectively use the diverse contexts for contextualizing the current elements to be encoded/decoded. By addressing the limitation of the previous approach, our proposed framework leads to significant performance improvements. Experimental results on popular datasets show that our proposed framework consistently improves rate-distortion performance across various bit-rate regions, e.g., 3.73% BD-rate gain over the state-of-the-art baseline on the Kodak dataset.", "sections": [{"title": "1 Introduction", "content": "Most neural image codecs [8, 9, 11, 15, 17, 18] first transform an image into a quantized latent representation. It is then encoded into a bitstream via an entropy coding algorithm, which relies on a learned probability model known as the entropy model. According to the Shannon's source coding theorem, the minimum expected length of a bitstream is equal to the entropy of the source. Thus, accurately modeling entropy of the quantized latent representation is crucial.\nEntropy models estimate a joint probability distribution over the elements of the quantized latent representation. Generally, it is assumed that all elements follow conditionally independent probability distributions. To satisfy this, the probability distributions are modeled in context-adaptive manners, which is key to accurate entropy modeling [18]. Recent methods are based on the joint backward and forward adaptation where the probability distributions adapt by leveraging contexts in two different ways: directly using previously encoded/decoded elements (i.e., backward adaptation), and extracting and utilizing an additional hyper latent representation (i.e., forward adaptation). Here, the type of contexts leveraged can be diverse depending on the spatial range they cover. First, each element has dependencies with other elements in the same spatial location along the channel dimension. Since the channel-wise dependencies correspond to the local image area (e.g., a 16 \u00d7 16 patch), we denote them as the \"local\" context. Second, dependencies exist among spatially adjacent elements, and we refer to them as the \u201cregional\" context. Lastly, long-range spatial dependencies span the entire image area, referred to as the \"global\" context.\nFor the backward adaptation, the modeling order, i.e., which elements are modeled first, is an important factor, and the key lies in how effectively we can utilize diverse contexts in the modeling process. Early studies employ spatial autoregressive (AR) models that access regional context including the most spatially adjacent elements. However, they suffer from significantly slow decoding times due to the inevitably large number of modeling steps, which is equal to the spatial dimensions [18]. \u03a4\u03bf enhance efficiency in entropy modeling, several attempts reduce the number of modeling steps while leveraging diverse contexts: a 10-step channel-wise AR model [17], a 2-step spatial non-AR model with a checkerboard pattern [8], and a 4-step non-AR model that operates across spatial and channel dimensions using a quadtree partition [14].\nEntropy models based on the efficient backward adaptation methods have led to significant improvements. However, they are still limited in fully leveraging contexts for forward adaptation. Since they use multiple neural layers with down-sampling and upsampling for modeling hyper latent representation, they can only access the regional context. This limits the performance improvement due to the insufficient contexts (Figure 1a). In particular, this limitation is exacerbated at the first step where only forward adaptation is utilized due to the absence of previous elements (Figure 6). Therefore, it is necessary to develop effective forward adaptation in synergy with the efficient backward adaptation.\nIn this paper, we propose a simple yet effective entropy modeling framework, called DCA (Diversify, Contextualize, and Adapt), leveraging sufficient contexts for forward adaptation without compromising on bit-rate (Figure 1b). Building on the quadtree partition-based backward adaptation [14], we introduce a strategy of diversification, i.e., extracting local, regional, and global hyper latent representations unlike only a single regional one in the previous approach. Note that simply using more contexts for forward adaptation does not guarantee performance improvements because forward adaptation requires additional bit allocation unlike backward adaptation. Then, we propose how to effectively utilize the diverse contexts along with the previously modeled elements for contextualizing the current elements to be encoded/decoded. To consider stepwise different situations, e.g., increased number of previous elements over steps, our contextualization method is designed to utilize each hyper latent representation separately in a step-adaptive manner. Additionally, our contextualization method proceeds in the sequence of regional, global, and local hyper latent representations. Similarly to backward adaptation, we empirically observe that modeling order also matters in forward adaptation.\nOur main contributions are summarized as follows:\n\u2022 We propose a strategy of diversifying contexts for forward adaptation by extracting three different hyper latent representations, i.e., local, regional, and global ones. This strategy can provide sufficient contexts for forward adaptation without compromising on bit-rate.\n\u2022 We introduce how to effectively leverage the diverse contexts, i.e., previously modeled elements and the three hyper latent representations. We empirically show that the modeling order of three types of contexts affects the performance.\n\u2022 Through the diversification and contextualization methods, our DCA effectively adapts, resulting in significant performance improvements. For example, DCA achieves 3.73% BD-rate gain over the state-of-the-art method [14] on the Kodak dataset."}, {"title": "2 Related work", "content": "Joint backward and forward adaptation. Ball\u00e9 et al. [3] propose a scale hyperprior for forward adaptation. A hyper latent representation is extracted and utilized for inferring local scale parameters of the parameterized entropy model. Minnen et al. [18] extend the hyperprior model by using an additional mean hyperprior, and introduce joint backward and forward adaptation by combining the extended hyperprior model with a spatial autoregressive (AR) model. A patch matching-based non-local referring model [20] and a multi-head attention-based global hyperprior [11] are proposed to enrich contexts for backward and forward adaptation, respectively.\nEfficient backward adaptation. To address the slow decoding times of spatial AR-based entropy models, several studies have proposed group-wise backward adaptation methods. They first divide the quantized latent representation into multiple groups and then process them in a group-wise manner, resulting in improved efficiency. He et al. [8] propose dividing the quantized latent representation into two groups using the checkerboard pattern, which is further improved by incorporating Transformer-based modules [21]. While they apply a group-wise modeling in spatial dimension, Minnen and Singh [17] introduce a channel-wise AR model that divides the quantized latent representation into ten groups along channel dimension. Some studies [22, 23] improve this model by applying Swin Transformer [16]. Based on the channel-wise AR model, He et al. [9] optimize the channel division and combine it with the checkerboard-based model. Recently, Li et al. [14] propose a quadtree partition-based backward adaptation that divides the quantized latent representation into four groups considering both channel and spatial dimensions.\nIn this paper, we propose a novel fast and effective entropy model that achieves better rate-distortion performance by diversifying not only the quantized latent representation but also the hyper latent representations for backward and forward adaptation, respectively."}, {"title": "3 Methods", "content": "We provide an overview of the proposed methods in Figure 2. The analysis transform $f_a(\\cdot)$ and the synthesis transform $f_s(\\cdot)$ (the gray blocks in Figure 2) are learned to find an effective mapping between an input image $x$ and a quantized latent representation $\\hat{y}$, i.e., $\\hat{y} = [f_a(x)]$ and $x = f_s(\\hat{y})$, where $[]$ is a round operation and $\\hat{x}$ is the decoded image. For the analysis and synthesis transforms, we adopt the same model structure as in the ELIC-sm model [9] due to its efficiency.\nAll other components are learned to model a prior probability distribution on the quantized latent representation $\\hat{y}$, i.e., the entropy model $p_{\\hat{y}}$. The learned entropy model is utilized in the process of entropy coding, for which we employ the asymmetric numeral systems [6]. Here, our goal is to design a fast and effective learned entropy model.\nQuadtree partition. We build our entropy model on the joint forward and backward adaptation where the quadtree partition is used, which is formulated as follows [14]:\n$p(y) = p(z) \\times p(\\hat{y}|z) = p(z) \\times \\prod_{i=1}^{4} p(\\hat{y}^i | \\hat{y}^{<i}, z)$\nwhere $\\hat{y}$ is the quantized latent representation, $z$ is the quantized regional hyper latent representation, $\\hat{y}^i$ is the elements to be modeled at the i-th step, and $\\hat{y}^{<i}$ is all the previous modeled elements before the i-th step. At each step, one-fourth of the total elements are modeled. The method partitions the quantized latent representation into four groups along the channel dimension, and then divides each group into non-overlapping 2\u00d72 patches along the spatial dimension. The entropy modeling proceeds over four steps, with each step modeling different elements as shown in Figure 3. This quadtree partition-based method uses diverse contexts for backward adaptation, capturing dependencies from both spatial and channel dimensions.\nMotivation. Recent studies to efficient modeling of backward adaptation have made significant advancements in terms of optimizing the rate-distortion-computation trade-off; however, there is still a gap between their assumptions in the probability modeling and actual data, leaving room for further performance enhancement. The assumptions are as follows: 1) All elements of $z$ are independent; 2) All elements of $\\hat{y}$ are conditionally independent given $\\hat{y}^{<i}$ and $z$. Here, the more the actual data deviates from the assumptions, the lower the accuracy of the entropy modeling. At the first modeling step, the elements are modeled conditioned only on the quantized hyper latent representation, i.e., $p(\\hat{y}^1|z)$, resulting in a hyperprior model known for deviating from to the second assumption [18]. This can be more problematic because the state-of-the-art methods process a relatively large number of elements at the first step in order to complete the overall modeling with a minimal number of steps. We also empirically show that this problem actually occurs in Figure 6.\nOne straightforward solution is to increase the number of steps so that fewer elements are modeled in the first step. However, this leads to slower modeling speeds, which conflict with the goal of our paper, i.e., developing a fast and effective entropy model. Another simple approach is to provide more quantized regional hyper latent representation $z$ when modeling the quantized latent representation $\\hat{y}^1$. However, paradoxically, this approach can introduce another issue due to the first assumption. Since all elements of hyper latent representation are the same type of information (i.e., regional context), there is a relatively high likelihood of dependencies among the elements. Therefore, to meet both assumptions, the newly added hyper latent representation is required to be independent from the existing regional hyper latent representation. This is why our proposed diversification method using"}, {"title": "3.1 Diversify", "content": "The proposed DCA aims to diversify the information that the hyper latent representations contain. Specifically, given the latent representation $y \\in \\mathbb{R}^{H \\times W \\times C}$, where $H$, $W$, and $C$ are the height, width, and the number of channels, respectively, DCA extracts three different types of hyper latent representations depending on the range they cover: a local hyper latent representation $\\hat{z}_l \\in \\mathbb{R}^{H \\times W \\times C_l}$, a regional hyper latent representation $z \\in \\mathbb{R}^{\\frac{H}{4} \\times \\frac{W}{4} \\times C_r}$, and a global hyper latent representation $z_g \\in \\mathbb{R}^{N \\times C_g}$. The whole process is illustrated in the orange blocks of Figure 2.\nLocal context. To model remaining dependencies along channel dimension at each spatial location, which correspond to a 16 \u00d7 16 local patch in the image domain, we introduce local hyper analysis and synthesis transforms, $l_a(\\cdot)$ and $l_s(\\cdot)$, based on Swin Transformer (SwinT) [16]. The local hyper analysis transform $l_a(\\cdot)$ analyzes local information in the latent representation, followed by the quantization operation to obtain a local hyper latent representation $\\hat{z}_l$. The local synthesis transform $l_s(\\cdot)$ synthesizes the local features $\\phi_l \\in \\mathbb{R}^{H \\times W \\times 2C}$ for contextualization from the local hyper latent representation $\\hat{z}_l$.\nEach transform proceeds in the order of a Patch Split block, a SwinT block, and a Patch Merge block. The Patch Split block serves the function of shifting all channel-wise elements at each spatial location to a 2 \u00d7 2 spatial resolution, consisting of the depth-to-space, layer normalization, and linear layers in sequence. The SwinT block then captures dependencies between elements within each non-overlapping window of the input, producing an output of the same size as the input. By setting the window size to 2 \u00d7 2 in conjunction with the use of the Split block, we enforce the local hyper transforms to focus only on the local image area. The Patch Merge block performs the opposite function of the Patch Split block, containing the layer normalization, linear, and space-to-depth layers in sequence.\nRegional context. While the receptive field of the local hyper transforms is limited to the local image area (i.e., 16 \u00d7 16 patches), regional hyper analysis transform $r_a(\\cdot)$ and regional hyper synthesis transform $r_s(\\cdot)$ model remaining dependencies between elements distributed across a relatively wide image area. The regional hyper analysis transform $r_a(\\cdot)$ analyzes regional information in the latent representation and yields a regional hyper latent representation $z$ after quantization. From the extracted regional hyper latent representation $z$, the regional synthesis transform $r_s(.)$ generates the regional features $\\phi_r \\in \\mathbb{R}^{H \\times W \\times 2C}$ for contextualization.\nTo do this, we stack multiple layers with the downsampling and upsampling operations for the regional hyper analysis and synthesis transforms, respectively. We adopt the same structure as the previous work [22], which is based on SwinT. Specifically, the regional hyper analysis transform $r_a(\\cdot)$ conducts a Patch Merge block, five SwinT blocks, a Patch Merge block, and a SwinT block. The regional hyper synthesis transform $r_s(\\cdot)$ is constructed in the opposite order of the hyper analysis transform $r_a(\\cdot)$, using the Patch Split block instead of the Patch Merge block.\nGlobal context. Lastly, to capture remaining dependencies between elements across the whole image area, we construct global hyper analysis and synthesis transforms $g_a(\\cdot)$ and $g_s(\\cdot)$ by adopting model structure of the global hyperprior model of Informer [11]. The global hyper analysis transform $g_a(\\cdot)$ extracts globally abstracted information from the latent representation using a Transformer block with cross-attention and a 1 \u00d7 1 convolutional layer. After quantization, it obtains a global hyper latent representation $z_g$. Using a 1 \u00d7 1 convolutional layer, the global synthesis transform $g_s(.)$ infers the global features $\\phi_g \\in \\mathbb{R}^{N \\times 2C}$ for contextualization."}, {"title": "3.2 Contextualize", "content": "Diverse contexts, i.e., previously modeled elements (i.e., $\\hat{y}^{<i}$) and hyper latent representations (i.e., $\\hat{z}_l$, $z_r$, and $z_g$) can be used for adapting probability distributions. Here, an important research question"}, {"title": "3.3 Adapt", "content": "We design an adaptive entropy model on the quantized latent representation $\\hat{y}$ where each element is assumed to follow the Gaussian distribution, and each distribution parameters are obtained from the previous diversification and contextualization stages. Following the previous works [3, 18], we formulate our entropy model as follows:\n$P_{\\hat{y}}(Y) = \\prod_{i}(N (\\mu^i, \\sigma^i)^*((\\hat{y_i}))$,\nwhere $\\mu^i$ and $\\sigma^i$ are the mean and scale of the Gaussian distribution for each element $\\hat{y}_i$, respectively. The transforms and entropy model are jointly trained in an end-to-end manner by minimizing the expected length of the bitstream (rate) and the expected distortion between the original image and the decoded image, $d(\\cdot, \\cdot)$. When a learned entropy model precisely matches the actual probability distribution, the entropy coding algorithm achieves the minimum rate. Therefore, we minimize the cross-entropy between the two distributions. We use mean squared error (MSE) for measuring image distortion. The objective function for our method is as follows:\n$\\mathcal{L} = E_{x \\sim p_a} [- \\log_2 P_{\\hat{y}} (\\hat{y}) \u2013 \\log_2 P_{\\hat{z}_l} (\\hat{z}_l) \u2013 \\log_2 P_{z_r} (z_r) \u2013 \\log_2 P_{z_g} (z_g) + \\lambda \\cdot d(x,\\hat{x})]$,\nwhere $p_a$ is the distribution of the training dataset, the entropy models $P_{\\hat{z}_l}$, $P_{z_r}$, and $P_{z_g}$ are the non-parametric fully factorized entropy models [2], and $\\lambda$ is the Lagrange multiplier that determines weighting between rate and distortion. As the value increases, a model is trained in a direction that reduces information loss, and consequently leads to a higher bit-rate."}, {"title": "4 Experiments", "content": "We use a PyTorch [19] based open-source library and evaluation platform, CompressAI [4], which has been widely used for developing and evaluating neural image codecs.\nTraining. We set our model parameters as follows: $C = 320, C_l = 10, C_r = 192$, and $N = 8$. We train our models corresponding six different bit-rates. We use 300,000 images randomly sampled from the OpenImages [13] dataset. We construct a batch size of 16 with 256\u00d7256 patches randomly cropped from different training images. All models are trained for 100 epochs using the Adam optimizer. The learning rate is set to $10^{-4}$ up to 90 epoch, and then decreases to $10^{-5}$. We use PyTorch v1.9.0, CUDA v11.1, CuDNN v8.0.5, and all experiments are conducted using a single NVIDIA A100 GPU.\nEvaluation. We evaluate our method on the two popular datasets: Kodak [7] and Tecnick [1]. The Kodak dataset consists of 24 images with a resolution of either 768\u00d7512 or 512\u00d7768 pixels. The Tecnick dataset is composed of 100 images with a resolution of 1200\u00d71200 pixels. We evaluate our method in terms of rate-distortion performance. For this, we calculate the bits per pixel (bpp) after the encoding phase, and measure distortion between the decoded image and the original image using the peak signal-to-noise ratio (PSNR)."}, {"title": "4.1 Comparison with state-of-the-art methods", "content": "We compare the proposed entropy model, DCA, with state-of-the-art entropy models. DCA can be combined with any transforms; in this paper, DCA is implemented with transforms that have the same structure as in ELIC-sm [9]. For the comparison, we further train image compression methods with four different entropy models [11, 14, 17, 18]. For a fair comparison, they are also implemented with transforms that have the same structure as in ELIC-sm [9].\nRate-Distortion. Figures 4a and 4b show the rate-distortion performance on the Kodak and Tecnick datasets, respectively. The proposed DCA consistently achieves the best rate-distortion performance across all bit-rate regions and two benchmark datasets. Specifically, DCA achieves 11.96% average rate savings over VTM-12.1 on the Kodak dataset, while the second (Lie et al. 2023) [14] and third best (Minnen & Singh, 2020) [17] methods obtain 8.55% and 4.86%, respectively.\nComplexity. We also evaluate DCA in terms of efficiency. To this end, we provide the de-coding time, the number of model parameters, and Bj\u00f8ntegaard delta rate (BD-rate) [5] in Fig-ure 5. Decoding time is measured on the Kodak dataset using a single NVIDIA V100 GPU."}, {"title": "5 Conclusion", "content": "In this paper, we proposed a fast and effective entropy modeling framework, DCA, which diversifies forward contexts by extracting local, regional, and global information, and contextualizes current elements with the diverse forward and backward contexts. We demonstrated that our DCA improves rate-distortion performance significantly compared to previous approach without compromising efficiency as much as possible. Furthermore, we provided diverse insights into entropy modeling by conducting a comprehensive and in-depth analysis of the design aspects of DCA.\nLimitation and future works. To address the limitation of the state-of-the-art entropy models, we focused on paving a novel framework with diverse contexts rather than designing neural architectures. Therefore, DCA can be limited by the architectural designs that are inspired by the existing works [11, 14, 22]. In the future, we expect that it would be further improved by neural architectures especially designed for the diverse contexts. In addition, it is worth exploring alternative criteria for diversification beyond the spatial range the contexts covers (i.e., local, regional, and global contexts)."}]}