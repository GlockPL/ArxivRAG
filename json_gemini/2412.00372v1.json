{"title": "2-Factor Retrieval for Improved Human-Al Decision Making in Radiology", "authors": ["JIM SOLOMON", "LALEH JALILIAN", "ALEXANDER VILESOV", "MERYL MATHEW", "TRISTAN GROGAN", "ARASH BEDAYAT", "ACHUTA KADAMBI"], "abstract": "Human-machine teaming in medical AI requires us to understand to what degree a trained clinician should weigh AI predictions. While previous work has shown the potential of AI assistance at improving clinical predictions, existing clinical decision support systems either provide no explainability of their predictions or use techniques like saliency and Shapley values, which do not allow for physician-based verification. To address this gap, this study compares previously used explainable AI techniques with a newly proposed technique termed '2-factor retrieval (2FR), which is a combination of interface design and search retrieval that returns similarly labeled data without processing this data. This results in a 2-factor security blanket where: (a) correct images need to be retrieved by the AI; and (b) humans should associate the retrieved images with the current pathology under test. We find that when tested on chest X-ray diagnoses, 2FR leads to increases in clinician accuracy, with particular improvements when clinicians are radiologists and have low confidence in their decision. Our results highlight the importance of understanding how different modes of human-AI decision making may impact clinician accuracy in clinical decision support systems.", "sections": [{"title": "INTRODUCTION", "content": "In medicine, barriers to the acceptance of artificial intelligence (AI) tools in clinical workflows occur due to the \"black box\" nature of AI, which make it difficult for clinicians to understand and trust the predictions of a model [11, 31, 36]. A popular method to elucidate a model's decision is to include explanations in the form of textual or visual elements to help clarify how the components of a given image, such as specific areas in an image, influence a model's prediction. For instance, a doctor may make more informed clinical decisions when a model offers clear and understandable explanations of its predictions, and a lack of understanding of the prediction's rationale could hinder clinicians from identifying and addressing errors, particularly in scenarios when model prediction and clinician intuition are discordant [32].\nHowever, prior work has shown that model explainability can lead to users falsely trusting an Al model's decision due to convincing explanations of incorrect decisions [23]. While the field has focused on developing models that can explain the reasoning behind a particular diagnosis or treatment recommendation, for example showing which factors or variables are most important in a model's prediction, a knowledge gap exists in understanding whether other aspects of AI-human decision may offer distinct advantages over another and how users account for these modes of decision-making in their acceptance of AI predictions. Specifically, we study the impact of verification-based AI-human decision making. Verification-based AI-human decision making encourages human's to attempt to verify an Al prediction before accepting the decision. To facilitate this, we introduce a simple technique that can be paired with any AI prediction tool that encourage a human evaluator's recall and verification abilities. The newly proposed technique termed '2-factor retrieval (2FR)' is a combination of interface design and search retrieval that returns similarly labeled data. This results in a 2-factor reasoning step where: (a) correct images are retrieved by the AI, and (b) humans should associate the retrieved images with the current pathology under test. Given an AI predicted diagnosis for an image, we present the evaluator with canonical image examples of the given Al diagnosis. This method allows the clinician to recall the salient features of the diagnosis and furthermore compare the canonical images with the current image. We evaluate the efficacy of our proposed method on a diverse set of clinicians with varying expertise and years of experience and compare against other modes of AI-human decision making. We present the following contributions of this work:"}, {"title": "RELATED WORKS", "content": "With the increasing interest to implement machine learning into real-world clinical settings, the role of explainable and interpretable machine learning has been one way in understanding if AI can facilitate more informed and accurate decision making [3, 9]. Early efforts in explainable AI (XAI) focused on feature-based explanations [35, 39]. Recent considerations of interpretability comprise a wider range of techniques, including uncertainty and confidence metrics [37], nearest-neighbors [21], and counterfactuals [54].\nHowever, there have been mixed results on whether explanations actually help clinicians who are making AI-supported decisions [16]. The literature demonstrates that individuals tend to be swayed by AI, frequently accepting its decisions without proper verification, a phenomenon termed overreliance [5]. Among various error types observed in human-AI decision-making, overreliance emerges as the most common issue identified in empirical studies. This tendency involves individuals ceding their decision-making responsibility and accountability to Al systems, which will be problematic in critical domains like healthcare. Such overreliance not only risks amplifying machine biases but does so under the pretense of human intervention and control. Additionally, models can also generate seemingly sensible explanations for incorrect predictions [6]."}, {"title": "Al-assisted Decision-making", "content": "With the advancements in AI models, we have seen an explosion of research into their applications as well as early adoption of the technology into industry. In healthcare, AI is viewed as a technology meant to augment clinicians in the quality of their decision making and not replace them. Such a collaboration between AI and humans requires understanding the explainability needs of end-users, in order to best develop appropriate reliance between the clinician and the AI. As an example, does the optimal interface require just the Al's answer, or do clinicians require some level of interpretibility to the model's predictions? A large area of scholarship focuses on advancing methods of explaining the decision-making process of the AI as summarized in section 2.1; however, it is equally important to understand to what extent do explainability methods help in improving performance [22, 27, 28] or in some cases even hurt performance [4, 5]. This research avenue has confirmed the potential of error in human reasoning such as confirmation bias [5, 26, 48], further worsened by anchor bias [15, 33, 48], as well as increased confidence in decision despite no correlation with accuracy [1, 26, 42]. Confirmation bias is becoming increasingly worrisome due to the ability of large language models (LLMs) to write convincing text even when the outputs are factually incorrect [41]. Fok and Weld [12] found in a survey that the majority of AI applications do not yield complementary performance when explanations are included unless the explanation helps verify the accuracy of the answer. Vasconcelos et al. [47] found that extra care is required in forming the explanation to reduce the likelihood of overreliance in Al systems which is a common issue that has been identified in using explainable AI."}, {"title": "Al Decision Support in Medicine", "content": "Improvements in AI diagnostic abilities have led researchers to explore methods that incorporate a model's prediction into clinicians' workflow. The majority of research has focused on creating AI decision support frameworks where Al augments human decision making [25]. Other works have focused on the proper presentation of AI decisions within clinical workflow such as data visualization, risk presentation, communication of system properties and other design considerations [7, 43-45, 52]. Several studies within this area have pointed out that adoption of AI decision support systems is low [8, 10], especially in prognostic focused applications, thus reducing the ability to analyze their effectiveness [50]. However, Scheetz et al. [38] performed a survey on clinicians preferences for such systems and found positive attitudes towards how AI could affect their workflow in increasing accuracy and reducing time. Despite low adoption, works have found utility in AI decision support systems increasing diagnostic accuracy and reducing time spent on repetitive tasks. In dermatology, simple merging of human and AI decisions have been shown to increase accuracy [20, 46], [2] used reinforcement learning to adjust the risk-reward of AI model decisions in clinician support systems to better represent human preferences, and [18] studied how fairness in accuracy of Al systems across skin tones impacts clinicians' overall diagnostic performance. In radiology, Xie et al. [51] conducted an iterative design of a support system based on clinician feedback, [53] found improvements in clinician accuracy while [13] found that non-radiologist clinicians benefited most from AI input, and [14] explored how incorrect AI decisions and explanations affected clinician decision making, showing evidence of over-reliance. Similar work has been conducted in other fields with most concentrating on ophthalmology [19, 29], cardiology [30, 34], and neurology [17, 40]."}, {"title": "STUDY METHODS", "content": ""}, {"title": "Research Aim", "content": "The purpose of this study was to assess the utility of different methods of AI-human decision making. We assess these systems in joint AI-human interpretations of clinical Chest X-Rays. We compare different modes of presenting AI predictions and their influence on clinician accuracy and confidence."}, {"title": "Participants", "content": "In total, N = 69 participants finished the online experiment and were included in the data analysis. The sample consisted of physicians with different levels of task expertise and different years of training. Physicians trained in internal medicine, anesthesiology, surgery, or emergency medicine often review chest X-rays but, compared to Radiology, have relatively little formal training in viewing medical images and were consequently classified as \"non-task experts\". Radiologists with specialized training in reviewing medical images were classified as \"task experts.\" Participants were recruited via email. Study invitations were sent to staff and residents at hospitals in the US and to residency program coordinators with the request to distribute the link. Table 1 displays the participant demographics."}, {"title": "IRB Approval", "content": "The UCLA Institutional Review Board (IRB) approved the study, and informed consent was obtained from all participants. This research complies with all relevant ethical regulations. The UCLA Institutional Review Board approved this study as IRB Exempt. At the beginning of the experiment, all participants were presented with the following informed consent statement: \"CXR Diagnosis is a UCLA research project. All submissions are collected anonymously for research purposes. You can leave this website anytime.\""}, {"title": "Experimental Design", "content": "The experiment was conducted online via a publicly accessible website which we developed. An example interface shown to radiologists can be seen in Fig. 2. Participants were given basic information about the purpose of the study and an estimated study duration of 10 to 15 min. They were informed that participation was completely voluntary and anonymous, that they could quit the study at any time without negative consequences, and about the option of being included in a raffle as compensation for their participation. Only individuals who gave written informed consent to take part in the study (by clicking a checkbox) and confirmed that they were currently practicing radiology, internal medicine, anesthesiology, surgery, or emergency medicine (residency included) in the USA or Canada could move on to the experiment. Participants completed a short survey, including questions about demographics, professional identification, and years of experience.\nThe remaining 12 questions were designed to determine whether different modes of explainability impact clinician confidence in AI-assisted predictions. 24 images were taken from the NIH Chest X-ray dataset [49]. The paper introducing the Chest X-ray dataset included a deep convolutional neural network (DCNN) that predicts a pathology in the chest X-ray images and provides saliency maps that explains which regions of the X-ray image contribute most to the Al's decision. We used the saliency maps from [49] as a benchmark for explainability-based methods. To understand human-Al decision making behavior when the AI is correct and incorrect, we manually selected 2/3 of presented instances to be when the DCNN was accurate and the rest are when the DCNN was inaccurate. We utilized the model's label predictions and the saliency maps for our experiments. We split the images randomly into two sets for survey versions A and B, and each chest X-ray presented one of four conditions: Mass/Nodule, Cardiomegaly, Pneumothorax, or Effusion. Both sets contained three images of each condition, and half of the chest X-rays had a diagnosis difficulty rating of \"Easy\" while the other half was labeled \"Hard.\"\nParticipants were assigned a random survey version and random ordering of the images in the corresponding set. Each question presented an image and participants were given 14 options to choose from in diagnosing it. The image was accompanied by either an AI diagnosis, an Al diagnosis with the option to view the highlighted"}, {"title": "Statistical Analyses", "content": "To determine whether confidence levels varied by modality, we conducted linear mixed-effects models with fixed effects for AI modality, question difficulty, participant specialty, years of practice, and participant age. A random intercept was included to account for within-participant variability. Similar models were constructed for accuracy. Least squares means (LSMeans) were used to compare confidence levels across AI modalities, with pairwise differences and 95 percent confidence intervals estimated. Analyses were conducted using SAS V9.4 (Cary, NC) and p-values <0.05 were considered statistically significant."}, {"title": "Procedure", "content": "In the survey, participants learned that their task was to review and diagnose 12 patient cases as accurately as possible, for which they received chest X-rays and the diagnostic advice that could be used for their final decisions. The chest X-rays were shown as a static image on the survey site. For each case, the participating physicians were asked to pick a diagnosis and judge how confident they were with their diagnosis."}, {"title": "Measures", "content": "The present study had two dependent variables: (1) diagnostic accuracy, and (2) confidence in the diagnosis.\nDiagnostic accuracy: After being presented with the AI-generated diagnosis, the participating physicians were asked \"What is your most important clinical finding?\" to provide their own diagnosis from a limited set of options, without being prompted to explicitly agree or disagree with the AI prediction. The accuracy of the physician's diagnosis was determined by comparing their selected diagnosis with the correct diagnosis associated with each case. Since the AI diagnosis was correct approximately two-thirds of the time, a tertiary variable was also analyzed: the alignment or correlation between physician's diagnoses and the AI-generated diagnoses. This alignment was used to explore how often physicians followed the Al's advice and therefore their confidence in AI predictions as a whole.\nConfidence in the diagnosis: For each case, participants rated the confidence in their final diagnosis with one item (\"How confident are you with your primary diagnosis?\") on a 10-point Likert scale from 1 (not at all) to 10 (extremely)."}, {"title": "RESULTS", "content": ""}, {"title": "Accuracy Across Modes of Al-Human Decision Making", "content": "One of our main experimental goals was to understand how various modes of AI-Human decision-making impacts clinician accuracy. For this, we recruited N = 69 physicians to participate in a survey. The result for mean accuracy across modalities are shown in Fig. 3, while Fig. 4 shows the same accuracy values, coupled with its standard error and confidence intervals.\nOverall. Across all modes where AI assistance is provided, physician accuracy is markedly higher when AI predictions are correct (0.35 (95% CI 0.28-0.41), p<0.001). The impact of AI being correct or not on accuracy does not significantly vary by modality type (e.g. 2FR, Saliency, AI, no AI). In the 2FR modality, physicians achieve the highest overall accuracy (70%), suggesting that providing physicians with Al-predicted diagnoses alongside the 2FR metric enhances AI-Human decision making. A similar trend is observed in the AI Saliency modality, where accuracy remains high (65%), indicating the utility of providing visual or contextual cues to support AI predictions. In contrast, when physicians rely solely on AI predictions without supplemental information (AI modality), their accuracy is slightly reduced (64%). This finding underscores the limitations of"}, {"title": "Experience", "content": "Fig. 5 highlights the influence of clinical experience on the effectiveness of AI-Human decision-making. When clinicians have less than 11 years of experience, 2FR achieves the highest accuracy when Al predictions are correct. The accuracy reaches approximately 70%, while for those with 10 or more years, it slightly declines to around 65%. This suggests that 2FR is most useful for clinicians with less experience. Across all modes, incorrect AI predictions lead to substantial performance declines. However, the accuracy values are comparable to the No AI, suggesting that clinicians rely more on their expertise when the Al is incorrect."}, {"title": "Expertise", "content": "In Fig. 6, radiologists using the 2FR modality achieve the highest accuracy when Al predictions are correct (65%). This demonstrates that incorporating 2FR metrics into AI assistance is highly effective for expert users. Scheetz [38] showed that radiologists have a high standard for Al correctness and prefer using AI to automate monotonous tasks. This explains the result where 2FR performs best when Al is correct. The questions in which the AI is correct can be interpreted as easy questions. This makes them a more monotonous task to a radiologist. Saliency and Only AI also yield good performance (50% and 60%, respectively) when Al predictions are correct, though lower than 2FR. In the case of Non-radiologists, the difference between the accuracy of 2FR and Saliency is marginal. This suggests that 2FR significantly aids expert and non-expert clinicians, but AI Saliency harms expert users. We observe a 20 point drop in accuracy from non-expert to expert with the Saliency modality, this suggests 2FR is a more robust modality across clinician expertise. The comparison reveals that radiologists, despite their domain expertise, benefit significantly from AI assistance, particularly when provided with supportive features such as 2FR. However, they are less reliant on AI and more resilient to errors compared to non-radiologists. Non-radiologists show greater dependence on Al outputs and are more vulnerable to incorrect predictions."}, {"title": "Chest X-Ray Difficulty", "content": "Fig. 7 reveals distinct trends in performance for easy versus hard chest X-ray cases. For easy questions, all modalities show a significant advantage when AI predictions are correct, with the 2FR yielding the highest accuracy (>70%). We see higher accuracy on 2FR on easier and correct questions, implying that 2FR assists clinicians in more accurate diagnoses. For hard questions, accuracy decreases across all modalities except Saliency. The accuracy of Saliency remains consistent when AI prediction is correct across Easy and Hard questions."}, {"title": "Reliance On Al", "content": "With p < 0.001, we observe a significant correlation between clinician accuracy and AI correctness across all AI-Human decision-making modalities. When the AI prediction is correct, clinician accuracy is substantially higher across 2FR, Saliency, and Only AI modalities compared to the No AI condition. This demonstrates that clinicians leverage AI effectively when it provides accurate information, enhancing their diagnostic performance.\nHowever, when the AI is incorrect, the difference in accuracy between the AI-assisted modalities and the No AI condition becomes marginal. For example in Fig. 3, clinician accuracy in 2FR, Saliency, and Only AI modalities (25%) is similar to the accuracy in the No AI condition (25%). This minimal difference suggests that clinicians do not heavily rely on Al predictions when they are incorrect. Instead,"}, {"title": "Clinician Confidence", "content": "A key observation in Fig. 8 and 9 is that changes in clinician confidence are marginal, irrespective of AI correctness or the question's difficulty. For overall performance, clinician confidence remains relatively stable across modalities, with only slight differences between correct and incorrect AI predictions. This suggests that while Al correctness influences confidence to a small degree, its overall impact on clinician self-assurance is limited. When analyzing hard questions, clinician confidence is slightly lower compared to easy questions, particularly when Al predictions are incorrect. However, the differences are minimal, with 2FR and Saliency showing only small reductions in confidence. For easy questions, confidence remains uniformly high across all modalities, regardless of whether the AI prediction is correct, further emphasizing the marginal effect"}, {"title": "Clinician Confidence and Accuracy", "content": "Fig. 10 illustrates the accuracy across clinician confidence levels. Focusing on when clinicians exhibit low confidence (light green bars), 2FR achieved a moderate accuracy (30%). While this is lower than medium and high confidence groups, it remains the highest among all low-confidence results across modalities. 2FR achieves 3X more performance in low confidence when compared to Saliency( 10%) and 2X when compared to Only AI (15%). This highlights the unique advantage of 2FR for individuals operating on questions they feel low confidence on. The 2FR approach likely aids participants by reinforcing diagnostic memory or offering explicit support, minimizing the cognitive burden experienced in uncertain scenarios. This could explain its significantly higher performance."}, {"title": "DISCUSSION", "content": "It is tempting to believe that integrating AI-generated predictions into clinical workflows will inherently enhance diagnostic accuracy and efficiency. However, as with any technological advancement in critical domains like healthcare, it is imperative to empirically evaluate its actual impact on human decision-making. This is an important topic for the broader impact of AI and has been explored in the medical literature [14, 24] but requires further study as human-Al systems are increasingly being integrated in healthcare. In this study, we conducted a rigorous investigation to assess how different modes of Al assistance influence clinicians' diagnostic performance and confidence when interpreting chest X-rays. We recruited 69 physicians across various specialties and levels of experience, and analyzing their responses to 12 diagnostic cases under different AI assistance modalities, we uncovered nuanced insights in AI-Human collaboration.\nOur findings reveal that when Al predictions are correct, providing clinicians with additional explanatory features-such as 2FR examples of similar cases or saliency maps highlighting pertinent image regions-can enhance diagnostic accuracy compared to providing AI predictions alone or offering no AI assistance. Specifically, the 2FR modality, which presented AI diagnoses alongside representative images recognized by other physicians, resulted in the highest overall accuracy (70%). This suggests that contextualizing Al outputs with relatable examples aids clinicians in better understanding and trusting AI recommendations. Conversely, when AI predictions were incorrect, clinician accuracy dropped significantly across all modalities to comparable levels if no AI predictions were given at all. This implies that when clinicians encounter questions that an Al fails on, they fall back to relying on their own expertise. Interestingly, clinician confidence remained relatively stable across different AI modalities and was not significantly influenced by AI correctness or the difficulty level of the cases. This resilience in self-assessed confidence, despite fluctuations in actual diagnostic accuracy, points to a complex relationship between confidence and"}, {"title": "CONCLUSION", "content": "This study shows how simple changes to AI decision making support systems that include a verification-based component can lead to improvements in clinician performance. The utility of our proposed method, '2FR', is not well explored in this domain, and we hope that our study will inspire a new line of research into improving this method from intelligently picking similar types of images to incorporating model uncertainty in how references images are presented. While our study focused on chest X-ray interpretation, it would be valuable to extend this research to other diagnostic domains and complex clinical tasks. Investigating the long-term effects of AI assistance on clinician learning, diagnostic strategies, and patient outcomes will provide deeper insights into optimizing human-Al collaboration in healthcare. Addressing these areas is crucial to harnessing the full potential of AI while safeguarding the quality and integrity of medical decision-making."}]}