{"title": "DataSciBench: An LLM Agent Benchmark for Data Science", "authors": ["Dan Zhang", "Sining Zhoubian", "Min Cai", "Fengzu Li", "Lekang Yang", "Wei Wang", "Tianjiao Dong", "Ziniu Hu", "Jie Tang", "Yisong Yue"], "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://datascibench.github.io/.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Achiam et al., 2023; Team et al., 2023; GLM et al., 2024) are increasingly used in data science and scientific domains, e.g., data analysis (Hong et al., 2024), protein generation (Jumper et al., 2021; Chen et al., 2024a), and scientific discovery (Lu et al., 2024) and reasoning (Zhang et al., 2024a,b). For data science tasks, given a publicly known problem, LLMs offer the potential to (semi-)autonomously conduct data analysis (Huang et al., 2023) and data visualization (Hong et al., 2024) by invoking code interpreters with corresponding Python libraries. These works are benchmarked on relatively straightforward tasks where ground truth (GT) labels can be precisely obtained. However, much of real-world data analysis requires reasoning over more complex scenarios (Chen et al., 2024b) as shown in Figure 1, such as calculating expenditure, and evaluating the quality of the images generated by the data visualization task. Properly evaluating these more complex data science tasks remains an open research direction.\nWhile some existing benchmarks are used to"}, {"title": "2 Background on using LLMs for Data Science", "content": "This section discusses the key aspects that underlie our benchmarking approach.\nGround Truth Generation. Ground truth serves as the cornerstone for evaluating the performance of LLMs in data science tasks. For diverse and challenging data science prompts, we aim to propose a semi-automated pipeline that leverages a robust LLM to generate GTs and employs self-consistency and human validation strategies to ensure the accuracy and reliability of generated GTs.\nEvaluation Metric Definition. Defining appropriate and meaningful evaluation metrics is essential for effectively comparing and analyzing the effectiveness of different LLMs in data science tasks. Our study meticulously defines evaluation metrics tailored to the specific tasks and challenges the collected prompts pose. These metrics are designed to capture the diverse nuances of data analysis and visualization tasks, enabling a comprehensive assessment of LLMs' capabilities.\nLimitation of Previous Studies. Prior research in benchmarking LLMs for data science has often been limited by focusing on single tasks, simplistic evaluation metrics, and readily available ground truth. These shortcomings hinder the thorough evaluation of LLMs and may not fully capture their strengths and weaknesses. By addressing these limitations, our study seeks to provide a more comprehensive and nuanced assessment of LLMs in data science. Through the development of DataSciBench and the implementation of a rigorous evaluation framework, we aim to push the boundaries of benchmarking practices in the field of data science and LLM research."}, {"title": "3 DataSciBench", "content": "DataSciBench consists of three important components as outlined in Figure 2.\n\u2022 Prompt Definition and Collection which defines 6 task types and collects 222 real, challenging, and high-quality prompts through question filtering and expert review.\n\u2022 Response Integration and Validation which proposes a novel Task - Function - Code (TFC) that produces 519 test cases to effectively assess the key tasks of each prompt through defined aggregate functions and programmatic rules."}, {"title": "3.1 Prompt Definition for Data Science", "content": "Task Type. We define six typical data science tasks in Appendix A.1 that include Data cleaning and preprocessing, Data exploration and statistics understanding, Data visualization, Predictive modeling, Data mining and Pattern recognition, and Interpretability and Report generation.\nTask Integration. To increase the difficulty of prompts, we chose more complex prompts that included multiple defined task types. These sequential tasks can be any combination of six task types."}, {"title": "3.2 Dataset Collection", "content": "Question Collection. We collect questions from four sources: 1) Extensive collection from a real-world online platform. We collect natural prompts from one online code-generation platform, CodeGeeX (Zheng et al., 2023). 2) Extracted and rewritten from a public code benchmark. We utilize 167 high-quality data science prompts from BigCodeBench (BCB) and then refine them to our specified format encompassing input data or file, prompt, and expected output file with TFC for standardized evaluation. 3) Hand-written by humans. We also write elaborated prompts to increase the difficulty and robustness of the evaluated benchmark by referring to relative websites\u00b9. 4) Synthesized from LLMs. We use a few shot examples drawn from human-written prompts to ask LLM to generate prompts.\nQuestion Filtering. We filter low-quality questions via the following principles: 1) Choose questions that include keywords, but are not limited to, \"machine learning\u201d, \u201cdeep learning\u201d, \u201cdata preprocessing\", and \u201cdata visualization\u201d; 2) Filter questions that require rewriting code, finding errors, and explaining basic concepts.\nExpert Review. We review the prompts we collect with experts in computer science and data analysis to ensure their quality. The review process includes three stages: 1) In stage 1, experts"}, {"title": "3.3 Response Integration and Validation", "content": "Ground Truth Generation and Verification. To obtain the response of the collected questions, we propose the following strategy to generate test cases for each question. Firstly, we generate the outputs of each prompt by sampling LLMs several times and then execute the generated code to obtain the final output. Then, we use two different validation methods to determine whether the LLM-generated answer aligns with the meaning specified in the aggregate functions to ensure the reliability of the answer. For questions originating from BCB, where reliable test cases are provided, we validate the generated answer by performing all test cases. Answers that pass all test cases are re-checked by humans and finally considered ground truth. As for other prompts, we initially adopt a self-consistency strategy (Wang et al., 2022) to obtain outputs and then ensure their reliability and precision by having six authors of the paper verify the default assigned prompts"}, {"title": "Function Aggregation.", "content": "To unify the key functions and improve the scalability of the evaluation, we aggregate all generated functions to the top-K function category, select top-K functions for each task type, and finally obtain 25 functions, as shown in Figure 3. Generally, K is set as 5. For example, the function category for data cleaning and preprocessing includes Data Cleaning Completeness, DataFrame Shape Validation, Data Completeness, Normalization Range Check, and Data Quality Score."}, {"title": "Programmatic Rules.", "content": "Regarding aggregate functions with corresponding codes, we define unified rules to verify generated code. Specifically, we unify all initial outputs as boolean or decimal types ranging between 0 and 1. Then, we obtain the final boolean value by comparing ground truth with prediction output depending on the specific task description of aggregate functions. For example, regarding Data Cleaning Completeness, which calculates the final number of rows/columns after preprocessing, the final output is 1 if the number is the same as the number of ground truths; otherwise, it is 0. For some specific tasks whose output type is decimal, we also set a corresponding threshold to transform the output to boolean for simplicity. For example, the threshold is set to 0.5 if the aggregate function is silhouette score for data mining and pattern recognition."}, {"title": "Summary.", "content": "Based on the above three submodules, we obtain 222 effective prompts and 519 corresponding test cases for each prompt, 25 aggregated functions, which help evaluations of 6 API-based and 17 open-sourced models."}, {"title": "4 Experiments", "content": "4.1 Settings\nWe construct a comprehensive benchmark on our collected prompts to assess the performance of 23 different models (e.g., API-based models and open-sourced general/code generation models).\n\u2022 Six API-based models include o1-mini (Jaech et al., 2024), GPT-4o-mini (Hurst et al., 2024), GPT-4o-2024-05-13, GPT-4-Turbo (Achiam et al., 2023), Claude-3.5-Sonnet\u00b2, and GLM-4-Flash (GLM et al., 2024).\n\u2022 Eight open-sourced general models include Llama3.1-8B-Instruct, Llama3-8B-Instruct, Qwen2.5-7B-Instruct, Qwen2-1.5/7B-Instruct (Yang et al., 2024), Gemma2-9B-it (Team et al., 2024), GLM-4-9B-chat, and Yi-1.5-9B-chat-16k (Young et al., 2024).\n\u2022 Nine open-sourced code models include Deepseek-Coder-1.3/6.7/33B-Instruct (Guo et al., 2024), CodeLlama-7/13/34B-Instruct (Roziere et al., 2023), Qwen2.5-Coder-1.5/7B-Instruct (Hui et al., 2024), and StarCoder2-15B (Lozhkov et al., 2024)."}, {"title": "4.2 Evaluation Metrics", "content": "Coarse-Grained Metrics. We define the coarse-grained metrics (CR and SR) for evaluating LLMs.\n\u2022 Completion Rate (CR). Following the CR in Data Interpreter (Hong et al., 2024), we calculate the CR for each TFC in R. Specifically, we give it a completion score, with a minimum score of 0 and a maximum score of 2. The step completion scores were given as follows: missing (score of 0), fail (score of 0), success-non-compliant (score of"}, {"title": "Success Rate (SR).", "content": "Similar to Codex (Chen et al., 2021), our success rate is defined as the rate of complete success on a single prompt estimated under 10 runs. Specifically, if all the TFC in R have passed within a run of a single prompt, it will count as a success. Otherwise, it will count as a failure. Note that for prompts acquired from BigCodeBench, we compare the completion function's outputs with the ground truth completion function's outputs to determine whether a single run passes, since R is derived based on demanded function outputs in this case. The formula for calculating SR is as follows:"}, {"title": "Fine-grained Aggregate Metrics.", "content": "We also define the fine-grained aggregate metrics for detail evaluating all LLMs, as shown in Figure 3 and Table 4. It's worth noting that data visualization is one of the more challenging tasks. It confirms the existence of outputs but also validates their accuracy and completeness. Specifically, for tasks like data visualization, we focus on metrics such as visualization quality and use VLM-as-a-judge to assess the quality of images. Finally, we select six representative aggregated metrics as follows:\n\u2022 VLM-as-a-judge assesses the overall score of two visual inputs based on predefined criteria for progressively adding scores (see Appendix A.10) to write the total score and provide a brief step-by-step reasoning explanation for its evaluation, with GPT-4o-mini serving as the VLM.\n\u2022 Data Quality Score (F1) in Data cleaning and preprocessing aims to assess the cleanliness of data post-processing. It yields a boolean output of 1 if it matches the ground truth or 0 otherwise.\n\u2022 Plot Validity (F2) in Data visualization pertains to the accuracy of visual representations, such as checking whether the shape of an association matrix is consistent with the ground truth. If"}, {"title": "5 Results and Analysis", "content": "5.1 Overall Performance\nWe demonstrate overall experiment results in Table 2 and Figure 4. The key findings are: 1) Concerning average performance, API-based models outperform open-sourced models. Among API-based models, GPT-4o achieves the highest total score of 64.51%, attaining a significant 9.86% advantage over GPT-4-Turbo, which achieves 54.65% total score. GPT-4o also surpasses all other models on all metrics, indicating its comprehensive capacity over various aspects. In comparison, the performance variance between API-based models is smaller than that of open-sourced models. 2) As for open-sourced models, the performance gap between general models and code generation models is insignificant. Among those, Deepseek-Coder-33B-Instruct achieves the highest score of 56.76%, even outperforming various API-based models like o1-mini and GPT-4-Turbo. Other models like Qwen2.5-Coder-7B-Instruct and Qwen2.5-7B-Instruct also show fair good capability, attaining total scores of 47.67% and 45.99%, respectively. In contrast, a few models only pass very few tasks, achieving total scores even lower than 5.0%. Of these, CodeLlama-34B-Instruct unexpectedly achieves a score of 1.33%, even lagging behind its small-scale version CodeLlama-7B-Instruct. In this regard, we also present an analysis of the anomaly in Section 5.4."}, {"title": "5.2 Study with Different Difficulty Levels", "content": "To evaluate multiple LLMs on their ability to complete prompts of varying difficulty, we categorized tasks using BCB and data formatted in CSV, human handwritten prompts, and data science-related DL tasks as easy - 167, medium 30, and hard levels - 25, respectively. We assessed multiple LLMs by combining different difficulty levels, overall average CR, and the average CR for each difficulty level. From the Figure 5, it can be observed that: 1) Some LLMs, like GPT-4o, GPT-4o-mini, GPT-4-Turbo, and Deepseek-Coder-33B-Instruct, exhibit consistent performance across all difficulty levels, indicating robustness. 2) Models such as GPT-4 series and Deepseek-Coder-Instruct series are among the top performers, scoring high average CRs, particularly excelling in complex, data-driven tasks defined as hard. 3) There are noticeable performance disparities among general models and small-scale"}, {"title": "5.3 Comparison with HumanEval", "content": "We compare our proposed DataSciBench with HumanEval. As shown in Figure 6, we observe that most LLMs are located in the upper triangular region of the graph and all tested models are divided into two groups, in which the green-dashed-line areas where LLMs perform well on the two benchmarks and the orange-solid-line area where performances on the two datasets with the same model indicate significant discrepancies."}, {"title": "5.4 Deeper Insights into LLMs' Ability", "content": "With curated metrics, we can obtain deeper research insights into LLMs' ability to plan and execute complex data science tasks. The experiment results also raise questions that are worth exploring since some results do not conform to conventional perceptions.\nModels demonstrate proficiency in reasoning tasks but may not consistently excel in complex data science tasks that necessitate precise adherence to detailed instructions, utilization of existing tools, and strategic planning. Although, indeed, data science coding tasks often involve scheduling and step-by-step execution similar to reasoning scenarios, results show that even the LLMs proficient in reasoning tasks can still fail to complete complex data science tasks. For"}, {"title": "6 Conclusion", "content": "This paper introduces DataSciBench, a novel framework tailored to assess the capabilities of LLMs in data science tasks. By meticulously curating challenging prompts and leveraging robust LLMs alongside a self-consistency strategy, we"}, {"title": "7 Limitations", "content": "In certain visualization tasks, our initial metrics and evaluation methods (e.g., VLM-as-a-judge) may lack precision. Further refinement of metrics is required to evaluate data visualization tasks effectively. One potential approach could involve employing VLMs to train critic models, enhancing the capability for fine-grained evaluations of visualizations."}, {"title": "A.1 Task Definition", "content": "We define six typical data science tasks as follows:\n1) Data cleaning and preprocessing. This task detects and processes missing values, outliers, and duplicate data; and standardizes data formats, such as a uniform format for dates and times.\n2) Data exploration and statistics understanding. This task calculates basic statistical indicators of data (mean, median, standard deviation, etc.), generates data distribution charts (histograms, box plots, etc.), calculates correlations between variables, and draws correlation matrices or maps.\n3) Data visualization. The goal of this task is to visualize and analyze data and create interactive charts so users can freely explore the data.\n4) Predictive modeling. The task aims to select the appropriate machine learning algorithm, such as linear regression, decision tree, random forest, etc.; carry out feature engineering, such as feature selection, feature transformation, feature combination, etc.; the data set is divided into the training set and test set, and the model is trained and evaluated; and select the corresponding evaluation indicators for different prediction problems, such as classification, regression or clustering.\n5) Data mining and Pattern recognition. This task uses association rule mining, frequent item set mining, and other methods to find interesting patterns in the data; Text mining technology is used to extract keywords, topics, and other information from text data; and apply cluster analysis, classification algorithms, etc. to identify underlying patterns and structures. Pattern recognition tasks can conduct these functions: image recognition, text clustering, and time series detection.\n6) Interpretability and Report generation. This task aims to provide explanations of model results, such as feature importance, model parameters, etc., and automatically generate reports and summaries that present the results of the analysis in a way that is easy to understand and share."}, {"title": "A.2 Comparison with Existing Benchmarks", "content": "We perform correlation analysis to evaluate the alignment between DataSciBench and coding evaluations like BCB and LCB. To achieve correlation analysis, we calculate both Pearson's r and Spearman's p correlation coefficients, which provide insights into the strength and direction of re-"}, {"title": "A.3 Motivation and Example of Task-Function-Code (TFC)", "content": "The TFC framework was developed to address several critical challenges in automated evaluation of data science tasks:"}, {"title": "Systematic Task Selection:", "content": "TFC provides a structured approach to identify and categorize key tasks across six established types. This systematic organization ensures comprehensive coverage of essential data science operations and helps maintain evaluation consistency and completeness."}, {"title": "Standardized Evaluation Metrics:", "content": "Data science tasks often lack standardized evaluation criteria. TFC addresses this by explicitly defining appropriate evaluation functions for each task. For example, data preprocessing tasks require specific metrics that differ from visualization tasks. This standardization ensures fair and consistent assessment."}, {"title": "Automated Execution Framework:", "content": "TFC includes executable code components for both tasks and evaluation metrics. This automation significantly improves evaluation efficiency, result reproducibility, and testing scalability."}, {"title": "Ground Truth Generation:", "content": "TFC serves as a crucial foundation for establishing ground truth, particularly valuable for complex tasks where ground truth is not readily available, and enables systematic verification and validation of model outputs."}, {"title": "A.4 Programmatic Rules", "content": "We list all programmatic rules in Table 4."}, {"title": "A.5 Caveats when Using LLMs for Data Science", "content": "Here we list the issues that occurred during test case generation, most of which have been addressed by modifying the prompts. We notice that some of the issues may be disruptive to the system and some may be subtle but important.\n1. Be careful when using LLMs on well-known open source datasets, especially with customized tasks and data split. LLMs may memorize some open-source datasets. For example, if we want to use part of the penguin dataset that does not contain certain columns, the model (GPT-4o) will still explicitly process those columns in the code.\n2. Hallucination during data pre-processing. For example, when the model is required to merge two CSV, it may hallucinate on a common column and not go through all the columns in the files to find the actual ones.\n3. On multilingual tasks. LLMs may not be able to select the correct encoding. For instance, when they are required to open a CSV file that has content in Chinese, they will struggle to choose the correct encoding to open the file. Even if they are hinted that the file is in Chinese, they may choose encodings other than \"gbk\", e.g., \u201clatin\u201d."}, {"title": "A.6 Related Works", "content": "A.6.1 LLMs for Data Science\nWith the popularity of large-scale language models, researchers have developed a series of LLM-based agents (Yao et al., 2022; Wang et al., 2024; Xu et al., 2024; Xia et al., 2024). For data science, SheetCopilot (Li et al., 2024) designs a tabular agent, which directly processes natural language-described tasks, and generates and executes a series of operation plans on datasheets to produce the desired results. Data Copilot (Zhang et al., 2024d) is an intelligent agent that serves as a bridge between users and data, which automatically executes data processing, prediction, and visualization tasks based on users' data needs. InsightPilot (Ma et al., 2023) focuses on exploratory data analysis and can automatically discover data insights related to fuzzy questions raised by users. Data interpreter (Hong et al., 2024) augments problem-solving in data science with dynamic planning with hierarchical graph structures, tool integration, and logical inconsistency identification in feedback. Furthermore, ReAct (Yao et al., 2022) is quite similar to our backbone agentic framework, DataInterpreter. Both frameworks offer foundational pipelines rather than specific evaluation metrics tailored for diverse data science"}, {"title": "Completion Rate (CR)", "content": "where the numerator was the sum of the completion scores for each step, and the denominator was the sum of the maximum possible scores for all steps (2 x T and T is the number of TFCs)."}, {"title": "Success Rate (SR).", "content": "where n = 10 and k = 1 in our case, e denotes the number of runs that have passed all TFCs in R."}, {"title": "Function Aggregation.", "content": "where N is the number of valuable task types per prompt, and this value is different for each question. Then we use the data interpreter (DI) (Hong et al., 2024) to generate a hierarchical directed acyclic graph (DAG) for each prompt, in which each task type is defined as a node at one level in a DAG, as illustrated in Figure 2. Based on the generated graphs, we take a powerful LLM as a backbone and run all evaluation functions to obtain the ground truth of each task type. To some extent, this way of verification can avoid the commonly used LLM-as-a-Judge black-box assessment."}]}