{"title": "\u03b2-DQN: Improving Deep Q-Learning By Evolving the Behavior", "authors": ["Hongming Zhang", "Fengshuo Bai", "Chenjun Xiao", "Chao Gao", "Bo Xu", "Martin M\u00fcller"], "abstract": "While many sophisticated exploration methods have been proposed, their lack of generality and high computational cost often lead researchers to favor simpler methods like \u03b5-greedy. Motivated by this, we introduce \u03b2-DQN, a simple and efficient exploration method that augments the standard DQN with a behavior function \u03b2. This function estimates the probability that each action has been taken at each state. By leveraging \u03b2, we generate a population of diverse policies that balance exploration between state-action coverage and overestimation bias correction. An adaptive meta-controller is designed to select an effective policy for each episode, enabling flexible and explainable exploration. \u03b2-DQN is straightforward to implement and adds minimal computational overhead to the standard DQN. Experiments on both simple and challenging exploration domains show that \u03b2-DQN outperforms existing baseline methods across a wide range of tasks, providing an effective solution for improving exploration in deep reinforcement learning.", "sections": [{"title": "1 INTRODUCTION", "content": "Exploration is considered as a major challenge in deep reinforcement learning (DRL) [53, 61]. The agent needs to trade off between exploiting current knowledge for known rewards and exploring the environment for future potential rewards. Despite many complex methods have been proposed for efficient exploration, the most commonly used ones are still simple methods like \u03b5-greedy and entropy regularization [5, 40, 47, 66]. Possible reasons stem from two aspects. First, these advanced methods require meticulous hyper-parameters tuning and much computational cost [2, 3, 20]. Second, these methods adopt specialized inductive biases, which may achieve high performance in specific hard exploration environments but tend to underperform simpler methods across a broader range of domains, highlighting the lack of generality [4, 11, 55].\nWe improve exploration while considering the following aspects:\n(1) Simplicity. We aim to achieve clear improvement while keeping the method simple. This ensure the method is straightforward to implement and minimizes the burden of hyper-parameters tuning.\n(2) Mild Increase in Computational Cost. While prioritizing sample efficiency in RL tasks, we aim to strike a balance that avoids substantial increase in training time. Our goal is to develop a method that is both effective and efficient. (3) Generality Across Tasks. The method should maintain generality, rather than being tailored to specific hard exploration environments.\nMotivated by these considerations, we propose \u03b2-DQN, a simple and efficient exploration method that augments the standard DQN with a behavior function \u03b2. The function \u03b2 represents the behavior policy that collects data in the replay memory\u00b9, estimating the probability that each action has been taken at each state. Combined with the Q function in DQN, we use \u03b2 for three purposes:\n(1) Exploration for state-action coverage. Taking actions with low probabilities based on \u03b2 encourages the agent to explore the state-action space for better coverage; (2) Exploration for overestimation bias correction. Exploring overestimated actions to get feedback and correct the overestimation bias in the Q function; (3) Pure exploitation. Using \u03b2 to mask the Q function at unseen actions derives a greedy policy that represents pure exploitation. Interpolating among them allows us to construct a population of temporally-extended policies that interleave exploration and exploitation at intra-episodic level with clear purposes [44]. We then design a meta-controller to adaptively select an effective policy for each episode, providing flexibility without an accompanying hyperparameter-tuning burden.\nOur method have the following advantages: (1) We only additionally learn a behavior function, which is straightforward to implement and computationally efficient compared to previous methods [2, 33]. (2) When constructing diverse polices, we do not inject inductive biases specialized for one specific task, making the method general and applicable across a wide range of domains. (3) Our method interleaves exploitation and exploration at the intra-episodic level, carries out temporal-extended exploration, and is state-dependent, which is considered the most effective approach [15, 41, 44]. We report promising results on dense-reward"}, {"title": "2 RELATED WORK", "content": "Reinforcement learning (RL) is known for learning through trial and error [17, 53]. If a state-action pair has not been encountered, it cannot be learned [44], making exploration a central challenge in RL. The most commonly used exploration strategies are simple dithering methods like \u03b5-greedy and entropy regularization [27, 40, 47, 63, 67]. While these methods are general, they are often inefficient because they are state-independent and lack temporal persistence [44, 61]. Therefore, inducing a consistent, state-dependent exploration policy over multiple time steps has been a key pursuit in the field [15, 18, 31, 41, 44, 48, 49].\nTemporally-Extended Exploration. Bootstrapped DQN [41] induces temporally-extended exploration by building K bootstrapped estimates of the Q-value function in parallel and sampling a single Q function for the duration of one episode. The computational cost increases linearly with the number of heads K. Temporally-extended \u03b5-Greedy (\u03b5\u03c4-greedy) [15] simply repeats the sampled random action for a random duration, but its exploration remains state-independent. Our method, based on the behavior function \u03b2, induces temporally-extended exploration by selecting actions according to state-dependent probabilities.\nBesides adding temporal persistence to the exploration policy explicitly, another line of work involves adding exploration bonuses to the environment reward [7, 26, 30, 37, 38, 42, 51, 56, 68]. These bonuses are designed to encourage the agent to visit states that are novel. The count-based exploration bonus encourages the agent to visit states with low visit counts [7, 42, 56]. Prediction error-based methods, such as the Intrinsic Curiosity Module (ICM) [43] and Random Network Distillation (RND) [3, 11], operate on the intuition that the prediction error will be low on states that are previously visited and high on newly visited states. These methods are designed to tackle difficult exploration problems and usually perform well in hard exploration environments. However, they often underperform compared to simple methods like \u03b5-greedy in easy exploration environments [55], highlighting their lack of generality. In contrast, our method is designed to be general and applicable across a wide range of tasks.\nPopulation-based Exploration. Recent promising works tried to handle the exploration problem using population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies [1, 2, 19, 20, 32, 33, 52]. These methods have demonstrated powerful performance, outperforming the standard human benchmark on all 57 Atari games [8]. They maintain a group of actors with independent parameters, build distributed systems, and interact with the environment around billions of frames. Although these methods achieve significant performance gains, the computational cost is substantial and often unaffordable for most research communities. This has the unfortunate side effect of widening the gap between those with ample access to computational resources and those without [12]. Our method introduces a population of diverse policies with minimal computational overhead, making it accessible to most researchers. Our goal is to absorb"}, {"title": "3 BACKGROUND", "content": "Markov Decision Process (MDP). Reinforcement learning (RL) [53] is a paradigm of agent learning via interaction. It can be modeled as a Markov Decision Process (MDP) M = (S, A, R, P, \u03c10, \u03b3). S is the state space, A is the action space, P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the environment transition dynamics, R : S \u00d7 A \u00d7 S \u2192 R is the reward function, \u03c10 : S \u2192 R is the initial state distribution and \u03b3 \u2208 (0, 1) is the discount factor. The goal of the agent is to learn an optimal policy that maximizes the expected discounted cumulative rewards E[\u2211t\u03b3trt].\nDeep Q-Network (DQN). Q-learning is a classic algorithm to learn the optimal policy. It learns the Q function with Bellman optimality equation [9], Q\u2217(s, a) = E[r + \u03b3 maxa\u2032 Q\u2217(s\u2032, a\u2032)]. An optimal policy is then derived by taking an action with maximum Q value at each state. DQN [40] scales up Q-learning by using deep neural networks and experience replay [36]. It stores transitions in a replay memory and samples batches of that data uniformly to estimate an action-value function Q\u03b8 with temporal-difference (TD) learning:\nQ\u03b8(s, a) \u2190 r(s, a) + \u03b3 maxa\u2032 Q\u03b8 (s\u2032, a\u2032)."}, {"title": "4 METHOD", "content": "Drawing from insights introduced in Section 2, promising exploration strategies should be state-dependent, temporally-extended, and consist of a set of diverse policies. Keeping simplicity and generality in mind, we design an exploration method for DQN that performs well across a wide range of domains and is computationally affordable for the research community. We additionally learn a behavior function \u03b2 and construct a set of policies that balance exploration between state coverage and bias correction. A meta-controller is then designed to adaptively select a policy for each episode. In Section 4.1, we introduce how to learn the behavior function \u03b2 and augment it with the Q function for three purposes: exploration for state-action coverage, exploration for overestimation bias correction, and pure exploitation. In Section 4.2, we derive a set of policies by interpolating exploration and exploitation at the intra-episodic level. Based on this policy set, we design an adaptive meta-controller in Section 4.3 to choose an effective policy for interacting with the environment in each episode. Figure 1 provides an overview of our method."}, {"title": "4.1 Behavior Function \u03b2", "content": "Learning the behavior function \u03b2 is straightforward. We sample a batch of data B from the replay memory and train a network using supervised learning with cross entropy loss:\nL\u03b2 = \u22121/|B| \u2211(s,a)\u2208B log \u03b2(s, a)."}, {"title": "4.2 Constructing Policy Set", "content": "Previous work [44] has shown that intra-episodic exploration, i.e., changing the mode of exploitation and exploration in one episode, is the most promising diagram. With the three policies in Equations (3), (4) and (6) with clear purposes, we interpolate exploration and exploitation at the intra-episodic level to construct a set of diverse policies that balance exploration between state coverage and overestimation bias correction."}, {"title": "4.3 Meta-Controller for Policy Selection", "content": "After constructing a set of policies, we need to select an effective policy to interact with the environment for each episode. Similar to previous work [2, 19, 20, 33], we consider the policy selection problem as a non-stationary multi-armed bandit (MAB) [24, 35], where each policy in the set is an arm. We design a meta-controller to select policies adaptively.\nAssume there are N policies in the policy set \u03a0 = {\u03c00, \u00b7 \u00b7 \u00b7 , \u03c0N\u22121}. For each episode k \u2208 N, the meta-controller selects a policy Ak = \u03c0i and receives an episodic return Rk(Ak). Our objective is to obtain a policy that maximizes the return within a given interaction budget K.\nDue to the non-stationarity of the policies, we consider the recent L < K results. Let Nk(\u03c0i, L) be the number of times policy \u03c0i has been selected after k episodes, and \u03bck(\u03c0i, L) be the mean return \u03c0i obtained by \u03c0i. We design a bonus b to encourage exploration. An action is considered exploratory if it differs from the pure exploitation action taken by Equation (6). The exploration bonus for policy \u03c0i is computed as:\nbk(\u03c0i, L) = 1/Nk(\u03c0i, L) \u2211m=max(0,k\u2212L)Bk(\u03c0i)I(Am = \u03c0i),\nwhere Bm(\u03c0i) computes the ratio of exploration actions taken by policy \u03c0i at episode m and I(\u00b7) is the indicator function.\nTo select a policy for episode k, we consider the return and exploration bonus of each policy within the sliding window L:\nAk = {\u03c0i, if Nk(\u03c0i, L) = 0, arg max\u03c0i (\u03bck(\u03c0i, L) + bk(\u03c0i, L)), otherwise.\nIn this formula, if a policy has not been selected in the last L episodes, we will prioritize selecting that policy. Otherwise, the policy that explores more frequently and also gets higher returns is preferred. Algorithm 1 summarizes our method."}, {"title": "5 EXPERIMENTS", "content": "In this section, we aim to answer the following questions:\n\u2022 Does our method lead to diverse exploration, thereby enhancing the learning process and overall performance?\n\u2022 Can \u03b2-DQN improve performance in both dense and sparse reward environments while maintaining a low computational cost?\n\u2022 How do the exploration policies cov and cor contribute to the learning process in different environments?\n\u2022 Is there a difference when learning Q with constraints imposed by \u03b2 compared to without such constraints?"}, {"title": "5.1 A Toy Example", "content": "In this section, we present a toy example using the CliffWalk environment [53], to illustrate the policy diversity and the learning efficacy of our method. The Cliff Walk environment comprises 48 states and 4 actions, as depicted in Figure 2. Starting from the bottom left, the goal is to navigate to state G located at the bottom right. Reaching G yields a reward of 1, while falling into the cliff incurs a penalty of -1; all other moves have a reward of 0.\nFor a clear illustration of policy diversity, we design a scenario with only one suboptimal trajectory in the replay memory (the left image in Figure 2(a)). The function \u03b2, learned from this trajectory, assigns a probability of 1 to the only existing action at each state, while assigning zero probabilities to other actions. The second and third images in Figure 2(a) show the action values masked/unmasked by \u03b2, and the corresponding actions taken at each state. This gives us a clear understanding of what actions are taken by different policies introduced in Section 4.1.\nIn Figure 2(b), by assigning different values to \u03b4 and \u03b1, we generate a group of diverse policies. Each policy takes different actions, leading to novel states or those with biased estimates. Colors are used to differentiate actions and clearly illustrate policy diversity. This indicates that our method can create a diverse set of policies by simply interpolating between exploration and exploitation policies."}, {"title": "6 CONCLUSION", "content": "In this paper, we enhance exploration by constructing a group of diverse polices through the additional learning of a behavior function \u03b2 from the replay memory using supervised learning. With \u03b2, we create a set of exploration policies that range from exploration for state-action coverage to overestimation bias correction. An adaptive meta-controller is then designed to select the most effective policy for interacting with the environment in each episode. Our method is simple, general, and adds minimal computational overhead to DQN. Experiments conducted on MinAtar and MiniGrid demonstrate that our method is effective and broadly applicable in both easy and hard exploration tasks. Future work could extend our method to environments with continuous action spaces."}]}