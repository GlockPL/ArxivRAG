{"title": "Regression Conformal Prediction under Bias", "authors": ["Matt Y. Cheung", "Tucker J. Netherton", "Laurence E. Court", "Ashok Veeraraghavan", "Guha Balakrishnan"], "abstract": "Uncertainty quantification is crucial to account for the imperfect predictions of machine learning\nalgorithms for high-impact applications. Conformal prediction (CP) is a powerful framework for un-\ncertainty quantification that generates calibrated prediction intervals with valid coverage. In this work,\nwe study how CP intervals are affected by bias the systematic deviation of a prediction from ground\ntruth values a phenomenon prevalent in many real-world applications. We investigate the influence of\nbias on interval lengths of two different types of adjustments - symmetric adjustments, the conventional\nmethod where both sides of the interval are adjusted equally, and asymmetric adjustments, a more flex-\nible method where the interval can be adjusted unequally in positive or negative directions. We present\ntheoretical and empirical analyses characterizing how symmetric and asymmetric adjustments impact\nthe \"tightness\" of CP intervals for regression tasks. Specifically for absolute residual and quantile-based\nnon-conformity scores, we prove: 1) the upper bound of symmetrically adjusted interval lengths increases\nby 2|b| where b is a globally applied scalar value representing bias, 2) asymmetrically adjusted interval\nlengths are not affected by bias, and 3) conditions when asymmetrically adjusted interval lengths are\nguaranteed to be smaller than symmetric ones. Our analyses suggest that even if predictions exhibit\nsignificant drift from ground truth values, asymmetrically adjusted intervals are still able to maintain\nthe same tightness and validity of intervals as if the drift had never happened, while symmetric ones\nsignificantly inflate the lengths. We demonstrate our theoretical results with two real-world prediction\ntasks: sparse-view computed tomography (CT) reconstruction and time-series weather forecasting. Our\nwork paves the way for more bias-robust machine learning systems.", "sections": [{"title": "1 Introduction", "content": "With the growing application of deep learning algorithms to high-impact applications such as healthcare,\nfinance, and climate science, it is equally crucial to develop methods that can robustly quantify their un-\ncertainties. This is particularly important since deep learning algorithms are known to yield confident yet\nincorrect prediction values [14, 48, 27]. Given a prediction by a learning algorithm on a fresh test example,\nuncertainty quantification methods typically aim to return a prediction set with some guarantee that the\ntrue value lies within that set. In particular, a prediction set for a regression problem consists of an interval\nwith lower and upper bounds [21, 34].\nConformal Prediction (CP) is a powerful family of uncertainty quantification methods that is distribution-\nagnostic, i.e., makes no assumptions about the underlying data distribution, and generates prediction sets\nwith guarantees of containing ground truth values with some probability [2, 13, 38, 29]. For example, split\nCP is based on collecting a separate (from training) calibration dataset containing both ground truth and\npredicted values from the algorithm of interest. Then, given the algorithm's prediction on a test example,\nsplit CP computes a non-conformity score quantifying how \u201cunusual\u201d new predictions will be with respect\nto the calibration dataset, and adjusts the prediction via the empirical quantile of the calibration scores to\ngenerate a prediction set."}, {"title": "2 Background: Split Conformal Prediction (CP)", "content": "We focus on a \"split\" CP setup [29, 21] in this work, but the same theoretical analysis can be applied to other\nCP forms and extensions [13, 4]. In split CP, we assume a calibration dataset $D_c = \\{(Y_1, \\hat{Y}_1), ..., (Y_n, \\hat{Y}_n)\\}$\nand test point $\\hat{Y}_{n+1}$, where $Y_i$ and $\\hat{Y}_i$ represent the i-th prediction and ground truth values. The calibration\ndata is separate from a training dataset used to train the ML algorithm of interest. The calibration dataset\nand test point are assumed to be exchangeable. The goal of CP is to construct a prediction interval $C(\\hat{Y}_{n+1}) =$\n$[L(\\hat{Y}_{n+1}), U(\\hat{Y}_{n+1})]$ for $\\hat{Y}_{n+1}$, where $L(\\hat{Y}_{n+1}), U(\\hat{Y}_{n+1}) \\in R$ are lower and upper bounds, such that $P[Y_{n+1} \\in$\n$C(\\hat{Y}_{n+1})] \\geq 1 \u2013 \\alpha$, for some user-specified mis-coverage rate $\\alpha \\in (0,1)$. To compute symmetric intervals, we\nperform the following steps. First, for each data point in the calibration set $D_c$, we compute non-conformity\nscores $S = \\{s_1,...,s_n\\}$. Next, we compute the $(1 \u2013 \\alpha)$-th empirical quantile of the non-conformity scores\n$q = Q_{1-\\hat{\\alpha}}(S)$, where $\\hat{\\alpha} = \\frac{\\lceil \\alpha(n+1) \\rceil}{n+1}$ denotes the finite-sample adjusted mis-coverage rate. Finally, we adjust\nthe predictions of the test data using $q$ to achieve valid prediction sets. This algorithm provides marginal\ncoverage: on average, the prediction sets contain ground truth $(1 - \\alpha)\\%$ of the time. More rigorously, based\non key CP results:\nLemma 1 Let $(\\hat{Y}_i, Y_i) \\in R \\times R, i = 1, ..., n + 1$ be exchangeable random variables. Assume that a predictor f\nhas been trained on a proper training set independent of and exchangeable with these n + 1 points. Consider\na calibration set $(\\hat{Y}_i, Y_i)_{i=1}^n$ and a fresh test point $\\hat{Y}_{n+1}$. Let $s_i$ be a non-conformity score computed using the\npredictor f for $i = 1, ..., n + 1$. Let $q = Q_{1-\\hat{\\alpha}}(\\{s_i\\}_{i=1}^n)$ be the $(1 \u2013 \\alpha)\\frac{(n + 1)}{n+1}$-th smallest value of $\\{s_i\\}_{i=1}^n$\nand $C(\\hat{Y}_{n+1}) = \\{y \\in R : s_{n+1} \\leq q\\}$ be the prediction set for the test point $\\hat{Y}_{n+1}$. Then, for any $\\alpha \\in (0,1)$:\n1. $P[Y_{n+1} \\in C(\\hat{Y}_{n+1})] \\geq 1 - \\alpha$ and\n2. $P[Y_{n+1} \\in C(\\hat{Y}_{n+1})] \\leq 1 - \\alpha + \\frac{1}{n+1}$ if random variables $Y_1,..., Y_{n+1}$ are almost surely distinct\nMany non-conformity scores exist [17], including absolute residuals (L1) [29] and quantile-based (Confor-\nmalized Quantile Regression or CQR [34]) scores.\nThis can be extend to asymmetric adjustments [23, 34, 9] by computing $(1 \u2013 \\alpha_{lo})$-th and $(1 \u2013 \\alpha_{hi})$-th\nempirical quantiles of the conformity scores, where $\\alpha_{lo}$ and $\\alpha_{hi}$ are lower and upper mis-coverage rates. In\nthe asymmetric case, the empirical quantiles for the lower and higher mis-coverage rates $\\hat{\\alpha}_{lo}$ and $\\hat{\\alpha}_{hi}$ are\ngiven by $\\hat{\\alpha}_{lo} = \\frac{\\lceil \\alpha_{lo}(n+1) \\rceil}{n+1}$ and $\\hat{\\alpha}_{hi} = \\frac{\\lceil \\alpha_{hi}(n+1) \\rceil}{n+1}$. The proof of validity relies on applying Lem. 1 twice\n[23, 34]. It is easy to see that when $\\alpha_{lo} + \\alpha_{hi} = \\alpha$ the asymmetric case yields empirically larger coverage"}, {"title": "3 Theoretical Analysis", "content": "We assume biased predictions $\\hat{Y} = \\hat{Y}^0 + b$ where $b \\in R$ is a constant (positive or negative) applied between\nall unbiased predicted values ($\\hat{Y}^0$) and ground truth values ($Y$) from the calibration set. For example, for\nprediction and ground truth distributions that are symmetric and centered around their means, we can\nuse Eq. 1. However, when the distributions are skewed, the mean may no longer be a good measure of\ncentral tendency [35, 15]. In Sec. 3.1, we will use results from our theoretical analyses to estimate bias more\naccurately in these cases.\nWe consider symmetric non-conformity scores with canonical expression:\n$s = max(f_{lo}(\\hat{Y}) \u2013 Y_i, Y_i \u2013 f_{hi}(\\hat{Y})),$ (2)\nwhere $f_{lo}$ and $f_{hi}$ are the lower adjustment and upper adjustment functions that have linear properties:\n$f_{lo}(\\hat{Y}) = f_{lo}(\\hat{Y}^0) + b$ and $f_{hi} (\\hat{Y}) = f_{hi}(\\hat{Y}^0)+b$. Eq. 2 covers the conventional L\u2081 and CQR non-conformity\nscores. For the L\u2081 non-conformity score given by $s = |Y_i - \\hat{Y}|$, $\\hat{Y}$ represents a point estimate, and\nthe score can be rewritten as $s = max(\\hat{Y} \u2013 Y, Y \u2013 \\hat{Y})$. For the CQR non-conformity score given by\n$s = max(Q_{\\alpha_{lo}} (\\hat{Y}) - Y_i, Y_i - Q_{1-\\alpha_{hi}} (\\hat{Y}))$, $\\hat{Y}$ represents a set of samples $\\hat{Y} = \\{Y_i\\}_{i=1}^n$. The adjustment is\ngiven by $q = Q_{1-\\hat{\\alpha}}(\\{s_i\\}_{i=1}^n)$, the prediction interval is given by $C(\\hat{Y}_{n+1}) = [f_{lo} (\\hat{Y}_{n+1}) \u2013 q, f_{hi}(\\hat{Y}_{n+1}) + q]$,\nand the interval length is given by $L_{sym}(\\hat{Y}_{n+1}) = f_{hi} (\\hat{Y}_{n+1}) \u2013 f_{lo}(\\hat{Y}_{n+1}) + 2q$. This setup does not cover\nlocally adaptive non-conformity scores [30, 31, 21] and variations of CQR such as CQR-r and CQR-m non-\nconformity scores [36]. Using Eq. 2, we first derive an upper bound for symmetrically adjusted interval\nlengths under bias (Thm. 2):\nTheorem 2 Given biased predictions for a fresh test point $\\hat{Y}_{n+1} = \\hat{Y}_{n+1}^0+b$, the upper bound on prediction\ninterval lengths of non-conformity scores described in Eq. 2 is:\n$L_{sym}(\\hat{Y}_{n+1}) \\leq L_{sym}(\\hat{Y}_{n+1}^0) +2|b|,$ (3)\nwhere $L_{sym}(\\hat{Y}_{n+1})$ and $L_{sym}(\\hat{Y}_{n+1}^0)$ are the interval lengths computed using symmetric adjustments for pre-\ndictions with and without bias.\nWe find the upper bounds of symmetrically adjusted interval lengths increase linearly with the magnitude\nof bias. Next, we show 1) that asymmetric adjustments are not affected by bias and 2) conditions when\nusing asymmetric adjustments produce shorter lengths than symmetric adjustments. To accomplish this, we\nintroduce a similar canonical expression for asymmetric non-conformity scores:\n$(s_{ilo}, s_{ihi}) = (f_{lo}(\\hat{Y}) \u2013 Y_i, Y_i \u2013 f_{hi} (\\hat{Y})),$ (4)\nwhere $s_{ilo}$ and $s_{ihi}$ represent lower and upper non-conformity score adjustments when predictions are biased\nwith b. The lower and upper asymmetric adjustments are computed by taking the $(1-\\alpha_{lo})$-th and $(1-\\alpha_{hi})$-th\nempirical quantile of the sets of non-conformity scores $q_{lo} = Q_{1-\\tilde{\\alpha}_{lo}} (\\{s_{ilo}\\}_{i=1}^n)$ and $q_{hi} = Q_{1-\\tilde{\\alpha}_{hi}} (\\{s_{i,hi}\\}_{i=1}^n)$.\nThus, the prediction interval is $C(\\hat{Y}_{n+1}) = [f_{lo}(\\hat{Y}_{n+1}) \u2013 q_{lo}, f_{hi}(\\hat{Y}_{n+1}) + q_{hi}]$.\nUsing this setup, we prove the following relationship for the length of a CP prediction interval using\nasymmetric non-conformity scores, under bias b:\nTheorem 3 Given biased predictions for a fresh test point $\\hat{Y}_{n+1} = \\hat{Y}_{n+1}^0 + b$, the lengths for L1 and CQR\nnon-conformity scores computed using asymmetric adjustments are bias-independent:\n$L_{asym}(\\hat{Y}_{n+1}) = L_{asym}(\\hat{Y}_{n+1}^0)$ (5)"}, {"title": "3.1 Estimating bias", "content": "One unanswered question is how to empirically determine bias b given data. To do so, we leverage Thm. 2\nand adjust the predictions by a scalar value $b_{eff}$ to minimize the maximum symmetrically adjusted interval\nlengths:\n$b_{eff} = argmin  max(\\left\\{L_{sym}(\\hat{Y}^b - C)\\right\\}_{i=1}^n  )$ (7)\n$L_{sym}(\\hat{Y}^b - b_{eff})$ achieves the minimum length for symmetric adjustments (when b = 0). $b_{eff}$ can be\nthought of as the \u201cdebiasing\u201d constant for biased predictions $\\hat{Y}^b$. We prove that the objective function in\nEq. 7 reduces to minimizing a vertically and horizontally translated absolute value function (App. A.2.1).\nTherefore, the objective function is convex, and Alg. 1 converges using gradient descent and its variants.\nFor our experiments, we implemented a PyTorch version for CQR-based and L\u2081 scores available at https:\n//github.com/matthewyccheung/conformal-metric, and optimize using AutoGrad [32]."}, {"title": "4 Experiments", "content": "We next evaluate Thm. 2 and 3 and Cor. 3.1 for L\u2081 and CQR non-conformity scores with synthetic and\nreal-life experiments. For synthetic experiments, we assume normally distributed ground truth data, and\nsimulate predictions by adding different types of noise. For real-life experiments, we consider two scenarios:\nwhen data is scarce (CT reconstructions for downstream radiotherapy planning using CQR) and when data\nis temporally varying (time series weather forecasting using L\u2081). The data distributions can be found in\nApp. B."}, {"title": "4.1 Synthetic Data", "content": "We first demonstrate the validity of the theoretical analysis Sec. 3 for CQR using Gaussian N and Weibull\nW distributions to simulate estimate, ground truth, and noise distributions. We used N(10,5) to simulate\na ground truth distribution. We added noise characterized by W(1,0,5), N(0,2), and -W(1, \u22122,5) to the\nground truth samples to simulate left-, no-, and right-skewing predictions. We used 1000 calibration data\npoints, 1000 test data points, and 1000 samples per data point to estimate the quantiles. We set \u03b1 = 0.1\nfor symmetric adjustments, and \u03b1lo = \u03b1hi = 0.05 for asymmetric adjustments. After determining $b_{eff}$ using\nAlg. 1, we added a constant bias term from -2 to 2 to the debiased predictions to examine the effect of\nbiased predictions on lengths."}, {"title": "4.2 Real Data", "content": "Next, we validate our theoretical analyses in two different real data scenarios: where upstream image recon-\nstruction tasks may not fully capture spatial dependencies in downstream metrics (sparse-view computed"}, {"title": "4.2.1 Limited Data", "content": "In scenarios with limited imaging capabilities, such as low-resource clinics [1, 10, 18], reconstruction algo-\nrithms work with observations that do not contain complete information. For example, sparse cone-beam\nCT algorithms use limited (< 100 instead of the standard 100s) 2D X-ray observations to generate 3D CT\nscans [41, 51, 39]. The observed information is insufficient to recover the true image with complete certainty,\nleading to potential biases such as systematically over- or under-estimating organ volumes.\nWe simulate a medical imaging pipeline, where a patient is imaged using sparse-CT, an image reconstruc-\ntion algorithm is applied to the projections, and the resulting volume is used for downstream radiotherapy\\planning (RT). We use Neural Attenuation Fields (NAF) [53], a self-supervised image reconstruction algo-\nrithm. We synthetically injected noise to the projections, reconstructed the volumes using different initial-\nizations of the reconstruction algorithm, and generated plans using the Radiation Planning Assistant (RPA,\nFDA 510(k) cleared) 1. More details about our experimental setup can be found in App. C.\nTo validate our theoretical analysis in Sec. 3 holds true even for extremely low n, we use 19 patients for\ncalibration and 1 patient for testing. We generate 10 reconstructions per patient by perturbing acquisition\nangles, injecting noise into the projections, and using random initializations of NAF. We perform leave-\none-out cross-validation to examine each patient's interval lengths when calibrated with the rest of the\npatients. We use \u03b1 = 0.15 for symmetric adjustments and \u03b1lo = \u03b1hi = 0.075 for asymmetric adjustments,\ncorresponding to $\\hat{\\alpha} = 0.0567$ for the symmetric case and $\\hat{\\alpha}_{lo} = \\hat{\\alpha}_{hi} = 0$ for the asymmetric case. In the\nasymmetric case, this corresponds to an extreme case of taking the maximum and minimum non-conformity\nscores."}, {"title": "4.2.2 Times series", "content": "Weather forecasting is important for many aspects of daily life, from public safety to agriculture to disaster\npreparedness and response. We use the Yandex Weather Prediction dataset and the average pre-trained\nCatBoost model from [2] to predict temperature changes. The temporal dependencies between points\nviolate the exchangeability assumption. Therefore, we use weighted conformal prediction where we use a\ndifferent adjustment for each new data point [43]. We use the L\u2081 non-conformity score and weight the data\npoints in the window of size K = 1000 equally. This setup effectively reduces to split CP applied each at\ntime window and the theoretical analyses in Sec. 3 apply. The symmetric non-conformity score for time t\nis given by $s_t = |\\hat{Y} \u2013 Y^0|$. The asymmetric non-conformity scores for time t are given by $s^0_{t} = Y - \\hat{Y}$\nand $s^{hi}_{t} = \\hat{Y} - \\hat{Y}^0$. We set \u03b1 = 0.1 and inject an increasing negative bias to the unbiased predicted values\n$\\hat{Y} = \\hat{Y}^0 \u2013 (2 \\times 10^{-4})t$. We plot temperature over time for predictions $\\hat{Y}^b$ and ground truth Y,\ncoverage over time for weighted (symmetric and asymmetric adjustments) and naive (unweighted, symmetric\nadjustments) CP, symmetrically (red) and asymmetrically (blue) adjusted intervals and where they\noverlap, and bias versus lengths for symmetric adjustments, lengths for asymmetric\nadjustments, and upper bound lengths for symmetric adjustments from Thm. 2.\nWe observe that symmetric and asymmetric adjustments produce valid coverage while naive approaches\ndo not, confirming prior work [2, 5]. We observe that asymmetric adjustments are independent of\nbias yet still produce valid prediction intervals. We observe that symmetrically adjusted interval\nlengths increase linearly with increasing bias, bounded by Thm. 2. Our results suggest that even\nif predictions drift \"far away\" from the ground truth values, asymmetrically adjusted intervals are still able\nto maintain the same tightness and validity of intervals as if the drift had never happened."}, {"title": "5 Discussion and Conclusion", "content": "We will never collect perfect data in practice, or build perfect predictive models that are robust over time.\nTherefore, it is integral to account for these imperfections when designing practical systems. In this work, we\nargue that the effects of bias on CP prediction interval lengths can be mitigated by computing asymmetric\nadjustments as opposed to the conventional symmetric adjustments. We prove the following for L\u2081 and\nCQR non-conformity scores. In Thm. 2 we showed that the upper bound of the prediction interval lengths\nwith symmetric adjustments increases by 2|b|. In Thm. 3, we showed that prediction interval lengths with\nasymmetric adjustments are not affected by bias. In Cor. 3.1, we showed the conditions when prediction\ninterval lengths with asymmetric adjustments are guaranteed to be smaller than those of symmetric intervals.\nWe proposed an algorithm to empirically determine the bias and showed empirical evidence using synthetic\nand real-life data. Our results have have important implications on accounting for algorithmic bias, while\nalso suggesting further areas of investigation:\nStability of bias estimation for low n. Our work suggests that estimating bias based on symmetrically\nadjusted intervals is a straightforward, practical, and computationally efficient way to account for systematic\nerrors in predictions. However, it is crucial to consider the impact of sample size on the reliability of these"}, {"title": "A Proofs", "content": ""}, {"title": "A.1 Proof of Theorem 2", "content": "Using Eq. 2, we show the behavior of prediction interval lengths with symmetric adjustments when pre-\ndictions are biased $\\hat{Y} = \\hat{Y}^0 + b$ where $b \\in R$. We show the behavior of symmetric adjustments under 1)\nno bias, 2) large negative bias, 3) large positive bias, 4) small negative bias and 5) small positive bias, and\ncompare the resulting interval lengths. We leverage a property of quantiles $Q_\\alpha(\\hat{Y} + b) = Q_\\alpha(\\hat{Y}) + b$ where b\nis a scalar value.\nWhen b = 0, we can write the adjustment , the prediction interval , and the prediction\ninterval length .\n$s^0 = max(f_{lo}(\\hat{Y}) \u2013 Y_i, Y_i \u2013 f_{hi}(\\hat{Y}))$ (8)\n$q^0 = Q_{1-\\hat{\\alpha}}(\\{s_i\\}_{i=1}^n)$ (9)\n$C_{sym}(\\hat{Y}_{n+1}) = [f_{lo} (\\hat{Y}_{n+1}) \u2013 q^0, f_{hi}(\\hat{Y}_{n+1}) + q^0]$ (10)\n$L_{sym}(\\hat{Y}_{n+1}) = f_{hi} (\\hat{Y}_{n+1}) \u2013 f_{lo}(\\hat{Y}_{n+1}) + 2q^0$ (11)\nFor biased predictions, the 0 is replaced with a b.\nNext, we examine when predictions are highly biased in the negative direction $\\hat{Y}^{--} = \\hat{Y}^0 + b^{--}$ where\n$b^{--} < Y_i \u2013 f_{hi}(\\hat{Y}^{--}) < 0$, so $Y_i > f_{hi}(\\hat{Y}^{--}) \\geq f_{lo}(\\hat{Y}^{--})$. The non-conformity score reduces to the\n$Y_i - f_{hi}(\\hat{Y}^{--})$ because $f_{lo}(\\hat{Y}^{--}) - Y_i < 0$ and can be written as:\n$s^b = Y_i - f_{hi}(\\hat{Y}^{--}) = Y_i \u2013 f_{hi} (\\hat{Y}^0) \u2013 b^{--} < s^0 \u2212 b^{--}$ (12)\nThe adjustment can be written as:\n$q^{--} = Q_{1-\\hat{\\alpha}}(\\{s_i\\}_{i=1}^n) < Q_{1-\\hat{\\alpha}}(\\{s_i\\}_{i=1}^n) - b^{--} = q^0 \u2212 b^{--}$ (13)"}, {"title": "A.2 Proof of Theorem 3", "content": "We analyze the behavior of asymmetric adjustments under bias. We model the biased predictions as $\\hat{Y} =$\n$Y+ b$ where b is a global constant (can be both positive and negative) added to unbiased predictions $\\hat{Y}^0$.\nFirst, the lower and upper scores can be written as:\n$s^b_{ilo} = f_{lo} (\\hat{Y}) - Y_i = f_{lo}(\\hat{Y}^0) \u2013 Y_i + b = s^0_{i,lo} + b$ (22)\n$s^b_{ihi} = Y_i - f_{hi}(\\hat{Y}) = Y_i \u2013 f_{hi} (\\hat{Y}^0) \u2013 b = s^0_{ihi} - b$ (23)\nNext, the lower and upper adjustments can be written as:\n$q^b_{lo} = Q_{1-\\alpha_{lo}} (\\{s^b_{ilo}\\}_{i=1}^n) = Q_{1-\\alpha_{lo}} (\\{s^0_{i,lo}\\}_{i=1}^n) + b = q^0_{L} + b$ (24)\n$q^b_{hi} = Q_{1-\\alpha_{hi}}(\\{s^b_{ihi}\\}_{i=1}^n) = Q_{1-\\alpha_{hi}} (\\{s^0_{i,hi}\\}_{i=1}^n) - b = q^0_{hi} - b$ (25)"}]}