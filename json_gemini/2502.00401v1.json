{"title": "SPECTRO-RIEMANNIAN GRAPH NEURAL NETWORKS", "authors": ["Karish Grover", "Haiyang Yu", "Xiang Song", "Qi Zhu", "Han Xie", "Vassilis N. Ioannidis", "Christos Faloutsos"], "abstract": "Can integrating spectral and curvature signals unlock new potential in graph rep-resentation learning? Non-Euclidean geometries, particularly Riemannian mani-folds such as hyperbolic (negative curvature) and spherical (positive curvature),offer powerful inductive biases for embedding complex graph structures likescale-free, hierarchical, and cyclic patterns. Meanwhile, spectral filtering excelsat processing signal variations across graphs, making it effective in homophilicand heterophilic settings. Leveraging both can significantly enhance the learnedrepresentations. To this end, we propose Spectro-Riemannian Graph Neural Net-works (CUSP) \u2013 the first graph representation learning paradigm that unifies bothCUrvature (geometric) and SPectral insights. CUSP is a mixed-curvature spec-tral GNN that learns spectral filters to optimize node embeddings in products ofconstant-curvature manifolds (hyperbolic, spherical, and Euclidean). Specifically,CUSP introduces three novel components: (a) Cusp Laplacian, an extension of thetraditional graph Laplacian based on Ollivier-Ricci curvature, designed to capturethe curvature signals better; (b) Cusp Filtering, which employs multiple Rieman-nian graph filters to obtain cues from various bands in the eigenspectrum; and(c) Cusp Pooling, a hierarchical attention mechanism combined with a curvature-based positional encoding to assess the relative importance of differently curvedsubstructures in our graph. Empirical evaluation across eight homophilic and het-erophilic datasets demonstrates the superiority of CUSP in node classification andlink prediction tasks, with a gain of up to 5.3% over state-of-the-art models.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph representation learning has garnered significant research interest in recent years, owing to itsfundamental relevance in domains such as natural language processing (Mihalcea & Radev, 2011),biology (Zhang et al., 2021a), and social network analysis (Grover et al., 2022). Recent advanceshave rigorously examined constant curvature spaces to learn distortion-free graph representations, asthey provide suitable inductive biases for particular structures while avoiding the intrinsic problems of very high dimensionality. For example, hyperbolic space (negative curvature) is optimal forhierarchical tree-like graphs (Chami et al., 2019), while spherical geometry (positive curvature)is best suited for cyclic graphs (Gu et al., 2019).The idea behind all Riemannian GNNs is thatoptimal embeddings are achieved when the underlying manifold's curvature aligns with the graphs'discrete curvature. To model real-world graphs with complex topologies that cannot be adequatelyrepresented within a single constant-curvature manifold, various mixed-curvature GNNs have beenproposed, that operate across multiple manifolds (Zhu et al., 2020).\nDespite their success in managing complex graph topologies, existing mixed-curvature GNNs stillhave significant limitations (Figure 1 (b)). (a) L1 (Low-pass filtering bias): State-of-the-art mixed-curvature Riemannian GNNs, such as KGCN (Bachmann et al., 2020) and QGCN (Xiong et al.,2022), inherently mimic low-pass spectral filters. These models are adaptations of the EuclideanGCN (Kipf & Welling, 2016) to Riemannian manifolds, which are known to operate under the ho-mophily assumption (low-pass filters) predominantly. Consequently, their performance degrades ongraphs with varying degrees of heterophily. (b) L2 (Task-specific relevance of curvature): Different"}, {"title": "2 RELATED WORKS", "content": "Riemannian Geometry in Graph Neural Networks. Graph Neural Networks (GNNs) have setnew benchmarks for tasks like node classification and link prediction. Recently, non-Euclidean(Riemannian) spaces \u2013 particularly hyperbolic (Sala et al., 2018) and spherical (Liu et al., 2017;Wilson et al., 2014) geometries \u2013 have garnered attention for their ability to produce less distortedrepresentations, aligning well with hierarchical and cyclic data structures, respectively. Several approaches have emerged in this context. (a) Single Manifold GNNs: GNNs such as HGAT (Zhanget al., 2021b), HGCN (Chami et al., 2019), and HVAE (Sun et al., 2021) have demonstrated state-of-the-art performance on tree-like or hierarchical graphs by learning representations in hyperbolicspace. (b) Mixed-Curvature GNNs: To model more complex topologies (for example, a tree branch-ing from a cyclic graph), mixed-curvature GNNs have been proposed. Gu et al. (2019) pioneeredthis direction by embedding graphs in a product manifold combining spherical, hyperbolic, and Eu-clidean spaces. Building on this, models like \u03ba-GCN (Bachmann et al., 2020) and Q-GCN (Xionget al., 2022) extended the GCN architecture (Kipf & Welling, 2016) to constant-curvature spaces us-ing the K-stereographic model and pseudo-Riemannian manifolds, respectively. More recently, Sunet al. (2022) proposed a mixed-curvature GNN for self-supervised learning, while FPS-T (Cho et al.,2023) generalized the Graph Transformer (Min et al., 2022) to operate across multiple manifolds.\nSpectral Graph Neural Networks. Spectral GNNs employ spectral graph filters (Liao et al., 2024)to process graph data. These models either use fixed filters, as seen in APPNP (Gasteiger et al.,2018) and GNN-LF/HF (Zhu et al., 2021), or learnable filters, as demonstrated by ChebyNet (Def-ferrard et al., 2016) and GPRGNN (Chien et al., 2020), which approximate polynomial filters usingChebyshev polynomials and generalized PageRank, respectively. BernNet (He et al., 2021) ex-presses filtering operations through Bernstein polynomials. However, many of these methods focusprimarily on low-frequency components of the eigenspectrum, potentially overlooking important in-formation from other frequency bands \u2013 particularly in heterophilic graphs. Models like GPRGNNand BernNet address this by exploring the entire spectrum, performing well across both homophilicand heterophilic graphs. GPRGNN stands out among them because it can express several poly-nomial filters and incorporate node features and topological information. Despite these advances,current mixed-curvature and spectral GNNs face significant limitations (L1, L2, L3) that constrain their performance. To the best of our knowledge, this work is the first to unify geometric and spec-tral information within a single model. Before presenting the architecture of CUSP, we introducesome key preliminary concepts in the following section."}, {"title": "3 PRELIMINARIES", "content": "We study graphs $G = (V, E, A)$, where $V$ is a finite set of $|V| = n$ vertices, $E$ is a set of edges and $A \\in R^{nxn}$ is a weighted graph adjacency matrix. The nodes are associated with the node featurematrix $F \\in R^{n\\times d_f}$ ($d_f$ is the feature node dimension). A graph signal $f : V \\rightarrow R$ on the nodes ofthe graph may be regarded as a vector $f \\in R^n$ where $f_i$ is the value of $f$ at the $i$th node. An essentialoperator in spectral graph analysis is the graph Laplacian $L = D - A \\in R^{n\\times n}$ where $D \\in R^{n\\times n}$ isthe diagonal degree matrix with $D_{ii} = \\sum_j A_{ij}$ (Kipf & Welling, 2016). The normalized Laplacianis defined as $L_n = I - A_n$ where $A_n = D^{-1/2}AD^{-1/2}$ is the normalized adjacency matrix, and$I \\in R^{n\\times n}$ is the identity matrix. As $L$ is a real symmetric positive semidefinite matrix, it has acomplete set of orthonormal eigenvectors $U = [{u_i}_{i=1}^n] \\in R^{n\\times n}$, and their associated orderedreal nonnegative eigenvalues $[{\\lambda_i}_{i=0}^n] \\in R^n$, identified as the frequencies of the graph.TheLaplacian can be diagonalized as $L = U\\Lambda U^T$ where $\\Lambda = diag([{\\lambda_i}_{i=0}^n]) \\in R^{n\\times n}$.\nRiemannian Geometry (Do Carmo & Flaherty Francis, 1992). A smooth manifold $M$ generalizesthe notion of surface to higher dimensions. Each point $x \\in M$ is associated with a tangent space$T_xM$, which is locally Euclidean. On tangent space $T_xM$, the Riemannian metric, $g_x(\\cdot, \\cdot) : T_xM\\timesT_xM \\rightarrow R$, defines an inner product so that geometric notions (like distance, angle, etc.) can beinduced. The pair $(M, g)$ is called a Riemannian manifold. For $x \\in M$, the exponential map at $x$,$exp_x(v): T_xM \\rightarrow M$, projects the vector $v \\in T_xM$ onto the manifold $M$, and the logarithmicmap, $log_x(y): M \\rightarrow T_xM$, projects the vector $y \\in M$ back to the tangent space $T_xM$. TheRiemannian metric defines a curvature ($\\kappa$) at each point on the manifold, indicating how the spaceis curved. There are three canonical types: positively curved Spherical ($S$) space ($\\kappa > 0$), negativelycurved Hyperbolic ($H$) space ($\\kappa < 0$), and flat Euclidean ($E$) space ($\\kappa = 0$)."}, {"title": "4 PROPOSED METHOD: CUSP", "content": "In this section, we present a comprehensive overview of the architecture of CUSP, as shown in Figure2. We start by introducing and deriving the Cusp Laplacian (Section 4.1). Building on this, wepropose Cusp Filtering, the core component of our approach, which is a GPR-based, mixed-curvaturespectral graph filtering network (Section 4.2). Next, we introduce a curvature embedding techniquegrounded in classical harmonic analysis (Section 4.3), which acts as the positional encoding in thehierarchical attention-based Cusp Pooling mechanism (Section 4.4). Throughout this paper, we use$\\kappa$ to denote the continuous manifold curvature and $\\digamma$ for the Ollivier-Ricci curvature."}, {"title": "4.1 CUSP LAPLACIAN", "content": "To effectively incorporate geometricinsights into spectral graph learning,we introduce the Cusp Laplacian,a curvature-aware Laplacian opera-tor. We begin by examining the con-cept of heat flow on a graph (Weber,2008). Suppose $\\psi$ describes a tem-perature distribution across a graph,where $\\psi(x)$ is the temperature at ver-tex $x$. According to Newton's lawof cooling (He, 2024), the heat trans-ferred from node $x$ to node $y$ is pro-portional to $\\psi(x) - \\psi(y)$ if nodes $x$and $y$ are connected (if they are not"}, {"title": "4.2 CUSP FILTERING", "content": "Now, we will talk about how we create a filter bank (i.e. multiple graph filters) to fuse informationfrom different parts of the eigenspectrum and learn node representations on a product manifold $\\mathbb{P}$.\nProduct manifold construction. $\\mathbb{P}$ can have multiple hyperbolic or spherical components withdistinct learnable curvatures (parameters). This enables us to be representative of a wider range ofcurvatures. However, we only need one Euclidean space, since the Cartesian product of Euclidean\nspace is $\\mathbb{E}^{d(e)} = \\times_{i=1}\\mathbb{E}^{d(i)}$ such that $\\sum_{i=1} d(i) = d(e)$. This is not the case with $\\mathbb{H}$ or $\\mathbb{S}$ (Eg.\nTorus i.e. $\\mathbb{S}^1 \\times \\mathbb{S}^1$ is topologically distinct from Sphere i.e. $\\mathbb{S}^2$). Thus, we can represent the productmanifold signature, $\\mathbb{p_{dM}} = \\times_{q=1}^Q M_{d(q)} = (\\times_{h=1}^H \\mathbb{H}_{d(h)}) \\times (\\times_{s=1}^S \\mathbb{S}_{d(s)}) \\times \\mathbb{E}^{d(e)}$ withtotal dimension $d_M = \\sum_{h=1}^H d(h) + \\sum_{s=1}^S d(s) + d(e)$. We use a simple combinatorial constructionof the mixed-curvature space, induced by the cartesian product (Sun et al., 2022). In Section 5 andAppendix 7.2.3 we describe how we heuristically identify the signature of $\\mathbb{P}_{dM}$.\nExtending PageRank GNN to $\\mathbb{P}$: We choose GPRGNN (Chien et al., 2020) as the spectral backbone of CUSP (See Appendix 7.4 for more background on GPRGNN), becasue it jointly optimizesnode feature and topological information extraction, which is different from other spectral GNNslike ChebyNet (Defferrard et al., 2016). The GPR weights automatically adjust to the node labelpattern (homophilic or heterophilic). Given the node feature matrix $F \\in R^{n\\times d_f}$ and normalizedsymmetric Cusp adjacency matrix $\\tilde{A}_n = D^{-\\frac{1}{2}} \\tilde{A} D^{-\\frac{1}{2}}$, we first extract hidden state features for"}, {"title": "4.3 FUNCTIONAL CURVATURE ENCODING", "content": "Recall our motivation that CUSP must be able to pay more attention to differently curved substructures in our model, depending on different tasks and datasets, while learning the final noderepresentations. Specifically, our goal is to obtain a continuous functional mapping $\\Phi : \\mathcal{K}_P \\rightarrow \\mathbb{P}_{dc}$from curvature domain to the $d_c$-dimensional product space to serve as positional encoding in ourattention mechanism (Section 4.4). We approximate this in two steps, by considering the Ollivier-Ricci (ORC) discretization of curvature on our graph and using the Bochner's theorem (Moelleret al., 2016) from classical harmonic analysis. First, we construct a translation-invariant Euclideancurvature encoding map, and then map it to the product manifold. Without loss of generality, weassume that the curvature domain can be represented by the interval: $\\mathcal{K} = [-1,1]$, where -1 to1 is the typical range\u00b9 of ORC in the observed data. Formally, we define the Curvature Kernel$K_R:\\mathcal{K}\\times\\mathcal{K} \\rightarrow \\mathbb{R}$ with $K_R(\\kappa_a,\\kappa_b) := \\langle\\Phi_{\\mathbb{R}^{dc}} (\\kappa_a), \\Phi_{\\mathbb{R}^{dc}} (\\kappa_b)\\rangle$ and $K_R(\\kappa_a, \\kappa_b) = \\Psi_{\\mathbb{R}}(\\kappa_a - \\kappa_b)$,$\\forall \\kappa_a, \\kappa_b\\in \\mathcal{K}$ for some $\\Psi_{\\mathbb{R}}: [-2,2] \\rightarrow \\mathbb{R}$. The intuition behind choosing such a kernel for curvature values, lies in the fact that when comparing $\\kappa_a$ and we are only concerned with the relativedifference between the two values. According to the Bochner's theroem (Moeller et al., 2016), acontinuous, translation-invariant kernel $K(x, y) = \\Psi(x - y)$ on $R^d$ is positive definite if $\\Psi$ is theFourier transform of a non-negative probability measure on $R$. Our kernel is translation-invariant,since $K_R (\\kappa_a+\\mathcal{c}, \\kappa_b+\\mathcal{c}) = \\Psi_R((\\kappa_a+\\mathcal{c})-(\\kappa_b+\\mathcal{c})) = K_R(\\kappa_a, \\kappa_b)$ for any constant$\\bar{c}$. The followingtheorems defines the Euclidean encoding and it's corresponding mapping to the product space.\nDefinition 2. Let $\\mathbb{R}^{d_c}$ be the ambient Euclidean space for a node's curvature encoding. The functional curvature encoding $\\Phi_{\\mathbb{R}^{dc}}: \\mathcal{K} \\rightarrow \\mathbb{R}^{d_c}$ for curvature $\\kappa(x) \\in \\mathcal{K}$ of node $x$, is defined as:\n$\\Phi_{\\mathbb{R}^{dc}} (\\kappa(x)) =  \\frac{1}{\\sqrt{d_c}}[cos(\\omega_1\\kappa(x)), sin(\\omega_1\\kappa(x)),..., cos(\\omega_{d_c}\\kappa(x)), sin(\\omega_{d_c}\\kappa(x))]$\nwhere $\\omega_1,...,\\omega_{d_c}  \\overset{iid}{\\sim} p(\\omega)$ are sampled from a distribution $p(\\omega)$. The corresponding mapping tothe product manifold $\\mathbb{P}_{dc}$, is defined as:\n$\\Omega = g_e (||_{q=1}^Q  exp_{\\kappa(q)}^q (\\Phi_{\\mathbb{R}^{dc}} (\\kappa(x)))||)$\nwhere $exp_{\\kappa(q)} : \\mathbb{R}^{d_c} \\rightarrow M_{q,d(q)}$ denotes the exponential map on the $qth$ component manifoldwith curvature $\\kappa(q)$, $||$ is the concatenation operator and $g_e : \\mathbb{P}_{dM} \\rightarrow \\mathbb{P}_{dc}$ is a Riemannian projector.\nIt is easy to show that $\\langle\\Phi_{\\mathbb{R}^{dc}} (\\kappa_a), \\Phi_{\\mathbb{R}^{dc}} (\\kappa_b)\\rangle \\approx K(\\kappa_a,\\kappa_b)$. The unknown distribution $p(\\omega)$ is estimated using the inverse cumulative distribution function (CDF) transformation as in Xu et al. (2020).Next, we prove the translation invariance of the curvature kernel in the product manifold setting.\nTheorem 2. The mixed-curvature kernel $K_{\\mathbb{P}}(\\kappa_a, \\kappa_b) := \\langle\\Phi_{\\mathbb{P}_{dc}} (\\kappa_a), \\Phi_{\\mathbb{P}_{dc}} (\\kappa_b)\\rangle$ is translation in-variant, i.e. $K_{\\mathbb{P}}(\\kappa_a, \\kappa_b) = \\Psi_{\\mathbb{P}}(\\kappa_a - \\kappa_b)$.\nPlease refer to Appendix 7.5 for the detailed proofs (derivations) of Definition 2 and Theorem 2.We employ $\\Phi_{\\mathbb{P}_{dc}} (\\kappa(x))$ as the ORC-based positional encoding for each node $x$ in the final poolinglayer, enabling it to incorporate local curvature information and compute the task-specific relevanceof substructures with varying curvatures within the graph."}, {"title": "4.4 CUSP POOLING", "content": "The importance of constant-curvature component spaces depends on the downstream task. To thisend, we propose a hierarchical attention mechanism called Cusp Pooling. We perform attentional"}, {"title": "5 EXPERIMENTATION", "content": "Datasets. We evaluate CUSP on the Node Classification (NC) and Link Prediction (LP) tasks usingeight benchmark datasets. These include (a) Homophilic datasets such as (i) Citation networksCora, Citeseer and PubMed (Sen et al., 2008; Yang et al., 2016), and (b) Heterophilic datasets,which comprise (i) Wikipedia graphs \u2013 Chameleon and Squirrel (Rozemberczki et al., 2021), (ii)Actor co-occurrence network (Tang et al., 2009), and (iii) Webpage graphs from WebKB 2 \u2013 Texasand Cornell. The dataset statistics and their respective homophily ratios are detailed in Table 1.Refer to Appendix 7.2.1 for the discrete curvature and eigenspectrum distributions of these datasets.\nBaselines. To ensure a fair comparison,we evaluate CUSP against three types ofbaselines: (a) Spatial-Euclidean, includ-ing traditional methods such as GCN (Kipf& Welling, 2016), GAT (Veli\u010dkovi\u0107 et al.,2017), and GraphSAGE (Hamilton et al.,2017). (b) Spatial-Riemannian, comprising(i) Constant-curvature models like HGCN (Chami et al., 2019) and HGAT (Zhang et al., 2021b),and (ii) Mixed-curvature GNNs such as KGCN (Bachmann et al., 2020), QGCN (Xiong et al., 2022),and SelfMGNN (Sun et al., 2022). (c) Spectral, including ChebyNet (Defferrard et al., 2016), Bern-Net (He et al., 2021), GPRGNN (Chien et al., 2020), and FiGURe (Ekbote et al., 2024) (More detailsin Appendix 7.6.2). There are no existing methods that integrate both spectral and geometric signals.\nExperimental Settings. For the transductive LP task, we randomly split edges into 85%/5%/10%for training, validation and test sets, while for transductive NC task, we use the 60%/20%/20% split.The results are averaged over 10 random splits and their 95% confidence intervals are reported.We report the AUC-ROC and F1 Score metrics for LP and NC respectively. For computing ORC(in Cusp Laplacian), we use d = 0.5, i.e. equal probability mass is retained by the node anddistributed among it's neighbors. We adopt the ORC implementation from Ni et al. (2019), and usethe Sinkhorn algorithm (Sinkhorn & Knopp, 1967) to approximate the W\u2081 distance. See Appendix7.2.2 for more computational details and complexity analysis. Next, we adopt the implementationof the K-stereographic product manifold from Geoopt library\u00b3. We heuristically determine thesignature of our manifold P (i.e. component manifolds) using the discrete ORC curvature of the"}, {"title": "5.1 ABLATION STUDY", "content": "Impact of product manifold signatures. Different datasets leverage substructures with varyingcurvatures as inductive biases for downstream tasks, leading to differing performance across prod-uct manifold signatures. Table 5 shows the learned curvature and weight (\u03b2) for the best-performing"}, {"title": "6 CONCLUSION", "content": "In this paper, we propose to unify Spectral and Curvature signals in a graph for learning optimalgraph representations, aiming to inspire further research in this area. CUSP introduces a graphlearning paradigm parameterized by spectral filters on a mixed-curvature product manifold. Wepropose a new curvature-informed Cusp Laplacian operator to capture the underlying geometry,use it to define a novel GPR-based spectral filter-bank (Cusp Filtering) and introduce an attention-based pooling mechanism to fuse representations from multiple mixed-curvature graph filters (CuspPooling). CUSP outperforms the state-of-the-art baselines for node classification and link predictionover homophilic and heterophilic datasets, highlighting the efficacy of combining spectrum andcurvature (geometry), in learning graph representations."}, {"title": "7.1 NOTATION TABLE", "content": null}, {"title": "7.2 MORE ON PRELIMINARIES", "content": null}, {"title": "7.2.1 ANALYSIS OF REAL-WORLD GRAPHS", "content": null}, {"title": "7.2.2 OLLIVIER-RICCI CURVATURE (ORC)", "content": "In an unweighted graph, the neighborhood of each node x, denoted as $\\mathcal{N}(x)$, is assigned a probability distribu-tion according to a lazy random walk formulation (Lin et al., 2011). Specifically, we define the distribution asfollows:\n$m_x(z) =  \\begin{cases}\\alpha, & \\text{if } z = x, \\\\\\frac{1-\\alpha}{|\\mathcal{N}(x)|}, & \\text{if } z \\in \\mathcal{N}(x), \\\\0, & \\text{otherwise.}\\end{cases}$\nHere, \u03b1 controls the probability that a random walk will remain at the current node, while the remainingprobability mass (1 \u2212 \u03b1) is uniformly distributed across the neighboring nodes. This formulation connects ORCwith lazy random walks and influences the balance between local exploration and the likelihood of revisiting anode. In this work, we use \u03b1 = 0.5, meaning that equal probability mass is distributed between the node itselfand its neighbors, striking a balance between breadth-first and depth-first search strategies. The choice of \u03b1 iscrucial and depends on the topology of the graph. A smaller \u03b1 value encourages more local exploration, whilea larger \u03b1 favors revisiting nodes, thereby promoting a \"lazy\" walk. For our experiments, \u03b1 = 0.5 was chosento reflect an equal probability mass distribution between the node and its neighbors.\n\u25a0 Computational Considerations. Computing ORC can be computationally intensive due to the need tocalculate the Wasserstein-1 distance (W\u2081), between the neighborhood distributions of connected nodes. In adiscrete setting, this corresponds to solving a linear program. Typically, $W_1(m_x, m_y)$ between two nodes xand y is computed using the Hungarian algorithm (Kuhn, 1955), which has a cubic time complexity. However,this becomes prohibitively expensive as the graph size increases. Alternatively, the Wasserstein-1 distance canbe approximated using the Sinkhorn algorithm (Sinkhorn & Knopp, 1967), which reduces the complexity toquadratic time. For this work, we employ the Sinkhorn approximation to compute ORC efficiently. Below,we provide an alternative to approximate ORC of an edge in linear time, in case of very large (million-scale)real-world graphs.\n\u25a0 Approximating ORC in Linear Time. Even with the quadratic complexity of the Sinkhorn algorithm,scaling to large networks remains challenging. To address this, a linear-time combinatorial approximation ofORC can be employed, as suggested by Tian et al. (2023). This method approximates the Wasserstein distanceby utilizing local structural information, making it much more computationally feasible. The approximationof ORC builds on classical bounds first introduced by Jost & Liu (2014). Let #(x, y) denote the number oftriangles formed by the edge (x, y), and define a \u2227 b = min(a, b), a \u2228 b = max(a, b) and dx is the degree ofnode x. The following bounds on ORC can be derived for an edge e = x, y:\nTheorem 3 (Jost & Liu (2014)). For an unweighted graph, the Ollivier-Ricci curvature of an edge e = x, ysatisfies the following bounds:\n1. Lower bound:\n$\\kappa(x, y) \\geq - \\left(1 - \\frac{\\frac{1}{d_x} + \\frac{1}{d_y}}{2 + \\frac{\\#(x,y)}{d_x \\wedge d_y}} \\right) - \\left(1 - \\frac{\\frac{1}{d_y} + \\frac{1}{d_x}}{2 + \\frac{\\#(x,y)}{d_y \\wedge d_x}} \\right)$\n2. Upper bound:\n$\\kappa(x,y) \\leq \\frac{\\#(x,y)}{d_x \\vee d_y}$\nThe ORC of an edge, can then be approximated as the arithmetic mean of these bounds:\n$\\kappa(x,y) := \\frac{1}{2} \\left(\\kappa^{upper}(x, y) + \\kappa^{lower}(x, y)\\right)$\nThe proof of these bounds has been detailed in Tian et al. (2023). This approximation is computationallyefficient, with linear-time complexity, and can be parallelized easily across edges, making it suitable for large-scale graphs. The computation relies solely on local structural information, such as the degree of the nodes andthe number of triangles."}, {"title": "7.2.3 PRODUCT MANIFOLDS", "content": "Let $M_1, M_2,..., M_k$ denote a set of smooth manifolds. Their Cartesian product forms a product manifold,denoted by $\\mathbb{P}$, such that $\\mathbb{P} = M_1 \\times M_2 \\times ... \\times M_k$. Any point $p\\in \\mathbb{P}$ is characterized by its coordinates$p = (p_1,p_2,..., p_k)$, where each $p_i$ corresponds to a point on the individual manifold $M_i$. Similarly, a tangentvector $v \\in T\\mathbb{P}$ can be expressed as $v = (v_1, v_2, ..., v_k)$, where each $v_i \\in T_{p_i} M_i$ represents the projectionof v in the tangent space of the respective component manifold $M_i$. Optimization over manifolds requires thenotion of taking steps along the manifold, which can be achieved by moving in the tangent space and mappingthose movements back onto the manifold through the exponential map. The exponential map at a point $p \\in \\mathbb{P}$,"}, {"title": "7.2.4 KAPPA-STEREOGRAPHIC MODEL", "content": "The K-stereographic model (Bachmann et al., 2020) unifies Hyperbolic and Spherical geometries under gy-rovector formalism. This model leverages the framework of gyrovector spaces to represent all three con-stant curvature geometries\u2014hyperbolic, Euclidean, and spherical simultaneously. Additionally, it facilitatessmooth transitions between these constant curvature geometries, enabling the joint learning of space curvaturesalongside the embeddings. It is a smooth manifold $\\mathbb{M}_{\\kappa,d} = {z \\in \\mathbb{R}^d| - \\kappa||z||^2 < 1}$, whose origin is0 \u2208 Rd, equipped with a Riemannian metric $g_z = \\frac{4}{(\u03bb_z)^2}I$, where $\u03bb_z$ is given by $\u03bb_z = \\frac{2}{(1 + \u03ba||z||^2)}$.The Riemannian operations under this model are elaborated in the table below:"}, {"title": "7.2.5 SPECTRAL GRAPH THEORY", "content": "Graph Fourier Transform (GFT) (Sandryhaila & Moura, 2013) lays the foundation for Graph Neural Networks(GNNs). A GFT is defined using a reference operator R which admits a spectral decomposition. Traditionally,in the case of GNNs, this reference operator has been the symmetric normalized Laplacian $L_n = I - A_n$(Kipf & Welling, 2016). The graph Fourier transform of a signal $f \u2208 R^n$ is then defined as $\\tilde{f} = U^Tf \u2208 R^n$,and its inverse as $f = U\\tilde{f}$. A graph filter is an operator that acts independently on the entire eigenspace of adiagonalisable and symmetric reference operator R, by modulating their corresponding eigenvalues. A graphfilter is defined via the graph filter function $g(.)$ operating on the reference operator as $g(R) = Ug(\u039b)UT."}, {"title": "7.3 MORE ON CUSP LAPLACIAN", "content": "Spectral graph theory has shown significant progress in relating geometric characteristics of graphs to propertiesof spectrum of graph Laplacians and related matrices. Several variants of the graph Laplacian matrices havebeen shown to capture specific inductive biases for different tasks (Ko et al., 2023; Belkin et al., 2008; Jacobson& Sorkine-Hornung, 2012).\nProof of Definition 1. Say the function $\\psi: V \\rightarrow R$ is defined on the vertex set V of the graph. Suppose $\\psidescribes a temperature distribution across a graph, where $\\psi(x)$ is the temperature at vertex x. According toNewton's law of cooling (He, 2024), the heat transferred from node x to node y is proportional to $\\psi(x) - \\psi(y)$"}, {"title": "7.3.1 RELEVENT THEOREMS FOR CUSP LAPLACIAN", "content": "Theorem 4 (Positive Semidefiniteness of Cusp Laplacian). The normalized Laplacian operator $\\mathcal{L}$ is positivesemidefinite, i.e., for any real vector $u \u2208 R^n$, we have $u^T\\mathcal{L}u \u2265 0."}, {"title": "7.4 GENERALISED PAGERANKS AND GPRGNN", "content": "\u25a0 Equivalence of the GPR method and polynomial graph filtering. If we truncate the infinite series in theGPR definition at some integer K, $\\sum_{k=0}^K\\gamma_k\u039b$ becomes a polynomial graph filter of degree K. Consequently,optimizing the GPR weights is tantamount to optimizing the polynomial graph filter. It is important to note thatany graph filter can be approximated using a polynomial graph filter, enabling the GPR method to handle awide variety of node label patterns. Additionally, increasing K enhances the approximation of the optimalgraph filter. This again illustrates the advantage of large-step propagation.\n\u25a0 GPRGNN architecture. GPR-GNN initially derives hidden state features for each node and subsequentlyemploys GPR to disseminate them. The GPR-GNN procedure can be represented as:\n$\\mathbb{P} = softmax(Z), Z = \\sum_{k=0}^K\\gamma_kH^{(k)}, H^{(k)} = A_{sym}H^{(k-1)}, H^{(0)} = f_{\\Theta}(X_{i:}),$\nHere, $f_{\\Theta} (.) $denotes a neural network parametrized by {\u0398}, which produces the hidden state features $H^{(0)}$. TheGPR weights \u03b3k are optimized alongside {\u0398} in an end-to-end manner. The GPR-GNN model is straightfor-ward to interpret: As previously mentioned, GPR-GNN is capable of adaptively managing the contribution ofeach propagation step to fit the node label pattern. Analyzing the trained GPR weights also aids in understanding the topological properties of a graph, such as identifying the optimal polynomial graph filter."}, {"title": "7.4.1 PROOF OF THEOREM 1", "content": "We first state the formal version of Theorem 1\nTheorem 6 (Formal version of Theorem 1). Assume the graph $G$ is connected. Let $\u03bb_1 \u2265 \u03bb_2 \u2265 ... > \u03bb_n$ and\\tilde{\u03bb}_1 \u2264 \\tilde{\u03bb}_2 < ... < \\tilde{\u03bb}_n be the eigenvalues"}]}