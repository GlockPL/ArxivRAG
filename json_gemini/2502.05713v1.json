{"title": "4D-VQ-GAN: SYNTHESISING MEDICAL SCANS AT ANY TIME\nPOINT FOR PERSONALISED DISEASE PROGRESSION MODELLING\nOF IDIOPATHIC PULMONARY FIBROSIS", "authors": ["An Zhao", "Moucheng Xu", "Ahmed H. Shahin", "Wim Wuyts", "Mark G. Jones", "Joseph Jacob", "Daniel C. Alexander"], "abstract": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective\ntreatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pul-\nmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many\ncancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for\nIPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better\ntreatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector\nQuantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic\nCT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In\nthe first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural\nOrdinary Differential Equation (ODE) based temporal model is trained to capture the temporal dy-\nnamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different\nconfigurations of our model for generating longitudinal CT scans and compare the results against\nground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis\nusing imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that\nof biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential\nclinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict\nsurvival outcomes.", "sections": [{"title": "1 Introduction", "content": "Disease progression modelling aims at discovering disease evolution trajectories and is crucial for understanding\ndiseases' biological mechanisms. While significant progress has been made in building disease progression models\nusing longitudinal, low-dimensional data (e.g., differential equation models [55, 24, 39] and self-modelling regression\nmethods [25, 9]), a significant challenge remains: developing such models for high-dimensional volumetric imaging\ndata. This task involves using a limited number of observations to generate imaging data for any given time point\nto mimic the disease progression trajectory. Compared to low-dimensional data, volumetric scans offer a wealth of\ninformation, but at the cost of increased computational complexity. This area remains relatively unexplored. However,\npromising initial attempts exist, leveraging generative models to simulate longitudinal magnetic resonance imaging\n(MRI) scans specifically in the context of Alzheimer's disease (AD) progression [5, 46, 30, 38, 11, 41, 42, 59].\nBeyond neurodegenerative diseases, disease progression models have also been investigated in progressive lung\nconditions such as idiopathic pulmonary fibrosis (IPF). However, there remains a need for models capable of generating\nlongitudinal CT scans for IPF using limited observational data. Existing methods for generating longitudinal MRI scans\nin AD are not directly applicable to IPF for several reasons. First, IPF is much rarer with shorter patient lifespans than\nAD resulting in less imaging being acquired. Furthermore, radiation side effects from CT scans discourage repeated\nscans, resulting in a scarcity of longitudinal CT data and hindering model development. Second, lung CT scans contain\nvastly more fine textured-structures like vessels, airways, and interstitial tissue compared to the smooth, homogenous\nbrain seen in MRI. Generating these intricate lung structures synthetically is more challenging than replicating brain\ntissue.\nSeveral problems need to be tackled to address this task. Modelling disease progression directly in the image space\nof volumetric CT scans is challenging due to the complexity of lung structures. A more effective approach is to use\nautoencoder-based frameworks to map the images to latent space for progression modelling, then project them back to\nthe original image space [46, 30, 38, 11, 41]. Another challenge lies in determining how to model the dynamics of\ndisease progression. Some previous disease progression models [46, 30] rely on explicit assumptions about evolution\ntrajectories (e.g. linear progression). This oversimplification fails to capture the complexity observed in real-world\ndisease progression. Other methods [38, 11] use recurrent neural networks to capture the temporal information. These\nmethods model dynamics in discrete steps, limiting their ability to capture the continuous nature of disease progression.\nPuglisi et al.[41] use the latent diffusion model and incorporate prior knowledge to model the disease progression.\nHowever, it also struggles to ensure continuous temporal changes. To overcome the limitations of these methods,\nNeural Ordinary Differential Equations (ODEs) emerge as a promising alternative [4]. Neural ODEs provide a powerful\nframework for modelling continuous disease progression by learning the underlying dynamics of the disease state.\nThey achieve this by parameterizing the derivative of the hidden state with neural networks, allowing them to capture\ncomplex, non-linear progression with greater flexibility.\nIn this paper, we introduce the 4D Vector Quantized Generative Adversarial Network (4D-VQ-GAN). Given 3D\nimaging data at irregular time points, our method can generate synthetic 3D images at any desired time point, effectively\nmodelling a continuous disease progression trajectory for each individual. We demonstrate the effectiveness of our\napproach in generating CT volumes for IPF patients. Additionally, we found that biomarkers derived from the generated\nCT volumes exhibit a strong clinical correlation with survival outcome, highlighting the potential of our method for\npersonalized treatment planning."}, {"title": "2 Methods", "content": "Our 4D-VQ-GAN is a self-supervised generative model trained on temporal CT volumes. The model consists of two\nkey components: a 3D-VQ-GAN and a temporal model, trained in a two-stage process. During inference, given two\nCT scans of the same patient at different time points, our model can generate new CT volumes at any desired time\npoint, effectively capturing disease progression. In this section, we first introduce these components, as shown in 1, and\npresent our proposed survival analysis method to evaluate the effectiveness of the generated CT scans in predicting\npatients' future outcomes.\n3D-VQ-GAN As shown in 1, the first stage of training involves a 3D-VQ-GAN [16] to reconstruct CT volumes for\neach case at every time point in the training set. Unlike the original 2D-VQ-GAN [10], our 3D-VQ-GAN employs 3D\nconvolutional layers, enabling it to capture spatial structures more effectively in volumetric imaging data. Following\nthe VQ-VAE framework [54], 3D-VQ-GAN compresses high-dimensional volumetric imaging data into a discrete set\nof latent codes, constrained by a predefined codebook size. Since the model is trained on imaging data, its codebook\nlearns to represent meaningful imaging patterns, with each code acting as a compact and discrete representation of local\nanatomical structures or texture features. We adapted the original 3D-VQ-GAN loss functions [16] for training. Further\ndetails can be found in Appendix D.2.\nTemporal Model In the second stage of the training, we train a temporal model that can reconstruct the temporal\ntrajectories of the latent embeddings (z) of the imaging data from different time-points, as demonstrated in 1. We realise\nthat generating the future or past latent embeddings from a few observed latent embeddings naturally formulates as an\nordinary differential equation. We therefore utilise a neural ODE solver [4] to predict the unknown embeddings at new\ntime-points. We found that it is more beneficial to adapt a 3D-ConvGRU [1] as the encoder of the neural ODE solver on\nour data, with much better computational efficiency. The outputs of the neural ODE are then fed into a light-weight\nprojector, two 3D convolutional layers, to reconstruct the latent embeddings z at inquired time points, with a specified\ntime interval. In practice, we also discovered that adding skip-connections for the generated embeddings gives better\nresults. This is because, by adding the skip-connections between consecutive latent embeddings, the generation task\nbecame easier, as the model only needs to learn the differences between the latent embeddings at every two adjacent\ntime-points. We train the temporal model in a self-supervised manner by using a L2 loss between the input embeddings\nand the reconstructed embeddings. Please refer to Appendix D.3 for more details.\nInference The inference follows the process shown in 2. The trained model needs two CT scans. The user can also\ninput two hyper-parameters for the neural ODE, namely the interval time and the total time duration. For example, as\nshown in 2, given two initial scans at time point 0 and 1, the interval as 1 year and the total time duration as 3 years, the\nmodel will reconstruct the scans at time point 0 and 1, and start to extrapolate future scans at time point 2 and 3.\nSurvival analysis and biomarker discovery To evaluate the potential clinical utility of the proposed method, we\nperform survival analysis based on scans generated by the trained model. As discussed before, each CT scan can be\nrepresented as a set of codebook indices in the latent space, with the most meaningful features of the scan embedded\nwithin these indices. Each code index represents a distinct imaging pattern in the CT. The frequency of each code index\nreflects the prevalence of the corresponding imaging pattern. We then use these normalized frequencies of code indices\nas candidate prognostic biomarkers. The survival analysis is conducted in three stages. In the first stage, we identify the\ntop five embedded imaging biomarkers from the above candidate biomarkers. Our biomarker selection process goes as\nfollows. For each candidate biomarker, we input it along with relevant covariates (e.g., sex, age, smoking status) from\nthe training dataset into the Cox proportional hazards model [6]. The biomarkers are then ranked by their p-values,\nand the top five are selected for further analysis. In the second stage, we input these five selected biomarkers, along\nwith the covariates, into the Cox model to compute the final C-index for both real and generated future scans in the test\ndataset. This step validates whether significant prognostic information is preserved in the generated future scans. These\nbiomarkers are derived from a single CT scan and are thus considered cross-sectional biomarkers. In the third stage, we\nobtain the longitudinal biomarkers from both real and generated CT scans by calculating the changes of the top five\nbiomarkers over a one-year period. These biomarkers, along with the covariates, are input into the Cox model to assess\ntheir prognostic value in the test dataset. The third stage highlights the potential utility of tracking changes in these\nbiomarkers over time."}, {"title": "3 Experiments", "content": "Datasets Our data comes from a longitudinal dataset comprising 681 volumetric CT scans from 219 IPF patients,\nobtained from University Hospitals Leuven, Belgium, a single centre in Leuven. We randomly divided the dataset into\n80% for training (552 CT scans from 175 patients) and 20% for validation (129 CT scans from 44 patients). The CT\nscans were acquired with varying slice thicknesses, ranging from 0.75 mm to 1 mm, and in-plane resolution varied with\npixel spacing ranging from 0.38 mm to 0.98 mm. Additionally, we use an external cross-sectional dataset of 98 IPF\npatients from the University Hospital Southampton NHS Foundation Trust, UK, to evaluate the generalizability of the\n3D-VQ-GAN trained in Stage 1 and more details can be found in Appendix D.4.\nPreprocessing We focus on modelling changes within the lung areas. We segment the lung regions using a pre-trained\nU-Net [44, 22] for all CT scans of IPF patients and visually inspect the lung masks. Subsequently, we register the\nlongitudinal lung scans to remove extraneous artifacts caused by lung motion or incorrect body positioning. Our lung\nscan registration method is a faster version (implemented in [17]) of the CorrField method [18]. Visualizations of the\nsegmentation and registration outcomes can be found in Appendix A.\nTraining Our models are trained on an NVIDIA A100 80GB GPU. The first training stage used a batch size of 1, with\nan accumulated batch size of 6, and lasted 20,000 steps, equivalent to 10 days. The second training stage also used a\nbatch size of 1 and lasted 15 hours. The hyperparameters of the training can be found in Appendix C.\nEvaluation Metrics To evaluate the image quality of the reconstructed CT scans in the first stage of training, we use\nMean Squared Error (MSE). For assessing the image quality of the generated temporal CT scans in the second stage,\nwe use Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). For survival analysis, we use the\nConcordance Index (C-index), a metric that measures the predictive accuracy of a model by evaluating the agreement\nbetween predicted risk scores and actual survival outcomes."}, {"title": "4 Results", "content": "In this section, we present the results of our model on the following tasks: 1) interpolation, for imputing missing CT\nscans between the two input scans; 2) extrapolation, for predicting the future CT scans beyond the time span of the two\ngiven input scans; 3) survival outcome prediction, for evaluating of the clinical utility of the proposed method."}, {"title": "5 Discussion and Conclusion", "content": "This paper represents a pioneering effort in building a deep-learning-based disease progression model to generate 4D\nimaging data continuously, mimicking disease progression trajectories of IPF patients from sparse discrete observations.\nBy integrating 3D-VQ-GAN and Neural ODE, the proposed method avoids the strong linear assumptions commonly\nused in previous disease progression models for 3D imaging data. This enables the modelling of more complex\ntrajectories and the generation of synthetic 3D imaging data, which is crucial for capturing the spatial and temporal\nheterogeneity of diseases like IPF. Being able to reliably predict evolution and disease progression in IPF is extremely\nchallenging. Today the main guide to disease progression is monitoring lung function test trajectories which are\nassociated with measurement variability and may be unreliable in the presence of other lung pathology such as\nemphysema. Accordingly, using imaging as shown, may help transform clinical management decisions such as\ninitiating medication or expediting referral for lung transplantation.\nBeyond disease progression analysis, the model has the potential for applications like synthetic data generation, data\naugmentation, and missing data imputation. However, the proposed method exhibits certain limitations. Firstly, the\ncurrent approach utilizes a deterministic Neural ODE in the latent space, which assumes a shared disease dynamic\nacross all patients. This might not be ideal for heterogeneous diseases with diverse subtypes and distinct progression\npatterns. Secondly, a crucial limitation of the current validation process is its reliance on a single dataset for evaluating\nthe model. Future work should address these limitations by incorporating diverse datasets and disease dynamics, to\nrefine the model and pave the way for its potential clinical applications. Our model is designed to be scalable, allowing\nfuture work to also explore larger-scale implementations. With additional pre-training and computational resources, the\nproposed method could generate even more refined results."}, {"title": "A Pre-processing", "content": "The registration process aligns corresponding structures across scans, ensuring the model focuses on disease-related\nchanges. The Learn2Reg challenge, associated with MICCAI 2020 and 2021, provides valuable insights for choosing\na suitable registration method for lung CT scans. This challenge compared various approaches on clinically relevant\ntasks. In the lung CT task, the goal was to register expiration scans to inspiration scans, with corresponding landmarks\nprovided for accuracy evaluation [19]. Notably, CorrField [18], a non-rigid registration method, emerged as the top\nperformer among 15 methods, including deep learning and conventional approaches. CorrField achieved a target\nregistration error (the Euclidean distance between corresponding landmarks in the warped fixed and moving scan) of\nonly 1.75mm [19].\nWe use the faster version (implemented in [17]) of CorrField method [18], a non-learning-based unsupervised method.\nCorrField first employs Foerstner operator [15] to extract distinctive keypoints in one 3D volume. Then a dissimilarity\ndistribution over a densely quantized space of displacements is calculated. Finally, a parts-based model is used to infer\nthe smooth motion of connected keypoints and regularize the correspondence field. Specifically, a minimum spanning\ntree (MST) is generated from the set of sparse keypoints, which enables exact message passing using belief propagation\non the graph to regularise the displacement costs. To ensure the disease progression model focuses solely on lung\ntissue, we define keypoints within lung masks for registration. After aligning the longitudinal CT scans to the baseline\nCT scan using image registration, we replaced the non-lung region in the registered scans with the corresponding\nregion from the baseline CT scan. This eliminates distractions from surrounding body parts. We leverage the default\nhyperparameters from the validated implementation (https://grand-challenge.org/algorithms/corrfield/)\nfor registration. To verify registration quality, all registered scans undergo visual inspection by me to identify and\nexclude those with significant errors. However, because of the non-clinical background, there is still risk compared with\nverification conducted by experienced radiologists."}, {"title": "B Additional experiments", "content": "In this section, we began by conducting experiments on the initial stage of the model to show the reconstruction\nperformance of 3D-VQ-GAN, both qualitatively and quantitatively. Additionally, we explore the impact of varying\nhyperparameters on the reconstruction performance. Subsequently, we assess the proposed two-stage model's effective-\nness in disease progression modelling and interpolation tasks, demonstrating its capability to capture the dynamics of\ndisease progression. Finally, we present visualizations of the learned codebook for enhanced interpretability."}, {"title": "C Training and implementation details", "content": "We train the model for two stages, 3D-VQ-GAN and latent disease ODE. All models are trained on a NVIDIA A100\n80GB GPU."}, {"title": "C.1 3D-VQ-GAN", "content": "In line with recommendations from [16, 10], the training of 3D-VQ-GAN begins on all CT scans in the training set\nusing the reconstruction loss. Subsequently, the GAN loss is introduced after 10,000 steps. Hyperparameters are set as\nfollows:  \u03bbp e r c = \u03bbr e c = 4  and  \u03bbG A N = 1 . The Adam optimizer [31] is employed with a learning rate of  3 \u00d7 1 0 \u2212 4  and\n \u03b21 = 0.5 , \u03b22 = 0.9 . Training the 3D-VQ-GAN spans 20,000 steps, and the best model, determined by the smallest\ntraining loss after adding the GAN loss, is selected. The batch size is set at 1, and the accumulated batch size is 6.\nTraining the first-stage model takes approximately 10 days."}, {"title": "C.2 Disease ODE", "content": "After completing the training of 3D-VQ-GAN, we proceed to train the latent disease ODE using the AdamW optimizer\n[37]. The training spans 100 epochs, employing a batch size of 1, a learning rate of  2 \u00d7 1 0 \u2212 4 , and  \u03b21 = 0.5 , \u03b22 = 0.9 . To enhance the model's performance at later time points, we implement a linearly increasing weights strategy, assigning\nhigher loss weights for later time points. The best model, identified by the smallest training loss, is selected. Training\nthe latent disease ODE takes approximately 15 hours."}, {"title": "C.3 Implementation details", "content": "For the implementation of 3D-VQ-GAN, we adopt a similar network structure as outlined in [16]. The codebook size\nis set to  M = 256  with an embedding size of  c = 16 . We employ a compression rate  r = 4 , calculated as the ratio\nbetween  D, H, W  and  d, h, w . All input 3D CT scans are resized to  D = 96 , H = 256 , W = 256  before being fed\ninto the model. In the latent disease ODE, the neural ODE solver is implemented by stacking three 3D convolution\nlayers. The code is implemented using PyTorch 1.8."}, {"title": "D Details of the Methodology", "content": ""}, {"title": "D.1 Problem formulation", "content": "We denote the irregularly sampled longitudinal 3D imaging data of a subject as  XT = { Xto , Xt1 , ..., Xti , ..., Xtr } ,\nwhere each  Xti \u2208 RD\u00d7H\u00d7W\u00d7C  ( D : depth,  H : height,  W : width,  C : channel) and  T = { to , t1 , ..., ti , ...tr } ( ti : the\ntime of the  ith  observation of the subject). Each subject can have an arbitrary number of observations. In the context of\nIPF disease progression modelling, this translates to each patient having an arbitrary number of longitudinal volumetric\nlung CT scans. The corresponding mask for the region of interest is also segmented. Given  XT , the objective of\nthis work is to build a model capable of generating synthetic 3D imaging data at any time point between  to  and  tr ,\nillustrating the progression of the disease within a patient over time. In this study, we use 3D volumetric CT scans of\nIPF patients as an example application.\nThe proposed method has two stages: In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the\nsecond stage, a latent (ODE) is trained to model the temporal dynamics from quantised embeddings of longitudinal CT\nscans generated by the encoder in the first stage, reconstructing continuous trajectories from discrete observations in the\nlatent space (Figure 7)."}, {"title": "D.2 3D-VQ-GAN for image reconstruction", "content": "To capture the dynamics of disease progression in the latent space, which can significantly decrease computational\ncosts, the model needs to first learn an effective and compact latent representation of the input image. In the first\nstage, we adopt 3D-VQ-GAN [16] (Figure 8), which replaces 2D operations of the original VQ-GAN [10] with 3D\noperations. VQ-GAN is a variant of VQ-VAE. VQ-VAE consists of an encoder  E  and a decoder  G  and keeps a\ndiscrete codebook of learned representations in latent space. Given an input  X , the VQ-VAE tries to represent the\ninput image with embeddings from the codebook in the latent space. More specifically, the encoder  E  projects  X\ninto the embedding  z = E ( X ) \u2208 Rd\u00d7h\u00d7w\u00d7c  in latent space followed by an element-wise quantization operation  q ( \u00b7 ) ,\nwhich approximates  z  by replacing each spatial code  zi,j,k \u2208 Rc  with its nearest neighbour in the trainable codebook\n Z = { Zm } M m =1 \u2208 Rc . The discrete latent indices and embeddings after quantization are denoted as  c \u2208 Z d\u00d7h\u00d7w\nand  zq \u2208 R d\u00d7h\u00d7w\u00d7c  respectively.  zq  then goes through the decoder to reconstruct the input  X = G ( zq ) , using:\n zq = q ( z ) = arg min Zm \u2208 Z \u2225 zi,j,k \u2212 Zm \u2225.\nFor the non-differentiable quantization operation, VQ-VAE uses a straight-through estimator [3] which copies gradients\nfrom decoder input  zq  to encoder output  z  [54].\n zq = q ( z ) = arg min Zm \u2208 Z \u2225 zi,j,k \u2212 Zm \u2225 (1)\nThe VQ-VAE loss  Lvqvae  consists of three terms: reconstruction loss  Lrec , codebook loss  Lcodebook  and commitment\nloss  Lcommit . Lrec  is used for optimizing both encoder and decoder.  Lcodebook  is used only for optimizing the codebook\nby pushing embeddings in the codebook to be close to the output of the encoder.  Lcommit  is employed to enforce the\nencoder commits to an embedding in the codebook [54].\n Lvqvae = \u2225 X \u2212 X \u2225 Lrec 1 + \u2225 sg [ E ( X )] \u2212 zq \u2225 Lcodebook 2 + \u03b2 \u2225 sg [ zq ] \u2212 E ( X ) \u2225 Lcommit (2)\nwhere  sg []  is the stop-gradient operator here.\nIn addition to the VQ-VAE loss, VQ-GAN also uses GAN loss  LGAN  and perceptual loss  Lperc  to improve the\nreconstruction quality as well as increase the compression rate. Similar to 3D-VQ-GAN [16], we use two discriminators\n D2d  and  D3d .  D2d  is used to distinguish the real slice and the reconstructed slice of 2D plane.  D3d  is used to distinguish\nreal 3D input  X  and reconstruction  X  to encourage the consistency between slices:\n LGAN = log D2d/3d ( X ) + log(1 \u2212 D2d/3d ( X )) (3)"}, {"title": "D.3 Disease ODE: modelling latent disease progression dynamics", "content": ""}, {"title": "D.3.1 Overview", "content": "As shown in Figure 7, given the trained encoder of 3D-VQ-GAN, longitudinal input 3D imaging data  Xto , Xt1 , ..., Xtr\nfor each patient can be projected to a series of quantized embeddings  Zato , Zqt1 , \u2026, Zqtr\u00b7 This sequence of embeddings\ncan be considered as samples from the continuous disease trajectory of that subject in the embedding space. To\nreconstruct the continuous trajectory from discrete observations, we adapt the common latent ODE structure, an\nencoder-decoder-based latent-variable time series model. In this application, the primary focus lies on capturing changes\nwithin specific Regions of Interest (ROIs), i.e. lung area. To isolate and emphasize these areas in the analysis, we\napply a masking technique that excludes regions outside of the ROI in the latent embedding series. The overview of\nthe latent disease ODE is shown in Figure 7. Firstly, we use convolution-based gated recurrent unit (3D-ConvGRU)\nneural network [1] as an encoder to embed the input sequence  Zato , Zqt1 , \u2026, Zqtr  into a latent initial state  hto . Then,\nthe continuous latent trajectory can be generated by using an ODE solver given  hto . Finally, the latent trajectory is\nprojected back to the embedding space of 3D-VQ-GAN to get embeddings  Zqs1 , Zqs2 , ..., Zqs5  at any target timesteps\n S = { s1 , s2 , ..., ss }. Feeding this sequence to the trained decoder  G  of 3D-VQ-GAN can reconstruct the 3D imaging\ndata at target timesteps  Xs1 , Xs2 , ..., Xss ."}, {"title": "D.3.2 Latent encoder: 3D-ConvGRU", "content": "ConvGRU [1] leverages convolutions within the GRU framework, enabling it to simultaneously process both spatial\nand temporal information in sequential data. Building on the concept of ConvGRU from [1], which employs 2D\nconvolutions, this application utilizes 3D convolutions instead. This modified unit is referred to as 3D-ConvGRU\nand the corresponding update function is named 3D-ConvGRUCell. Given the sequence of quantized embeddings\n Zato , Zqt1,..., Zqtr , 3D-ConvGRU models the time series by making next-step prediction based on previous hidden\nstate in an autoregressive way. The 3D-ConvGRU is run backwards as suggested by [4] and can be formulated as:\n hti\u22121 = 3D-ConvGRUCell ( hti , Zqti ), where  hti  is the hidden state on  ti ."}, {"title": "D.3.3 Latent decoder", "content": "The latent decoder comprises three components: a neural ODE, a 3D convolution layer, and a linear composition layer.\nThis decoder architecture is adapted from [40].\nThe neural ODE defines a continuous hidden state  h ( t )  which is the solution of an ODE initial-value problem (IVP) as\nfollows [4, 45]. Here the initial status is  hto  produced by the above 3D-ConvGRU.\n dh ( t ) dt = fo ( h ( t ) , t ) , h ( to ) = hto (6)\n fo  is a neural network parameterized by  \u03b8  and  f\u03b8  defines the dynamics of  h ( t ) . By employing a numerical ODE solver,\nthe hidden states  hs 1 , s 2 , ..., ss  at any target timesteps can be obtained based on the initial status  hto . This method excels\nby accommodating more complex dynamics within the latent state, as opposed to relying on restrictive assumptions\nlike linearity [46, 30]. This enables more flexible disease progression modelling by using neural networks to directly\nparameterize the changes in the hidden state. Subsequently, a single 3D convolution layer takes two hidden states\n hsi , hsi\u22121  and outputs the difference map  Ds , approximating the difference  \u0394zqsi  between the current  zqs , and the\nprevious  zqs \u2212 1 ."}, {"title": "D.4 Reconstruction performance of 3D-VQ-GAN", "content": "This experiment investigates the influence of two critical hyperparameters\u2014codebook vocabulary size and compression\nrate\u2014on the performance of the 3D-VQ-GAN model. The codebook vocabulary size specifies the number of discrete\nlatent vectors in the codebook  Z , which serve as building blocks for representing input data in the latent space. A larger\nvocabulary size enables the model to capture finer details in 3D CT scans but increases computational demands. In\ncontrast, the compression rate controls the degree of dimensionality reduction during encoding, reducing the complexity\nof the latent space representation. While a higher compression rate simplifies the model, it risks losing information\nand compromising reconstruction quality. To identify the optimal configuration, the model was trained on a training\ndataset and evaluated on internal and external test sets (Table 3) using quantitative metrics (e.g., reconstruction error)\nand qualitative visual inspection. A vocabulary size of 256 and a compression rate of 4 were chosen as they offered the\nbest balance between detail preservation, computational efficiency, and reconstruction performance. Figure 6 showcases\nexamples of input 3D CT scans and the corresponding reconstructions generated by VQ-GAN models trained with\ndifferent hyperparameter settings."}, {"title": "D.5 Visualization of codebook", "content": "Every entry in the codebook corresponds to a distinctive representation or code assigned to a specific region or pattern\nwithin the input space. Utilizing the techniques outlined in [23], we visually represent each code in the learned codebook\nby creating a latent representation  zq  using only that specific code. The resulting latent representation is then projected\nback into the 3D image space (see Figure 9). These visualizations exhibit varying grayscale intensities and textures,\nhighlighting the diverse characteristics associated with different codes. This diversity within the codebook suggests that\nthe model has effectively captured a broad array of features, enabling it to generate samples with varied and realistic\nqualities."}, {"title": "E Related Work", "content": ""}, {"title": "E.1 Synthetic medical image generation", "content": "Synthetic medical image generation proves particularly useful in various applications", "types": "nunconditional and conditional, depending on whether constraints (e.g., images, a specific disease state, imaging\nmodality, etc.) are applied respectively. The most common generative models used for image generation include\nGenerative Adversarial Network (GAN) [7", "32": "and diffusion models [20", "world behind the image\". This requires not only higher computational resources but\nalso more advanced techniques to model these detailed 3D structures with fidelity. The challenge is further amplified in\nthe medical domain, where accurate representation of anatomical structures and simulation of physiological processes\nadd significant layers of complexity.\nMethods employing 3D-GANs have been proposed for the synthesis of 3D imaging data [13, 49": ".", "53": ".", "14": ".", "2": ".", "47": "."}, {"32": "has gained popularity for its explicit latent space representation and stable training process.\nThe Vector Quantized Variational Autoencoder (VQ-VAE) [54", "VQ-VAE-2\n[43": "utilized hierarchical multi-scale latent maps for large-scale image generation. VQ-GAN [10", "16": "extended VQ-GAN for image modelling to 3D-VQ-GAN for video modelling.\nWhile pure GAN-based models dominate 3D medical image generation, VAE and VQ-VAE architectures are gaining\ntraction. Existing applications focus primarily on brain and heart MRI scans [35, 52"}]}