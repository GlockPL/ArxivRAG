{"title": "ExPath: Targeted Pathway Inference for Biological Knowledge\nBases via Graph Learning and Explanation", "authors": ["Rikuto Kotoge", "Ziwei Yang", "Zheng Chen", "Yushun Dong", "Yasuko Matsubara", "Jimeng Sun", "Yasushi Sakurai"], "abstract": "Biological knowledge bases provide systemically functional path-\nways of cells or organisms in terms of molecular interaction. How-\never, recognizing more targeted pathways, particularly when in-\ncorporating wet-lab experimental data, remains challenging and\ntypically requires downstream biological analyses and expertise. In\nthis paper, we frame this challenge as a solvable graph learning and\nexplaining task and propose a novel pathway inference framework,\nExPath, that explicitly integrates experimental data, specifically\namino acid sequences (AA-seqs), to classify various graphs (bio-\nnetworks) in biological databases. The links (representing path-\nways) that contribute more to classification can be considered as\ntargeted pathways. Technically, ExPath comprises three compo-\nnents: (1) a large protein language model (pLM) that encodes and\nembeds AA-seqs into graph, overcoming traditional obstacles in\nprocessing AA-seq data, such as BLAST; (2) PathMamba, a hybrid\narchitecture combining graph neural networks (GNNs) with state-\nspace sequence modeling (Mamba) to capture both local interactions\nand global pathway-level dependencies; and (3) PathExplainer, a\nsubgraph learning module that identifies functionally critical nodes\nand edges through trainable pathway masks. We also propose ML-\noriented biological evaluations and a new metric. The experiments\ninvolving 301 bio-networks evaluations demonstrate that pathways\ninferred by ExPath maintain biological meaningfulness. We will\npublicly release curated 301 bio-network data soon. The source\ncode is available at: https://anonymous.4open.science/r/ExPath", "sections": [{"title": "1 INTRODUCTION", "content": "Decades of research have generated extensive network biology data,\nrevealing that systems, from cells to organisms, can be considered\nmolecular networks and have been reported in extensive literature\n[1, 6, 23]. These data resources have been compiled into functional\nbiological knowledge bases, such as KEGG [27] and STRING [55],\nframing bio-network resources [32, 46] that document various inter-\nactions between molecules (e.g., genes or proteins) to describe how\nmolecular behaviors relate to biological systems. These knowledge\nbases are now widely used to mine and interpret wet-lab experi-\nmental data, enabling researchers to study disease mechanisms [5],\ndrug interactions [69], and potential therapeutic targets [37].\nWhile biological knowledge bases are comprehensive and con-\ntinuously updated, a main concern remains: they lack specificity\nfor experimental data. The bases cannot provide the information\non which interactions are more relevant to the given data, even\nthough their main purpose is to interpret the data. As shown in\nFigure 1, two experimental datasets from the same disease with\ndifferent mutations share the same network structure, but fail to\nreveal which distinct interactions account for these differences.\nHowever, inferring key molecular interactions is crucial for un-\nderstanding the potential roles of genes or proteins, potentially\naccelerating new biomarker discovery [72]. Conceptually, in-lab\nexperimental data, such as amino acid sequences or gene expression\nprofiles, is typically generated under specific research objectives\nor experiments, often focusing on proteins or genes of interest.\nHowever, knowledge bases provide only generic networks. They\ncollect \"meaningful\" molecular interactions via automated text min-\ning and manual curation [26], but do not incorporate the specific\ndata that \"confer such meaningfulness\" into the network-building\nprocess. As a result, they cannot provide data-specific interactions,\nand this generalization can lead to misinterpretations of experi-\nmental results. A few complementary bio-tools, such as BLAST\n[2], combined with downstream analytic methods [21], can help\ninfer bio-networks. However, these tools are not user-friendly for\nnon-experts, as they require domain expertise for implementation\nand information management [40]. Moreover, the downstream anal-\nysis generally provides empirical explanations for the data, and\nsometimes, extensive in-lab evaluation, such as pairwise examina-\ntions among hundreds to thousands of genes [68], is needed. There\nis thus an urgent need for a method that integrates bio-network\nresources with experimental data to infer targeted interactions,\nthereby facilitating the efficient use of knowledge bases.\nBio-network inference, including computational and learning-\nbased methods, has emerged as a promising solution for mining\ntargeted interactions. Such bio-network inference has attracted\ngreat attention from both computational biology and machine\nlearning researchers [30, 62]. Existing methods typically form bio-\nnetwork topology as graph data, embed experimental data as node\nfeatures, and define proper objective functions to infer meaningful\nsubgraphs. Computational methods typically incorporate statistical\nnode-centric metrics in graph theory, such as node degrees [31, 33],\ncentrality [20, 60], betweenness [44], or PageRank scores [24, 41],\nto evaluate the importance of nodes within in a graph. The edges\nconnecting these top-ranked nodes, or those highlighted during\nnode evaluations, can be identified as more targeted interactions.\nHowever, such objectives lack explicit inference of interactions,\ni.e., edges, and are often computationally intractable for large bio-\nnetworks [45]. Moreover, computational methods cannot fully ex-\nploit experimental data, as the statistical metrics do not integrate\nnode features into scoring schemes. By contrast, machine learning\nmethods, particularly graph neural networks (GNNs), model both\nnetwork topology and node-level attributes for subgraph inference\nin a data-driven manner [49, 61]. These methods set up objective\nfunctions such as link prediction and graph reconstruction to ex-\nplicitly infer targeted interactions. More importantly, experimental\ndata can directly influence the objective functions through node\naggregation in GNN learning, ensuring that the model outputs\nare more specific to the data. However, most existing works are\ntask-specific, and their objectives aim to learn the general graph\nstructure accurately, including irrelevant interactions [62, 71]. Some\nworks propose to gradually infer subgraph structure, mitigating\nthe constraints of prior general bio-network information [30, 36].\nNonetheless, they do not explicitly learn the distinct interactions\nunique to different experimental data. Furthermore, existing meth-\nods typically require downstream biological analysis to interpret\nthe model outputs or interactions. We acknowledge that current\nexplorations in bio-network inference remain nascent.\nIn this paper, we study a novel and critical problem of develop-\ning a bio-network inference framework that explicitly generates\ndata-specific interactions while maintaining biological plausibil-\nity. We note that this is a non-trivial task. In essence, we mainly\nface three challenges. (1) Qualitative interaction inference objective.\nThe first challenge involves formulating a new objective that not\nonly learns but also qualitatively assesses interactions directly. The\nmodel outputs, therefore, enable the direct interpretation of tar-\ngeted interactions, eliminating the need for downstream analysis."}, {"title": "(2) Pathway modeling. One primary focus of knowledge bases is\non \"pathways\" [26], i.e., connected multi-interactions, representing\na sequence of events where one protein interaction triggers the\nnext, ultimately leading to a defined outcome. However, existing\nworks often treat all interactions equally and uniformly, focusing\nprimarily on isolated interactions. A reliable method should explic-\nitly incorporate pathway information into the modeling process to\nmaintain biological plausibility. (3) ML-oriented evaluation. Cur-\nrently, there is no standardized quantitative evaluation framework\ntailored for machine learning models. Most evaluations depend on\nqualitative methods, such as enrichment analysis [21], to determine\nwhether the resulting interactions are biologically relevant, which\nrequires domain expertise. A quantitative evaluation method, de-\nsigned to directly assess the model outputs, would bridge the gap\nbetween computational and biological disciplines.\nIn this paper, we present ExPath, a deep learning framework for\ninferring targeted pathways for bio-networks. To tackle the above\nchallenges, (1) we formulate bio-network inference as a subgraph\nlearning and explanation task. Subgraphs, contributing most sig-\nnificantly to the learning objective, can be identified as targeted\ninteractions. (2) To ensure these subgraphs capture high-order\npathways, technically, we propose two novel models: PathMamba,\na hybrid learning model, combines graph neural networks (GNNs)\nwith state-space sequence modeling (Mamba) to learn both local in-\nteractions and global pathway-level dependencies; PathExplainer,\na novel variation of GNNExplainer, designed as a subgraph explana-\ntion module, identifies objective-critical pathways. We take amino\nacid (AA) sequences as the reference experimental data since many\nbiological databases organize pathway information at the protein\nlevel [15]. We utilize a large protein language model (PLM) (ESM-\n2 [38]) to encode AA sequences into graph embeddings, driving\nExPath to mine targeted pathways effectively. (3) We propose\nan evaluation workflow that directly incorporates model-derived\nweights of subgraphs to quantitatively assess their biological sig-\nnificance. Overall, our contributions are:\n\u2022 Formulating Bio-network Inference Problem. We formulate\nand make an initial investigation on a novel research problem of\ninferring data-specific pathways for bio-networks.\n\u2022 Proposing New Framework. ExPath consists of PathMamba\nand PathExplainer, tailored for pathway-level modeling, can\ninterpret data directly. It achieves the best fidelity+ and fidelity-\nacross 10 baselines, showing both sufficiency and necessity.\n\u2022 Impacting Biological Relevance. We propose ML-oriented\nbiological evaluations and a new metric. The experiments involv-\ning 301 bio-networks evaluations demonstrate that pathways\ninferred by ExPath maintain biological meaningfulness.\n\u2022 Datasets. We collected all available human networks from the\nKEGG, constructed machine-learning-ready datasets, and will\nrelease them soon."}, {"title": "2 PRELIMINARY AND PROBLEM SETTING", "content": "2.1 Pathways in Biological Knowledge Bases\nKnowledge bases organize these pathways as interconnected graphs,\nwhere nodes represent proteins and edges denote interactions (e.g.,\nenzymatic reactions, regulatory effects). These pathways help re-\nsearchers understand how different molecules work together rather\nthan looking at just one interaction at a time. For example, while\nprotein-protein interactions (PPI) [46] capture pair-wise molecu-\nlar associations, pathways show how many proteins are linked in\ncomplex networks. This helps scientists see the \"big picture,\" reveal-\ning how different parts of a cell cooperate to perform important\ntasks [26]. Learning from amino acid (AA) sequence data is chal-\nlenging due to its inherent complexity. Even slight variations can\nlead to significant structural changes, potentially disrupting protein\nfunctionality within pathways. Several studies focus on extracting\nmeaningful features from AA sequences, like like AlphaFold [25].\nIn this paper, we investigate the mapping of AA sequence data to\ncorresponding pathway bio-networks."}, {"title": "2.2 Problem Setting", "content": "Definition 1 (Amino Acid sequence data). Consider a collec-\ntion of M AA sequences, denoted as S = {s(m) }M\nm=1, where each\nsequence S(m) has a length L(m) (which may vary across m) and\nis represented as S(m) = [s(m)\n1, S\n2, ...,s\nL(m) ]. Here, each amino\nacid s(m) belongs to the standard set of 20 canonical amino acids.\nDefinition 2 (Knowledge bio-networks). The bio-networks can\nbe represented as a graph G = (V,E), where V denotes the\nvertices (e.g., AA sequences) and & is the set of edges, represent-\ning molecular interactions. Let G = {G(m)}M\nm=1 denote a dataset\ncomprising M bio-networks. Each G(m) is associated with a label\ny(m) \u2208 Y, indicating its primary biologically functional class (e.g.,\nmetabolism, genetic information processing, and human diseases).\nProblem (Targeted Pathway Inference). We frame this research\nproblem as a two-phase graph learning and subgraph explanation\ntask. Given G and the labels Y, we focus on two main tasks:\n\u2022 Task-1 (Classification): Train a classifier F(.) to predict the\nfunction label y(m) of an unseen bio-network G(m), driven by\nthe node features, i.e., AA sequence data S. To perform accu-\nrate classification, it can effectively learn various pathways in\nbiological knowledge bases.\n\u2022 Task-2 (Explanation): For each predicted function class y \u2208 Y,\nemploy an explainer E(\u00b7) to extract a class-specific subgraph G\nGy that highlights the most influential pathways contributing to\nthe classification outcome.\nIn plain words, a subgraph learned from various biological path-\nways and capable of accurately predicting its associated bio-network\ncan be considered as representing targeted pathways. Let Gy =\nE(G, y; S) denote subgraphs for bio-network y, integrated to exper-\nimental data S. Here, we hold three hypotheses for our method:\n\u2022 Hypothesis-1. The classifier F() leverages bio-network topol-\nogy and AA seq features to achieve high prediction accuracy\nwhile maintaining balanced performance across all classes.\n\u2022 Hypothesis-2. The explainer E() infers subgraphs that retain\nhigh fidelity for class discrimination and exhibit distinct struc-\ntures across classes, reflecting unique biological mechanisms.\n\u2022 Hypothesis-3. Both F() and E() must maintain biologically\nplausible mechanisms, ensuring that the inferred subgraphs cap-\nture high-order pathways and hold biological meaningfulness."}, {"title": "3 PROPOSED METHOD", "content": "3.1 Framework\nExPath comprises three components: large protein model encod-\nings, graph-based classification, and post-hoc subgraph explana-\ntion, as shown in Figure 2. First, we encode AA sequence data\nusing a pre-trained large protein model, ESM-2 [38] (see Section\n3.2), to serve as node attributes. To address (Task-1, Hypothe-\nsis-1), PathMamba, a classifier combining graph neural networks\n(GNNs) with state-space sequence modeling (Mamba), is to capture\nboth local node-pair interactions and global pathway-level de-\npendencies (see Section 3.3). To address (Task-2, Hypothesis-2),\nPathExplainer, an explainer method trained with pathway-wise\nmasks (see Section 3.4), aims to identify the most influential sub-\ngraphs. We explicitly integrate pathway-level information into both\nmodels to satisfy Hypothesis-3. The large protein model ensures\nbiologically meaningful protein representations, PathMamba lever-\nages various pathway information in knowledge bases for robust\nclassification, and PathExplainer highlights minimal subgraphs\nthat drive the final predictions, offering interpretable insights into\nkey pathways. We evaluate ExPath from both machine learning\nand biological perspectives, as detailed in Section 4.\n3.2 Data Encoding for Node Attributes\n3.2.1 Large Protein Language Model Encoding. The ESM-2 model\n[38], pre-trained on over 60 million AA sequences with parameter\nscaling up to 15 billion, is employed to encode our data. We evaluate\ndifferent parameter variants of ESM-2, and the results are presented\nin Table 1. Formally, each S(m) is tokenized and passed through\nstacked Transformers. The output is a token vector, denoted as: h\u2081 =\nH(L), where H(L) \u2208 Rd is the embedding of the first token from\nthe L-th (last) Transformer layer, serving as data representation.\n3.2.2 Positional Encoding. To address a fundamental limitation\nof GNNs [64] or hybrid models [48] to distinguish nodes with\nidentical local structures, we apply a random-walk-based positional\nencoding (RWPE) base on a diffusion process [14], defined as: pi =\n[RWii, RWij, RWik] \u2208 Rk where RW = AD\u22121 is the random\nwalk operator, constructed by the adjacency matrix A and degree\nmatrix D. For each node i, RW captures the probability of returning\nto node i after k steps of random walk.\nThe final node representation combines the sequence-level fea-\ntures from ESM-2 and the structural information from the graph.\nSpecifically, the sequence embedding hi and the positional encoding\npi are concatenated and passed through a linear layer to obtain the\nfinal representation: x\u2081 = Linear([hi||pi]), where [hi||pi] \u2208 Rd+K\ndenotes the concatenation of h\u012f \u2208 Rd and pi \u2208 RK. The linear\ntransformation ensures dimensionality reduction and effective in-\ntegration of both global protein sequence features and local graph\nstructural information. This final node feature x\u00a1 e Rd is optimized\nfor downstream tasks such as graph classification.\n3.3 PathMamba: Pathway Information Learning\nPathMamba integrates the Graph Isomorphism Network (GIN) with\na novel pathway-wise Mamba model. It leverages the strengths of\nboth global selective modeling mechanisms and message-passing"}, {"title": "GNNs. Specifically, inspired by GPS [48], our model avoids early-\nstage information loss that could arise from using GNNs exclusively\nin the initial layers. We hence employ pathway-wise global aggre-\ngation in combination with an efficient Mamba mechanism [18]. At\neach layer, node and edge features are updated by aggregating the\noutputs of a pathway-wise Mamba aggregation as:", "content": "$$x^{l+1}_{i} = PathMamba(X^l, A),$$\n$$x^{l+1}_{i} = LocalGIN^l(X^l, A),$$\n$$x^{l+1}_{i} = GlobalMamba^l(X^l, A),$$\n$$X^{l+1}= MLP^l(X^{l+1}_l +X^{l+1}_g),$$\nwhere A \u2208 R^{N\u00d7N} is the adjacency matrix of a graph with N\nnodes and E edges; Xl \u2208 R^{N\u00d7d} represents the d-dimensional node\nfeatures at layer l; LocalGINl is a GIN; GlobalMambal is a global\npathway-wise aggregation layer; and MLPl is a two-layer multilayer\nperceptron (MLP) used to combine local and global features.\n3.3.1 Node-wise local aggregation. Node features are updated by\naggregating information from their local neighbors. The GIN oper-\nation can be expressed as:\n$$X^{l+1} = ReLU(W^l \\cdot ((1+ \\epsilon)X^l + \\sum_{j \\in \\mathcal{N}(i)} X^l_j )).$$\nwhere N(i) represents the set of neighbors of node i, Wl is the\nlearnable weight matrix at layer l, and \u03f5 is a trainable parameter\ncontrolling the importance of self-loops. This ensures a high level\nof expressivity for local feature aggregation.\n3.3.2 Pathway-wise global aggregation. To capture long-range de-\npendencies within pathways, we propose (1) random pathway sam-\npling and (2) sequential pathway modeling in PathMamba.\nRandom Pathway Sampling. Formally, for each node vi, we\nrandomly sample a varied, single pathway with a maximum length\nof L. The sampling process is defined as:\n$$Q = \\{q^i | q^i \\sim Pathway(v_i, L), |q^i| \\le L\\},$$\nwhere N is the number of nodes in the graph, and qi represents\nthe sampled pathway for node vi. Each pathway qi is a sequence\nof nodes {vi, vi\u2081, vi\u2082, ..., vi\u2097 }, sampled according to a random walk\nprocess [56]. The sampling process Pathway (vi, L) involves select-\ning a sequence of connected nodes starting from vi. The selection\nof each subsequent node is determined probabilistically, guided by\nthe graph adjacency structure.\nSequential Pathway Modeling. The forward propagation of\nthe Mamba layer aggregates long-range dependencies along the\nsampled pathways. For each sampled pathway qi \u2208 Q(Xl), the\nMamba layer processes the pathway sequentially as:\n$$\\Delta_t = \\tau \\Delta(f(x^l_t)), B_t = f(x^l_t), C_t = f_c(x^l_t),$$\n$$h_t = (1 - \\Delta_t \\cdot D)h_{t-1} + \\Delta_t \\cdot B_t x^l_t,$$\n$$x^{l+1}_t = C_t h_t,$$\nwhere xlt is the t-th input node feature matrix in pathway qi at\nlayer l. f are learnable projections and ht is hidden state. t^ is"}, {"title": "the softplus function. The forgetting term (1 \u2013 \u0394t \u00b7 D) implements\na selective mechanism analogous to synaptic decay or inhibitory\nprocesses that diminish outdated or irrelevant information. Con-\nversely, the update term \u0394t \u00b7 B mirrors gating that selectively\nreinforces and integrates salient new information. The projection\nCt translates the internal state into observable outputs. By process-\ning each sampled pathway individually, the Mamba layer effectively\naggregates information along each pathway. The aggregated path-\nway representations are then combined to form the updated node\nfeatures Xl+1 for the next layer.\n3.3.3 Graph Classification. The updated node features are aggre-\ngated using a max pooling to generate a graph representation. This\nrepresentation is passed through an MLP layer for classification:", "content": "$$\\hat{y} = MLP(MaxPooling(\\{h_v^l\\}_{v=1}^N)),$$\nwhere N is the number of nodes, and \u0177 is the predicted class label\nfor the pathway. The model is trained using the cross-entropy loss:\n$\\mathcal{L}_{cross-entropy} = - \\sum_{i=1}^C y_i log \\hat{y}_i$, where C is the number of classes,\nyi is the ground truth label, and \u0177i is the prediction.\n3.4 PathExplainer: Targeted Pathway Inference\nPathExplainer directly infer subgraphs to generate targeted path-\nways by leveraging the interpretability of PathMamba. Vallina GN-\nNexplainers [39, 65], which focus primarily on the node or edge\nlevel, often struggle to capture the global structures at the path-\nway level. In contrast, PathExplainer introduces a key technical\nnovelty by training pathway masks, where entire pathways (i.e.,\nsequences of connected nodes and edges) are selectively masked\nduring training to evaluate their contributions to PathMamba.\nPathExplainer formalizes the identification of important sub-\ngraphs as an optimization problem. For a given graph G = (V, E),\nthe explanation is defined as (Gs, Fs), where Gs \u2286 G is the sub-\ngraph and Fs represents the selected features. The explanation is\nderived by optimizing the mutual information MI (\u00b7) between the\nsubgraph and the model's prediction:\n$$max MI (Y, (G_s, F_s)) = H(Y) - H(Y | G = G_s, X = F_s),$$\nwhere H(Y) is the entropy of the predictions and H(Y | G =\nGs, X = Fs) is the conditional entropy given the explanation.\nThe optimization is approached by learning a pathway mask M\nfor the sampled pathway's edges and nodes. To enhance the inter-\npretability and biological relevance of the pathway mask, random\npathways Q are sampled as described in Section 3.3.2. For each\nnode vi, a single random pathway qi of length up to L is sampled.\nThese pathways are then used to restrict the mask learning process\nto edges within the sampled pathways, ensuring that the learned\nM focuses on them. Specifically, the adjacency matrix A is mod-\nified based on the pathway mask M as A' = A \u2299 \u03c3(M), where \u03c3\ndenotes the sigmoid function. Similarly, the features are masked as\nX' = X \u2299 \u03c3(M). The loss function for PathExplainer combines two\ncomponents: a cross-entropy term for prediction consistency and\nregularization terms for sparsity:"}, {"title": "$$\\mathcal{L}_{mask} := - \\sum_{c=1}^C 1[y=c] log P(Y=c | G=A', X=X')d + \\lambda ||M||,$$\nwhere ||M|| encourages sparsity in the edge selection, and \u03bb bal-\nances the trade-off between the classification loss and the sparsity\nregularization. Hence, the identified important subgraphs and node\nfeatures (referring to AA sequence data) that contribute most to\nspecific bio-networks can considered as targeted pathways.", "content": "4 EXPERIMENTS AND RESULTS\nDataset and Preprocessing. We collected all available human\npathway networks from the widely used knowledge database, KEGG\n[27]. Our dataset consists of four main classes: Human Diseases,\nMetabolism, Molecular and Cellular Processes, and Organismal Sys-\ntems [26], covering 301 bio-networks. We searched for and down-\nloaded all the raw data for the human pathway network (referred\nto as Homo sapiens) using KEGG APIs. To construct high-quality,\ntrainable graphs, for nodes, we ensured that all protein nodes in\nthe network were linked to their reference AA sequence data [29].\nProtein nodes with a lack of or incomplete sequencing data were\nexcluded. For edges, we streamlined the network structure by re-\nmoving redundant or biologically insignificant interactions. We\ntransformed preprocessed data into machine-learning-ready revi-\nsion. The detailed data description and preprocessing pipeline can\nbe found in Appendix C and Table 4.\nExperimental Setup. We conducted 10-fold stratified K-Fold cross-\nvalidation repeated 5 times. The mean and standard deviation of\nthe results across all folds were reported for evaluation. The hy-\nperparameters were determined using grid search to identify the\noptimal configuration for the model. Training for all models was\naccomplished on NVIDIA A6000 GPU and Xeon Gold 6258R CPU.\n4.1 Experiment-I: Pathway Representation\nObjective. Evaluate the classification performance on unseen bio-\nnetworks, in line with Hypothesis-1, and benchmark the results\nagainst baseline models.\nBaselines and Metrics. We collected baselines from both message-\npassing GNNs and more advanced graph models, including GCN\n[35], GraphSAGE [19], GAT [58], GIN [64], GPS [48], and Graph-\nMamba [59]. The detailed introduction of these baselines and the\nselection motivation can be found in Appendix D. We used precision,\nrecall, and overall accuracy for the performance evaluation. We\nused 650M ESM-2 for PathMamba and all baselines.\nResults. Table 1 presents PathMamba achieves the highest accuracy\n(0.754), outperforming all GNNs, GPS (0.726), GraphMamba (0.723).\nFurthermore, it secures best or second-best positions across all\nfunctional categories, demonstrating its robust ability to generalize\nacross diverse pathway structures. The superior performance of the\nproposed method highlights its effectiveness in extracting and lever-\naging biologically meaningful structural information from pathway\nnetworks. The gray-shaded rows indicate the results of removing\nESM-2 and modifying the model size in terms of F1 scores. When\nESM-2 is removed, the accuracy deteriorate significantly (0.75 \u2192\n0.44), highlighting the importance of AA-seq and the limitations"}, {"title": "of prior studies that were unable to leverage this information. In-\ncreasing the model size gradually improves the results; however,\naccuracy does not incline with the 3B model (0.754 \u2192 0.742). This\nsuggests that excessively large features may lead to overfitting or\nnoise, particularly in capturing functional pathways. Table 2 com-\npares the training and inference times of our model with other\nexpressive hybrid models, using a batch size of 32. Our training\ntime is 30% faster than GPS and inference time is 27% faster than\nGraph-Mamba (complexity analysis can be found in Appendix B).\n4.2 Experiment-II: Pathway Inference\nObjective. Quantify the fidelity of extracted subgraphs, following\nour Hypothesis-2, and validate the importance of pathways specific\nto biological functions.\nBaselines and Metrics. We collected baselines from conventional\nstatistical methods (Random Sampling [34], PersonalizedPageR-\nank [24], and MinimumDominatingSet [44, 63]), gradient-based\nmethods (Saliency [52], InputXGradient [51], Deconvolution [42],\nShapleyValueSampling [54], and GuidedBackpropagation [53]), and\nGNN-specific explainer methods (GNNExplainer [65] and PGEx-\nplainer [39]). All details can be found in Appendix D. We evaluated\nthe distinctiveness of the pathways inferred by PathExplainer\nusing fidelity metrics, specifically Fidelity+ and Fidelity-. Fidelity+\nmeasures how well the important features identified by the model\ncontribute to accurate predictions. In contrast, the fidelity- evalu-\nates the drop in prediction accuracy when the identified important\nfeatures are retained while others are removed. All details can be\nfound in Appendix D and E.\nResults. Figure 3 illustrates that PathExplainer achieves the high-\nest fidelity+ and the lowest fidelity-, indicating its ability to explain\nnecessary and sufficient subgraphs effectively. GNN-specific or"}, {"title": "gradient-based methods (blue points) show lower fidelity- com-\npared to traditional methods (green points), demonstrating that the\nlearned AA-seq enables the identification of sufficient subgraphs.\nPGExplainer exhibits lower fidelity+ than GNNExplainer, suggest-\ning that instance-based approaches may be better suited for ex-\nplaining diverse pathways.\n4.3 Experiment-III: Biological Meaningfulness", "content": "Overview. We proposed an evaluation workflow to analyze the bi-\nological significance of the extracted subgraphs and pathways. This\nworkflow directly integrates the weighting/ranking scores of path-\nway inferred by PathExplainer into biological metrics, enabling\nthe direct quantification of outputs from deep learning models.\nSpecifically, this includes: Breadth: The diversity of biological func-\ntions represented by the subgraphs. Depth: The extent to which\ngene nodes contribute to these biological functions. Reliability: The"}, {"title": "5 RELATED WORK", "content": "Existing methods for analyzing the structure of biological networks\ntypically represent these networks as graphs, aiming to infer sub-\ngraphs or extract relevant interactions, and can generally be cate-\ngorized into statistical topology-driven and data-driven deep graph\nlearning methods.\nTopology-driven methods utilize statistical metrics on struc-\ntural properties of graphs, such as node degrees [31, 33], centrality\n[20, 44, 60], betweenness, or PageRank scores [24, 41] to infer which\nsubstructures or edges exert a more significant influence on the\noverall topology, thereby identifying more targeted interactions\namong genes or proteins."}, {"title": "Deep graph learning methods incorporate experimental data dur-\ning the learning process by embedding data as node representations.\nThey train graph neural networks (GNNs) with suitable objectives,\nsuch as link prediction or graph reconstruction [43, 67, 70], and the\nlinks that contribute most to these objectives can be considered\nthe targeted interactions. For instance, the classic GCN algorithm,\nGraphSAGE [19], has been validated on protein-protein interaction\n(PPI) datasets to predict protein functions within networks. The\nwork of [17] introduced a GCN-based method for predicting pro-\ntein functions, leveraging sequence features derived from a protein\nlanguage model alongside structural information. Chen et al. [11]\nemployed a graph attention network to extract drug and protein AA-\nSeq features for predicting drug-target interactions. Moreover, GNN\nmodels have been applied to incorporate RNA-Seq data, for tasks\nlike predicting disease states and cell-cell relationships [49, 61].", "content": "6 CONCLUSION\nWe introduced ExPath, a novel framework for understanding tar-\ngeted pathways within biological database. ExPath integrates (1)\na protein language model (PLM) for encoding AA-seqs into graph\nfeatures, (2) PathMamba, a hybrid model to capture local and global\ndependencies, and (3) PathExplainer, a subgraph learning module\nthat identifies key nodes and edges via trainable pathway masks.\nWe also introduced ML-oriented biological evaluations and a new"}, {"title": "metric.The experiments involving 301 bio-networks evaluations\ndemonstrated that pathways inferred by ExPath maintain biologi-\ncal meaningfulness. We will release all code and curated 301 bio-\nnetwork data to facilitate reproducibility and enable future research\nin data-specific bio-network inference. Future work will expand\nExPath to analyze other types of bio-networks, enabling broader\napplications in systems biology and medicine.", "content": "APPENDIX\nA GRAPH ISOMORPHISM AND WL TEST\nGraph isomorphism refers to the problem of determining whether\ntwo graphs are structurally identical, meaning there exists a one-\nto-one correspondence between their nodes and edges. This is\na crucial challenge in graph classification tasks, where the goal\nis to assign labels to entire graphs based on their structures. A\nmodel that can effectively differentiate non-isomorphic graphs is\nsaid to have high expressiveness, which is essential for accurate\nclassification. In many cases, graph classification models like GNNS\nrely on graph isomorphism tests to ensure that structurally distinct\ngraphs receive different embeddings, which improves the model's\nability to correctly classify graphs.\nWeisfeiler-Lehman (WL) test is a widely used graph isomor-\nphism test that forms the foundation of many GNNs. In the 1-WL\nframework, each node's representation is iteratively updated by\naggregating information from its neighboring nodes, followed by\na hashing process to capture the structural patterns of the graph.\nGNNs leveraging this concept, such as Graph Convolutional Net-\nworks (GCNs) and Graph Attention Networks (GATs), essentially\nperform a similar neighborhood aggregation, making them as ex-\npressive as the 1-WL test in distinguishing non-isomorphic graphs\n[64"}]}