{"title": "Freehand Sketch Generation from Mechanical Components", "authors": ["Zhichao Liao", "Di Huang", "Heming Fang", "Yue Ma", "Fengyuan Piao", "Xinghui Li", "Long Zeng", "Pingfa Feng"], "abstract": "Drawing freehand sketches of mechanical components on multimedia devices for AI-based engineering modeling has become a new trend. However, its development is being impeded because existing works cannot produce suitable sketches for data-driven research. These works either generate sketches lacking a freehand style or utilize generative models not originally designed for this task resulting in poor effectiveness. To address this issue, we design a two-stage generative framework mimicking the human sketching behavior pattern, called MSFormer, which is the first time to produce humanoid freehand sketches tailored for mechanical components. The first stage employs Open CASCADE technology to obtain multi-view contour sketches from mechanical components, filtering perturbing signals for the ensuing generation process. Meanwhile, we design a view selector to simulate viewpoint selection tasks during human sketching for picking out information-rich sketches. The second stage translates contour sketches into freehand sketches by a transformer-based generator. To retain essential modeling features as much as possible and rationalize stroke distribution, we introduce a novel edge-constraint stroke initialization. Furthermore, we utilize a CLIP vision encoder and a new loss function incorporating the Hausdorff distance to enhance the generalizability and robustness of the model. Extensive experiments demonstrate that our approach achieves state-of-the-art performance for generating freehand sketches in the mechanical domain.", "sections": [{"title": "1 Introduction", "content": "Nowadays, with the vigorous development of multimedia technology, a new mechanical modeling approach has gradually emerged, known as freehand sketch modeling [27, 28]. Different from traditional mechanical modeling paradigms [67], freehand sketch modeling on multimedia devices does not require users to undergo prior training with CAD tools. In the process of freehand sketch modeling for mechanical components, engineers can utilize sketches to achieve tasks such as component sketch recognition, components fine-grained retrieval based on sketches [66], and three-dimensional reconstruction from sketches to components. Modeling in this way greatly improves modeling efficiency. However, limited by the lack of appropriate freehand sketches for these data-driven studies in the sketch community, the development of freehand sketch modeling for mechanical components is hindered. It is worth emphasizing that manual sketching and collecting mechanical sketches is a time-consuming and resource-demanding endeavor. To address the bottleneck, we propose a novel two-stage generative model to produce freehand sketches from mechanical components automatically.\nTo meet the requirements of information richness and accuracy for modeling, we expect that freehand sketches used for mechanical modeling maintain a style of hand-drawn while preserving essential model information as much as possible. Previous works that generate engineering sketches [15, 40, 45, 51, 59], primarily focus on perspective and geometric features of models. As a result, their sketches lack a hand-drawn style, making them unsuitable as the solution of data generation for freehand sketch modeling. Existing data-driven freehand sketch generation methods [3, 5, 11, 30, 31, 34, 50, 54, 57, 68] also fall short in this task because they require the existence and availability of relevant datasets. While CLIPasso [56] and LBS [26] can produce abstract sketches without additional datasets"}, {"title": "2 Related Work", "content": "Due to little research on freehand sketch generation from mechanical components, there is a review of mainstream generation methods relevant to our work in the sketch community.\nTraditional Generation Method In the early stages of sketch research, sketches from 3D models were predominantly produced via traditional edge extraction methodologies [4, 40, 42, 47, 52, 60]. Among them, Occluding contours [42] which detects the occluding boundaries between foreground objects and the background to obtain contours, is the foundation of non-photorealistic 3D computer graphics. Progressions in occluding contours [42] have catalyzed advancements in contour generation, starting with Suggestive contours [7], and continuing with Ridge-valley lines [44] and kinds of other approaches [19, 40]. A comprehensive overview [6] is available in existing contour generalizations. Similarly to the results of generating contours, Han et al. [15] present an approach to generate line drawings from 3D models based on modeling information. Building upon previous work that solely focused on outlines of models, CAD2Sketch [14] addresses the challenge of representing line solidity and transparency in results, which also incorporates certain drawing styles. However, all of these traditional approaches lack a freehand style like ours.\nLearning Based Methods Coupled with deep learning [17, 35-39, 64], sketch generation approaches [3, 5, 11, 31, 45, 54, 63, 68] have been further developed. Combining the advantage of traditional edge extraction approaches for 3D models and deep learning, Neural Contours [31] employs a dual-branch structure to leverage edge maps as a substitution for sketches. SketchGen [45], SketchGraphs [51], and CurveGen and TurtleGen [59] produce engineering sketches for Computer-Aided Design. However, such approaches generate sketches that only emphasize the perspective and geometric features of models, which align more closely with regular outlines, the results do not contain a freehand style. Generative adversarial networks (GANs) [12] provide new possibilities for adding a freehand style to sketches [11, 29, 32, 41, 58]. These approaches are based on pixel-level sketch generation, which is fundamentally different from how humans draw sketches by pens, resulting in unsuitability for freehand sketch modeling. Therefore, sketches are preferred to be treated as continuous stroke sequences. Recently, Sketch-RNN [13] based on recurrent neural networks (RNNs) [65] and variational autoencoders (VAEs) [21], reinforcement learning [9, 61, 70], diffusion models [18, 34, 57] are explored for generating sketches. However, they perform poorly in generating mechanical sketches with a freehand style due to the lack of relevant training datasets. Following the integration of Transformer [55] architectures into the sketch generation, the sketch community has witnessed the emergence of innovative models [30, 50, 59]. CLIPasso [56] provides a powerful image to abstract sketch model based on CLIP [48] to generate vector sketches, but this method will take a long time to generate a single sketch. More critically, CLIPasso [56] initializes strokes by sampling randomly, and optimizes strokes by using an optimizer for thousands of steps rather than based on pre-trained weights, leading to numerical instability. Despite LBS [26] being an improvement over Clipasso [56], it performs unsatisfactorily in generalization capability for inputs unseen or transformed. Compared to many previous approaches, our proposed generative model can produce vector sketches based on mechanical components, persevering key modeling features and a freehand style, greatly meeting the development needs of freehand sketch modeling."}, {"title": "3 Method", "content": "We first elaborate on problem setting in section 3.1. Then, we introduce our sketch generation process that presents Stage-One (CSG) and Stage-Two (FSG) of MSFormer in sections 3.2 and 3.3"}, {"title": "3.1 Problem Setting", "content": "Given a mechanical component, our goal is to produce a freehand sketch. As depicted in Figure 2, it is carried out by stages: contour sketch generator and freehand sketch generator. We describe an mechanical component as $M \\in \\mathbb{A}^3$, where $\\mathbb{A}^3$ represents 3D homogeneous physical space. Each point on model corresponds to a coordinate $(x_i, y_i, z_i) \\in \\mathbb{R}^3$, where $\\mathbb{R}$ is information dimension. Through an affine transformation, a 3D model is transformed into 2D contour sketches $C \\in \\mathbb{A}^2$, which consists of a series of black curves expressed by pixel coordinates $(x_i, y_i) \\in \\mathbb{R}^2$. In the gradual optimization process of Stage-Two, process sketches ${P_i}_{K_1}$ are guided by guidance sketches ${G_i}_{1}^{K}$. $K$ is the number of sketches. Deriving from features of contour sketch $C$ and guidance sketches ${G_i}_{1}^{K}$, our model produces an ultimate output freehand sketch $S$, which is defined as a set of n two-dimensional B\u00e9zier curves ${S_1, S_2,..., S_n}$. Each of curve strokes is composed by four control points $s_i = {(x_1, y_1)^{(i)}, (x_2, y_2)^{(i)}, (x_3, y_3)^{(i)}, (x_4, y_4)^{(i)}} \\in \\mathbb{R}^8, \\forall i \\in n$."}, {"title": "3.2 Stage-One: Contour Sketch Generator", "content": "Contour Sketch Generator (CSG), called Stage-One, is designed for filtering noise (colors, shadows, textures, etc.) and simulating the viewpoint selection during human sketching to obtain recognizable and informative contour sketches from mechanical components. Previous methods optimize sketches using details such as the distribution of different colors and variations in texture. However, mechanical components typically exhibit monotonic colors and subtle texture changes. We experimentally observe that referencing this information within components not only fails to aid inference but also introduces biases in final output stroke sequences, resulting in the loss of critical features. As a result, when generating mechanical sketches, the main focus is on utilizing the contours of components to create modeling features.\nModeling engineers generally choose specific perspectives for sketching rather than random ones, such as three-view (Front/Top/Right views), isometric view (pairwise angles between all three projected principal axes are equal), etc. As shown in Figure 2 Stage-One, we can imagine placing a mechanical component within a cube and selecting centers of the six faces, midpoints of the twelve edges, and eight vertices of the cube as 26 viewpoints. Subsequently, we use PythonOCC [46], a Python wrapper for the CAD-Kernel Open-CASCADE, to infer engineering modeling information and render regular contour sketches of the model from these 26 viewpoints. Generated contour sketches are not directly suitable for subsequent processes. By padding, we ensure all sketches are presented in appropriate proportions. Given that most mechanical components exhibit symmetry, the same sketch may be rendered from different perspectives. We utilize ImageHash technology for deduplication. Additionally, not all of generated sketches are useful and information-rich for freehand sketch modeling. For instance, some viewpoints of mechanical components may represent simple or misleading geometric shapes that are not recognizable nor effective for freehand sketch modeling. Therefore, we design a viewpoint selector based on ICNet [69], which is trained by excellent viewpoint sketches picked out by modeling experts, to simulate the viewpoint selection task engineers face during sketching, as shown in Figure 2. Through viewpoint selection, we obtained several of the most informative and representative optimal contour sketches for each mechanical component. The detailed procedure of Stage-One is outlined in Algorithm 1."}, {"title": "3.3 Stage-Two: Freehand Sketch Generator", "content": "Stage-Two, in Figure 2, comprises the Freehand Sketch Generator (FSG), which aims to generate freehand sketches based on regular contour sketches obtained from Stage-One. To achieve this goal, we design a transformers-based [26, 33, 50] generator trained by guidance sketches, which stably generates freehand sketches with precise geometric modeling information. Our generative model does not require additional datasets for training. All training data are derived from the excellent procedural sketches produced by the guidance sketch generator.\nGenerative Process As illustrated in Figure 2, freehand sketch generator consists of four components: an encoder, a stroke generator, a guidance sketch generator, and a differentiable raster-izer. Our encoder utilizes CLIP ViT-B/32 [48] and an adapter to extract essential vision and semantic information from input. Although, in previous works, CLIPasso [56] performs strongly in creating abstract sketches, it initializes strokes by sampling randomly and uses an optimizer for thousands of steps to optimize sketches, resulting in a high diversity of outputs and numerical instability. To a ensure stable generation of sketches, we design a training-based stroke generator that employs improved CLIPasso [56] from the guidance sketch generator as ideal guidance. It allows us to infer high-quality sketches stably by utilizing pre-trained weights. Our stroke generator consists of eight transformer decoder layers and two MLP decoder layers. During training, to guarantee the stroke generator learns features better, process sketches ${P_i}_{1}^{K}$ (K=8 in this paper) extracted from each intermediate layer are guided by guidance sketches ${P_i}_{x\\_1}$ generated at the corresponding intermediate step of the optimization process in the guidance sketch generator. In the inference phase, the stroke generator optimizes initial strokes generated from trainable parameters into a set of n Bezier curves ${S_1, S_2, ..., S_n}$. These strokes are then fed into the differentiable rasterizer $\\mathbb{R}$ to produce a vector sketch $S = \\mathbb{R}(s_1,..., s_n) = \\mathbb{R}({(x_j, y_j)^{(1)}}_{j=1},..., {(x_j, y_j)^{(n)}}_{j=1})$.\nEdge-constraint Initialization The quality of guidance sketches plays a pivotal role in determining our outcomes' quality. Original CLIPasso [56] initializes strokes via stochastic sampling from the saliency map. It could lead to the failure to accurately capture features, as well as the aggregation of initial strokes in localized areas, resulting in generated stroke clutter. To address these issues, as shown in Figure 3, we modify the mechanism for initializing strokes in our guidance sketch generator. We segment contour sketches using SAM [22] and based on segmentation results accurately place the initial stroke on the edges of component's features to constraint stroke locations. It ensures guidance generator not only generates precise geometric modeling information but also optimizes the distribution of strokes. Initialization comparison to original CLIPasso [56] is provided in the .\nEncoder FSG requires an encoder to capture features. Previous works for similar tasks predominantly employ a CNN encoder that solely relies on local receptive fields to capture features, making it susceptible to local variations and resulting in poor robustness for inputs unseen or transformed. While vision transformer (ViT) uses a self-attention mechanism [55] to establish global relationships between features. It enables the model to attend to overall information in inputs, unconstrained by fixed posture or shape. Therefore, we utilize ViT-B/32 model of CLIP [48] to encode semantic understanding of visual depictions, which is trained on 400 million image-text pairs. And we combine it with an adapter that consists of two fully connected layers to fine-tune based on training data. As shown in Figure 7 and Table 1, our encoder substantially improves the robustness to unseen models during training and the equivariance. Loss Function During training, we employ CLIP-based perceptual loss to quantify the resemblance between generated freehand sketch S and contour sketch C considering both geometric and semantic differences [48, 56]. For synthesis of a sketch that is semantically similar to the given contour sketch, the goal is to constrict the distance in the embedding space of the CLIP model represented by CLIP(x), defined as:\n$L_{semantic} = \\phi(CLIP(C), CLIP(S)),$ (1)\nwhere $\\phi$ represents the cosine proximity of the CLIP embeddings, i.e., $\\phi(x, y) = 1 - cos(x, y)$. Beyond this, the geometric similarity is measured by contrasting low-level features of output sketch and input contour, as follows:\n$L_{geometric} = \\sum_{i=3,4} dist(CLIP_i(C), CLIP_i(S)),$ (2)\nwhere $dist$ represents the $L_2$ norm, explicitly, $dist(x, y) = ||x - y||_2$, and $CLIP_i$ is the i-th layer CLIP encoder activation. As recommended by [56], we use layers 3 and 4 of the ResNet101 CLIP model. Finally, the perceptual loss is given by:\n$L_{percept} = L_{geometric} + \\beta_s L_{semantic},$ (3)\nwhere $\\beta_s$ is set to 0.1.\nIn the process of optimizing the stroke generator, a guidance loss is employed to quantify the resemblance between guidance sketches G and process sketches P. Firstly, we introduce the Jonker-Volgenant algorithm [24] to ensure that guidance loss is invariant to arrangement of each stroke's order, which is extensively utilized in assignment problems. The mathematical expression is as follows:\n$L_{JK} = \\sum_{k=1}^{K} min_{\\alpha} \\sum_{i=1}^{n} L_1(g_{\\alpha^{(i)}}, p^{(i)}),$ (4)\nwhere $L_1$ is the manhattan distance, n is the number of strokes in the sketch. $p^{(i)}$ $p$ is the i-th stroke of the sketch from the k-th middle process layer (with a total of K layers), and $g_k$ is the guidance stroke corresponding to $p_{\\alpha^{(i)}}$, $\\alpha$ is an arrangement of stroke indices. Additionally, we innovatively integrate bidirectional Hausdorff distance into the guidance loss, which serves as a metric quantifying the similarity between two non-empty point sets that our strokes can be considered as. It aids the model in achieving more precise matching of guidance sketch edges and maintaining structural relationships between shapes during training, thereby capturing more global features and enhancing the model's robustness to input with transformations. The specific mathematical expression is as follows:\n$\\delta_H = max{\\delta_H (G, P), \\delta_H (P, G)},$ (5)\nwhere $P = {p_1,..., p_n}$ is the process sketch from each layer and $G = {g_1,..., g_n}$ is the guidance sketch corresponding to P. $g_i$ and $p_i$ represent the strokes that constitute corresponding sketch. Both P and G are sets containing n 8-dimensional vectors. $\\delta_H (G,P)$ signifies the one-sided Hausdorff distance from set G to set P:\n$\\delta_H(G, P) = max_{g \\in G}{min_{p \\in P}{||g - p||}},$ (6)\nwhere || || is the Euclidean distance. Similarly, $\\delta_H (P, G)$ represents the unidirectional Hausdorff distance from set P to set G:\n$\\delta_H(P, G) = max_{p \\in P}{min_{g \\in G}{||p - g||}}.$ (7)\nThe guidance loss is as follows:\n$L_{guidance} = L_{JK} + \\beta_h \\delta_H,$ (8)\nwhere $\\beta_h$ is set to 0.8.\nOur final loss function is as follows:\n$L_{toatl} = L_{percept} + L_{guidance}.$ (9)"}, {"title": "4 Experiments", "content": "Dataset We collect mechanical components in STEP format from TraceParts [1] databases, encompassing various categories. On the collected dataset, we employ hashing techniques for deduplication ensuring the uniqueness of models. Additionally, we remove models with poor quality, which are excessively simplistic or intricate, as well as exceptionally rare instances. Following this, we classify these models based on ICS [2] into 24 main categories. Ultimately, we obtain a clean dataset consisting of 926 models for experiments.\nImplementation Details All experiments are conducted on the Ubuntu 20.04 operating system. Our hardware specifications include an Intel Xeon Gold 6326 CPU, 32GB RAM, and an NVIDIA GeForce RTX 4090. The batch size is set to 32. Contour sketches from Stage-one are processed to a size of 224 x 224 pixels.\nQualitative Evaluation\nDue to the absence of research on the same task, we intend to compare our approach from two perspectives, which involve approaches designed for generating engineering sketches and existing state-of-the-art freehand sketches generative methods."}, {"title": "4.3 Quantitative Evaluation", "content": "Metrics Evaluation We rasterize vector sketches into images and utilize evaluation metrics for image generation to assess the quality of generated sketches. FID (Fr\u00e9chet Inception Distance) [16] quantifies the dissimilarity between generated sketches and standard data by evaluating the mean and variance of sketch features, which are extracted from Inception-V3 [53] pre-trained on ImageNet [23]. GS (Geometry Score) [20] is used to contrast the geometric information of data manifold between generated sketches and standard ones. Additionally, we apply the improved precision and recall [25] as supplementary metrics following other generative works [43]. In this experiment, we employ model outlines processed by PythonOCC [46] as standard data, which encapsulate the most comprehensive engineering information. The lower FID and GS scores and higher Prec and Rec scores indicate a greater degree of consistency in preserving modeling features between the generated sketches and the standard data. As shown in Table 1, we classify generated sketches into three levels based on the number of strokes (NoS): Simple (16 \u2264 NoS < 24 strokes), Moderate (24 \u2264 NoS < 32 strokes), and Complex (32 \u2264 NoS < 40 strokes). The first part of Table 1 showcases comparisons between our approach and other competitors, revealing superior FID, GS, Precision, and Recall scores across all three complexity levels. Consistent with the conclusions of qualitative evaluation, our approach retains more precise modeling features while generating freehand sketches. Additional metrics evaluation (standard data employ human-drawn sketches) is provided in the Appendix.\nUser Study We randomly select 592 mechanical components from 15 main categories in collected dataset as the test dataset utilized in user study. We compare results produced by Han et al. [15], Manda et al. [40], CLIPasso [56], LBS [26] and our method (the last three methods create sketches in 25 strokes). We invite 47 mechanical modeling researchers and ask them to score sketches based on two aspects: engineering information and the freehand style. Scores range from 0 to 5, with higher scores indicating better performance in creating features and possessing a hand-drawn style. Finally, we compute average scores for all components in each method. As shown in Table 2, the result of user study indicates that our method achieves the highest style score and overall score. These reveal our results have a human-prefer freehand style and a better comprehensive performance in balancing information with style."}, {"title": "4.4 Performance of the Model", "content": "Different from traditional sketch generation methods, our generative model does not require additional sketch datasets. All training sketches are produced from our guidance sketch generator, which is optimized via CLIP [48], a model pre-trained on four billion text-image pairs, producing high-quality guidance sketches. Benefiting from the guidance sketch generation process not being limited to specific categories, our method demonstrates robustness across a wide variety of mechanical components. In Figures 1 and 6, we showcase excellent generation results for various mechanical components. More qualitative results are provided in Appendix."}, {"title": "4.5 Ablation Study", "content": "Stage-One As shown in Figure 9, the results of the method lacking Stage-One are susceptible to issues such as producing unstructured features and line distortions in qualitative ablation experiment. Excellent metric scores in Table 3 demonstrate our complete framework can create richer and more accurate modeling information. This improvement is attributed to Stage-One, which filters out noise information such as color, texture, and shadows, mitigating their interference with the generation process.\nEdge-constraint Initialization In order to verify whether edge-constraint initialization can make precise geometric modeling features, we remove the optimized mechanism in the initial process. Comparison in Figure 9 clearly demonstrates that sketches generated with edge-constraint initialization(E-I) exhibit better performance in details generation and more reasonable stroke distribution. These benefit from E-I ensuring that initial strokes are accurately distributed on the edges of model features. Similarly, we utilized quantitative metrics to measure the generation performance. As shown in Table 3, sketches generated after initialization optimization achieve improvements in metrics such as FID, GS, and so on. Hausdorff distance Loss Hausdorff distance is a metric used to measure the distance between two shapes, considering not only the spatial positions but also the structural relationships between shapes. By learning shape invariance and semantic features, the model can more accurately match shapes with different transformations and morphologies, aiding in the model's equivariance. The ablation experimental result is depicted in Table 3. It is evident that all the quantitative metrics for our method training with Hausdorff distance become better on the transformed test dataset."}, {"title": "5 Conclusion and Future Work", "content": "This paper proposes a novel two-stage framework, which is the first time to generate freehand sketches for mechanical components. We mimic the human sketching behavior pattern that produces optimal-view contour sketches in Stage-One and then translate them into freehand sketches in Stage-Two. To retain abundant and precise modeling features, we introduce an innovative edge-constraint initialization. Additionally, we utilize a CLIP vision encoder and propose a Hausdorff distance-based guidance loss to improve the robustness of the model. Our approach aims to promote research on data-driven algorithms in the freehand sketch domain. Extensive experiments demonstrate that our approach performs superiorly compared to state-of-the-art methods.\nThrough experiments, we discover that we would better utilize a comprehensive model rather than direct inference to obtain desirable outcomes for unseen models with significant geometric differences. In future work, we will explore methods to address this issue, further enhancing the model's generalizability."}, {"title": "A Initialization Analysis", "content": "In this section, we will meticulously contrast and analyze our Edge-constant initialization with the original CLIPasso [56] initialization method.\nAs described in CLIPasso [56], it utilizes the ViT-32/B CLIP [48] to obtain the salient regions of a target image. This is achieved by averaging the attention outputs from all attention heads across each self-attention layer, generating a total of 12 attention maps. These maps are further averaged to derive the relevancy map, obtained by examining the attention between the final class embedding and all 49 patches. Subsequently, this relevancy map is combined with the edge map obtained through XDOG [60] extraction. The resulting attention map is then utilized to determine the locations for the initial strokes. In the process of determining the initial positions of strokes, CLIPasso [56] utilizes random seeds on the saliency map to sample positions for the first control point of each curve. Following this, it randomly selects the subsequent three control points of each Bezier curve within a small radius (0.05) of the initial point.\nSuch random initialization methods often result in the initial points of strokes being inadequately distributed around the critical features of mechanical components during sketch generation, leading to the loss of substantial modeling information. Moreover, this approach frequently results in an excessive placement of initial stroke points in certain prominent feature areas, causing confusion in generating sketch strokes and preventing accurate representation of modeling features. To address this issue, we propose the edge-constant initialization to deterministically sample. We utilize SAM [22] to perform feature segmentation on the input contour sketch. Based on the segmentation results, we predefine four stroke initialization points evenly spaced along the edge of each segmented feature. Subsequently, we dynamically change the initialization points based on the comparison with the manually required number of generated strokes. If the requested number of strokes is less than the total predefined initialization points, we evenly discard points contained within each segmented feature. Conversely, if the requested number of strokes exceeds the total predefined initialization points, we employ a greedy algorithm on the saliency map of the target image to determine additional stroke initialization points in the most salient regions [26]. This initialization method not only ensures the precise generation of mechanical component features but also optimizes the distribution of generated strokes, resulting in clearer generated sketches."}, {"title": "B Stability Analysis", "content": "In this section, we will evaluate the stability of our transformer-based [26, 33, 50] stroke generator.\nIn Stage-Two, our improved initialization method has enhanced the guidance sketch generator to produce informative freehand sketches. However, the guidance sketch generator employs an optimizer to create sketches through thousands of optimization iterations during sketch generation, leading to uncertainty in the outcomes. Each step of this optimization-based process is guided by CLIP [48] in terms of both semantic and geometric similarities to create strokes. This optimization process is uncontrollable and the optimized result from each step exhibits variability. It results in unstable and uncontrollable quality performance of the generated sketches. In order to consistently generate high-quality sketches, we adopt a transformer-based [26, 33, 50] generative framework. We extract intermediate sketches from the optimization process of the guidance sketch generator as ideal guides for process sketches from each intermediate layer in the stroke generator. we utilize guidance loss during training to ensure that the stroke generator learns features from corresponding intermediate process guidance sketches. We employ CLIP-based [48] perceptual loss to ensure the similarity between the generated freehand sketches and contour sketches in both geometry and semantic information. Through training, all learned features are fixed into determined weights. During the inference phase, our model can rapidly infer freehand sketches based on the trained weights. This generation approach ensures output consistency and achieves satisfactory generation quality."}, {"title": "C Implementation Details", "content": "In order to tailor our method specifically for freehand sketch generation in engineering freehand sketch modeling, we build a CAD dataset exclusively comprising mechanical components in the STEP format. we invite numerous mechanical modeling researchers to collect mechanical components from the TraceParts [1]. They are asked to encompass a diverse array of categories to enhance the inference generalization of our generative model. In the end, we obtain a Raw dataset including nearly 2,000 mechanical components. For the collected raw dataset, we employ hashing techniques for deduplication, ensuring the uniqueness of models in the dataset. Subsequently, we remove models with poor quality, which are excessively simplistic or intricate, as well as exceptionally rare instances. Following this, we classify these models based on the International Classification for Standards (ICS) [2] into 24 main categories, comprising 180 corresponding subcategories. Ultimately, we obtained a clean dataset consisting of 926 models.\nWe implement the methods of Stage One using Python3 with PythonOCC and PyTorch, where PyTorch supports the viewpoint selector. For Stage Two, PyTorch and DiffVG are used to implement the model, where DiffVG is used for the differentiable rasterizer."}, {"title": "D Additional Quantitative Evaluation", "content": "Metrics Evaluation In section 4.3 of this paper, we employ evaluation metrics for image generation to assess the quality of generated sketches. Given the absence of benchmark datasets specifically for mechanical component freehand sketches within the sketch community, we utilize component outlines processed through PythonOCC [46], which encapsulate the most comprehensive engineering information, as the standard data. The experiment results demonstrate the superiority of our method over existing freehand sketch generation methods in preserving the modeling features of mechanical components. In this section, we will conduct quantitative metric evaluations on our method and other competitors using real human-drawn sketches of mechanical components collected by ourselves.\nWe firstly introduce the construction process of the real human-drawn sketch dataset of mechanical component. From our collection of 926 three-dimensional mechanical component dataset, we randomly select 500 components. We invite 58 researchers with sketching expertise in the mechanical modeling domain, requesting them to draw a sketch for each component from a given perspective. We then obtain a test dataset comprising 500 mechanical sketches drawn by human engineers. As shown in the Figure 12, we showcase the collection of real human-drawn sketches. Sketches of components drawn by researchers in the mechanical modeling domain preserve crucial modeling features which are essential for freehand sketch modeling. Correspondingly, certain minor details for modeling may be simplified, or overlooked by the researchers and not drawn. Moreover, it is evident that sketches crafted by humans exhibit a distinctive freehand style."}, {"title": "E Additional Qualitative Results", "content": "Figure 13, Figure 14, and Figure 15 show a large number of excellent freehand sketches of mechanical components generated by our method."}]}