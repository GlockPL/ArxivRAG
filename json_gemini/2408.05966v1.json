{"title": "Freehand Sketch Generation from Mechanical Components", "authors": ["Zhichao Liao", "Di Huang", "Heming Fang", "Yue Ma", "Fengyuan Piao", "Xinghui Li", "Long Zeng", "Pingfa Feng"], "abstract": "Drawing freehand sketches of mechanical components on multimedia devices for AI-based engineering modeling has become a new trend. However, its development is being impeded because existing works cannot produce suitable sketches for data-driven research. These works either generate sketches lacking a freehand style or utilize generative models not originally designed for this task resulting in poor effectiveness. To address this issue, we design a two-stage generative framework mimicking the human sketching behavior pattern, called MSFormer, which is the first time to produce humanoid freehand sketches tailored for mechanical components. The first stage employs Open CASCADE technology to obtain multi-view contour sketches from mechanical components, filtering perturbing signals for the ensuing generation process. Meanwhile, we design a view selector to simulate viewpoint selection tasks during human sketching for picking out information-rich sketches. The second stage translates contour sketches into freehand sketches by a transformer-based generator. To retain essential modeling features as much as possible and rationalize stroke distribution, we introduce a novel edge-constraint stroke initialization. Furthermore, we utilize a CLIP vision encoder and a new loss function incorporating the Hausdorff distance to enhance the generalizability and robustness of the model. Extensive experiments demonstrate that our approach achieves state-of-the-art performance for generating freehand sketches in the mechanical domain.", "sections": [{"title": "1 Introduction", "content": "Nowadays, with the vigorous development of multimedia technology, a new mechanical modeling approach has gradually emerged, known as freehand sketch modeling [27, 28]. Different from traditional mechanical modeling paradigms [67], freehand sketch modeling on multimedia devices does not require users to undergo prior training with CAD tools. In the process of freehand sketch modeling for mechanical components, engineers can utilize sketches to achieve tasks such as component sketch recognition, components fine-grained retrieval based on sketches [66], and three-dimensional reconstruction from sketches to components. Modeling in this way greatly improves modeling efficiency. However, limited by the lack of appropriate freehand sketches for these data-driven studies in the sketch community, the development of freehand sketch modeling for mechanical components is hindered.\nIt is worth emphasizing that manual sketching and collecting mechanical sketches is a time-consuming and resource-demanding endeavor. To address the bottleneck, we propose a novel two-stage generative model to produce freehand sketches from mechanical components automatically.\nTo meet the requirements of information richness and accuracy for modeling, we expect that freehand sketches used for mechanical modeling maintain a style of hand-drawn while preserving essential model information as much as possible. Previous works that generate engineering sketches [15, 40, 45, 51, 59], primarily focus on perspective and geometric features of models. As a result, their sketches lack a hand-drawn style, making them unsuitable as the solution of data generation for freehand sketch modeling. Existing data-driven freehand sketch generation methods [3, 5, 11, 30, 31, 34, 50, 54, 57, 68] also fall short in this task because they require the existence and availability of relevant datasets. While CLIPasso [56] and LBS [26] can produce abstract sketches without additional datasets, as shown in Figure 1, their results for mechanical components are afflicted by issues such as losing features, line distortions, and random strokes. In contrast, we propose a mechanical vector sketch generation technique that excels in maintaining precise and abundant modeling features and a freehand style without additional sketch datasets.\nOur method, the first time to generate freehand sketches for mechanical components, employs a novel two-stage architecture. It mimics the human sketching behavior pattern which commences with selecting optimal viewpoints, followed by hand-sketching. In Stage-One, we generate multi-perspective contour sketches from mechanical components via Open CASCADE, removing irrelevant information for engineering modeling which may also mislead stroke distribution in generated sketches. To select information-rich sketches, we devise a view selector to simulate the viewpoint choices made by engineers during sketching. Stage-Two translates regular contour sketches into humanoid freehand sketches by a transformer-based generator. It is trained by sketches created by a guidance sketch generator that utilizes our innovative edge-constraint initialization to retain more modeling features. Our inference process relies on trained weights to stably produce sketches defined as a set of B\u00e9zier curves. Additionally, we employ a CLIP vision encoder combining a pretrained vision transformer [8] ViT-B/32 model of CLIP [48] with an adapter [10], which utilizes a self-attention mechanism [55] to establish global relations among graph blocks, enhancing the capture of overall features. It fortifies the method's generalization capability for unseen models during training and inputs with geometric transformation (equivariance). Furthermore, our proposed new guidance loss, incorporating the Hausdorff distance, considers not only the spatial positions but also the boundary features and structural relationships between shapes. It improves model's ability to capture global information leading to better equivariance. Finally, we evaluate our method both quantitatively and qualitatively on the collected mechanical component dataset, which demonstrates the superiority of our proposed framework. We also conduct ablation experiments on key modules to validate their effectiveness.\nIn summary, our contributions are the following:\n\u2022 As far as our knowledge goes, this is the first time to produce freehand sketches tailored for mechanical components. To address this task, we imitate the human sketching behavior pattern to design a novel two-stage sketch generation framework.\n\u2022 We introduce an innovative edge-constraint initialization method to optimize strokes of guidance sketches, ensuring that outcomes retain essential modeling features and rationalize stroke distribution.\n\u2022 We utilize an encoder constituted by CLIP ViT-B/32 model and an adapter to improve the generalization and equivariance of the model. Furthermore, we propose a novel Hausdorff distance-based guidance loss to capture global features of sketches, enhancing the method's equivariance.\n\u2022 Extensive quantitative and qualitative experiments demonstrate that our approach can achieve state-of-the-art performance compared to previous methods."}, {"title": "2 Related Work", "content": "Due to little research on freehand sketch generation from mechanical components, there is a review of mainstream generation methods relevant to our work in the sketch community.\nTraditional Generation Method In the early stages of sketch research, sketches from 3D models were predominantly produced via traditional edge extraction methodologies [4, 40, 42, 47, 52, 60]. Among them, Occluding contours [42] which detects the occluding boundaries between foreground objects and the background to obtain contours, is the foundation of non-photorealistic 3D computer graphics. Progressions in occluding contours [42] have catalyzed advancements in contour generation, starting with Suggestive contours [7], and continuing with Ridge-valley lines [44] and kinds of other approaches [19, 40]. A comprehensive overview [6] is available in existing contour generalizations. Similarly to the results of generating contours, Han et al. [15] present an approach to generate line drawings from 3D models based on modeling information. Building upon previous work that solely focused on outlines of models, CAD2Sketch [14] addresses the challenge of representing line solidity and transparency in results, which also incorporates certain drawing styles. However, all of these traditional approaches lack a freehand style like ours.\nLearning Based Methods Coupled with deep learning [17, 35-39, 64], sketch generation approaches [3, 5, 11, 31, 45, 54, 63, 68] have been further developed. Combining the advantage of traditional edge extraction approaches for 3D models and deep learning, Neural Contours [31] employs a dual-branch structure to leverage edge maps as a substitution for sketches. SketchGen [45], SketchGraphs [51], and CurveGen and TurtleGen [59] produce engineering sketches for Computer-Aided Design. However, such approaches generate sketches that only emphasize the perspective and geometric features of models, which align more closely with regular outlines, the results do not contain a freehand style. Generative adversarial networks (GANs) [12] provide new possibilities for adding a freehand style to sketches [11, 29, 32, 41, 58]. These approaches are based on pixel-level sketch generation, which is fundamentally different from how humans draw sketches by pens, resulting in unsuitability for freehand sketch modeling. Therefore, sketches are preferred to be treated as continuous stroke sequences. Recently, Sketch-RNN [13] based on recurrent neural networks (RNNs) [65] and variational autoencoders (VAEs) [21], reinforcement learning [9, 61, 70], diffusion models [18, 34, 57] are explored for generating sketches. However, they perform poorly in generating mechanical sketches with a freehand style due to the lack of relevant training datasets. Following the integration of Transformer [55] architectures into the sketch generation, the sketch community has witnessed the emergence of innovative models [30, 50, 59]. CLIPasso [56] provides a powerful image to abstract sketch model based on CLIP [48] to generate vector sketches, but this method will take a long time to generate a single sketch. More critically, CLIPasso [56] initializes strokes by sampling randomly, and optimizes strokes by using an optimizer for thousands of steps rather than based on pre-trained weights, leading to numerical instability. Despite LBS [26] being an improvement over Clipasso [56], it performs unsatisfactorily in generalization capability for inputs unseen or transformed. Compared to many previous approaches, our proposed generative model can produce vector sketches based on mechanical components, persevering key modeling features and a freehand style, greatly meeting the development needs of freehand sketch modeling."}, {"title": "3 Method", "content": "We first elaborate on problem setting in section 3.1. Then, we introduce our sketch generation process that presents Stage-One (CSG) and Stage-Two (FSG) of MSFormer in sections 3.2 and 3.3."}, {"title": "3.1 Problem Setting", "content": "Given a mechanical component, our goal is to produce a freehand sketch. As depicted in Figure 2, it is carried out by stages: contour sketch generator and freehand sketch generator. We describe an mechanical component as $M \\in A^3$, where $A^3$ represents 3D homogeneous physical space. Each point on model corresponds to a coordinate $(x_i, y_i, z_i) \\in R^3$, where $R$ is information dimension. Through an affine transformation, a 3D model is transformed into 2D contour sketches $C \\in A^2$, which consists of a series of black curves expressed by pixel coordinates $(x_i, y_i) \\in R^2$. In the gradual optimization process of Stage-Two, process sketches ${P_i}_{i=1}^{K}$ are guided by guidance sketches ${G_i}_{i=1}^{K}$, K is the number of sketches. Deriving from features of contour sketch C and guidance sketches ${G_i}_{i=1}^{K}$, our model produces an ultimate output freehand sketch S, which is defined as a set of n two-dimensional B\u00e9zier curves ${S_1, S_2,..., S_n}$. Each of curve strokes is composed by four control points $s_i = \\{(x_1, y_1)^{(i)}, (x_2, y_2)^{(i)}, (x_3, y_3)^{(i)}, (x_4, y_4)^{(i)} \\} \\in R^8$, $ \\forall i \\in n$."}, {"title": "3.2 Stage-One: Contour Sketch Generator", "content": "Contour Sketch Generator (CSG), called Stage-One, is designed for filtering noise (colors, shadows, textures, etc.) and simulating the viewpoint selection during human sketching to obtain recognizable and informative contour sketches from mechanical components. Previous methods optimize sketches using details such as the distribution of different colors and variations in texture. However, mechanical components typically exhibit monotonic colors and subtle texture changes. We experimentally observe that referencing this information within components not only fails to aid inference but also introduces biases in final output stroke sequences, resulting in the loss of critical features. As a result, when generating mechanical sketches, the main focus is on utilizing the contours of components to create modeling features.\nModeling engineers generally choose specific perspectives for sketching rather than random ones, such as three-view (Front/Top/ Right views), isometric view (pairwise angles between all three projected principal axes are equal), etc. As shown in Figure 2 Stage-One, we can imagine placing a mechanical component within a cube and selecting centers of the six faces, midpoints of the twelve edges, and eight vertices of the cube as 26 viewpoints. Subsequently, we use PythonOCC [46], a Python wrapper for the CAD-Kernel Open- CASCADE, to infer engineering modeling information and render regular contour sketches of the model from these 26 viewpoints. Generated contour sketches are not directly suitable for subsequent processes. By padding, we ensure all sketches are presented in appropriate proportions. Given that most mechanical components exhibit symmetry, the same sketch may be rendered from different perspectives. We utilize ImageHash technology for deduplication. Additionally, not all of generated sketches are useful and information-rich for freehand sketch modeling. For instance, some viewpoints of mechanical components may represent simple or misleading geometric shapes that are not recognizable nor effective for freehand sketch modeling. Therefore, we design a viewpoint selector based on ICNet [69], which is trained by excellent viewpoint sketches picked out by modeling experts, to simulate the viewpoint selection task engineers face during sketching, as shown in Figure 2. Through viewpoint selection, we obtained several of the most informative and representative optimal contour sketches for each mechanical component. The detailed procedure of Stage-One is outlined in Algorithm 1."}, {"title": "3.3 Stage-Two: Freehand Sketch Generator", "content": "Stage-Two, in Figure 2, comprises the Freehand Sketch Generator (FSG), which aims to generate freehand sketches based on regular contour sketches obtained from Stage-One. To achieve this goal, we design a transformers-based [26, 33, 50] generator trained by guidance sketches, which stably generates freehand sketches with precise geometric modeling information. Our generative model does not require additional datasets for training. All training data are derived from the excellent procedural sketches produced by the guidance sketch generator.\nGenerative Process As illustrated in Figure 2, freehand sketch generator consists of four components: an encoder, a stroke generator, a guidance sketch generator, and a differentiable raster-izer. Our encoder utilizes CLIP ViT-B/32 [48] and an adapter to extract essential vision and semantic information from input. Although, in previous works, CLIPasso [56] performs strongly in creating abstract sketches, it initializes strokes by sampling randomly and uses an optimizer for thousands of steps to optimize sketches, resulting in a high diversity of outputs and numerical instability. To a ensure stable generation of sketches, we design a training-based stroke generator that employs improved CLIPasso [56] from the guidance sketch generator as ideal guidance. It allows us to infer high-quality sketches stably by utilizing pre-trained weights. Our stroke generator consists of eight transformer decoder layers and two MLP decoder layers. During training, to guarantee the stroke generator learns features better, process sketches ${P_i}_{i=1}^{K}$ (K=8 in this paper) extracted from each intermediate layer are guided by guidance sketches ${P_i}_{i=1}^{x}$, generated at the corresponding intermediate step of the optimization process in the guidance sketch generator. In the inference phase, the stroke generator optimizes initial strokes generated from trainable parameters into a set of n Bezier curves ${S_1, S_2, ..., S_n}$. These strokes are then fed into the differentiable rasterizer R to produce a vector sketch $S = R(s_1,..., s_n) = R(\\{(x_j, y_j)^{(1)}\\}_{j=1}..., \\{(x_j, y_j)^{(n)}\\}_{j=1})$.\nEdge-constraint Initialization The quality of guidance sketches plays a pivotal role in determining our outcomes' quality. Original CLIPasso [56] initializes strokes via stochastic sampling from the saliency map. It could lead to the failure to accurately capture features, as well as the aggregation of initial strokes in localized areas, resulting in generated stroke clutter. To address these issues, as shown in Figure 3, we modify the mechanism for initializing strokes in our guidance sketch generator. We segment contour sketches using SAM [22] and based on segmentation results accurately place the initial stroke on the edges of component's features to constraint stroke locations. It ensures guidance generator not only generates precise geometric modeling information but also optimizes the distribution of strokes. Initialization comparison to original CLIPasso [56] is provided in the .\nEncoder FSG requires an encoder to capture features. Previous works for similar tasks predominantly employ a CNN encoder that solely relies on local receptive fields to capture features, making it susceptible to local variations and resulting in poor robustness for inputs unseen or transformed. While vision transformer (ViT) uses a self-attention mechanism [55] to establish global relationships between features. It enables the model to attend to overall information in inputs, unconstrained by fixed posture or shape. Therefore, we utilize ViT-B/32 model of CLIP [48] to encode semantic understanding of visual depictions, which is trained on 400 million image-text pairs. And we combine it with an adapter that consists of two fully connected layers to fine-tune based on training data. As shown in Figure 7 and Table 1, our encoder substantially improves the robustness to unseen models during training and the equivariance.\nLoss Function During training, we employ CLIP-based perceptual loss to quantify the resemblance between generated freehand sketch S and contour sketch C considering both geometric and semantic differences [48, 56]. For synthesis of a sketch that is semantically similar to the given contour sketch, the goal is to constrict the distance in the embedding space of the CLIP model represented by CLIP(x), defined as:\n$L_{semantic} = \\phi(CLIP(C), CLIP(S))$,\nwhere $ \\phi $ represents the cosine proximity of the CLIP embeddings, \u0456.\u0435., $ \\phi(x, y) = 1 - cos(x, y)$. Beyond this, the geometric similarity is measured by contrasting low-level features of output sketch and input contour, as follows:\n$L_{geometric} = \\sum_{i=3,4} dist(CLIP_{i}(C), CLIP_{i}(S))$,\nwhere $dist$ represents the L2 norm, explicitly, $dist(x, y) = ||x - y||_2$, and $CLIP_i$ is the i-th layer CLIP encoder activation. As recommended by [56], we use layers 3 and 4 of the ResNet101 CLIP model. Finally, the perceptual loss is given by:\n$L_{percept} = L_{geometric} + \\beta_sL_{semantic}$,\nwhere $ \\beta_s $ is set to 0.1.\nIn the process of optimizing the stroke generator, a guidance loss is employed to quantify the resemblance between guidance sketches G and process sketches P. Firstly, we introduce the Jonker-Volgenant algorithm [24] to ensure that guidance loss is invariant to arrangement of each stroke's order, which is extensively utilized in assignment problems. The mathematical expression is as follows:\n$L_{JK} = \\sum_{k=1}^{K} \\sum_{i=1}^{n}min_\\alpha L_1(g_k, p_{\\alpha(i)})$,                                           (4)\nwhere $L_1$is the manhattan distance, n is the number of strokes in the sketch. $p_{(i)}$ p is the i-th stroke of the sketch from the k-th middle process layer (with a total of K layers), and $g_k$ is the guidance stroke corresponding to $p_{\\alpha(i)}$, \u03b1 is an arrangement of stroke indices. Additionally, we innovatively integrate bidirectional Hausdorff distance into the guidance loss, which serves as a metric quantifying the similarity between two non-empty point sets that our strokes can be considered as. It aids the model in achieving more precise matching of guidance sketch edges and maintaining structural relationships between shapes during training, thereby capturing more global features and enhancing the model's robustness to input with transformations. Experiment evaluation can be seen in section 4.5, The specific mathematical expression is as follows:\n$\\delta_H = max{\\{\\delta_H(G, P), \\delta_H(P, G)\\}}$,\nwhere P = {$p_1,..., p_n$} is the process sketch from each layer and G = {$g_1,..., g_n$} is the guidance sketch corresponding to P. $g_i$ and $p_i$ represent the strokes that constitute corresponding sketch. Both P and G are sets containing n 8-dimensional vectors. $\\delta_H (G,P)$ signifies the one-sided Hausdorff distance from set G to set P:\n$\\delta_H(G, P) = max_{g \\in G} \\{min_{p \\in P} ||g - p||\\}$,\nwhere || || is the Euclidean distance. Similarly, $\\delta_H (P, G)$ represents the unidirectional Hausdorff distance from set P to set G:\n$\\delta_H(P, G) = max_{p \\in P} \\{min_{g \\in G} ||p - g||\\}$.\nThe guidance loss is as follows:\n$L_{guidance} = L_{JK} + \\beta_h\\delta_H$,\nwhere $ \\beta_h $ is set to 0.8.\nOur final loss function is as follows:\n$L_{toatl} = L_{percept} + L_{guidance}$."}, {"title": "4 Experiments", "content": "We collect mechanical components in STEP format from TraceParts [1] databases, encompassing various categories. On the collected dataset, we employ hashing techniques for deduplication ensuring the uniqueness of models. Additionally, we remove models with poor quality, which are excessively simplistic or intricate, as well as exceptionally rare instances. Following this, we classify these models based on ICS [2] into 24 main categories. Ultimately, we obtain a clean dataset consisting of 926 models for experiments.\nAll experiments are conducted on the Ubuntu 20.04 operating system. Our hardware specifications include an Intel Xeon Gold 6326 CPU, 32GB RAM, and an NVIDIA GeForce RTX 4090. The batch size is set to 32. Contour sketches from Stage-one are processed to a size of 224 x 224 pixels. Detailed information about experiments is provided in the Appendix."}, {"title": "4.2 Qualitative Evaluation", "content": "Due to the absence of research on the same task, we intend to compare our approach from two perspectives, which involve approaches designed for generating engineering sketches and existing state-of-the-art freehand sketches generative methods.\nSketches of mechanical components In Figure 4, we contrast our method with Han et al. [15] and Manda et al. [40], using our collected components as inputs. Han et al. [15] use PythonOCC [46] to produce view drawings, while Manda et al. [40] create sketches through image-based edge extraction techniques. Although their results preserve plentiful engineering features, it is apparent that their outcomes resembling extracted outlines from models lack the style of freehand, which limits applicability in freehand sketch modeling. In contrast, our approach almost retains essential information of mechanical components equivalent to their results, such as through holes, gear tooth, slots, and overall recognizable features, while our results also demonstrate an excellent freehand style.\nSketch with a freehand style We compare our method with excellent freehand sketch generative methods like CLIPasso [56] and LBS [26]. Moreover, we present a contrast with DALL-E [49] which is a mainstream large-model-based image generation approach. As shown in Figure 5, all results are produced by 25 strokes using our collected dataset. In the first example, CLIPasso's [56] result exhibits significant disorganized strokes, and LBS [26] almost completely covers the handle of valve with numerous strokes, leading to inaccurate representation of features. In the second and third examples, results by CLIPasso [56] lose key features, such as the gear hole and the pulley grooves. For LBS [26], unexpected stroke connections appear between modeling features and its stroke distribution is chaotic. In contrast, our strokes accurately and clearly are distributed over the features of components. These differences are attributed to the fact that CLIPasso [56] initializes strokes via sampling randomly from the saliency map resulting in features that may not always be captured. Although LBS [26] modifies initialization of CLIPasso [56], it initializes strokes still relying on saliency maps influenced by noise information like monotonous colors and textures in mechanical components. Our method addresses this issue by introducing a novel edge-constraint initialization, which accurately places initial strokes on feature edges. Additionally, as LBS reported that its transformer-based model uses a CNN encoder. So its robustness comparison to our method will be similar to the results in Figure 7. In contrast to DALL-E [49], we employ inputs consistent with previous experiments coupled with the prompt (\"Create a pure white background abstract freehand sketch of input in 25 strokes\") as the final inputs. It is evident that the large-model- based sketch generation method is still inadequate for our task."}, {"title": "4.3 Quantitative Evaluation", "content": "Metrics Evaluation We rasterize vector sketches into images and utilize evaluation metrics for image generation to assess the quality of generated sketches. FID (Fr\u00e9chet Inception Distance) [16] quantifies the dissimilarity between generated sketches and standard data by evaluating the mean and variance of sketch features, which are extracted from Inception-V3 [53] pre-trained on ImageNet [23]. GS (Geometry Score) [20] is used to contrast the geometric information of data manifold between generated sketches and standard ones. Additionally, we apply the improved precision and recall [25] as supplementary metrics following other generative works [43]. In this experiment, we employ model outlines processed by PythonOCC [46] as standard data, which encapsulate the most comprehensive engineering information. The lower FID and GS scores and higher Prec and Rec scores indicate a greater degree of consistency in preserving modeling features between the generated sketches and the standard data. As shown in Table 1, we classify generated sketches into three levels based on the number of strokes (NoS): Simple (16 \u2264 NoS < 24 strokes), Moderate (24 \u2264 NoS < 32 strokes), and Complex (32 \u2264 NoS < 40 strokes). The first part of Table 1 showcases comparisons between our approach and other competitors, revealing superior FID, GS, Precision, and Recall scores across all three complexity levels. Consistent with the conclusions of qualitative evaluation, our approach retains more precise modeling features while generating freehand sketches. Additional metrics evaluation (standard data employ human-drawn sketches) is provided in the Appendix.\nUser Study We randomly select 592 mechanical components from 15 main categories in collected dataset as the test dataset utilized in user study. We compare results produced by Han et al. [15], Manda et al. [40], CLIPasso [56], LBS [26] and our method (the last three methods create sketches in 25 strokes). We invite 47 mechanical modeling researchers and ask them to score sketches based on two aspects: engineering information and the freehand style. Scores range from 0 to 5, with higher scores indicating better performance in creating features and possessing a hand-drawn style. Finally, we compute average scores for all components in each method. As shown in Table 2, the result of user study indicates that our method achieves the highest style score and overall score. These reveal our results have a human-prefer freehand style and a better comprehensive performance in balancing information with style."}, {"title": "4.4 Performance of the Model", "content": "Different from traditional sketch generation methods, our generative model does not require additional sketch datasets. All training sketches are produced from our guidance sketch generator, which is optimized via CLIP [48], a model pre-trained on four billion text-image pairs, producing high-quality guidance sketches. Benefiting from the guidance sketch generation process not being limited to specific categories, our method demonstrates robustness across a wide variety of mechanical components. In Figures 1 and 6, we showcase excellent generation results for various mechanical components. More qualitative results are provided in Appendix."}, {"title": "4.5 Ablation Study", "content": "Stage-One As shown in Figure 9, the results of the method lacking Stage-One are susceptible to issues such as producing unstructured features and line distortions in qualitative ablation experiment. Excellent metric scores in Table 3 demonstrate our complete framework can create richer and more accurate modeling information. This improvement is attributed to Stage-One, which filters out noise information such as color, texture, and shadows, mitigating their interference with the generation process.\nEdge-constraint Initialization In order to verify whether edge- constraint initialization can make precise geometric modeling features, we remove the optimized mechanism in the initial process. Comparison in Figure 9 clearly demonstrates that sketches generated with edge-constraint initialization(E-I) exhibit better performance in details generation and more reasonable stroke distribution. These benefit from E-I ensuring that initial strokes are accurately distributed on the edges of model features. Similarly, we utilized quantitative metrics to measure the generation performance. As shown in Table 3, sketches generated after initialization optimization achieve improvements in metrics such as FID, GS, and so on.\nHausdorff distance Loss Hausdorff distance is a metric used to measure the distance between two shapes, considering not only the spatial positions but also the structural relationships between shapes. By learning shape invariance and semantic features, the model can more accurately match shapes with different transformations and morphologies, aiding in the model's equivariance. The ablation experimental result is depicted in Table 3. It is evident that all the quantitative metrics for our method training with Hausdorff distance become better on the transformed test dataset."}, {"title": "5 Conclusion and Future Work", "content": "This paper proposes a novel two-stage framework, which is the first time to generate freehand sketches for mechanical components. We mimic the human sketching behavior pattern that produces optimal- view contour sketches in Stage-One and then translate them into freehand sketches in Stage-Two. To retain abundant and precise modeling features, we introduce an innovative edge-constraint initialization. Additionally, we utilize a CLIP vision encoder and propose a Hausdorff distance-based guidance loss to improve the robustness of the model. Our approach aims to promote research on data-driven algorithms in the freehand sketch domain. Extensive experiments demonstrate that our approach performs superiorly compared to state-of-the-art methods.\nThrough experiments, we discover that we would better utilize a comprehensive model rather than direct inference to obtain desirable outcomes for unseen models with significant geometric differences. In future work, we will explore methods to address this issue, further enhancing the model's generalizability."}, {"title": "A Initialization Analysis", "content": "In this section, we will meticulously contrast and analyze our Edge- constant initialization with the original CLIPasso [56] initialization method.\nAs described in CLIPasso [56], it utilizes the ViT-32/B CLIP [48] to obtain the salient regions of a target image. This is achieved by averaging the attention outputs from all attention heads across each self-attention layer, generating a total of 12 attention maps. These maps are further averaged to derive the relevancy map, obtained by examining the attention between the final class embedding and all 49 patches. Subsequently, this relevancy map is combined with the edge map obtained through XDOG [60] extraction. The resulting attention map is then utilized to determine the locations for the initial strokes. In the process of determining the initial positions of strokes, CLIPasso [56] utilizes random seeds on the saliency map to sample positions for the first control point of each curve. Following this, it randomly selects the subsequent three control points of each Bezier curve within a small radius (0.05) of the initial point.\nSuch random initialization methods often result in the initial points of strokes being inadequately distributed around the critical features of mechanical components during sketch generation, leading to the loss of substantial modeling information. Moreover, this approach frequently results in an excessive placement of initial stroke points in certain prominent feature areas, causing confusion in generating sketch strokes and preventing accurate representation of modeling features. To address this issue, we propose the edge- constant initialization to deterministically sample. We utilize SAM [22] to perform feature segmentation on the input contour sketch. Based on the segmentation results, we predefine four stroke initialization points evenly spaced along the edge of each segmented feature. Subsequently, we dynamically change the initialization points based on the comparison with the manually required number of generated strokes. If the requested number of strokes is less than the total predefined initialization points, we evenly discard points contained within each segmented feature. Conversely, if the requested number of strokes exceeds the total predefined initialization points, we employ a greedy algorithm on the saliency map of the target image to determine additional stroke initialization points in the most salient regions [26]. This initialization method not only ensures the precise generation of mechanical component features but also optimizes the distribution of generated strokes, resulting in clearer generated sketches.\nAs shown in Figure 10, we conduct experiments using three sampling strategies of random seeds provided by CLIPasso[56]. It is evident that in the first instance, no stroke initialization points are placed at the three through-holes of the input flange contour sketch, resulting in the loss of this important feature in the result. In the second instance, the placement of three stroke initialization points on the upper right through-hole is unnecessary, as it is a simple feature that does not require three strokes to depict. In the third instance, three stroke initialization points are clustered around the edge contour of the flange, while only two initialization points are placed on the structurally complex cylindrical section. These illustrate the irrational distribution of stroke initialization points caused by the random seed sampling method, ultimately leading to unsatisfactory sketch generation results. In contrast, our proposed edge-constant initialization optimizes the placement of stroke initialization points, ensuring their rational distribution on modeling feature edges. It can be observed that sketches generated through our improved method adequately preserve crucial modeling features, with a clear and rational distribution of strokes."}, {"title": "B Stability Analysis", "content": "In this section, we will evaluate the stability of our transformer- based [26, 33, 50", "48": "in terms of both semantic and geometric similarities to create strokes. This optimization process is uncontrollable and the optimized result from each step exhibits variability. It results in unstable and uncontrollable quality performance of the generated sketches. In order to consistently generate high-quality sketches, we adopt a transformer-based [26, 33, 50"}, {"48": "perceptual loss to ensure the similarity between the generated freehand sketches and contour sketches in both geometry and semantic information. Through training, all learned features are fixed into determined weights. During the inference phase, our model can rapidly infer freehand sketches based on the trained weights. This generation approach ensures output consistency and achieves satisfactory generation quality.\nWe design comparative experiments to validate the numerical stability of our generative framework. Using the same inputs, we conduct five rounds of sketch generation experiments separately with only the guidance sketch generator (GSG) and the complete generative framework (trained on the collected mechanical component dataset). As shown in Figure 11, the outcomes produced by the guidance sketch generator (GSG) for mechanical freehand sketches vary each time, and some of them exhibit suboptimal performance. For instance, in the case of the first instance, the distribution of the gear teeth slots varies significantly in each generated result, and due to the instability of the optimization-based generation method, issues arise such as chaotic stroke composition in the second round's results and erroneous connections between teeth slot strokes and through-hole strokes in the fifth round's generated sketches. Similar situations are also evident in the second and third instances. For example, in the second instance, the distribution of continuous sections of the flat-head screws differs in each round of the experiment. And results occasionally are accompanied by contour loss such as the loss of the bottom circle of the screw in the second round of experiments, and the loss of connection at the head of the screw in the fourth round. In the third instance, involving a complex motor model, the strokes creating the main body of the motor within the area marked by the red rectangle exhibit significant variations in distribution across each experimental round. Additionally, some results accurately depict small through-holes on the motor surface, while others fail to capture that information. The reason for these issues arises from the uncontrollable nature of the optimization"}]}