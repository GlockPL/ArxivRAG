{"title": "AGENTOCCAM: A Simple Yet Strong BASELINE FOR LLM-BASED WEB AGENTS", "authors": ["Ke Yang", "Yao Liu", "Sapana Chaudhary", "Rasool Fakoor", "Pratik Chaudhari", "George Karypis", "Huzefa Rangwala"], "abstract": "Autonomy via agents based on large language models (LLMs) that can carry out personalized yet standardized tasks presents a significant opportunity to drive human efficiency. There is an emerging need and interest in automating web tasks (e.g., booking a hotel for a given date within a budget). Being a practical use case itself, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Meanwhile, much prior research focuses on handcrafting their web agent strategies (e.g., agent's prompting templates, reflective workflow, role-play and multi-agent systems, search or sampling methods, etc.) and the corresponding in-context examples. However, these custom strategies often struggle with generalizability across all potential real-world applications. On the other hand, there has been limited study on the misalignment between a web agent's observation and action representation, and the data on which the agent's underlying LLM has been pre-trained. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. In our study, we enhance an LLM-based web agent by simply refining its observation and action space, aligning these more closely with the LLM's capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AGENTOCCAM surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AGENTOCCAM's simple design highlights LLMs' impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.", "sections": [{"title": "INTRODUCTION", "content": "AI agents leveraging large language models (LLMs) show great potential in automating repetitive and programmatic tasks and thereby alleviating human workloads (Gao et al., 2024; Xi et al., 2023; Yang et al., 2024). LLMs showcase remarkable capabilities in perception, reasoning and planning primarily due to their pre-training and post-learning. However, their effectiveness is significantly constrained when task-specific observation and action representations diverge from the parametric knowledge encoded during training of LLMs. For instance, in web-based tasks, these agents perform notably below human levels (Zhou et al., 2023b; Koh et al., 2024a).\nTo improve web task performance by LLM-based agents, recent work focuses on designing better agent policies with either handcrafted prompting templates (Sodhi et al., 2024) or hard-coded auto-prompting strategies (Fu et al., 2024; Wang et al., 2024). While those pre-defined strategies can be"}, {"title": "RELATED WORK", "content": "LLM-based Web Agent Advances in large language and multi-modal foundation models have significantly boosted the development of autonomous agents to solve web tasks. Techniques translating LLMs to powerful decision-making agents (Yao et al., 2022b; Shinn et al., 2024) have led to progress in web agents, and have inspired many techniques that design inference time agent strategies. Many prior approaches improve the agent system by designing modular systems with specialized LLMs or roles, aiming to break down complex tasks (Sun et al., 2024; Prasad et al., 2024). Other works leverage LLMs to extract common patterns from examples or past experience (Zheng et al., 2023; Fu et al., 2024; Wang et al., 2024). However, this line of work often relies on pre-defined control hierarchy, prompt templates or examples to act accurately in the test environments. For example, SteP (Sodhi et al., 2024) utilizes a stack-based approach for dynamic multi-level control in the web tasks but relies on task-specific atomic policies with environment-related information hard-coded in prompt template. Another line of work focuses on improving web agents' performance by leveraging more online examples from the environments. Many of them (Zhou et al., 2023a; Zhang et al., 2024; Putta et al., 2024) adapt Monte Carlo Tree Search (MCTS) methods, expanding intermediate states (tree nodes) in one task repeatedly by multiple trials over that task. Among them, WebPilot (Zhang et al., 2024) also adds a global optimization layer for high-level planning. Koh et al. (2024b) use a trained value function to guide search and to back-trace on the task execution tree. Auto Eval and Refine (Pan et al., 2024) trains a separate evaluator, and improves the task execution using reflective thinking (Shinn et al., 2024) on past trials in the same task. However, sampling or resetting multiple times in the same task, not only increases the inference cost significantly, but also limits its applicability when failed task is not revocable. As a comparison, we highlight the simplicity of our method and its difference with related agent approaches in Table 1.\nFine-tuned or Trained Models for Web Tasks Fine-tuning language or multimodal models for web tasks is another effective approach to enhance decision-making capabilities on the web tasks (Yin et al., 2024; Hong et al., 2024; Lai et al., 2024; Putta et al., 2024). While fine-tuning promises more adaptivity and broader optimization space, the size of task-specific fine-tuned models are typically not comparable with the most powerful closed-source models. As for training models to follow natural language command on the computer or the web, there is also some early research before LLMs emerged, using semantic parsing (Artzi & Zettlemoyer, 2013), reinforcement learning (Branavan et al., 2009) and imitation learning (Liu et al., 2018; Humphreys et al., 2022). However, those fine-tuned agents, limited by the base model's capacities or training data volume, often fail to match those constructed with LLMs regarding performance or/and generalizability, and is beyond the scope of this work.\nSimulated Web Agent Environments Web agent development has been supported by increasingly complex web simulators for training and evaluation. These range from basic platforms like MiniWoB (Shi et al., 2017) and its extension MiniWoB++ (Liu et al., 2018), to more sophisticated environments such as WebShop (Yao et al., 2022a), WebArena (Zhou et al., 2023b), and VisualWebArena (Koh et al., 2024a). These simulators progressively incorporate real-world complexities, from simple form-filling to tasks across multiple full-featured websites. In this work, we focus only on the text modality, and use WebArena to evaluate our method's task success and generalizability as it contains different types of websites and task-intents in a single suite."}, {"title": "PROBLEM FORMULATION", "content": "We formalize the web interaction process by a Partially Observable Markov Decision Process (POMDP, Littman (2009); Spaan (2012)): $(O, S, A, P, R, P_0, \\gamma)$. In POMDPs, an observation $o \\in O$ consist of information that the agent receives from the web environment, e.g. HTMLs, as well as any instructions and prompts. In this work, we only consider the text modality. A state $s \\in S$ denotes the whole underlying (unobserved) state of the agent and the environment such that the state transition is Markovian. An action $a \\in A$ is either a command recognized by the web environment, or any other unrecognized token sequence that will lead to a stay in the current state. $P$ denotes a deterministic state transition function that records the change in the webpage state given the current state and agent action. $R$ is the reward function that decides the success or failure of the agent's sequence of actions. In the WebArena environment used in our work, the reward is only assigned at the end of an agent-web interaction episode. $P_0$ denotes the initial state distribution which is uniform over 812 tasks in WebArena and discounting factor $\\gamma$ is set to 1.\nTo solve POMDP, a common goal is to find a decision policy $\\pi(a_t|h_t)$ maximizing the expected cumulative reward, where $h_t$ denotes the observation history ${o_0, o_1, ..., o_t}$. In LLM-based web agent design, that is translated to designing a policy $\\pi(a_t|h_t)$ with the help of one or more base LLM policy $A_{LLM}$ and a set of algorithmic modules. In this work, we work on a special class of policies that can be expressed as: $\\pi(g(a_t)|h_t) = \\pi_{lm}(a_t|f(h_t))$, where $f$ and $g$ are rule-based functions that process the observation (including action instructions) and actions for the LLM policy. We name it the observation and action space alignment problem. Notice that under such problem setting, all of our changes apply only to the observations and the actions. We emphasize not all agent strategies in previous approaches can be represented in this way. For example: search-based algorithms require a control program on the top to select actions and trigger back-tracing; methods with evaluators, reflective thinking or memory modules also necessitate a managing center to alternate between the main LLM and these helper segments or other role-playing LLMs. In contrast, we aim to answer the following question in our work: Can we build a strong web agent with the base LLM policy $A_{LLM}$ by optimizing only the observation and action mapping $f$ and $g$?"}, {"title": "METHOD", "content": "Rather than introducing any new modules or hierarchical structures on top of the base LLM, our method focuses on a simple web agent workflow that inputs the web observations to a general-purpose LLM-API and uses the LLM outputs as actions directly. In this section, we describe the process of aligning web tasks, which necessitates embodiment knowledge, with the predominantly static and text-centric nature of LLM training. Section 4.1 discusses our strategies (summarized in Figure 2) for refining the action space to be more compact and reducing the need for the agent's embodiment capabilities. Section 4.2 outlines our methods (summarized in Figure 4) for condensing web content descriptions to be both brief and informative, and identifying key web elements and relevant steps for retention to organize the agent's memory in a pertinent manner."}, {"title": "ACTION SPACE ALIGNMENT", "content": "A web agent's action space defines the valid commands it can use to interact with web environment. The WebArena simulator supports translating three categories of actions into mouse and keyboard operations: basic actions (e.g., click, type), tab operations (e.g., tab_focus for managing active tabs), and page operations (e.g., go_back for navigation). These actions are detailed in Appendix A, along with a comparison of our changes to the action space.\nBased on our observation of common failure modes in web agents, there are two key problems that need to be solved by editing the action space: i) removing irrelevant actions that LLMs struggle to understand and frequently misuse, and ii) improve the memorization and planning ability when the task execution requires navigating multiple potential paths to successfully execute. We propose that the first can be corrected simply by removing and combining actions. The second issue was often addressed in the previous work using handcrafted rules or strategies, making these approaches hard to generalize. In this work, we address the second problem by introducing actions that allow the LLM to autonomously generate plans and manage the task workflow. These proposed solutions are explained in details below and in Figure 2."}, {"title": "OBSERVATION SPACE ALIGNMENT", "content": "The observation space of web agents consists of task objectives, instructions, previous interaction, the current web text descriptions or screenshots (see Figure 3 and Appendix D for our agent). Among them, previous interactions and current web content consumes the most number of tokens, which scales with the length of a single page and the length of history. This often results in a long context window, which not only increases the LLM inference cost but also poses challenges for LLM to extract related information accurately. Therefore, our main goal in refining the observation is to address these two aspects. Additionally, the alignment of observations is outlined in Figure 4.\nSimplifying Web Page Observations. The content on web pages are represented in HTML or ac-cessibility tree format in most text-only web agents. These formats are designed towards front-end"}, {"title": "EXPERIMENTAL RESUTS AND ANALYSIS", "content": "Environment. We utilize WebArena (Zhou et al., 2023b), an interactive web simulator, as our benchmark. WebArena consists of fully functional websites from four common domains: e-commerce platforms (OneStopShop), social forums for idea and opinion exchange (Reddit), collaborative software development (GitLab), and content management for creation and management of online data (online store management). The platform additionally includes utility tools: a map, a calculator and a scratchpad, and Wikipedia to enable human-like task-solving. The benchmark consists of 812 tasks generated from 241 templates. A template here is a parametric form of a task intent, allowing for multiple instantiations with different keywords. Each task is accompanied by a evaluator/reward function that programmatically checks the correctness of the final information with respect to the desired ground truth information\u00b2. We use GPT-4-turbo-2024-04-09 (Achiam et al., 2023) to build our AGENTOCCAM.\nBaselines. We compare AGENTOCCAM with the following prior and concurrent work: 1) WebArena agent: the Chain-of-Thought (CoT) prompted agent included in the WebArena benchmark (Zhou et al., 2023b). 2) SteP (Sodhi et al., 2024): a stack-based approach on top of 14 human-written atomic strategies tailored to solving WebArena. 3) WebPilot (Zhang et al., 2024): a multi-agent, multi-level MCTS based agent that reports state-of-the-art overall performance on WebArena. 4) Agent Workflow Memory (AWM) (Wang et al., 2024): a method automatically summarizing workflow from past experience. SteP has made their code and interaction trajectories public. Hence, we"}, {"title": "CONCLUSION", "content": "In this paper, we proposed a simple but efficient LLM-based web agent AGENTOCCAM that refines its action and observation spaces to be more comprehensible for LLMs primarily trained on static text. Unlike other methods, AGENTOCCAM stands out for its remarkably simple policy workflow, requiring no extra modules, LLM calls, or in-context examples. This simplicity does not compromise its performance; AGENTOCCAM surpasses previous and contemporary approaches on WebArena by 9.8 (SteP) and 5.9 (WebPilot) absolute points, respectively. Our result emphasize the importance of maintaining a simple agent architecture for better generalizability, echoing the Oc-cam's razor principle. In summary, AGENTOCCAM aims to lay a solid foundation and offer valuable insights for future web agent research and development."}, {"title": "A COMPARISON OF THE VANILLA AND THE ALIGNED ACTION SPACE", "content": "A COMPARISON OF THE VANILLA AND THE ALIGNED ACTION SPACE"}, {"title": "B EVALUATOR RECTIFICATIONS", "content": "We only modify the evaluator when it's deemed erroneous due to the wrong task labels or misuse of evaluating functions. When the task definition and corresponding evaluation metric match to some"}, {"title": "D AGENT PROMPTS", "content": "The general prompt template:\n\u2022 With planning\nYou are an Al assistant performing tasks on a web browser. You will be provided with task objective, current step, web page observations, previous plans, and interaction history. You need to issue an action for this step.\nGenerate the response in the following format:\n{output_specifications}\nYou are ONLY allowed to use the following action commands. Strictly adheres to the given format. Only issue one single action.\nIf you think you should refine the plan, use the following actions:\n{planning_action_specifications}\nOtherwise, use the following actions:\n{navigation-action-specifications}\n\u2022 Without planning\nYou are an Al assistant performing tasks on a web browser. You will be provided with task objective, current step, web page observations, and other relevant information. You need to issue an action for this step.\nGenerate the response in the following format:\n{output-specifications}"}]}