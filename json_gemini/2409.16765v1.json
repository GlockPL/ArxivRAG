{"title": "MaViLS, a Benchmark Dataset for Video-to-Slide Alignment, Assessing Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech, OCR, and Visual Features", "authors": ["Katharina Anderer", "Andreas Reich", "Matthias W\u00f6lfel"], "abstract": "This paper presents a benchmark dataset for aligning lecture videos with corresponding slides and introduces a novel multimodal algorithm leveraging features from speech, text, and images. It achieves an average accuracy of 0.82 in comparison to SIFT (0.56) while being approximately 11 times faster. Using dynamic programming the algorithm tries to determine the optimal slide sequence. The results show that penalizing slide transitions increases accuracy. Features obtained via optical character recognition (OCR) contribute the most to a high matching accuracy, followed by image features. The findings highlight that audio transcripts alone provide valuable information for alignment and are beneficial if OCR data is lacking. Variations in matching accuracy across different lectures highlight the challenges associated with video quality and lecture style. The novel multimodal algorithm demonstrates robustness to some of these challenges, underscoring the potential of the approach.", "sections": [{"title": "1. Introduction", "content": "Latest since the COVID-19 pandemic, online lectures have become an integral part of academic settings. The shift towards virtual education brings forth both challenges and opportunities to ensure that educational content is not only accessible but also engaging and tailored to meet diverse learning preferences [1]. Integrating video lectures with corresponding slide presentations enhances the learning experience by combining auditory and visual elements. This multimodal approach is particularly beneficial when learners have impaired auditory or visual processing, reflecting findings by [2, 3] that emphasize the importance of not relying solely on a single sensory modality in accessible software and web design. Moreover, content revision and self-paced study can be enhanced. A lecture tutoring system called PET [4, 5] aligns user questions to corresponding slides and navigates the user to the best-fitting slide by calculating the similarity between the slide text and the user question. If lecture slides are aligned with the lecture's speech as well, this could enhance navigation through slides to a large degree. The synchronization of audio and slides can also enhance chatbot functionality in general, improving their ability to link user questions with the most relevant content. Another application could be the improvement of alternative text descriptions for visuals. Visuals on slides often lack alternative text descriptions [6] leading to a lack of information for visually impaired people. If a lecturer is talking about the visuals on a particular slide, the aligned audio transcript can be used to automatically generate alternative text about these visuals.\nThe task of matching video frames to lecture slides can be quite challenging under circumstances where e.g. lectures include demonstrations, external videos, or web pages not present in the slides, or where instructors navigate slides non-linearly in response to student questions. Additional challenges arise from poor video or audio quality, which can hinder audio transcription or OCR accuracy, as well as from suboptimal recording angles or videos focusing on the instructor rather than the slides.\nPrevious research has employed methods like scale-invariant feature transform (SIFT) [7, 8, 9, 10] for slide alignment, an algorithm first proposed by [11]. Others have used global pixel differences or color histograms [12]. A tool called Talkminer, introduced by [13], presents a search engine for webcasts that uses OCR to extract keywords, enabling users to locate specific video sections using a keyword search [13]. A study by [14] already aligns electronic slides with the audio transcript, but with a focus solely on OCR features and assuming a strictly linear sequence of slides, not allowing for possible backward jumps [14]. Slidecho, a tool introduced by [15] extracts slides from lecture videos using OCR to identify the most text-rich bounding boxes and align them with speech. Limitations of this tool are that it disregards visual features and does not match video frames with high-quality PDF slides. High-resolution PDF slides can encapsulate text and graphical elements more effectively and may even accommodate alternative text descriptions for images or tables, enhancing accessibility for individuals with visual impairment, when saved in an accessible format such as PDF/UA. The presence of PDF slides can therefore play a vital role in preserving information when aligning them with video frames. Our approach combines multiple features, namely OCR, image features, and audio features, making it more robust than algorithms that consider either text or image features. It matches video frames to PDF slides to preserve as much information as possible. Additionally, it presents a more efficient method than SIFT which is rather time-consuming.\nWe publish a broad benchmark dataset, including 20 labeled lectures of various study fields, varying lengths, and different lecturers such that various presenting styles are included. This dataset, which is published together with our novel alignment algorithm on Github\u00b9, provides a diverse open-source dataset concerning domains, video and speech quality and lecturers and enables a structured analysis of different alignment algorithms."}, {"title": "2. MaViLS Dataset", "content": "The dataset MaViLS, an acronym for 'Matching Videos to Lecture Slides', includes ground truth files for 20 lectures from various fields. To create a diverse dataset, we selected lectures from the medical, engineering, and natural sciences, including a lecture on computer vision, psychology, reinforcement learning, climate science, numerics, and game design. Most of the lectures are taken from the MIT OpenCourseWare\u00b2, a broad selection of lectures with corresponding material from the Massachusetts Institute of Technology. Furthermore, two lectures by the University of Tuebingen, Germany, and one by DeepMind are selected.\nMaViLS includes the audio transcripts of the lectures, the videos, the corresponding PDF slides, and the ground truth files that match slides to video frames and spoken sentences. The audio transcription is done by faster-whisper\u00b3 with 1550 million parameters, an efficient reimplementation of OpenAI's speech recognition tool Whisper [16]. For the ground truth files, each sentence of a lecturer is mapped to a slide manually by human raters and has a video timestamp. If either no slide is captured or the slide is not uniquely identifiable, '-1' is returned as the slide label.\nDesigned to test the robustness of matching algorithms, MaViLS intentionally features lectures with challenges such as low-quality videos, frequent perspective shifts, and slides with minor differences. The video image and audio quality are assessed on a 7-point Likert scale, where 1 indicates inferior quality and 7 very good quality. Further, a volatility score is calculated that describes how often the lecturer jumps back and forth in the slides according to the total number of slides. It is calculated as the ratio between the count of slide number changes and the total amount of slides. A number close to 1 indicates that the number of slide changes was close to the amount of slides, while a high number (e.g. above 1.5) indicates a relatively high volatility. Additionally, the no slide/slide ratio is indicating the prevalence of slides shown for each video frame in comparison to video frames without slides. The ratio varies largely across lectures, from zero where every frame is showing a slide, to a ratio of ten, indicating that a frame without a slide is ten times more likely compared to a frame capturing a slide.\nMaViLS encompasses over 22 hours of video content, amounting to 12,830 distinct video segments with accompanying audio transcripts and slide labels."}, {"title": "3. Methods", "content": "We propose a novel MaViLS algorithm that utilizes a combination of features, including text utilizing OCR, visual information, and audio transcripts to accurately link each video frame to the most similar lecture slide."}, {"title": "3.1. Features", "content": "To extract text from the video frames and lecture slides, we employ the open-source framework Tesseract [17]. Subsequently, the sentence transformer model 'distiluse-base-multilingual-cased' available on Hugging Face is used to create embeddings of the extracted texts. Cosine similarity is then computed between the text embeddings to produce a similarity matrix for each slide and video frame pair. This constructed similarity matrix encapsulates the text features.\nFor the audio transcript of each video frame and the extracted text from lecture slides, an analogous procedure is employed. The sentence transformer model converts the texts to embeddings. The cosine similarities between these embeddings provide a second similarity matrix, representing the speech features.\nFor visual feature extraction, we choose the transformer model 'MBZUAI/swiftformer-xs' for its high performance regarding efficiency and accuracy. To derive the image feature embeddings, both video frames and slides are input into the model. The resultant last hidden states form the basis of our features. Computing cosine similarities between these embeddings gives a third matrix, representing the image-based features.\nThese three similarity matrices serve in the next step as the foundation for the subsequent determination of the optimal sequence of slides accompanying a lecture video."}, {"title": "3.2. Dynamic programming optimization", "content": "Our approach uses dynamic programming to systematically choose the best sequence of slides to accompany a video. We construct a decision matrix, hereafter referred to as the D matrix, where each entry $D_{i,j}$ represents the highest cumulative similarity score when matching video frame i with slide j.\nFollowing, i designates the index of a video frame and j designates the index of the current lecture slide. A third index k is used for the optimization formula below corresponding to the lecture slide index with the potentially highest cumulative score. The similarity score, denoted by $S_{i,j}$, is a measure of how well video frame i matches with slide j. $S_{i,j} \\in [-1,1]$, where 1 defines the highest possible similarity.\nThe D matrix maintains a record of the optimal cumulative scores. Each entry $D_{i,j}$ is derived by locating the best score from the previous step, denoted as $D_{i-1,j}(k)$, and adding the current similarity score $S_{i,j}$. Simultaneously, a penalty, $p_{jump}$, is imposed to discourage slide transitions that are likely coming from noise, while a reward for linear succession of slides, $p_{linear}$ is also factored in. This results in the following formula for a particular entry for D:\n$D_{i,j} = \\max_{k} (D_{i-1,j}(k) - p_{jump}(k) - p_{linear}) + S_{i,j}$                                                                                             (1)\nPenalty $p_{jump}(k)$ imposes a penalty proportional to the magnitude of the jump. The penalty is higher for backward transitions (k < j) as compared to forward ones (k > j):\n$p_{jump}(k) = \\begin{cases}\n2 \\cdot |k - j| \\cdot A_{jump} & k < j\\\\\n0 & k = j \\\\\n|k - j| \\cdot A_{jump} & k > j,\n\\end{cases}$                                                                              (2)\nwhere $A_{jump}$ is a small constant for weighting the penalty.\n$p_{linear}$ presents an incentive to follow a sequential order considering the expected slide index at a given frame. The expected slide index $e_i$ is deduced from the ratio of the total slide number m to the total frame number n and is calculated as:\n$e_i = 1 + \\frac{m}{n-1} i$                                                                                               (3)\n$p_{linear}$ is then defined as the deviation between the expected frame index and the actual index, multiplied by a small constant $A_{linear}$.\n$p_{linear} = |e_i - i| A_{linear}$                                                                                          (4)\nIn the results section different values for $A_{jump}$ (0, 0.1, 0.15, 0.2, 0.25) and for $A_{linear}$ (0, 10\u207b\u2074, 10\u207b\u00b3) are compared."}, {"title": "3.3. Combination techniques", "content": "The final step integrates the text-, image feature-, and audioscript-based similarity matrices into a combined matrix, which then feeds into the dynamic programming optimization process. As first approach, hereinafter referred to as 'mean' combination, the average value for each entry i, j is calculated, maintaining matrix dimensions for the combined similarity matrix. Similarly, as a second approach, the maximum value for each entry i, j is calculated ('max' combination).\nA third approach adopts a weighted sum of the three matrices. To identify the optimal weights $w_a, w_b, w_c$, gradient descent is performed per lecture, aiming to maximize the cumulative similarity score.\nThe combined similarity matrix S is calculated using the single feature matrices A, B, C in the following way:\n$S = w_a A + w_b B + w_c C$                                                                                             (5)\nGradient descent is started with all initial weights set equally to $w_a = w_b = w_c = 1/3$.\nThe objective is to maximize the D score per lecture (Eq. 1) by finding the optimal weights, using S as defined in Eq. 5. Gradient descent with an integrated Adam optimizer is deployed, using a learning rate of 0.001 and a total of 50 iterations to keep computational costs low."}, {"title": "4. Results", "content": "To assess the accuracy of different algorithms, we compute recall, precision and F1 scores. The task is defined as a binary classification and for each video frame it is determined whether the slide number was correctly aligned or not, ignoring labels of '-1' where no slide was captured. The variation between F1, recall, and precision scores is notably minimal across our findings. For better readability, we only discuss F1 scores subsequently."}, {"title": "4.1. Features", "content": "Text embeddings achieve the highest average score with 0.76 accuracy, succeeded by image embeddings (0.64). Audio embeddings also prove valuable, achieving an average of 0.53. For 'Product Design' audio embeddings even demonstrate superior outcomes over the other embeddings.\nThese observations suggest a contextual effectiveness of each embedding; no single embedding method uniformly outperforms the others across lectures. For instance, in scenarios where slides are devoid of text, text embeddings are naturally at a disadvantage. Further, audio embeddings can give valuable information in cases where the image quality of the video frame is inferior. Correlating audio and image quality with the accuracy of text and audio embeddings confirms this argument. Text feature accuracy correlates strongly with the image quality of the video (r = 0.78), whereas the audio-based algorithm is not affected by the image quality (r = 0.18), but is affected by the audio quality of the video (r = 0.60) as shown in Fig. 1.\n$A_{jump}$ = 0.1 is used for comparison, as this is the best-performing penalty value on average (see Sec. 4.3)."}, {"title": "4.2. Combination techniques", "content": "An analysis of the different methods for combining the text, audio, and image similarity matrices does not reveal any superior approach. On average, all combination techniques (max, mean, weighted sum) achieve an accuracy of 0.82, significantly higher than the accuracies of the single feature algorithms reported in Table 1. For a baseline, SIFT matching is computed achieving an average accuracy of 0.56. The python cv2 package with the functions SIFT_create and BFMatcher are used to compute local features and to match them between PDF slides and video frames. $A_{jump}$ = 0.1 is used for comparing the combination techniques here."}, {"title": "4.3. Penalties", "content": "For the linear penalty term $p_{linear}$, setting $A_{linear}$ = 0 yields the most favorable outcome, with higher penalty values leading to a decline in performance across most cases. However, for two lectures, the penalty $A_{linear}$ = 10\u207b\u00b3 results in slight enhancements, indicating that a penalty for rewarding a linear slide order might be beneficial under some circumstances.\nFor $p_{jump}(k)$, setting $A_{jump}$ = 0.1 achieves the highest accuracy, yielding an average score of 0.82, compared to the average score of 0.76 for $A_{jump}$ = 0 as reported in Table 2. Setting $A_{jump}$ = 0.15 or higher decreases performance again for the majority of lectures, although it does show slightly enhanced performance for 'Cognitive Robotics' and 'Reinforcement Learning', indicating that these lectures profit from a higher penalty.\nA higher jump penalty leads to a stronger negative correlation between F1 score and volatility score as depicted in Fig. 2. This is not surprising as lectures with a high volatility, like for instance 'Phonetics' that has a volatility score of 4.78, perform worse if a high jump penalty is introduced which punishes jumping back and forth. Equally, a higher jump penalty leads to a stronger negative correlation between F1 score and the no slide/slide ratio which is depicted on the right side of Fig. 2.\nIn summary, introducing a jump penalty is beneficial on average, but might be counterproductive for lectures with high volatility and a high occurrence of 'no slide' views."}, {"title": "4.4. Runtime", "content": "Given the transcribed audio, calculating features and alignment for only audio features takes around 45 seconds on our local CPU (i7, 3.0 GHz), for image features it takes around 64 seconds and for text features it takes three minutes on average. The combined alignment using either 'max' or 'mean' combination takes 3.5 minutes in sum, while the weighted sum approach takes around 4.6 minutes. SIFT alignment takes 39.5 minutes on average, being the most time-consuming. Therefore, the MaViLS algorithm is approximately 11 times faster."}, {"title": "5. Discussion", "content": "This paper analyses the effectiveness of single-feature embeddings versus combined-feature embeddings, demonstrating that multi-feature composites improve the alignment of lecture slides with the corresponding audio transcripts or video frames. Potentially, adding more algorithms or embeddings to the mix could further increase the results. However, one must balance between high multimodality and time efficiency.\nNo notable performance disparities are observed among the three combination techniques. The weighted sum method is less effective when lectures vary between parts where text excels and segments where audio is more informative, since it uniformly weights across the entire lecture rather than adjusting to individual video frames. As the approach is additionally slower than the other techniques, its utility seems limited.\nFurther, the implementation of a jump penalty that discourages frequent switching between slide numbers is explored. While incorporating such a penalty bolsters accuracy on average, it is counterproductive in lectures with frequent slide switches. Future research could investigate methods for detecting volatility. Also, alternative (e.g. non-linear) jump penalty calculations might be beneficial.\nAnother key insight of this paper is that the audio transcript features are more resilient to suboptimal quality of the recordings than text-based features, offering valuable information for alignment even with no text captured by the video frame. This underscores the power of a multimodal approach.\nThe creation of rich embedding spaces presents opportunities for digital academic settings. Enhanced matching capabilities can facilitate students' inquiries, linking questions to relevant lecture content more efficiently. This can be particularly beneficial for visually impaired individuals by synchronizing spoken content with visual materials.\nMaViLS is limited to English language lectures and might lack comprehensive field representation. Domains such as mathematics are underrepresented, as lectures from these areas seldom come with accompanying PDF slides. Further, some images are shown in the video, but not on the PDF slides due to copyright restrictions, which may impact our results.\nIn addition, embeddings are calculated on a sentence level only. Flexible text blocks can potentially improve alignment, presenting an interesting direction for future research."}]}