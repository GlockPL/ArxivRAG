{"title": "Distribution Matching for Self-Supervised Transfer Learning", "authors": ["Yuling Jiao", "Wensen Ma", "Defeng Sun", "Hansheng Wang", "Yang Wang"], "abstract": "In this paper, we propose a novel self-supervised transfer learning method called Distribution Matching (DM), which drives the representation distribution toward a predefined reference distribution while preserving augmentation invariance. The design of DM results in a learned representation space that is intuitively structured and offers easily interpretable hyperparameters. Experimental results across multiple real-world datasets and evaluation metrics demonstrate that DM performs competitively on target classification tasks compared to existing self-supervised transfer learning methods. Additionally, we provide robust theoretical guarantees for DM, including a population theorem and an end-to-end sample theorem. The population theorem bridges the gap between the self-supervised learning task and target classification accuracy, while the sample theorem shows that, even with a limited number of samples from the target domain, DM can deliver exceptional classification performance, provided the unlabeled sample size is sufficiently large.", "sections": [{"title": "1 Introduction", "content": "Collecting abundant labeled data in real-world scenarios is often prohibitively expensive, particularly in specialized domains such as medical imaging, autonomous driving, robotics, rare disease prediction, financial fraud detection, and law enforcement surveillance. It is widely believed that knowledge from different tasks shares commonalities. This implies that, despite the differences between tasks or domains, there exist underlying patterns or structures that can be exploited across them. This belief forms the foundation of transfer learning. Transfer learning seeks to leverage knowledge from a source task to improve model performance in the target task, while simultaneously reducing the required sample size from target domain.\nRecently, a variety of transfer learning methodologies have been proposed, including linear models (Li et al., 2021; Singh and Diggavi, 2023; Zhao et al., 2024; Liu, 2024), generalized linear models (Tian and Feng, 2022; Li et al., 2023), and nonparametric models (Shimodaira, 2000; Ben-David et al., 2006; Blitzer et al., 2007; Sugiyama et al., 2007; Mansour et al., 2009; Wang et al., 2016; Cai and Wei, 2019; Reeve et al., 2021; Fan et al., 2023; Maity et al., 2024; Lin and Reimherr, 2024; Cai and Pu, 2024). However, these methods either impose constraints that the model must be inherently parametric or suffer from the curse of dimensionality (Hollander et al., 2013; Wainwright, 2019) in practical applications. In contrast, deep learning has demonstrated a remarkable ability to mitigate the curse of dimensionality, both empirically (LeCun et al., 2015; Zhang et al., 2021) and theoretically (Kohler and Krzyzak, 2004; Kohler and Krzy\u017cak, 2016; Bauer and Kohler, 2019; Schmidt-Hieber, 2020). Consequently, deep transfer learning has garnered significant attention within the research community.\nA particularly effective paradigm within deep transfer learning is pretraining followed"}, {"title": "1.1 Contributions", "content": "Our main contributions are summarized as follows:\n\u2022 We introduce a novel self-supervised learning method, termed Distribution Matching (DM). DM drives the representation distribution towards a predefined reference distribution, resulting in a learned representation space with strong geometric intuition, while the hyperparameters are easily interpretable.\n\u2022 The experimental results across various real-world datasets and evaluation metrics"}, {"title": "1.2 Related works", "content": "Huang et al. (2023) establish a theoretical foundation for various self-supervised losses at the population level, while Duan et al. (2024) extend this analysis to the sample level for the adversarial loss they propose. We provide theoretical guarantees at both the population and sample levels. Wang and Isola (2020); Awasthi et al. (2022); Huang et al. (2023); Duan et al. (2024) have investigated the structure of the representation space learned by various self-supervised learning methods, both empirically and theoretically. In contrast, DM naturally exhibits a clear geometric structure. HaoChen et al. (2021, 2022); HaoChen and Ma (2023) suggest the existence of a potential subclass structure within their graph-theoretical framework, though without empirical support. By leveraging the clear geometric structure and the interpretability of DM's hyperparameters, the ablation experiment presented in Section 4.2 empirically verifies this hypothesis."}, {"title": "2 Methodology", "content": "Let $x = (x_1,\\ldots,x_d)^\\top \\in \\mathbb{R}^d$ be an arbitrary d-dimensional vector, we define $\\|x\\|_p = (\\sum_i |x_i|^p)^{\\frac{1}{p}}$ be its p-norm with $p\\in \\{1,2,\\infty\\}$. In particular, for $p = \\infty$, $\\|x\\|_\\infty = \\max_i |x_i|$.\nLet $f$ be a function from $\\mathbb{R}^{d_1}$ to $\\mathbb{R}^{d_2}$, and let $\\text{dom}(f)$ represent the domain of $f$. For a constant $c \\geq 0$, we say that $f$ satisfies $\\|f\\|_2 = c$ if $\\| f(x)\\|_2 = c$ holds for any $x \\in \\text{dom}(f)$. Additionally, we define the functional set as:\n$\\text{Lip}(L) = \\{f : \\mathbb{R}^{d_1} \\rightarrow \\mathbb{R}^{d_2} \\mid \\sup_{x_1,x_2\\in \\text{dom}(f) \\atop x_1\\neq x_2} \\frac{\\| f(x_1) - f(x_2)\\|_{2}}{\\|x_1 - x_2\\|_2} < L\\}.$\nLet $f$ and $g$ be two functions defined on $\\mathbb{N} = \\{1,2,\\ldots\\}$. We say that $f(n) \\leq O(g(n))$ if and only if there exist two fixed constants $0 < C_1 < C_2$ and a positive integer $n_0 \\in \\mathbb{N}$, such that for all $n \\geq n_0$, $C_1g(n) \\leq f(n) \\leq c_2g(n)$. It immediately follows that $c_2^{-1} f(n) \\leq g(n) \\leq c_1^{-1} f(n)$ for any $n \\geq n_0$. Therefore, the statement $f(n) = O(g(n))$ implies that $g(n) = O(f(n))$. Given two quantities $X$ and $Y$, we use $X \\leq Y$ or $Y \\geq X$ to denote $X < cY$ for some constant $c > 0$.\nAssume a source dataset containing a total of $n_s$ unlabeled image instances, denoted by $\\mathcal{D}_s = \\{X_s^{(i)}: 1 \\leq i \\leq n_s\\}$, Here $X_s^{(i)} \\in \\mathcal{X}_s \\subseteq [0, 1]^d$ represents the i-th instance, which are independently and identically generated from a source distribution $P_s$ on the source domain $\\mathcal{X}_s$. To fix the idea, consider the ImageNet dataset as an example for $\\mathcal{D}_s$. We then have a total of $n_s = 1.28 \\times 10^6$ instances (Deng et al., 2009). Since ImageNet instance are of 224 \\times 224 \\times 3 resolution, we thus have $d = 150,528 = 224 \\times 224 \\times 3$. Next, assume a target dataset as $\\mathcal{D}_T = \\{(X_i^T, Y_i): 1 \\leq i \\leq n_T\\}$ with $X_i^T \\in [0, 1]^d$ and $Y_i \\in \\{1,2,\\ldots, K\\}$ being the class label. Assume $(X_i^T, Y_i)$s are independently and identically generated from a target distribution $P_T$. For most real applications, we typically have $n_s \\gg n_T$. How to leverage $\\mathcal{D}_s$ so that a model with excellent classification accuracy on $\\mathcal{D}_T$ is a problem of"}, {"title": "2.1 Data Representation", "content": "Pixel images pose significant challenges for statistical learning for at least two reasons. First, their high dimensionality, as exemplified by ImageNet with 150,528 dimensions per image, complicates statistical modeling. Second, pixel images are inherently noisy. For example, consider Figure 2, where the left panel (x1) shows a photo of a dog, the middle panel (x2) shows a different image, and the right panel (x3) shows a cropped version of x1. Intuitively, x1 and x3 should be more similar, yet Euclidean distance calculations reveal $\\|x_1 - x_2\\|_2 < \\|x_1 - x_3\\|_2$. This counterintuitive result highlights that pixel vectors encode both useful semantic information and significant noise, making the transformation to a lower-dimensional, less noisy representation crucial.\nThis leads to the concept of data representation (Rumelhart et al., 1986; Bengio et al., 2012; LeCun et al., 2015). By \u201cdata representation\u201d, we refer to mapping an original image $X \\in \\mathbb{R}^d$ to a lower-dimensional space $f(X) \\in \\mathbb{R}^{d^*}$, where $d^* \\ll d$. Here $f$ is"}, {"title": "2.2 Self-Supervised Contrastive Learning", "content": "In the absence of labeled data, the need for effective representations has driven the development of contrastive learning. The core idea is to learn representations invariant to augmentations. By augmentation, we refer to a predefined function that transforms an image $A(\\cdot)$ into a similar, but not identical image $A(X) \\in \\mathbb{R}^d$. In practice, $X$ and $A(X)$ might be of different dimensions. For notation simplicity, we assume they share the same dimension in this work. Since $A(X)$ is derived from $X$, they are expected to share similar semantic meanings. Commonly used augmentation include random cropping,"}, {"title": "2.3 Distribution Matching", "content": "Before introducing the DM method, we briefly review the Mallows' distance (Mallows, 1972; Shao and Tu, 2012), also known as the Wasserstein distance (Villani, 2009). To do so, we first define some key concepts. Let $\\nu$ be a measure on $\\mathbb{R}^{d_1}$ and $f : \\mathbb{R}^{d_1} \\rightarrow \\mathbb{R}^{d_2}$ a measurable function. The push-forward measure $f_\\#\\nu$ is defined as $f_\\#\\nu(E) = \\nu(f^{-1}(E))$ for any $f_\\#\\nu$-measurable set $E \\subset \\mathbb{R}^{d_2}$. In this context, the Mallows' distance is defined as:\nDefinition 1 (Mallows' distance). Let $(\\mathcal{X}_1, \\nu_1)$ and $(\\mathcal{X}_2, \\nu_2)$ are two probability spaces with $\\mathcal{X}_1, \\mathcal{X}_2 \\subset \\mathbb{R}^k$ for some positive integer $k$. Then the Mallow's distance is defined as\n$W(\\nu_1, \\nu_2) = \\inf_{(\\mathcal{X}_1,\\mathcal{X}_2) \\in \\Pi(\\nu_1,\\nu_2)} \\mathbb{E}_{(\\mathcal{X}_1,\\mathcal{X}_2)} (\\|\\mathcal{X}_1 - \\mathcal{X}_2\\|_1),$\nwhere $\\Pi(\\nu_1, \\nu_2)$ denotes the collection of all possible joint distributions of the pairs $(\\mathcal{X}_1, \\mathcal{X}_2)$ with marginal distributions given by $\\nu_1$ and $\\nu_2$, respectively. Here we implicitly assume that there exists a probability space $(\\Omega, \\mathcal{P})$ such that $\\mathcal{X}_1 : \\Omega \\rightarrow \\mathcal{X}_1$ and $\\mathcal{X}_2 : \\Omega \\rightarrow \\mathcal{X}_2$ are measurable and satisfy $(\\mathcal{X}_1)_\\#\\mathcal{P} = \\nu_1$ and $(\\mathcal{X}_2)_\\#\\mathcal{P} = \\nu_2$.\nTo better understand the Definition 1, we explore a special case in detail. Let $\\mathcal{X}_1 = \\{x_{1,1}, x_{1,2},\\ldots, x_{1,n_1}\\} \\subseteq \\mathbb{R}^k$ and $\\mathcal{X}_2 = \\{x_{2,1}, x_{2,2},\\ldots, x_{2,n_2}\\} \\subseteq \\mathbb{R}^k$, where $k, n_1$ and $n_2$ are positive integers. Suppose $\\nu_1$ and $\\nu_2$ are discrete probability distributions on $\\mathcal{X}_1$ and $\\mathcal{X}_2$, respectively. Then each element in $\\Pi(\\nu_1, \\nu_2)$ can be completely determined by a discrete probability distribution on the cartesian product $\\mathcal{X}_1 \\times \\mathcal{X}_2$, represented by $\\Psi(x_1,x_2) \\in \\mathbb{R}^{n_1\\times n_2}$. Accordingly, it should satisfy that (i) $\\Psi(x_1,x_2) \\geq 0$ for any $x_1 \\in \\mathcal{X}_1$ and $x_2 \\in \\mathcal{X}_2$, (ii) $\\sum_{x_1,x_2} \\Psi(x_1, x_2) = 1$, (iii) $\\sum_{x_2\\in \\mathcal{X}_2} \\Psi(x_1, x_2) = \\nu_1(x_1)$ for any $x_1 \\in \\mathcal{X}_1$ and (iv) $\\sum_{x_1\\in \\mathcal{X}_1} \\Psi(x_1, x_2) = \\nu_2(x_2)$ for any $x_2 \\in \\mathcal{X}_2$. The Mallows' distance between $\\nu_1$ and $\\nu_2$ is then given by: $W(\\nu_1, \\nu_2) = \\inf_{\\Psi\\in \\Pi(\\nu_1,\\nu_2)} \\sum_{x_1\\in \\mathcal{X}_1, x_2\\in \\mathcal{X}_2} \\Psi (x_1, x_2) \\cdot \\|x_1 - x_2\\|_1$. Intuitively, $\\nu_1$ and $\\nu_2$ can be regarded as two piles of probability masses, with $\\nu_1(x_1)$ and $\\nu_2(x_2)$ indicating"}, {"title": null, "content": "the mass at $x_1$ and $x_2$, respectively. The transport plan $\\Psi(x_1, x_2)$ can be thought of as the amount of mass transported from $x_1$ and $x_2$, while the term $\\|x_1 - x_2\\|_1$ represents the transportation cost. Thus, the Mallows' distance quantifies the minimal cost to transport one probability distribution to another.\nAlthough Definition 1 is intuitive, computing it is challenging due to the difficulty of finding the optimal coupling in $\\Pi(\\nu_1,\\nu_2)$. To address this, a dual formulation is provided in Remark 6.5 of Villani (2009):\n$W(\\nu_1, \\nu_2) = \\sup_{g\\in Lip(1)} \\mathbb{E}_{X_1 \\sim \\nu_1}\\{g(X_1)\\} - \\mathbb{E}_{X_2 \\sim \\nu_2}\\{g(X_2)\\},$\nwhere the task reduces to finding the optimal function $g$ in Lip(1),, a problem that can be solved using a neural network with gradient penalty (Gulrajani et al., 2017), as detailed in (19). Notably, the Mallows' distance remains effective even when $\\nu_1$ and $\\nu_2$ have different supports, unlike many other divergence measures (e.g., Kullback-Leibler and Jensen-Shannon divergence), which either diverge to infinite or become constant in such cases. Furthermore, the Mallows' distance satisfies the triangle inequality, making it a true distance metric, an important property not shared by many other divergence measures.\nFor a thorough theoretical treatment of Mallows' distance, we refer to Villani (2009).\nWith the Mallows' distance defined, we can now proceed to develop the DM method. The key idea is to prevent model collapse by minimizing the Mallows' distance between the representation distribution and the predefined reference distribution. As a result, constructing the reference distribution becomes the most crucial step, which can be broken down into three sub-steps. In the first sub-step, we design K' centers in $\\mathbb{R}^{d^*}$, where K' < d*. The i-th center $c_i$ is chosen to be either $e_i$ or $-e_i$ with equal probability, where $e_i$ is the standard basis vector in $\\mathbb{R}^{d^*}$ with the i-th component equal to 1 and all others components equal to 0. In the second sub-step, we define the i-th reference part a random"}, {"title": null, "content": "vector as:\n$P_i = R \\frac{c_i + \\epsilon \\gamma_{d^*}}{\\|c_i + \\epsilon \\gamma_{d^*}\\|_2},$\nwhere $\\epsilon > 0$ is a tuning parameter, and $\\gamma_{d^*}$ is a standard Gaussian random vector in $\\mathbb{R}^{d^*}$. To gain an intuitive understanding of $P_i$, let $B(a,r)$ denote the ball centered at $a \\in \\mathbb{R}^{d^*}$ with radius $r > 0$. It is straightforward to observe that the vector $\\frac{\\gamma_{d^*}}{\\|\\gamma_{d^*}\\|_2}$ follows a uniform distribution on the surface of the unit ball $B(0, 1)$. We then scale and translate this vector to lie within the ball $B(c_i, \\epsilon)$ by multiplying by $\\epsilon$ and adding the center $c_i$. To ensure that the resulting random variable $P_i$ lies on the surface of the ball $B(0, R)$, we normalize the vector and scale it by $R$. As shown in the left-hand side of Figure 4, the process results in $P_i$ follows a uniform distribution over the orange region of the sphere. Next, we define a categorical random variable $C \\in \\{1,2,\\ldots, K'\\}$ with $\\mathbb{P}(C = i) = \\alpha_i$, where $\\alpha_i$ are the probabilities associated with i-th part, and $C$ is independent of $P_i$ for all $1 \\leq i \\leq K'$. We then construct a new random variable $R$ as $R = \\sum_{i=1}^{K'} \\mathbb{I}(C = i)P_i$. The distribution of $R$ is referred to as the reference distribution, denoted by $\\mathbb{P}_R$. DM aim to cluster augmented views with similar semantic meaning according to the same part of the reference distribution by minimizing $W(P_f, \\mathbb{P}_R)$, as illustrated on the right hand side of Figure 4.\nWe define the representation distribution $P_f = f_\\#P_A$, where $P_A$ is the distribution of augmented views. This is rigorously given by $P_A(E) = \\int \\sum_{A \\in \\mathcal{A}} \\mathbb{I}\\{A(x) \\in E\\} P_s(dx)$ for any measurable set $E$. The DM learning problem is then formulated as the following minimization problem:\n$f^* \\in \\arg \\min_{f\\in \\mathcal{F}} \\mathcal{L}(f) := \\mathcal{L}_{align}(f) + \\lambda \\cdot W(P_f, P_R).$\nwhere $\\mathcal{L}(f)$ is the objective function that consists of alignment loss and the Wasserstein distance between $P_f$ and $P_R$. The tuning parameter $\\lambda > 0$ balances the relative importance"}, {"title": null, "content": "of $\\mathcal{L}_{align}(f)$ and $W(P_f, P_R)$. The function class $\\mathcal{F}$ is defined in (3). It is important to note that the solution $f^*$ to this minimization problem may bot be unique, as $f \\in \\min_{f\\in \\mathcal{F}} \\mathcal{L}(f)$ implies multiple possible minimizer. Let $\\mathcal{G} := Lip(1)$ and plug (2) and (5) into (7) gives the following formulation of the DM learning problem:\n$f^* \\in \\arg \\min_{f\\in \\mathcal{F}} \\mathbb{E}_{X_S\\sim \\mathbb{P}_S} \\mathbb{E}_{X_{S,1},X_{S,2}\\sim A(X_S)} {\\|f(X_{S,1}) - f(X_{S,2})\\|_2^2} + \\lambda \\sup_{g \\in \\mathcal{G}} \\mathbb{E}_{Z\\sim P_f} g(Z) - \\mathbb{E}_{R\\sim P_R} g(R)$.\nIt is evident that (8) can be interpreted as a mini-max optimization problem. To emphasize"}, {"title": null, "content": "this, we rewrite it as follows:\n$(f^*, g^*) \\in \\arg \\min_{f\\in \\mathcal{F}} \\max_{g\\in \\mathcal{G}} \\mathcal{L}(f, g) := \\mathcal{L}_{align}(f) + \\lambda\\cdot W(f, g),$\nwhere $W(f, g) = \\mathbb{E}_{Z\\sim P_f}\\{g(Z)\\} \u2013 \\mathbb{E}_{R\\sim P_R}\\{g(R)\\}$, g is referred to as a critic. It immediately follows that $W(P_f, P_R) = \\sup_{g\\in \\mathcal{G}} W(f, g)$ and $\\mathcal{L}(f) = \\sup_{g\\in \\mathcal{G}} \\mathcal{L}(f, g)$.\nTo solve (9) in practice, we face two challenges. The first challenge is the population distribution of the original images $P_s$ is unknown. We therefore have to replace it by its finite sample counterpart. Specifically, for each instance $X_s^{(i)}$, we sample two augmentations $A_{i,1}$ and $A_{i,2}$ from $\\mathcal{A}$ uniformly. These augmentations produce two views, $\\mathcal{X}_S^{(i)} = (X_{S,1}^{(i)}, X_{S,2}^{(i)}) = (A_{i,1}(X_s^{(i)}), A_{i,2}(X_s^{(i)})) \\in \\mathbb{R}^{2d}$. Simultaneously, we independently collect $n_S$ instances $\\{R^{(i)} : 1 \\leq i \\leq n_S\\}$ from $\\mathbb{P}_R$. The resulting augmentation-reference dataset is $\\mathcal{D}_S = \\{(\\mathcal{X}_S^{(i)}, R^{(i)}) : 1 \\leq i \\leq n_S\\}$. The finite sample approximation of $\\mathcal{L}(f, g)$ is then:\n$\\widetilde{\\mathcal{L}}(f,g) := \\widetilde{\\mathcal{L}}_{align}(f) + \\lambda \\widetilde{W}(f,g)$,\n$\\widetilde{\\mathcal{L}}_{align}(f) = \\frac{1}{n_S} \\sum_{i=1}^{n_S} \\|f(X_{S,1}^{(i)}) - f(X_{S,2}^{(i)})\\|_2^2,$\n$\\widetilde{W}(f,g) = \\frac{1}{n_S} \\sum_{i=1}^{n_S} [g(R^{(i)}) - \\frac{1}{2} \\{g(f(X_{S,1}^{(i)})) + g(f(X_{S,2}^{(i)}))\\}].$\nIt is evident that $W(f,g) = \\mathbb{E}_{P_{\\mathcal{D}_S}} \\{\\widetilde{W}(f,g)\\}$ and $\\mathcal{L}(f,g) = \\mathbb{E}_{P_{\\mathcal{D}_S}}\\{\\widetilde{\\mathcal{L}}(f,g)\\}$, which justifies calling $\\widetilde{\\mathcal{L}}(f, g)$ the finite sample counterpart of $\\mathcal{L}(f, g)$.\nThe second challenge stems from the complexity of the functional spaces $\\mathcal{F}$ and $\\mathcal{G}$, which complicates practical search. To overcome this, we parametrize them using deep ReLU networks. Specifically, we define a class of deep ReLU networks as follows:\nDefinition 2 (Deep ReLU network class). The function $f_\\theta(x) : \\mathbb{R}^p \\rightarrow \\mathbb{R}^q$ implemented by a deep ReLU network with parameter $\\theta$ is expressed as composition of a sequence of"}, {"title": null, "content": "functions\n$f_\\theta(x) := l_D \\circ \\rho \\circ l_{D-1} \\circ \\rho \\circ \\cdots \\circ l_1 \\circ \\rho \\circ l_0(x)$\nfor any $x \\in \\mathbb{R}^p$, where $\\rho(x)$ is the ReLU activation function and the depth $D$ is the number of hidden layers. For $1 \\leq i \\leq D$, the i-th layer is represented by $l_i(x) := A_ix + b_i$, where $A_i \\in \\mathbb{R}^{d_{i+1}\\times d_i}$ is the weight matrix, $b_i \\in \\mathbb{R}^{d_{i+1}}$ is the bias vector, $d_i$ is the width of the i-th layer and $\\theta = \\{(A_0, b_0), \\ldots, (A_D, b_D)\\}$. The network $f_\\theta$ contains $(D+1)$ layers in all. We use a $(D+1)$-dimension vector $(d_0, d_1,\\ldots, d_D)$ to describe the width of each layer. In particular, $d_0 = p$ is the dimension of the domain and $d_q = q$ is the dimension of the codomain. The width $W$ is defined as the maximum width of hidden layers, that is, $W = \\max \\{d_1, d_2,\\cdots, d_D\\}$. The bound $B$ denotes the $L^\\infty$ bound of $f_\\theta(\\cdot)$, that is, $\\sup_{x \\in \\mathbb{R}^p} \\| f_\\theta(x) \\|_\\infty \\leq B$. We denote the function class $\\{f_\\theta : \\mathbb{R}^p \\rightarrow \\mathbb{R}^q\\}$ implemented by deep ReLU network class with width $W$, depth $D$, and bound $B$ as $\\text{NN}_{p,q}(W, D, B)$.\nBy parametrizing $\\mathcal{F}$ and $\\mathcal{G}$ as two deep ReLU network classes, the optimization problem in (9) is reformulated as:\n$(\\widehat{f}_{n_S}, \\widehat{g}_{n_S}) \\in \\arg \\min_{f\\in \\widehat{\\mathcal{F}}} \\max_{g\\in \\widehat{\\mathcal{G}}} \\widetilde{\\mathcal{L}}(f, g),$\nwhere $\\widehat{\\mathcal{F}} = \\text{NN}_{d,d^*}(W_1, D_1, B_1)$ and $\\widehat{\\mathcal{G}} = \\text{NN}_{d^*,1}(W_2, D_2, B_2)$. In practice, we set $W_1 \\geq W_2$ and $D_1 \\geq D_2$, ensuring that $W_1D_1 \\geq W_2D_2$ in subsequent analysis."}, {"title": "2.4 Transfer Learning", "content": "One significant application of learned representations is transfer learning. Recall $\\mathcal{D}_T = \\{(X_i^T, Y_i) : 1 \\leq i \\leq n_T\\}$ denotes the target dataset. For each $X_i^T \\in \\mathcal{D}_T$, we sample two augmentations $A_{i,1}, A_{i,2}$ from $\\mathcal{A}$ uniformly, resulting in $\\mathcal{X}_{T,A}^{(i)} = (X_{1,A}^{T,(i)}, X_{2,A}^{T,(i)}) = (A_{i,1}(X_i^T), A_{i,2}(X_i^T))$. The augmented dataset is then $\\mathcal{D}_T = \\{(\\mathcal{X}_{T,A}^{(i)}, Y_i) : 1 \\leq i \\leq n_T\\}$. We next consider a linear classifier\n$G_f(x) = \\arg \\max_{1<k<K} (W f(x))_k,$\nwhere $(\\cdot)_k$ denotes the k-th entry of the vector, and $W$ is a $K \\times d^*$ matrix with its k-th row given by\n$\\mu_T(k) = \\frac{1}{2n_T(k)} \\sum_{i=1}^{n_T} \\{f(X_{1,A}^{T,(i)})+f(X_{2,A}^{T,(i)})\\}\\mathbb{I}\\{Y_i = k\\},$\nwhere $n_T(k) = \\sum_{i=1}^{n_T} \\mathbb{I}\\{Y_i = k\\}$ represents the sample size of the k-th class. It is evident that $\\widehat{\\mu}_T(k)$ serves as an unbiased estimator of $\\mu_T(k) = \\mathbb{E}_{(X_T,Y)\\sim P_T} \\mathbb{E}_{X_T\\sim A(X_T)}\\{f(X_T)|Y = k\\}$, which denotes the center of the k-th class in the representation space. To evaluate its performance, we examine its misclassification rate by\n$\\text{Err}(G_f) = \\mathbb{P}_T\\{G_f(X_T) \\neq Y\\},$\nwhere $(X_T, Y)$ represents an independent copy of $(X_i^T, Y_i)$."}, {"title": "3 Theoretical Guarantee", "content": "We assume that any upstream data $X_s \\sim P_s$ can be categorized into categorized into some of $K$ latent classes, each corresponding to a distinct downstream class. The term \"latent\" implies that these classes are not directly observable to us, but do exist. For $1 \\leq k \\leq K$, we define $C_S(k)$ as the set of data points belonging to the k-th latent class. The conditional probability distribution $P_f(k)$ is given by $P_f(k)(\\cdot) = P_f\\{\\cdot|X_S \\in C_S(k)\\}$, with its population center $\\mu_S(k) = \\mathbb{E}_{X_S\\sim P_S} \\mathbb{E}_{X_S\\sim A(X_S)}\\{f(X_S)|X_S \\in C_S(k)\\}$."}, {"title": "3.1 Population Theorem", "content": "The goal of DM is to render source data well-separated. Specifically", "pushing $P_f$ in parts": "thereby inheriting the characteristics of $P_R$. However", "C_k": 1, "C_R(k)": "C_{\\tau^*(k)}$, where $\\tau^* = \\arg \\max_{\\tau \\in P_K} \\sum_{k=1}^K Q^*(C_S(k) \\rightarrow C_{\\tau(k)})$. Therein, $Q^*(C_S(k) \\rightarrow C_{\\tau(k)})$ represents the transport mass from $C_S(k)$ to $C_{\\tau(k)}$ according to $Q^*$. To better understand this assignment, consider an example with $K = 3$ and $Q^*$ such that $Q^*(C_S(1) \\rightarrow C_1) = 1/5, Q^*(C_S(1) \\rightarrow C_2) = 0, Q^*(C_S(1) \\rightarrow C_3) = 2/15; Q^*(C_S(2) \\rightarrow C_1) = 1/15, Q^*(C_S(2) \\rightarrow C_2) = 1/30, Q^*(C_S(2) \\rightarrow C_3) = 7/30$ and $Q^*(C_S(3) \\rightarrow C_1"}]}