{"title": "Primary Care Diagnoses as a Reliable Predictor for Orthopedic Surgical Interventions", "authors": ["Khushboo Verma", "Alan Michels", "Ergi Gumusaneli", "Shilpa Chitnis", "Smita Sinha Kumar", "Christopher Thompson", "Lena Esmail", "Guruprasath Srinivasan", "Chandini Panchada", "Sushovan Guha", "Satwant Kumar"], "abstract": "Referral workflow inefficiencies, including misaligned referrals and delays, contribute to suboptimal patient outcomes and higher healthcare costs. In this study, we investigated the possibility of predicting procedural needs based on primary care diagnostic entries, thereby improving referral accuracy, streamlining workflows, and providing better care to patients.\nA de-identified dataset of 2,086 orthopedic referrals from the University of Texas Health at Tyler was analyzed using machine learning models built on Base General Embeddings (BGE) for semantic extraction. Model performance was assessed using the area under the receiver operating characteristic curve (ROC-AUC), Precision-Recall Curve (PR-AUC), and Matthews Correlation Coefficient (MCC). To ensure real-world applicability, noise tolerance experiments were conducted, and oversampling techniques were employed to mitigate class imbalance.\nThe selected optimum and parsimonious embedding model demonstrated high predictive accuracy (ROC-AUC: 0.874, MCC: 0.540), effectively distinguishing patients requiring surgical intervention. Dimensionality reduction techniques confirmed the model's ability to capture meaningful clinical relationships. A threshold sensitivity analysis identified an optimal decision threshold (0.30) to balance precision and recall, maximizing referral efficiency. In the predictive modeling analysis, the procedure rate increased from 11.27% to an optimal 60.1%, representing a 433% improvement with significant implications for operational efficiency and healthcare revenue.\nThe results of our study demonstrate that referral optimization can enhance primary and surgical care integration. Through this approach, precise and timely predictions of procedural requirements can be made, thereby minimizing delays, improving surgical planning, and reducing administrative burdens. In addition, the findings highlight the potential of clinical decision support as a scalable solution for improving patient outcomes and the efficiency of the healthcare system.", "sections": [{"title": "Introduction", "content": "The referral system can be described as the organizational structure for referring medical problems from generalists to specialists\u00b9. The purpose of this system is to improve patient outcomes by providing access to specialized diagnostics and treatments when the patient's medical needs exceed the expertise of general practitioners. In this system, primary care providers serve as the \"gatekeepers\" to specialty care providers\u00b9. Surgical specialties such as orthopedic surgery require seamless collaboration between primary care providers and surgical teams, for timely interventions and optimal patient outcomes. As the population ages and the need for specialized orthopedic care grows, there is an ever-growing unmet demand for orthopedic surgeons \u00b2. The average wait time for orthopedic surgery referrals in the US is 37.7 days \u00b3. However, this wait time can be significantly longer in rural settings given fewer specialists and a larger aging population\u2074. It is therefore crucial for primary care providers to accurately determine which patients will benefit from orthopedic surgery referrals.\nDespite the significant impact of referrals on patient care, healthcare systems, and healthcare costs, referral processes are frequently suboptimal and stochastic, emphasizing the need for augmenting and standardizing them. Referral pathways for orthopedic procedures are often negatively impacted by misaligned referrals and incomplete preoperative evaluations, likely due to the ever-growing need for orthopedic evaluations. This not only delays the necessary surgical interventions resulting in poor patient outcomes, but also places considerable administrative and operational strains on healthcare systems, resulting in economic losses. Addressing these inadequacies is critical to optimize referral workflows, as well as to enhance healthcare delivery\u2076.\nRecent advances in machine learning have emerged as transformative tools in healthcare, enabling the processing of complex clinical data, extracting actionable insights, and supporting evidence-based decision-making\u2077\u207b\u2079. The use of natural language processing (NLP) techniques allows us to draw meaningful inferences from unstructured clinical text\u00b9\u2070,\u00b9\u00b9. Despite these promising prospects, artificial intelligence (AI) remains underexplored in orthopedic referral systems. Therefore, in this study, we examined whether orthopedic procedural requirements can be predicted from primary care diagnostic entries. This capability can improve referral accuracy, thereby reducing administrative burdens and facilitating timely surgical interventions. However, the major challenges with clinical text data include its inherent variability, noise, and context- dependence which necessitates the use of robust embedding models that can generalize\u00b9\u00b9.\nBesides generalizability, the model should be capable of drawing semantic and contextual relationships from diagnostic text. In this regard, the pre-trained text embeddings, such as the Base General Embeddings (BGE) family, have demonstrated state-of-the-art performance\u00b9\u2070,\u00b9\u00b9. Therefore, we utilized these embeddings as they are particularly well-suited to address the complexities of clinical data, thereby enabling precise identification of patients that require surgical intervention\u00b9\u00b2.\nA de-identified dataset of 2,086 orthopedic referrals from the University of Texas Health at Tyler was examined to determine whether diagnostic text could predict procedural needs in orthopedic surgery. In order to enhance interpretability and performance, we assessed the machine learning models for robustness under real-world noisy conditions, data balancing, and threshold"}, {"title": "Materials and Methods:", "content": "The dataset consisted of clinical diagnostic text entries paired with procedural labels, extracted from anonymized electronic health records (EHRs) from January 4, 2024, to November 15, 2024. This study has been exempted from Institutional Review Board (IRB) review by The University of Texas at Tyler IRB (IRB 2025-031). The diagnostic entries comprised International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) codes, accompanied by their corresponding brief descriptions (Diagnoses). Each entry was assigned a binary target label: 1) Class 0 (No Procedures): Diagnostic entries without associated medical procedures and 2) Class 1 (With Procedures): Diagnostic entries linked to at least one medical procedure. The binary labeling system facilitated the dataset's utilization as the foundation for a classification task aimed at predicting the presence or absence of associated medical procedures based on diagnostic text. \nTo ensure the dataset's integrity and completeness for downstream analysis, missing entries were replaced with empty strings. Additionally, an enriched version of the diagnostic text, referred to as HyDE Enriched Diagnoses, was created. This enrichment was achieved using Hypothetical Document Embeddings (HyDE), wherein contextual information derived from ICD-10-CM descriptions was appended to the original diagnostic text \u00b9\u00b3. The enriched text aimed to improve semantic representation and enhance the performance of downstream machine learning models.\nA detailed exploration of the dataset's structure revealed key insights into its size, target label distribution, and text characteristics."}, {"title": "Preprocessing", "content": "The dataset exhibited a significant class imbalance, with most entries (88.73%) belonging to Class 0, compared to only 11.27% in Class 1. This imbalance posed challenges for model training and evaluation, necessitating the application of oversampling techniques, such as the Synthetic Minority Oversampling Technique (SMOTE), to address the disparity and ensure balanced representation in subsequent modeling experiments."}, {"title": "Embedding Models", "content": "To transform textual diagnostic entries into numerical representations suitable for machine learning, three pre-trained embedding models from the BGE (Base General Embeddings) family were utilized. These models, developed as part of the C-Pack project, represent state-of-the-art advancements in general-purpose text embeddings \u00b9\u2074. The BGE family of models has demonstrated superior performance across multiple tasks in the MTEB (Massive Text Embedding Benchmark) compared to other leading embedding models \u00b9\u2075.\nThe three BGE models varied in size and dimensionality, offering different trade-offs between computational efficiency and semantic richness. \nThe SentenceTransformers library was used to encode the clinical text into dense vector representations \u00b9\u2076. For each BGE model two distinct types of embeddings were generated: 1) Base Representations: Derived directly from the raw diagnostic text (Diagnoses) and 2) Enriched Representations: Derived from the enriched diagnostic text (HyDE Enriched Diagnoses), which augmented raw text with additional contextual information. This dual encoding approach aimed to assess the impact of text enrichment on downstream classification tasks."}, {"title": "Framework for Embedding Generation", "content": "To visualize the high-dimensional embeddings and assess their structural properties, Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP) were employed. PCA was chosen for its ability to preserve global variance in the data, while UMAP was selected for its strength in capturing local neighborhood structures \u00b9\u2077. For UMAP, fixed implementation parameters including: a cosine metric, 15 neighbors, and a minimum distance of 0.1 were used to optimize the balance between global and local data representations. These techniques were applied to embeddings generated by BGE models, and the results were visualized as two-dimensional scatterplots, with data points colored by their binary procedural labels. PCA revealed variance-driven separability trends, while UMAP highlighted finer clustering patterns reflecting the embedding model's ability to encode meaningful semantic relationships. These methods provided interpretable insights into the embeddings' properties, with UMAP outperforming in identifying local data structures."}, {"title": "Dimensionality Reduction", "content": "To ensure robust evaluation and mitigate the effects of class imbalance, a stratified 5-fold cross- validation strategy was employed, ensuring proportional representation of the minority class (Class 1: 11.27%) in both training and test splits. The Synthetic Minority Oversampling Technique (SMOTE) was applied exclusively to the training data to generate synthetic minority class samples \u00b9\u2078. By interpolating between existing datapoints, SMOTE enhanced the minority class representation, facilitating improved model training on underrepresented patterns without introducing noise into the evaluation process. This approach ensured a balanced and systematic assessment of model performance, critical for datasets with significant class imbalances."}, {"title": "Cross-Validation and Oversampling", "content": "Random Forest classifiers were chosen as the baseline model for their robustness, interpretability, and ability to handle high-dimensional data. Hyperparameter tuning was conducted using GridSearchCV to identify optimal configurations for the number of estimators (`n_estimators` = [50, 100, 200]), maximum tree depth (`max_depth` = [None, 10, 20]), minimum samples required to split (`min_samples_split` = [2, 5, 10]), and minimum samples at leaf nodes (`min_samples_leaf = [1, 2, 4]). The grid search utilized three-fold internal cross- validation within each training set, with ROC-AUC guiding the selection of hyperparameters. This methodology ensured that the final models were both optimized for performance and generalizable across diverse embeddings."}, {"title": "Model Training and Hyperparameter Optimization", "content": "Model performance was assessed using a comprehensive set of metrics: area under the Receiver Operating Characteristic curve (ROC-AUC), Precision-Recall Curve (PR-AUC), accuracy, and Matthews Correlation Coefficient (MCC). ROC-AUC quantified the ability to discriminate between classes, while PR-AUC captured performance under imbalanced class distributions, focusing on the precision-recall trade-off. MCC, a robust metric accounting for true and false predictions across all classes, was prioritized for its reliability in imbalanced datasets \u00b9\u2079. Accuracy, although commonly reported, was secondary due to its susceptibility to bias in skewed datasets. Metrics were computed for each cross-validation fold, and the mean and standard deviation were reported to capture performance variability."}, {"title": "Performance Evaluation Metrics", "content": "To investigate the robustness of embeddings to textual perturbations, a controlled noise tolerance experiment was conducted. Four noise types were applied to the diagnostic text: 1) character substitution (char_sub), 2) character deletion (char_del), 3) word swapping (word_swap), and 4) word deletion (word_del). Noise levels ranged from 0 (no noise) to 0.5 (50% perturbation), simulating scenarios of incomplete or erroneous clinical entries. For each noise type and level, embeddings were regenerated, and models were retrained and evaluated using cross-validation. This analysis provided critical insights into embedding stability under noisy conditions, highlighting the resilience of BGE embeddings, which demonstrated minimal performance degradation across word swapping and word deletion noise types."}, {"title": "Noise Tolerance Experiment", "content": "Three data balancing techniques: 1) SMOTE, 2) Adaptive Synthetic Sampling Approach for Imbalanced Learning (ADASYN), and 3) random undersampling were evaluated for their efficacy in mitigating class imbalance during model training \u00b9\u2078,\u00b2\u2070. Concurrently, four machine learning models: 1) Random Forest, 2) Gradient Boosting, 3) Support Vector Machines (SVM), and 4) Multi-Layer Perceptron (MLP) were compared across the balanced datasets. Model performance was evaluated using mean and standard deviation of ROC-AUC, PR-AUC, accuracy, and MCC metrics across 5-fold cross-validation."}, {"title": "Balancing Techniques and Model Comparison", "content": "Threshold sensitivity analysis was conducted to identify optimal decision thresholds for the Random Forest classifier. Precision, recall, and F1-scores were calculated across thresholds ranging from 0 to 1 and interpolated for consistent comparison. These metrics were averaged across cross-validation folds, enabling the generation of smooth sensitivity curves. The analysis highlighted the trade-offs between precision and recall at varying thresholds, offering actionable insights for optimizing decision-making processes in clinical contexts, where the prioritization of precision or recall depends on the specific application."}, {"title": "Threshold Sensitivity Analysis", "content": "Bootstrap resampling (n = 1000) was used to estimate 95% confidence intervals for performance metrics, reported as mean \u00b1 95% CI across cross-validation folds. The significance of the difference between the two proportions was compared using the Binomial test. Pairwise differences between embedding models were assessed using the Wilcoxon signed-rank test, with p-values adjusted for multiple comparisons using the Benjamini-Hochberg method \u00b2\u00b9. The adjusted p-values are reported as q-values. A significance threshold of q < 0.05 was applied. In the absence of significant differences, the most parsimonious and computationally efficient model was selected."}, {"title": "Statistical Analysis", "content": "Based on primary care diagnostic text, this study examines advanced embedding models, dimensionality reduction techniques, and classification strategies to predict procedural needs. This work aims to improve clinical decision support, streamline clinical workflows, and provide timely and appropriate patient care by allowing accurate procedure predictions directly from diagnostic entries. In addition to highlighting the effectiveness of embedding methods and noise tolerance, the results provide actionable insights for optimizing clinical decision support systems."}, {"title": "Results", "content": "The predictive performance of the embedding models including BGE-small, BGE-base, and BGE-large in both Base and HyDE variations was evaluated using cross-validation. Metrics included the area under the Receiver Operating Characteristic curve (ROC-AUC) and the Matthews Correlation Coefficient (MCC), which is particularly effective for imbalanced datasets. Mean values and 95% confidence intervals (CIs) across five cross-validation folds are reported in Table\nAll embedding models demonstrated robust predictive capabilities, with mean ROC-AUC values ranging between 0.865 and 0.874 across variations (see Figure 1 and Table 3). The highest mean ROC-AUC was observed for BGE-small-en-v1.5 (Base) at 0.874 (95% CI: 0.835\u20130.913), closely followed by BGE-large-en-v1.5 (HyDE) at 0.870 (95% CI: 0.805\u20130.935), and BGE-base-en-v1.5 (HyDE) at 0.868 (95% CI: 0.811\u20130.925). However, statistical tests revealed no significant differences in performance between Base and HyDE variations within each embedding model (q- values > 0.05; Wilcoxon signed-rank test), indicating that while HyDE offers contextual enrichment, its impact on ROC-AUC may not be substantial in this setting."}, {"title": "Embedding Model Performance", "content": "The Matthews Correlation Coefficient (MCC), summarized in Table 3, provides additional insight into the models' predictive accuracy across both classes. Among all variations, BGE- large-en-v1.5 (HyDE) achieved the highest MCC of 0.580 (95% CI: 0.492\u20130.668). Other models also performed well, with MCC values ranging from 0.540 (95% CI: 0.440\u20130.640) for BGE- small-en-v1.5 (Base) to 0.574 (95% CI: 0.478\u20130.670) for BGE-base-en-v1.5 (HyDE). Similar to ROC-AUC, the differences in MCC between Base and HyDE variations were not statistically significant (q-values > 0.05; Wilcoxon signed-rank test).\nWhile HyDE variations showed slightly higher MCC and ROC-AUC values than Base counterparts, the lack of significant differences indicates comparable performance. Thus, the most parsimonious and efficient model (\u201cBGE-small-en-v1.5 (Base)\u201d) was chosen to optimize resources without compromising accuracy. These results highlight the robustness of all evaluated models, with HyDE offering interpretative value without substantial metric improvement.\nDimensionality reduction using Principal Component Analysis (PCA) and Uniform Manifold Approximation and Projection (UMAP) was performed to visualize the separability of binary target labels: Class 0 (No Procedures) and Class 1 (With Procedures). These techniques reduced the high-dimensional BGE-small-en-v1.5 (Base) embeddings into two dimensions for qualitative evaluation."}, {"title": "Statistical Analysis", "content": "As shown in Figure 2, PCA revealed overlapping regions between Class 0 and Class 1, with limited clustering of Class 1 instances. This suggests that while the embeddings encode variance relevant to procedural prediction, the linear nature of PCA limits its ability to capture more complex structures. Figure 3 demonstrates that UMAP produced more distinct clusters, particularly for Class 1, with reduced but still noticeable overlap with Class 0. UMAP's ability to preserve local structures highlights non-linear relationships within the embedding space, offering better class separability than PCA.\nThese visualizations affirm the capacity of BGE-small-en-v1.5 embeddings to capture diagnostic features relevant to procedural prediction. The more defined clustering in UMAP suggests its utility for understanding embedding structures and supports the effectiveness of BGE embeddings for downstream classification tasks."}, {"title": "Dimensionality Reduction", "content": "The impact of oversampling techniques on model performance (Random Forest) was evaluated using SMOTE, ADASYN, and undersampling. Key metrics, including ROC-AUC, Precision, Recall, F1-Score, and MCC, were calculated with 95% confidence intervals (Table 4). Both SMOTE and ADASYN significantly improved the minority class representation without compromising overall performance, achieving comparable results in terms of ROC-AUC (SMOTE: 0.863 [0.819\u20130.906], ADASYN: 0.863 [0.821\u20130.904]). Notably, ADASYN showed slightly higher recall (0.540 [0.404\u20130.676]) compared to SMOTE (0.532 [0.400\u20130.664]), while SMOTE maintained a marginally higher precision (0.667 [0.598\u20130.737]) and MCC (0.551 [0.448-0.654]).\nIn contrast, undersampling achieved the highest recall (0.800 [0.652\u20130.948]) at the cost of significantly reduced precision (0.355 [0.316\u20130.395]) and MCC (0.448 [0.355\u20130.540]), highlighting its trade-off between sensitivity and specificity. Overall, SMOTE was selected for subsequent analyses as it provided the best balance between predictive accuracy and minority class representation, ensuring robust performance across metrics."}, {"title": "Oversampling Techniques Comparison", "content": "The performance of Random Forest, Gradient Boosting, Support Vector Machine (SVM), and Neural Network classifiers was evaluated using 5-fold cross-validation to identify the most effective model for predicting procedural requirements from diagnostic text. Evaluation metrics included ROC-AUC, Precision, Recall, F1-Score, and the Matthews Correlation Coefficient (MCC), with results reported as means and 95% confidence intervals (CIs) (Table 5).\nGradient Boosting achieved the highest ROC-AUC (0.870 [0.846\u20130.895]), followed closely by Random Forest (0.864 [0.815\u20130.912]). Random Forest demonstrated a balanced performance across all metrics, with the highest MCC (0.552 [0.453\u20130.651]) and precision (0.803 [0.688\u20130.917]), coupled with adequate recall (0.430 [0.339\u20130.520])."}, {"title": "Model Comparison", "content": "Neural Network models exhibited competitive recall (0.536 [0.414\u20130.658]) but fell slightly behind in overall performance, as reflected in their MCC (0.549 [0.458\u20130.639]). In contrast, SVM displayed the lowest performance, with an ROC-AUC of 0.832 [0.743\u20130.922] and an MCC of 0.502 [0.390\u20130.613], suggesting relatively weaker discriminative ability.\nRandom Forest emerged as the optimal model due to its robust performance across key metrics and computational efficiency, offering a practical balance between predictive power and resource utilization. Its selection highlights the importance of considering both accuracy and scalability in deploying machine learning models for clinical decision support."}, {"title": "Model Comparison", "content": "To assess the robustness of embedding (BGE-small-en-v1.5) to noise, we introduced systematic perturbations in the input text across four noise types: character substitution (char_sub), character deletion (char_del), word swapping (word_swap), and word deletion (word_del). Noise levels ranged from 0% to 50%, and the predictive performance was quantified using the ROC- AUC metric. The results, summarized in Figure 4, demonstrate the embeddings' varying degrees of resilience to these perturbations.\nOverall, embeddings were most robust at low noise levels (\u226410%), maintaining mean ROC-AUC values above 0.85 across all noise types. However, the performance degraded substantially as noise levels increased, particularly for character-based perturbations. At 50% noise, character substitution exhibited the steepest decline, with a mean ROC-AUC of 0.582 [0.537\u20130.627], underscoring the susceptibility of embeddings to frequent character-level distortions. Similarly, character deletion led to significant performance losses, albeit slightly less severe than character substitution. In contrast, word-based noise types (word swapping and word deletion) demonstrated greater tolerance, with word swapping achieving the highest robustness, retaining a mean ROC-AUC of 0.861 [0.804\u20130.918] even at the highest noise level.\nThese findings highlight the critical importance of noise-tolerant embeddings for clinical text applications, where real-world data often contain typographical errors or omissions. While character-level perturbations severely disrupted predictive performance, word-based noise had a comparatively moderate impact, making BGE embeddings particularly well-suited for noisy environments. This resilience is vital for ensuring the reliability of predictive models in healthcare contexts, where noisy input data is inevitable."}, {"title": "Noise Tolerance Experiment", "content": "The threshold sensitivity analysis evaluated the interplay between precision, recall, and F1-score across varying decision thresholds (0\u20131). As illustrated in Figure 5, these metrics demonstrated inverse relationships, with precision increasing at higher thresholds and recall exhibiting a declining trend. F1-score, a harmonic mean of precision and recall, provided a balanced measure of the model's performance and peaked at an optimal threshold of 0.30.\nAt lower thresholds, the model achieved high recall, ensuring that most true positives were identified. However, this came at the expense of precision due to the inclusion of false positives. In contrast, higher thresholds prioritized precision by limiting predictions to cases with higher confidence, thereby reducing recall. The F1-score reached its maximum at a threshold of 0.30,"}, {"title": "Threshold Sensitivity Analysis", "content": "Our predictive modeling approach showed significant potential for improving procedure rates. The optimal procedure rate, calculated using model predictions based on primary care diagnostic entries, is 60.1%, compared with the current 11.27% procedure rate calculated from data. In terms of relative increase, this represents a substantial 433% increase. Statistically significant differences between the current and optimal procedure rates were confirmed by a two-proportion binomial test (p < 0.001). These findings emphasize the capacity of predictive models to improve healthcare efficiency by optimizing referral workflows in orthopedic surgery."}, {"title": "Impact on Procedure Rate Improvements and Capture Efficiency", "content": "This study highlights the transformative potential of AI in optimizing orthopedic referral workflows through the prediction of procedural requirements directly from primary care diagnoses. Utilizing a real-world, de-identified dataset of 2,086 orthopedic referrals from the University of Texas Health at Tyler, this research emphasizes the applicability of pre-trained text embeddings, robust machine learning algorithms, and noise-tolerant models in predicting optimum clinical referral pathways.\nThis research advances the application of AI in healthcare by addressing the underexplored domain of procedural predictions in orthopedic referrals. While prior studies predominantly focus on diagnostic predictions or treatment recommendations \u00b9\u00b2,\u00b2\u00b2,\u00b2\u00b3, this work bridges the gap between diagnosis and intervention, providing actionable insights to optimize orthopedic surgical workflows. As discussed previously, orthopedic surgery is a highly limited resource, and this work illustrates Al's role in optimizing the utilization of this critical resource. As illustrated in Table 6, by predicting whether a patient will undergo a procedure based on a primary care referral ICD-10 codes and descriptions, the proposed model not only improves appropriate patient-physician allocation but also increases revenue for healthcare organizations.\nIn this study, the BGE-small-en-v1.5 (Base) model was identified as the optimal embedding model for procedural predictions based on diagnostic texts. Moreover, the unique contribution of this study is the evaluation of this embedding model's performance under simulated noise conditions. In line with previous studies that found that pre-trained embeddings such as ClinicalBERT and BioBERT are effective in capturing nuanced linguistic patterns in healthcare texts, our study demonstrates that BGE embeddings can accurately represent semantic and contextual representations in clinical diagnosis text \u2078,\u00b9\u2070,\u00b9\u00b9,\u00b2\u2074. Moreover, in this study, we examined the embedding model's performance under simulated noise conditions. Noise is common in medical documentation and is most commonly observed in the form of spelling or word substitution errors. These occur frequently in free text entries such as notes \u00b2\u2075. A study found one spelling error for every five sentences in discharge summaries and surgical reports, and the error rate rose to 10% in follow-up notes \u00b2\u2075. By examining the effects of character- and word-level perturbations, our study highlights the robustness of BGE embeddings, offering valuable insights into real-world applications where data variability is common \u00b2\u2076.\nA major challenge in adopting AI models in healthcare has been the interpretability of the results as Al models often operate as \u201cblack boxes\u201d \u00b2\u2077. Therefore, we have utilized Random Forest models to align with the goal of explainable AI in healthcare. Random Forest models provide transparency and enable clinicians to critically appraise the decision-making processes, thereby fostering trust and supporting regulatory compliance \u00b2\u2078,\u00b2\u2079. Another major challenge associated with clinical datasets is class imbalance, which is also addressed in this research. In clinical datasets, class imbalance is a pervasive problem that has been addressed through oversampling techniques, with SMOTE emerging as the most effective approach The effectiveness of SMOTE in improving model performance corroborates previous findings \u00b9\u2078,\u00b3\u2070, while the comparative analysis of balancing techniques provides practical insights for optimizing predictive models in healthcare settings. Through the threshold sensitivity analysis, we extend previous research on precision-recall trade-offs in diagnostic prediction \u00b3\u00b9,\u00b3\u00b2. By identifying an optimal threshold for"}, {"title": "Discussion", "content": "balancing precision and recall, the study offers a pragmatic approach to resource allocation optimization and clinical decision-making in imbalanced settings \u00b3\u00b2. Furthermore, we examined the financial and procedural implications of predictive modeling as a function of capture efficiency, which represents the proportion of model-predicted referrals that are successfully integrated into the healthcare system. By simulating 5,000 referrals to UT Health Tyler Orthopedics, the model demonstrated substantial improvements over baseline procedure rates (11.27%), with an optimal predicted procedure rate of 60.1%. Even at modest capture efficiencies, significant gains were observed; for instance, a 5% capture efficiency increased the effective procedure rate to 13.33% (18.27% improvement) and resulted in an additional $1.67 million in revenue."}, {"title": "Discussion", "content": "Accurate prediction of procedural requirements can minimize care delays, optimize preoperative planning, and enhance referral accuracy \u2075. The interpretability of Random Forest models ensures that predictive decisions remain transparent, aligned with clinical reasoning, and trusted by clinicians, promoting their adoption in practice. Additionally, ambient AI technologies can further enhance these benefits by reducing administrative burdens and providing real-time clinical decision support, which is especially relevant in resource-limited and high-need demographics \u00b3\u00b3. These systems adapt to evolving healthcare needs through continuous learning, seamlessly integrating into workflows to improve patient care efficiency and outcomes while alleviating clinician workload \u00b3\u00b3.\nAdditionally, closed-loop feedback systems have demonstrated success in enhancing medical decision-making and resource efficiency. For example, telemedicine encounters with a primary care provider and co-author (AM) for back pain patients in rural Arizona initially showed only 10% of referrals leading to surgery. After targeted education of the primary care provider from the neurosurgeon on diagnostic questions and physical exam findings, referral accuracy improved dramatically, with 80\u201390% of cases leading to appropriate surgical interventions. This empirical observation is consistent with a systematic review of 17 studies, which indicated that structured referral sheets and the involvement of specialists in primary care providers' education were the most effective strategies for improving referrals \u00b9. This closed-loop approach reduces wasted referrals, expedites access to alternative treatments, and improves satisfaction among patients, families, and providers \u00b3\u2074. As a result of the prediction model developed in this study, primary and secondary healthcare providers can receive real-time decision support, and referral performance analytics to scale up feedback mechanisms. Additionally, this approach may apply to other subspecialties, highlighting the possibility of a broader impact \u00b9."}, {"title": "Clinical Implications", "content": "The dataset, sourced from a single healthcare system, may limit generalizability to other clinical settings. Validation using multicenter datasets with diverse demographics is essential \u00b3\u2075. While focused on diagnostic text, incorporating additional data, such as imaging reports, laboratory results, and patient histories, could enhance accuracy and applicability. Finally, while BGE embeddings performed robustly, fine-tuning on orthopedic-specific datasets could further"}, {"title": "Clinical Implications", "content": "improve their ability to capture domain-specific nuances, enhancing predictive performance in complex clinical scenarios \u00b3\u2076. Future research should explore broader applications of AI in surgical care, including predicting surgical outcomes, optimizing resource allocation, and identifying patients at risk for postoperative complications. The ethical dimensions of AI, including bias mitigation, accountability, and privacy, require ongoing attention to ensure equitable and responsible implementation \u00b3\u2077. Additionally, the integration of real-time feedback loops into clinical workflows can enable continuous improvement, ensuring sustained performance and adaptability to emerging clinical challenges."}, {"title": "Limitations and Future Directions", "content": "By predicting surgical requirements directly from primary care diagnostic entries, this study highlights the transformative potential of AI in reshaping orthopedic referral workflows. These findings are based on robust text embeddings, noise-tolerant models, and interpretable machine learning algorithms that provide actionable insights for improving clinical decision-making, optimizing financial outcomes, and improving operational efficiency. By bridging diagnostic and procedural gaps, this study demonstrates the significance of AI as a catalyst for precision-driven, patient-centered care that enhances the efficiency and effectiveness of healthcare delivery."}, {"title": "Conclusion", "content": "The threshold sensitivity analysis evaluated the interplay between precision, recall, and F1-score across varying decision thresholds (0\u20131). As illustrated in Figure 5, these metrics demonstrated inverse relationships, with precision increasing at higher thresholds and recall exhibiting a declining trend. F1-score, a harmonic mean of precision and recall, provided a balanced measure of the model's performance and peaked at an optimal threshold of 0.30.\nAt lower thresholds, the model achieved high recall, ensuring that most true positives were identified. However, this came at the expense of precision due to the inclusion of false positives. In contrast, higher thresholds prioritized precision by limiting predictions to cases with higher confidence, thereby reducing recall. The F1-score reached its maximum at a threshold of 0.30, marking the point of equilibrium between precision and recall. This threshold represents the most effective compromise for maximizing overall model performance in imbalanced classification settings.\nThe optimal threshold of 0.30 holds significant implications for clinical decision-making. In contexts such as predicting procedural requirements, a balanced approach is essential to minimize false positives while maintaining high sensitivity to identify true cases. The identified threshold ensures that the model can provide reliable predictions without overburdening healthcare systems with unnecessary referrals or missing critical cases. This adaptability underscores the model's utility in real-world clinical applications."}]}