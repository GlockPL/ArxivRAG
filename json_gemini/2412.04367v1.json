{"title": "Machine Theory of Mind for Autonomous Cyber-Defence", "authors": ["Luke Swaby", "Matthew Stewart", "Daniel Harrold", "Chris Willis", "Gregory Palmer"], "abstract": "Intelligent autonomous agents hold much potential for the domain of cyber security. However, due to many state-of-the-art approaches relying on uninterpretable black-box models, there is growing demand for methods that offer stakeholders clear and actionable insights into their latent beliefs and motivations. To address this, we evaluate Theory of Mind (ToM) approaches for Autonomous Cyber Operations. Upon learning a robust prior, ToM models can predict an agent's goals, behaviours, and contextual beliefs given only a handful of past behaviour observations. In this paper, we introduce a novel Graph Neural Network (GNN)-based ToM architecture tailored for cyber-defence, Graph-In, Graph-Out (GIGO)-ToM, which can accurately predict both the targets and attack trajectories of adversarial cyber agents over arbitrary computer network topologies. To evaluate the latter, we propose a novel extension of the Wasserstein distance for measuring the similarity of graph-based probability distributions. Whereas the standard Wasserstein distance lacks a fixed reference scale, we introduce a graph-theoretic normalization factor that enables a standardized comparison between networks of different sizes. We furnish this metric, which we term the Network Transport Distance (NTD), with a weighting function that emphasizes predictions according to custom node features, allowing network operators to explore arbitrary strategic considerations. Benchmarked against a Graph-In, Dense-Out (GIDO)-ToM architecture in an abstract cyber-defence environment, our empirical evaluations show that GIGO-ToM can accurately predict the goals and behaviours of various unseen cyber-attacking agents across a range of network topologies, as well as learn embeddings that can effectively characterize their policies.", "sections": [{"title": "Introduction", "content": "As autonomous systems are entrusted with increasingly critical tasks across various industries, the need for explainability has become paramount.1-6 Recent advancements have established deep neural networks (DNNs) as the cornerstone of autonomous systems due to their proficiency in handling the complex, high-dimensional data required to scale to real-world environments.7-10 However, in safety-critical domains where decisions can have far-reaching consequences, the opacity of these \u2018black-box' models poses significant risks, raising concerns around trust, accountability, and safety.11,12 One such domain is that of cybersecurity, where the proliferation of complex computer networks with ever-growing attack surfaces has fueled a cyber arms race with both attackers and defenders increasingly leveraging DNNs for their respective strategies. 13\u201319 Consequently, there is growing urgency for methods to interpret and analyze the latent decision-making processes of these models.\nIn recent years, a plethora of approaches have emerged aimed at demystifying DNNs. In the context of cybersecurity, these include Explainable Artificial Intelligence (XAI) techniques for malware and anomaly detection,20 as well as solutions that rationalize the actions of deep reinforcement learning agents.21,22 However, these approaches typically focus on post-hoc, data-centric clarifications of how a model arrived at a particular decision rather than why, and thus lack the level of contextual detail that humans generally deem important when making high-stakes decisions.11,23,24,24\u201326 This limitation becomes especially pronounced in multi-agent decision-making scenarios where understanding the intentions, strategies, and circumstances of other entities can significantly impact outcomes. For example, a computer network defender capable only of identifying the presence and mechanics of a given attack is at a significant disadvantage compared to one who can additionally infer the adversary's objectives. Without understanding why certain actions occur, the defensive strategies of the former risk being reactive and insufficient in anticipating evolving threats.\nIn the psychological literature, the human ability to infer the latent mental states of others is known as a 'Theory of Mind' (ToM).27 An extensive corpus of cognitive studies has linked the absence of ToM with significant social deficiencies in humans.28-32 This has lead several researchers to investigate whether the intersubjective capabilities of AI systems can be improved with ToM-inspired model architectures. This topic was first explored in Machine Theory of Mind (MToM) by Rabinowitz et al.,33 who implemented an observing agent\u2014a parameterized model, dubbed \u2018ToMnet\u2019\u2014that employs meta-learning to make predictions over the goals, strategies, and characters of observed agents based solely on past behaviour observations."}, {"title": "Preliminaries", "content": "In this report we evaluate the ability of ToMnet architectures to characterize agents situated within cyber-defence games. Our focus is on scenarios featuring two agents, a Red cyber-attacking agent and a Blue cyber-defence agent. For our specific use-case, we consider a scenario that we term the hot-desking user problem. The problem consists of a computer network G with a static topology (i.e. the set of nodes v \u2208 V and the edges e \u2208 E connecting them remains the same for a given topology). In each episode we have changes with respect to:\n(i) the vulnerabilities of each node v (e.g. due to different services running);\n(ii) the location of the high-value nodes, which are determined based on the location of a set of users u EU.\nWe assume that the number of users |U| remains consistent across episodes. However, due to hot-desking, the locations of the users change after each episode. Therefore, in each episode we have a different set of tuples (ui,vj), where i represents each user and j represents each high-value node. In our scenarios, a Red cyber-attacking agent is attempting to evade a Blue cyber-defence agent in order to reach the high-value nodes, over which Red has a preference. This preference could be due to, say, a certain user working on a classified topic of interest to the Red agent. Based on the log-files from previous attacks\u2014the past trajectories (obs) \u2014our ToMnet architectures are tasked with predicting:\n(i) the exact node that Red is targeting: v;\n(ii) the attack trajectory followed by Red on the path towards \u00ee (or, the successor representation): SR.34\nThe hot-desking user problem can be thought of as a partially observable Markov game (POMG). Unlike in (fully observable) Markov Games, also known as stochastic games,49 the full state of the environment is hidden from all players within a POMG. This closely reflects the conditions of real-world cyber-defence scenarios, wherein cyber-attacking agents will typically be unable to observe the complete layout of the network and all the services running on each node. Similarly, cyber-defence agents will rarely be able to observe the full state of the network given the computational overhead that would be required to relay all of this information at every time-step.\nFormally, our task for ToMnet is to make predictions over a family of POMGs:\n$M=\\bigcup_j M_j$.\nEach Mj represents a POMG and is defined as a tuple (N,S,A,P,R, \u03b3, \u03a9, \u039f), where:\n\u2022 N is a set of agents.\n\u2022 S is a finite state space.\n\u2022 A is a joint action space (A1 \u00d7 \u00d7 An) for each state s \u2208 S, with A; being the number of actions available to player i within a state s.\n\u2022 P is a state transition function: S\u2081 \u00d7 A\u2081 \u00d7 ... \u00d7 An \u00d7 St+1 \u2192 [0, 1], returning the probability of transitioning from a state St to St+1 given an action profile a\u2081 \u00d7 ... \u00d7 an. Here, each action ai belongs to the set of actions available to agent i: ai \u2208 Ai.\n\u2022 \u03a9 is a set of observations.\n\u2022 O is an observation probability function defined as O : S \u00d7 A\u2081 \u00d7 ... \u00d7 An \u00d7 \u03a9 \u2192 [0, 1], where, for agent i, a distribution over observations o that may occur in state s is returned, given an action profile a\u2081 \u00d7 ... \u00d7 an.\n\u2022 R is a reward function: R : S\u2081 \u00d7 A\u2081 \u00d7 ... \u00d7 An \u00d7 St+1 \u2192 R, that returns a reward r.\n\u2022 y is a discount rate defining the agent's preference over immediate and future rewards within the decision-making process. Larger values for y define a preference for long-term rewards.\nFor the environments considered in this paper we also allow terminal states at which the game ends.\nThe ToMnet architectures in this report are tasked with making predictions over a range of network topologies with different node settings. We also assume we have a family of agents:\n$N = \\bigcup_i N_i$,\nwith each Ni representing a specific agent. Each agent is implemented by a policy \u03c0\u2081, defining the agent's behaviour. We shall use the same problem formulation as Rabinowitz et al.,33 and associate the reward functions, discount factors, and conditional observation functions with the agents N rather than the POMGs M. Therefore, for each POMG Mj we have a tuple (S, A,P), and for each agent N; we have (\u03a9, O,R, \u03b3, \u03c0).\nFor our ToMnet approaches we assume access to (potentially partial and/or noisy) observations of the agents, via a state-observation function w(obs)(\u00b7) : S \u2192 \u03a9(obs) and an action-observation function a(obs) (\u00b7) : A \u2192 a(obs). For each agent Ni in a POMG Mj, we assume our observer can see a set of trajectories, defined as sets of state-action pairs: \u03c4(obs) = {(s(obs), a(obs))}=0, where a(obs) = a(obs) (at) and s(obs) = (obs) (st). Critically, setting w(obs)(\u00b7) and (obs)(.) to the identity function grants the observer full observability of the POMG state along with all overt actions taken by the agents while keeping their parameters, reward functions, policies, and identifiers concealed. This formulation closely approximates the depth of knowledge that would be available to a human observer in a real-world scenario.\nOur approaches are based on the original ToMnet33. To recap, ToMnet consists of three modules, a character net, mental net, and a prediction net. The character net fe yields a character embedding ec,i upon parsing Npast past trajectories \u03c4(obs) for an agent Ni, {\u03c4(obs)}(obs) for an agent Ni, {\u03c4(obs)(obs) for an agent Ni, {\u03c4(obs)"}, {"title": "Graph-Based YAWNING-TITAN", "content": "We simulate the hot-desking user problem in an adapted version of the YAWNING-TITAN cyber-defence environment. This is an open-source framework originally built to train cyber-defence agents to defend arbitrary network topologies.35 Each machine in the network has parameters that affect the extent to which they can be impacted by Blue and Red agent behaviour. These include vulnerability scores that determinine how easy it is for a node to be compromised. Our extension includes graph observations for both graph-based ToMnet architectures and graph-based Blue cyber-defence agents. Critically, these observations will contain different combinations of node features depending on the observability of the agent being trained. For example, a ToMnet architecture with full observability will receive observations containing relevant labels in the node features (such as high-value nodes), whereas these will be absent for cyber agents with partial observability."}, {"title": "Networks", "content": "Custom network configurations are required to systematically evaluate GIGO-TOM across different network sizes and topologies. Therefore, we have added a new network generator, TreeNetwork, which resembles a tree network with layers for the core, edge, aggregation, access, and subnet nodes and can be easily visualized (Figure 3a \u2013 3e)."}, {"title": "Agents", "content": "To enable an extensive evaluation, we have additionally implemented a range of rule-based Blue cyber-defence and Red cyber-attacking agents to our graph-based YAWNING-TITAN framework. While our agent repertoire is broad (a comprehensive overview is provided in Appendix B), our evaluations suggest that only two were useful in generating interesting behaviour for our experiments:\n\u2022 BlueMSN-D: A deterministic agent that removes the infection from a compromised node (via the MakeSafeNode action) if it is within three hops of any high-value node, otherwise scans the network for hidden compromised nodes (via the Scan action). The strategy here is to protect only the core of the network. Blue is content with sacrificing the edge nodes to do so.\n\u2022 RedHVTPreferenceSP: Selects a particular high-value node to target at the start of the episode and takes the shortest path towards it. The selected high-value node is based on a high-value node preference vector sampled from a Dirichlet distribution \u03c0 ~ Dir(a) as well as the shortest distance between high-value nodes and entry nodes. Therefore, following,33 we can define an agent species as corresponding to a particular value of a, with its members being the various parameterizations of a sampled from Dir(a). This attacker has prior knowledge of the network structure, for example, due to insider information, meaning it can calculate path lengths and take the most direct path to its chosen high-value node.\nThe motivation for using rules-based agents over incentive-based learning approaches lies in their interpretability. For example, we can define a preference over high-value targets and actions, as well as reducing the amount of time required for gathering training and evaluation data for our experiments. However, we note that, in principle, ToM approaches are agnostic with respect to how the Blue and Red agent policies are obtained, and can therefore also be applied to learnt policies."}, {"title": "Methods", "content": "As previously mentioned, one of the challenges when applying ToMnet to cyber-defence scenarios is that that the number of nodes in the network is not necessarily known in advance. This has implications for both the input and output layers of ToMnet. For our current problem domain, neural network layers are required that can handle variable sized inputs.13 There are now a number of solutions to this problem. For example, using transformers51 or applying one dimensional convolutions with global max pooling. However, we consider that a natural format for representing a cyber-defence scenario on a computer network is to use a graph-based representation. Therefore, we have implemented a graph-based observation wrapper for the YAWNING-TITAN cyber-defence environment.35\nIn order to process the graph-based state-observations, we introduce two graph-based ToMnet architectures for making predictions for cyber-defence scenarios: i.) a Graph-in, Graph-Out ToMnet (GIGO-ToM), where both input and output layers are implemented using graph neural networks, and; ii.) a Graph-in, Dense-Out ToMnet (GIDO-TOM), where inputs are processed by graph neural network layers and outputs are generated by subsequent dense neural network layers, more closely approximating the original ToMnet formulation to benchmark our evaluations.\nFollowing evidence from the literature that graph attention layers52 are less susceptible to the \u2018over-squashing' of node features than other popular candidates,53 these are used for node feature extraction end-to-end in both models.\nFor both GIDO-TOM and GIGO-ToM, the character network processes sequences of graph-based state-observations $\\tau_{0}^{(obs)}$ called trajectories. From the trajectories, each graph is fed through two GATv2 layers combined with dropout layers (p = 0.5). The outputs from each layer are then subjected to both global max and global average pooling, with the outputs subsequently being concatenated. The resulting features for each graph are then sequentially fed into an LSTM that generates a character embedding ec,i for each individual trajectory i. The outputs from the LSTM are then summed as described in Equation 3, resulting in the character embeddings.\nGATv2 layers are also used to extract features for the mental embedding em and for extracting features that are fed into the prediction network. While our GIDO-ToM and GIGO-TOM architectures share the same feature extraction for the character and mental network components, they differ with respect to the implementation of the prediction network and output layers. Below we describe these differences in more detail."}, {"title": "Graph-In, Dense-Out ToMnet", "content": "Similar to the original ToMnet, for GIDO-TOM, the character and mental embeddings, ec and em, along with the current state observation s(obs), are fed into a prediction network. The prediction network first uses GATv2 layers to extract features from the s(obs), before concatenating the resulting embedding with ec and em. The concatenated embeddings are subsequently fed through a number of dense layers before being passed to the individual output layers: i.) a prediction over the consumable \u00ea, representing a prediction over which high-value target the Red cyber-attacking agent wants to reach, and; ii.) the successor representation SR.\nAs for the original ToMnet,33 past and current trajectories will be sampled for each agent N\u012f to obtain predictions. Each prediction provides a contribution to the loss, where the average of the respective losses across each of the agents in the minibatch is used to give an equal weighting to each loss component."}, {"title": "Graph-In, Graph-Out ToMnet", "content": "In contrast to GIDO-TOM, for GIGO-TOM we concatenate the character and mental network embeddings directly with each node's features within the current state-observation. These are then directly processed by GNN layers before each of the prediction tasks. As mentioned above, for GIGO-ToM, the high-value target and successor representation predictions are generated using GATv2 layers. The final GNN layer of each respective prediction branch represents the outputs, i.e. a weighted binary cross entropy loss is applied to each node for optimizing high value node predictions:\n$L_{hvt,i} = -\\sum_n w_n(y_n logx_n + (1 - y_n) log(1 - x_n))$.\nwhere x represents the ground-truth with respect to whether or not a node was consumed at the end of the episode, and y is the prediction. The positive weighting term wn is added to account for the positive / negative node imbalance within an individual sample, i.e., only one out of n nodes will (potentially) be consumed at the end of a given episode.\nFor the successor representation loss Ls, we stick with the soft label cross entropy loss from Rabinowitz et al.33 For batches where the number of nodes on each graph is inconsistent we zero-pad while computing the loss. We note this formulation only requires padding when computing the loss, meaning at inference time there are no superfluous nodes (in contrast to when specifying the max number of nodes for GIDO-TOM).\nThe approach outlined above provides a flexible formulation regarding the number of nodes in the network, with the output topology reflecting that of the current state-observation fed into the prediction net."}, {"title": "The Network Transport Distance", "content": "Of the ToMnet outputs, one that is particularly salient for the depth of insights on offer to network administrators is the successor representation. In the field of reinforcement learning, these are formally defined as matrices representing the expected discounted future state occupancy following a given policy from a given state.34 In the case of our hot-desking user problem, these represent predictions regarding the likely attack trajectory that Red will take against the current Blue cyber-defence agent. Predicted successor representations are often evaluated using generic metrics that assume input data points are independent and identically distributed (i.i.d.). However, in real-world cyber-defence scenarios, the nodes of a network will often have individual features granting them varying degrees of vulnerability, utility, and overall strategic importance,54 and will adhere to a certain topological configuration. As such, a metric for evaluating predicted successor representations that captures these contextual nuances is desirable.\nWe achieve this with the Network Transport Distance (NTD); a unit-bounded extension of the Wasserstein Distance (WD) that includes a graph-theoretic normalization factor that represents the result as a fraction of the worst-case predictive scenario for the network in question.\nThe metric can be calculated as follows:\n$NTD(P, Q, D) = \\frac{1}{\\max(D)} \\inf_{\\mu \\in \\mathcal{M}(P, Q)} [\\int_{X \\times X} d(i, j) d\\mu(i, j)]$,\nwhere:\n\u2022 P and Q represent probability distributions in which individual elements correspond to nodes in a graph G.\n\u2022 D represents the matrix of pairwise shortest path lengths between nodes in G. The diameter of G is therefore max(D).\n\u2022 $inf_{\\mu \\in \\mathcal{M}(P, Q)}$ denotes the infimum (i.e. the greatest lower bound) over all possible transport plans \u03bc. $\\mathcal{M}(P, Q)$ represents the set of all joint distributions whose marginals are P and Q.\n\u2022 $\\int_{X \\times X} d(i, j) ,d\\mu(i, j)$ calculates the total cost of transporting mass from distribution P to distribution Q. The function d(i, j) is the ground metric on the graphical space X, which here is the shortest path length between nodes i and j in G. Therefore, d(i, j) = D[i, j]. du(i, j) denotes the amount of probability mass transported from i to j.\nLike the Wasserstein Distance (WD), the NTD is symmetric and non-negative:\n$NTD(P, Q, D) = 0 \\Leftrightarrow P = Q$.\nUnlike the WD, however, the NTD has an upper bound of 1:\n$NTD(P, Q, D) = 1 \\Leftrightarrow WD(P, Q, D) = \\max(D)$.\nThis makes the metric network-agnostic and more interpretable to a broad audience, including non-technical stakeholders.\nFor added contextual awareness, we weight the Network Transport Distance by selecting a set of node features from a network based on their relevance to the strategic importance of node attacks, then linearly combining them with a corresponding set of user-specified coefficients using a customizable weighting function that can be applied to arbitrary metrics. We denote the end-to-end metric NTD\u04e8.\nMathematically, for a network G with n nodes, we have:\n\u2022 A set of m user-selected node feature vectors: X = [\u00d1o,\u2026\u2026\u2026,\u012am] \u2208 Rn\u00d7m.\n\u2022 A corresponding set of m user-specified feature weights: C = [co,..., Cm] \u2208 [-1,1]m.\nOur weighting function W : Rn\u00d7m \u00d7 [-1,1]m \u00d7 [0, 1] \u2192 [0,1]\u201d linearly combines these and normalizes the result:\n$W(x,C,f) = \\frac{\\sum_{i=0}^m c_i||x_i||_f}{\\sum_i ||x||} = w,$\nwhere:\n\u2022 X\u00a1 \u2208 X is a node feature vector.\n\u2022 ci \u2208 C is a user-specified constant that determines the weight of x; in the final metric (\u22121 < ci < 1).\n\u2022 ||\u00b7|| f is the min-max scaling function with \u2018floor' parameter f, with f defining the minimum weight that can be assigned to an input value (0 < f < 1). This forces all outputs to lie in the interval [f, 1], thereby guaranteeing a minimum level of influence on the output metric:\n$||x||_f = \\frac{(x - \\min(x))(1 - f)}{\\max(x) - \\min(x)} + f$.\nThe computed weighting w is used to rescale the input probability distributions before passing them into the Network Transport Distance. Therefore, for two G-based probability distributions P, Q \u2208 R\", we have:\n$NTD_{W}(P, Q) = NTD(P_{\\Theta}, Q_{\\Theta}),$\nwhere xe = w.x/ \u03a3w.x It follows that W is theoretically decoupled from the metric it is applied to, and can therefore be applied to any metric that is amenable to input weighting.\""}, {"title": "Experiments", "content": "In this section, we empirically evaluate GIGO-ToM's ablity to provide actionable insights into the characters, goals, and behaviours of various cyber-attacking agents in the YAWNING-TITAN environment, using the hot-desking user problem as our task formulation.\nFirst, we explore GIGO-ToM's ability to characterize the policies of unseen cyber-attacking agents. We next investigate how well GIGO-ToM can predict the services targeted by these agents as well at their attack routes towards them.\nBenchmarked against our dense-layered ToMnet implementation (GIDO-ToM), we find that, through observing past episodes of Blue and Red agents interacting with each other, GIGO-ToM can learn character embeddings that can effectively differentiate between Red agents' preferred outcomes, providing a means to characterize their policies. We also observe that GIGO-TOM can accurately predict Red's preference over high-value nodes as well as their attack trajectories towards them, occasionally predicting multiple attack paths in cases where the target node is uncertain.\nWe now describe our experimental set-up before presenting these results in detail.\nGAMES: For the following experiments, we consider games consisting of the BlueMSN-D cyber-defence agent against 1,000 possible parameterizations of the RedHVTPreferenceSP cyber-attacking agent. The policy of each Red agent N; is defined by a fixed vector \u03c0\u2081 specifying its preference over high-value targets. These are sampled from a single agent species: a Dirichlet distribution with concentration parameter a = 0.01, ensuring a diverse range of sparse policies. For generality, we build a mixed topology setting named TreeNetworkMixed for which each game is rolled out in one of the following network topologies: {TreeNetwork30, TreeNetwork40, TreeNetwork50, TreeNetwork70, TreeNetwork90}. Each network has three high-value users/nodes. The location of the high-value nodes (and therefore the location of the high-value users) are randomly selected at the start of each episode. One entry node is provided to the Red cyber-attacking agent. Given that our focus is on our models' ability to predict high-value node locations, we simplify the task with respect to the entry node by keeping it consistent across episodes. To ensure a dataset of distinctive trajectories for our experiments, high-value nodes are always situated on leaf nodes.\nDATA: To build a ToMnet dataset DTOM, a set of Blue cyber-defence agents Nblue and a set of Red cyber-attacking agents Nred is defined, along with a set of network configurations M for them to compete within. The Cartesian product of these three sets defines a set of game configurations: G = Nblue \u00d7 Nred \u00d7 M. For each game g \u2208 G, nc current episodes are generated, for each of which a distinct set of np further past episodes is generated so that the sets of past trajectories Pi for all samples si \u2208 DTOM are mutually exclusive: \u2229DTOM P\u2081 = 0. This precludes data leakage between training and validation sets inflating performance metrics. Thus, Ntotal = nc+ ncnp episodes are generated for each game configuration g \u2208 G, resulting in a database of trajectories: U{ij} total. We fix nc = 3 and np = 8 for all experiments, generating 27 total trajectories for each game configuration g\u2208 G. Therefore, using a single BlueMSN-D agent against 1,000 parameterizations of the RedHVTPreferenceSP agent in the TreeNetworkMixed topology setting, a set of 3,000 current episodes are generated along with a pool of 24,000 episodes from which past and current trajectories could be extracted for building data samples. Each data sample si \u2208 DTOM consists of Npast past trajectories for the character network, a current trajectory up to a certain time-step t for the mental network, and the current state observation at that step s(obs) for the prediction network, along with any ground-truth variables required for evaluation (e.g. Vtrue and SRtrue). Given that trajectories often comprise hundreds of time-steps under our settings, we sample state-observations from past trajectories at regular intervals to reduce wall-times. Five state-observations were sampled from each past trajectory for the following experiments. For each current trajectory parsed by the prediction network, we uniformly select state observations s(obs) from the first two time-steps of the current episode. We split our resulting dataset into train and validation sets according to the ratio of 75% / 25%.\nEVALUATION: To evaluate GIGO-TOM, we systematically examine each of its modules' outputs over a hold-out test set of 200 unseen Red agents. Character networks are qualitatively evaluated through inspecting the embeddings echar they generate for each agent. We collect the embeddings obtained from our test samples and apply t-SNE55 in order to obtain 2D visualizations of the clusters that emerge. Prediction networks are assessed with respect to their outputs: the high-value node \u00fb and the successor represenation SR. Predictions regarding targeted high-value nodes \u0176pred are evaluated against ground-truth labels \u0176true using weighted F1 scores and confusion matrices. Predicted successor representations SRpred are evaluated against true attack paths SRtrue using our novel distance metric: the Network Transport Distance (NTD). Given that NTD scores are computed on a per-sample basis, violin and box plots are used to visualize their distribution and summary statistics over the test set.\nFor each prediction task, we explore the influence of both past trajectory exposure (Npast) and network size on predictive performance. To investigate how SR predictions vary along spatial and temporal dimensions, we add two further evalution conditions. First, we evaluate predicted successor representations over three discount factors \u03b3\u2208 {0.5,0.95,0.999} to emphasize rewards at various points in the future (y = 0.5 represents short-term attack trajectories, while y = 0.999 represents long-term trajectories). Second, to better understand any irregularities in GIGO-ToM's predictions or Red agent behaviour, we equip the NTD with our novel weighting function W (Equation 12) to evaluate how accuracy varies with a custom node remoteness feature, defined as the length of the shortest path from a given node to the entry or targeted high-value node. This allows us to investigate if and when predicted or ground-truth attack paths venture into remote regions of the network that lie away from the path between the entry and high-value node. Weighted NTD scores are denoted NTD. We fix f = 0.1 wherever W is used throughout this report. For brevity, since we are only using a single node feature in our weighting configuration, @ will refer to the node remoteness weighting coefficient for the remainder of the report.\nFor each prediction task, GIGO-ToM's performance is benchmarked against our dense-layered ToMnet variant, GIDO-TOM, on the same test set. Results for each of these are summarized below."}, {"title": "How well can GIDO-ToM/GIGO-ToM characterize various cyber-attacking agents?", "content": "shows clustered character network embeddings echar for both GIGO-TOM and our benchmark model, GIDO-TOM, over hold-out test sets of unseen Red agents when given access to varing numbers of past behaviour observations (Npast \u2208 {1,2,3,4}). We find that while GIDO-ToM generally produces compact and clearly separable clusters, they exhibit poor intra-cluster consistency, indicating confusion between the policies of observed agents. By contrast, GIGO-ToM exhibits strong generalization capabilities across all evaluation settings, consistently producing coherent clusters that are each predominantly characterised by one of the target users. For GIDO-TOM, performance peaks at Npast = 3 and degrades again for Npast = 4, heavily overlapping the clusters for two of the users. For GIGO-ToM, by contrast, the consistency, compactness, and separateness of clusters improves with higher values of Npast, demonstrating its ability to learn agents' policies and generalize effectively in a few-shot manner. We accordingly hypothesize that GIGO-ToM's performance on this task would further improve with access to more past data samples."}, {"title": "How well can GIDO-ToM/GIGO-ToM predict the services targeted by cyber-attacking agents?", "content": "For target high-value node prediction we observe that GIGO-TOM vastly outperforms the benchmark, consistently achieving weighted F1 scores exceeding an order of magnitude higher across all evaluation settings . At first glance, GIGO-TOM's peak F1 score of 0.6893 (Npast = 4) suggests moderate performance. However, given the complexity of the classification problem, with 60 possible high-value nodes across the TreeNetworkMixed topology, this score indicates a strong predictive capability. Such is clear from , which depicts row-normalized confusion matrices over the set of all viable high-value nodes accross the TreeNetworkMixed topologies. The clearly distinguishable diagonals for each evaluation setting demonstrate minimal confusion between predicted target nodes.\nWe initially hypothesized that the class proliferation introduced by the larger networks within TreeNetworkMixed would degrade GIGO-TOM's performance on this task. However, by evaluating GIGO-TOM's performance on the individual network topologies within TreeNetworkMixed, we found no clear correlation between the number of nodes present in a network and predictive performance . What did affect performance, however, was the number of branches (and therefore possible high-value target locations) present in the network. This is clear from the performance drop observed for the TreeNetwork40 and TreeNetwork70 topologies, which had 6 and 8 branches respectively, compared with just 4 for the others. This suggests that while GIGO-ToM's high-value target prediction capabilities are robust to high-dimensional feature spaces within the range tested in this paper, it is vulnerable to higher rates of misclassification as the number of network branches increases."}, {"title": "How well can GIDO-ToM/GIGO-ToM predict the attack trajectories of cyber-attacking agents?", "content": "In this section, we evaluate GIGO-ToM's ability to predict Red agent attack routes given a handful of past behaviour observations. displays mean baseline (unweighted) NTD scores for each Npast run for both GIGO-ToM and our benchmark model, GIDO-ToM. We find that GIGO-TOM consistently predicts more accurate attack trajectories than GIDO-ToM, with performance improving with increased past behaviour exposure, as for our previous experiments. Furthermore, we observe a negative correlation between GIGO-ToM's predictive performance and network size up to the 50-node mark, after which median NTD scores plateau and variability improves slightly . This finding suggests that GIGO-ToM's successor representation prediction capabilities are robust to the larger networks in our test range despite having to predict progressively longer attack routes.\nTo investigate how NTD scores vary across temporal and spatial dimensions, we plot the distribution and summary statistics of NTD scores across different values of y (predictive time horizon) and different values of the coefficient of our node remoteness feature (spatial proximity to the entry or targeted high-value node in graph space) in our metric weighting function W in Figure 13a.\nWe firstly find that performance is significantly affected by one's choice of y. Unsurprisingly, as before, short-term predictions (\u03b3 = 0.5) are best. This is due to the relative complexity of the YAWNING-TITAN environment, which presents a significantly more challenging learning problem when the model is forced to account for more distant future states. The same is true spatially, underscored by the significant NTD deterioration observed when inverting the node remoteness weighting from -1 to 1 (thereby switching the emphasis from nodes closer to the Red agent's start and end points to those further away).\nThis could indicate one of two possibilities: either GIGO-TOM predicts paths into remote regions of the network that the Red agent does not in fact visit, or the Red agent visits remote regions of the network that GIGO-TOM fails to predict. One would expect to see instances of the latter if the Red agent were often pushed off-course by the Blue agent's defensive activity. However, given the strong determinstic policies of the RedHVTPreferenceSP agents used here and our inclusion of only episodes that result in Red agent victory, the former is more likely."}, {"title": "Discussion", "content": "In this work, we evaluate the efficacy of Theory of Mind-inspired metacognitive model architectures in providing actionable insights into the goals and behavioural patterns of adversarial cyber agents. We find that, through observing past episodes of Blue and Red cyber agents interacting with each other within the YAWNING-TITAN environment, our graph-based Machine Theory of Mind approach, GIGO-TOM, can learn character embeddings that can be clustered based on the Red agent's preferred outcome, thereby providing a means to characterize observed offensive cyber agents. We also find that our architecture can accurately predict Red's preference over high-value nodes, as well as the intermediate nodes that are likely to come under attack as the episode unfolds.\nWe note that there are numerous avenues for future research in this area, including a number of experiments that one could run with our framework. For example, we are keen to design representative \u2018Sally Anne' test28,56 scenarios for cyber"}]}