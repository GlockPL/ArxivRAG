{"title": "PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning", "authors": ["Jiaru Zou", "Mengyu Zhou", "Tao Li", "Shi Han", "Dongmei Zhang"], "abstract": "Large language models (LLMs) have played a fundamental role in various natural language processing tasks with powerful prompt techniques. However, in real-world applications, there are often similar prompt components for repeated queries, which causes significant computational burdens during inference. Existing prompt compression and direct fine-tuning methods aim to tackle these challenges, yet they frequently struggle to strike an optimal balance between cost-efficiency and performance effectiveness, especially in complex tasks such as NL2Code. In this paper, we propose a novel method namely PromptIntern to internalize the prompt knowledge into model parameters via progressive fine-tuning. Our method enables LLMs to emulate the human learning process for a new task, where detailed templates and examples in a prompt are gradually internalized and phased out progressively as the model grows accustomed to the task. Extensive experiments demonstrate that our method reduces inference tokens over 90%, speedups inference by 4.2 times, and saves 88.3% monetary cost.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have become pivotal in numerous natural language processing (NLP) applications, such as natural language generation (Dong et al., 2019), reasoning (Zhu et al., 2023; Sui et al., 2023), and code generation (Luo et al., 2023; He et al., 2024; Rozi\u00e8re et al., 2024). To enhance the predictive accuracy of LLMs in domain-specific tasks, recent techniques in fine-tuning, such as parameter-efficient fine-tuning (PEFT) (He et al., 2021; Hu et al., 2021; Lester et al., 2021), have been developed for pre-trained models to excel in specific tasks by adjusting their parameters to better align with targeted datasets (Hu et al., 2021). Many of these fine-tuning approaches typically adopt prompts that are optimized and integrated with detailed instructions, examples, and retrieved documents through prompt engineering techniques such as chain-of-thought (Wei et al., 2022), few-shot prompting (Brown et al., 2020), and retrieval-augmented generation (Lewis et al., 2020). (Cheng et al., 2023).\nAlthough these advancements enhance the capabilities of LLMs during fine-tuning, they also present new challenges: Prompt engineering often necessitates longer prompts, and directly integrating lengthy prompts into training process further increases computational costs during inference (VM et al., 2024). This increase in cost precludes LLMs in many cost-sensitive scenarios where computational resources are constrained. Several prompt compression methods (Li et al., 2023; Jiang et al., 2023a; Pan et al., 2024) have been proposed to re-"}, {"title": "2 Related Work", "content": "Prompt compression rephrases original prompts more concisely and is classified into task-aware and task-agnostic approaches. Task-aware approaches, like LongLLMLingua (Jiang et al., 2023b), adopt a question-aware coarse-to-fine strategy, adapting information based on the query, while methods like soft prompts (Wingate et al., 2022; Liu et al., 2022; Mu et al., 2024) use learnable tokens to condense prompts. Conversely, task-agnostic methods utilize metrics such as information entropy to eliminate redundant prompt information, with systems like LLMLingua (Jiang et al., 2023a; Li et al., 2023) estimating token importance using a smaller model. Despite the demonstrated effectiveness of these methods, producing compressed text that can generalize across different tasks and be effectively integrated into training scenarios remains a challenge.\nModel fine-tuning adapts pre-trained LLMs to specific tasks by modifying parameters. Based on the assumption that fine-tuning adds less new information to the model pre-trained on large internet-scale datasets, Parameter-Efficient Fine-Tuning (PEFT) methods aim to curtail the costs of tuning large models by adjusting a subset of parameters. Existing PEFT methods can be broadly categorized into three main approaches: 1) Adapter-based methods (Houlsby et al., 2019; He et al., 2021): Introduce trainable modules within a static \"back-bone\" network, offering flexibility but potentially increasing model size. 2) Prompt-based methods (Lester and Constant; Razdaibiedina et al., 2023; Nashid et al., 2023): Employ trainable \"soft tokens\" at input sequence start, requiring effective prompt design per task. 3) Low-rank adaptation methods (Hu et al., 2021; Dettmers et al., 2024; Liu et al., 2024): Use low-rank matrices to approximate required weight adjustments, avoiding additional inference burden and often delivering strong performance."}, {"title": "3 Problem Formulation", "content": "In this paper, we define a input prompt as \\(x = (x_{tmp}, x_{egs}, x_{que})\\), where each input prompt x is considered as a tuple of three components: \\(x_{tmp}\\) as the template such as fixed instructions, API docs, etc., \\(x_{egs}\\) as the examples, and \\(x_{que}\\) as the query. Typically, \\(x_{tmp}\\) and \\(x_{egs}\\) are relatively fixed and lengthy but essential for complex tasks. Let \\(f_{\\theta}(\\cdot)\\) denotes the neural network function of a LLM model, typically transformer (Vaswani et al., 2017), parameterized by \\(\\theta\\). The generated output by LLM can be represented as \\(f_{\\theta}(x)\\).\nWe then consider the following problem of prompt internalization. Given a training dataset \\(D_{train} = \\{(x_i, y_i)\\}_{i=1}^n\\) where n is the number of training samples, \\(x_i\\) is an input prompt defined above, and \\(y_i\\) is the corresponding groundtruth output. Our goal is to internalize the knowledge contained in templates and examples of each input prompt i.e. \\(\\{(x_{tmp}, x_{egs})\\}_{i=1}^n\\) into model parameters \\(\\theta\\) during fine-tuning, enabling efficient inference while maintaining high prediction performance through \\(\\{x_{que}\\}_{i=1}^n\\) only. Formally, the prompt internalization objective can be formulated as follows:\n\\[\\min_{\\overline{\\theta}} \\sum_{i=1}^n L(y_i, f_{\\overline{\\theta}}(x_{que}))\\]\nwhere \\(L(\\cdot)\\) denotes the loss function and \\(\\overline{\\theta}\\) denotes the updated weights with internalized prompt knowledge. For a new incoming prompt only containing the query, the updated LLM with \\(f_{\\overline{\\theta}}(\\cdot)\\) can internally recover the output without the assistance of instruction and examples."}, {"title": "4 Methodology", "content": "In this section, we introduce the detailed procedures of PromptIntern. We first discuss the template compression that is designed to compress the entire fixed template part inside an input prompt."}, {"title": "4.1 Template Compression", "content": "We first introduce template compression, which is designed to compress the common template information existed across training instances. The motivation of the template compression stems from the following aspects: 1) Redundancy. The instruction is repetitive across prompts for a given task, often containing unnecessary tokens that do not contribute to the language model's understanding, posing significant memory and computational burdens when the instruction is lengthy; and 2) Noise. Excessively long prompts may incorporate extraneous elements\u2014either irrelevant or misleading information\u2014that serves as noise and can adversely affect the model's generation.\nTo mitigate the issues stated above, we propose a template compression system, which can generally be expressed as:\n\\[x_{tmp}' = C(x_{tmp}, r_{tmp})\\]\nwhere C is a specific template compressor, \\(x'_{tmp}\\) is the compressed template, and \\(r_{tmp}\\) is the template compression rate as defined in (Jiang et al., 2023a), varying at differnt training interations. We then adopt a predetermined schedule \\(S_{tmp}(t)\\) to progressively reduce and internalize the prompt template information during the t-th training iteration. Specifically, for a total of T training iterations, we initially set \\(r_{tmp}\\) to 1 at \\(S_{tmp}(0)\\) and gradually decrease the value of \\(r_{tmp}\\) at \\(S_{tmp}(t)\\) to zero at end to achieve fully template internalization. Note that such compression system is also flexible, allowing it to halt at a desired non-zero compression rate. This flexibility allows to maintain a certain level of compressed template, serving as a trade-off to preserve inference accuracy in specific scenarios, as discussed in Section 5.3. In addition of the progressively decreasing template schedule, we also specify the template compressor C for better utilization. we categorize it into two types which exactly reflects the primary components of the template defined in the problem formulation: the instruction compressor and document compressor:\n1) Instruction Compressor targets the static elements within prompts, specifically focusing on the instructional content. Instructions in training data often consist of repeated directives, guidelines, or predefined tasks which are common across multiple training scenarios. The primary goal of the instruction compressor is to distill these instructions down to their essential components, eliminating verbosity and redundancy without compromising the clarity or intent of the instructions.\n2) Document Compressor is designed to handle the bulkier and more detailed portions of the prompts, such as API documentation or static demonstrations. These sections typically include extensive technical descriptions and examples that, while informative, often contain a significant amount of repetitive or non-essential information (Xu et al., 2023). The goal of the document compressor is to reduce the information unnecessary for understanding and applying the technical content, thereby streamlining the training process."}, {"title": "4.2 Example Absorption", "content": "Incorporating few-shot examples into fine-tuning not only improves information retrieval and memory recall (H\u00fcbotter et al., 2024) but also yields substantial benefits in handling a variety of tasks with minimal data input (Mosbach et al., 2023; Snell et al., 2017). However, directly adding lengthy few-shot examples to input prompts burdens the context window and increases inference latency. Motivated by this, we propose example absorption to benefit from the enhanced performance afforded by few-shot examples while prevent incurring significant additional overhead. Specifically, the example absorption mainly contains two stages: example retrieval and example removal.\n1) Example Retrieval is designed to identify and select the most related few-shot examples from the training dataset and incorporate them into each training instance. The underlying rationale is to choose examples that closely align with the training instance so as to accelerate model's internalization during training. We employ a straightforward approach that utilizes a relevance scoring function s(,) to assess the similarity between examples and the training instance. Specifically, we select the top k examples, varying at different training iterations, with the highest relevance scores to serve as our few-shot examples. For a training instance (xi, yi) with xi being the input prompt and yi being the corresponding groundtruth output, the selection process can be expressed as follows:\n\\[x_{egs}' = \\{(x_j, y_j) | j \\neq i, s(y_i, y_j) \\in top k scores\\}\\]\nNote that the scoring function is calculated based on common similarity metrics (Rubin et al., 2022; Chen et al., 2022; Dai et al., 2022). In our experiment, we use the BLEU as the scoring function.\n2) Example Removal aims to progressively internalize the prompt knowledge from few-shot examples into model parameters. To achieve this, we also adopt a predetermined schedule \\(S_{egs}(t)\\) to gradually decrease the number of demonstration examples in each prompt instance during the t-th iteration. Specifically, for a total of T training iterations, we initially set k examples at \\(S_{egs}(0)\\) and then gradually decrease the value of k at each \\(S_{egs}(t)\\) to zero at end in order to achieve fully example internalization."}, {"title": "4.3 PromptIntern Pipeline", "content": "In this subsection, we describe the detailed pipeline of PromptIntern. As demonstrated in Algorithm 1, PromptIntern consists of three stages: preprocess (line 1-7), progressive fine-tuning (line 8-12), and inference (line 13-14).\n1) Preprocess. We first preprocess the input prompts to prepare them for the progressive training stage. Specifically, we process the prompt template to different compression rates based on the schedule \\(S_{tmp}(t)\\) and retrieve examples for each training instance based on the schedule \\(S_{egs}(t)\\). For better illustration, we provide an example of a pre-processed prompt with respect to schedule in Appendix B.\n2) Progressive Fine-tuning. We then fine-tune the model parameters for internalizing. Given the training iteration t, we update the model parameters as follows:\n\\[\\theta_{t+1} = \\theta_t - \\frac{\\eta}{b} \\sum_{i=1}^b \\nabla_{\\theta_t} (f_{\\theta_t}(x_{tmp}'(t), x_{egs}'(t), x_{que}), y_i)\\]\nwhere \\(\\eta\\) is the learning rate, L is the cross-entropy loss function, b is the batch size, \\(B = \\{(x_i, Y_i)\\}_{i=1}^b\\) is the data batch, and y is the groundtruth label.\n3) Inference. After the progressive fine-tuning, we have trained the LLMs with updated model parameters \\(\\theta_t\\) to perform inference without adding instructions or any examples. Thus, we can predict the output simply with \\(f_{\\theta_T}(x_{que})\\)."}, {"title": "5 Experiment", "content": "In this section, we evaluate the performance of PromptIntern across various benchmarks on the NL2Code task, which is widely recognized for its utility in evaluating LLMs on both fine-tuning efficacy and cost-effectiveness in real-world applications (Zan et al., 2022). Our experiments primarily focus on two key perspectives: 1) Effectiveness: assessing the performance of textbf during inference phases; 2) Efficiency: quantifying the reduction in token usage and corresponding cost savings achievable through PromptIntern."}, {"title": "5.1 Settings", "content": "Datasets We apply three typical NL2Code datasets: MBPP (Austin et al., 2021) for NL to python code generalization, NL2F (Zhao et al., 2024) for NL to Excel spreadsheet formulas generation, NL2Bash (Lin et al., 2018) for NL to Bash Shell commands generation. Please refer to Appendix A.1 for the dataset details.\nEvaluation Metrics We use one-shot pass accuracy Pass@1 (Austin et al., 2021) for MBPP, Exact Match (E.M.) for NL2F, and BLEU score for NL2Bash. In addition, we calculate the input token usage and compression rate \\(\\tau\\) for each dataset.\nBaselines We consider two types of baselines with setups below:\n1) Prompt Compression approaches. We employ the latest advancements in prompt compression techniques. Specifically, we utilize Gist Tokens (Mu et al., 2024), GPT-4 Generation (Jiang et al., 2023b), Selective Context(Li et al., 2023), and LLMLingua series (Jiang et al., 2023a,b; Pan et al., 2024). Each prompt compression method is initially applied to compress the entire dataset to a predetermined compression rate. Then, the compressed dataset is utilized for both fine-tuning and inference evaluation.\n2) Direct Fine-tuning approaches. We use \u201cDirect\u201d as the counterpart to our progressive fine-tuning strategy. Specifically, we adopt several conventional direct fine-tuning configurations, including i) direct fine-tuning with complete template and examples (e.g. Template with 5-shots in Table 2), ii) direct fine-tuning with compressed template and reduced examples (e.g. Template x0.6 with 2-shots in Table 2), iii) direct fine-tuning with template only (Template only), and iv) direct fine-tuning without template and examples (No template).\nModels To demonstrate the broad applicability of PromptIntern, we utilize both closed-source and open-source LLMs with different parameter sizes for fine-tuning and inference processes.1) Closed-Source: We apply GPT-4-0613 (OpenAI, 2023), abbreviated as GPT-4, and GPT-3.5-turbo-01251, abbreviated as GPT-3.5. 2) Open-Source: We apply Mixtral-8x7B-v0.1 (Jiang et al., 2024), abbreviated as Mixtral-8x7B, Llama2-7B (Touvron et al., 2023), and Llama2-13B (Touvron et al., 2023).\nImplementation Details Please refer to Appendix A for the additional experiments settings and implementation details."}, {"title": "5.2 Main results", "content": "Prompt Compression Approaches Comparison\nTable 1 reports the overall result of PromptIntern with the prompt compression baselines inferenced on GPT-3.5 across all datasets. Here we establish the template compression rate Ttmp at 0.3 across all prompt compression approaches as well as PromptIntern to ensure a fair comparison. And Tall in the table represents the overall prompt's compression rate. We observe that while utilizing a comparable number of tokens for inference, our approach significantly outperforms all baselines, achieving improvements of 5.6% on MBPP, 11.0% on NL2F, and 7.7% on NL2Bash. The result demonstrates that PromptIntern generally offers the best balance of efficiency and effectiveness across varied tasks. Note that since the Gist Token(Mu et al., 2024) baseline is only applicable on open-source LLMs, we separately compare it with our approach which can be found at Appendix A.3.\nDirect Fine-tuning Approaches Comparison\nTable 2, 3, and 4 demonstrate the comparison"}, {"title": "5.3 Ablation Study", "content": "To effectively assess the impact of various components within PromptIntern, we introduce three variants of PromptIntern for ablation studies:\n\u2022 PromptIntern w/Ttmp=0.3, where we set the compression rate to 0.3 instead of 0 in template compression.\n\u2022 PromptIntern w/o Example Absorption, in which we omit the example absorption for retrieving and internalizing few-shot examples during fine-tuning.\n\u2022 PromptIntern w/o Template Compression, where template compression is excluded for both fine-tuning and inference prompt instances.\nThe overall results is shown in Table 5. When comparing PromptIntern with PromptIntern w/Ttmp = 0.3, we observe an average of 2.4% drop on performance but a 3.7x compression on tokens across all three datasets. This highlights the balance between compression rate and accuracy performance. When comparing our with our w/o Example Absorption, we observe a significant performance drop in the latter variant, despite both approaches utilizing the same number of tokens for inference. This outcome highlights the importance of example absorption in internalizing essential information during the fine-tuning process. When comparing PromptIntern with PromptIntern w/o Template Compression, we note that adding the template compression saves an average of 280 tokens across the datasets but experiences an average performance drop of 5%. This demonstrates that while totally internalizing the template into model parameters significantly reduces token usage, it requires a trade-off in terms of inference performance."}, {"title": "5.4 Analysis on Schedule Pattern", "content": "In Table 6, we test the effectiveness of different scheduling patterns during the progressive fine-tuning process, specifically focusing on how the decreasing speed curve influences the compression of the template and absorption of few-shot examples. The patterns tested include exponential, inverse-exponential, and linear decrease.\nFrom the data in the table, we observe that the linear decreasing schedule delivers the most consistent and highest performance across all three evaluation metrics, indicating superior performance in both parsing efficiency and language model understanding. Conversely, the inverse-exponential schedule shows the least effectiveness, with scores considerably lower in all metrics compared to the linear schedule. The exponential decrease performs moderately, but still falls short of the linear schedule, suggesting that a steady, predictable reduction is more beneficial than more aggressive decrease. This analysis suggests that for adopting a linearly decreasing schedule for progressive fine-tuning may lead to better performance in terms of accuracy compared to other scheduling patterns."}, {"title": "5.5 Analysis on Examples Retrieval Bank", "content": "Table 6 examines the impact of varying proportion of the training set used for constructing relevant examples in the examples retrieval bank. The options tested include using 25%, 50%, and 100% of the training set. The results clearly show a trend where increasing the percentage of the training set used in the examples retrieval bank correlates with improved performance. This suggests that larger examples retrieval bank provides a richer set of few-shots for the model to learn from, thereby enhancing its ability to generalize and perform accurately across tasks."}, {"title": "6 Conclusion", "content": "In this paper, we propose PromptIntern, a novel method for prompt internalization that internalizes repetitive prompt knowledge into LLMs parameters. We develop specific compression strategies for different components of the prompt, accompanied by a tailored progressive fine-tuning pipeline. Extensive experiments demonstrate that our method maintains comparable performance effectiveness while significantly accelerating inference speed and reducing token usage."}, {"title": "7 Limitations", "content": "While PromptIntern significantly reduces costs during the inference stage, the progressive fine-tuning approach incurs additional computational expenses during training. Specifically, our methodology demands substantial manual intervention for pre-processing and parameter adjustments throughout the fine-tuning process. Moreover, the current evaluation of our method is limited to a single task, specifically NL2Code. This restricts our understanding of its generalizability and effectiveness across a broader range of tasks. In future work, we plan to conduct extensive evaluations on more complex and varied tasks, such as long document summarization and question answering within specialized technical domains."}, {"title": "8 Ethics Statement", "content": "This research does not raise any ethical concerns. We obtained data only from publicly available sources where users have consented to the public sharing of their posts. We have conducted a thorough assessment to ensure that our research does not pose any potential harm."}, {"title": "A Additional Experiments", "content": "A.1 Dataset Details\nMBPP The MBPP dataset, as detailed by (Mosbach et al., 2023), consists of Python programming tasks, each accompanied by a description in natural language that has been expertly curated. The dataset is segmented into training and test sets, with 974 and 102 examples, respectively.\nNL2F The NL2F dataset, as detailed by (Zhao et al., 2024), consists of 70,799 pairs of NL queries and spreadsheet formulas and covers 21,670 tables. We follow the dataset instructions (Zhao et al., 2024) to randomly split data into a training set (75%), validation set (10%), and test set (15%).\nNL2Bash The NL2Bash dataset, as described by (Lin et al., 2018), comprises snippets of Bash code, each paired with a natural language description expertly curated. The dataset is divided into training and test sets, containing 8,090 and 606 examples, respectively."}, {"title": "A.2 Implementation Details", "content": "Fine-tuning Procedures For PromptIntern training, we adopt LoRA (Hu et al., 2021) with a rank of 32. For GPT-series and open-source model fine-tuning we train models for MBPP/NL2F/NL2Bash with 6/12/12 epochs, 16/128/128 batch size, 200/200/200 checkpoint interval, and 4096/4096/4096 context window length , respectively.\nModel Inference We provide the detailed parameters we adopted during fine-tuned LLM inference: temperature equal to 0, max tokens equal to 1028, top p equal to 0.95, presence penalty equal to 0, and frequency penalty equal to 0.\nBaseline Settings For prompt compression baselines comparison, we set the template compression ratio \\(T_{tmp}\\) = 0.3. For direct fine-tuning baselines, we apply LLMLingua-2 (Pan et al., 2024) as the default template compressor as it performs the best in Table 1.\nParameter Settings for PromptIntern\n1) Number of top-k for example absorption: We set the initial k as 5/10/10 across MBPP/NL2F/NL2Bash for the initial number of few-shot examples for example absorption. During progressive fine-tuning, we decrease k linearly in the order of 5-2-0/10-5-0/10-5-0 across MBPP/NL2F/NL2Bash.\n2) Number of \\(T_{tmp}\\) for template compression: For the prompt compression baseline experiments, we set the final template rate to 0.3, which is used in the last stage of fine-tuning as well as inference. For the other experiments and ablation studies, we set the final template rate to 0 to achieve fully internalization.\nCost Evaluation We compute the total costs based on the price shown in OpenAI Pricing2\nComputational Resource We conduct all experiments on AzureAI Machine Learning Studio with one A100x1-80G computational cluster ."}, {"title": "A.3 Comparison with Gist Tokens", "content": "We report the comparison result of PromptIntern with Gist Tokens (Mu et al., 2024) on Table 7. Gist Tokens showcases consistent performance, with notable results in NL2Bash where it achieves a BLEU score of 22.7, suggesting a moderate alignment with the dataset's requirements. In contrast, PromptIntern demonstrates superior performance across all metrics and datasets, particularly excelling in the NL2Bash dataset with a BLEU score of 31.6 and maintaining similar efficiency in token usage. The results demonstrate the our approach significantly outperforms the Gist token while conducting overall the same compression rate."}, {"title": "A.4 Experiments on Inference Speed", "content": "The experimental results presented in Table 8 illustrate the low latency characteristics of PromptIntern during inference across three datasets, MBPP,NL2F, and NL2Bash. Specifically, for the MBPP dataset, PromptIntern achieves an inference speed of 4.17 s/instance on the GPT-4 model, closely aligning with the 4.36 s/instance observed in the no template setup and far surpassing the more resource-intensive template with 5-shots configuration at 10.21 s/instance. In the NL2F dataset, PromptIntern similarly demonstrates its efficiency with an inference speed of 2.15 s/instance for GPT-4, which is nearly equivalent to the 2.12 s/instance observed without any template and significantly outperforms the elaborate template with 10-shots configuration, which achieves 12.47 s/instance. The experimental results outlined in the table also highlight the efficiency of PromptIntern in the NL2Bash dataset. Notably, for GPT-4 under the NL2Bash benchmark, PromptIntern maintains a competitive inference speed of 1.18 s/instance, matching the performance seen in the no template scenario and markedly better than the template with 5-shots setup, which records a slower speed of 4.46 s/instance. The result across three NL2Code benchmarks highlight PromptIntern 's capability to maintain competitive inference speeds while minimizing latency efficiently."}, {"title": "B Example Demonstration", "content": "We demonstrate an example on how we schedule and pre-process an input prompt through both template compression and example absorption in Figure 3.\nDuring the initial phase of fine-tuning, the input prompt will fully incorporate both the template and 10-shot examples for the NL2F dataset. After a specified number of training iterations, the template will undergo a compression at a rate of 0.3, and the number of examples will be reduced to five. This modified prompt is then used for the intermediate stage of fine-tuning. In the final phase, the template and few-shot examples are completely removed from the training prompt. It is important to note that the query remains unchanged throughout the entire progressive fine-tuning process. The prompt used in the last stage, which consists solely of the query, will also serve as the input for subsequent model inference. This method enables the fine-tuned language model to perform zero-shot inference without the need for a instruction or document template."}, {"title": "C Prompts", "content": "C.1 GPT-4 Generation (Baseline) instruction\nC.2 Prompts of PromptIntern on NL2Code\nPlease refer to Figures 5,6,7 for detailed prompts."}]}