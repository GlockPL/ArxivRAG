{"title": "Key Algorithms for Keyphrase Generation:\nInstruction-Based LLMs for Russian Scientific\nKeyphrases", "authors": ["Anna Glazkova", "Dmitry Morozov", "Timur Garipov"], "abstract": "Keyphrase selection is a challenging task in natural language\nprocessing that has a wide range of applications. Adapting existing su-\npervised and unsupervised solutions for the Russian language faces sev-\neral limitations due to the rich morphology of Russian and the limited\nnumber of training datasets available. Recent studies conducted on Eng-\nlish texts show that large language models (LLMs) successfully address\nthe task of generating keyphrases. LLMs allow achieving impressive res-\nlts without task-specific fine-tuning, using text prompts instead. In this\nwork, we access the performance of prompt-based methods for generating\nkeyphrases for Russian scientific abstracts. First, we compare the per-\nformance of zero-shot and few-shot prompt-based methods, fine-tuned\nmodels, and unsupervised methods. Then we assess strategies for select-\ning keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths\nand weaknesses of the models through expert assessment. Our results\nsuggest that prompt-based methods can outperform common baselines\neven using simple text prompts.", "sections": [{"title": "1 Introduction", "content": "A keyphrase or a keyword is a brief and summative content that captures the\nmain idea of a longer text. In this work, the term \"keyphrase\" is used to refer to\nboth keyphrases and keywords, and it means that a keyphrase can consist of one\nor more words. Effective keyphrases can enhance comprehension, organization,\nand retrieval of document content. They are widely used in digital libraries and\nsearchable document collections for systematizing texts. In particular, a list of\nkeyphrases is an important component of a research paper as they help summar-\nize the main ideas discussed in a text, thus simplifying the process of information\nretrieval and selecting relevant papers. Current studies divide keyphrases into"}, {"title": "2 Related Work", "content": "Much of the current literature on keyphrase selection for Russian texts pays\nparticular attention to unsupervised methods. Various studies [17,21,28,29,36]\nhave assessed the efficacy of statistical methods, such as TFIDF, YAKE! [8],\nRAKE [35], and graph-based methods, such as TextRank [27] and TopicRank [7].\nSome studies [14,15,17,30,39] used the approaches based on machine learning,\nsuch as KEA [46] and pre-trained language models."}, {"title": "2.1 Keyphrase Selection for Russian Texts", "content": "Much of the current literature on keyphrase selection for Russian texts pays\nparticular attention to unsupervised methods. Various studies [17,21,28,29,36]\nhave assessed the efficacy of statistical methods, such as TFIDF, YAKE! [8],\nRAKE [35], and graph-based methods, such as TextRank [27] and TopicRank [7].\nSome studies [14,15,17,30,39] used the approaches based on machine learning,\nsuch as KEA [46] and pre-trained language models."}, {"title": "2.2 Prompt-based Methods for Keyphrase Generation", "content": "Recent advances in LLMs that can communicate with humans and generate\ncoherent and meaningful responses [1,2,4,19,44] are beneficial for developing ef-\nfective solutions to various NLP tasks, including keyphrase selection. To date,\nseveral studies have investigated zero-shot and few-shot prompt-based methods\nfor keyphrase generation. So far, most research on prompt-based keyphrase gen-\neration has focused on evaluating datasets in English.\nAttempts have been made to evaluate the ability of prompt-based LLMs\nto generate keyphrases. Song et al. [40,41] verified a zero-shot performance of\nChatGPT on four keyphrase extraction datasets. In [26], the performance of\nChatGPT using an instructional prompt was compared with the results of sev-\neral neural models for keyphrase selection. ChatGPT outperformed other models\non all benchmarks, notably in handling long documents and non-scientific texts.\nThe paper [9] explored keyphrase extraction using Llama2-7B, GPT-3.5, and\nFalcon-7B for two English scientific datasets and emphasized the role of prompt\nengineering in LLMs for a better keyphrase selection. The paper [22] examined\nthe performance of Galactica, a model pre-trained on open-access scientific text\nand data [43], for generating keyphrases. The authors of [20] compared author\nkeyphrases from papers on the digital divide with those generated using BERT\nand ChatGPT. The correlation between author keyphrases and ChatGPT was\nhigher than that between author keyphrases and BERT. Er et al. [12] studied the\nperformance of unsupervised methods such as TF-IDF, YAKE!, and TextRank\nagainst several prompt-based methods based on GPT3.5, GPT4, and Gemini\nand fine-tuned models, such as T5 [34] and BART [23]. This study was per-\nformed on a Turkish corpus of customer reviews. The most accurate keyphrases\nwere generated by GPT4 in a few-shot setting.\nOverall, the review of related work reveals that prompt-based methods demon-\nstrate high results for keyphrase generation. The scholars reported that LLMs\ngenerate keyphrases more accurately in a few-shot setting. Most current stud-\nies have been conducted on English-language datasets. Although LLMs have\nshown impressive multilingual performance, their ability to generate keyphrases\nfor Russian texts has not been investigated. Current studies use various metrics\nto evaluate exact matches and semantic similarity between selected keyphrases\nand the gold standard keyphrases. Researchers [3,17] emphasize that different"}, {"title": "3 Methods", "content": "The study used the Math&CS dataset\u00b3 presented in [29], which consists of 8348\nabstracts of research papers in the fields of mathematics and computer science\nsampled from the Cyberleninka online library with keyphrases tagged by the\nauthors of the papers. Each dataset example includes an abstract and its cor-\nresponding list of keyphrases. Math&CS contains keyphrases that are present\nas well as those that are absent in the abstract. The training set contains 5844\nexamples. 554 examples from the training set contain only present keyphrases,\n655 examples include only absent keyphrases, other 4635 examples involve mixed\nkeyphrases.\nThe dataset statistics is presented in Table 1. The average numbers of tokens\nand sentences are calculated using the NLTK package [6]. The bottom row\npresents the overall percentage of absent keyphrases in the dataset."}, {"title": "3.1 Dataset", "content": "The study used the Math&CS dataset\u00b3 presented in [29], which consists of 8348\nabstracts of research papers in the fields of mathematics and computer science\nsampled from the Cyberleninka online library with keyphrases tagged by the\nauthors of the papers. Each dataset example includes an abstract and its cor-\nresponding list of keyphrases. Math&CS contains keyphrases that are present\nas well as those that are absent in the abstract. The training set contains 5844\nexamples. 554 examples from the training set contain only present keyphrases,\n655 examples include only absent keyphrases, other 4635 examples involve mixed\nkeyphrases.\nThe dataset statistics is presented in Table 1. The average numbers of tokens\nand sentences are calculated using the NLTK package [6]. The bottom row\npresents the overall percentage of absent keyphrases in the dataset."}, {"title": "3.2 Models", "content": "For prompt-based learning, we used three open-source instruction-following LLMs.\nTo obtain more reliable results, we choose one English-oriented LLM and two\nmodels specifically adapted for the Russian language using different approaches.\nSaiga/Mistral 7B (Saiga)4 [16], a Russian Mistral-based chatbot adapted by\ntraining LoRA adapters. This model was tuned on a dataset of ChatGPT-\ngenerated chats in Russian.\nVikhr-7B-instruct_0.4 (Vikhr)5 [31]. Contrary to Saiga, Vikhr uses adapted\ntokenizer vocabulary as well as continued pre-training and instruction tuning\nof all weights instead of LoRA adapters.\nmT57 [47], a multilingual text-to-text transformer pre-trained on a Common\nCrawl-based dataset covering 101 languages. The architecture and training\nprocedure are similar to T5 [34].\nmBART8 [42], a machine translation sequence-to-sequence model that uses\nthe same baseline architecture as that of BART [23]. mBART was trained on\nmore than 50 languages with a combination of span masking and sentence\nshuffling."}, {"title": "4 Experiments and Results", "content": "We used the following metrics for evaluation: the full-match F1-score, ROUGE-1\n[24], and BERTScore [48]. The full match F1-score assesses the number of ex-\nact matches between the list of keyphrases tagged by the author of the paper\nand the keyphrases selected by the model. To calculate Fl-score, the keyphrases\nwere lemmatized to reduce the number of mismatches. Next, the True Positive\nvalue was calculated as the size of the intersection between the sets of produced\nand author's keyphrases. The False Positive value represented the difference\nbetween the set of generated keyphrases and the author's ones. The False Neg-\native value was defined as the difference between the sets of author's and gener-\nated keyphrases. ROUGE-1 accesses the number of matching unigrams between\nthe selected keyphrases and the author's list of keyphrases. ROUGE-1 scores\nwere calculated without preliminary lemmatization of keyphrases. BERTScore\nassesses the cosine similarity between the contextual embeddings of the tokens\nfor both lists of keyphrases. The multilingual BERT (mBERT) [11] was used as\na basic language model. For ROUGE-1 and BERTScore, the lists of keyphrases\nwere represented as comma-separated strings.\nTable 3 presents the results of the model comparison in terms of the selected\nmetrics. The highest result for each metric in shown in bold, the second best\nresult is underlined, and the third best result is double underlined. As expected,\nfew-shot learning increased the performance of the prompt-based methods. Nev-\nertheless, our results demonstrated that even the use of a zero-shot approach with\na fairly simple prompt showed the performance comparable to other models. The\nbest and second best scores in terms all metrics were obtained using Saiga&few-\nshot (random keyphrases) and Saiga&few-shot (present keyphrases). The third\nbest result for different metrics was shown by mBART and Saiga&few-shot (ab-\nsent keyphrases). Among both fine-tuned models and unsupervised methods, the\nhighest scores were achieved by mBART. mT5, YAKE!, and RuTermExtract"}, {"title": "4.1 Comparing Models in Terms of Evaluation Metrics", "content": "We used the following metrics for evaluation: the full-match F1-score, ROUGE-1\n[24], and BERTScore [48]. The full match F1-score assesses the number of ex-\nact matches between the list of keyphrases tagged by the author of the paper\nand the keyphrases selected by the model. To calculate Fl-score, the keyphrases\nwere lemmatized to reduce the number of mismatches. Next, the True Positive\nvalue was calculated as the size of the intersection between the sets of produced\nand author's keyphrases. The False Positive value represented the difference\nbetween the set of generated keyphrases and the author's ones. The False Neg-\native value was defined as the difference between the sets of author's and gener-\nated keyphrases. ROUGE-1 accesses the number of matching unigrams between\nthe selected keyphrases and the author's list of keyphrases. ROUGE-1 scores\nwere calculated without preliminary lemmatization of keyphrases. BERTScore\nassesses the cosine similarity between the contextual embeddings of the tokens\nfor both lists of keyphrases. The multilingual BERT (mBERT) [11] was used as\na basic language model. For ROUGE-1 and BERTScore, the lists of keyphrases\nwere represented as comma-separated strings.\nTable 3 presents the results of the model comparison in terms of the selected\nmetrics. The highest result for each metric in shown in bold, the second best\nresult is underlined, and the third best result is double underlined. As expected,\nfew-shot learning increased the performance of the prompt-based methods. Nev-\nertheless, our results demonstrated that even the use of a zero-shot approach with\na fairly simple prompt showed the performance comparable to other models. The\nbest and second best scores in terms all metrics were obtained using Saiga&few-\nshot (random keyphrases) and Saiga&few-shot (present keyphrases). The third\nbest result for different metrics was shown by mBART and Saiga&few-shot (ab-\nsent keyphrases). Among both fine-tuned models and unsupervised methods, the\nhighest scores were achieved by mBART. mT5, YAKE!, and RuTermExtract"}, {"title": "4.2 Human Evaluation", "content": "In addition to calculating metrics, human evaluation was used to assess the qual-\nity of keyphrases. To perform human evaluation, we selected Saiga&few-shot\n(random keyphrases) (hereinafter - Saiga), mBART, RuTermExtract as they\ndemonstrated the highest performance among prompt-based methods, fine-tuned\nmodels, and unsupervised methods respectively. One hundred random texts from\nthe test set were randomly selected. For each text, the outputs of Saiga, mBART,\nand RuTermExtract were collected (300 outputs in total). Then, three experts\nwith a background of writing academic papers in computer science marked each\noutput according to the following criteria: (a) Presence of grammar and\nspelling mistakes. True if the list of keyphrases contains any grammar or\nspelling mistakes; (b) Redundancy. True if keyphrases are redundant, for ex-\nample, contain a lot of cognate words or synonyms; (c) Insufficiency. True if"}, {"title": "4.3 Discussion and Limitations", "content": "As in previous research carried out on text corpora in other languages [12,26,41],\nprompt-based LLMs have shown promising results in keyphrase generation for\nRussian. These models offer several advantages, including the capacity for ef-\nfective few-shot learning, as well as the lemmatization of generated keyphrases.\nWe have tested open-source instruction-following LLMs on scientific texts and\ndemonstrated that prompt-based methods have the ability to generate quite in-\nformative and comprehensive keyphrases according to human evaluation results.\nAdditionally, our findings suggest that all the considered prompt-based LLMs\nperform better using the examples with present keyphrases or random examples\nfrom the dataset than using the examples with absent keyphrases.\nThe current study is limited by the dataset features. First, we believe that\nprompt-based LLMs have a greater potential for keyphrase generation in other\ndomains and text genres, namely, for news. However, domain efficiency and trans-\nferability needs further research. The effectiveness of keyphrase generation using\nprompt-based LLMs on long scientific texts, such as full paper texts, also requires\nfurther investigation. Second, since the examples for fine-tuning and creating"}, {"title": "5 Conclusion", "content": "This study explored the ability of prompt-based LLMs to generate keyphrases\nfor Russian scientific abstracts. We compared prompt-based methods with fine-\ntuned models and unsupervised methods and found that prompt-based LLMs\nachieve superior performance in comparison with baselines, even when employ-\ning basic text prompts. We employed different strategies for selecting keyphrase\nexamples in a few-shot setting and observed that the use of the examples con-\ntaining only absent keyphrases leads to lower performance. Finally, we provided\nthe results of human evaluation across three models and discussed their strengths\nand limitations."}]}