{"title": "Parallel Sampling via Counting", "authors": ["Nima Anari", "Ruiquan Gao", "Aviad Rubinstein"], "abstract": "We show how to use parallelization to speed up sampling from an arbitrary distribution u on\na product space [q]^n, given oracle access to counting queries: P_{X~\\mu}[X_S = \\sigma_S] for any S \\subseteq [n] and\n\\sigma_S \\in [q]^S. Our algorithm takes O(n^{2/3} \\cdot polylog(n, q)) parallel time, to the best of our knowledge,\nthe first sublinear in n runtime for arbitrary distributions. Our results have implications for\nsampling in autoregressive models. Our algorithm directly works with an equivalent oracle\nthat answers conditional marginal queries P_{X~\\mu}[X_i = \\sigma_i | X_S = \\sigma_S], whose role is played by a\ntrained neural network in autoregressive models. This suggests a roughly n^{1/3}-factor speedup\nis possible for sampling in any-order autoregressive models. We complement our positive\nresult by showing a lower bound of \\Omega(n^{1/3}) for the runtime of any parallel sampling algorithm\nmaking at most poly(n) queries to the counting oracle, even for q = 2.", "sections": [{"title": "Introduction", "content": "The seminal work of Jerrum, Valiant, and Vazirani [JVV86] established an algorithmic equivalence\nbetween the tasks of approximate sampling and approximate counting, for the ubiquitous class of\nself-reducible problems. This key equivalence is at the heart of the Monte Carlo Markov Chain\napproach to approximate counting [\u0160VV09], which has enabled breakthroughs like approximating\nthe permanent [JSV04] or the volume of convex sets [DFK91]. In this paper, we focus on one side\nof this equivalence, sampling via counting.\nSelf-reducibility, in its most widely applied form, concerns distributions \\mu on a product space\n[q]^n and their pinnings: conditional distributions obtained by selecting a subset S \\subseteq [n] and\npartial configuration \\sigma_S \\in [q]^S and conditioning X ~ \\mu to have coordinates in S pinned to \\sigma_S:\nX_S = \\sigma_S. In this setting, sampling means producing a random X distributed according to a\nspecified pinning of \\mu. Counting, on the other hand, refers to computing the partition functions\nof pinnings: P_{X~\\mu}[X_S = \\sigma_S].1 Sampling via counting is in fact very easy to describe in this\nsetting. Assuming access to a counting oracle, we can produce samples from \\mu via the following\nautoregressive generation process:\nInitialize \\sigma \\leftarrow 0\nfor i = 1,...,n do\nfor x \\in [q] do\n[ p_x \\leftarrow P_{X~\\mu}[X_i = x | X_{[i-1]} = \\sigma_{[i-1]} ]\n\\sigma_i \\leftarrow random sample in [q] distributed ~ (p_1, ..., p_q)\nreturn \\sigma\nNote that we only need to use the counting oracle to compute the computationally equivalent\nconditional marginals:\nP_{X~\\mu}[X_i = x | X_{[i-1]} = \\sigma_{[i-1]}] = \\frac{P_{X~\\mu}[X_{[i-1]} = \\sigma_{[i-1]}, X_i = x]}{P_{X~\\mu}[X_{[i-1]} = \\sigma_{[i-1]}]}\nThis process, despite its simplicity, is how the widely successful autoregressive models generate\ntheir output [see, e.g., LM11; VKK16; Vas+17; Dev+18; Yan+19; Bro+20]. State-of-the-art large\nlanguage models, or even some competitive vision models, train a neural network to answer\nconditional marginal queries and then use the aforementioned process to generate samples. In the\ncontext of language models, [q] represents a token space, and n is the length of generated text or\ncontext length, while in pixel-space vision models, [q] is possible values for a pixel, and n is the\nnumber of pixels in the image.\nOne downside of this simple sampling process is that it is extremely sequential. One has to generate\ncoordinates 1, . . ., i \u2212 1, to know which conditional marginals need to be queried in the ith iteration.\nSo, it is natural to ask if there is a more parallelizable sampling process. More precisely, suppose that\nan oracle2 can answer conditional marginal queries of the form P[X_i = x | X_S = \\sigma_S], and we can\ninteract with this oracle in rounds, each time asking polynomially many queries simultaneously.\nWe are interested in finding the adaptive complexity of sampling:"}, {"title": "Techniques", "content": "Our algorithm works by modifying the autoregressive sampling process in two ways. First, we\nchoose the order of coordinates according to a uniformly random permutation. Second, to break\nsequentiality, we generate \u201cguesses\u201d of future coordinates, by computing the marginal of each X_i\nconditioned on the current pinning, in parallel, and sampling from these marginal distributions\nindependently for each i. While these independent samples clearly ignore dependencies between\ncoordinates, we can in a second stage verify in parallel that each X_i would have been the sample\nproduced if we had continued sequentially. We advance up to the point where our guesses\nsuccessfully pass verification and then iterate.\nThe key idea behind our analysis is that as we pin more and more random coordinates, the\ndependencies between the remaining coordinates weaken in an average sense. This intuitive idea\nis formalized by the so-called pinning lemmas [RT12; Mon08] which we use in our analysis, see\nSection 2.1. Weakened dependencies intuitively mean that our guesses are not likely to deviate\nfrom what sequential sampling would have produced. This is formally proved in Section 3.\nFinally, a tool we use from existing literature on parallel sampling is a universal coupler [LY22].\nThis is used to ensure consistency between the guessing and verification stages. In both of these\nphases, for each X_i, we would like to sample from a marginal distribution. Universal couplers\nensure that when the marginal distributions are \u201cclose\u201d, the samples are likely to be exactly equal.\nWe extend the analysis of universal coupling to multiple distributions, as needed by our work, see\nSection 2.2.\nTo prove our lower bound, we construct a challenge distribution that is a uniform distribution on an\naffine subspace of F^n = {0, 1}^n, and we show that it is hard to even output anything in its support in\nfewer than \\Omega(n^{1/3}) rounds. We group the coordinates in [n] into roughly \\Omega(n^{1/3}) randomly chosen\nbuckets and put varying numbers of affine constraints on each bucket. We prove that with high\nprobability the buckets can only be discovered one at a time, from the most constrained bucket\nto the least. This is because queries pinning too many coordinates will not be useful at all, as\nthey will violate the constraints of the most constrained bucket. On the other hand, if the number\nof pinnings is just right for the most-constrained undiscovered bucket, no information is gained\nabout less-constrained buckets; with high probability all of the marginals in the less-constrained\nbuckets remain uniform."}, {"title": "Organization", "content": "In Section 2 we discuss and further develop two of the main tools we use for parallelization:\npinning lemmas and universal coupling. In Section 3, we describe our parallel sampling algorithm\nand prove our main result Theorem 2. In Section 4, we provide an application of Theorem 2 to the\nproblem of sampling planar perfect matchings. In Section 5, we prove a lower bound against all\nalgorithms, i.e., Theorem 4. In Section 6, we prove that our analysis of the algorithm presented in\nSection 3 is tight, and n^{2/3} cannot be improved for this particular algorithm."}, {"title": "Preliminaries", "content": "We use [n] to denote the set {1,2,\u2026\u2026, n}. For any vector x and set S, we use x_S to denote x restricted\nto S. We use \\pm x to indicate the interval [-x, x]. We use S_n to denote the set of permutations on\nn elements. For any two sets A, B, we use A \\times B to denote the Cartesian product of A and B, i.e.,\nA \\times B = {(a, b) | a \\in A,b \\in B}.\nFor a distribution \\mu, we use x ~ \\mu to denote that x is sampled from \\mu. Similarly, for a set S, we use\nx ~ S to indicate that x is sampled uniformly at random from S."}, {"title": "Pinning Lemmas", "content": "The pinning lemma formalizes an intuition that randomly pinning coordinates of an arbitrary\ndistribution should in an average sense lower the correlation between remaining coordinates.\nIntuitively, this should make parallel sampling easier; for example, if all coordinates become fully\nindependent, one can in parallel sample from the marginals. We do not directly use the classical\nstatement of the pinning lemma, mentioned below for comparison, but rather prove a statement\nin the same vein and using the same proof strategy.\nLemma 5 (pinning lemma [RT12; Mon08]). Let X_1, X_2, ..., X_n be random variables, each supported on\n{0,1}. For any l \\in [n], there exists a set S such that |S| \\leq l and\nE_{X_S}[E_{U,\\sigma ~ U_{[l]}}[Cov(X_u, X_v | X_S)^2]] \\leq \\frac{O(1)}{l}.\nWe now define and state well-known statements about entropy, building up to state and prove our\nnew variant of the pinning lemma.\nDefinition 6 (entropy). Let X, Y be random variables on [q]. The entropy of random variable X is\ndefined to be\nH(X) = \\sum_{i\\in[q]} P[X = i] \\cdot log P[X = i].\nThe conditional entropy of X conditioned on Y is defined to be\nH(X | Y) = \\sum_{i\\in[q]} H(X | Y = i) \\cdot P[Y = i].\nDefinition 7 (KL divergence). For a pair of distributions \\nu, \\mu, we let\nD_{KL}(\\nu || \\mu) = E_{x~\\nu} log \\frac{\\nu(x)}{\\mu(x)}\nAbusing notation, we extend the definition to random variables. If X ~ \\nu,Y ~ \\mu, we use D_{KL}(X ||\nY) to denote D_{KL}(\\nu || \\mu)."}, {"title": "Universal Coupling", "content": "For any integer q > 0, let A_q be the probability simplex on [q], i.e., A_q = {\\mu \\in [0,1]^q | \\sum_{i=1}^q \\mu(i) = 1}.\nIn our main algorithm, we use a \u201cuniversal coupler\u201d as a subroutine. Informally, this is an algorithm\nthat maps a distribution \u0438 \u2208 A_q and a random source r to a sample from \u0438, with the property\nthat the output is unlikely to change if u is perturbed slightly (while keeping r fixed). Such an\nalgorithm naturally induces a coupling between any two distributions \\mu, \\mu'. In any coupling, there\nmust be at least d_{TV}(\\mu, \\mu') chance that the samples for \\mu, \\mu' are unequal; and this lower bound can\nbe achieved if we design a tailor-made coupling knowing both \\mu and \\mu'. Surprisingly, one can\nachieve the same bound within constant factors without knowing both distributions in advance.\nThe existence of these robust universal couplers appears to have been discovered and rediscovered\nmany times. The earliest works that we are aware of are the MinHash algorithm of Broder [Bro97]\nfor uniform distributions, and a rejection-sampling-based strategy of Kleinberg and Tardos [KT02]\nand Holenstein [Hol07] for general distributions. See [Bav+20] for more on the history and\noptimality of these strategies. Recently, in the context of parallel sampling algorithms, the work of\nLiu and Yin [LY22] has rediscovered the same rejection-sampling-based algorithm; we borrowed\nthe terminology of \u201cuniversal coupling\" from the latter work.\nA universal coupler is defined as follows:\nDefinition 11 (universal coupling, [LY22]). A deterministic function f : A_q \\times [0,1] \\rightarrow [q] is a\nuniversal coupling on [q] if, when r\\in [0,1] is chosen uniformly at random, for any distribution\n\\mu \\in A_q and x \\in [q],\nPr_{r~[0,1]}[f(\\mu, r) = x] = \\mu(x).\nNote that instead of r ~ [0, 1], one can use other sources of randomness with infinite entropy, such\nas an infinite sequence of random bits, etc. Since it is easy to translate between these sources,\nwe pick the notationally most convenient form of random source when describing each universal\ncoupler.\nThe main characteristic we would like for universal couplers is that on \u201cclose\u201d distributions \\mu, \\mu',\nthe chance that f (\\mu, r) \\neq f(\\mu', r) is small. A lower bound on this chance is P_r[f(\\mu, r) \\neq f(\\mu',r)] >\nd_{TV}(\\mu, \\mu'). Surprisingly, this can be matched up to a factor of 2; in fact the optimal f has been\nshown [KT02; Hol07; Bav+20; LY22] to satisfy\nP_r[f(\\mu, r) \\neq f(\\mu',r)] \\leq \\frac{2 d_{TV}(\\mu, \\mu')}{1 + d_{TV}(\\mu, \\mu')} \\leq 2 d_{TV} (\\mu, \\mu').\nIn our algorithm, we need a slightly stronger guarantee that holds for not just two, but an arbitrary\nnumber of distributions.\nDefinition 12 (robust universal coupler). We call a universal coupler f robust if for any number of\ndistributions \\mu_1, ..., \\mu_m it satisfies\nP_r [\\exists i, j\\in [m] : f(\\mu_i, r) \\neq f(\\mu_j, r)] \\leq \\frac{\\sum_{x\\in[q]}(max_{i\\in[m]} \\mu_i(x) - min_{i\\in[m]} \\mu_i(x))}{\\sum_{x\\in[q]} max_{i\\in[m]} \\mu_i(x)}\nNote that for m = 2, the numerator on the r.h.s. becomes 2 d_{TV}(\\mu_1, \\mu_2), and the denominator\nbecomes 1 + d_{TV}(\\mu_1, \\mu_2). Thus, this matches the same optimal bound derived by prior works.\nWe show that the rejection-sampling-based algorithm used in prior works, which we call the\nMINCOUPLER, Satisfies this more general robustness guarantee. Additionally, we show that another\nwidely used algorithm called the \u201cGumbel trick\u201d also satisfies the same robustness guarantee."}, {"title": "Sublinear Parallel Sampling via Counting Oracles", "content": "In this section, we show our main Theorem 2 that we can (approximately) sample from a distri-\nbution after a sublinear number of rounds (in terms of the number of variables) of querying a\npolynomial number of the distribution's counting oracles."}, {"title": "Algorithm", "content": "We present our algorithm in Algorithm 3. Let u_1,..., u_n be the random seeds of the algorithm.\nThe algorithm also shuffles the coordinates with a uniformly random permutation that we ignore\nin this description for simplicity.\nAs a subroutine, we use a universal coupler UNIVERSALCOUPLER that is robust, see Definition 12.\nFor instance, this can be either the MINCOUPLER (Algorithm 1), or the GUMBEL TRICK (Algorithm 2).\nWe let \\tilde{x} \\in [q]^n denote a sample from the target distribution generated using the naive sequential\nalgorithm that iteratively samples the i-th entry conditioning on all previous entries:\nX_i \\leftarrow UNIVERSALCOUPLER(X_i | {X_j = \\tilde{x_j}}_{ j\\in [i-1]}, u_i) .\nThe goal of the algorithm is to sample faster than one coordinate per iteration. The algorithm\nmaintains an index a where the a-th and earlier entries are all correctly sampled. At the t-th\niteration, the algorithm attempts to resample all entries after a by conditioning on the a-th and\nearlier entries:\nx^t \\leftarrow UNIVERSALCOUPLER (X_i | {X_j = y^t_j}_{ j\\in [a]}, {X_j = \\tilde{x_j}}_{ j\\in [a'-1]}, u_i).\nThen, the algorithm uses x^t to find the earliest entry a' where sampling conditioning on x^t differs from sampling conditioning on \\tilde{x} and immediately fixes the entry a': then a' is a new\nindex where the a'-th and earlier entries are all correctly sampled."}, {"title": "Correctness", "content": "We consider a function \\tilde{x} : S_n \\times [0, 1]^n \\rightarrow [q]^n defined iteratively as follows:\n\\tilde{x_i}(\\sigma, u) = UNIVERSALCOUPLER(X_{\\sigma(i)} | {X_{\\sigma(j)} = \\tilde{x_j}(\\sigma, u)}_{j\\in[i-1]}, u_i) .\nFor each i \\in [n], because UNIVERSALCOUPLER is a universal coupler, \\tilde{x_i}(\\sigma, u) follows the marginal\ndistribution of X_{\\sigma(i)} conditioning on X_{\\sigma(1)} = \\tilde{x_1}(\\sigma, u), X_{\\sigma(2)} = \\tilde{x_2}(\\sigma, u),...,X_{\\sigma(i-1)} = \\tilde{x_{i-1}}(\\sigma, u)\n(considering only the randomness of u_i). Therefore, this function can serve as an objective output\nof the algorithm when we have fixed the randomness \\sigma and u.\nLemma 20. If Algorithm 3 always outputs \\tilde{x^t_{\\sigma(i)}} = \\tilde{x_i}(\\sigma, u), it samples perfectly from the distribution of X.\nLet a^t be the value of a at the end of round t (if the algorithm does not terminate with x^t = y^t\nbefore updating the value of a in round t). For simplicity, we suppose a^0 = 0. Next, we show that\nthe vector x^t produced by the algorithm in each round matches this objective vector in the first a^t\nentries.\nLemma 21. For any \\sigma, u, after each round t of Algorithm 3,\n\\forall i\\in [a^t], x^t_{\\sigma(i)} = \\tilde{x_i}(\\sigma, u).\nProof. According to the definition of a^t, we have \\forall i \\in [a^t - 1], x^t_{\\sigma(i)} = y^t_{\\sigma(i)}. Therefore, according to\nthe definition of x^t, we have for any i \\in [a^t],\nx^t_{\\sigma(i)} = UNIVERSALCOUPLER\\left(X_{\\sigma(i)} | {X_{\\sigma(j)} = y^t_{\\sigma(j)}}_{j\\in[i-1]}, u_i\\right).\nNote that this recursion matches the recursion Eq. (2) used in the definition of \\tilde{x_i}(\\sigma, u) for any\ni \\in [a^t]. Therefore, \\forall i \\in [a^t], x^t_{\\sigma(i)} = \\tilde{x_i}(\\sigma, u).\nTo show that Algorithm 3 is making progress every iteration, we prove a^t is (strictly) monotone in\nterms of t.\nLemma 22. a^t is strictly increasing with t.\nProof. According to the definition of y^t, for any i \\in [a^{t-1}], y^t_{\\sigma(i)} = x^{t-1}_{\\sigma(i)} = \\tilde{x_i}(\\sigma, u). Therefore,\naccording to the definition of x^t, for any i \\in [a^{t-1} + 1],\nx^t_{\\sigma(i)} = UNIVERSALCOUPLER\\left(X_{\\sigma(i)} | {X_{\\sigma(j)} = x^{t-1}_{\\sigma(j)}}_{j\\in[i-1]}\\right)\n= UNIVERSALCOUPLER \\left(X_{\\sigma(i)} | {X_{\\sigma(j)} = \\tilde{x_j}(\\sigma, u)}_{j\\in[i-1]}\\right)\n= \\tilde{x_i}(\\sigma, u) = y^t_{\\sigma(i)}.\nBy the definition of a^t, if x^t \\neq y^t, we get a^t > a^{t-1} + 1.\nNote that the algorithm terminates when either of the following two conditions is satisfied in some\nround t: a^t = n or x^t = y^t. Due to Lemma 22, the algorithm always terminates. If it terminates\nbecause of the first condition a^t = n, due to Lemma 21, the output of the algorithm matches\nthe objective in Lemma 20. Otherwise, because of the definition of x^t and the fact that x^t = y^t,\nthe output satisfies Eq. (3), which matches the recursion Eq. (2) used in the definition of \\tilde{x}(\\sigma, u),\nand thus matches the objective in Lemma 20. As a conclusion, we obtain the correctness of our\nalgorithm.\nLemma 23. Algorithm 3 returns a sample x ~ \\mu."}, {"title": "Round Complexity", "content": "We establish our sublinear round complexity via two steps. First, we ignore the randomness of\n\\sigma and u, and establish a worst-case round complexity, which can be linear with some choices\nof \\sigma, u. Second, we show that the expectation of this round complexity is actually \\tilde{O}(n^{2/3} log q)\nwith the random choices of \\sigma, u. This bound is established by the robustness of the universal\ncoupler on randomly constructed distributions that satisfy the martingale property (Definition 12\nand Lemma 17) and a pinning lemma (Lemma 10).\nTo use this algorithm to nontrivially speed up sampling of planar perfect matchings, we also need a tail\nbound for the round complexity. Because of Markov's inequality, the expected round complexity\nimplies a simple tail bound \u2013 the round complexity is less than c \\cdot n^{2/3} log q with probability \\Omega(1/c).\nAt the end of this subsection, we boost this tail bound to 2^{-\\Omega(c)} via the simple observation that\nrunning several rounds of the algorithm is equivalent to reinitiating the algorithm with a smaller\ninstance.\nWorst-case round complexity. First, we consider the randomness \\sigma, u used in the algorithm as\npart of the input, and give an upper bound for the round complexity. For each \\sigma\\in S_n, u \\in [0,1]^n\nand i \\in [n], let \\bar{a_i}(\\sigma, u) be the maximum a such that Algorithm 3 will not correctly sample X_{\\sigma(i)}\nto \\tilde{x_i}(\\sigma, u), the value of X_{\\sigma(i)} in the final output, under the correct conditioning of X_{\\sigma(1)},..., X_{\\sigma(a)}.\nFormally, \\bar{a_i}(\\sigma, u) is defined as follows:\n\\bar{a_i}(\\sigma, u) = max \\left\\{a \\geq 0 : \\tilde{x_i}(\\sigma, u) \\neq UNIVERSALCOUPLER \\left(X_{\\sigma(i)} \\middle| {X_{\\sigma(j)} = \\tilde{x_j}(\\sigma, u)}_{j\\in[a]}, u_i\\right)\\right\\},\nwhere we define the maximum of an empty set to be 0 for simplicity. Using this definition, we can\nestablish a worst-case round complexity of Algorithm 3.\nLemma 24. For any integer \\theta \\geq 1 and randomness \\sigma \\in S_n,u \\in [0,1]^n, the round complexity of\nAlgorithm 3 is at most\n|{i \\in [n] | \\bar{a_i}(\\sigma, u) \\geq i - \\theta}| + \\frac{n}{\\theta} + 1 + \\frac{n}{\\theta}.\nProof. Recall that we define a^t as the value of a after round t, and we define a^0 as 0. The algorithm\nhas at most one round t without computing a^t, when it terminates with x^t = y^t. Suppose that the\nstep size of any round t, where the algorithm computes a^t, is the increment a^t \u2013 a^{t-1}. Based on\nthe step sizes, we divide the rounds into two classes: small-progress rounds that have step sizes\n< \\theta, and large-progress rounds that have step sizes \\geq \\theta. Note that the number of large-progress\nrounds is at most n/\\theta because otherwise a will exceed n in some round. It suffices to upper bound\nthe number of small-progress rounds by |{i \\in [n] : \\bar{a_i}(\\sigma, u) \\geq i - \\theta}| to finish the proof.\nConsider any round t such that a^t \u2013 a^{t-1} < \\theta. Due to the definition of the algorithm and Lemma 21,\nthe algorithm finds y^t_{\\sigma(i)} = x^t_{\\sigma(i)} = \\tilde{x_i}(\\sigma, u) for any i < a^t. The algorithm also finds x^t_{\\sigma(a^t)}\\neq y^t_{\\sigma(a^t)}.\nAccording to the definition of x^t and y^t and Lemma 21, y^t_{\\sigma(a^t)} =\nUNIVERSALCOUPLER \\left(X_{\\sigma(a^t+1)} \\middle| {X_{\\sigma(j)} = \\tilde{x_j}(\\sigma, u)}_{j\\in[a^{t-1}]}, u_{a^t}\\right),\nand x^t_{\\sigma(a^t)} =\nUNIVERSALCOUPLER \\left(X_{\\sigma(a^t+1)} \\middle| {X_{\\sigma(j)} = \\tilde{x_j}(\\sigma, u)}_{j\\in[a^{t}-1]}, u_{a^t}\\right) = \\tilde{x_{a^t}}(\\sigma, u)."}, {"title": "Applications", "content": "In this section, we show an example application of Theorem 2, to the problem of sampling uniformly\nrandom perfect matchings in planar graphs. The famous FKT algorithm allows parallel counting\nof the number of perfect matchings [see, e.g., Ana+23a]. The previous best parallel runtime for\nthis problem is \\tilde{O}(n^{1/2}) for planar graphs of size n [Ana+23a].\nRemark 31. Two key techniques for deterministic (approximate) counting, namely the tree recur-\nsion/correlation decay method [Wei06] and the polynomial interpolation method [Bar16] can often\nbe trivially parallelized. The former involves solving a recursion on a tree of logarithmic depth,\nand the latter involves enumerating structures of logarithmic size in a host object (e.g., a graph).\nAs such, our results automatically provide a parallel speedup wherever these methods apply.\nTheorem 32. Let G = (V, E) be a planar graph. There exists an algorithm that samples a uniformly random\nperfect matching in G with a parallel runtime of \\tilde{O}(n^{1/3}) and poly(n) work.\nProof. Similar to [Ana+23a], we use the planar separator theorem to find a separator of size \\tilde{O}(\\sqrt{n}),\nsample the portion of the perfect matching incident to the separator, and then recursively sample\nthe rest of the perfect matching in the now-disjoint halves of the graph, in parallel. Our modification\nis that, while na\u00efvely sampling the separator edges takes \\tilde{O}(\\sqrt{n}) time, using Theorem 2, we can\nspeed it up to \\tilde{O}(n^{1/3}).\nTo be more specific, given the input graph G = (V, E), we find a planar separator S \\subseteq V of size\n\\tilde{O}(\\sqrt{n}), such that G \u2013 S is composed of two smaller graphs, on vertex sets A, B, each of size\n\\leq (1 \u2013 \\Omega(1))n. This can be done in parallel [GM87].\nNext, we consider the distribution \\mu on E^S, where \\mu(x) is proportional to the number of perfect\nmatchings that have edge x_v incident to v for all v \\in S. Note that many configurations x \\in E^S are\ninvalid, for example, those where v is not even an endpoint of x_v, or those with clashing edges\nfor two vertices in S. All of these invalid configurations are assigned a measure of 0 under \\mu. We\nclaim that there is a parallel (NC) counting oracle for \\mu. Indeed, given a partial pinning, we can\ncheck if it is valid, and if so, remove the edges in the pinning from the graph, and simply count\nperfect matchings in the resulting subgraph. The number of perfect matchings in planar graphs\ncan be efficiently computed in parallel by the FKT algorithm [see, e.g., Ana+23a].\nNow we use Algorithm 3 to sample from \\mu. Once the sample is produced, we remove all the\nendpoints of this partial matching from G (in particular, this removes all of S), and now we have\ntwo disjoint subgraphs of geometrically smaller size. In parallel, we recurse on each.\nNote that the total number of calls to Algorithm 3 is \\leq poly(n). By using the tail bounds for our\nalgorithm, Theorem 28, each call finishes in at most \\tilde{O}(\\sqrt{n}^{2/3}) = \\tilde{O}(n^{1/3}) time, with probability at\nleast 1-1/poly(n). Taking a union bound, and using the fact that recursively the subgraphs shrink\ngeometrically, we get that the overall parallel runtime is \\tilde{O}(n^{1/3}) with high probability."}, {"title": "Hardness", "content": "In this section", "1": "any c \\in (0", "mu": {"n": "and y \\in [q", "q": "n | x_S = y"}}, {"mu": {"mu(H)": "sum_{x\\in H"}, "1": ".", "a_i": "first we independently\nand uniformly choose a matrix B_i \\in {0", "m": "and then we define the boolean function \\mu_i as follows:\n\\forall x \\in {0", "v_i": "nwhere all the operations are under F_2. Then", "mu_i": "n\\forall x \\in {0", "prod_{i\\in[r": ""}, {"S_i}": "n{x \\in {0,1}^{S_i} | x_{S_i \\cap S} = y_{S \\cap S_i}}. Since S_1,..., S_r is a partition of the n variables, we have H =\nH_{S_1} \\times H_{S_2} \\times ..."}]}