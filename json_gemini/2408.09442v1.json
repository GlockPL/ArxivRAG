{"title": "Parallel Sampling via Counting", "authors": ["Nima Anari", "Ruiquan Gao", "Aviad Rubinstein"], "abstract": "We show how to use parallelization to speed up sampling from an arbitrary distribution u on\na product space [q]^n, given oracle access to counting queries: P_{X~\\mu}[X_S = \\sigma_S] for any S \\subseteq [n] and\n\\sigma_S \\in [q]^S. Our algorithm takes O(n^{2/3} \\cdot polylog(n, q)) parallel time, to the best of our knowledge,\nthe first sublinear in n runtime for arbitrary distributions. Our results have implications for\nsampling in autoregressive models. Our algorithm directly works with an equivalent oracle\nthat answers conditional marginal queries P_{X~\\mu}[X_i = \\sigma_i | X_S = \\sigma_S], whose role is played by a\ntrained neural network in autoregressive models. This suggests a roughly n^{1/3}-factor speedup\nis possible for sampling in any-order autoregressive models. We complement our positive\nresult by showing a lower bound of \\Omega(n^{1/3}) for the runtime of any parallel sampling algorithm\nmaking at most poly(n) queries to the counting oracle, even for q = 2.", "sections": [{"title": "1 Introduction", "content": "The seminal work of Jerrum, Valiant, and Vazirani [JVV86] established an algorithmic equivalence\nbetween the tasks of approximate sampling and approximate counting, for the ubiquitous class of\nself-reducible problems. This key equivalence is at the heart of the Monte Carlo Markov Chain\napproach to approximate counting [\u0160VV09], which has enabled breakthroughs like approximating\nthe permanent [JSV04] or the volume of convex sets [DFK91]. In this paper, we focus on one side\nof this equivalence, sampling via counting.\nSelf-reducibility, in its most widely applied form, concerns distributions \\mu on a product space\n[q]^n and their pinnings: conditional distributions obtained by selecting a subset S \\subseteq [n] and\npartial configuration \\sigma_S \\in [q]^S and conditioning X ~ \\mu to have coordinates in S pinned to \\sigma_S:\nX_S = \\sigma_S. In this setting, sampling means producing a random X distributed according to a\nspecified pinning of \\mu. Counting, on the other hand, refers to computing the partition functions\nof pinnings: P_{X~\\mu}[X_S = \\sigma_S].1 Sampling via counting is in fact very easy to describe in this\nsetting. Assuming access to a counting oracle, we can produce samples from u via the following\nautoregressive generation process:\nInitialize \\sigma \\leftarrow 0\nfor i = 1,...,n do\nfor x \\in [q] do\n p_x \\leftarrow P_{X~\\mu}[X_i = x | X_{[i-1]} = \\sigma_{[i-1]} ]\n \\sigma_i \\leftarrow random sample in [q] distributed ~ (p_1, ..., p_q)\nreturn \\sigma\nNote that we only need to use the counting oracle to compute the computationally equivalent\nconditional marginals:\nP_{X~\\mu}[X_i = x | X_{[i-1]} = \\sigma_{[i-1]}] = \\frac{P_{X~\\mu}[X_{[i-1]} = \\sigma_{[i-1]}, X_i = x]}{P_{X~\\mu}[X_{[i-1]} = \\sigma_{[i-1]}]}\nThis process, despite its simplicity, is how the widely successful autoregressive models generate\ntheir output [see, e.g., LM11; VKK16; Vas+17; Dev+18; Yan+19; Bro+20]. State-of-the-art large\nlanguage models, or even some competitive vision models, train a neural network to answer\nconditional marginal queries and then use the aforementioned process to generate samples. In the\ncontext of language models, [q] represents a token space, and n is the length of generated text or\ncontext length, while in pixel-space vision models, [q] is possible values for a pixel, and n is the\nnumber of pixels in the image.\nOne downside of this simple sampling process is that it is extremely sequential. One has to generate\ncoordinates 1, . . ., i \u2212 1, to know which conditional marginals need to be queried in the ith iteration.\nSo, it is natural to ask if there is a more parallelizable sampling process. More precisely, suppose that\nan oracle2 can answer conditional marginal queries of the form P[X_i = x | X_S = \\sigma_S], and we can\ninteract with this oracle in rounds, each time asking polynomially many queries simultaneously.\nWe are interested in finding the adaptive complexity of sampling:"}, {"title": "1.2 Techniques", "content": "Our algorithm works by modifying the autoregressive sampling process in two ways. First, we\nchoose the order of coordinates according to a uniformly random permutation. Second, to break\nsequentiality, we generate \u201cguesses\u201d of future coordinates, by computing the marginal of each X\u00a1\nconditioned on the current pinning, in parallel, and sampling from these marginal distributions\nindependently for each i. While these independent samples clearly ignore dependencies between\ncoordinates, we can in a second stage verify in parallel that each X_i would have been the sample\nproduced if we had continued sequentially. We advance up to the point where our guesses\nsuccessfully pass verification and then iterate.\nThe key idea behind our analysis is that as we pin more and more random coordinates, the\ndependencies between the remaining coordinates weaken in an average sense. This intuitive idea\nis formalized by the so-called pinning lemmas [RT12; Mon08] which we use in our analysis, see\nSection 2.1. Weakened dependencies intuitively mean that our guesses are not likely to deviate\nfrom what sequential sampling would have produced. This is formally proved in Section 3.\nFinally, a tool we use from existing literature on parallel sampling is a universal coupler [LY22].\nThis is used to ensure consistency between the guessing and verification stages. In both of these\nphases, for each X_i, we would like to sample from a marginal distribution. Universal couplers\nensure that when the marginal distributions are \u201cclose\u201d, the samples are likely to be exactly equal.\nWe extend the analysis of universal coupling to multiple distributions, as needed by our work, see\nSection 2.2."}, {"title": "1.3 Organization", "content": "In Section 2 we discuss and further develop two of the main tools we use for parallelization:\npinning lemmas and universal coupling. In Section 3, we describe our parallel sampling algorithm\nand prove our main result Theorem 2. In Section 4, we provide an application of Theorem 2 to the\nproblem of sampling planar perfect matchings. In Section 5, we prove a lower bound against all\nalgorithms, i.e., Theorem 4. In Section 6, we prove that our analysis of the algorithm presented in\nSection 3 is tight, and n^{2/3} cannot be improved for this particular algorithm."}, {"title": "2 Preliminaries", "content": "We use [n] to denote the set {1,2,\u2026\u2026, n}. For any vector x and set S, we use x_S to denote x restricted\nto S. We use \u00b1x to indicate the interval [\u2212x, x]. We use S_n to denote the set of permutations on\nn elements. For any two sets A, B, we use A \u00d7 B to denote the Cartesian product of A and B, i.e.,\nA \u00d7 B = {(a, b) | a \\in A,b \\in B}.\nFor a distribution \u00b5, we use x ~ \u00b5 to denote that x is sampled from \u00b5. Similarly, for a set S, we use\nx ~ S to indicate that x is sampled uniformly at random from S."}, {"title": "2.1 Pinning Lemmas", "content": "The pinning lemma formalizes an intuition that randomly pinning coordinates of an arbitrary\ndistribution should in an average sense lower the correlation between remaining coordinates.\nIntuitively, this should make parallel sampling easier; for example, if all coordinates become fully\nindependent, one can in parallel sample from the marginals. We do not directly use the classical\nstatement of the pinning lemma, mentioned below for comparison, but rather prove a statement\nin the same vein and using the same proof strategy.\nLemma 5 (pinning lemma [RT12; Mon08]). Let X_1, X_2, ..., X_n be random variables, each supported on\n{0,1}. For any l \\in [n], there exists a set S such that |S| \\leq l and\nE_{X_S} [E_{u, \\sigma ~ S^{(l)}}[Cov(X_u, X_v | X_S)^2]] \\leq \\frac{O(1)}{l}.\nWe now define and state well-known statements about entropy, building up to state and prove our\nnew variant of the pinning lemma.\nDefinition 6 (entropy). Let X, Y be random variables on [q]. The entropy of random variable X is\ndefined to be\nH(X) = \\sum_{i\\in[q]} P[X = i] \\cdot log P[X = i].\nThe conditional entropy of X conditioned on Y is defined to be\nH(X | Y) = \\sum_{i\\in[q]} H(X | Y = i) \\cdot P[Y = i].\nDefinition 7 (KL divergence). For a pair of distributions \\nu, \\mu, we let\nD_{KL}(\\nu || \\mu) = E_{x~\\nu} log \\frac{\\nu(x)}{\\mu(x)}.\nAbusing notation, we extend the definition to random variables. If X ~ \\nu,Y ~ \\mu, we use D_{KL}(X ||\nY) to denote D_{KL}(\\nu || \\mu)."}, {"title": "2.2 Universal Coupling", "content": "For any integer q > 0, let A_q be the probability simplex on [q], i.e., A_q = {\\mu \\in [0,1]^q | \\sum_{i=1}^{q} \\mu(i) = 1}.\nIn our main algorithm, we use a \u201cuniversal coupler\u201d as a subroutine. Informally, this is an algorithm\nthat maps a distribution \u0438 \u2208 A_q and a random source r to a sample from u, with the property\nthat the output is unlikely to change if u is perturbed slightly (while keeping r fixed). Such an\nalgorithm naturally induces a coupling between any two distributions \u03bc, \u03bc'. In any coupling, there\nmust be at least d_{TV}(\\mu, \\mu') chance that the samples for \u03bc, \u03bc' are unequal; and this lower bound can\nbe achieved if we design a tailor-made coupling knowing both \u03bc and \u03bc'. Surprisingly, one can\nachieve the same bound within constant factors without knowing both distributions in advance.\nThe existence of these robust universal couplers appears to have been discovered and rediscovered\nmany times. The earliest works that we are aware of are the MinHash algorithm of Broder [Bro97]\nfor uniform distributions, and a rejection-sampling-based strategy of Kleinberg and Tardos [KT02]\nand Holenstein [Hol07] for general distributions. See [Bav+20] for more on the history and\noptimality of these strategies. Recently, in the context of parallel sampling algorithms, the work of\nLiu and Yin [LY22] has rediscovered the same rejection-sampling-based algorithm; we borrowed\nthe terminology of \u201cuniversal coupling\" from the latter work.\nA universal coupler is defined as follows:\nDefinition 11 (universal coupling, [LY22]). A deterministic function f : A_q \u00d7 [0,1] \u2192 [q] is a\nuniversal coupling on [q] if, when r\u2208 [0,1] is chosen uniformly at random, for any distribution\n\\mu\\in A_q and x \\in [q],\nPr_{r~[0,1]}[f(\\mu, r) = x] = \\mu(x).\nNote that instead of r ~ [0, 1], one can use other sources of randomness with infinite entropy, such\nas an infinite sequence of random bits, etc. Since it is easy to translate between these sources,\nwe pick the notationally most convenient form of random source when describing each universal\ncoupler.\nThe main characteristic we would like for universal couplers is that on \u201cclose\u201d distributions \u03bc, \u03bc',\nthe chance that f (\u00b5, r) \u2260 f(\u00b5', r) is small. A lower bound on this chance is P_r[f(\\mu, r) \u2260 f(\\mu',r)] >\nd_{TV}(\\mu, \\mu'). Surprisingly, this can be matched up to a factor of 2; in fact the optimal f has been\nshown [KT02; Hol07; Bav+20; LY22] to satisfy\nP_r[f(\\mu,r) \\neq f(\\mu',r)] \\leq \\frac{2 d_{TV}(\\mu, \\mu')}{1 + d_{TV}(\\mu, \\mu')} \\leq 2 d_{TV} (\\mu, \\mu').\nIn our algorithm, we need a slightly stronger guarantee that holds for not just two, but an arbitrary\nnumber of distributions.\nDefinition 12 (robust universal coupler). We call a universal coupler f robust if for any number of\ndistributions \u03bc\u2081, ..., \u00b5_m it satisfies\nP_r [\\exists i, j\\in [m] : f(\\mu_i, r) \\neq f(\\mu_j, r)] \\leq \\frac{\\sum_{x\\in[q]}(max_{i\\in[m]} \\mu_i(x) \u2013 min_{i\\in[m]} \\mu_i(x))}{\\sum_{x\\in[q]} max_{i\\in[m]} \\mu_i(x)}\nNote that for m = 2, the numerator on the r.h.s. becomes 2 d_{TV}(\\mu_1, \\mu_2), and the denominator\nbecomes 1 + d_{TV}(\\mu_1, \\mu_2). Thus, this matches the same optimal bound derived by prior works.\nWe show that the rejection-sampling-based algorithm used in prior works, which we call the\nMinCoupler, Satisfies this more general robustness guarantee. Additionally, we show that another\nwidely used algorithm called the \u201cGumbel trick\u201d also satisfies the same robustness guarantee."}, {"title": "3 Sublinear Parallel Sampling via Counting Oracles", "content": "In this section, we show our main Theorem 2 that we can (approximately) sample from a distri-\nbution after a sublinear number of rounds (in terms of the number of variables) of querying a\npolynomial number of the distribution's counting oracles."}, {"title": "3.1 Algorithm", "content": "We present our algorithm in Algorithm 3. Let u\u2081, ..., U_n be the random seeds of the algorithm.\nThe algorithm also shuffles the coordinates with a uniformly random permutation that we ignore\nin this description for simplicity."}, {"title": "3.2 Correctness", "content": "We consider a function x : S_n \u00d7 [0, 1]^n \u2192 [q]^n defined iteratively as follows:\n\\tilde{x}_i(\\sigma, u) = UNIVERSALCOUPLER\\left(X_{\\sigma(i)} \\middle| \\{X_{\\sigma(j)} = \\tilde{x}_j(\\sigma, u)\\} _{j\\in[i-1]}, u_i\\right).\nFor each i \u2208 [n], because UNIVERSALCOUPLER is a universal coupler, \\tilde{x}_i(\\sigma, u) follows the marginal\ndistribution of X_{\\sigma(i)} conditioning on X_{\\sigma(1)} = \\tilde{x}_1(\\sigma, u), X_{\\sigma(2)} = \\tilde{x}_2(\\sigma, u),\u2026\u2026,X_{\\sigma(i-1)} = \\tilde{x}_{i-1}(\\sigma, u)\n(considering only the randomness of u_i). Therefore, this function can serve as an objective output\nof the algorithm when we have fixed the randomness \\sigma and u.\nLemma 20. If Algorithm 3 always outputs x^t_{\\sigma(i)} = \\tilde{x}_i(\\sigma, u), it samples perfectly from the distribution of X.\nLet a^t be the value of a at the end of round t (if the algorithm does not terminate with x^t = y^t\nbefore updating the value of a in round t). For simplicity, we suppose a^0 = 0. Next, we show that\nthe vector x^t produced by the algorithm in each round matches this objective vector in the first a^t\nentries.\nLemma 21. For any \\sigma, u, after each round t of Algorithm 3,\n\\forall i\\in [a^t], x^t_{\\sigma(i)} = \\tilde{x}_i(\\sigma, u).\nProof. According to the definition of a^t, we have \\forall i \\in [a^{t-1}], x^t_{\\sigma(i)} = y^t_{\\sigma(i)}. Therefore, according to\nthe definition of x^t, we have for any i \\in [a^t],\nx^t_{\\sigma(i)} = UNIVERSALCOUPLER\\left(X_{\\sigma(i)} \\middle| \\{X_{\\sigma(j)} = y^t_{\\sigma(j)}\\} _{j\\in[i-1]}, u_i\\right).\nNote that this recursion matches the recursion Eq. (2) used in the definition of \\tilde{x}_i(\\sigma, u) for any\ni \\in [a^t]. Therefore, \\forall i \\in [a^t], x^t_{\\sigma(i)} = \\tilde{x}_i(\\sigma, u).\nTo show that Algorithm 3 is making progress every iteration, we prove a^t is (strictly) monotone in\nterms of t.\nLemma 22. a^t is strictly increasing with t.\nProof. According to the definition of y^t, for any i \\in [a^{t-1}], y^t_{\\sigma(i)} = x^{t-1}_{\\sigma(i)} = \\tilde{x}_i(\\sigma, u). Therefore,\naccording to the definition of x^t, for any i \\in [a^{t-1} + 1],\nx^t_{\\sigma(i)} = UNIVERSALCOUPLER\\left(X_{\\sigma(i)} \\middle| \\{X_{\\sigma(j)} = x^{t-1}_{\\sigma(j)}\\} _{j\\in[i-1]}, u_i\\right)\n= UNIVERSALCOUPLER\\left(X_{\\sigma(i)} \\middle| \\{X_{\\sigma(j)} = \\tilde{x}_j(\\sigma, u)\\} _{j\\in[i-1]}, u_i\\right) (definition of a^{t-1})\n= \\tilde{x}_i(\\sigma, u) = y^t_{\\sigma(i)}. (definition of \\tilde{x}_i(\\sigma, u))\nBy the definition of a^t, if x^t \\neq y^t, we get a^t > a^{t-1} + 1.\nNote that the algorithm terminates when either of the following two conditions is satisfied in some\nround t: a^t = n or x^t = y^t. Due to Lemma 22, the algorithm always terminates. If it terminates\nbecause of the first condition a^t = n, due to Lemma 21, the output of the algorithm matches\nthe objective in Lemma 20. Otherwise, because of the definition of x^t and the fact that x^t = y^t,\nthe output satisfies Eq. (3), which matches the recursion Eq. (2) used in the definition of \\tilde{x}_i(\\sigma, u),\nand thus matches the objective in Lemma 20. As a conclusion, we obtain the correctness of our\nalgorithm.\nLemma 23. Algorithm 3 returns a sample x ~ \\mu."}, {"title": "3.3 Round Complexity", "content": "We establish our sublinear round complexity via two steps. First, we ignore the randomness of\n\\sigma and u, and establish a worst-case round complexity, which can be linear with some choices\nof \\sigma, u. Second, we show that the expectation of this round complexity is actually \\tilde{O}(n^{2/3} log q)\nwith the random choices of \\sigma, u. This bound is established by the robustness of the universal\ncoupler on randomly constructed distributions that satisfy the martingale property (Definition 12\nand Lemma 17) and a pinning lemma (Lemma 10).\nTo use this algorithm to nontrivially speed up sampling of planar perfect matchings, we also need a tail\nbound for the round complexity. Because of Markov's inequality, the expected round complexity\nimplies a simple tail bound \u2013 the round complexity is less than c \u2022 n^{2/3} log q with probability O(1/c).\nAt the end of this subsection, we boost this tail bound to 2^{-\\Omega(c)} via the simple observation that\nrunning several rounds of the algorithm is equivalent to reinitiating the algorithm with a smaller\ninstance.\nWorst-case round complexity. First, we consider the randomness \\sigma, u used in the algorithm as\npart of the input, and give an upper bound for the round complexity. For each \\sigma\\in S_n, u \\in [0, 1]^n\nand i \\in [n], let \\bar{a}_i(\\sigma, u) be the maximum a such that Algorithm 3 will not correctly sample X_{\\sigma(i)}\nto \\tilde{x}_i(\\sigma, u), the value of X_{\\sigma(i)} in the final output, under the correct conditioning of X_{\\sigma(1)},..., X_{\\sigma(a)}.\nFormally, \\bar{a}_i(\\sigma, u) is defined as follows:\n\\bar{a}_i(\\sigma, u) = max\\left\\{a \\geq 0 \\middle|  \\tilde{x}_i(\\sigma, u) \\neq UNIVERSALCOUPLER\\left(X_{\\sigma(i)} \\middle| \\{X_{\\sigma(j)} = \\tilde{x}_j(\\sigma, u)\\} _{j\\in[a]}, u_i\\right)\\right\\},\nwhere we define the maximum of an empty set to be 0 for simplicity. Using this definition, we can\nestablish a worst-case round complexity of Algorithm 3.\nLemma 24. For any integer \\theta \\geq 1 and randomness \\sigma \\in S_n,u \\in [0,1]^n, the round complexity of\nAlgorithm 3 is at most\n|\\{i \\in [n] | \\bar{a}_i(\\sigma, u) \\geq i - \\theta\\}| + \\frac{n}{\\theta} + 1.\nProof. Recall that we define a^t as the value of a after round t, and we define a^0 as 0. The algorithm\nhas at most one round t without computing a^t, when it terminates with x^t = y^t. Suppose that the\nstep size of any round t, where the algorithm computes a^t, is the increment a^t \u2013 a^{t\u22121}. Based on\nthe step sizes, we divide the rounds into two classes: small-progress rounds that have step sizes\n< \\theta, and large-progress rounds that have step sizes \\geq \\theta. Note that the number of large-progress\nrounds is at most n/\\theta because otherwise a will exceed n in some round. It suffices to upper bound\nthe number of small-progress rounds by |\\{i \\in [n] : \\bar{a}_i(\\sigma, u) \\geq i - \\theta\\}| to finish the proof.\nConsider any round t such that a^t \u2212 a^{t\u22121} < \\theta. Due to the definition of the algorithm and Lemma 21,\nthe algorithm finds y^t_{\\sigma(i)} = x^t_{\\sigma(i)} = \\tilde{x}_i(\\sigma, u) for any i < a^t. The algorithm also finds \\tilde{x}^t_{\\sigma(a^t)}\nAccording to the definition of x^t and y^t and Lemma 21, y^t_{\\sigma(a^t)} = \\\nUNIVERSALCOUPLER\\left(X_{\\sigma(a^t+1)} \\middle| \\{X_{\\sigma(j)} = \\tilde{x}_j(\\sigma, u)\\} _{j\\in[a^{t-1}]}, u_{a^t}\\right),\nand x^t_{\\sigma(a^t+1)} = \nUNIVERSALCOUPLER\\left(X_{\\sigma(a^t+1)} \\middle| \\{X_{\\sigma(j)} = \\tilde{x}_j(\\sigma, u)\\} _{j\\in[a^t-1]}, u_{a^t}\\right) = \\tilde{x}_{a^t}(\\sigma, u)."}, {"title": "4 Applications", "content": "In this section, we show an example application of Theorem 2, to the problem of sampling uniformly\nrandom perfect matchings in planar graphs. The famous FKT algorithm allows parallel counting\nof the number of perfect matchings [see, e.g., Ana+23a]. The previous best parallel runtime for\nthis problem is O(n^{1/2}) for planar graphs of size n [Ana+23a].\nRemark 31. Two key techniques for deterministic (approximate) counting, namely the tree recur-\nsion/correlation decay method [Wei06] and the polynomial interpolation method [Bar16] can often\nbe trivially parallelized. The former involves solving a recursion on a tree of logarithmic depth,\nand the latter involves enumerating structures of logarithmic size in a host object (e.g., a graph).\nAs such, our results automatically provide a parallel speedup wherever these methods apply."}, {"title": "5 Hardness", "content": "In this section, we prove that any algorithm cannot approximately sample within a constant total\nvariation distance of arbitrary distribution \u03bc with n^{1/3-\\Omega(1)} round complexity and a polynomial\nnumber of queries in each round to the exact counting oracle. More generally, we shall prove the\nfollowing hardness result on parallel search via counting oracles for q = 2.\nTheorem 33. For any constant \\delta \\in (0, 1], any c \\in (0, n^{1-\\delta}) and any (randomized) algorithm ALG making\nat most n^c queries to the counting oracle in each round, there exists an instance \\mu : {0,1}^n \u2192 {0,1} such\nthat ALG can only find a solution x (such that \\mu(x) = 1) with probability at most 0.01 after (strictly) less\nthan \\Omega(\\frac{n}{(\\delta (c+2) log n)^{1/3}}) rounds of queries.\nIn the rest of this section, we use H = (S, y), where S \u2286 [n] and y \u2208 [q]^S, to denote a hypercube by\nH = {x \u2208 [q]^n | x_S = y}. For the abuse of notation, for any function \\mu : {0,1} \u2192 {0,1} and any\nhypercube H, we define \u03bc(H) := \\sum_{x\\in H} \u03bc(x) as the output of the counting oracle.\nThe (random) hard instances. We consider deterministic algorithms that make at most n^c queries\nin each round, where c < n^{1-\\delta} for some constant \u03b4 \u2208 (0, 1]. We randomly partition the n variables"}, {"title": "6 Lower bound of Algorithm 3", "content": "In this section, we show that our analysis of Algorithm 3 is tight up to polylogarithmic factors. For\nconcreteness, we assume the UNIVERSALCOUPLER used is MINCOUPLER.\nTheorem 39. There exists an instance such that Algorithm 3 terminates in \\Omega(\\frac{n^{2/3}}{(log n)}) rounds with probability\nat least 0.99.\nThe proof of this lower bound uses the explicit construction of the universal coupler used in the\nalgorithm (MINCOUPLER), but we expect it to also hold with small modifications for GumbelTrick.\nWe are unaware whether this lower bound holds for an arbitrary universal coupler.\nErrors of the universal coupler. Given a distribution \u03bc, we identify a set of randomness r where\nMinCoupler produces different samples with constant probability for u and v when the second\ndistribution v is very different from \u03bc. Here, we say v is very different from \u03bc, if we sample x\nfrom \u03bc, there is a constant probability of having v(x) < (1 \u2212 \u03b4)\u03bc(x) for some reasonably large \u03b4.\nRecall the construction of MinCoupler we encode r as pairs (x_1, p_1), (x_2, p_2),\u2026\u2026 \u2208 [q] \u00d7 [0,1],\nchoose the minimum index i^* such that p_{i^*} \\leq \u03bc(x_{i^*}) and let x_{i^*} be the output of the universal\ncoupler. The output x_{i^*} follows the distribution \u03bc. If we consider the restricted set of r such that\np_{i^*} \\geq (1 \u2013 \u03b4)\u03bc(x_{i^*}), for any distribution v that is very different from \u03bc, MinCoupler will produce\ndifferent samples for u and v with constant probability. We formalize the above argument as the\nfollowing lemma, whose proof is deferred to Appendix A.3.\nLemma 40 (Sure mistakes made by MinCoupler). Consider any distribution \\mu \\in \\Delta_q and any \\delta > 0.\nSuppose R(\\mu, \\delta) is the set of randomness r = ((x_1, p_1), (x_2, p_2),\u2026) used by MinCoupler such that\np_{i^*} \\geq (1 \u2013 \\delta)\\mu(x_{i^*}), where i^* = min\\{i : p_i \\leq \\mu(x_i)\\} is the index chosen by MinCoupler.\nThen, for any distribution v \u2208 \\Delta_q, we have\nPr_{r~R(\\mu,\\delta)} [MinCoupler(\\mu, r) \u2260 MinCoupler(\\nu, r)] \\geq \\frac{1 - \\nu_{max}}{2} \\cdot P_{x~\\mu}[\\nu(x) < (1 \u2013 \\delta)\\mu(x)],\nwhere \\nu_{max} is the maximum mass max_{x\\in[q]} \\nu(x) of the distribution \\nu.\nThe (random) hard instances. Suppose m = 20n. Consider parameters Y_1, Y_2,\u2026\u2026, Y_n e R^m,\nwhich are randomly constructed in symmetry and will be stated later. Let z ~ N(0, I_m) be a\nrandom vector. For each i \u2208 [n], let the variable X_i = round((y_i, z)), where the rounding function\nround(x) is constructed as follows:\nround(x) = min\\{n^4log n, max\\{-n^4log n, \\lfloor n^4x\\rfloor\\}\\}.\nTo prove the lower bound, we consider the parameters Y_1, Y_2,\u2026, Y_n as i.i.d.s following the distri-\nbution N(0, I_m) and use V_i = (y_i, z) to denote the variables before rounding. It can be shown\nthat, with high probability, X_i equals the floor of n^4V_i for any i \u2208 [n]."}]}