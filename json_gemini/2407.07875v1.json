{"title": "Generative Image as Action Models", "authors": ["Mohit Shridhar", "Yat Long Lo", "Stephen James"], "abstract": "Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, \u0430\nbehavior-cloning agent that fine-tunes Stable Diffusion to \"draw joint-actions\" as\ntargets on RGB images. These images are fed into a controller that maps the visual\ntargets into a sequence of joint-positions. We study GENIMA on 25 RLBench and\n9 real-world manipulation tasks. We find that, by lifting actions into image-space,\ninternet pre-trained diffusion models can generate policies that outperform state-\nof-the-art visuomotor approaches, especially in robustness to scene perturbations\nand generalizing to novel objects. Our method is also competitive with 3D agents,\ndespite lacking priors such as depth, keypoints, or motion-planners.", "sections": [{"title": "1 Introduction", "content": "Image-generation diffusion models [1, 2, 3] are generalists in producing visual-patterns. From\nphoto-realistic images [4] to abstract art [5], diffusion models can generate high-fidelity images\nby distilling massive datasets of captioned images [6]. Moreover, if both inputs and outputs are in\nimage-space, these models can be fine-tuned to unlock new capabilities such as image-editing [7, 8],\nsemantic correspondences [9, 10], or novel view synthesis [11, 12]. Can we similarly unlock image-\ngeneration models for generating robot actions?"}, {"title": "2 GENIMA", "content": "GENIMA is a behavior-cloning agent that maps RGB observations \\(O_t\\) and a language goal g into\n\\(A^{image}_{t+K}\\). The key objective is to lift actions into image-space such that internet-\npretrained diffusion models can learn action-patterns as visual-patterns. We achieve this with a two-\nstage process: (1) fine-tune Stable Diffusion [1] to draw images with target joint-positions \\(A^{image}_{t+K}\\)\nthat are K timesteps ahead, and (2) train a controller to translate these targets into to a sequence\nof executable joint-positions \\(A^{joints} = \\{a_{t_1}, a_{t_2}.. a_{t_K}\\}\\). This simple two-stage process offloads se-\nmantic and task-level reasoning to a generalist image-generation model, while the controller reaches\nnearby joint-positions indicated by the visual targets. The sections below describe the two stages in\ndetail, and Figure 1 provides an overview."}, {"title": "2.1 Diffusion Agent", "content": "The diffusion agent controls what to do next. The agent takes RGB observations \\(O_t\\) and language\ngoal g as input, and outputs an image with target joint-positions \\(A^{image}_{t+K}\\). This problem formulation\nis a classic image-to-image generation setup, so any diffusion fine-tuning pipeline [7, 28, 29] can be\nused. We specifically use ControlNet [8] to preserve spatial layouts and for data-efficient fine-tuning.\nFine-Tuning Data. To supervise the fine-tuning, we take expert demonstrations, and randomly\nsample observations and target joint-positions from t + K timesteps. For that timestep, we obtain\n6-DoF poses of each robot joint, which is available through robot-APIs (from forward-kinematics).\nWe place spheres at those poses with pyrender\u00b9, and render them on four camera observations\n\\(O_t = \\{o_{front}, o_{wrist}, o_{left}, o_{right}\\}\\) with known intrinsics and extrinsics. On a 7-DoF Franka Panda, we\nonly render four joints: base, elbow, wrist, and gripper, to avoid cluttering the image. Each\njoint is represented with an identifying color, with separate colors for gripper open and close. The\nspheres include horizontal stripes parallel to the joint's rotation axis, acting as graduations indicating\nthe degree of rotation. Only horizontal stripes are needed as each joint has only one rotation axis.\nFine-Tuning with ControlNet. Given an image-to-image dataset, we finetune Stable Diffusion [27]\nwith ControlNet [8] to draw targets on observations. ControlNet is a two-stream architecture: one\nstream with a frozen Stable Diffusion UNet that gets noisy input and language descriptions, and a\ntrainable second stream that gets a conditioning image to modulate the output. This architecture re-\ntains the text-to-image capabilities of Stable Diffusion, while fine-tuning outputs to spatial layouts in\nthe conditioning image. GENIMA uses RGB observations as the conditioning image to draw precise\ntargets. We use SD-Turbo [27] \u2013 a distilled model that can generate high-quality images within 1 to\n4 diffusion steps \u2013 as the base model for fine-tuning. We use the HuggingFace implementation [30]\u00b2\nof ControlNet without modifications. See Appendix D for more details on fine-tuning.\nTiled Diffusion. Fine-tuning Stable Diffusion on robot data poses three key challenges. Firstly,\nStable Diffusion models work best with image resolutions of 512\u00d7512 or higher due to their training\ndata. In robotics, large images increase inference latency. Secondly, multi-view generation suffers\nfrom inconsistencies across viewpoints. However, multi-view setups are crucial to avoid occlusions\nand improve spatial-robustness. Thirdly, diffusion is quite slow, especially for generating four target\nimages at every timestep. Inspired by view-synthesis works [31, 32], we solve all three challenges\nwith a simple solution: tiling. We tile four observations of size 256 \u00d7 256 into a single image of\n512x 512. Tiling generates four images at 5 Hz on an NVIDIA A100 or 4 Hz on an RTX 3090."}, {"title": "2.2 Controller", "content": "The controller translates target images \\(A^{image}_{t+1}\\) into executable joint-positions \\(A^{joints}\\). The controller\ncan be implemented with any visuomotor policy that maps RGB observations to joint-positions. We\nspecifically use ACT [23, 33] \u2013 a Transformer-based policy architecture [34] \u2013 for its fast inference-\nspeed and training stability. However, in theory, any framework like Diffusion Policies [24] or\nRL-based methods [35] can be used. Even classical controllers can be used if pose-estimates of\ntarget spheres are provided, but in our implementation, we opt for learned controllers for simplicity.\nTraining. During training, the controller receives current joint-\npositions, the language goal, and RGB images with ground-truth\ntargets overlaid on random backgrounds. The random back-\ngrounds, as shown in Figure 2, force ACT to follow targets and\nignore any contextual information in the scene. We use the same\nhyperparameters and settings from the original ACT codebase\u00b3\nwith minor modifications. To improve robustness to fuzzy diffu-\nsion outputs, we augment images with random-crops [36], color\njitters, elastic transforms, and Gaussian noise. We use L1 loss for"}, {"title": "3 Experiments", "content": "We study GENIMA in both simulated and real-world environments. Specifically, we are interested\nin answering the following questions:\n\u00a7 3.1 How does GENIMA compare against state-of-art visuomotor policies and 3D baselines?\n\u00a7 3.2 What are the benefits of drawing actions with internet-pretrained image-generation models?\n\u00a7 3.3 Which factors affect GENIMA's performance?\n\u00a7 3.4 How well does GENIMA perform on real-world tasks?\nWe start with benchmarking our method in simulated environments for reproducible and fair com-\nparisons. The following sections describe our simulation setup and evaluation methodology.\nSimulation Setup. All simulated experiments are set in CoppeliaSim [38] interfaced through\nPyRep [39]. The robot is a 7-DoF Franka Emika Panda placed behind a tabletop. Observations are\ncaptured from four RGB cameras: front, left_shoulder, right_shoulder, and wrist, each\nwith a resolution of 256 \u00d7 256. The robot is commanded with joint-position actions via PID control,\nor end-effector actions via an IK solver.\n25 RLBench Tasks. We choose 25 (out of 100) tasks from RLBench [22]. While most RLBench\ntasks are suited for discrete, quasi-static motions that benefit 3D next-best-pose agents [25, 26],\nwe pick tasks that are difficult to execute with end-effector control. Tasks such as open box and\nopen microwave require smooth non-linear motions that sampling-based motion-planners and IK\nsolvers struggle with. Each RLBench task includes several variations, but we only use variation0\nto reduce training time with limited resources. However, our method should be applicable to multi-\nvariation settings without any modifications. We generate two datasets: 50 training demos and\n50 evaluation episodes per task. For both datasets, objects are placed randomly, and each episode\nis sanity-checked for solvability. Language goals are constructed from instruction templates. See\nAppendix A for details on individual tasks.\nEvaluation Metric. Multi-task agents are trained on all 25 tasks, and evaluated individually on\neach task. Scores are either 0 for failures or 100 for successes, with no partial successes. We report\naverage success rates on 50 evaluation episodes across the last three epoch checkpoints: 50\u00d73 = 150\nepisodes per task, which adds up to 150 \u00d7 25 = 3750 in total. We use a single set of checkpoints\nfor all tasks without any task-specific optimizations or cherry-picking.\nVisuomotor Baselines. We benchmark GENIMA against three state-of-the-art visuomotor ap-\nproaches: ACT [19, 23], DiffusionPolicies [24], and SuSIE [13]. ACT is a transformer-based policy\nthat has achieved compelling results in bimanual manipulation. Although GENIMA uses ACT as\nthe controller, our controller has never seen RGB observations from demonstrations, just sphere\ntargets with random backgrounds. DiffusionPolicies is a widely adopted visuomotor approach that\ngenerates multi-modal trajectories through diffusion. SuSIE is the closest approach to GENIMA,\nbut instead of drawing target actions, SuSIE generates target RGB observations as goal images. We\nadapt SuSIE to our setting by training a controller that maps target and current RGB observations\nto joint-position actions. All multi-task baselines are conditioned with language goals. ACT and\nDiffusionPolicies use FiLM conditioning [37], whereas SuSIE uses the goal as a prompt."}, {"title": "3.1 Visuomotor and 3D Baselines", "content": "Our key result is that we show GENIMA\nan image-generation model fine-tuned to draw actions\nworks at all for visuomotor tasks. In the sections below, we go beyond this initial result and quantify\nGENIMA's performance against state-of-the-art visuomotor and 3D baselines.\nGENIMA outpeforms ACT, DiffusionPolicies, and SuSIE. Table 1 presents results from RLBench\nevaluations. GENIMA outperforms ACT [19, 23] in 16/25 tasks, particularly in tasks with occlusions\n(e.g., open window) and complex motions (e.g., turn tap). Against SuSIE [13], GENIMA per-\nforms better on 23/25 tasks, as SuSIE struggles to generate exact pixel-level details for goals. Dif-\nfusionPolicy [24] performs poorly in multi-task settings with joint-position control. We ensured that\nour implementation is correct by training DiffusionPolicy on just take lid off, which achieved a\nreasonable success rate of 75%, but we could not scale it to 25 tasks.\nRGB-to-joint agents approach the performance of 3D next-best-pose agents. Without 3D input\nand motion-planners, most prior works [42, 46] report zero-performance for RGB-only agents in\nRLBench. However, our results in Table 1 show that RGB-to-joint agents like GENIMA and ACT\ncan be competitive with 3D next-best-pose agents. GENIMA outperforms 3D Diffuser Actor in\n6/25 tasks, particularly in tasks with non-linear trajectories (e.g., open box, open door), and tiny\nobjects (e.g., turn on lamp). GENIMA also performs comparably (within 3%) on 3 more tasks:\ninsert usb, play jenga, toilet seat up, despite lacking priors. 3D Diffuser Actor performs\nbetter on most tasks, but training GENIMA for longer or with more data could bridge this gap."}, {"title": "3.2 Semantic and Spatial Generalization", "content": "While all evaluations in Section 3.1 train and test on the same environment, the key benefit of using\nimage-generation models is in improving generalization of visuomotor policies. In this section, we\nexamine semantic and spatial generalization aspects of visuomotor policies.\nGENIMA is robust to semantic perturbations on Colosseum tasks. We evaluate the same multi-\ntask GENIMA and ACT agents (from Section 3.1) on 6 perturbation categories in Colosseum [46]:\nrandomized object and part colors, distractor objects, lighting color and brightness variations, ran-\ndomized table textures, randomized scene backgrounds, and camera pose changes. Figure 4 presents\nresults from these perturbation tests. Despite being initialized with a pre-trained ResNet [47], ACT\noverfits and significantly drops in performance with changes to object color, distractors, lighting,\nand table textures. Whereas GENIMA has minimal drops in performance from an emergent property\nthat reverts scenes to canonical textures and colors from the training data. See supplementary videos\nfor examples. However, both methods fail to generalize to unseen camera poses.\nGENIMA extrapolates to spatial loca-\ntions with aligned image-action spaces.\nBy drawing actions on images, GEN-\nIMA keeps the image-space and action-\nspace aligned. This alignment has been\nshown to improve spatial generalization\nand data-efficiency in prior works [42, 48,\n49]. We observe similar benefits in Fig-\nure 5, where ACT struggles in the upper-\nright region with minimal training exam-\nples, but GENIMA succeeds in extrapolat-\ning to those locations."}, {"title": "3.3 Ablations and Sensitivity Analyses", "content": "We investigate factors that affect GENIMA's performance. We report average success rates from\nmulti-task GENIMA trained on 3 tasks: take lid off, open box, and slide block. Our key\nresults are presented in Figure 3, and the sections below summarize our findings.\nAbsolute joint-position is the best performing action-mode. Delta action-modes accumulate er-\nrors, and end-effector control through IK struggles with non-linear trajectories. Joint-position ac-\ntions are also more expressive, allowing for full-body control and other embodiments.\nLonger action sequence predictions are crucial. In line with prior works [23, 24], modeling\ntrajectory distributions requires predicting longer action sequences. We find that predicting K = 20\nactions is optimal, since observations are recorded at 20Hz in RLBench.\nLonger execution horizons avoid error accumulation. Shorter execution horizons lead to jerky\nmotions that put the robot in unfamiliar states. We find that executing all 20 actions works best.\nSDXL improves performance over SD. Larger base models such as SDXL-Turbo [27] have more\ncapacity to model action-patterns. Newer Transformer-based models [50] might scale even better."}, {"title": "3.4 Real-robot Evaluations", "content": "We validate our results by benchmark-\ning GENIMA and ACT on a real-robot\nsetup. Our setup consists of a Franka\nEmika Panda with 2 external and 2 wrist\ncameras. We train multi-task agents from\nscratch on 9 tasks with 50 demos per\ntask. These tasks involve dynamic be-\nhaviors (e.g., slide book), transparent\nobjects (e.g., lift bag), deformable ob-"}, {"title": "4 Related Work", "content": "Visuomotor agents map images to actions in an end-to-end manner [52, 53, 54]. ACT [23] uses a\ntransformer-based policy architecture [34] to encode ResNet [47] features and predict action-chunks.\nMT-ACT [19] extends ACT to the multi-task settings with language-conditioning. MVP [55] uses\nself-supervised visual pre-training on in-the-wild videos and fine-tunes for real-world robotic tasks.\nDiffusion Policy [24] uses the diffusion process to learn multi-modal trajectories. RT-2 [56] fine-\ntune vision-language models to predict tokenized actions. Octo [57] can be adapted to new sensory"}, {"title": "5 Conclusion and Limitations", "content": "We presented GENIMA, a multi-task agent that fine-tunes Stable Diffusion to draw joint-actions.\nOur experiments both in simulation and real-world tasks indicate that fine-tuned image-generation\nmodels are effective in visuomotor control. While this paper is a proof-of-concept, GENIMA could\nbe adapted to other embodiments, and also to draw physical attributes like forces and accelerations.\nGENIMA is quite capable, but not without limitations. Like all BC-agents, GENIMA only distills ex-\npert behaviors and does not discover new behaviors. GENIMA also uses camera calibration to render\ntargets, assuming the robot is always visible from some viewpoint. We discuss these limitations and\noffer potential solutions in Appendix J. But overall, we are excited about the potential of pre-trained\ndiffusion models in revolutionizing robotics, akin to how they revolutionized image-generation."}, {"title": "A RLBench Tasks", "content": "We select 25 out of 100 tasks from RLBench [22] for our simulation experiments. Only variation0\nis used to reduce training time with limited resources. In the following sections, we describe each\nof the 25 tasks in detail, including any modifications from the original codebase.\nA.1 Basketball in Hoop\nTask: Pick up the basketball and dunk it in the hoop.\nfilename: basketball_in_hoop.py\nModified: No.\nSuccess Metric: The basketball goes through the hoop.\nA.2 Insert USB in Computer\nTask: Pick up the USB and insert it into the computer.\nfilename: insert_usb_in_computer.py\nModified: No.\nSuccess Metric: The USB tip is inserted into the USB port on the computer.\nA.3 Move Hanger\nTask: Move the hanger from one rack to another.\nfilename: move_hanger.py\nModified: No.\nSuccess Metric: The hanger is hung on the other rack and the gripper is not grasping anything.\nA.4 Open Box\nTask: Grasp the lid and open the box.\nfilename: open_box.py\nModified: For the data efficiency experiment in Figure 6, we perform grid sampling of box poses\nfor both training and evaluation following R&D [49]. A grid size of 5cm \u00d7 20cm is used, with a yaw\nrotation range of 45\u00b0 around the z axis. All other experiments use the default random sampling.\nSuccess Metric: The joint between the lid and the box is at 90\u00b0.\nA.5 Open Door\nTask: Grip the handle and push the door open.\nfilename: open_door.py\nModified: No.\nSuccess Metric: The door is opened with the door joint at 25\u00b0.\nA.6 Open Drawer\nTask: Open the bottom drawer.\nfilename: open_drawer.py\nModified: No.\nSuccess Metric: The prismatic joint of the button drawer is fully extended.\nA.7 Open Grill\nTask: Grasp the handle and raise the lid to open the grill.\nfilename: open_grill.py\nModified: No.\nSuccess Metric: The lid joint of the grill cover reaches 50\u00b0."}, {"title": "A.8 Open Microwave", "content": "Task: Pull open the microwave door.\nfilename: open microwave.py\nModified: No.\nSuccess Metric: The microwave door is open with its joint reaches 80\u00b0.\nA.9 Open Washer\nTask: Pull open the washing machine door.\nfilename: open_washing_machine.py\nModified: No.\nSuccess Metric: The washing machine door is open with its joint reaches 40 \u00b0.\nA.10 Open Window\nTask: Rotate the handle to unlock the left window, then open it.\nfilename: open_window.py\nModified: No.\nSuccess Metric: The window is open with its joint reaches 30\u00b0.\nA.11 Phone On Base\nTask: Put the phone on the holder base\nfilename: phone_on_base.py\nModified: No.\nSuccess Metric: The phone is placed on the base and the gripper is not holding the phone.\nA.12 Pick Up Cup\nTask: Pick up the red cup.\nfilename: pick_up_cup.py\nModified: No.\nSuccess Metric: The red cup is picked up by the gripper and held within the success region.\nA.13 Play Jenga\nTask: Take the protruding block out of the Jenga tower without the tower toppling.\nfilename: play-jenga.py\nModified: No.\nSuccess Metric: The protruding block is no longer on the Jenga tower, the rest of the tower remains\nstanding.\nA.14 Press Switch\nTask: Flick the switch.\nfilename: press_switch.py\nModified: No.\nSuccess Metric: The switch is turned on.\nA.15 Push Button\nTask: Push down the maroon button.\nfilename: push_button.py\nModified: No.\nSuccess Metric: The maroon button is pushed down."}, {"title": "A.16 Put Books on Shelf", "content": "Task: Pick up books and place them on the top shelf.\nfilename: put_books_on_bookshelf.py\nModified: No.\nSuccess Metric: The books are on the top shelf.\nA.17 Put Knife on Board\nTask: Pick up the knife and put it on the chopping board.\nfilename: put_knife_on_chopping_board.py\nModified: No.\nSuccess Metric: The knife is on the chopping board and the gripper is not holding it.\nA.18 Put Rubbish in Bin\nTask: Pick up the rubbish and place it in the bin.\nfilename: put_rubbish_in_bin.py\nModified: No.\nSuccess Metric: The rubbish is inside the bin.\nA.19 Scoop with Spatula\nTask: Scoop up the block and lift it up with the spatula.\nfilename: scoop_with_spatula.py\nModified: No.\nSuccess Metric: The cube is within the success region, lifted up by the spatula.\nA.20 Slide Block to Target\nTask: Slide the block towards the green square target.\nfilename: slide_block_to_target.py\nModified: For the data efficiency experiment in Figure 6, we perform grid sampling of block poses\nfor both training and evaluation following R&D [49]. A grid size of 15cm \u00d7 40cm is used, with a\nyaw rotation range of 90\u00b0 around the z axis. All other experiments use the default random sampling.\nSuccess Metric: The block is in side the green target area.\nA.21 Take Lid off Saucepan\nTask: Take the lid off the saucepan\nfilename: take_lid_off_saucepan.py\nModified: For the data efficiency experiment in Figure 6, we perform grid sampling of saucepan\nposes for both training and evaluation following R&D [49]. A grid size of 35cm \u00d7 44cm is used,\nwith a yaw rotation range of 90\u00b0 around the z axis. All other experiments use the default random\nsampling.\nSuccess Metric: The lid is lifted off from the saucepan to the success region above it.\nA.22 Take Plate off Colored Dish Rack\nTask: Take the plate off the black dish rack and leave it on the tabletop.\nfilename: take_plate_off_colored_dish_rack.py\nModified: No.\nSuccess Metric: The plate is lifted off the black disk rack and placed within the success region on\nthe tabletop."}, {"title": "A.23 Toilet Seat Up", "content": "Task: Lift the lid of the toilet seat to an upright position.\nfilename: toilet_seat_up.py\nModified: No.\nSuccess Metric: The lid joint is at 90\u00b0.\nA.24 Turn on Lamp\nTask: Press the button to turn on the lamp.\nfilename: lamp_on.py\nModified: No.\nSuccess Metric: The lamp is turned on by pressing the button.\nA.25 Turn Tap\nTask: Grasp the left tap and turn it.\nfilename: turn_tap.py\nModified: No.\nSuccess Metric: The left tap is rotated by 90\u00b0 from the initial position."}, {"title": "B Real-World Tasks", "content": "We evaluate on 9 real-world tasks. In the following sections, we describe each of 9 tasks in detail,\nincluding tests we perform to assess out-of-distribution generalization. Figure 8 shows objects and\nscene perturbations.\nB.1 Lid Off\nTask: Take the lid off the saucepan.\nIn-Distribution: A black saucepan with an oval-shaped lid handle seen during training.\nOut-of-Distribution: An unnseen smaller saucepan with a round-shaped lid handle.\nSuccess Metric: The lid is picked up from the saucepan and placed on the right side.\nB.2 Place Teddy\nTask: Place the teddy into the drawer\nIn-Distribution: A beige-color teddy bear toy seen during training.\nOut-of-Distribution: An unseen blue plush toy.\nSuccess Metric: The toy is inside the drawer.\nB.3 Elbow Touch\nTask: Touch the red button with the elbow joint.\nIn-Distribution: The button is placed over a blue cloth seen during training.\nOut-of-Distribution: The button is placed over an unseen pink cloth.\nSuccess Metric: The robot touches the button with its elbow joint.\nB.4 Hang Scarf\nTask: Hang the scarf on the hanger.\nIn-Distribution: A seen green-and-black checkered scarf seen during training.\nOut-of-Distribution: An unseen red checkered scarf with a different thickness.\nSuccess Metric: The scarf hangs still on the lowest peg of the hanger."}, {"title": "B.5 Put Marker", "content": "Task: Put the highlighter into the mug.\nIn-Distribution: A highlighter and mug seen during training.\nOut-of-Distribution: The same highlighter and mug is moved around during execution.\nSuccess Metric: The highlighter is inside the mug.\nB.6 Slide Book\nTask: Slide the book to the drawer's edge and pick it up from the side.\nIn-Distribution: A book seen during training.\nOut-of-Distribution: The same book and scene but with darker lighting conditions.\nSuccess Metric: The book is lifted up from the drawer.\nB.7 Avoid Lamp\nTask: Pick up the sponge and place it inside the drawer without bumping into the obstacle.\nIn-Distribution: A sponge and lamp (as the obstacle) seen during training.\nOut-of-Distribution: The same sponge, but with either cup stand or water bottle as the obstacle.\nSuccess Metric: The sponge is placed into the drawer without bumping into the obstacle.\nB.8 Lift Bag\nTask: Lift up the plastic bag.\nIn-Distribution: A plastic bag seen during training.\nOut-of-Distribution: The same plastic bag, but placed on distractors of different heights.\nSuccess Metric: The bag is lifted up from the drawer.\nB.9 Flip Cup\nTask: Pick up the cup, rotate it, and place it in an upright position.\nIn-Distribution: A plastic wine glass seen during training.\nOut-of-Distribution: A unseen ceramic coffee cup.\nSuccess Metric: The cup is standing upright on the drawer."}, {"title": "C Hardware Setup", "content": "C.1 Simulation\nOur simulated experiments use a four-camera setup: front, left shoulder, right shoulder,\nand wrist. All cameras are set to default camera poses from RLBench [22] without any modifica-\ntions, except for the perturbation tests in Section 3.2.\nC.2 Real-Robot\nHardware Setup. Real-robot experiments use a 7-DoF Franka Emika Panda equipped with a\nRobotiq 2F-140 gripper. We use four RealSense D415 cameras to capture RGB images. Two\ncameras on the end-effector (upper wrist, lower wrist) to provide a wide field-of-view, and\ntwo external cameras (front, right shoulder) that are fixed on the base. We use a TARION\ncamera mount for the right shoulder camera. The extrinsics between the cameras and robot\nbase-frame are calibrated with the easy handeye packages in ROS.\nData Collection. We collect demonstrations for real-\nworld tasks using a joint-mirroring setup similar to\nALOHA [23]. Figure 9 shows the data collection setup. A\nLeader Franka is moved by the operator and the Follower\nFranka mirrors the Leader's movement in joint space. Vi-\nsual observations and joint states are recorded at 30 FPS.\nWhen training controllers, we set the action prediction\nhorizon to match the data recording frequency to avoid\nbig jumps or slow trajectory execution.\nC.3 Training and Evaluation Hardware\nThe diffusion agents of GENIMA and SuSIE [13], and the 3D Diffuser Actor [40] baseline were\ntrained on a single NVIDIA A100 GPU with 80GB VRAM. The controllers were trained on a single\nNVIDIA L4 GPU with 24GB VRAM. Evaluation inference for real-world agents was done on an\nNVIDIA GeForce RTX 3090 GPU with 24GB VRAM."}, {"title": "D ControlNet Overview", "content": "ControlNet [8] is a fine-tuning architecture that preserves the text-to-image capabilities of Stable\nDiffusion while following the spatial layout of a conditioning image. ControlNet has achieved com-\npelling results in several image-to-image domains such as sketch-to-image, normal-map-to-image,\ndepth-to-image, canny-edge-to-image, segmentations-to-image, and human-pose-to-image. Partic-\nularly, the method preserves spatial structures and can be trained on small datasets. This is achieved\nthrough a two-stream architecture similar to prior works like CLIPort [94].\nTwo-stream architecture. ControlNet's architecture is composed of two streams: frozen and train-\nable. The frozen-stream is a pre-trained copy of Stable Diffusion's UNet, whose parameters are kept\nfrozen throughout fine-tuning. The trainable-stream is another copy of the UNet's downsampling\nencoder, whose parameters are fine-tuned. The frozen-stream gets sampled latents, a prompt, and\ntime embeddings as input. The trainable-stream gets latents of the conditioning image (that is en-\ncoded with a frozen autoencoder), a prompt, and time embeddings as input. The two streams are\nconnected through zero-convolution layers where outputs from each layer of the trainable-stream\nare added to the decoder layers of the frozen-stream."}, {"title": "E ACT Overview", "content": "Action Chunking with Transformers (ACT) [23] predicts action chunks (or sequences) to reduce\nthe effective horizon of long-horizon tasks. This helps alleviate compounding errors in behavior-\ncloning when learning from human demonstrations. The chunk size is fixed at length K. Given an\nobservation, the model outputs the next K actions to be executed sequentially.\nArchitecture. Images are encoded with a pre-trained ResNet-18 [47]. The vision features from each\ncamera and proprioceptive features are then fed into a conditional variational autoencoder (CVAE).\nThe CVAE consists of a BERT-like [95] transformer encoder and decoder. The encoder takes in\nthe current joint position and target action sequence to predict the mean and variance of a style\nvariable z. This style variable z helps in dealing with multi-modal demonstrations. It is only used to\ncondition the action decoder during training and is discarded at test time by zeroing it out. The action\ndecoder is based on DETR [34], and is trained to maximize the log-likelihood of action chunks from\nhuman demonstrations using two losses: an action reconstruction loss and a KL regularization term\nto encourage a Gaussian prior for z.\nTemporal Smoothing. To avoid jerky robot motions, ACT [23] uses temporal ensembling at each\ntimestep. An exponential weighted scheme \\(w_i = exp(-m * i)\\) is applied to obtain a weighted\naverage of actions from overlapping predictions across timesteps, where \\(w_0\\) is the coefficient for the\noldest action and m controls the speed for incorporating new observations.\nOur modifications to ACT. We made several modifications to ACT [23] in our implementation to\nimprove data-efficiency and robustness:\n\u2022 Data augmentation: we use random-crops [36], color jitters, elastic transforms and Gaus-\nsian noise from Torchvision. The original ACT [23] does not use any data augmentation.\n\u2022 Sliding window sampling: we apply a sliding window along each demonstration trajectory\nto obtain action chunks. The original ACT [23] samples action chunks sparsely from each\ntrajectory. The sliding-window ensures full data-coverage every epoch.\n\u2022 Discrete gripper loss: we use cross entropy loss for gripper open and close actions instead\nof an L1-loss. This makes the prediction closer to how the data was collected.\n\u2022 Temporal ensemble smoothing: we do not use temporal smoothing for our ACT [23] con-\ntrollers. It oversmoothens trajectories, which reduces precision and recovery behaviors.\n\u2022 Transformer decoder features: the original implementation conditions action predictions\non only the first decoder layer, leaving some unused layers. We replace it with the last\ndecoder feature instead."}, {"title": "F Hyperparameters", "content": "In this section, we provide training and evaluation hyperparameters for GENIMA and other baselines.\nNote that GENIMA's controller, SuSIE [96], and the ACT [23] baseline all share the same hyperpa-\nrameters and augmentation settings for fair one-to-one comparisons. Real-world GENIMA and ACT\nagents also use the same hyperparameters (except for the camera setup)."}, {"title": "G Tiled vs. Non-Tiled Generation", "content": "Figure 10 shows an example of tiled vs non-tiled generation. Non-tiled generation only gets one\ncamera-view input at a time during diffusion. Without the full scenecontext, non-tiled generation\ntends to produce inconsistent and ambiguous predictions like duplicate elbow targets."}, {"title": "H Base Diffusion Models and Fine-Tuning Pipelines", "content": "GENIMA's formulation is agnostic to the choice of base Stable Diffusion model and also the fine-\ntuning pipeline. The SD-Turbo [27] base model used in GENIMA can be replaced with a bigger base\nmodel like SDXL-Turbo [27, 5] that is trained on larger images. Likewise, instead of fine-tuning\nwith ControlNet [8], we can also use Instruct-pix2pix [7]. See Figure 11 for examples."}, {"title": "I SUSIE Goal Predictions", "content": "Figure 12 shows examples of goal images generated by SuSIE [13] with fined-tuned ControlNet [8].\nIn general, SuSIE struggles to precisely predict pixel-level details of dynamic scenes with complex\nobject interactions such as turn tap and take plate off."}, {"title": "J Limitations and Potential Solutions", "content": "While GENIMA is quite capable, it is not without limitations. In the following sections, we discuss\nsome of these limitations and potential solutions.\nCamera extrinsics during training. To create a fine-tuning dataset, GENIMA relies on calibrated\ncameras with known extrinsics to render target spheres. While the calibration process is quick, it\ncan be difficult to obtain extrinsics for pre-existing datasets or in-the-wild data. A simple solution\ncould be to use camera pose-estimation methods like DREAM [99]. DREAM takes a single RGB\nimage of a known robot and outputs extrinsics with comparable error rates to traditional hand-eye\ncalibration.\nRobot visibility in observations. One strong assumption our method\nmakes is that the robot is always visible from some viewpoint in order\nto draw actions near joints. This assumption might not always hold,\nespecially in camera setups with heavy occlusion or wrist-only input. A\npotential solution could be to provide a virtual rendering of the robot-\nstate, which is commonly available from visualization and debugging\ntools like RViz. The virtual rendering can be tiled with observations\nsuch that the diffusion agent can incorporate both the virtual robot-state\nand observations. See Figure 13 for an illustration.\nSlow speed of diffusion agent. The diffusion agent runs at a consid-\nerably lower frequency (5 Hz) than the controller (50 Hz). This makes GENIMA less reactive and\nprone to error accumulation. But diffusion speed is unlikely to be a major issue in the future with\nrapid-advances from the image-generation community. Recent works like StreamDiffusion [100]\nrun at 91.07 Hz on a single NVIDIA RTX 4090.\nJerky motions. The actions generated by GENIMA, especially right after generating a new target\nimage, can sometimes result in jerky motions. This behavior is noticeable for some tasks in the\nsupplementary videos. Such behavior could be a result of the agent not being trained enough. We\nalso tried temporal ensembling [23] to smoothen outputs, but this hurt the ability to recover from\nmistakes. Future works could experiment with other smoothing techniques.\nController fails to follow targets. Sometimes the controller visibly fails to reach the target provided\nby the diffusion agent. This could be because the controller does not know how to reach the target\nfrom the current state, given its limited training data. One solution could be to pre-train the controller\nto reach arbitrary robot-configurations to maximize the workspace coverage.\nObject rotations. All RGB-to-joint agents in Section 3.1 struggle with tasks that randomize initial\nobject poses with a wide-range of rotations. For instance, in phone on base, GENIMA achieves\n19%, whereas 3D Diffuser Actor [40] achieves 94%. A small change to the phone's rotation, re-\nsults in widely different trajectories for picking and placing it on the base. This effect could make\nbehavior-cloning difficult. A potential solution could be to pre-train RGB-to-joint agents on tasks\nthat involve heavy rotations such that they acquire some rotation-equivariant behavior.\nDiscovering new behaviors. Like all behavior-cloning agents, GENIMA only distills behaviors\nfrom human demonstrations, and does not discover new behaviors. It might be possible to fine-tune\nGENIMA on new tasks with Reinforcement-Learning (RL). Furthermore, advances in preference\noptimization [96, 101] for Stable Diffusion models could be incorporated to shape new behaviors."}, {"title": "K Things that did not work", "content": "In this section, we briefly describe things we tried but did not work in practice.\nPredicting target spheres at fixed intervals. Instead of predicting spheres continuously at t + K\ntimesteps, we tried fixed intervals of K e.g., 20, 40, 60 etc. These fixed intervals act as waypoints,\nwhere the spheres hover in place until the target is reached (instead of always being K steps ahead).\nIn this setting, controllers cannot be trained with random backgrounds, because the trajectory be-\ntween fixed intervals is offloaded to the controller without any visual context. So we trained con-\ntrollers with full context, but found that these controllers tend to ignore target spheres, and directly\nuse the visual context for predicting actions.\nOther sphere shapes and textures. We experimented with a few vari-\nations of target spheres. Our goal was to design simple shapes that are\neasy to draw with image-generation models, without having to draw the\nfull robot with proper links and joints. Figure 14 illustrates an example\nin which we made the visual appearance more asymmetric. The sphere's\nsurface is divided into octants with different colors and black dots. The\ndots indicate gripper open and close actions. But in practice, we found\nthat a simple sphere with horizontal stripes works the best. Future works\ncould improve GENIMA's performance by simply iterating on the tar-\nget's appearance.\nController with discrete actions. We tried training controllers that output discrete joint actions\nsimilar to RT-2's [56] discrete end-effector actions. We discretized joint actions into bins. Each\njoint has its own prediction head and is trained with cross entropy loss. We found that the discrete\ncontrollers tend to generate smoother trajectories but lacks lack the precision for fine-grained tasks\nlike manipulating small door handles or pressing tiny buttons.\nObservations with rendered current joint-states. Instead of just RGB\nobservations as input to the diffusion agent, we experimented with ren-\ndering the current joint-states with spheres as a visual reference. This\nled to agents with significantly worse performance, likely due to the in-\nput spheres occluding important visual information in the scene.\nSegmenting target spheres for the controller. We experimented with\nproviding binary segmentation masks of the target spheres to the con-\ntroller. They had a negligible effect on success. Training with random\nbackgrounds seems sufficient to make the controller focus on targets.\nDepth-based ControlNet. We tried conditioning ControlNet on depth input instead of RGB. We\nused DepthAnything [102] to generate depth maps from RGB observations as shown in Figure 15.\nThe performance with depth was worse or comparable to RGB input. Future works could experiment\nwith fusing RGB and depth-based ControlNets [8]."}, {"title": "L Safety Considerations", "content": "Real-robot systems controlled with Stable Diffusion [1] requires thorough and extensive safety eval-\nuations. Internet pre-trained models exhibit harmful biases [103, 104], which may affect models\nfine-tuned for action prediction. This issue is not particular to Stable Diffusion, and even com-\nmonly used ImageNet-pretrained ResNets [47] exhibit similar biases. Potential solutions include\nsafety guidance [105] and detecting out-of-distribution or inappropriate generations with classifiers\nto pause action prediction and ask for human assistance. Keeping humans-in-the-loop with live vi-\nsualizations of action predictions, and incorporating language-driven feedback, could further help in\nmitigating issues."}]}