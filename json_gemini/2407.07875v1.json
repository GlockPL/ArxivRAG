{"title": "Generative Image as Action Models", "authors": ["Mohit Shridhar", "Yat Long Lo", "Stephen James"], "abstract": "Image-generation diffusion models have been fine-tuned to unlock new capabilities such as image-editing and novel view synthesis. Can we similarly unlock image-generation models for visuomotor control? We present GENIMA, \u0430 behavior-cloning agent that fine-tunes Stable Diffusion to \"draw joint-actions\" as targets on RGB images. These images are fed into a controller that maps the visual targets into a sequence of joint-positions. We study GENIMA on 25 RLBench and 9 real-world manipulation tasks. We find that, by lifting actions into image-space, internet pre-trained diffusion models can generate policies that outperform state-of-the-art visuomotor approaches, especially in robustness to scene perturbations and generalizing to novel objects. Our method is also competitive with 3D agents, despite lacking priors such as depth, keypoints, or motion-planners.", "sections": [{"title": "1 Introduction", "content": "Image-generation diffusion models [1, 2, 3] are generalists in producing visual-patterns. From photo-realistic images [4] to abstract art [5], diffusion models can generate high-fidelity images by distilling massive datasets of captioned images [6]. Moreover, if both inputs and outputs are in image-space, these models can be fine-tuned to unlock new capabilities such as image-editing [7, 8], semantic correspondences [9, 10], or novel view synthesis [11, 12]. Can we similarly unlock image-generation models for generating robot actions?"}, {"title": "2 GENIMA", "content": "GENIMA is a behavior-cloning agent that maps RGB observations \\(O_t\\) and a language goal g into joint-position actions \\(A_{t+K}^{joints}\\). The key objective is to lift actions into image-space such that internet-pretrained diffusion models can learn action-patterns as visual-patterns. We achieve this with a two-stage process: (1) fine-tune Stable Diffusion [1] to draw images with target joint-positions \\(A_{t+K}^{image}\\) that are K timesteps ahead, and (2) train a controller to translate these targets into to a sequence of executable joint-positions \\(A^{joints} = \\{a_{t_1}, a_{t_2}.. a_{t_K}\\}\\). This simple two-stage process offloads semantic and task-level reasoning to a generalist image-generation model, while the controller reaches nearby joint-positions indicated by the visual targets. The sections below describe the two stages in detail, and Figure 1 provides an overview."}, {"title": "2.1 Diffusion Agent", "content": "The diffusion agent controls what to do next. The agent takes RGB observations \\(O_t\\) and language goal g as input, and outputs an image with target joint-positions \\(A_{t+k}^{image}\\). This problem formulation is a classic image-to-image generation setup, so any diffusion fine-tuning pipeline [7, 28, 29] can be used. We specifically use ControlNet [8] to preserve spatial layouts and for data-efficient fine-tuning.\nFine-Tuning Data. To supervise the fine-tuning, we take expert demonstrations, and randomly sample observations and target joint-positions from t + K timesteps. For that timestep, we obtain 6-DoF poses of each robot joint, which is available through robot-APIs (from forward-kinematics). We place spheres at those poses with pyrender\u00b9, and render them on four camera observations \\(O_t = \\{o_{front}, o_{wrist}, o_{left}, o_{right}\\}\\) with known intrinsics and extrinsics. On a 7-DoF Franka Panda, we only render four joints: base, elbow, wrist, and gripper, to avoid cluttering the image. Each joint is represented with an identifying color, with separate colors for gripper open and close. The spheres include horizontal stripes parallel to the joint's rotation axis, acting as graduations indicating the degree of rotation. Only horizontal stripes are needed as each joint has only one rotation axis.\nFine-Tuning with ControlNet. Given an image-to-image dataset, we finetune Stable Diffusion [27] with ControlNet [8] to draw targets on observations. ControlNet is a two-stream architecture: one stream with a frozen Stable Diffusion UNet that gets noisy input and language descriptions, and a trainable second stream that gets a conditioning image to modulate the output. This architecture re-tains the text-to-image capabilities of Stable Diffusion, while fine-tuning outputs to spatial layouts in the conditioning image. GENIMA uses RGB observations as the conditioning image to draw precise targets. We use SD-Turbo [27] \u2013 a distilled model that can generate high-quality images within 1 to 4 diffusion steps \u2013 as the base model for fine-tuning. We use the HuggingFace implementation [30]2 of ControlNet without modifications. See Appendix D for more details on fine-tuning.\nTiled Diffusion. Fine-tuning Stable Diffusion on robot data poses three key challenges. Firstly, Stable Diffusion models work best with image resolutions of 512\u00d7512 or higher due to their training data. In robotics, large images increase inference latency. Secondly, multi-view generation suffers from inconsistencies across viewpoints. However, multi-view setups are crucial to avoid occlusions and improve spatial-robustness. Thirdly, diffusion is quite slow, especially for generating four target images at every timestep. Inspired by view-synthesis works [31, 32], we solve all three challenges with a simple solution: tiling. We tile four observations of size 256 \u00d7 256 into a single image of 512x 512. Tiling generates four images at 5 Hz on an NVIDIA A100 or 4 Hz on an RTX 3090."}, {"title": "2.2 Controller", "content": "The controller translates target images \\(A_{t+1}^{image}\\) into executable joint-positions \\(A_{t+1}^{joints}\\). The controller can be implemented with any visuomotor policy that maps RGB observations to joint-positions. We specifically use ACT [23, 33] \u2013 a Transformer-based policy architecture [34] \u2013 for its fast inference- speed and training stability. However, in theory, any framework like Diffusion Policies [24] or RL-based methods [35] can be used. Even classical controllers can be used if pose-estimates of target spheres are provided, but in our implementation, we opt for learned controllers for simplicity.\nTraining. During training, the controller receives current joint-positions, the language goal, and RGB images with ground-truth targets overlaid on random backgrounds. The random back- grounds, as shown in Figure 2, force ACT to follow targets and ignore any contextual information in the scene. We use the same hyperparameters and settings from the original ACT codebase\u00b3 with minor modifications. To improve robustness to fuzzy diffu- sion outputs, we augment images with random-crops [36], color jitters, elastic transforms, and Gaussian noise. We use L1 loss for joint-actions, and cross-entropy loss for gripper open and close actions. For language-conditioning, we use FiLM [37] layers following MT-ACT [19]. The controller is trained independently from the diffusion agent. See Appendix F for hyperparameters and Appendix E for controller details.\nInference. During inference, the controller gets target images from the diffusion agent. The con-troller predicts a sequence of K joint-actions, and executes K actions or less before querying the diffusion agent in a closed-loop fashion. The controller runs at ~ 50 Hz on an NVIDIA RTX 3090."}, {"title": "3 Experiments", "content": "We study GENIMA in both simulated and real-world environments. Specifically, we are interested in answering the following questions:\n\u00a7 3.1 How does GENIMA compare against state-of-art visuomotor policies and 3D baselines?\n\u00a7 3.2 What are the benefits of drawing actions with internet-pretrained image-generation models?\n\u00a7 3.3 Which factors affect GENIMA's performance?\n\u00a7 3.4 How well does GENIMA perform on real-world tasks?\nWe start with benchmarking our method in simulated environments for reproducible and fair com-parisons. The following sections describe our simulation setup and evaluation methodology.\nSimulation Setup. All simulated experiments are set in CoppeliaSim [38] interfaced through PyRep [39]. The robot is a 7-DoF Franka Emika Panda placed behind a tabletop. Observations are captured from four RGB cameras: front, left_shoulder, right_shoulder, and wrist, each with a resolution of 256 \u00d7 256. The robot is commanded with joint-position actions via PID control, or end-effector actions via an IK solver.\n25 RLBench Tasks. We choose 25 (out of 100) tasks from RLBench [22]. While most RLBench tasks are suited for discrete, quasi-static motions that benefit 3D next-best-pose agents [25, 26], we pick tasks that are difficult to execute with end-effector control. Tasks such as open box and open microwave require smooth non-linear motions that sampling-based motion-planners and IK solvers struggle with. Each RLBench task includes several variations, but we only use variation0 to reduce training time with limited resources. However, our method should be applicable to multi-variation settings without any modifications. We generate two datasets: 50 training demos and 50 evaluation episodes per task. For both datasets, objects are placed randomly, and each episode is sanity-checked for solvability. Language goals are constructed from instruction templates. See Appendix A for details on individual tasks.\nEvaluation Metric. Multi-task agents are trained on all 25 tasks, and evaluated individually on each task. Scores are either 0 for failures or 100 for successes, with no partial successes. We report average success rates on 50 evaluation episodes across the last three epoch checkpoints: 50\u00d73 = 150 episodes per task, which adds up to 150 \u00d7 25 = 3750 in total. We use a single set of checkpoints for all tasks without any task-specific optimizations or cherry-picking.\nVisuomotor Baselines. We benchmark GENIMA against three state-of-the-art visuomotor ap-proaches: ACT [19, 23], DiffusionPolicies [24], and SuSIE [13]. ACT is a transformer-based policy that has achieved compelling results in bimanual manipulation. Although GENIMA uses ACT as the controller, our controller has never seen RGB observations from demonstrations, just sphere targets with random backgrounds. DiffusionPolicies is a widely adopted visuomotor approach that generates multi-modal trajectories through diffusion. SuSIE is the closest approach to GENIMA, but instead of drawing target actions, SuSIE generates target RGB observations as goal images. We adapt SuSIE to our setting by training a controller that maps target and current RGB observations to joint-position actions. All multi-task baselines are conditioned with language goals. ACT and DiffusionPolicies use FiLM conditioning [37], whereas SuSIE uses the goal as a prompt."}, {"title": "3.1 Visuomotor and 3D Baselines", "content": "Our key result is that we show GENIMA \u2013 an image-generation model fine-tuned to draw actions \u2013 works at all for visuomotor tasks. In the sections below, we go beyond this initial result and quantify GENIMA's performance against state-of-the-art visuomotor and 3D baselines.\nGENIMA outpeforms ACT, DiffusionPolicies, and SuSIE. Table 1 presents results from RLBench evaluations. GENIMA outperforms ACT [19, 23] in 16/25 tasks, particularly in tasks with occlusions (e.g., open window) and complex motions (e.g., turn tap). Against SuSIE [13], GENIMA per-forms better on 23/25 tasks, as SuSIE struggles to generate exact pixel-level details for goals. Dif-fusionPolicy [24] performs poorly in multi-task settings with joint-position control. We ensured that our implementation is correct by training DiffusionPolicy on just take lid off, which achieved a reasonable success rate of 75%, but we could not scale it to 25 tasks.\nRGB-to-joint agents approach the performance of 3D next-best-pose agents. Without 3D input and motion-planners, most prior works [42, 46] report zero-performance for RGB-only agents in RLBench. However, our results in Table 1 show that RGB-to-joint agents like GENIMA and ACT can be competitive with 3D next-best-pose agents. GENIMA outperforms 3D Diffuser Actor in 6/25 tasks, particularly in tasks with non-linear trajectories (e.g., open box, open door), and tiny objects (e.g., turn on lamp). GENIMA also performs comparably (within 3%) on 3 more tasks: insert usb, play jenga, toilet seat up, despite lacking priors. 3D Diffuser Actor performs better on most tasks, but training GENIMA for longer or with more data could bridge this gap."}, {"title": "3.2 Semantic and Spatial Generalization", "content": "While all evaluations in Section 3.1 train and test on the same environment, the key benefit of using image-generation models is in improving generalization of visuomotor policies. In this section, we examine semantic and spatial generalization aspects of visuomotor policies.\nGENIMA is robust to semantic perturbations on Colosseum tasks. We evaluate the same multi-task GENIMA and ACT agents (from Section 3.1) on 6 perturbation categories in Colosseum [46]: randomized object and part colors, distractor objects, lighting color and brightness variations, ran-domized table textures, randomized scene backgrounds, and camera pose changes. Figure 4 presents results from these perturbation tests. Despite being initialized with a pre-trained ResNet [47], ACT overfits and significantly drops in performance with changes to object color, distractors, lighting, and table textures. Whereas GENIMA has minimal drops in performance from an emergent property that reverts scenes to canonical textures and colors from the training data. See supplementary videos for examples. However, both methods fail to generalize to unseen camera poses.\nGENIMA extrapolates to spatial locations with aligned image-action spaces. By drawing actions on images, GEN-IMA keeps the image-space and action-space aligned. This alignment has been shown to improve spatial generalization and data-efficiency in prior works [42, 48, 49]. We observe similar benefits in Fig-ure 5, where ACT struggles in the upper-right region with minimal training exam-ples, but GENIMA succeeds in extrapolat-ing to those locations."}, {"title": "3.3 Ablations and Sensitivity Analyses", "content": "We investigate factors that affect GENIMA's performance. We report average success rates from multi-task GENIMA trained on 3 tasks: take lid off, open box, and slide block. Our key results are presented in Figure 3, and the sections below summarize our findings.\nAbsolute joint-position is the best performing action-mode. Delta action-modes accumulate er-rors, and end-effector control through IK struggles with non-linear trajectories. Joint-position ac-tions are also more expressive, allowing for full-body control and other embodiments.\nLonger action sequence predictions are crucial. In line with prior works [23, 24], modeling trajectory distributions requires predicting longer action sequences. We find that predicting K = 20 actions is optimal, since observations are recorded at 20Hz in RLBench.\nLonger execution horizons avoid error accumulation. Shorter execution horizons lead to jerky motions that put the robot in unfamiliar states. We find that executing all 20 actions works best.\nSDXL improves performance over SD. Larger base models such as SDXL-Turbo [27] have more capacity to model action-patterns. Newer Transformer-based models [50] might scale even better."}, {"title": "3.4 Real-robot Evaluations", "content": "We validate our results by benchmark- ing GENIMA and ACT on a real-robot setup. Our setup consists of a Franka Emika Panda with 2 external and 2 wrist cameras. We train multi-task agents from scratch on 9 tasks with 50 demos per task. These tasks involve dynamic be- haviors (e.g., slide book), transparent objects (e.g., lift bag), deformable ob- jects (e.g., hang scarf), and full-body control (e.g., elbow touch). See Figure 7 and supplemen- tary videos for examples. Appendix B covers task details. When comparing GENIMA and ACT, we ensure that the initial state is exactly the same by using an image-overlay tool to position objects. Table 2 reports average success rates from 5 evaluation episodes per task. We also report out- of-distribution performance with unseen objects and scene perturbations. In line with Section 3.2, GENIMA is better than ACT at generalizing to out-of-distribution tasks. GENIMA also exhibits some recovery behavior from mistakes, but DAgger-style [51] training might improve robustness."}, {"title": "4 Related Work", "content": "Visuomotor agents map images to actions in an end-to-end manner [52, 53, 54]. ACT [23] uses a transformer-based policy architecture [34] to encode ResNet [47] features and predict action-chunks. MT-ACT [19] extends ACT to the multi-task settings with language-conditioning. MVP [55] uses self-supervised visual pre-training on in-the-wild videos and fine-tunes for real-world robotic tasks. Diffusion Policy [24] uses the diffusion process to learn multi-modal trajectories. RT-2 [56] fine-tune vision-language models to predict tokenized actions. Octo [57] can be adapted to new sensory"}, {"title": "5 Conclusion and Limitations", "content": "We presented GENIMA, a multi-task agent that fine-tunes Stable Diffusion to draw joint-actions. Our experiments both in simulation and real-world tasks indicate that fine-tuned image-generation models are effective in visuomotor control. While this paper is a proof-of-concept, GENIMA could be adapted to other embodiments, and also to draw physical attributes like forces and accelerations.\nGENIMA is quite capable, but not without limitations. Like all BC-agents, GENIMA only distills ex-pert behaviors and does not discover new behaviors. GENIMA also uses camera calibration to render targets, assuming the robot is always visible from some viewpoint. We discuss these limitations and offer potential solutions in Appendix J. But overall, we are excited about the potential of pre-trained diffusion models in revolutionizing robotics, akin to how they revolutionized image-generation."}, {"title": "A RLBench Tasks", "content": "We select 25 out of 100 tasks from RLBench [22] for our simulation experiments. Only variation0 is used to reduce training time with limited resources. In the following sections, we describe each of the 25 tasks in detail, including any modifications from the original codebase.\nA.1 Basketball in Hoop\nTask: Pick up the basketball and dunk it in the hoop.\nfilename: basketball_in_hoop.py\nModified: No.\nSuccess Metric: The basketball goes through the hoop.\nA.2 Insert USB in Computer\nTask: Pick up the USB and insert it into the computer.\nfilename: insert_usb_in_computer.py\nModified: No.\nSuccess Metric: The USB tip is inserted into the USB port on the computer.\nA.3 Move Hanger\nTask: Move the hanger from one rack to another.\nfilename: move_hanger.py\nModified: No.\nSuccess Metric: The hanger is hung on the other rack and the gripper is not grasping anything.\nA.4 Open Box\nTask: Grasp the lid and open the box.\nfilename: open_box.py\nModified: For the data efficiency experiment in Figure 6, we perform grid sampling of box poses for both training and evaluation following R&D [49]. A grid size of 5cm \u00d7 20cm is used, with a yaw rotation range of 45\u00b0 around the z axis. All other experiments use the default random sampling.\nSuccess Metric: The joint between the lid and the box is at 90\u00b0.\nA.5 Open Door\nTask: Grip the handle and push the door open.\nfilename: open_door.py\nModified: No.\nSuccess Metric: The door is opened with the door joint at 25\u00b0.\nA.6 Open Drawer\nTask: Open the bottom drawer.\nfilename: open_drawer.py\nModified: No.\nSuccess Metric: The prismatic joint of the button drawer is fully extended.\nA.7 Open Grill\nTask: Grasp the handle and raise the lid to open the grill.\nfilename: open_grill.py\nModified: No.\nSuccess Metric: The lid joint of the grill cover reaches 50\u00b0.\nA.8 Open Microwave\nTask: Pull open the microwave door.\nfilename: open_microwave.py\nModified: No.\nSuccess Metric: The microwave door is open with its joint reaches 80\u00b0.\nA.9 Open Washer\nTask: Pull open the washing machine door.\nfilename: open_washing_machine.py\nModified: No.\nSuccess Metric: The washing machine door is open with its joint reaches 40 \u00b0.\nA.10 Open Window\nTask: Rotate the handle to unlock the left window, then open it.\nfilename: open_window.py\nModified: No.\nSuccess Metric: The window is open with its joint reaches 30\u00b0.\nA.11 Phone On Base\nTask: Put the phone on the holder base\nfilename: phone_on_base.py\nModified: No.\nSuccess Metric: The phone is placed on the base and the gripper is not holding the phone.\nA.12 Pick Up Cup\nTask: Pick up the red cup.\nfilename: pick_up_cup.py\nModified: No.\nSuccess Metric: The red cup is picked up by the gripper and held within the success region.\nA.13 Play Jenga\nTask: Take the protruding block out of the Jenga tower without the tower toppling.\nfilename: play-jenga.py\nModified: No.\nSuccess Metric: The protruding block is no longer on the Jenga tower, the rest of the tower remains standing.\nA.14 Press Switch\nTask: Flick the switch.\nfilename: press_switch.py\nModified: No.\nSuccess Metric: The switch is turned on.\nA.15 Push Button\nTask: Push down the maroon button.\nfilename: push_button.py\nModified: No.\nSuccess Metric: The maroon button is pushed down.\nA.16 Put Books on Shelf\nTask: Pick up books and place them on the top shelf.\nfilename: put_books_on_bookshelf.py\nModified: No.\nSuccess Metric: The books are on the top shelf.\nA.17 Put Knife on Board\nTask: Pick up the knife and put it on the chopping board.\nfilename: put_knife_on_chopping_board.py\nModified: No.\nSuccess Metric: The knife is on the chopping board and the gripper is not holding it.\nA.18 Put Rubbish in Bin\nTask: Pick up the rubbish and place it in the bin.\nfilename: put_rubbish_in_bin.py\nModified: No.\nSuccess Metric: The rubbish is inside the bin.\nA.19 Scoop with Spatula\nTask: Scoop up the block and lift it up with the spatula.\nfilename: scoop_with_spatula.py\nModified: No.\nSuccess Metric: The cube is within the success region, lifted up by the spatula.\nA.20 Slide Block to Target\nTask: Slide the block towards the green square target.\nfilename: slide_block_to_target.py\nModified: For the data efficiency experiment in Figure 6, we perform grid sampling of block poses for both training and evaluation following R&D [49]. A grid size of 15cm \u00d7 40cm is used, with a yaw rotation range of 90\u00b0 around the z axis. All other experiments use the default random sampling.\nSuccess Metric: The block is in side the green target area.\nA.21 Take Lid off Saucepan\nTask: Take the lid off the saucepan\nfilename: take_lid_off_saucepan.py\nModified: For the data efficiency experiment in Figure 6, we perform grid sampling of saucepan poses for both training and evaluation following R&D [49]. A grid size of 35cm \u00d7 44cm is used, with a yaw rotation range of 90\u00b0 around the z axis. All other experiments use the default random sampling.\nSuccess Metric: The lid is lifted off from the saucepan to the success region above it.\nA.22 Take Plate off Colored Dish Rack\nTask: Take the plate off the black dish rack and leave it on the tabletop.\nfilename: take_plate_off_colored_dish_rack.py\nModified: No.\nSuccess Metric: The plate is lifted off the black disk rack and placed within the success region on the tabletop.\nA.23 Toilet Seat Up\nTask: Lift the lid of the toilet seat to an upright position.\nfilename: toilet_seat_up.py\nModified: No.\nSuccess Metric: The lid joint is at 90\u00b0.\nA.24 Turn on Lamp\nTask: Press the button to turn on the lamp.\nfilename: lamp_on.py\nModified: No.\nSuccess Metric: The lamp is turned on by pressing the button.\nA.25 Turn Tap\nTask: Grasp the left tap and turn it.\nfilename: turn_tap.py\nModified: No.\nSuccess Metric: The left tap is rotated by 90\u00b0 from the initial position."}, {"title": "B Real-World Tasks", "content": "We evaluate on 9 real-world tasks. In the following sections, we describe each of 9 tasks in detail, including tests we perform to assess out-of-distribution generalization. Figure 8 shows objects and scene perturbations.\nB.1 Lid Off\nTask: Take the lid off the saucepan.\nIn-Distribution: A black saucepan with an oval-shaped lid handle seen during training.\nOut-of-Distribution: An unnseen smaller saucepan with a round-shaped lid handle.\nSuccess Metric: The lid is picked up from the saucepan and placed on the right side.\nB.2 Place Teddy\nTask: Place the teddy into the drawer\nIn-Distribution: A beige-color teddy bear toy seen during training.\nOut-of-Distribution: An unseen blue plush toy.\nSuccess Metric: The toy is inside the drawer.\nB.3 Elbow Touch\nTask: Touch the red button with the elbow joint.\nIn-Distribution: The button is placed over a blue cloth seen during training.\nOut-of-Distribution: The button is placed over an unseen pink cloth.\nSuccess Metric: The robot touches the button with its elbow joint.\nB.4 Hang Scarf\nTask: Hang the scarf on the hanger.\nIn-Distribution: A seen green-and-black checkered scarf seen during training.\nOut-of-Distribution: An unseen red checkered scarf with a different thickness.\nSuccess Metric: The scarf hangs still on the lowest peg of the hanger."}, {"title": "C Hardware Setup", "content": "C.1 Simulation\nOur simulated experiments use a four-camera setup: front, left shoulder, right shoulder, and wrist. All cameras are set to default camera poses from RLBench [22] without any modifica-tions, except for the perturbation tests in Section 3.2.\nC.2 Real-Robot\nHardware Setup. Real-robot experiments use a 7-DoF Franka Emika Panda equipped with a Robotiq 2F-140 gripper. We use four RealSense D415 cameras to capture RGB images. Two cameras on the end-effector (upper wrist, lower wrist) to provide a wide field-of-view, and two external cameras (front, right shoulder) that are fixed on the base. We use a TARION camera mount for the right shoulder camera. The extrinsics between the cameras and robot base-frame are calibrated with the easy handeye packages in ROS.\nData Collection. We collect demonstrations for real- world tasks using a joint-mirroring setup similar to ALOHA [23]. Figure 9 shows the data collection setup. A Leader Franka is moved by the operator and the Follower Franka mirrors the Leader's movement in joint space. Vi- sual observations and joint states are recorded at 30 FPS. When training controllers, we set the action prediction horizon to match the data recording frequency to avoid big jumps or slow trajectory execution.\nC.3 Training and Evaluation Hardware\nThe diffusion agents of GENIMA and SuSIE [13], and the 3D Diffuser Actor [40] baseline were trained on a single NVIDIA A100 GPU with 80GB VRAM. The controllers were trained on a single NVIDIA L4 GPU with 24GB VRAM. Evaluation inference for real-world agents was done on an NVIDIA GeForce RTX 3090 GPU with 24GB VRAM."}, {"title": "D ControlNet Overview", "content": "ControlNet [8] is a fine-tuning architecture that preserves the text-to-image capabilities of Stable Diffusion while following the spatial layout of a conditioning image. ControlNet has achieved com-pelling results in several image-to-image domains such as sketch-to-image, normal-map-to-image, depth-to-image, canny-edge-to-image, segmentations-to-image, and human-pose-to-image. Partic-ularly, the method preserves spatial structures and can be trained on small datasets. This is achieved through a two-stream architecture similar to prior works like CLIPort [94].\nTwo-stream architecture. ControlNet's architecture is composed of two streams: frozen and train-able. The frozen-stream is a pre-trained copy of Stable Diffusion's UNet, whose parameters are kept frozen throughout fine-tuning. The trainable-stream is another copy of the UNet's downsampling encoder, whose parameters are fine-tuned. The frozen-stream gets sampled latents, a prompt, and time embeddings as input. The trainable-stream gets latents of the conditioning image (that is en-coded with a frozen autoencoder), a prompt, and time embeddings as input. The two streams are connected through zero-convolution layers where outputs from each layer of the trainable-stream are added to the decoder layers of the frozen-stream.\nZero-convolution connections regulate the flow of information from the trainable-stream to the frozen-stream. Zero-convolution layers are 1 \u00d7 1 convs initialized with zeroed weights and biases. At the start of the fine-tuning process, the trainable-stream makes no contribution to the final output because of the zero-intialization. But after fine-tuning, the trainable layers modulate the output to follow the spatial layout in the conditioning image.\nTraining Loss. ControlNet is trained with a standard diffusion loss that predicts noise added to a noise image. This is implemented as an L2-loss on the latents. For more details on the training process, refer to the original ControlNet paper [8]."}, {"title": "E ACT Overview", "content": "Action Chunking with Transformers (ACT) [23] predicts action chunks (or sequences) to reduce the effective horizon of long-horizon tasks. This helps alleviate compounding errors in behavior-cloning when learning from human demonstrations. The chunk size is fixed at length K. Given an observation, the model outputs the next K actions to be executed sequentially.\nArchitecture. Images are encoded with a pre-trained ResNet-18 [47]. The vision features from each camera and proprioceptive features are then fed into a conditional variational autoencoder (CVAE). The CVAE consists of a BERT-like [95] transformer encoder and decoder. The encoder takes in the current joint position and target action sequence to predict the mean and variance of a style variable z. This style variable z helps in dealing with multi-modal demonstrations. It is only used to condition the action decoder during training and is discarded at test time by zeroing it out. The action decoder is based on DETR [34], and is trained to maximize the log-likelihood of action chunks from human demonstrations using two losses: an action reconstruction loss and a KL regularization term to encourage a Gaussian prior for z.\nTemporal Smoothing. To avoid jerky robot motions, ACT [23] uses temporal ensembling at each timestep. An exponential weighted scheme \\(w_i = exp(-m * i)\\) is applied to obtain a weighted average of actions from overlapping predictions across timesteps, where \\(w_0\\) is the coefficient for the oldest action and m controls the speed for incorporating new observations.\nOur modifications to ACT. We made several modifications to ACT [23] in our implementation to improve data-efficiency and robustness:\n\u2022 Data augmentation: we use random-crops [36], color jitters, elastic transforms and Gaus-sian noise from Torchvision. The original ACT [23] does not use any data augmentation.\n\u2022 Sliding window sampling: we apply a sliding window along each demonstration trajectory to obtain action chunks. The original ACT [23] samples action chunks sparsely from each trajectory. The sliding-window ensures full data-coverage every epoch.\n\u2022 Discrete gripper loss: we use cross entropy loss for gripper open and close actions instead of an L1-loss. This makes the prediction closer to how the data was collected.\n\u2022 Temporal ensemble smoothing: we do not use temporal smoothing for our ACT [23] con-trollers. It oversmoothens trajectories, which reduces precision and recovery behaviors.\n\u2022 Transformer decoder features: the original implementation conditions action predictions on only the first decoder layer, leaving some unused layers7. We replace it with the last decoder feature instead."}, {"title": "F Hyperparameters", "content": "In this section, we provide training and evaluation hyperparameters for GENIMA and other baselines. Note that GENIMA's controller, SuSIE [96], and the ACT [23] baseline all share the same hyperpa- rameters and augmentation settings for fair one-to-one comparisons. Real-world GENIMA and ACT agents also use the same hyperparameters (except for the camera setup)."}, {"title": "G Tiled vs. Non-Tiled Generation", "content": "Figure 10 shows an example of tiled vs non-tiled generation. Non-tiled generation only gets one camera-view input at a time during diffusion. Without the full scene context, non-tiled generation tends to produce inconsistent and ambiguous predictions like duplicate elbow targets."}, {"title": "H Base Diffusion Models and Fine-Tuning Pipelines", "content": "GENIMA's formulation is agnostic to the choice of base Stable Diffusion model and also the fine- tuning pipeline. The SD-Turbo [27] base model used in GENIMA can be replaced with a bigger base model like SDXL-Turbo [27, 5] that is trained on larger images. Likewise, instead of fine-tuning with ControlNet [8], we can also use Instruct-pix2pix [7]. See Figure 11 for examples."}, {"title": "I SUSIE Goal Predictions", "content": "Figure 12 shows examples of goal images generated by SuSIE [13] with fined-tuned ControlNet [8]. In general, SuSIE struggles to precisely predict pixel-level details of dynamic scenes with complex object interactions such as turn tap and take plate off."}, {"title": "J Limitations and Potential Solutions", "content": "While GENIMA is quite capable, it is not without limitations. In the following sections, we discuss some of these limitations and potential solutions.\nCamera extrinsics during training. To create a fine-tuning dataset, GENIMA relies on calibrated cameras with known extrinsics to render target spheres. While the calibration process is quick, it can be difficult to obtain extrinsics for pre-existing datasets or in-the-wild data. A simple solution could be to use camera pose-estimation methods like DREAM [99"}]}