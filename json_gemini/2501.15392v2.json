{"title": "Faster Configuration Performance Bug Testing with Neural Dual-level Prioritization", "authors": ["Youpeng Ma", "Tao Chen", "Ke Li"], "abstract": "As software systems become more complex and configurable, more performance problems tend to arise from the configuration designs. This has caused some configuration options to unexpectedly degrade performance which deviates from their original expectations designed by the developers. Such discrepancies, namely configuration performance bugs (CPBugs), are devastating and can be deeply hidden in the source code. Yet, efficiently testing CPBugs is difficult, not only due to the test oracle is hard to set, but also because the configuration measurement is expensive and there are simply too many possible configurations to test. As such, existing testing tools suffer from lengthy runtime or have been ineffective in detecting CPBugs when the budget is limited, compounded by inaccurate test oracle.\nIn this paper, we seek to achieve significantly faster CP-Bug testing by neurally prioritizing the testing at both the configuration option and value range levels with automated oracle estimation. Our proposed tool, dubbed NDP, is a general framework that works with different heuristic generators. The idea is to leverage two neural language models: one to estimate the CPBug types that serve as the oracle while, more vitally, the other to infer the probabilities of an option being CPBug-related, based on which the options and the value ranges to be searched can be prioritized. Experiments on several widely-used systems of different versions reveal that NDP can, in general, better predict CPBug type in 87% cases and find more CPBugs with up to 88.88\u00d7 testing efficiency speedup over the state-of-the-art tools.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern software systems typically have a high degree of configurability wherein the configuration options directly (e.g., certain optimization) or indirectly (e.g., resource allocation) af-fect software performance, such as throughput and latency [1]\u2013[7]. As software configurability continues to improve, they are also more likely to be buggy. We refer to these performance bugs caused by configuration errors as Configuration Perfor-mance Bugs (CPBugs). It is worth noting that CPBugs differ from the typical misconfigurations that concern user-induced configuration errors [8]; instead, they are the errors of the configuration design in the source code that are unintentionally introduced by the developers of the systems [2]. A typical example of CPBugs has been illustrated in Table I. Here, we can see that the option read_buffer_size in MYSQL is mainly used to change the size of the read buffer allocated for each sequential table scan request. In the developers' expectation, increasing this option value should allow MYSQL to cache more results, hence a larger buffer should improve performance. However, the performance actually drops when increasing the value beyond 256K. This is because, in the code logic, My I SAM initializes an IO_CACHE for writing and uses read_buffer_size for bulk inserts. my_malloc is called with the MY_ZEROFILL flag, which causes memset to be called on the size of read_buffer_size, hence mis-takenly restricting the permitted memory quote for processing SQL commands and resulting in a large performance decrease.\nCPBugs can lead to devastating outcomes [2], [9], [10]. For example, there have been several large-scale flight delays, which were mainly caused by problematic configuration de-signs of the systems [11]. Systems from Google and Meta [12], [13] have also suffered performance degradation or outage due to configuration issues, leading to a huge loss of revenue.\nHowever, testing and finding CPBugs are challenging, due primarily to the fact that (1) there are simply too many configurations to examine (MYSQL has hundreds of options with more than millions of configurations [14]); (2) measuring configuration performance is highly expensive, e.g., it can take up to 166 minutes to merely measure one configuration on MARIADB [15]; and (3) the test oracle is often unclear, i.e., we do not know when a CPBug occurs. While some tools exist for testing CPBugs [2], [16], [17], they are limited in the efficiency of testing and the accuracy of oracle estimation. That is, they have not effectively handled the discriminative importance of the options and their value range with respect to the CPBugs, together with restricted rule-driven oracle inference. Therefore, those tools suffer from issues such as long running times or are ineffective in detecting CPBugs"}, {"title": "II. PRELIMINARIES", "content": "In general, CPBugs naturally incur from the mismatch between the expected performance (as specified in the docu-mentation) and the actual performance observed by changing a configuration option. Therefore, measuring the performance deviation between the source and target option value serves as a strong oracle for identifying the CPBugs. In particular, the performance deviations that cause the CPBugs can be mainly observed from a common scenario: the direction of expected and actual performance changes is different, e.g., the expectation is performance raises but the actual effect is a performance drop\u00b9 under at least one workload, implying defects in the code segments of the configuration option.\nWe follow the categorization of the CPBug types proposed by He et al. [2], as articulated in Table II and below:\n\u2022 Optimization Switch: When enabled, an optimization strategy is activated, and the performance is expected to improve. Yet, a performance drop implies a likely CPBug.\n\u2022 Non-functional Trade-off: Configuration options are used to balance the performance and other non-functional needs, such as the ACID properties. Whether the option needs to be increased or decreased is case-dependent. This involves two subtypes (see Type-2 in Table II). In this work, we do not distinguish these two as the threshold for the \"drop beyond expectation\" is highly subjective. Instead, for an option under this type, there is a CPBug as long as a performance drop is observed\u00b2.\n\u2022 Resource Allocation: Options influence resource alloca-tion; more resources are expected to boost performance.\n\u2022 Functional Switch: Options control non-performance functionalities but indirectly affect the system's perfor-mance. When an option disables a function, system performance usually improves. This also involves two situations (see Type-4 in Table II). We do not distinguish those two cases due to the same aforementioned reason."}, {"title": "C. Testing CPBugs", "content": "In essence, CPBug testing aims to generate diverse test cases, represented as a pair of configurations (a source c, and a target ct), which differ only on the configuration option to be tested, i.e., o at index 1 (highlighted in red):\nCs = {0, 10, 15, 43, 1}\nct = {0, 20, 15, 43, 1}\nAn automated CPBug testing tool would perturb the values of the option o, such that the performance deviation from the source configuration to the target one under at least one workload matches with a CPBug type from Table II, which serves as the oracle. For example, if changing from cs to ct (from a smaller value of a resource option to a larger value) under a workload causes a performance drop while in the documentation it should have been a rise, then we find a CPBug of Type-3. Yet, due to the large number of options and their values, testing CPBugs is extremely expensive."}, {"title": "III. CHARACTERISTICS OF CPBUGS", "content": "CPBugs naturally come with certain characteristics that can help us design more effective testing. From the systems/ver-sions/options tested in this work, which are taken from a prior study [2] and it is worth noting that the number of total options per system we tested has already exceeded what is considered for them. We have discovered in Table III\u00b3 that:\nCharacteristic 1: Overall, only 13.39% of the con-figuration options can trigger CPBugs.\nThis means that, although CPBugs can be devastating, testing on all unique options to find them is not cost-effective. The numeric configuration options also have known charac-teristics. For example, He et al. [2] have shown that:\nCharacteristic 2: The majority of the numeric con-figuration options studied can trigger CPBugs when they are changed to near one extreme of their values.\nFrom Table IV, we have identified a similar pattern from the systems tested: when fixing the source of an option as close to either its maximal or minimal values, all the 18 numerical options can trigger CPBugs when the option in the target is"}, {"title": "IV. DUALLY PRIORITIZED CPBUG TESTING WITH NEURAL LANGUAGE MODEL", "content": "NDP seeks to expedite the CPBug testing via dual-prioritization at two levels: the option level that determines which option to test earlier and the search space level of the numeric options, which sets the order of search space to explore. This is supported by two fine-tuned neural language models, one for estimating the probability of an option being CPBug-related and the other for predicting the most relevant CPBug type that determines the test oracle. Similar to the other tools [2], NDP tests the system's configuration option one by one, in which all the combinations of workloads and related versions are also examined in turn. Specifically, we design the two phases in NDP, as shown in Figure 1. For Initialization, NDP focuses on a one-off process that fine-tunes two neural language models using existing data from different systems:\n\u2022 CPBug Types Prediction: Here, the goal is for a neural language model to parse the documentation and predict which CPBug type is most relevant to an option. This then serves as the essential oracle for CPBug testing.\n\u2022 Option-CPBugs Relevance Estimation: We fine-tune another neural language model that takes both the de-scription of options from the documentation and the related code snippet as inputs and estimates the probabil-ities of those options being CPBug-related. This allows us to handle the identified characteristics of CPBugs.\nIn the Testing phase, NDP contains four components:\n\u2022 Options Prioritization (high-level): The probabilities of whether the options are CPBug-related are used to prioritize their testing order (due to Characteristic 1).\n\u2022 Exhaustive Generator: This is mainly for non-numeric options in which all the possible pairs will be covered under all workloads and related versions considered.\n\u2022 Search Prioritization (low-level): For numeric options, the actual testing would also need to be conducted via a certain search depth. In NDP, we design three search depths, which are prioritized differently depending on the commonality of the probabilities for being CPBug-related. Each of the search depths would trigger an independent run of the heuristic generator, which can be any search algorithm, to generate the test cases. This fits with Characteristic 2 and Characteristic 3. According to taint analysis of code semantic, buffer-related numeric options are specifically handled given Characteristic 4.\n\u2022 Heuristic Generator: A stochastic search algorithm that samples the values of numeric options in CPBug testing under all workloads and related versions.\nFor each option under a version, if its value alteration and the performance change (on any concerned performance attribute) in both configurations of a pair match with the pattern in the predicted CPBug type (which serves as the oracle) under at least one workload, then we found a CPBug;"}, {"title": "A. Neural CPBug Types Inference", "content": "An essential task in CPBug testing is to determine the oracle, i.e., identifying what types of CPbugs an option is most likely to be associated with, hence triggering the corresponding way to verify whether such an option can trigger CPBugs. To that end, we leverage a single-modal ROBERTa (denoted Ms), a particular type of neural language model, to predict the CPBug type of a given option to be tested. In NDP, we fine-tune the RoBERTa using the data collected from previous work [2], such that the inputs are the natural description of an option from the documentation with a known label from the five CPBug type or a label of \u201cno CPBug\u201d, since CPBugs are mainly related to the deviation from the expected performance of an option specified in the documentation (e.g., Table VI). Notably, the fine-tuning process is naturally cross-project since the naturalness of the documents ensures its generalization. In particular, RoBERTa is chosen for three reasons:\n\u2022 Compared with the rule mining approach [2], it exhibits a stronger generalization ability that can learn hidden information in the documentation in the latent space. In Section VI-A, we will experimentally verify this.\n\u2022 In contrast to LLM, ROBERTa fits our problem better: we need a classification of CPBugs type rather than text generation. Further, ROBERTa is more cost-effective [21].\n\u2022 It has been reported that RoBERTa is the generally most promising BERT variant for software engineering [22]."}, {"title": "B. Neural Multi-Modal Option-CPBugs Relevance Estimation", "content": "From Characteristic 1, we note that only a small number of options can potentially be the cause of CPBugs. As a result, a natural idea is to estimate which options are more related to CPBugs. NDP leverage both documentation and the corre-sponding code of an option (see Table VII), together with the corresponding label of whether the option is CPBug related, to fine-tune another multi-modal ROBERTa model, Mm, in a binary classification problem. Our goal here, however, is not to use the model to make a binary prediction but to extract its probability related to the likelihood of an option being CPBug-related, based on which we can rank those options. As such, forming this binary classification to train yet another new ROBERTa model has the benefit of simplicity without producing much noise to fulfill our goal.\nTo that end, we locate the code for a corresponding option in the documentation using a pattern matching-based heuristic:\n\u2022 Direct way: Some systems have a centralized file (or a few files) to maintain all configuration options, such as MYSQL. In those cases, we look at the variable with a similar name to those in the documentation and identify the relevant code snippets from the centralized file(s).\n\u2022 Indirect way: Other systems might not have a file(s) that share the same name as those in the documentation. We consider two cases: (1) there are mechanisms that allow access to those configuration variables via setter()\nand getter(). In those cases, we can write a script\nthat searches through the relevant files and outputs the\nsimilarity of the setter() and getter() to each\noption in the documentation. We can then manually\nidentify the corresponding code snippets. (2) there are\nno setter()/getter(), in which case we scan all\nvariables and related functions in the related files.\nOnce the code snippets and the texts of an option are mapped, our heuristic uses the rules below to clean the code:\n\u2022 Remove useless comments, e.g., those with timestep, certain license information, etc.\n\u2022 Remove duplicated comments or code snippets.\n\u2022 Remove usage examples of the options in the comments.\ne.g., formats or order of changes.\n\u2022 Remove code snippets of incomplete functions extracted.\nIn NDP, the texts from documentation and the code snippets of an option are concatenated together (e.g., Table VII) and we use standard steps such as text cleaning, tokenization, and serialization to parse the data. Those inputs, together with a label of whether the option is CPBug-related, form the data to fine-tune the ROBERTa model. To make ROBERTa work for our simplify binary classification problem, we add a task-specific classification layer with a cross-entropy loss function in the fine-tuning process. Upon predicting a given option, the probability of being a true label (CPBug-related) extracted from the softmax layer is what we are interested in. Again, we use the CPBugs data that has been reported previously [2]."}, {"title": "C. All Options Prioritization (High-level)", "content": "In NDP, at the high level, we firstly leverage the probabilities of all the options produced by Mm to determine their order in CPBug testing. This is important as if an option is more likely to cause the CPBugs, then prioritizing it beforehand would help us to identify the bugs quicker. Here, although we do not distinguish the type of options in this prioritization, e.g., numeric and non-numeric ones, their actual testing strategies can be different: for non-numeric options, we generate and test all the combinatorial values of the configurations in the pair using an exhaustive generator, since often those possible values are of limited range [23]. Notably, all workloads and related versions are considered: we at first pick a version and test all workloads therein in turn; if a CPBug is found for a non-numeric option under a workload, then the remaining untested workloads would be skipped and we switch to the next related version. Finally, NDP moves to the next option when all related versions have been tested for the current option."}, {"title": "D. Search Prioritization for Numeric Options (Low-level)", "content": "When the option to be tested is a numeric option, we propose three different search depths that bound the search spaces of the underlying heuristic generator:\n\u2022 Extreme search: As in Figure 2a, this is the search with the most restricted search space: the search for test cases happens within 10% of the upper/lower bounds range of values for both configurations in the pair. Yet, we ensure that the two configurations in the pair are searched over the opposed bounds (Characteristic 2).\n\u2022 Midmost search: Here, in Figure 2b, one configuration in the pair is searched within 10% of the upper or lower bound range values while the other can be changed within the middle 80% of the values (Characteristic 3).\n\u2022 Comprehensive search: This is the search that basically means all possible values of the configurations in the pair can be explored (Figure 2c).\nThose search depths differ in terms of the number of tests (for the pairs) required. According to the probabilities produced from Mm for the numeric options, we can then prioritize how the above three search depths are used on them. In NDP, the idea is to divide the probabilities of all numeric options into three divisions, based on which different prioritization of the search depth is used. To systematically perform such a division, we leverage the Gaussian Kernel Density Estimation (GKDE). In essence, GKDE serves as a one-dimensional binning algorithm that divides the probability density of options being CPBug-related into three bins with no thresholds. This is achieved by dividing the options into the top three peaks, which are separated by a local trough, based on their closeness of probabilities to those peaks.\nFigure 3 shows an example: we see that the numeric options are divided into three divisions according to their commonality on the probabilities of being CPBug-related. These divisions derive three prioritizations of the search depths:\n\u2022 High-density: For the numeric options belonging to the most frequent bin, we prioritize the more restricted search depth: starting from extreme search, midmost search, and finally comprehensive search. This will speedup testing if CPBugs can be detected within an extreme/middle value.\n\u2022 Medium-density: Here, we prioritize the midmost search first, followed by the extreme search, and then the com-prehensive search. The reason is that since the numeric options are of medium density, we start from the midmost search that also assumes a medium size of search space.\n\u2022 Low-density: For the least frequent bin of numeric options, we adopt the comprehensive search only.\nAccording to Characteristic 4, numeric options that control buffers are most likely to cause CPBugs at their extreme or middle values. Hence, in NDP, we adopt taint tracking using the option as the source while the buffer-related operations as the sink (e.g., release_sysvar_source_service() for MYSQL), hence analyzing the code semantic to identify whether an option controls buffer. These sinks, which are system-dependent, are domain knowledge specified by soft-ware engineers. For all buffer-related numeric options, we force their search to follow the depth order of high-density.\nFor a given numeric option, NDP follows the steps below:\n1) Pick a related version of the system under test.\n2) If the numeric option controls buffer, then make it uses the order of search depths for high-density; otherwise, following the order of the assigned density level.\n3) Test the option with the order of search depths via the heuristic generator for all workloads (see Section IV-E).\n4) If a CPBug is found, then jump to 5); otherwise, return to 2) and move to the next search depth.\n5) Repeat from 1) for the next related version, if any; otherwise, move to the next option."}, {"title": "E. Heuristic Generator", "content": "NDP can be paired with any heuristic algorithms for gen-erating test cases for the numeric options. In this work, we use the population-based Genetic Algorithm (GA) [25], but it can be easily replaced by other algorithms. Since NDP tests one numeric option under a version each time, the solution representation is a pair of values for the tested option.\nTo determine which pairs to preserve, we use the fitness function below to compare the pairs:\nfitness = max i\u2208{1...m} |f(c1, Wi) \u2013 f(c2, Wi)|\nwhereby c\u2081 and c2 are the configurations in a pair with different values on the numeric option to be tested; w\u2081 denoted the ith workloads out of a total of m ones. Since based on the CPBug types, either configurations in the pair can be the source and the options would trigger CPBugs if the performance drops, this fitness reflects the maximum perfor-mance deviation between the two paired configurations with different values of the tested numeric option across different workloads the larger the deviation, the higher possibility of triggering more CPBugs, which should be preserved in testing.\nUnder each of the above search depths, the search space of GA is bounded correspondingly. Whether a configuration in the pair starts from the upper or lower sides (for extreme search); or whether it is searched on the middle range (for midmost search) is decided randomly. When GA consumes all of its budget or all pairs within the bound have been explored, NDP terminates the GA and checks whether a CPBug has been found according to the estimated CPBug type."}, {"title": "F. Handling Dependency", "content": "When changing the tested options, their dependencies need to be complied [26]. For example, in MYSQL, there is a dependency that option innodb_buffer_pool_size (the buffer pool size) must be set as an integer product of that of the option innodb_buffer_pool_chunk_size (the granularity of buffer pool resizing) while being greater.\nNDP leverages GPTuner [27] a large language model-based tool that predicts configuration dependency based on the documentation. If, when a value of the tested option violates any dependency, we then (randomly) change the other affected option correspondingly. For example, if we change the value of innodb_buffer_pool_chunk_size to 128MB, then we should also set the innodb_buffer_pool_size to a value that is an integer product of 128MB, e.g., 128MB, 256MB, or 384MB. Note that, in that case, if either of the two options triggers CPBug, then both are CPBug-related with the same CPBug type. We chose GPTuner for two reasons:\n\u2022 it is highly flexibility and can be conveniently used without any fine-tuning.\n\u2022 thanks to the GPT3.5, it is generalizable to different systems. This is the key advantage compared with other rule-based tools such as cDep [28]."}, {"title": "V. EXPERIMENTS SETUP", "content": "In this work, we answer the following research questions:\n\u2022 RQ1: How well can NDP estimate the CPBug types?\n\u2022 RQ2: How effective dose NDP in options prioritization against the state-of-the-art tools?\n\u2022 RQ3: How well dose the prioritized search in NDP per-form over the state-of-the-art tools?\n\u2022 RQ4: Can NDP discover unknown CPBugs?\nIn this work, we use the datasets of 12 systems provided by He et al. [2] for assessing the CPBug type prediction. For the actual testing, we conduct experiments on five widely used configurable systems therein with known CPBugs, as shown in Table VIII. The reason is that we have not been able to reproduce the CPBugs for all 12 systems used by He et al. [2] because, e.g., the related versions are discarded; or the CPBugs have not been documented clearly. Yet, the five systems used for testing are still of diverse domains and scales, including database systems (i.e., MYSQL and MARIADB), web servers (i.e., APACHE), and compilers (i.e., GCC and CLANG).\nTo reproduce the CPBugs in testing, we test the options of each system under various versions. Note that not all the options would go through the same versions, since some do not exist in certain versions, hence NDP maintains a mapping"}, {"title": "A. Research Questions", "content": "B. Systems, Versions, Workloads, and Known CPBugs"}, {"title": "A. RQ1: CPBugs Type Estimation", "content": "To examine oracle prediction via RQ1, we compare NDP with CPD and KS under the same 500 samples from 12 systems used by He et al. [2] with no sampling method change, following the same training (fine-tuning)/testing splits for 10-fold cross-validation. He et al. [2] state that those are randomly sampled from the systems, but they have ensured data quality and representative nature. The mean recall, pre-cision, and F1 scores for each of the five CPBug types are reported, i.e., a total of 15 types/metrics."}, {"title": "2) Results:", "content": "From Table IX, we clearly see that the neural language model in NDP achieves considerably better results than the others, particularly on the F1 score, leading to superior results on 13 out of 15 types/metrics. In particular, the KS is clearly insufficient due to the limitation of a human-defined keywords sets; CPD is also restricted by the rule mining capability: due to the naturalness, the vast ways of describing the potential CPBugs in the documentation cannot be fully captured by the rules identified. Indeed, CPD marginally"}, {"title": "B. RQ2: Tested Option Prioritization", "content": "To verify high-level option prioritization in RQ2, we use five systems (and their versions) for which we have successfully reproduced the CPBugs. We compare NDP against CPD and US, which test the options in random order. The mean/deviation of the cumulative number of CP-Bugs found with respect to the number of options tested for each system over 10 runs are reported. We also calculate the speedup of NDP via o/o', where o is the number of options tested to find the most CPBugs by the other tool and o' is the number of tested options tested for NDP to achieve the same."}, {"title": "2) Results:", "content": "As can be seen in Figure 4, NDP can reveal more CPBugs for MYSQL and APACHE due to the prioritiza-tion in testing numeric options (if we compare the last point), which we will evaluate in RQ3. More importantly, it runs CPBug testing with much better efficiency: we see that for all systems, NDP exhibits much stepper slops, meaning that many more CPBugs are discovered in an earlier stage of the testing a significant contribution made by the prioritization at the tested options level. In particular, NDP achieves speedup range from 1.11x to 1.73\u00d7 against both CPD and US.\nNotably, testing a single option can be rather expensive, i.e., 2 hours on average; some can be days (since we need to go through many versions/workloads), hence there will be significant savings if we can find the same (or more) CPBugs"}, {"title": "C. RQ3: Search Prioritization for Numeric Options", "content": "In RQ3, we examine the search prioritization when testing numeric options on the systems/versions from RQ2 (only MYSQL, MARIADB, and APACHE contain nu-meric options) against others. Since the number of tests-testing a pair of configurations is one test is crucial for testing numeric options, for all systems, we report on the mean/deviation of the cumulative number of CPBugs found along with the number of tests for numeric options across 10 runs. All tools follow the same order of testing the numeric options prioritized by NDP: among all the numeric options of each system, NDP remarkably prioritizes the CPBug-related ones before the others. We measure the cumulative CPBugs found for every 10% tests (rounded) when the number of tests required is greater than 10; otherwise, we report every test. We calculate the speedup of NDP via the same way as for RQ2."}, {"title": "2) Results:", "content": "Figure 5 shows the traces of testing nu-meric options. Clearly, we see that NDP exhibits remarkably better results compared with the others: it discovers the same (MARIADB) or more numeric options-related CPBugs (MYSQL and APACHE) than CPD and US, e.g., 10 for NDP while the other two can only find 7 CPBugs on MYSQL. In particular, NDP achieves such with significantly less number of tests for all three systems, NDP does so with as few as 4-13 tests while CPD and US need 21\u201380 tests and 104\u2013711 tests to reach their maximum number of CPBugs, respectively. Notably, to find the same maximum number of CPBugs as achieved by others, NDP has 3.13\u00d7 to 10.50\u00d7 and 14.38\u00d7 to 88.88\u00d7 speedup over CPD and US, respectively.\nSince a single test run is highly expensive, the saving is thereby significant. Table XI shows the least number of tests/clock time required to find per CPBug: NDP only needs 1 test runs (as small as 1.1\u20134.4 minutes) against the 2.5\u201388.8 test runs (4.25-432.4 minutes) for the state-of-the-art tools.\nAll above demonstrate the effectiveness of the search level prioritization for numeric options in NDP. Thus, we conclude:"}, {"title": "D. RQ4: Detecting New CPBugs", "content": "To verify whether NDP can reveal unknown CPBugs, we apply NDP to further extended sets of versions"}, {"title": "VII. DISCUSSION: WHY NDP WORKS?", "content": "RQ1-RQ3 serve as the ablation analysis of NDP. Here, we further explain why NDP work with a qualitative analysis."}, {"title": "A. Predicting CPBugs Types", "content": "A key benefit of the neural language model in NDP is the significant reduction of false negatives compared with CPD and KS. For example, option innodb_fill_factor for MYSQL has the description of \u201cinnodb_fill_factor defines the percentage of space on each B-tree page that is filled during a sorted index build, with the remaining space reserved for future index growth. For example, setting innodb_fill_factor to 80 reserves 20 percent of the space on each B-tree page for future index growth...\". This option should belong to Type-2 since both a too small or a too large value could downgrade the performance as the former creates many recursions while the latter processes too many pages. Yet, for consistency, a smaller value is preferred since fewer pages need to be maintained, hence there is a trade-off. However, the description has no clear pattern to indicate such, hence both CPD and KS have wrongly classified it as Type-3 due to the presence of the word \"space\". NDP, in contrast, has\""}, {"title": "B. Prioritizing Options", "content": "To understand why prioritizing at the options level helps NDP to significantly improve the testing efficiency, Figure 6 plots the first few options to be tested by all approaches. We see that, with NDP, the prioritized options generally have higher probabilities of being CPBug-related than those of the other two, within which up to 60% options can trigger CPBugs (for MYSQL) while only one option from CPD and US can do so (for GCC). This considerably impacts the CPBug testing."}, {"title": "C. Prioritizing Search", "content": "We also analyze why prioritizing the different bounds of search space when testing the numeric options can help. We found that the extreme search and mid-most search are of great benefit therein. For example, innodb_buffer_pool_size is a CPBug-related numeric option on MYSQL. Yet, such a CPBug can only be discovered when we change it from a near-minimal value, i.e., 10MB, to one that is closer to its maximum value, i.e., 256GB. With NDP, such an option is categorized as the high-density region (Characteristic 2 and 4), thus NDP would prioritize its bounds as extreme search first. This fits perfectly with its range of values that causes a CPBug, since with the extreme search, a configuration in a pair would be explored within 10% close to its minimum value while the other would take a value close to 10% of its maximum extreme. In contrast, CPD would fix one configuration to the option value of 1MB while increasing the other as 2MB, 4MB, and 8MB, etc, each pair of which needs to be tested. Unlike the others, US does not use a heuristic as it aims to sample randomly and uniformly. Therefore, for options like innodb_buffer_pool_size, NDP needs significantly less number of tests to reveal the CPBugs compared with the others, which might even fail to find the CPBugs due to exhaustion of budget."}, {"title": "VIII. THREATS TO VALIDITY", "content": "Threats to internal validity: We set the parameters either adopting pragmatic values or following widely-used defaults, e.g., the inner budget of 100 tests for GA under a bound is a pragmatic setting, achieving a good balance between quality and cost. For confirming performance drop in the oracle, we set a minimum of 5% change as prior work [19]. However, we agree that some settings might not be the best.\nThreats to external validity: For evaluating the estimation of CPBugs types, we use prior datasets of 12 systems [2]. For testing the CPBugs, we use five systems with reproduced CPBugs. In both cases, the systems are of diverse languages, domains, and scales. We have also considered a wide range of workloads (2-10) and versions (4-31), which are the most commonly used ones from existing work [2], [34]\u2013[36]. Indeed, more subjects might strengthen the conclusion.\nThreats to construct validity: We use several metrics, including precision, recall, and F1 score, together with the trajectory of finding CPBugs and the best efficiency of each approach. Yet, unintended programming errors or mis-considerations are always possible."}, {"title": "IX. RELATED WORK", "content": "A vast amount of early work has been conducted to un-derstand the implications of configurations for performance issues. For example, Jin et al. [37] and Han et al. [38] reveal that 59% of the performance problems can be traced back to configuration errors. Xiang et al. [39] further suggest that configuration option documentation is a significant resource for analyzing configuration-related performance expectations, which serve as a foundation for identifying CPBug oracle. Those studies provide insights into how configuration caused performance issues while NDP automatically testing CPBugs."}, {"title": "A. Implication of Configuration to Performance Issues", "content": "B. Performance Bug Testing\nThere exist tools that detect general performance bugs using a fixed set of patterns, such as loops and memory access [17", "40": [44], "45": "propose a GA-based testing framework with contrast data mining. However, they are not related to configurations.\nAmong configuration-related testing approaches, ctest [8", "19": ""}]}