{"title": "OPMOS: Ordered Parallel Multi-Objective Shortest-Path", "authors": ["Leo Gold", "Adam Bienkowski", "Krishna Pattipati", "David Sidoti", "Omer Khan"], "abstract": "The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimal solutions from a start node to a destination node in a multi-attribute graph. To solve the NP-hard MOS problem, the literature explores heuristic multi-objective A*-style algorithmic approaches. A generalized MOS algorithm maintains a \"frontier\" of partial paths at each node and performs ordered processing to ensure that Pareto-optimal paths are generated to reach the goal node. The algorithm becomes computationally intractable as the number of objectives increases due to a rapid increase in the non-dominated paths, and the concomitantly large increase in Pareto-optimal solutions. While prior works have focused on algorithmic methods to reduce the complexity, we tackle this challenge by exploiting parallelism using an algorithm-architecture approach. The key insight is that MOS algorithms rely on the ordered execution of partial paths to maintain high work efficiency. The OPMOS framework, proposed herein, unlocks ordered parallelism and efficiently exploits the concurrent execution of multiple paths in MOS. Experimental evaluation using the NVIDIA GH200 Superchip shows the performance scaling potential of OPMOS on work efficiency and parallelism using a real-world application to ship routing.", "sections": [{"title": "Introduction", "content": "In many optimization problems, several distinct (and often competing) objectives need to be optimized. For example, when planning a road trip, one may wish to minimize the driving distance, driving time, and cost of tolls along the route. Similarly, when planning a journey by sea, one may be interested in the fastest and the most fuel-efficient routes, but deciding the right trade-off may depend on the urgency of the matter and the meteorological and oceanographic (METOC) environment.\nThis paper explores the NP-hard multi-objective shortest-path (MOS) problem, a generalization of the well-known (and polynomial) single-source shortest-path problem [28]. Given a weighted graph with non-negative edge weights, the shortest path problem computes the minimum-cost path from a start node to a goal/destination node in the graph [33]. In a multi-objective setting, each edge is given a non-negative cost vector (constant length for each edge in a graph), with each element corresponding to an objective. When these objectives compete, generally no single path can optimize all the objectives simultaneously. MOS aims to find a set of Pareto-optimal (non-dominated) solution paths, where a path is Pareto-optimal if no single objective of the path can be improved without causing at least one of the other objectives to deteriorate in quality. For example, (5,4) and (4,5) can be Pareto-optimal path costs to a node. During execution, the intermediate path cost vectors form the so-called Pareto-optimal labels that comprise potential candidate solutions. However, computing this front is computationally hard [2, 5, 20, 28], even for two objectives [7]. As the number of objectives increases, so does the computational complexity and the number of Pareto-optimal solution paths [25].\nTo efficiently compute an exact or approximated Pareto-optimal front, algorithmic solutions are developed based on the multi-objective extension of the A* algorithm, originally designed for the single-objective search [15, 32, 35]. Multi-objective A* (MOA*), unlike A* which exits once the first solution is found, needs to store a set of Pareto-optimal solution paths to the goal node. MOA* maintains a priority queue with lexicographical ordering of candidate paths (or labels) to guarantee that only the globally Pareto-optimal paths to nodes are processed. It terminates once the queue is empty.\nA New Approach to Multi-Objective A* (NAMOA*) [15] uses consistent heuristics to improve over MOA*, and handles an arbitrary number of objectives [25, 26].\nIn a single-objective shortest path, there can be only one minimum solution path cost for each node in the graph. However, MOS does not have a single solution path guarantee since multiple non-dominated paths can exist from the start node to any other node in the graph. If a label l(v) is defined to be a path cost from the source node to a node v, when a new"}, {"title": "Related Work", "content": "The MOS problem is well-studied from an algorithmic perspective, and it is known that generating an exact Pareto front is NP-hard [28]. Alternative genetic and evolutionary algorithms [1, 13, 40, 44] have been explored in the literature but they suffer from computational inefficiencies and poor explainability of the quality of their solutions. Therefore, researchers have focused mostly on tackling the algorithmic complexity of the generative Pareto front approaches as summarized in a recent survey paper [26]. Algorithmic techniques have been explored to reduce the runtime complexity or approximate the Pareto front using label-setting or label-correcting approaches. Martin's algorithm [16] is a label-setting algorithm that extends single-objective Dijkstra to the multi-objective setting. MOA* [32] introduces A* to the multi-objective domain. Since then many improvements over MOA* have been explored in the literature, with NAMOA* [15] serving as the basis of most modern advancements [26]. The algorithmic enhancements include: dimensionality reduction (NAMOA*-dr [24]), lazy versus eager dominance checks (BOA* [35]), and enhanced data structures (EMOA* [25]), among others [26].\nWhile these algorithmic optimizations have been proposed, most focus on two or three objectives due to the exploding size of the search space induced by the larger number of objectives [25, 26]. NAMOA* remains one of the only modern MOS algorithms that applies to an arbitrary number of objectives, establishing itself as the baseline for this paper. However, the proposed ordered parallel model applies to other MOS algorithms as we will explore in future work. Due to the NP-hard nature of the exact generative algorithms, approximations to the Pareto front have also been explored to lower the complexity at higher objective counts through runtime state-space reductions. Warburton [39] introduces an e-based procedure that allows approximate dominance checks to enable pruning of paths within an e-bounded range. Several optimizations to the approximation strategy have been introduced [2, 3, 10, 34]. The quality of solutions is impacted with approximations, introducing a trade-off between runtime efficiency and solution quality [42]. However, the e-optimal approximation enables the NAMOA* to push the limits on the number of objectives.\nSo far, all discussed strategies tackle the MOS complexity from an algorithmic perspective. Parallel MOS is an under-explored method of handling the complexity as the number of objectives increases [26]. Sanders and Mandow [27] present a parallel variant of Martin's algorithm in the bi-objective setting. It constructs a true Pareto queue to allow parallel extraction of all globally Pareto-optimal labels at each iteration, which the authors assert is not practical for more than two objectives. Focusing on theoretical analysis, this paper does not introduce an implementation or experiments and notes that the proposed algorithm may not be practical. Others attempt parallelization by launching multiple MOS instances with different lexicographical orderings of objectives [26]. However, there is no known single-instance parallel MOS model in the literature capable of handling an arbitrary number of objectives. In this paper, we aim to address the complexity problem for higher numbers of objectives using ordered parallelism.\nOrdered graph processing is the cornerstone of extracting parallelism in modern graph applications [9, 21]. Various concurrent priority schedulers for graph analytics have been introduced in the literature, ranging from hardware-centric to software approaches. Swarm [14] and its variant Hive [22] propose speculative execution of tasks in hardware to achieve super-linear speedups for task parallel graph problems, such as single-source shortest paths (SSSP). HD-CPS [29] proposes a hardware-software co-design to trade off work efficiency and parallelism in concurrent priority schedulers for task parallel graph processing. Many CPU and GPU frameworks have also been proposed for ordered graph processing, such as MBQ [41], Galois [18], GraphIt [43], and Gunrock [38]. Recently, researchers have explored specialized approaches for hard-to-scale graph problems (like SSSP) using GPU architecture-specific optimizations [37]. While all these"}, {"title": "Background and Complexity of MOS", "content": "Consider an input graph G = (V, E, c) with a set of nodes V, edges E, where each edge has a set of cost values in c with d objectives. For each edge e \u2208 E, there is a non-negative cost vector c(e) of length d. Given a source node us and a goal node ug in the graph G, the path from us to an intermediate node vi is defined as \u03c0(vi), represented by a sequence of nodes where each node is connected to its predecessor on the path. For each path \u03c0(vi), \u011d(\u03c0(vi)) denotes the path cost from us to vi, calculated as the sum of the cost vectors c(e) for all edges present on the path. Since multiple objectives may compete, MOS introduces a dominance check such that given two paths a = \u03c0\u2081(u), b = \u03c0\u2082(u) with d objectives, a dominates b (denoted a > b) if and only if \u011d(a) [i] \u2264 \u011d(b) [i], Vi \u2208 1, 2, ..., d, and \u011d(a)[i] < \u011d(b)[i], \u2203i \u2208 1, 2, ..., d. All non-dominated paths from us to ug constitute the Pareto-optimal solution set. MOS aims to find a cost-unique Pareto-optimal solution set where no two paths in the subset have the same cost vector.\nA few additional terms must be introduced to describe the MOS framework. A label l = (v, \u011d) is a tuple containing a node v \u2208 V and a cost vector \u011d. This represents an intermediate solution path from us to v with a cost vector g. For simplicity, we denote v(1) to be the vertex and \u011d(l) to be the cost vector contained in l. A label l is dominated by another label l' if they share the same vertex (v(1) = v(l')) and \u011d(1) \u2265 \u011d(l'). A heuristic vector \u0125(v) is an admissible heuristic such that it dominates (less than or equal for all objectives) all Pareto-optimal solutions from node v to the goal node [15]. A vector f(l) denotes the estimated path cost from the start node to the goal node for a given label, calculated as f(1) = \u011d(l) + \u0125(v(l)). Let OPEN be a queue of labels prioritized by F(I) in increasing lexicographic order. For each vertex u \u2208 V, let a(u) denote the frontier set at node u, holding all non-dominated labels l at node u. Each label in a(u) is a non-dominated partial solution path from vs to u. In NAMOA*, a is split into two sets GOP and GCL, the open and closed sets, respectively. Here, Gop contains a per-node set of all partial solution labels in OPEN, while GCL contains the remaining non-dominated solution labels in the frontier set of each node. Every label in a(u) can be found in"}, {"title": "Characterization and Motivation", "content": "There are no standard benchmarks for evaluating multi-objective shortest-path and search algorithms, thus making it difficult to characterize computational challenges and compare results [26]. Many representative graph datasets, such as road networks, are too big for MOS to handle due to the rapid explosion of the state space with increasing numbers of objectives [25]. TMPLAR attempts to solve this challenge through the use of a variety of graph state-space reduction techniques [31, 42]. A forward and backward single-source shortest path (SSSP) is performed to compute a bounding box of reachable nodes to reduce the search space while also creating a time expansion of the graph to account for weather conditions over time. Then, an admissible heuristic for NAMOA* is evaluated using SSSP for each objective and populates the edge weights. Using these techniques, TMPLAR generates directed spatio-temporal graphs with >10 objectives for the maritime ship routing application. The details about the routes and objectives are discussed in the methodology Section 6. However, to characterize the computation challenges, the NAMOA* algorithm is evaluated for the sequential MOS Algorithm 1 using the representative TMPLAR route."}, {"title": "OPMOS Framework", "content": "Ordered Parallel Multi-Objective Shortest-Path (OPMOS) builds upon the MOS framework in Alg. 1, introducing parallel computations in a load-balanced manner while maintaining as close to the global priority of label extractions as possible. At a high level, the main worker extracts a set of labels from a queue of prioritized labels in lexicographic order (OPEN) and distributes them among workers for parallel execution using a MOS-centric load-balancing scheduler. The extraction and parallel execution of labels is done asynchronously to hide the latency of serial label extractions from OPEN. The individual workers perform the relevant dominance and pruning checks and create updates for the frontier sets and OPEN. The label-centric parallel approach leads to the possible creation of duplicate label updates and labels that may dominate others during the concurrent execution of workers. Since the label updates are performed serially in the main worker to ensure consistency, the volume of these updates can become a bottleneck. Parallel duplicate and dominance checks are proposed to reduce the update volume in the critical code section.\nAlgorithm 2 presents the pseudo-code for the proposed OPMOS framework. The data structures in OPMOS remain mostly unchanged from Alg. 1. However, OPEN, GOP, GCL, and P are initialized in shared memory for efficient parallel access by workers. Four new data structures, OPEN_INS, OPEN_DEL, GCL_DEL, and P_INS are also initialized in shared memory to allow each worker to track its local updates during parallel execution of labels. These data structures are maintained for each worker so they process their updates independently and enable parallel duplicate and dominance checks among the updates. The bag data structure, B contains the metadata for the set of labels to be processed in parallel workers. The labels are extracted from the priority queue (OPEN) using a user-specified number of labels, set as the num_to_pop parameter. The work done for each label in B is not only graph-dependent, its complexity grows with the number of objectives and the associated dominance and pruning checks leading to load imbalance. At each iteration, the high-priority labels in B execute out-of-order in each worker exploiting the work-efficiency and parallelism trade-off discussed in Section 4. OPMOS uses this insight to explore a novel load-balancing scheduler for efficient label-centric parallel execution.\nOPMOS executes in three phases at each iteration, as described in Algorithm 2. In Phase 1, the main worker distributes high-priority labels in B to workers for parallel processing. Immediately following work distribution, the main worker moves to Phase 2 where labels are extracted from OPEN and stored in B for the next iteration. This allows decoupled (asynchronous) execution of OPEN extractions and parallel label processing, enabling OPMOS to hide the latency of expensive priority-queue operations. After extracting labels from OPEN, the main worker moves to Phase 3 and waits for parallel workers to return with updates. Once all worker updates are received and processed, the next OPMOS iteration commences. Execution continues until there is no more work to be done. This phased execution of OPMOS is discussed in more detail next."}, {"title": "Phase 1: Label Distribution & Parallel Execution", "content": "OPMOS initiates execution by inserting the start node ls = (vs, \u00d4) into the B bag (lines 3-4). After that B contains the labels for parallel processing in each iteration, and the algorithm continues as long as the bag has active labels for processing (line 5). Details on how labels are inserted in B are discussed in the next Section 5.2. A na\u00efve load balancer simply distributes each label or a set of labels in B among workers for parallel execution. However, labels at the goal node, ug may have more complexity than regular labels. The full-index searches in PruneOPEN and PruneGOP (lines 9-10 in Alg. 1) are expensive (cf. Section 3), resulting in a long critical path of computations. If these expensive searches are not handled in parallel and instead performed alongside other labels, it leads to a significant load imbalance. On the other hand, if predominantly goal node labels are processed in iterations and the amount of exploitable parallelism is limited in them, then it is beneficial to distribute these labels alongside other labels. A quantitative comparison against the na\u00efve scheduler is presented in Section 7. However, the proposed OPMOS scheduler assumes goal node labels are expensive and treats them separately from other labels for balanced work distribution among workers.\nAt the start of Phase 1, if a goal node is present in B (line 8), then a goal node iteration is kicked off to process that label in a load-balanced manner. The goal node label is first removed from B, potentially postponing the other labels in B for processing in the next iteration (line 10). In the ProcessGoalNode procedure (line 11), the PruneOPEN and PruneGOP functions and P-dominance and inserts (lines 9-12 in Alg. 1) are performed in parallel. Since OPEN and Gop store the same labels, only one must be searched and pruned. GOP is organized on a labels-per-node granularity. This allows load distribution to be performed on Gop by splitting the nodes in the graph into equal chunks and allowing each worker to compute only the sections of Gop for its assigned range of nodes. The node-centric distribution allows the full-index search to be spread across multiple workers. However, this can suffer from load balancing challenges when there is insufficient parallelism due to few labels in OPEN/GOP per node or high load imbalance due to a few nodes with a high number of labels. A more advanced scheduler that explores more fine-grained parallelism and load balancing will be explored in future work. After load distribution, the ProcessGoalNode stores the relevant updates for each worker in OPEN_DEL and P_INS, and once each worker finishes its work, it synchronizes on a barrier. This execution phase is discussed in Section 5.3.\nWhen B does not contain a label corresponding to the goal node, all labels are removed and distributed among the parallel workers for execution (lines 12-14). A na\u00efve load balancer distributes each label to a worker. However, there is a variation in the amount of work performed for each"}, {"title": "Phase 2: Asynchronous Label Extraction", "content": "Extractions from OPEN are increasingly expensive at high numbers of objectives due to the higher complexity in the lexicographic ordering of labels and the higher number of active labels in OPEN. To hide the latency, OPEN extractions are decoupled from label processing. In Phase 2, at each iteration, labels are sequentially removed from OPEN and Gop, and inserted into GCL and the bag B (lines 18-20). If a label corresponding to the goal node, ug, is extracted (line 21), then the isGoalNode flag in B is set, and the label extractions are paused until the next iteration. Otherwise, the label extractions are continued until either num_to_pop labels are inserted in B or OPEN has been emptied.\nThe main worker operates Phase 2 concurrently with parallel label processing initiated in Phase 1. The decoupled execution allows the latency of OPEN extractions to be hidden. However, depending on the workload of a given iteration, there may not be enough time to hide the latency of (expensive) OPEN extractions. Additionally, since the extraction of labels and their respective updates are now separated across contiguous iterations, this may lead to work inefficiency due to the relaxation of the order of label processing. A na\u00efve approach to label extraction would be to perform serial extractions at the start of each iteration, and then send the bag of labels to the parallel workers (i.e., swap Phase 1 and Phase 2). To quantify, Section 7 compares the asynchronous"}, {"title": "Phase 3: Label Updates", "content": "When the main worker reaches Phase 3, it waits on a barrier synchronization for the other workers to finish. Like with Phase 1, the next step is determined by the type of bag being processed (goal node or batch of labels). For a goal node iteration (lines 25-27), once all workers reach the barrier synchronization, the main worker calls the ApplyUpdates procedure to apply the updates from OPEN_DEL to both OPEN and GOP and from P_INS to P (line 27). Note that the synchronization barrier on line 26 is non-blocking since the updates created by each worker are for an independent set of nodes, and thus do not conflict. Therefore, the main worker begins applying updates from any worker as soon as it finishes, and overlaps computation and communication overheads.\nFor regular label iterations, once each worker finishes, it synchronizes on a barrier to wait for the other workers to complete. Candidate labels that dominate each other may be produced in a single iteration due to the parallel execution of workers. Although all redundant updates can be applied, they may result in a high volume of updates, leading to significant communication and serialization overheads. To reduce this bottleneck, OPEN inserts (new candidate labels) are compared with each other to check for dominance. The procedure DominanceCheck is called in parallel (line 30), where each worker checks its own OPEN_INS buffer against the other workers, and removes the label updates based on the outcome of the dominance check. This procedure also checks for duplicates and uses a worker-ID arbitration strategy to ensure both duplicates are not destroyed (i.e., at least one remains) by only allowing deletes if the current worker's ID is less than the other worker ID. For the other update data structures, OPEN_DEL, and GCL_DEL, only duplicate checks are performed (using the DuplicateCheck procedure on line 31). The synchronization barrier on line 29 is non-blocking since each worker only compares its updates to workers who completed the duplicate and dominance checks. This allows for the parallel overlap of the duplicate and dominance check computations with communication overhead. Additionally, the main worker does not need to participate in the first barrier (line 29), since this synchronization ensures all parallel workers complete their assigned processing of labels. This allows the main worker to hide additional OPEN extractions while the duplicate and dominance checks are performed. Once both procedures are complete, another non-blocking barrier is reached (line 32), allowing the main worker to apply the updates from OPEN_INS, OPEN_DEL, and GCL_DEL to OPEN, Gop, and GCL in ApplyUpdates (line 33). This synchronization is non-blocking since each worker that reaches this barrier has completed all of its update checks, allowing the main worker to apply them as they are received from the parallel workers."}, {"title": "Challenges for OPMOS Framework", "content": "Two key challenges arise in the OPMOS framework. First is the sequential nature of the main worker, which extracts labels from OPEN and performs the iteration updates serially. As the number of workers increases, redundant (duplicate) updates may be introduced, which leads to a high update volume. To solve this challenge, the parallel procedures DominanceCheck and DuplicateCheck are proposed to reduce the update volume by pruning redundancy update operations. Additionally, an asynchronous OPEN extraction method is employed to hide as much of the sequential latency as possible. The second key challenge is the complexity of computations for each label, which unlike other graph algorithms is non-trivial and varies depending on graph properties and the number of objectives in MOS. To solve this challenge, a novel load-balancing approach is proposed to unlock efficient parallel execution. Next, the efficacy of OPMOS is evaluated to quantify its performance scaling potential."}, {"title": "Methods", "content": "The NVIDIA GH200 [19] Superchip's CPU is used for evaluation. It integrates 72 Neoverse V2 Armv9 cores operating at 3.1GHz in a single chip. The memory hierarchy supports 64KB L1 instruction and data caches, 1MB private L2 cache per core, a shared 114MB last-level cache, and ~500GB of on-package LPDDR5X unified memory with 512GB/s memory bandwidth. Although the system package includes a Hopper NVIDIA H100 GPU interconnected with the CPU using NVLink, it is not used in this paper.\nFor characterization, OPMOS is implemented in Python 3.10.13 [36], meshing cleanly with the existing Python-based TMPLAR system. OPMOS is compared against a sequential optimized NAMOA* MOS algorithm variant. In both the sequential and parallel OPMOS, the frontier sets GOP, GCL, and P are implemented using optimized NumPy [8] arrays in shared memory with user-managed dynamic array sizing. OPEN is implemented using a heap-based priority queue, which is only accessible in the main worker. In the parallel setting, the main worker uses shared memory buffers to communicate work distribution and retrieve updates from the other workers. The non-blocking barrier in OPMOS (see Section 5) is also implemented using shared memory. To avoid the serialization enforced by Python's Global Interpreter Lock (GIL) [4], processes are used instead of threads using Python's multiprocessing library. This allows each worker to run independently in a core, with performance scaling evaluated by varying the number of workers. Faster"}, {"title": "TMPLAR MOS Benchmark", "content": "TMPLAR generates ship routing graphs with up to twelve objectives (as shown in Table 1). The distance objective measures the distance along a line with constant bearing to the true North (the so-called rhumb line distance). Three weather and oceanic parameters are directly used as objectives: wave height, wave direction (relative to ship bearing), and wave period. These parameters are obtained from the ERA5 dataset [11], analyzed at 3-hour intervals starting January 1, 2016. The oceanographic parameters are also used to generate seven objectives: fuel consumption (based on required propulsion in calm water [12] and due to wave resistance [6]), and six ship dynamic response objectives (calculated using a nonlinear wave-load analysis [30]): roll, pitch, vertical acceleration, horizontal acceleration, vertical bending moment, and vertical shear force. A pseudo-randomly generated objective is calculated using a seed of the latitude, longitude, and time window information at each graph edge. TMPLAR allows any number of objectives to be run for a given route, where the objectives are selected in the order specified in Table 1.\nTMPLAR's state-space reduced graph routes used for evaluation are shown in Table 2. These are generated using start and end locations as inputs, along with the start date of January 1st, 2016 for weather data information, minimum and maximum ship speeds of 5 and 30 knots respectively, and"}, {"title": "Evaluation", "content": "OPMOS aims to extract parallelism for multi-objective optimization problems. Figure 5 evaluates the performance of OPMOS for two (lowest), three, and the maximum number of objectives achievable for each route (as shown in Table 2). The performance of OPMOS is evaluated by setting the number of label extractions for each iteration (num_to_pop parameter) equal to the number of workers and increasing the number of workers from 1 (sequential) to 72. The speedup is shown relative to the sequential MOS framework. For the maximum number of objectives, the performance scales for all routes as the number of workers increases. However, at 72 workers, the number of active processes exceeds the available cores in the CPU, and performance degrades due to scheduling interference by the operating system. The speedups at 64 workers range from 8\u00d7 to 23\u00d7 with a geometric mean of 14x. For two objectives, performance scales to a limited number of workers (16-32) and degrades afterward, achieving a geometric mean speedup of only 2.7\u00d7. For Route 3, the performance even falls below sequential at 64 workers. Three objectives observe improved performance speedups"}, {"title": "Two Objectives", "content": "For two objectives, Figure 6 shows the normalized OPMOS runtime breakdown with per-iteration label extractions swept from 1 to 128. The normalized number of label extractions is also plotted as a metric of work efficiency. At a single extraction per iteration, the optimized sequential MOS is used and the runtime is dominated by label processing. The work inefficiency grows with the number of OPEN extractions but at different rates for each route. Routes 1, 2, and 5 scale to 16-32 workers, and runtime grows with increasing label extractions after that. When work inefficiency is high, more redundant labels are processed, leading to higher label processing times and requiring more duplicate and dominance checks to rectify these inefficiencies. The low-priority labels introduced into the system can also result in more load imbalance and a potential increase in the updates performed. However, the load imbalance (communication overhead component) decreases since more work is exposed to the workers. Routes 3 and 4 observe a massive increase in work inefficiency even when the label extractions are set to 8. Consequently, workers process redundant and wasteful computations that impact all runtime components adversely."}, {"title": "Three Objectives", "content": "For three objectives, Figure 8 shows the normalized OPMOS runtime and label extractions with per-iteration label extractions swept from 1 to 128. The work inefficiency trends continue to increase across all routes but at much-reduced rates. This signifies that with just three objectives many labels close to the global Pareto front are extracted from OPEN. All routes scale to 32 label extractions and routes 2 and 5 up to 128. Even though the increase in work inefficiency causes more label processing times, the communication overheads scale as much of the computations are load-balanced and processed asynchronously. In routes 1, 3, and 4 the update volume and duplicate and dominance checks increase due to low-priority redundant labels, which causes these routes to stop scaling around 32 label extractions.\nFigure 9 shows the comparisons to na\u00efve OPMOS optimizations. Unlike the two objectives, the asynchronous OPEN extractions are now more important due to the significant time spent in OPEN extractions. The load balancer keeps the work inefficiency and communication overheads in check, signifying its continued importance. Duplicate and dominance checks are less important at 3 objectives, and can"}, {"title": "Maximum Number of Objectives", "content": "Figure 10 shows the normalized runtime and label extractions for the maximum objectives for each route (as described in Table 2). The trends observed from two to three objectives continue with a lower rate of work inefficiency at increasing label extractions. More time is spent in label processing, OPEN extractions, and communication overheads at these higher objective counts. Updates and duplicate/dominance checks are less of an issue, attributed to the low increase in work inefficiency. The more labels extracted, the more room for further performance scaling. The jump from one (sequential) to 8 label extractions is huge, with a geometric mean speedup of 10x. In all cases, scaling continues up to 128 labels extracted per iteration, with a geometric mean speedup of 14x. These further speedups are obtained mainly by communication overhead reductions, suggesting lower load imbalance with increased parallelism from more labels available for processing. However, OPEN extractions begin to dominate at the higher number of label extractions. Despite this, the results suggest that even higher speedups may be achievable on a CPU with more available cores to exploit parallelism."}, {"title": "Accuracy of Solutions", "content": "MOS optimizes all objectives simultaneously and finds a set of Pareto-optimal (non-dominated) solution paths. As the number of objectives increases, so does the number of Pareto-optimal solution paths. The OPMOS framework maintains"}, {"title": "Discussion", "content": "While each route in TMPLAR can be generated for 12 objectives, not all can execute to completion optimally within the time limit, even within OPMOS. To reduce the complexity of MOS, the approximation parameter \u0454 has been used to influence the dominance checks [39] to enable pruning of paths within an e-bounded range. An e of 0.25 allows all 12 objectives to be completed for all evaluated routes, as shown in Table 4. OPMOS is evaluated using e-based NAMOA* and a geometric mean speedup of 15\u00d7 is observed across all five routes. The gain in performance is not only due to exploitable parallelism but also through a decrease in labels processed relative to the sequential implementation. This is due to the aggressive pruning of candidate labels using the relaxation of the dominance check, thereby preventing similar candidate labels from being introduced. However, this improved performance through parallelism and work-efficiency improvements comes at the cost of solution quality. The number of Pareto-optimal solutions for OPMOS significantly decreases (by as much as 33%) compared to sequential execution with \u0454 = 0.25. This raises concerns about the explainability of using approximate techniques to reduce the complexity of MOS. Therefore, on the algorithmic front, future research must devise methods to understand the relationship between parallelism, work efficiency, and solution quality.\nIn OPMOS, we explore parallelism at the candidate label level. However, there is significant room for fine-grained parallelism to be explored. Due to computationally intensive labels, individual operators such as Prune and NotDominated can be parallelized at the granularity of label level checks. Massively parallel single instruction multiple threads (SIMT) architectures are well suited to accelerate dominance and pruning checks. Even novel hardware accelerators can be envisioned to perform these associative lookups efficiently. To unlock parallelism further, an approach that favors parallel execution by disregarding ordered label processing can be implemented where each node in the graph performs dominance and pruning checks in parallel until all nodes settle on a set of Pareto-optimal solutions. This approach will require massively parallel hardware with fast communication to support label-level checks. SIMT architectures, such as modern GPUs, and multi-node high-performance computing (HPC) clusters are candidate architectures that enable such a paradigm. However, on the architectural front, future research must devise work-efficient and load-balanced parallel execution to unlock the performance potential of MOS.\nAlthough this paper introduces TMPLAR's ship routing application as a benchmark for multi-objective shortest paths and search, more research is needed to support routes with increasing graph sizes and diverse applications. MOS can benefit real-world applications, such as road networks, energy grids, autonomous systems, recommendation systems, and social networks, to name a few. Dealing with increasing objective counts requires state space reductions and preprocessing of graph data akin to TMPLAR that generates Pareto-optimal solutions within a time limit. The benchmarking front requires future research to create open-source MOS benchmarks for broader community adoption."}, {"title": "Conclusion", "content": "This paper explores the NP-hard multi-objective shortest path (MOS) problem. State-of-the-art MOS algorithms maintain a set of partial paths at each node and perform ordered processing to ensure that Pareto-optimal solution paths are generated. Finding a set of exact solutions becomes computationally intractable as the number of objectives increases. A MOS benchmark capable of handling multiple objectives is proposed for a real-world ship-routing application. It enables state-space graph reduction that enables a tractable execution of MOS for an arbitrary number of objectives. The MOS framework performance characterization using the NVIDIA GH200 Superchip reveals that the computational complexity of MOS grows substantially with the number of objectives. However, due to long computational paths in label processing, there is potential for parallelism. It is concluded that ordered processing of candidate paths is needed for work-efficient parallel execution. Using this insight, the Ordered Parallel MOS (OPMOS) framework is proposed to handle"}]}