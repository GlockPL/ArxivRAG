{"title": "FreeAvatar: Robust 3D Facial Animation Transfer by Learning an Expression Foundation Model", "authors": ["FENG QIU", "WEI ZHANG", "CHEN LIU", "RUDONG AN", "LINCHENG LI*", "YU DING", "CHANGJIE FAN", "ZHIPENG HU", "XIN YU"], "abstract": "Video-driven 3D facial animation transfer aims to drive avatars to reproduce the expressions of actors. Existing methods have achieved remarkable results by constraining both geometric and perceptual consistency. However, geometric constraints (like those designed on facial landmarks) are insufficient to capture subtle emotions, while expression features trained on classification tasks lack fine granularity for complex emotions. To address this, we propose FreeAvatar, a robust facial animation transfer method that relies solely on our learned expression representation. Specifically, FreeAvatar consists of two main components: the expression foundation model and the facial animation transfer model. In the first component, we initially construct a facial feature space through a face reconstruction task and then optimize the expression feature space by exploring the similarities among different expressions. Benefiting from training on the amounts of unlabeled facial images and re-collected expression comparison dataset, our model adapts freely and effectively to any in-the-wild input facial images. In the facial animation transfer component, we propose a novel Expression-driven Multi-avatar Animator, which first maps expressive semantics to the facial control parameters of 3D avatars and then imposes perceptual constraints between the input and output images to maintain expression consistency. To make the entire process differentiable, we employ a trained neural renderer to translate rig parameters into corresponding images. Furthermore, unlike previous methods that require separate decoders for each avatar, we propose a dynamic identity injection module that allows for the joint training of multiple avatars within a single network. The comparisons show that our method achieves prominent performance even without introducing any geometric constraints, highlighting the robustness of our FreeAvatar.", "sections": [{"title": "1 INTRODUCTION", "content": "3D facial animation transfer methods aim to capture human facial expressions and movements to create realistic animations for digital avatars, which have vast application prospects in digital human, CG games, VR and AR, etc. [Davis et al. 2009; Nowak and Fox 2018; Zollh\u00f6fer et al. 2018]. Facial motion capture systems, such as Faceware [Faceware Technologies, Inc. 2023] and ARKit [Apple Inc. 2022], are extensively utilized in practical applications [Cao et al. 2013; Furukawa and Ponce 2009]. Compared to manually created animations, it can present more delicate facial expressions. With the development of computer vision, video-driven facial animation transfer methods have gained considerable attention due to their convenience and low cost. However, achieving natural and accurate expression transfer while ensuring consistency in facial emotions is still a challenge.\nExisting methods [Aneja et al. 2018; Pan et al. 2023; Zhang et al. 2020] simultaneously employ facial geometry priors and expression features to maintain emotional semantic consistency between source and target faces. We find that these methods often fail to drive the target avatar to generate high-fidelity expressions, especially for in-the-wild data. There are several reasons for this. Firstly, the geometric constraints, like metrics based on facial landmarks, struggle to effectively capture subtle changes in expressions, such as slight frowning and the compression of lips. Secondly, the expression features are commonly trained on the discrete emotion classification task based on limited categories. However, human emotional variations are often diverse and continuous. Therefore, these expression features are unable to capture fine-grained emotional differences.\nTo tackle the aforementioned issues, we introduce FreeAvatar, a robust facial animation method that relies solely on the expression representation and is capable of maintaining high-fidelity expressions. The core idea of FreeAvatar is first to learn a continuous and semantic distinguishable expression representation that can be derived from any facial image and then devise an animation transfer model that can precisely decode the expression representation into the target avatar expression.\nWe begin by learning an expression foundation model to construct a fine-grained and expressive latent space. Within this space, facial images with similar expressions cluster together, while those with dissimilar expressions distance themselves apart. Specifically, we first utilize the Masked Autoencoder (MAE) [He et al. 2022] to learn the intrinsic facial features from a large amount of unlabeled facial images. Based on the pre-trained ViT encoder, we then incorporate contrastive learning to finetune it. Unlike previous works [Larey et al. 2023; Pan et al. 2023; Zhang et al. 2020] that constrain the expression consistency by coarse features trained on a limited set of discrete categories, our method mimics human perception by learning to recognize the subtle nuances of facial expressions. This approach allows the model to achieve a fine-grained and continuous expression feature. After that, we attain the powerful expression foundation model.\nAfterward, we propose a novel Expression-driven Multi-avatar Animator to produce facial animations from the extracted expression representations. In this component, we first leverage a rig parameter encoder to map the expression representation into the facial control parameters of 3D avatars. To capture high-frequency details of emotions, we then employ a neural renderer to translate these parameters into the facial images of target avatars. Considering that previous works require training separate decoders for each target avatar, which significantly limits model scalability, we also propose a dynamic identity injection module. This module enables joint training of multiple avatars by randomly assigning the avatar ID during training. Additionally, to enhance the model's generalizability, we devise an identity-conditional loss to achieve semi-supervised training. This loss functions by enforcing constraints between facial control parameters and image pixels only when the IDs of the input and output match.\nExtensive experiments demonstrate our FreeAvatar can achieve state-of-the-art performance from in-the-wild images, which allows our method to be freely and conveniently applied to various scenarios. In summary, the contributions of this work are listed as:\n\u2022 We propose FreeAvatar, the first method that relies solely on expression representations for 3D facial animation transfer. Leveraging our expression triplet dataset, this approach enables high-fidelity 3D facial animation transfer, even with in-the-wild face images.\n\u2022 We introduce an expression foundation model designed to construct a universal, fine-grained, and continuous latent space that adapts well to various faces, including stylized avatars. Benefiting from this model, FreeAvatar can maintain a high level of expression consistency during facial animation transfer.\n\u2022 We devise an Expression-driven Multi-avatar Animator to decode expression representations into facial control parameters and maintain expression consistency. The dynamic identity injection module and the identity-conditional loss allow us to implement animations for multiple avatars with only one decoder."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 3D Facial Animation Transfer", "content": "In the film and gaming industries, facial animations are mainly controlled by facial rigs using a method called blendshape-based animation [Lewis et al. 2014]. The process of 3D facial animation transfer involves replicating the performer's facial movements accurately by capturing the blendshape weights to manipulate the 3D face, which may not have the same physiognomy as the performer. While facial motion capture systems such as Faceware [Faceware Technologies, Inc. 2023], ARKit [Apple Inc. 2022], and Metahuman Animator [Epic Games, Inc. 2023] enhance the naturalness and realism of animations, these approaches are typically expensive, hardware-dependent, and necessitate intricate calibration processes.\nRecent advances in computer vision have led to two main methods for 3D facial animation transfer. The first method involves obtaining actor-specific 3D facial animations through monocular face reconstruction technology [Dan\u011b\u010dek et al. 2022; Feng et al. 2021; Lei et al. 2023; Tewari et al. 2017; Wang et al. 2022], which are then retargeted for the target avatars [Chandran et al. 2022; Pighin and Lewis 2006; Ribera et al. 2017]. While these approaches perform well, they are limited by the requirements of manually set up mappings for expression semantic or facial geometric, which reduces their flexibility and scalability. The second method involves using neural networks to directly produce the 3D control parameters of target avatars from source facial images. For instance, ExprGen [Aneja et al. 2018] learns to correlate 2D images with avatar expressions via perceptual and facial geometric constraints. Animatomy [Choi et al. 2022] introduces muscle fiber curves to build a modular deformation system. Larey et al. [2023] propose a deep-learning architecture that en Larey et al. [2023] propose a deep-learning architecture that encodes the landmarks of each facial organ to the blendshape weights of target avatars. Pan et al. [2023] presents a blendshape adaption network that maps the source facial images to the rig parameters of target avatars by minimizing the geometric and emotional distances. Unlike previous work using both geometric and perception constraints, the work by Moser et al. [2021] is the only one that exclusively utilizes the expression representation for 3D facial animation transfer. This approach involves training separate image-to-image models first and then image-to-geometry models.\nDespite the above methods simplifying the process of animation transfer, these methods struggle to maintain emotional semantic consistency, especially for in-the-wild data. In contrast, our FreeAvatar eliminates geometric priors and constructs a fine-grained, continuous expression representation to produce facial animations with high-fidelity expressions for multiple avatars."}, {"title": "2.2 Facial Expression Representation", "content": "Emotion analysis within the realm of deep learning has seen considerable achievements over the years, driven by the requirements of understanding and replicating human emotions in machines [Dan\u011b\u010dek et al. 2023; Hakak et al. 2017; Poria et al. 2017; Scherer et al. 2010]. Typical methods utilize discrete emotion categories to represent facial expressions, including basic emotions outlined by Ekman [1992] and Kollias [2022] and compound emotions discussed by EkmanP [1978] and Shao et al. [2018]. The Facial Action Coding System and Action Unit (AU) proposed by EkmanP [1978] and enhanced by Shao et al. [2018] focuses on identifying specific facial muscle movements related to expressions. Despite their widespread adoption, these discrete categories usually fail to capture the nuanced aspects of facial expressions. Several studies [Drobyshev et al. 2022; Trevithick et al. 2023] predict canonical triplane representations from images to extract facial features. However, these methods struggle to disentangle expressions from appearance effectively. In addition, speech-driven 3D avatar animation approaches [Aneja et al. 2024; Fan et al. 2022; Karras et al. 2017] are limited to dialogue or speech scenarios, which renders them unsuitable for non-verbal contexts.\nTo address this problem, continuous Valence and Arousal (VA) [Russell et al. 1989] along with the emotion intensity [Kollias 2022] provide a new way to give the fine-grained emotion representation. 3D morphable models [Chandran et al. 2020; Li et al. 2017; Paysan et al. 2009; Tewari et al. 2021; Tran et al. 2019] offer a robust framework for representing and analyzing 3D facial characteristics. However, these approaches rely on high-cost training data. Therefore, Zhang et al. [2021] introduces an identity-invariant expression embedding space for expression recognition. However, since the work focuses on eliminating the impacts of identities, the space is built on the real-human dataset and cannot handle the stylized cartoon characters. Additionally, the expression space learned in this work overlooks the asymmetry of expressions. In contrast, FreeAvatar leverages a re-collected and more extensive dataset to reconstruct a more expressive and comprehensive expression space for 3D facial animation transfer."}, {"title": "3 METHOD", "content": "Existing 3D facial animation transfer methods [Larey et al. 2023; Moser et al. 2021; Pan et al. 2023] leverage facial geometric information and expression representation to ensure consistency of expression between input and output images. However, as mentioned before, existing constraints are not effective at capturing the subtle details of facial expression contents. In our FreeAvatar, we introduce a novel expression representation to capture subtle expressions for high-fidelity facial animation transfer. To achieve this, as depicted in Fig. 2, we first construct an expression foundation model in two steps: facial feature space construction and expression feature space optimization. Afterward, we propose an Expression-driven Multi-avatar Animator that achieves high-fidelity facial animation transfer and adapts effectively to in-the-wild facial images."}, {"title": "3.1 Expression Foundation Model", "content": "Previous works [Dan\u011b\u010dek et al. 2022; Pan et al. 2023] typically derive the expression extractor via implementing an emotion recognition task. However, we observe that this approach has several limitations: (i) the limited set of expression categories cannot cover all possible expression variations, (ii) the discrete predictions fail to capture the subtle differences between emotions within the same category, (iii) existing datasets only contain real human images, leading to a domain gap between them and stylized avatars, which can result in poor generalization of the extracted features.\nTo tackle these issues, we incorporate contrastive learning to build an expression foundation model, which captures fine-grained and continuous emotion features. To enhance the model's generalization capabilities, we initially use the masked autoencoder (MAE) [He et al. 2022] to develop a robust facial feature extractor through a self-reconstruction task. Leveraging the pretrained ViT encoder with MAE, we then fine-tune this model on the re-collected expression comparison dataset to attain our expression foundation model."}, {"title": "3.1.1 Facial feature space construction", "content": "To effectively utilize a large amount of unlabeled facial images, we first employ MAE pre-training to learn the intrinsic features and structure of faces, thereby enhancing the model's generalization capability. Given a facial image $I$, we divide it into 16 \u00d7 16 non-overlapping patches. Then we mask 75% of the image area as He et al. [2022] and obtain $I_m$. After that, we employ a ViT encoder $E$ to encode the $I_m$ into a latent feature $f_E$ and a ViT decoder $D$ to decode the representation into the original portrait image. For learning of the facial feature space, we optimize the parameters of the autoencoder by the L2 loss. Afterward, the ViT encoder $E$ can serve as a powerful facial feature extractor."}, {"title": "3.1.2 Expression feature space optimization", "content": "Based on the pretrained encoder $E$, we fine-tune the model on an expression comparison dataset and optimize the latent space of expression. Specifically, given an expression triplet {$I_a, I_p, I_n$} with comparison annotation, we first use the encoder $E$ to map them into the same latent space.\n$f_a = E(I_a), f_p = E(I_p), and f_n = E(I_n),$ (1)\nwhere $a, p, n$ refer to anchor, positive and negative in a triplet, and compared to $I_n$, the facial expressions of $I_a$ and $I_p$ are more similar. During training, we use triplet loss to ensure that the representation distance between $I_a$ and $I_p$ is greater than that between $I_a$ and $I_n$. In other words, within the expression latent space, we pull $f_a$ and $f_p$ closer together and push away $f_a$ from $f_n$.\nThe expression feature space is optimized by the weighted triplet loss $L_{tri}$, which can be formulated as:\n$L_{tri} = w \\cdot Max\\big(0, ||f_a - f_p||_2 - ||f_a \u2013 f_n||_2 + m\\big).$ (2)\nHere, $w$ indicates the confidence score, calculated by the ratio of agreed annotations to the total annotations of a sample. $m$ is the margin to ensure the anchor and positive images are closer together in the latent space than the anchor and negative images. Once trained, the encoder $E$ can serve as the expression foundation model."}, {"title": "3.2 Expression-driven Multi-avatar Animator", "content": "Based on the feature extracted by our expression foundation model, we are able to drive avatars to produce corresponding facial animations. Unlike previous works that learned in an avatar-specific manner, our method achieves multi-avatar facial transfer within a single network. Specifically, we first devise a dynamic identity injection module that allows for the joint training of multiple avatars. Following this, we train a rig parameter decoder $R$ to map the expression representation into the facial rigs of 3D avatars and employ perceptual constraints to ensure the consistency of the transferred expressions. In order to make the entire training process differentiable, we use a neural renderer to translate the rig parameters $N$ into facial images of the target avatar."}, {"title": "3.2.1 Expression feature extraction", "content": "Based on our expression foundation model $E$, we are able to obtain the expression representation $f$ from the source facial image $I_a$.\n$f = E(I_a), a \u2208 {0, 1, ..., \u039a},$ (3)\nwhere $a$ indicates the identity of the input face. To enhance the model's generalizability, the training data includes not only target avatar images paired with rigs $r_a$ but also unlabeled in-the-wild facial data (e.g., real human images or other stylized cartoon character images). Specifically, $a \u2208 {1, ..., K}$ refers to the identity number of the target avatars, and $a = 0$ indicates that the data is an in-the-wild facial image. And $K$ is the total number of the target avatars."}, {"title": "3.2.2 Dynamic identity injection", "content": "To accomplish joint training for multiple avatars, during the training process, we randomly assign the target avatar and dynamically inject them into the rig decoder $R$ and the neural renderer $N$. Specifically, for each iteration, we randomly choose $\u00e2 \u2208 {1, ..., K}$, which indicates the identity number of the target avatar. Then we employ an Embedding Layer as the identity encoder $E_{ID}$ to extract identity embedding $e_\u00e2 = E_{ID}(\u00e2)$."}, {"title": "3.2.3 Rig parameter decoder", "content": "After that, we should map the expressive semantic information to the facial controllers of the 3D avatar. To achieve this, we devise the Rig Parameter Decoder $R$ consisting of Multi-Perceptron Layers (MLPs). Specifically, it decodes the expression representation $f$ into rig parameters of the target avatar. Since different avatars possess varying rigs and unique physiognomy, the generated rig parameters of different avatars not only need to contain consistent expression information but also possess unique facial attributes. Hence, in the decoding process, we also incorporate the identity embedding $\u00e2$ and concatenate it to the expression representation. The whole process is expressed by:\n$\\hat{r}_\\hat{a} = R(f, e_\\hat{a}), \u00e2 \u2208 {1, ..., \u039a}.$ (4)"}, {"title": "3.2.4 Neural renderer", "content": "After that, the rig parameters should be translated into the facial images of target avatars to enable expression supervision and capture high-frequency details during training. To make this process differentiable, we employ a pre-trained neural renderer $N$ to mimic the 3D render engine. Specifically, we take the architecture of DCGAN generator [Radford et al. 2015] as the backbone of $N$ and train it on the rig-image paired dataset. It takes the rigs as input and produces the corresponding avatar images. Moreover, to ensure the identity consistency of avatars, we also add the encoded identity information to the neural renderer, formulated as:\n$\\widehat{I}_\\hat{a} = N(\\hat{r}_\\hat{a}, e_\\hat{a}),$ (5)\nwhere $\\widehat{I}_a$ refers to the rendered image of avatar $\u00e2$."}, {"title": "3.2.5 Training objectives", "content": "As mentioned before, the training data consists of two parts, and the in-the-wild facial images have no ground truth for rig parameters or rendered images of target avatars. Therefore, our pipeline is trained in a semi-supervised manner.\nPerception loss: First, we extract expression embeddings from the output image $I_a$ and the input image $I_a$ and encourage consistency between them and obtain expression perception loss:\n$L_{expr} = ||f \u2013 E(\\widehat{I}_a)||^2.$ (6)\nGenerative adversial loss: To enhance the quality of the generated rig parameters and make them as close as possible to real data, our framework also incorporates a Discriminator $D$ to form a Gener-ative Adversarial Network (GAN). The generative adversarial loss can be formulated as follows:\n$L_{GAN} = E_{r_a} [log D (I_a)] + E_{\\hat{r}_a} [log (1-D (\\widehat{I}_a))].$ (7)\nCycle loss: To enhance the model's generalization ability, we fur-ther introduce a cycle consistency loss that transfers the generated expressions of $\u00ce\u00e0$ back onto the target character. This practice can reduce the domain gap between target avatars and in-the-wild faces, thereby improving the generalization to unseen facial images. The cycle consistency loss can be formulated as follows:\n$L_{cycle} = ||\\hat{r}_\\hat{a} - R(E(\\widehat{I}_a), \u00e2) ||^2.$ (8)\nIdentity-conditional loss: In this work, we curate two types of facial image datasets: one comprising avatar images paired with rig parameters $r_a$, and another without. Considering leveraging paired data in training can enhance the model's generalization and performance, we propose an identity-conditional loss within a semi-supervised learning framework. During training, this constraint is applied only to the image data with paired rig parameters. Specifi-cally, when the target avatar identity $\u00e2$ matches the identity $a$ from the input image, the rig-image pairs can be used to enhance the network's accuracy and accelerate convergence. Otherwise, this supervision is not performed. Therefore, the identity-conditional loss $L_{IDC}$ can be formulated as:\n$L_{IDC} = \n\\begin{cases}\n||I_a - \\widehat{I}_a||^2 + ||r_a - \\hat{r}_a||^2, & \\text{if } a = \u00e2, \\\\\n0, & \\text{otherwise}.\n\\end{cases}$ (9)\nTotal loss: In summary, the final loss can be defined as\n$L = \u03bb_1 L_{expr} + \u03bb_2 L_{GAN} + \u03bb_3 L_{cycle} + \u03bb_4 L_{IDC},$ (10)\nwhere $\u03bb_1, \u03bb_2, \u03bb_3, \u03bb_4$ are the weights of different loss. And we set $\u03bb_1 = 100, \u03bb_2 = \u03bb_3 = 1e-3$ and $\u03bb_4 = 1$ in this work."}, {"title": "4 EXPERIMENTS", "content": "In this section, we assess the effectiveness of our framework by extensive experiments. Given a fully rigged Avatar model, our method achieves 3D facial animation transfer with only RGB images for in-the-wild scenarios."}, {"title": "4.1 Data Acquisition", "content": "Unlabled facial images. To enhance the generalization ability of our feature extractor, we integrate five facial databases (i.e., Affect-Net [Mollahosseini et al. 2017], CASIA-WebFace [Yi et al. 2014], CelebA [Liu et al. 2015], IMDB-WIKI [Rothe et al. 2018], and Web-Face260M [Zhu et al. 2021]) to form a large-scale facial dataset. Additionally, considering the gap between avatars and real humans, we also gather a part of stylized facial images from cartoons and animations as a supplement. Our dataset includes various races and genders. In total, this dataset comprises around 4.5 million facial images.\nLabeled expression triplets. To enable the expression latent space to be continuous and expressive, we construct a multi-identity dataset consisting of 914K face image triplets with the expression comparison annotations. These triplets consist of two parts: one part (~500K triplets) is data with a high ratio of agreement from the FEC database [Vemulapalli and Agarwala 2019], and the other (~414K triplets) is randomly constructed from unlabeled facial images, involving real humans and cartoon images, which are then annotated. The scarcity of asymmetrical expression samples in the FEC dataset makes a trained model fail to represent facial asymmetry effectively. Therefore, we supplement the dataset with additional images of asymmetrical expressions. To maintain the labeling consistency, annotators are guided to focus on overall facial expressions rather than specific facial features. Moreover, we discard triplets if over 40% of annotators cannot reach a consensus.\nRig-image pairs. To facilitate the facial animation transfer, we build a dataset consisting of facial rig parameters paired with cor-responding avatar facial images. Notably, our method does not necessitate temporally continuous samples; thus, we obtain a set of rig parameters via random sampling. Then these rigs are rendered into the facial images with a fixed front-facing head pose and a fixed camera setup. In our experiments, we collect data from four different avatars, amassing 100K pairs of images and rig parameters for each avatar."}, {"title": "4.2 Experimental Setup", "content": "We implement the proposed approach in PyTorch and adopt AdamW optimizer [Loshchilov and Hutter 2017]. For the facial feature space construction, training occurs over 800 epochs with a learning rate of 1e-4. We use a batch size of 4096, and the training is performed on 8 NVIDIA A30 GPUs. For the expression feature space optimization, the training spans 150 epochs with a learning rate of 0.002 and a batch size of 128. The training for facial animation transfer is conducted for up to 100,000 steps, employing a batch size of 16 and a learning rate set to 1e-5. Besides, we incorporate 200K unlabeled facial images along with the rig-image pairs to train our model."}, {"title": "4.3 Comparisons with Commercial Products", "content": "To assess the effectiveness of our pipeline, we first compare our FreeAvatar with the commercial facial motion capture systems, i.e. Faceware and MetaHuman Animator, which are the only viable and equivalent systems to our pipeline. These methods only adapt well to data collected under controlled conditions.\nComparison with Faceware. In this scenario, we compare our method with Faceware on the frontal facial images from two actors. Qualitative results can be seen in Fig.3. It shows that our method excels in capturing the details of mouth movements during dialogue compared with Faceware. Specifically, Faceware frequently fails to capture key mouth movements along the viewing axis, such as puckering, which are vital for adding realism to the mouth animations. In contrast, thanks to the powerful expressive capabilities of our FreeAvatar, which utilizes only frontal view images, it effec-tively transfers such nuanced lip motions. This clearly demonstrates that the animations generated by our FreeAvatar not only replicate the facial expressions from the input images more accurately but also perceive the subtle differences between various facial actions, affirming its superior performance.\nComparison with MetaHuman Animator. MetaHuman Anima-tor is a framework proposed by EPIC Games to create and animate highly realistic digital human characters. We use an iPhone to ac-quire the data and import them into the MetaHuman Animator for solving to obtain facial animation. As indicated in Fig. 4, our results are significantly better in the lip region than those of the MetaHuman Animator, even though it requires depth information. Additionally, the MetaHuman Animator demands stringent condi-tions for data collection. Minor changes in head posture, background variations, and obstructions can severely degrade the quality of the animations. Differently, our framework exhibits good robustness. Even when parts of the face are obscured, our approach is able to stably transfer facial movements of the visible parts, demonstrating strong robustness.\nUser study. In addition, we conduct a user study to evaluate the performance of expression transfer in our framework. A total of 21,451 survey questions were administered to over 39 participants. These questions are divided into two categories: 8,332 questions comparing our method with Faceware, and the remainder comparing our method with MetaHuman Animator. Each question presents three images: one input facial image and two 3D avatar images generated by different methods. Participants are asked to select the avatar image that best matches the input expression without knowing the source of each image. We use the Wilson confidence interval [Yan and Su 2010] to establish the confidence level for each response and analyze the result distributions. As shown in Fig. 5, high-confidence results (>=0.99) indicate that our method significantly surpasses Faceware (84% vs. 5%) and Animator (77% vs. 21%) in maintaining expression consistency. This substantiates our approach's effectiveness and practical utility in producing superior facial animation."}, {"title": "4.4 Comparisons with Face Reconstruction Methods", "content": "In this study, we compare our methods with three state-of-the-art monocular face reconstruction methods, i.e. 3DDFA-V2 [Guo et al. 2020], DECA [Feng et al. 2021], EMOCA [Dan\u011b\u010dek et al. 2022], HRN [Lei et al. 2023], FaceVerse [Wang et al. 2022] in terms of maintaining consistent expressions when handling in-the-wild data. These data exhibit a high level of diversity because they are not strictly constrained by factors such as head posture, background, occlusion, lighting conditions, and identity. This greatly simplifies and enhances the efficiency of the data collection process.\nReal footage. We first test real footage with expressive emotions extracted from movies and TV shows. The results are shown in Fig. 6. Although HRN performs better in maintaining facial expression consistency compared to other facial reconstruction methods, factors such as occlusion and head posture can severely compromise its effectiveness. In contrast, our results more accurately convey the facial movements of the input images across different scenarios, including expressions such as smiling, screaming, and frowning. It is worth noting that our method still maintains better consistency in expressions, despite the differences in identity between the input and output, which could potentially complicate the process.\nStylized cartoon characters. We also conduct experiments on var-ious stylized cartoon characters. These characters often have facial shapes and features that greatly differ from those of real humans. As shown in Fig. 6, facial reconstruction methods such as HRN and FaceVerse fail to generate face models in some cases. This is because these methods rely on the detection of facial landmarks. When facial landmarks cannot be detected, these methods do not work. In con-trast, our method still works and maintains the consistency of their facial expressions. We attribute this to the robust expressiveness and generalizability of our expression foundation model. These results suggest that our pipeline exhibits better performance and stronger applicability under real-world, non-ideal conditions.\nMulti-Avatars. Additionally, our method is capable of training on multiple distinctively-rigged avatars simultaneously, without the need to train a separate network for each character as previous works required. In Fig. 6, we present the facial animation transfer results for three avatars with different appearances. Thanks to our identity-conditional semi-supervised training strategy and our foundational expression model, our method can adaptively gen-erate consistent expression results for different avatars and their respective rigs. We also conducted user study to compare the ef-fectiveness of multi-avatars and single-avatar decoders. The user study shows a comparable preference for the multi-avatar and single-avatar decoders with a ratio of 51.6% to 48.4%. This indicates that the strategy does not compromise the transfer quality. This method is particularly useful for multi-character inference in resource-limited scenarios, such as mobile games."}, {"title": "4.5 Ablation Study", "content": "Ablation on expression representations. For facial animation transfer, it is crucial to explore an expression representation with sufficient capability and generalizability to constrain expression consistency. We first substitute our expression foundation model by randomly initializing the weights of the ViT encoder. As depicted in Fig. 7, it fails to capture the emotions from in-the-wild input images, which validates the necessity of the expression model.\nWe then explore the effectiveness of facial feature space recon-struction with MAE pretraining. Specifically, we train a ViT encoder with randomly initialized weights on the expression triplet datasets as a substitute for our expression foundation model. It is obvious that without MAE pretraining, the generated facial animations often fail to ensure consistency in expression, especially for stylized cartoon characters. This validates that MAE pretraining for facial auto-reconstruction can significantly enhance the generalizability of expression features, allowing for the extraction of semantically consistent expression features even from in-the-wild facial images.\nMoreover, we validate the effectiveness of expression feature space optimization on our triplet dataset. Without triplet optimiza-tion, the accuracy of the ViT encoder in the expression comparison task drops significantly from 87.73% to 34.06%. In the animation transfer task, as depicted in the fourth column of Fig. 7, the trans-ferred results fail to maintain expression consistency. This highlights the critical effort of the triplet optimization in accurate expression feature extraction.\nWe also investigate the difference between our expression foundation model and EmoNet [Toisoul et al. 2021", "2022": "to capture emotions. EmoNet uses ResNet-50 [He et al. 2016", "2017": ".", "2004": "."}]}