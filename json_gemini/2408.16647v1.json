{"title": "DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving", "authors": ["Yongjie Fu", "Anmol Jain", "Xuan Di", "Xu Chen", "Zhaobin Mo"], "abstract": "The advancement of autonomous driving tech- nologies necessitates increasingly sophisticated methods for understanding and predicting real-world scenarios. Vision lan- guage models (VLMs) are emerging as revolutionary tools with significant potential to influence autonomous driving. In this paper, we propose the DriveGenVLM framework to generate driving videos and use VLMs to understand them. To achieve this, we employ a video generation framework grounded in denoising diffusion probabilistic models (DDPM) aimed at predicting real-world video sequences. We then explore the adequacy of our generated videos for use in VLMs by employing a pre-trained model known as Efficient In-context Learning on Egocentric Videos (EILEV). The diffusion model is trained with the Waymo open dataset and evaluated using the Fr\u00e9chet Video Distance (FVD) score to ensure the quality and realism of the generated videos. Corresponding narrations are provided by EILEV for these generated videos, which may be beneficial in the autonomous driving domain. These narrations can enhance traffic scene understanding, aid in navigation, and improve planning capabilities. The integration of video generation with VLMs in the DriveGenVLM framework represents a significant step forward in leveraging advanced Al models to address complex challenges in autonomous driving.", "sections": [{"title": "I. INTRODUCTION", "content": "In the rapidly evolving field of autonomous driving, the integration of advanced predictive models into vehicular systems or transportation systems has become increasingly critical for enhancing safety and efficiency [1], [2]. Among the myriad of sensory technologies employed, camera-based video prediction stands out as a pivotal component, offering a dynamic and rich source of real-world data. Through the adoption of a cutting-edge diffusion model approach, this research not only contributes to the advancement of autonomous driving technologies but also sets a new bench- mark for the application of predictive models in enhancing vehicular safety and navigational precision.\nContent generated by artificial intelligence is presently a leading area of study within the domains of computer vision and artificial intelligence. The generation of photo-realistic and coherent videos is one of the challenging areas because of the limitations of memory and computation time. In the autonomous vehicle area, predicting the video from a vehi- cle's front camera is crucial for several reasons, particularly in the context of autonomous driving and advanced driver- assistance systems (ADAS) [3]. In this paper, we utilize the videos from the vehicle's surrounding cameras and predict future frames.\nThe generative model has also been utilized in the area of transportation and autonomous driving [4], [5]. Models are increasingly recognized for their capability to understand driving environments. Vision language models (VLMs) are now being utilized for autonomous driving applications. To enhance the utility of VLMs and explore the application of generative models to video content within VLMs, it is essential to validate generative models' predictions to confirm their relevance and accuracy in real-world scenarios. DriveGen VLM introduces the in-context VLM as a method to validate predicted videos from a diffusion-based generative model by providing textual descriptions of driving scenarios."}, {"title": "A. Related Work", "content": "Diffusion-based architectures have become increasingly popular in recent research for generating images and videos. Diffusion models have been applied to a variety of tasks for images, including image generation [6], image editing [7], and image-to-image translation [8]. Video generation and prediction are effective approaches to understand the real world. Several standard architectures have been utilized in this task, including Generative Adversarial Networks (GANs) [9], flow-based models, auto-regressive models, and Varia- tional Autoencoders (VAEs) [10]. Recently, more diffusion models have been applied in this domain and achieve better video quality and more realistic frames, such as video generation [11] and text prompt to video generation [12].\nDiffusion models are a class of deep generative models characterized by two main phases: (i) a forward diffusion phase, where the initial data is incrementally disturbed by the addition of Gaussian noise across multiple steps, and (ii) a reverse diffusion phase, where a generative model aims to reconstruct the original data from the noise-added version by progressively learning to invert the diffusion process, step by step. Denoising Diffusion Probabilistic Models (DDPM) represent a common type of generative model designed to learn and generate a specific target probability distribution through a diffusion process. DDPMs have been validated to be more effective than the traditional generation models, such as GANs and VAE.\nGenerating long videos requires a large amount of com- putation sources. Some works overcome this challenge with autoregressive based models, such as Phenaki [12] and [13]. However, autoregressive models may lead to unrealistic scene transitions and persistent inconsistencies in extended video sequences because these models lack the opportunity to assimilate patterns from longer footage. To overcome this, MCVD [14] employs a training approach that prepares the model for various video generation tasks by independently and randomly masking either all preceding or subsequent frames. Meanwhile, FDM [11] introduces a framework based on Diffusion Probabilistic Models (DDPMs) that is capable of generating extended video sequences with realistic and coherent scene completion across diverse settings. NUWA- XL [15] introduces a \"Diffusion over Diffusion\" architecture designed for generating extended videos through a \"coarse- to-fine\" method.\nIn recent years, large language models (LLMs), which are text-based, have seen a surge in popularity [16]. Additionally, various generative vision-language models (VLMs) have been introduced in the autonomous driving domain. RAG- Driver [17] was proposed to leverage in-context learning for high performance, explainable autonomous driving. We leverage the in-context learning capabilities of EILEV [18] to generate descriptions of driving scenarios. In DriveGenVLM, the in-context VLMs allow us to process videos predicted by the diffusion framework, which can then be recognized by other vision-based models, potentially contributing to decision-making algorithms in autonomous driving. To the best of our knowledge, DriveGenVLM is the first work to integrate a video generation model and a Vision Language Model (VLM) into the autonomous driving domain."}, {"title": "B. Contributions of This Work", "content": "The key contributions of DriveGenVLM are summarized as follows:\n1) Apply conditional denoising diffusion probabilistic models to the domain of driving video prediction.\n2) Test the video generation framework in the Waymo open dataset of different camera angles to validate the feasibility for real world driving scenarios.\n3) Utilize in-context vision language model to generate descriptions of the predicted video and validate that these videos can be applied for Vision language model based autonomous driving.\nThe rest of the paper is organized as follows. Sec. II introduces the preliminary knowledge used in this paper. Sec. III illustrates the solution approach. Sec. IV introduces the setting and results of the experiments. And Sec.V concludes this study."}, {"title": "II. PRELIMINORY", "content": ""}, {"title": "A. Denoising Diffusion Probabilistic Models (DDPM)", "content": "The Denoising Diffusion Probabilistic Model is a type of generative model that has gained significant attention in the field of machine learning and computer vision [19]. DDPM operates through a forward process that transforms data into noise, and a backward process that reconstructs the original data from the noise. The goal of the forward process is to convert any data into a basic prior distribution, whereas the subsequent objective involves developing transition kernels to undo this conversion. To generate new data points, one begins by drawing a random vector from the prior distribu- tion, then proceeds with ancestral sampling via the reverse Markov chain. The key to this sampling technique is to train the reverse Markov chain to replicate the time-reversed progression of the forward Markov chain accurately.\nFor the conditional extension, in which the modeled x is conditioned on observationsy. Given a data distribution $x_0 \\sim q(x_0)$, the forward process generates a sequence of random variables $x_1,x_2,...,x_T$. $X_0$ represents the original, noise-free data, while $x_1$ incorporates a slight amount of noise. This process continues up to $x_T$, which is nearly uncorrelated with $x_0$ and resembles a random sample drawn from a unit Gaussian distribution. The distribution of $x_t$ depends only on $x_{t-1}$, the transition kernel is:\n$q(x_t/x_{t-1}) = N(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)I)$.\n(1)\nThe joint distribution is defined by the diffusion process and a data distribution q(x0,y) in Equ. 2.\n$q(x_{0:T}|y) = q(x_0, y) \\Pi_{t=1}q(x_t|x_{t-1})$\n(2)\nDenoting Diffusion Probabilistic Models (DPMs), these models operate by reversing the diffusion sequence. For given $x_t$ and y, we use a neural network to estimate $p_\\theta(x_{t-1}|x_t, y)$, serving as an approximation for $q(x_{t-1}|x_t,y)$. This estimation grants us the capability to procure samples of $x_0$ by commencing with the sampling of $x_T$ from a standard Gaussian distribution, a choice made due to the diffusion process's initial state resembling a Gaussian distribution. Subsequently, we iteratively sample backwards, from $x_T$ to $x_0$, through $p_\\theta$. The aggregate distribution of the sampled $X_{0:T}$ conditional on y is expressed as:\n$p_\\theta (x_{0:T}|y) = P(x_T) \\Pi_{t=1}p_\\theta(x_{t-1}|x_t,y)$.\n(3)\nHere, $p(x_T)$ signifies a unit Gaussian distribution independent of $\\theta$. Training a conditional DPM entails the adjustment of $p_\\theta(x_{t-1}|x_t,y)$ to closely match $q(x_{t-1}|x_t,y)$ across the full range of t, xt, and y values."}, {"title": "B. In-context Learning on VLMs", "content": "In-context learning was originally proposed in the paper of GPT-3 [20], which refers to the ability of a model to learn or adapt its responses based on the context provided within a single interaction, without any explicit updates or retraining on its underlying model.\nWe employ EILEV [18], a training technique developed to enhance in-context learning in Vision Language Models (VLMs) for first-person videos. As shown in Figure. 3, EILEV's architecture for an interleaved context-query sce- nario involves using the unmodified Vision transformer from BLIP-2 [21] to process video clips. The resulting compressed tokens are mixed with text tokens in the sequence of the initial context-query instance. These combined tokens are then input into BLIP-2's static language model, which pro- duces new text tokens. This method can generalize out-of- distribution videos and texts and rare actions vis in-context learning. We make use of the pre-trained model to generate language narrations for the driving videos to validate that the generated results are explainable and realistic."}, {"title": "III. METHODOLEGY", "content": "Generating long, coherent, and photorealistic videos is still a challenge. The Flexible Diffusion Model (FDM) addresses this issue using a conditional generative model. We adopt a similar approach in DriveGenVLM. To sample coherent videos with a large number of frames, we can sample an arbitrary length of video condition on a small number of frames with a generative model. The goal is to sample coherent photo-realistic videos of driving scenarios with some frames. We utilize a sequential procedure to sample an arbitrarily long video with a generative model that can sample or condition on only a small number of frames at once.\nBroadly, we define a sampling scheme as a series of tuples $[(X_s, Y_s)]_{s=1}^S$, where each tuple consists of a vector Xs indicating the indices of frames to be sampled and a vector Ys showing the indices of frames to use as conditions for the stages s = 1,..., S."}, {"title": "A. Training Architecture", "content": "We utilize a U-net structure for the DDPM image frame- work. This architecture is characterized by a sequence of lay- ers that downscale spatial dimensions and then upscale them, interspersed with convolutional residual network blocks and layers that focus on spatial attention. The architecture is illustrated in Figure. 2. The DDPM iteratively transforms noise XT to video frames X0. The boxes with red borders are conditions. The right side shows the UNet architecture of each DDPM step.\nAgorithm. 1 illustrates how we sample a video using a sample scheme. The generative model can sample any subsets of the video frames conditioned on other subsets. The model can generate any choice of X and Y."}, {"title": "B. Sampling Schemes", "content": "Each sampling scheme's relative efficacy heavily relies on the dataset at hand, and there is no universally optimal option. In this work, we experimented with three sampling schemes as shown in Table I. The first and the most straight- forward scheme adopted is the Autoreg, which samples ten consecutive frames at each step by conditioning on the previous ten frames. Another scheme used was Hierarchy- 2 which employs a multi-tiered sampling approach, first tier with ten equidistantly chosen frames covering the unobserved portion of the video, conditioned upon ten observed frames. In the second tier, consecutive frames are sampled in groups, considering the nearest preceding and proceeding frames until all frames are sampled. Lastly, we used Adaptive Hierarchy-2 (Ad), which is only achievable through the implementation of FDM. Adaptive Hierarchy-2 strategically selects conditioning frames during testing to optimize frame diversity, measured by the pairwise LPIPS distance between them."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Datasets", "content": "The Waymo Open Dataset [22] is a wide-ranging dataset that uses many sensors to aid in the progress of self- driving technology. It contains high-quality sensor data from Waymo's group of autonomous vehicles and is made up of more than 1,000 hours of videos. These videos are taken with various sensors such as LIDARs, radars, and five cameras (front and sides); they give a complete view around the car at all times or what we call 360-degree visibility. This group of data has very careful labeling, including marks for vehicles, people walking, bicycle riders and other things found on the road. This makes it extremely helpful for those working as researchers or engineers in this area to enhance their skills with perception (understanding), prediction (guessing what happens next) and simulation algorithms in self-driving cars. The Dataset V2 format is designed to be usable with Apache Parquet file formats and supported components. Here, a component is a set of related fields/columns that are required to understand each individual field."}, {"title": "B. Experiment Setup", "content": "To validate the algorithm in real-world driving scenarios, we utilize the Waymo Open Dataset, which encompasses diverse real-world environments across several cities. We extracted data for all the five present cameras in the dataset. We then pre-processed the datasets and extract the data from the three cameras being Front, Front-left, Front-right. In total we processed 138 videos. A total of 108 videos comprising of all three cameras divided equally were taken for training purposes, while the 10 videos for each of the three cameras for the test set. The maximum number of frames found for train videos was 199 frames, minimum contained around 175 frames. So we used 175 frames as the limit for all videos. The resolution was reduced to 128 \u00d7 128, and transformed into 4D tensors.\nThe model was operated on an 8-core Intel Cascade Lake processor and an NVIDIA L4 GPU with 24 GB memory in Debian GNU/Linux 11. A batch size of 1 with a learning rate of 0.0001 was used. The details of each camera training are shown in table II. The front was trained from scratch without using any pre-trained weights for 200,000 iterations. Front-right used pre-trained weights from Camera-1 and was trained for 150,000 iterations, and Front-left used pre-trained weights from Camera-3, trained for 100,000 iterations. A total of 108 GPU hours were spent on training."}, {"title": "C. Metrics", "content": "We utilize FVD (Fr\u00e9chet Video Distance) [23], a metric used to evaluate the quality of videos generated by models in tasks like video generation or future frame prediction. Simi- lar to the Fr\u00e9chet Inception Distance (FID) used for images, FVD measures the similarity between the distribution of generated videos and real videos. FVD is useful for assessing the temporal coherence and visual quality of videos, making it a valuable tool for benchmarking video synthesis models."}, {"title": "D. Results", "content": "The FVD scores from our experiments on the Waymo Open Dataset for three cameras, which are tested using various sampling schemes, are summarized in Tables III IV V. The adaptive hierarchy-2 sampling method outperforms the other two methods."}, {"title": "E. Prediction Validation by in-context learning.", "content": "To validate that our generated videos are explainable and usable in vision language models, we employ the EILEV pre- trained model on Ego4D, eilev-blip2-opt-2.7b [18] to test our generated driving videos.\nWe utilize video clips and text pairs that describe the camera angle, driving environment, and time of day. The results are illustrated in Figure. 7. The action narrations generated by the model are displayed in an orange box. Notably, none of the verb and noun class combinations are shared in the first two videos, as shown in the blue box. As we can observe, the model can identify that the vehicle is driving on a highway with the camera positioned at the front. For the second video, the model recognizes that the vehicle is driving at night with its front camera. The in-context learning pre-trained model on VLMs performs well with the generated model, indicating that the videos are explainable and potentially usable by VLMs-based algorithms."}, {"title": "V. CONCLUSIONS", "content": "In summary, training the Denoising Diffusion Probabilis- tic Model (DDPM) on the Waymo dataset has shown its capability to produce coherent and lifelike images from both front and side cameras. However, it continues to face challenges in accurately capturing the complex dynamics of real-world driving, such as detailing buildings and tracking pedestrian movements. These difficulties are likely due to the complexities inherent in actual driving conditions, which are absent in synthetic datasets.\nTo explore potential applications of generated videos in Vision-Language Models (VLMs) for autonomous driving, we utilize the pre-trained EILEV model, an in-context VLM, to generate action narrations for the videos. The results indicate that the model can recognize unseen scenarios and generate accurate narrations, demonstrating the potential for deploying VLM-based autonomous driving models that leverage outputs from generative models. The DriveGenVLM framework highlights the potential for using generative mod- els and Vision Language Models (VLMs) together in au- tonomous driving tasks. For downstream applications, once we obtain narrations of driving scenarios, we can employ large language models to provide guidance to the driver or some language model-based algorithms."}]}