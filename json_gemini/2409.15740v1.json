{"title": "Real-Time Pedestrian Detection on IoT Edge Devices: A Lightweight Deep Learning Approach", "authors": ["Muhammad Dany Alfikri", "Rafael Kaliski"], "abstract": "Artificial intelligence (AI) has become integral to our everyday lives. Computer vision has advanced to the point where it can play the safety critical role of detecting pedestrians at road intersections in intelligent transportation systems and alert vehicular traffic as to potential collisions. Centralized computing analyzes camera feeds and generates alerts for nearby vehicles. However, real-time applications face challenges such as latency, limited data transfer speeds, and the risk of life loss. Edge servers offer a potential solution for real-time applications, providing localized computing and storage resources and lower response times. Unfortunately, edge servers have limited processing power. Lightweight deep learning (DL) techniques enable edge servers to utilize compressed deep neural network (DNN) models.\nThis research explores implementing a lightweight DL model on Artificial Intelligence of Things (AIoT) edge devices. An optimized You Only Look Once (YOLO) based DL model is deployed for real-time pedestrian detection, with detection events transmitted to the edge server using the Message Queuing Telemetry Transport (MQTT) protocol. The simulation results demonstrate that the optimized YOLO model can achieve real-time pedestrian detection, with a fast inference speed of 147 milliseconds, a frame rate of 2.3 frames per second, and an accuracy of 78%, representing significant improvements over baseline models.", "sections": [{"title": "I. INTRODUCTION", "content": "AI has become indispensable in everyday life due to its ability to learn and adapt based on empirical data. It has found application in various fields such as medicine, governance, finance, communications, and transportation. Machine Learning (ML) offers a means of gaining insight into and addressing the ever more complex environments and demands requested of AI. However, ML typically requires hand-tuned features to perform well. By comparison, deep learning (DL) can automatically extract latent features using a general-purpose learning procedure [1].\nDL has several applications, one being Computer Vision, where the DL model extracts high-level information and features similar to a human-level visual understanding based on visual input. There are areas related to computer vision, such as object detection, image segmentation, video tracking, object recognition, and motion estimation. Object detection and video tracking can work in tandem to perform pedestrian detection. Pedestrian detection is a critical task for intelligent transport systems and autonomous vehicles. As such, it has gained interest and investment from corporations and governments [2] [3] [4].\nBased on the traffic accident data [5] from the Republic of China's Ministry of Transportation and Communication, in 2021, there were 16,649 instances of accidents involving pedestrians in Taiwan. In those accidents, approximately 90% of the people involved sustained bodily harm, and 410 people lost their lives. In densely populated countries, this data is worrying. While many factors are involved in accidents, one of the most common is human error. Human error occurs due to pedestrian and driver errors, such as being distracted. Some examples of human errors involve pedestrians who ignore their surroundings while walking, look at their phones, or illegally cross the road. The driver who uses their phone while driving is the most common example of human error involving drivers. While enacting laws can help reduce human errors, intelligent transportation systems must make roads safer for pedestrians and drivers.\nIntelligent transportation systems enable the use of video and photos for object recognition. Multiple traffic cameras are deployed at signalized intersections and can detect pedestrians accurately. Then, cloud computing processes the camera feed to detect pedestrians and generate warnings for nearby vehicles around the intersections. However, there are several issues related to cloud computing. Some of them include latency, limited data transfer speed, and the possibility of packet loss, which makes cloud-based computing unsuitable for real-time applications [6]. The other issue is that to receive warnings, the vehicle must have an onboard unit (OBU), which consists of multiple sensors that send the driver information situation-related data. OBUs communicate with surrounding vehicles and network infrastructure through dedicated short-range communication (DSRC) [7].\nOne method to address network latency is to place edge servers in the radio access network. This method transfers computing and storage from the cloud to the edge to reduce latency [8]. The network can better handle response time-critical situations by using the edge servers. Due to their capabilities, Edge Servers often perform various AI tasks, such as object detection [9], anomaly detection [10], and reinforcement learning [11].\nWhile edge servers have many advantages, they also have many disadvantages. Compared to cloud computing servers, edge servers have lower computing power due to their compact form factor. Edge servers can overcome this limitation by using methods and models that do not require high processing power. One such model is lightweight DL, wherein a compressed DNN model with smaller, more efficient models with similar performance metrics to the original model executes on edge devices [12]."}, {"title": "A. Research Contributions", "content": "In this research, we design an edge-device-compatible com-puter vision-based lightweight DL model for pedestrian detec-tion, employing a novel process for detecting and localizing an object. We then train and evaluate the DL model, optimize it to become lightweight, and compare the results to other baseline models. Finally, we deploy the model on an edge device (Nvidia Jetson Nano) to test the model's capabilities in a real-time scenario.\nThe contributions of this research are as follows:\n\u2022 We design a lightweight DL model that can be deployed on an inexpensive edge device to detect pedestrian traffic in real-time.\n\u2022 We develop an Al on edge system that can operate in a low bandwidth environment by transmitting the essential information of interest.\n\u2022 We compare our model to other popular non-YOLO models and demonstrate that other models require at least 3x the computing power for over 2\u00d7 the memory for similar accuracy metrics."}, {"title": "B. Research Organization", "content": "In \u00a7II, we present related works. Then, in \u00a7III, the system model and proposed methods are presented. The simulation setup and topology are discussed in \u00a7IV. Then, the simulation results and analysis are presented in \u00a7V. Finally, in \u00a7VI, the research conclusions and future research directions are presented."}, {"title": "II. RELATED WORKS", "content": "Pedestrian detection is a computer vision problem that involves recognizing the presence of pedestrians in image or video sequences while ignoring other objects. It has found application in autonomous driving [13] [14] [15], monitoring [16] [17], and human-computer interfaces [18]. Pedestrian detection algorithms can be categorized into two main ap-proaches: handcrafted features, such as extracting channel fea-tures that consist of different color [19] and gradient channels [20], and DL, such as using convolutional neural networks (CNNs) [21] [22]. Handcrafted feature-based methods use features that capture pedestrians' visual characteristics, such as shape, texture, and color. These features are then fed into a classifier, such as a support vector machine (SVM), to distinguish between pedestrian and non-pedestrian regions in the image. On the other hand, DL-based methods use CNNs to automatically learn to discriminate features from the raw pixel values of the image. These features are then used to classify each region in the image as pedestrian or non-pedestrian. Both algorithms typically consist of two main stages: candidate generation and classification. Candidate generation involves generating a set of regions in the image that may contain pedestrians. Sliding windows or region proposal methods, such as selective search or region-based CNNs (R-CNNs) [23], assist in pedestrian detection. Classification involves determin-ing whether each candidate region contains pedestrians or not [21], using the classifier trained on handcrafted or DL-based features."}, {"title": "A. Handcrafted Features based Pedestrian Detection", "content": "In the early development of pedestrian detection, various handcrafted features were used. Handcrafted features consist of variations of color, texture, or edge [24]. The most used handcrafted feature in pedestrian detection is the histogram of gradients (HOG). Its popularity is mainly due to its ability to extract a specific shape from the background images. The algorithm used for HOG can obtain the shape by sliding a detection window over grids of overlapping cells to form a block called a HOG feature vector [25]. The handcrafted features are usually fed into ML algorithms like SVM [26], boosting algorithms [27], and others. In [28] [29] [30], various methods of HOG feature extraction are used together with SVMs as a classifier. While in [14], HOG features are used as training data for the ML model that combines the AdaBoost and SVM classifiers.\nDue to the growing interest in real-time applications and the increased capability of computer processing power, the handcrafted feature methods are no longer practical due to their inflexibility owing to their requirement to search the entire image to detect objects at different locations and scales, making the model slow and impractical to use in a scenario where time is critical. New methods relied on DL, which is believed to have the ability to generate features independently and is, therefore, not reliant on handcrafted feature generation. As such, DL models are considered faster and more flexible."}, {"title": "B. Deep Learning based pedestrian detection", "content": "Following the development of AI, DL models were de-veloped for use in various computer vision tasks, such as image classification [31] [32], semantic segmentation [33] [34], and object detection [21] [22]. Based on the DL model's ability to perform various computer vision tasks, models were developed to tackle the pedestrian detection problem in recent years. DL-based pedestrian detection can be divided into two types: hybrid-based pedestrian detection and fully DL-based pedestrian detection.\nIn hybrid-based pedestrian detection, DL models use CNNs to extract features from the input images using a predetermined step direction. Following the step, the CNN yields a detection window containing the features classified as a positive class (contains an object) or a negative class. The extracted features are then used to train a shallow classifier, such as an SVM or a boosting algorithm. In [35], the author utilizes an ensemble of boosting algorithms trained on the features collected from the CNN. While in [36] and [37], region proposal network\u00b9 was proposed in [22] is used as an initial detector, the results of which refined via a shallow network to improve detection results.\nIn fully DL-based pedestrian detection, DL models are trained end-to-end to learn how to perform feature extraction and how to detect and classify the results. The step usually includes training the backbone network, which is usually a CNN using backpropagation methods that work by propagat-ing the error from the output layer back through the network to adjust the weights of each neuron in each layer. The error is calculated as the difference between predicted and actual output. Afterward, the error is used to adjust the weights in each layer using gradient descent; therefore, the CNN would be able to extract features automatically. The next step is to generate proposals, explained as candidate regions in the image that may contain objects, in this case, pedestrians. This process can use sliding window techniques [21] or a region proposal network [36]. Then, a CNN will classify each proposal from the previous step into a different class, usually one containing objects. The final step is to apply post-processing techniques such as non-maximum suppression [38], sorting the detected bounding boxes by their confidence scores, representing the likelihood that the box contains an object of interest. Then, starting with the highest-scoring box, it is compared to all the other boxes that have not yet been suppressed. If the overlap between the two boxes, measured by their intersection over union, is above a certain threshold, then the box with the lower score is suppressed. We repeat this process for all remaining boxes until no more boxes can be suppressed. The threshold is often set at 0.5, so if two bounding boxes overlap by more than 50%, we consider them duplicates and suppress the one with a lower confidence score. While hybrid-based methods can achieve good detection performance, they are typically slower than DL-based methods due to their reliance on handcrafted features and separate pro-posal generation and classification steps. Additionally, hybrid-based methods require more manual parameter tuning than end-to-end trained DL models. In a DL-based method, the CNN is trained end-to-end to learn the feature extraction and classification steps. The CNN generates proposals directly from the input images and then classifies each as containing a pedestrian or not using the same CNN. This approach eliminates the need for handcrafted features and allows faster and more accurate detection performance. Therefore, it is more suitable for real-time use."}, {"title": "III. SYSTEM MODEL AND PROPOSED METHODS", "content": "Our AI camera system is deployed on the traffic light infrastructure, as shown in Fig. 1. The AI camera captures pedestrians crossing the road. After detecting a pedestrian, the AI camera system forwards the captured images to the integrated edge devices for processing. The edge devices analyze the images and generate warnings or alerts for nearby drivers in case of any extraordinary occurrences or potential safety hazards. Due to the critical nature of these warnings for public safety, they are given high priority within the net-work infrastructure, thus ensuring that the alerts are promptly communicated to drivers, allowing them to take necessary precautions and respond accordingly.\nIn the deployment, we assume the camera is mounted on the traffic light. Based on this assumption, we can estimate the camera's location based on the traffic light's height and width. We can estimate the camera's location per Fig. 2. For example, assuming that the height of the traffic light (AB) is 6 meters, the width (BC) is 3 meters, the length of the two road lanes (AE) is 9.2 meters, and the distance between the camera and the opposite road lane is 6.2 meters. Therefore, the distance from the side where the traffic light is mounted (AC) is 6.7 meters, while the distance from the opposite side of the crosswalk is 8.63 meters.\nPutting the camera on top of the traffic light provides the optimal view to detect pedestrians crossing the other side of the road. Fig. 3 details the detection area. By mounting the camera on top of the traffic poles, the camera would have the optimal field of view (FoV) to capture data for the model, resulting in a larger and broader FoV. A greater FoV can capture more contextual information, potentially assisting the object detection algorithm to understand the scene better and make more accurate predictions. It can be helpful when objects of interest interact with others or the environment. A larger FoV can help the model understand the spatial relationships between objects, which is especially important when objects may occlude each other or when the scene requires more context for accurate detection."}, {"title": "A. Proposed pedestrian Model for edge computing", "content": "The most important component for real-time pedestrian detection is the DL model. In order to be deployed in real-time, DL must have a fast detection time and use minimal computational power to fulfill this requirement. DL models may be divided into two categories: single-stage and two-stage detectors. Single-stage detectors directly predict object bound-ing boxes and class probabilities from a single pass through the network without requiring additional region proposal process-ing steps. They use dense sampling and predefined boxes/key points of various scales and aspect ratios to localize objects in a single shot. Examples of this category are YOLO [39], Single Shot Detector [40], and RetinaNet [41]. On the other hand, two-stage detectors have a separate module to generate region proposals in the first stage, followed by the classification and localization of objects in the second stage. These models try to find an arbitrary number of potential objects in an image during the first stage. In the second stage, the model attempts to classify and localize objects. Examples of this category are faster RCNN [22], EfficientNet [42], and CenterNet [43]. This research used a YOLOv3-based DL model due to its fast detection time and minimal computational power. We used transfer learning to train the model. The main reason for using this method is the DL model's ability to adapt to new data. Because the DL model has acquired information from the previous data, its knowledge can be leveraged to learn new information. This is due to the ability of a CNN to recognize patterns with similar connections or features.\nWe implement the model explained in [44] to further opti-mize the model to fulfill the requirement. The author explained that by reducing the depth of the convolutional layer, the model would have a decreased running speed, but its accuracy would decrease.\nThe DL model will then be deployed on AIoT devices at road intersections to monitor pedestrian crossings. The AIOT devices stream real-time detection results to the edge server via an MQTT broker. The edge server then processes the results based on specific events. If the event is detected, the edge server sends a notification to the nearby vehicles."}, {"title": "B. Deep Learning Model", "content": "In this research, we will implement a two-stage detection model, with each stage handling a specific computer vision task. Stage 1 will deal with object detection, which is used to determine the pedestrian's coordinates relative to the input received from the camera. Then, it will pass into Stage 2, which deals with pose estimation to predict the pedestrian's direction. The more detailed system model is explained in the figure below.\nIn this research, we implement a lightweight DL model optimized for IoT devices. One critical component of this optimization is reducing the depth of the neural network with-out reducing its performance. While deeper neural networks generally produce better results, their main disadvantage is the resource requirement. A DNN requires many resources, memory requirements, and computing power, which may not be available on edge devices with limited memory and computing power.\nWe based our model on YOLO-V3 Tiny [44]. It was discovered that adding a pooling layer and reducing the depth of the convolutional layer can still achieve a high accuracy yet a faster inference time. In our DL model, we use a CNN architecture and MobileNet, wherein the architecture implements a technique called inverted residuals and linear bottlenecks. The technique involves changing the input into a low-dimensional representation and then expanding the input model into a high-dimensional representation after implement-ing depth-wise convolution. A benefit of this architecture is that features can be extracted without requiring a deep convolutional layer, i.e., the architecture is considered mobile-friendly and could be implemented on edge devices. The model has three parts: the backbone, neck, and head. The backbone is a neural network architecture used to extract features."}, {"title": "C. Model Backbone", "content": "In this research, we implement MobileNetv2 as the back-bone network. The main reasons we used MobileNetv2 are the inverted residual blocks and the linear bottleneck layers. The inverted residual block consists of three components:\n1) Expansion: In this step, the number of channels $C$ of the input are expanded by a factor $t$. When $t > 1$, the number of channels in the intermediate feature maps $O$ increases per (1).\n$O= C*t$                                                                                                                      (1)\n2) Depthwise Convolution: The depthwise separable con-volution involves applying a depthwise convolution with a kernel size of $k \\times k$ expanded on the feature maps. Depthwise convolution applies a separate convolution filter to each channel of the input. The number of depthwise convolution parameters $D$ is per (2).\n$D = k^2 \\times C$                                                                                             (2)\n3) Pointwise Convolution (Projection) uses a 1x1 kernel applied to the feature maps to project back to the desired number of channels. The number of pointwise convolution output channels $C$. The Total Number of Parameters in the Inverted Residual Block $Tp$ is per (3).\n$Tp = C * t^2 + k^2 * C + C * C$                                            (3)\nIn MobileNetV2, the output of an inverted residual block is the sum of the input to the block and the output of the last bottleneck layer. This \"shortcut connection\" aids in the preservation of information across. The input to the block is specifically passed through a bottleneck layer, which reduces the number of channels in the feature map, followed by a series of depthwise separable convolutions, which act as a spatial filter to the feature map. The depthwise separable convolutions are then followed by another bottleneck layer that restores the number of channels to its original size. After that, the output of the last bottleneck layer is added to the input of the inverted residual block to produce its output. This output is then passed to the next layer in the neural network."}, {"title": "D. Model Neck and Head", "content": "You Only Look Once (YOLO) version 3 is a popular object detection model known for its real-time performance and efficiency. It performs detection in a single pass over the image via a single fully connected layer, enabling high frame rates and responsiveness. The model utilizes multiple output layers with different scales to detect objects of various sizes effec-tively. Despite not achieving the highest accuracy, YOLOv3 balances speed and accuracy, making it suitable for real-world applications. It is versatile and adaptable to different object detection tasks and custom datasets. As YOLOv3 is open-source and has an active community, it receives continuous improvements and support.\nYOLOv3 receives input with different scales of feature maps from the MobileNet backbone model. In YOLOv3, the network's neck utilizes three different scales of features for effective object detection: 1) High-scale features originate from the output layer with the lowest stride (32), capturing fine details and being ideal for detecting smaller objects. 2) Medium-scale features are derived from the output layer with a higher stride (16), offering a coarser spatial resolution and excelling at detecting medium-sized objects. 3) Low-scale features come from the output layer with the highest stride (8), which provides the coarsest spatial resolution and specializes in detecting large objects. The corresponding detection heads predict bounding boxes, objectness scores, and class probabil-ities for small, medium, and large objects. Combining these scales enhances YOLOv3's ability to handle objects of varying sizes efficiently and makes it a favored choice for real-time object detection tasks. The convolutional layers in the neck of YOLOv3 extract visual features from the input feature maps. The convolutional layers vary in filter size and number to better capture features at different scales and complexities. The upsampling layer increases the spatial resolution of the feature map of a lower-scale detection layer to match the resolution of a higher-scale detection layer, ensuring concatenation compatibility. The concatenation layer combines feature maps from different scales along the channel dimension, allowing the model to fuse multi-scale information and more comprehensively represent the input image. This multi-scale feature fusion is critical for YOLOv3's ability to detect objects of varying sizes, making it robust for real-time applications. The combined feature maps from different scales are further processed using additional convolutional layers after the concatenation operation in the neck of YOLOv3. These convolutional layers are critical in refining the multi-scale feature representation and extracting more abstract features for final object detection.\nTo optimize the model, we reduce the number of convolu-tional layers to make it more compact and efficient. However, balancing reducing model complexity and maintaining detec-tion performance is critical. Removing too many convolutional layers may lose essential features and reduce the model's accuracy."}, {"title": "E. Dataset", "content": "We train our model using the Crowdhuman dataset [45]. The Crowdhuman dataset comprises over 15,000 images with over 470,000 annotated human instances at different scales, viewpoints, and occlusion levels. The dataset is gathered from various locations, such as streets, shopping centers, and airports, and is captured using different types of cameras. It is usually used to train the model for tasks involving detecting people in dense crowds. The Crowdhuman dataset is large and rich and contains a high diversity of people in different occlusion scenarios.\nIn order to prove that our model performs better than the available models, we compare widely used DL models pre-trained with the Crowdhuman dataset. The models that we compare with are Retinanet [41], Faster RCNN [36], EfficientNet [42], and Single Shot Detector [40]."}, {"title": "F. MQTT protocol for Internet of Things", "content": "In order to communicate effectively, IoT devices need reliable real-time protocols that operate in low-bandwidth unreliable networks. Initially developed by IBM [46], Mes-sage Queuing Telemetry Transport (MQTT) is a lightweight messaging protocol enabling IoT device communication. It is designed to be efficient and reliable, even in low-bandwidth and unreliable network environments. MQTT supports several Quality of Service (QoS) levels that ensure message delivery during network disruptions or failures.\nMQTT operates on the publish-subscribe model wherein a client that publishes a message is decoupled from the other clients that receive the message in the publish-subscribe asynchronous pattern. Only one client receives the message, reducing the risk of leaked messages. A typical MQTT system consists of a publisher, a broker, and one or more subscribers [46]. The device or client that sends messages to the broker is the publisher. The messages can contain various data, including sensor readings, device states, and commands, and are categorized by topic. The broker, an intermediary between publishers and subscribers, distributes messages to all connected topic subscribers. The broker is responsible for managing all the connections, handling the message queue, and ensuring that messages are only delivered to the intended recipients. Subscribers, i.e., clients who receive broker mes-sages, may elect to receive messages from one or more topics. After receiving a message, a subscriber can process or act on the message. Subscribers can also unsubscribe from topics if they elect to do so."}, {"title": "IV. SIMULATION SETUP", "content": "Simulating pedestrian crossing scenarios using videos has several advantages. One of the main advantages is that it allows researchers to create controlled environments for testing and evaluating pedestrian models, algorithms, and systems. Simulations also allow for testing extreme scenarios that may not be safe or feasible to replicate in real-life testing. For example, simulating a pedestrian crossing in a dense traffic area at peak hours allows researchers to test the effectiveness of different traffic management strategies while minimizing pedestrian risks.\nIn order to replicate the pedestrian crossing the road sce-nario, in this research, we utilized Unity [47] combined with the Traffic3D library. Unity is intended to be versatile and adaptable, allowing developers to create games and simula-tions of any genre or style. Traffic3D is a traffic simulation tool that offers a rich 3D environment for training intelligent traffic management agents. It is intended to create visually and physically intelligent traffic simulation models to test new technology for eventual deployment in the real world. Traffic3D allows for scene generation programmatically, even during runtime, and supports a plug-and-play architecture to ensure the stability and generalizability of agents trained with Traffic3D to variations of the environment."}, {"title": "A. Deep Learning Model", "content": "In this experiment, we will use MMDetection [48], an open-source framework based on Pytorch [49] developed by the OpenMMLab team from Nanyang Technological University, to build custom DL models. Using the MMDetection framework, we train Retinanet [41], Faster RCNN [36], EfficientNet [42], and Single Shot Detector [40] as well as our optimized Tiny-YOLOv3 model on the Crowdhuman [45] dataset using pre-trained weights to maintain fairness across each category of the protected features when evaluating the model.\nOur model is then deployed on an inexpensive Nvidia Jetson Nano, i.e. a powerful single-board computer designed specifi-cally for AI applications. A Jetson Nano is particularly well-suited for applications such as autonomous vehicles, robotics, and smart cameras, where high-performance processing and low power consumption are critical. The main Jetson Nano B01 parameters are detailed in table I."}, {"title": "B. MQTT Broker Setup", "content": "In this research, we use Eclipse Mosquito [50], a small, lightweight MQTT server, as the MQTT Broker. In our testbed, we use a personal computer as the MQTT broker is linked to the Jetson Nano via a wireless network. The MQTT broker controls the message flow and ensures that publisher messages reach their intended subscribers. The wireless network acts as a conduit for communication between the MQTT broker and the MQTT subscriber, both connected to the same wireless network.\nThe MQTT subscriber, a device or an application, indicates a desire to be interested in specific topics and connects to the MQTT broker via the wireless network. This configuration simulates a wireless network environment similar to LTE or 5G networks wherein the MQTT server would reside within the cellular network, and its subscribers would be connected to the same network."}, {"title": "C. Deployment Diagram", "content": "Fig. 7 shows our system testbed topology. For DL process-ing, all input images must be resized to be similar to the images used for training. After that, the images are processed by three functional blocks: MobileNet, Tiny YOLO, and the MQTT server."}, {"title": "V. SIMULATION AND ANALYSIS", "content": "We conducted simulations to evaluate the performance of our pedestrian detection model using two types of testing to showcase the model's capabilities. The first type of testing was conducted before deployment, aiming to assess the model's effectiveness in detecting pedestrians in the given context. We evaluated the model's performance using multiple metrics during this initial testing phase. The primary metric used for assessment was the Mean Average Precision (mAP), which measures the model's accuracy in detecting pedestrians. Ad-ditionally, we analyzed the model's computational efficiency by considering the number of Floating Point Operations Per Second (FLOPS) required to execute the DL model. Finally, we examined the model's size in terms of its parameters, which provides insights into the memory requirements and storage space. We used an RTX 3090 GPU with the Ubuntu 18.04 operating system to conduct these tests. The simulation used MMDetection version 2.25.1, Pytorch v.13.1, and Python 3.8. This configuration allowed us to thoroughly evaluate the model's performance and determine its suitability for pedestrian detection in our target scenario.\nIn the second testing stage, we deployed the pedestrian detection model on an edge AIoT device (Nvidia Jetson Nano). The objective of this phase was to evaluate the model's performance in real-time scenarios. Given the time-sensitive nature of pedestrian detection tasks, it was crucial for the model to accurately and swiftly identify pedestrians. We con-sidered several key metrics to assess the model's capabilities. First, we examined the confidence score of the model, which represents its accuracy in correctly detecting pedestrians. Then, we analyzed the inference times, which indicate the speed at which the model can process and generate predictions. Finally, we evaluated the memory usage of the model to ensure it met the resource constraints of the edge AIoT device. We opted to deploy the model on a Jetson Nano device for this evaluation. The selection of the Jetson Nano was justified by its suitability for edge computing tasks, optimized hardware design for DL inference, and cost. The Jetson Nano balances computational power and energy efficiency, making it suitable for real-time pedestrian detection on edge devices."}, {"title": "A. Analysis", "content": "The testing was conducted using an RTX 3090 GPU and a subset of the Crowdhuman dataset, consisting of 4,370 annotated images. A summary of the results obtained during the initial testing phase is provided in table II. It can be observed that the size of the model does not exhibit a linear relationship with its performance. Despite the Single Shot Detector model having the largest size and number of FLOPS, it performed poorly compared to other models. On the other hand, our model demonstrated compactness in terms of model size and FLOPS, but its mAP results were not the best. The EfficientNet model has the highest mAP, yet it requires over twice as many parameters as our model. While these findings provide insights into our model's compactness and performance, it is important to note that further analysis is required to evaluate its real-time deployment capabilities. The speed and performance of the model in a real-time scenario need to be assessed to draw a comprehensive conclusion.\nFig. 8 shows that our model exhibits the highest inference rate / lowest inference time compared to the other models. It surpassed the slowest model by nearly 20\u00d7 and was over three times as fast as the second fastest model. This remarkable speed can be attributed to the optimization techniques that precisely reduce the convolutional layers. We achieved a more streamlined and efficient system by simplifying the model's architecture. Reducing convolutional layers led to faster in-ference times and contributed to a smaller model size. In the subsequent evaluation phase, we focus on assessing the accuracy of our optimized model.\nFigs. 9 illustrate that our model achieves a satisfactory accu-racy of 78% while demonstrating significantly faster inference times. Although our model's accuracy is not the highest, it outperformed the Faster RCNN model in speed. In [51], Huang et al. explained that in some cases, we have to decide whether to use a model with high accuracy or a fast inference time. In pedestrian detection, where real-time processing is cru-cial, a model with faster inference time is preferred. When considering pedestrian detection speed, we also consider the system model latency, pedestrian detection model inference time + time to transmit detection result to the MQTT server. This is captured by the system model latency shown in Fig. and 10. Based on the results, our system model is over twice as fast as the second-fastest model. Our proposed pedestrian detection system model has a latency of 424 ms; this is twice as fast as the second-fastest model. When considering the detection frame rate, as shown in Fig. 11, it becomes evident that the highest accuracy model (Faster RCNN) is unable to achieve sub-second pedestrian detection, i.e., it cannot effectively detect pedestrians in real-time (frame rate > 1) scenarios.\nFig. 12 presents the model evaluation results, indicating that our optimized model requires significantly less memory than larger models. Despite its compact size, our model performs similarly to the larger models. Specifically, our model requires only 10% of the resources used by the largest model, Faster RCNN, while achieving a similar performance. This highlights the efficiency and resource-saving benefits of our optimized model.\nWe now compare our optimized YOLOv3 model to the original YOLOv3 model to determine if the optimizations"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "Based on our research, deploying the optimized DL model on IoT edge devices has yielded excellent real-time pedestrian detection results. The model's efficient optimization allows it to run on resource-constrained devices with minimal compu-tational power, achieving high accuracy and fast detection.\nIntegrating our system with commercially available LTE or 5G networks in the future presents a significant challenge. This would require ensuring seamless compatibility with cel-lular networks to enhance system performance. Ultimately, our system provides an affordable and intelligent alternative for intelligent transportation systems, promising significant benefits in cost-effective and efficient solutions."}]}