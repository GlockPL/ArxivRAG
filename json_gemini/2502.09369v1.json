{"title": "Language Agents as Digital Representatives in Collective Decision-Making", "authors": ["Daniel Jarrett", "Miruna P\u00eeslar", "Michiel A. Bakker", "Michael Henry Tessler", "Raphael K\u00f6ster", "Jan Balaguer", "Romuald Elie", "Christopher Summerfield", "Andrea Tacchetti"], "abstract": "Consider the process of collective decision-making, in which a group of individuals interactively select a preferred outcome from among a universe of alternatives. In this context, \"representation\u201d is the activity of making an individual's preferences present in the process via participation by a proxy agent\u2014i.e. their \u201crepresentative\u201d. To this end, learned models of human behavior have the potential to fill this role, with practical implications for multi-agent scenario studies and mechanism design. In this work, we investigate the possibility of training language agents to behave in the capacity of representatives of human agents, appropriately expressing the preferences of those individuals whom they stand for. First, we formalize the setting of collective decision-making-as the episodic process of interaction between a group of agents and a decision mechanism. On this basis, we then formalize the problem of digital representation\u2014as the simulation of an agent's behavior to yield equivalent outcomes from the mechanism. Finally, we conduct an empirical case study in the setting of consensus-finding among diverse humans, and demonstrate the feasibility of fine-tuning large language models to act as digital representatives.", "sections": [{"title": "Introduction", "content": "Collective decision-making is a hallmark of intelligent behavior [1,2], and is ubiquitous in economic, social, and political spheres of society [3,4]. Consider any process in which a group of individuals in-teractively select a preferred outcome from a universe of alternatives: For instance, consider a diverse group of humans communicating in a mediated environment, seeking to arrive at a consensus opinion on a topic of debate [5,6]. In this context, \u201crepresentation\u201d is the activity of making an (otherwise absent) individual's preferences present in the process through participation by a proxy agent-i.e. their \"representative\" [7]. To this end, learned models of human behavior have the potential to fill this role, with practical implications for scenario studies and mechanism design. For instance, it can facil-itate personalized and detailed simulations of collective interactions, allowing iterative refinement of mechanisms before real-world deployment. The key question is: What makes a good representative?\nSimulation for Representation In this work, we explore the possibility of training language agents to behave in the capacity of representatives of human agents, appropriately expressing the preferences of those individuals whom they stand for. On the one hand, this has natural connections to existing work in simulating human behavior, such as learning from demonstrations [8\u201311], generating syn-thetic data, [12-15], and creating plausible simulacra of social agents [16\u201321]. On the other hand, unlike such prior work, our objective in simulating human behavior is specifically for \u201crepresentation\". This entails unique requirements: (a) grounding in the context of collective interaction, (b) attending to the granularity of individual fidelity, and (c) operating in the high-dimensional domain of language space. Specifically, in this work, we are exploring what representation means, how to measure repre-sentativity, and whether language agents can be trained to act as representatives on behalf of humans."}, {"title": "Collective Decision-Making", "content": "Consider a general setting for collective decision-making, where interactions between a group of individuals are mediated by a decision mechanism, yielding a process that produces a final outcome. By way of preliminaries, we first formalize the notion of a social choice function, which is an abstract mapping from preferences to outcomes that is implemented by a mechanism:\nDefinition 1 (Social Choice Function) Let $N := \\{1, ..., n\\}$ denote a set of participants, who are engaged in the process of making a collective decision. Denote with $\\Omega$ the space of outcomes, from which the participants are required to make a final selection $\\omega \\in \\Omega$. Denote with $\\theta_i \\in \\Theta_i$ the type characterizing each participant $i \\in N$, which captures their preferences over different outcomes. Moreover, let $\\theta := (\\theta_i)_{i \\in N} \\in \\Theta := \\Theta_1 \\times \\dots \\times \\Theta_n$ indicate the type profile of all participants. Then a social choice function $h$ is a mapping from the space of type profiles into the space of outcomes,\n$h: \\Theta \\rightarrow \\Omega$\n(1)\nIn line with related fields, \u201cparticipants\u201d may also be referred to as \u201cagents\u201d and \u201cplayers\u201d, and an \"outcome\" may be synonymous with an \u201calternative\u201d and a \u201ccollective decision\".\nWe operate in the standard setting for discrete-time Markov decision processes. Let $x \\in X$ denote the state variable: While we keep notation simple, x may play the role of \"contexts\u201d and \u201chistories\u201d that capture all observations prior to the current time step. Let $u_i \\in U_i$ denote the action variable for $i \\in N$, and let $u := (u_i)_{i \\in N} \\in U := U_1 \\times \\dots \\times U_n$ indicate the action profile for all participants. In our setting, u will largely play the role of \"utterances\u201d in language space. Denote with $T$ the finite length of each decision episode. The outcome of each episode is precisely the terminal state $\\omega := x_T$.\nDefinition 2 (Decision Mechanism) Each participant $i \\in N$ is associated with a behavior denoted with $\\pi_i \\in \\Pi_i \\subseteq \\Delta(U_i)^X$, and let $\\pi := (\\pi_i)_{i \\in N} \\in \\Pi := \\Pi_1 \\times \\dots \\times \\Pi_n$ give the behavior profile for all participants. A decision mechanism $\\tau$ is a mapping from states and action profiles to next states,\n$\\tau \\in T \\subseteq \\Delta(X)^{X \\times U}$\n(2)\nand an outcome function $f$ maps behavior profiles and mechanisms into distributions over outcomes. In our Markov decision process setting, the outcome function is simply the result of \u201crolling out\u201d an episode of the interactive process between $\\pi$ and $\\tau$ to arrive at (a distribution over) terminal states,\n$f : \\Pi \\times T \\rightarrow \\Delta(\\Omega)$\n(3)\nA \"behavior\" may synonymously be referred to as a \u201cpolicy\u201d or \u201cstrategy\u201d, and a \u201cmechanism\" may be referred to as a \u201cgame\u201d or \u201cenvironment\u201d. Our setting is in general non-stationary: We may use superscripts $t \\in \\{1, . . ., T\\}$ to indicate time steps, but omit them unless explicitly required.\nFinally, a mechanism $\\tau$ is said to implement a social choice function $h$ when $h(\\theta) = f(\\pi, \\tau)$ for all $\\pi \\in \\Pi$. Both $\\pi, \\tau$ may depend on $\\theta$, though not required. Often, mechanisms are designed with an optimization objective in mind. For instance, let each participant be associated with a payoff function,\n$g_i: \\Omega \\times \\Theta_i \\rightarrow \\mathbb{R}$\n(4)\nsuch that $g_i(\\omega, \\theta_i)$ gives the payoff that participant $i \\in N$ enjoys from the collective decision $\\omega$. As before, for convenience, let $g := (g_i)_{i \\in N}$ and (with some abuse of notation) write $g : \\Omega \\times \\Theta \\rightarrow \\mathbb{R}^n$. Then a \u201cutilitarian\u201d social choice function is implemented as $h(\\theta) = f(\\pi, \\tau^*)$, by the mechanism:\n$\\tau^* \\in \\arg \\max_{\\tau \\in T} \\mathbb{E}_{\\omega \\sim f(\\pi, \\tau)} G(\\omega, \\theta) \\quad \\text{where} \\quad G(\\omega, \\theta) := \\frac{1}{n} \\sum_{i=1}^n g_i(\\omega, \\theta_i)$\n(5)\nNote that a \"payoff function\" is often interchangeable with a \u201creward function\u201d or \u201cutility function\". In this work, we use the scenario of consensus-finding among individuals as our illustrative example."}, {"title": "Digital Representatives", "content": "Suppose the true policy profile $\\pi^* \\in \\Pi$, and we are interested in a model policy profile $\\tilde{\\pi}$ that approxi-mates some or all of $\\pi^*$ with digital representatives, in the context of decision mechanisms $\\tau \\in T$. (For example, this might involve substituting a part of a single participant's policy, or multiple participants' policies, within $\\pi^*$). What defines the set of profiles $\\tilde{\\pi}$ that we can consider as \u201cequivalent\u201d to $\\pi^*$?\nDigital Clones\nOne obvious choice is to require that $\\pi^*(u|x) = \\tilde{\\pi}(u|x)$ for all $x \\in X$ and $u \\in U$, essentially seeking an identical clone of $\\pi^*$. This is fine as an objective for training language agents to serve as digital representatives\u2014in fact, likelihood-based training is exactly what we perform in Section 4. However, we argue that matching conditionals alone is incomplete for evaluating digital representatives, as it may be deemed both \"too strong\u201d and \u201ctoo weak\u201d. On one hand, it is \u201ctoo strong\u201d as $\\tilde{\\pi}$ may not need to clone the entirety of $\\pi^*$\u2014in the context of $\\tau$. Intuitively, suppose there were some decomposition,\n$U = U_{\\parallel} \\times U_{\\perp}$\n(6)\nsuch that $U_{\\parallel}$ somehow encapsulated the \u201crelevant\u201d dimensions of utterances, whereas $U_{\\perp}$ encapsulated the"}, {"content": "s. \u201cstyle\u201d distinction in the context of image classification [23], or the \u201cmeaning\u201d vs. \u201cform\u201d distinction in the context of semantic clustering [24]. If this is true, we would only care about matching the conditional,\n$\\pi(u_{\\parallel}|x) = \\int_{U_{\\perp}} \\pi(u|x) du_{\\perp}$\n(7)\nThat is, we would simply require that $\\pi^*(u_{\\parallel}|x) = \\tilde{\\pi}(u_{\\parallel}|x)$, for $x \\in X$ and $u_{\\parallel} \\in U_{\\parallel}$. If we had access to such a decomposition, the matter would be settled. But such a factoring is seldom apparent.\nOn the other hand, defining representativity purely on the basis of conditionals is also \u201ctoo weak\", as the policies in $\\tilde{\\pi}$ are ultimately unrolled in one or more transitions by interacting through the mecha-nism. Any discrepancy in conditionals may lead to compounding errors through such interactions.\nDigital Representatives\nA complete measure of representativity should also consider the dynamics of interaction. Here, we draw connections with value-aware learning [25, 26] and value equivalence [27-30] in model-based reinforcement learning. This allows us to define representation in a way that addresses both concerns at once. First, some more notation: Given any $\\pi \\in \\Pi$, $\\tau \\in T$, the vector of expected payoffs to participants at state $x$ taking action $u$ at time $t$ is given by the \u201cvalue function\u201d $Q_{\\pi, \\tau}^t : X \\times U \\rightarrow \\mathbb{R}^n$:\n$Q_{\\pi, \\tau}^t(x, u) = \\mathbb{E}_{x_T \\sim f(\\pi, \\tau)} [g(x_T, \\theta) | x_t = x, u_t = u]$\n(8)\nMoreover, define the Bellman operator $B_{\\pi, \\tau}$ over the space of functions $Q : X \\times U \\rightarrow \\mathbb{R}^n$ such that\n$(B_{\\pi, \\tau} Q)(x, u) := \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, u)} \\mathbb{E}_{u' \\sim \\pi(\\cdot|x')} Q(x', u')$\n(9)\nThen the sequence of functions $Q_{\\pi, \\tau}^t$ is the unique solution to backward recursions $Q^t = B_{\\pi, \\tau} Q^{t+1}$ for $t \\in \\{1, ..., T - 1\\}$, and $Q^T(x, u) := g(x, \\theta)$. Now we examine three notions of equivalence:\nDefinition 3 (Representational Equivalence) Fix $\\Pi$ and $T$. Define first the set of policy profiles $\\pi \\in \\Pi$ for which actions profiles $u \\sim \\pi(\\cdot|x)$ are equal in distribution to $u \\sim \\pi^*(\\cdot|x)$ given any state:\n$\\Pi(\\pi^*) := \\{\\pi \\in \\Pi : \\pi^*(\\cdot|x) = \\pi(\\cdot|x) \\, \\forall x \\in X \\}$\n(10)\nInstead of matching conditionals in isolation, we may focus on transitions: Define the set of policy profiles $\\pi \\in \\Pi$ whose operators $B_{\\pi, \\tau}$ have equal effect to $B_{\\pi^*, \\tau}$ on any function $Q \\in \\mathcal{Q} \\subseteq (\\mathbb{R}^n)^{X \\times U}$.\n$\\Pi(\\pi^*, T, \\mathcal{Q}) := \\{\\pi \\in \\Pi : B_{\\pi^*,\\tau} Q = B_{\\pi,\\tau} Q \\, \\forall \\tau \\in T \\,\\text{and}\\, Q \\in \\mathcal{Q} \\}$\n(11)\nFinally, we may assess equivalence based on payoffs of trajectories: Define the set of policy profiles whose operators $B_{\\pi, \\tau}^T$ have identical effect to $B_{\\pi^*, \\tau}^T$ when all applied onto any $Q^T \\in \\mathcal{Q} \\subseteq (\\mathbb{R}^n)^{X \\times U}$.\n$\\Pi^\\backsim(\\pi^*, T, \\mathcal{Q}) := \\{\\pi \\in \\Pi : B_{\\pi^*,\\tau}^T Q^T = B_{\\pi,\\tau}^T Q^T \\, \\forall \\tau \\in T, Q^T \\in \\mathcal{Q} \\}$\n(12)\nSince repeated application of a Bellman operator $T$ times turns any function $Q^T$ into a value function for the beginning of an episode, this condition effectively asks for equality in expected payoffs."}, {"title": "Case Study: Consensus-Finding", "content": "In the context of consensus-finding (viz. Figure 1) as collective decision-making, we now turn to the empirical question: Is it feasible to train language agents to act as digital representatives of humans? Indeed, large language models are often pre-trained on datasets rich in diverse preferences, and recent work has shown that language agents can generate plausible behavior in interactive settings [21], and to simulate subpopulations of humans in studies in economics [20], psychology [17], and social science [19]. However, while socio-demographically prompted models can capture the sentiment of general subpopulations of humans [31,32], they are not as reliable on more granular levels [18,33]. Is it possible to fine-tune language models on a personalized level to represent individuals in a group?\nExperiment Setup\nIn this work, we train and evaluate digital representatives using a consensus-finding dataset from [6]. For completeness, we briefly recall the input questions and data collection process [6]. Then we give an overview of the final dataset and the training process for digital representatives in our experiment."}, {"title": "Experiment Setup", "content": "In this work, we train and evaluate digital representatives using a consensus-finding dataset from [6]. For completeness, we briefly recall the input questions and data collection process [6]. Then we give an overview of the final dataset and the training process for digital representatives in our experiment."}, {"title": "Discussion", "content": "We fine-tune language agents to act as digital representatives in the context of collective decision-making, offering practical implications for scenario studies and mechanism design. In the sequel, we provide an overview of related work and conclude with a discussion on limitations and future work.\nRelated Work Our work in \u201csimulation for representation\u201d straddles two domains of research: in (1) how human behavior is simulated with models, and (2) how human behavior is represented by models.\nSimulation: First, imitation learning deals with training an artificial agent to mimic a demonstrator [8-11], to match some notion of performance [36\u201339], or to recover some underlying motive for their behavior [40-43]. Multiple artificial agents have also been cloned [44, 45], such as for simple control tasks [46], bandit environments [47], and driving simulation [48]. This contrasts with our high-dimensional language domain, and with our focus on \u201crepresentativity\u201d in collective decision-making. Second, synthetic data generation deals with sampling from learned distributions to approximate real phenomena, such as in sequential data [49\u201351], medical environments [14,52], driving simulation [15], as well as in language space for social science [12], privacy preservation, [53], and to augment text mining [13]. However, this contrasts with our emphasis on learning models to act in an interactive context. Third, recent work has explored creating plausible simulacra of social agents using language models, such as in creating populated prototypes for social computing [16], in prompting models to simulate human sub-populations [19] and replicate human subject studies [17, 18], in simulating economic agents [20], and in creating believable social behavior in sandbox environments [21]. In contrast, we fine-tune models on a personalized level to represent specific individuals within a group.\nRepresentation: Since large language models are often pre-trained on human data with diverse per-spectives, a recent line of work has seeks to measure the preferences that pre-trained models inherently reflect with prompting, such as the representation of broad-based global demographic groups [54], the alignment of their views with demographic categories in the United States [31], as well as to uncover the intrinsic ideological biases that pre-trained models exhibit [55]. Secondly, a related strand of research systematically probes pre-trained models to actively simulate biased perspectives, such as by prompting with socio-demographic profiles [33], as well as by prompting with liberal or conservative profiles to sample text with corresponding moral biases [32]. Some models have also been explicitly fine-tuned to query the opinions and worldviews of demographic categories of people [56, 57]. In contrast, however, in this work we seek to simulate behavior at the granularity of individual repre-sentativity, specifically in the context of collective interaction. Finally, while we draw an analogy with value-aware [25, 26] and value equivalent learning [27\u201330], our work has close connections with broader notions of representation [7] in collective interaction [3], consensus-based decision-making [4], and has potential implications for real and simulated deliberative democracy [58\u201361].\nFuture Work We began with the question: \"What makes a good representative?\". Our argument contends that a representative should not only capture the conditional dynamics of behavior but also ensure that its interaction through a mechanism preserves the trajectory-wise dynamics of outcomes. Several caveats are in order: First, there is no escaping the fact that the fidelity of any notion of \"representative\u201d is most solidly grounded in human endorsement. While we use a black-box payoff model as proxy, future work would greatly benefit from human validation of digital representatives. Second, in this work we focused on learning representatives for the critique-writing step only. Future work would benefit from naturally extending this to the opinion-writing step as well, yielding fully-simulated representatives. Thirdly, although we defined and evaluated representatives based on an equivalence objective, in our experiments the models were trained on a standard likelihood-based fine-tuning objective. Future work could explore directly training for the equivalence objective.\nBroader Impact It is important to emphasize that our training of digital representatives is geared toward simulation, not advocating for their deployment as substitutes for human accountability in decision-making. For instance, suppose we were interested in using digital representatives for the purpose of simulating outcomes to improve some downstream tasks. Then evaluation of those tasks must be performed with real humans. This is crucial, since we do not train representatives on any notion of factuality or transparency. In fact, representatives are specifically trained to mimic their human inputs per se, thereby carrying over any biases from those corresponding humans. That said, this research was conducted with a focus on societal benefit. The ultimate goal is to facilitate granular simulations of collective interactions for policy design, allowing decision mechanisms to benefit from scalable and cost-effective iterative refinement before real-world deployment."}, {"title": "Proof of Proposition 1", "content": "Proposition 1 (Representational Equivalence) Fix $\\Pi$ and $T$, and let $\\mathcal{Q}$ be closed under Bellman updates. Consider the equivalence classes of policy profiles induced by $\\pi^*$ from Definition 3. We have\n$\\Pi(\\pi^*) \\subseteq \\Pi(\\pi^*, T, \\mathcal{Q}) \\subseteq \\Pi^\\backsim(\\pi^*, T, \\mathcal{Q})$\n(13)\nIn particular, consider the maximal space of functions $\\mathcal{Q} = (\\mathbb{R}^n)^{X \\times U}$. If the space of mechanisms is also maximal, that is $T = \\Delta(X)^{X \\times U}$, then we have that the first subset relation is an equality, that is\n$\\Pi(\\pi^*) = \\Pi(\\pi^*, T, \\mathcal{Q}) \\subseteq \\Pi^\\backsim(\\pi^*, T, \\mathcal{Q})$\n(14)\nLet action profiles be decomposable as $U = U_{\\parallel} \\times U_{\\perp}$ with $\\text{card}(U_{\\perp}) > 1$, and the interaction between the mechanism and policies be such that values $Q^t(x, u) = Q^t(x, (u_{\\parallel}, u_{\\perp})) = Q^t(x, u_{\\parallel})$ for all $t < T$, $x \\in X$, $u \\in U$, $\\pi \\in \\Pi$, and $\\tau \\in T \\subseteq \\Delta(X)^{X \\times U}$. Then the second subset relation is proper,\n$\\Pi(\\pi^*) \\subseteq \\Pi(\\pi^*, T, \\mathcal{Q}) \\subset \\Pi^\\backsim(\\pi^*, T, \\mathcal{Q})$\n(15)\nProof. The first subset in Expression 13 is immediate from Definitions 9 and 11. The second subset can be seen as follows: Fix $\\tilde{\\pi} \\in \\Pi(\\pi^*, T, \\mathcal{Q})$, so for any $\\tau \\in T$ and $Q \\in \\mathcal{Q}$ we have the following,\n$B_{\\pi^*,\\tau} Q = B_{\\tilde{\\pi},\\tau} Q$\n(23)\nBut $B_{\\pi^*,\\tau} Q$ and $B_{\\tilde{\\pi},\\tau} Q$ are also in $\\mathcal{Q}$ from the closure assumption, so we can repeatedly apply the Bellman operators $B_{\\pi^*,\\tau}$ and $B_{\\tilde{\\pi},\\tau}$ on the left and right hand sides for a total of $T$ times, therefore\n$B_{\\pi^*,\\tau}^T Q = B_{\\tilde{\\pi},\\tau}^T Q$\n(24)\nso $\\tilde{\\pi} \\in \\Pi^\\backsim(\\pi^*, T, \\mathcal{Q})$. Next, the equality in Expression 14 is obvious. Finally, the proper subset in Expression 15 can be shown by picking the following model policy $\\tilde{\\pi}$ as proof (note that $\\tilde{\\pi} \\notin \\Pi(\\pi^*)$):\n$\\tilde{\\pi}((u_{\\parallel}, u_{\\perp})|x) := 1\\{u_{\\perp}=\\tilde{u}_{\\perp}\\} \\pi^*(u_{\\parallel}|x)$\n(25)\nFirst, note that the value function $Q_{\\pi^*,\\tau}^t$ for the true policy $\\pi^*$ profile satisfies the following recursion:\n$Q_{\\pi^*,\\tau}^t(x, (u_{\\parallel}, u_{\\perp}))$\n(26)\n$= (B_{\\pi^*,\\tau} Q^{t+1})(x, (u_{\\parallel}, u_{\\perp}))$\n(27)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\mathbb{E}_{(u'_{\\parallel}, u'_{\\perp}) \\sim \\pi^*(\\cdot|x')} Q_{\\pi^*,\\tau}^{t+1}(x', (u'_{\\parallel}, u'_{\\perp}))$\n(28)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\int_{U'_{\\perp}} \\int_{U'_{\\parallel}} \\pi^*((u'_{\\parallel}, u'_{\\perp})|x') Q_{\\pi^*,\\tau}^{t+1}(x', (u'_{\\parallel}, u'_{\\perp})) du'_{\\parallel} du'_{\\perp}$\n(29)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\int_{U'_{\\perp}} \\int_{U'_{\\parallel}} \\pi^*(u'_{\\parallel}|x') \\pi^*(u'_{\\perp}|x') Q_{\\pi^*,\\tau}^{t+1}(x', (u'_{\\parallel}, u'_{\\perp})) du'_{\\parallel} du'_{\\perp}$\n(30)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\int_{U'_{\\parallel}} \\pi^*(u'_{\\parallel}|x') \\big[ \\int_{U'_{\\perp}} Q_{\\pi^*,\\tau}^{t+1}(x', u'_{\\parallel})du' \\big] du'_{\\parallel}$\n(31)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\mathbb{E}_{u'_{\\parallel} \\sim \\pi^*(\\cdot|x')} Q_{\\pi^*,\\tau}^{t+1}(x', u'_{\\parallel})$\n(32)\nfor $t \\in \\{1, ..., T - 1\\}$, and $\\mathbb{Q}^T_{\\pi^*,\\tau}(x, u) = g(x, \\theta)$. Likewise, the value function $Q_{\\tilde{\\pi},\\tau}^t$, for the model policy profile $\\tilde{\\pi}$ satisfies the following recursion:\n$Q_{\\tilde{\\pi},\\tau}^t(x, (u_{\\parallel}, u_{\\perp}))$\n(33)\n$= (B_{\\tilde{\\pi},\\tau} Q^{t+1})(x, (u_{\\parallel}, u_{\\perp}))$\n(34)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\mathbb{E}_{(u'_{\\parallel}, u'_{\\perp}) \\sim \\tilde{\\pi}(\\cdot|x')} Q_{\\tilde{\\pi},\\tau}^{t+1}(x', (u'_{\\parallel}, u'_{\\perp}))$\n(35)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\int_{U'_{\\perp}} \\int_{U'_{\\parallel}} \\tilde{\\pi}((u'_{\\parallel}, u'_{\\perp})|x') Q_{\\tilde{\\pi},\\tau}^{t+1}(x', (u'_{\\parallel}, u'_{\\perp})) du'_{\\parallel} du'_{\\perp}$\n(36)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\int_{U'_{\\perp}} \\int_{U'_{\\parallel}} 1\\{u'_{\\perp} = \\tilde{u}_{\\perp}\\} \\pi^*(u'_{\\parallel}|x') Q_{\\tilde{\\pi},\\tau}^{t+1}(x', (u'_{\\parallel}, \\tilde{u}_{\\perp})) du'_{\\parallel} du'_{\\perp}$\n(37)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\int_{U'_{\\parallel}} \\pi^*(u'_{\\parallel}|x') Q_{\\tilde{\\pi},\\tau}^{t+1}(x', (u'_{\\parallel}, \\tilde{u}_{\\perp})) du'_{\\parallel}$\n(38)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\mathbb{E}_{u'_{\\parallel} \\sim \\pi^*(\\cdot|x')} Q_{\\tilde{\\pi},\\tau}^{t+1}(x', (u'_{\\parallel}, \\tilde{u}_{\\perp}))$\n(39)\nfor $t \\in \\{1, ..., T - 1\\}$, and $\\mathbb{Q}^T(x, u) = g(x, \\theta)$. But $Q^t_{\\tilde{\\pi},\\tau}$ is a solution to this recursion, and as solutions to backward recursions are unique, we have that $Q^t_{\\tilde{\\pi},\\tau} = Q^t_{\\pi^*,\\tau}$, hence $\\tilde{\\pi} \\in \\Pi^\\backsim(\\pi^*, T, \\mathcal{Q})$.\nNext, we show $\\tilde{\\pi} \\notin \\Pi(\\pi^*, T)$: Pick $Q(x, (u_{\\parallel}, u_{\\perp})) := 1\\{u_{\\perp} \\neq \\tilde{u}_{\\perp}\\}$. Then we have that\n$(B_{\\pi^*,\\tau} Q)(x, (u_{\\parallel}, u_{\\perp})) = \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\mathbb{E}_{(u'_{\\parallel}, u'_{\\perp}) \\sim \\pi^*(\\cdot|x')} Q(x', (u'_{\\parallel}, u'_{\\perp}))$\n(40)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} P(u'_{\\perp} \\neq \\tilde{u}_{\\perp}|x', \\pi^*)$\n(41)\n$= P(u_{\\perp} \\neq \\tilde{u}_{\\perp}|x, (u_{\\parallel}, u_{\\perp}), \\pi^*, \\tau)$\n(42)\nand likewise,\n$(B_{\\tilde{\\pi},\\tau} Q)(x, (u_{\\parallel}, u_{\\perp})) = \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} \\mathbb{E}_{u'_{\\parallel} \\sim \\pi^*(\\cdot|x')} Q(x', (u'_{\\parallel}, \\tilde{u}_{\\perp}))$\n(43)\n$= \\mathbb{E}_{x' \\sim \\tau(\\cdot|x, (u_{\\parallel}, u_{\\perp}))} P(\\tilde{u} \\neq \\tilde{u}_{\\perp})$\n(44)\n$= 0$\n(45)\nSuppose it were true that $\\tilde{\\pi} \\in \\Pi(\\pi^*, T, \\mathcal{Q})$, so that for all $Q$ we have that $(B_{\\pi^*,\\tau} Q)(x, (u_{\\parallel}, u_{\\perp})) = (B_{\\tilde{\\pi},\\tau} Q)(x, (u_{\\parallel}, u_{\\perp}))$, which means $P(u'_{\\perp} = \\tilde{u}_{\\perp}|x, (u_{\\parallel}, u_{\\perp}), \\pi^*, \\tau) = 1$. But this is only possible if $U_{\\perp} = \\{\\tilde{u}\\}$, hence $\\text{card}(U_{\\perp}) = 1$, which is a contradiction with the premise that $\\text{card}(U_{\\perp}) > 1$."}]}