{"title": "STAR: Synthesis of Tailored Architectures", "authors": ["Armin W. Thomas", "Rom Parnichkun", "Alexander Amini", "Stefano Massaroli", "Michael Poli"], "abstract": "Iterative improvement of model architectures is fundamental to deep learning: Transformers first enabled scaling, and recent advances in model hybridization have pushed the quality-efficiency frontier. However, optimizing architectures remains challenging and expensive. Current automated or manual approaches fall short, largely due to limited progress in the design of search spaces and due to the simplicity of resulting patterns and heuristics. In this work, we propose a new approach for the synthesis of tailored architectures (STAR). Our approach combines a novel search space based on the theory of linear input-varying systems, supporting a hierarchical numerical encoding into architecture genomes. STAR genomes are automatically refined and recombined with gradient-free, evolutionary algorithms to optimize for multiple model quality and efficiency metrics. Using STAR, we optimize large populations of new architectures, leveraging diverse computational units and interconnection patterns, improving over highly-optimized Transformers and striped hybrid models on the frontier of quality, parameter size, and inference cache for autoregressive language modeling.", "sections": [{"title": "1 Introduction", "content": "Most domains of applications for AI have seen a gradual convergence towards similar model architecture designs, based on stacking multi-head attention and gated linear units (Transformers) (Vaswani et al., 2017; Shazeer, 2020; Brown, 2020) or combinations of other basic computational units grounded in signal processing, such as recurrences or convolutions (Martin & Cundy, 2017; Romero et al., 2021; Gu et al., 2022; Smith et al., 2023; Peng et al., 2023; Poli et al., 2023; Massaroli et al., 2023; Yang et al., 2024b).\nBroadly, there are two prominent paths to improve model architectures: automated and manual. Automated design, leveraging optimization (e.g. evolutionary methods) within a predefined search space, has seen success in highly-targeted domains, such as the refinement of convolutional neural networks for resource-constrained applications (Pham et al., 2018; Liu et al., 2018; Howard et al., 2019; Li et al., 2021; Tan & Le, 2021). Automated methods have also been utilized to identify candidate improvements to standard computational primitives (So et al., 2021), e.g., depthwise convolutions in projections. Nevertheless, to date, automated methods have fallen short of providing a unified framework that enables significant improvements in quality and efficiency, across domains and objectives, over models using standard generalizable recipes. The homogeneity of architectures applied at scale during the Transformer era highlights this shortcoming.\nThe main challenge for automated methods lies in curating a search space for computational units and architectures that is both (a) well-conditioned i.e., populations of model candidates can be trained effectively, without numerical instability or unpredictable degradation in performance, and (b) comprehensive i.e., the design space includes candidates with significantly different properties from existing variants, expanding the range of potential improvements that can be identified.\nDespite a wealth of automated approaches for the search and refinement of computational units and composition strategies (White et al., 2023), the current generation has been obtained mostly through an iterative manual process, guided by intuition and tuning on representative smaller-scale tasks via e.g., synthetics and scaling laws (Hoffmann et al., 2022; Arora et al., 2023; Bi et al., 2024). Manual design has led to a variety of results, most notably in the introduction or improvement of computational units (Poli et al., 2023; Massaroli"}, {"title": "2 Foundations of the Search Space", "content": "We detail how the framework behind our search space is grounded in the theory of linear systems.\nLinear input-varying systems The class of inputs under consideration are sequences of vectors ${x_0,x_1,...,x_e}$ where each element $x_i$ is referred to as a token. Each token $x_i$ is a real-valued vector in $R^d$, represented as $x_i = (x_i^0,x_i^1,...,x_i^{d-1})$. The individual components $x_i^b$ of each token are called channels.\nThe attention operator (Bahdanau et al., 2014; Vaswani et al., 2017), provides a valuable starting point to define a search space for model architectures, as it defines a prototype of what we call linear input-varying (LIV) operators (Massaroli & Poli, 2024). Attention, in its common formulation, can be expressed as a linear operator applied to the input, with the operator's action determined by the input itself:\n$y = \\sum_{B \\in [d]} \\sum_{j \\in [l]} \\sigma(q_j k_B)V_{\\alpha \\beta} x_j^B, (q_i, k_i) = (\\varphi(x_i), \\psi(x_i))$\nwhere $\\sigma: R \\rightarrow R$ is a nonlinear function and $V \\in R^{d \\times d}$. The intermediate quantities $q, k$, obtained through functions $\\varphi, \\psi: R^d \\rightarrow R^h$ of the input tokens and used to construct the linear operator $T$, are referred to as feature groups. We extend this idea to include a broader family of LIVs, expressed in their most general form as:\n$y^{\\alpha} = \\sum_{j \\in [l]} \\sum_{B \\in [d]} T_{\\alpha}^\\beta (x) x_j^B$.\nThe LIV framework decouples the (often) nonlinear and linear computation required to materialize the operator $T(x)$ and apply it to obtain the outputs, $y = Tx$. LIVs include and generalize a diverse array of computational units commonly used in model architectures, whose class is defined by the structure of the operator: attention variants, linear attention, convolutions, gated linear models and convolutions, gated recurrences with linear state transitions, state-space layers, and various novel forms of input-varying structured operators:"}, {"title": "3 Describing Operators and Backbones with STAR Genomes", "content": "The new design space of LIVs and their compositions serves as the foundation for the automated synthesis of tailored architectures (STAR). In the following, we will describe how we map the three hierarchical levels of the LIV description-featurization, structure, and composition-into a numerical representation suitable for optimization: the STAR genome. Each level of the hierarchy can be summarized into a single integer, yielding a numerical representation that can be optimized at different levels of granularity (see Fig. 3.1). In this work, we focus on backbone optimization but are reporting the full description of the genome for completeness. See Appendix A.6 for a full description of all genome values considered in this work.\nWe begin by describing the highest abstraction level of the STAR genome, the backbone genome, which defines the composition of LIVs in the backbone. We recall that under the LIV framework, LIVs can be connected with featurizer and feature group sharing, as described in Section 2. Specifically, the backbone genome represents a set of integer-valued sequences of length five, one for each LIV of the backbone. Each of these 5-number segments defines the following properties of the LIV:\n1. LIV class: integer summary of lower levels of the STAR genome, i.e., operator and featurizer genomes (see Section 3.2.\n2. Featurizer sharing: determines the weight-sharing structure between featurizers of LIVs at different depth in the backbone. LIVs with the same value at this position share featurizer weights, as defined by the featurization sharing strategy.\n3. Featurization sharing strategy: defines how featurizer sharing is implemented for the LIV class. Featurizer weights can be shared partially, for example, only those responsible for computing $B(x)$, in contrast to also sharing weights that compute $C(x)$. We explore all combinations of sharing strategies based on the number of feature groups of the LIV class."}, {"title": "4 Synthesizing Architectures by Evolving Genomes", "content": "We have devised the STAR genome as a hierarchical numerical representation that encodes a specific LIV backbone, suitable for gradient-free optimization. In the following, we will outline how a STAR genome can be optimized via evolutionary methods (Beyer & Schwefel, 2002); a process subsequently referred to as STAR evolution. To allow for the application of evolutionary optimization methods to the STAR genome, we adapted methods commonly used to iteratively evolve an initial population of genomes.\nSTAR evolution begins by evaluating the quality of each genome in the initial population. This involves realizing the model encoded in each genome and scoring it against the objective functions of"}]}