{"title": "01 Replication Journey \u2013 Part 2:\nSurpassing 01-preview through Simple Distillation\nBig Progress or Bitter Lesson?", "authors": ["Zhen Huang", "Haoyang Zou", "Xuefeng Li", "Yixiu Liu", "Yuxiang Zheng", "Ethan Chern", "Shijie Xia", "Yiwei Qin", "Weizhe Yuan", "Pengfei Liu"], "abstract": "This paper presents a critical examination of current approaches to replicating OpenAI's O1 model\ncapabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation\ntechniques. While our previous work (Part 1 (Qin et al., 2024)) explored the fundamental technical path\nto O1 replication, this study reveals how simple distillation from O1's API, combined with supervised\nfine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through\nextensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples\n01-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics\nExamination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond\nmathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse\ntasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical\nproblem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became\nsignificantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to\npromote transparency in AI research and to challenge the current trend of obscured technical claims in the\nfield. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness,\n(2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based\non their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential\nrisks of over-relying on distillation approaches, our analysis culminates in a crucial \u201cbitter lesson\": while\nthe pursuit of more capable AI systems is important, the development of researchers grounded in first-\nprinciples thinking is paramount. This educational imperative represents not just a technical consideration,\nbut a fundamental human mission that will shape the future of AI innovation. Relevant resources will be\navailable at https://github.com/GAIR-NLP/01-Journey.", "sections": [{"title": "Introduction", "content": "The landscape of AI research has been dramatically transformed since OpenAI's announcement of their Ol\nmodel (OpenAI, 2024), which demonstrates unprecedented capabilities in complex reasoning tasks, particularly in\nmathematical problem-solving. This breakthrough has catalyzed a race among research institutions and companies\nworldwide to replicate these capabilities, leading to numerous claimed successes in recent weeks (Team, 2024b;\nQin et al., 2024; Team, 2024a; kimi, 2024; kunlun, 2024; deepseek, 2024). However, this surge of announcements\nhas brought to light a concerning trend in the research community - one that prioritizes rapid performance gains\nover transparent technical innovation. In exploring recent developments in O1 replication efforts, we demonstrate\na straightforward yet powerful approach: knowledge distillation (Hinton, 2015) from Ol's API. This method\ninvolves directly prompting O1 with complex problems to generate long-thought chains, which are then used for\nsupervised fine-tuning or reinforcement learning (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022)\nof other models. Through our experiments, we show that with just tens of thousands of distilled samples and\nstandard supervised fine-tuning, a base model can surpass O1-preview's performance on the American Invitational\nMathematics Examination (AIME).\nWhile this approach can indeed yield impressive performance metrics, its widespread but undisclosed use raises\nsignificant concerns about the current state and future direction of AI research. The implications of this \u201cshortcut\"\napproach extend far beyond mere technical considerations: (1) First, the lack of transparency in methodology\nreporting makes it increasingly difficult for the research community to accurately assess and build upon claimed\nadvances. Many institutions may obscure their actual methodologies while making ambitious claims about their\ntechnical capabilities, creating a distorted picture of the field's progress. (2) Second, this trend is fostering a\nconcerning pattern of innovation stagnation, where researchers become increasingly reliant on existing powerful\nmodels rather than developing fundamental new techniques. The focus shifts from original technical contributions\nto sophisticated prompt engineering, potentially stunting the field's long-term growth. (3) Moreover, models\ntrained through distillation face inherent limitations - they are naturally bounded by the capabilities of their teacher\nmodel (in this case, O1), creating a ceiling effect that may impede genuine advancement. This dependency cycle\nnot only limits potential breakthroughs but also restricts the ability to extend capabilities to new domains or\nsurpass existing benchmarks. (4) Perhaps most concerning is the educational impact: we are missing crucial\nopportunities to cultivate genuine research skills and problem-solving abilities in the next generation of AI\nresearchers.\nTo facilitate this transparency, we introduce a novel benchmark framework for categorizing and evaluating 01\nreplication attempts based on their technical transparency and reproducibility. This framework provides clear\nmetrics for assessing the transparency and the openness of different approaches, creating a standardized platform\nfor comparing various replication efforts. Through this systematic evaluation, we hope to encourage more rigorous\nand honest reporting of technical achievements in the field. Our work serves not only as a technical contribution but\nalso as a call to action for the AI research community. We argue that while distillation approaches offer immediate\nperformance gains, they risk creating a dependency cycle that could ultimately impede genuine technological\nadvancement. As the field continues to pursue increasingly advanced reasoning capabilities, we believe it is crucial\nto maintain a balance between performance improvements and genuine technical innovation. The path forward\nrequires a renewed commitment to the fundamental values of scientific inquiry: transparency, originality, and\ngenuine innovation. By openly acknowledging both the power and limitations of current approaches, we hope to\nfoster an environment that encourages researchers to invest in fundamental technical innovations rather than relying\nsolely on existing solutions. This paper aims to initiate a broader discussion about research practices in AI and\nadvocate for a return to more transparent and innovative approaches to advancing the field."}, {"title": "The \u201cShortcut\u201d Path to O1 Replication", "content": ""}, {"title": "Core Technical Stack for O1 Replication", "content": "In the first part of our o1 replication journey (Qin et al., 2024), we introduce a novel method to synthesize long\nthinking processes called \u201cjourney learning\", as illustrated in Figure 2. The approach utilizes tree-searching\nalgorithms (e.g., Monte Carlo) to explore different solution paths, followed by strategic node selection to construct\npromising exploration trajectories. These exploration trajectories often contain incorrect results or unpromising\nmethods and end with the correct answers. To address the lack of reflection content in the trees, we leverage\nLLMs to analyze previous steps and identify reasoning errors, enabling better course correction. This process\nproduces complete trajectories leading to correct answers. We collect these trajectories, including both reflection\nand correction steps, to fine-tune the LLMs. The tuned LLMs can then be utilized for subsequent iterations of\ntraining."}, {"title": "Alternative Methods for Long-thought Synthesis", "content": "In the O1 technical pipeline, one of the most challenging aspects is effectively synthesizing long chains of reasoning\nfor solving complex problems. These chains typically incorporate reflection, error correction, and backtracking steps.\nWhile tree search, as discussed above, represents one of the most effective approaches, it can be computationally\nexpensive and time-consuming. Beyond tree search, alternative methods for synthesizing long reasoning chains are\nlisted as follows. Each of these methods offers different trade-offs between computational efficiency and reasoning\nthoroughness.\nMethod I: Complete Human Thought Process Annotation Human problem-solving rarely follows a linear\npath to success or failure. Instead, people regularly pause to reflect, backtrack, and revise their approach when\nencountering obstacles. This natural process mirrors the characteristics of long thought. By thoroughly documenting\nhow humans solve problems, we can generate authentic long thought training data.\nMethod II: Multi-Agent Approach Different from journey learning where the policy model does not react to\nfeedback directly, we can involve multi-agents to complete the exploration process, instructing them to play different\nroles. For example, we can construct a multi-agent debate system where a policy model generates continuous\nreasoning while a critique model evaluates whether to proceed or backtrack. This interactive process naturally\nproduces long thought training data when solutions are found.\nMethod III: Distillation from Advanced Models Advanced models like ol demonstrate strong reflection and\nself-correction abilities. Following common practice of instructing weaker models using stronger ones, distilling\nresponses from ol is a natural approach. However, careful prompting is needed since ol restricts access to its\ninternal thought processes.\nWhile diverse methods exist for generating long thoughts, the distillation method offers a cost-effective and\nreliable approach to obtaining high-quality data."}, {"title": "Distillation-based Long Thought Synthesis", "content": "Background of Distillation In the era of Large Language Models (LLMs), the quality of training data has\nemerged as a critical factor in model development. Recent research indicates that data quality exerts a more\nsubstantial influence on model performance than either model size or data volume. For instance, LIMA (Zhou et al.,\n2024) demonstrated superior performance through Supervised Fine-Tuning (SFT) using only 1,000 meticulously\ncurated prompts and responses, outperforming models trained on extensive but lower-quality datasets. Similarly,\nPhi-1 (Gunasekar et al., 2023) achieved remarkable results by leveraging high-quality data synthesized from\nGPT-3.5, surpassing models with significantly larger parameter counts on both MBPP (Austin et al., 2021) and\nHumanEval (Chen et al., 2021a) benchmarks. Given advanced LLMs' comprehensive knowledge base, sophisticated\nreasoning capabilities, and robust instruction-following abilities (Wei et al., 2022; Brown et al., 2020), coupled with\ntheir decreasing operational costs, the practice of distilling high-quality data from these models to train smaller\nmodels has become increasingly prevalent. Notable examples include Alpaca (Taori et al., 2023), an instruction fine-\ntuning dataset derived from GPT-3.5, and WizardLM (Xu et al., 2023), which enhances the complexity and diversity\nof existing instruction data. For reasoning tasks, which also have verifiable solutions, researchers have implemented\nrejection sampling methodologies that, when combined with distillation, enable the extraction and validation of\nadvanced models' reasoning processes (Zelikman et al., 2022; Yu et al., 2023). Given Ol's exceptional performance\nand sophisticated reasoning capabilities, implementing a distillation process of its cognitive mechanisms represents\nthe most viable approach for model replication.\nPost-training Data Curation To prepare the dataset for downstream post-training (e.g. SFT), we start with a\nsubset of Olympic-level problems from the open-source datasets and self-curated datasets. A filtering process is\napplied to refine the dataset: we remove problems dependent on images, those lacking explicitly labeled answers,\nand all proof-based problems using carefully-designed rules, while retaining problems where the answer type is\nnumerical.\nReformatted Technology We use the reformatted technology (Fan et al., 2024) to further enhance the dataset, we\nuse GPT-40-mini to rewrite the original solutions. The rewriting process adheres to specific guidelines, ensuring\nthat solutions are step-by-step, highly detailed, and longer in length. This step also standardizes the output format,\nrequiring the final answers to be explicitly highlighted using $\\boxed$, aligning with the long thought format.\nQuality Control Mechanism We select Qwen2.5-Math-72B (Yang et al., 2024b) as our base model due to its\nexceptional foundational capability in mathematical reasoning. This strong baseline provides a robust foundation\nfor further enhancing the model's reasoning abilities, ensuring a solid starting point for subsequent improvements."}, {"title": "Supervised fine-tuning approach", "content": "To familiarize and adapt the model to the long thought format, we perform an initial SFT phase before distillation.\nUsing the refined and reformatted dataset described above, we train the model to generate longer, more fine-grained\nstep-by-step solutions. This phase focuses on ensuring that the model becomes proficient in both producing detailed\nreasoning and adhering to a standardized output style, preparing it for subsequent distillation phases. Following\nthis, we proceed with the next SFT phase using the distilled dataset. This dataset, generated through our distillation\nprocess, is specifically curated to capture high-quality, detailed reasoning aligned with the long-thought format.\nDuring this phase, the model is further fine-tuned to not only enhance its reasoning capabilities but also to ensure\nconsistency in producing precise and coherent outputs."}, {"title": "Experiment", "content": ""}, {"title": "Benchmark Usage", "content": "We select several widely recognized and commonly used benchmarks in the field of mathematical reasoning, chosen\nfor their challenging nature. These include MATH (Hendrycks et al., 2021) and AIME. Specifically, we use the\nstreamlined MATH500 subset to facilitate more extensive inference-time scaling experiments. For AIME, we\nutilize the newly released problems in 2024 to minimize the risk of data leakage (we refer to it as AIME2024).\nAdditionally, we curate a set of 30 problems from the 2024 China National High School Mathematics Competition,\nserving as an additional benchmark (MATH2024) to diversify and enrich our evaluation. This combination of\nbenchmarks ensures a comprehensive assessment of our model's mathematical reasoning capabilities."}, {"title": "Evaluation Metric for Inference-Time Scaling", "content": "Unlike conventional evaluation strategies that rely solely on metrics such as Pass@k (Chen et al., 2021b),\nMaj@k (Wang et al., 2022), or RM@k (Lightman et al., 2024), we introduce a novel metric designed to eval-\nuate model performance across varying computational cost scenarios. This new approach reflects the realities\nof inference-time scaling (Snell et al., 2024), where test-time compute plays a crucial role in determining the\neffectiveness and efficiency of modern large-scale models. In the era of inference-time scaling, models like\nOpenAI's O1-series have demonstrated that performance is not solely dependent on training-time compute but\nalso significantly influenced by the time spent \"thinking\" during inference. This shift necessitates a more nuanced\nevaluation framework that accounts for the trade-off between computational cost and performance. Our proposed"}, {"title": "Performance Analysis", "content": ""}, {"title": "Performance Analysis", "content": "metric directly addresses this by measuring the model's reasoning ability under constrained test-token budgets,\nensuring that evaluations reflect real-world constraints and deployment scenarios.\nSpecifically, we measure the computational cost of a model on a given benchmark test set using the average token\ncount for its outputs. This metric reflects the test-time computational expense, where longer average token outputs\ncorrespond to more extensive reasoning steps. Models capable of generating longer, more detailed outputs are often\nable to capture complex reasoning patterns more effectively, demonstrating their scalability under inference-time\ncompute. Furthermore, this average token metric is inherently extensible. In scenarios where the evaluation requires\na higher average token count than what is typically generated in a single response, we leverage the Maj@k metric\nto approximate the model's performance without using any extra reward model. This approach reflects the model's\nreasoning ability at extended computational costs, even when a single output does not naturally reach the desired\ntoken length.\nBy employing this method, we ensure a scalable and fair evaluation framework that captures model performance\nacross different inference-time compute settings. This approach avoids artificial constraints and allows for\nmeaningful comparisons without relying on external reward signals, focusing solely on the model's intrinsic\nreasoning capabilities."}, {"title": "Comparison with Ol's performance", "content": "As is shown in Table 1, under similar \u201creasoning computational costs\"\ni.e., with comparable average output tokens on the corresponding benchmark, the distilled model demonstrates\noutstanding performance, surpassing the results of O1-preview on AIME2024."}, {"title": "Analysis of model behavior and limitations", "content": "While the model achieves impressive results, there remains a\nnoticeable gap compared to O1-mini in terms of mathematical reasoning performance. Additionally, the generated\nlong thought solutions still exhibit imperfections. Addressing these limitations is critical for closing the performance\ngap and ensuring the generated long thought solutions meet the highest standards of clarity and correctness."}, {"title": "Application Beyond Math Reasoning", "content": "In this section, we investigate how the model trained on mathematic long thoughts generalizes when applied to\nother tasks or applications.\nTraining Details To investigate the model's generalization capability across different domains, we first construct\na diverse bilingual dataset through a systematic data extraction and translation process. From our distilled O1\nmodel outputs, we carefully select approximately 5,000 high-quality samples containing retrospective thinking\nand self-reflection elements. These samples are then translated into Chinese using GPT-40 mini model, resulting\nin a balanced bilingual dataset. The final training dataset comprises 10,750 mixed Chinese-English sample pairs,\nwhere each sample consists of a query-response pair. We then perform Supervised Fine-Tuning (SFT) on the\nQwen2.5-72B-Instruct (Yang et al., 2024a) model (named as \"baseline\") using this curated dataset to obtain our\nfinal model (named as \"Ours\".)"}, {"title": "Safety", "content": "Setup To comprehensively assess the safety aspects of our model's generalization capabilities, we construct\na diverse test set comprising 600 questions carefully selected from three established safety evaluation datasets:\nFlames (Huang et al., 2023), DiaSafety (Sun et al., 2022), and WildSafety (Liu et al., 2024). Specifically, we extract\n200 questions from each dataset to ensure balanced representation across different safety scenarios. We utilize the\nSafety-J (Liu et al., 2024) to evaluate the responses from both the original and fine-tuned models."}, {"title": "Safety", "content": "Results & Insights The evaluation results reveal interesting insights about the impact of our fine-tuning process\non model safety. While performance improves slightly on Flames (91% to 92.5%) and remains stable on DiaSafety\n(100%), there is a notable decrease on WildSafety (92% to 86.5%). Overall, the safety score drops marginally\nfrom 94.3% to 93.0% after fine-tuning. This slight decrease in safety metrics highlights a crucial finding: even\nwhen using high-quality, O1-like long thought training data focused on retrospection and reflection, models can\nexperience a subtle degradation in safety performance if the training data lacks explicit safety alignment. We\nhypothesize that the improvement on Flames dataset might be attributed to its unique focus on testing models' deep\nreflection capabilities compared to other datasets, which aligns well with our O1-like training data emphasizing\nthoughtful deliberation.\nCase Study To investigate why our fine-tuned model achieves better performance on the Flames dataset (from\n91% to 92.5%), we conduct a detailed analysis of typical cases from Flames. We find that most queries in Flames\nare designed to tempt models into prioritizing utility over safety, often leading to unsafe responses. Figure 4\npresents a representative case about storing and charging an electric bicycle in a building corridor.\nQwen2.5-72B-Instruct's (the baseline's) response demonstrates this utility-focused tendency by concentrating\nsolely on anti-theft measures. The model provides detailed recommendations about lock selection, installation\nmethods, and surveillance, directly addressing the user's immediate concern about property security. However, it\ncompletely overlooks critical safety hazards, particularly the fire risks associated with charging electric bicycles in\ncorridors, which could endanger multiple residents' lives. In contrast, our model, after training on long-thought data,\nexhibits more comprehensive and systematic thinking patterns. Instead of immediately addressing the theft\nconcern, it first identifies the fundamental safety issues: fire hazards from corridor charging, regulatory compliance,\nand community safety. The response demonstrates enhanced analytical depth through prioritizing life-threatening\nrisks over property risks, considering multiple stakeholders including residents and property management, providing\nhierarchical analysis of different safety dimensions, and suggesting alternative solutions that balance both utility\nand safety. This case study reveals an important insight: the improved systematic thinking and long-form reasoning\ncapabilities developed through our fine-tuning process contribute significantly to enhanced safety performance,"}, {"title": "Hallucination", "content": "Setup We evaluated the factuality of the models before and after SFT. We used datasets from SimpleQA (Wei\net al., 2024), ChineseSimpleQA (He et al., 2024), and ChineseFactEval (Wang et al., 2023). These datasets contain\nChinese and English knowledge-based questions to verify model factuality. Notably, the ChineseFactEval dataset\ncontains two subsets: general QA and sycophancy QA. The sycophancy QA subset includes misleading answers in\nthe prompts to test the models' propensity for sycophancy, while the general QA subset follows a format similar\nto SimpleQA. All questions in these datasets require verifiable short-form answers. We evaluated the models'\nresponses against the golden answers using GPT-40 for more robust answer matching.\nResults & Insights Our results showed that models after SFT did not demonstrate significant improvement in\nfactuality (10.58% to 10.41%, 47.08% to 45.76%, 69.08% to 62.65%). This was largely due to longer reasoning\nchains leading to additional hallucinations\u2014specifically, models attempting to use search engines and fabricating\nsearch results (Fig. 5). Nevertheless, these attempts to actively use search engines suggest a promising direction,\nand we believe that providing models with actual web access or tool-use (Gao et al., 2022; Chern et al., 2023)\nwould significantly improve their factuality. Additionally, the enhanced reasoning chains in the post-SFT models\noffer detailed analysis and self-reflection capabilities that could help prevent hallucinations (Fig. 6).\nWe also found that models became slightly less susceptible to sycophancy after SFT (89.70% to 92.65%). This\nimprovement can be attributed to the self-reflection process, where models are able to discern and think deeply\nabout unreasonable assumptions presented in the prompt rather than accepting them without question (Fig. 7).\nCase Study In Fig. 5, we observed that our model attempts to utilize search engines and has the potential to\ncollect and cross-verify results from multiple sources. Although these search engine interactions are simulated (as\nwe did not incorporate access to external databases), this behavior demonstrates promising potential. In Fig. 6, we\nobserved that our model systematically documented all of Argentina's FIFA World Cup matches and results to\nensure thoroughness. Furthermore, the model verified its initial findings through a self-reflection process. In Fig. 7,\nthrough self-reflection, the model successfully corrected the false assumption in the prompt (that the Pearl River\nis the second-longest river) and correctly identified the Yellow River as China's second-longest river. The model\nalso provided valuable insights from different perspectives (e.g., economic importance, water flow), making the"}, {"title": "Hallucination", "content": "particularly in scenarios where safety considerations might be overshadowed by immediate utility concerns. The\nmodel's ability to pause, reflect, and analyze situations comprehensively helps it identify potential safety issues that\nmight be overlooked in more direct, utility-focused responses.\nHowever, the decreased performance on WildSafety (from 92% to 86.5%) suggests that enhanced thinking\ncapabilities alone are insufficient for comprehensive safety alignment. While systematic thinking helps models\nidentify potential safety issues, proper safety alignment remains crucial for consistently maintaining high safety\nstandards across diverse scenarios. This finding indicates that future work should focus on combining systematic\nthinking capabilities with explicit safety alignment to achieve more robust and comprehensive safety performance."}, {"title": "General Scenario", "content": "Setup To evaluate our model's performance in general scenarios, we curate a test set of 100 queries equally\nsampled from the Auto-J (Li et al., 2023) and LIMA (Zhou et al., 2024) datasets (50 each), with a specific focus on\nlong-term planning tasks through manual adaptation. Three domain experts assess the response quality on a scale\nof 0-100.\nResults & Insights The evaluation results show notable improvement after fine-tuning. The scores increase from\n81.6% to 88% on Auto-J queries and from 77.2% to 87.2% on LIMA queries. This performance enhancement\nsuggests that our fine-tuning approach not only improves bilingual conversation capabilities but also strengthens the\nmodel's ability in handling general open-domain QA tasks, particularly for scenarios requiring long-term planning\nand structured thinking.\nCase Study Figure 8 presents a detailed case study comparing responses from Qwen2.5-72B-Instruct and our\nmodel on a technical programming query about Python's asyncio library. The query \u201cWhy in python await\nasyncio.sleep() is stuck?\" represents a common programming challenge that requires both technical accuracy and\nclear explanation.\nQwen2.5-72B-Instruct's response, while technically accurate, provided a relatively basic structure with five\nmain points and corresponding code examples. It covered essential aspects like event loop issues, blocking code,\nand incorrect await usage, but lacked depth in several areas. Notable limitations included insufficient debugging\nguidance, potentially misleading thread-safe operation suggestions, and absence of performance considerations and\nbest practices.\nOur model demonstrated substantial improvements across multiple dimensions. First, the response adopted\na more sophisticated structure with clear hierarchical sections and logical flow, making complex concepts more\naccessible. Second, it significantly expanded the technical coverage to include advanced topics such as systematic\ndebugging approaches, event loop management strategies, and detailed analysis of blocking code scenarios. Third,\nit enhanced practical value by incorporating comprehensive debugging tips, concrete examples of common mistake\npatterns, and systematic troubleshooting steps. Finally, it integrated references to official documentation and\nreliable learning resources, supporting continued learning.\nDespite our SFT dataset being exclusively focused on mathematical problem-solving, our model demonstrates\nremarkable generalization abilities across diverse domains. This suggests that the systematic thinking patterns\nand structured approaches inherent in mathematical problem-solving can effectively transfer to other fields. The\nimprovements seen in our case study, particularly in terms of structural organization, comprehensive analysis, and\nlogical flow, reflect the successful transfer of mathematical reasoning patterns to general problem-solving scenarios.\nThis finding indicates that carefully curated mathematical instruction data can serve as an effective foundation for\ndeveloping general-purpose reasoning capabilities in LLMs.\""}, {"title": "Suggestions", "content": ""}, {"title": "Suggestions", "content": "RESEARCH CULTURE SHIFT The impact on research culture is equally concerning. The availability of\n\"easy wins\" through distillation has begun to shift research focus away from tackling fundamental challenges.\nThis trend manifests in reduced investment in advanced computing infrastructure and diminished emphasis on\ndeveloping sophisticated search and reasoning algorithms. The resulting self-reinforcing cycle - where lack of\ninfrastructure limits research possibilities, further encouraging reliance on distillation approaches - threatens to\ncreate an innovation bottleneck that could stifle future breakthroughs.\nEROSION OF FUNDAMENTALS Perhaps most alarming is the impact on educational development within the\nfield. The widespread adoption of distillation approaches poses a significant risk to the development of future AI\nresearchers. When students and early-career researchers are primarily exposed to \u201cshortcut\" solutions, they miss\ncrucial opportunities to develop deep problem-solving skills. The ability to tackle complex technical challenges\nfrom first principles - a cornerstone of scientific innovation - may be gradually eroded as quick solutions become\nthe norm. We are witnessing a transformation in how the next generation of AI researchers approaches problem-\nsolving. Instead of developing deep understanding through wrestling with fundamental challenges, many are being\ntrained primarily in optimization and prompt engineering. This shift from \"how it works\u201d to \u201cwhat works\"\nrepresents a fundamental change in research mentality that could have far-reaching consequences for the\nfield's future innovation capacity.\nFIRST PRINCIPLES DECAY This erosion of first-principles thinking is particularly concerning as it\nundermines the very foundation of scientific innovation. The process of developing search algorithms, optimizing\ninference time, and building reasoning mechanisms from scratch provides invaluable learning experiences that\ncannot be replicated through distillation approaches. These challenges force researchers to deeply understand\nmodel behavior and limitations, develop systematic problem-solving strategies, and build intuition for algorithm\ndesign and optimization. Without these experiences, we risk creating a generation of researchers who are more\ncomfortable with applying existing solutions than developing new ones from fundamental principles.\nACADEMIC IMPACT The educational implications extend beyond individual skill development. The academic\nresearch environment, traditionally a crucible for fundamental innovation, is particularly vulnerable to these effects.\nPressure to produce quick results may overshadow the value of deeper technical investigations, while students\nmay be discouraged from pursuing more challenging, fundamental research directions. The emphasis on\nperformance metrics over understanding threatens to create a generation of researchers skilled in optimization but\nlacking in innovative capacity.\nGROWING DIVIDE Looking ahead, the cumulative effect of these factors paints a troubling picture. The\ntechnical capability gap between organizations that have developed fundamental search and inference technologies\nand those relying primarily on distillation may become increasingly unbridgeable. This divide could lead to a\nresearch ecosystem where genuine breakthroughs become the exclusive domain of a small number of well-\nresourced organizations, while the broader community remains trapped in a cycle of incremental improvements\nthrough distillation."}, {"title": "Suggestions", "content": "To address these challenges, we propose several crucial recommendations.\nGROWING DIVIDE First, organizations must maintain a balanced research portfolio that includes both\ndistillation-based approaches and fundamental research into search and inference optimization. Second, de-\nspite the immediate availability of distillation-based solutions, continued investment in advanced computing\ninfrastructure remains essential. Third, research programs should prioritize building core competencies in search\nalgorithms and inference optimization alongside performance improvements.\nEDUCATIONAL REFORM In the educational context, we must redesign our approach to training future\nresearchers. This includes developing balanced curricula that emphasize both practical applications and fundamental\ntheory, structuring research projects to encourage deep understanding alongside performance optimization, and\nfostering a research culture that values long-term innovation over quick gains.\nThe bitter lesson here is not that distillation is inherently problematic - it remains a valuable tool in our\ntechnical arsenal. Rather, the danger lies in allowing the convenience of distillation to divert us from the\nharder but ultimately more rewarding path of fundamental innovation. As we move forward, maintaining\nthis balance between immediate gains and long-term development will be crucial for ensuring the continued\nadvancement of AI capabilities and the cultivation of future innovators in the field.\nBuilding intelligent AI is crucial, but cultivating human minds with first-principles thinking is our ultimate\nmission - they are, after all, the true architects of AI's future."}, {"title": "A Framework for Evaluating O1 Replication Claims: The Technical Transparency\nIndex", "content": "To systematically evaluate and compare various Ol replication attempts, we propose the Technical Transparency\nIndex (TTI), a comprehensive framework that quantifies the transparency and reproducibility of claimed implemen-\ntations. This framework aims to provide the research community with objective metrics for assessing the openness\nand verifiability of different approaches."}, {"title": "Evaluation Dimensions of Transparency", "content": "The framework evaluates O1 replication efforts with a primary focus on transparency, which is assessed across\nseveral interconnected aspects. These include the data transparency, encompassing the accessibility, quality, and\ndocumentation of datasets used for downstream search or post-training; methodological transparency, reflected in\nthe clarity and detail of the described techniques, processes, and experimental setups; and evaluation transparency,\nwhich considers the reproducibility and comprehensiveness of performance evaluations. Additionally, the framework\nexamines the openness of resources, such as the availability of code, datasets, and models, to ensure that the work\ncan be independently verified and effectively utilized by the research community. This comprehensive perspective\ncaptures the multifaceted nature of transparency in replication efforts. Details will be introduced below."}, {"title": "Data Transparency", "content": "This aspect evaluates whether the origin of the data is clearly specified, including detailed descriptions of the\ndatasets used and their respective sources. It considers whether the dataset names, providers, or publications from\nwhich the data is derived are explicitly mentioned. This applies to all datasets used in downstream tasks such as\nsupervised fine-tuning (SFT), reinforcement learning (RL), or search algorithms and becomes even more crucial\nwhen the datasets serve as seed data for synthesizing long thought data."}, {"title": "Methodology Transparency", "content": "Methodology transparency ensures that the approach, techniques, and processes employed in the work are described\nin sufficient detail to enable independent reproduction and validation. This section evaluates multiple components,\nfrom foundational model descriptions to training and data curation methods. Moreover, in addition to detailing how\na method is implemented, it is even more important to validate the effectiveness of the method itself. It highlights\nthe importance of validating the effectiveness of each method employed. A thorough evaluation should quantify the\ncontributions of individual techniques to the overall system performance, rather than simply reporting final results."}, {"title": "Checklist for O1-style Technique", "content": ""}, {"title": "The Bitter Lesson of Simple Distillation", "content": "The remarkable success of knowledge distillation from O1 presents an alluring shortcut to achieving impressive\nperformance gains in mathematical reasoning tasks. While this approach offers immediate and tangible benefits", "solution": "by learning directly from\nOl's sophisticated reasoning patterns, models can quickly achieve significant performance improvements with\nrelatively straightforward implementation. This accessibility has led to widespread adoption, particularly among\norganizations seeking to rapidly demonstrate capabilities comparable to 01. However, this convenience comes at a\nprice that may not be immediately apparent but could prove devastating to the field's long-term progress.\nPERFORMANCE CEILING Perhaps the most immediate technical concern lies in the inherent limitations\nof distillation-based approaches. Models trained through distillation are invariably bounded by the capabilities\nof their teacher model - in this case, O1. This creates an implicit ceiling effect, where improvements, no matter\nhow sophisticated the distillation process, can never truly surpass the original model's capabilities. This limitation\nbecomes particularly problematic when considering the need to extend capabilities to new domains or tackle\npreviously unseen challenges.\nMISSED INNOVATION More fundamentally, the widespread adoption of distillation approaches is causing us\nto miss crucial opportunities in core"}]}