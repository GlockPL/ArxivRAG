{"title": "Annealing Machine-assisted Learning of Graph Neural\nNetwork for Combinatorial Optimization", "authors": ["Pablo Loyola", "Kento Hasegawa", "Andres Hoyos-Idobro", "Kazuo Ono", "Toyotaro Suzumura", "Yu Hirate", "Masanao Yamaoka"], "abstract": "While Annealing Machines (AM) have shown increasing capabilities in solving\ncomplex combinatorial problems, positioning themselves as a more immediate\nalternative to the expected advances of future fully quantum solutions, there are\nstill scaling limitations. In parallel, Graph Neural Networks (GNN) have been\nrecently adapted to solve combinatorial problems, showing competitive results\nand potentially high scalability due to their distributed nature. We propose a\nmerging approach that aims at retaining both the accuracy exhibited by AMs and\nthe representational flexibility and scalability of GNNs. Our model considers a\ncompression step, followed by a supervised interaction where partial solutions\nobtained from the AM are used to guide local GNNs from where node feature\nrepresentations are obtained and combined to initialize an additional GNN-based\nsolver that handles the original graph's target problem. Intuitively, the AM can\nsolve the combinatorial problem indirectly by infusing its knowledge into the GNN.\nExperiments on canonical optimization problems show that the idea is feasible,\neffectively allowing the AM to solve size problems beyond its original limits.", "sections": [{"title": "Introduction", "content": "Graph-based approaches are one of the most predominant techniques for learning combinatorial\noptimization solvers. Their distributed nature allows them to scale up to millions of nodes [33, 19].\nNevertheless, their probabilistic nature, which provides soft assignments to decision variables, may\nproduce solutions at a different level than their classic counterparts [25]. Annealing Machines (AM),\na concurrent line of research, is seen as a more immediate alternative to the expected advances of\nfuture fully quantum solutions, which are currently in use in several industries [30]. Their major\ndrawback, similarly suffered by the fully quantum versions, is their scalability, handling a limited\nnumber of variables, sometimes forcing problem reformulations to fit hardware limitations [28].\nWe see these two approaches working towards the same goal: i) GNN-based methods enable\nscalability, but their solutions could be noisy; ii) AM-based methods provide high precision, yet they\nare limited in the number of variables they can handle. This apparent trade-off motivates us to design\na framework to capture the best of each technology: high scalability and high precision. This work\nproposes a way to combine the solving capabilities of both graph and annealing-based methods into a\nsingle workflow that handles combinatorial optimization problems at scale."}, {"title": "Background and Related Work", "content": "Annealing Machines for Combinatorial Optimization AM can operate based on various mech-\nanisms, including both quantum and classical: superconducting flux qubits, degenerating optical\nparametric oscillators, and semiconductor CMOS integrated circuits. However, there are various types\nof AM since D-Wave released the first commercial quantum AM [18, 31, 23, 30, 12, 11, 14, 17, 1, 20].\nAM requires casting the target problem as a Quadratic Unconstrained Binary Optimization (QUBO)\nproblem [10, 8]. The QUBO formulation models the problem as a graph, with nodes as decision\nvariables and edges as (energy) couplings that encode their relationships. This formulation has proven\nversatile, with AM covering a wide range of applications [7, 26, 6, 22, 24, 21, 16, 32]. We refer the\nreader to [10] for a comprehensive QUBO formulation tutorial. We consider the following problem:\nminimize $x^T Q x =: H_{QUBO}(x)$\nsubject to $x \\in \\{0,1\\}^n$,\nwhere $H_{QUBO}$ is the Hamiltonian associated with the QUBO matrix $Q \\in \\mathbb{R}^{n \\times n}$, which is the\nsymmetric matrix that encodes the target problem, and $x$ is the vector of binary assignments. In this\nwork, we explore to what extent the GNNs can enable AMs to handle larger problems, hopefully\nwithout sacrificing accuracy.\nQUBO-based Graph Learning for Combinatorial Optimization Recently [25] exploited the\nrelationship between GNNs and QUBO for solving combinatorial problems. This method relies on\nthe QUBO formulation of a target problem, where a GNN takes the Hamiltonian as the cost function\nand minimizes it in an unsupervised way. Thus, for the $k$-th GNN layer and a given node $v \\in V$,\nwe obtain a node feature representation that depends on both of its previous representation at the\n$(k-1)$-th layer and the aggregated representations of the direct neighbors,\n$h_v^k = \\Phi_\\Theta \\Big(h_v^{k-1}, \\big\\{h_u^{k-1} | \\forall u \\in \\mathcal{N}_v \\big\\} \\Big), \\forall k \\in [K]$,\nwhere $h_v^k \\in \\mathbb{R}^{d_k}$, $d_k$ is the dimension of the $k$-th representation, $\\mathcal{N}_v$ is the set of neighbors of $v$,\nand $\\Phi_\\Theta$ a learnable function [15]. The resulting representation passes through a linear layer and an\nactivation function to obtain a single positive real value, and is then projected to integer values to\nobtain a final binary node assignment.\nWe write it in compact form as:\n$x^{GNN} = \\psi_\\omega(GNN_\\theta(G, F)),$\nwhere $F \\in \\mathbb{R}^{|V| \\times d_0}$ is the matrix of initial node features$^1$, $GNN_\\theta$ maps the graph $G$ and $F$ to $\\hat{F}$, and\n$\\psi_\\omega : \\mathbb{R} \\to \\{0,1\\}$ is the composition of a linear layer and an activation function. Let $f_v \\in \\mathbb{R}^{d_0}$ be the\n$v$-row of $F$, the feature vector of the node $v$. Thus, the initial embedding in Eq. 2 corresponds to\n$h_v^0 = f_v, \\forall v \\in V$. Therefore, finding a solution $x^{GNN}$ amounts to minimizing $H_{QUBO}(x^{GNN})$ in an\nunsupervised manner."}, {"title": "Methodology", "content": "Problem Statement Our main goal is to assess AM and GNN complementarity. We assume a scenario\nwhere the target problem is large enough that it cannot be solved solely by the AM. Therefore, we\npropose a framework that divides the problem into smaller pieces so the AM can consume and solve,\nand the GNN can act as a bridge, aggregating information to achieve a global solution.\nGraph Compression We reduce the size of the original graph $G_p$ using Louvain decomposition for\nnetwork community detection [2]. The output of this decomposition is a list of size-decreasing graphs\n$\\left\\{G_i\\right\\}_{i=1}^s$. For each $G_i = (V_i, E_i)$, the algorithm admits a mapping back to the original graph. Thus,\nwe can relate a single artificial node $n^i \\in G_i$ with the corresponding set of actual nodes in the original\ngraph $G_p$. We ensure the size of all resulting graphs is smaller than the aforesaid feasibility limit\nexhibited by the AM. We chose Louvain decomposition because i) it is one of the most cited and\nwell-understood methods for community detection ii) it is hierarchical, allowing reconstruction from\na given granularity to the original graph, iii) among hierarchical models, Louvain provides the most\nhomogeneous results, as literature shows [9]. Given the expected diversity of graphs our method\nshould handle, we consider homogeneity to be a desirable factor.\nMultiresolution Guidance I : Locally-assisted Solvers Interaction We get a QUBO matrix $Q_i\n$ for each $G_i$. We assume that while these derived matrices are not equivalent to $G_p$, there should be\ncertain alignment as they are working on different granularities of the original graph $G_p$. As each\n$G_i$ is smaller than the AM's limit, we can apply it to solve them. This step outputs, for each $G_i$, the\nsolution found by the AM, in the form of a binary solution vector $X^{AM}_i \\in \\mathbb{R}^{|V_i|}$.\nMultiresolution Guidance II : Guiding block (GB) We consider AM solutions a good source of\nsupervision and use them to drive local GNNs that solve upon the same set of $G_i$ graphs. Instead\nof just minimizing the Hamiltonian in an unsupervised way, as described in Sec. 2, we propose to\ncombine the Hamiltonian cost with a measure of alignment between the AM's solution and GNN's\npartial solutions at each training time/epoch $t$. For a graph $G_i$, let $x^{GNN}_{it}$ be the GNN's partial solution.\nXNN denotes assignment scores. Thus, we have $\\Theta^i = arg\\underset{\\theta \\in \\Theta}{mineol} (X^{GNN}_{it}, X^{AM}_i) \\forall i \\in [s]$, where\n$X^{GNN}_{it} = \\psi_\\omega(GNN_\\theta(G_i, F))$. We set $l(a, b) = ||a - b||_2$. Local GNNs can quickly converge for\neach compressed graph due to this guidance. While we check these local results' specific behavior\nand quality, we focus more on the final node representations. For each $G_i$, this step gets a matrix\n$\\hat{F}_i \\in \\mathbb{R}^{|V_i| \\times d_k}$ (with $d_k$ a predefined vector dimensionality) with the node vectors after the GNN\nconverged. Fig. 1b depicts this process for one compressed graph.\nAggregating Partial Solutions The output of the previous step gives a node feature vector for\neach compressed graph $G_i$. We reuse these feature vectors to initialize a larger GNN to solve P\non the original graph $G_p$, called Main GNN solver in Fig. 1a. We hypothesize that these feature\nrepresentations associated with AM-guided solutions on compressed versions of $G_p$ may encode\nvaluable information that, if passed as initial node vectors for the main solver, could benefit the\nsolution-finding process, in contrast, to initialize those node vectors randomly ($h^0$ is random).\nMapping Module Let $V_{n^i}$ denote the set of real nodes in the original graph $G_p$ associated with a given\nnode $n^i \\in G_i$, the i-th compressed version of $G_p$. Thus, $V_{n^i} = \\{v \\in V_p | v \\in Louvainmap(n^i)\\}$,"}, {"title": "Empirical Study", "content": "Data Generation We generated a set of synthetic random d-regular graphs of sizes n \u2208\n[50 000, 100 000, 150000] and node degree d \u2208 [3,4,5], leading to a total of nine graphs. Ran-\ndom graphs are used for testing, as they allow us to evaluate the generalizability of our approach\nwithout the bias of structured adjacency matrices. The current AM has a limit of 100k nodes/variables,\nso the selected range allows us to study before and after such a feasibility threshold. For each graph,\nLouvain decomposition available on Networkx[13] was used.\nOptimization Problems Let V and E be the set of all nodes and edges, respectively. Maximum Cut\n(MaxCut) finds a partition of nodes into two subsets such that the number of edges between different\nsubsets is maximal. Its QUBO formulation is $H_{MaxCut}(x) := \\sum_{(i,j) \\in E} 2x_ix_j - \\sum_j x_j$, where $x_i$\nis 1 if node i is in one set and 0 otherwise. Maximum Independent Set (MIS) finds the largest subset of\nnodes that are not connected. Its QUBO formulation of is $H_{MIS}(x) := \\sum_{i \\in V} x_i + \\beta \\sum_{(i,j) \\in E} x_i x_j$,\nwhere $x_i$ is 1 if a node i belongs to the independent set and 0 otherwise; $\\beta > 0$ is the penalty\ncoefficient. We used $\\beta = 2$. Graph Partition (GP) partitions a graph into two equal-sized parts\nsuch that the number of divided edges is minimized. It is a generalization of MaxCut and its QUBO\nformulation is $H_{GP}(x) := -H_{MaxCut}(x) - \\beta \\sum_{i \\in V}((1 - |V|) x_i + \\sum_{j>i} 2x_ix_j)$. We used $\\beta = 10$.\nGraph Neural Network Both Local and Main GNN-based solvers have the same architecture: two\nconvolutional layers (GraphConv) linked via ReLu activations. We pass the output from the second\nlayer through a sigmoid function to get soft node assignments. Experiments were performed on a\nsingle GPU Tesla V100 with 32 GB of memory. Local solvers use as a loss function the sum of the\nHamiltonian, see Sec. 2, and the Mean Squared Error (MSE) between the solution at time t and the\nAM solution. We tested several alternatives, but MSE had better consistency across graphs. For the"}, {"title": "Results and Discussion", "content": "Solution Quality and Convergence We sampled solutions 50 times per target graph and obtained\nthe final solution. We experimented on large graphs; therefore, no ground truth is available. Given\nthis limitation, we used the final loss value as a measure of the quality of the solution, assuming\nthat, in the absence of violations, a lower value means a better solution. Let $loss$ be the evaluation\nof the Hamiltonian on a binarized solution; see Sec. 2. Unlike the relaxed version we used during\ntraining, this loss is our original optimization objective. We also checked violations based on the\nproblem definition. In this scenario, a good solver achieves the lowest loss and, simultaneously,\nthe minimum number of violations. For MIS, rGNN is faster than the other alternatives, reaching\nloss values 48% larger with 15% more violations on average. This pattern persists across problems,\nproviding evidence that a single GNN block trained in a purely unsupervised way, while fast, seems\nunable to provide high-quality results. Between mrGNN and mrGNN +AM, while mrGNN is\nconsistently faster, it also produces more violations across graphs, mainly for the largest graph. This\nbehavior indicates that the information from the local GNN-based solver is indeed useful, compared\nto rGNN, but not enough to beat the contribution of an AM-based solver. We omit rGNN from the\nfollowing analyses and focus on the trade-off between quality and speed of mrGNN and mrGNN\n+AM. Our comparison deals with i) a main GNN solver that received AM's information against\nand ii) a main GNN solver that received information only from the GNN-based solvers. Given the\nlack of ground truth (global minimum), we employed the Relative loss difference. It computes the\ndifference between absolute values of the minimum loss achieved by mrGNN and mrGNN +AM\n$A_{rel} = \\frac{|loss_{mrGNN+AM}|-|loss_{mrGNN}|}{loss_{mrGNN}} \\times 100$. $A_{rel} > 0$ means mrGNN+AM has a lower loss\nthan mrGNN. Table 4 presents the values of $A_{rel}$ across all target graphs for the three selected\nproblems. In addition, Table 2 provides insights into how well models handle constraints. For MIS,\nwe present the number of violations and how balanced the resulting sets are in the case of GP (ratio\nof their number of nodes).\nOur approach is particularly effective for larger graphs. For smaller graphs (up to 100k nodes),\nmrGNN performs comparatively well. This performance reinforces our initial goal of using the GNN\nas a bridge to bring the accurate problem-solving capabilities of the AM to large-scale graphs. The\nquality of mrGNN degrades as we expand to more complex graphs (in terms of n and d), where\nviolations increase compared to mrGNN+AM. Table 4 shows the relative differences in terms of\ntotal execution time, $AT = \\frac{time_{mrGNN+AM}-time_{mrGNN}}{time_{mrGNN}} \\times 100$. $\\Delta T > 0$ means mrGNN is faster than\nmrGNN+AM. Interestingly, if we look at the execution time comparison, there is a considerable\ndifference depending on the combinatorial problem. This difference is evident when comparing MIS,\nwhere mrGNN is much faster. We hypothesize that the primary input discrepancy resides in the\nQUBO matrix's shape, as the underlying graph structures remain the same. Therefore, differences\nmay be due to solver technicalities, such as the sparsity$^2$. Note that, for a given graph, a local\nGNN-based solver is, on average, 9\u00d7 faster than the local AM-based solver, ignoring solution quality\naspects. This speed advantage of the GNN solver underscores its practicality in real-world scenarios.\nWe report the total time: the sum of local AM and GNN global solver times; the price to pay for a\nbetter solution is the extra time the AM takes. It is worth noting that such time comparison assumes\nthe Louvain compression has been performed in advance, a realistic scenario in the real world.\nImpact of GNN module selection We compared GCN against a Graph Attention Network (GAT)\n[29, 4], as the latter automatically learns to weight incoming node vectors from the set of neighbors\nduring the aggregation step. For MIS, we could see a slight decrease in the achieved loss by up to\n100k nodes, but at the expense of 8\u00d7 total time (average for all target graphs). Unfortunately for the\nother problems, no conclusive evidence was found across graphs, which may suggest the GNN layer\ncould be problem-specific."}, {"title": "Conclusion and Future Work", "content": "We explore how to combine the accuracy of AM and the flexibility of GNN to solve combinatorial\noptimization problems. Our approach was tested on canonical combinatorial problems, showing that\nthe flexibility of GNNs can allow the transfer of the accurate capabilities of the AM to graphs that are\ninitially out of its reach. For future work, we are interested in i) the reuse of partial solutions across\nsimilar problems and ii) an end-to-end differentiable framework."}]}