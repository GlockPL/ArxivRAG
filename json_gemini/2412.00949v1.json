{"title": "STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft", "authors": ["Nicholas Lenzen", "Amogh Raut", "Andrew Melnik"], "abstract": "Recently, the STEVE-1 approach has been introduced as a method for training generative agents to follow instructions in the form of latent CLIP embeddings. In this work, we present a methodology to extend the control modalities by learning a mapping from new input modalities to the latent goal space of the agent. We apply our approach to the challenging Minecraft domain, and extend the goal conditioning to include the audio modality. The resulting audio-conditioned agent is able to perform on a comparable level to the original text-conditioned and visual-conditioned agents. Specifically, we create an Audio-Video CLIP foundation model for Minecraft and an audio prior network which together map audio samples to the latent goal space of the STEVE-1 policy. Additionally, we highlight the tradeoffs that occur when conditioning on different modalities. Our training code, evaluation code, and Audio-Video CLIP foundation model for Minecraft are made open-source to help foster further research into multi-modal generalist sequential decision-making agents.", "sections": [{"title": "1 Introduction", "content": "Recent works have shown that we can train generalist sequential decision-making agents [1, 2, 3, 4]. In particular, Lifshitz et al. [4] introduced the STEVE-1 approach for creating generative instruction-following agents to follow short-horizon instructions without being trained on a specific set of tasks, by learning to follow instructions represented as latent vectors in a CLIP [5] embedding space. However, this approach produces generative agents that are limited to following instructions in the specific input modalities of the CLIP model.\nCreating agents which are increasingly multi-modal enables the creation of singular agents that can leverage the advantages provided by each prompting modality. Modalities like audio and video often co-occur together, which facilitates the collection of internet-scale datasets with relatively low effort [3, 4]. Thus, the ability to create increasingly multi-modal agents or extending the prompting modalities of existing agents is an important area for investigation, as multi-modal agents can leverage the advantages provided by each prompting modality.\nIn this work, we introduce a methodology for extending the prompting modalities of generative agents trained to follow instructions. We apply our methodology to the Minecraft domain by enhancing the STEVE-1 Minecraft agent [4] to follow audio prompts in addition to its original text and visual prompting modalities. Our findings show that the audio-conditioned agent surpasses both"}, {"title": "2 Related work", "content": "Minecraft for AI Minecraft has become a popular environment for testing AI agents due to its open-ended nature, providing a broad spectrum of tasks (e.g., [3, 6, 7, 4, 8, 9, 10, 11, 12, 13, 14]). Frameworks like MineRL [7, 11] and MineDojo [6] facilitated this trend by providing the necessary tools to run AI agents in Minecraft. Notable models include MineCLIP [6], which integrates Minecraft video and text prompts into a shared latent space, VPT [3], a generative model for behavior in Minecraft, STEVE-1 [4], which generates behavior based on visual and text goals. These models are specifically designed to operate within the Minecraft environment, enabling AI to perform a variety of complex tasks. These range from simple activities like collecting blocks and defeating enemies to more complex challenges such as crafting items, which require accomplishing numerous secondary tasks.\nMulti-modal Decision-Making Several prior works explored the use of LLMs in creating Minecraft agents that can follow instructions [15, 14, 13, 16]. These works typically use LLMs to make high-level plans that are then executed by lower-level RL [13] or scripted [17] policies. JARVIS-1 [14] is an open-world agent that uses a memory-augmented multimodal language model to achieve planning and control in Minecraft, capable of completing over 200 tasks. Groot [15] is an agent that learns to follow open-ended instructions by watching gameplay videos, achieving advanced goal specification and control. MineDreamer [16] is an embodied agent that uses a Chain-of-Imagination mechanism to effectively follow diverse, abstract, and sequential instructions in Minecraft. MP5 [18] is a multimodal embodied Minecraft agent that decomposes complex tasks, designs context-aware plans, and performs goal-conditioned actions in process-dependent and context-dependent tasks. MUTEX [19] introduces a transformer-based approach for policy learning from multimodal task specifications, enabling agents to follow instructions and goals across six"}, {"title": "3 Method", "content": "In STEVE-1 [4], Lifshitz et al. introduce an approach to train generative instruction-following agents by learning to follow instructions represented as goal embeddings in the latent space of a CLIP model [5]. This approach consists of two parts, the policy and a prior. The policy extends the VPT model [3] in order to enable it to fullfill goals in the form of visual MineCLIP [6] embeddings. In order to enable the agent to follow instructions given through text, the text encoder of MineCLIP is employed to encode the text prompt, which is then mapped onto a visual MineCLIP embedding by the prior network which is a CVAE [29, 30] trained to sample latent visual MineCLIP embeddings conditioned on a text embedding. However, the resulting agent can only follow the two modalities of the employed CLIP model (in the case of STEVE-1, the two modalities of MineCLIP are text and video).\nTo address this problem, we introduce a methodology for extending the prompting modality of generative agents trained with the approach proposed by Lifshitz et al. [4]. We train a new CLIP model where one of the modalities is the new prompting modality. Then, we learn a mapping from the new CLIP latent space to the CLIP latent space originally used to train the generative agent. Thus, to follow instructions in the new prompting modality, we encode the instruction with the corresponding encoder for the new CLIP model, pass it through the prior to obtain a latent goal, and then pass this latent goal to the policy of the agent trained using the approach from [4].\nWe apply our methodology to the challenging Minecraft domain, where we extend the STEVE-1 agent to follow audio prompts by creating an Audio-Visual CLIP model for Minecraft. We pick video as the other modality for our CLIP model since both audio and video are naturally occurring modalities, which means that we can scalably train the model using unlabelled Minecraft videos."}, {"title": "3.1 Audio-Video CLIP Foundation Model for Minecraft", "content": "Our proposed Audio-Video CLIP foundation model consists of a frozen video-encoder and frozen audio-encoder on top of which we train non-linear transformation networks to transform the encoder embeddings into a new, shared latent space. We train this model on the Audio-Video dataset as described in the next subsection. Below, we outline the details of the video encoder, audio encoder, and transformation networks. See Figure 2 for a visualization of the Audio-Video CLIP model architecture.\nVideo Encoder We use the pretrained video encoder from the MineCLIP model [6] which maps 16-frames of video input and text to a joint embedding space. Specifically, the MineCLIP video encoder consists of a frame-wise image encoder, which creates an embedding for each of the 16 input frames followed by a temporal pooling network. This network pools the 16 frame-wise embeddings into a single embedding for the whole video input. We use the attention-based pooling version of MineCLIP. See [6] for more details about MineCLIP."}, {"title": "Audio Encoder", "content": "We use the pretrained Audio Spectrogram Transfomer (AST) [32] model which was originally trained to classify audio spectrograms into different categories (i.e., speech, vehicle, musical instrument, etc.). We embed Minecraft audio samples using AST by first computing the corresponding spectrogram representation of the audio sample, then passing this spectrogram to the AST encoder, and using the logits of the model as audio embeddings."}, {"title": "Transformation Networks", "content": "The MineCLIP video encoder and AST audio encoder weights are frozen, but we also train non-linear transformation networks on top of these models to transform the latent embeddings from each encoder to a new shared latent space. These transformation networks serve two purposes. First, training one transformation network is necessary to ensure that the latent vectors have the same size. Second, training both transformation networks allows our Audio-Video CLIP model to learn its own latent space. For example, if we only added a learned transformation network to the audio encoder, then the objective of the audio encoder's transformation network would be to mimic the latent space of the MineCLIP video encoder, which could potentially restrict the expressivity of the learned embeddings. We found that the best-performing architecture for the transformation network is an upscaled version of the mapping network used by StyleGAN 3 [31, 33]. Specifically, we increase the layer count from eight to ten and the hidden dimension from 512 to 1024. Further, we use cosine similarity as the similarity metric between both the two latent vectors, as in the original CLIP paper [5]. Both transformation networks map their input dimensionality (527 for audio embeddings and 512 for video embeddings) to a 512-vector. Training goal of the Audio-Video CLIP model is to maximize the cosine similarity of matching audio-video pairs, while simultaneously minimizing the cosine similarity of not matching pairs found in a training batch. This is achieved by employing a contrastive learning scheme as was used to train the original CLIP model [5]."}, {"title": "3.2 Audio-Video Dataset", "content": "To train the Audio-Video CLIP foundation model, we collect a dataset of Minecraft video and ac-companying audio (where the audio is not overlayed with music or commentary). Most of the dataset is sourced from YouTube videos of Minecraft gameplay without commentary, where we cut off the first two minutes of gameplay since many videos start with a short, irrelevant introduction sequence. We used 25 hours of such data to train the Audio-Video CLIP model which we use to generate our results. However, the full dataset which we plan to release contains about 600 hours of data. See appendix for more details about the dataset."}, {"title": "3.3 Training the Audio-Video CLIP Foundation Model", "content": "Our training scheme is the same as the original CLIP model [5], except that we do not use a cosine scheduler [34] to decay the learning rate. As previously mentioned, we freeze the weights of both encoder models during training and only update the weights of the transformation networks, which we train for 100 epochs. See Table 1 and 2 in appendix A for a list of hyperparameters."}, {"title": "3.4 Extending STEVE-1 to Condition on Audio Prompts", "content": "We aim to extend the STEVE-1 agent to condition on audio prompts. To that end, we train the Audio-Video CLIP foundation model which learns a new shared latent space for audio and video modalities. However, we cannot directly condition the STEVE-1 policy on embeddings in this new latent space, as it is different to the MineCLIP latent space that the policy was trained on. Thus, to condition the STEVE-1 policy on audio, we must train a prior which maps audio embeddings from the Audio-Video CLIP model to visual MineCLIP embeddings. Note that this prior does not map audio embeddings to visual embeddings from our Audio-Video CLIP model, but rather to visual embedding from the MineCLIP model (which the STEVE-1 policy was trained to follow).\nThe architecture of our prior, which maps audio embeddings to visual MineCLIP embeddings, is similar to the prior in STEVE-1 [4], which maps text MineCLIP embeddings to visual MineCLIP embeddings. It is implemented as a CVAE [29, 30] where the encoder and decoder are both two-layer MLPs with a hidden dimension of 256 and layer normalization between layers. Thus, to condition STEVE-1 on an audio sample, we first compute the audio embedding using the audio encoder from our Audio-Video CLIP model. Then, we use the prior to map this audio embedding into the latent goal space of the policy (which is the visual MineCLIP embedding space), which we can use to condition the STEVE-1 policy and generate instruction-following behavior with keyboard/mouse controls in Minecraft. See Figure 3 for the architecture of STEVE-Audio."}, {"title": "3.5 Evaluation", "content": "Following the programmatic evaluation methodology from Baker et al. [3] and Lifshitz et al. [4], we evaluate the performance of the audio-conditioned STEVE-1 agent [4] on a set of short-horizon item-collection tasks in Minecraft (collecting dirt, wooden logs, seeds, sand, cobblestone, and leaves). We compare the performance of the audio, text, and visual conditioned versions of STEVE-1 with minimal prompt engineering. Each task was evaluated on 10 different seeds for 2 minutes each (2400 timesteps at 20 frames-per-second). See Figure 1 for the goal conditioning tasks with corresponding audio and text prompts."}, {"title": "4 Experimental Results", "content": "In our experiments, we aim to answer the following questions:\n1. How well does our audio-conditioned STEVE-1 agent perform compared to the original text-conditioned and visual-conditioned versions?\n2. What are the tradeoffs that occur when switching between different prompting modalities?"}, {"title": "4.1 Audio-Conditioned STEVE-1", "content": "In Figure 4, we compare the performance of the audio-conditioned STEVE-1 agent to the original text-conditioned and visual-conditioned STEVE-1 agents [4] on various short-horizon tasks. The audio-conditioned agent performs better than the visual-conditioned agent in four of the six evaluation tasks. Specifically, the audio-conditioned agent collects 6.4\u00d7 more wood, 7.25\u00d7 more dirt, 5.1x more sand, and 1.6\u00d7 more leaves than the visual-conditioned agent, while it collects 0.9\u00d7 the amount of seeds and 0.7\u00d7 the amount of cobblestone. When compared to the text-conditioned agent, the audio-conditioned agent collects 1.8\u00d7 more wood, 8\u00d7 more dirt, 2.2\u00d7 more seeds, 17.7\u00d7 more sand, and 2.9\u00d7 more leaves, while it collects 0.7\u00d7 the amount of cobblestone (the only task where the audio-conditioned agent performed worse than the text-conditioned agent). These results indicate that our audio-conditioned agent generally performs better than the original STEVE-1 modalities, which suggests that our proposed methodology is an effective way to extend the prompting modalities of generative agents created using the STEVE-1 approach.\nWe will make the generated videos available on our website as part of the supplementary material submission."}, {"title": "4.2 Multi-Modality Tradeoffs", "content": "Our proposed method enables adding new prompting modalities to existing generative agents created with the STEVE-1 approach [4]. In this section, we investigate the tradeoffs that occur when switching between different prompting modalities. In fact, as described below, these positive and negative tradeoffs are a significant motivation for creating such multi-modal agents and extending the prompting modalities of existing agents. That is, creating increasingly multi-modal agents allows them to take advantage of the positive tradeoffs provided by each prompting modality."}, {"title": "4.2.1 Versatility VS. Performance", "content": "An important tradeoff to consider between modalities is the ability of each modality to unambiguously express complex instructions. For example, the text modality facilitates communicating complex tasks with decision-making agents (i.e., \u201cobtain a bed and place it on your roof next to your dog, but don't steal the bed from a village\"). On the other hand, such complex instructions would be difficult to communicate with other modalities like audio. Furthermore, beyond specifying complex instructions, it seems that prompting with audio can lead to scenarios where the instruction is too ambiguous or non-specific to be completed. To investigate this, we evaluate the text, visual, and audio versions of STEVE-1 on three different \u201cplacing\u201d tasks, where the audio is more ambiguous. For example, the audio samples for placing dirt sounds very similar to the ones for digging dirt, and audio samples for placing wooden planks or cobblestone sounds very similar to the audio samples for placing any Minecraft item that is made out of wood or stone, respectively. The last row of Figure 4 shows that audio prompting generally performs much poorer than text and visual prompting, which supports the idea that some tasks are more ambiguous in other modalities (i.e., audio).\nHowever, despite audio being a more ambiguous way to specify some tasks, the audio-conditioned STEVE-1 agent generally performs better than the text and visual counterparts. We hypothesize that this is due to the higher correlation between the audio and video modalities, than exists between text and video. That is, the Text-Video dataset used to train MineCLIP [6] was sourced from YouTube videos of Minecraft gameplay and their time-aligned captions. Thus, much of the text in the dataset is noisy in the sense that the text is unrelated or uncorrelated to the Minecraft behavior observed in the video (i.e., when someone says \u201cplease like this video\u201d). On the other hand, audio and video are both highly correlated, naturally co-occurring modalities. This means that the audio-video pairs in our Audio-Video dataset are more highly correlated.\""}, {"title": "4.2.2 Prompt Engineering", "content": "In our experiments, prompts for all three modalities were selected with minimal prompt engineering. Thus, since audio-conditioning generally yields better performance, our results suggest that audio prompting might require less prompt engineering than text and visual prompting to perform well. This requires further investigation but could be explained due to the fact that audio samples are usually very similar across different demonstrations of the same task. Thus, the audio embeddings from our Audio-Video CLIP model could be more semantically relevant representations of the task, less affected by small changes in the prompt (i.e., visual embeddings for the same task could be affected by random objects moving in the periphery). This may play a role in the improved audio-conditioning performance. However, more investigation is needed."}, {"title": "5 Conclusion", "content": "This paper introduces a methodology for extending the prompting modalities of generative instruction-following agents. By creating increasingly multi-modal agents, singular agents can leverage the advantages and tradeoffs provided by different prompting modalities when completing a task. We apply our methodology to the challenging Minecraft domain and extend the existing STEVE-1 agent to follow audio prompts as well. The resulting audio-prompted STEVE-Audio agent outperforms the original text and visual-prompted versions of STEVE-1 on short-horizon item-collection tasks in Minecraft. Future work should investigate applying this methodology for extending other generative agents to different sensory modalities and in different domains.\nWhile our method achieves strong results, it has several limitations. First, it requires training a CLIP model [5] which takes the new prompting modality as one of its inputs and which requires a dataset of correlated modality-to-modality pairs. Second, our approach is limited by the tradeoffs of the prompting modalities to which we extend the agent. For example, it is often harder to specify some tasks using audio prompts compared to text prompts."}]}