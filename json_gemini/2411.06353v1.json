{"title": "Deep Active Learning in the Open World", "authors": ["Tian Xie", "Jifan Zhang", "Haoyue Bai", "Robert Nowak"], "abstract": "Machine learning models deployed in open-world scenarios often encounter unfamiliar conditions and perform poorly in unanticipated situations. As AI systems advance and find application in safety-critical domains, effectively handling out-of-distribution (OOD) data is crucial to building open-world learning systems. In this work, we introduce ALOE, a novel active learning algorithm for open-world environments designed to enhance model adaptation by incorporating new OOD classes via a two-stage approach. First, diversity sampling selects a representative set of examples, followed by energy-based OOD detection to prioritize likely unknown classes for annotation. This strategy accelerates class discovery and learning, even under constrained annotation budgets. Evaluations on three long-tailed image classification benchmarks demonstrate that ALOE outperforms traditional active learning baselines, effectively expanding known categories while balancing annotation cost. Our findings reveal a crucial tradeoff between enhancing known-class performance and discovering new classes, setting the stage for future advancements in open-world machine learning.", "sections": [{"title": "1 Introduction", "content": "Modern machine learning models have achieved remarkable progress by leveraging large amounts of labeled data LeCun et al. (2015); He et al. (2016); Khosla et al. (2020). Despite this success, most models are developed for closed-world settings, assuming that both training and test data originate from the same distribution. However, this assumption does not align with real-world environments, where models are inevitably encounter out-of-distribution (OOD) data with previously unseen classes Hendrycks & Gimpel (2022); Hendrycks et al. (2022); Salehi et al. (2022). Constrained by their fixed class boundaries, traditional models often struggle to generalize effectively to these novel classes, limiting their adaptability in open-world scenarios. Furthermore, obtaining human supervision for these novel examples in open-world scenarios is often time-consuming and costly, posing a significant challenge to model adaptation and improvement. Active learning, which iteratively selects the most informative examples for labeling, has emerged as a promising approach to address the expensive nature of gathering human supervision. By prioritizing examples that provide the most significant information gain, active learning can enhance the model's learning efficiency while reducing the need for extensive human annotation. This approach is particularly valuable in open-world scenarios, where the high annotation cost and time-consuming nature of labeling make traditional supervised learning methods impractical.\nDespite its potential, existing active deep learning algorithms (see (Zhan et al., 2022; Zhang et al., 2024a) for overviews) have rarely been studied under open-world scenarios, particularly those involving novel classes and imbalanced data distributions. In this work, we addresses this gap by developing a novel active learning algorithm that integrates energy-based OOD detection techniques to handle the complexity of open-world environments. Our method is designed for multi-class classification tasks in the open-world, where the model encounters both known and unknown classes after deployment. Of the few works that study active learning under open-world settings, existing methods are often tailored to specific vision tasks, which result in highly specialized algorithm designs. When these methods are simplified for more generic problems like classification, they often reduce to basic uncertainty or diversity sampling techniques. In contrast, our"}, {"title": "2 Related Work", "content": "The earliest machine learning methods typically employed passive learning in a closed-world setting, where the model passively received training data and all data was fully labeled. However, in modern research and practice, it is common to encounter situations where i) some or all of the data is unlabeled, ii) the unlabeled data contains new classes, or iii) it is necessary to actively select training data. Our research combines these aspects and proposes a novel approach tailored to these scenarios."}, {"title": "2.1 Open-World Learning", "content": "Open-world learning addresses the challenge of operating in dynamic environments where the model starts with a set of known classes and must detect and manage instances from unknown classes. This paradigm involves both labeled and unlabeled data, with a mixture of known and unknown classes, requiring the model to either classify examples into known categories or recognize them as novel Rizve et al. (2022); Zhu et al. (2024); Xiao et al. (2024). Unlike novel class discovery Fini et al. (2021); Zhong et al. (2021b); Han et al.; Zhong et al. (2021a); Roy et al. (2022), which focuses solely on discovering new object categories, open-world learning must also correctly identify instances from previously known classes, making the task more complex.\nOpen-world semi-supervised learning Cao et al. (2022); Sun et al. (2024) generalizes semi-supervised learning by considering scenarios where the data includes both labeled and unlabeled instances from known and unknown classes. The model must learn to classify known classes and identify novel ones from unlabeled"}, {"title": "2.2 Out-of-Distribution Detection", "content": "Out-of-distribution (OOD) detection is a fundamental challenge for machine learning models deployed in open-world environments. It involves identifying whether inputs belong to unknown classes that the model has not encountered during training. The overconfidence of neural networks when handling out-of-distribution data was first revealed in Nguyen et al. (2015). Research in OOD detection has taken several main directions: post-hoc methods that devise scoring functions for detecting OOD inputs Liu et al. (2020); Lee et al. (2018); Sun et al. (2022), training-time regularization methods that leverage additional auxiliary OOD datasets to address OOD detection Hendrycks et al. (2018); Van Amersfoort et al. (2020); Katz-Samuels et al. (2022); Bai et al. (2023; 2024), and approaches exploring representation learning, such as exploring multiview contrastive losses Khosla et al. (2020); Chen et al. (2020) for OOD detection Winkens et al. (2020); Sehwag et al. (2021); Ming et al. (2022).\nIn particular, post-hoc methods address OOD detection by deriving test-time OOD scoring functions for a pre-trained classifier. These methods include maximum softmax probability Hendrycks & Gimpel (2016), distance-based scores Lee et al. (2018); Sun et al. (2022), energy-based scores Liu et al. (2020), activation rectification Sun et al. (2021), ViM score Wang et al. (2022), gradient-based scores Huang et al. (2021), among others. The energy score Liu et al. (2020) provides a unified framework that more effectively distinguishes between in-distribution (ID) and OOD examples. This approach theoretically aligns with the probability density of the inputs and is less susceptible to the overconfidence issue. In this work, we employ the free energy score for input examples to identify OOD data and cluster patterns, with the aim of effectively discovering and learning new classes from the data distribution."}, {"title": "2.3 Deep Active Learning", "content": "Active learning studies the problem of minimizing annotation cost while training high performance models. Active learning methods typically follows a sequential and adaptive procedure, where the algorithms first trains a model based on the annotated examples so far followed by annotating more examples selected from a large number of unlabeled examples. The algorithm strategically chooses from the unlabeled examples for annotation, typically relying on uncertainty, diversity or expected model change types of metrics.\nUncertainty based strategies chooses examples that are determined to be the most uncertain for the model trained on the labeled set thus far. This includes many of the traditional uncertainty metrics such as margin, entropy and confidence scores (Lewis & Gale, 1994; Tong & Koller, 2001; Balcan et al., 2006; Settles, 2009; Kremer et al., 2014). More advanced approaches for deep learning also includes Bayesian uncertainty estimation and adversarial training (Gal et al., 2017; Ducoffe & Precioso, 2018; Beluch et al., 2018). Diversity based strategies aim to choose a set of unlabeled examples that are maximally different in"}, {"title": "2.4 Open-World Active Learning", "content": "Overall, there has been few attempts in studying active learning under an open world setting. Ma et al. (2024) also studies the active learning setting where some classes do not have labeled examples. However, their paper assumes the knowledge of the total number of classes beforehand, making their study closer to the traditional active learning than truly addressing the open world challenge, where the total number of classes is agnostic to the learner. As we will discuss in Section 6, not knowing the number of classes poses a challenging tradeoff between exploring new classes and learning existing classes well.\nOther existing literature shares our setting but targets specific tasks. Chen et al. (2023) propose OpenCRB for open-world 3D object detection, proposing an uncertainty-based scoring method that leverages the spatial distribution and density of data points for 3D point clouds. In the more general settings, this work essentially reduces to simple uncertainty methods, which we will show to be less effective than our methods in open world settings. Zamzmi et al. (2022) utilizes a simple diversity-based method, k-medoids clustering, in annotating a diverse set of images for echocardiography view classification. As we will show, our method that combines OOD detection and diversity-based methods is superior than only considering diversity alone. In Section 5.2.1 and Table 2, we provide details of the OOD scores we conduct experiments on."}, {"title": "3 Problem Setup", "content": "We consider a pool-based batched active learning setting in an open-world scenario. The learner has access to a large pool of unlabeled data X = {X1,X2, ..., XN }, and the true label distribution is defined by an unknown ground truth function f* : X \u2192 {1, 2, ..., K}, where K is the total number of classes in X. At the beginning the labeling process the labeled set L is set to be empty. At each iteration t, the learner selects a small batch of B unlabeled examples, {x(t)b}Bb=1 X, from the pool. The learner observes the labels {f*(x(t)b)}Bb=1 and adds these examples to L, the set of labeled examples available for training so far. In an open world setting, examples in L may only cover a fraction of the classes, which we denote by Kt \u2286 [K], the known classes. After each batch query, the learner trains a Kt-class classification model ft on L, which is used to inform the selection of the next batch of examples for labeling. Furthermore, we let pt(x) denote the softmax distribution score based on the classification model ft.\nThis iterative process continues, with the goal of selecting the most informative examples to annotate in each round, minimizing the number of labeled examples needed to achieve good generalization performance. The"}, {"title": "3.1 Out-of-Distribution (OOD) Scores", "content": "During each step t of the active learning process, recall Kt represents the set of classes that have at least one labeled example. When training a model using the labeled dataset, examples from classes not in Kt (i.e., [K]\\Kt) are considered out-of-distribution (OOD) by definition. Traditional OOD detection research focuses on identifying OOD examples within a test set. In this study, we use an OOD scoring function, denoted as \u03a9(x, f), where x is an example and f is any neural network classification model. This function \u03a9(x, f) provides a measure of how likely an example is to be in-distribution (ID) versus OOD. A higher OOD score suggests a greater likelihood that the example is OOD, which in turn is more likely to belong to an unknown class. We apply these OOD scoring techniques to determine which unlabeled examples are most likely to belong to unknown OOD classes."}, {"title": "4 Methodology", "content": "To address the open world active annotation problem, we focus on two key objectives: i) maximizing the selected examples' coverage of OOD classes; ii) optimizing the labeling budget to efficiently collect examples from these OOD classes. A straightforward approach would be to simply annotate examples with the highest OOD scores during each round of annotation. However, our analysis in Figure 1 reveals that examples with high OOD scores often cluster within a small subset of OOD classes, making this approach suboptimal for achieving broad coverage. This observation suggests the need for a diversity-based strategy to ensure our annotated examples span a wide range of concepts."}, {"title": "4.1 ALOE: Our Actively Learning Algorithm Under Open-world Environments", "content": "In this section, we describe our proposed algorithm for querying unlabeled data in a pool-based batched active learning framework under open-world conditions. As detailed in Algorithm 1, our algorithm ALOE leverages OOD scores and k-means clustering method to ensure a balance between identifying novel OOD examples and maintaining diversity within the queried batch.\nThe algorithm proceeds iteratively for T iterations, with each iteration t comprising several key steps, as illustrated in Figure 2. Initially, a deep neural network undergoes is trained on the current labeled set Lt-1, resulting in an updated model ft-1 that incorporates information from the newly labeled data. Subsequently,"}, {"title": "4.2 Reverse ALOE: Alternative Query Strategy", "content": "In addition to the approach described above, we also considered an alternative method reverse ALOE. As the name suggests, the order of using OOD scores and clustering methods are flipped. OOD scores were used first to select an initial set of candidates in this algorithm. Similarly with ALOE, we calculated a threshold \u03c4, which ensures a 95% true positive rate (TPR) in labeled set Lt. The initial set are examples with OOD score larger than the threshold \u03c4.\nThese candidates were then clustered with methods described in section 5.2.5 to ensure diversity among the queried examples with the cluster number equal to batch size. In each cluster, the sample with highest OOD score is queried to be labeled. However, this method performed poorly in our experiments (section 5.2.2), as it did not select a super diverse set of examples, reducing the overall efficiency of the active learning process."}, {"title": "5 Experiment", "content": "In this section, we describe our proposed algorithm for querying unlabeled data in a pool-based batched active learning framework under open-world conditions. As detailed in Algorithm 1, our algorithm ALOE leverages OOD scores and k-means clustering method to ensure a balance between identifying novel OOD examples and maintaining diversity within the queried batch.\nThe algorithm proceeds iteratively for T iterations, with each iteration t comprising several key steps, as illustrated in Figure 2. Initially, a deep neural network undergoes is trained on the current labeled set Lt-1, resulting in an updated model ft-1 that incorporates information from the newly labeled data. Subsequently,"}, {"title": "5.1 Experiment Setup", "content": "Dataset. Open world challenges often arise from dataset imbalance with a large number of classes, where smaller classes are often unknown at the beginning of the annotation process. In this paper, we focus our experiment evaluations on common long tail imbalanced datasets with a large number of classes. Specifically, we utilize three image classification benchmark datasets, CIFAR100-LT (Alex, 2009), ImageNet-LT (Deng et al., 2009) and Places365-LT (Zhou et al., 2017). The distribution of the three long-tailed datasets are given in Appendix 8.\nModel. For evaluation, we follow the latest LabelBench framework (Zhang et al., 2024a), while introducing the new open world setting with dynamic number of classes at each iteration. Specifically, we fine-tune the pretrained CLIP ViT-B32 image encoder (Radford et al., 2021) with a linear classification head attached. For every iteration of the active learning algorithm, the model is reinitialized to the pretraining checkpoint and finetuned end-to-end on all labeled examples thus far.\nMetric. To evaluate the performance of our method in the open-world active learning setting, we use two primary metrics: the number of annotated classes and the balanced accuracy. The number of annotated classes (denoted as Kt in Section 3) is crucial as missing certain categories could hinder the model's per-formance when deployed in practice. On the other hand, balanced accuracy gives a more wholistic view of the model's generalization performance. Specifically, given a test set of examples (x'1,y'1),..., (x'M, Y'M) and"}, {"title": "5.2 Main Results and Analysis", "content": "Dataset. Open world challenges often arise from dataset imbalance with a large number of classes, where smaller classes are often unknown at the beginning of the annotation process. In this paper, we focus our experiment evaluations on common long tail imbalanced datasets with a large number of classes. Specifically, we utilize three image classification benchmark datasets, CIFAR100-LT (Alex, 2009), ImageNet-LT (Deng et al., 2009) and Places365-LT (Zhou et al., 2017). The distribution of the three long-tailed datasets are given in Appendix 8.\nModel. For evaluation, we follow the latest LabelBench framework (Zhang et al., 2024a), while introducing the new open world setting with dynamic number of classes at each iteration. Specifically, we fine-tune the pretrained CLIP ViT-B32 image encoder (Radford et al., 2021) with a linear classification head attached. For every iteration of the active learning algorithm, the model is reinitialized to the pretraining checkpoint and finetuned end-to-end on all labeled examples thus far.\nMetric. To evaluate the performance of our method in the open-world active learning setting, we use two primary metrics: the number of annotated classes and the balanced accuracy. The number of annotated classes (denoted as Kt in Section 3) is crucial as missing certain categories could hinder the model's per-formance when deployed in practice. On the other hand, balanced accuracy gives a more wholistic view of the model's generalization performance. Specifically, given a test set of examples (x'1,y'1),..., (x'M, Y'M) and"}, {"title": "5.2.1 OOD Scoring Functions and Baseline Active Learning Algorithms", "content": "Before we present our experimental results, we introduce the OOD scoring functions and baseline active learning algorithms we use.\nOOD Scoring Functions: In Table 2 we summarize the OOD scoring methods from previous literature, which are used throughout our research. Energy (Liu et al., 2020) score provides a global view of the uncertainty by aggregating information over all classes. These scores are particularly useful when the model outputs soft probabilities that span multiple classes. Margin (Scheffer et al., 2001) scores offer a more focused view by comparing only the top two predicted probabilities, making them sensitive to near-boundary decisions. GradNorm (Huang et al., 2021) considers the gradient information, which can provide a more direct measure of how uncertain the model is with respect to its parameters, especially in deep learning models. Mahalanobis distance (Lee et al., 2018) is more geometric, considering how far a given example is from the expected distribution of a particular class. This distance metric is often used in embedding spaces. Gradient-based (Bai et al., 2024) score measures the model's sensitivity to input perturbations by calculating gradients, capturing how the model would change with slight variations.\nBaseline Active Learning Algorithms: We evaluated our algorithm against several standard active learning baselines: Random, Margin, Coreset, Galaxy, and Badge. Random is a straightforward baseline where examples are selected uniformly at random from the pool of unlabeled data, offering a comparison with non-strategic sampling. Margin (Scheffer et al., 2001) is an uncertainty-based sampling"}, {"title": "5.2.2 Main Results on Multiple Datasets", "content": "We now present the results of our algorithm applied to CIFAR100-LT, ImageNet-LT, and Places365-LT. As mentioned before, the long-tailed distribution of classes is a well-known challenge in practical applications. Due to this issue, we often encounter open world learning scenarios where rare classes are likely to be missed during the initial annotation phase. In the following experiments, we receive annotations from only a small number of classes in the initial batch of annotation (three for CIFAR100-LT and Places365-LT, and ten for ImageNet-LT). In addition, our algorithm ALOE use energy and GradNorm for OOD scoreing function \u03a9 and k-means for clustering method. We also include ablation studies around different initial number of classes, OOD scores, and clustering methods in the later sections.\nAs shown in Figure 3, on CIFAR100-LT, our method ALOE significantly outperforms all baseline methods in both balanced accuracy and the number of newly discovered classes. Starting with only three initially labeled classes, our method quickly expands the set of annotated classes, maintaining a higher discovery rate throughout the iterations. By leveraging the combination of OOD detection and diversity-based sampling, ALOE is able to efficiently explore unknown classes while ensuring strong performance on the known classes. On ImageNet-LT, ALOE saves 70% of annotation cost to achieve the same accuracy comparing to random sampling. We also note that while Coreset and Galaxy occasionally match ALOE's performance, they struggle with consistency across different datasets. In contrast, ALOE consistently performs best among all methods on all datasets."}, {"title": "5.2.3 Ablation Study of Initial Number of Annotated Classes", "content": "In Figure 4, we analyze the robustness of our algorithm by varying the initial number of annotated classes on CIFAR100-LT. We experiment with three settings, including the initial number of annotated class being 10, 30 and 50 respectively. As observed in this experiment, ALOE consistently performs the best among all algorithms and is robust to different initial number of classes. It is also worth noting that despite maintaining strong performance, the improvement over the second best algorithm narrows slightly compare to the experiment in the previous section, where only three classes are initially labeled. This observation"}, {"title": "5.2.4 Ablation Study of OOD Scores", "content": "Figure 5 shows that different OOD scores affect the performance of our algorithm. On CIFAR100-LT, Energy, MSP and GradNorm scores perform similarly. While MSP and Energy scores both focus on predictions, Energy's consideration of all class predictions makes it potentially more resilient on complex datasets. Thus, Energy and GradNorm scores are used in main experiments 5.2.2. Energy score is used for CIFAR100-LT and ImageNet-LT, while GradNorm is applied to Places365-LT."}, {"title": "5.2.5 Ablation Study of Clustering Methods", "content": "We also investigated various clustering methods. We experimented with popular clustering method used in prior active learning algorithms. For instance, TypiClust (Hacohen et al., 2022) employs k-means, Coreset (Sener & Savarese, 2017) utilizes k-center, and BADGE (Ash et al., 2019) uses k-means++. As shown in Figure 6, k-means yielded the best results, and we adopted it for all other experiments."}, {"title": "6 Balancing Novel Class Discovery and Known Class Learning", "content": "When running ALOE further on a larger amount of annotation budget, our experiments (Figure 7) demonstrate an intriguing phenomenon - baseline methods can even achieve slightly better performance than ALOE once most classes have been identified. Similarly, as mentioned in 5.2.3, on CIFAR100-LT, when the number of initial classes increases, the improvement gap of using ALOE narrows. These findings are expected, as ALOE uniquely prioritizes annotating diverse OOD classes, while baseline approaches focus on learning known ID classes. While this suggests the potential for an algorithm that excels at both tasks, achieving this dual objective presents significant challenges.\nThis challenge arises because the total number of classes is unknown to the learner. Even after most classes have been discovered in our ImageNet-LT and Places365-LT experiments, the learner cannot determine whether significant numbers of classes remain undiscovered. Furthermore, our experiments demonstrate that ALOE, while perform the best during the class discovery phase, may not be the ideal strategy for learning in-distribution (ID) classes. This reveals a fundamental tradeoff in open-world active learning: the need to balance annotation between exploring novel classes and consolidating knowledge of existing ones.\nFuture research could approach this challenge from a theoretical perspective by developing strategies that dynamically alternate between exploration and consolidation. One promising direction is an iterative approach that first ensures discovery of classes above a certain size threshold, followed by focused learning of these identified classes. This process could then repeat with progressively lower the size thresholds, ultimately yielding a model that performs well across classes of varying frequencies."}, {"title": "7 Conclusion", "content": "In this paper, we propose ALOE, a novel active learning algorithm specifically designed for open-world scenarios. Our approach leverages a two-stage process that combines diversity-based sampling with out-of-distribution detection, enabling the discovery and learning of new classes in a dynamic environment. Through experiments on three long-tailed datasets, ALOE demonstrates a consistent and clear advantage"}, {"title": "A Long Tail Datasets", "content": "CIFAR100. CIFAR100 contains 60,000 images across 100 classes, with 500 training images and 100 testing images per class. To create a long-tailed version of CIFAR100, we use an exponential distribution where the number of examples per class Ni is given by:\n$N_{i} = N_{0} a^{i}$,\nwhere n is the total number of classes, No is the number of examples in the most frequent class, and a is the imbalance factor. In our experiments, we set a = 0.01, creating a highly imbalanced version of the dataset.\nImageNet. We use the ImageNet-1k subset, containing 1.28 million training images across 1,000 classes. Similar to CIFAR100, we create a long-tailed version of ImageNet using an exponential distribution with the same formula. Again, we set a = 0.01 to generate the imbalance, resulting in a diverse distribution of images per class.\nPlaces365. Places365 is a large-scale scene recognition dataset consisting of over 10 million images across 365 classes, designed for training and evaluating models on a wide variety of scene categories, including indoor and outdoor environments. Each class has up to 5,000 images for training, providing a balanced dataset for scene classification tasks.\nThe Places365-LT we used in our experiments are based on the distribution described in Liu et al. (2019). This version follows an exponential distribution, where the number of examples per class ranges from a maximum of 4,980 images to as few as 5 images. The total is 62500 images. The distribution was implemented by referring to the publicly available code associated with the paper."}]}