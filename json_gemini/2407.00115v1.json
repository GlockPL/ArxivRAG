{"title": "Instance Temperature Knowledge Distillation", "authors": ["Zhengbo Zhang", "Yuxi Zhou", "Jia Gong", "Jun Liu", "Zhigang Tu"], "abstract": "Knowledge distillation (KD) enhances the performance of a student network by allowing it to learn the knowledge transferred from a teacher network incrementally. Existing methods dynamically adjust the temperature to enable the student network to adapt to the varying learning difficulties at different learning stages of KD. KD is a continuous process, but when adjusting the temperature, these methods consider only the immediate benefits of the operation in the current learning phase and fail to take into account its future returns. To address this issue, we formulate the adjustment of temperature as a sequential decision-making task and propose a method based on reinforcement learning, termed RLKD. Importantly, we design a novel state representation to enable the agent to make more informed action (i.e., instance temperature adjustment). To handle the problem of delayed rewards in our method due to the KD setting, we explore an instance reward calibration approach. In addition, we devise an efficient exploration strategy that enables the agent to learn valuable instance temperature adjustment policy more efficiently. Our framework can serve as a plug-and-play technique to be inserted into various KD methods easily, and we validate its effectiveness on both image classification and object detection tasks. Our code is at https://github.com/Zhengbo-Zhang/ITKD.", "sections": [{"title": "1. Introduction", "content": "Over the past few decades, the field of computer vision has undergone a transformative shift thanks to the remarkable progress of deep neural networks (DNNs). Nonetheless, the significant computation and storage demands of DNNs, present great challenges, especially in industrial applications where there is a preference for efficient and lightweight models. Typically, lightweight networks do not perform as well as deeper networks. To solve this issue, knowledge distillation (KD), which aims to enable smaller (student) models to compete with larger (teacher) models in performance, has been introduced [14]. Due to its remarkable effectiveness in boosting the capability of lightweight models, KD has been widely used in various tasks, e.g., object detection [7, 39], semantic segmentation [24, 36, 41], and natural language processing [11, 30].\nKD enhances the student network by transferring knowledge from a higher-capacity teacher network. During the process of KD, the capability of the student network is constantly changing. This results in the same piece of knowledge (training instance) has varying degrees of value to the student network at different learning stages [19]. Moreover, even within the same learning stage, the difficulty of learning varies between instances. The student network should assign more weight to examples that are difficult to learn [17]. However, most previous KD methods [10, 14, 35, 42] have not simultaneously taken into account the learning difficulty of each training instance as well as the learning stage they are in.\nTo address this issue, recent efforts [20, 23] have been made, where the temperature for each instance is adjusted to match its respective learning difficulty. This is because temperature, as a critical hyperparameter in KD, modulates the smoothness of the predictive distribution and sets the difficulty of the KD process."}, {"title": "2. Related Work", "content": "KD, as a model compression method, can trace back its origin in [14]. In the KD process, KL-divergence loss between teacher and student model predictions is minimized using a key hyperparameter known as \"temperature\". As [6, 14, 20, 23] stated, temperature helps adjusting the smoothness of the prediction distribution and sets the KD process's difficulty effectively. Due to the student model's learning capacity varies at different stages [19], some works [20, 23] explore to adjust the temperature dynamically based on the current learning stage to help the student network learn better from the teacher network.\nMKD [23] learns a dynamically varying temperature via the method of meta-learning [33] as the KD process progresses, but it is primarily designed for scenarios involving vision transformer [9] and strong data augmentation. The limitations of MKD preclude its effective application in temperature adjustment within the majority of KD methods, and previous studies [20] have confirmed that directly applying MKD to KD models results in a significant degradation in performance. CTKD [20] utilizes a curriculum learning approach [5] to progressively learn a dynamic temperature parameter, starting from simple to complex scenarios. Particularly, CTKD progressively learns two versions of temperature: the global temperature and the instance temperature. However, CTKD does not take into account the future benefits (the performance enhancement of the student network between adjacent learning stages) when adjusting the instance temperature, and it also does not consider the student network's mastery of the instance. These shortcomings of CTKD causing its instance temperature being not robust, preventing the student network trained with CTKD from learning knowledge effectively. To overcome these drawbacks, we formulate the instance temperature adjustment in the KD process as a sequential decision-making task, where adjusting instance temperature is considered as an action. Besides, we present a novel state representation, which includes a feature that reflects the student network's degree of mastery over a training instance."}, {"title": "3. Preliminary on Reinforcement Learning", "content": "Reinforcement Learning (RL) involves an agent aiming to gain maximum cumulative rewards through interactions"}, {"title": "4. Method", "content": "The previous KD methods [20, 23] attempt to adjust the temperature to improve the student network's knowledge acquisition, but they overlook the nature of KD is continuous. When adjusting the temperature, they only consider the benefits in the current stage, neglecting the potential rewards of temperature adjustments in future learning stages. To address this issue, we treat the adjustment of instance temperature during the KD process as a sequential decision-making task, where the temperature adjustment for each instance is considered as the action within the task. Based on this insight, we propose the RLKD method (see Fig. 2) based on RL with a novel state representation (described in Sec. 4.1), allowing us to take into account the future rewards of temperature adjustment on training instances at the current stage. In our RLKD method, the reward is designed to measure the improvement in the student network's performance; thus, we calculate the reward during the parameter update of the student network. According to the KD setup, the student network updates its parameters after training on each batch of data (typically comprising 32 training instances), which means that we can only compute the reward once after every 32 actions. This leads to a significant delayed reward issue. To solve this problem, we design an instance reward calibration scheme (described in Sec. 4.2). Furthermore, we formulate a strategy for efficient exploration, enabling the agent to rapidly learn effective temperature adjustment policy (described in Sec. 4.3)."}, {"title": "4.1. Instance Temperature Adjustment as a Sequential Decision-making Task", "content": "In this work, we aim at learning a policy that directly maximizes the performance of the student network driven by the maximization of our designed reward. To achieve this goal, we formulate the instance temperature adjustment in the KD process as a sequential decision-making task: $(s_t, a_t, r_{t+1}, s_{t+1})$. Specifically, the process includes the following steps: 1) Estimate the state $s_t$ based on the performance of the teacher and student networks on the current training instance. 2) Given the current state $s_t$ and informed by the prior experiences, the agent evaluates each state-action pair and execute the action $a_t$ of temperature adjustment for each training instance. 3) After the agent performs the optimized action $a_t$, the environment transfers to a subsequent state $s_{t+1}$ and provides a reward $r_{t+1}$ to the agent. 4) The agent updates its policy based on the received reward $r_{t+1}$ and the newly observed state $s_{t+1}$.\nWe utilize the PPO framework [28] to model this process. Subsequently, we provide a detailed introduction to the definitions of state $s_t$, action $a_t$, and reward $r_t$.\nState. The state $s_t$ serves as input for the agent, providing critical support for agent making the instance temperature decision. The design of the state should align with the needs of the instance temperature decision-making policy. Intuitively, when the policy makes a temperature decision for a training instance $x$, it needs to consider the performance of both the teacher and the student networks on this instance. Moreover, due to the varying difficulty of the knowledge embodied in each training instance, the student network's mastery over each instance differs [17, 34]. The state $s_t$ should also include a measure of the student network's grasp on that particular instance.\nBased on these intuitions, given an instance $x$, we collect cues from three aspects to form the state $s_t$: the performance of the teacher network, the performance of the student network, and the extent to which the student network has mastered the instance. Particularly, the teacher network outputs its prediction $p_t$ at instance $x$ is expressed as:\n$p_t = \\underset{i \\in [k]}{\\operatorname{argmax}} f_{teacher}(x)_i,$\nwhere, $k$ represents the total number of categories, and $f_{teacher}$ denotes the teacher network. We use the probability $f_{teacher}(x)_{p_t}$ associated with the teacher network's prediction $p_t$ for the instance $x$ to measure the performance of the teacher network. Similarly, to measure the performance of the student network, we use the probability $f_{student}(x)_{p_s}$ associated with the student network's prediction $p_s$ for that instance $x$. To assess the mastery level of a student network over the instance $x$, we draw inspiration from uncertainty-based sampling in active learning [4, 27], and determine the mastery level by measuring the uncertainty score $U_{student}(x)$ in the student network's prediction distribution for the instance. The uncertainty score for the student network with respect to instance $x$ is calculated according to:\n$U_{student}(x) = 1 - (f_{student}(x)_{p_s} - \\underset{i \\in [k]\\backslash p_s}{\\operatorname{max}} f_{student}(x)_i)$.\nOur uncertainty score $U_{student}(x)$ is positively correlated with the degree of uncertainty exhibited by the student network towards instance $x$, which is because if the student network has a good grasp of the instance, the network is very confident in its prediction, resulting in a prediction distribution with a single high-probability predicted value. Conversely, if the mastery is poor, the student network exhibits uncertainty in its prediction, leading to multiple high-probability predicted values that are close to each other.\nIn summary, for a given instance $x$, we define our state $s_t$ as $(f_{teacher}(x)_{p_t}, f_{student}(x)_{p_s}, U_{student}(x))$, encompassing the predicted probabilities from the teacher network, the predicted probabilities from the student network, and the uncertainty score of the student network.\nAction. Our action is the decision-making regarding instance temperature $T$. To overcome the limitations of exploration in a discrete action space, we opt to explore instance temperature $T$ in a continuous action space. Below, we elaborate how to obtain the instance temperature $T$.\nUpon receiving the state $s_t$ for the instance $x$, we use $s_t$ as input to the actor network within the PPO framework. To better explore various actions of the actor network and to smooth its learning process, we design our actions to follow a Gaussian distribution $N(\\mu, \\sigma^2)$. Thus our actor outputs the mean $\\mu$ and variance $\\sigma$ of a Gaussian distribution. To boost the flexibility and randomness of action exploration, we randomly sample a value from the Gaussian distribution to serve as our instance temperature $T$. Finally, based on our experience that almost all the instance temperature varies within the range of 0 to 10, we restrict the temperature to a range by following formula:\n$T = 10 \\cdot sigmoid(\\hat{T}),$\nwhere sigmoid refers to the sigmoid activation function.\nReward. The reward function is a critical component of our framework, providing feedback regarding the quality of the agent's action, thereby assisting the agent in refining its action policy. The action of our agent is to select an appropriate instance temperature $T$ based on the $s_t$ of the instance $x$, which can facilitate knowledge acquisition by the student network, aiming to maximize the performance of the student network as much as possible. To achieve this objective, we integrate the settings of KD and consider the improvement of the student network's performance between two consecutive batches as the reward. Moreover, a common characteristic in deep learning is that the student network's performance shows significant improvement during the initial stages of training, this may bring disproportionately large reward values. However, these large values do not necessarily reflect the agent's astute action choices. To mitigate the impact of this phenomenon, we progressively increase the reward size during the early training stages. The formula for the reward is defined as:\n$reward = sigmoid(E/n) \\cdot reward.$\nHerein, $E$ represents the current epoch number, $n$ is a hyperparameter denotes the first $n$ epochs during which the reward incrementally grows."}, {"title": "4.2. Instance Reward Calibration", "content": "In our RLKD method, the action is to adjust the temperature for each training instance. To evaluate the quality of a particular action, we should calculate the corresponding reward for that action. However, we are unable to directly obtain the instance reward for each action. This is because our reward is based on the performance improvement of the student network. The student model is trained on batches of instance data and updates its parameters accordingly. The reward can only be computed after the student model updates its parameters. Typically, in KD, the batch size is set to 32, meaning that we have to go through 32 actions before we can receive a reward. This delayed reward characteristic (known as the credit assignment problem [13, 16]) makes it is difficult to assess and improve the policy network.\nTo address this issue, we design a reward corrector $C$ based on the refinement of the reward decomposition [3]. The reward corrector, which redistributes the reward $r_b$ for the current batch based on the state $s_b$ of the current batch and the action $a_b$ taken by the agent for each instance, to obtain the corrected reward $r'$ that corresponds to the action for each instance. The corrected reward $r_b'$ is calculated as:\n$r_b' = C(s_b, a_b, r_b)$,\nTo account for the contribution of each instance's action to the reward of the entire batch, we introduce an auxiliary task that allows the reward corrector to predict the sequence-wide return $G$ at each time step. The loss function for our reward corrector $C$ is defined as:\n$L_c = \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^{n} (r_i - r_b')^2 + \\beta \\cdot (\\frac{1}{n} \\sum_{i=1}^{n} (G_i' - G)^2).$\nHere, $r_i'$ represents the $n$-th corrected reward, and $n$ is the batch size. The variables alpha and beta are weights, which we set to 1 and 0.5, respectively. The variable $G_i$ denotes the return at the $i$-th time step. Additionally, to ensure that the states $s_i$ recorded in the replay buffer match the corrected rewards, we devise a state updater $U$ and update the states $s_b$ accordingly. The updated state $s_b'$ is calculated as follows:\n$s_b' = U(s_b)$\nThe loss function for our state updater $U$ is defined as:\n$L_u = (E(s_b') - G)^2.$\nHere, $E$ refers to an estimator to predict the corresponding return $G$ based on the input state."}, {"title": "4.3. Efficient Exploration", "content": "Due to the lack of ground truth for instance temperature, the update process in our RL component is conducted online. In this training setup, it is imperative for the agent within the RL framework to quickly learn effective temperature adjustment policy. To enable our agent to adjust the temperature for each instance with higher accuracy, we set the action space as a continuous space. This often implies that, in the initial stages of training, the agent may engage in inefficient exploration across a vast action space [2], which is not conducive to rapidly learning valuable instance temperature adjustment policy. To solve this problem, we propose an efficient exploration strategy. In which, during the early stages of training the RL component, we guide the agent to learn on high-quality training instances, which is helpful to drive the agent towards more effective exploration.\nFirstly, we need to define what constitutes high-quality training data in the context of KD. We consider that in KD, high-quality training samples mean those can provide more knowledge to the student network. The prior work [19] reveals the predictive entropy of a student network for a training instance can be used to measure the knowledge value of that instance. The higher the prediction entropy, the greater the knowledge value of the instance. The prediction entropy of a student network for a training instance is:\n$H(y | x) = - \\sum_{c=1}^{C} p(y = c | x) log p(y = c | x).$"}, {"title": "4.4. Training and Usage of RLKD", "content": "Given a dataset $D$, a teacher network $f_T$, and a student network $f_S$, our RLKD proceeds as follows. At beginning, we calculate the instance temperature $T$ for all training instances in the batch, organized by Sec. 4.1. We record the state $s_t$, action $a_t$, and value $V_t$ of this batch into the replay buffer, which serves as a reference for the agent's subsequent decision-making. Next, we calibrate the instance reward and update the state, as described in Sec. 4.2. Finally, as demonstrated in Sec. 4.3, we filter out high-quality training samples based on the performance of each training instance during this training stage and execute our efficient exploration strategy by utilizing these high-quality training instances. The procedure is depicted in Algorithm 1.\nHere, $n$ represents the total number of training examples, and $I_n$ represents the training instance ranked at ranking $n$. To mitigate the risk of overfitting, we take the top 10% to 20% training samples in the sequence $S_{(10~20)\\%}$ as our high-quality training samples for efficient exploration.\nSecondly, since the student network is typically a small model, to prevent overfitting through our high-quality learning and to enhance the robustness of the student model, we utilize mix-up [38] on our high-quality training samples with the training instances ranked from 40% to 50%, denoted as $S_{(40~50)\\%}$, in the sequence $S_e$. The sequence of training instance $S_t$ after mix-up is calculated as:\n$S_t = \\lambda \\cdot S_{(10~20)\\%} + (1 - \\lambda) \\cdot S_{(40~50)\\%}$\nThe parameter $\\lambda$ is set to ensure that the knowledge in $S_t$ predominantly comes from higher-ranked training instances (i.e. higher quality training instances)."}, {"title": "5. Experiments", "content": "For a fair comparison, we follow the experimental settings of CTKD [20] to conduct experiments to verify the effectiveness of our RLKD. Experiments are tested on a variety of well-known neural network architectures, such as VGG [29], ResNet (RN) [12], Wide ResNet (WRN) [37], ShuffleNet (SN) [40], and MobileNet (MN) [15]. We also evaluate RLKD as a plug-and-play technique across various distillation frameworks, including Vanilla KD [14], PKT [25], SP [32], VID [1], CRD [31], SRRL [36], and DKD [42]. Furthermore, we perform ablation studies to validate the effectiveness of our designed state representation, instance reward calibration, efficient exploration strategy, and selection of high-quality training examples.\nTasks and datasets. Following [20], we conduct experiments on two tasks: image classification and object detection. For the image classification task, we carry out experiments on CIFAR-100 [18] and ImageNet [8]. For the object detection task, we conduct our experiments on MS-COCO [21]. CIFAR-100 is a prominent dataset for image classification, comprising 32\u00d732 pixel images across 100 different categories, with a training set of 50,000 images and a validation set of 10,000 images. ImageNet, another significant dataset for large-scale image classification, encompasses 1,000 categories with a training set of approximately 1.28 million images and a validation set of 50,000 images. MS-COCO is a famous dataset used for general object detection that includes 80 categories. It has a training set (train 2017) with 118,000 images and a validation set (val 2017) with 5,000 images."}, {"title": "5.1. Main results", "content": "CIFAR-100: image classification. As shown in Tab. 1, we conduct image classification on the CIFAR-100 dataset to demonstrate the generalization performance of our RLKD method across 11 teacher-student pairs, including RN-56 & RN-20, etc. Among them, 5 pairs of teacher and student models (VGG-13 & MN-V2, etc.) are characterized by distinguishing architectural frameworks. These experimental designs we employed provide a diverse and comprehensive assessment environment. When the teacher and student networks share the same architecture, the experimental results show that our RLKD method has a strong generalization capacity, also exhibits a superior performance compared to CTKD. Specifically, in the case of RN-110 & RN-20, our method outperforms Vanilla KD by 0.78% (71.44% vs 70.66%) and CTKD by 0.36% (71.44% vs 71.08%). Moreover, in the case where the teacher and student networks have different architectures, the powerful generalization capacity of our RLKD is also validated.\nTo validate the generalization of our RLKD method across different KD frameworks, we conduct experiments on 6 currently leading KD frameworks (see Tab. 3), including DKD, PKT, etc. When applied to the teacher-student pair RN110 & RN32, our RLKD brings an improvement of 0.61% (74.27% vs 73.66%) in the DKD framework, which surpasses the accuracy of CTKD by 0.36% (74.27% vs 73.91%). Experiments conducted on other 5 KD frameworks (e.g. PKT, etc.) further confirm the strong generalization of our RLKD. Both the accuracy and stability of the proposed RLKD are significantly superior to CTKD, this can be attributed to our RL-based framework in instance temperature adjustment, which considers the future rewards of the instance temperature adjustment operations.\nImageNet: image classification. To validate the scalability of our method and its applicability in complex scenarios involving large datasets, we further conduct image classification on ImageNet. Tab. 2 details the top-1 and top-5 accuracy. Using CTKD and our RLKD as the adaptable plug-in approach, we incorporate them into 5 current leading distillation frameworks (i.e. KD, PKT, RKD, SRRL, and DKD). The experimental results obtained from these 5 KD frameworks unequivocally demonstrate the excellent scalability of our method. Remarkably, our RLKD exhibits robust performance on large dataset like ImageNet. For instance, in the Vanilla KD and SRRL frameworks, our method achieves improvement of 0.2% (90.51% vs 90.31%) and 0.11% (90.52% vs 90.41%) respectively. In contrast, CTKD obtains much fewer improvement on these KD frameworks, with gains of just 0.02% (90.33% vs 90.31%) and 0.01% (90.42% vs 90.41%) respectively, about 10 times lower. We think the superior performance of RLKD can be attributed to its RL-based framework in instance temperature adjustment, which considers the future benefits of these adjustments. Additionally, unlike CTKD, our RLKD also takes into account the student model's grasp of individual instances during instance temperature adjustment.\nMS-COCO: object detection. To verify whether our RLKD method possesses robustness across other visual tasks, we execute object detection on the MS-COCO dataset. As shown in Tab. 4, in the case of RN-50 & MN-V2, regarding the mAP metric, our RLKD outperforms Vanilla KD by 1.36% (31.49% vs 30.13%) and CTKD by 0.28% (31.49% vs 31.21%), respectively. Additionally, for detecting objects with varying sizes \u2013 evaluated by the AP metrics for large (API), medium (APm) and small (APs) objects, our RLKD also shows a significant enhancement, consistently surpasses CTKD across all size categories. Results demonstrate the robustness of our approach, where instance temperature adjustment is treated as a sequential decision-making task, enabling consideration of future benefits."}, {"title": "5.2. Ablation studies", "content": "In the ablation studies, we evaluate the performance of the uncertainty score that is included in our state representation, the instance reward calibration scheme, the efficient exploration strategy, and different high-quality training example selection strategies. All experiments are conducted on the CIFAR-100 dataset with respect to the image classification task, and utilize the Vanilla KD framework.\nUncertainty score. We conduct experiments on 4 sets of teacher-student network pairs to test the effectiveness of the uncertainty score in our state representation. As shown in Tab. 5, when incorporating uncertainty score into state representation, our method shows an improvement of 0.24% (71.40% vs 71.16%) in the RN-56 & RN-20 teacher-student pair. This enhancement verifies the effectiveness of our designed uncertainty score, which enables the agent to make wiser decisions by taking into account the student model's mastery of the training instances.\nInstance reward calibration. As shown in Tab. 6, when incorporating an instance reward calibration strategy into our RLKD method, a promotive effect across 4 different sets of the teacher-student pairs (RN-56 & RN-20, etc.) is achieved. E.g., our instance temperature calibration strategy boosts the performance of RN-110 & RN-32 pair by 0.55% (73.81% vs 73.26%). We believe the effectiveness of the instance reward calibration strategy lies in its ability to enable the agent to more accurately perceive the rewards resulting from each of its instance temperature adjustment actions, thereby enhancing its capacity to update its policy for performing the action.\nEfficient exploration. As shown in Tab. 7, we conduct ablation experiments on our efficient exploration strategy across 4 teacher-student pairs. The experimental results demonstrate that our effective exploration strategy facilitates performance of the student model across 4 teacher-student pairs. In the experiments involving the RN-56 & RN-20 teacher-student pair, our efficient exploration strategy results in a performance improvement of 0.37% (71.40% vs 71.03%). We attribute this success to the strategy enables the agent to learn valuable instance temperature adjustment policy faster, allowing the student model to acquire more useful knowledge during the early stages of KD.\nSelection of high-quality training examples. As shown in Tab. 8, we conduct experiments on CIFAR-100 to compare different strategies for selecting the high-quality training examples. Interestingly, we observe that when using the top 10% of high-quality training data, the performance of the student model in the teacher-student pair RN-56 & RN-20 is 70.92%, which is not as good as the performance 71.21% of the student model when using the training data ranked from 10% to 20%. This phenomenon is also observed in the teacher-student pair WRN-40-2 & WRN-16-2. We think this may due to utilizing the top 10% samples caused overfitting in the agent. Furthermore, in the teacher-student pair RN-56 & RN-20, when conducting the mix-up method on the training data ranked from 10% to 20% using the training data ranked 40% to 50%, there is a performance increase of 0.19% (71.40% vs 71.21%). The experimental results verify the validity of our mix-up method that combines instances of varying knowledge values can produce high-quality training data."}, {"title": "6. Conclusion", "content": "In current knowledge distillation domain, the methods [20, 23] applied to temperature adjustment neglect the consideration of future benefits associated with the adjustment. To address this issue, we approach the instance temperature adjustment as a sequential decision-making task and propose a novel method RLKD. Specifically, we design a comprehensive state representation to enable the agent in our framework to make informed adjustment to the instance temperature. Besides, we explore an instance reward calibration scheme to provide the agent with more accurate reward signals. In addition, we develop an efficient exploration strategy to boost the agent's capability to learn valuable temperature adjustment policy fastly. Extensive experiments are conducted on three famous datasets for the tasks of image classification and object detection, demonstrating the effectiveness of our plug-and-play instance temperature adjustment method RLKD."}]}