{"title": "Influence-based Attributions can be Manipulated", "authors": ["Chhavi Yadav", "Ruihan Wu", "Kamalika Chaudhuri"], "abstract": "Influence Functions are a standard tool for attributing predictions to training data in a principled manner and are widely used in applications such as data valuation and fairness. In this work, we present realistic incentives to manipulate influence-based attributions and investigate whether these attributions can be systematically tampered by an adversary. We show that this is indeed possible and provide efficient attacks with backward-friendly implementations. Our work raises questions on the reliability of influence-based attributions under adversarial circumstances.", "sections": [{"title": "Introduction", "content": "Influence Functions are a popular tool for data attribution and have been widely used in many applications such as data valuation [35, 18, 39, 20], data filtering/subsampling/cleaning [43, 42, 30, 40, 29], fairness [28, 41, 36, 24, 31, 9, 8, 44, 14] and so on. Many of these applications involve scenarios where participants have an incentive to manipulate influence scores; for example, in data valuation a higher monetary sum is given to samples with a higher influence score and since good data is hard to collect, there is an incentive to superficially raise influence scores for existing data. Thus, an understanding of whether and how influence functions can be manipulated is essential to determine their proper usage and for putting guardrails in place. While a lot of work in the literature has studied manipulation of feature-based attributions [17, 2], whether data attribution methods, specifically influence functions, can be manipulated has not been explored. To this end, our paper investigates the question and shows that it is indeed possible to systematically manipulate influence-based attributions according to the manipulator's incentives.\nWe formalize the influence function pipeline in terms of three entities \u2013 Data Provider who provides training data, Model Trainer who trains a model on this data and Influence Calculator who finds the influence of each training sample on model predictions. Out of these, model trainer is considered to be the adversary who wishes to change the influence scores for training samples and does so covertly through a malicious model which is indistinguishable from the original model in terms of test accuracy but leads to desired influence scores. This setting captures two important downstream applications where incentives are meaningful: data valuation, where the adversary has an incentive to raise influence scores for monetary gain and fairness, where the adversary wants to manipulate influence scores for reducing the fairness of a downstream model.\nWe next define and provide algorithms to carry out two kinds of attacks in this setup: Targeted and Untargeted. Targeted attacks are for the data valuation application and specifically manipulate influence scores for certain target samples. The primary challenge with targeted attacks is that calculating gradients of influence-based loss objectives is highly computationally infeasible. We address this challenge by proposing a memory-time efficient and backward-friendly algorithm to compute the gradients while using existing PyTorch machinery for implementation. This contribution is of independent technical interest, as the literature has only focused on making forward computation of influence functions feasible, while we study techniques to make the backward pass viable. Our algorithm brings down the time required for one forward + backward pass on a GPU to ~2 minutes."}, {"title": "Preliminaries", "content": "Consider a classification task with an input space X = $R^d$ and labels in set Y. Let the training set of size n be denoted by $Z_{train}$ = {$z_i$}$_{i=1}^n$ where each sample $z_i$ is an input-label pair, $z_i$ = $(x_i, y_i) \\in X \\times Y$. Let the loss function at a particular sample z and parameters $\\theta \\in \\Theta$ be denoted by L(z, $\\theta$). Using the loss function and the training set, a model parameterized by $\\theta \\in \\Theta$ is learnt through empirical risk minimization, resulting in the optimal parameters $\\theta^*$ := arg min$\\limits_{\\theta \\in \\Theta}$ $\\sum_{i=1}^{n} L(z_i, \\theta)$. The gradient of the loss w.r.t. parameters $\\theta$ for the minimizer at a sample z is given by $\\nabla_{\\theta}L(z, \\theta^*)$. Hessian of the loss for the minimizer is denoted by $H_{\\theta^*}$ := $\\sum_{i=1}^{n} \\nabla^2_{\\theta}L(z_i, \\theta^*)$. For brevity, we call the model parameterized by $\\theta$ as model $\\theta$ if needed. Next we give the definition of Influence Functions used in our paper."}, {"title": "Definition 1 (Influence Function [23])", "content": "Assuming that the empirical risk is twice-differentiable and strictly convex in $\\theta$, the influence of a training point z on the loss at a test point $z_{test}$ is given by,\n$I_{\\theta^*}(z, z_{test}) := -\\nabla_{\\theta}L(z_{test}, \\theta^*) H_{\\theta^*}^{-1} \\nabla_{\\theta}L(z, \\theta^*)$                                                                 (1)\nwhere $\\nabla_{\\theta}L(z_{test}, \\theta^*)$ and $\\nabla_{\\theta}L(z, \\theta^*)$ denote the loss gradients at $z_{test}$ and z respectively, while $H_{\\theta^*}^{-1}$ denotes the hessian inverse.\nGiven a test set of size m, $Z_{test}$ = {$z_{test\\_i}$}$_{i=1}^{m}$, we define the overall influence of a training point z on the loss of the test set to be the sum of its influence on all test points $z_{test\\_i}$ individually, written as\n$I_{\\theta^*}(z, Z_{test}) := \\sum_{i=1}^{m} I_{\\theta^*}(z_{test\\_i}, z)$                                                                                             (2)"}, {"title": "General Threat Model", "content": "In this section, we give a general description of our setup and threat model. We later instantiate these with two downstream applications where influence functions have been used in the literature but the incentives and objectives differ: data valuation and fairness. Note that while we ground our discussion on two applications, the attacks or their slight variations can apply to other applications.\nSetup. The standard influence function pipeline consists of three entities : a Data Provider, a Model Trainer and an Influence Calculator. Data Provider holds all the training data and supplies it to the other two entities. Model Trainer trains a model on the supplied training data and checks the performance of the trained model on a test set. We assume that the training and test set come from the same underlying distribution. Model Trainer supplies the trained model and test set to the Influence Calculator. Influence Calculator computes the influence of each sample in the training set on the predictions made by the model for the aforementioned test set (with Eq.2). It ranks the training"}, {"title": "Downstream Application 1: Data Valuation", "content": "Data valuation determines the contribution of each training sample to model training and assigns a proportional monetary sum to each one. One of the techniques to find this contribution is through influence functions, by ranking training samples according to their influence scores in a decreasing order [35, 18, 39, 20]. A higher influence ranking implies a more valuable sample, resulting in a higher monetary sum. Therefore, a malicious entity with financial incentives can manipulate influence scores in order to increase its financial gains from pre-existing data, since collecting real-world data is an expensive task and it may not be even possible to manipulate real-world data, especially in biological domains. See Fig.2 for a pictorial representation of the data valuation setting.\nThreat Model. The canonical setting of data valuation consists of three basic entities : multiple data vendors, model trainer and influence calculator. Each vendor supplies a set of data; the collection of data from all vendors corresponds to the fixed training set of the data provider. The influence"}, {"title": "Single-Target Attack", "content": "Let us first consider the case where $Z_{target}$ has only one element, $Z_{target}$ = {$z_{target}$}. We formulate the adversary's attack as a constrained optimization problem where the objective function, $l_{attack}$, captures the intent to raise the influence ranking of the target sample to top-k while the constraint function, dist, limits the distance between the original and manipulated model, so that the two models have similar test accuracies. The resulting optimization problem is given as follows, where C\u2208 R is the model manipulation radius,\n$\\min\\limits_{\\theta':dist(\\theta,\\theta') <C} l_{attack} (z_{target}, (I_{\\theta'} (z, Z_{test}) : z \\in Z))$                                                                    (3)"}, {"title": "Multi-Target Attack", "content": "When the target set consists of multiple target samples, $Z_{target}$ = {$z_{target_1}, z_{target_2} ... z_{target_q}$}, the adversary's attack can be formulated as repeated applications of the Single-Target Attack, formally given as,\n$\\min\\limits_{\\theta':dist(\\theta,\\theta') <C} \\sum\\limits_{z_{target_i} \\in Z_{target}} l_{attack} (z_{target_i}, (I_{\\theta'} (z, Z_{test}) : z \\in Z))$                                                             (4)"}, {"title": "Efficient Backward Pass for Influence-based Objectives", "content": "A natural algorithm to solve complicated optimization problems as our attacks in Eq. 3 & 4 is Gradient Descent, which involves a forward and backward pass. However, for influence-based attack objectives, naive gradient descent is not feasible for either of the steps, mainly due to Hessian-Inverse-Vector Products (HIVPs) in the influence function definition which lead to a polynomial scaling of memory and time requirements w.r.t model parameters. Backward pass on our attack objectives is even harder as it involves gradients of influence-based loss objectives, making the attacks too expensive even for linear models trained on top of ResNet50 features used in our experiments where #model parameters range from ~76k to 206k.\nWhile literature has studied ways to make the forward computation of influence functions efficient [37, 15, 23, 26], not much work has been done on making the backward pass efficient. To this end, we propose an innovative technique \u2013 rewriting the original objective into a backward-friendly form which renders the gradient computations efficient for influence-based objectives. This allows us to still use gradient descent and other existing machinery in PyTorch [33]. The forward pass algorithm we employ is same as that of [23] which uses implicit hessian vector products, outlined in Appendix Alg.2. Coming to the backward pass, our idea of rewriting the attack objective involves two essential steps: (1) linearizing the objective (2) making the linearized objective backward-friendly in PyTorch."}, {"title": "Linearize the attack objective", "content": ": Generally the attack objective can be a non-linear combination of influence functions over different training samples, which makes the backward pass inefficient. Therefore we first transform the given objective into a linear combination of influence functions, $l_{attack}((I_\\theta(z, Z_{test}) : z \\in Z)) := u^T (I_\\theta(z, Z_{test}) : z \\in Z)$ for some vector $u \\in R^n$, such that, the objective and gradient values are the same, $l_{attack}(\\cdot) = \\overline{l_{attack}}(\\cdot)$ and $\\nabla_\\theta l_{attack}(\\cdot) = \\nabla_\\theta \\overline{l_{attack}}(\\cdot)$.\nObserve that setting $u := (\\frac{\\partial l_{attack}}{\\partial I_\\theta(z, Z_{test})}: z \\in Z)$ satisfies all our constraints; influence scores in this vector are computed using an efficient forward pass algorithm, given in Alg. 2."}, {"title": "Get a PyTorch backward-friendly attack objective", "content": ": Simply expanding our linearized attack ob-jective gives, $\\overline{l_{attack}}() = v_{\\theta,1}^T u_1 + u_1^T v_{\\theta,2}$ where $v_{\\theta,1} = (-\\nabla_\\theta \\sum_{i=1}^n L(z_{test_i},\\theta))$ and $v_{\\theta,2} = (\\nabla_\\theta \\sum_{z\\in Z} u_z \\cdot L(z,\\theta))$. Gradient computations for this objective will have to go through HIVPs, which is highly inefficient. Therefore we next convert the linearized objective into one which does not involve HIVPs, again such that the objective and gradient values are same as the original objective.\nUsing chain rule, the gradient of the expanded linearized attack objective can be written as $\\nabla_\\theta \\overline{l_{attack}}() = (\\nabla_\\theta v_{\\theta,1})^T u_1 + u_1^T (\\nabla_\\theta v_{\\theta,2}) - u_1^T (\\nabla_\\theta H_\\theta) u_2$ where $u_1 = H_\\theta^{-1} v_{\\theta,1}$ and $u_2 = H_\\theta^{-1} v_{\\theta,2}$. PyTorch supports the gradient computation for functions of gradient, making $\\nabla_\\theta v_{\\theta,1}$ and $\\nabla_\\theta v_{\\theta,2}$ backward-friendly. PyTorch also calculates gradients for functions of hessian vector products implicitly, which leads to efficiency. Additionally, we can precompute $u_1$, $u_2$ and freeze them.\nAs a result, our final backward-friendly objective function is efficient and backward-friendly with PyTorch and is given as, $\\overline{l_{attack}}(\\theta) = v_{\\theta,1}^T u_1 + u_1^T v_{\\theta,2} - u_1^T H_\\theta^{-1} u_2$. The algorithm for computing our backward-friendly objective $\\overline{l_{attack}}$ is elucidated in Alg. 3.\nWhen using the original influence-based objective naively, it was not possible to even do one backward pass due to memory constraints. With our innovations, we can now do a forward + backward pass in only ~2 minutes in a memory efficient way."}, {"title": "Downstream Application 2: Fairness", "content": "Recently, a lot of studies have used influence functions in different ways to achieve fair models[28, 41, 36, 24, 31, 9, 8, 44, 14]. In our paper, we focus on the study by [28] as they use the same definition of influence functions as us. The suggested approach in [28] to achieve a fair model is by reweighing training data based on influence scores for a base model and then using this reweighed data to train a new downstream model from scratch. This downstream model is expected to have high fairness as a result of the reweighing. For a pictorial representation of this process, see Fig. 3. Weights for training data are found by solving an optimization problem which places influence functions in the constraints. See Appendix Sec.A.2 for more details on the optimization problem."}, {"title": "Attack", "content": "Since the goal of the adversary in this case is not tied to specific target samples, we propose an untargeted attack for the adversary. Our attack is deceptively simple \u2013 scale the base model $\\theta^*$ by a constant $x > 0$. The malicious base model output by the model trainer is now $\\theta' = \\lambda \\cdot \\theta^*$, instead of $\\theta^*$. Note that for logistic regression the malicious and original base model are indistinguishable since scaling with a positive constant maintains the sign of the predictions, leading to the same accuracy. We show experimentally in Sec. 6.2 that our scaling attack indeed reduces the fairness of the downstream model."}, {"title": "Experiments", "content": "In this section, we investigate if the attacks we proposed for data valuation in Sec. 4 can succeed empirically. Specifically, we ask the following questions : (1) do our influence-based attacks perform better than a non-influence baseline?, (2) what is the behavior of our attacks w.r.t. different parameters such as C and target set size? (3) what components contribute to the success of our attacks? and (4) lastly, can our attacks transfer to an unknown test set?. In what follows, we first explain our experimental setup and then discuss the results."}, {"title": "Datasets & Models", "content": "We use three standard image datasets for experimentation : CIFAR10 [25], Oxford-IIIT Pet [32] and Caltech-101 [27]. We split the respective test sets into two halves while maintaining the original class ratios for each. The first half is the test set shared between the model trainer and influence calculator used to optimize influence scores while the second is used as a pristine set for calculating the accuracy of models and also for transfer experiments discussed later. We obtain feature vectors of size 2048 for all images by passing them through a pretrained ResNet50 model [16] from PyTorch and train linear models ($\\theta^*$) on top of these features with the cross-entropy loss."}, {"title": "Attack Setup & Evaluation", "content": "For the Single-Target Attack, we randomly pick a training sample (which is not already in the top-k influence rankings) as the target and carry out our attack on it. We repeat this process for 50 samples and report the fraction out of 50 which could be (individually) moved to top-k in influence rankings as the success rate. To carry out the Multi-Target Attack, we randomly pick target sets of different sizes from the training set. The success rate now is the fraction of samples in the target set which could be moved top-k in influence rankings. For many of our results, the success rates are reported under two regimes : (1) the high-accuracy similarity regime where the manipulated and original models are within 3% accuracy difference and (2) the best success rate irrespective of accuracy difference. We optimize every attack from 5 different initializations and pick the runs which eventually lead to the highest success rates."}, {"title": "Baseline: Loss Reweighing Attack", "content": "While our attacks are based on influence functions, we propose a non-influence-function baseline attack for increasing the importance of a training sample : reweigh the training loss, with a high weight on the loss for the target sample. We call this baseline the Loss Reweighing Attack, formally defined as, min$\\limits_{\\theta'}\\sum_{z \\in Z_{train} \\backslash \\{z_{target} \\}} L(z; \\theta') + \\lambda \\cdot L(z_{target}; \\theta')$, where L is the model training loss. Intuitively, a larger $\\lambda$ increases the influence of $z_{target}$ on the final model, but results in a lower model accuracy and vice-versa. We implement the baseline with weighted sampling according to $\\lambda$ rather than uniform sampling in each batch."}, {"title": "Can our Single-Target Attack perform better than the Baseline?", "content": "As demonstrated in Table 1, our influence-based attacks indeed performs better than the baseline \u2013 while the baseline has a low success rate across the board, our attack achieves a success rate of 64-88% in the high accuracy regime and 85-94% without accuracy constraints. The baseline is able to achieve a high success rate when k is large, but only with a massive accuracy drop."}, {"title": "Behavior of our Single-Target Attack w.r.t parameters C & training set size", "content": "Theoretically, the parameter C in our attack objectives (Eq. 3 & 4) is expected to create a trade-off between the manipulated model's accuracy and the attack success rate. Increasing C should result in a higher success rate as the manipulated model is allowed to diverge more from the (optimal) original model but on the other hand its accuracy will drop and vice-versa. We observe this trade-off for all three datasets and different values of k in Fig.4 (solid lines).\nWe also anticipate our attack to work better with smaller training sets, as there will be fewer samples competing for top-k rankings. Experimentally, this is found to be true \u2013 Pet dataset with the smallest training set has the highest success rates, as shown in Fig.4 & Table 1."}, {"title": "Ablation Study: What components contribute to the success of our attack?", "content": "Since our attack is a combination of several ideas, we conduct an ablation study to understand the effect of each idea on the success rate, as reported in Table 2. The different ideas are as follows.\n\u2022 Maximize the target data's influence: Given a target sample, the simplest idea to move it to top-k influence rankings is to maximize its influence score, $\\max_{\\theta':dist(\\theta^*,\\theta')<C}I_{\\theta'} (z_{target}, Z_{test})$. This attack doesn't achieve high success rates, even without accuracy constraints which could be due to an inherent drawback : this objective doesn't consider other training samples' influence scores.\n\u2022 + Minimize the influence of samples that are ranked top-k: The goal of this objective is to increase the influence score of the target sample while lowering it for the samples currently ranked top-k, given as $\\max_{\\theta':dist(\\theta^*,\\theta')<C} I_{\\theta'} (z_{target}, Z_{test}) - K \\sum_{z: rank \\ of \\ z < k} I_{\\theta'} (z, Z_{test})$. We observe empirically that the optimization procedure of this objective gets stuck in local minima easily."}, {"title": "+ (Our objective) Minimize the influence score of all samples whose influence is larger than that of the target sample", "content": ": This is the final objective used by us and considers all training samples which have a higher influence score than that of the target sample instead of just the top-k, min$\\limits_{\\theta':dist(\\theta^*,\\theta')<C}  \\sum_{z \\in S_0} I_{\\theta'} (z, Z_{test}) - I_{\\theta'} (z_{target}, Z_{test})$ where $S_0 \\subset Z_{train}$ has all z s.t. $I_{\\theta'} (z, Z_{test}) > I_{\\theta'} (z_{target}, Z_{test})$. Empirically, we find that this objective function decreases the chance of being stuck at suboptimal solutions and the loss keeps reducing throughout the optimization trajectory resulting in higher success rates."}, {"title": "+ (Our final attack) Multiple random initializations", "content": ": Because the above objective function is non-convex, we find that using multiple random initializations helps to obtain a better solution, especially with a larger value of parameter C, when the search space is bigger. As a result, we observe significant improvement in terms of 'best' success rates (where C can be very large)."}, {"title": "How do our attacks transfer when influence scores are computed with an unknown test set?", "content": "When an unknown test set is used to compute influence scores, our attacks perform better as k increases, as shown in Fig.4. This occurs because the target sample's rank, optimized with the original test set, deteriorates with the unknown test set and a larger k increases the likelihood of the target still being in the top-k rankings."}, {"title": "Imposibility Theorem for Data Valuation Attacks", "content": "We observe in Fig.4 that even with a large C, our attacks still cannot achieve a 100% success rate. Motivated by this, we wonder if there exist target samples for which the influence score cannot be moved to top-k rank? The answer is yes and we formally state this impossibility result as follows, with the proof in Appendix Sec. A.1.3."}, {"title": "Theorem 1", "content": "For a logistic regression family of models and any target influence ranking k \u2208 N, there exists a training set Ztrain, test set Ztest and target sample ztarget \u2208 Ztrain, such that no model in the family can have the target sample ztarget in top-k influence rankings."}, {"title": "Easy vs. Hard Samples", "content": "Upon investigation, we find that target samples which rank very high or low in the original influence rankings are easier to push to top-k rankings upon manipulation (or equivalently which have a high magnitude of influence either positive or negative). This is so because the influence scores of extreme rank samples are more sensitive to model parameters as shown experimentally in Fig.5, thus making them more susceptible to influence-based attacks.\nNext we discuss results for the Multi-Target Attack scenario, where the target is not a single training sample, but rather a collection of multiple training samples. We investigate the following question."}, {"title": "How does our Multi-Target Attack perform with changing target set size and desired ranking k?", "content": "Intuitively, our attack should perform better when the size of the target set is larger compared to k - this is simply because a larger target set offers more candidates to take the top-k rankings spots, thus increasing the chances of some of them making it to top-k. Our experimental results confirm this intuition; as demonstrated in Fig.6, we observe that (1) for a fixed value of k, a larger target set size leads to a higher success rate; target set size of 100 has the highest success rates for all values of k across the board, and (2) the success rate decreases with increasing value of k for all target set sizes and datasets. These results are for the high-accuracy similarity regime where the original and manipulated model accuracy differ by less than 3%."}, {"title": "Fairness Manipulation", "content": "Coming to the fairness use-case, we now investigate experimentally if our scaling attack can be successful in reducing the fairness of the final downstream model. We use three standard fairness"}, {"title": "Related Work", "content": "Fragility of Influence Functions. Influence functions proposed in [23] are an approximation to the effect of upweighting a training sample on the loss at a test point. This approximation error can be large as shown by [5, 4, 12], making influence functions fragile especially for deep learning models. Our work is orthogonal to this line of work as we study the robustness of influence functions w.r.t. model parameters instead of approximation error of influence functions w.r.t. the true influence.\nModel Manipulation in the Threat Model. Manipulating models to execute attacks is a prevalent theme in the literature. [17] and [2] corrupt feature attributions with successful model manipulations in vision models. [34] corrupt attention-based explanations for language models while maintaining model accuracy. [38] show that it is possible to corrupt a fairness metric by manipulating an interpretable surrogate of a black-box model while maintaining empirical performance of the surrogate. Similar to these works, our threat model also allows the adversary to manipulate models while maintaining the test accuracy, as a constraint. However, our adversarial goal is different we corrupt influence function outputs.\nData Manipulation Attack on Explanations. A line of work [13, 1, 45, 10, 21] has studied how data can be manipulated to corrupt feature attributions. On the contrary, firstly, we keep data fixed and manipulate the model and secondly, we work with data attributions rather than feature attributions."}, {"title": "Conclusion & Future Work", "content": "While past work has mostly focused on feature attributions, in this paper we exhibit realistic incentives to manipulate data attributions. Motivated by the incentives, we propose attacks to manipulate outputs from a popular data attribution tool \u2013 Influence Functions. We demonstrate the success of our attacks experimentally on standard datasets. Our work lays bare the vulnerablility of influence-based attributions to manipulation and serves as a cautionary tale when using them in adversarial circumstances. Some future directions include exploring different threat models, additional use-cases, manipulating other kinds of data attribution tools and scaling influence manipulation to larger models."}, {"title": "A.1 Data Manipulation Attack Details", "content": "Dataset Details. CIFAR10 [25] has a training/test set size of 50000/10000 with 10 output classes,\nOxford-IIIT Pet [32] has a training/test set size of 3680/3669 with 37 output classes and Caltech-\n101 [27] has a training/test set size of 6941/1736 with 101 output classes.\nAttack Details. To optimize our attack objective, we use algorithm Alg. 3 for computing gradients with Adam as the optimizer [22]. We use two learning rates {0.01,0.1} and 100 steps of updates.\nWe optimize every attack from 5 different initializations. We run our attacks with multiple values of the constraint threshold C = {0.05, 0.1, 0.2, 0.5}. For each regime, the reported number is the highest we could obtain with different values of constants C or \u03bb."}, {"title": "How do we find the vector u?", "content": "Observe that from chain rule $\\nabla_\\theta l_{attack}((I_\\theta(z, Z_{test}) : z \\in Z)) = \\sum_{z \\in Z} \\frac{\\partial l_{attack}}{\\partial I_\\theta(z, Z_{test})} \\nabla_\\theta I_\\theta(z, Z_{test})$. Therefore, we can set u as $(\\frac{\\partial l_{attack}}{\\partial I_\\theta(z, Z_{test})}:z\\in Z)$ while meeting the equal objective and gradient value requirement."}, {"title": "Expanding the linearized objective:", "content": "$\\overline{l_{attack}}((I_\\theta(z, Z_{test}) : z \\in Z)) = u^T (I_\\theta(z, Z_{test}) : z \\in Z)$\n$= \\sum_{z \\in Z} u_z \\cdot I_\\theta (z, Z_{test})$\n$= \\sum_{z \\in Z} u_z \\cdot \\sum_{i=1}^{m} I_\\theta (z_{test_i}, z)$\n$= \\sum_{z \\in Z} u_z \\cdot \\sum_{i=1}^{m} -\\nabla_\\theta L (z_{test_i}, \\theta) H_\\theta^{-1} \\nabla_\\theta L(z, \\theta)$\n$= -(\\nabla_\\theta \\sum_{i=1}^{m} L (z_{test_i}, \\theta))^T H_\\theta^{-1} (\\nabla_\\theta \\sum_{z \\in Z} u_z \\cdot L(z, \\theta))$\n$= -v_{\\theta,1}^T H_\\theta^{-1} v_{\\theta,2}$"}, {"title": "Chain Rule for gradient of expanded linearized objective", "content": "$\\nabla_\\theta \\overline{l_{attack}}((I_\\theta(z, Z_{test}) : z \\in Z)) = \\nabla_\\theta v_{\\theta,1}^T H_\\theta^{-1} v_{\\theta,2}$\n$= (\\nabla_\\theta v_{\\theta,1})^T \\cdot H_\\theta^{-1} v_{\\theta,2} + v_{\\theta,1}^T \\nabla_\\theta (H_\\theta^{-1} v_{\\theta,2}) - v_{\\theta,1}^T H_\\theta^{-1} (\\nabla_\\theta H_\\theta) H_\\theta^{-1} v_{\\theta,2}$\n$= (\\nabla_\\theta v_{\\theta,1})^T H_\\theta^{-1} v_{\\theta,2} + v_{\\theta,1}^T H_\\theta^{-1} (\\nabla_\\theta v_{\\theta,2}) - v_{\\theta,1}^T H_\\theta^{-1} (\\nabla_\\theta H_\\theta) H_\\theta^{-1} v_{\\theta,2}$\n$= (\\nabla v_{\\theta,1})^T u_2 + u_1^T (\\nabla v_{\\theta,2}) - u_1^T (\\nabla H_\\theta) u_2$"}, {"title": "A.2 Fairness Manipulation Attack Details", "content": "Optimization Problem for Reweighing Training Data as proposed by (author?) [28] is given as follows,"}, {"title": "minimize", "content": "$\\min\\limits_{w} \\sum_i w_i$\nsubject to\n$\\sum_i w_i I_{fair}(e_i) = -I_{fair}$\n$\\sum_i w_i Z_{util}(e_i) \\leq 0$\n$w_i \\in [0, 1]$                                                                                                          (5)\nwhere wi refers to the weight of the ith training sample, $e_i$ refers to the ith training sample, $Z_{util}$\nrefers to our influence function, $I_{fair}$ refers to some fairness influence function and $I_{fair}$ corresponds\nto a differentiable fairness metric. In our threat model, the adversary manipulates the base model,\nwhich changes the influence scores Tutil."}, {"title": "An advanced version of the above optimization problem using additional parameters (\u03b2, \u03b3) which lead to various tradeoffs is given as", "content": "minimize\nsubject to\n$\\sum_i w_i$\n$\\sum_i w_i I_{fair}(e_i) < -(1-\\beta) I_{fair}$,\n$\\sum_i w_i Z_{util}(e_i) < \\gamma (\\min_i \\sum_i v_i Z_{util} (e_i))$,\n$w_i \\in [0, 1]$.                                                                                                                     (6)\nFairness Metric. We define the fairness metric used in our paper, Demographic Parity."}, {"title": "Definition 2 (Demographic Parity Gap (DP) [11]", "content": "). Given a data distribution D over X \u00d7 {0,1} from which features $x^{\\backslash a}$ and sensitive attribute $x^a \\in$ {0,1} are jointly drawn from, Demographic Parity gap for a model $f_\\theta$ is defined to be the difference in the rate of positive predictions between the two groups, |Pr(\u0177 | $x^a$ = 0) \u2013 Pr(\u0177 | $x^a$ = 1)| where \u0177 is the prediction $f_\\theta(x^{\\backslash a}, x^a)$."}]}