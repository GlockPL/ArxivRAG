{"title": "INSPIRED BY AI?\nA NOVEL GENERATIVE AI SYSTEM TO ASSIST CONCEPTUAL AUTOMOTIVE DESIGN", "authors": ["Ye Wang", "Nicole B. Damen", "Thomas Gale", "Voho Seo", "Hooman Shayani"], "abstract": "Design inspiration is crucial for establishing the direction\nof a design as well as evoking feelings and conveying meanings\nduring the conceptual design process. Many practice design-\ners use text-based searches on platforms like Pinterest to gather\nimage ideas, followed by sketching on paper or using digital\ntools to develop concepts. Emerging generative AI techniques,\nsuch as diffusion models, offer a promising avenue to stream-\nline these processes by swiftly generating design concepts based\non text and image inspiration inputs, subsequently using the AI\ngenerated design concepts as fresh sources of inspiration for fur-\nther concept development. However, applying these generative\nAl techniques directly within a design context has challenges.\nFirstly, generative AI tools may exhibit a bias towards particular\nstyles, resulting in a lack of diversity of design outputs. Secondly,\nthese tools may struggle to grasp the nuanced meanings of texts\nor images in a design context. Lastly, the lack of integration\nwith established design processes within design teams can result\nin fragmented use scenarios. Focusing on these challenges, we\nconducted workshops, surveys, and data augmentation involving\nteams of experienced automotive designers to investigate their\ncurrent practices in generating concepts inspired by texts and im-\nages, as well as their preferred interaction modes for generative\nAl systems to support the concept generation workflow. Finally,\nwe developed a novel generative Al system based on diffusion\nmodels to assist conceptual automotive design.", "sections": [{"title": "1. INTRODUCTION", "content": "Conceptual design is a fluid and creative phase where de-\nsigners collaborate to generate early designs that set the direction\nfor development. Within this phase, designers work both col-\nlaboratively and independently. They are given the freedom to\nuse a diverse set of tools, with some leaning towards traditional\nsketches while others prefer digital tools [1]. Sketching plays a\npivotal role in automotive concept development and is considered\na way of visual thinking and communicating [2]. To foster col-\nlaboration, teams are often given the same context and concept\nkeywords to guide their exploration. The role of the designers is\nto create novel designs that embody a strong concept, showcase\nuniqueness, and remain feasible and functional [3].\nBoth textual and image-based stimuli serve as frequent\nsources of inspiration in the concept design process. Textual stim-\nuli excel at establishing a clear direction; for instance, a car design\nteam might define \u201cbold\" and \"dynamic\" as key inspirations for\nthe upcoming season's cars. In the automotive context, these key-\nwords are not merely descriptive of a feeling, but have also come\nto refer to specific shapes as the field has evolved over the years\n[4]. These keywords can then be communicated across differ-\nent design teams\u2014exterior, interior, and component\u2014allowing\nthem to work relatively autonomously while still achieving cohe-\nsive designs.\nOn the other hand, images excel at evoking feelings and con-\nveying meaning, with a closer connection to the physicality of\ndesign compared to text. From an image, designers can extract\nvarious visual elements such as colors, shapes, and textures, di-\nrectly influencing their designs. Such elements can be used to\nevaluate the sales performance of previous models, and guide fu-\nture developments [5]. Moreover, designers can interpret textual\nconcepts using imagery that goes beyond its car-related meaning;\nfor instance, interpreting \"bold\" as reminiscent of the solidity of\na concrete building or the stability of a vast river in nature.\nDesigners encounter various challenges when seeking inspi-\nration and developing them into concepts. Firstly, the accessi-\nbility of online inspiration means that widely visible results are\nreadily available to everyone. However, it can be time-consuming\nand idiosyncratic to manually curate or generate unique content\n[6], and then translate them into design patterns that match the"}, {"title": "2. BACKGROUND", "content": "2.1 Design Inspiration\nInspiration plays a crucial role in conceptual design. It serves\nas both an analogy and a guide, facilitating exploration across a\nvast design space [15\u201317]. Additionally, inspiration helps design-\ners in overcoming fixation and generating novel ideas [18-20],\ndefining design directions [21], and communicating design con-\ncepts and contexts [22]. Inspiration manifests in various forms,\nincluding drawings, texts, images, 3D designs, and physical prod-\nucts such as teardowns [1, 23\u201325], and can be acquired intention-\nally through search or naturally through experience [6, 26]. These\nforms of inspiration can be encountered physically, digitally, and\nvirtually [6, 27, 28], facilitating the creation of new concepts.\nThis work concentrates on text and image inspiration for\nseveral reasons. Firstly, these forms are easily and affordably\naccessible in large quantities and varieties via the internet, and\nare commonly utilized by designers. Secondly, text and images\nare typically integrated into design processes, serving as tools for\nestablishing design directions and communicating ideas. Lastly,\nmany advanced generative Al models focus on text and image\ninputs and outputs, benefiting from the abundance of high-quality\ncontent available."}, {"title": "2.2 Generative Al for Image Generation", "content": "Variational Auto-Encoders (VAEs) [30] and Generative Ad-\nversarial Networks (GANs) [31] have been two classical deep\ngenerative models that were used for generating images with rela-\ntive success. Recently, Denoising Diffusion Probabilistic Models\n(DDPMs) [9] have shown even greater success in generating high-\nquality and diverse images with better training stability. These\nmodels can learn to predict the reverse of a diffusion process\n(adding Gaussian noise) applied to the data, gradually denoising\nan image from noise to clean data points similar to the images\nin the training dataset [32]. However, the need to iterate through\nmany denoising steps makes the original DDPMs slower than\nGANs and VAEs.\nTo tackle this problem, Latent Diffusion Models (LDMs)\nsuch as Stable Diffusion [33] first train a high-quality Auto-\nEncoder that learns a latent representation for the images and\nthen perform the denoising diffusion process on that smaller la-\ntent tensor. This reduces the size and computational cost of these\nmodels substantially. These models are usually conditioned on a\ntextual prompt that describes the image semantically.\nTo provide users with fine-grained control over the general\ncomposition, shape, and position of the generated images, new\nmodels such as ControlNet [34] provide additional conditional\ninputs that influence the generation. Models that can map text\nand images into a common latent space, such as Kandinsky [35],\nallow these modalities to be mixed and collectively used to control\nthe generated images. Versatile Diffusion [36] supports many\ndifferent use-cases, including using multiple images and textual\nprompts as conditional inputs using cross-attention mechanisms.\nParticularly, Versatile Diffusion uses a global signal over the input\nimages that can be used in the cross-attention, which causes the\ninput image to influence the semantics of the generated image\nmore than its style and local features."}, {"title": "2.3 Al Usage in Conceptual Design", "content": "Various design tools have been developed previously for\nsearching and exploring inspiration [37, 38], as well as generat-\ning design concepts. Many of these tools leverage large language\nmodels (LLMs) to aid designers in brainstorming, exploring user\nand engineering requirements, searching for design examples,\nand synthesizing design ideas [39-42]."}, {"title": "3. METHOD", "content": "In order to address the three challenges mentioned earlier and\ndevelop an innovative generative AI system, data was gathered\nfrom diverse sources. As illustrated in Figure 2, first, designers\nwere asked to fill in a survey about textual inspiration (Section\n3.1), then complete a Data Augmentation Task (Section 3.2),\nand lastly a three-day in-person workshop (Section 3.3). These\nmethods were selected to provide both qualitative insights and\nquantitative coverage, ensuring that the conclusions drawn from\nin-depth analysis are representative of a broader spectrum of\ndesigners. The following sections detail these methods (see 2)."}, {"title": "3.1 Text Inspiration Survey", "content": "We conducted a survey to explore how automotive designers\nincorporate text inspiration into their design processes. A total of\n47 designers participated in the survey, boasting an average of 12\nyears of design experience in various design specialties such as\ninterior, exterior, component, design modeling, and future design.\nAmong them, 6 designers have five years of experience or less,\n15 have six to ten years, and 26 have more than ten years of\nexperience.\nFrom the survey, we focused on two sets of questions for\nanalysis in this paper. The first set inquired about the usefulness"}, {"title": "3.2 Data Augmentation Task", "content": "To establish a foundational understanding of the relationship\nbetween inspiration keywords and car wheel designs, we con-\nducted an annotation task with 16 automotive designers. These\ndesigners possess an average of 12 years of experience, with 4\nhaving five years of experience or less, 3 with six to ten years,\nand 9 with over 10 years of experience. The task, lasting ap-\nproximately 30 minutes, involved presenting designers with 25\ncar wheels and asking them to select the top 10 wheels that best\nrepresent a given keyword, such as \u201cdynamic\u201d. These 25 car\nwheels corresponded to ten keywords selected from those most\ncommonly used, as identified in the text inspiration survey. An\nexample task for the keyword \"dynamic\" is illustrated in Figure\n3."}, {"title": "3.3 In-Person Workshop", "content": "We organized a three-day workshop with six automotive de-\nsigners to explore their approach to drawing inspiration in concep-\ntual design and their preferred interaction modes with generative\nAI technologies. Although they worked at the same company,\nthese designers were not familiar with each other prior to the\nworkshop as they worked in different teams. Their expertise\nspans interior, exterior, and component car design, as well as\ndigital modeling, product planning, and future design.\nThe designers were split evenly between those with 2 to\n5 years of design experience and those with over 10 years of\nexperience. Throughout the in-person workshop, we guided them\nthrough the following three activities using a combination of\ndigital (Mural) and analog (Pen and paper, paper prototype) tools."}, {"title": "3.3.1 Brainstorm activity and open discussion:.", "content": "Design-\ners utilized Mural, a collaborative virtual canvas where people\ncan brainstorm and organize ideas using sticky notes, images, and\ndrawings. They were tasked with generating image inspiration for\neight keyword prompts, such as \"bold\", and elaborating on how\nspecific designs were associated with each keyword. This exer-\ncise provided insights into the connection between text inspiration\nand designs in the minds of the designers. The findings from this\nactivity are detailed in Section 4.1.3 Translating keywords into\ndesigns. Additionally, we prompted designers to provide ex-\namples illustrating the distinction between being \u201cinspired\" and\n\u201cinfluenced\", recognizing concerns regarding inadvertently repli-\ncating designs through generative AI technologies. Section 4.2.3\nInspired v.s. influenced addresses this aspect."}, {"title": "3.3.2 Poll on interaction modes:.", "content": "Designers were asked\nto answer seven questions regarding their preferred interaction\nmodes when using generative AI for design inspiration and con-\ncept development. These questions were delivered via Mentime-\nter [46]. Each participant used their own mobile device to provide\ntheir answer. Once all answers were collected, the results were\ndisplayed, and designers engaged in group discussions. Table 1\nshows the list of questions. The findings informed our develop-\nment of our system. Section 5 Findings on Interaction Modes\npresents and discusses these findings."}, {"title": "3.3.3 User experience testing with paper prototypes:.", "content": "We conducted tests with paper prototypes of an AI interface to\ninvestigate designers' preferred interaction modes for integrating\ninspiration into concept development. Six designers were equally\ndivided into two groups. Each group interacted with the paper\nprototype, assuming the role of the user inputting inspiration,\nwhile a facilitator acted as the AI generating design outputs. De-\nsigners were encouraged to think aloud and engage in interactions\nbeyond the constraints of the paper prototype. For example, de-\nsigners could directly point at a generated design, and tell the\nfacilitator the actions they wanted to perform on that design. An\nexample of designer feedback on the paper prototype is shown in\nFigure 4."}, {"title": "4. FINDINGS ON DESIGN INSPIRATION", "content": "In conceptual design, inspiration plays a crucial role in estab-\nlishing and communicating design directions, as well as evoking\nfeelings and conveying meanings. In this section, we present and\nanalyze the findings from both survey and the workshop to explore\nthe use of text and image inspiration in the process of car design\nsuch as the distinction between \"inspired\" versus \u201cinfluenced\"."}, {"title": "4.1 Text Inspiration", "content": "The design of complex products such as cars typically in-\nvolves the collaborative efforts of multiple teams. To facilitate\nautonomous work while ensuring a cohesive design concept, de-\nsigners across various teams are often provided with a set of key-\nwords to establish the direction. Text serves as a valuable tool for\nconveying ideas without imposing the same level of constraints\nas images [3, 4]. Our goal in this study is threefold: firstly, to\ncomprehend how designers integrate text inspiration into concep-\ntual design; secondly, to identify specific texts that hold greater\nsignificance within a particular design context, in our case, a car\ndesign company; and finally, to examine the connection between\nthese texts and the final designs."}, {"title": "4.1.1 Using keywords to define design directions.", "content": "To\nbetter understand how designers employ keywords and evaluate"}, {"title": "4.1.2 Shared vocabularies within a design context.", "content": "In\nthe same survey, we prompted designers to reflect on the top ten\nkeywords they used most often in projects in the past year. De-"}, {"title": "4.1.3 Translating keywords into designs.", "content": "During the\nworkshop, designers were tasked with generating keywords to\ndescribe a particular design. For the wheel example in Figure\n5, designers unanimously used \"dynamic\" to define the design.\nSubsequently, we asked them to elaborate on why they perceived\nthe design as \"dynamic\". Here are some insights shared by the\ndesigners:\n\u2022 \"This wheel is dynamic since the gaps have different sizes.\nThat creates a feeling of motion. \"\n\u2022 \"The varied radial patterns engraved at the center of the\nwheel creates a sense of speed. \"\n\u2022 \"The angled cutout from the tip of the smaller gaps indicate\ndirections, which give a feeling of speed.\"\nThrough this activity, it became evident that designers inter-\npret the keywords in diverse ways, and translate these interpre-\ntations into visual characteristics within their designs. Notably,\nnone of these explanations and translations were formally docu-\nmented in the design process. This lack of documentation poses\nchallenges for creating AI systems capable of understanding text\ninspiration in design contexts. Consequently, additional design\ndata annotation is necessary to facilitate AI comprehension of the"}, {"title": "4.2 Image Inspiration", "content": "During the workshop, designers were tasked with searching\nfor images that would inspire their design concept. It was ob-\nserved that designers use image inspiration to convey feelings,\nmeanings, and incorporate visual elements, such as shapes, tex-\ntures, and colors into their designs. This section explores how\ndesigners leverage image inspiration and demonstrates how it\ncompliments text inspiration in concept development."}, {"title": "4.2.1 Convey feelings with image inspiration.", "content": "In re-\nsponse to Question 1 of the workshop poll, \u201cwhat types of inspi-\nration image do you want?\u201d, two out of six designers emphasized\nthe importance of images to communicate feelings. One design-,\ners suggested \"an instance image that represents a certain feeling,\"\nwhile another simply mentioned \"mood\". During user interac-\ntion testing with paper prototypes, designers consistently chose\nimages that convey feelings rather focusing solely on shapes. For\nexample, one designer selected an image of a gun to communicate\nthe feeling of \"agressive\u201d, stating, \u201cit is the symbolism behind the\nobject, not its literal shape, that matter\"."}, {"title": "4.2.2 Extract visual elements from image inspiration.", "content": "When tasked with finding image inspiration for \"not dynamic\"\nduring the workshop, designers selected images depicting straight\nlines, symmetry and repeated as in Figure 6. Designers can\nextract the shapes and patterns directly from these inspiration\nimages, integrating these visual elements into their designs. In\nresponse to Question 1 of the workshop poll, designers mentioned\nvarious types of image inspiration, including \u201csimple thumbnail\nstyled basic geometry, not so much detail\u201d, \u201call industrial product\nimage", "color": "Furthermore, during the paper prototype,\ndesigners incorporated image inspiration similar to those shown\nin Figure 4, highlighting specific shapes to inspire the generation\nof new concepts."}, {"title": "4.2.3 Inspired v.s. influenced.", "content": "During the workshop, we\nasked designers to illustrate examples of designs they consid-\nered as \"inspired\u201d versus \u201cinfluenced\". As depicted in Figure 7\nby the designers, they explained that extracting feelings or visual\nelements from designs originating in different fields\u2014such as na-\nture, art, and furniture-and translating and incorporating them\ninto their own design context\u2014such as home appliances-is con-\nsidered as \"inspired\". However, directly using elements from a\nsimilar design field and applying them in their own design would\nbe seen as \"influenced\".\nIn response to Question 1 of the workshop poll regarding\nthe types of inspiration image designers want, three out of six\ndesigners mentioned designs unrelated to cars. We observed\nsimilar behavior during user experience testing with paper proto-\ntypes, where designers frequently used image related to art and\nproduct design. In the rare cases where they used car-related\nimages, they drew simple lines with white markers, expressing a\npreference for AI to only use these lines.\nNovelty is crucial in conceptual design. Understanding de-\nsigners' expectations on the types of text and image inspiration\nthey wish to use, as well as how they anticipate the inspiration\nbeing incorporated into the generation of new concepts, is critical\nfor designing new AI assistance for this design phase. In the fol-\nlowing section, we will explore designers' preferred interaction\nmodes with Al systems."}, {"title": "5. FINDINGS ON INTERACTION MODES", "content": "This section will present and discuss the findings from both\nthe workshop poll and the user experience testing with paper\nprototypes, focusing on designers' preferred interaction with AI.\nThe findings are organized into three subsections: designers'\ninput, design output, and integration into the design process."}, {"title": "5.1 Inspiration Inputs from Designers", "content": "5.1.1 Keywords for concept definition. In Question 2 in\nthe workshop poll, \"What are the top text input option(s) you\nwould prefer most?\", designers were presented with six options\""}, {"title": "5.1.2 Quantity of inspiration input.", "content": "In Question 3 and 4\nduring the workshop, designers were asked about their preferred\nnumber of text and image inspiration inputs. For text inspira-\ntion, designers preferred two to four inputs to maintain a focused\ndesign direction. As one designer articulated, \"I'll input 3 to 4\nconcepts. This is because too little information is insufficient, and"}, {"title": "5.1.3 Hierarchical approach to inspiration.", "content": "Rather than\ncategorizing input inspiration into traditional types like image,\ntext, and sketch, designers adopt a hierarchical perspective. They\nuse keywords, such as \"bold\" and \"dynamic\", to establish design\ndirections. As designers develop these concepts, they incorporate\ndetailed text and image inspiration and explore the generative AI\noutput. Figure 8 illustrates this approach. This preference is evi-\ndent in the user experience testing, where designers consistently\nassociate inspiration images with specific concept keywords de-\nspite the paper prototype setup and inquire whether the AI system\ncan interpret them accurately. One designer explained their de-\nsire for the AI not only to generate the perfect outcome but also to\naccompany them as they refine their concepts. Designers seek de-\ntailed image and text inspiration to interpret the keyword concept\nand draw further inspiration from the AI outputs."}, {"title": "5.2 Design outputs", "content": "5.2.1 Expected and unexpected. In Question 5 in the\nworkshop, designers were asked to select preferred qualities of\nthe output from the AI generation. The results are shown in\nTable 3. Interestingly, designers expressed a desire for both \u201cex-\npected\" and \"unexpected\" outputs. All designers choose they\nwant unexpected and interesting outputs. All designers indicated\na preference for unexpected and interesting outputs, as they value\nthe AI's ability to generate ideas they might not conceive alone."}, {"title": "5.2.2 Novel and useful.", "content": "Designers expressed a desire for\nboth \"useful\" and \"novel\" designs from the AI. To better under-\nstand how these terms correspond to design output, we asked\ndesigners to rank six pre-generated designs on usefulness and\nnovelty and discuss their responses (Question 6 during the work-\nshop). The purpose of this exercise was to better understand what\nkind of designs or design elements designers consider to capture\nboth of these aspects they previously indicated as important met-\nrics. These designs ranged from more functional wheels to more\nabstract concepts. The results are depicted in Figure 9."}, {"title": "5.3 Design Process Integration", "content": "5.3.1 Continuous generation. In the workshop poll, de-\nsigners were prompted in Question 7 to specify when they pre-\nferred the generation to occur. They were presented with three\noptions: generating the output only upon clicking \"generate\",\ngenerating the output after adding design feedback, and contin-\nuously generating in real-time. Designers had the flexibility to\nchoose multiple options. Surprisingly, out of the six designers,\nfive chose continuous real-time generation, one preferred gener-\nating the output after adding design feedback, and only one chose\ngenerating the output only upon clicking \"generate\".\nDesigners draw parallels to their experience with image\nsearch platforms like Pinterest, where scrolling infinitely is pos-\nsible. This preference stems from the recognition that inspiration\nis boundless, and good concepts emerge sporadically. Designers\nare accustomed to sifting through vast amounts of content to find\ndesigns that resonate with them. Continuous generation provides\nthem with an abundance of choices and may ensure that each\ninteraction leads to meaningful directions in their design process."}, {"title": "5.3.2 Regenerate with feedback.", "content": "Subsequently in Ques-\ntion 8, designers were asked to rank the design organization fea-\ntures based on their usefulness. The most useful feature is the\nability to provide design feedback on a generated result, followed\nby assigning a novelty score to a design, star/like a design, as-\nsigning a usefulness score, grouping designs, and adding tags to\ndesigns.\nDuring the paper prototype testing phase, we observed a\nconsistent inclination towards this preference. Designers directly\nattached sticky notes as comments to the generated results and re-\nquested the system to regenerate them accordingly. Additionally,\nthey exhibited a tendency to swiftly discard designs rather than\nsorting through them. Designers expressed a desire to seamlessly\nintegrate AI into their creative process, envisioning it as a junior\ndesigner aiding in idea generation and collectively critiquing de-\nsigns to improve them."}, {"title": "6. GENERATIVE AI APPLICATION FOR DESIGN INSPIRATION", "content": "Drawing from the insights presented in Section 4 Findings on\nDesign Inspiration and Section 5 Findings on Interaction Modes,\nwe developed a functional prototype of a generative AI system\naimed at assisting designers with developing concepts using text\nand image inspiration. User testing was conducted on this proto-\ntype, engaging both individual designers and their design teams\nto gather feedback."}, {"title": "6.1 User Interface Design", "content": "The screenshot of the user interface is shown in Figure 10.\nDesigners start with a rudimentary initial sketch, selecting design\nkeywords to establish design directions. Within each keyword de-\nfined direction, they can further refine their concepts by adding\nimage inspirations. Designers also have the flexibility to adjust\nthe symmetry settings to control the repetition in the generated\ndesigns and specify the quantity of designs to be generated si-\nmultaneously.\nBelow, we outline the detailed features prioritized in the AI\ntool, along with references to corresponding findings in studies"}, {"title": "6.2 Al Model Architecture", "content": "The AI model for concept generation, based on both image\nand text inspiration, is required to integrate visual elements from\nimage inspiration and understand the stylistic meaning behind text\ninspiration. Additionally, the model needs to generate designs\nthat exhibit key characteristics of wheels, including their general\nshape, position, and orientation.\nTo address this requirement, we evaluated four different dif-\nfusion models. Section 2.2 Generative AI for Image Generation\nintroduces the backgrounds on diffusion models and explains why\nthey are suitable for design tasks.\n\u2022 Stable Diffusion [33] is a commonly used model, but it only\naccepts a textual prompt as input and cannot incorporate\nimage inspiration.\n\u2022 ControlNet [34] and its successors allow more precise con-\ntrol of the general shape, position, and orientation of the\nwheel through different conditional inputs such as depth and\nnormal maps, segmentation maps, and line drawings. How-\never, they do not provide stylistic control beyond a color\npalette.\n\u2022 Kandinsky [35] can support both image and text inspira-\ntion by mapping multiple image and text inputs into a shared\nlatent space and mixing them together. However, the seman-\ntics and functionality of the generated images are affected"}, {"title": "6.3 Application Architecture", "content": "The application's architecture is deployed on Amazon\nWeb Services (AWS), utilizing GPU-accelerated EC2 instances\n(p3.2xlarge) with NVIDIA V100 GPUs, each providing 16GB\nof video RAM. These instances were chosen to meet high-\nperformance computing needs essential for executing complex\noperations efficiently. To ensure scalability and effective resource\nmanagement, EC2 Auto Scaling groups are implemented, allow-\ning the system to adjust computational resources dynamically in\nresponse to changing workloads.\nThe front-end of the application is constructed using Type-\nScript, React, and Next.js, incorporating Autodesk Platform Ser-\nvice for authentication. This setup is based on the T3 stack, indi-\ncating a selection of technologies known for their robustness and\ndeveloper-friendly nature in creating scalable web applications.\nThe front-end serves as the user interface, facilitating secure ac-\ncess and interaction with the application's services. Among these\nservices is the diffusion wheel design generator, which leverages\ntheme images and initial designs to generate customized wheel\ndesigns, showcasing the application's capability to produce tai-\nlored outputs based on specific inputs."}, {"title": "6.4 User Feedback", "content": "We conducted three 30-minute testing sessions with indi-\nvidual designers. The examples generated by designers in these\nsessions are shown in Figure 1. Following this, we organized a\n90-minute in-person group design session involving twelve de-\nsigners. In this session, one designer controlled the application\nwhile sharing the screen on a large projector, while others pro-\nvided live suggestions on inspiration and actions within the appli-\ncation. Designers were encouraged to think aloud in both testing\nscenarios.\nOverall, designers found the interface highly intuitive. They\nwere able to use all features with minimal guidance. Designers\nused Pinterest to drag images for inspiration alongside their initial\nsketches into our AI application. They selected three to six images\nfor simultaneous generation. Designers particularly appreciated\nthe ability to reinforce symmetry, often adjusting the symmetry\nnumber to explore various design variations and save time.\nThey also liked the rapid incorporation of visual elements\nfrom inspiration images into designs, enabling them to start with"}, {"title": "7. LIMITATIONS AND FUTURE WORK", "content": "This work provides a unique opportunity to contextualize the\nchallenges in conceptual automotive design and test the limits of\ncutting-edge generative AI research. We developed the final pro-\ntotype and tested it with designers. This closed-loop approach\nallowed us to validate our assumptions and identify future oppor-\ntunities. The most important feedback regarding the tool itself\nwas that while the designers found the interaction with our gen-\nerative AI tool to be intuitive, they desired more control over the\ndesign outcomes and more explainable results. These functional\nlimitations are detailed in Section 6.4 User Feedback.\nCentering this work on one company allowed us to propose\nnovel and meaningful interactions with generative AI in an au-\ntomotive context. The automotive industry is undergoing signif-\nicant changes with the rise of startups and the growing demand"}, {"title": "8. CONCLUSION", "content": "The rapid evolution of generative AI technology significantly\nimpacts the design domain, particularly within conceptual design,\nwhere its open-ended nature and reliance on creative ideas pro-\nvide numerous opportunities for AI creativity to enhance human\ncreativity. This paper seeks to explore three fundamental ques-\ntions: 1. How do designers currently use inspiration in their\nconceptual design workflows? 2. What preferences do designers\nhave regarding interaction with generative Al systems for con-\ncept development? 3. How can a generative AI application be\ndeveloped to meet these identified needs and design practices?\nDespite the widespread availability of generative AI tech-\nnology, its meaningful integration into design processes remains\nconstrained. Understanding that designs happen both individu-\nally and collaboratively, and that a wealth of knowledge is embed-\nded and shared within a common design context, we employed\na diverse set of methods including surveys, workshops, and\nuser testing and recruited designers and design teams that share\nsimilar design contexts. This integrated approach allowed us"}]}