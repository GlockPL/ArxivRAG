{"title": "INSPIRED BY AI?\nA NOVEL GENERATIVE AI SYSTEM TO ASSIST CONCEPTUAL AUTOMOTIVE DESIGN", "authors": ["Ye Wang", "Nicole B. Damen", "Thomas Gale", "Voho Seo", "Hooman Shayani"], "abstract": "Design inspiration is crucial for establishing the direction\nof a design as well as evoking feelings and conveying meanings\nduring the conceptual design process. Many practice design-\ners use text-based searches on platforms like Pinterest to gather\nimage ideas, followed by sketching on paper or using digital\ntools to develop concepts. Emerging generative AI techniques,\nsuch as diffusion models, offer a promising avenue to stream-\nline these processes by swiftly generating design concepts based\non text and image inspiration inputs, subsequently using the AI\ngenerated design concepts as fresh sources of inspiration for fur-\nther concept development. However, applying these generative\nAl techniques directly within a design context has challenges.\nFirstly, generative AI tools may exhibit a bias towards particular\nstyles, resulting in a lack of diversity of design outputs. Secondly,\nthese tools may struggle to grasp the nuanced meanings of texts\nor images in a design context. Lastly, the lack of integration\nwith established design processes within design teams can result\nin fragmented use scenarios. Focusing on these challenges, we\nconducted workshops, surveys, and data augmentation involving\nteams of experienced automotive designers to investigate their\ncurrent practices in generating concepts inspired by texts and im-\nages, as well as their preferred interaction modes for generative\nAl systems to support the concept generation workflow. Finally,\nwe developed a novel generative Al system based on diffusion\nmodels to assist conceptual automotive design.", "sections": [{"title": "1. INTRODUCTION", "content": "Conceptual design is a fluid and creative phase where de-signers collaborate to generate early designs that set the directionfor development. Within this phase, designers work both col-laboratively and independently. They are given the freedom to\nuse a diverse set of tools, with some leaning towards traditionalsketches while others prefer digital tools [1]. Sketching plays apivotal role in automotive concept development and is considereda way of visual thinking and communicating [2]. To foster col-laboration, teams are often given the same context and conceptkeywords to guide their exploration. The role of the designers isto create novel designs that embody a strong concept, showcaseuniqueness, and remain feasible and functional [3].\nBoth textual and image-based stimuli serve as frequent\nsources of inspiration in the concept design process. Textual stim-uli excel at establishing a clear direction; for instance, a car designteam might define \u201cbold\" and \"dynamic\" as key inspirations forthe upcoming season's cars. In the automotive context, these key-words are not merely descriptive of a feeling, but have also cometo refer to specific shapes as the field has evolved over the years[4]. These keywords can then be communicated across differ-ent design teams\u2014exterior, interior, and component\u2014allowingthem to work relatively autonomously while still achieving cohe-sive designs.\nOn the other hand, images excel at evoking feelings and con-veying meaning, with a closer connection to the physicality ofdesign compared to text. From an image, designers can extractvarious visual elements such as colors, shapes, and textures, di-rectly influencing their designs. Such elements can be used toevaluate the sales performance of previous models, and guide fu-ture developments [5]. Moreover, designers can interpret textualconcepts using imagery that goes beyond its car-related meaning;for instance, interpreting \"bold\" as reminiscent of the solidity ofa concrete building or the stability of a vast river in nature.\nDesigners encounter various challenges when seeking inspi-ration and developing them into concepts. Firstly, the accessi-bility of online inspiration means that widely visible results arereadily available to everyone. However, it can be time-consumingand idiosyncratic to manually curate or generate unique content[6], and then translate them into design patterns that match the"}, {"title": "2. BACKGROUND", "content": "2.1 Design Inspiration\nInspiration plays a crucial role in conceptual design. It servesas both an analogy and a guide, facilitating exploration across avast design space [15\u201317]. Additionally, inspiration helps design-ers in overcoming fixation and generating novel ideas [18-20],defining design directions [21], and communicating design con-cepts and contexts [22]. Inspiration manifests in various forms,including drawings, texts, images, 3D designs, and physical prod-ucts such as teardowns [1, 23\u201325], and can be acquired intention-ally through search or naturally through experience [6, 26]. Theseforms of inspiration can be encountered physically, digitally, andvirtually [6, 27, 28], facilitating the creation of new concepts.\nThis work concentrates on text and image inspiration forsevereal reasons. Firstly, these forms are easily and affordablyaccessible in large quantities and varieties via the internet, andare commonly utilized by designers. Secondly, text and imagesare typically integrated into design processes, serving as tools forestablishing design directions and communicating ideas. Lastly,many advanced generative Al models focus on text and imageinputs and outputs, benefiting from the abundance of high-qualitycontent available."}, {"title": "2.2 Generative Al for Image Generation", "content": "Variational Auto-Encoders (VAEs) [30] and Generative Ad-versarial Networks (GANs) [31] have been two classical deepgenerative models that were used for generating images with rela-tive success. Recently, Denoising Diffusion Probabilistic Models(DDPMs) [9] have shown even greater success in generating high-quality and diverse images with better training stability. Thesemodels can learn to predict the reverse of a diffusion process(adding Gaussian noise) applied to the data, gradually denoisingan image from noise to clean data points similar to the imagesin the training dataset [32]. However, the need to iterate throughmany denoising steps makes the original DDPMs slower thanGANs and VAEs.\nTo tackle this problem, Latent Diffusion Models (LDMs)such as Stable Diffusion [33] first train a high-quality Auto-Encoder that learns a latent representation for the images andthen perform the denoising diffusion process on that smaller la-tent tensor. This reduces the size and computational cost of thesemodels substantially. These models are usually conditioned on atextual prompt that describes the image semantically.\nTo provide users with fine-grained control over the generalcomposition, shape, and position of the generated images, newmodels such as ControlNet [34] provide additional conditionalinputs that influence the generation. Models that can map textand images into a common latent space, such as Kandinsky [35],allow these modalities to be mixed and collectively used to controlthe generated images. Versatile Diffusion [36] supports manydifferent use-cases, including using multiple images and textualprompts as conditional inputs using cross-attention mechanisms.Particularly, Versatile Diffusion uses a global signal over the inputimages that can be used in the cross-attention, which causes theinput image to influence the semantics of the generated imagemore than its style and local features."}, {"title": "2.3 Al Usage in Conceptual Design", "content": "Various design tools have been developed previously forsearching and exploring inspiration [37, 38], as well as generat-ing design concepts. Many of these tools leverage large languagemodels (LLMs) to aid designers in brainstorming, exploring userand engineering requirements, searching for design examples,and synthesizing design ideas [39-42]."}, {"title": "3. METHOD", "content": "In order to address the three challenges mentioned earlier anddevelop an innovative generative AI system, data was gatheredfrom diverse sources. As illustrated in Figure 2, first, designerswere asked to fill in a survey about textual inspiration (Section3.1), then complete a Data Augmentation Task (Section 3.2),and lastly a three-day in-person workshop (Section 3.3). Thesemethods were selected to provide both qualitative insights andquantitative coverage, ensuring that the conclusions drawn fromin-depth analysis are representative of a broader spectrum ofdesigners. The following sections detail these methods (see 2)."}, {"title": "3.1 Text Inspiration Survey", "content": "We conducted a survey to explore how automotive designersincorporate text inspiration into their design processes. A total of47 designers participated in the survey, boasting an average of 12years of design experience in various design specialties such asinterior, exterior, component, design modeling, and future design.Among them, 6 designers have five years of experience or less,15 have six to ten years, and 26 have more than ten years ofexperience.\nFrom the survey, we focused on two sets of questions foranalysis in this paper. The first set inquired about the usefulness"}, {"title": "3.2 Data Augmentation Task", "content": "To establish a foundational understanding of the relationshipbetween inspiration keywords and car wheel designs, we con-ducted an annotation task with 16 automotive designers. Thesedesigners possess an average of 12 years of experience, with 4having five years of experience or less, 3 with six to ten years,and 9 with over 10 years of experience. The task, lasting ap-proximately 30 minutes, involved presenting designers with 25car wheels and asking them to select the top 10 wheels that bestrepresent a given keyword, such as \u201cdynamic\u201d. These 25 carwheels corresponded to ten keywords selected from those mostcommonly used, as identified in the text inspiration survey. Anexample task for the keyword \"dynamic\" is illustrated in Figure3."}, {"title": "3.3 In-Person Workshop", "content": "We organized a three-day workshop with six automotive de-signers to explore their approach to drawing inspiration in concep-tual design and their preferred interaction modes with generativeAI technologies. Although they worked at the same company,these designers were not familiar with each other prior to theworkshop as they worked in different teams. Their expertisespans interior, exterior, and component car design, as well asdigital modeling, product planning, and future design.\nThe designers were split evenly between those with 2 to5 years of design experience and those with over 10 years ofexperience. Throughout the in-person workshop, we guided themthrough the following three activities using a combination ofdigital (Mural) and analog (Pen and paper, paper prototype) tools.\n3.3.1 Brainstorm activity and open discussion:. Design-ers utilized Mural, a collaborative virtual canvas where peoplecan brainstorm and organize ideas using sticky notes, images, anddrawings. They were tasked with generating image inspiration foreight keyword prompts, such as \"bold\", and elaborating on howspecific designs were associated with each keyword. This exer-cise provided insights into the connection between text inspirationand designs in the minds of the designers. The findings from thisactivity are detailed in Section 4.1.3 Translating keywords intodesigns. Additionally, we prompted designers to provide ex-amples illustrating the distinction between being \u201cinspired\" and\u201cinfluenced\", recognizing concerns regarding inadvertently repli-cating designs through generative AI technologies. Section 4.2.3Inspired v.s. influenced addresses this aspect.\n3.3.2 Poll on interaction modes:. Designers were askedto answer seven questions regarding their preferred interactionmodes when using generative AI for design inspiration and con-cept development. These questions were delivered via Mentime-ter [46]. Each participant used their own mobile device to providetheir answer. Once all answers were collected, the results weredisplayed, and designers engaged in group discussions. Table 1shows the list of questions. The findings informed our develop-ment of our system. Section 5 Findings on Interaction Modespresents and discusses these findings.\n3.3.3 User experience testing with paper prototypes:.We conducted tests with paper prototypes of an AI interface toinvestigate designers' preferred interaction modes for integratinginspiration into concept development. Six designers were equallydivided into two groups. Each group interacted with the paperprototype, assuming the role of the user inputting inspiration,while a facilitator acted as the AI generating design outputs. De-signers were encouraged to think aloud and engage in interactionsbeyond the constraints of the paper prototype. For example, de-signers could directly point at a generated design, and tell thefacilitator the actions they wanted to perform on that design. Anexample of designer feedback on the paper prototype is shown inFigure 4."}, {"title": "4. FINDINGS ON DESIGN INSPIRATION", "content": "In conceptual design, inspiration plays a crucial role in estab-lishing and communicating design directions, as well as evokingfeelings and conveying meanings. In this section, we present andanalyze the findings from both survey and the workshop to explorethe use of text and image inspiration in the process of car designsuch as the distinction between \"inspired\" versus \u201cinfluenced\".\n4.1 Text Inspiration\nThe design of complex products such as cars typically in-volves the collaborative efforts of multiple teams. To facilitateautonomous work while ensuring a cohesive design concept, de-signers across various teams are often provided with a set of key-words to establish the direction. Text serves as a valuable tool forconveying ideas without imposing the same level of constraintsas images [3, 4]. Our goal in this study is threefold: firstly, tocomprehend how designers integrate text inspiration into concep-tual design; secondly, to identify specific texts that hold greatersignificance within a particular design context, in our case, a cardesign company; and finally, to examine the connection betweenthese texts and the final designs.\n4.1.1 Using keywords to define design directions. Tobetter understand how designers employ keywords and evaluate"}, {"title": "4.1.2 Shared vocabularies within a design context. In", "content": "the same survey, we prompted designers to reflect on the top tenkeywords they used most often in projects in the past year. De-"}, {"title": "4.1.3 Translating keywords into designs. During the", "content": "workshop, designers were tasked with generating keywords todescribe a particular design. For the wheel example in Figure5, designers unanimously used \"dynamic\" to define the design.Subsequently, we asked them to elaborate on why they perceivedthe design as \"dynamic\". Here are some insights shared by thedesigners:\n\u2022 \"This wheel is dynamic since the gaps have different sizes.That creates a feeling of motion. \"\n\u2022 \"The varied radial patterns engraved at the center of thewheel creates a sense of speed. \"\n\u2022 \"The angled cutout from the tip of the smaller gaps indicatedirections, which give a feeling of speed.\"\nThrough this activity, it became evident that designers inter-pret the keywords in diverse ways, and translate these interpre-tations into visual characteristics within their designs. Notably,none of these explanations and translations were formally docu-mented in the design process. This lack of documentation poseschallenges for creating AI systems capable of understanding textinspiration in design contexts. Consequently, additional designdata annotation is necessary to facilitate AI comprehension of the"}, {"title": "4.2 Image Inspiration", "content": "During the workshop, designers were tasked with searchingfor images that would inspire their design concept. It was ob-served that designers use image inspiration to convey feelings,meanings, and incorporate visual elements, such as shapes, tex-tures, and colors into their designs. This section explores howdesigners leverage image inspiration and demonstrates how itcompliments text inspiration in concept development.\n4.2.1 Convey feelings with image inspiration. In re-sponse to Question 1 of the workshop poll, \u201cwhat types of inspi-ration image do you want?\u201d, two out of six designers emphasizedthe importance of images to communicate feelings. One design-ers suggested \u201can instance image that represents a certain feeling,\u201dwhile another simply mentioned \u201cmood\u201d. During user interac-tion testing with paper prototypes, designers consistently choseimages that convey feelings rather focusing solely on shapes. Forexample, one designer selected an image of a gun to communicatethe feeling of \"agressive\u201d, stating, \u201cit is the symbolism behind theobject, not its literal shape, that matter\".\n4.2.2 Extract visual elements from image inspiration.When tasked with finding image inspiration for \"not dynamic\"during the workshop, designers selected images depicting straightlines, symmetry and repeated as in Figure 6. Designers canextract the shapes and patterns directly from these inspirationimages, integrating these visual elements into their designs. Inresponse to Question 1 of the workshop poll, designers mentionedvarious types of image inspiration, including \u201csimple thumbnailstyled basic geometry, not so much detail\u201d, \u201call industrial productimage\", and \"color\". Furthermore, during the paper prototype,designers incorporated image inspiration similar to those shownin Figure 4, highlighting specific shapes to inspire the generationof new concepts.\""}, {"title": "4.2.3 Inspired v.s. influenced. During the workshop, we", "content": "asked designers to illustrate examples of designs they consid-ered as \u201cinspired\u201d versus \u201cinfluenced\". As depicted in Figure 7by the designers, they explained that extracting feelings or visualelements from designs originating in different fields\u2014such as na-ture, art, and furniture-and translating and incorporating theminto their own design context\u2014such as home appliances-is con-sidered as \"inspired\". However, directly using elements from asimilar design field and applying them in their own design wouldbe seen as \"influenced\".\nIn response to Question 1 of the workshop poll regardingthe types of inspiration image designers want, three out of sixdesigners mentioned designs unrelated to cars. We observedsimilar behavior during user experience testing with paper proto-types, where designers frequently used image related to art andproduct design. In the rare cases where they used car-relatedimages, they drew simple lines with white markers, expressing apreference for AI to only use these lines.\nNovelty is crucial in conceptual design. Understanding de-signers' expectations on the types of text and image inspirationthey wish to use, as well as how they anticipate the inspirationbeing incorporated into the generation of new concepts, is criticalfor designing new AI assistance for this design phase. In the fol-lowing section, we will explore designers' preferred interactionmodes with Al systems.\""}, {"title": "5. FINDINGS ON INTERACTION MODES", "content": "This section will present and discuss the findings from boththe workshop poll and the user experience testing with paperprototypes, focusing on designers' preferred interaction with AI.The findings are organized into three subsections: designers'input, design output, and integration into the design process.\n5.1 Inspiration Inputs from Designers\n5.1.1 Keywords for concept definition. In Question 2 inthe workshop poll, \"What are the top text input option(s) youwould prefer most?\", designers were presented with six options"}, {"title": "5.1.2 Quantity of inspiration input. In Question 3 and 4", "content": "during the workshop, designers were asked about their preferrednumber of text and image inspiration inputs. For text inspi-ration, designers preferred two to four inputs to maintain a focuseddesign direction. As one designer articulated, \"I'll input 3 to 4concepts. This is because too little information is insufficient, and"}, {"title": "5.1.3 Hierarchical approach to inspiration. Rather than", "content": "categorizing input inspiration into traditional types like image,text, and sketch, designers adopt a hierarchical perspective. Theyuse keywords, such as \"bold\" and \"dynamic\", to establish designdirections. As designers develop these concepts, they incorporate\ndetailed text and image inspiration and explore the generative AIoutput. Figure 8 illustrates this approach. This preference is evi-dent in the user experience testing, where designers consistentlyassociate inspiration images with specific concept keywords de-spite the paper prototype setup and inquire whether the AI systemcan interpret them accurately. One designer explained their de-sire for the AI not only to generate the perfect outcome but also toaccompany them as they refine their concepts. Designers seek de-tailed image and text inspiration to interpret the keyword conceptand draw further inspiration from the AI outputs."}, {"title": "5.2 Design outputs", "content": "5.2.1 Expected and unexpected. In Question 5 in theworkshop, designers were asked to select preferred qualities ofthe output from the AI generation. The results are shown inTable 3. Interestingly, designers expressed a desire for both \u201cex-pected\" and \"unexpected\" outputs. All designers choose theywant unexpected and interesting outputs. All designers indicateda preference for unexpected and interesting outputs, as they valuethe AI's ability to generate ideas they might not conceive alone."}, {"title": "5.2.2 Novel and useful. Designers expressed a desire for", "content": "both \"useful\" and \"novel\" designs from the AI. To better under-stand how these terms correspond to design output, we askeddesigners to rank six pre-generated designs on usefulness andnovelty and discuss their responses (Question 6 during the work-shop). The purpose of this exercise was to better understand whatkind of designs or design elements designers consider to captureboth of these aspects they previously indicated as important met-rics. These designs ranged from more functional wheels to moreabstract concepts. The results are depicted in Figure 9."}, {"title": "5.3 Design Process Integration", "content": "5.3.1 Continuous generation. In the workshop poll, de-signers were prompted in Question 7 to specify when they pre-ferred the generation to occur. They were presented with threeoptions: generating the output only upon clicking \"generate\",generating the output after adding design feedback, and contin-uously generating in real-time. Designers had the flexibility tochoose multiple options. Surprisingly, out of the six designers,five chose continuous real-time generation, one preferred gener-ating the output after adding design feedback, and only one chosegenerating the output only upon clicking \"generate\".\nDesigners draw parallels to their experience with imagesearch platforms like Pinterest, where scrolling infinitely is pos-sible. This preference stems from the recognition that inspirationis boundless, and good concepts emerge sporadically. Designersare accustomed to sifting through vast amounts of content to finddesigns that resonate with them. Continuous generation providesthem with an abundance of choices and may ensure that eachinteraction leads to meaningful directions in their design process.\n5.3.2 Regenerate with feedback. Subsequently in Ques-tion 8, designers were asked to rank the design organization fea-tures based on their usefulness. The most useful feature is theability to provide design feedback on a generated result, followedby assigning a novelty score to a design, star/like a design, as-signing a usefulness score, grouping designs, and adding tags todesigns.\nDuring the paper prototype testing phase, we observed aconsistent inclination towards this preference. Designers directlyattached sticky notes as comments to the generated results and re-quested the system to regenerate them accordingly. Additionally,they exhibited a tendency to swiftly discard designs rather thansorting through them. Designers expressed a desire to seamlesslyintegrate AI into their creative process, envisioning it as a juniordesigner aiding in idea generation and collectively critiquing designs to improve them."}, {"title": "6. GENERATIVE AI APPLICATION FOR DESIGN", "content": "INSPIRATION\nDrawing from the insights presented in Section 4 Findings onDesign Inspiration and Section 5 Findings on Interaction Modes,we developed a functional prototype of a generative AI systemaimed at assisting designers with developing concepts using textand image inspiration. User testing was conducted on this proto-type, engaging both individual designers and their design teams,to gather feedback.\n6.1 User Interface Design\nThe screenshot of the user interface is shown in Figure 10.Designers start with a rudimentary initial sketch, selecting designkeywords to establish design directions. Within each keyword de-fined direction, they can further refine their concepts by addingimage inspirations. Designers also have the flexibility to adjustthe symmetry settings to control the repetition in the generateddesigns and specify the quantity of designs to be generated si-multaneously.\nBelow, we outline the detailed features prioritized in the AItool, along with references to corresponding findings in studies"}, {"title": "6.2 Al Model Architecture", "content": "The AI model for concept generation, based on both imageand text inspiration, is required to integrate visual elements fromimage inspiration and understand the stylistic meaning behind textinspiration. Additionally, the model needs to generate designsthat exhibit key characteristics of wheels, including their generalshape, position, and orientation.\nTo address this requirement, we evaluated four different dif-fusion models. Section 2.2 Generative AI for Image Generationintroduces the backgrounds on diffusion models and explains whythey are suitable for design tasks.\n\u2022 Stable Diffusion [33] is a commonly used model, but it onlyaccepts a textual prompt as input and cannot incorporateimage inspiration.\n\u2022 ControlNet [34] and its successors allow more precise con-trol of the general shape, position, and orientation of thewheel through different conditional inputs such as depth andnormal maps, segmentation maps, and line drawings. How-ever, they do not provide stylistic control beyond a colorpalette.\n\u2022 Kandinsky [35] can support both image and text inspira-tion by mapping multiple image and text inputs into a sharedlatent space and mixing them together. However, the seman-tics and functionality of the generated images are affected"}, {"title": "6.3 Application Architecture", "content": "The application's architecture is deployed on AmazonWeb Services (AWS), utilizing GPU-accelerated EC2 instances(p3.2xlarge) with NVIDIA V100 GPUs, each providing 16GBof video RAM. These instances were chosen to meet high-performance computing needs essential for executing complexoperations efficiently. To ensure scalability and effective resourcemanagement, EC2 Auto Scaling groups are implemented, allow-ing the system to adjust computational resources dynamically inresponse to changing workloads.\nThe front-end of the application is constructed using Type-Script, React, and Next.js, incorporating Autodesk Platform Ser-vice for authentication. This setup is based on the T3 stack, indi-cating a selection of technologies known for their robustness anddeveloper-friendly nature in creating scalable web applications.The front-end serves as the user interface, facilitating secure ac-cess and interaction with the application's services. Among theseservices is the diffusion wheel design generator, which leveragestheme images and initial designs to generate customized wheeldesigns, showcasing the application's capability to produce tai-lored outputs based on specific inputs."}, {"title": "6.4 User Feedback", "content": "We conducted three 30-minute testing sessions with indi-vidual designers. The examples generated by designers in thesesessions are shown in Figure 1. Following this, we organized a90-minute in-person group design session involving twelve de-signers. In this session, one designer controlled the applicationwhile sharing the screen on a large projector, while others pro-vided live suggestions on inspiration and actions within the appli-cation. Designers were encouraged to think aloud in both testingscenarios.\nOverall, designers found the interface highly intuitive. Theywere able to use all features with minimal guidance. Designersused Pinterest to drag images for inspiration alongside their initialsketches into our AI application. They selected three to six imagesfor simultaneous generation. Designers particularly appreciatedthe ability to reinforce symmetry, often adjusting the symmetrynumber to explore various design variations and save time.\nThey also liked the rapid incorporation of visual elementsfrom inspiration images into designs, enabling them to start with"}, {"title": "7. LIMITATIONS AND FUTURE WORK", "content": "This work provides a unique opportunity to contextualize thechallenges in conceptual automotive design and test the limits ofcutting-edge generative AI research. We developed the final pro-totype and tested it with designers. This closed-loop approachallowed us to validate our assumptions and identify future oppor-tunities. The most important feedback regarding the tool itselfwas that while the designers found the interaction with our gen-erative AI tool to be intuitive, they desired more control over thedesign outcomes and more explainable results. These functionallimitations are detailed in Section 6.4 User Feedback.\nCentering this work on one company allowed us to proposenovel and meaningful interactions with generative AI in an au-tomotive context. The automotive industry is undergoing signif-icant changes with the rise of startups and the growing demand"}, {"title": "8. CONCLUSION", "content": "The rapid evolution of generative AI technology significantlyimpacts the design domain, particularly within conceptual design,where its open-ended nature and reliance on creative ideas pro-vide numerous opportunities for AI creativity to enhance humancreativity. This paper seeks to explore three fundamental ques-tions: 1. How do designers currently use inspiration in theirconceptual design workflows? 2. What preferences do designershave regarding interaction with generative Al systems for con-cept development? 3. How can a generative AI application bedeveloped to meet these identified needs and design practices?\nDespite the widespread availability of generative AI tech-nology, its meaningful integration into design processes remainsconstrained. Understanding that designs happen both individu-ally and collaboratively, and that a wealth of knowledge is embed-ded and shared within a common design context, we employeda diverse set of methods including surveys, workshops, anduser testing and recruited designers and design teams that sharesimilar design contexts. This integrated approach allowed us"}]}