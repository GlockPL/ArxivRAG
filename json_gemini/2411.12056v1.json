{"title": "Benchmarking pre-trained text embedding models in aligning built asset information", "authors": ["Mehrzad Shahinmoghadam", "Ali Motamedi"], "abstract": "Accurate mapping of the built asset information to established data classification systems and taxonomies is crucial for effective asset management, whether for compliance at project handover or ad-hoc data integration scenarios. Due to the complex nature of built asset data, which predominantly comprises technical text elements, this process remains largely manual and reliant on domain expert input. Recent breakthroughs in contextual text representation learning (text embedding), particularly through pre-trained large language models, offer promising approaches that can facilitate the automation of cross-mapping of the built asset data. However, no comprehensive evaluation has yet been conducted to assess these models' ability to effectively represent the complex semantics specific to built asset technical terminology. This study presents a comparative benchmark of state-of-the-art text embedding models to evaluate their effectiveness in aligning built asset informa- tion with domain-specific technical concepts. Our proposed datasets are derived from two renowned built asset data classification dictionaries. The results of our benchmarking across six proposed datasets, covering three tasks of clustering, retrieval, and reranking, highlight the need for future research on domain adaptation techniques. The benchmarking resources are published as an open-source library, which will be maintained and extended to support future evaluations in this field.", "sections": [{"title": "Introduction", "content": "Asset management plays a pivotal role in ensuring optimal performance and extended life span of the built environment through a systematic process of monitoring and maintaining various facilities and equipment. The rapid advancement of digital technologies has led asset owners to increasingly demand enriched digital twins at project handover to support real-time operations and maintenance of the built assets [Love and Matthews, 2019]. Simultaneously, the growing awareness of the benefits of digitized asset management highlights the essential need for federated access to built asset data [Moretti et al., 2023]. This requires aligning extensive data sources and their underlying schema with established data models, classification systems, or taxonomies to facilitate data accessibility for diverse stakeholders and improve interoperability across various software environments. However, aligning built asset data with pre-defined classification systems poses significant challenges in practice. A key challenge stems from the multi-source and multi-disciplinary nature of built asset data, which leads to the use of diverse formats and terminologies across different projects and stakeholders. For example, the terminology that architects utilize to describe the specifications for a particular building component or system can vastly differ from those used by structural engineers or subcontractors. Moreover, the structures of domain-specific classifications used in different disciplines often vary in granularity. For instance, the detailed engineering descriptions of an HVAC system provided by mechanical engineers may be far more comprehensive than those required and used by operations and maintenance teams. Finally, variations in local regulations and standards can further complicate the alignment process, particularly for large-scale or international projects. These issues, combined with the dynamic and evolving nature of built asset data throughout an asset's lifecycle, lead to potential inconsistencies when integrating this data into a unified digital asset management environment.\nIn response, there have been several initiatives aimed at facilitating the digital delivery of built asset information while ensuring its conformity with predefined or standardized descriptions (data models, taxonomies, etc.). One major initiative is buildingSMART Data Dictionary (bSDD)[buildingSmart International, 2024a], an international and ongoing effort whose main objective is to create shared def- initions for describing the built environment. This is achieved through a collection of interconnected data dictionaries that are both human-readable and machine-readable[buildingSmart International, 2024a]. Although making various data dictionaries programmatically accessible will facilitate access to agreed and consistent terms, the complexity and dynamic diversity of the built asset terminology necessitate robust data mapping strategies to accommodate various data descriptions and updates [Forth et al., 2024]. As a result, the asset information alignment process remains predominantly manual, relying heavily on the expertise of domain specialists to accurately map complex technical data [Roberts et al., 2018]. The significant challenges associated with the manual alignment process, including high costs, time consumption, and potential for human error, highlight the need for more automated and reliable data mapping solutions.\nThe central thesis of our research builds upon the argument that recent advancements in natural language processing/understanding research can significantly enhance automated data mapping processes. In particular, the rich and contextualized representation of textual inputs as numeric vectors, commonly known as text embedding [Pennington et al., 2014, Lee et al., 2024b], provides advanced capabilities for machines to understand the semantics of the intricate terminologies. Earlier methods such as word2vec [Mikolov et al., 2013] and GloVe [Pennington et al., 2014] relied on static embeddings, i.e., generating fixed representations of numerical vectors for each word based on their co-occurrence in large corpora. However, recent neural language models, dominantly built on top of the transformer architecture [Vaswani et al., 2017], can generate dynamic, context-sensitive embeddings. The capability of recent embedding models in adapting the representation of words (or sub-word tokens) based on their surrounding context has motivated researchers and practitioners across diverse fields to leverage the power of contextual text embeddings to drive advancements in their respective domains. From traditional databases integration [Cappuzzo et al., 2020], to information management in biomedicine [Zhang et al., 2019], or public figure perceptions in social science studies [Cao and Kosinski, 2024], the increasing volume of encouraging reports on leveraging text embedding models to deliver a more nuanced text understanding in various specialized domains [Rasmy et al., 2021, Ostendorff et al., 2021, Rouhizadeh et al., 2024, Wilkho et al., 2024, Cao and Kosinski, 2024] reinforces the relevance of these models in automating data alignment in the domain of built asset information management.\nBased on the observation that built asset data predominantly exists in textual form [Wu et al., 2022], we argue that state-of-the-art text embedding models present promising opportunities to refine the automated alignment of built asset information. However, the extensive and increasing availability of pre-trained language models has led to the proliferation of potential text embedding models, creating confusion regarding model selection for different use cases [Muennighoff et al., 2022]. Moreover, recent research indicates that general-purpose text embedding models often struggle to maintain consistent performance across diverse tasks and domains [Lee et al., 2024b]. This is while most previous studies utilizing pre-trained or fine-tuned language models in built environment research have been significantly limited in scope, primarily focusing on ad-hoc downstream tasks with small evaluation datasets [Shahinmoghadam et al., 2024, Jung et al., 2024, Wang et al., 2024, Forth et al., 2024, Jeon et al., 2024]. Such limitations can result in a potentially skewed perspective on the overall domain-specific text understanding of these models [Shahinmoghadam et al., 2024]. Additionally, scarce public access to the datasets used in previous works poses another important challenge to the transparency and reproducibility of the reported results. This motivates us to examine the extent to which existing language models can be directly leveraged to deliver contextually accurate mappings of domain-specific terminology within the context of built asset information management. In this work, we present a comprehensive benchmark of state-of-the-art text embedding models to evaluate their effectiveness in capturing and representing the semantics of textual descriptions related to built assets. Through this evaluation, we aim to identify the strengths and limitations of existing language models in enhancing data alignment practices within the built asset domain. Our proposed benchmark is aligned with the Massive Text Embedding Benchmark (MTEB) [Muennighoff et al., 2022], a benchmark recognized extensively in both academic and practical contexts for its robustness and utility. We benchmark 24 text embedding models on our developed datasets that amount to a total of more than ten thousand data entries across six tasks, making our evaluations the most comprehensive ones in this specialized field to date. By making our datasets and benchmark software publicly available, we encourage future research to build upon our work, contributing to continuous improvements in this domain."}, {"title": "Methods", "content": "Given the built environment's multidisciplinary nature, the datasets included in the benchmark must encompass an expansive spectrum of sub-domain subjects, including architectural, structural, mechanical, and electrical systems. To ensure a diverse coverage of built products in our benchmark, we carefully examined the selection of data sources used for creating task-specific datasets. A detailed description of the corpus development and data extraction processes is provided below.\nThe initial step in creating the benchmark's task-specific datasets is the development of a consistent corpus of built products. Based on the requirements of the tasks within our benchmark, the core corpus needed to include the following key information for each product: name or title, description, and corresponding labels (group categories). The two primary sources used to develop the built product corpora are as follows:\nIndustry Foundation Classes (IFC). Published and maintained by buildingSMART International[bsi, 2024], IFC is an open international data model offering comprehensive digital descriptions of various aspects of building and infrastructure projects. Originally designed to facilitate interoperability and information exchange among different software applications and stakeholders, IFC provides a comprehensive representation of various aspects of built asset entities. We utilize IFC version 4.3.2.0 [buildingSmart International, 2024b], recently approved as an ISO standard (ISO 16739-1:2024).\nUniclass. Developed and maintained by the National Building Specification (NBS)[NBS, 2024a], Uniclass is a unified classification system for the built environment. We utilize version 1.33 of the Uniclass Pr Product Table[NBS, 2024b]. Uniclass has extensive coverage, encompassing over 8,000 product types, making it one of the most recognized and widely adopted classification systems in the built asset industry."}, {"title": "Data extraction", "content": "To create a corpus of products with corresponding names, descriptions, and labels, we undertook the following steps: For Uniclass, we utilize the publicly-available CSV format of the products table, which comprises over 8,000 products categorized into three hierarchical levels. Product names were directly extracted from the table, while product categories were inferred from the numeric codes associated with hierarchical categories (see Figure 1). To automatically extract the corresponding textual labels for each product, we developed a script to scrape the table programmatically. As the original table does not include product descriptions, we propose a method (detailed in the subsequent subsection) to synthesize a description for each product. We retained only those products that have labels for all three classification levels. After applying this filtering process, the Uniclass corpus comprises 4,234 instances, which remains sufficiently large for our benchmarking purposes.\nRegarding the IFC schema, we parse the official schema content by utilizing resources from an open-source Python library [ifc, 2024] that enables programmatic access to IFC entities. Initially, we extracted entities of interest from a JSON-formatted file containing the complete list of IFC entities, their type enumeration, and their definition (derived from IFC's official documentation). An analysis of the \"IfcProduct\" class within the IFC schema indicated that a significant majority of product entities are classified under the \"IfcElement\" class. Therefore, we focused exclusively on the \"IfcElement\" subclasses. After removing IFC entities with missing descriptions (less than 1% of total \"IfcElement\" entities), we developed a script to extract each entity's top three parent classes to serve as the product category labels. In addition to entity superclass groups, we use the domain-specific schemas (e.g., structural, HVAC, building control) from IFC's official documentations[buildingSmart International, 2024b] as an additional source for entity label assignment. The resulting IFC corpus comprises 977 entities (total of parent entities and type enumerations)."}, {"title": "Data augmentation and curation", "content": "The process of generating textual descriptions for Uniclass entities is depicted in Figure 1(a). Initial entity descriptions are synthesized by sequentially concatenating the entity's category titles, pro- gressing from the most specific to the most general. An example of the synthesized descriptions is provided in Figure 1(a). These concatenated descriptions are then paraphrased using a generative language model to create more nuanced and natural descriptions, relaxing the text from the rigid template initially employed. We generated paraphrased descriptions using the most advanced version of the GPT-4 model available at the time of conducting the experiments (gpt-4-turbo-2024-04-09). Although the prompts used for generating paraphrased descriptions were designed to prevent the alteration or addition of facts, it was essential to manually review all generated descriptions due to known potential inaccuracies of generative language models. The review is carried out by two domain experts, each with over ten years of experience in the field. Each expert cross-checked the issues identified by the other, and the final decisions were made based on mutual agreement. The following curation steps are undertaken to ensure the accuracy and consistency of the extracted product names and descriptions. We preprocess native IFC entity names and convert them into a readable form (e.g., \"IfcHeatExchanger\" to Heat Exchanger; see examples in Figure 1(b) and (c)). For IFC class enumeration types, where the enumeration name alone might be ambiguous, we append the parent class type in parentheses. For example, the enumeration WATER, a subclass of \"IfcBoilerTypeEnum\", is represented as \"WATER (Boiler Type)\" (see examples in Figure 1(c)). Following the same logic, we enrich the product descriptions by concatenating the product's name at the beginning of the description for both Unicalss and IFC entities. This step reinforces contextual clarity, as the natural entity names carry significant semantic information. Finally, we manually review and modify the entity descriptions that contain inconsistent information, such as notes related to the schema version history or future depreciation notes."}, {"title": "Sampling", "content": "To ensure a robust entity selection when creating task-specific datasets, we implemented the following sampling strategies: For positive sampling, we adopt a semantic diversity approach. Given a targetted subset of built products, we generate text embeddings for all corresponding text inputs, i.e., product names and descriptions. Embeddings are generated using a state-of-the-art text embedding model (\"mxbai-embed-large-v1\"[Li and Li, 2023]). From this set of embeddings, we randomly choose an initial sample as a starting point. Subsequently, we iteratively select additional samples by identifying those that exhibit the slightest similarity to the most recently selected sample, as determined by cosine similarity scores, i.e., the cosine of the angle between two embedding vectors. This process repeats until the desired number of samples is achieved. This method ensures that the samples selected for a particular subset (e.g., products of the same category) yield diverse representations within the embedding space by selecting inputs that are semantically dissimilar to the ones already chosen. For negative sampling, we prioritize the selection of product samples that yield closer semantic similarity to a given query (a product name or description) but belong to a different class. We compute the cosine similarities between the query and negative samples using the same embedding model used in the semantic diversity sampling and select samples with higher similarities. By selecting more similar candidates as negative samples, the dataset can better benchmark the model's capability to capture the subtle differences between closely related classes. This method, commonly known as hard negative sampling, is particularly effective for evaluations involving fine-grained classifications, such as differentiating between closely related categories in IFC and Uniclass classification hierarchies. In all sampling methods, including plain random sampling, once a sample is selected, it is only reused in another subset once all samples included within the pool have been exhausted. This way, we maximize the utilization of available samples and maintain diversity within the datasets."}, {"title": "Benchmark", "content": "Evaluating text embeddings across different tasks is crucial for assessing the transferability of their capabilities to various downstream applications. Hence, our proposed benchmark covers three main tasks: clustering, retrieval, and reranking. In addition to domain coverage and cross-task adaptability, evaluating text embedding models requires careful consideration of input text length. To ensure the coverage for varying input lengths, the text entities included in our datasets fall into two categories: (a) sentences, which are derived from product titles/names, and (b) paragraphs, which are derived from product descriptions/definitions. Accordingly, each task-specific dataset in our benchmark is grouped into one of the following categories:\n\u2022 Sentence to Sentence (S2S): Utilizing product titles as input text.\n\u2022 Paragraph to Paragraph (P2P): Utilizing product descriptions (which can be concatenated with the product name) as input text.\n\u2022 Sentence to Paragraph (S2P): Comparing product titles against product descriptions.\nOur proposed benchmark follows MTEB [Muennighoff et al., 2022] for reporting text embedding performance scores. Hence, various metrics are implemented within our benchmark, which can be computed with flexible parameter configurations. The primary metrics, which serve as default scores for task-specific as well as overall comparisons reported in this study, are outlined in each task's description."}, {"title": "Clustering", "content": "Clustering tasks involve grouping similar built products into meaningful clusters based on their similarities in textual representation. Our proposed tasks include S2S and P2P categories, where product names and descriptions act as input text for each dataset type, respectively. Each clustering task dataset is comprised of various subsets, covering diverse subdomain subjects and different levels of granularity. To create the subsets within each clustering dataset, we first select a subset of product labels from one of the three levels of product hierarchy, either from one specific corpus or across both corpora. We then apply the previously described diversity-based sampling method to sample product names (S2S datasets) or descriptions (P2P datasets) for selected labels.\nTo ensure the quality of the subsets, we evaluate the baseline scores using two embedding models, one for the upper threshold (\"mxbai-embed-large-v1\"[Li and Li, 2023]) and one for the lower threshold (\"paraphrase-multilingual-MiniLM-L12-v2\"[Reimers and Gurevych, 2019]). A subset is included in the dataset only if its score with the upper threshold model is below 0.8 and greater than $1/N$ with the baseline model, where N is the number of unique labels. The upper and lower thresholds are set to maintain task difficulty and ensure the task performs better than random guessing, respectively. Subsets meeting these criteria are shuffled to eliminate order bias before being added to the dataset.\nWe compute V-measure scores [Rosenberg and Hirschberg, 2007] by training a mini-batch k-means model using vector embeddings, with k set to the number of unique labels in each clustering subset. The V-measure, ranging from 0 to 1 (higher is better), represents the harmonic mean of two distinct metrics: homogeneity and completeness. Here, homogeneity measures the extent to which clusters contain only products from a single category, while completeness indicates how well all products from a given category are grouped into the same cluster. More details regarding the calculation of V-measure can be found in [Rosenberg and Hirschberg, 2007]."}, {"title": "Retrieval", "content": "Retrieval tasks aim to identify relevant documents, i.e., product textual descriptions, in response to a given query. Our proposed retrieval datasets are framed as S2P and P2P tasks, where built asset descriptions serve as the corpus (the documents to be retrieved), and product titles and descriptions act as queries for the S2P and P2P tasks, respectively. The query-document relevancy ground truth is derived from existing mappings that identify the alignment between IFC and Unicalss product entities. These mappings, validated and published by NBS[NBS, 2024a], can be found in the official Unicalss table release [NBS, 2024b].\nFirst, we encode all queries and product descriptions into corresponding embedding vectors. These embeddings are then used to calculate the pairwise similarity between a given query and all product descriptions using cosine similarity. Subsequently, product descriptions included in each retrieval dataset are ranked according to descending cosine similarity scores. Finally, we compute nDCG@10 (Normalized Discounted Cumulative Gain [J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002] at rank 10) as the primary metric. This score, which can range between 0 and 1 (higher is better), reflects the relevancy of the ranked products based on their positions within the top 10 ranks by applying a logarithmic discount factor to penalize results that appear lower."}, {"title": "Reranking", "content": "In our reranking tasks, the aim is to rank a set of product descriptions with reference to their relevance to a product query. Similar to retrieval tasks, reranking tasks are framed as S2P and P2P types, and pairwise similarity between query and product description embeddings is computed based on cosine similarity. The primary distinction between retrieval and reranking tasks lies in their scope and focus. While our retrieval tasks involve ranking the entire product corpus, reranking narrows the focus to a smaller set of positive and negative subsets, which are selected using the methods outlined in the previous section to ensure diversity and difficulty (avoiding very high scores from overfitting) within the dataset. Positive and negative samples are selected using the methods described in the previous section, thereby maintaining the diversity and difficulty of the dataset. By concentrating on a smaller and more challenging group of product descriptions, our reranking tasks aim to provide a more fine-grained evaluation of the model's ability to rank relevant items accurately.\nSimilar to retrieval tasks, we use cosine similarity to compute pairwise similarity between a given query and product descriptions included in corresponding positive and negative sets. Subsequent to ranking the descriptions based on the cosine similarity scores, we compute MAP (Mean Average Precision) as our primary metric. MAP provides an averaged measure of precision across all relevant products, ranging between 0 and 1, with higher values indicating better performance. It is worth noting that retrieval metrics reflect overall ranking quality while reranking metrics focus on how early relevant products appear in the list."}, {"title": "Results", "content": "Table 1 provides a comprehensive summary of the dataset statistics across the three main tasks in our benchmark. The unique number of sample entries in our clustering datasets shows that more than half of the samples available from the combined product corpora could pass the quality thresholds explained in the methods section. In the retrieval and reranking task, the same retrieval and reranking document corpus is shared between the subtasks of each task category. This design enables a comparative analysis of model performance on different query types, with S2P focusing on shorter product names and P2P targeting longer product descriptions. We applied a 1:3 positive- to-negative sampling ratio to create a balanced yet challenging evaluation set, ensuring that models must distinguish effectively between relevant and irrelevant documents.\nTo outline the distinctions between our newly constructed datasets and existing ones, we conducted a thematic semantic similarity comparison between our clustering datasets and those from MTEB benchmark. Using the \"stella-en-400M-v5\" model, which is the most performant small-sized model in our evaluations (see Table 2), we generated embeddings for 200 randomly selected samples and averaged them within each dataset. Figure 2 depicts the cosine similarity matrix as a heatmap, where darker shades indicate higher content similarity. The high similarity scores between our proposed subtasks confirm strong internal consistency within our benchmark. Moreover, moderate to high similarities with StackExchange, Reddit, and Arxiv datasets reflect thematic overlaps with broader domain content. A discussion of the observed similarities is provided in the next section.\nIn our benchmarking experiments, we evaluated models across a broad range of sizes, from relatively small models with 33 million parameters to significantly larger models exceeding seven billion parameters. However, due to computational constraints, the majority of models tested have less than one billion parameters. The selected models span various positions on the most recent record of MTEB leaderboard (as of September 21, 2024), ranging from first place (i.e., \"NV-Embed-v2\"[Lee et al., 2024a]) to 136th place (i.e., \"paraphrase-multilingual-MiniLM-L12-v2\"). For models that are pre-trained with instruction-based data, we used built-in or recommended prompts as provided in the model card's official web page or associated research papers, when available. For example, \"mxbai-embed-large-v1\" requires custom prompts only for retrieval and reranking tasks, while \"NV- Embed-v2\" needs specific task-based prompts for clustering tasks as well. For models without built-in task instructions, we applied a general set of prompts to ensure consistency across tasks (prompts are available at the project's public GitHub repository[Mehrzad, 2024]).\nThe top-ranked model in our benchmark, \"NV-Embed-v2\", also holds first place on the latest MTEB leaderboard. However, it does not consistently outperform all other models across all tasks. In fact, a closer examination reveals variability in model size and performance relationship. For example, \"gte-small\", the smallest model in our evaluation with 33 million parameters, delivers competitive scores, nearly matching the average scores of models ten times its size and even outperforming larger models in specific tasks. Despite the previously reported strong correlation between model size and performance[Muennighoff et al., 2022], our experiments show that superior performance associated with larger models is only evident at the extreme upper end of the parameter scale. This observation supports the growing emphasis on developing and deploying smaller, more efficient models for both research and real-world applications in this specialized field.\nMotivated by the hypothesis that existing datasets with similar thematic content would yield com- parable performance evaluations, we examined the consistency of relative model performances as follows: Given the observed thematic similarity between our clustering datasets and specific MTEB datasets, particularly \"StackExchange\" and \"Reddit\" (see Figure 2), we compared the rankings of model performance across both our datasets and the selected MTEB datasets. As it can be seen from Table 3, the comparative evaluation of the relative rankings indicates a notable variation in model per- formances, notably in the case of \"multilingual-e5-large-instruct\", \"gte-small\", \"stella_en_1.5B_v5\", and \"text-embedding-3-small\". These observed variabilities further highlight the limitations of relying on general-purpose benchmark datasets, even when relatively high thematic similarities are present, underscoring the importance of domain-specific evaluations.\nWhile our benchmarking experiments primarily focused on open-source models, we also included the proprietary text embedding models from OpenAI, both the small and large versions. The inclusion of the proprietary models is motivated by a recent study where closed-source models tend to achieve relatively higher performance when embedding text in underrepresented languages [Enevoldsen et al., 2024]. We hypothesize that built asset text, as an underexplored domain, might be similarly better represented by proprietary models. Notably, text-embedding-3-large ranks second in our benchmark, performing nearly on par with the top-ranked model. In contrast, the smaller model performed more moderately, ranking in the middle of our benchmark. While the former observation aligns with the findings of [Enevoldsen et al., 2024], the latter is in line with the latest MTEB leaderboard results where closed-source commercial embedding APIs generally underperform compared to their open-source counterparts. These observations raise questions about the underlying factors. However, the lack of knowledge about the key characteristics of proprietary models, such as their size and diversity in training data, prevents us from offering a detailed, conclusive account of their relative performance.\nOur benchmarking results reveal a notable difference in performance between shorter and longer text inputs in different tasks. In particular, across the board, models consistently show lower performance in the S2S clustering task compared to the P2P one. This observation can be attributed to the limited presence of contextual clues given the significantly short length of the input text in the S2S clustering task (see Table 1). On the other hand, in reranking and retrieval tasks, the majority of the models yield moderately higher scores in S2P tasks. The likely explanation for the latter observation is that the shorter length of the sentences (product names) in S2P tasks can lead to a lower amount of irrelevant information (noise) in the input query. Since product names tend only to encapsulate the critical information about the target product, they can yield more precise and discriminative text (query) representations for similarity matching."}, {"title": "Discussion", "content": "Our benchmarking results offer critical insights into the effectiveness of state-of-the-art pre-trained text embedding models in aligning built asset information. One of the key findings of our study is the variability in performance across tasks, even among top-performing models. Our results suggest that model effectiveness is not strongly correlated across model sizes, emphasizing that size alone is not a reliable predictor of model performance in the specialized domain of built asset information management. The interpretation of the relationship between model size and embedding effectiveness is further complicated by the performance gap observed when comparing models pre-trained with and without instruction tuning. Instruction-tuned models showed higher performance in the majority of our benchmark tasks. Considering the larger size of the instruction-tuned models included in our experiments, the latter observation raises an important question for future research: To what extent can instruction-tuning help smaller models adapt to the specialized domain of the built environment? This opens a promising line of investigation into how task-specific training with instruction-based data can better align a model's understanding with the intricate semantics of built asset data, particularly for models with smaller sizes. Finally, in addition to the variability in model performance across different tasks and text input lengths, the results of our comparative examinations highlight the limited transferability of evaluations based on general benchmarks. Our experiments indicate that, even with relatively high thematic similarity, general-purpose benchmarks remain inadequate in capturing the unique semantic complexity and contextual dependencies present in the textual descriptions of the built asset.\nThe above-mentioned points highlight the critical need for tailored benchmarking datasets to examine the effectiveness of various domain adaptation strategies in this field of research. Our work contributes to the body of research by laying a robust foundation for future evaluations and providing a benchmark that is carefully constructed to reflect the complexities of built asset data. Our proposed datasets cover diverse subdomains and exhibit varying levels of granularity, mirroring real-world scenarios where built products are required to be mapped across various data dictionaries. The datasets can be used not only for evaluating new or fine-tuned text embedding models for cross-mapping built asset data but also as a contextually rich text corpus to support the training of task-specific language models for other downstream tasks, such as information extraction. Finally, this work contributes to the broader discourse on the transferability of the general-purpose language models' capabilities by focusing on built asset data as a representative example of niche and underexplored domains.\nOne key limitation of our study is that the text sources used in our work are exclusively in English, limiting the generalizability of our findings to other languages. Another significant challenge was identifying data sources that were both of high quality and could be redistributed as public datasets. In this light, although the developed datasets proved sufficient for our current analysis, future work could benefit from larger-scale datasets and introduce training and validation splits to support new tasks. It is recommended to prioritize exploring more extensive and diverse text sources to include multiple languages and new tasks where the availability of large training splits plays a crucial role, such as text classification or reranking based on cross-encoder architectures. Finally, through the public release of our benchmarking resources in alignment with the MTEB's open-source software, we aim to ensure the reproducibility and extendability of our work through community-driven enhancements."}, {"title": "Data availability", "content": "The datasets and codes developed in this study are openly accessible at the following GitHub repository: https://github.com/mehrzadshm/built-bench-paper. All materials are licensed under the Creative Commons Attribution-NoDerivatives 4.0 International License (CC BY-ND 4.0). Any future updates, including references to additional data and relevant resources, will be incorporated into this repository."}]}