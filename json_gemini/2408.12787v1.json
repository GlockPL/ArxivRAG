{"title": "LLM-PBE: Assessing Data Privacy in Large Language Models", "authors": ["Qinbin Li", "Junyuan Hong", "Chulin Xie", "Jeffrey Tan", "Rachel Xin", "Junyi Hou", "Xavier Yin", "Zhun Wang", "Dan Hendrycks", "Zhangyang Wang", "Bo Li", "Bingsheng He", "Dawn Song"], "abstract": "Large Language Models (LLMs) have become integral to numerous\ndomains, significantly advancing applications in data management,\nmining, and analysis. Their profound capabilities in processing\nand interpreting complex language data, however, bring to light\npressing concerns regarding data privacy, especially the risk of un-\nintentional training data leakage. Despite the critical nature of this\nissue, there has been no existing literature to offer a comprehensive\nassessment of data privacy risks in LLMs. Addressing this gap, our\npaper introduces LLM-PBE, a toolkit crafted specifically for the\nsystematic evaluation of data privacy risks in LLMs. LLM-PBE is\ndesigned to analyze privacy across the entire lifecycle of LLMs,\nincorporating diverse attack and defense strategies, and handling\nvarious data types and metrics. Through detailed experimentation\nwith multiple LLMs, LLM-PBE facilitates an in-depth exploration of\ndata privacy concerns, shedding light on influential factors such as\nmodel size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in\nLLMs but also serves as a vital resource for future research in the\nfield. Aimed at enhancing the breadth of knowledge in this area,\nthe findings, resources, and our full technical report are made avail-\nable at https://llm-pbe.github.io/, providing an open platform for\nacademic and practical advancements in LLM privacy assessment.", "sections": [{"title": "1 INTRODUCTION", "content": "In the contemporary landscape of technology, Large Language Mod-\nels (LLMs) [83, 87, 102, 105] have rapidly ascended to prominence,\nrevolutionizing the way we interact with data. These advanced\nmodels are not just tools for natural language processing; they have\nbecome integral in data management [41, 42, 61, 108-110], and\nmining [15, 45, 132]. LLMs, with their sophisticated algorithms, are\ncapable of extracting meaningful insights from vast datasets, mak-\ning complex data more accessible and actionable. This has led to\ntheir widespread adoption across various domains, fundamentally\naltering the approach to data handling and information processing.\nThere have been some earlier discussions about the impact of\nLLMs on database research [10, 41, 136]. Among them, Amer-Yahia\net al. [10] and Zhou et al. [136] pointed out that data privacy is an\nimportant research challenge in LLMs and databases. It advocates\ndeveloping privacy-preserving schemes to help LLMs to protect the\nprivacy of individuals. In contrast, we aim to thoroughly understand\nand analyze the data privacy leakage in LLMs.\nThe extensive use of LLMs brings forth significant data privacy\nconcerns. Trained on massive datasets, these models are at risk of\nunintentionally exposing sensitive information. Instances where\nLLMs have inadvertently revealed personal details such as email\naddresses and phone numbers [26, 28, 81] from training data in"}, {"title": "2 PRELIMINARIES AND RELATED WORK", "content": "2.1 Large Language Models\nLLMs [83, 87, 102, 105] are a class of advanced models designed to\nunderstand, interpret, and generate human-like text, representing\na significant milestone in the field of NLP. Fundamentally, these\nmodels are built on sophisticated neural network architectures,\nprimarily transformer-based [113] designs, known for their deep\nlearning capabilities in handling sequential data. The architecture\nof LLMs typically involves multiple layers of self-attention mech-\nanisms, which enable the models to process and generate text by\neffectively capturing the context and nuances of language over\nlarge spans of text. The applications of LLMs are remarkably di-\nverse, extending far beyond basic text generation. In the realm\nof data management, LLMs have revolutionized information re-\ntrieval, making it possible to extract and synthesize information\nfrom unstructured data sources with unprecedented efficiency. The\nemergence of LLMs has thus not only pushed the boundaries of"}, {"title": "2.2 Data Privacy Leakage in LLMs", "content": "Data privacy in the context of LLMs concerns the protection of\nsensitive information that these models might access, learn, and po-\ntentially disclose during their operation. This encompasses personal\ndata, confidential information, and any content that, if exposed,\ncould lead to privacy breaches. The challenge in ensuring data pri-\nvacy in LLMs arises from their training process, which involves\nlarge-scale datasets that can contain such sensitive information.\nEnsuring that these models respect user privacy and adhere to"}, {"title": "2.3 Privacy Assessment of LLMs", "content": "As detailed in Table 1, current research in the field typically evalu-\nates the privacy of LLMs using a limited range of models, datasets,\nand attack methodologies. For example, DecodingTrust [114] eval-\nuates the trustworthiness in GPT models on many aspects such\nas robustness, fairness, and privacy. However, for the privacy part,\nit only evaluates GPT models with a single attack method using\ndifferent prompting context lengths. It finds that GPT-4 leaks more\ndata than GPT-3.5, while our study aims to systematically compare\ndifferent series of LLMs (e.g., Llama and GPTs) with different factors.\nPan et al. [88] demonstrate the privacy risks of language models as-\nsuming that the adversary has access to the text embedding, which\ndoes not fit in the current era of LLMs as adversaries usually do\nnot have access to the embedding of training data. There are also\nmany studies [82, 92, 118, 133] that attack LLMs to demonstrate\nthe existence of data leakage, but they focus on proposing a single\nattack/defend method instead of systematically benchmarking the\nprivacy of LLMs to reveal the insights related to data privacy.\nTo our knowledge, there is currently no existing platform that\noffers a comprehensive and systematic assessment of privacy in\nLLMs. Addressing this significant gap, our study introduces the\nfirst toolkit specifically designed to facilitate a thorough evaluation\nof data privacy in LLMs. Our toolkit stands out due to its exten-\nsive coverage, encompassing a wide variety of LLMs and diverse\ndata types. Furthermore, it incorporates a multifaceted approach"}, {"title": "2.4 Privacy Enhancing Technologies for LLMs", "content": "There have been many data privacy protection approaches [8, 14,\n119, 120]. One popular approach is differential privacy (DP) [37, 39,\n120, 121], which guarantees that the output does not change with a\nhigh probability even though an input data record changes. DP has\nbeen used in the training of machine learning models [7, 93, 99],\nwhich is usually achieved by adding noises to gradients when using\nstochastic gradient descent. While using DP to retrain LLMs re-\nquires massive computing resources, it is possible to use DP to fine-\ntune LLMs as we will demonstrate in Section 3.6.2 and Section 4.4.\nBesides DP, we also exploit the potential usage of scrubbing [94],\nmachine unlearning [55, 115, 116], and defensive prompting [1] for\nthe data privacy protection in LLMs, which we will introduce in\nSection 3.6."}, {"title": "3 LLM-PBE: A COMPREHENSIVE TOOLKIT\nFOR ASSESSING THE PRIVACY OF LLMS", "content": "In this section, we introduce the design of LLM-PBE, an extensive\ntoolkit designed to aid researchers and developers in assessing the\nprivacy vulnerabilities of various LLMs. This toolkit incorporates\nvarious attack and defense methods tailored to the unique privacy\nchallenges posed by LLMs."}, {"title": "3.1 Design Goals", "content": "In developing our toolkit, we adhered to a set of clearly defined\ndesign goals, ensuring its effectiveness and relevance in benchmark-\ning the data privacy of LLMs.\nComprehensiveness: Our foremost objective is to deliver a com-\nprehensive toolkit for evaluating the data privacy of LLMs. To this\nend, we have incorporated a broad spectrum of components en-\ncompassing various datasets, stages of LLM development, diverse\nLLMs, a range of attack and defense strategies, and multiple assess-\nment metrics. For each of these aspects, we offer an extensive array\nof types and methodologies, thereby facilitating a systematic and\nthorough exploration of data privacy concerns in LLMs.\nUsability: We prioritize usability to ensure that our toolkit is easily\naccessible to both researchers and developers. By adopting a modu-\nlar design and providing Python-based interfaces, we have made\nour toolkit user-friendly and adaptable for diverse needs. Users can\nleverage the toolkit as a comprehensive end-to-end platform for\nprivacy risk assessment or selectively utilize its modules for spe-\ncific functions, such as data importing and analysis. This approach\nsimplifies the process of assessing data privacy in LLMs, making it\nmore approachable for users with varying levels of expertise.\nPortability: Recognizing the dynamic nature of the field, we have\ndesigned our toolkit with portability in mind. It is structured to\neasily adapt to new LLMs, datasets, and evolving metrics. Users can\neffortlessly integrate new models by providing local paths or links,\nthanks to our abstracted interfaces for model and data access. Addi-\ntionally, the modular nature of the toolkit allows for easy extension"}, {"title": "3.2 Overview", "content": "The structure and functionality of LLM-PBE are presented in Fig-\nure 2, showcasing our toolkit's modular design which enhances\nits usability and adaptability. LLM-PBE consists of several integral\ncomponents, each contributing to its comprehensive assessment\ncapabilities:\nData: To ensure thorough and contextually relevant testing, LLM-\nPBE includes a diverse array of datasets. These range from corporate\ncommunications in Enron to legal documents in ECHR, code reposi-\ntories from GitHub, and medical literature in PubMed. This variety\nallows for extensive testing across different data types including\nPII, domain knowledge, copyrighted work, and prompts, ensuring\na more robust and comprehensive evaluation of LLMs in various\nreal-world scenarios.\nModels: Addressing the complete lifecycle of LLMs, our toolkit\nencompasses stages from initial training, including pretraining,\nsupervised fine-tuning, and Reinforcement Learning from Human\nFeedback (RLHF), to practical applications like in-context learning.\nLLM-PBE provides seamless integration with a range of models,\nboth open-sourced, such as Llama-2, and closed-sourced, including\nGPT-3.5 and GPT-4. This feature allows users to conduct evaluations\non a wide spectrum of LLMs, catering to diverse research needs\nand interests.\nAttacks: Recognizing the potential for data leakage in LLMs through\nmemorization of sensitive information or prompts, our toolkit en-\ncompasses multiple attack methods. These include data extraction,\nmembership inference, prompt leakage attacks, and jailbreak at-\ntacks. By integrating these varied methods, LLM-PBE stays at the\nforefront of identifying and analyzing the latest privacy exploitation\ntechniques in LLMs.\nDefenses: In response to these privacy threats, LLM-PBE incorpo-\nrates an array of defense strategies. Notably, it includes differential\nprivacy techniques and machine unlearning approaches, among\nothers. This diversity in defense methods enables users to compre-\nhensively test and enhance the privacy resilience of LLMs against\na multitude of potential vulnerabilities.\nIn summary, LLM-PBE represents a state-of-the-art toolkit in\nthe field of LLM privacy assessment. Its extensive coverage of data\ntypes, lifecycle stages, models, attack, and defense strategies po-\nsitions it as a crucial resource for researchers and practitioners\naiming to understand and mitigate privacy risks in LLMs."}, {"title": "3.3 Data Collection", "content": "Our toolkit considers the following datasets from four different\naspects that might be used in the training or customization of\nLLMs:\nPersonally Identifiable Information (PII) The training corpus\nmay contain PII such as email addresses, which is a common con-\ncern. We incorporate the widely used Enron dataset [62], which\ncontains emails generated by employees of the Enron Corporation.\nMany studies [81, 114] have provided evidence that Enron has been\nused in the training of many LLMs such as GPTs. Thus, Enron is"}, {"title": "3.4 Model Integration", "content": "Our toolkit is designed to comprehensively address both the de-\nvelopment and customization stages of LLMs. In the development\nphase, LLMs typically undergo training processes that include pre-\ntraining, supervised fine-tuning, and RLHF, often utilizing a variety\nof data types. This data can range from general information to\nmore sensitive categories like PII, copyrighted content, and spe-\ncific domain knowledge. While general-purpose LLMs may not\nbe inherently tailored for specialized tasks, the customization of\nthese models through fine-tuning or in-context learning (e.g., the\ninsertion of instructional prompts) is a widespread approach. Our\ntoolkit is designed to assess potential data leakage at each of these\nstages, ensuring a thorough privacy evaluation.\nTo cater to a diverse range of LLM applications, our toolkit offers\nAPIs for both black-box models, such as GPT-3.5 and GPT-4, which\nprovide only inference services, and white-box models like Llama-2,\nwhere users have access to the model weights. Additionally, we\nhave developed abstractions for easy access to LLMs hosted on"}, {"title": "3.5 Privacy Assessment", "content": "How to assess the data privacy risks in LLMs is an important ongo-\ning problem. LLMs are usually released with providing inference\nservices, but without detailed information on privacy-related data\nprocessing. Like most existing studies on the privacy of LLMs, we\nmainly consider the following threat model in our study.\nThreat Model The adversary has access to the LLM as a black-box\nmodel, which takes a query as input and generates the correspond-\ning outputs.\nWe specifically examine two popular forms of data leakage in\nLLMs: 1) Leakage of training corpus due to data memorization dur-\ning the training or tuning of LLMs; 2) Breach of system/instruction\nprompts as they were imprinted into LLMs during the training\nor customization processes. Under these two leakages, we mainly\nconsider the corresponding attack methods including Data Extrac-\ntion Attacks (DEAs), Membership Inference Attacks (MIAs), and\nPrompt Leaking Attacks (PLAs). Additionally, since LLMs are typi-\ncally trained with instructional safety alignment to refuse unsafe\nqueries, we also incorporate Jailbreak Attacks (JAs) to circumvent\nthese restrictions."}, {"title": "3.5.1 Data Extraction Attacks.", "content": "DEAs aim to extract the training\ndata from language models. Given that vast amounts of web-collected\ndata are often used as training data for LLMs, this data could contain\nsensitive information, such as PII and copyrighted work, leading to\ngrowing concern over potential data leakage from LLMs.\nWe conclude that there are mainly two kinds of DEAs: query-\nbased methods (inference-time attack) [26, 28, 81] and poisoning-\nbased methods (training-time attack) [56, 91]. Query-based DEAs\ntypically query LLMs to make them output training data. Poisoning-\nbased methods modify the training data to insert poisons with a\nsimilar pattern as the target secret, and then easily extract this\nsecret during inference. Since poisoning-based DEAs have a strong\nassumption that the attacker can access the training data, we only\nconsider the query-based method in our toolkit. Specifically, we\nadopt the query-based method that prompts model with training\ndata prefixes [25] (e.g., query 'to: Alice <' to make LLMs output\nthe email address of Alice), and further explore different decoding\nconfigurations following [125]."}, {"title": "3.5.2 Membership Inference Attacks.", "content": "MIA was first proposed by\nShokri et al. [100] to serve as an empirical evaluation of private-\ninformation leakage in trained models. Given a trained model, an\nMIA adversary aims to discriminate the member samples that were\nused in training from the non-member samples by exploring the\noutputs of the model. Generally, the victim model is assumed to\nbe black-box when many models are deployed as API services. In\nthe black-box setting, the adversary can query and get prediction\nvectors from the model with knowledge of the input/output formats\nand ranges. The breach of membership could have a serious effect"}, {"title": "3.5.3 Prompt Leaking Attacks.", "content": "PLAs [53, 92] aim to steal system or\nuser prompts from LLMs. For example, a user instructed Bing Chat\nto \"Ignore previous instructions\" and reveal its system prompt [72].\nThese prompts could serve as important functionalities to enhance\nLLMs and make LLMs safer.\nPLAs have model-generated attack prompts [53] and manically\ncrafted attack prompts. For simplicity, we incorporate six simple and\neffective manually designed prompts [4, 72, 92] in our toolkit that\npotentially can lead to prompt leakage, which uses different ways\nto ask LLMs to print the previous prompts (e.g., directly printing,\ntranslation)."}, {"title": "3.5.4 Jailbreaking Attacks.", "content": "LLMs usually comply with the policies\nset by the developer to avoid breaching user privacy. These policies\nare typically given as extensive system prompts hidden from the end\nuser. However, users have developed many jailbreaking prompts\nto make LLMs bypass the policy restrictions [3], which increases\nthe risks of privacy leakage. Jailbreaking prompts, representing a\ndistinct attack approach for LLMs, warrant special attention.\nLike PLAs, JA prompts also have manually designed prompts\nand model-generated prompts. For manually designed prompts, we\nincorporate 15 JA prompting templates from public resources such as\nwebsites and papers [3, 59, 67, 118], which bypass the embedded\nsafety requirements by obfuscating the input prompts or restricting\nthe output format. For model-generated prompts, we use an existing\napproach [31] to generate the JA prompts using LLMs. Specifically,\nit uses one LLM to generate prompts, while using another LLM\nto judge whether the generated prompt successfully jailbreaks the\ntarget model. The generated prompts and responses are appended\nto the attack prompts in each round until successful jailbreaking."}, {"title": "3.6 Privacy Enhancing Technologies", "content": "To systematically assess the data privacy of LLMs, it is also impor-\ntant to understand whether the data can be protected by Privacy\nEnhancing Technologies (PETs). We consider four practical ap-\nproaches: scrubbing, differential privacy, machine unlearning, and\ndefensive prompting."}, {"title": "3.6.1 Scrubbing.", "content": "When PII is the major privacy concern, scrubbing\nis a practical method that directly removes the recognized PII to\navoid privacy leakage [94]. The key steps include tagging PII by pre-\ntrained Name-Entity Recognition (ENR) models and then removing\nor replacing tagged PII. The pre-trained models could be obtained\nfrom public Python packages, such as Flair [9] or spaCy [112]. For\nexample, Lukas et al. [73] replace the names with \"[NAME]\".\nThe scrubbing may retain partial semantics of the PII in the sentence\nand therefore trade off privacy and utility. Therefore, the model will\nbe robust to scrubbing when further fine-tuned on private scrubbed\ndata. In our toolkit, we adopt Flair for data scrubbing due to its\npopularity."}, {"title": "3.6.2 Differential Privacy.", "content": "Differential privacy (DP) [37, 38] is a\ngolden standard for bounding privacy risks. Depending on the\ndefinition of privacy, DP has different notions. Formally, we use\nD, D' \u2208 \\mathbb{N}_X to denote two datasets with an unspecified size over\nspace X. We call two datasets D and D' adjacent (denoted as D ~ D')\nif there is only one data point differing one from the other, e.g.,\nD = D' \\cup {z} for some z \u2208 X.\nDP has been applied in the training of machine learning models\nto protect training data [7]. However, since the training of LLMs\nrequires a long time with massive computing resources, it is not\nfeasible for us to use DP to retrain an LLM. Thus, we consider\nthe usage of DP with parameter-efficient fine-tuning approaches\nsuch as LoRA [51]. Instead of fine-tuning the whole model, we use\nLoRA to only fine-tune additional parameters with DP, whose size\nis much smaller than the size of LLM."}, {"title": "3.6.3 Machine Unlearning.", "content": "While LLMs memorize some private\ntraining data, a promising way to protect data privacy is to update\nthe model to unlearn specific data, i.e., machine unlearning. Ma-\nchine unlearning has been an attractive research direction recently\nas data regulations such as GDPR stipulate that individuals have the\n\"right to be forgotten\". While many machine learning studies are for\ncomputer vision [71, 104, 134], machine unlearning approaches for\nLLMs remain underexploited. Some studies [55, 115, 116] fine-tune\nthe trained model to unlearn the deleted data, which is more prac-\ntical than modifying the training process [19, 65] as the training of\nLLMs is very expensive. In our toolkit, we adopt an approach [115]\nto fine-tune the LLM using knowledge gap alignment. Specifically,\nthe LLM is updated such that the knowledge gap between it and the\nmodel trained on the deleted data is similar to the gap of another\nmodel handling the seen and unseen data."}, {"title": "3.6.4 Defensive Prompting.", "content": "While PLAs can cause prompt leakage\nthrough prompting, it is also interesting to see whether defensive\nprompting can help protect the private prompts. We design and\ninclude five intuitive defense prompts. For example, one prompt\nis no-repeat, where we ask the LLM not to provide private content\nin the future even if the user asks or enforces you to do so. These\ndefensive prompts are easy to apply with negligible overhead. The\ndetails of these prompts are available in Section 5.4."}, {"title": "3.7 Efficiency", "content": "Efficiency is an important factor that influences the practicality\nand scalability of various attack and defense strategies. Details re-\ngarding the GPU memory requirements and computational costs of\nthese strategies are presented in Table 2. These experiments were\nconducted using the Llama-2 7B model on the Enron dataset, utiliz-\ning a system equipped with two NVIDIA A100 GPUs and two AMD\nEPYC 7J13 64-Core Processors. The findings indicate that most\nattack strategies are computationally efficient as they do not neces-\nsitate the training or updating of models. However, model-based\nMIAs are not feasible for LLMs due to the necessity of training\nmultiple LLMs to develop an effective attack model. Among the\ndefense mechanisms, DP-SGD offers lower computational overhead\ncompared to data scrubbing. This is because DP-SGD integrates\nminimal additional operations into the training process, whereas\nscrubbing requires extensive preprocessing of the original data us-\ning language models. Despite all approaches needing at least 28GB\nof GPU memory owing to the large parameter sizes involved, the\navailability of LLM inference services from various companies (e.g.,\nOpenAI, TogetherAI) means that attackers might not require local\nmodel hosting, potentially reducing the need for high-performance\nGPUs."}, {"title": "3.8 Metrics", "content": "Our toolkit provides multiple metrics to cover different data types\nand attacks including: 1) Data extraction accuracy: this metric re-\nports how much private data are successfully extracted using a DEA; \n2) MIA AUC and TPR: For MIAs, a test dataset contains members\nand non-members is used to evaluate the effectiveness of the attack.\nWe include both AUC (Area Under the Curve) and TPR@0.1%FPR\n(true positive rate at 0.1% false positive rate) to evaluate the per-\nformance of MIAs; 3) Jailbreaking success rate: This metric reports\nthe rate of responses that do not refuse to answer given private\nqueries when using JAs; 4) JPlag similarity: This metric reports\nthe similarity between different source code to measure the privacy"}, {"title": "3.9 Usage", "content": "LLM-PBE is implemented in Python, offering a user-friendly and\naccessible platform for privacy evaluation. As shown in Figure 3,\nusers can effortlessly import different modules from our toolkit to\nassess and analyze the privacy risks of LLMs. This implementation\nnot only simplifies the evaluation process but also enables users\nto customize their assessments based on specific needs or research\nfocuses. Whether for academic research or practical development,\nLLM-PBE serves as an invaluable tool in the ongoing effort to\nsafeguard privacy in the realm of Large Language Models."}, {"title": "4 LEAKAGE OF TRAINING DATA", "content": "In this section, we conduct extensive experiments to assess the\nprivacy of training data of LLMs with existing attack methods,\nincluding data used for pertaining and fine-tuning. We focus on\nanswering the following research questions: 1) Does the privacy risks\nof in LLMs correspond proportionally with their increasing scale and\neffectiveness? 2) How are different data characteristics associated with\nthe privacy risks of LLMs? 3) Are there practical privacy-preserving\napproaches when deploying LLMs? Due to the page limit, we present\nrepresentative experiments in the main paper and put additional\nresults in Appendix."}, {"title": "4.1 Experimental Setup", "content": "Attack Approaches We evaluate the privacy risks of training data\nmainly with two attack methodologies, including 1) Data Extraction\nAttacks (DEAs): we consider the query-based method that prompts\nmodel with training data prefixes [25], and further explore different\ndecoding configurations following [125]. 2) Membership Inference\nAttacks (MIAs): We utilize several recent attack methods on LLMs.\nPPL thresholds perplexity to predict membership. Refer computes\nthe ratio of the log-perplexity of the tested model against a reference\nmodel [28]. Instead of using log-perplexity, LiRA uses the ratio of\nlikelihood instead [23, 79, 117, 122]. LiRA assumes the availability\nof high-quality data distributed similarly to the training set, which\nwas thought to be impractical [106]. Therefore, we follow [76] to\nuse the pre-trained model as a reference. MIN-K [98] determines the"}, {"title": "4.2 Effect of Model Size", "content": "The continuous increase in model size raises an important question\nabout the corresponding changes in privacy risks associated with\nthese models. To explore this, we employ DEAs to assess the privacy\nrisks of Pythia models [16] of varying sizes on Enron, as distinct\nversions of Pythia are trained on identical datasets (including Enron)\nusing the same sequence of training.\nThe results are presented in Figure 4. We use the ARC-Easy\n(accuracy on the AI2's Reasoning Challenge Easy dataset) [32] to\nreflect the utility of LLMs. The results highlight a significant pattern:\nas the model size expands, both the utility of the model and the\naccuracy of the complete email address extraction (as shown in DEA\nEnron) increase. Moreover, the rate of increase in data extraction\naccuracy on Enron is even higher than the rate of increase in model\nutility, indicating a potentially higher risk in the future as models\ncontinue to scale up.\nAs demonstrated in existing studies [101, 107], LLMs can also\ninfer private information from the input context. To investigate\nwhether memorization or reasoning primarily contributes to DEAs,\nwe also conduct DEAs on a synthetic email dataset that the model\nhas never seen (as shown in DEA Synthetic). From the results, we\nobserve that DEA accuracy is zero in most cases, indicating that\nthe model is not able to infer complete email addresses accurately\nthrough reasoning. Thus, LLMs indeed memorize training data,\nwhich poses potential privacy risks."}, {"title": "4.3 Effect of Data Characteristics", "content": "We conduct experiments to study the effect of different data char-\nacteristics including 1) data length, 2) position of private data, 3)\ndata type, and 4) pretraining data size."}, {"title": "4.4 Practicality of PETs on Fine-tuning of LLMs", "content": "We investigate the effectiveness of scrubbing in mitigating privacy\nrisks. Specifically, we fine-tune Llama-2 7b on the ECHR dataset for\n4 epochs and use four MIA approaches (PPL, Refer, LiRA, and MIN-K)\nwith ECHR to assess privacy leakage from the fine-tuned model. \nOur focus is on the impact of these techniques on privacy leakage,\nwithout considering potential overfitting. The results, presented\nin Table 4, indicate that scrubbing and DP can effectively reduce\nthe MIA AUC. However, we observe that the scrubbing process\nsignificantly degrades model performance, highlighting a critical\nchallenge in balancing privacy protection and model utility."}, {"title": "4.5 Privacy Risks over Different Attacks", "content": "We compare different types of attacks in Table 5, including two\ntypes of data extraction attacks and two types of jailbreak attacks.\nSpecifically, for DEAs, besides the query-based attacks, we eval-\nuate existing poisoning-based attack [89], which injects fake PII\ninto the finetuning data with similar contextual patterns as PII\nin the pretraining data to exacerbate LLM memorization. For JAs,\nbesides manually designed prompts, we have added model-based\napproaches [31] to generate the attack prompts. From Table 5, we\nobserve that 1) model-generated attack prompts are more effective\nthan manually designed attack prompts; 2) this poisoning-based"}, {"title": "5 LEAKAGE OF PROMPTS", "content": "Leaking in-store system prompts can cause significant financial\nlosses. In this section, we conduct a comprehensive evaluation of\nprompt privacy using different PLAs methods, models, and potential\ndefenses. We focus on answering the following research questions: \n1) Is prompt easily leaked using attack prompts? 2) How does the\nrisk of prompt leakage vary across different LLMs? 3) Is it possible to\nprotect the prompts by using defensive prompting?"}, {"title": "5.1 Experimental Setup", "content": "Attack Approaches. The adversary retrieves the system prompts\nby querying LLMs by PLA prompts. In total, 6 attack prompts are\nevaluated. Prompt contents are included in Appendix C.1. We ex-\nclude other prompts that have minor variants. The design principles\nof the prompts are explained as follows. 1) What-was [72]: The at-\ntacker simply lets LLMs ignore previous instructions and ask what\nwere the words at the beginning. 2) Translation: The attacker first\nasks LLM to translate 'previous content' into another language and\nthen translate it back to English. 3) Repeat with Head [4]: The at-\ntacker asks LLMs to repeat the words above starting with the phrase\n\"You are X\" where X could be ChatGPT, AI, Assistant or any entity. \n4) Encode base64: The attacker lets LLMs write code to encode all\nprevious content into base64-encoded. 5) Spell Checking [92]: The"}, {"title": "5.2 Comparison of Different Attacks", "content": "In Figure 7, we report the average FuzzRate for each attack. For\nGPT-4 and GPT-3.5, the most risky attack is by repeat_w_head.\nThis is probably because many system prompts start with \"You are\nChatGPT\" or its variant. Note that the default system prompt of\nChatGPT also starts with \"You are ChatGPT\". It is possible that\nGPT-4 was pre-trained or ever aligned with the head. In Figure 8,\nwe report the ratio of samples that have FuzzRate over 90. The\ntranslate_french attack becomes stronger for GPT-4. Consistently,\nthe ignore_print attack is more effective for larger models, like\nLlama-2-70b and GPT-4, than smaller ones."}, {"title": "5.3 Comparison of Different Models", "content": "In Table 6, we compare both GPT and open-source models on\nthe BlackFriday dataset. For each system prompt, we identify the\nbest prompt using 8 attack prompts and report the ratio of system\nprompts leaked at FuzzRate over 90 (LR@90FR). Vicuna-13b-v1.5\nand Llama-2-70b are the most vulnerable, performing worse than\nGPT-4. Approximately 64% of system prompts are leaked with\na FuzzRate over 99. At 99.9FR, Vicuna-13b-v1.5 leaks half of the"}, {"title": "5.4 Effectiveness of Defensive Prompting", "content": "A possible defense against the PLA may be appending defensive\ninstructions to the system prompt. We evaluate five defense prompts\nagainst different PLAs on the GPT-4 model since GPT-4 is often\nused with the GPT Stores. The detailed prompts are as follows:\n1"}]}