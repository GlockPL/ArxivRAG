{"title": "LLM-PBE: Assessing Data Privacy in Large Language Models", "authors": ["Qinbin Li", "Junyuan Hong", "Chulin Xie", "Jeffrey Tan", "Rachel Xin", "Junyi Hou", "Xavier Yin", "Zhun Wang", "Dan Hendrycks", "Zhangyang Wang", "Bo Li", "Bingsheng He", "Dawn Song"], "abstract": "Large Language Models (LLMs) have become integral to numerous\ndomains, significantly advancing applications in data management,\nmining, and analysis. Their profound capabilities in processing\nand interpreting complex language data, however, bring to light\npressing concerns regarding data privacy, especially the risk of un-\nintentional training data leakage. Despite the critical nature of this\nissue, there has been no existing literature to offer a comprehensive\nassessment of data privacy risks in LLMs. Addressing this gap, our\npaper introduces LLM-PBE, a toolkit crafted specifically for the\nsystematic evaluation of data privacy risks in LLMs. LLM-PBE is\ndesigned to analyze privacy across the entire lifecycle of LLMs,\nincorporating diverse attack and defense strategies, and handling\nvarious data types and metrics. Through detailed experimentation\nwith multiple LLMs, LLM-PBE facilitates an in-depth exploration of\ndata privacy concerns, shedding light on influential factors such as\nmodel size, data characteristics, and evolving temporal dimensions.\nThis study not only enriches the understanding of privacy issues in\nLLMs but also serves as a vital resource for future research in the\nfield. Aimed at enhancing the breadth of knowledge in this area,\nthe findings, resources, and our full technical report are made avail-\nable at https://llm-pbe.github.io/, providing an open platform for\nacademic and practical advancements in LLM privacy assessment.", "sections": [{"title": "1 INTRODUCTION", "content": "In the contemporary landscape of technology, Large Language Mod-\nels (LLMs) [83, 87, 102, 105] have rapidly ascended to prominence,\nrevolutionizing the way we interact with data. These advanced\nmodels are not just tools for natural language processing; they have\nbecome integral in data management [41, 42, 61, 108-110], and\nmining [15, 45, 132]. LLMs, with their sophisticated algorithms, are\ncapable of extracting meaningful insights from vast datasets, mak-\ning complex data more accessible and actionable. This has led to\ntheir widespread adoption across various domains, fundamentally\naltering the approach to data handling and information processing.\nThere have been some earlier discussions about the impact of\nLLMs on database research [10, 41, 136]. Among them, Amer-Yahia\net al. [10] and Zhou et al. [136] pointed out that data privacy is an\nimportant research challenge in LLMs and databases. It advocates\ndeveloping privacy-preserving schemes to help LLMs to protect the\nprivacy of individuals. In contrast, we aim to thoroughly understand\nand analyze the data privacy leakage in LLMs.\nThe extensive use of LLMs brings forth significant data privacy\nconcerns. Trained on massive datasets, these models are at risk of\nunintentionally exposing sensitive information. Instances where\nLLMs have inadvertently revealed personal details such as email\naddresses and phone numbers [26, 28, 81] from training data in\ntheir outputs have sparked serious discussions about the potential\nmisuse of private data and subsequent breaches of privacy. Another\nreal-world example is that The New York Times discovered that mil-\nlions of their articles were utilized in the training of ChatGPT [78]\nby querying the model, which underscores the severity of data\nbreaches associated with LLMs.\nDespite these concerns, there exists a notable gap in the cur-\nrent research landscape: a lack of systematic analysis regarding\nthe privacy of LLMs. Existing studies [82, 88, 92, 114, 118, 133]\nhave the following limitations: 1) Limited evaluated data types:\nWhile the deployment of LLMs involves multiple stages and dif-\nferent types of data, most studies [114, 118, 133] only consider the\npotential leakage of a single type of data (e.g., Personally identifi-\nable information (PII), prompts); 2) Limited models: While there\nare a rich set of LLMs currently, many analyses [92, 118, 133] are\nconstrained to a few LLMs or smaller models such as GPT-2. 3)\nLimited attack approaches: Existing studies usually only con-\nsider a single attack method (e.g., data extraction attack [26, 81])\nand do not cover a broad range of attack metrics; 4) Limited con-\nsideration of privacy protection approaches: Existing stud-\nies [82, 88, 92, 114, 118, 133] usually lack the consideration of the\neffect of using privacy protection approaches on the data leakage. In\nsummary, while these studies have touched upon specific aspects\nof privacy risks, a comprehensive evaluation encompassing the\ndiverse facets of LLMs' data privacy implications remains largely\nunexplored. This gap is evident in the fragmented approach of ex-\nisting research, which often fails to consider the multi-dimensional\nnature of privacy risks in LLMs.\nTo address this gap, we developed LLM-PBE (LLM Privacy BEnch-\nmark), a specialized toolkit for evaluating privacy risks in LLMs.\nThis innovative solution enables a systematic and comprehensive\nassessment of privacy vulnerabilities, equipped to analyze various\nmodels, attack methodologies, defense strategies, and diverse data\ntypes and metrics. LLM-PBE considers potential data leakage across\nthe entire lifecycle of LLMs, including pretrained data, fine-tuned\ndata, and custom prompts. It provides APIs for accessing LLMs\nfrom platforms like OpenAI, TogetherAI, and HuggingFace and\nintegrates a broad spectrum of attack and defense approaches. A\ncomparison between LLM-PBE and existing studies is presented in\nTable 1.\nEmploying this toolkit, we conducted extensive studies on nu-\nmerous LLMs to analyze their data privacy aspects. Our experi-\nments were meticulously designed to cover a broad spectrum of\nscenarios, offering a deep dive into how different LLMs handle\nprivacy concerns. We investigated three primary factors that in-\nfluence the privacy risks of LLMs: model size, data characteristics,\nand time. The analysis of model size examines how the scale of an\nLLM impacts its vulnerability to privacy breaches. The study of\ndata characteristics focuses on how the nature of the training data,\nincluding its diversity and sensitivity, affects the model's privacy\nrisks. Lastly, the temporal aspect examines how privacy risks evolve\nover time with the development of LLMs. In addition to the attacks,\nwe also investigated whether existing privacy-enhancing technolo-\ngies such as differential privacy [37] would be helpful in mitigating\nthe privacy risks of LLMs. This comprehensive examination aims\nto shed light on the multifaceted nature of privacy risks in LLMs."}, {"title": "2 PRELIMINARIES AND RELATED WORK", "content": "2.1 Large Language Models\nLLMs [83, 87, 102, 105] are a class of advanced models designed to\nunderstand, interpret, and generate human-like text, representing\na significant milestone in the field of NLP. Fundamentally, these\nmodels are built on sophisticated neural network architectures,\nprimarily transformer-based [113] designs, known for their deep\nlearning capabilities in handling sequential data. The architecture\nof LLMs typically involves multiple layers of self-attention mech-\nanisms, which enable the models to process and generate text by\neffectively capturing the context and nuances of language over\nlarge spans of text. The applications of LLMs are remarkably di-\nverse, extending far beyond basic text generation. In the realm\nof data management, LLMs have revolutionized information re-\ntrieval, making it possible to extract and synthesize information\nfrom unstructured data sources with unprecedented efficiency. The\nemergence of LLMs has thus not only pushed the boundaries of\nmachine understanding of language but also opened up new possi-\nbilities for data analysis and interaction, marking a transformative\nphase in the intersection of AI, linguistics, and data science.\nTraining of LLMs The training of LLMs usually involves three\nstages: pretraining, supervised fine-tuning, and Reinforcement Learn-\ning from Human Feedback (RLHF) [86, 137]. The first stage is pre-\ntraining, where the model is trained on a vast and diverse dataset.\nThis stage involves unsupervised learning [46], where the model\nlearns to understand and predict language patterns by processing\nextensive amounts of text data. The goal here is to develop a broad\nunderstanding of language and its nuances.\nFollowing pretraining, the model undergoes supervised fine-\ntuning. In this stage, the LLM is further trained on more specific\ndatasets, often tailored to particular tasks or domains. This process\nadjusts and refines the model's parameters to align with specific\nobjectives, such as translation, question-answering, or topic clas-\nsification. The fine-tuning process enables the model to transfer\nits general language understanding from the pretraining phase to\nspecialized tasks, enhancing its accuracy in practical applications.\nThe final stage involves RLHF, a more recent development in the\ntraining process. This stage optimizes the model's outputs based on\nqualitative feedback from human evaluators. By interacting with\nusers and incorporating their responses, the LLM learns to generate\noutputs that are not only accurate and contextually relevant but also\naligned with human preferences and nuances in communication.\nThis feedback loop allows for continuous improvement of the model,\nensuring its outputs remain high-quality and user-centric."}, {"title": "2.2 Data Privacy Leakage in LLMs", "content": "Data privacy in the context of LLMs concerns the protection of\nsensitive information that these models might access, learn, and po-\ntentially disclose during their operation. This encompasses personal\ndata, confidential information, and any content that, if exposed,\ncould lead to privacy breaches. The challenge in ensuring data pri-\nvacy in LLMs arises from their training process, which involves\nlarge-scale datasets that can contain such sensitive information.\nEnsuring that these models respect user privacy and adhere to\ndata protection standards is thus a critical concern. While devel-\nopers usually provide inference services to LLMs without detailed\ninformation on the data collection and processing, numerous stud-\nies [24, 28, 56, 91, 125] have shown that sensitive data may leak\nby just prompting LLMs as demonstrated in Figure 1. Thus, it is\nimportant to systematically assess the data privacy risks of LLMs."}, {"title": "2.3 Privacy Assessment of LLMs", "content": "As detailed in Table 1, current research in the field typically evalu-\nates the privacy of LLMs using a limited range of models, datasets,\nand attack methodologies. For example, DecodingTrust [114] eval-\nuates the trustworthiness in GPT models on many aspects such\nas robustness, fairness, and privacy. However, for the privacy part,\nit only evaluates GPT models with a single attack method using\ndifferent prompting context lengths. It finds that GPT-4 leaks more\ndata than GPT-3.5, while our study aims to systematically compare\ndifferent series of LLMs (e.g., Llama and GPTs) with different factors.\nPan et al. [88] demonstrate the privacy risks of language models as-\nsuming that the adversary has access to the text embedding, which\ndoes not fit in the current era of LLMs as adversaries usually do\nnot have access to the embedding of training data. There are also\nmany studies [82, 92, 118, 133] that attack LLMs to demonstrate\nthe existence of data leakage, but they focus on proposing a single\nattack/defend method instead of systematically benchmarking the\nprivacy of LLMs to reveal the insights related to data privacy.\nTo our knowledge, there is currently no existing platform that\noffers a comprehensive and systematic assessment of privacy in\nLLMs. Addressing this significant gap, our study introduces the\nfirst toolkit specifically designed to facilitate a thorough evaluation\nof data privacy in LLMs. Our toolkit stands out due to its exten-\nsive coverage, encompassing a wide variety of LLMs and diverse\ndata types. Furthermore, it incorporates a multifaceted approach"}, {"title": "2.4 Privacy Enhancing Technologies for LLMs", "content": "There have been many data privacy protection approaches [8, 14,\n119, 120]. One popular approach is differential privacy (DP) [37, 39,\n120, 121], which guarantees that the output does not change with a\nhigh probability even though an input data record changes. DP has\nbeen used in the training of machine learning models [7, 93, 99],\nwhich is usually achieved by adding noises to gradients when using\nstochastic gradient descent. While using DP to retrain LLMs re-\nquires massive computing resources, it is possible to use DP to fine-\ntune LLMs as we will demonstrate in Section 3.6.2 and Section 4.4.\nBesides DP, we also exploit the potential usage of scrubbing [94],\nmachine unlearning [55, 115, 116], and defensive prompting [1] for\nthe data privacy protection in LLMs, which we will introduce in\nSection 3.6."}, {"title": "3 LLM-PBE: A COMPREHENSIVE TOOLKIT\nFOR ASSESSING THE PRIVACY OF LLMS", "content": "In this section, we introduce the design of LLM-PBE, an extensive\ntoolkit designed to aid researchers and developers in assessing the\nprivacy vulnerabilities of various LLMs. This toolkit incorporates\nvarious attack and defense methods tailored to the unique privacy\nchallenges posed by LLMs."}, {"title": "3.1 Design Goals", "content": "In developing our toolkit, we adhered to a set of clearly defined\ndesign goals, ensuring its effectiveness and relevance in benchmark-\ning the data privacy of LLMs.\nComprehensiveness: Our foremost objective is to deliver a com-\nprehensive toolkit for evaluating the data privacy of LLMs. To this\nend, we have incorporated a broad spectrum of components en-\ncompassing various datasets, stages of LLM development, diverse\nLLMs, a range of attack and defense strategies, and multiple assess-\nment metrics. For each of these aspects, we offer an extensive array\nof types and methodologies, thereby facilitating a systematic and\nthorough exploration of data privacy concerns in LLMs.\nUsability: We prioritize usability to ensure that our toolkit is easily\naccessible to both researchers and developers. By adopting a modu-\nlar design and providing Python-based interfaces, we have made\nour toolkit user-friendly and adaptable for diverse needs. Users can\nleverage the toolkit as a comprehensive end-to-end platform for\nprivacy risk assessment or selectively utilize its modules for spe-\ncific functions, such as data importing and analysis. This approach\nsimplifies the process of assessing data privacy in LLMs, making it\nmore approachable for users with varying levels of expertise.\nPortability: Recognizing the dynamic nature of the field, we have\ndesigned our toolkit with portability in mind. It is structured to\neasily adapt to new LLMs, datasets, and evolving metrics. Users can\neffortlessly integrate new models by providing local paths or links,\nthanks to our abstracted interfaces for model and data access. Addi-\ntionally, the modular nature of the toolkit allows for easy extension"}, {"title": "3.2 Overview", "content": "The structure and functionality of LLM-PBE are presented in Fig-\nure 2, showcasing our toolkit's modular design which enhances\nits usability and adaptability. LLM-PBE consists of several integral\ncomponents, each contributing to its comprehensive assessment\ncapabilities:\nData: To ensure thorough and contextually relevant testing, LLM-\nPBE includes a diverse array of datasets. These range from corporate\ncommunications in Enron to legal documents in ECHR, code reposi-\ntories from GitHub, and medical literature in PubMed. This variety\nallows for extensive testing across different data types including\nPII, domain knowledge, copyrighted work, and prompts, ensuring\na more robust and comprehensive evaluation of LLMs in various\nreal-world scenarios.\nModels: Addressing the complete lifecycle of LLMs, our toolkit\nencompasses stages from initial training, including pretraining,\nsupervised fine-tuning, and Reinforcement Learning from Human\nFeedback (RLHF), to practical applications like in-context learning.\nLLM-PBE provides seamless integration with a range of models,\nboth open-sourced, such as Llama-2, and closed-sourced, including\nGPT-3.5 and GPT-4. This feature allows users to conduct evaluations\non a wide spectrum of LLMs, catering to diverse research needs\nand interests.\nAttacks: Recognizing the potential for data leakage in LLMs through\nmemorization of sensitive information or prompts, our toolkit en-\ncompasses multiple attack methods. These include data extraction,\nmembership inference, prompt leakage attacks, and jailbreak at-\ntacks. By integrating these varied methods, LLM-PBE stays at the\nforefront of identifying and analyzing the latest privacy exploitation\ntechniques in LLMs.\nDefenses: In response to these privacy threats, LLM-PBE incorpo-\nrates an array of defense strategies. Notably, it includes differential\nprivacy techniques and machine unlearning approaches, among\nothers. This diversity in defense methods enables users to compre-\nhensively test and enhance the privacy resilience of LLMs against\na multitude of potential vulnerabilities.\nIn summary, LLM-PBE represents a state-of-the-art toolkit in\nthe field of LLM privacy assessment. Its extensive coverage of data\ntypes, lifecycle stages, models, attack, and defense strategies po-\nsitions it as a crucial resource for researchers and practitioners\naiming to understand and mitigate privacy risks in LLMs."}, {"title": "3.3 Data Collection", "content": "Our toolkit considers the following datasets from four different\naspects that might be used in the training or customization of\nLLMs:\nPersonally Identifiable Information (PII) The training corpus\nmay contain PII such as email addresses, which is a common con-\ncern. We incorporate the widely used Enron dataset [62], which\ncontains emails generated by employees of the Enron Corporation.\nMany studies [81, 114] have provided evidence that Enron has been\nused in the training of many LLMs such as GPTs. Thus, Enron is"}, {"title": "3.4 Model Integration", "content": "Our toolkit is designed to comprehensively address both the de-\nvelopment and customization stages of LLMs. In the development\nphase, LLMs typically undergo training processes that include pre-\ntraining, supervised fine-tuning, and RLHF, often utilizing a variety\nof data types. This data can range from general information to\nmore sensitive categories like PII, copyrighted content, and spe-\ncific domain knowledge. While general-purpose LLMs may not\nbe inherently tailored for specialized tasks, the customization of\nthese models through fine-tuning or in-context learning (e.g., the\ninsertion of instructional prompts) is a widespread approach. Our\ntoolkit is designed to assess potential data leakage at each of these\nstages, ensuring a thorough privacy evaluation.\nTo cater to a diverse range of LLM applications, our toolkit offers\nAPIs for both black-box models, such as GPT-3.5 and GPT-4, which\nprovide only inference services, and white-box models like Llama-2,\nwhere users have access to the model weights. Additionally, we\nhave developed abstractions for easy access to LLMs hosted on\nopen platforms such as Hugging Face [5] and Together AI [6]. For\nuser convenience, accessing these LLMs is streamlined and requires\nonly the API key or the path to the downloaded models. This in-\ntegration approach in our toolkit facilitates seamless interaction\nwith various LLMs, making it an adaptable and user-friendly tool\nfor comprehensive privacy assessment in LLMs."}, {"title": "3.5 Privacy Assessment", "content": "How to assess the data privacy risks in LLMs is an important ongo-\ning problem. LLMs are usually released with providing inference\nservices, but without detailed information on privacy-related data\nprocessing. Like most existing studies on the privacy of LLMs, we\nmainly consider the following threat model in our study.\nThreat Model The adversary has access to the LLM as a black-box\nmodel, which takes a query as input and generates the correspond-\ning outputs.\nWe specifically examine two popular forms of data leakage in\nLLMs: 1) Leakage of training corpus due to data memorization dur-\ning the training or tuning of LLMs; 2) Breach of system/instruction\nprompts as they were imprinted into LLMs during the training\nor customization processes. Under these two leakages, we mainly\nconsider the corresponding attack methods including Data Extrac-\ntion Attacks (DEAs), Membership Inference Attacks (MIAs), and\nPrompt Leaking Attacks (PLAs). Additionally, since LLMs are typi-\ncally trained with instructional safety alignment to refuse unsafe\nqueries, we also incorporate Jailbreak Attacks (JAs) to circumvent\nthese restrictions."}, {"title": "3.5.1 Data Extraction Attacks", "content": "DEAs aim to extract the training\ndata from language models. Given that vast amounts of web-collected\ndata are often used as training data for LLMs, this data could contain\nsensitive information, such as PII and copyrighted work, leading to\ngrowing concern over potential data leakage from LLMs.\nWe conclude that there are mainly two kinds of DEAs: query-\nbased methods (inference-time attack) [26, 28, 81] and poisoning-\nbased methods (training-time attack) [56, 91]. Query-based DEAs\ntypically query LLMs to make them output training data. Poisoning-\nbased methods modify the training data to insert poisons with a\nsimilar pattern as the target secret, and then easily extract this\nsecret during inference. Since poisoning-based DEAs have a strong\nassumption that the attacker can access the training data, we only\nconsider the query-based method in our toolkit. Specifically, we\nadopt the query-based method that prompts model with training\ndata prefixes [25] (e.g., query 'to: Alice <' to make LLMs output\nthe email address of Alice), and further explore different decoding\nconfigurations following [125]."}, {"title": "3.5.2 Membership Inference Attacks", "content": "MIA was first proposed by\nShokri et al. [100] to serve as an empirical evaluation of private-\ninformation leakage in trained models. Given a trained model, an\nMIA adversary aims to discriminate the member samples that were\nused in training from the non-member samples by exploring the\noutputs of the model. Generally, the victim model is assumed to\nbe black-box when many models are deployed as API services. In\nthe black-box setting, the adversary can query and get prediction\nvectors from the model with knowledge of the input/output formats\nand ranges. The breach of membership could have a serious effect"}, {"title": "3.5.3 Prompt Leaking Attacks", "content": "PLAs [53, 92] aim to steal system or\nuser prompts from LLMs. For example, a user instructed Bing Chat\nto \"Ignore previous instructions\" and reveal its system prompt [72].\nThese prompts could serve as important functionalities to enhance\nLLMs and make LLMs safer.\nPLAs have model-generated attack prompts [53] and manically\ncrafted attack prompts. For simplicity, we incorporate six simple and\neffective manually designed prompts [4, 72, 92] in our toolkit that\npotentially can lead to prompt leakage, which uses different ways\nto ask LLMs to print the previous prompts (e.g., directly printing,\ntranslation)."}, {"title": "3.5.4 Jailbreaking Attacks", "content": "LLMs usually comply with the policies\nset by the developer to avoid breaching user privacy. These policies\nare typically given as extensive system prompts hidden from the end\nuser. However, users have developed many jailbreaking prompts\nto make LLMs bypass the policy restrictions [3], which increases\nthe risks of privacy leakage. Jailbreaking prompts, representing a\ndistinct attack approach for LLMs, warrant special attention.\nLike PLAs, JA prompts also have manually designed prompts\nand model-generated prompts. For manually designed prompts, we\nincorporate 15 JA prompting templates from public resources such as\nwebsites and papers [3, 59, 67, 118], which bypass the embedded\nsafety requirements by obfuscating the input prompts or restricting\nthe output format. For model-generated prompts, we use an existing\napproach [31] to generate the JA prompts using LLMs. Specifically,\nit uses one LLM to generate prompts, while using another LLM\nto judge whether the generated prompt successfully jailbreaks the\ntarget model. The generated prompts and responses are appended\nto the attack prompts in each round until successful jailbreaking."}, {"title": "3.6 Privacy Enhancing Technologies", "content": "To systematically assess the data privacy of LLMs, it is also impor-\ntant to understand whether the data can be protected by Privacy\nEnhancing Technologies (PETs). We consider four practical ap-\nproaches: scrubbing, differential privacy, machine unlearning, and\ndefensive prompting."}, {"title": "3.6.1 Scrubbing", "content": "When PII is the major privacy concern, scrubbing\nis a practical method that directly removes the recognized PII to\navoid privacy leakage [94]. The key steps include tagging PII by pre-\ntrained Name-Entity Recognition (ENR) models and then removing\nor replacing tagged PII. The pre-trained models could be obtained\nfrom public Python packages, such as Flair [9] or spaCy [112]. For\nexample, Lukas et al. [73] replace the names with \"[NAME]\". The\nscrubbing may retain partial semantics of the PII in the sentence\nand therefore trade off privacy and utility. Therefore, the model will\nbe robust to scrubbing when further fine-tuned on private scrubbed\ndata. In our toolkit, we adopt Flair\u00b3 for data scrubbing due to its\npopularity."}, {"title": "3.6.2 Differential Privacy", "content": "Differential privacy (DP) [37, 38] is a\ngolden standard for bounding privacy risks. Depending on the\ndefinition of privacy, DP has different notions. Formally, we use\nD, D' \u2208 NX to denote two datasets with an unspecified size over\nspace X. We call two datasets D and D' adjacent (denoted as D ~ D')\nif there is only one data point differing one from the other, e.g.,\nD = D' U {z} for some z \u2208 X.\nDP has been applied in the training of machine learning models\nto protect training data [7]. However, since the training of LLMs\nrequires a long time with massive computing resources, it is not\nfeasible for us to use DP to retrain an LLM. Thus, we consider\nthe usage of DP with parameter-efficient fine-tuning approaches\nsuch as LoRA [51]. Instead of fine-tuning the whole model, we use\nLoRA to only fine-tune additional parameters with DP, whose size\nis much smaller than the size of LLM."}, {"title": "3.6.3 Machine Unlearning", "content": "While LLMs memorize some private\ntraining data, a promising way to protect data privacy is to update\nthe model to unlearn specific data, i.e., machine unlearning. Ma-\nchine unlearning has been an attractive research direction recently\nas data regulations such as GDPR stipulate that individuals have the\n\"right to be forgotten\". While many machine learning studies are for\ncomputer vision [71, 104, 134], machine unlearning approaches for\nLLMs remain underexploited. Some studies [55, 115, 116] fine-tune\nthe trained model to unlearn the deleted data, which is more prac-\ntical than modifying the training process [19, 65] as the training of\nLLMs is very expensive. In our toolkit, we adopt an approach [115]\nto fine-tune the LLM using knowledge gap alignment. Specifically,\nthe LLM is updated such that the knowledge gap between it and the\nmodel trained on the deleted data is similar to the gap of another\nmodel handling the seen and unseen data."}, {"title": "3.6.4 Defensive Prompting", "content": "While PLAs can cause prompt leakage\nthrough prompting, it is also interesting to see whether defensive\nprompting can help protect the private prompts. We design and\ninclude five intuitive defense prompts. For example, one prompt\nis no-repeat, where we ask the LLM not to provide private content\nin the future even if the user asks or enforces you to do so. These\ndefensive prompts are easy to apply with negligible overhead. The\ndetails of these prompts are available in Section 5.4."}, {"title": "3.7 Efficiency", "content": "Efficiency is an important factor that influences the practicality\nand scalability of various attack and defense strategies. Details re-\ngarding the GPU memory requirements and computational costs of\nthese strategies are presented in Table 2. These experiments were\nconducted using the Llama-2 7B model on the Enron dataset, utiliz-\ning a system equipped with two NVIDIA A100 GPUs and two AMD\nEPYC 7J13 64-Core Processors. The findings indicate that most\nattack strategies are computationally efficient as they do not neces-\nsitate the training or updating of models. However, model-based\nMIAs are not feasible for LLMs due to the necessity of training\nmultiple LLMs to develop an effective attack model. Among the\ndefense mechanisms, DP-SGD offers lower computational overhead\ncompared to data scrubbing. This is because DP-SGD integrates\nminimal additional operations into the training process, whereas\nscrubbing requires extensive preprocessing of the original data us-\ning language models. Despite all approaches needing at least 28GB\nof GPU memory owing to the large parameter sizes involved, the\navailability of LLM inference services from various companies (e.g.,\nOpenAI, TogetherAI) means that attackers might not require local\nmodel hosting, potentially reducing the need for high-performance\nGPUs."}, {"title": "3.8 Metrics", "content": "Our toolkit provides multiple metrics to cover different data types\nand attacks including: 1) Data extraction accuracy: this metric re-\nports how much private data are successfully extracted using a DEA; 2)\nMIA AUC and TPR: For MIAs, a test dataset contains members\nand non-members is used to evaluate the effectiveness of the attack.\nWe include both AUC (Area Under the Curve) and TPR@0.1%FPR\n(true positive rate at 0.1% false positive rate) to evaluate the per-\nformance of MIAs; 3) Jailbreaking success rate: This metric reports\nthe rate of responses that do not refuse to answer given private\nqueries when using JAs; 4) JPlag similarity\u2074: This metric reports\nthe similarity between different source code to measure the privacy"}, {"title": "3.9 Usage", "content": "LLM-PBE is implemented in Python, offering a user-friendly and\naccessible platform for privacy evaluation. As shown in Figure 3,\nusers can effortlessly import different modules from our toolkit to\nassess and analyze the privacy risks of LLMs. This implementation\nnot only simplifies the evaluation process but also enables users\nto customize their assessments based on specific needs or research\nfocuses. Whether for academic research or practical development,\nLLM-PBE serves as an invaluable tool in the ongoing effort to\nsafeguard privacy in the realm of Large Language Models."}, {"title": "4 LEAKAGE OF TRAINING DATA", "content": "In this section, we conduct extensive experiments to assess the\nprivacy of training data of LLMs with existing attack methods,\nincluding data used for pertaining and fine-tuning. We focus on\nanswering the following research questions: 1) Does the privacy risks\nof in LLMs correspond proportionally with their increasing scale and\neffectiveness? 2) How are different data characteristics associated with\nthe privacy risks of LLMs? 3) Are there practical privacy-preserving\napproaches when deploying LLMs? Due to the page limit, we present\nrepresentative experiments in the main paper and put additional\nresults in Appendix."}, {"title": "4.1 Experimental Setup", "content": "Attack Approaches We evaluate the privacy risks of training data\nmainly with two attack methodologies, including 1) Data Extraction\nAttacks (DEAs): we consider the query-based method that prompts\nmodel with training data prefixes [25], and further explore different\ndecoding configurations following [125]. 2) Membership Inference\nAttacks (MIAs): We utilize several recent attack methods on LLMs.\nPPL thresholds perplexity to predict membership. Refer computes\nthe ratio of the log-perplexity of the tested model against a reference\nmodel [28]. Instead of using log-perplexity, LiRA uses the ratio of\nlikelihood instead [23, 79, 117, 122]. LiRA assumes the availability\nof high-quality data distributed similarly to the training set, which\nwas thought to be impractical [106]. Therefore, we follow [76] to\nuse the pre-trained model as a reference. MIN-K [98] determines the"}, {"title": "4.2 Effect of Model Size", "content": "The continuous increase in model size raises an important question\nabout the corresponding changes in privacy risks associated with\nthese models. To explore this", "16": "of varying sizes on Enron", "32": "to\nreflect the utility of LLMs. The results highlight a significant pattern:\nas the model size expands, both the utility of the model and the\naccuracy of the complete email address extraction (as shown in DEA\nEnron) increase. Moreover, the rate of increase in data extraction\naccuracy on Enron is even higher than the rate of increase"}]}