{"title": "Proposing Hierarchical Goal-Conditioned Policy Planning\nin Multi-Goal Reinforcement Learning", "authors": ["Gavin B. Rens"], "abstract": "Humanoid robots must master numerous tasks with sparse rewards, posing a challenge for reinforcement\nlearning (RL). We propose a method combining RL and automated planning to address this. Our approach\nuses short goal-conditioned policies (GCPs) organized hierarchically, with Monte Carlo Tree Search (MCTS)\nplanning using high-level actions (HLAs). Instead of primitive actions, the planning process generates HLAs.\nA single plan-tree, maintained during the agent's lifetime, holds knowledge about goal achievement. This hi-\nerarchy enhances sample efficiency and speeds up reasoning by reusing HLAs and anticipating future actions.\nOur Hierarchical Goal-Conditioned Policy Planning (HGCPP) framework uniquely integrates GCPs, MCTS,\nand hierarchical RL, potentially improving exploration and planning in complex tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Humanoid robots have to learn to perform several, if\nnot hundreds of tasks. For instance, a single robot\nworking in a house will be expected to pack and un-\npack the dishwasher, pack and unpack the washing\nmachine, make tea and coffee, fetch items on demand,\ntidy up a room, etc. For a reinforcement learning (RL)\nagent to discover good policies or action sequences\nwhen the tasks produce relatively sparse rewards is\nchallenging (Pertsch et al., 2020; Ecoffet et al., 2021;\nShin and Kim, 2023; Hu et al., 2023; Li et al., 2023).\nExcept for (Ecoffet et al., 2021), the other four ref-\nerences use hierarchical approaches. This paper pro-\nposes an approach for agents to learn multiple tasks\n(goals) drawing from techniques in hierarchical RL\nand automated planning.\nA high-level description of our approach fol-\nlows. The agent learns short goal-conditioned poli-\ncies which are organized into a hierarchical structure.\nMonte Carlo Tree Search (MCTS) is then used to plan\nto complete all the tasks. Typical actions in MCTS are\nreplaced by high-level actions (HLAs) from the hier-\narchical structure. The lowest-level kind of HLA is\na goal-conditioned policy (GCP). Higher-level HLAS\nare composed of lower-level HLAs. Actions in our\nversion of MCTS can be any HLAs at any level. We\nassume that the primitive actions making up GCPs are\ngiven/known. But the planning process does not op-\nerate directly on primitive actions; it involves gener-\nating HLAs during planning."}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Goal-Conditioned Reinforcement\nLearning", "content": "Reinforcement learning (RL) is based on the Markov\ndecision process (MDP). An MDP is described as a\ntuple (S,A,T, R, \u03b3), where S is a set of states, A is a\nset of (primitive) actions, T is a transition function\n(the probability of reaching a state from a state via an\naction), R: S\u00d7A\u00d7SR and y is the discount factor.\nThe value of a state s is the expected total discounted\nreward an agent will get from s onward, that is,\n$V(s) = E_{st+1~T(s_t,a_t)} [\\sum_{t=0}^{\\infty} \\gamma^tR(s_t, a_t, s_{t+1}) | s_0 = s]$,\nwhere st+1 is the state reached by executing action a\nin state st. Similarly, the value of executing a in s is\ndefined by\n$Q(a,s) = R(s,a,s') +\\gamma\\sum_{s'\\in S} T(s,a,s')V(s').$\nWe call it the Q function and its value is a q-value.\nThe aim in MDPs and RL is to maximize V(s) for all\nreachable states s. A policy \\(\\pi : S \\rightarrow A\\) tells the agent\nwhat to do: execute a = \\(\\pi(s)\\) when in s. A policy can\nbe defined in terms of a Q function:\n$\\sqrt{s \\in S, \\pi(s) = arg \\underset{a \\in A}{max} Q(a, s)}$.\nIt is thus desirable to find 'good' q-values (see later).\nGoal-conditioned reinforcement learning (GCRL)\n(Schaul et al., 2015; Liu et al., 2022) is based on the\nGCMDP, defined as the tuple (S,A,T, R, G, \u03b3,), where\nS, A, T and y are as before, G is a set of (desired)\ngoals and R: S\u00d7A\u00d7S\u00d7G\u2192 R is goal-conditioned\nreward function. In GCRL, the value of a state and the\nQ function are conditioned on a goal g \u2208 G: V(s,g),\nrespectively, Q(a,s,g). A goal-conditioned policy is\n$\\pi : S X G \\rightarrow A$\nsuch that \u03c0(s, g) is the action to execute in state s to-wards achieving g."}, {"title": "2.2 Hierarchical Reinforcement\nLearning", "content": "Traditionally, hierarchical RL (HRL) has been a\ndivide-and-conquer approach, that is, determine the\nend-goal, divide it into a set of subgoals, then select\nor learn the best way to reach the subgoals, and fi-\nnally, achieve each subgoal in an order appropriate to\nreach the end-goal.\n\"Hierarchical Reinforcement Learning (HRL)\nrests on finding good re-usable temporally extended\nactions that may also provide opportunities for state\nabstraction. Methods for reinforcement learning can\nbe extended to work with abstract states and actions\nover a hierarchy of subtasks that decompose the orig-\ninal problem, potentially reducing its computational\ncomplexity.\" (Hengst, 2012)\n\"The hierarchical approach has three challenges\n[619, 435]: find subgoals, find a meta-policy over\nthese subgoals, and find subpolicies for these sub-\ngoals.\" (Plaat, 2023)\nThe divide-and-conquer approach can be thought\nof as a top-down approach. The approach that\nour framework takes is bottom-up: the agent learns\n'skills' that could be employed for achieving different\nend-goals, then memorizes sets of connected skills as\nmore complex skills. Even more complex skills may\nbe memorized based on less complex skills, and so\non. Higher-level skills are always based on already-\nlearned skills. In this work, we call a skill (of any\ncomplexity) a high-level action."}, {"title": "2.3 Monte Carlo Tree Search", "content": "The version Monte Carlo tree search (MCTS) we are\ninterested in is applicable to single agents based on\nMDPs (Kocsis and Szepesvari, 2006).\nAn MCTS-based agent in state s that wants to se-\nlect its next action loops thru four phases to generate\na search tree rooted at a node representing s. Once\nthe agent's planning budget is depleted, it selects the\naction extending from the root that was most visited\n(see below) or has the highest q-value. While there\nstill is planning budget, the algorithm loops thru the\nfollowing phases (Browne et al., 2012).\nSelection A route from the root until a leaf node\nis chosen.\u00b9 Let UCBX : Nodes \u00d7 A \u2192 R denote any\nvariant of the Upper Confidence Bound, like UCB1\n(Kocsis and Szepesvari, 2006). For every non-leaf\nnode n in the tree, follow the path via action a* =\n$\\arg \\max_{a \\in A} UCBX(n,a)$.\nExpansion When a leaf node n is encountered, se-\nlect an action a not yet tried in n and generate a new\nnode representing s' as child of n.\nRollout To estimate a value for s' (or n' represent-\ning it), simulate one or more Monte Carlo trajecto-\nries (rollouts) and use them to calculate a reward-to-\ngo from n'. If the rollout value for s' is V(s'), the\nQ(a, s) can be calculated.\nBackpropagation If n' has just been created and\nthe q-value associated with the action leading to it has\nbeen determined, then all action branches on the path\nfrom the root till n must be updated. That is, the\nchange in value at the end of a path must be propa-\ngated bach up the tree.\nIn this paper, when we write f(n) (where n might\nhave indices and f is some function), we generally\nmean f(n.s) and f(s'), where n.s = s' is the state rep-\nresented by node n."}, {"title": "2.4 Multi-Objective/Goal\nReinforcement Learning", "content": "We make a distinction between multi-objective RL\n(MORL) and multi-goal RL (MGRL). MORL (Liu\net al., 2015) attempts to achieve all goals simultane-\nously. There is often a weight or priority assigned to\ngoals. MGRL (Kaelbling, 1993; Sutton et al., 2011)\ndoes not weight any goal as more or less important\nand each goal is assumed to eventually be pursued in-\ndividually. There is no question about which goal is\nmore important; the agent simply pursues the goal it\nis commanded to. In both cases, the agent can learn"}, {"title": "3 RELATED WORK", "content": "The areas of hierarchical reinforcement learning\n(HRL) and goal-conditioned reinforcement learning\n(GCRL) are very large. The overlapping area of goal-\nconditioned hierarchical RL is also quite large. Ta-\nble 1 shows where some of the related work falls\nwith respect to the dimensions of 1) goal-conditioned\n(GC), 2) hierarchical (H), 3) planning (P) and 4) RL.\nWe found only two papers involving hierarchical\nMCTS: (Lu et al., 2020) does not involve RL nor goal-\nconditioning, nor multiple-goals. It is placed in the\nbottom right-hand corner of Table 1.\nThe work of Pinto and Coutinho (2019) is the\nclosest to our framework. They proposes a method"}, {"title": "4 THE PROPOSED ALGORITHM", "content": "We first give an overview, then give the pseudo-code.\nThen we discuss the novel and unconventional con-\ncepts in more detail.\nWe make two assumptions-that\n\u2022 the agent will always start a task from a specific,\npre-defined state (location, battery-level, etc.) and\n\u2022 the agent may be given some subgoals to assist it\nin learning how to complete a task.\nFuture work could investigate methods to deal with\ntask environments where these assumptions are re-\nlaxed.\nWe define a contextual goal-conditioned policy\n(CGCP) as a policy parameterized by a context CC S\nand a (behavioral) goal g CS, where S is the state-\nspace. The context gives the region from which the\ngoal will be pursued. In this paper, we simplify the\ndiscussion by equating C with one state ss and by\nequating g with one state se. A CGCP is something\nbetween an option (Sutton et al., 1999) and a GCP.\nFormally,\n$\\pi: C \\times S \\times S \\rightarrow A$  or  $\\pi[s_s, s_e]: S \\rightarrow A$\nis a CGCP. The idea is that$\\pi[s_s,s_e]$ can be used to\nguide an agent from ss to se. Whereas GCPs in tra-\nditional GCRL do not specify a context, we include\nit so that one can reason about all policies relevant to\na given context. In the rest of this paper we refer to\nCGCPs simply as GCPs.\nTwo GCPs, \\(\\pi[C, g]\\) as predecessor and \\(\\pi'[C', g']\\) as\nsuccessor, are linked if and only if \\(g \\cap C' \\neq 0\\). When\ng = \\({se}\\) and C' = \\({s}\\), then we require that se = s"}, {"title": "4.1 Sampling behavioral goals", "content": "If the agent decides to explore and thus to generate a\nnew GCP, what should the end-point of the new GCP\nbe? That is, if the agent is at s, how does the agent\nselect s' to learn \u03c0[s, s']? The 'quality' of a behav-\nioral goal s' to be achieved from current exploration\npoint s is likely to be based on curiosity (Schmid-\nhuber, 2006, 1991) and/or novelty-seeking (Lehman\nand Stanley, 2011; Conti et al., 2017) and/or coverage"}, {"title": "4.2 The expansion strategy", "content": "For every iteration thru the MCTS tree, for every in-\nternal node on the search path, a decision must be\nmade whether to follow one of the generated HLAs or\nto explore and thus expand the tree with a new HLA.\nLet \u03b7(s, \u03b4) be an estimate for the number can-\ndidate behavioral goals go around s, with 8 being\na hyper-parameter proportional to the distance of gb"}, {"title": "4.3 The value of a GCP", "content": "Suppose that we are learning policy [ss, se]. Let\n\\(\\sigma[s_s,s_e]\\) be a sequence $s_1,a_1,s_2,a_2,...,s_j,s_{j+1}$ of\nstates and primitive actions such that s\u2081 = ss and\nsj+1 = Se. Let traj(ss,se) be all such trajectories (se-\nquences) \\(\\sigma[s_s, s_e]\\) experienced by the agent in its life-time. Then we define the value\\(R^{g}_{[ss, se]}\\) of \\(\\pi[s_s, s_e]\\) with respect to goal g as\n$\\frac{1}{|traj(s_s, s_e)|} \\sum_{\\sigma[s_s, s_e] \\in traj(s_s, s_e)} \\sum_{s_i, a_i \\in \\sigma[s_s, s_e]} R_g(a_i, s_i)$.\nIn words, \\(R^{g}_{[ss,se]}\\) is the average sum of rewards expe-rienced of actions executes in trajectories from ss tose for pursuing g. At the moment, we do not specifywhether Rg (ai, si) is given or learned."}, {"title": "4.4 Backpropagation", "content": "Only GCPs are directly involved in backpropagation.\nIn other words, we backpropagate values from node to\nnode up the plan-tree as usual in MCTS while treat-ing the GCPs as primitive actions in regular MCTS.\nThe way the hierarchy is constructed guarantees thatthere is a sequence of linked GCPs between any nodereached and the root node.\nEvery time a GCP [n.s,n'.s] and correspondingnode n' are added to the plan-tree, the value of \u03c0de-termined just after learning \u03c0 (i.e. R) is propagatedback up the tree, for every desired goal. In general, forevery GCP \u03c0[n.s, \u00b7] (and every non-leaf node n) on thepath from the root until n', for every goal g \u2208 G, wemaintain a goal-conditioned value Q(\u03c0,n,g) repre-senting the estimated reward-to-go from n.s onwards.\nAs a procedure, backpropagation is defined by Al-gorithm 1. Note that q and fv are arrays indexed bygoals in G."}, {"title": "4.5 The value of a non-GCP HLA", "content": "Every non-GCP HLA has a q-value. They are main-tained as follows. Let \\(\\pi_i \\rightarrow ... \\rightarrow \\pi_j\\) be the sequence"}, {"title": "4.6 Desired goal focusing", "content": "An idea is to use a progress-based strategy: Fix theorder of the goals in G arbitrarily: 81,82,...,n.Let Prog(g,c,t,w, 0) be true iff: the average rewardwith respect to g experienced over w GCP-steps c\nis at least 0 or the number of steps is less than t.Parameter t is the minimum time the agent shouldspend learning how to achieve a goal, per attempt.If Prog(gi, c,t, w, 0) becomes false while pursuing gi,then set c to zero, and start pursuing gi+1 if i \u2260 norstart pursuing g\u2081 if i = n. Colas et al. (2022) discussthis issue in a section called \"How to Prioritize GoalSelection?\""}, {"title": "4.7 Executing a task", "content": "Once training is finished, the robot can use the gen-erated plan-tree to achieve any particular goal g \u2208 G.The execution process is depicted in Figure 4. Note"}, {"title": "5 DISCUSSION", "content": ""}, {"title": "5.1 Learning", "content": "Almost any RL algorithm can be used to learn GCPs,once generated. They might need to be slightly mod-ified to fit the GCRL setting. For instance, Kulkarni\net al. (2016); Zadem et al. (2023) use DQN (Mnih"}, {"title": "5.2 Representation of Functions", "content": "The three main functions that have to be representedare the contextual goal-conditioned policies (GCPs),GCP values and goal-conditioned Q functions. Wepropose to approximate these functions with neuralnetworks.\nIn the following, we assume that every state s isdescribed by a set of features, that is, a feature vectorFs. Recall that a GCP \u03c0[ss, se] has context state ss andbehavioral goal state se. Hence, every GCP can beidentified by its context and behavioral goal.\nThere could be one policy network for all GCPs\u03c0[\u00b7,\u00b7] that takes as input three feature vectors: a pairof vectors to identify the GCP and one vector to iden-tify the state for which an action recommendation issought. The output is a probability distribution overactions A; the output layer thus has A nodes. Theaction to execute is sampled from this distribution.\nThere could be one policy-value network for all\\(R^{g}_{[ss, se]}\\) that takes as input three feature vectors: a pairof vectors to identify the GCP and one vector to iden-tify the state representing the desired goal in G. Theoutput is a single node for a real number.\nThere could be one q-value network for Q(h, s,g)that takes as input four feature vectors: a pair of vec-tors to identify the HLA h, one vector to identify thestate s and one vector to identify the state representingthe desired goal g \u2208 G. The output is a single node fora real number.\nWe could also look at universal value function ap-proximators (UVFAs) for inspiration (Schaul et al.,2015)."}, {"title": "5.3 Memory", "content": "\"Experience replay was first introduced by Lin (1992)and applied in the Deep-Q-Network (DQN) algorithm"}]}