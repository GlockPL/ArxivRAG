{"title": "EXPLAINABLE AI THROUGH A DEMOCRATIC LENS:\nDHONDTX\u0391\u0399 FOR PROPORTIONAL FEATURE IMPORTANCE\nUSING THE D'HONDT METHOD", "authors": ["T\u00fcrker Berk D\u00d6NMEZ"], "abstract": "In democratic societies, electoral systems play a crucial role in translating public preferences into\npolitical representation. Among these, the D'Hondt method is widely used to ensure proportional\nrepresentation, balancing fair representation with governmental stability. Recently, there has been\na growing interest in applying similar principles of proportional representation to enhance inter-\npretability in machine learning, specifically in Explainable AI (XAI). This study investigates the\nintegration of D'Hondt-based voting principles in the DhondtXAI method, which leverages resource\nallocation concepts to interpret feature importance within AI models. Through a comparison of SHAP\n(Shapley Additive Explanations) and DhondtXAI, we evaluate their effectiveness in feature attribution\nwithin CatBoost and XGBoost models for breast cancer and diabetes prediction, respectively. The\nDhondtXAI approach allows for alliance formation and thresholding to enhance interpretability, rep-\nresenting feature importance as seats in a parliamentary view. Statistical correlation analyses between\nSHAP values and DhondtXAI allocations support the consistency of interpretations, demonstrating\nDhondtXAI's potential as a complementary tool for understanding feature importance in AI models.\nThe results highlight that integrating electoral principles, such as proportional representation and\nalliances, into AI explainability can improve user understanding, especially in high-stakes fields like\nhealthcare.", "sections": [{"title": "1 Introduction", "content": "In democratic societies, election systems play a vital role in translating public preferences into political representation.\nThese systems range from majority and proportional systems to mixed methods, each with unique mechanisms and\nimpacts on representation [1]. Majority systems tend to favor larger parties and provide stable governments, whereas\nproportional systems, such as the D'Hondt method, aim to reflect the diversity of voter support across multiple parties.\nThis proportional representation method, while intended to provide a balanced view of political preferences, can\nsometimes present complexities in accurately measuring democratic outcomes, as factors like district size and voter\ndistribution affect representation fairness [2]. Therefore, understanding election systems is essential for assessing how\ndemocracies work and how well they reflect the population's intent.\nThe D'Hondt method, widely used in proportional representation systems, allocates seats to parties based on the number\nof votes they receive, adjusted by a divisor sequence to ensure representation proportional to each party's share of votes\n[3]. Its fairness is often debated, as some argue it slightly favors larger parties due to the nature of seat allocation, which\ncan contribute to governmental stability by reducing the fragmentation of legislative bodies [4]. On the other hand,\nstudies on auditing, such as Stark et al. (2014)[5], indicate that the D'Hondt method offers advantages in transparency\nand verifiability, especially when risk-limiting audits are applied, ensuring election accuracy and fairness. This balance\nbetween stability and fair representation is a core consideration in evaluating the suitability of the D'Hondt method for\ndifferent political contexts.\nCountries across Europe and beyond utilize the D'Hondt method to various extents, including Spain, Portugal, and\nPoland, where it has been integral in ensuring proportional representation within parliamentary systems [6]. The\nmethod's influence extends to both national and regional elections, as observed in nations like Turkey, which employs\nthe D'Hondt method with a national threshold to balance between fair representation and governmental effectiveness\n[4]. This widespread adoption underscores the method's adaptability and its perceived fairness in representing diverse\npolitical voices while maintaining governance stability.\nArtificial intelligence (AI) has significantly evolved, with neural networks and tree-based models becoming central\ntechniques in various domains. Neural networks, such as multilayer perceptrons (MLPs), excel at handling complex,\nnon-linear relationships by adjusting their layered connections between neurons, a structure that enables robust pattern\nrecognition in tasks like disease prediction and environmental modeling [7, 8]. Tree-based models, like decision\ntrees and random forests, offer a simpler yet effective structure by segmenting data into branches based on feature\nvalues. While tree-based models provide transparent, interpretable decision paths, neural networks remain powerful for\ncapturing intricate relationships, albeit with a trade-off in interpretability. Studies comparing these approaches, like\nSorano et al. [9], highlight that while neural networks often achieve higher accuracy, tree-based models are preferable\nin applications where clarity in model decisions is essential, underscoring the practical considerations when choosing\nbetween these methods.\nExplainable AI (XAI) has emerged to address the interpretability of complex AI models, aiming to make model decisions\nmore understandable for human users. Traditional AI models, especially deep neural networks, operate as \"black\nboxes,\" with decision processes often opaque to end users. Techniques like SHAP (Shapley Additive Explanations) and\nLIME (Local Interpretable Model-agnostic Explanations) offer post-hoc explanations by highlighting the influence\nof individual features on predictions, making AI more accessible in fields such as healthcare and finance, where\naccountability is critical [10]. Furthermore, innovative methods like delta-XAI, which leverages sensitivity analysis to\nrank features' impact on outcomes, are being developed to enhance local explanations of predictions, thus increasing\ntrust and satisfaction in AI-driven decisions [11]. Counterfactual explanations, which suggest minimal changes to\nachieve a different outcome, have also shown promise in improving user understanding and confidence, further bridging\nthe gap between AI's technical complexity and human interpretability [12].\nIn extending the principles of the D'Hondt method and proportional representation to AI model interpretability, it is\nconceivable that concepts like alliance systems and threshold applications\u2014commonly used in real-world electoral\nprocesses-could inspire structured approaches to feature importance in complex machine learning models. For\ninstance, just as electoral alliances allow smaller parties to gain representation by aligning with larger ones, AI models\ncould adopt alliance-like groupings to evaluate how minor features contribute collectively alongside major features to\ninfluence predictions. This approach could help highlight nuanced interactions that might otherwise be overshadowed\nin traditional feature importance analyses, similar to how electoral alliances bring forward diverse voices that might not\nindependently meet representation thresholds. Additionally, thresholds-like electoral cutoffs that parties must surpass\nto gain seats could be applied in feature selection to filter out the least impactful variables, ensuring that only features\nwith significant predictive influence are highlighted. This \"thresholding\" can enhance model clarity by focusing on the\nmost informative aspects of data, improving interpretability without overwhelming users with less relevant details.\nBy applying these voting principles, such as the D'Hondt system's proportional allocation and alliance thresholds, to\nthe interpretability of tree-based models, AI researchers can create a more nuanced ranking of feature importance that\nemphasizes both individual and collective feature contributions. This structured prioritization aligns with Explainable\nAl's goals, bridging model complexity with user understanding. Moreover, leveraging alliance systems and thresholds\ncould offer valuable insights for managing feature importance in neural networks and ensemble models, where\ninterpreting non-linear relationships is often challenging. Such methods can support greater transparency in high-stakes\nfields like healthcare and finance by ensuring that each feature's role in decision-making is proportionally represented\nand appropriately highlighted. Ultimately, incorporating real-world electoral principles into AI could pave the way for\nmore interpretable, balanced, and democratic approaches to machine learning, enabling clearer, more accountable AI\nsystems that resonate with human understanding and societal values. Here, a novel unified approach to interpreting\nmodel predictions is presented."}, {"title": "2 Methodology", "content": "The DhondtXAI library applies a structured methodology to evaluate feature importance in machine learning models,\nfocusing on decision tree-based algorithms. It uses the D'Hondt proportional representation method to distribute\nimportance fairly and transparently across features or feature groups. This method allows users to specify custom\nparameters, such as thresholds, exclusions, alliances, and the total numbers of votes and seats, enabling a flexible and\ninsightful feature analysis."}, {"title": "2.1 User-Defined Parameters", "content": "The following parameters are defined by the user to establish the scope and constraints of the analysis:\n\u2022 Total Number of Votes (V): The user-specified total number of votes to be allocated proportionally among\nfeatures based on their calculated importance values. This parameter enables direct control over the scale of\nthe voting distribution.\n\u2022 Total Number of Seats (M): The user-defined total number of seats (representative units) that will be allocated\nacross features based on their final calculated vote shares, reflecting the overall distribution of influence among\nfeatures.\n\u2022 Excluded Features (Fexclude): A list of features the user wishes to remove from the analysis. These features\nwill be excluded from all stages of the analysis, ensuring that they do not affect the importance-based voting or\nseating calculations.\n\u2022 Feature Alliances (Alliances): Specific groupings of features that the user defines to be evaluated together as\na single unit. More than one alliance can be formed, and each alliance can include any number of features.\nFor instance, if the user defines alliances such as {f1, f2}, {f3, f4, f5}, and {f6, f9}, each group is treated as\na single alliance with a cumulative importance score. This score represents the combined importance of all\nfeatures in the alliance, treated as a singular voting entity. The library is flexible in that alliances can contain\noverlapping or independent groups of features as specified by the user.\n\u2022 Threshold Value (Threshold): A minimum importance threshold defined as a percentage. Features or alliances\nwhose relative importance is below this threshold are filtered out and do not receive any seats in the final\nseating allocation. This parameter allows the user to focus the analysis on only the most influential features or\nalliances.\nThese parameters create a customized analytical framework, ensuring that only the user-defined aspects of the feature\nset contribute to the voting and seating allocations."}, {"title": "2.2 Calculation of Feature Importances in Tree-Based Models", "content": "The foundation of the DhondtXAI process is the feature importance calculation, derived from tree-based models like\nRandom Forests and Gradient Boosted Trees. In these models, feature importance is calculated by evaluating each\nfeature's contribution to reducing impurity at each decision node where the feature is used as a split criterion.\nReduction in Impurity at Node Level: For any feature A used at a split node n, the impurity before the split, I(n),\nand after the split, I'(n), are compared to assess the feature's contribution to predictive improvement. The impurity\nafter the split is a weighted sum of the impurities in the child nodes n\u2081 and NR:\n$I'(n) = \\frac{NL}{|n|}I(N_L) + \\frac{NR}{|n|}I(N_R)$\nwhere \u2758n is the number of samples in the parent node n, and |NL| and |NR| represent the numbers of samples in the left\nand right child nodes, respectively.\nInformation Gain for Feature A: The reduction in impurity, or information gain \u0394IA,n, from splitting on feature A\nat node n is then calculated as:\n$\\Delta I_{A,n} = I(n) \u2013 I'(n)$\nThis score reflects the extent to which feature A contributes to predicting the target variable at that specific split."}, {"title": "Aggregation Across Nodes and Trees", "content": "Summing \u2206IA,n over all nodes where feature A is used across all trees in the\nensemble provides the cumulative importance score for that feature. For feature A in a single tree, the importance is:\n$Importance_{A, tree} = \\sum_{n \\in N_A} \\Delta I_{A,n}$\nwhere NA is the set of nodes where A is used. Across all trees in an ensemble, the feature importance for A is averaged:\n$Importance_{A, ensemble} = \\frac{1}{|T|}\\sum_{t\\in T} Importance_{A, t}$\nHere, T represents the set of all trees, and Importance A,t is feature A's importance in tree t.\nThese feature importance values form the basis for the proportional voting distribution used in the DhondtXAI\nmethodology."}, {"title": "2.3 Feature Exclusion and Alliance Formation", "content": "The first step involves applying exclusions and forming alliances to establish the final feature set that will be analyzed.\nFeature Exclusion: Given the initial feature set F, features specified by the user for exclusion are removed from the\nanalysis. The resulting set of features used for voting and seat allocation, F', is defined as:\n$F' = F \\setminus F_{exclude}$\nwhere Fexclude denotes the set of features explicitly excluded by the user. Only features in F' are considered in\nsubsequent steps, allowing for a refined analysis that adheres to user-defined constraints.\nAlliance Formation: Following feature exclusion, alliances are formed among features in F' based on user-defined\ngroupings. Each alliance combines the importance values of its constituent features, enabling them to act as a unified\nfeature. Multiple alliances are supported, each forming an independent entity that will participate in the voting and seat\nallocation process.\nLet's assume the user specifies several alliances: {f1, f2}, {f3, f4, 5}, and {f6, f9}. Each of these alliances is assigned\na combined importance score based on the sum of the individual importance values of the features within the group.\nThe importance of each alliance alliance; is calculated as follows:\n$importance_{alliance_j} = \\sum_{i \\in alliance_j} importance_i$\nwhere alliance; represents each unique alliance defined by the user, and i represents the individual features within that\nalliance.\nEach alliance is treated as a single unit in the voting and seating calculations, meaning that it will receive votes and\nseats proportionate to its cumulative importance score rather than the individual scores of its constituent features. This\ncumulative score importancealliance; reflects the joint influence of all features within the alliance.\nFinalized Feature and Alliance Sets: After exclusions and alliances are processed, the final analysis set consists of:\n\u2022 Individual features not excluded or grouped into alliances.\n\u2022 Each user-defined alliance, treated as a unified entity with a combined importance score.\nThese elements form the basis for the proportional vote and seat distribution, with each alliance's total importance score\ninfluencing its share of votes relative to individual features."}, {"title": "2.4 Initial Vote Distribution Using the D'Hondt Method", "content": "With the final feature and alliance sets established, the initial distribution of votes is calculated based on the relative\nimportance values of each feature or alliance."}, {"title": "Vote Distribution Across Features and Alliances", "content": "Using the total votes V specified by the user, each feature or\nalliance i in the finalized set is allocated a proportional share of the total votes based on its importance score. The initial\nvote allocation for each feature or alliance i is calculated as follows:\n$initial\\_vote_i = \\frac{importance_i}{\\Sigma_{j \\in F'} importance_j} X V$\nwhere V represents the total votes, importance, is the importance score for feature or alliance i, and F' represents the\nfinal set of features and alliances after exclusions and groupings.\nThis formula ensures that features or alliances with higher importance scores receive a larger share of the votes, aligning\nthe voting allocation directly with each feature's or alliance's calculated significance within the model.\nConsideration for Multiple Alliances: Since each alliance's importance score is cumulative, the total importance\nscore for alliances is the sum of the importance scores for all individual features within each alliance. This cumulative\napproach means that, despite multiple alliances or groupings, each alliance acts as a single voting entity.\nFor instance, if alliance {f1, f2} has a combined importance score of 0.30, and {f3, f4, f5} has a combined score of\n0.25, these alliances would receive initial votes proportionate to their total influence compared to other features or\nalliances.\nEstablishing the Vote Basis for Each Feature and Alliance: The initial votes calculated for each feature or alliance\ninitial_vote serve as the foundation for further calculations. These initial votes directly influence both the threshold\napplication (in section 2.5) and the final seat allocation process (in section 2.6), ensuring that every feature's or alliance's\nshare of the total votes is transparently proportional to its relative importance within the model.\nIn this way, the initial vote distribution allows for a democratic and transparent reflection of each feature or alliance's\ninfluence within the analysis framework, laying the groundwork for the remaining steps in the DhondtXAI process."}, {"title": "2.5 Threshold Application and Redistribution of Votes", "content": "The third step introduces the concept of a threshold a user-defined minimum vote percentage that each feature or\nalliance must meet to be eligible for seat allocation. This threshold allows the user to filter out features or alliances with\nlower influence, ensuring that only those with significant importance values contribute to the final seat distribution.\nThreshold Calculation: The threshold value threshold_vote is determined as a percentage of the total votes V. This\nvalue acts as a minimum criterion that all features and alliances must meet to proceed to the seat allocation phase.\nMathematically, the threshold vote amount is calculated as follows:\n$threshold\\_vote = \\frac{threshold}{100} X V$\nwhere threshold is the user-defined percentage (for example, 5%).\nIf threshold = 0, this means there is no minimum vote requirement, allowing all features and alliances to move directly\nto the seat allocation phase. If threshold > 0, any feature or alliance whose initial vote count initial_vote; falls below\nthreshold_vote will be excluded from seat allocation, and their votes will be redistributed to the remaining features and\nalliances.\nIdentifying Below-Threshold Features and Alliances: After calculating threshold_vote, each feature or alliance\ni is evaluated to see if it meets the threshold. If a feature's or alliance's initial vote count initial_vote is less than\nthreshold_vote, it is classified as a below-threshold entity and is excluded from receiving seats:\n$below\\_threshold = \\{i \\in F' : initial\\_vote\u00bf < threshold\\_vote\\}$"}, {"title": "Redistribution of Below-Threshold Votes", "content": "The votes of all below-threshold features and alliances are collected\nand proportionally redistributed to the above-threshold group based on their relative importance. This redistribution\nstep ensures that the votes of lower-importance features continue to contribute to the final allocation process, albeit\nindirectly, by strengthening the influence of more impactful features.\nThe total votes from below-threshold features, $\\sum_{l\\in below\\_threshold} initial\\_vote\u012b$, is distributed across each feature j in the\nabove-threshold group, proportional to each feature's importance score:\n$redistributed\\_vote_j = initial\\_vote_j + (\\frac{importance_j}{\\sum_{k \\in above\\_threshold} importance_k} X (\\sum_{l\\in below\\_threshold} initial\\_vote_l))$\nHere:\n\u2022 redistributed_vote; represents the final vote total for each feature or alliance in the above-threshold group after\nreceiving redistributed votes.\n\u2022 importance; is the importance of the feature or alliance j within the above-threshold set.\n$\\Sigma_{k \\in above\\_threshold} importance$ normalizes the redistributed votes based on the importance scores of the remain-\ning, eligible features.\nThis redistribution process provides a final vote count for each eligible feature or alliance, ensuring that even below-\nthreshold features indirectly impact the results, thus preserving a fair representation of their cumulative influence."}, {"title": "2.6 Seat Allocation Using the D'Hondt Method", "content": "In the final step, seats are allocated to features and alliances based on their redistributed vote totals, using the D'Hondt\nmethod to proportionally distribute the specified total seats M. This iterative process ensures that each feature or\nalliance receives seats in alignment with its importance and final vote count.\nInitial Seat Ratios: Each feature or alliance begins with an initial seat count of k = 0, meaning no seats have been\nallocated at the start. The D'Hondt method calculates a seat ratio Si for each feature or alliance i based on its final vote\ncount and current seat count. This initial ratio Si is defined as:\n$S_i = \\frac{redistributed\\_vote_i}{k+1}$\nwhere:\n\u2022 redistributed_vote represents the final vote count after redistribution for feature or alliance i,\n\u2022 k is the current number of seats allocated to i (initially k 0).\nThis initial seat ratio serves as the basis for the first iteration of seat allocation, with each feature or alliance's eligibility\nfor a seat determined by the highest current Si value.\nIterative Seat Assignment: The D'Hondt method proceeds iteratively, allocating one seat at a time based on the\ncurrent seat ratios of all eligible features and alliances. In each iteration:\n\u2022 The feature or alliance with the highest Si value receives one seat.\n\u2022 Once a seat is allocated, the seat count k for the awarded feature or alliance is increased by 1, and its seat ratio\nSi is recalculated to account for the updated seat count:\n$S_i = \\frac{redistributed\\_vote_i}{k+1}$\nThis recalculated Si adjusts the feature or alliance's priority for receiving additional seats in subsequent iterations.\nCompletion of Seat Allocation: The iterative seat assignment continues until all M seats have been allocated. Each\ntime a seat is awarded, the Si values are updated to reflect the new seat distribution, ensuring that seats are consistently\ndistributed according to the highest remaining influence in each iteration."}, {"title": "Example of Seat Calculation in Practice", "content": "Consider a feature with a final redistributed vote count of 1000 and an\ninitial seat count k = 2. For this feature, the recalculated seat ratio Si after receiving two seats would be:\n$S_i = \\frac{1000}{2+1} = \\frac{1000}{3} \u2248 333.33$\nThis updated seat ratio is then compared to the Si values of other features and alliances to determine the next recipient\nof a seat. By recalculating Si at each step, the D'Hondt method ensures that seats are dynamically assigned based on\nthe most current distribution of influence.\nThe seat allocation phase results in a final distribution of seats across features and alliances that reflects their relative\nimportance, adhering to the principles of proportional representation. This approach allows users to clearly interpret the\nrole of each feature or alliance within the model's decision-making framework, providing a transparent and equitable\nview of feature influence."}, {"title": "3 Results", "content": ""}, {"title": "3.1 Wisconsin Breast Cancer Dataset", "content": "In this study, our focus is on comparing SHAP (SHapley Additive exPlanations) and DhondtXAI in assessing feature\nimportance for a breast cancer classification model. Using the Wisconsin Breast Cancer Dataset [13] with only the\nmean feature values, we aim to demonstrate how each explainability technique identifies the contributions of individual\nfeatures to the model's predictions. By contrasting SHAP and DhondtXAI, we can evaluate the interpretability and\ninsights provided by each method, highlighting their effectiveness in explaining the model's decision-making process\nin a medical context. This analysis will emphasize the practical value of explainable AI techniques in understanding\nmodel behavior and supporting diagnostic decision-making.\nIn this analysis, we utilized the Breast Cancer Wisconsin (Diagnostic) dataset to develop a machine learning model\nusing only the mean features, reducing the dataset to 10 core attributes related to the mean measurements of cell nuclei.\nWe applied the CatBoost classifier to this reduced feature set to build a model capable of distinguishing between benign\nand malignant tumors. The dataset, consisting of 569 observations, was split into training and testing sets, with 70%\nallocated for training and 30% for testing. The trained CatBoost model achieved an impressive accuracy of 96%,\nwith an F1 score of 0.97, recall of 0.96, and precision of 0.98. The AUC-ROC curve 1a, with an AUC value of 1.00,\ndemonstrated the model's high effectiveness in class separation. Confusion matrix of the model also given in 1b"}, {"title": "3.1.1 Applying SHAP", "content": "SHAP (SHapley Additive exPlanations) was applied to the CatBoost model to gain insights into the impact of each\nfeature on the model's predictions. The first visualization, a SHAP summary plot(Figure 2a, was generated to provide\nan overview of the distribution of SHAP values for each feature across all observations. Through this plot, features with\nthe highest influence were identified, as well as how different feature values (high or low) affected the model output.\n\"Mean concave points,\" \"mean texture,\" and \"mean concavity\" were highlighted as highly impactful features, with\ntheir SHAP values showing the most significant effect on model predictions. In examining the direction of the SHAP\nvalues, it was observed that most features, including \"mean concave points,\" \"mean texture,\" and \"mean concavity,\""}]}