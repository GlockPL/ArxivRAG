{"title": "Multiple Abstraction Level Retrieve Augment Generation", "authors": ["Zheng Zheng", "Xinyi Ni", "Pengyu Hong"], "abstract": "A Retrieval-Augmented Generation (RAG) model powered by a large language model (LLM) provides a faster and more cost-effective solution for adapting to new data and knowledge. It also delivers more specialized responses compared to pre-trained LLMs. However, most existing approaches rely on retrieving prefix-sized chunks as references to support question-answering (Q/A). This approach is often deployed to address information needs at a single level of abstraction, as it struggles to generate answers across multiple levels of abstraction. In an RAG setting, while LLMs can summarize and answer questions effectively when provided with sufficient details, retrieving excessive information often leads to the 'lost in the middle' problem and exceeds token limitations. We propose a novel RAG approach that uses chunks of multiple abstraction levels (MAL), including multi-sentence-level, paragraph-level, section-level, and document-level. The effectiveness of our approach is demonstrated in an under-explored scientific domain of Glycoscience. Compared to traditional single-level RAG approaches, our approach improves AI evaluated answer correctness of Q/A by 25.739% on Glyco-related papers.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [Brown, 2020] have achieved unprecedented success, demonstrating remarkable capabilities in various downstream tasks, which range from traditional NLP applications, such as text classification [Abburi et al., 2023; Zhang et al., 2024b; Zhang et al., 2024c], translation [Koshkin et al., 2024; Elshin et al., 2024; Donthi et al., 2024], and summarization [Jin et al., 2024b; Ding et al., 2024; Pu et al., 2023], to emerging areas, such as code generation [Ugare et al., 2024; Ouyang et al., 2023] and LLM-assisted decision-making [Eigner and H\u00e4ndler, 2024; Chiang et al., 2024]. Despite these advancements, LLMs face significant challenges [Nie et al., 2024; Huang et al., 2024; Yang et al., 2024; Ahn et al., 2024; Wang et al., 2024b] to be solved. The reliance on pretrained LLMs and their black-box nature hinders their ability to generate accurate and specialized responses for professional domains. They may generate incorrect or nonsensical facts (hallucinations [Huang et al., 2023; Feng et al., 2024] and outdated knowledge [Zhang et al., 2023; Mousavi et al., 2024]). The professionalism of responses generated by LLMs remains inadequate from the perspective of experts [Ettinger et al., 2023]. RAG is a promising approach to address these challenges [Lewis et al., 2020; Guu et al., 2020]. It improves LLMs by combining retrieval and generation, enhancing accuracy with up-to-date, domain-specific knowledge. It scales efficiently by retrieving relevant information at inference and adapts to new data without retraining. RAG provides explainable, evidence-based responses and supports domain expertise through specialized datasets. It reduces costs by minimizing retraining, ensures contextual relevance, and mitigates hallucinations. Customizable and capable of handling multi-modal data, RAG is well-suited for applications requiring accurate, adaptable, and context-aware responses.\nHowever, existing RAG approaches tend to utilize external knowledge from a single perspective, referring to fixed-size chunks in generating answers (e.g., LangChain\u00b9 and LLa-maIndex\u00b2), which can lead to the extraction of fragmented and/or incomplete information. In real applications, the information needs of a user can be of various abstraction levels, which cannot be fulfilled by individual chunks. For example,"}, {"title": "2 Related Works", "content": "Retrieval-Augmented Generation (RAG), introduced by Lewis et al. [Lewis et al., 2020], promoted the application of LLMs across multiple tasks by providing external information to the model [Borgeaud et al., 2022; Li et al., 2024a; Khandelwal et al., 2019; Min et al., 2020; Izacard and Grave, 2020]. Modern LLM frameworks, such as LangChain and LlamaIndex, offer foundational RAG implementations. These typically involve converting documents into text chunks, embedding them as indices, and retrieving semantically relevant references during inference. Beyond these basic implementations, advanced RAG methodologies have been developed to address specific challenges and optimize different stages of the retrieval process. These improvements can be categorized into pre-retrieval and post-retrieval enhancements. In the pre-retrieval process, previous works are focusing on optimizing the indexing structure and the original query, using technologies such as data granularity, optimizing index structures, adding metadata [Hayashi et al., 2024; Ghasemi and Shakery, 2024], a query rewriting [Mao et al., 2024; Li et al., 2024b; Ma et al., 2023; Peng et al., 2024; Ma et al., 2023], query transformation [Chan et al., 2024], and query expansion [Zheng et al., 2023a; Gao et al., 2022]. For post-trieval process, the critical problem to solve is determining whether the retrieved context is effective for the query [Jin et al., 2024c]. These techniques include re-ranking the retrieved information [Hwang et al., 2024; Mishra et al., 2024; Glass et al., 2022], emphasizing critical sections [Shi et al., 2024; \u015eakar and Emekci, 2024; Shi et al., 2024], and shortening the context to be processed [Zhang et al., 2024a; Bai et al., 2024; Zhao et al., 2024]."}, {"title": "2.2 Scientific Domain-Specific RAG", "content": "As RAG LLM techniques are beneficial for domain-specific scientific areas, they have been widely adopted in fields such as medicine, biology, and finance. In the medical domain, Lozano et al. [Lozano et al., 2023] proposes an open-source RAG-based LLM system designed for answering medical questions using scientific literature. Similarly, Wang et al. [Wang et al., 2024a] introduce a robust pipeline comprising document parsing, indexing, segmentation of extensive research papers, embedding model training, and LLM fine-tuning to enhance performance. Jiang et al. [Jiang et al., 2024] integrate a Turing Complete System for efficient document retrieval and management, enabling accurate responses to medical queries. Additionally, some RAG systems have been extended to molecular research by integrating the retrieval of molecular structures and biomedical entities such as proteins, molecules, and diseases [Liu et al., 2023; Wang et al., 2022; Wang et al., 2023; Yang et al., 2023]. In the financial sector, Lin [Lin, 2024] proposed a PDF parser combined with RAG-based LLMs to extract knowledge from financial reports. Yepes et al. [Yepes et al., 2024] introduced a novel document chunking approach based on structural elements rather than traditional paragraph-based chunking, enhancing the retrieval process. Despite their widespread application, these RAG systems rely on basic methods with fixed-size chunks and lack attention to the completeness and higher-level background information of the retrieved contexts."}, {"title": "2.3 Chunking Optimization", "content": "Focusing on the quality of retrieved chunks and effectively capturing background information, many chunking strategies have been proposed for optimization. Common approaches include fixed-size chunking, recursive chunking, sliding window chunking, paragraph-based chunking, and semantic chunking. While longer text chunks preserve more semantic coherence, they can also introduce noise, dilute the model's attention, and lead to the \"lost in the middle\" phenomenon [Liu et al., 2024]. Advanced methods address these challenges by dynamically determining the appropriate level of detail and selecting chunks with optimal granularity [Sarthi et al., 2024; Zhong et al., 2024; Chen et al., 2024]. Other approaches refine text into smaller, information-rich segments to maintain high completeness [Wu et al., 2021; Angelidis and Lapata, 2018; Chen et al., 2023]. For instance, Gao et al. [Gao et al., 2023] showed that summarizing passages into shorter text improves accuracy, while Zhao et al. [Zhao et al., 2024] proposed LongRAG, which condenses retrieved contexts into summaries that balance informativeness and conciseness. Similarly, Edge et al. [Edge et al., 2024] used LLMs to construct graph representations of corpora, creating detailed nodes and summaries for retrieval. Although these strategies achieve state-of-the-art performance, many rely on model predictions or clustering to group chunks, generating higher-level abstractions or summaries. In practice, chunks sequentially sourced from the same section or document, especially when following the intrinsic structure"}, {"title": "3 Multiple Abstraction Level Retrieval-Augmented Generation Framework", "content": "Our MAL-RAG framework aims to utilize the native structures of articles to answer questions that require information at different levels of abstraction. When building indexes, we preprocess each reference document d \u2208 D into four level chunks: document-level $D_c$, section-level $S_e$, paragraph-level $P_c$, and multi-sentence-level $M_c$. During the inference time, we apply a retriever R to utilize different abstraction-level chunks. To balance information richness and retrieval efficiency, the method dynamically adjusts the number of chunks extracted until the length of the accumulated text reaches a predefined ranger. Subsequently, the probability of each chunk is calculated using a similarity-based softmax approach. Chunks are iteratively accepted until the accumulated probability reaches a pre-specified threshold p. After the relevant chunks are retrieved, the question and chunks are fed into an Answer Generator to generate the answer. The technical details are explained in the following subsections."}, {"title": "3.1 MAL-RAG Chunks Generation", "content": "As illustrated in Figure 2, chunks are designed at four levels of abstraction, corresponding to varying granularities of the original documents, which are document-level chunks $D_e$, section-level chunks $S_e$, paragraph-level chunks $P_c$, and multi-sentence-level chunks $M_c$. During the generation process, the articles are split into these four levels based on their inherent structure.\nFor document-level and section-level chunks, directly using the original content extracted from the document can result in excessive length and dilute the LLM's attention. To address this, we employ a map-reduce approach to generate summarized information. In this approach, we first produce summaries for each paragraph. These paragraph summaries are then used to generate the section-level summary, which serves as the content for the section-level chunks. Finally, we aggregate these section summaries to create document summaries, which serve as the content for the document-level chunks.\nFor all summary tasks in the generation of document-level and section-level chunks, we instantiated & as the pre-trained large language model Vicuna-13B-v1.3 [Zheng et al., 2023b] with customized prompts designed to summarize the key information from the provided contexts. Discussion of the prompt details is included in supplementary material section A.\nFor paragraph-level and multi-sentence-level chunks, we retain the original content and store it directly in the chunk database to provide detailed information during inference.\nGiven a document $d_i \u2208 D$, which contains $N_i$ sections $S_{i,1}, S_{i,2},\u2026, S_{i, N_i}$. Consider an arbitrary section $s_{i,n} \u2208 d$"}, {"title": "3.2 Chunk Retrieval", "content": "Given a question and a chunk, the retriever applies the embedding model Linq-Embed-Mistral [Kim et al., 2024] to generate the embedding vector q for the question and c for the chunk. The similarity between the question q and the chunk c is computed by the cosine similarity:\n$Sim(q, c) = \\frac{q.c}{\\|q\\| \\|C\\|},  c \u2208 D \u222a S \u222a P \u222a M$\nThis process is repeated until the similarity scores are calculated with all chunks. The retriever selects the most relevant chunks while adhering to a length constraint C, which specifies the maximum total length of the selected chunks. To further optimize the retrieval process and minimize noise, we employ a softmax equation to convert similarity scores into probabilities. Suppose k chunks are selected. For an arbitrary chunk $c_i$, the probability is defined as:\n$P(c_i | q) = \\frac{exp(Sim(q, c_i))}{\\sum_{i=1}^{k}exp(Sim(q, c_i))}$,\nThe chunks are sorted in descending order based on their probabilities. Top chunks, whose cumulative probability does not exceed a pre-defined threshold 7, are selected for generating an answer."}, {"title": "3.3 Answer Generation", "content": "The retrieved chunks are concatenated and represented as $C_s$. We carefully prompt with the open-source LLM, Vicuna-13B-v1.3 [Zheng et al., 2023b], to generate an answer:\n$a = LLM(prompt(C_s, q))$,\nwhere $prompt(C_s, q)$ encapsulates the context (i.e., $C_s$) and the question (q), ensuring the LLM can produce a coherent and accurate response. In this prompt, we applied the ICL method to further guide the LLM in generating the desired data. The details about this prompt can be found in Section B of the supplementary material."}, {"title": "3.4 Metrics for Evaluating Answers", "content": "We assess the quality of the answers generated by the LLM using a set of metrics implemented in the Ragas package\u00b3, including Faithfulness, Answer Relevancy, Answer Similarity, Answer Correctness, Context Precision, Context Utilization, Context Recall, and Context Entity Recall. When calculating the metrics, we split both the ground truth and the generated answers into sentences. Each sentence is treated as a statement, and an LLM model is introduced to assess whether two statements match. In this research, we used GPT-40-mini for this task. The primary evaluation metric is Answer Correctness, which is measured by the F1 score:\n$F1 score = \\frac{|TP|}{|TP| + 0.5 \u00d7 (|FP| + |FN|)}$\nwhere TP refers to the statements that are present in both the ground truth and the generated answer, FP represents the statements that are present in the generated answer but not in the ground truth, FN denotes the statements that are present in the ground truth but missing in the generated answer."}, {"title": "4 Experiment Results", "content": "Dataset We constructed a set of 7,652 academic articles in English that are closely relevant to Glycoscience or Glycomaterials. After preprocessing, the chunk database contains 7,652 document-level chunks, 138,259 section-level chunks, 494,613 paragraph-level chunks, and 1,176,259 multi-sentence-level chunks (see Table 1).\nEvaluation Questions/Answer Dataset To evaluate the effectiveness of the RAG system for a customized database lacking human-curated Q/A datasets, we generated a dataset of 1,118 Q/A pairs using GPT-40-mini and selected 200 pairs from each level, totaling 800 pairs for the evaluation dataset. Each question in this dataset is a short phrase-answer query, where the task of the large language model (LLM) is to provide an answer based on the given question and the retrieved context. These Q/A pairs were derived from randomly selected articles, with 200 Q/A pairs assigned to each granularity level, employing distinct generation strategies for each level. At the document-level, three questions were generated for each selected document chunk.\nAt the section-level, we identified seven types of sections, including the titles 'Introduction', 'Discussion', 'Conclusions', 'Conclusion', 'Statistical Analysis', 'Results and"}, {"title": "4.2 Performance", "content": "The experimental results (see Table 2) demonstrate the effectiveness of our MAL-RAG approach on the Question-Answering dataset derived from scientific papers in the Glyco-domain. The comparison results indicate that"}, {"title": "5\nLimitation", "content": "In this research, the document-level summaries and section-level summaries were derived by the open source LLM model Vicuna-13B-v1.3 [Zheng et al., 2023b]. Hence, their quality entirely depends on the summarization capability of Vicuna-13B-v1.3. Document-level summaries, in particular, are more susceptible to the model's limitations as they are generated by aggregating the summaries of sections within each document."}, {"title": "6 Conclusion", "content": "In this work, we introduced a novel RAG approach that leverages MAL chunking to enhance information retrieval and Q/A performance. Our approach addresses the limitations of traditional single-level chunking methods by incorporating multiple levels of abstraction, ranging from multi-sentence-level to document-level, which allows LLMs to generate more accurate and coherent responses while mitigating the challenges associated with token limitations and the 'lost in the middle' problem. We demonstrated the effectiveness of our MAL-RAG framework in the under-explored scientific Glyco-domain, where it achieved a significant 25.739% improvement in answer correctness compared to conventional single-level RAG methods. These results highlight the potential of our approach to enhance knowledge retrieval and adaptation in specialized domains where nuanced information processing is critical. We constructed a domain-specific Q/A dataset, which includes 800 curated Q/A pairs and can"}]}