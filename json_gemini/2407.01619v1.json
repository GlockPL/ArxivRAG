{"title": "TabSketchFM: Sketch-based Tabular Representation Learning for Data Discovery over Data Lakes", "authors": ["Aamod Khatiwada", "Harsha Kokel", "Ibrahim Abdelaziz", "Subhajit Chaudhury", "Julian Dolby", "Oktie Hassanzadeh", "Zhenhan Huang", "Tejaswini Pedapati", "Horst Samulowitz", "Kavitha Srinivas"], "abstract": "Enterprises have a growing need to identify relevant tables in data lakes; e.g. tables that are unionable, joinable, or subsets of each other. Tabular neural models can be helpful for such data discovery tasks. In this paper, we present TabSketchFM, a neural tabular model for data discovery over data lakes. First, we propose a novel pre-training sketch-based approach to enhance the effectiveness of data discovery techniques in neural tabular models. Second, to further finetune the pretrained model for several downstream tasks, we develop LakeBench, a collection of 8 benchmarks to help with different data discovery tasks such as finding tasks that are unionable, joinable, or subsets of each other. We then show on these finetuning tasks that TabSketchFM achieves state-of-the art performance compared to existing neural models. Third, we use these finetuned models to search for tables that are unionable, joinable, or can be subsets of each other. Our results demonstrate improvements in F1 scores for search compared to state-of-the art techniques (even up to 70% improvement in a joinable search benchmark). Finally, we show significant transfer across datasets and tasks establishing that our model can generalize across different tasks over different data lakes.", "sections": [{"title": "1 INTRODUCTION", "content": "Enterprises store critical data in data lakes, large repositories of tabular data, for both governance and analytics [38]. It is essential to effectively find relevant tables (e.g., joinable, unionable, subsets) within lakes for reporting, decision-making, statistical analysis, and more [29, 30]. For instance, Unionable table search helps to augment an existing table with new rows, and to enrich analytics over it [39]. Joinable table search is useful to run analytics that needs data in columns from multiple tables, e.g., to identify how many customers in some region purchased a product due to an email campaign. Also, identifying subsets or supersets of a table is essential, for example, when searching for potential copies of personal data to comply with the General Data Privacy Regulation of the European Union\u00b9. Motivated by such use cases, we focus on the problem of identifying tables from the data lakes that can be unionable, joinable, and/or subsets of each other [28, 39, 59]. Specifically, in this work, we propose a pre-trained tabular model that could help in enhancing the effectiveness of data discovery techniques.\nCentral to the problem of identifying relevant tables in the data lakes is identifying similar columns in tables. However, the columns in a table could be ambiguous. For instance, a column called Age is different when it is in a table about buildings from when it is in a table about people. Furthermore, values within a column Age play a key role in determining semantics; Age of buildings in New York are unlikely to match Age of historical buildings in Egypt. Contextualizing columns could be useful in resolving such ambiguities. Language Models (LMs) are shown to be successful in contextualizing words in a sentence [10]. Consequently, we use them to contextualize columns and use such contextualization to determine similar columns or tables.\nExisting pre-trained tabular models [9, 27, 52, 53] based on LMs [10] do not focus on identifying relevant tables. Instead, they focus on language-style tasks such as fact checking [52], question answering [22], converting natural language to SQL [53], table cell content population [9], and table metadata prediction [9, 27]. In addition, input to these models is the actual cell values or a row because of the focus on language style tasks. However, this has two limitations for understanding relevant tables: (a) only a small number of values can be passed in as input because of severe limits in the length of input token (e.g. 512 tokens in BERT [10]), and this potentially limits the information available to the model to understand the input table; (b) treating numerical values in the table as text tends to lose their semantics; it maybe better to pass numerical information directly as vectors directly into the model."}, {"title": "2 RELATED WORK", "content": "Tabular Pretrained Models. Neural models are pretrained on tabular datasets to either recover masked tokens in the table or to detect corrupt values [9, 27, 52, 53]. Badaro et al. [2] surveys neural models for tabular data representation and their applications. Deng et al. [9] combined the Masked Language Model (MLM) objective from BERT [10] with a novel Masked Entity Recovery to train TURL. Wang et al. [52] use MLM with novel Cell-level Cloze and Table Context Retrival for TUTA. Yin et al. [53] used Masked Column Prediction and Cell Value Recovery for TABERT while Iida et al. [27] repurposed ELECTRA's objective function [7] for TABBIE. These models are finetuned and evaluated for different downstream tasks such as table metadata prediction, table content population, fact-checking, question answering, semantic parsing, and so on. For example, TABERT is evaluated on neural semantic parsing; TABBIE for column, row, and column type population tasks; and TUTA is finetuned and evaluated for cell and table type classification tasks. These downstream tasks take either a table or a table with a question as input; they do not consider the inter-table tasks such as dataset discovery, which we consider in this paper. Furthermore, they adopt serialization techniques from LMs and represent table values as sentences, whereas we represent the tables efficiently by using sketches.\nDataset Discovery Methods. Different methods such as keyword search [4, 5] and table search [19, 33, 45] have been used for the dataset discovery over data lakes. Nargesian et al. [39] (TUS) presented finding top-k data lake tables that are unionable with a query table. They determine the column unionability using three statistical tests based on value overlap, semantic overlap, and word embedding similarity, and aggregate them to infer table unionability. In D\u00b3L, Bogatu et al. [3] adopted value overlap and word embedding measures from TUS and added three additional measures to measure column unionability: numerical column distributions, column header similarity, and regular expression matching. Khatiwada et al. [28] introduced SANTOS which considers the relationships between the column pairs in both query and data lake tables. Fan et al. [18] developed a contrastive-learning-based table union search technique called Starmie that captures the context of the whole table in the form of column embeddings. Recently, Hu et al. [24] presented AutoTUS which searches for unionable tables by contextualizing entire tables with embeddings of the relationship between the column pairs. Furthermore, there are other table search techniques that focus on finding joinable tables, i.e., given a query table marked with a query column, find the data lake tables that contain a column with overlapping values with the query column. For instance, Zhu et al. [61] defines joinable table search as a problem of finding data lake tables that have a column having high set containment with the query column. They define a novel LSH Ensemble index that approximates set containment computation. Zhu et al. [59] developed JOSIE that searches for top-k joinable tables based on exact set containment. Dong et al. [11]"}, {"title": "3 TABSKETCHFM", "content": "In this section, we detail the architecture of TabSketchFM. Like existing tabular models [22, 52], we pretrain a transformer-based BERT model [10], but change the architecture to accommodate numerical inputs. The goal of building the pretrained model is to create cross encoders that can be used for ranking in data discovery using significantly smaller sized labeled datasets.\nThe first significant departure from other tabular representation models is in how we represent the table as input. While other models linearize table cells as text, we create a sketch for each column which are numerical vectors that need to be translated into a form that can be consumed by the transformer architecture. The sketches capture different features of the columns. Specifically, we create the following sketches for each column.\n1. Numerical sketches. For each column, we record the number of NaNs and the number of unique values (normalized by the number of rows in the table). For string columns, we also record cell width in bytes, since that can be an important determinant in governing if the column is likely to be a join column. For instance, very long strings are unlikely to be join candidates. When possible, we convert date columns into timestamps and then treat them as numeric columns. For numeric columns, we additionally compute a percentile sketch, and add the mean, the standard deviation, and minimum and maximum values. All these features are encoded as individual elements of a single input vector we call the numerical sketch.\n2. MinHash sketches. For each column, we compute a Min-Hash signature of cell values. As shown in Fig. 1, the cell value of Austria Vienna is used as a single element in the set passed to the MinHash computation. For string columns, we also compute another MinHash signature for the tokens within the column. The rationale for this second MinHash is that the tokens can sometimes capture its semantics (e.g., if a token street appears in two different columns, they maybe both about address, which is useful if one considers a union of two tables that do not necessarily share cell values). As shown in Fig. 1, the cell values of Austria and Salzburg get tokenized and used as two different elements of the set passed to the MinHash computation. We concatenate both MinHash sketches into a single input vector for string columns. For numerical and date columns, only the MinHash for the cell values is included in the input vector.\n3. Content snapshot. Row-based information could be crucial in understanding the table semantics for data discovery [28]. We concatenate values within each row to form a string and create MinHash signature for each of them. For instance, we concatenate the last row of table in Fig. 1, surrounded by a green rectangle, into \"Quarterly Austria Salzburg 800,000 28/03/23\" and hash it as another minhash style sketch.\nNow that we have these sketches, which are arrays of numbers, a key question is how to adapt BERT's architecture to consume these numeric values. Transformer models take sentences as input, and internally break it into different inputs such as the tokens of a sentence, the position of tokens in the sentence, and a scalar type indicating if the token belongs to the first sentence or the second. We leverage this mechanism to pass the table-specific scalar inputs as shown in Fig. 1 to the BERT Embedding Layers. For vector inputs (e.g., numerical sketch, MinHash sketch), we expand the notion of BERT Embeddings to add some linear layers, as described below.\nTabSketchFM's specialized layers for processing tabular input are shown in the light gray box, middle-right of Fig. 1. The token"}, {"title": "4 PRETRAINING", "content": "Dataset. Existing works generally use non-public data [2] or web tables [34] to pretrain the models. Unlike enterprise data, web tables often have few rows and columns to ease human consumption [5] and they focus on entities popular on the web (e.g., countries). Their performance may not generalize to enterprise data, where the tables have a large number of rows, the entities are often domain-specific, contain cryptic code words, and have a lot of numerical information. Therefore, we created a de-duplicated pretraining dataset of 197, 254 enterprise-like tables (CSVs) from CKAN and Socrata that are legally cleared to have the open licenses. This pretrain dataset contains 2234.5 rows and 35.8 columns in each table on average. About 66% of columns were non-string, resembling an enterprise datalake.\nData Augmentation. In the image processing literature [1], different transformations are applied over an original image to generate new image samples for training. This makes the trained model robust on different variants of the same image. We want the model to be robust to the order of columns such that shuffling"}, {"title": "5 FINETUNING DATASETS", "content": "After pretraining, we need finetuning to adapt the pretrained model to particular tasks of identifying relevant tables such as unionability, joinability, or subset. Existing finetuning datasets target other tasks such as table metadata prediction [9], question answering over tables [53], and so on, and none focus on relevant table understanding. Therefore, we create LakeBench, a collection of finetuning datasets to identify relavant table pairs using multiple data sources such"}, {"title": "5.1 Unionability", "content": "Two tables A and B are unionable if a subset of their columns are unionable [39], and fully unionable if all their columns are unionable. A column c\u2081 from Table A is considered unionable with a column c2 from Table B if they contain values that are drawn from the same domain. While overlap between values in the columns can indicate that they belong to the same domain, columns with few or no overlapping values may also map to the same semantic type or concept. For instance, two columns titled \"movie\" and \"film\" might have different movie names in them with no overlap, but they can still be unionable because they map to the same semantic type. Generally, we take a union of a new table with an existing table to expand it vertically (i.e., add more rows) [28]. LakeBench contains three datasets for the unionability task: one adapted from existing table search benchmarks [28, 39], one synthesized from Wikidata, and one from ECB data.\n1. TUS-SANTOS. Nargesian et al. [39] created the TUS Benchmark to evaluate the performance of table union search methods over an open data lake. They use 32 large seed tables from Canada's Open Government Data Portal [40] and U.S. Government's Open Data [21]. The seed tables are selected from unique domains such that they are not unionable with each other. Then, they generate 5000 smaller tables by randomly sampling the rows and columns from each seed. The smaller tables generated from the same seed are unionable to each other and the tables from different seeds are not unionable. Khatiwada et al. [28] created SANTOS Benchmark by adapting the same benchmark creation technique [39], but preserving the context and the key entity column during the split. For example, if a seed table contains columns (\"person\", \"birthplace\", and \"birth year\"), the TUS approach might result in a table with"}, {"title": "5.2 Joinability", "content": "We consider a column from Table A as joinable with a column from Table B if the two columns map to the same semantic type and have overlapping values. We join a new table to an existing table to expand it horizontally (i.e. to add more features or columns) [59]. LakeBench contains four datasets for the joinability task; two created from Wikidata, another which was designed from Spider and CKAN/Socrata; and the last reflects existing joins from ECB.\n1. & 2. Wiki Jaccard and Wiki Containment. We use the same corpus of Wikidata tables we used for Wiki Union to derive a benchmark for table joins. For this benchmark, we use the cell-entity (CE) mappings in the ground truth mappings and assign joinability scores to pairs of columns in the collection. The score is either the Jaccard similarity (size of intersection over the size of the union) across sets of CE mappings, or the minimum containment ratio across the sets of CE mappings which indicates an overlap in the entities in those columns and so a potential for joining. Tables T1GB2LNK\u03a4\u039f\u03a7\u03a1. and SW4T7EFEI2DK.csv in Fig. 4 are examples of such tables in our benchmark, because they share a number of values in their first column that map to the same knowledge base entity. These two tasks are modeled as regression tasks.\n3. Spider-OpenData Join. To create this benchmark, we used two data sources, Spider [55] and CKAN/Socrata open government data. Spider is a large-scale human-annotated text-to-SQL dataset annotated by 11 Yale students. It comes with 10K questions and 200 databases with multiple tables covering 138 different domains. Within each database, joinability is clearly identified via"}, {"title": "5.3 Table Subsets", "content": "Table A is a subset of Table B if the rows of Table A are contained in B. The subset task is a useful one for governance as it is often useful to trace the provenance of data across tables in a data lake, and to find potential copies of a table (with some modifications). Table subset is defined as a binary classification task. We provided the models with the column names in the table, which meant that positive and negative examples had the exact same schema, but differed in values. In LakeBench, we provide a subset benchmark created using tables from CKAN/Socrata."}, {"title": "6 EXPERIMENTS", "content": "Now we empirically address the following research questions:\nQ1. How does sketch-based TabSketchFM compare to existing cell-based models for building cross encoders for the tasks in LakeBench?\nQ2. How do different sketches of TabSketchFM impact different tasks?\nQ3. How can one use the cross encoders built from TabSketchFM for search over data lakes?\nWe pretrain TabSketchFM using 4 A100 40GB GPUs for 2 days, until the model converged. Our pretrained model contains 118M parameters, similar to other row-based models in our experiments. We use patience of 5, i.e., we consider the model as having converged if the validation loss does not decrease for more than 5 epochs. As studied in the literature [58], this not only reduces training time but also helps us avoid overfitting. For finetuning the cross encoders, for all tasks, we use 1 A100 40GB GPU. Most finetuning tasks converged within 6 hours using the same 5 epoch patience as pretraining except for the Wiki Union benchmark which took 24 hours. We use cross-entropy loss for classification tasks, mean squared error for regression tasks, and binary cross-entropy with logit loss for multi-class classification tasks. For each finetuning"}, {"title": "6.1 TabSketchFM on LakeBench Tasks", "content": "We evaluate our finetuned cross-encoders and the existing value-based tabular foundational models for the LakeBench unionability, joinability, and subset tasks.\n6.1.1 Baselines. We compare TabSketchFM against four publicly available tabular foundational models: TUTA, TaBERT, TABBIE, and TAPAS. We adapted these models for Lakebench data discovery tasks by building a dual encoder architecture. Each encoder represents the pretrained model with shared parameters; and the same model is used to encode each element of a table pair. The embeddings from the last layer of the encoders were concatenated and passed through a two-layered MLP (multi-layered perceptron). We used this architecture as opposed to the cross-encoder architecture we used for TabSketchFM because in general it is unclear how to feed two different tables with different row sizes into a cross-encoder. It gets even more complicated when one considers models such as TUTA that model hierarchical information in the table as a tree. While we were able to finetune the TUTA and TaBERT code in this dual encoder architecture, the code of TAPAS and TABBIE were less amenable to finetuning. Hence, for TAPAS and TABBIE, we froze their pretrained models while finetuning, but allowed the two layers above the model to learn the weights. In this case, the pretrained model can be seen as producing an embedding that is then fed into two layered network that tunes for a specific task. We will make the code for all the baselines available. Below, we provide details on model-specific implementation issues.\n\u2022 TaBERT provides two types of embeddings for a table: context embeddings corresponding to the tokens in the context, and column embeddings corresponding to each column in the table [53]. For each table, we compute both embeddings for top 10, 000 rows. In Yin et al. [53], a content snapshot approach is described which creates synthetic rows based on the n-gram matches of the input question with the cell values. We could not experiment with this feature, since 1. this technique is not part of the authors' provided source code and 2. LakeBench tasks do not contain a question. So, we compute these embeddings for each table,"}, {"title": "6.2 Effectiveness of sketch types on task types", "content": "To explore the relevance of the sketches and content snapshots for the different data discovery tasks, we conduct an ablation study where we use only one type of sketch. In Table 4 we present another ablation study where we remove one sketch and retained the rest. Not surprisingly, MinHash sketches are crucial for join tasks, where MinHash alone seems to dominate overall performance of TabSketchFM. Subset selection clearly relies on the"}, {"title": "6.3 Application to search", "content": "In the different search benchmarks we describe below, we combined the embeddings of columns derived from TabSketchFM models along with value embeddings for each column because as we describe in the introduction, using numerical and MinHash vectors as inputs to the transformer model made it difficult to include any cell values of a given column. To add cell values, which can contain important semantic information about a column, we created a single sentence by concatenating the top 100 values of a columnn as a sentence, and then embedded it using an off the shelf sentence embedding model (sentence-transformers/all-MiniLM-L12-v2). The\n6.3.1 Join Search. Here, the task is to find all the tables from the data lake that can be joined on one or more columns with the given query table. Note that prior join works [11, 15, 59] evaluate the efficiency of searching for data lake columns having value overlap with the query column(s), and assume that the columns with overlapping values are always joinable. However, we are interested in whether it makes sense to join tables. For example, two columns: (i) explaining people's Ages with values such as (50, 24, 66, 78, 92, 95) and (ii) explaining students' marks out of 100 (rounded to integer) with values such as (78, 92, 95, 100, 20) can have overlapping values but it is not sensible to join them because they have different semantics. Because TabSketchFM is built to generate contextual embeddings of columns that include value overlap, column headings, its context with other columns, and so on, it should be possible to get rid of such useless joins. To our knowledge, there are no such benchmarks with annotated gold standards that consider sensibility. So, we construct a search benchmark named Wiki Join from Wikidata, by following the cell-entity mapping approach used in the curation of the Wiki Jaccard dataset (see Section 5.2). Precisely, we generate the tables and then create a list of all the pairs of joinable columns, i.e., columns with the same entity annotations and overlapping values. We consider the columns having the same entity annotations to be sensible to join. We assign each pair an associated overlap score, which is the Jaccard score of the entity annotation sets. For each column in the list of pairs, we create a ranked list of joinable columns in descending order of their scores.\nWe used existing join search methods such as LSHForest and JOSIE [59], along with embeddings derived from fine tuned versions of other pretrained models such as TABERT as our baselines. A recent embedding-based join search system, WarpGate [8], is not available publicly. So, we implemented a similar Glove-embedding-based baseline [42] (EmbedJoin) that embeds columns into an embedding space and performs a search over HNSW index [36]. Other openly available systems, such as MATE [16] were not easy to install due to software versioning requirements that we could not resolve. Figure 8 shows the significant advantage of TabSketchFM over all other baselines.\n6.3.2 Union Search. As in previous works [18, 24], we use two publicly available benchmark datasets: TUSSmall and SANTOSSmall from [28, 39] because the larger versions of the dataset are not labeled. As a baseline, we compare against state-of-the-art table union search techniques: D\u00b3L [3], SANTOS [28], and Starmie [18]. We report results on D\u00b3L, SANTOS, and Starmie using their publicly available code. We could not compare against AutoTUS [24], another recent union search system, because its code was not public at the time of writing this paper. For each technique, we use default parameters suggested in the respective papers. The results for TabSketchFM compared to the baselines show that TabSketchFM matched the best system [18] on SANTOS, see Figure 9. For TUS, on smaller K, STARMIE was better as shown in Figure 10, due to better recall."}, {"title": "6.3.3 Generalization across tasks and domains", "content": "Figures 11, 12, and 13, show that the fine tuned cross encoder models generalize rather well across tasks as well as domain, which is encouraging particularly because it shows that the models can be trained on a datalake that is quite different from the datalake that it is deployed to. In enterprise contexts, this is an important requirement, because it means that we can basically train models offline and apply it on-line. All models shown in the Figure are models that include the value embeddings from a column for maximal generalization, but using the models without values also produced the same type of generalization."}, {"title": "7 CONCLUSION AND DISCUSSION", "content": "We presented TabSketchFM, a novel transformer-based tabular representation model that inputs the table in the form of several sketches. We empirically show that cross encoders based on TabSketchFM outperform tabular models that serialize tables as texts, for three different data discovery tasks. We also show that the fine-tuned TabSketchFM can be effectively used to rerank the union and join search results. To further the research, we share 8 new datasets (LakeBench) that can be used to finetune neural models to understand tables for data discovery tasks. In the future, we will identify other data discovery tasks that TabSketchFM can be useful, and also make the model more robust for example, by incorporating other hashing mechanisms and cell values to create the sketches."}, {"title": "Equation", "content": " \u03a3{N}_{n=1} (y_i * log(\u0177_i)) (1)"}]}