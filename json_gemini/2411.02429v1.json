{"title": "IdeaBench: Benchmarking Large Language Models for Research Idea Generation", "authors": ["Sikun Guo", "Amir Hassan Shariatmadari", "Guangzhi Xiong", "Albert Huang", "Eric Xie", "Stefan Bekiranov", "Aidong Zhang"], "abstract": "Large Language Models (LLMs) have transformed how people interact with artificial intelligence (AI) systems, achieving state-of-the-art results in various tasks, including scientific discovery and hypothesis generation. However, the lack of a comprehensive and systematic evaluation framework for generating research ideas using LLMs poses a significant obstacle to understanding and assessing their generative capabilities in scientific discovery. To address this gap, we propose IdeaBench, a benchmark system that includes a comprehensive dataset and an evaluation framework for standardizing the assessment of research idea generation using LLMs. Our dataset comprises titles and abstracts from a diverse range of influential papers, along with their referenced works. To emulate the human process of generating research ideas, we profile LLMs as domain-specific researchers and ground them in the same context considered by human researchers. This maximizes the utilization of the LLMs' parametric knowledge to dynamically generate new research ideas. We also introduce an evaluation framework for assessing the quality of generated research ideas. Our evaluation framework is a two-stage process: first, using GPT-40 to rank ideas based on user-specified quality indicators such as novelty and feasibility, enabling scalable personalization; and second, calculating relative ranking based \"Insight Score\" to quantify the chosen quality indicator. The proposed benchmark system will be a valuable asset for the community to measure and compare different LLMs, ultimately advancing the automation of the scientific discovery process. Our code and dataset are available at: https://anonymous.4open.science/r/IdeaBench-2747/.", "sections": [{"title": "Introduction", "content": "Recent years have witnessed the rapid development of Large Language Models (LLMs). LLMs like GPT-4 (OpenAI 2023) and LLama series (Touvron et al. 2023) introduced advanced capabilities that set them apart from previous generations of machine learning models. Among these capabilities, in-context learning allows LLMs to understand and respond to user prompts in a nuanced manner without requiring additional training for each specific task, enabling LLMs to generalize across a wide range of tasks, providing robust state-of-the-art performance even with limited data (Brown et al. 2020). As a result, LLMs have revolutionized the way humans interact with AI systems, making it possible to generate coherent text, translate languages, answer questions, and even compose creative content with unprecedented accuracy and fluency (Bubeck et al. 2023). The impact of these advancements extends beyond consumer applications, influencing various sophisticated domains such as education (Moore et al. 2023), healthcare (Yang et al. 2023a), and scientific research (Wysocki et al. 2024).\nRecently, the impressive performance of LLMs in everyday applications has sparked significant interest in academia, particularly for their potential use in scientific discovery or hypothesis generation (AI4Science and Quantum 2023). Several studies have explored leveraging LLMs to generate hypotheses or research ideas (Yang et al. 2023b; Wang et al. 2023b; Zhou et al. 2024; Baek et al. 2024; Qiu et al. 2023). However, despite numerous results, a unified and comprehensive framework for evaluating generated research ideas is still lacking, making it difficult for the community to clearly understand the performance spectrum of different techniques for generating research ideas.\nTo address this limitation, we introduce a standardized evaluation framework designed to emulate how human researchers generate research ideas. This framework, termed IdeaBench, comprises three main components: dataset construction, research idea generation, and a novel metric to evaluate the quality of the generated research ideas. The intuition behind this framework is grounded in the typical research process of how researchers generate new scientific research ideas as described below:\n1. Targeting a specific topic.\n2. Reviewing related literature, focusing on recent findings and methodologies.\n3. Identifying gaps in knowledge or methods within these recent findings.\n4. Proposing research ideas to address these gaps.\nWe first construct a benchmark dataset that includes meticulously filtered 2,374 target papers' abstracts from biomedical research fields. These target papers serve as the ground-truth sources of research ideas. Additionally, the dataset contains the abstracts of the papers referenced by the target papers, providing the context necessary for LLMs to generate relevant research ideas. This comprehensive dataset aims to capture the complexity and specificity of scientific"}, {"title": "Related work", "content": "Machine Learning for Hypothesis Generation. Most existing research on hypothesis generation has concentrated on literature-based discovery (LBD), aiming to predict pairwise relationships between discrete concepts (Wang et al. 2023a). This approach involves uncovering new scientific knowledge by mining literature to identify meaningful implicit associations between unrelated biomedical concepts. The majority of prior studies have focused on identifying these implicit connections from snapshots of the corpus.\nWhile these LBD-based approaches are accurate and verifiable, they assume that all concepts are known beforehand and need only to be connected, without considering the contextual factors that human scientists incorporate during ideation. Moreover, these methods do not address the inductive and generative nature of scientific inquiry. Recently, several new studies have explored the use of large language models (LLMs) for hypothesis generation. For instance, in (Wang et al. 2023b), the authors presented a framework called SciMON that leverages past scientific literature as context for fine-tuning LLMs for hypothesis generation. MOOSE (Yang et al. 2023b) utilized multi-level LLM self-feedback to boost scientific hypotheses discovery in social science. ResearchAgent (Baek et al. 2024) employed LLMs to automatically generate and refine problems, methods, and experiment designs starting with a core paper and entity-centric knowledge graphs. (Zhou et al. 2024) proposed a prompting approach to iteratively generate hypotheses using LLMs based on training examples.\nEvaluation for Open-ended Text Generation. Although human judgment is still considered the golden standard for evaluating open-ended text generation, the Natural Language Processing community has tried to develop different approaches to approximate human evaluation in a scalable way. Traditional metrics like BLEU (Papineni et al. 2002) and ROUGE (Lin 2004) measure the lexical overlap between model generated content and ground-truth reference. Later on, several efforts use pre-trained language models to measure distributional similarity (Zhang et al. 2019; Zhao et al. 2019) or token probabilities (Yuan, Neubig, and Liu 2021; Thompson and Post 2020). With the increasing popularity and impressive performance of Large Language Models, recent endeavors employ LLMs as autoraters for open-ended text generation (Chiang and Lee 2023; Liu et al. 2023; Bubeck et al. 2023; Bai et al. 2024; Fu et al. 2024; Vu et al. 2024), the effectiveness of using LLMs as autoraters is often reflected by its correlation with human-ratings, making autoraters a promising alternative to human evaluators for large-scale evaluation."}, {"title": "Methodology", "content": "In this section, we introduce the details of the three components of our framework, namely, dataset construction, research idea generation, and evaluation of the generated ideas. The first component is to collect a set of valuable target papers and reference papers so that the reference papers can be used to generate new research ideas and compare with those in the target papers. The second component is to design an LLM prompt tailored for generating research ideas, and the last component is to formulate an evaluation metric to measure the quality of the generated ideas."}, {"title": "Dataset Construction", "content": "The dataset construction consists of two components: curating a set of valuable papers which will be used as the target papers, and accumulating the reference papers which were used to generate ideas in the target papers.\nData Collection. To create a benchmark dataset for evaluating the research idea generation capabilities of LLMs, we meticulously curated a set of high-quality biomedical primary research papers published in 2024. Our goal is to construct a dataset that accurately reflects the state-of-the-art in the field and provides a robust foundation for evaluating LLMs' capabilities for generating research ideas. Motivated by our desire to include only high-quality, peer-reviewed research, as well as those recognized by the scientific community through citations, we retrieve papers either from top venues or from other venues but are recognized by a significant number of citations. We use the Semantic Scholar API (Kinney et al. 2023) to retrieve all biomedical papers published in top biomedical conferences according to Google Scholar venue rankings (Google Scholar 2024) in the year 2024 with at least one citation. We also retrieve papers published from other biomedical venues in the year 2024 that have at least 20 citations. Any duplicate papers are removed. We refer to these selected papers as target papers in which the ground-truth research ideas lie.\nTo further enrich our dataset and provide context, we also extracted the reference papers cited by these target papers. This is done using the Semantic Scholar API as well. These reference papers contain the foundational ideas that motivated the research in the target papers, offering valuable insights into the background and rationale behind each study. By mapping each target paper to its corresponding set of reference papers, we create a comprehensive contextual framework that can aid LLMs in generating coherent and relevant research ideas. Also, to ensure the completeness and usability of our dataset, we disregard papers with critical missing information, such as abstracts. This is crucial for maintaining the integrity of our evaluation, as missing information or poor contextualization could hinder LLMs in understanding the main ideas and prevent fair comparisons with generated research ideas.\nRelevance and Significance Based Reference Filtering. We believe that the reference papers provide the most significant information for generating the new research ideas in the target papers. However, not all references cited in a paper are equally relevant to its central theme. Especially when computing resources are limited, it's vital to focus on the most pertinent and significant references in the target papers. Our motivation for implementing a significance-relevancy-based filtering process is to ensure that the reference papers align closely with the target paper's primary research ideas, thus maximizing the relevance and utility of the information provided to the LLMs. To enhance the relevance of the reference papers, we propose a filtering process that prioritizes references directly contributing to the main research idea of the target paper. This approach excludes irrelevant or overly specific references that do not align with the overarching research theme, thereby optimizing the dataset for the generation of new research ideas under constrained resources.\nThe filtering process is guided by three conditions:\n1. Citation Count Threshold. We exclude reference papers with fewer than five citations to ensure the inclusion of high-quality, widely recognized references.\n2. Non-Primary Research Exclusion. We remove non-primary research references, such as reviews, editorials, letters, or books, as labeled by Semantic Scholar. These sources often contain diverse ideas not directly relevant to the target paper's core research.\n3. Background Section Relevance. We also exclude reference papers that are not cited in the background section of the target paper, as they are less likely to contribute directly to the target paper's research idea.\nThis filtering process ensures that the LLMs are provided with highly relevant and focused information, facilitating the generation of new and meaningful research ideas. We will use random filtering as a baseline, and the effectiveness of our filtering method will be further discussed in the ablation study section. Our approach aims to strike a balance between resource efficiency and the richness of information, thereby advancing the quality of research idea generation."}, {"title": "Research Idea Generation", "content": "In the process of generating a research idea, human scientists rely on relevant background information, typically reflected in the references cited in their published work. To harness the capabilities of LLMs for generating research ideas, we adopt a similar approach by grounding the LLMs in the same context considered by human researchers. Our motivation for this is to emulate human thought processes in LLMs, ensuring that the generated ideas are informed and contextually relevant. Providing LLMs with related information or context is crucial; without it, the models may struggle to meaningfully connect relevant parametric knowledge learned from their pretraining corpus.\nTo achieve this, the abstract of each target paper encapsulates the primary research idea developed by human researchers, while the abstracts of the reference papers contain the key ideas considered during the formulation of these main research ideas. For each target paper, we prompt the LLM with the abstracts of the reference papers as background information. This is accompanied by a specially designed prompt to guide the generation of new research ideas. This process is illustrated in Figure 1, where all the {reference_paper_x_abstract} placeholders are instantiated with the corresponding abstracts of the reference papers. We profile the LLMs as biomedical researchers at"}, {"title": "Evaluation of the Generated Ideas", "content": "A straightforward approach to evaluate the quality of generated ideas is to measure the semantic similarity between the generated ideas and the idea from the target paper. However, a similarity-only metric may fail to capture the nuanced qualities of ideas generated by LLMs, such as novelty and feasibility. To address this, we develop a metric called the \"Insight Score\", which goes beyond a similarity-only approach to assess the quality of generated ideas in a scalable and rigorous manner. The core of our metric is a personalized quality ranking which allows the users to specify any quality indicators, such as novelty, feasibility, etc. By combining personalized quality rankings with the number of generated ideas, our metric provides a nuanced measurement for various quality indicators, effectively highlighting the strengths and areas for improvement in LLMs' ability to generate research ideas. The components of our evaluation framework are detailed in the following subsections.\nPersonalized Quality Ranking for Generated Research Ideas. The first step in our evaluation framework involves a personalized quality ranking. For a given target paper and reference papers pair, we first create an idea set that includes both the generated ideas and the original idea from the target paper. Details on how the original idea is extracted from the target paper are provided in the Appendix. Then we use GPT-40 to rank the quality of these ideas based on user-specified quality indicators, without revealing which idea is the original from the target paper. The motivation behind this approach is to provide a flexible and tailored assessment that aligns with the specific interests of human researchers.\nThe prompt template used to achieve this is shown in Figure 2. In the template, placeholders, denoted by curly brankets {} allow the system to adapt to different scenarios. For instance, if a user wishes to rank research ideas based on their novelty, the system replaces {quality_indicator} with \"novelty\u201d in the prompt. Similarly, {target_paper_idea} is replaced with the target paper's research idea, and {generated_idea_1}, ..., {generated_idea_n} are replaced with generated research ideas. The flexibility of this approach allows other quality indicators, such as feasibility, clarity, ethics, etc., to be used to rank research ideas.\nFurthermore, fueled by the impressive in-context-learning ability (Kojima et al. 2022) of LLMs, the system is able to accommodate a more nuanced understanding of quality indicators held in {quality_indicator}.For example, Bob may define \"novelty\u201d as \u201cdeveloping new methodologies, techniques, or instruments that allow researchers to explore questions in ways that were not possible before,\u201d while Alice might consider \"novelty\" as \"applying existing knowledge or technologies to address new problems or in new contexts.\" The system allows them to instantiate {quality_indicator} with their respective definitions, ensuring the ranking reflects their specific interpretations. Personalized quality ranking ensures that the evaluation is aligned with the user's perspective, providing a more accurate and meaningful assessment of the generated research ideas. Additionally, by not disclosing which idea is from the target paper, the system ensures a fair and unbiased ranking of all ideas.\nRelative Quality Scoring for Generated Research Ideas. The second step in our evaluation framework is relative quality scoring, which builds upon the personalized quality ranking. The position of the target paper's idea within the ranked list of research ideas indicates the quality of the generated ideas with respect to the specified quality indicators. Intuitively, if the target paper's idea ranks higher on the list, it suggests that the generated ideas are of lower quality compared to the target paper's idea. Conversely, if the generated ideas rank higher than the target paper's idea, it indicates that the LLM is capable of producing ideas that may be of better quality than those in the target papers. To quantify different quality indicators, we introduce the following notations:\n\u2022 m: the number of target papers in our dataset.\n\u2022 n: the number of research ideas an LLM generates per query.\n\u2022 $r_{target, q}$: the ith target paper's idea's rank within the corresponding ranked list of ideas given quality indicator q. When n ideas are generated, $r_{target, q} \\in \\{1, ..., n + 1\\}$.\nWe define $I(LLM, q)$ to represent the \u201cInsight Score\" for a given LLM based on a specific quality indicator q as follows:\n$I(LLM, q) = \\frac{1}{1-m} \\sum_{i=1}^{m} \\frac{r_{target q}}{n}$    (1)\nIntuitively, $I(LLM,q) \\in [0,1]$. If all the target papers' ideas rank first on the list, then all the $r_{target, q} = 1$, so $I(LLM,q) = 0$, indicating that the LLM is not capable of generating any research idea that surpass the quality of the target paper's idea with respect to q. Conversely, if all the target papers' ideas rank below all the generated ideas, that is, all the $r_{target, q} = n + 1$, then $I(LLM,q) = 1$, indicating that any idea generated by the LLM is superior to the target paper's idea with respect to q. Relative quality scoring provides a detailed and adaptable framework for assessing LLM performance, allowing for the consideration of user-defined quality indicators and offering insights into the model's strengths and areas for improvement.\nTo ensure a fair comparison across different LLMs using $I(LLM, q)$, it's important to generate the same number of research ideas n for all compared LLMs. Our experiments show that, for a given set of target papers, the ranking of a target paper's research idea can vary depending on the number of generated ideas in the list. This shifting of ranking positions can affect the Insight Scores of the LLMs. We will further discuss the effect of n has on the Insight Score in the Appendix.\""}, {"title": "Experiments", "content": "Experimental setup\nDataset. We curated 2,374 target papers and their corresponding 29,408 reference papers. The total number of filtered reference papers is 23,460. We will present the descriptive statistics of the number of references a target paper has, with and without our filtering process in the Appendix.\nModels. To evaluate LLMs' capability of generating research ideas, we test the latest version of several most popular commercial and open-sourced LLM series with different sizes: Meta LLama Series (Touvron et al. 2023), Google Gemini Series (Reid et al. 2024), and OpenAI GPT Series (OpenAI 2023). All of these models were trained on data with cutoff dates before January 1, 2024, so the target papers published after January 1, 2024 guarantee a fair comparison by avoiding the data leakage issue.\nBaseline Comparison Metrics. To demonstrate the advantage of the Insight Score, we compare it with two similarity metrics: Semantic similarity and idea overlap. BERTScore (F1 score) (Zhang et al. 2019) is used to measure semantic similarity. The practical upper limit of BERTScore is task dependent. To find this upper limit, we compute the BERTScore of the target papers' abstracts and their LLM-summarized research ideas and obtain an average score of 0.718. Although BERTScore ranges from 0 to 1, 0.718 is our practical upper limit.\nThe LLM similarity rating, which uses GPT-40, measures the overlap in ideas between a generated research idea and the abstract of its target paper. It outputs a rating between 0 to 10 for the overlap in ideas, along with an explanation of"}, {"title": "Main Results", "content": "We benchmark LLMs in low and high resource scenarios to assess their ability to generate research ideas. We use semantic similarity and idea overlap to measure their similarity to target papers. We also evaluate research idea generation based on two quality indicators: novelty and feasibility, using the Insight Score. In the implementation, we generate n = 3 research ideas per query. The results for semantic similarity, idea overlap, and the novelty and feasibility Insight Scores are in Table 1. Below we will answer specific questions through the analysis of the results.\nCan LLMs generate research ideas? Most LLMs can generate research ideas that align well with their target papers. Table 1 shows high semantic similarity and idea overlap with target papers for most models, with GPT-40 Mini (high resource) followed by GPT-3.5 Turbo (high resource) exhibiting the highest scores. Generally, we observe that the high resource scenario generates ideas that have higher similarity scores than in the low resource scenario. These similarity scores demonstrate alignment with target paper ideas, indicating that LLMs, although they cannot see the target papers, can comprehend the background information enough to generate research ideas similar to those generated by human researchers.\nHow well can LLMs generate novel research ideas? Most LLMs are capable to generate research ideas that are just as, if not, more novel than their target papers' research ideas. Any Insight Score greater than 0.5 indicates that most generated research ideas are ranked above their target papers' research ideas, concerning a quality indicator. Most of the LLMs yield novelty Insight Scores of over 0.6 with GPT-40 (high resource) having the highest score of 0.766. This means that for most LLMs, most of their generated research ideas are potentially more novel than the research idea of their target paper. This is significant as it demonstrates the potential of LLMs to drive scientific discovery forward with new and innovative research ideas.\nHow well can LLMs generate feasible research ideas? Most LLMs generate research ideas with lower feasibility than their target papers. As shown in Table 1, these ideas generally have low feasibility Insight Scores. GPT-3.5 Turbo (high resource) and Gemini 1.5 Pro (low and high resource) achieve the highest scores, yet all LLMs score below 0.5, indicating that most of their ideas rank lower in feasibility compared to their target papers. Although LLMs can produce novel ideas, their feasibility often remains inferior to human-generated research ideas.\nWhat is the relationship between generating novel and feasible research ideas? For all LLMs, there is a gap between the novelty and the feasibility of their research ideas. Table 1 shows that with the exception of GPT-3.5 Turbo (high resource), all models yield higher novelty Insight Scores than feasibility Insight Scores. The intensity of this gap varies across models. GPT-40 and the LLama 3.1 models exhibit the largest gaps, while GPT-3.5 Turbo, GPT-40 Mini, and the Gemini series of models have smaller gaps. This indicates a general trend toward a trade-off between generating research ideas that are more novel or feasible, with the degree of the gap varying across models. This trade-off is intuitive, as research ideas that propose pursuing more novel, unexplored approaches may be less feasible to implement than ideas suggesting more incremental contributions.\nCan reference filtering help lower-capacity models produce more novel research ideas? Reference filtering plays a crucial role in enabling lower-capacity models to generate more novel research ideas. As shown in Table 1, GPT-3.5 Turbo and Llama 3.1 70B-Instruct, both smaller models in their respective families, yield higher novelty Insight Scores in the low resource scenario compared to the high resource scenario. Due to their lower capacity, these models are likely distracted by irrelevant references from target papers with fewer total references since most target papers have less than 16 references. Thus, reference filtering becomes essential to help smaller models focus on the most relevant ideas, boosting their ability to generate more novel research ideas."}, {"title": "Ablation Study", "content": "Effectiveness of the Insight Score. We explore the effectiveness of our Insight Score by applying it to the quality indicators novelty and feasibility. Figure 4 shows how the Insight Score for novelty and feasibility evolves as we incorporate more references to generate research ideas. As the number of references increases, whether filtered or unfiltered, the Insight Score for novelty also increases. This shows that LLM-generated ideas tend to display more novelty when they are generated with more references.\nHowever, the feasibility of these LLM-generated ideas do not follow the same pattern. As shown in Figure 4, increasing the number of references leads the feasibility Insight Score to plateau at a low level, regardless of whether the references are filtered. Notably, the feasibility Insight Scores remain consistently lower than the novelty Insight Scores, except in the case where the research ideas are generated with only a single reference. In this instance, the novelty Insight Score is low.\nThese findings demonstrate the utility of our Insight Score in capturing complex patterns that similarity metrics may overlook. Our results demonstrate that once a certain threshold of novelty is surpassed, the feasibility of generated ideas tends to decline and stabilize at a lower level. This observation supports the trade-off between novelty and feasibility identified in our main results, further highlighting the importance of our Insight Score in assessing the dynamics between these two important quality indicators.\nEffect of reference filtering on generated research ideas. The alignment of the LLM generated research ideas to the target papers improves as the number of references increases. Specifically, filtering plays a critical role in enhancing the similarity of the generated ideas to the target paper when not all references are provided. Table 2 shows that when all references are not available, filtered references lead to more alignment compared to unfiltered ones. Irrelevant information can cause the LLM's output to diverge when given limited context. By filtering out less relevant references, the LLM is guided to produce ideas that are more closely aligned with the target paper.\nHowever, when all references are available, the benefits of filtering are lost. Table 2 shows that with all references available, unfiltered references produce the most aligned research ideas. This indicates that with sufficient references, the LLM is better equipped to ignore irrelevant information and leverage the comprehensive knowledge provided by all unfiltered references, resulting in research ideas that are most similar to the target papers.\nOverall, using unfiltered references tends to produce the most aligned research ideas when all references are available. However, in scenarios with limited references, reference filtering is beneficial. This is especially relevant given the resource-intensive nature of generating ideas with LLMs as well as the input constraints of some models."}, {"title": "Conclusion", "content": "In this work, we introduced IdeaBench, a benchmark system for evaluating LLMs' ability to generate research ideas based on user-defined quality indicators. The dataset is constructed by emulating human researchers' literature review process, providing grounded contextualization for LLMs to generate research ideas. For evaluation, we proposed the \"Insight Score\", a metric that surpasses similarity-based measures by capturing nuanced, user-specified quality indicators through personalized quality ranking and relative quality scoring. This work can serve as the cornerstone for academia to build up confidence in leveraging LLMs to accelerate ideation in scientific discovery."}, {"title": "Appendix", "content": "Code and Dataset Availability\nDue to the complexity of our dataset, we combined our dataset with all the code needed to generate our results and made them available at: https://anonymous.4open.science/r/IdeaBench-2747/\nImplementation Details\nWe describe the resources used to generate and evaluate research ideas. We used various API services to generate research ideas and to evaluate them. Additionally, we employed accelerated hardware to compute semantic similarity scores between generated research ideas and their corresponding target paper abstracts.\nThe OpenAI API service 1 was employed to generate research ideas that used the OpenAI suite of models. The service was also used for extracting research ideas from target paper abstracts and evaluating research ideas with the LLM similarity rating and the Insight Score. To generate research ideas with the Gemini family of LLMs, Google AI's API service 2 was used. To generate research ideas with the Llama 3.1 family of LLMs, DeepInfra's API service 3 was used.\nTo evaluate the semantic similarity between research ideas and their corresponding target paper abstracts, we computed BERTScores using one NVIDIA A6000 48GB GPU. This hardware allowed for the efficient computation of BERTScores.\nTotal counts of the dataset's target papers and references.\nDescriptive statistics of the number of references per target paper.\nMore details about the OpenAI API service can be found here: https://platform.openai.com/docs/overview\n2More details about the Google AI API service can be found here: https://ai.google.dev/gemini-api/docs/api-key\n3More details about the DeepInfra's API service can be found here: https://deepinfra.com/\nExtracting Research Idea from a Target Paper\nTo ensure a fair comparison between the research ideas generated by LLMs and those in the target paper when ranking the ideas, we extract the core research idea from the target paper's abstract using GPT-4o with a specifically designed prompt. Abstracts often contain distracting information, such as detailed results, which may not directly reflect the central research idea and may bias the Insight Score when ranking research ideas. Therefore, we designed a prompt that focuses on summarizing the main research idea in a way that aligns with how our LLM generates ideas. Figure 5 shows the prompt template. This process enables a fair ranking of the target paper's idea alongside the LLM generated ideas.\nEffect of the Number of Generated Research Ideas on Insight Score\nWe assess how the number of generated research ideas n affects the target paper's absolute rank using GPT-40 Mini, in the same ablation study setting. Figure 6 shows that as n increases, the target paper is ranked farther in the list for both novelty and feasibility, regardless of reference filtering. Changes in the absolute rank of the target paper will affect the Insight Score.\nTo illustrate the effect varying n has on the Insight Score, consider we acquire the Insight Score for an LLM that generates 3 research ideas for one target paper. If the target paper ranks 3rd, then its Insight Score would be 0.667. Now, if"}]}