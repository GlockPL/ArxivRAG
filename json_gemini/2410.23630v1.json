{"title": "Adaptive Alignment: Dynamic Preference Adjustments via Multi-Objective Reinforcement Learning for Pluralistic AI", "authors": ["Hadassah Harland", "Richard Dazeley", "Peter Vamplew", "Hashini Senaratne", "Bahareh Nakisa", "Francisco Cruz"], "abstract": "Emerging research in Pluralistic Artificial Intelligence (AI) alignment seeks to address how intelligent systems can be designed and deployed in accordance with diverse human needs and values. We contribute to this pursuit with a dynamic approach for aligning AI with diverse and shifting user preferences through Multi-Objective Reinforcement Learning (MORL), via post-learning policy selection adjustment. In this paper, we introduce the proposed framework for this approach, outline its anticipated advantages and assumptions, and discuss technical details about the implementation. We also examine the broader implications of adopting a retroactive alignment approach through the sociotechnical systems perspective.", "sections": [{"title": "1 Introduction", "content": "Pluralistic alignment has recently emerged as an area of growing interest within Artificial Intelligence (AI) research [Sorensen et al., 2024a,b]. The term unifies a set of ideas from a developing theme regarding the diverse, multifaceted, and evolving nature of human values and the corresponding challenges this presents to human-aligned AI [Jain et al., 2024, Vamplew et al., 2018]. Given the pluralistic nature of human needs and preferences, human-aligned AI systems must be designed with the capability to autonomously and independently adapt to fit individual users, use cases, and contexts.\nMulti-objective reinforcement learning (MORL) is a powerful AI technique for autonomous sequential decision-making tasks involving multiple, often conflicting, objectives [Hayes et al., 2021]. Through multi-policy techniques, MORL algorithms can learn a spectrum of potential solutions in parallel, each optimised for different objective trade-offs, thereby providing a diverse set of policies that can be dynamically selected at runtime. This adaptability and capacity for balancing competing objectives presents MORL as a promising platform for pluralistic alignment research."}, {"title": "2 Challenges in AI alignment", "content": "Research in AI alignment has grown increasingly critical as AI systems continue gaining ability and prevalence [Ji et al., 2023, Taylor et al., 2020]. Ji et al. [2023] describe these efforts according to two main streams; forward alignment considers how to design new systems that meet these demands, whereas backward alignment looks to ensure alignment of existing systems through regulation, governance, and assurance.\nReinforcement learning (RL)-based approaches feature prominently in forward alignment [Ji et al., 2023], leveraging the premise of learning an optimal policy by seeking to maximise an expected cumulative reward [Sutton and Barto, 2018]. In particular, Reinforcement Learning from Human Feedback is a popular approach [Ouyang et al., 2022], where the reward function is derived from human preferences. These models can be criticised as resource intensive, both computationally and in requiring manually labelled data [Cao et al., 2024, Casper et al., 2023], leading to the emergence of alternative approaches for automating alignment. For example, the Constitutional AI [Bai et al., 2022] and Reinforcement Learning from AI Feedback [Lee et al., 2023] algorithms replace the human in the training loop with another AI model to enable self-improvement with fewer instances of human feedback.\nHowever, these approaches can oversimplify the alignment problem by not accounting for the pluralistic nature of human values [Sorensen et al., 2024b]. The needs and values of different people may vary broadly; even for a single individual across differing contexts, the exact requirements cannot be universally defined [Gabriel, 2020, Mishra, 2023]. By resolving to a single, static solution, these algorithms leave no space to accommodate the natural variability in values and preferences between users and contexts. Furthermore, without the ability to adapt, these solutions may become outdated as preferences change over time.\nAdaptive alignment may help to address this limitation, enabling pluralistic system expression to represent diverse human values and perspectives. Thus far, interactive machine learning approaches such as In-context [Dong et al., 2022] and Active [Taylor et al., 2021] Learning have largely focused on task generalisation. However, some RL-based approaches have recently emerged, enabling adaptive value alignment by representing the task as a Multi-Objective Markov Decision Process (MOMDP) and employing MORL techniques [Harland et al., 2023, Peschl et al., 2022, Rame et al., 2023, Yang et al., 2024].\nThe need for multi-objective approaches for human-alignment in RL is well established [Casper et al., 2023, Mannion et al., 2021, Vamplew et al., 2018], as a priori scalarisation of objectives does not allow for the necessary exploration, visibility, or flexibility of the solution to support alignment [Hayes et al., 2021]. Conversely, by representing human values as distinct objectives, the agent can separately evaluate and balance competing priorities, enabling it to explore a diverse range of potential solutions [Vamplew et al., 2018]. For example, whether to prioritise cleaning or avoid disruptions [Harland et al., 2023, Peschl et al., 2022], or how to balance humour, helpfulness, and harmlessness in a chatbot response [Yang et al., 2024]. Of particular relevance to pluralistic alignment, MORL enables multi-policy learning, such that the specific policy to be executed can be selected a posteriori to the learning process [Hayes et al., 2021].\nYet, a major challenge remains; how may a suitable policy be selected given potentially unknown and dynamic user preferences? Hayes et al. [2021] propose the review and adjust scenario to address how a MORL system may adapt to dynamic user preferences, describing a process of retroactive updates via manual user selection. We propose an extension to this scenario with an approach that circumvents the manual selection process to dynamically adapt to diverse user preferences."}, {"title": "3 An adaptive alignment framework", "content": "In this section, we introduce a framework for pluralistic AI through adaptive alignment in MORL, modelled after the review and adjust process (Section 2). The proposed agent adapts to the user's preferences through a self-review process, using context and informal signals to minimise the need for direct and specific feedback from the user (Figure 1). Two key features are distinct: an initial default policy is chosen in the selection phase, and the role of reviewer is shifted from the user to the agent.\nThe basis of this framework is a trained, multi-objective, multi-policy RL algorithm that has learned a set of solutions representing the scope of possible human preferences across multiple different values. The algorithm represents these values as distinct objectives, and each solution describes an optimal policy for a particular set of preferences over these objectives.\nThe initial policy is selected according to the predicted best fit for the user. For an unknown user, this may either be a universal default selection, or can be initially personalised according to what information is available; for example, the system might prioritise brevity over detail when responding to a voice query. For a familiar user, the choice of policy is informed by previous interactions.\nWith each execution, the agent observes the user's reaction (e.g., facial expression, nonverbal audio) and performs a self-review. The process draws on information collected about the interaction and relevant contextual factors to identify any misalignment between the current policy and the user's preferences. The agent then selects a new policy to dynamically adjust its behaviour accordingly.\nWe anticipate some of the advantages to this approach to be the following:\nFeedback efficiency and focus: The approach alleviates the need for explicit feedback by using the user's reaction as a signal, which should be less burdensome on the user and minimise the influence of response bias. Furthermore, the brevity of the feedback signal provides a narrow focus, which should inherently reduce the dimensionality of the feedback according to its importance to the user.\nAligning with multiple users: This framework has been designed with multiple users in mind. If the current user changes, the previous user's preferred policy could be stored when the new user's profile is created or loaded, so the system can retain what it has learned about each previous user while operating according to the new user's preferences.\nContinuous evolution: The repeating self-review process provides a continuous feedback loop that should enable the system to accommodate new preferences as they arise and so maintain alignment with the users' evolving needs. By also updating the average users' preferences, the system should also be able to improve the initial select for new users and evolve at a broader social level."}, {"title": "4 Techniques for adaptive alignment", "content": "The framework described in the previous section relies on two key assumptions: 1) a suitable model can be developed to accurately detect and attribute misalignment in the system based on the information available to the agent at execution, and 2) given the output from this model, the system can perform an update by selecting an alternative policy that is better aligned with the user's preferences. In this section, we discuss specific techniques to address these assumptions."}, {"title": "4.1 Interpreting user reactions as feedback", "content": "To satisfy the first assumption, we require an interpretation model that enables the user's reaction to act as a feedback signal. That is, we want to find a model M to transform a reaction signal ( into an update \u0394\u039e to the user's preferences \u039e. This model should incorporate information about the interaction (0) so that the signal can be interpreted in context (M \u2192 M(0)). Constituent factors should include the outcome of the execution [MacGlashan et al., 2017], as well as the usual distribution of user preferences for the given use case, and the history of any prior interactions with this user.\nOne possible approach could be to define A explicitly using a loss measure derived from individually idealised reward values Rideal for each objective i. For this example, we assume the reaction signal \u03b6 \u2208 \u039d(\u03bc, \u03c3\u00b2) to be a normally distributed scalar, but other forms are possible [Jeon et al., 2020]. Equation 1 provides an example, given \u00a7 \u2208 N(0, 1) transformed via Bayesian estimation, scaling factor ai, and activation threshold Ti.\n$\\Delta_i = \\alpha_i \\zeta (R_{\\text{observed}} - R_i^{\\text{ideal}}) - T_i \\forall i$ (1)\nAn alternative approach could be to employ a RL algorithm by representing the task as a contextual bandit problem [Bouneffouf et al., 2020]. The model would use the context 0 and a reward signal derived from the transformed reaction \u00a7. Similar models have previously been used for simulating cognitive empathy in human-robot interaction [Bagheri et al., 2021]."}, {"title": "4.2 Solution updates via post-learning policy selection adjustment", "content": "To satisfy the second assumption, we require a process to select a new policy \u03c0' \u2208 II that best aligns with the user's preferences = from the set of known Pareto-optimal policies II [Hayes et al., 2021]. Possible approaches for identifying \u03c0' depend on how II is represented. Pareto-based methods, such as Pareto Q-Learning [Moffaert and Now\u00e9, 2014], store learned policies as vector returns. The format eases direct policy comparison at the cost of high computational complexity to reproduce a policy based on its expected return [Felten, 2024]. Conversely, approximate methods, such as conditioned networks [Abels et al., 2019], may learn a parametric policy representation \u03c0(\u03c6), or use interpolation to compute a mixture policy using a weighted combination of learned policies [Rame et al., 2023, Yang et al., 2024].\nIf each policy in II can be mapped directly to a return vector, it is possible to calculate an ordering over the policies using a utility function u derived from \u039e. The definition ofu could be as simple as applying \u2261 directly as weights for linear scalarisation [Hayes et al., 2021], or may incorporate non-linear features such as thresholds and lexicographical ordering [Harland et al., 2023].\nIf a direct ordering over the policies is not feasible, it might be more suitable to employ a steering- based approach [Vamplew et al., 2017]. Instead of selecting a completely new policy, steering enables stepwise updates to the policy selection, by moving along the Pareto front to the next closest policy or mixture of policies in the direction of the update \u0394\u039e. This approach benefits from a smaller search space and may appear more stable to the user due to the progressive updates, although it may be slower to implement large-scale changes.\nThe agent adapts its behaviour by executing the updated policy selection \u03c0'. The update itself is strictly not a learning process, as the underlying policies are fixed. However, this update process might also help inform aspects of earlier phases (Figure 1): contributing additional data towards the average user preferences to continuously adapt the default selection, and providing an indication of possible objectives not captured in the MOMDP."}, {"title": "5 Implications of a retroactive approach", "content": "The adaptive alignment framework we proposed in this paper follows a retroactive approach to pluralistic AI, with some accompanying implications. We consider these implications through the sociotechnical systems perspective; in matters related to human users, AI algorithms are inseparable from the sociotechnical systems within which they are embedded [Kudina and van de Poel, 2024].\nTechnical challenges for safety: As noted by Ji et al. [2023], algorithms that learn through human feedback may be particularly susceptible to risks of reward hacking and scalable oversight [Amodei et al., 2016]. This could be further aggravated by a self-supervisory method such as we have described that may allow potential issues to be obscured. To minimise this risk, it may be beneficial to incorporate backward alignment features such as explanations to provide transparency on how update decisions are made.\nInevitability of misalignment: Prior to the first interaction with a given user, it is not possible to know that user's preferences perfectly. This fact goes beyond any question of feasibility; even if you were to assume the most advanced superintelligence conceivable, it is not philosophically impossible to predict a user's preferences with absolute certainty. Thus, there will always be a need for AI systems to perform retroactive corrections to realign with the evolving needs of the user. The framework proposed herein is an example of one such system, but it does not need to be used in isolation. Rather, this approach should be combined with suitable and effective predictive alignment techniques to minimise the use of adaptive alignment to only where it is necessary.\nThe need for explanations and repair: The retroactive approach relies on an information signal provided by the user to identify the discrepancy between the current settings and the user's true preferences. This necessitates that misaligned behaviour has already occurred, and corrective action alone may be insufficient to address any harms incurred. Furthermore, users may also adapt their own behaviour throughout the process of interacting with a system as they develop an understanding of how the system behaves. The system may adapt after initially misaligned behaviour, but any subsequent solution will be one step behind, calibrated according to the context of the previous interaction. Thus, when employing a retroactive approach for alignment, it may be appropriate to incorporate both reparations and explanations to support the needs of the user at the sociotechnical scale."}]}