{"title": "Backdoor Attacks against Image-to-Image Networks", "authors": ["Wenbo Jiang", "Hongwei Li", "Jiaming He", "Rui Zhang", "Guowen Xu", "Tianwei Zhang", "Rongxing Lu"], "abstract": "Recently, deep learning-based Image-to-Image (I2I) networks have become the predominant choice for I2I tasks such as image super-resolution and denoising. Despite their remarkable performance, the backdoor vulnerability of I2I networks has not been explored. To fill this research gap, we conduct a comprehensive investigation on the susceptibility of I2I networks to backdoor attacks. Specifically, we propose a novel backdoor attack technique, where the compromised I2I network behaves normally on clean input images, yet outputs a predefined image of the adversary for malicious input images containing the trigger. To achieve this I2I backdoor attack, we propose a targeted universal adversarial perturbation (UAP) generation algorithm for I2I networks, where the generated UAP is used as the backdoor trigger. Additionally, in the backdoor training process that contains the main task and the backdoor task, multi-task learning (MTL) with dynamic weighting methods is employed to accelerate convergence rates. In addition to attacking I2I tasks, we extend our I2I backdoor to attack downstream tasks, including image classification and object detection. Extensive experiments demonstrate the effectiveness of the I2I backdoor on state-of-the-art I2I network architectures, as well as the robustness against different mainstream backdoor defenses.", "sections": [{"title": "1 INTRODUCTION", "content": "In the realm of computer vision, numerous tasks involve the transformation of images from one domain to another, commonly referred to as Image-to-Image (I2I) tasks. For instance, image super-resolution [1] maps low-resolution images to high-resolution images; image denoising [2] maps noisy images to noise-free images; image style transfer [3] maps images of one style to images of another style; image colorization [4] maps grayscale images to color images, etc. In addition, these I2I tasks also serve as crucial pre-processing steps for some downstream tasks like image classification [5] and object detection [6]. For example, image classification tasks are often preceded by the preprocessing of image denoising.\nIn recent years, due to the outstanding performance of deep neural networks, deep learning-based I2I networks (such as MPRNet [7], SCUNet [2], etc.) have increasingly outperformed other techniques in I2I tasks. Despite the spectacular advances of I2I networks, their security has not yet been explored in depth. While some works have explored the vulnerability of I2I networks against adversarial attacks [8], [9], [10], [11], backdoor attacks against I2I networks have been left unstudied. To fill this research gap, this work conducts a comprehensive investigation of the backdoor vulnerability of I2I networks. As depicted in Figure 1, we first introduce a backdoor attack targeting I2I networks. The compromised I2I network functions normally when processing clean input images, i.e., yielding denoised or high-resolution images. However, it consistently exhibits backdoor behavior when the trigger appears in the input image, e.g., producing a predefined image of the adversary. In addition, we further extend our I2I backdoor to attack downstream tasks (such as image classification and object detection), where the attacker has no knowledge of the downstream classifier or detector. As illustrated in Figure 2, the upstream denoising network appears to function normally on input noisy images. However, the denoised version of the backdoor-triggered input image will induce a mis-classification/misdetection\u00b9 of arbitrary clean downstream classification/detection models. It should be pointed out that the backdoor behavior of our I2I backdoor can also be configured to degrade the quality of output images\u00b2, which is similar with adversarial attacks against I2I networks [8], [9], [10], [11]. In this work, we set the backdoor behavior as outputting a predefined image of the adversary. It is more challenging, and can lead to more serious security conse-"}, {"title": "2 BACKGROUND", "content": "2.1 Image-to-image Networks\nOwing to the remarkable advancements in deep learning within the field of computer vision, numerous deep learning-based I2I network architectures have emerged to deal with a diverse range of I2I tasks, encompassing image super-resolution, image denoising, etc. For instance, Wang et al. proposed ESRGAN [21], instead of using the MSE (mean square error) loss, ESRGAN proposes a perceptual loss that contains the adversarial loss and the content loss to enhance image super-resolution performance; DPIR, as proposed by Zhang et al. [22], offers a plug-and-play solution for image super-resolution, streamlining the super-resolution process; Zamir et al. proposed MPRNet [7], a multi-stage I2I architecture used for image restoration; Zhang et al. proposed SCUNet [2], which combines the strengths of residual convolutional layers and Swin Transformer blocks [23], yielding superior image denoising results; Zamir et al. [24] proposed MIRNet, which excels in feature extraction across multiple spatial scales, producing high-quality and high-resolution images.\nIn this work, we conduct comprehensive evaluations on these SOTA I2I architectures to investigate the backdoor vulnerability of I2I networks.\n2.2 Adversarial Attacks against 121 Networks\nA few works have delved into the susceptibility of I2I networks to adversarial attacks. For example, Yin et al. [9] employed the gradient-based adversarial attacks in classification problems to attack the denoising networks with three downstream tasks: image style transfer [25], image classification and image caption [26]; Choi et al. [8], [10] investigated adversarial attacks against various deep I2I networks including colorization networks, super-resolution networks, denoising networks and deblurring networks; Yan et al. [11] proposed an adversarial attack against image denoising networks and developed an adversarial training strategy to enhance the robustness of denoising networks.\nHowever, none of the existing studies explores backdoor attacks against I2I networks. Compared with adversarial attacks that aim to degrade the quality of output images, the I2I backdoor attacks proposed in this work exhibit more severe security threats\u00b3 and can be used for positive appli-cations. This underscores the imperative need to investigate the vulnerability of I2I networks against backdoor attacks.\n2.3 Backdoor Attacks against Image Generative Net-works\nSeveral works have explored backdoor attacks on generative models such as GAN [15], [16], [17] and diffusion model [18], [19], [20]. Concretely, Salem et al. [15] and Rawat et al. [16] proposed backdoor attacks against GANs, where they modified the loss functions of the generator and discriminator to make GAN output the backdoor target image for the triggered input image; Jin et al. [17] extended this backdoor attack in federated learning GAN; Chou et al. [19] and Chen et al. [18] embedded backdoor in diffusion models by manipulating the diffusion process; Struppek et al. [20] developed a backdoor attack against text-image models, where they inject a backdoor into the text encoder to achieve different attack goals, e.g., producing an image of a particular style.\nNevertheless, these backdoor methods cannot be applied and compared in our I2I backdoor attack, because most I2I networks do not have a generator and discriminator and do not entail a diffusion process."}, {"title": "3 THREAT MODEL", "content": "In this work, we consider a malicious I2I network provider, who has control of the training process of the victim I2I network. The adversary trains the backdoored I2I network and makes it accessible for users to download. For the I2I backdoor attack that targets downstream image classification and object detection tasks, the attacker has no knowledge of the downstream classifier or detector. The I2I backdoor attack must satisfy the following requirements:\n\u2022 Normal-functionality. The I2I backdoor must preserve the performance of the I2I network when processing clean input images. In the context of the I2I backdoor attack against I2I tasks, this requirement implies that the compromised denoising/super-resolution network should output normal denoised/high-resolution images for clean input images. In the case of the I2I backdoor attack targeting downstream tasks, for clean input images and the compromised upstream denoising model, the"}, {"title": "4 121 BACKDOOR ATTACK AGAINST 121 TASKS", "content": "In this section, we present the details of I2I backdoor attack against I2I tasks. The workflow is illustrated in Figure 4.\n4.1 Problem Formulation\nWe denote $X_n$ as the normal input image (i.e., the normal low-resolution/noisy image), $Y_n$ as the normal output image (i.e., the high-resolution/noise-free image), $X_b$ as the backdoor-triggered input image, $Y_b$ as the backdoor target image5, $F$ as the target I2I network. According to the requirements described in Section 3, the goal of the I2I backdoor against I2I tasks can be formulated as:\nNormal-functionality goal: $F(X_n) = Y_n$\nEffectiveness goal: $F(X_b) = Y_b$\n(1)\n(2)\n4.2 Backdoor Trigger\nDifferent from backdoor attacks on classification models that map a triggered image to a predefined target class, the mapping relationship in our I2I backdoor (i.e., from a triggered image to a predefined backdoor target image) is more complicated. Directly adopting existing classification triggers can not strike a good balance between preserving normal-functionality and enhancing attack effectiveness (see Section 6.2.1 for detailed experimental results).\nTo resolve this problem, we propose a targeted UAP generation algorithm for I2I networks and use the targeted UAP as the trigger. Different from the UAP for classifiers that induces a targeted misclassification, our proposed targeted UAP for I2I networks is designed to make the output images closer to the predefined backdoor target image. This UAP trigger is thus more conducive to the subsequent backdoor embedding process.\nThe detailed targeted UAP generation algorithm for I2I networks is presented in Algorithm 1. Specifically, for a small set of normal input images $S$, we iteratively pick one sample ($X_i$) from $S$ and employ the gradient descent algorithm to minimize $L_t$ for $I$ rounds to optimize trigger $t$:\n$L_t = ||F(X_i + t) - Y_b||_2$\n(3)\nThe optimization process is performed for all samples in $S$ one by one and the final $t$ is returned as the UAP trigger."}, {"title": "4.3 Backdoor Training", "content": "4.3.1 Backdoor Training with Multi-task Learning (MTL)\nAfter identifying the backdoor trigger pattern, the subsequent step is to embed the backdoor into the I2I model via the backdoor training process. In order to accomplish the dual objectives of ensuring normal-functionality and enhancing attack effectiveness simultaneously, we have devised two loss functions for the main task and the backdoor task. After that, we leverage the MTL framework to conduct the backdoor training process.\nThe main task is to satisfy the normal-functionality goal, i.e., the compromised model is expected to perform normally on normal input images. The loss function can be defined as:\n$L_m = ||F(X_n) - Y_n ||_2$\n(4)\nThe backdoor task is to achieve the attack effectiveness goal, i.e., the compromised model is expected to output the backdoor target image for the backdoor-triggered input image. The loss function can be formulated as:\n$L_b = ||F(X_b) - Y_b||_2$\n(5)\nTherefore, the total loss for backdoor training can be formulated as:\n$L_{total} = L_m + L_b$\n(6)\n4.3.2 Dynamic Weighting Methods\nHowever, in the training process with Equation (6), the $L_{total}$ is prone to be dominated by the task with a larger loss and fall into the local optimum, resulting in lower attack performance. This is attributed to the complicated"}, {"title": "5 121 BACKDOOR ATTACK THAT TARGETS AT THE DOWNSTREAM TASKS", "content": "In addition to attacking I2I tasks, we further extend our I2I backdoor to attack downstream image classification or object detection tasks, where the attacker has no knowledge of the downstream model.\nSpecifically, we first conduct the UAP generation al-gorithm for the surrogate classification/detection model. Subsequently, we attach the UAP to the noise-free image and utilize this compromised image as the backdoor target image for embedding a backdoor into the upstream image denoising model. Capitalizing on the transferability of the UAP, the denoised output of the triggered image will con-tain the classification/detection UAP, thereby inducing mis-classification/misdetection of arbitrary clean downstream classification/detection models."}, {"title": "5.1 Targeting Downstream Image Classification Task", "content": "According to the requirements described in Section 3, the normal-functionality and effectiveness goal of the I2I backdoor attack against the downstream classification task can be formulated as:\nNormal-functionality goal: $C(F(X_n)) = C(Y_n)$\nEffectiveness goal: $C(F(X_b)) \\neq C(Y_n)$\n(10)\n(11)\nwhere $C$ is the clean downstream image classifier.\nFor backdoor trigger types and backdoor training methods, we adopt the same attack configurations described in Section 4.2 and 4.3. Differently, we attach the classification UAP to the noise-free image and use this compromised image as the backdoor target image. Consequently, the denoised version of the input triggered image will contain the classification UAP, thereby leading to a misclassification.\nThe generation algorithm of the classification UAP is presented in Algorithm 2. Specifically, for each sample ($X_i$) in the dataset $S$, the algorithm first determines whether $X_i+u$ is able to cause the misclassification of the model $C$. If not, the algorithm performs an adversarial attack algorithm (such as DeepFool [37], PGD [38]) to optimize $u$ so that $X_i + u$ crosses the classification boundary. The optimization process is conducted for all samples in $S$ and the final $u$ is returned as the classification UAP."}, {"title": "5.2 Targeting Downstream Object Detection Task", "content": "The normal-functionality and effectiveness goal of the I2I backdoor attack against the downstream detection task can be defined as Equation (12) and (13), respectively.\nNormal-functionality goal: $D(F(X_n)) = D(Y_n)$\nEffectiveness goal: $D(F(X_b)) \\neq D(Y_n)$\n(12)\n(13)\nwhere $D$ is the clean downstream object detector.\nSimilarly, we employ the same backdoor trigger types and backdoor training methods described in Section 4.2"}, {"title": "6 EVALUATION", "content": "We perform extensive experiments over different datasets and I2I networks to evaluate the performance of our I2I backdoor attacks. All experiments are implemented in Python and run on a NVIDIA RTX A6000.\n6.1 Experimental Setup\n6.1.1 Model Architecture\n\u2022 I2I backdoor against I2I tasks: this work considers the two most commonly used I2I tasks (image denoising and image super-resolution) as examples to evaluate the backdoor vulnerability of I2I networks. For the two tasks, we have selected several state-of-the-art (SOTA) I2I network architectures, including SCUNet [2], MPRNet [7], MIRNet [24], DPIR [22] and ESRGAN [21], for experimental evaluations. We firmly believe that other I2I tasks and other I2I network architectures are also susceptible to the I2I backdoor attacks in this work.\n\u2022 I2I backdoor against downstream tasks: in the context of the I2I backdoor that targets downstream classification/detection tasks, we employ the aforementioned im-age denoising networks to conduct the upstream image denoising task. For the downstream classification task, we use the pre-trained ResNet50, VGG19 and MobileNetv2 model to perform image classification; for the down-stream detection task, we use the pre-trained MobileNet-YOLOv3, EfficientNet-YOLOv3 and Darknet53-YOLOv3 model to perform object detection.\n6.1.2 Datasets\n\u2022 Image denoising task: we use Color400 [40], [41] as the training data, and CSet8 as the testing data.\n\u2022 Image super-resolution task: we choose BSD100 [42] as the training data, and Set14 [43] as the testing data.\n\u2022 Downstream image classification task: we evaluate our I2I backdoor against the downstream image classification task on the ImageNet-1k [44] dataset.\n\u2022 Downstream object detection task: we evaluate our I2I backdoor against the downstream object detection task on the Pascal VOC dataset [45].\n6.1.3 Attack Configuration\n\u2022 UAP trigger generation process: the number of normal images in D is set to 10, the update step size of the trigger s is set to 5/255, the maximum number of iterations I is set to 20, the range of the trigger is set to (-20/255, +20/255).\n\u2022 Backdoor training process: we follow the hyperparameter settings in UW [33], DWA [34] and PCGrad [35], and train the backdoor model with the Adam optimizer (the"}, {"title": "6.2 Attack Performance Evaluation", "content": "6.2.1 Ablation Study of the Backdoor Trigger\nWe have conducted extensive experiments of I2I backdoor attacks with different backdoor triggers and MTL methods on various I2I network architectures.\nAs presented in Table 1, most triggers achieve high attack effectiveness in attacking image denoising task. However, most of them fail to preserve the normal-functionality. In comparison, the UAP trigger is superior to other triggers in maintaining normal-functionality. As provided in Table 2, only the UAP trigger achieves good attack performance on all these I2I models. To roughly characterize the overall performance of these triggers, we also calculate the sum of the normal-functionality and the attack effectiveness. The results show that the UAP trigger achieves the highest sum score for most cases. It demonstrates that the UAP trigger is more suitable for our I2I backdoor attacks and can obtain a better balance between preserving normal-functionality and enhancing attack effectiveness. This is attributed to the design of the targeted UAP generation algorithm for I2I networks, which makes the output images closer to the predefined backdoor target image.\nBesides, we have assessed the computational overhead of generating the UAP trigger. As provided in Table 3, the computational overhead of the UAP trigger generation algo-rithm is relatively small and falls within acceptable bounds for potential backdoor attackers. Hence, in the subsequent experiments, we use the UAP trigger to perform our I2I backdoor attack.\n6.2.2 Ablation Study of the MTL methods\nWe have further carried out thorough ablation studies focused on the considered MTL methods. In particular, we have monitored the convergence rates of training loss func-tions of these MTL methods. As illustrated in Figure 6, it can be observed that the dynamic weighting methods, including UW, DWA, and PCGrad, always outperform the static weighting method in terms of convergence rates. This phenomenon can be attributed to the inherent complexity of the I2I backdoor task, which involves mapping a triggered image to an unrelated predefined backdoor target image. Such complexity invariably leads to conflicts with the main task. The static weighting method struggles to achieve an optimal balance between these competing tasks, resulting in reduced backdoor training efficiency. Hence, the dynamic weight methods emerge as the more sensible choice for facilitating the I2I backdoor training process.\nFurthermore, we have also evaluated the computational overhead of different MTL methods. As outlined in Table 4, the difference between the computational overhead of these MTL methods is relatively negligible. Therefore, without loss of generality, we have opted to employ the PCGrad method for MTL in the subsequent experiments."}, {"title": "6.3 Robustness Evaluation", "content": "In this section, we turn our attention to the robustness evaluation of the I2I backdoor attack against various defense methods. It should be pointed out that many backdoor defense techniques are designed for neural network clas-sifiers, such as Neural Cleanse [47], STRIP [48], and Spectral Signature [49], they are not directly applicable to our I2I backdoor attacks. We have selected three defense methods, including bit depth reduction [50], image compression [51] and model fine-tuning to evaluate the robustness of the I2I backdoor attacks.\nBit depth reduction. We reduce the bit depth of input images before sending them to I2I models. As illustrated in Figure 7, the effectiveness of the attack consistently maintains a high level as the bit depth decreases. It demonstrates that the preprocessing of bit depth reduction is ineffective in mitigating our I2I backdoor attack.\nImage compression. We compress input images before sending them to I2I models. As depicted in Figure 8, the"}, {"title": "6.4 Evaluation on 121 Backdoor Attack against Down-stream Tasks", "content": "To perform the I2I backdoor attack against the downstream classification task, we first employ the Algorithm 2 to gen-erate the UAP against the pre-trained ResNet152 classifier (the surrogate model). After that, we employ this UAP to embed the I2I backdoor attack into the upstream image denoising model. Finally, we evaluate the attack performance on other clean classifiers, including ResNet50, VGG19 and MobileNetV2.\nIn the case of the I2I backdoor attack against the downstream object detection task, we first construct the detection UAP [39] against the pre-trained MobileNetv1-YOLOv3 detector (the surrogate model). After that, we employ this UAP to embed the I2I backdoor attack into the upstream image denoising model. Finally, we evaluate the attack performance on other clean object detectors, including MobileNetv2-YOLOv3, Darknet53-YOLOv3 and EfficientNet-YOLOv3.\nAs presented in Table 6 and 7, for clean input images, the downstream denoised accuracy/mAP of the backdoor"}, {"title": "7 CONCLUSIONS", "content": "This work fills the research gap in the backdoor vulnerability of I2I networks. Specifically, we propose a novel backdoor attack against I2I networks. To achieve a good balance between normal-functionality and attack effectiveness, the targeted UAP generation algorithm for I2I networks is proposed and the UAP is utilized as the backdoor trigger. To improve the convergence rate of the backdoor training process, MTL with dynamic weighting methods is employed to balance the main task and the backdoor task. Furthermore, we propose an I2I backdoor attack that targets downstream image classification/object detection tasks. Concretely, the backdoor is embedded into the upstream image denoising and the denoised result of the triggered image will induce misclassification/misdetection of arbitrary clean downstream classification/detection models. Extensive experiments demonstrate the effectiveness and the robustness of the proposed I2I backdoor attacks. We hope that the insights and solutions proposed in this work will inspire more advanced studies on I2I backdoor attacks and defenses in the future."}]}