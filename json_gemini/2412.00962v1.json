{"title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics", "authors": ["Mijntje Meijer", "Hadi Mohammadi", "Ayoub Bagheri"], "abstract": "Large language models (LLMs) have become increasingly pivotal in various domains due the recent advancements in their performance capabilities. However, concerns persist regarding biases in LLMs, including gender, racial, and cultural biases derived from their training data. These biases raise critical questions about the ethical deployment and societal impact of LLMs. Acknowledging these concerns, this study investigates whether LLMs accurately reflect cross-cultural variations and similarities in moral perspectives. In assessing whether the chosen LLMs capture patterns of divergence and agreement on moral topics across cultures, three main methods are employed: (1) comparison of model-generated and survey-based moral score variances, (2) cluster alignment analysis to evaluate the correspondence between country clusters derived from model-generated moral scores and those derived from survey data, and (3) probing LLMs with direct comparative prompts. All three methods involve the use of systematic prompts and token pairs designed to assess how well LLMs understand and reflect cultural variations in moral attitudes. The findings of this study indicate overall variable and low performance in reflecting cross-cultural differences and similarities in moral values across the models tested, highlighting the necessity for improving models' accuracy in capturing these nuances effectively. The insights gained from this study aim to inform discussions on the ethical development and deployment of LLMs in global contexts, emphasizing the importance of mitigating biases and promoting fair representation across diverse cultural perspectives.", "sections": [{"title": "1 Introduction", "content": "Over the past few years, large language models (LLMs) have become increasingly prominent in current discussions, both in the scientific and public realm (Bender et al., 2021). Due to significant advances in model performance, LLMs now offer promising avenues for applications across a wide range of fields. For instance, large language models are increasingly being used in various applications that impact people's daily lives profoundly, such as search engines, recommendation systems, and automated decision-making systems. However, while the recent performance of LLMs, such as OpenAI's newly released GPT-4, is impressive, there are also areas of concern. An important concern regarding LLMs is whether and in which areas these models may exhibit bias, such as gender, racial and cultural bias.\nLarge language models are sensitive to embedded bias due to the way they are trained, as they pick up the societal and cultural biases present in the training data. Due to the fact that LLMs are trained on large amounts of data, the models may be (partially) trained on data that reflects embedded societal and cultural prejudices, such as news articles or social media posts (Karpouzis, 2024; Mishra et al., 2024). Consequently, if a language model is trained on data that consistently portrays certain cultural groups negatively or inaccurately, it may adopt and replicate those biased views. Thus, as applications based on LLM outputs become more prevalent, the potential risk of perpetuating cultural bias present in these models increases as well. Therefore, it is important to assess whether LLMs accurately reflect the empirically observed moral judgments present in different cultures. Despite its importance, this issue remains understudied in the literature (Arora et al., 2022; Liu et al., 2023). In evaluating whether LLM's faithfully capture an understanding of the moral judgments across different cultures in a broad sense, it is crucial to assess how well these models reflect inter-cultural differences and similarities in moral judgments. This can be captured in the following research question: \"To what extent do language models capture cultural diversity and common tendencies regarding topics on which people around the world tend to diverge or agree in their moral judgments?\".\nAssessing how well LLMs reflect the differences and similarities across cultures on various moral topics will contribute to developing a wider understanding of how accurately these models grasp empirically observed cultural moral values, such as those recorded in surveys or historical data. As such, this research adds to the scientific debate regarding LLMs' understanding of cross-cultural moral values, which, as mentioned above, has been underexposed. Benkler et al. (2023) emphasize the value of testing and comparing a variety of LLMs, since previous works have shown subtle differences in the ways different LLMs generate output and may embed bias. Moreover, the research question holds significant societal relevance: as the use of LLMs becomes more prevalent in all spheres of life, it is important to assess whether these models accurately represent the diversity of cultural perspectives and moral judgments across the globe (Liu et al., 2023). If this is not the case, model output may perpetuate bias, prejudice and unfairness by representing cultural differences and similarities on moral topics among groups inaccurately. LLMs that do accurately capture the differences and similarities regarding moral judgment across cultural groups, on the other hand, can help identify shared values and ethical principles across diverse communities, thereby aiding in fostering cross-cultural understanding and collaboration.\nIn short, assessing whether LLMs reflect cultural diversity and common tendencies regarding moral topics adds to the scientific debate, and holds significant societal value due to its implications for minimizing bias and prejudice, promoting accurate depictions of cultural groups and cultivating cross-cultural understanding. The aim of this study is to provide insights into the capabilities and societal implications of large language models, specifically related to their understanding of cross-cultural moral values. The study employs three main methods in evaluating models' abilities in capturing and replicating cross-cultural variations and similarities in moral perspectives: (1) comparing variances of model-generated and survey-based moral scores across countries, (2) assessing alignment between country clusters derived from model-generated moral scores and those derived from survey-based moral scores, and (3) probing LLMs with direct comparative prompts to evaluate their recognition of cultural differences and similarities in moral judgments. By assessing LLMs' capabilities in understanding and reflecting cross-cultural differences and similarities in moral perspectives through these methods, the insights gained from this study significantly contribute to informed discussions on the ethical deployment of LLMs.\nThis paper is structured as follows: first, an overview of related literature will be provided. Next, the data and methodology used in this study will be described. Then, the results are presented, followed by the discussion and conclusion."}, {"title": "2 Literature review", "content": ""}, {"title": "2.1 Cross-cultural understanding of moral judgments in LLMs", "content": "Moral judgments refer to evaluations of certain actions, intentions, and individuals somewhere along a spectrum of 'good' or 'bad'. These judgments can significantly vary across different cultures, influenced by factors such as religion, societal norms, and historical contexts (Haidt (2001); Shweder et al. (1997)). As highlighted by Graham et al. (2016), Western, Educated, Industrialized, Rich and Democratic (W.E.I.R.D.) cultures are generally more inclined to endorse moral codes that emphasize individual rights and independence, while non-W.E.I.R.D. cultures tend to more strongly emphasize duty-based communal obligations and spiritual purity. This leads people in W.E.I.R.D. (autonomy-endorsing) cultures to view personal actions such as sexual behaviors as a matter of individual rights, while those in non-W.E.I.R.D. (community-endorsing) cultures are inclined to perceive them as a collective moral concern. Johnson et al. (2022) point out that, while there are many resonant and overlapping values amongst the world's cultures, there are also many conflicting yet equally valid values. Johnson et al. (2022) and Benkler et al. (2023) refer to this notion as 'moral value pluralism' and underscore its importance. Kharchenko et al. (2024) emphasize LLMs' limitations in accurately representing moral value pluralism, stating that general values become improperly embedded in transformer driven models due to the lack of diversity in training data. Du et al. (2024) also state that the emphasis of English in LLMs' training data overshadows the linguistic diversity inherent to human languages and limits the scope of model applicability and innovation. The authors highlight that it is therefore important for LLMs to be trained on multilingual data, and mention that larger data volumes and bigger model sizes enhance performance as well. Arora et al. (2022), too, have suggested that multilingual LLMs, which are trained on text in many languages, may have the potential to pick up cultural values due to the diversity in languages in their training data. However, the lack of diversity within available multilingual training data may still cause multilingual LLMs to perform inconsistently across different languages and cultural contexts. Benkler et al. (2023) highlight that most current AI systems reflect the dominant values of the culture that produces the majority of training data and models. The authors argue that, due to this largely Western and, more specifically, English nature of the training data, LLMs have a moral bias wherein the values of W.E.I.R.D. societies are wrongfully assumed to be universal.\nStudies on AI ethics emphasize the need for models that respect cultural differences and promote equitable treatment (Floridi et al., 2018). Accurately reflecting diverse cultural perspectives is crucial for AI systems to be fair and inclusive (Zowghi and Francesca, 2023; Cachat-Rosset and Klarsfeld, 2023; Karpouzis, 2024; Mehrabi et al., 2021). However, research has shown that biases embedded in training data or model design can lead to disparities in how AI systems interpret and respond to inputs from different cultural backgrounds and contexts. This variability raises questions about the universal applicability and fairness of AI systems (Karpouzis, 2024). Studies such as those by Arora et al. (2022) and Benkler et al. (2023) have highlighted that LLMs may struggle to accurately represent diverse moral frameworks across different cultures. Work by Ramezani and Xu (2023), on the other hand, shows more promising results regarding LLMs' capability of capturing cultural diversity. This divergence in findings underscores the need for further research to bridge the gap in understanding how LLMs perceive and represent moral values across diverse cultural contexts. While most studies suggest that LLMs can mirror some cultural biases present in their training data, their accuracy in representing diverse moral judgments is not yet well understood (Caliskan et al., 2017)."}, {"title": "2.2 The risk of bias in LLMs", "content": "The training process of LLMs involves vast datasets sourced from the internet, which inherently contain societal and cultural biases. These biases can be reflected and even amplified in model outputs, leading to concerns about fairness and representation. This works as follows: LLMs learn to understand language through word embeddings, which are dense vector representations of words that capture the semantic and syntactic relationships between them. Based on the co-occurrence patterns present in large text corpora, these vectors encode contextual information about words. Through these word embeddings, LLM's can pick up biases similar to those of humans from the word associations in their training data (Nemani et al., 2024). In short, LLM's form associations between words and concepts through word embeddings based on their co-occurrence in the training data, which may lead to biased predictions and outputs. Biased outputs from LLMs should be combated, as they can perpetuate stereotypes, reinforce prejudices, and lead to unfair treatment of certain groups.\nVarious studies have documented biases in LLMs, including gender, racial, and cultural biases (Bender et al., 2021; Buolamwini and Gebru, 2018). Bolukbasi et al. (2016), for example, showed that word embeddings can encode significant gender stereotypes regarding profession: the authors found notable associations stereotypically linking 'woman' with 'homemaker' and 'man' with 'computer programmer'. Moreover, Johnson et al. (2022) highlight GPT-3's stereotyping bias shown by association of the word \"Muslims\" with violent actions much more often than \"Christians\". Despite efforts to mitigate bias in LLMs (Mishra et al., 2024), significant challenges remain in eliminating bias. Striving to eliminate bias is an important task as the incorporation of biased language in AI-systems can influence public opinion and decision-making processes, thereby potentially causing harm (Noble, 2018). For example, if an LLM is trained on biased data, it might generate job recommendations that favor men over women for technical roles, thus promoting gender inequality (Bolukbasi et al., 2016). Similarly, GPT-3 linking Muslims to violent actions can lead to heightened social prejudice and discrimination against individuals who identify as Muslim. This biased association in AI-generated content may reinforce negative stereotypes and lead to increased stigmatization of Muslim communities. These significant societal implications of biased AI outputs underscore the importance of developing models that accurately reflect cultural diversity and moral values (Zou and Schiebinger, 2018)."}, {"title": "3 Datasets", "content": ""}, {"title": "3.1 World Values Survey", "content": "World Values Survey \u00b9 collects data on people's values across cultures in a detailed way. The Ethical Values and Norms section in World Values Survey Wave 7 is the first dataset used in this study. This wave ran from 2017 to 2020 and is publicly available (Haerpfer et al., 2022). In this segment, participants from 55 countries were surveyed on their views regarding 19 morally-related statements, such as divorce, euthanasia, political violence, and cheating on taxes. The questionnaire was translated into the primary languages spoken in each country and offered multiple response options.\nFrom the original dataset, only the country name and the answer to each question were retained, which were then normalized between -1 and 1. These scores indicate how justifiable a moral value is, ranging from -1 ('never justifiable') to 1 ('always justifiable'). Normalization is applied to standardize data, enhance comparability, facilitate statistical analysis, and aid in interpretation. To calculate the moral rating for each country-moral value pair, the survey responses were averaged. This approach offers a snapshot of collective attitudes toward moral values within each country. However, it's important to note potential limitations. Averaging may oversimplify diverse viewpoints and obscure outlier perspectives. Additionally, the averaging process may obscure outliers or minority perspectives that could offer insights into the complexities of moral reasoning within a society. However, in the context of this study, averaging emerged as the most viable approach."}, {"title": "3.2 PEW 2013 Global Attitudes Survey", "content": "The second dataset is a survey conducted by the Pew Global Attitudes Project \u00b2, offering a comprehensive collection of data reflecting people's assessments of their views on current global affairs and significant contemporary issues. Undertaken in 2013, this survey offers insights into 8 morally related topics like getting a divorce or drinking alcohol, with 100 participants from each of the 39"}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Pre-processing", "content": "In the preprocessing of version 5 of the World Values Survey (WVS) data, the dataset was initially filtered to retain only the columns corresponding to the moral questions Q177 to Q195 and the country code (B_COUNTRY). These questions cover a range of moral issues, such as tax cheating, accepting bribes, and attitudes towards homosexuality. Following the initial filtering, country names were assigned to each row based on the B_COUNTRY codes using a predefined country mapping dataset. Responses with values of -1, -2, -4, and -5, which represent 'Don't know,' 'No answer,' 'Not asked in survey,' and 'Missing; Not available,' respectively, were replaced with zero. This adjustment was made to ensure that calculations, such as averaging, were not affected by non-responses. The decision to replace with 0 ensures that the structure of the dataset remains intact. It avoids introducing NaN values or leaving cells empty, which could complicate subsequent data analysis tasks such as averaging or statistical modeling. Moreover, a replacement value of 0 ensures that non-responses do not influence the computed averages or other aggregated measures artificially. After replacing non-response values with 0, the dataset was aggregated by country, calculating the mean response for each moral question per country. This provided country-specific average score for each ethical issue. To enable comparisons across different countries and questions, these average scores were normalized on a scale from -1 to 1, where 1 signifies that the behavior is justifiable in every case and -1 denotes it is never justifiable. This normalization involved adjusting the mean responses, which initially ranged from 1 to 10, to fit the new scale. This step was needed for cross-national comparisons. Finally, normalized values were rounded to four decimal places to enhance clarity."}, {"title": "4.2 Models", "content": "In this study, various natural language processing (NLP) models with the same architecture type are used to explore how moral values differ across cultures, based on responses to a series of statements. To ensure a standardized basis for comparison, all models selected are transformer-based and well-suited for the task of text generation. All of the models tested have a decoder-only architecture. Models of this architecture were chosen due to their ability to generate text based on contextual prompts. This makes them suitable for the task of exploring and comparing moral values across different cultural contexts flexibly. The models used in this research originate from Hugging Face \u00b3, a well-known provider of cutting-edge NLP models. Hugging Face models are recognized for their robust performance and reliability, making them a suitable choice for our analysis of moral values across different cultural contexts. Importantly, none of the models were trained or fine-tuned for this study, as the goal is to understand the inherent perspectives these models hold regarding moral topics without the influence of training on similar datasets."}, {"title": "4.2.1 Monolingual Models", "content": "The first part of the study involves employing two monolingual models. The first one is the GPT-2 language model, which is primarily trained on English text. GPT-2 was chosen for its strong performance in generating coherent and contextually relevant text, as demonstrated in various studies. This model has been fine-tuned to accurately predict the probability of a word based on its context within a sentence. Its architecture and training process enable it to generate human-like text, making it a suitable choice for tasks involving nuanced language understanding (Radford et al., 2019).\nIn particular, two versions of GPT-2 were utilized to assess the influence of model size on moral understanding. The models utilized are 'GPT-2 Medium' with 355 million parameters, and 'GPT-2 Large' with 774 million parameters. All models were sourced from Hugging Face. The selection of multiple versions allowed for a comparative analysis of how increasing the number of parameters and computational complexity might increase the model's ability to process and interpret morally charged content. Larger models generally have a"}, {"title": "4.2.2 Multilingual Models", "content": "The second part of the study involves employing multiple multilingual models. Using multilingual models allows for an analysis of how these models, trained on a diverse and extensive dataset, influence moral judgments across different countries compared to monolingual models.\nThe first multilingual model used is the Big-Science Large Open-science Open-access Multi-lingual Language Model, commonly known as BLOOM. BLOOM is a transformer-based, auto-regressive language model designed to support a wide range of languages and was developed as part of the BigScience project. It has been trained transparently on diverse datasets encompassing 46 natural and 13 programming languages, making it highly versatile and capable of generating text across various languages and contexts (Le Scao et al., 2022). BLOOM was chosen for its strong multilingual capabilities, its free open-access nature, and its ability to be instructed to perform text tasks it hasn't been explicitly trained for by casting them as text generation tasks.\nA variant of BLOOM-560M, known as BLOOMZ-560M, which also has 560 million parameters and is provided by BigScience"}, {"title": "4.3 Calculating moral judgment scores", "content": "In assessing whether the chosen LLMs capture patterns of divergence and agreement on topics across cultures, three methods are used: (1) comparing variances of model-generated and survey-based moral scores across countries, (2) assessing alignment between country clusters derived from model-generated moral scores and those derived from survey-based moral scores, and (3) probing LLMs with direct comparative prompts to evaluate their recognition of cultural differences and similarities in moral judgments. For all three methods, moral scores obtained by probing the LLMs with certain prompts are used. For the first two methods, moral scores are computed for all country-topic pairs by assessing the log probabilities that follow from probing the model. In probing the model, the following two prompt structures are used for all country-topic pairs:\n\u2022 In {country} {topic} is {moral_judgment}.\n\u2022 People in {country} believe {topic} is {moral_judgment}.\nThe model is prompted with the above prompts, making use of five different token pairs. Each token pair consists of two contrasting statements for the moral judgment:\n\u2022 pair1 = (always justifiable, never justifiable)\n\u2022 pair2 = (right, wrong)\n\u2022 pair3 = (morally good, morally bad)\n\u2022 pair4 = (ethically right, ethically wrong)\n\u2022 pair5 = (ethical, unethical)\nFor instance, when probing the model to obtain the moral score for the United States on the topic of abortion, the contrasting prompts for the first prompt structure and the first token pair are: \"In the United States abortion is always justifiable\" and \"In the United States abortion is never justifiable\". The model responds to these prompts in the form of log probabilities that indicate the model's predicted moral judgment for each prompt. The responses are then used to compute moral scores for each country-topic pair as follows: the moral score for each moral-immoral token pair is computed by subtracting the log probability for the non-moral statement from the log probability for the moral statement. The result of all token pairs is then averaged to obtain the final model-generated moral score. This is done for both prompt-styles, 'people' and 'in', and the results are averaged to obtain a final moral score for each country-topic pair, akin to the ground-truth WVS scores for each country-topic pair. In summary, these prompts and token pairs are systematically applied across all country-topic pairs to obtain model-generated moral scores. By using contrasting token pairs and averaging results from different prompt styles, this method allows for a nuanced analysis of LLMs' reflection of moral attitudes across various topics and cultural contexts.\nFor the third method, the models will be probed with prompts consisting of direct comparative statements. The prompts used for this method will be described in paragraph 4.3.3."}, {"title": "4.4 Methods", "content": ""}, {"title": "4.4.1 Comparison of variances", "content": "Firstly, a comparison of variances between countries' moral scores on the given ethical topics was conducted. For each ethical topic, the cross-cultural variance in scores was calculated, identifying topics with high variance (more controversial) and low variance (more agreed upon) across cultures. This process was applied to both the WVS and PEW survey moral scores as well as the model-generated moral scores. For every survey-model pair, the two sets of variance scores (one originating from the survey moral scores, and one originating from the model-generated moral scores) were then compared and assessed for association. A strong positive association indicates that the language model effectively captures cross-country variations in moral scores, while a weak positive association or a negative association indicates the opposite. Pearson's correlation coefficient was used to test the strength and significance of the association. This method was also used by Ramezani and Xu (2023) in assessing the ability of LLM's in capturing which topics people across cultures tend to diverge or agree on in their moral judgment. Furthermore, the variance scores were also used to assess which topics were identified as most controversial and most agreed upon according to the surveys and the models, thereby aiming to detect any notable alignments or discrepancies."}, {"title": "4.4.2 Cluster alignment", "content": "Secondly, a clustering approach was utilized to examine the models' abilities to capture the differences and similarities in moral judgments on ethical topics between different countries. The aim of this method was to assess the alignment between country clusterings resulting from the survey scores with country clusterings resulting from the model scores. Hereby, the model's ability to replicate empirically observed cultural patterns of divergence and agreement could be assessed. For this method, countries were first clustered based on their WVS moral scores for the specified ethical topics using K-means clustering. The value for hyperparameter K (i.e. the number of clusters) was chosen by running the clustering algorithm multiple times, each time with a different value for K from a pre-specified range of [1,10]. The value for K that yielded the clustering result with the highest silhouette score was chosen. After having obtained the clustering result based on the WVS moral scores, K-means clustering was also used on the model-generated moral scores for countries. Since the goal is to compare how well the model clusterings align with the ground truth survey clusterings, the value for K that was chosen for clustering countries based on the survey moral scores was also used in this clustering step. The resulting two clusterings of countries (one clustering based on the ground truth moral scores, and the other clustering based on the model-generated moral scores) were then compared for alignment. Subsequently, the degree of alignment between the clusterings based on the survey scores and the model-generated scores was assessed using the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) metrics, which are well-known measures of the similarity between two data clusterings (Nazaretsky et al., 2020; Lazarenko and Bonald, 2021). Additionally, the average of the ARI and AMI was computed to produce a Combined Alignment Score (CAS), providing a holistic measure of alignment between the survey-based and model-generated clusterings.\nFor both the WVS and PEW survey scores, the survey-based clusterings were compared with the model-based clusterings thrice: first including the moral scores on all topics, then including only the moral scores on the most controversial topics, and lastly including only the moral scores on the most agreed upon topics. By first including all moral topics, the general alignment across a broad spectrum of moral topics is assessed. By then focusing on the most controversial topics, the model's ability to capture areas where moral judgments are highly variable and contentious is examined. Lastly, by including only the topics with the highest agreement, the model's ability to recognize common moral ground across different cultures is evaluated. Using the clustering method on the three subsets allows for a comprehensive analysis of the alignment between survey-based and model-generated clusterings, thereby providing a detailed and nuanced understanding of how well the models align with human moral judgments."}, {"title": "4.4.3 Probing with direct comparative prompts", "content": "Thirdly, the models were probed with prompts designed to directly assess their understanding of cultural differences and similarities regarding moral judgments on isolated ethical topics. Specifically, probing with direct comparative prompts was used to assess whether the models recognize that countries belonging to the same cluster (intracluster countries) are similar regarding their moral judgment on the topic in question, while countries belonging to different clusters (intercluster countries) are different in this regard. For each topic, hierarchical clustering was used to cluster countries based on their survey moral score for that topic. For this, the elbow method as described by Saputra et al. (2020) was used to find the optimal number of K clusters. Once the clusters were found, the two most differing clusters for the topic were selected, i.e. the two clusters with the largest difference in their average moral score. Only the two most differing clusters were selected for further analysis. The two most differing clusters were isolated to determine whether the model recognizes the differences in moral attitudes between the most empirically divergent country clusters regarding the moral scores they ascribe to different topics. As mentioned above, this was tested by probing the model with prompts that directly include this comparison. Over the course of 50 trials, two representative countries were randomly chosen from both clusters for the generation of comparative statements. The prompt structure used was:\n\u2022 Regarding the morality of {topic}, the judgments of people in {country x} and {country y} are {comparative token}\nFor the intracluster prompts, 'country x' and 'country y' belong to the same cluster, while they belong to different clusters for the intercluster prompts. The model was prompted with the above intra- and intercluster country comparisons, making use of three different token pairs. Each token pair consists of two contrasting terms for the comparative token:\n\u2022 pair1 = (similar, dissimilar)\n\u2022 pair2 = (alike, unalike)\n\u2022 pair3 = (aligned, misaligned)\nAfter prompting the model using the above token pairs, the average log probabilities in favour of the tokens signifying that the countries are similar or different were calculated. If the average log probability in favour of 'similar' was higher than the average log probability in favour of 'different', the country-pair was labeled 'similar' in their model-estimated attitude on the topic in question, and vice versa if the average log probability in favour of 'different' was higher. The country-pairs on the topic in question were also empirically labeled 'similar' in the case of intracluster country pairs, and 'different' in the case of intercluster country pairs. The results were then evaluated using confusion matrix metrics, as well as a Chi-squared test for association, assessing how well the model-estimated labels aligned with the empirically determined cluster labels for similarity or dissimilarity."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Comparison of variances", "content": "The analysis reveals that the moral score variances of the WVS survey and the model are weakly negatively correlated. The weak negative correlations observed between the WVS variance and model variation are not statistically significant, implying that the model's variance does not correspond significantly to the WVS variance. There is no evidence from this analysis to suggest that the variance in WVS scores is related to the variance in the moral scores generated by the LLMs for these topics. This suggests that the examined models do not accurately capture the variability in moral judgments across different cultures. In short, the weak negative correlations, combined with their insignificance, highlight limitations of the models in their capability to capture the intercultural nuances of moral dimensions as reflected in the WVS survey.\nGenerally, the models ascribe more positive moral scores and lower variance scores to the various WVS topics than is empirically observed in the survey, as can be seen in table 2, showcasing the models' tendencies to mistakenly judge most topics as more morally accepted and uniformly agreed upon globally than they are in reality. When looking at the WVS variance scores, we can identify the topics that are empirically most controversial (highest variance scores) and most agreed upon (lowest variance scores). These topics are shown in tables 3 and 4, respectively. The most controversial and agreed upon topics according to the models can be found in the appendix, in tables 21 to 30. It can be observed that the models do not correctly grasp which topics are most controversial and agreed upon across cultures. Most notably, we observe that sex before marriage and homosexuality are by far the two most controversial topics according to the WVS survey. The models, however, generally do not capture this extremity. Saliently, Qwen, and BLOOM even include one of these two topics in the top 3 most agreed upon topics, as can be observed in tables 29 and 30.\nCompared to the correlation scores for the WVS dataset, the correlation scores between the PEW dataset moral score variance and the model-generated moral score variance are slightly more favorable, as can be inferred from table 5. Notably, a moderate to strong positive correlation can be observed for GPT-2 Medium and BLOOM. This suggests that these models are better at capturing the cultural variance on the topics present in the PEW survey. While the correlation scores for the PEW dataset include moderate to strong positive correlations for GPT-2 Large and BLOOM, none of the correlations reach statistical significance."}, {"title": "5.2 Cluster alignment", "content": ""}, {"title": "5.2.1 All topics", "content": "The cluster alignment scores for most models are very low. The model that performs best is QWEN, with alignment scores much higher than those of the other models, suggesting that the moral scores generated by QWEN yield a clustering that, out of the tested models, best reflects the empirical moral scores for all WVS topics."}, {"title": "5.2.2 Most controversial topics", "content": "The alignment scores for clustering based on WVS moral scores and model-generated moral scores reveal consistent negative values across all models tested. This indicates a lack of agreement in clustering patterns between the models and the WVS dataset for the most controversial topics. GPT-2 Medium, GPT-2 Large, QWEN, OPT-125, and BLOOM all exhibit negative ARI scores. The AMI scores are negative for all models except for GPT-2 Large and OPT-125. This suggests divergent cluster structures compared to the WVS moral scores for all models, with GPT-2 Large showing the best performance relatively while QWEN performs worst."}, {"title": "5.2.3 Most agreed upon topics", "content": "The alignment scores scores show mixed results across the models tested for the most agreed upon topics. GPT-2 Medium and OPT-125 demonstrate positive alignment scores across all metrics, suggesting some agreement in cluster structures with the WVS dataset. In contrast, GPT-2 Large, QWEN and BLOOM exhibit negative alignment scores, indicating divergence in clustering patterns compared to the WVS moral scores and suggesting minimal agreement in clustering for these topics.\nThe alignment scores show varied results across the models tested. GPT-2 Medium demonstrates negative alignment scores across all metrics, indicating significant divergence in cluster structures compared to the PEW dataset. In contrast, GPT-2 Large, OPT-125, and BLOOM exhibit positive alignment scores, suggesting some degree of agreement in clustering patterns with the PEW moral scores. Notably, OPT-125 shows the highest alignment scores among all models tested, indicating a stronger correspondence with the moral judgments reflected in the PEW dataset."}, {"title": "5.3 Probing with direct comparative prompts", "content": "The confusion matrix scores for direct probing based on WVS data as displayed in table 17 show varying performance across the models tested. While all models achieve similar accuracy scores of around 0.5, indicating comparable overall prediction performance, their precision, recall, and F1 scores differ significantly.\nGPT-2 Large and QWEN stand out with high recall scores of 0.946 and 0.831, respectively, caused by their ability to correctly identify instances of one of the two classes. OPT-125 and GPT-2 Medium, which achieve similar accuracy, show lower recall and F1 scores. Although the F1 scores are meagre, they are higher than the F1 score for BLOOM, which demonstrates the lowest overall performance with an accuracy score below 0.5 and notably lower precision, recall, and F1 scores. This suggests challenges in the model's predictive capabilities.\nAgain, all models achieve an accuracy of around 0.5, indicating moderate overall prediction performance. GPT-2 Large shows an accuracy of 0.495 with strong recall (0.954), indicating its ability to correctly identify a high proportion of positive instances. Similarly, QWEN demonstrates an accuracy of 0.493 and a recall score of 0.694, showing similar performance to GPT-2 Large in recall. While GPT-2 Medium's accuracy is on par with the other models, its precision, recall and F1 scores suggest room for improvement. OPT-125 achieves an accuracy of 0.506 with balanced precision (0.506) and recall (0.480), but its F1 score is slightly lower at 0.493. BLOOM performs with an accuracy of 0.497, but its precision, recall, and F1 scores are notably lower, indicating limitations in correctly predicting both positive and negative instances."}, {"title": "6 Discussion and conclusion", "content": "The findings of this study shed light on the capability of large language models to accurately capture cultural diversity and common tendencies across different moral topics. The investigation utilized multiple methodologies that were based on probing LLMs with prompts derived from the World Values Survey (WVS) and PEW datasets, focusing on a range of moral topics."}, {"title": "6.1 Comparison of variance", "content": "The correlation analysis between model-generated moral scores and empirical survey data revealed mixed"}]}