{"title": "Federated Learning with Sample-level Client Drift Mitigation", "authors": ["Haoran Xu", "Jiaze Li", "Wanyi Wu", "Hao Ren"], "abstract": "Federated Learning (FL) suffers from severe performance degradation due to the data heterogeneity among clients. Existing works reveal that the fundamental reason is that data heterogeneity can cause client drift where the local model update deviates from the global one, and thus they usually tackle this problem from the perspective of calibrating the obtained local update. Despite effectiveness, existing methods substantially lack a deep understanding of how heterogeneous data samples contribute to the formation of client drift. In this paper, we bridge this gap by identifying that the drift can be viewed as a cumulative manifestation of biases present in all local samples and the bias between samples is different. Besides, the bias dynamically changes as the FL training progresses. Motivated by this, we propose FedBSS that first mitigates the heterogeneity issue in a sample-level manner, orthogonal to existing methods. Specifically, the core idea of our method is to adopt a bias-aware sample selection scheme that dynamically selects the samples from small biases to large epoch by epoch to train progressively the local model in each round. In order to ensure the stability of training, we set the diversified knowledge acquisition stage as the warm-up stage to avoid the local optimality caused by knowledge deviation in the early stage of the model. Evaluation results show that FedBSS outperforms state-of-the-art baselines. In addition, we also achieved effective results on feature distribution skew and noise label dataset setting, which proves that FedBSS can not only reduce heterogeneity, but also has scalability and robustness.", "sections": [{"title": "Introduction", "content": "Federated Learning (FL), which enables collaborative training of models by sharing parameters among clients without exchanging raw data, has garnered significant attention for its ability to leverage vast amounts of data on clients while preserving privacy. The basic steps of FL are to iteratively execute local model training on multiple clients individually and subsequently aggregate all updated models at the server. Despite its implementation simplicity, the challenge arises from the inherently heterogeneous distribution of FL's training data across clients-characterized by non-identical and independent (Non-IID) data\u2014substantially deteriorating the performance of the obtained global model.\nMany approaches have been devoted to addressing the data heterogeneity problem of FL. One of the representative categories is to consider that the essence of performance degradation is due to the client drift caused by the data heterogeneity where the local updates of each client greatly deviate from the aggregated global one (Karimireddy et al. 2020; Li et al. 2020; Zhang et al. 2022; Li and Zhan 2021; Gao et al. 2022; Louizos, Reisser, and Korzhenkov 2024). To mitigate this drift, many methods are proposed to calibrate the local update from the perspective of optimization (Gao et al. 2022; Karimireddy et al. 2020). For example, some methods leverages the difference between the local update and the global update in old rounds to compensate the local update in current round (Karimireddy et al. 2020; Gao et al. 2022) and other works add regularization on the local loss function to facilitate the local update to approach the global one (Zhang et al. 2022; Li and Zhan 2021; Louizos, Reisser, and Korzhenkov 2024). Besides a few works explore different aggregation strategies on the server (Chan et al. 2024).\nAlthough these approaches have made great achievements, they lack a deep understanding of how the heterogeneous data samples contribute to the formation of client drift.\nIn this paper, we seek to tackle the Non-IID challenge of FL from the sample level by delving into the impact of each local sample on the drift of local update. Specifically, we identify that there exists substantial bias in each sample and the client drift can be viewed as a cumulative manifestation of the biases of all samples. Besides, the bias between samples is different from each other where the sample with large loss is considered to have a relatively large impact on the client drift. Also, we find that the bias of each sample dynamically changes during the FL training process.\nMotivated by this finding, we propose a simple yet effective method, FedBSS, which includes two stages: Diversified knowledge acquisition stage and Progressive knowledge learning stage. The diversified knowledge acquisition phase acts as a model's cold start, warming it up by introducing varied knowledge through standard aggregation techniques. This step prevents early-stage deviations caused by limited knowledge scope, essentially initializing the global model with a diverse set of preliminary information guided by the principle of diversity. The second stage is to dynamically select samples based on their bias to train the local model. Specifically, we measure the bias of each sample by the loss of the global model. Then, during each round, each client sorts the bias of all samples and selects the samples with small loss to train the local model. Considering that filtering out samples with large biases may reduce the Knowledge acquisition efficiency, we further propose progressively adding samples with large biases to the training process. Finally, in order to give the partition boundary of samples with different biases adaptively, we introduce the concept of uncertainty.\nExperiment results on different datasets and models show that FedBSS can significantly improve the performance of the global model under label distribution skew, feature distribution skew and noise data settings compared to baselines. Our contributions can be summarized as:\n\u2022 To the best of our knowledge, this paper is the first to consider to solve client drift issues in the sample level. Our findings reveal that the biases of client samples are different from each other which cumulatively formalizes the client drift.\n\u2022 We propose a novel and fine-grained two-stage approach, FedBSS, which has the diversified knowledge acquisition stage and the progressive knowledge learning stage. The former is based on the principle of diversity and infuses diversified knowledge into the global model. On the basis of the former, the latter sorts the loss of client sample based on the current global model, adaptively divides the client sample by uncertainty, and guides the global model from unbiased sample to biased samples learning through progressive knowledge learning strategies.\n\u2022 To validate the efficiency of the proposed method, we compare FedBSS with state-of-the-art methods on Non-IID data settings. By conducting extensive experiments over various deep-learning models and datasets, we show that FedBSS can not only reduce data heterogeneity but also has scalability and robustness."}, {"title": "Related Works", "content": "Many approaches (Li, He, and Song 2021; Zhu, Hong, and Zhou 2021; Lee et al. 2022; Acar et al. 2021a) have been dedicated to addressing the issue of data heterogeneity in Federated Learning (FL). These methods can be classified into two primary categories: those that calibrate local updates from an optimization standpoint and those that devise aggregation strategies. The specifics of these related works are elaborated upon as follows.\nCalibrating Local Update Previous research has tackled the challenge of Non-IID data in federated learning by calibrating the local model updates. Works such as SCAFFOLD (Karimireddy et al. 2020) address client drift by employing control variates for gradients, albeit without directly considering inconsistencies between local and global objectives. FedDC (Gao et al. 2022), in a different approach, utilizes auxiliary drift variables to monitor and mitigate discrepancies between local and global model parameters. Regarding statistical heterogeneity, FedProx (Li et al. 2020) integrates an additional term into the local model's objective function, suggesting that over-updating the local model could hinder global convergence. This proximal term serves as a penalty, discouraging significant deviations of the local model from the global one. Another strategy, presented by FedLC (Zhang et al. 2022), introduces a fine-tuned calibrated cross-entropy loss for local updates. It does so by incorporating a pairwise label margin, enhancing the sensitivity of the learning process to inter-class differences. Similarly, FedRS (Li and Zhan 2021) addresses the challenge of missing classes by proposing a \"Restricted Softmax\" mechanism, which constrains the updating of weights associated with missing classes during local training. FedDyn (Acar et al. 2021b), on the other hand, introduces a dynamic regularizer tailored to each client to align individual client models with the global model, thereby reducing communication overhead. More recently, integrating contrastive learning and mutual information into the loss function has emerged as another optimization strategy, as illustrated by Louizos et al. (Louizos, Reisser, and Korzhenkov 2024). Strategies such as incorporating additional regularizers into loss functions or leveraging gradient control mechanisms are all aimed at mitigating the drift of local models.\nAggregation Strategy The parameters of client models are inherently prone to drift, making server aggregation strategies another pivotal approach to mitigating the effects of Non-IID data. One such strategy, InCo (Chan et al. 2024)Aggregation, harnesses internal cross-layer gradients-combining gradients from both shallow and deep layers within a server model\u2014to enhance similarity in the deeper layers without necessitating extra client-server communication. Recently, FedCDA (Wang et al. 2024), on the other hand, employs a selective aggregation of cross-round local models, effectively minimizing disparities between the global and local models."}, {"title": "Problem Formulation", "content": "In this setting, we have N clients with their private datasets $D_{n}=\\left{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1}^{S_{n}}$ where $x_{i}$ is the data sample, $y_{i}$ is its label and $S_{n}$ is the number of training sample on n-th client. The objective of federated learning framework is to learn the global model parameter v which minimizes the loss function F(9) on training data of all clients without access to original data:\n$\\begin{array}{l}\\min _{v \\in \\mathbb{R}^{d}} F(v)=\\frac{1}{N} \\sum_{n=1}^{N} F_{n}(v)\\\\F_{n}(v)=\\mathbb{E}_{x \\sim D_{n}} f_{n}(v ; x)\\end{array}$ (1)\n(2)\n$f_{n}(v ; x)$ denotes the loss value with respect to model v and random data sample x."}, {"title": "Motivation", "content": "In this section, we revisit the principles of client drift and ask three questions:\nQ1: What is the impact of different local samples on client drift?"}, {"title": "Methodology", "content": "Motivated by the above findings, we propose a novel and fine-grained framework named FedBSS which is illustrated in Figure 1, where the detailed workflow is shown in Algorithm 1. Our method FedBSS has the following innovations. First, considering that different samples on client have a different degree of drift to the local updates, we evaluate and sort the loss set $S_{n}$ of all the samples on nth client by global model from the server as follows:\n$S_{n}=\\left{\\left(x_{i}^{n}, S_{i}^{n}\\right)\\right\\}_{i=1}^{N_{n}}$, with $S_{i}^{n} \\leq S_{j}^{n}$ if $i<j$ (3)\nwhere $S_{n}$ is the evalution loss set of n-th client and $S_{i}^{n}$ denotes the bottom kth loss in all sample from nth client. Second, in order to divide all sample into biased samples and unbiased sample from nth client, we need to set a classification sample $S_{m i d}$ which when the loss value $S_{i}^{n} \\leq S_{m i d}$, x sample corresponding to the loss value $S_{i}^{n}$ is unbiased sample while the rest is biased samples. However, as mentioned above, different rounds, different clients, and different samples will lead to different thresholds of classification points. Thus, we introduce the uncertainty to adaptively set the classification point for all clients on each round:\n$\\alpha(\\chi ; v))=1-(\\max (p(\\chi^{n} ; v))-\\min (p(\\chi^{n} ; v)))$ (4)\nwhere $\\alpha(\\chi ; v))$ denotes the uncertainty of the model v for the sample x on nth client.\nWe plot the relationship between loss and uncertainty in the model's assessment of the sample as Figure 3b. With the increase of sample loss value, the model's perception of the sample is essentially divided into three stages from figure. Specifically, the model's judgment of sample is certain and accurate, the model's judgment of sample is uncertain and inaccurate, and the model's judgment of sample is certain but inaccurate. Based on the above properties, we can adaptively divide the sample into biased and unbiased sample by the highest uncertainty. Therefore, we take the sample with the highest uncertainty as the classification point at the beginning of client model training:\n$\\left(S_{m i d}^{n}\\right)=\\max (\\alpha(\\chi ; v)))$ (5)\nThen, we propose progressively adding biased samples along during local training process in a epoch-by-epoch manner as follows:\n$\\chi^{e}=\\chi_{n o}+\\alpha \\chi_{b i a s}, \\quad \\alpha=\\frac{1-\\cos \\left(\\frac{e}{\\text { total }} \\pi\\right)}{2}$ (6)\nwhere $\\chi^{e}$ denotes the nth client training sample on client epoch e and global communication round t, $\\chi_{n o}$ is the unbiased sample on nth client, $\\chi_{b i a s}$ is biased samples on nth client, Ee is current client training epoch and Etotal is total epoch in the client training. Note that these samples are in the global round communication on the nth client.\nOne detail is that we do not adopt the scheme of linearly increasing biased samples epoch by epoch, such as $\\chi^{t r e}=\\chi_{n o}+\\frac{e}{E_{\\text { total }}} \\chi_{b i a s}$. The specific comparison results are in section. Concretely, we can discover that the samples near the classification point are relatively dense, while the samples at the edge of loss are relatively scattered, and the span of loss is large from figure 3c. This may result in the linear increment method not fitting the sample curve well. More discussion is demonstrated in section.\nIn addition to the above, we also need an extra first stage as the warmup. This is because the model's initial judgment"}, {"title": "Experiment Setting", "content": "Datasets and models. We evaluate the performance of the proposed FedBSS over two models and four mainstream datasets. In Non-IID setting with label distribution skew, we consider Fashion-MNIST (Xiao, Rasul, and Vollgraf 2017), CIFAR-10 (Krizhevsky, Hinton et al. 2009) and CIFAR-100 (Krizhevsky, Hinton et al. 2009), which contains 10, 10, 100 classes respectively. For CIFAR-10 and CIFAR-100 datasets, we use ResNet-50 (He et al. 2016) as the backbone to train and test the performance while for Fashion-MNIST we use a simple CNN instead. The simple CNN has two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 3x3 max pooling), a fully connected layer with 512 units and ReLU activation. In Non-IID setting with feature distribution skew, we utilize DomainNet dataset.\nData Partition. Follow the setting (Kairouz et al. 2021), we adopt two classic data partitioning strategies, namely label distribution skew and feature distribution skew.\n\u2022 Label Distribution Skew: To evaluate the performance of our work in a heterogeneous scenario with label distribution skew, we specify two Non-IID data partition methods called Shards (McMahan et al. 2017) and Dirichlet (Lin et al. 2020). In the Shards setting, the sorted samples are shuffled into N * S shards, and assigned to N clients randomly. Each client owns an equal number of pieces. In the second setting, data distribution over clients satisfies the Dirichlet distribution by using \u03b1 to characterize the degree of heterogeneity. We set \u03b1 of Dirichlet: {0.1, 0.3, 0.5} and shards for each client: {2,4,8}.\n\u2022 Feature Distribution Skew: In this setting, clients share the same label space while different feature distribution, which has been extensively studied in previous works (Li et al. 2021; Peng et al. 2019; Yang et al. 2023). We conduct the classification task on natural images sourced from DomainNet (Peng et al. 2019), which consists of diverse distributions of images from six distinct data sources. we utilize all six distinct data domains. We selected ten categories-airplane, clock, axe, ball, bicycle, bird, strawberry, flower, pizza, and bracelet-for the classification task. Each client operates exclusively with the data from one domain, and in this setting, all six clients participated in the aggregation of the model.\nBaselines. Beside of FedAvg, we also compare against various types of Non-IID federated learning approaches with the proposed method in our experiments. The first main type includes typical non-aggregation methods that calibrate local update, including Scaffold (Karimireddy et al. 2020), FedProx (Li et al. 2020), FedExP (Jhunjhunwala, Wang, and Joshi 2023), FedLC (Zhang et al. 2022) and FedRS (Li and Zhan 2021). Besides, another representative strategy is to select aggregation on the sever, such as InCo (Chan et al. 2024) and FedCDA (Wang et al. 2024).\nImplementation Details. We implement the whole experiment in a simulation environment based on RTX 3090 GPUs. We use 100 clients in total and randomly choose 10% each round for local training. We set the local epoch to 10, batch size to 64, and learning rate to le - 3. We employ SGD optimizer with momentum of 1e - 4 and weight decay of le - 5 for all methods and datasets. At the same time, we set the number of global communication rounds to 200. For evaluation, we compute the average accuracy and standard deviation over the final 10 rounds of each run. For our method, we set the number of warmup rounds to 50."}, {"title": "Experiments", "content": "Result with Label Distribution Skew\nWe report the comparison results with other baselines in Table 1. In order to demonstrate the generalization of our method, we compare them on two different Non-IID settings, Shards and Dirichlet distribution. We apply different data distributions on different datasets. We can see that our proposed FedBSS achieves the best performance on almost all settings. It demonstrates the effectiveness and benefit of FedBSS. Specifically, on relatively larger datasets such as CIFAR-10 Dirichlet 0.5, FedBSS with ResNet-50 achieves 45.27% accuracy whereas the best baseline method FedLC achieves 40.76% accuracy. In addition, FedBSS with simple CNN also makes improvements on relatively smaller datasets, and the improvement is not less than in large models. At the same time, we can also see that the results of our method on relatively small datasets and simple CNN are not the best, which may be because the client biased samples of different rounds is less drift to the global model on small datasets and simple models, and can not provide better performance of the global model by progressive learning. In conclusion, we can notice our FedBSS makes more improvements on the large model, complex datasets than small model, simple datasets.\nNote that the baseline approaches we compare can actually be divided into two categories. One is calibrating local update, and the other is setting aggregating strategy. The performance of InCo in the table is not good enough. The main reason is that its perception of the client is relatively limited, so the improvement is small. Furtherly, we can see that scaffold outperforms our method FedBSS on some datasets, while it has not the good performance on some other datasets. This indicates that Scaffold is not stable and always sensitive to datasets and hyperparameters. The performance of our method FedBSS is usually more prominent when there is high heterogeneity setting. This may be because the biased samples drifts the model to a large extent when there is high heterogeneity. Therefore, our method can more effectively improve its performance in such scenarios. Besides, our experimental results show that the server aggregating strategy-based methods also improve less on small models and simple datasets. We consider that the main reason is that small models have little parameter, which the effect of directly improving the model parameters is not obvious in this scenario.\nResult with Feature Distribution Skew\nIn the above experiments, we define data heterogeneity with label distribution skew. In this section, we test our approach on feature Non-IID setting with feature distribution skew. We utilize domainNet dataset which contains six domains: clipart, infograph, painting, quickdraw, real, and sketch. We selected ten categories for the classification task and ResNet-18 as backbone. Each client operates exclusively with the data from one domain, and in this setting, all six clients participated in the aggregation of the model. Figure 4 is the result. We can find that our approach FedBSS outperforms FedAvg's performance not only on six very different domains, but also on the final global model.\nResult on Noise Label Datasets\nIn real scenarios, data labels are often noisy or absent. In this section, we employ FedBSS on noise label dataset to test the robustness of our method. Table 2 shows results of our method FedBSS. We set different noise label radio which is 0.1, 0.3 and 0.5 on three datasets. We can find that our method achieve better performance compared with other Non-IID methods. Specifically, our approach offers a significant improvement over other approaches for both relatively smaller dataset Fashion-MNIST and relatively larger datasets CIFAR-100. With the increase of noise label ratio, our method improves more and more significantly, while FedBSS can still maintain good performance even under the most severe noise label ratio condition which is 0.5. It proves that our approach is robust and scalable.\nMore discussion and experiments\nIn this section, we discuss two parts which are mentioned on section in our method FedBSS. The first one is about first warmup stage. The second one is about sample selection strategy.\nWarmup Stage For the first one, we compare with three baselines. Our method sets the warmup rounds as 50. We compare with the baseline which varies the warmup rounds as {0, 25, 100}. In fact, we show the result as Figure 5a. From this, we can see that our method which sets the warmup rounds as 50. Note that this does not mean that 50 rounds is always optimal. However, we can discover that when the warmup round is zero which means the first warmup stage doesn't exist, the performance of the model is always worse. This may be due to the lack of diversity knowledge in the first stage. Certainly, when warmup rounds are too high, the performance is worse too. This is because the first stage is too long, resulting in too many global model drifts, which makes the progressive learning effect of the second stage not good. Therefore, the warmup rounds are a relatively sensitive parameter. We think the first warmup phase is a good trade-off between minimizing client drift and minimizing knowledge sacrification. Specifically, the first phase helps the model acquire diversified knowledge, while the second phase mitigated the phenomenon of client draft.\nSample Selection Strategy We have discussed the biased sample selection strategy in section. To summarize, there are three strategies: filter, linearly addition and our method. We compared these three strategies on Cifar10 dataset and use ResNet-18 as the backbone while other parameters are the same as above. Figure 5b shows the result. Our method proves the superiority and it outperforms the other two strategies. It proves that filtering biased samples may cause the model to lose part of its knowledge and fall into local optimal solutions or slow convergence. On the other hand, the effect of linear inclusion of biased sample is not good, which may be due to the sample distribution, which is more concentrated in the middle part and dispersed in the edge part. This may result in a linear function that does not fit the sample distribution well."}, {"title": "Conclusion", "content": "This paper focuses on Non-IID setting in federated learning. We have observed that there exists substantial bias in each sample and the client drift can be viewed as a cumulative manifestation of the biases of all samples. Besides, the bias between samples is different from each other. Motivated by this finding, we propose a novel two-stage effective method for FL called FedBSS that exploits the drift properties of different samples. Our method is to adopt a bias-aware sample selection scheme that dynamically selects the samples from small biases to large epoch by epoch to train progressively the local model in each round. We analyze and demonstrate its effectiveness and robustness through a lot of experiments."}]}