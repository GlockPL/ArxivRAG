{"title": "Large Language Models for cross-language code clone detection", "authors": ["Micheline B\u00e9n\u00e9dicte MOUMOULA", "Jacques KLEIN", "Abdoul Kader KABORE", "Tegawend\u00e9 BISSYANDE"], "abstract": "With the involvement of multiple programming languages in modern software development, cross-lingual code clone detection has gained traction with the software engineering community. Numerous studies have explored this topic, proposing various promising approaches. Inspired by the significant advances in machine learning in recent years, particularly Large Language Models (LLMs), which have demonstrated their ability to tackle various tasks, this paper revisits cross-lingual code clone detection.\nWe investigate the capabilities of four (04) LLMs and eight (08) prompts for the identification of cross-lingual code clones. Additionally, we evaluate a pre-trained embedding model to assess the effectiveness of the generated representations for classifying clone and non-clone pairs. Both studies (based on LLMs and Embedding models) are evaluated using two widely used cross-lingual datasets, XLCOST and CodeNet. Our results show that LLMs can achieve high F1 scores, up to 0.98, for straightforward programming examples (e.g., from XLCOST). However, they not only perform less well on programs associated with complex programming challenges but also do not necessarily understand the meaning of code clones in a cross-lingual setting. We show that embedding models used to represent code fragments from different programming languages in the same representation space enable the training of a basic classifier that outperforms all LLMs by ~2 and ~24 percentage points on the XLCOST and CodeNet datasets, respectively. This finding suggests that, despite the apparent capabilities of LLMs, embeddings provided by embedding models offer suitable representations to achieve state-of-the-art performance in cross-lingual code clone detection.", "sections": [{"title": "1 INTRODUCTION", "content": "Copy-pasting a code fragment with or without change is common practice in software development, and this action produces code clones [29]. It reduces the quality of the code, making maintenance costly, and can be a source of bugs [13, 29]. Research shows that we can find between 5% to 23% of clones in a software system [18, 41]. We distinguish four types of clones ranging from type 1 to type 4 [15]. Types 1, 2, and 3 clones are generally classified as syntactic clones, and type 4 is referred to as semantic clones. Syntactic clones occur when there are textual similarities, while semantic clones represent functional similarity. Type 4 is the most complicated clone and the hardest to detect [6].\nWe often retrieve code clones within the same programming language in software development. However, in modern and complex software development, the multiplicity of software components implies using several programming languages to solve the challenges [17]. For instance, developers can use C, C++, Java, or Kotlin for Android applications and Objective-C for iPhone applications. Intentionally, developers may introduce clones for building identical systems for different platforms. Also, the development of those systems is, for the most part, collaborative. So, if an expert in a certain type of programming language changes one part of the software for a given language, the exact functional change must be done on the other variants. This is more resource and time-consuming than modifying a software system based on a single language, as it requires prior knowledge of the system architecture and an understanding of the code changes performed by the first developer [23]. To manage cross-lingual systems in an easy, time-effective, and cost-effective way, developers need an automatic system that can detect clones of various languages at the same time[23].\nThe literature includes several approaches and tools for code clone detection. For instance, CCFinder [14], Deckard [12], NiCad [28], SourcererCC [33], NIL [24], and OREO [32] have been developed for type 1, 2, and 3 clones detection within the same programming language. For semantic clones within the same language, RtvNN [41], CDLH [40], DeepSim [44], and FA-AST [39] have been proposed. Concerning cross-lingual clones detection, tools such as LICCA [38], CLCMiner [3], CLCDSA [23], C4 [35], TCCCD [9], CCT-Code [34], AdaCCD [7] and [16] have been introduced. They rely on deep learning techniques to capture the syntactic and semantic relationships between different parts of the source code. CLCDSA employed on-the-fly and deep neural network-based approaches using Abstract Syntax Trees (ASTs), a fundamental data structure in computer science for representing code structure, to extract features for cross-lingual code clone detection. On the fly approach measures cosine similarity between the two matrices generated by extracting values of the features from source code fragments while the deep neural network-based approach learns automatically the values of features from data and detects cross-lingual clones. TCCCD employed the UniXcoder [11] pre-trained model to map code clones from different languages into vector space and fine-tuned the model using triplet learning. Sorokin et al. proposed an approach using cross-consistency training (CCT) for language models training on"}, {"title": "2 BACKGROUND", "content": "This section presents a short introduction to the concepts of code clone and LLMs."}, {"title": "2.1 Code Clones", "content": "A pair of code clones represents two code fragments that are similar, by some given definition of similarity [30]. We can enumerate four types of clones :\n\u2022 Type-1 or identical code fragments represent the same code except for white space, comments, and layout.\n\u2022 Type-2 or lexical code snippets represent identical clone pairs except for differences in variables or function names with Type-1 clone differences.\n\u2022 Type-3 or syntactically represent similar code fragments that differ at the statement level. The code fragments differ in some lines with removed or added of some lines in addition to type-2 clone differences.\n\u2022 Type-4 or semantic code clone represents code snippets that perform the same functionality but the implementation is different. In global they are syntactically dissimilar.\n\u2022 Cross lingual clone is a semantic clone when code fragments are written in different programming languages. Our work is focused on semantic code clone detection for cross-lingual code snippets."}, {"title": "2.2 Large Language Models", "content": "LLMs are a type of artificial intelligence (AI) trained on massive datasets of text and code. This training enables them to perform a variety of natural language processing (NLP) tasks, including text summarization, text completion, machine translation, and even creative text generation. While LLMs are still under development, they have demonstrated the ability to generate human-quality responses to prompts.\nPrompts are instructions or queries provided to an LLM to guide its output towards a specific task. Prompt engineering is a crucial aspect of LLM performance and refers to the optimization of prompts to achieve the most accurate and desirable outputs [31]. Chain-of-thought prompting is a specific technique that utilizes a sequence of connected instructions to guide the LLM's reasoning process step-by-step, resulting in demonstrably more accurate solutions for complex problems"}, {"title": "3 EXPERIMENTAL SETUP", "content": "First, we articulate the research questions that we aim to address. Second, we introduce the models that are leveraged in this study before presenting the datasets and describing the metrics. Finally, we introduce our experimental methodology on applying LLMs and embedding models for cross-lingual code clone detection."}, {"title": "3.1 Research questions", "content": "We conduct our study around five research questions. The first four research questions are related to evaluating LLMs' performances in detecting cross-lingual code clones. In the last research question, we also evaluated the effectiveness of an embedding model and compared its performance against LLMs."}, {"title": "3.2 Models", "content": "We evaluated four LLMs and one embedding model for cross-lingual code clone detection. We chose the popular GPT-3.5-Turbo model from OpenAI due to its prevalence in LLM-based software engineering research. To explore open-source options, we included Falcon-7B-Instruct, LLAMA2-Chat-7B, and Starchat-\u00df, all trained on multiple programming languages. Unlike prior studies focusing solely on LLMs, we incorporated the Text-embedding-Ada-002 model, also from OpenAI's web API, to investigate the effectiveness of pre-trained embeddings for this task. We provide detailed descriptions of each model in the following section.\n\u2022 GPT-3.5-Turbo is an LLM developed by OpenAI, known for its strong performance in various language tasks. It excels at understanding and generating text in a conversational context, producing coherent and relevant responses based on user input. Notably, the model can retain information short-term, facilitating meaningful dialogue. Trained on a massive dataset of internet text, GPT-3.5-Turbo possesses a broad knowledge base and can leverage this information for diverse language processing applications.\n\u2022 The Falcon-7B-Instruct model is a large language model (LLM) with 7 billion parameters. It is based on the Falcon-7B architecture and has been fine-tuned on a combined dataset of conversational and instructional text. This fine-tuning process optimizes the model for performing tasks typically associated with virtual assistants."}, {"title": "3.3 Datasets", "content": "We selected two cross-lingual benchmarks to ensure a robust and comprehensive evaluation across all considered programming languages: XLCOST [45] and CodeNet [27].\n\u2022 The XLCoST dataset [45] is collected from GeeksForGeeks\u00b2, a popular online platform for computer science education and skill development. Notably, it hosts coding challenges for programmers. The dataset includes samples from seven different programming languages (C, C++, C#, Java, JavaScript, PHP, Python ). Since the dataset includes only true pairs of cross-lingual code clones, we must undertake to create negative clone pairs.\n\u2022 The CodeNet dataset [27] comprises a large collection of code samples with extensive metadata across 4000 coding problems and over 50 programming languages (although C++, C, Python, and Java are the dominant languages). The code samples are annotated with a rich set of information, such as the problem ID and the status, which indicates acceptance or error types. The data is collected from AIZU Online Judge 3 and AtCoder 4. Based on the problem ID, we can construct both, positive and negative clone pairs\nFor our experiments, we consider random subsets of 3000 clone pairs from XLCOST and 3000 clone pairs from CodeNet.\nConstruction of negative sets. To adequately benchmark the different clone detection approaches, we must create negative samples of non-clone pairs. We thus consider creating negative pairs within and accross programming languages. For the XLCost dataset, each problem has a short description and a unique ID. We therefore mixed the clone pairs having different IDs but from the same"}, {"title": "3.4 Metrics", "content": "Code clone detection can be considered as a binary classification problem. Therefore, we rely on classical metrics for classification evaluation, notably Precision, Recall, and F1 score. Precision represents the proportion of true, correct positive predictions to all of the positive predictions (TP/(TP+FP)). The recall represents the proportion of true correct positive prediction to all of the true positive samples (TP/(TP+FN)). F1 score is defined as the harmonic mean of precision and recall."}, {"title": "3.5 Methodology", "content": "We overview the experimental methodology that is employed for assessing LLMs as well as classification models for the task of cross-lingual code clone detection.\nCross-lingual code clone detection as an NLP task. We investigate the potential of LLMs for cross-lingual code clone detection using prompt engineering to explore the highest potential that LLMs can provide for this task.\nWe build on zero-shot and chain-of-thought prompting techniques and design eight prompts which, each include a specially-crafted instruction as well as the raw content of the pair of code fragments to analyse for clone detection. The expected output format varies depending on the prompt design: it can be either a straightforward binary \"yes/no\" answer or a numerical value representing a similarity score.\nThe proposed experimental methodology aims to assess the LLMs' ability for actual analysis of code semantics across different programming languages. Ultimately, we can provide measurements on the efficacy of LLMs for accurate cross-lingual code clone detection.\nLLM Prompting. Our experiments are considering several prompts designed following two strategies: Zero-shot prompting, which is a classical and baseline approach used in the current literature [7] and Chain-of-Thought, which is a state-of-the-art prompting strategy that has been shown successful for several software engineering tasks [6].\nCross-lingual code clone detection as a Classification task. We investigate, under the same conditions as LLMs, the potential for traditional classification mechanisms based on machine learning models. To that end, we follow the steps used by Keller et al. [15] in their learning-based approach for code clone detection. In their approach a classifier is trained based on sample sets of clone"}, {"title": "4 EVALUATION", "content": "For each research question presented in 3.1, we first present the experiment we conducted and then show the result."}, {"title": "4.1 [RQ.1] LLM Performance on Cross-Lingual Code Clone Detection and Impact of Prompt Engineering", "content": "Goal. We aim to fill a gap in the literature with a comprehensive analysis of the effectiveness of LLMs on the task of cross-lingual code clone detection. In particular, we investigate the impact of state-of-art prompt engineering strategy on the performance of the LLMs: we thus experiment with Zero-shot and Chain-of-Thought (CoT)-based prompts. Depending on the desired output formats, the designed prompts fall into two categories:\n\u2022 output is a score [0-10]: We design a prompt that instructs the models to generate a similarity score between 0 and 10 when given a pair of code fragments: the lowest score (0) means that the model finds no similarity between the fragments, while the highest score (10) means that the model finds that the similarity is perfect. This prompt is designed using the CoT prompting strategy.\n\u2022 output is Yes/No: We design several prompts to guide the model towards answering with \"Yes\" or \"No\" when given a pair of code fragments. One of these prompts is based on a straightforward Zero-Shot strategy, while we provide five (5) prompts building on Chain-of-Thought. The design of the prompts is varied to take into account the requested depth of analysis (line by line or overall block), the specification of need to provide a reasoning of the process, the breaking of the steps for decision, etc. This methodology is inspired by prior work [6] leveraging LLMs for clone detection, which has not assessed cross-lingual capabilities.\nExperiments. We evaluated the four LLMs considered in this study (cf. Section 3.2), namely GPT-3.5-Turbo, Starchat-\u1e9e-16B, Falcon-7B-Instruct, and LLAMA2-Chat-7B, on the randomly constructed subsets Section 3.3) from XLCOST and CodeNet. We use the ChatGPT API to prompt the GPT-3.5-Turbo LLM and parse the outputs to compute the precision, recall, and F1 scores finally. For prompts leading to outputs with a similarity score, we analyzed the LLMs' outputs across a range of threshold values for code clone decisions.\nResults. Our experiments confirmed that many LLMs actually fail to support the Chain-of-Thought prompting strategy, confirming a claim by Dou et al. [6]. Consequently, to analyze the performance of LLMs across different prompts, we focus on GPT-3.5-Turbo, which produced results when instructed with all our designed prompts.\nOn the XLCOST dataset, the CoT-based \"Separate explanations\" prompt enables the model to achieve the highest F1 score (98% for non-clones and clones). However, on the CodeNet dataset, the same prompt achieves the second-lowest score (%), just ahead of the \"Separate code\" prompts. This discrepancy in performance is due to the difference in complexity between the two datasets. Indeed, XLCOST is mainly composed of source code implementing simple problems such as \u201cCompute modulus division by a power\u201d while CodeNet is built based on coding challenges of higher complexity such as \u201cGiven a string, find the minimum number of swaps needed to rearrange the characters into a palindrome\". Furthermore, some LLMs exhibit a clear difference in their ability to detect code clones compared to non-clones. For instance, on the CodeNet dataset, the GPT-3.5-turbo model shows a 26 percentage point variation in the f1 score (73% on non-clone vs. 47% on clones). On the same dataset, we also observe a high variation of 53 for the 'Falcon-Instruct-7B' (12% on non-clones vs. 65% on clones) model. Manual analysis revealed a tendency for this model to classify nearly all input code snippet pairs as clones, resulting in an inflated number of false positives.\nIn general, for simple and recurrent programming tasks, LLMs show a high ability to detect cross-lingual clones (e.g., XLCOST samples). However, when applied to code samples of challenging programming tasks (e.g., CodeNet), LLMs performance drop by 30% on average in terms of F1 score. This result is insightful as it calls for a different perspective in benchmarking cross-lingual code clone detection approaches. Indeed, while XLCost has been built for such purpose, CodeNet appears to offer a more realistic setting, in terms of complexity for assessment.\nThe \"Simple prompt\" exhibits a relatively consistent performance across both datasets, suggesting its potential for broader applicability. A detailed performance comparison of all four LLMs using this prompt is presented in Notably, the GPT-3.5-turbo model outperformed other LLMs in this analysis."}, {"title": "4.2 [RQ2] LLMs' Reasoning for Cross-Lingual Code Clone Detection", "content": "Goal. Our investigation of RQ.1 on the performance of LLMS for the cross-lingual code clone detection task has revealed, based on a manual analysis of some sample cases, that the code pairs are often obviously (for a human) a clone, and yet the LLM fails to identify it. In other cases, the pair is obviously (for a human) not a clone and yet the LLM concludes that it is. We thus now aim to further study the extent to which the LLMs understand the task. Based on our analysis, notably of whether the LLMs understand what \"clone\" means in a cross-lingual setting, we propose an improved version of the Zero-shot prompt to validate that LLMs performance can be improved with a hint on the definition of clone.\nExperiments. We qualitatively analyzed the outputs of all the experiments conducted in the first research question to collect insights in the failures of the LLMs for the task of cross-lingual code clone detection. Taking into account those results, we improve the Zero-shot prompt design and repeat the same experiments as in Section 4.1 based. In this RQ, we focus on the Zero-shot prompting, since it is a baseline, in order to better evaluate the impact of the improvement in the definition.\nResults. Our qualitative review of LLMs' outputs reveals that Falcon-Instruct-7B and LLAMA2-Chat-7B consider, most of the time, any input pair of code fragments as clones, leading to an excessive amount of false positives. Conversely, Starchat-\u1e9e considers most of the pairs as non-clones, yielding a significant number of false negatives.\nThe behavior of Starchat-\u1e9e is explained by the fact the reasoning of the LLM completely breaks down for the task of cross-lingual code clone detection. The LLM even goes far as to over-confidently state that two code snippets of different programming languages cannot be clones. Similar breakdowns in LLM reasoning have recently been documented in the literature [25]. Our experiments reveal for future research development in LLMs that cross-lingual code clone detection is a relevant and easy-to-assess task for checking potential reasoning breakdown of LLMs. Unfortunately, Starchat-\u1e9e is not the only LLM with a breakdown in reasoning: Similar situations are also apparent in several outputs of GPT-3.5-turbo.\nWe analyze specifically the cases where the prompts require the outputs to include explanations from the Chain of Thought. A major finding was that in most of the cases where the LLM output was accurate, the LLMs actually stated that their decision was made taking into the \"overall structure and logic\" of the code fragments. Based on this observation, we propose to design a new prompt, the 'improved simple prompt,' that instructs the LLMs on what to consider as code clones. In other words, the improved prompt clearly defines the concept of code clones without using the term, which may be misleading to the LLMs: we instruct the model, in a Zero-Shot manner, to consider the overall structure and logic of the code snippets independently of their programming language."}, {"title": "4.3 [RQ3] Influence of the Programming Languages Syntactical Similarity on LLMs Performances", "content": "Goal. In the studied task, a clone pair involves two fragments from two distinct programming languages. Previous research questions focused on highlight the overall performance of LLMs on randomly built pairs. We now attempt to check whether the specific combinations of programming languages may affect the performance of the LLM in identifying clones. In particular, we investigate whether the syntactic similarity of programming languages has an influence on the effectiveness of the LLMs.\nExperiments. Some programming languages share common core concepts, such as for controlling program flow (e.g., loops, conditionals, etc.) or storing data (e.g., variables). In terms of Syntax, we note for example that Java's syntax is most like C++ and C. In fact, Java borrows heavily from C++ syntax, and therefore their programs may present similar structures. Similarly, languages like C# (pronounced C-Sharp) share similarities with Java because they are object-oriented languages with similar features like classes, objects, inheritance, and encapsulation. To better highlight the potential combinations, we focus on the case of Java, and consider sets of pairs where the second programming language is varied across C, C#, C++, Go, Ocalm, PHP, Python, Ruby, and Rust.\nResults. Table 5 shows all the results across various prompts and by programming language pair. We note that the performance difference is at least of ~10 percentage points in terms of F1 score when the LLM is applied to fragments of Java-C# language pair vs fragments of Java-Python language pair. C# is indeed very close to Java in terms of syntax and core concepts, while Python employs different programming paradigms.\nNevertheless, the results also show that, while syntactic similarity of programming languages influences the performance of LLMs when instructed with a simple prompt, complex prompts with reasoning instructions (e.g., Reasoning) and the Improved simple prompt (which guides the LLM to consider logic) manages to reduce the performance gap for language pairs that are supposedly relatively different (e.g., Java-PHP)."}, {"title": "4.4 [RQ4] Traditional classification vs. LLMs", "content": "Goal. Code clone detection has been studied in the literature as a binary classification problem. Various techniques leveraging machine learning has then been adopted and showed promising results. We compare LLMs against implementations of such techniques. Since we aim to assess the added value of LLMs (beyond the representations), we build on the embedding model that is provided alongside the LLM by OpenAI.\nExperiments. We rely on the identified embedding model (Text-Embedding-Ada-002 - cf. Section 3.2) to generate the representations of code fragments in our constructed benchmark. We conduct two separate experiments:\n(1) straightforward similarity computation: we use cosine similarity [26] to compute the distance between the embeddings of two code fragments in a given pair. We can identify an optimal"}, {"title": "5 DISCUSSION", "content": "This section explores the implications of our findings for code clone detection. Our investigation into the ability of LLMs to interpret cross-language code clones yielded valuable insights.\nLLM interpretation and performance. Our experiments showed that LLMs can perform substantially better when we give them specific instructions about what to look for in the code (e.g., with the improved simple prompt) compared to when the prompt is generic and zero-shot based. The improved prompt, which instruct the LLM to reason about the overall structure and logic of the code leads LLMs to identify a large number of cross-lingual code clones (cf. Table 4). This finding suggests that LLMs can avoid dramatic reasoning failures when the prompt is sufficiently informative on the task: LLMs may need some guidance to see the bigger picture.\nRepresentations vs. Prompt Engineering. The superiority of the learned classifier based on embedding models highlights the value of representations. Unlike prompts, which require careful design, an embedding model is a powerful, straightforward tool. It produces representations that capture the code's semantic meaning and functionality across different programming languages. Then based on the similarity of representations in a training set, a simple algorithm can learn decision boundaries, effectively overcoming language barriers in code clone detection. In contrast, prompts adds to the complexity and opacity of the reasoning workflow performed in an LLM.\nLimitations and Threats to validity. \u2460 This study evaluated only four LLMs and one embedding model. Expanding the research to include a broader range of models with different architectures and training data could provide a more comprehensive understanding of LLM effectiveness in this task. \u2461 The dataset used for training and evaluation consisted of 6,000 samples per dataset. Investigating the impact of larger and more diverse datasets on LLM performance and embedding model training could be highly beneficial. \u2462 Further exploration of prompt design strategies is needed. This could involve investigating variations of Prompt 2, exploring entirely different prompt structures, and leveraging insights from this study to refine prompts for more optimal LLM performance. Due to limitations in current LLM capabilities, most prompt approaches did not always provide definitive \"yes/no\" responses for clone detection. This necessitated manual verification and correction of results, which constitute an internal threat to validity. Future research can focus on developing prompt design or LLM training strategies that yield more reliable and automated outputs. \u2464 The Text-embedding-Ada-002 model is likely based on an earlier architecture of the GPT family. Newer text embedding models may help achieve even better performance."}, {"title": "6 RELATED WORK", "content": "Prompt engineering is a challenging yet essential endeavor aimed at enhancing the performance of LLMs for specific tasks [42]. Ekin et al. [8] have demonstrated recently that to obtain accurate, relevant, and coherent output, users should design, refine, and optimize the prompt. Model understanding, domain specificity, and clarity are essential to identify the strengths and limits of the LLM, avoid ambiguity, and help it generate more accurate and relevant outputs. Our experimental results with a quick improvement of simple zero-shot prompts by providing a domain definition of the concept of clone confirms this finding for the field of software engineering.\nRegarding code clone detection with LLMs, we can principally enumerate works by Dou et al. [6] and Khajezade et al. [16]. In [6], the authors proposed an automated code clone detection for all types of clones with various prompt designs for a comprehensive empirical evaluation of several LLMs applied to clones from a single language (Java). In alignment with our findings, they show that LLMs of the GPT family outperform others and that introducing intermediate reasoning steps through a chain of thought improves the performance of LLMs for the clone detection task.\nKhajezade et al. [16] worked on mono and cross-lingual code clone detection using only GPT-3.5-Turbo (in contrast to our work, which explored various LLMs). They also used only zero-shot prompts and considered only Java-Ruby combinations (in contrast to our work, which considered 10 language pairs). Their study furthermore does not investigate the power of representations over prompts."}, {"title": "7 CONCLUSION", "content": "This study explored the potential of LLMs for cross-language code clone detection. We evaluated four LLMs and investigated how prompt engineering enhances their performance. Additionally, we examined the effectiveness of code representations in the identification of code clones. Overall, our findings suggest the potential of using powerful LLMs for cross-language code clone detection. Nevertheless, While LLMs show promise, our results suggest that simpler binary classifiers leveraging robust language-agnostic representations can achieve higher, or at least comparable, performance for cross-language code clone detection."}]}