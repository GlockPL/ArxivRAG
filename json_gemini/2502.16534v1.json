{"title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs", "authors": ["Jonathan Rystr\u00f8m", "Hannah Rose Kirk", "Scott Hale"], "abstract": "Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B-27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.", "sections": [{"title": "Introduction", "content": "Spearheaded by accessible chat interfaces to powerful models like ChatGPT (OpenAI, 2022), LLMs are reaching hundreds of millions of users (Milmo, 2023). These models are deployed across diverse contexts: from tutoring mathematics (Khan, 2023) to building software applications (Peng et al., 2023) to assisting in legal cases (Tan et al., 2023). While most LLMs demonstrate multilingual abilities (\u00dcst\u00fcn et al., 2024), the ability to communicate across languages does not necessarily translate into appropriate cultural representations. Disentangling language capabilities and cultural alignment is crucial for understanding how LLMs should be examined and audited (M\u00f6kander et al., 2024) and for ensuring these technologies work for diverse people (D'Ignazio and Klein, 2020; Weidinger et al., 2022).\nA priori, we might expect these models to exhibit US-centric cultural biases despite their multilingual capabilities, given their development context: Although millions use LLMs, they are developed by a select few. Many model providers, such as OpenAI, Anthropic, and Google, are American technology companies headquartered in Silicon Valley. These companies comprise a narrow slice of human experience, limiting the voices that contribute to critical design decisions in LLMs (D'Ignazio and Klein, 2020). The companies typically train LLMs on massive amounts of predominantly English text and employ American crowd workers to rate and evaluate the LLMs' responses (Johnson et al., 2022; Kirk et al., 2023). Far too often, the benefits and harms of data technologies are unequally distributed, reinforcing biases and harming already minoritized groups (Birhane, 2020; Milan and Trer\u00e9, 2019; Khandelwal et al., 2024). Understanding how LLMs represent different cultures is thus paramount to establishing risks of representational harm (Rauh et al., 2022) and ensuring the technology's utility is shared across diverse communities. Increasing diversity and cross-cultural understanding is stymied by unchecked assumptions in both alignment techniques and evaluation methodologies. First, there is an assumption that bigger and more capable LLMs trained on more data will be inherently easier to align (Zhou et al., 2023; Kundu et al., 2023), but this sidesteps the thorny question of pluralistic variation and cultural representations (Kirk et al., 2024b). Thus, it is unclear whether improvements in architecture (Fedus et al., 2022) and post-training methods (Kirk et al., 2023; Rafailov et al., 2023) translate into improvements in cultural alignment.\nAlthough studies like the World Values Survey (WVS) have documented how values vary across cultures (EVS/WVS, 2022), it remains unclear whether more capable LLMs-through scaling or improved training-better align with these cultural differences (Bai et al., 2022; Kirk et al., 2023). While the WVS has been used in prior research on values in LLMs, these studies have focused predominately on individual models' performance within an English-language context. (Cao et al., 2023; Arora et al., 2023; AlKhamissi et al., 2024). This paper addresses this gap by developing a methodology for assessing how well families of LLMs represent different cultural contexts across multiple languages. We compare two distinct paths to model improvement: systematic scaling of instruction-tuned models and commercial product development comprising scaling and innovation in post-training to accommodate pressures from capabilities, cost, and preferences (OpenAI et al., 2024b).\nGiven these considerations, we investigate the following research questions:\nRQ1 Multilingual Cultural Alignment: Does improved multilingual capability increase LLM alignment with population-specific value distributions?\nRQ2 US-centric Bias: When using different languages, do LLMs align more with US values or with values from the countries where these languages are native?\nWe operationalise multilingual capability as an LLM's performance on a range of multilingual benchmarks across languages (see, e.g., Nielsen, 2023). We describe the specific benchmarks and performances in Appendix F.\nThis work makes several key contributions. First, we introduce a novel distribution-based methodology for probing cultural alignment across languages, moving beyond direct survey approaches to better capture underlying cultural values (Sorensen et al., 2024). Second, we provide the first systematic comparison of how improvements in scale and post-training affect cultural alignment and US-centric bias across English, Danish, Dutch, and Portuguese through a series of robust statistical models. Third, we release a dataset of model-generated responses across multiple languages and cultural contexts as well as our code, enabling future research into cultural alignment and bias. Together, these contributions advance our understanding of how LLM development choices influence cultural representation while providing tools for ongoing investigation of these critical issues."}, {"title": "Measuring Cultural Alignment", "content": "This section defines what we mean by 'cultural alignment' and how to measure it in LLMs. First, we provide a high-level intuition of the cultural alignment as reproducing distributions of values in a particular population. Then we show how to a) get a ground-truth distribution of values using the World Values Survey (\u00a72.1) and b) elicit value distributions from LLMs (\u00a72.2)\nCultural alignment as value reproduction: Within a culture there will be a variety of stances to any particular topic. However, the distribution of stances will be characteristic among cultures. For instance, while some Danes are opposed to abortion, it is a much less contentious topic than in the US (Adamczyk et al., 2020; Norup, 1997).\nWe posit that cultural alignment for a specific group of people can be operationalised as how well an LLM reproduces the distribution of values over a wide range of topics (Sorensen et al., 2024). By investigating distributions of responses, we differ from previous work that directly surveys the LLMs as a regular participant (e.g., Cao et al., 2023). Our goal is to get more naturalistic elicitations of the underlying values and avoid sycophancy and response bias (Sharma et al., 2023).\nMore specifically, we operationalise reproduction as having a high correlation between value polarity score: the fraction of people in favour of a topic in the population and the fraction of responses in favour of the topic elicited by the LLM. Note, that we binarise issues to allow for simpler operationalisation. Below, we describe how we empirically estimate the value polarity score for ground truth (\u00a72.1) and LLMs (\u00a72.2).\nTo get a 'ground truth' distribution of cultural values, we use the joint World Values Survey and European Values Survey (EVS; EVS/WVS, 2022). These large-scale international surveys cover adults across 92 countries with samples that are nationally representative for gender, age, education, and religion. The surveys' broad coverage enables cross-cultural comparability, though some scholars note challenges in ensuring response comparability across countries (Alem\u00e1n and Woods, 2016).\nWe select a subset of questions corresponding to values about the environment, work, family, politics & society, religion & morals, and security-all questions where there is are clear 'for' and 'against' positions. These questions comprise our topics. Appendix A shows the full list of included questions.\nThe final step is calculating the value polarity score. For each question, we define what an 'affirmative' response means (i.e., a Likert score higher/lower than the middle score, depending on the directionality of the question) and then define the value polarity score as the (weighted) fraction of respondents with an affirmative stance to the topic within the population. Thus, a culture's values can be represented as a vector, where each element corresponds to a value polarity score for a specific topic.\nTesting cultural alignment effectively requires embedding contextual and cultural elements in ways that maintain ecological validity. At a high level, eliciting values from an LLM consist of two steps: 1) Iteratively prompting the model with the selected topics and 2) extracting the stances from each model response.\nSetting prompt context: Developing ecologically valid prompts requires careful consideration. When evaluating LLM responses to value-laden topics, simply asking questions like \u201cWhat proportion of people support topic X?\u201d or \u201cDo you support topic X?\u201d proves inadequate (e.g., Hartmann et al., 2023). Such direct approaches suffer from three key limitations: they generate false positives through excessive agreement, fail to reflect realistic usage patterns, and provide insufficient variation to assess cultural alignment (R\u00f6ttger et al., 2024). They also struggle to capture instance-specific harms that emerge when systems misalign with users' cultural contexts (Rauh et al., 2022).\nInstead, we adopt an implicit approach by asking the model to generate responses from hypothetical respondents. For example, prompting \u201cimagine surveying 10 random people on topic X. What are their responses?\u201d This method reveals the model's latent opinion distribution while avoiding the limitations of direct questioning. Details for prompt construction are provided in Appendix B.\nSeeding cultural responses: Having a method for eliciting distributions of values, the next step is to seed culture. One typical way of seeding a specific culture is to explicitly instruct the LLM either by mentioning a specific country ('imagine surveying 10 random Americans') or through describing specific personas (\u2018Imagine surveying a 85 year old Danish woman...'; AlKhamissi et al., 2024). The problem with these approaches is that they stray from actual uses of LLMs. Users are unlikely to explicitly mention their demographic information or nationality (Zheng et al., 2023a)."}, {"title": "Experimental Setup", "content": "To investigate whether improving the multilingual capabilities of LLMs improves cultural alignment, we set up an experiment using a carefully chosen set of models and languages. We examine two different kinds of model improvements: scaling and commercial product development. These cases provide complementary perspectives on the effects of multilingual capabilities on cultural alignment. Scaling is the most well-studied path to improving LLMs (Kaplan et al., 2020; Ganguli et al., 2022). Commercial product development, on the other hand, comprises both scale and innovation in post-training to accommodate different pressures from capabilities, cost, and preferences (Kirk et al., 2024a). For scaling, we use the instruction-tuned Gemma models (Gemma et al., 2024), while for product development, we use OpenAI's turbo-series models (OpenAI, 2022; OpenAI et al., 2024a,b). We provide details of these model families in \u00a73.1. A breakdown of the computational cost can be seen in Appendix E.\nLanguages: For the languages, we compare English with Danish, Dutch, and Portuguese. This set allows us to test multiple assumptions about cultural alignment. English represents a widely-used case: it's a global language with speakers across many countries represented in the WVS/EVS (see Fig. 2). This diversity allows us to assess whether LLMs align more strongly with US values or those of other English-speaking nations.\nDanish and Dutch serve as controlled test cases since they are primarily used in single countries. If cultural alignment stems from pre-training data, models should show strong Danish/Dutch cultural alignment when using these languages, despite their small and low-quality share of training data (Kreutzer et al., 2022). Alternatively, if alignment emerges from English-based post-training processes (which are predominately English-based; Blevins and Zettlemoyer, 2022), responses in these languages should align more with US values.\nPortuguese presents an interesting case since it is an official language in several countries. We investigate whether the LLM responses are more aligned to Portugal or Brazil-two countries that show distinct value patterns in relation to each other and the US (see Fig. 2). This allows us to test whether an LLM aligns more strongly with one country's values, the aggregate values of all language users, or US values.\nFor each language/model combination we collect 300 prompt-response pairs to sufficiently power our statistical analysis (see \u00a73.2). After filtering out responses that either lacked the required hypothetical survey format or were in another language than the prompt, we obtained between 111-299 valid responses per combination. We calculate the correlation in value polarity scores at three levels:"}, {"title": "RQ1: Multilingual Cultural Alignment", "content": "To statistically assess whether improving the multilingual capabilities of LLMs improves cultural alignment, we construct a linear mixed effects regression model (Luke, 2017) based on the experimental setup described above. Here, we relate the cultural alignment of the model to the improvements in multilingual capabilities controlled by relevant factors:\n$CA_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$\n$\\mu_i = a_{j[i]} + \\beta_1 X_{cons,i} + \\sum_{f,l} \\beta_{flm} X_{fl,i} X_{m,i}$\n$a_j \\sim \\mathcal{N}(\\mu_a, \\sigma_a)$, for $j = 1, ..., J$\nHere, Eq. 1 models the cultural alignment (CA) of an LLM, i, as a linear mixed-effects model. The mean \\( \\mu_i \\) includes a random intercept \\( a_j \\) for each model, the fixed effect of consistency (\\( \\beta_1 X_{cons,i} \\)), and interaction effects (\\( \\sum_{f,l} \\beta_{flm} X_{fl,i} X_{m} \\)) between model family (either \u2018OpenAI' or 'Gemma'), language (e.g., Portuguese or Danish), and multilingual capability (average score on selected benchmarks; see Appendix F). Here, consistency is an LLM's stability in value stances across repeated questions, calculated as the Pearson correlation between value polarity scores (defined in \u00a72) of repeated responses to identical topics (see Appendix A). A score of 1.0 indicates perfect consistency, 0.0 indicates randomness. Population-level resampling of human survey responses yields values between 0.66 and 0.84 (see Fig 3). Assumption checks for the regression can be seen in Appendix D.\nThe above statistical model allows us to analyse the relationship between multilingual capabilities and cultural alignment in model families at the level of individual languages. For instance, we might find an improvement for Gemma models for Danish but not for Dutch or vice versa."}, {"title": "RQ2: US-Centric Bias", "content": "We analyze model bias by comparing cultural alignment between US and local values, where \"local\" refers to values in country or countries where a given language is natively spoken. We define US-centric bias as an LLM showing higher cultural alignment with US value distributions compared to local ones. To quantify this bias, we use a linear regression model that measures the differential effect of US versus local value alignment:\n$CA = \\beta_0 + \\beta_1 (US)$\n$+ \\sum_{m \\in M} \\sum_{l \\in L} \\beta_{ml} (m \\times l)$\n$+ \\sum_{m \\in M} \\sum_{l \\in L} \\beta_{m(US \\times l)} (US \\times m \\times l) + \\epsilon$\nThe regression's intercept (i.e., the base case) is a baseline that produces uniformly random value polarity scores. M is the set of models and L is the set of languages. US is a boolean feature denoting whether the cultural alignment is to the US (if 1) or the local values (if 0). We primarily analyse the coefficients with US (\\( \\beta_{US} \\)) since these provide the partial effect of US-centric bias, i.e., how much more/less a given LLM is aligned to US rather than local values. Assumption checks for the regression can be seen in Appendix D."}, {"title": "Results", "content": "To understand how improving multicultural capabilities affects cultural alignment, we must first examine the stability of LLMs' cultural values. When evaluating LLMs that lack stable internal values, apparent improvements in cultural alignment may simply reflect reduced response variance rather than genuine advances in cultural alignment (R\u00f6ttger et al., 2024; Kahneman et al., 2021). We therefore analyse both the self-consistency of LLM responses and how alignment changes with model improvements.\nLLMs have low self-consistency: We find consistently low self-consistency scores across all models and languages compared to human responses in the WVS data (Fig. 3). For WVS data, self-consistency scores (measured as Pearson correlations between bootstrapped samples of responses) range from 0.66 to 0.84 across countries (see Fig 3). In contrast, LLM responses to the same questions under different prompt variations show significantly lower consistency, even in English where performance is highest due to English-dominated training data (OpenAI et al., 2024a; Gemma et al., 2024).\nThis lower self-consistency complicates our cultural alignment analysis (Wright et al., 2024). Drawing on Kahneman et al. (2021)'s noise framework, we recognise that inconsistent responses can be as detrimental as bias. To address the noise, we employ larger sample sizes and incorporate consistency measures in our regression analyses.\nMultilinguality does not imply cultural alignment: The relationship between model improvements and cultural alignment varies substantially across languages and model families (Fig. 1). For Gemma, there is a significant positive relationship between multilingual capabilities and cultural alignment for all languages. In contrast, OpenAI only has a significant and positive relationship for English (\\( \\beta_{OpenAI,en} = 0.11, p = 0.04\\)), whereas for Danish and Portuguese the relationship is insignificant (\\( \\beta_{OpenAI,pt} = 0.089, p = 0.10; \\beta_{OpenAI,da} = 0.009, p = 0.89\\)) and for Dutch the effect is significant and negative (\\( \\beta_{OpenAI,nl} = -0.19, p = 0.004\\)).\nA capability threshold may explain the discrepancy between multilingual performance and cultural alignment. Up to a point, multilingual capabilities might improve alignment by enhancing cultural knowledge and instruction-following. Beyond this threshold, other factors likely become more influential (Kirk et al., 2024a). This could explain the stronger relationship in Gemma versus OpenAI's LLMs (see Fig 4). Future work with the Qwen-2.5 family (Qwen et al., 2025), which range from 500M to 72B parameters, could help validate this hypothesis.\nLooking at specific languages, we find nuanced patterns of improvement in cultural representations across model versions. Portuguese shows consistent improvements across both model families, par"}, {"title": "US-centric Bias (RQ2)", "content": "Here, we answer RQ2 by examining US bias across languages. Specifically, we investigate relative alignment between local and US values (Fig. 5).\nOur analysis reveals distinct patterns of US centric bias across both languages and model families (Fig. 5). Languages show different susceptibilities to US bias: five out of six LLMs exhibit US-centric bias in Dutch, all in English, none in Portuguese, and Danish falls in between with three of the six LLMs showing bias.\nWithin model families, we observe varying patterns of changes in US-centric bias. The Gemma models show a clear trajectory in Dutch, becoming significantly less biased in successive progressions. However, this improvement isn\u2019t universal: for Portuguese and English, there are no significant differences in bias between progressions within either family. For Danish, early models (gpt-3.5-turbo and gemma-2-2b-it) exhibit US-centric bias, while later progressions show reduced bias, particularly in the OpenAI family.\nIn conclusion, we find significant US-centric bias in certain LLM-language combinations, particularly in Dutch and Danish. However, the progression of this bias across model versions varies by language and model family. While some improvements are visible (e.g., Gemma models\u2019 reduced bias in Dutch), the overall pattern suggests that advancing model capabilities does not consistently reduce or increase US-centric bias."}, {"title": "Related Work", "content": "Recent work emphasizes the need for systematic auditing of LLMs\u2019 cultural alignment, particularly as these models are deployed globally (Kirk et al., 2024a; M\u00f6kander et al., 2024; Kirk et al., 2024b). Prior empirical approaches have primarily taken two paths: using tranformations based on Hofstede\u2019s cultural dimensions framework or directly comparing against survey responses. Studies using Hofstede\u2019s dimensions (Masoud et al., 2025; Cao et al., 2023) provide structured cross-cultural com"}, {"title": "Limitations", "content": "Our systematic investigation of cultural alignment across multiple languages and model families reveals important nuances about the relationship between language, culture, and LLM development. While our methodology enables robust cross-cultural comparison through established survey data (WVS/EVS) and distribution matching, our findings suggest that examining alignment at the national level may be too simplistic. Even in cases where we might expect strong alignment due to tight coupling between language and culture (e.g., Danish and Dutch), we find that other factors like model self-consistency have greater influence on alignment patterns.\nUsing the World Value Survey as a ground truth also brings limitations. First, we adopt the challenges and limitations inherent in the data (Alem\u00e1n and Woods, 2016). Second, future research using our methodology is limited to the countries surveyed by the WVS/EVS. While their coverage is broad, it is not universal.\nOur examination of Gemma as a case study in capability scaling provides initial insights into how cultural alignment evolves with model size. The reproducible evaluation pipeline we develop enables future work to extend this analysis to other model families and architectures, helping build a more comprehensive understanding of how different approaches to model development affect cultural representation (Johnson et al., 2022; Cao et al., 2023; Arora et al., 2023). As previously mentioned, our methodology requires a fairly high level of instruction following and the ability to respond in the prompted language. Some models, like LLaMA (Touvron et al., 2023), tended to reply in English regardless of prompt language in our preliminary experiments.\nThese findings highlight that metrics for cultural alignment are inherently contextual (Selbst et al., 2019). While our methodology provides systematic evaluation through distribution matching, the complex patterns we observe suggest that cultural alignment may need to be understood at multiple granularities beyond the national level. This is particularly relevant for languages spoken across many cultures-such as English or Portuguese\u2014where the relationship between language and cultural context is more diffuse (Sharifian, 2014).\nThe challenges we identify in achieving consistent cultural alignment, even for well-resourced languages, suggest that improving alignment may require approaches beyond current training paradigms (Hartmann et al., 2023). Future work might explore explicit personalization approaches or methods that don't rely solely on language as a signal for cultural context while being mindful of the inherent complexity in representing diverse cultural perspectives (Birhane, 2020; Kirk et al., 2024b)."}, {"title": "WVS questions", "content": "As mentioned in the main text, we use a subset of questions in the WVS as basis for the prompts. The complete list can be seen in Table 1."}, {"title": "Prompt construction", "content": "Our process for constructing prompts has three stages: 1) template, 2) topic variations, and 3) translations. The first two stages are in English, while translations is used to construct prompts for arbitrary languages. The first stage is to construct a prompt template that produces semi-structured output following our hypothetical survey approach. Our base template can be seen below:\n\"Imagine the following scenario: Person A is a researcher. Person A interviews 10 representative citizens. For each citizen, they ask for their opinion on topic. Please provide plausible brief summaries of the 10 interviews that Person A did. Output format: 1.  2.  etc...\"\nNote, the template avoids country-specific information. Furthermore, the template provides instructions for structuring the data (an enumerated list), which helps in our post-processing stage (see \u00a72.2). To get sufficient variations, we prompt an LLM (gpt-3.5-turbo) to construct 10 variations of the above template, keeping the '{topic}' element.\nThe second stage is to construct variations of the topics seen in Table 1. Here, we also get an LLM to construct five variations of each topic. We then manually verify that the constructed variations match the original meaning within the context. For instance, the topic 'Men make better political leaders than women do' can be transformed into, e.g., 'Men are more competent political leaders than women' and 'The political arena is better suited for men than women'\u2014both of which are different but semantically similar.\nThis combined approach allows us to construct 1750 unique prompts (35 topics x 5 topic variations x 10 template variations). We then subsample 300 prompts to get the required power level.\nFinally, we translate the English prompts to our target languages. This ensures comparability and consistency across languages. As previously mentioned, the translations are done using an LLM (gpt-3.5-turbo."}, {"title": "Language Performance Breakdown", "content": "Here, we expand on the results in \u00a74.1 also to include Portuguese (Fig. 7) and English (Fig. 8). The figures both compare bootstrapped cultural alignment scores (i.e., spearman correlation of value polarity scores) with different reference classes. Specifically, we compare country level (left column), where we contrast local countries where the language is native with the US; language-level, where we compare all native speakers with all English speakers; and global cultural alignment, where we measure cultural alignment aggregated to all survey participants. Note, that we compare the raw cultural alignment scores and thus do not account for the effects of self-consistency - see \u00a74.1 for a discussion.\nFor Portuguese, the two LLM families have diverging progressions (Fig. 7). For Gemma, the LLMs monotonically improve for all populations. For OpenAI, gpt-4-turbo performs much better than gpt-3.5-turbo, whereas gpt-40 is roughly on par with gpt-3.5-turbo. This roughly mirrors the progression in self-consistency, which we observe in Fig. 3.\nFor English, the primary new comparison is a cross-country analysis (left pane; Fig 8). Here, we compare the cultural alignment of the LLMs to nine countries with substantial English speaking populations from across the world (Australia (AU), Canada (CA), United Kingdom (GB), Kenya (KE), Nigeria (NG), Northern Ireland (NIR), Singapore (SG), and the USA (US)). For OpenAI, we see monotonic improvement in cultural alignment for all countries except New Zealand. For Gemma, on the other hand, only Canada and Northern Ireland exhibit monotonic improvements; the rest stagnate or deteriorate between the 9B and 27B model. For the OpenAI family, the most aligned populations ends up being the US, whereas for Gemma this ends up being Canada."}, {"title": "Regression Assumption Check", "content": "This section contains both the raw regression coefficient tables and assumption checks for the two regression models. Starting with assumption checks, the main important checks for the LMER model in RQ1 randomly distributed residuals (Schielzeth et al., 2020). Since we have a fairly large dataset, the model is relatively robust to other measures and"}]}