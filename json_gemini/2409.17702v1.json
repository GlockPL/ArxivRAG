{"title": "Episodic Memory Verbalization using Hierarchical Representations of Life-Long Robot Experience", "authors": ["Leonard B\u00e4rmann", "Chad DeChant", "Joana Plewnia", "Fabian Peller-Konrad", "Daniel Bauer", "Tamim Asfour", "Alex Waibel"], "abstract": "Verbalization of robot experience, i. e., summarization of and question answering about a robot's past, is a crucial ability for improving human-robot interaction. Previous works applied rule-based systems or fine-tuned deep models to verbalize short (several-minute-long) streams of episodic data, limiting generalization and transferability. In our work, we apply large pretrained models to tackle this task with zero or few examples, and specifically focus on verbalizing life-long experiences. For this, we derive a tree-like data structure from episodic memory (EM), with lower levels representing raw perception and proprioception data, and higher levels abstracting events to natural language concepts. Given such a hierarchical representation built from the experience stream, we apply a large language model as an agent to interactively search the EM given a user's query, dynamically expanding (initially collapsed) tree nodes to find the relevant information. The approach keeps computational costs low even when scaling to months of robot experience data. We evaluate our method on simulated household robot data, human egocentric videos, and real-world robot recordings, demonstrating its flexibility and scalability.", "sections": [{"title": "I. INTRODUCTION", "content": "Verbalizing their own experiences is an important ability robots should have to improve natural and intuitive human-robot interaction [1], [2], [3], [4]. It involves summarization of and question answering (QA) about a robot's past actions, observations and interactions, such as the dialog shown on the right of Fig. 1. Building a representation of an agent's Episodic Memory (EM) [5] is crucial to enable such verbal-izations, as a system must efficiently store the information from the continuous stream of experience, organize it, and retrieve relevant past events from its EM in response to a user's query. This is particularly challenging as the time horizon of the EM grows.\nExisting work on Episodic Memory Verbalization (EMV) either relies on rule-based verbalization of log files [2], [3], or fine-tuning deep models on hand-crafted or auto-generated datasets [1], [6] to perform QA and summarization tasks given the recorded experiences. Both approaches are limited, as they require designing vast numbers of rules or collecting large amounts of experience data.\nTo avoid training a system, which typically entails collect-ing large amounts of multimodal experience data, previous works [7], [8] use language-based representations of the past which can be obtained from pretrained multimodal models. Given such a language-based representation of an agent's history of episodic events, a straightforward way to perform QA is to pass the question and the history to a large language model (LLM), and prompt it to produce an answer. While this works nicely for short histories [7], [9], in this paper, we focus on how to scale such approaches for verbalization of life-long experience streams. Although recent LLMs of-fer increasingly long context windows (i. e., the maximum number of tokens they can process), up to 2M tokens [10], previous studies [11], [12] have shown that these models have difficulty in using all information contained in such long contexts. Furthermore, the computation of transformer models scales quadratically with context length - reducing the number of tokens is thus time- and cost-effective.\nTherefore, to scale EMV to life-long experience streams while maintaining a low token budget, we propose to derive a tree-like representation from EM and use an LLM agent for QA to interactively search the tree to find relevant informa-tion. Our system, H-EMV (Hierarchical Episodic Memory Verbalization, Fig. 1), processes the continuous stream of experiences and inserts it into a hierarchical representation of the robot's history of episodic events. Different levels of this hierarchy represent different abstraction levels, with the lowest level being raw observations and proprioception and higher levels being represented as natural language concepts. An LLM is prompted for segmentation and summarization in order to recursively create higher-level abstractions. To process queries to the EM, we repurpose the interactive prompting scheme described in our previous work [13]. An LLM is provided with the user's query and functions to access the history tree, and the LLM itself decides which"}, {"title": "II. RELATED WORK", "content": "Episodic Memory for Robots: The concept of EM stems from human cognition [5] and is useful for various tech-nologies including smart wearables [16], [18], smart rooms [19], and especially robotics. For instance, robotic EM can be represented using latent vectors created by deep neural models [20], [1], [21], or by explicitly storing relevant information in a memory system [22], [23], [24]. Another approach is to represent the history of episodic events as text produced by pretrained models [7], [8]. In this paper, we also represent the history tree in form of text, following REFLECT [8] for the broad structure of the hierarchy's lower levels. However, we extend this by adding hierarchical summarization. Furthermore, our multimodal episodic his-tory tree can be dynamically explored by an LLM to gather information from all levels, including the raw observations.\nRobot Experience Verbalization: The first work to intro-duce the term of \"verbalizing\" robot experiences was [2]. With a rule-based system, they converted a navigation route taken by a mobile service robot to natural language. [3] adapted this framework to verbalization of manipulation ac-tivities performed by a humanoid household robot. Similarly, [25] use templates to convert their robot's observations and actions to natural language. More recent works phrase EMV in a more interactive setting, defined as summarization and QA on robot experiences [4]. Both [1], [6] propose end-to-end trained networks receiving multimodal experiences and a question to produce an answer. While [6] work on visual data only, [1] additionally use symbolic and subsymbolic information from the robot's task execution and perception components. Both train on data from simulated household tasks. In contrast to these systems, H-EMV uses pretrained foundation models and does not require additional training data, thus increasing its versatility and easing deployment to the real world. Similar to our setting, QA from streaming data [26], [27] tackles the problem of answering questions based on a long stream of data, where the question is not known in advance and the raw data cannot be stored. However, we apply this to robotics, and approach it with an interpretable, modular system, instead of end-to-end trained memory models.\nVideo Understanding: Video Understanding, especially Video Question Answering (VideoQA), is related to EMV as it also involves QA on a data stream, which, however, is only a video instead of a multimodal robotic experience stream. VideoQA is an active research area [28] where current major challenges include long-form videos beyond clips of a few seconds as well as egocentric video understanding. Ego4D [16] is a large collection of unconstrained egocentric videos showing daily activities of human camera wearers. Ego4D GoalStep [29] and HCap [9] provide hierarchical annotations for subsets of Ego4D, facilitating reasoning on different abstraction levels. Recent long-form egocentric VideoQA benchmarks include QAEGO4D [18] and EgoSchema [30].\nRecent methods for VideoQA can be grouped into (i) end-to-end approaches [31], [32], [33], [9], [34] that typically connect pretrained frozen visual encoders with LLMs by some trained adapter, and (ii) training-free \"socratic\" [7] approaches [35], [36], [37], [38], [39], [40], [41] that in-voke various off-the-shelf models to convert the video into text to be processed by a few-/zero-shot prompted LLM. For instance, [39], [41] use video captioning to produce a transcript of the video and then apply an LLM for QA based on this transcript. VideoTree [36] adaptively selects the frames to caption using a top-down query-relevance-based tree expansion instead of uniform sampling. [40] generate executable Python code from a question, invoking different APIs to query visual and language foundation mod-els. MoReVQA [38] decomposes this into multiple stages, making the LLM's job easier at each stage by focusing on either event parsing, grounding, or reasoning, instead of all at once. In contrast to these predefined prompting schemes, both [35], [37] use an LLM as an agent to analyze the video content in an interactive loop. While [37] iteratively ask the LLM whether to gather more detailed information (by captioning more intermediate frames) or produce the final answer, [35] provide the LLM with API functions invoking tools to search in a database of tracked objects or a memory of frame captions.\nOur method similarly treats the LLM as an agent, thus not relying on any predefined information flow. However, we use the full flexibility of code [42] instead of single API calls like in [35], [37]. Compared to VideoTree [36], our history tree is constructed independently of the user's query, since future questions cannot be known in advance in realistic settings, and storing lifelong \"raw\" video experiences is prohibitive [18]. In contrast to all of the above works, we consider real-world dates and times an integral part of the process. While the recent work TimeChat [43] is also time-sensitive, they refer to video timestamps instead of real-world date-times. Furthermore, and most crucially, we deal with long sequences of multimodal robotic experiences, with the longest experiment having over six hours of video or nearly two months on a simulated timeline."}, {"title": "III. METHOD", "content": "Our goal is to enable an artificial agent to verbalize and answer questions about its past. Given the continuous, multimodal stream of experiences of a robot agent, we build up a hierarchical and interpretable representation of EM (Sec. III-A). When a user later asks a question, an LLM interactively explores the history tree to gather relevant information, detailed in Sec. III-B.\nFrom a stream of multimodal robot experiences, we derive a hierarchical representation of the robot's EM, a history tree, as shown in Fig. 2, with the lower levels broadly following [8]. Specifically, the tree's levels are:\nLO Raw Experiences: Leaf nodes collect the raw information available at a specific timestep during the robot's task execution. This includes all modalities that can be perceived by the robot: RGB and depth camera images and recorded audio, as well as information deduced from this data, i. e., recognized objects, their positions, and a text transcription of the audio, if there is user speech. Furthermore, we include everything the agent knows about its state: robot proprioception (joint configuration, mobile platform position), symbolic information about the current action and goal, and text to be spoken by the robot's text-to-speech component.\nL1 Scene Graphs: The first level of non-leaf nodes in the history tree has a one-to-one mapping to the LO leafs. On this level, we derive a scene graph from the given observations, consisting of the detected objects as nodes and their spatial relations (e.g., on top, inside) as edges. The exact method for constructing the scene graph varies in our experiments. For the pure vision-based approach, objects are detected using pretrained models and heuristics are applied to infer semantically meaningful relations [8]. In our real-robot experiments, we use the existing components in our robot software framework ArmarX [44] that already provide semantic scene information.\nL2 - Events: Next, we group and summarize the nodes from the previous level based on changes in the scene graph, the currently executed action or goal, as well as when there is a new speech recognition. We also create a template-based natural-language summary, including the latest scene graph, the current action, and recognized speech command. In our real-robot experiments, we use an LLM to filter and summarize the raw action parameters, which would be excessively detailed otherwise.\nL3 Goals: Based on the current goal from the LO node, we group event nodes and again create a rule-based natural-language summary containing the current goal and the verbalization of the latest event. Note that we allow goal nodes to have children of mixed types: either events or other goal nodes. This allows representing subgoals of complex tasks and is used in our real-robot experiments.\nL4+ - Higher-Level Summaries: Summaries are gener-ated dynamically by recursively asking an LLM to summa-rize the previous level's nodes. Specifically, given the set of nodes \\(S_e\\) at level \\(l > 3\\), we list them in order and prompt an LLM to identify consecutive ranges of items that belong together considering their times and content, and provide a summary for each range. The output is parsed to group the child nodes and create the summary nodes \\(S_{e+1}\\) for the next level. We apply this strategy recursively, until \\(|S_e| = 1\\) or there is no further reduction, i. e., \\(S_{e+1} = S_e\\). In the latter, the LLM is explicitly prompted to provide a single concise summary of all items to force obtaining one root node.\nGiven a user's query and the history tree built from all experiences so far, we use an LLM as an agent [45] to explore the tree, search relevant information, and eventually answer the question. For this, we define an API to interact with the history tree. We initially define each node of the tree to be in a collapsed state, i. e., its textual representation will only contain the node's time range and natural-language summary, but not list the child nodes. The LLM can then interactively expand and collapse nodes, according to what seems relevant given the user's query. Furthermore, we provide different tools to the LLM, e. g., to invoke a Vision-Language-Model (VLM) to perform visual QA on the images associated with leaf nodes. Moreover, there is a function to perform tree search based on semantic similarity, selectively expanding the children of the searched node in the tree that match the search query.\nFig. 3 illustrates typical steps the LLM performs to answer a user's question. Given the initially collapsed tree, the LLM first expands the root node's children based on the requested date. It then selectively explores the respective child nodes that seem relevant to the question using the search function. Note that the LLM is prompted to collapse irrelevant nodes again in order to save token budget and speed up further re-quests. In the given example, when reaching a leaf node, the answer to the question is not evident from any of the natural-language summaries on each level, so the LLM decides to invoke a VLM to gather more information. Finally, it invokes the answer function to answer the user's question.\nOur implementation of the LLM agent uses a prompting style inspired by the simulated Python console of [13]. The LLM can issue any command \u2013 including compound statements such as loops \u2013 using the provided API. After the execution of the respective code, the LLM can \"see\" the output of its command(s), or any execution error. This process is repeated, and the prompt to the LLM always con-tains the (growing) execution history. Zero-shot experiments prompt the LLM with only a static prefix to explain the task"}, {"title": "IV. EVALUATION", "content": "Following our previous work [6], [1], we use simulated household episodes and automatically annotate them with QA pairs based on the ground-truth (GT) simulation state. Specifically, we use the TEACh dataset [15], featuring episodes of two real humans, one commander, and one follower, interacting with the AI2THOR environment [46]. We adapt this data by rephrasing the commander to be a human user, and the follower to be a robot interacting with the environment (and the user). Thus, each TEACh episode describes a robot experience, comprising egocentric images, robot states and actions, and dialog with the user.\nEpisodes in TEACh are on average \\(6.2 \\pm 5.3\\) min long. Since we are interested in very long histories of robot experience, we randomly combine them to form histories of up to 100 episodes. We also randomize dates and times for each episode, ensuring realistic sequences by picking one to five episodes per day, avoiding nighttimes, and occasionally skipping some days; the longest histories thus span nearly two months. Based on these histories, we generate QA pairs by adjusting the generation grammar from [6]. Specifically, we generate ten types of questions. These ask for: a list of high-level summaries of episodes (task descriptions); a detailed description of one particular episode in a history; a summary of an episode that happened either before or after a particular episode; a list of episodes in which a particular object was seen or action performed; a summary of an episode that occurred at a given time or a specified number of days ago; a list of times or number of days ago at which a given task was performed. From the TEACh \u201cvalid unseen\u201d set, we generate test sets with 10 histories per sequence length (combining \\(|h| = 5, 15, 25, 50\\), and 100 episodes). Each history is annotated with 10 QA pairs, making up 100 samples per history length.\nEvaluation Metrics: Evaluation of free-form EMV an-swers is hard since there can be many ways to formulate the correct answer, questions can be underspecified, and ver-ifying abstract statements by grounding them in the history tree is a research question in itself. Following [1], we define a semantic categorization of a model's hypothesis h given the GT g and question q: correct when h is semantically equivalent to q; correctly summarized if h is a correctly summarized version of g, still containing all relevant facts (in context of q); correct TMI (too much information) if h is correct but overly specific; partially correct TMI if parts of h are correct, but there are TMI parts and these are wrong; partially correct missing if parts of h are correct, but relevant facts from g are missing; wrong when h could be an answer to q but is none of the above; and no answer if h is empty, completely irrelevant to q, or the model threw an error. Since categorizing each evaluated sample by hand is prohibitively expensive, we prompt GPT-40 [47] to perform this evaluation. For this, we started by annotating 60 samples by hand, and use these as a database to retrieve few-shot samples based on maximal marginal relevance [48]. We further tuned the prompts on a validation set of 100 hand-categorized model outputs. For reporting, we aggregate these semantic categories as the percentage of correct and partially correct samples, \\(S_c\\) and \\(S_p\\), respectively.\nWe evaluate the agreement of the LLM's predicted cate-gories with manually annotated ones on 200 model results from our test data, resulting in an aggregated category accuracy of 88%, and per-class f-scores of \\(F_1(\text{correct}) = 0.89\\), \\(F_1(\text{partially_correct}) = 0.84\\), \\(F_1(\text{wrong}) = 0.91\\). The LLM categorizes correct and wrong samples very well and has the most difficulties on the partially correct labels. However, these categories are also defined imprecisely, and the inter-annotator agreement [49] between the first two authors has only a value of Cohen's \\(\\kappa = 0.66\\) (n = 110), vs. \\(\\kappa = 0.91\\) (n = 68) when only considering correct/wrong. Thus, while not perfect, we use the LLM to automatically obtain reasonable score estimates."}, {"title": "B. Egocentric Human Videos", "content": "Next to verbalizing robot experience, EMV can be applied to human egocentric recordings, e. g., in the context of smart"}, {"title": "C. Real-World Robot Recordings", "content": "Finally, we apply our method on the real-world humanoid robot ARMAR-7. To obtain an EM, we record multiple robot sessions of typical household tasks, spanning a total duration of 3.3 hours of robot actions over the scope of two months. We record all entries made to the memory system introduced in [22], in particular: vision (RGB and depth images), robot state (proprioception), skill events (executed actions and goals), speech (speech-to-text output and text-to-speech input), symbolic scene (objects and their relations). From such recordings, we build up a history tree by populating LO with images, speech, and proprioception, L1 scene graphs with the symbolic scene information, L2 and L3 with robot action events (where L2 contains low-level actions and L3 contains actions that themselves invoke other actions). Note that L3 nodes can be nested in this case (goals and their subgoals). Higher levels (L4+) are constructed dynamically by an LLM as described in Sec. III-A, with two manually created few-shot samples.\nSubsequently, we annotate the recordings with 30 QA-pairs, apply our method, and again manually categorize the results. Results can be seen on the right part of Table II. In general, our task is very challenging, and the 1-pass Gemini baseline which has direct access to the complete stream of episodic data (without images) scores only 40%/10% of correct/partially correct samples. Compared to the Ego4D experiment, the quality of the text history is better, as most content (esp. current action, goal) is not inferred from vision. Our interactive hierarchical system achieves slightly better performance, with 1/17 of the token costs. The numbers also highlight that the hierarchical aspect is crucial, as H-EMV with only L3 has notably lower performance with more than 5 times the token cost. See the supplementary video for a demonstration of our system in action, enabling ARMAR-7 to answer questions about its past interactively."}, {"title": "V. CONCLUSION & DISCUSSION", "content": "We present H-EMV, a system for verbalization of life-long robot experience. The multimodal, hierarchical repre-sentation of EM is interactively accessed by an LLM to answer user questions, keeping token costs low even for extremely long histories. Despite the promising results and versatility of our system, it has some limitations: First, as a modulated approach, it is limited by the performance of each component and can suffer from error propagation. While the interactive tree search improves interpretability, there are no performance guarantees. Moreover, our system could integrate more modalities and tools. For instance, joint angle proprioception data could be rendered in simulation and then verbalized by a VLM. Adding personalization, both to EM and verbalization, is desirable for improved human-robot interactions. We hope our code and data will foster research on EMV, and will continue addressing these challenges in future work."}]}