{"title": "GHOST: Gaussian Hypothesis Open-Set Technique", "authors": ["Ryan Rabinowitz", "Steve Cruz", "Manuel G\u00fcnther", "Terrance E. Boult"], "abstract": "Evaluations of large-scale recognition methods typically focus on overall performance. While this approach is common, it often fails to provide insights into performance across individual classes, which can lead to fairness issues and misrepresentation. Addressing these gaps is crucial for accurately assessing how well methods handle novel or unseen classes and ensuring a fair evaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate that per-class performance can vary dramatically. We introduce Gaussian Hypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm that models deep features using class-wise multivariate Gaussian distributions with diagonal covariance matrices. We apply Z-score normalization to logits to mitigate the impact of feature magnitudes that deviate from the model's expectations, thereby reducing the likelihood of the network assigning a high score to an unknown sample. We evaluate GHOST across multiple ImageNet-1K pre-trained deep networks and test it with four different unknown datasets. Using standard metrics such as AUOSCR, AUROC and FPR95, we achieve statistically significant improvements, advancing the state-of-the-art in large-scale OSR. Source code is provided online.", "sections": [{"title": "Introduction", "content": "When deploying deep neural networks (DNNs) in real-world environments, they must handle a wide range of inputs. The \"closed-set assumption,\u201d prevalent in most evaluations, represents a significant limitation of traditional recognition-oriented machine learning algorithms (Scheirer et al. 2012). This assumption presumes that the set of possible classes an algorithm will encounter is known a priori, meaning that these algorithms are not evaluated for robustness against samples from previously unseen classes. Open-Set Recognition (OSR) challenges this assumption by requiring designs that anticipate encountering samples from unknown classes during testing.\nOften, OSR is performed by thresholding on confidence (Hendrycks and Gimpel 2017; Vaze et al. 2022) or having an explicit \"other\" class (Ge, Demyanov, and Garnavi 2017) and computing overall performance, ignoring the effects of per-class performance differentials (Li, Wu, and Su 2023). However, evaluating recognition systems under OSR conditions is crucial for understanding their behavior in real-world scenarios. This paper shows that as more unknowns are rejected, there is great variation in per-class accuracy, which could lead to unfair treatment of underperforming classes, see Fig. 1.\nRecently, research has followed two primary methodologies for adapting DNNs to OSR problems: (1) training processes that enhance feature spaces and (2) post-processing techniques applied to pre-trained DNNs to adjust their outputs for identifying known and unknown samples (Roady et al. 2020). Although OSR training methods have occasionally proven effective (Zhou, Ye, and Zhan 2021; Miller et al. 2021; Dhamija, G\u00fcnther, and Boult 2018), their application is complex due to the evolving nature of DNNs and the specific, often costly training requirements for each. If different DNNs are trained in various ways, why should a single OSR training technique be universally applicable? Furthermore, if an OSR technique is specific to a particular DNN, its value diminishes as state-of-the-art DNNs evolve. In contrast, post-processing"}, {"title": "", "content": "methods, such as leveraging network embeddings (Bendale and Boult 2016), are more straightforward to implement and can be applied to almost any DNN. These methods avoid the complexities associated with training techniques and focus instead on evaluating performance. Thus, the challenge becomes: how can various DNNs designed with a closed-set assumption be adapted for OSR problems?\nInitial post-processing OSR algorithms (Bendale and Boult 2016; Rudd et al. 2017) used distance metrics in high-dimensional feature spaces to relate inference samples to known class data from training. However, choosing appropriate hyperparameters, such as a distance metric, is not straightforward, particularly for networks trained without distance metric learning, leading to an expensive parameter search. Further, large-scale datasets like ImageNet (Deng et al. 2009) and small-scale splits (Neal et al. 2018; Perera et al. 2020; Yang et al. 2020; Geng, Huang, and Chen 2020; Zhou, Ye, and Zhan 2021) often lack suitable train-validation-test splits for a fair parameter search.\nAdditionally, a major limitation with prior evaluations is their emphasis on overall performance, rather than ensuring robust performance for each individual class. This focus can obscure significant disparities between classes, leading to an incomplete understanding of the algorithm's effectiveness and potentially resulting in unfair treatment of some classes. For example, an algorithm might achieve high overall accuracy but fail to recognize rare or challenging classes, which is critical for applications requiring high precision across all classes. Such evaluations can misrepresent the algorithm's performance for underrepresented or underperforming classes, which may be overlooked when only aggregate metrics are considered. This lack of detailed analysis can lead to skewed evaluations, where the model's weaknesses in specific areas are not addressed, ultimately affecting its real-world applicability and fairness. While fairness is not a major concern for ImageNet-1K, the dataset used herein, we consider it a reasonable proxy for operational open-set problems due to its size and widespread use as a feature extractor or for fine-tuning domain-specific models.\nWe propose a novel post-processing OSR algorithm, the Gaussian Hypothesis Open-Set Technique (GHOST), which uses per-class multivariate Gaussian models with diagonal covariance of DNN embeddings to reduce network overconfidence for unknown samples. The use of per-class modeling is crucial for ensuring fairness across all classes. By modeling each feature dimension separately for each class, GHOST evaluates each class on its own merits, rather than grouping them together. This technique helps address the challenge of handling the worst-performing classes fairly and reduces the risk of the model being overly confident about samples from these difficult classes. Importantly, GHOST eliminates the need for hyperparameters, simplifying the application of OSR techniques for end-users. Our novel GHOST algorithm improves traditional OSR measures and fairness, achieving a win-win outcome in line with recent fairness goals presented by Islam, Pan, and Foulds (2021); Li, Wu, and Su (2023).\nIn summary, our main contributions are:\n\u2022 We introduce GHOST, a novel, state-of-the-art, hyper-parameter-free post-processing algorithm that models per-"}, {"title": "", "content": "feature, per-class distributions to improve per-class OSR.\n\u2022 We present an extensive experimental analysis that adapts both the previous and recent state-of-the-art methods while evaluating multiple state-of-the-art DNNs, with results showing that GHOST is statistically significantly better on both global OSR and OOD metrics.\n\u2022 We provide the first fairness analysis in OSR, identify significant per-class differences in large-scale OSR evaluations, and demonstrate that GHOST improves fairness."}, {"title": "Related Work", "content": "Some methods have been proposed to improve the training of DNNs for OSR (Zhang et al. 2022; Xu, Shen, and Zhao 2023; Wan et al. 2024; Wang et al. 2024; Li et al. 2024a,b; Sensoy, Kaplan, and Kandemir 2018). We do not consider these as direct competitors, as they go beyond statistical inference and train reconstruction models and use generative techniques or other additional training processes. Post-processing methods, including GHOST, can all use better features, but as Vaze et al. (2022) pointed out, better closed-set classifiers improve performance more and are continuing to evolve rapidly, so our focus is on post-processing algorithms.\nPost-hoc approaches are well-explored in out-of-distribution (OOD) detection. Moreover, they are used in various practical settings requiring large pre-trained networks. The first attempt to adapt pre-trained DNNs for OSR using statistical inference on representations extracted from a pre-trained backbone was made by Bendale and Boult (2016). They sought to replace the popular SoftMax layer, which is problematic for OSR, with OpenMax. OpenMax computes the centroid for each known class from training data and uses Extreme Value Theory to fit Weibull distributions over the distance from the centroid to the training samples. During inference, the probabilities that a sample belongs to a known class are converted to probabilities of unknown, which are summed and effectively form an additional category representing the probability of unknown. The Extreme Value Machine (EVM) proposed by Rudd et al. (2017) is another OSR system based on statistical inference using distance between samples. It finds a set of extreme vectors in each training-set class and fits a Weibull distribution on the distance between them and the closest samples of other \"negative\u201d classes in high-dimensional feature space. Both systems compute distances in high-dimensional space, so a practitioner must select a distance metric that applies to their DNN backbone. This process often requires a search over possible metrics and other algorithm-related hyperparameters. We might consider these methods to be direct competitors as they employ straightforward statistical measures to recognize known samples, but large scale evaluation shows they are not as effective as some simple baselines (Bisgin et al. 2024).\nUsing network outputs to reject unknowns is widely used, and Hendrycks and Gimpel (2017); Hendrycks et al. (2022) showed that thresholding on Maximum Softmax Probability (MSP) or Maximum Logits (MaxLogit) from a closed-set DNN provides good baselines for OSR. In addition, Vaze et al. (2022) went so far as to argue that good closed-set classifiers with logit-based thresholding are sufficient for OSR. We also"}, {"title": "", "content": "consider the popular energy-based OOD detection technique (Liu et al. 2020), which computes energy based on the logit vector of a DNN (this method's performance is subpar, and so it is relegated to the supplemental). A recent collection of OOD methods, OpenOOD (Yang et al. 2022; Zhang et al. 2023), has compared many of these post-hoc methods using recent, large-scale datasets. Herein, we consider only the best performing: Nearest Neighbor Guidance (NNGuide) (Park, Jung, and Teoh 2023) for OOD detection (others in the supplemental). This method scales the confidence output from a DNN's classification layer by the sample's cosine similarity to a subset of training data, and is currently leading in the OpenOOD ImageNet-1K leaderboard\u00b9 and so we use it as a primary comparison. We show that GHOST normalization, which does not need a reference set, improves performance overall, setting a new standard for large-scale OSR and OOD."}, {"title": "Approach", "content": "The first works on open-set recognition and open-set deep networks (Scheirer et al. 2012; Scheirer, Jain, and Boult 2014; Bendale and Boult 2016; Rudd et al. 2017) all focused on the most distant points within a class or the values at the class boundaries. Hence, it is natural that they employed extreme-value theory as their underlying model. Having evaluated many of these EVT-based approaches in practical settings, we found a few significant difficulties: These methods are sensitive to outliers/mislabeled data (due to their reliance on a small percentage of extreme data) and have a high cost and sensitivity of tuning their hyperparameters. A final difficulty with this approach is reducing the high-dimensional features into a 1-dimensional distance, typically Euclidean or Cosine.\nFeatures within a DNN are learned using large amounts of data. Various papers have shown that, with some mild assumptions, convergence in a two-layer network follows a central-limit theory (Sirignano and Spiliopoulos 2020), and using a mean-field analysis that was extended to some older deep network architectures (Lu et al. 2020) so there are inherently some reasons to hypothesize Gaussian models.\nWe start by summarizing the main simple NN central-limit theory of (Sirignano and Spiliopoulos 2020), which indicates that for a large number M of neurons, the empirical distribution of the neural network's parameters behaves as Gaussian distribution. Their theorems show that, given their assumptions, the empirical distribution of the parameters behaves as a Gaussian distribution with a specific variance-covariance structure. Central to the proofs of these theories is mean-field theory, and the convergence of the parameters to the mean follows from the central limit theorem. These mean-field distributional convergence results were then extended to some older deep networks (Lu et al. 2020), but extending to new networks is complex. We believe that empirical testing, as we do in our experiments, is a sufficient and much easier way to evaluate the Gaussian hypothesis for any new network.\nInspired by those theories, we hypothesize that similarly, when input is from a class seen in training, each value in"}, {"title": "A Gaussian Hypothesis", "content": ""}]}