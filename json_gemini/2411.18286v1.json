{"title": "DualCast: Disentangling Aperiodic Events from Traffic Series with a Dual-Branch Model", "authors": ["Xinyu Su", "Feng Liu", "Yanchuan Chang", "Egemen Tanin", "Majid Sarvi", "Jianzhong Qi"], "abstract": "Traffic forecasting is an important problem in the operation and optimisation of transportation systems. State-of-the-art solutions train machine learning models by minimising the mean forecasting errors on the training data. The trained models often favour periodic events instead of aperiodic ones in their prediction results, as periodic events often prevail in the training data. While offering critical optimisation opportunities, aperiodic events such as traffic incidents may be missed by the existing models. To address this issue, we propose DualCast \u2013 a model framework to enhance the learning capability of traffic forecasting models, especially for aperiodic events. DualCast takes a dual-branch architecture, to disentangle traffic signals into two types, one reflecting intrinsic spatial-temporal patterns and the other reflecting external environment contexts including aperiodic events. We further propose a cross-time attention mechanism, to capture high-order spatial-temporal relationships from both periodic and aperiodic patterns. DualCast is versatile. We integrate it with recent traffic forecasting models, consistently reducing their forecasting errors by up to 9.6% on multiple real datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Traffic series forecasting is a vital component of intelligent transportation systems (ITS) powered by the Internet of Things (IoT). It enables real-time smart urban mobility solutions [1]- [3] such as route planning as well as better transportation scheduling and mobility management strategies [4].\nDeep learning-based solutions have dominated the traffic forecasting literature in recent years [5], [6]. They typically adopt graph neural networks (GNNs) for modelling spatial patterns and sequential models for modelling temporal pat- terns [5]\u2013[11]. Besides, a series of recent studies adopt the attention mechanism to capture dynamic relationships in traffic patterns [6], [11]\u2013[15].\nThe design of these solutions is largely driven by minimis- ing the mean forecasting errors, which are an intuitive and commonly used evaluation metric [5], [11]\u2013[13]. Implicitly, the resulting models are optimised for forecasting periodic traffic patterns, which are both easier to predict and prevalent in the traffic data, leading to an easier reduction in mean errors. However, we observe substantial performance degradation from these models when facing more complex scenarios with aperiodic events, such as traffic incidents. Such aperiodic events are inherently challenging to predict due to their low frequency and randomness. A model capable of promptly identifying and adapting to such events will greatly enhance the effectiveness of not only traffic forecasting but also other IoT applications in general that require real-time response to sensed signals.\nIn this paper, we propose DualCast a model framework to address the issue above. DualCast is not yet another traffic forecasting model. Instead, we aim to present a generic structure to power current traffic forecasting models with stronger learning capability to handle aperiodic patterns from traffic series. DualCast has a dual-branch design to dis- entangle a traffic observation into two signals: (1) the intrinsic branch learns intrinsic (periodic) spatial-temporal patterns, and (2) the environment branch learns external environment contexts that contain aperiodic patterns. We implement Dual- Cast with three representative traffic forecasting models, i.e., GMAN [12], STTN [11], and PDFormer [13], due to their reported strong learning outcomes.\nThe success of our dual-branch framework relies on three loss functions: filter loss, environment loss, and DBI loss. These functions guide DualCast to disentangle the two types of signals and later fuse the learning outcomes to generate the forecasting results.\n(1) The filter loss computes the reciprocal of Kullback- Leibler (KL) divergence between the feature representations learned from two branches, ensuring that each branch captures distinct signals from the input. (2) The environment loss is designed for the environment branch. It computes the recip- rocal of KL divergence between a batch of training samples and a randomly permuted sequence of those samples in the same batch. This loss encourages DualCast to learn the diverse environment contexts at different times, as the samples of the training pair used in the KL divergence are drawn from different periods. (3) The DBI loss is designed for the intrin- sic branch. It encourages DualCast to learn more separated representations for training samples with different (periodic) traffic patterns while closer representations for samples within the same traffic patterns.\nAs the three models [11]-[13] with which DualCast is implemented all use self-attention, we further optimise their self-attention modules to better capture the spatial cor- relation across time. We observe two issues with these ex- isting self-attention-based models: (1) Existing self-attention- based models [11]\u2013[13] learn spatial and temporal patterns separately. They compute attention between nodes (i.e., sensor locations with recorded traffic observations) at the same time step and between different time steps at the same node. The correlations between different nodes at different times have not been modelled explicitly, while such correlations are important for modelling the impact of aperiodic events. For example, traffic congestion caused by an incident may propagate in the road network (spatial correlations) over time (temporal correlations). (2) Existing models take either a local attention [13] or a global attention [12] setup. They compute attention only among connected nodes (based on adjacency matrix) or among all nodes. Either approach may result in limited receptive fields or loss of hierarchical relationships among the nodes (e.g., layers of traffic flow propagation).\nTo address these issues, we propose: (1) a cross-time atten- tion module based on a (conceptual) space-time tree structure for every node starting from every time step, using hierar- chical message passing. This attention mechanism computes attention between different nodes at different time steps to learn their spatial-temporal correlations, which better models traffic propagation patterns. Note that the tree structure is conceptual. It does not incur extra space costs for storage or expensive computational time costs. (2) an attention fusion module to fuse the local attention with the global attention, thereby enlarging the model's receptive field and enabling the learning of the hierarchical relationships among the nodes.\nOverall, this paper makes the following contributions:\n(1) We propose DualCast \u2013 a model framework equipped with two branches and three loss functions to disentangle complex traffic observations into two types of signals for more accurate forecasting. DualCast is versatile in terms of the models to form its two branches \u2013 we use self-attention- based models for their reported strong learning outcomes.\n(2) We propose two enhancements for self-attention-based forecasting models: (i) A cross-time attention module to capture high-order spatial-temporal correlations, and (ii) An attention fusion module to fuse global attention with local attention for enlarging DualCast's receptive field and enabling the learning of the hierarchical relationships among the nodes."}, {"title": "II. RELATED WORK", "content": "We briefly summarise the relevant literature on traffic forecasting models and how existing solutions (1) model correlations between different locations and times, (2) address aperiodic events, and (3) disentangle traffic series.\nTraffic forecasting models. Traffic forecasting is a spatial- temporal forecasting problem. Initially, traffic forecasting stud- ies focused on temporal pattern modelling [16], employing statistics time-series forecasting models such as ARIMA [17]. To better capture temporal patterns, more advanced models, such as the recurrent neural network (RNN) and its variants, are used in follow-up studies [18], [19.\nDynamic spatial correlations are considered in later studies. Since road networks can be represented as graphs, graph neu- ral networks (GNNs)-based models [5]\u2013[11], [20]\u2013[23] have recently become dominant in traffic forecasting. These studies typically adopt GNNs to capture spatial features, along with RNNs [20] or 1-D temporal convolution models (TCNs) [5], [7], [9], [10] to capture temporal features. Compared with RNNs, TCNs are more efficient and can alleviate gradient explosion (or vanishing) issues in long-term forecasting.\nBeyond spatial correlations, the semantic correlations be- tween sensors also attract much attention. Different methods have been proposed to build the adjacency matrix, such as using road network distances [24], [25], node embedding similarity [9], [10], [26], time-series similarity [27], and region similarity [25], [28]. However, these adjacency matrices can only model static relationships, while real traffic patterns (e.g., traffic flows between residential and industrial areas) may vary across time. To address this gap, the attention mechanism is used in recent models to capture further dynamic (i.e., time-varying) relationships [6], [11]\u2013[13], [15], [29] and has achieved strong outcomes. Learning different spatial corre- lations for different periods, such as one adjacency pattern for workday peak hours and another for weekends, is another solution [3].\nModelling correlations between different locations at different times. Though the models above are effective for capturing periodic patterns, they struggle with modelling ape- riodic events and the ripple effect of these events, because they model the spatial and temporal correlations separately. To adapt to aperiodic events, a few GCN-based models [5], [30], [31] connect graph snapshots captured at different time steps to learn correlations between different sensor locations at different times. Such models treat impact of nodes from different hops equally, which does not reflect real-world obser- vations where traffic propagation between nodes at different distances takes different times. Therefore, they still struggle with capturing the propagation patterns of aperiodic events. Our proposed cross-time attention learns different weights to model such varying impacts.\nAperiodic event-aware traffic forecasting. Aperiodic events, such as traffic incidents and social events, are com- mon in real-world (spatial-temporal) traffic series. Accurate forecasting with such events is important for downstream ap- plications, e.g., transportation optimisation and route planning. However, most existing models have poor generalisation of aperiodic events. East-Net [32] and MegaCRN [26] adopt memory augment to alleviate this issue, where the memory augment (i.e., memory network) enhances model representa- tions by learning prototype representations of spatial-temporal patterns, including aperiodic patterns. Though such memory networks can enhance model generalisation, it is difficult to predefine the number of memories to model different peri- odic and aperiodic patterns. Our proposed DualCast utilises environment loss to encourage the model to learn diverse aperiodic patterns without predefined numbers of aperiodic patterns, which is more flexible.\nDisentangling traffic series. Real-world traffic series are complex and entangle multiple kinds of signals. Disentangling traffic series is an intuitive yet non-trivial task for more accurate traffic forecasting. Previous study denoises traffic data [33], while the noise could contain useful information, such as a sudden change caused by a traffic incident \u2013 ignoring such \"noise\" limits model generalisation. MUSE- Net [4] focuses on the disentanglement of multiple periodic signals rather than aperiodic signals. TimeDRL [34] also uses a dual-branch structure, where one learns representations at the time-stamp level (i.e., each time step in the input window has an embedding), and the other at the instance level (i.e., each input window has an embedding). A couple of other studies extract different traffic modes from traffic data using wavelet transform and decoupling [35], [36]. This approach struggles when encountering long-term aperiodic events, such as serious car accidents or major social events, which may confuse the predefined wavelets.\nDisentangling and fusing high-frequency and low-frequency signals have been used to enhance model performance [37], [38]. More recent studies disentangle traffic series into invari- ant signals [30], [39] and environment signals, formed by a predefined set of city zones (e.g., commercial vs. residential) or environmental factors. These models are not based on self- attention, while we do. More importantly, our environment loss functions together with a dual-branch model structure enable capturing environment contexts with a higher flexibility, without the need for predefined aperiodic patterns."}, {"title": "III. PRELIMINARIES", "content": "We start with the necessary background in traffic forecasting and self-attention-based traffic forecasting models."}, {"title": "A. Traffic Forecasting", "content": "We model a network of traffic sensors as a graph $G = (V, E, A)$, where $V$ denotes a set of $N$ nodes (each repre- senting a sensor) and $E$ denotes a set of edges representing the spatial connectivity between the sensors based on the underlying road network. $A \\in \\mathbb{R}^{N \\times N}$ is an adjacency matrix derived from the graph. If $v_i, v_j \\in V$ and $(v_i,v_j) \\in E$, then $A_{i,j} = 1$; otherwise, $A_{i,j} = 0$.\nFor each sensor (i.e., a node hereafter, for consistency) $v_i \\in V$, we use $x_{i,t} \\in \\mathbb{R}^C$ to represent the traffic observa- tion of $v_i$ at time step $t$, where $C$ is the number of types of observations, e.g., traffic flow and traffic speed. Further, $X_t = [x_{1,t}, x_{2,t}...,x_{N,t}] \\in \\mathbb{R}^{N \\times C}$ denotes the observations of all nodes in $G$ at time step $t$, while $\\widehat{X_t} \\in \\mathbb{R}^{N \\times C}$ denotes the forecasts of the nodes in $G$ at time step $t$. We use $X_{t_i:t_j}$ to denote the consecutive observations from $t_i$ to $t_j$, i.e., $[X_{t_i},\u2026\u2026, X_{t_j}]$\nProblem statement. Given a sensor graph $G = (V, E, A)$, a traffic forecasting model generally adopts an encoder-decoder structure to learn from the traffic observations of the previous $T$ steps and generate forecasts for the following $T'$ steps\n$$\\widehat{X_{t+1:t+T'}} = g_\\omega(f_\\theta(X_{t-T+1:t})), \\tag{1}$$ where $f_\\theta$ and $g_\\omega$ denote the encoder and the decoder, respec- tively, and $\\theta\\in \\Theta$ and $\\omega \\in \\Omega$ denote the learnable parameters. We aim to find $f_\\theta$ and $g_\\omega$ to minimise the errors between the ground-truth observations and the forecasts:\n$$\\underset{\\theta\\in\\Theta, \\omega\\in\\Omega}{\\operatorname{argmin}} E_{T\\in \\mathbb{T}} || g_\\omega(f_\\theta(X_{t-T+1:t})) - X_{t+1:t+T'} ||_p \\tag{2}$$ where $\\mathbb{T}$ denotes the time range of traffic observations of the dataset, and $p$ is commonly set as 1 or 2."}, {"title": "B. Self-Attention-Based Traffic Forecasting", "content": "We describe typical structures followed by self-attention- based traffic forecasting (STF hereafter) models [11]-[13], [23]. Our DualCast framework and attention-based optimisa- tions are designed to be compatible with such structures. STF models typically stack spatial-temporal layers (STLayers) as the encoder $f_\\theta$ to learn spatial-temporal correlations between nodes. The hidden representations learned by $f_\\theta$ are fed into a decoder $g_\\omega$ to generate the forecasting results."}, {"title": "A. Dual-branches Structure and Optimisation", "content": "Dual-branch structure. The dual-branch structure disen- tangles the traffic observations into two types of underlying signals, i.e., intrinsic signals and environment signals. The two branches, i.e., the intrinsic branch (IBranch) and the environment branch (EBranch), share an identical structure but use separate parameters. The intrinsic signals reflect intrinsic (periodic) traffic patterns, while the environment signals reflect the external environment (aperiodic) contexts, such as weather, temperatures, or traffic incidents. The two signals together determine the traffic forecasts.\nGiven a batch of B input observations $X \\in \\mathbb{R}^{B \\times T \\times N \\times C}$, we compute disentangling coefficients for both types of signals:\n$$\\mu_i, \\mu_e = \\text{softmax}(Linear(X)), \\tag{7}$$ where $Linear$ denotes a linear layer with an output size of 2; $\\mu_i$ and $\\mu_e$ (both in shape $\\mathbb{R}^{B \\times T \\times N}$) are the disentangling coefficients for the intrinsic and the environment signals, re- spectively. Then, we produce the intrinsic signals $X^i = \\mu_i \\odot X$ and the environment signals $X^e = \\mu_e \\odot X$, where $\\odot$ denotes element-wise product, using $\\mu_i$ and $\\mu_e$ expanded by repeating their values in the last dimension. $X^i$ and $X^e$ are then fed into IBranch and EBranch to produce the output representations $Z_i$ and $Z_e$, respectively. We use IBranch to detail this process, while EBranch runs in the same manner.\nAt IBranch, $X^i$ is fed into the spatial-temporal encoder $f^i_\\theta$ to produce a hidden representation $H^i$, which is then forwarded to the decoder $g^i_\\omega$ to produce the output representation of the branch, $Z_i \\in \\mathbb{R}^{B \\times T \\times N \\times D}$. We concatenate the outputs of both branches to obtain $Z = Concat(Z_i, Z_e)$ and generate the forecasts $\\widehat{X}$ from $Z$ through another linear layer $g_{out}$.\nModel training. We use three loss functions to train Du- alCast: (1) a filter loss to separate the feature spaces of the two branches, (2) an environment loss to learn the impact of environment contexts, and (3) a DBI loss to learn different periodic patterns."}, {"title": "Filter loss.", "content": "The filter loss, $L_{flt}$, is based on KL divergence. It minimises the mutual information between the hidden spaces of the two branches to distinguish the two types of signals. Specifically, we aggregate the output representation $Z^i$ of IBranch along the time dimension by a linear layer and the node dimension by mean pooling, to produce an overall representation $g^i \\in \\mathbb{R}^{B \\times D}$ of each input sample. We denote this aggregation process as $r$, where $\\delta$ denotes the parameters in the linear layer. Following the same process, we can obtain $g^e$ through $r$ in EBranch.\nWe use $softmax(.)$ to map $g^e$ and $g^i$ into distribution and compute the filter loss as follows:\n$$L_{flt} = KL(g^i, g^e)^{-1} \\tag{8}$$"}, {"title": "Environment loss.", "content": "The environment loss, $L_{env}$, guides the EBranch to learn external environment signals (aperi- odic events). Our intuition is that the environment context of different samples from different time periods should be random and hence different (otherwise this becomes a periodic signal). To capture such varying environment contexts, the environment loss guides different samples to generate different environment representations. We randomly permute $g^e$ along the batch dimension to obtain $g^{e'} = \\pi(g^e)$. We then obtain B sample pairs $(g^e_i, g^{e'}_i), i \\in [1, B]$. We use $softmax(\\cdot)$ to map $g^{e'}$ and $g^{e}$. We aim to separate the representations of the sample pairs to guide DualCast to generate diverse environment representations for different times. Thus:\n$$L_{env} = KL(\\pi(g^e), g^e)^{-1} \\tag{9}$$"}, {"title": "DBI loss.", "content": "The DBI loss, $L_{dbi}$, is inspired by the clustering metric Davies-Bouldin index (DBI) [41], to guide IBranch to learn representative intrinsic patterns.\nTraffic observations have different periodic patterns at dif- ferent times, e.g., workdays vs. weekends, and peak hours vs. off hours. We empirically define 17 periodic patterns based on the time, including 15 patterns for workdays (three per day: morning peak-hour, off-hour, and evening peak-hour), one for Saturdays and one for Sunday (cf. Fig. 4). Public holidays are treated as Sundays.\nThe output representation of IBranch, $Z_i$, also contains B samples. We can obtain the representation for each sample by slicing $Z_i$ along the batch size dimension, i.e., $Z_i = [Z^1_i,\u2026\u2026,Z^B_i]$. We classify each sample into one of the 17 patterns based on the starting time of the sample. Then, we obtain a set of sample representations for each pattern. We use $p \\in P$ to denote one such set, and $P$ to denote the set of all 17 sets corresponding to the 17 patterns. We define a matrix $\\Psi \\in \\mathbb{R}^{|P| \\times T \\times N \\times D}$ to be the prototype of the 17 patterns and optimise $\\Psi$ at model training. The intuition of $\\Psi$ is that there may be T distinct sub-patterns forming the observation of each node belonging to one of the 17 periodic patterns. Each sub-pattern is represented as a vector of size D.\nThe DBI loss guides DualCast to learn more separated representations for the training samples with different periodic patterns."}, {"title": "Periodic patterns for the intrinsic branch.", "content": "and closer representations for those with the same periodic patterns. We first compute two metrics $S$ and $P$ that evaluate the compactness of a pattern and the separation among patterns, respectively.\n$$S_p(Z^i, \\Psi_p) = \\frac{1}{|Z_i^p|} \\sum_{j \\in Z_i^p} || \\Psi_p - Z^i_j ||_2 \\tag{10}$$ Here, p is an element (also a set) of set P, j is the j-th sample in $Z^i$, and $Z^i_p$ denotes the slicing of $I$ along the dimension of number of patterns that corresponds to p.\nNext, we compute $P_{p,q}$ to evaluate the separation between sets p,q \u2208 P, $P_{p,q} = ||\\Psi_p \u2013 \\Psi_q||_2.\nAnother metric $R_{p,q}$ balances the compactness of the two sets and the separation between them:\n$$R_{p,q}(Z^i, \\Psi) = \\frac{(S_p + S_q)}{P_{p,q}}. \\tag{11}$$ Based on $R_{p,q}(Z^i, \\Psi)$, we obtain a quality (in terms of compactness and separation) score of set p, denoted by $D_p$:\n$$D_p(Z^i, \\Psi) = \\underset{p \\neq q}{\\operatorname{max}}R_{p,q}. \\tag{12}$$ Finally, we can compute the DBI loss:\n$$L_{dbi} = \\frac{1}{|P|} \\sum_{p \\in P} D_p(Z^i, \\Psi) = \\frac{1}{|P|} \\sum_{p \\in P} D_p(g_\\omega(f_\\theta(X)), \\Psi). \\tag{13}$$ Based on the DBI loss, we can optimise prototype repre- sentations for each periodic pattern. We enhance the represen- tation $Z = Concat(Z_i, Z_e)$ by aggregating $\\Psi$ as follows:\n$$Z' = Z + W\\Psi. \\tag{14}$$ Here, $W \\in \\mathbb{R}^{B \\times |P|}$ is a matrix where each row contains a one-hot vector indicating the pattern set to which each sample belongs, i.e., $W_{j,p} = 1$ if $Z^i_j \\in p$, otherwise $w_{j,p} = 0$. Based on Eq. 14, we rewrite the prediction loss as follows:\n$$L_{pred} = E(||g_{out}(Z') - Y||_p) \\tag{15}$$ Final loss. Our final loss combines the three loss terms above with the prediction loss (Eq. 15), using hyper- parameters $\u03b1, \u03b2$, and $\u03b3$ to control each loss term's contribution:\n$$L = L_{pred} + \u03b1L_{flt} + \u03b2L_{env} + \u03b3L_{dbi}. \\tag{16}$$"}, {"title": "Time complexity.", "content": "DualCast takes $O(T \\cdot N \\cdot D^2 + F)$ time to map $X_{t-T+1:t}$ to $\\widehat{X_{t+1:t+T'}}$, where T denotes the length of the input (output) time window, N denotes the number of sensors, D is the feature dimensionality of the hidden layers, and F denotes the time complexity of the spatial-temporal models (GMAN [12], STTN [11], or PDFormer [13]). Note that D is usually smaller than N in traffic forecasting tasks. Given that the time complexity of the spatial-temporal models is typically $O(F) = O(T \\cdot N^2 \\cdot D)$, the time complexity of DualCast then becomes $O(T \\cdot N^2 \\cdot D)$. This time complexity suggests that applying DualCast to power existing spatial- temporal models will not increase their time complexity, although there is a hidden constant of 2, i.e., DualCast has two branches of spatial-temporal models."}, {"title": "B. Rooted Sub-tree Cross-time Attention", "content": "The rooted sub-tree cross-time attention module consists of global attention and local attention to jointly learn the high- order and dynamic spatial-temporal relationships between nodes, by learning their correlations across time. This module only applies to the spatial layers. We omit the superscript \u2018sp\u2019 in the symbols for brevity.\nComputing cross-time attention adds nodes from other time steps into the graph Gt at time t. Due to the $O(N^2)$ time complexity for attention coefficient computation (recall that N is the number of nodes), we use a feature mapping function [42] that helps alleviate the high computation costs to derive the attention coefficients by the $sim$ function (Eq. 4), i.e., Eq. 4 is replaced by:\n$$h_{t,n}^{loc} = \\frac{\\phi(Q_{t,n}) \\sum_{m=1}^N (M_{n,m} \\phi(K_{t,m}))^T V_{t,m}}{\\phi(Q_{t,n}) \\sum_{m=1}^N M_{n,m} \\phi(K_{t,m})^T}, \\tag{17}$$ where $\\phi$ is a ReLU activation function; n and m denote node n and node m, respectively.\nGlobal attention. We first apply Eq. 17 to compute the self-attention among all nodes at time t, obtaining N vectors $h^{glo}_{t,n} (n \\in [1, N])$, which form a global attention matrix $H^{glo}_t$. As Fig. 5(c) shows, the global attention computes attention coefficients between all nodes, where $M$ in Eq. 17 is a matrix of 1\u2019s. We then update the representations of nodes by aggregating those from all other nodes, weighted by the attention coefficients. In Eq. 17, the two summations terms $\\sum_{m=1}^N M_{n,m} \\phi(K_{t,m})$ and $\\sum_{m=1}^N (M_{n,m} \\phi(K_{t,m}))^T V_{t,m}$ are shared by all nodes, which are computed once. Meanwhile, we do not need to compute $sim(\\cdot, \\cdot)$ in Eq. 4 any more. Thus, we reduce the time complexity from $O(N^2)$ to $O(N)$.\nLocal attention. The local attention captures high-level correlations between nodes within a local area across different times. We achieve this goal by constructing an elaborate graph, where nodes from different times are connected. To learn high-level correlations, we reuse the feature mapping function-enhanced self-attention (Eq. 17), instead of the stacked spatial-temporal GNN layers, for better efficiency and effectiveness.\nWe use two types of edges to help learn cross-time cor- relations between the nodes. For a series of graph snapshots between times $t'$ and $t''$, we construct (1) edges between the same node across different times (red dashed line in Fig. 5(a)), and (2) edges between a node and its one-hop neighbours from other times (green dotted line in Fig. 5(a)). This process yields a large cross-time graph with N|t\" - t' + 1| nodes and edges from the original sensor graph at every time step, and the newly created edges.\nAdjacency matrices used for local attention computa- tion. As Fig. 6 shows, there are two ways to build the adjacent matrices M (not a matrix of 1's any more) used in Eq. 17 for local attention computation in DualCast. In the figure, $A^1$ is the original adjacency matrix that comes with the input graph $G$ (cf. Section III-A) and $A^k$ the k-th-order matrix of A indicating the connectivity between k-hop neighbours.\nFig 6(a) elaborates a simple way to learn high-order rela- tionships, i.e., to pre-compute all k-hop adjacency matrices to connect all neighbours for up to k hops. However, this approach ignores the local hierarchical information. For ex- ample, the red dashed line and the green dotted line have different traffic propagation patterns and propagation time costs in Fig. 5(a), as the red dashed line only concerns the same node across different times, while the green dotted line concerns nodes at different space and times, which should not be ignored. This approach cannot such a scenario as all neighbours are considered to share the same impact, regardless of their distances (i.e., number of hops).\nTo fill this gap, we use a sub-tree structure. As Fig. 5(b) shows, we disentangle the attention into several levels. In the sub-tree building process, we follow the red dashed lines to form 1-hop neighbours and the green dotted lines to form 2- hop neighbours. We compute the attention weights among the neighbours of node n at level k, where k \u2208 [1, K] and K denote the number of levels. Then, we aggregate representations from such neighbours to obtain $h^{k,loc}_t$, as shown in Eq. 18. After computing these representations for all nodes, we form matrix $H^{k,loc}_t$ by assembling the vectors $h^{k,loc}_{t,n}$ for n \u2208 [1, N]. Then, we aggregate representations from all levels to obtain $H^{loc}_t = \\sum_{k=0}^K w_k H^{k,loc}_t$. Here, $w_k$ is a learnable parameter to control the contribution of each hop.\n$$h_{t,n}^{0,loc} = V_{t,n}, \\tag{18}$$ $$\\hat{h}_{t,n}^{k,loc} = \\frac{\\phi(Q_{t,n}) \\sum_{m=1}^N (M^k_{n,m} \\phi(K_{t,m}))^T V_{t,m}}{\\phi(Q_{t,n}) \\sum_{m=1}^N M^k_{n,m} \\phi(K_{t,m})^T} \\tag{18}$$ The above computation process can be seen as a k-hop message-passing process. Based on the k-hop message-passing process, the mask $M^k$ in each step equals to $A^1$, denoted as A. Fig. 6(b) shows this matrix, where $I_N$ denotes an identity ma- trix of size N. During this process, we first compute $\\phi(K_{t,n})$ and a message, denoted as $Mess_n = \\phi(K_{t,n})^T V_{t,m}$, for each node n. Then, we let $\\phi(K_{t,n})$ and $Mess_n$ undergo k steps of message passing. Finally, we update the node representa- tions by aggregating keys and values $\\sum_{m=1}^N A \\phi Mess_m$ and $\\sum_{m=1}^N A \\phi(K_{t,m})$ with a node's own query $\\phi(Q_{t,n})$. Since the message-passing process runs for each edge and among all nodes, we obtain $H^{loc}_t$ with time complexity $O(|E'|)$. Here, E' represents the number of edges after we build the edges across graphs from different time steps (i.e., number of 1's in A). Our proposed full DualCast model uses this strategy to effectively and efficiently capture the relationships between multi-hops neighbours.\nAfter obtaining $H^{loc}_t$ and $H^{glo}_t$, we fuse them as follows:\n$$H_t = H^{loc}_t + W^{glo} H^{glo}_t, \\tag{19}$$ where $W^{glo}$ is a learnable parameter. Then, we concatenate $H_t$ from each time step t to obtain the output of the spatial self-attention layer $H_{sp} = ||_{t=0}^T H_t$.\nWe also use a temporal self-attention module to capture the temporal features from the full input time window. As detailed in Section III-B, we merge $H^{sp}$ with $H^{te}$ to obtain the final output of the spatial-temporal layer.\nDiscussion: The high-order and dynamic spatial-temporal relationships play an important role in traffic forecasting. Previous graph-based methods [5], [8] stack GNNs to capture such correlations, with sub-optimal effectiveness, while the vanilla self-attention models suffer in their quadratic time complexity. Our work addresses these issues and presents a versatile self-attention-based method to exploit the high- order and dynamic spatial-temporal relationships effectively and efficiently."}, {"title": "V. EXPERIMENTS", "content": "Datasets. We use two freeway traffic datasets and an urban traffic dataset: PEMS03 and PEMS08 [43] contain traffic flow data collected by 358 and 170 sensors on freeways in North Central and San Bernardino (California), respectively, from July to August 2017; Melbourne [27] contains traffic flow data collected by 182 sensors in the City of Melbourne, Australia, from July to September 2022."}, {"title": "B. Overall Results", "content": "Model performance across all times. Table III reports the forecasting errors averaged over a one-hour time span. Pow- ered by our DualCast, DualCast-G, DualCast-P, and DualCast- S all outperform their respective vanilla counterparts consis- tently. DualCast-P has the best performance on freeway traffic datasets PEMS03 and PEMS08, while DualCast-G performs the best on the urban traffic dataset Melbourne. Compared to PEMS03 and PEMS08, Melbourne represents an urban environment with greater variability in the traffic flow series (cf. Table I). GMAN with a simple yet effective structure is more robust across both types of datasets, explaining for its (and DualCast-G\u2019s) strong performance on Melbourne. PDFormer features a complex model structure that utilises time series clustering to learn typical patterns and enhance performance. While it excels over PEMS03 and PEMS08, its performance on Melbourne is suboptimal."}]}