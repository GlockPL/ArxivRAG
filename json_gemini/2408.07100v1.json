{"title": "Pattern-Matching Dynamic Memory Network for Dual-Mode Traffic Prediction", "authors": ["Wenchao Weng", "Mei Wu", "Hanyu Jiang", "Wanzeng Kong", "Xiangjie Kong", "Feng Xia"], "abstract": "In recent years, deep learning has increasingly gained attention in the field of traffic prediction. Existing traffic prediction models often rely on GCNs or attention mechanisms with O(N^2) complexity to dynamically extract traffic node features, which lack efficiency and are not lightweight. Additionally, these models typically only utilize historical data for prediction, without considering the impact of the target information on the prediction. To address these issues, we propose a Pattern-Matching Dynamic Memory Network (PM-DMNet). PM-DMNet employs a novel dynamic memory network to capture traffic pattern features with only O(N) complexity, significantly reducing computational overhead while achieving excellent performance. The PM-DMNet also introduces two prediction methods: Recursive Multi-step Prediction (RMP) and Parallel Multi-step Prediction (PMP), which leverage the time features of the prediction targets to assist in the forecasting process. Furthermore, a transfer attention mechanism is integrated into PMP, transforming historical data features to better align with the predicted target states, thereby capturing trend changes more accurately and reducing errors. Extensive experiments demonstrate the superiority of the proposed model over existing benchmarks. The source codes are available at: https://github.com/wengwenchao123/PM-DMNet.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of society and technology, there has been a significant increase in vehicles within cities, as well as the growing popularity of services like shared bicycles and ride-hailing platforms such as Uber and Didi. This expansion has broadened the application of urban traffic management by governments and heightened public transportation demands. However, issues such as limited resources and inadequate scheduling systems have increasingly highlighted challenges in traffic management and the imbalance of transportation demand. As a result, accurate traffic forecasting has become a crucial issue in fields such as traffic management, urban planning, and the sharing economy. Precise traffic prediction enables governments to better allocate social resources to maintain urban transportation operations. It also allows companies to distribute resources such as shared bicycles and taxis to areas with high demand, thereby avoiding their idle presence in low-demand areas. This approach can reduce energy consumption and passenger waiting times.\nIn recent years, researchers have conducted extensive studies in traffic prediction to promote the development of intelligent transportation systems. Early traffic prediction methods utilized statistical approaches for prediction. Auto-regressive (AR), Moving Average (MA), and Auto-Regressive Integrated Moving Average (ARIMA) models [1], as the most representative classical statistical methods, have been extensively employed in traffic prediction. Additionally, machine learning techniques represented by Support Vector Regression (SVR) [2] and Kalman filters [3] have also been applied to traffic prediction to achieve more accurate predictions and handle more complex sequences. However, these methods require data to exhibit stationarity to be effective, which limits their ability to capture the intricate non-linear spatio-temporal correlations present in traffic condition.\nIn recent years, the advancements of deep learning in domains such as Computer Vision and Natural Language Processing have motivated researchers to explore its application in traffic prediction for improved outcomes. Early deep learning prediction models conceptualized urban traffic as images and segmented them into grids. Convolutional Neural Networks (CNNs) [4] were employed to analyze spatial correlations within these grids, while Recurrent Neural Networks (RNNs) [5], [6], [7] or CNNs [8], [9] were utilized to capture temporal dependencies. However, the structure of the transportation network can be viewed as a topological graph, containing non-Euclidean attributes. CNNs only extract features from the surrounding nodes and cannot capture features from other locations across space. As Graph Convolutional Networks (GCNs) [10] are effective in handling non-Euclidean structures, it has been widely applied in the field of transportation [11], [8], [6]. Additionally, attention mechanisms [12], [13], [14] have been incorporated for spatio-temporal feature modeling.\nHowever, current methods still possess the following limitations:\n1) Lack of Effective Traffic Feature Extraction: Traffic data inherently exhibits complex spatio-temporal correlations. To capture these spatio-temporal correlations, researchers have employed GCN to capture spatial relationships between nodes,"}, {"title": "II. RELATED WORK", "content": "As one of the most representative tasks in spatio-temporal prediction, researchers employed a myriad of methodologies to model the spatio-temporal characteristics within traffic condition. STGCN [20] leveraged GCN and predefined matrices to capture spatial correlations between nodes, employing Gate CNNs to model such spatial dependencies. DCRNN [7] integrated diffusion convolution with GRU to model the spatio-temporal relationships inherent in traffic condition. MTGNN [21] utilized adaptive embeddings to generate an adaptive graph structure, capturing spatial correlations among diverse nodes. CCRNN [22] introduced a novel graph convolutional structure termed as CGC and employed a hierarchical coupling mechanism, linking upper-layer graph structures with underlying ones to extract temporal-spatial features. GMAN [13] harnessed three distinct attention mechanisms to capture the spatio-temporal characteristics present in traffic condition. MPGCN [15] utilized GCN to identify mobility patterns at bus stops through clustering and employed GCN2Flow to predict passenger flow based on various mobility patterns. Building on the foundation of MPGCN, MPGNNFormer [16] designed a STGNNFormer to extract both temporal and spatial dependencies. Although these spatiotemporal prediction models have achieved notable success, the GCNs and attention mechanisms they use often require O(N^2) or even higher complexity, resulting in substantial computational costs.\nThe Memory Network [23] introduced an external memory mechanism, enabling it to better handle and utilize long-term information. Memory networks have found extensive applications in the domains of natural language processing and machine translation. MemN2N [24] introduced a novel end-to-end memory network framework that facilitates its straightforward application in real-world environments. Kaiser et al. [25] proposed memory networks with the capability to adapt to various zero-shot scenarios. Mem2seq [26] integrated multi-hop attention mechanisms with memory networks, enabling their deployment in dialog systems. MemAE [27] explored the application of memory networks in video anomaly detection tasks, subsequent studies [28] validating the feasibility of this approach. MTNet [29] endeavored to apply memory networks in multi-variate time series prediction, yielding promising results. In the most recent advancements, PM-MemNet [30] devised a novel Graph Convolutional Memory Network (GCMem) to model the spatio-temporal correlations inherent in given traffic condition. Additionally, MegaCRN [31], inspired by memory network principles, designed a Meta-Graph Learner to construct dynamic graphs, addressing temporal-spatial heterogeneities. Although memory networks have been applied in traffic prediction, they still require integration with other feature extraction methods (e.g., GCN) to perform effectively.\nUnlike previous spatio-temporal prediction models, PM-DMNet uses a dynamic memory network to extract traffic pattern features, achieving superior performance while reducing complexity to O(N), which significantly lowers computational costs. Additionally, prior research overlooks the impact of time features corresponding to the prediction targets on the targets themselves. PM-DMNet fully considers this characteristic and designs two prediction methods to utilize these time features, leading to successful outcomes."}, {"title": "III. PRELIMINARIES", "content": "Given that traffic condition is collected at regular time intervals, each set of traffic condition possesses unique and systematic temporal information. To harness these temporal characteristics effectively, we employ a temporal indexing function to extract time-related information. Let d(t) and w(t) represent the intra-daily and weekly indexing functions, respectively. These functions transform the temporal information of the traffic condition into corresponding intra-daily and weekly time-related attributes. For specific examples, refer to B. Traffic Prediction\nThe objective of traffic prediction is to utilize historical traffic condition to forecast future traffic condition.\nWe represent the traffic condition Xt \u2208 RN\u00d7C for N nodes in the road network at time t, where C is the dimensionality of traffic condition, signifying C types of traffic condition. We model the historical traffic condition X = [X1, X2, ..., Xn] \u2208 Rn\u00d7N\u00d7C over the past n time steps using the model f to predict the traffic condition Y = [Yn+1, Yn+2,..., Yn+m] \u2208"}, {"title": "IV. MODEL ARCHITHECTURE", "content": "Figure 3 illustrates the comprehensive architecture of PM-DMNet, which comprises a Time Embedding Generator (TE Generator), Dynamic Pattern Matching Gated Recurrent Unit (DPMGRU), and Transfer Attention Mechanism (TAM). In the subsequent sections, we will provide a detailed exposition of each module.Traffic condition is influenced by people's travel habits and lifestyles, exhibiting clear temporal such as rush hours during mornings and evenings. To fully leverage temporal features, we introduce two independent embedding pools TD \u2208 RNa\u00d7p,TW \u2208 RNw\u00d7p to learn features for intra-daily and weekly patterns. Here, Na represents the number of time slots in a day, and Nw = 7 represents the number of days in a week. As depicted in Figure 4, based on the time information t, we derive the intra-daily index d(t) and the weekly index w(t). Utilizing d(t) and w(t), we obtain where denotes the hadamard product.\nThe memory module incorporates a learnable memory matrix P = [P1, P2, ..., PM] \u2208 RM\u00d7p, where symbolizes a unique traffic pattern. To dynamically adjust the memory matrix, thereby avoiding pattern singularization and adapting to the prevailing traffic conditions at time t, we integrate the current time embedding Tt with P. This fusion can be represented as:\nPt = P \u2a00 Tt\nwhere Pt \u2208 RMXP represents the memory network module at time t. Through training, Pt can learn the most representative traffic patterns at time t. By integrating the time embedding Tt dynamically, the model can adjust its memory P\u0142 to better capture evolving traffic patterns and conditions over time.\nAs shown in Figure 5, we extract dynamic signals from the traffic condition, which can be represented as:\nFi = MLP(xi)\nwhere Fi \u2208 RP represents the dynamic signal extracted from the traffic condition xi at node i. It is used to query the memory matrix for the traffic pattern most similar to xi.\nAfterwards, the similarity weight between Fi and the memory matrix P\u0142 is computed through a similarity calculation:\nw = softmax(FPt)\nwhere wi \u2208 RM represents the similarity weight vector.\nSubsequently, Pt is linearly transformed to obtain the pattern features corresponding to various traffic patterns. It is then multiplied with the similarity weight vector w to extract the pattern features most similar to xt, as follows:\nh = wiPt\nwhere ht \u2208 RMX Fout represents the pattern features in the memory matrix Pt, and hi represents the extracted traffic pattern features.\nFinally, the residual connection is employed to concatenate hand x for extracting hidden features:\nH = (ht||xi)\u0398\nAll node hidden states H are aggregated into Ht = (H1, H2, ..., HN), serving as the final output of the dynamic memory network.\nTo enable each node to learn its unique traffic pattern, enhancing the model's robustness and effectiveness, we utilize two parameter matrices to optimize the learnable parameters \u0398. Specifically, we use the node embedding matrix E \u2208RN\u00d7d and the weight pool W \u2208 Rd\u00d7Fin\u00d7Fout to generate \u0398\u2208 RN\u00d7Fin\u00d7Fout, which can be expressed as:\nTo capture the spatio-temporal features inherent in traffic condition, we integrate the gated recurrent unit (GRU) with a dynamic memory network to construct a framework that encapsulates both temporal dynamics and spatial correlations. Specifically, we replace the MLP layer in the GRU with a dynamic memory network, resulting in the Dynamic Pattern Matching Gated Recurrent Unit (DPMGRU). Mathematically, DPMGRU can be formulated as:\nrt = \u03c3(Vr*G(Xt||Ht-1))\nut = \u03c3(Vu*G(Xt||Ht-1))\nht = tanh(vh*G(Xt||UtHt-1))\nHt = rt \u2a00 Ht-1 + (1 - rt) \u2a00 ht\nwhere Xt and Ht denote the input and output at time step t, respectively. o represents the sigmoid activation function. r and u correspond to the reset gate and update gate, respectively. \u2729G denotes the dynamic memory network module, while vr, vu, vh are the learnable parameters associated with the relevant memory network module.\nTo mitigate the discrepancy between historical data and the prediction target leading to errors, we employ a transfer attention mechanism to transform the learned hidden features from historical data. Specifically, we first linearly transform the encoder's output Hn \u2208 RN\u00d7D, historical time embedding Tn \u2208 RP, and future embeddings TF = (Tn+1,Tn+2,..., Tn+m) \u2208 Rmxp into queries, keys, and values, represented as:\nQ = V(Hn,TF)WQ, K = \u2200(Hn,Tn)WK, V = \u2200(Hn,Tn)WV\nwhere WQ, WK, WV \u2208 R(D+p)\u00d7dk serve as learnable parameters, and V() denotes a broadcasting operation. Subsequently, the transfer attention can be expressed as:\n= softmax(\nFinally, the feature fusion between Hn and HTA \u2208 Rm\u00d7N\u00d7D is achieved using residual connections to obtain the input for the decoder:\nThe traditional encoder-decoder architecture typically employs the Recurrent Multi-step Prediction (RMP) method for forecasting. However, recurrent decoding has inherent limitations, including: (i) error accumulation due to recurrent predictions, and (ii) the sequential nature of recursion, which restricts the model's ability for parallel computation, thus limiting the improvement of inference speed. [32] demonstrates that Parallel Multi-step Prediction (PMP) methods can achieve comparable or even better results than RMP when appropriate techniques are applied. Therefore, two variants are designed to implement and investigate these prediction methods:"}, {"title": "V. EXPERIMENTAL SETUP", "content": "In this section, experiments are conducted on ten real-world datasets to validate the effectiveness of the proposed PM-DMNet. The datasets used are categorized into four types: bike demand datasets include NYC-Bike14 [4], NYC-Bike15 [33], and NYC-Bike16 [22]; taxi demand datasets include NYC-Taxi15 [33] and NYC-Taxi16 [22]; traffic flow datasets include PEMSD4 [34], PEMSD7 [20], and PEMSD8 [34]; and traffic speed datasets include PEMSD7(M) and PEMSD7(L) [9]. Moreover, Unlike traffic flow and traffic speed datasets, the traffic demand datasets have two dimensions: 'Pick-up' and 'Drop-off'. We set n = 12 historical time steps to predict m = 12 future time steps.\nAll experiments are conducted on a server equipped with an NVIDIA GeForce GTX 4090 GPU. The Adam optimizer is used for model optimization, and the Mean Absolute Error (MAE) is adopted as the loss function. The hyper-parameter settings for the model under the two different prediction methods, such as the temporal embedding dimension p, node embedding dimension d, memory network matrix dimension M, batch size, and learning rate, are detailed in During training, an early stopping strategy is employed to terminate training and prevent over-fitting. Additionally, a scheduled sampling strategy [35] is applied to PM-DMNet(R) to enhance its robustness."}, {"title": "VI. EXPERIMENTS", "content": "Table IV presents the results of our model and baselines across different datasets. Clearly, optimal results are achieved by our model across all five datasets. XGBoost, being a machine learning model, fails to capture the nonlinear relationships within traffic condition, resulting in its inferior performance. DCRNN, STGCN, and STG2Seq utilize predefined graph structures to capture spatio-temporal correlations within traffic condition, yielding satisfactory outcomes. However, due to the fixed weights in these predefined graph structures, the inability to capture dynamic correlations leaves significant room for improvement. MTGNN and GTS demonstrate commendable progress by learning graph structures adaptively from the data. Nevertheless, these adaptive graphs remain static and fail to capture the dynamic relationships between nodes. MegaCRN employs a meta-graph learner to construct dynamic graphs for extracting correlations between nodes. However, it does not consider the influence of temporal information on traffic patterns, which limits its performance. PM-DMNet excels by leveraging a dynamic memory network to dynamically extract features by identifying the most analogous traffic patterns based on historical data. Figure 7 illustrates the prediction errors of PM-DMNet compared to three baseline models across different prediction horizons. It is observed that, except for the initial three prediction steps, PM-DMNet consistently achieves lower prediction errors than the baseline models. Additionally, the error growth rate of PM-DMNet across all time horizons is slower than that of the baseline models. Benefiting from the functionality of the evolving graph, ESG achieves comparable short-term prediction performance to PM-DMNet. However, as the prediction horizon expands, the error growth rate of ESG becomes significantly faster than that of PM-DMNet, resulting in an overall performance inferior to PM-DMNet. By leveraging temporal information corresponding to the prediction targets, PM-DMNet substantially reduces prediction uncertainty, thereby enhancing performance.\nTable V presents the results of our model and baseline models on traffic flow/speed datasets. It is observed that, except for PEMSD8 where STWave slightly outperforms PM-DMNet (P) and is comparable to PM-DMNet (R), our model achieves the best performance across all datasets. Figure 8 shows the prediction errors of PM-DMNet and the two other best baseline models at different prediction horizons. From Figure 8, it is evident that the error gaps between models are more pronounced in the flow datasets compared to the speed datasets, indicating that predicting traffic speed is more challenging than predicting traffic flow. STWave utilizes the DWT algorithm to decompose traffic data into two separate low-frequency and high-frequency sequences, modeling them independently while considering the impact of temporal information, resulting in good performance on traffic flow datasets. However, on speed datasets, due to the inherent differences between traffic speed and traffic flow, the DWT algorithm struggles to decompose useful high and low-frequency sequences, causing STWave's performance to be on par with MegaCRN. PM-DMNet does not rely on sequence decomposition for modeling, thus avoiding the difficulties associated with ineffective decomposition, leading to excellent performance on both flow and speed datasets."}, {"title": "B. Computation Cost", "content": "To compare and demonstrate the computational efficiency of our model, we evaluate the training time, inference time, and GPU cost of selected models. The batch size for all models is set to 32. Table VI shows the computational costs of PM-DMNet compared to baseline models. As observed in Table VI, the training and inference times of PM-DMNet(P) are significantly lower than those of other baselines, and it also outperforms PM-DMNet(R), demonstrating the advantages of the dynamic memory network and PMP in terms of computational speed and memory usage. Despite ESG's strong performance, its high GPU cost and relatively slow processing speed present challenges in deployment. Although STWave employs"}, {"title": "VII. ABLATION STUDY", "content": "In this section, ablation experiments are conducted on the key components of PM-DMNet to validate their effectiveness. To investigate the impact of different modules, the following variants are designed:\nW/O Decoder: This variant removes the decoder component and predicts using an MLP layer directly applied to the encoder's output. Since the decoding process is omitted, this variant is identical for both PM-DMNet(P) and PM-DMNet(R).\nW/O TAM: In this variant, the Transfer Attention Module (TAM) is excluded. Instead, the prediction is made using the output Hn from the encoder, replacing the output Hn+1 from the transfer attention mechanism.\nW/O DMN: This variant substitutes the Dynamic Memory Network (DMN) module with an MLP layer for making predictions.\nW/O NAPL: This variant removes the Node Adaptive Parameter learning (NAPL) module and uses a linear layer instead.\nTable VII presents the performance of PM-DMNet(P) and PM-DMNet(R) alongside their variants. It is evident from the table that PM-DMNet(P) and PM-DMNet(R) outperform all other variants, demonstrating the effectiveness of each component.\nFor the W/O Decoder variant, the pattern matching process is omitted during the decoding stage, and predictions are made directly using an MLP layer. As a result, this variant can only utilize historical data information and lacks the ability to leverage the time point information of the prediction target. Consequently, its performance is inferior to PM-DMNet(P) and PM-DMNet(R).\nThe performance of the W/O TAM variant also falls short of PM-DMNet(P). This indicates that the discrepancy between historical data and the prediction target leads to a performance decline, validating our proposed solution. This shows that using a suitable method for parallel prediction can achieve results comparable to or better than serial prediction.\nThe W/O DMN variant's performance is significantly inferior to both PM-DMNet models, highlighting the feasibility of our approach to use a memory network to match and extract the most representative traffic patterns.\nSimilarly, the performance of the W/O NAPL variant is lower than that of the two PM-DMNet models, underscoring"}, {"title": "B. Effectiveness Analysis of GCN and DMN", "content": "To validate the differences in performance and computational cost between GCN and DMN, a variant named DGCNet is designed. This variant uses dynamic graph convolution instead of DMN. The formula for dynamic graph convolution is expressed as follows:\nE = Ft \u2729 Tt\nAd = ReLU(Et\u2729Et)\nH\u2081 = (IN + D\u00af\u00bd AD\u00af)\u0425\u04e8\nTable VIII presents the results of GCN and DMN on these two datasets. It can be observed that PM-DMNet outperforms DGCNet, indicating that DMN can achieve excellent performance without relying on GCN. Additionally, while PM-DMNet's computational metrics are slightly better than DGCNet on the smaller PEMSD8 dataset, the difference is not significant. However, on the larger PEMSD7 dataset, PM-DMNet's computational metrics are significantly superior to those of DGCNet, demonstrating the advantage of DMN's O(N) complexity over GCN's O(N^2) complexity in large-scale node scenarios."}, {"title": "C. Effectiveness Analysis of time embedding", "content": "To validate the impact of intra-daily time features and weekly time features on the model, two variants are designed for this subsection:\nuse day: The dynamic memory network is updated using only intra-daily time feature embeddings in this variant.\nuse week: The dynamic memory network is updated using only weekly time feature embeddings in this variant.\nExperiments are conducted on four datasets to observe the influence of time information on model performance across different types of data.\nFigure 9 presents the performance of PM-DMNet(P) and PM-DMNet(R) along with their variants. It can be observed that when only one type of time feature embedding is used, the model's performance generally decreases. Except for the NYC-Taxi16 dataset, where use week outperforms use day in PM-DMNet(P), the performance of use day is superior to use week in all other cases. This indicates that intra-daily information typically has a greater impact on model performance than weekly information. Additionally, in the PEMSD7(M) dataset, the performance of use day is comparable to that of their original models, while the performance of use week varies significantly. This suggests that, unlike other types of data, traffic speed data shows less pronounced differences between weekdays and weekends, exhibiting high similarity."}, {"title": "VIII. HYPER-PARAMETER ANALYSIS", "content": "To validate the impact of hyperparameters on model performance, hyperparameter experiments are conducted on the PEMSD8 dataset. Specifically, in this section, we investigate the effects of the temporal embedding dimension p, the dimension d of the node embedding matrix E in the node adaptive module, and the dimension M of the memory network matrix. In these experiments, other parameters are kept constant while only the parameter under study is changed.\nIt is observed that the model achieves stable and excellent performance when M is between 5 and 20. Therefore, M is set to 10."}, {"title": "IX. VISUALIZATION", "content": "To explore whether the Node Adaptive Parameter module captures the unique traffic patterns of each node, we utilize T-SNE [45] to visualize the node embedding matrix E used in the module trained on NYC-Taxi16 dataset.\nFigure 13 illustrates the visualization results of the node embeddings E. From the figure, it can be observed that certain nodes exhibit a clustering phenomenon, while a few nodes overlap, indicating high similarity in traffic patterns among them. Moreover, there are nodes that are far apart, suggesting significant differences in their traffic patterns.\nTo further verify the high similarity in traffic patterns among nearby nodes and the differences in traffic patterns among distant nodes, we select adjacent nodes within the red-bordered area, specifically Node 215 and Node 222, as well as a distant node within the blue-bordered area, Node 26, for visualization of their traffic demand data. Figures 14(a) and 14(b) respectively illustrate the trend changes in the 'Pick-up' and 'Drop-off' features of the traffic demand for these three nodes. It is evident that the trends for Node 215 and Node 222 are highly similar, indicating a strong correlation between them. Meanwhile, the trend for Node 26 is notably different from the other two nodes, suggesting a significant difference in their traffic patterns. The visualization results above confirm that the Node Adaptive Parameter module can learn the traffic patterns of individual nodes effectively."}, {"title": "X. CONCLUSION", "content": "This paper proposes a novel traffic prediction model, PM-DMNet. PM-DMNet employs a new dynamic memory network module that learns the most representative traffic patterns into a memory network matrix. During prediction, the model extracts pattern features by matching the current traffic pattern with the memory network matrix. Additionally, PM-DMNet supports both parallel and sequential Multi-step prediction methods to meet different needs. To further enhance the accuracy of parallel Multi-step prediction, a transfer attention mechanism is introduced to mitigate the disparity between historical data and prediction targets. Extensive experiments validate the effectiveness of PM-DMNet. In future work, further methods for extracting features from patterns are planned to be explored."}]}