{"title": "Pattern-Matching Dynamic Memory Network for Dual-Mode Traffic Prediction", "authors": ["Wenchao Weng", "Mei Wu", "Hanyu Jiang", "Wanzeng Kong", "Xiangjie Kong", "Feng Xia"], "abstract": "In recent years, deep learning has increasingly gained attention in the field of traffic prediction. Existing traffic prediction models often rely on GCNs or attention mechanisms with $O(N^2)$ complexity to dynamically extract traffic node features, which lack efficiency and are not lightweight. Additionally, these models typically only utilize historical data for prediction, without considering the impact of the target information on the prediction. To address these issues, we propose a Pattern-Matching Dynamic Memory Network (PM-DMNet). PM-DMNet employs a novel dynamic memory network to capture traffic pattern features with only $O(N)$ complexity, significantly reducing computational overhead while achieving excellent performance. The PM-DMNet also introduces two prediction methods: Recursive Multi-step Prediction (RMP) and Parallel Multi-step Prediction (PMP), which leverage the time features of the prediction targets to assist in the forecasting process. Furthermore, a transfer attention mechanism is integrated into PMP, transforming historical data features to better align with the predicted target states, thereby capturing trend changes more accurately and reducing errors. Extensive experiments demonstrate the superiority of the proposed model over existing benchmarks. The source codes are available at: https://github.com/wengwenchao123/PM-DMNet.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of society and technology, there has been a significant increase in vehicles within cities, as well as the growing popularity of services like shared bicycles and ride-hailing platforms such as Uber and Didi. This expansion has broadened the application of urban traffic management by governments and heightened public transportation demands. However, issues such as limited resources and inadequate scheduling systems have increasingly highlighted challenges in traffic management and the imbalance of transportation demand. As a result, accurate traffic forecasting has become a crucial issue in fields such as traffic management, urban planning, and the sharing economy. Precise traffic prediction enables governments to better allocate social resources to maintain urban transportation operations. It also allows companies to distribute resources such as shared bicycles and taxis to areas with high demand, thereby avoiding their idle presence in low-demand areas. This approach can reduce energy consumption and passenger waiting times.\nIn recent years, researchers have conducted extensive studies in traffic prediction to promote the development of intelligent transportation systems. Early traffic prediction methods utilized statistical approaches for prediction. Auto-regressive (AR), Moving Average (MA), and Auto-Regressive Integrated Moving Average (ARIMA) models [1], as the most representative classical statistical methods, have been extensively employed in traffic prediction. Additionally, machine learning techniques represented by Support Vector Regression (SVR) [2] and Kalman filters [3] have also been applied to traffic prediction to achieve more accurate predictions and handle more complex sequences. However, these methods require data to exhibit stationarity to be effective, which limits their ability to capture the intricate non-linear spatio-temporal correlations present in traffic condition.\nIn recent years, the advancements of deep learning in domains such as Computer Vision and Natural Language Processing have motivated researchers to explore its application in traffic prediction for improved outcomes. Early deep learning prediction models conceptualized urban traffic as images and segmented them into grids. Convolutional Neural Networks (CNNs) [4] were employed to analyze spatial correlations within these grids, while Recurrent Neural Networks (RNNs) [5], [6], [7] or CNNs [8], [9] were utilized to capture temporal dependencies. However, the structure of the transportation network can be viewed as a topological graph, containing non-Euclidean attributes. CNNs only extract features from the surrounding nodes and cannot capture features from other locations across space. As Graph Convolutional Networks (GCNs) [10] are effective in handling non-Euclidean structures, it has been widely applied in the field of transportation [11], [8], [6]. Additionally, attention mechanisms [12], [13], [14] have been incorporated for spatio-temporal feature modeling.\nHowever, current methods still possess the following limitations:\n1) Lack of Effective Traffic Feature Extraction: Traffic data inherently exhibits complex spatio-temporal correlations. To capture these spatio-temporal correlations, researchers have employed GCN to capture spatial relationships between nodes,"}, {"title": "II. RELATED WORK", "content": "As one of the most representative tasks in spatio-temporal prediction, researchers employed a myriad of methodologies to model the spatio-temporal characteristics within traffic condition. STGCN [20] leveraged GCN and predefined matrices to capture spatial correlations between nodes, employing Gate CNNs to model such spatial dependencies. DCRNN [7] integrated diffusion convolution with GRU to model the spatio-temporal relationships inherent in traffic condition. MTGNN [21] utilized adaptive embeddings to generate an adaptive graph structure, capturing spatial correlations among diverse nodes. CCRNN [22] introduced a novel graph convolutional structure termed as CGC and employed a hierarchical coupling mechanism, linking upper-layer graph structures with underlying ones to extract temporal-spatial features. GMAN [13] harnessed three distinct attention mechanisms to capture the spatio-temporal characteristics present in traffic condition. MPGCN [15] utilized GCN to identify mobility patterns at bus stops through clustering and employed GCN2Flow to predict passenger flow based on various mobility patterns. Building on the foundation of MPGCN, MPGNNFormer [16] designed a STGNNFormer to extract both temporal and spatial dependencies. Although these spatiotemporal prediction models have achieved notable success, the GCNs and attention mechanisms they use often require $O(N^2)$ or even higher complexity, resulting in substantial computational costs.\nThe Memory Network [23] introduced an external memory mechanism, enabling it to better handle and utilize long-term information. Memory networks have found extensive applications in the domains of natural language processing and machine translation. MemN2N [24] introduced a novel end-to-end memory network framework that facilitates its straightforward application in real-world environments. Kaiser et al. [25] proposed memory networks with the capability to adapt to various zero-shot scenarios. Mem2seq [26] integrated multi-hop attention mechanisms with memory networks, enabling their deployment in dialog systems. MemAE [27] explored the application of memory networks in video anomaly detection tasks, subsequent studies [28] validating the feasibility of this approach. MTNet [29] endeavored to apply memory networks in multi-variate time series prediction, yielding promising results. In the most recent advancements, PM-MemNet [30] devised a novel Graph Convolutional Memory Network (GCMem) to model the spatio-temporal correlations inherent in given traffic condition. Additionally, MegaCRN [31], inspired by memory network principles, designed a Meta-Graph Learner to construct dynamic graphs, addressing temporal-spatial heterogeneities. Although memory networks have been applied in traffic prediction, they still require integration with other feature extraction methods (e.g., GCN) to perform effectively.\nUnlike previous spatio-temporal prediction models, PM-DMNet uses a dynamic memory network to extract traffic pattern features, achieving superior performance while reducing complexity to $O(N)$, which significantly lowers computational costs. Additionally, prior research overlooks the impact of time features corresponding to the prediction targets on the targets themselves. PM-DMNet fully considers this characteristic and designs two prediction methods to utilize these time features, leading to successful outcomes."}, {"title": "III. PRELIMINARIES", "content": "Given that traffic condition is collected at regular time intervals, each set of traffic condition possesses unique and systematic temporal information. To harness these temporal characteristics effectively, we employ a temporal indexing function to extract time-related information. Let $d(t)$ and $w(t)$ represent the intra-daily and weekly indexing functions, respectively. These functions transform the temporal information of the traffic condition into corresponding intra-daily and weekly time-related attributes. For specific examples, refer to Table I.\nThe objective of traffic prediction is to utilize historical traffic condition to forecast future traffic condition.\nWe represent the traffic condition $X_t \\in \\mathbb{R}^{N \\times C}$ for $N$ nodes in the road network at time $t$, where $C$ is the dimensionality of traffic condition, signifying $C$ types of traffic condition. We model the historical traffic condition $X = [X_1, X_2, ..., X_n] \\in \\mathbb{R}^{n \\times N \\times C}$ over the past $n$ time steps using the model $f$ to predict the traffic condition $Y = [Y_{n+1}, Y_{n+2},..., Y_{n+m}] \\in \\mathbb{R}^{m \\times N \\times C}$ for the future $m$ time steps, which can be expressed as:\n$[X_1, X_2, ..., X_n] \\overset{f}{\\Rightarrow} [Y_{n+1}, Y_{n+2},..., Y_{n+m}]$\n(1)\nIn addition, The corresponding actual values are represented by $\\hat{Y} = [\\hat{Y}_{n+1}, \\hat{Y}_{n+2},..., \\hat{Y}_{n+m}] \\in \\mathbb{R}^{m \\times N \\times C}$."}, {"title": "IV. MODEL ARCHITHECTURE", "content": "Figure 3 illustrates the comprehensive architecture of PM-DMNet, which comprises a Time Embedding Generator (TE Generator), Dynamic Pattern Matching Gated Recurrent Unit (DPMGRU), and Transfer Attention Mechanism (TAM). In the subsequent sections, we will provide a detailed exposition of each module.\nTraffic condition is influenced by people's travel habits and lifestyles, exhibiting clear temporal such as rush hours during mornings and evenings. To fully leverage temporal features, we introduce two independent embedding pools $T^D \\in \\mathbb{R}^{N_d \\times p}$, $T^W \\in \\mathbb{R}^{N_w \\times p}$ to learn features for intra-daily and weekly patterns. Here, $N_d$ represents the number of time slots in a day, and $N_w = 7$ represents the number of days in a week. As depicted in Figure 4, based on the time information $t$, we derive the intra-daily index $d(t)$ and the weekly index $w(t)$. Utilizing $d(t)$ and $w(t)$, we obtain the intra-daily time feature embedding $T^{d(t)}_t$ and the weekly time feature embedding $T^{w(t)}_t$ corresponding to the specific time point. Ultimately, these $T^{d(t)}_t \\in \\mathbb{R}^p$ and $T^{w(t)}_t \\in \\mathbb{R}^p$ are integrated to yield a combined time embedding, which can be expressed as follows:\n$T_t = T^{d(t)}_t \\oplus T^{w(t)}_t$\n(2)\nwhere $\\oplus$ denotes the hadamard product.\nThe memory module incorporates a learnable memory matrix $P = [P_1, P_2, ..., P_M] \\in \\mathbb{R}^{M \\times p}$, where symbolizes a unique traffic pattern. To dynamically adjust the memory matrix, thereby avoiding pattern singularization and adapting to the prevailing traffic conditions at time $t$, we integrate the current time embedding $T_t$ with $P$. This fusion can be represented as:\n$P_t = P \\oplus T_t$\n(3)\nwhere $P_t \\in \\mathbb{R}^{M \\times p}$ represents the memory network module at time $t$. Through training, $P_t$ can learn the most representative traffic patterns at time $t$. By integrating the time embedding $T_t$ dynamically, the model can adjust its memory $P_t$ to better capture evolving traffic patterns and conditions over time.\nAs shown in Figure 5, we extract dynamic signals from the traffic condition, which can be represented as:\n$F_i = MLP(x_i)$\n(4)\nwhere $F_i \\in \\mathbb{R}^p$ represents the dynamic signal extracted from the traffic condition $x_i$ at node $i$. It is used to query the memory matrix for the traffic pattern most similar to $x_i$.\nAfterwards, the similarity weight between $F_i$ and the memory matrix $P_t$ is computed through a similarity calculation:\n$w_i = softmax(F_i P_t^T)$\n(5)\nwhere $w_i \\in \\mathbb{R}^M$ represents the similarity weight vector."}, {"title": "V. EXPERIMENTAL SETUP", "content": "In this section, experiments are conducted on ten real-world datasets to validate the effectiveness of the proposed PM-DMNet. The datasets used are categorized into four types: bike demand datasets include NYC-Bike14 [4], NYC-Bike15 [33], and NYC-Bike16 [22]; taxi demand datasets include NYC-Taxi15 [33] and NYC-Taxi16 [22]; traffic flow datasets include PEMSD4 [34], PEMSD7 [20], and PEMSD8 [34]; and traffic speed datasets include PEMSD7(M) and PEMSD7(L) [9]. Detailed information about the datasets and the training set divisions can be found in Table II. Moreover, Unlike traffic flow and traffic speed datasets, the traffic demand datasets have two dimensions: 'Pick-up' and 'Drop-off'. We set $n = 12$ historical time steps to predict $m = 12$ future time steps.\nAll experiments are conducted on a server equipped with an NVIDIA GeForce GTX 4090 GPU. The Adam optimizer is used for model optimization, and the Mean Absolute Error (MAE) is adopted as the loss function. The hyper-parameter settings for the model under the two different prediction methods, such as the temporal embedding dimension $p$, node embedding dimension $d$, memory network matrix dimension $M$, batch size, and learning rate, are detailed in Table III. During training, an early stopping strategy is employed to terminate training and prevent over-fitting. Additionally, a scheduled sampling strategy [35] is applied to PM-DMNet(R) to enhance its robustness."}, {"title": "VI. EXPERIMENTS", "content": "Table IV presents the results of our model and baselines across different datasets. Clearly, optimal results are achieved by our model across all five datasets. XGBoost, being a machine learning model, fails to capture the nonlinear relationships within traffic condition, resulting in its inferior performance. DCRNN, STGCN, and STG2Seq utilize predefined graph structures to capture spatio-temporal correlations within traffic condition, yielding satisfactory outcomes. However, due to the fixed weights in these predefined graph structures, the inability to capture dynamic correlations leaves significant room for improvement. MTGNN and GTS demonstrate commendable progress by learning graph structures adaptively from the data. Nevertheless, these adaptive graphs remain static and fail to capture the dynamic relationships between nodes. MegaCRN employs a meta-graph learner to construct dynamic graphs for extracting correlations between nodes. However, it does not consider the influence of temporal information on traffic patterns, which limits its performance. PM-DMNet excels by leveraging a dynamic memory network to dynamically extract features by identifying the most analogous traffic patterns based on historical data. Figure 7 illustrates the prediction errors of PM-DMNet compared to three baseline models across different prediction horizons. It is observed that, except for the initial three prediction steps, PM-DMNet consistently achieves lower prediction errors than the baseline models. Additionally, the error growth rate of PM-DMNet across all time horizons is slower than that of the baseline models. Benefiting from the functionality of the evolving graph, ESG achieves comparable short-term prediction performance to PM-DMNet. However, as the prediction horizon expands, the error growth rate of ESG becomes significantly faster than that of PM-DMNet, resulting in an overall performance inferior to PM-DMNet. By leveraging temporal information corresponding to the prediction targets, PM-DMNet substantially reduces prediction uncertainty, thereby enhancing performance.\nTable V presents the results of our model and baseline models on traffic flow/speed datasets. It is observed that, except for PEMSD8 where STWave slightly outperforms PM-DMNet (P) and is comparable to PM-DMNet (R), our model achieves the best performance across all datasets. Figure 8 shows the prediction errors of PM-DMNet and the two other best baseline models at different prediction horizons. From Figure 8, it is evident that the error gaps between models are more pronounced in the flow datasets compared to the speed datasets, indicating that predicting traffic speed is more challenging than predicting traffic flow. STWave utilizes the DWT algorithm to decompose traffic data into two separate low-frequency and high-frequency sequences, modeling them independently while considering the impact of temporal information, resulting in good performance on traffic flow datasets. However, on speed datasets, due to the inherent differences between traffic speed and traffic flow, the DWT algorithm struggles to decompose useful high and low-frequency sequences, causing STWave's performance to be on par with MegaCRN. PM-DMNet does not rely on sequence decomposition for modeling, thus avoiding the difficulties associated with ineffective decomposition, leading to excellent performance on both flow and speed datasets."}, {"title": "VII. ABLATION STUDY", "content": "In this section, ablation experiments are conducted on the key components of PM-DMNet to validate their effectiveness. To investigate the impact of different modules, the following variants are designed:\nW/O Decoder: This variant removes the decoder component and predicts using an MLP layer directly applied to the encoder's output. Since the decoding process is omitted, this variant is identical for both PM-DMNet(P) and PM-DMNet(R).\nW/O TAM: In this variant, the Transfer Attention Module (TAM) is excluded. Instead, the prediction is made using the output $H_n$ from the encoder, replacing the output $H_{n+1}$ from the transfer attention mechanism.\nW/O DMN: This variant substitutes the Dynamic Memory Network (DMN) module with an MLP layer for making predictions.\nW/O NAPL: This variant removes the Node Adaptive Parameter learning (NAPL) module and uses a linear layer instead."}, {"title": "VIII. HYPER-PARAMETER ANALYSIS", "content": "To validate the impact of hyperparameters on model performance, hyperparameter experiments are conducted on the PEMSD8 dataset. Specifically, in this section, we investigate the effects of the temporal embedding dimension $p$, the dimension $d$ of the node embedding matrix $E$ in the node adaptive module, and the dimension $M$ of the memory network matrix. In these experiments, other parameters are kept constant while only the parameter under study is changed."}, {"title": "IX. VISUALIZATION", "content": "To explore whether the Node Adaptive Parameter module captures the unique traffic patterns of each node, we utilize T-SNE [45] to visualize the node embedding matrix $E$ used in the module trained on NYC-Taxi16 dataset.\nFigure 13 illustrates the visualization results of the node embeddings $E$. From the figure, it can be observed that certain nodes exhibit a clustering phenomenon, while a few nodes overlap, indicating high similarity in traffic patterns among them. Moreover, there are nodes that are far apart, suggesting significant differences in their traffic patterns.\nTo further verify the high similarity in traffic patterns among nearby nodes and the differences in traffic patterns among distant nodes, we select adjacent nodes within the red-bordered area, specifically Node 215 and Node 222, as well as a distant node within the blue-bordered area, Node 26, for visualization of their traffic demand data. Figures 14(a) and 14(b) respectively illustrate the trend changes in the 'Pick-up' and 'Drop-off' features of the traffic demand for these three nodes. It is evident that the trends for Node 215 and Node 222 are highly similar, indicating a strong correlation between them. Meanwhile, the trend for Node 26 is notably different from the other two nodes, suggesting a significant difference in their traffic patterns. The visualization results above confirm that the Node Adaptive Parameter module can learn the traffic patterns of individual nodes effectively."}, {"title": "X. CONCLUSION", "content": "This paper proposes a novel traffic prediction model, PM-DMNet. PM-DMNet employs a new dynamic memory network module that learns the most representative traffic patterns into a memory network matrix. During prediction, the model extracts pattern features by matching the current traffic pattern with the memory network matrix. Additionally, PM-DMNet supports both parallel and sequential Multi-step prediction methods to meet different needs. To further enhance the accuracy of parallel Multi-step prediction, a transfer attention mechanism is introduced to mitigate the disparity between historical data and prediction targets. Extensive experiments validate the effectiveness of PM-DMNet. In future work, further methods for extracting features from patterns are planned to be explored."}]}