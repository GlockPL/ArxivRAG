{"title": "ENHANCING TRUST AND SAFETY IN DIGITAL PAYMENTS: AN LLM-POWERED APPROACH", "authors": ["Devendra Dahiphale", "Naveen Madiraju", "Justin Lin", "Rutvik Karve", "Monu Agrawal", "Anant Modwal", "Ramanan Balakrishnan", "Shanay Shah", "Govind Kaushal", "Priya Mandawat", "Prakash Hariramani", "Arif Merchant"], "abstract": "Digital payment systems have revolutionized financial transactions, offering unparalleled convenience and accessibility to users worldwide. However, the increasing popularity of these platforms has also attracted malicious actors seeking to exploit their vulnerabilities for financial gain. To address this challenge, robust and adaptable scam detection mechanisms are crucial for maintaining the trust and safety of digital payment ecosystems. This paper presents a comprehensive approach to scam detection, focusing on the Unified Payments Interface (UPI) in India, Google Pay (GPay) as a specific use case. The approach leverages Large Language Models (LLMs) to enhance scam classification accuracy and designs a digital assistant to aid human reviewers in identifying and mitigating fraudulent activities. The results demonstrate the potential of LLMs in augmenting existing machine learning models and improving the efficiency, accuracy, quality, and consistency of scam reviews, ultimately contributing to a safer and more secure digital payment landscape. Our evaluation of the Gemini Ultra model on curated transaction data showed a 93.33% accuracy in scam classification. Furthermore, the model demonstrated 89% accuracy in generating reasoning for these classifications. A promising fact, the model identified 32% new accurate reasons for suspected scams that human reviewers had not included in the review notes.", "sections": [{"title": "I. INTRODUCTION", "content": "Digital payment systems have become an integral part of modern life, facilitating seamless and secure transactions across various platforms. The convenience and accessibility of these systems have democratized access to financial ser- vices and driven a surge in digital transactions. However, the widespread adoption of digital payments has also attracted scammers and fraudulent actors seeking to exploit their vul- nerabilities for financial gain. The financial losses incurred due to these fraudulent activities can be substantial, not only for individual users but also for the payment platforms themselves, with some estimates suggesting that annual losses due to scams in India alone could reach billions of dollars [7], [11]. To maintain the trust and safety of digital payment ecosys- tems, robust and adaptable scam detection mechanisms are imperative. Traditional machine learning (ML) [10] models, while effective to some extent, often struggle to keep pace with the evolving tactics of scammers. These models typically rely on structured data and predefined rules, which may not be sufficient to capture the subtle nuances and contextual cues that are often present in scam transactions. Moreover, the sheer volume of transactions processed by digital payment platforms can overwhelm traditional ML models, leading to delays in detection and potential financial losses. In addition to ML models that can automatically detect and block scam where there is a high degree of confidence in detection, many transactions also require a \"human review\" where the signals are less clear / more ambiguous. As such, a team of human reviewers trained on scam detection acts as a fallback mechanism to detect and prevent scam that is not easily caught by ML models. Therefore, both scam classification and the reasoning be- hind those classifications offer promising use cases for Large Language Models (LLMs) [14]. LLMs can act as classifiers to directly detect and block scams. Additionally, when trained on expert knowledge, they can serve as digital assistants for human reviewers, empowering them to make faster, consis- tent, more informed decisions by identifying key indicators, interpreting them accurately, and predicting potential outcomes (scam or not scam). The integration of LLMs into the scam detection workflow is motivated by their ability to process and interpret textual data, which is often crucial in identifying fraudulent activities. LLMs can analyze transaction descriptions, user messages, and other textual cues to discern patterns that may not be readily apparent to traditional ML models. Furthermore, LLMs can be trained on large datasets of labeled scam and non-scam transactions, enabling them to learn the subtle differences between legitimate and fraudulent activities. Despite the po- tential of LLMs, their application in scam detection within digital payment systems, particularly in the context of Unified Payments Interface (UPI) transactions in India, remains largely unexplored. In the context of UPI [1] transactions in India, the problem of scam detection is particularly acute due to the sheer volume and diversity of transactions. In India, UPI transactions account for $1.7 trillion annually, and GPay processes about a third of all UPI transactions [11]. Google Pay (GPay), as a leading UPI-enabled payment platform, processes a massive"}, {"title": "II. LITERATURE SURVEY", "content": "The application of Machine Learning (ML) [3] in financial fraud detection has been a topic of growing interest in recent years. Numerous studies have explored various ML algorithms, such as decision trees [5], support vector machines [18], and neural networks [16], for detecting fraudulent transactions in credit cards [6], online payments [13], and insurance claims. Studies have shown the effectiveness of LLMs in various security contexts, including credit card fraud detection anomaly detection in financial data [3], [9], and ensuring data protection in e-commerce transactions [2]. These models have demonstrated proficiency in analyzing large datasets, identifying patterns indicative of fraudulent activities, and responding to potential threats in real-time. In addition to traditional ML approaches, researchers have explored the potential of deep learning for fraud detection [6], [21]. Deep learning models, such as autoencoders and recur- rent neural networks (RNNs) [6], [17] have shown promis- ing results in capturing complex patterns and anomalies in financial data. Some studies have also investigated the use of LLMs for scam detection in specific contexts, such as email phishing scams [15]. However, their application in the context of digital payment systems, particularly UPI transactions in India, remains a nascent area of research."}, {"title": "III. VISION AND OBJECTIVE", "content": "In this paper we develop a comprehensive and adaptable scam detection solution using LLMs for digital payment systems, with a specific focus on UPI transactions in India, Google Pay (GPay) as a primary use case. We aim to achieve the following objectives:\nLLM Classifier (Enhance Scam Classification Accu- racy): Leverage LLMs to improve the accuracy of scam classification by analyzing tabular data (both numerical and textual) and identifying subtle patterns indicative of fraudulent activities. Reduce the reviewing burden on human reviewers by automating the identification of potential scams. To evaluate the usefulness of LLMs in classifying scam transactions we focus on assessing the performance of Gemini [19], an LLM, in accurately identifying scam transactions. Using different prompting techniques and fine-tuning on GPay India data, the goal is to achieve a specified level of precision, recall, and confidence in scam detection to demonstrate the significant potential of LLMs for scam detection.\nLLM Digital Assistant with Reasoning: Alternatively, we refer Digital Assistant by Reasoning Engine through- out the paper. Create a digital assistant that can assist human reviewers by providing reasoning and explanations for scam classifications, thereby improving the efficiency and accuracy of manual reviews. The Digital Assistant aims to streamline the review process by highlighting key features of interest to scam reviewers quickly and provid- ing insights into the rationale behind scam classifications. To achieve this, we design and build an LLM-based Reasoning Engine and Digital Assistant (that also has classification capabilities) that can reason about trans- action features to detect potential scams from provided guidelines. The Digital Assistant provides human inter- pretable explanations for scam or non-scam classifica- tions, assisting human reviewers in making informed decisions and evaluating its accuracy. The digital assistant synthesizes signals and features from transactions and accounts, presenting them in an easily understandable format.\nGeneralizable Solution for scam detection: Propose a scam detection approach with reasoning that is not only effective for UPI and payments transactions but can also be generalized and applied to other digital payment"}, {"title": "IV. METHODOLOGY & USE CASE", "content": "In this section we detail the implementation of Large Lan- guage Models (LLMs) for the purpose of scam classification. The approach involves two key components: the development of an LLM Classifier and a Reasoning Engine. We present an LLM classifier and a reasoning engine using Google's Gemini to improve scam detection in GPay India and augment current ML models that are used for scam detection. Currently, GPay transactions undergo various checks to distinguish between legitimate and fraudulent activities and existing models are able to detect some scams with very high confidence and others with a lower but still significant confidence. Scams that are detected with high confidence are Auto-Denied while others get sent for manual review. Our work improves the latter transaction classification by augmenting current classi- fiers with LLM models that provide extra reasoning for each verdict.\nFigure 1 elucidates the input data utilized in the study. The LLM classifier was trained on datasets containing up to 4 million transactions, while the reasoning engine was run using a curated set of approximately 40 transactions. The experiments involved varying the number of transactions used for training to assess its impact on model performance. The features encompass information about the order itself (such as amount, type, memo, and order text), payer & payee behavior (including transaction history and payment methods), and external factors like spam reports. We applied the following techniques to prepare our dataset:\nFeature Selection & Anonymization: We carefully selected a subset of features (top 20) encompassing transaction details, payer and payee information, and their relationships. To pro- tect user privacy, all data was anonymized prior.\nData Transformation: The numerical features were converted into a text-friendly format that the LLM can interpret. To bridge the gap between numerical data and the text-based nature of LLMs, we applied several transformations:\nSerialization: We transformed numerical values such as transaction amounts into descriptive categories (e.g., \"very low,\" \"low,\" \"medium,\" \"high\" \"very high\") based on quantiles calculated from the training data [20] [4].\nMissing Value Handling: We addressed missing values by replacing them with a placeholder like \"unknown\" due to its simplicity and interpretability by the LLM.\nData Splitting & Balancing: To ensure robust model evaluation and mitigate potential bias from class imbal- ance, we partitioned the data into training, validation, and test sets using a stratified split strategy. This ensured a balanced representation of both scam and non-scam transactions across all sets."}, {"title": "B. Models Used", "content": "Our work utilized various Gemini models, leveraging their unique strengths:\nGemini Flash: Employed initially for efficient scam classifi- cation.\nGemini Ultra: Used for advanced scam classification incor- porating reasoning capabilities.\nGemini Pro and Nano: Explored for learning and develop- ment purposes.\nOur primary findings are presented in Section uses Gemini Flash's and Ultra's earlier versions.\nOur objective is to demonstrate the broader strategic advan- tages and future possibilities of LLMs in combating fraud and improving security within the digital payment landscape and also in general trust & safety domain."}, {"title": "C. LLM Classifier", "content": "The LLM Classifier, on top of feature engineering, data cleaning and preprocessing, employs few-shot prompting and fine-tuning. For both of these strategies prompts are used to train the model. The primary goal of the classifier prompts are to guide the LLM in determining whether a given UPI transaction is fraudulent or legitimate. A classifier prompt is a set of instructions that guide an LLM to classify a given input. To achieve this, the prompt includes:\nContext: Provides background information about GPay, the UPI system, and common scam types. This helps the LLM understand the specific domain it's working in;\nFeature Descriptions: Explains the different features present in the transaction data, such as transaction amount, merchant type, order type, etc. This ensures the LLM knows what information is available to it;\nInstructions: Clearly states the task: classify the trans- action as \"scam\" or \"not scam\" and asks for a brief explanation. This sets the expectation for the LLM's output;\nExamples: Some prompts may include a few examples of labeled transactions and their explanations. This can help the LLM learn the patterns to look for.\nThese classifier prompts were then used as follows:\nFew-shot prompting: Involves providing the LLM with a lim- ited number of examples of scam and non-scam transactions, along with explanations for the classifications. This approach leverages the LLM's ability to learn from a few examples and generalize to new, unseen instances.\nFine-tuning: Involves training the model on a larger labeled dataset to optimize its performance on the specific task of scam detection. Prompt design is critical also for fine-tuning, with prompts carefully crafted to provide context and instructions. The model is trained to classify transactions as \"scam\" or \"not scam\". Prompts include contextual information about GPay, common scam types, and features used for classification. In addition to prompting and fine-tuning, model variations and experiments were also employed to train the LLM classi- fier. During Phase 1, various experiments were conducted to"}, {"title": "D. Reasoning Engine and Digital Assistant", "content": "Building on the LLM classifier from Phase 1, Phase 2 fo- cused on developing a digital assistant to aid human reviewers in the scam review process. This assistant provides the top N reasons why a transaction could be a scam and the top N reasons why it might be legitimate. Along with these reasons \"for\" and \"against,\u201d the LLM provides a verdict on whether the transaction is fraudulent, improving the accuracy, efficiency, and consistency of manual reviews. Central to this digital assistant is the reasoning engine. This engine generates human-interpretable explanations for scam classifications by analyzing transaction data and identifying the most salient features contributing to the LLM's verdict. It then synthesizes these features into a coherent explanation, highlighting the specific aspects of the transaction that raised suspicion. To achieve this, the reasoning engine was trained on a curated dataset of scam and non-scam transactions, including detailed reviewer notes explaining the rationale behind each classification. This training allows the engine to learn intricate patterns and features associated with different types of scams and generate explanations aligned with human reviewers' reasoning processes. The explanations are designed for easy comprehension by human reviewers, ensuring they can quickly grasp the rationale behind a scam classification and make informed decisions. These explanations are not mere restatements of the data but rather a synthesis of the most relevant information, presented clearly and concisely. The reasoning engine employs a structured approach mirroring a human reviewer's thought process. It identifies key features indicative of a scam, such as unusual transaction patterns, suspicious merchant behavior, or anomalies in user profiles. The engine then analyzes these features' values, comparing them to established norms and thresholds, and synthesizes this information into a comprehen- sive explanation highlighting why a transaction was flagged as suspicious. To build the Reasoning Engine, the LLM is prompted on a curated dataset of transactions, accompanied by detailed reviewer notes that explain the reasoning behind each clas-"}, {"title": "V. EVALUATIONS AND RESULTS", "content": "A. LLM as a Classifier Results\nThe LLM classifier's performance is evaluated using a held- out test set of labeled UPI transactions. The evaluation metrics included precision, recall, F1 score, and area under the re- ceiver operating characteristic curve (AUC-ROC). The results (as shown in Figure 5) demonstrated the LLM classifier's effectiveness in identifying scam transactions, achieving a high recall rate while maintaining reasonable precision. The model's ability to analyze textual data, such as transaction descriptions, proved to be a significant advantage, enabling it to capture subtle patterns indicative of fraudulent activities. In addition to overall performance, the LLM's performance was assessed on specific transaction segments, such as external merchant transactions, high-value transactions, and transac- tions initiated via applications. The LLM exhibited varying performance across these segments, highlighting the need for further refinement and customization to address the unique characteristics of different transaction types.\nLearning & Discussions: Table I summarizes the out- comes of various experiments conducted to fine-tune the LLM for scam detection. The experiments explored the impact of different factors, such as the inclusion of numeric data, the time period of the training data, the size of the LLM model, the presence of textual context, the volume of training data, and the assessment of model convergence. The table highlights key findings and conclusions drawn from each experiment, offering"}, {"title": "B. LLM as a Reasoning Engine Results", "content": "The reasoning engine's performance was evaluated through a series of user studies involving experienced scam reviewers. The reviewers were presented with a set of scam and non- scam transactions, along with the LLM's classifications and explanations generated by the reasoning engine. The reviewers were asked to assess the accuracy, relevance, and helpfulness of the explanations.\nReasoning Accuracy: A key aspect of the evaluation fo- cused on Reasoning Engine's ability to generate accurate reasoning for scam classifications. By comparing Rea- soning Engine's outputs to a curated set of high-quality human reviewer notes, we found that Reasoning Engine achieved an 89% accuracy rate in providing reasoning that either matched human reviewers or offered new, valid insights. This high accuracy rate suggests that Reasoning Engine can effectively replicate and even enhance human decision-making processes in scam detection.\nCorrect Reasoning (C): In 57% of the cases, Reason- ing Engine's reasoning aligned perfectly with that of experienced human reviewers, demonstrating its ability to learn and apply established scam detection patterns.\nIncorrect Reasoning (I): A small percentage (6%) of Reasoning Engine's reasoning was deemed incorrect compared to human reviewers, indicating areas for further refinement and learning.\nHallucination (H): Less than 1% of Reasoning Engine's responses were irrelevant or nonsensical, highlighting the need for ongoing monitoring and improvement of the model's output generation.\nMissed (M): In 4% of cases, Reasoning Engine failed to identify a reason that human reviewers had detected, suggesting potential gaps in the model's understanding or application of certain scam indicators.\nNew (N): Notably, Reasoning Engine generated new reasoning in 32% of cases that human reviewers had not added in reviewers notes, showcasing its poten- tial to uncover novel patterns and insights that could enhance scam detection efforts in case human review- ers missed those. The additional 32% of reasons did not improve the accuracy of classification within the chosen accounts. However, they did reduce the time reviewers spent on reviews.\nReasoning Quality: To assess the quality of Reasoning Engine's reasoning, experienced Trust and Safety Pro- gram Managers (PgMs) evaluated its outputs. The results were promising, with 38% of reasons rated as \"Excel- lent\" and 41% as \"Acceptable.\" This positive feedback indicates that Reasoning Engine's reasoning is not only accurate but also well-structured, coherent, and valuable for human reviewers, totaling 79% positive feedback.\nAdditional Considerations: The evaluation also consid- ered factors such as the limited number of data fields used (12 out of all possible fields) and the manual nature of the evaluation process. While these factors may have in- fluenced the results, the overall findings strongly suggest that Reasoning Engine has the potential to significantly improve the efficiency and accuracy of scam reviews.\nExtensibility and Scalability: Our experiments in other projects show that the approach used to develop digital assistant is easily extensible to other areas and data fields.\nVerdict Accuracy: While not the primary focus of this evaluation, Reasoning Engine showed approximately 93.33% accuracy in determining scam verdicts, although this result is based on a small sample size."}, {"title": "VI. GENERALIZATION TO OTHER TRANSACTION PLATFORMS", "content": "This research focused on GPay as a use case, but the devel- oped approach is inherently generalizable to other UPI-enabled payment platforms and even beyond the payments domain. The LLM-based classifier and the reasoning engine can be adapted to different platforms by training them on platform- specific data and incorporating relevant features. The flexibility of LLMs allows for the customization of prompts and training data to cater to the specific nuances and characteristics of each platform. For instance, the LLM-based classifier and reasoning engine could be adapted to identify and prevent online harassment, hate speech, misinformation, and other forms of harmful content. The ability of LLMs to understand and interpret natural language makes them a powerful tool for addressing a wide range of Trust & Safety challenges in the digital age."}, {"title": "VII. LIMITATIONS", "content": "While the LLM-based approach presented in this paper shows promising results in scam detection, it is important to acknowledge its limitations. Firstly, the model may need be constantly updated with latest types of scams or scammer's evolving tactics, it is important to see how does this model hold over time. Secondly, the interpretability of the LLM's decisions can be a challenge, as the model's reasoning may not always be transparent or easily understandable by human reviewers. Thirdly, the model's performance may vary across different transaction types and platforms, requiring careful customization and fine-tuning for optimal results. Finally, the computational cost associated with LLM inference can be a significant factor, especially when dealing with large volumes of transactions. Future research should focus on addressing these limitations by exploring ways to enhance the model's interpretability, improving its performance on diverse transaction types, and optimizing its computational efficiency."}, {"title": "VIII. REAL-WORLD IMPACT AND FUTURE WORK", "content": "Fraud & scam has a real economic and human cost. Con- sumers in the US lost 10 billion dollars to fraud in 2023 [7], [11] (with investment scams being the leading category), and in India (where this research got its start with GPay), digital payments scam India grew five-fold in 2024 [7] from one year ago. The successful deployment of LLM-based scam detection systems has the potential to significantly reduce financial losses due to scams in digital payment systems. By automating the identification of suspicious transactions and providing human reviewers with actionable insights, the system can enhance the efficiency and accuracy of scam detection and reviews, leading to faster, more accurate, and more effective mitigation of fraudulent activities. On the technical front, future work can be undertaken in several directions, three of which are on-device models, distillation, and RLHF. First, future models will likely be small enough to deploy directly on a payment device such as a mobile phone, giving relatively quick response. Second, distillation can help reduce the feature space and model size to have similar effectiveness with a much smaller and condensed parameter space, thus driving efficiency. Third, RLHF mechanisms can be built into the digital assistant so that if it gets an evaluation wrong, expert reviewers can train it to do better (a 'thumbs up' / 'thumbs down' followed by a feedback field with explanation on how to evaluate a particular case). Beyond payments, scam detection as described in our work can be extended to several other domains. E-commerce, B2B payments, 3rd party merchant transactions, invoicing, tax scams, and other such areas also get targeted by scammers quite frequently. So long as model features and prompts are appropriately designed, guidelines (based on known modus operandi) are fed to the LLM, and sufficient controls are established to ensure validity of results through automated or human evaluations (evals), the approach is extensible to a vast array of digital scam prevention. Many LLM models are inherently internationalized because of in-built support for many languages. Because most LLMs now support international languages, this also presents an opportunity to extend a set of learnings from one geography to others with simple prompting. Moreover, simple tweaks can be made to prompts based on local context (in a human readable, interpretable form) that can help extend a base model to a localized \"expert\" scam detector. Additionally, it is conceivable to extend this approach beyond digital and text-based scam to other modalities. For example, a phone call can be transcribed and run through a scam detector (in real-time). Our 'classifier and assistant' approach, based on human-provided guidelines and up-to- date refinements is generally extensible. It can deal relatively easily with the latest type of scam actions and modalities with simple, centralized prompt updates, reducing or preventing the delay, cost, and effort involved in disseminating new guide-"}, {"title": "IX. CONCLUSION", "content": "In conclusion, this paper demonstrates the significant po- tential of Large Language Models (LLMs) in revolutionizing scam detection within digital payment systems. By harnessing the power of LLMs to analyze textual data and discern intricate patterns, we have developed a robust and adaptable scam detection solution that can be applied to various platforms. Our findings reveal that LLMs, when fine-tuned on labeled datasets and guided by carefully crafted prompts, can achieve high recall rates in identifying scam transactions while main- taining reasonable precision. This is particularly noteworthy in high-value segments and transactions with descriptive text, where traditional ML models often struggle. Moreover, the development of the digital assistant, powered by an LLM- based reasoning engine, has proven to be a valuable tool for human reviewers, enhancing their efficiency and accuracy in the scam review process. While this work primarily focused on GPay, the methodol- ogy and framework presented here are inherently generalizable to other UPI-enabled platforms and even extendable to other domains within Trust & Safety. The flexibility of LLMs allows for seamless adaptation to different platforms and use cases, making them a powerful tool for addressing a wide range of challenges in the digital age. However, it is important to acknowledge the limitations of the LLM-based approach detailed in the section VII. Future research should focus on addressing these limitations by exploring ways to enhance the model's interpretability, improving its performance on diverse transaction types, and optimizing its computational efficiency. Despite these challenges, the potential of LLMs to trans- form the financial fraud detection landscape is undeniable. By augmenting existing ML models and empowering human reviewers, LLMs can significantly contribute to creating a safer and more secure digital payment ecosystem. As research in this field progresses, we can anticipate even more sophisticated and effective LLM-based solutions that will further strengthen the fight against scams and fraud in the digital age."}, {"title": "X. FUTURE SCOPE", "content": "Bias Monitoring and Mitigation: We will continuously monitor the scam classification and reasoning system for potential biases that may arise during its operation. If any biases are identified, we will retrain the models on carefully curated datasets to counteract these biases and ensure fairness and accuracy in scam detection and reasoning.\nAccuracy and Efficiency Enhancements: We will continue to refine our scam classification and reasoning system by leveraging the latest advancements in Gemini Models."}]}