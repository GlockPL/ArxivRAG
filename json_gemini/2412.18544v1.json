{"title": "Consistency Checks for Language Model Forecasters", "authors": ["Daniel Paleka", "Abhimanyu Pallavi Sudhir", "Alejandro Alvarez", "Vineeth Bhat", "Adam Shen", "Evan Wang", "Florian Tram\u00e8r"], "abstract": "Forecasting is a task that is difficult to evaluate: the ground truth can only be known in the future. Recent work showing LLM forecasters rapidly approaching human-level performance begs the question: how can we benchmark and evaluate these forecasters instantaneously? Following the consistency check framework, we measure the performance of forecasters in terms of the consistency of their predictions on different logically-related questions. We propose a new, general consistency metric based on arbitrage: for example, if a forecasting AI illogically predicts that both the Democratic and Republican parties have 60% probability of winning the 2024 US presidential election, an arbitrageur can trade against the forecaster's predictions and make a profit. We build an automated evaluation system that generates a set of base questions, instantiates consistency checks from these questions, elicits the predictions of the forecaster, and measures the consistency of the predictions. We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate with LLM forecasters' ground truth Brier scores (which are only known in the future). We also release a consistency benchmark that resolves in 2028, providing a long-term evaluation tool for forecasting.", "sections": [{"title": "1 INTRODUCTION", "content": "Prediction markets are markets that pay out contingent on an event. For a market such as \"$1 if Jeb Bush is elected President in 2028\", the price reflects the \"market estimate\" for the probability of that event. Prediction markets are a promising tool for aggregating information from disparate sources to arrive at the most correct possible belief after taking into account all relevant information.\nUntil 2024, LLM forecasters generally performed poorly relative to human forecasters. However, recent works suggest that LLM-based forecasters can rival human forecasts on forecasting websites such as Metaculus, PredictIt, and Manifold Markets.\nA key question emerges: once LLM forecasters are better than human ones, how can we efficiently evaluate their predictions? In particular, long-term forecasting questions are very important for decision-making, and finding ground truth for evaluation in such contexts is infeasible by virtue of the questions resolving far in the future.\nOne approach, proposed by Fluri et al. (2023), is that even when we cannot evaluate the correctness of LLM decisions, we can evaluate their logical consistency. For example, if an LLM forecaster gives probabilities 0.5 and 0.7 to \"Will Trump be elected US president?\" and \"Will someone other than Trump be elected US president?\", this is necessarily inconsistent. Fluri et al. (2023) demonstrated that GPT-4 and GPT-3.5-turbo, when asked one-sentence forecasting questions, were inconsistent on simple logical checks such as negation.\nOur contributions in this work are as follows:"}, {"title": "2 A THEORETICAL FRAMEWORK FOR FORECASTING CONSISTENCY", "content": "Notation. Let Prop denote the set of forecasting questions we are interested in, denote the set of possible outcomes/resolutions for an individual questions. In this paper, we focus on Prop as a set of binary forecasting questions, so = {T,1}. A Forecaster is then a map IF: Prop\u2192 [0, 1]. One special forecaster is the ground truth resolutions \u03b8 : Prop \u2192 \u0398, returning 1 and 0 probability for {T, 1}, respectively.\nFor conditional questions that can resolve to None, we also have optional resolutions \u0398' := \u222a {None} = {T, 1, None}. We focus on binary questions following Halawi et al. (2024). Our methods can in principle be extended to study consistency between general probability distributions in forecasting, such as the ones discussed in Gooen (2024).\n2.1 CONSISTENCY CHECKS AND INCONSISTENCY METRICS\nIn line with Fluri et al. (2023), a consistency check is conceptualized as a pair of n-ary relations: R : Prop\" \u2192 {T,1} in question space, S: [0,1]\u2033 \u2192 {T,1} in forecast space, and a predicate for IF such that R(x1,...xn) \u21d2 S(IF(x1),... F(xn)). In particular, this assertion must be satisfied by all feasible 0, and also any \"correct\" forecasts generated by a world model that accurately accounts for aleatoric uncertainty. Violation of consistency is measured by some violation metric V : [0,1]n \u2192 R which must satisfy V(IF(x1),... F(xn)) = 0 \u21d4 S(F(x1),... IF(xn)). For example, intuitively, the \"negation\" check NEGATION is given by the relation R(x1, x2) := x1 = \u00abx2 on questions, and the relation S(IF(x1), F(x2)) := IF(x1) + IF(x2) \u2248 1 on forecasts. The full table of the consistency checks we use is given in Appendix B."}, {"title": "2.1.1 ARBITRAGE METRIC", "content": "The arbitrage metric is conceptualized as the minimum profit that an arbitrageur can be guaranteed making bets against the forecaster's predictions. More precisely: suppose that the forecaster's probabilities IF(x1),... F(xn) were prices offered by a logarithmic market maker 3 with market subsidy parameter $1. If these probabilities are inconsistent, then there are prices p1,... pn that an arbitrageur could bring to the market such that it is guaranteed to make a profit against the market-maker, no matter the outcome of each question. We define V(IF(x1), ... IF(xn)) as the maximum achievable \u201cminimum profit\" that the arbitrageur can guarantee by choosing appropriate p1,... Pn. We further denote by A(IF(x1), ... IF(xn)) the set of prices P1,... Pn that maximize the minimum profit:\n$$(\\arg \\max, \\max) \\min _{\\Theta \\in \\Omega} \\sum_{i=1}^{n} (\\log p_i - \\log \\mathbb{F}(x_i)) \\delta_{\\omega(i)=+} + (\\log (1 - p_i) \u2013 \\log (1 \u2013 \\mathbb{F}(x_i))) \\delta_{\\omega(i)=1}$$ \nHere \u03a9 := {\u03c9 \u2208 \u0398'm | R(w)} is the set of all possible consistent resolutions of this tuple. A more general version of 1 is given in Appendix C, along with specific worked-out examples of the arbitrage metric for each consistency check, and details on how we compute it; as an example, the arbitrage metric for the Negation Check can be derived exactly (Appendix C.2):\nV(IF(x), F(x)) = -2log (\u221aF(x)(1 \u2212 F(\u00acx)) + \u221a(1 \u2212 F(x))F(-x))\nTo illustrate: V(0.5,0.6) \u2248 0.01, V(0.5,0.51) \u2248 10-4. The metric is more sensitive to violations for probabilities very close to 0 or 1, due to the logarithmic market maker. In our evals, for all types of checks, we say that a sampled check does not pass if V \u2265 0.01. We have to pick some hyperparameter as an inconsistency threshold; we set it to correspond to giving 110% probability in total to the events of Republican and Democratic parties winning the US presidential election."}, {"title": "2.1.2 FREQUENTIST METRIC", "content": "We also compute a different, frequentist consistency metric. Consider a Monte Carlo forecaster that samples a world model n times, and for any event, returns the fraction of samples in which the event occurs. The frequentist metric is the number of standard deviations a given tuple forecast is off from the mean Monte Carlo forecast, scaled to be independent of n. We say that a consistency violation happened if the number of standard deviations away from the mean of the null is at least as in the (0.5,0.6) case described in Section 2.1.1. The full description is given in Appendix D."}, {"title": "2.1.3 INTUITION ON CONSISTENCY METRICS", "content": "Our metrics address two major obstacle with measuring inconsistency: tolerance to noise and principled aggregation of inconsistency scores.\nTolerance to noise. In the standard Bayesian setting, beliefs are either consistent or not: there either is a Dutch book (a way to bet against the forecaster's beliefs to get infinite profit) or the probabilities are perfectly consistent. In practice, an forecaster's beliefs (even on the same question) are never perfectly consistent across runs. If an election model has a presidential candidate at 48% with one random seed and 50% on the other, this is not a reason to discard it as completely flawed. Hence, instead of a binary measure of consistency, our metrics increase smoothly with inconsistency.\nPrincipled aggregation and comparison of inconsistency scores. Fluri et al. (2023) developed a set of inconsistency checks, used an ad-hoc metric for each check they used, and normalized the scores to [0, 1]. There are two major issues with their approach:"}, {"title": "3 PIPELINE OVERVIEW", "content": "We illustrate the steps in our data collection pipeline below, and provide more details on each individual steps:\n\u2022 (\u2026 \u2192 P) We first prepare datasets of base questions in multiple ways:\n(a) Scraping questions from online platforms such as Manifold and Metaculus;\n(b) A ground-truth resolved dataset synthetically generated from news articles;\n(c) Synthetic generation on questions on a list of topics such as Politics, Science, Economics, etc.\nFor the first two of the above, we also include the ground truth resolution for each question. We discuss all of these in more detail in Section 3.1.\n\u2022 (P \u2192 (P,Q)) The base questions are synthetically instantiated into tuples that must satisfy certain consistency checks. For example, every single base question P is instantiated into a tuple (P,P); and pairs of mutually relevant base questions P, Q are instantiated into tuples like (P, Q, P^ Q, PVQ).\n\u2022 ((P,Q)IF\u2192 (p,q)) The forecaster is separately queried to elicit forecasts on each question, resulting in forecast tuples that should, if the forecaster is consistent, satisfy consistency properties. For example, for a size-two tuple where Q = \u00acP, it should satisfy p + q = 1.\n\u2022 ((p,q)\u2192V(p,q)) We score each tuple of forecasts for consistency with both of our violation metrics.\nExamples of data at each step of the pipeline are given in Appendix A. The prompts and LLM calls used in each step before forecasting are given in Appendix G."}, {"title": "3.1 GENERATING AND SCRAPING FORECASTING QUESTIONS", "content": "Forecasting question format. Each forecasting question includes a title that states the main question, a body that provides detailed resolution criteria, and a resolution date, along with optional fields such as metadata and creation date.\nReal prediction market questions. We scrape questions from two forecasting platforms, Metaculus and Manifold Markets, and only use questions that both resolved and were initially set to resolve between May 1, 2024, and August 15, 2024. This leaves us with over 500 questions, of which 242 pass our verification step (see end of this subsection). An example of a processed question, including its relevant details, is provided in Appendix A.1.\nGenerating forecasting questions from NewsAPI articles. To generate forecasting questions with known resolutions, we use articles sourced from NewsAPI. We focus on articles describing concrete events rather than opinion pieces. To mitigate biases towards positive resolutions (as most questions derived from an article would typically resolve to True), we employ reference class spanning - using an LLM to modify key entities in the questions while keeping the overall thematic structure intact. Each question's ground-truth resolution is verified using the Perplexity API with internet access, yielding ground truth resolution labels with less than a 5% error rate in our testing. We compile a total of 2,621 ground-truth resolved forecasting questions resolving between July 1, 2024, and August 21, 2024. Of these, we use a subset of 1,000 to test the relationship between consistency violation and accuracy. Further details regarding the pipeline can be found in Appendix J."}, {"title": "Synthetic question generation", "content": "We generate questions by few-shot prompting, we sample six examples of forecasting questions, as style examples, as well as a set of tags (Brazil, NBA...) to diversify the generated questions. We generate question titles, deduplicate them using text-embedding-3-small embeddings from OpenAI, and then for each title we use gpt-40 to create the question body and resolution date. With this method we create 1,000 forecasting questions that resolve either by or in 2028. More details are in Appendix G."}, {"title": "Verification and improvement from human feedback", "content": "In all of the above steps, we filter generated questions in using gpt-40 to check for properties such as the coherence between the body and title, the clarity and precision of the resolution criteria, and whether the question is about actual world events. Questions failing this step are discarded. To develop this step, we used a feedback form for human reviewers (authors of this paper) to suggest modifications to generated questions. These suggestions inform refinements to prompts and few-shot examples in our pipeline. An example of the feedback form is provided in Appendix H."}, {"title": "3.2 INSTANTIATING TUPLES OF QUESTIONS FOR CONSISTENCY CHECKS", "content": "The base forecasting questions are subsequently used to synthetically generate tuples of logically related questions. For example, a pair of base questions (P,Q) can be used to generate a 4-tuple (P, Q, P\u2227Q, P\u2228Q) for ANDOR, or a 3-tuple (P, \u00abP>Q, P\u2228Q) for BUT (see Appendix B for details). The main question content (titles and bodies) were generated synthetically (using gpt-40), while the resolution dates and other properties were calculated systematically (e.g. the max of the resolution dates of the base questions).\nWe then conduct two measures to ensure the instantiated tuples are correct and sensible: relevance scoring, and verification that the tuples of questions indeed describe logically related events."}, {"title": "Relevance scoring", "content": "When combining base questions into tuples, we have to take care to avoid off- distribution questions like \"Is SpaceX going to be worth $200B by 2030, given that Sri Lanka's rice production grows 40% by 2040?\". For tuples instantiated from more than one base question, we sort 2000 potential base question combinations by their \"relevance score\", obtained by querying an LLM and asking it to score how relevant the questions are to one another, and choose the top 200 for each consistency check. See Figure 17 for details."}, {"title": "Verification", "content": "The instantiated tuples of questions are then passed to another LLM call to reject if they do not fit their intended structure; for example, we detect if the resolution criteria of the second question are not truly a negation of the resolution criteria of the first question. Examples of verification prompts are given in Appendix G."}, {"title": "3.3 ELICITING FORECASTS", "content": "We test a range of forecasters based on various LLM models (gpt-40, gpt-4o-mini, claude-3.5-sonnet, llama-3.1-8B, llama-3.1-70B, llama-3.1-405B, o1-mini and ol-preview) with and without chain-of- thought prompting: see Appendix E for details. We run each of these forecasters on 5000 tuples in total (for each of the 10 checks, we use 200 tuples from scraped questions and 300 from NewsAPI questions), except for 01-preview, which we test on 50 tuples per check only due to cost constraints. We could not test forecasters from Halawi et al. (2024) due to API deprecations; see Section 7."}, {"title": "4 RESULTS", "content": "We evaluate a range of forecasters on the datasets described above, for both consistency and ground truth Brier score. We note that the Brier score as the standard metric of forecasting accuracy depends both on model capabilities and the training data cutoff: it should not be surprising for a stronger model to have a worse Brier score if its training data cutoff is earlier than for a weaker model. The full list of forecasters is in Appendix E.\nFor all data analysis in this section, we exclude forecasters that have Brier score worse than random guessing (0.25), such as the basic setup with llama-3.1-8B, as it would unfairly advantage our case of \"correlating consistency with accuracy\"."}, {"title": "5 ARBITRAGEFORECASTER: CAN WE DESIGN A MORE CONSISTENT FORECASTER?", "content": "Let (x1,...xm) be a question tuple for some consistency check R, e.g. (P, \u00acP). Given forecasts IF(x1),...F(xn), the arbitrage metric maximization problem in Equation 1 computes the following (as the argmax and max of the arbitrage respectively):\n1. Improved forecasts IF\u2032(x1), ...F\u2032(xn) which are consistent, i.e. satisfy S; and\n2. The profit earned by an arbitrageur who bets these improved forecasts against the original ones this is the actual metric.\nThis leads us to wonder: can we use these \u201cimproved consistent forecasts\u201d to build a new forecaster which builds on the base forecaster IF, but is more consistent on R?\nWe introduce: the ArbitrageForecaster with base IF arbitraged on consistency check R, denoted by (IF)R, which computes its forecast on a question x as follows:\n1. Instantiates a tuple (x1,...xm) satisfying R;\n2. Queries IF to obtain F(x1),...IF(xn);\n3. Arbitrages these base forecasts per Eq 1 and returns the arbitraged forecast for x1.\nDespite what one might assume, however, an ArbitrageForecaster is not \"definitionally\" consistent on the check it is arbitraged on, but rather its consistency must be investigated empirically. Suppose, for example, that a forecaster produces forecasts IF(P) = 0.5, IF(para(P)) = 0.6, IF(para(para(P))) = 0.7. Then F' := (IF)PARAPHRASE would produce forecasts IF'(P) \u2248 0.55, IF'(para(P)) \u2248 0.65, which are not consistent.\nAppendix F contains a precise definition of ArbitrageForecaster, including the case of sequentially arbitrag- ing on multiple checks (IF) [R1,...Rs], and a theoretical discussion of its consistency properties. In particular, we list strong theoretical reasons to expect consistency gains from recursive ArbitrageForecaster setups, i.e. (IF):= <<IFR\u00b9)R, RR, in particular with NEGATION, as well as in a non-recursive ArbitrageForecaster with EXPEVIDENCE.\nDue to these priorities and the high costs of running recursive ArbitrageForecasters (see Appendix F.1), we limited ourselves to studying only a small number of ArbitrageForecaster setups, with a limited number of checks rather than the whole list; specifically: (9), (9)p, (9)[N,P], (9)[E]*s where g :=gpt-40-mini, N, P, E are NEGATION, PARAPHRASE, EXPEVIDENCE respectively, and r and s vary from 0 to 4.\nThe full results of our experiments with these forecasters are reported in Appendix F.2; our key takeaways from these preliminary runs look hopeful:"}, {"title": "6 RELATED WORK", "content": "Metamorphic and consistency checks. Checking logical properties of outputs of programs under semantic- preserving transforms has a long history."}, {"title": "7 FUTURE WORK", "content": "We have developed a comprehensive benchmark of static consistency checks for LLM forecasters, and demonstrated its correlation with ground truth accuracy, suggesting that our consistency metrics could serve as a proxy for accuracy when we do not have access to ground truth. We envision several directions in which our framework could be extended:\nConsistency in decision-making. AI systems may be used not only to make forecasts that inform decisions, but also to take decisions directly. Here too, we can have a notion of inconsistency: for example, intransitive preferences 5 \u2013 and analogously, an inconsistent decision-maker may be exploited by an arbitrageur.\nTraining for consistency. Modulo consideration of the cost-benefit to safety, our methods could be used train LLMs for consistency, minimizing our violation metrics. This may or may not impact overall forecasting performance and other AI capabilities. One may also imagine an AlphaZero-style set-up, where an LLM Fis trained on the outputs of (IF)\u201d, i.e. a recursive ArbitrageForecaster wrapped around it.\nFurther experiments with ArbitrageForecaster. Most of our experiments with ArbitrageForecaster involved arbitraging on only a single check (apart from one experiment with both NEGATION and PARAPHRASE), due to the cost limitations described in F.1. It is easy to imagine how a bad forecaster could still overfit a single check: simply forecasting 50% probability for all questions will pass PARAPHRASE, EXPEVIDENCE and NEGATION - but we expect that being consistent under a variety of checks is difficult without a consistent world model. One approach to using more checks cheaply, particularly in training, may be to randomly sample a number of consistency checks for each question.\nDynamic generation of consistency checks. Although we found strong correlations between ground truth accuracy and consistency among existing LLM forecasters, our results with ArbitrageForecaster demonstrate that this isn't necessarily the case: it is possible to do well on consistency without improving ground truth. In particular, this means that consistency as a training metric could be \"Goodharted\" by a learning AI model. One way to prevent this may be via adversarial training: i.e. have an adversarial agent instantiate consistency checks that it believes the agent will perform poorly on.\nEvaluating RAG-augmented forecasters. We have conducted some preliminary experiments evaluating state-of-the-art forecasters such as Halawi et al. (2024). Unfortunately, we could not reproduce the system from Halawi et al. (2024) at the time of writing, due to deprecations in the Google News API (we could not obtain access to the alternative Newscatcher API). At the time of writing, we are not aware of other publicly-available LLM forecasting systems that are competitive with the results of Halawi et al. (2024) (there exist proprietary systems that may be competitive, such as FutureSearch (2024)). We thus leave the evaluation of better forecasters like Halawi et al. (2024) and Phan et al. (2024) to future work, once such forecasters are more widely available."}, {"title": "CONTRIBUTIONS", "content": "DP and APS developed consistency checks and the arbitrage and frequentist metrics. DP, AA, APS, and EW worked on the LLM question to evaluation pipeline. APS thought of and implemented ArbitrageForecaster. VB created the news-derived question dataset. AS and DP created the scraped question dataset. AA and DP created the 2028 synthetic question dataset. DP started and led the project. FT proposed correlating consistency with forecasting accuracy and advised the project. All authors helped with the writing. DP and APS wrote the first draft of the paper."}]}