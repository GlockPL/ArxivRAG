{"title": "PROTEIN STRUCTURE PREDICTION IN THE 3D HP MODEL USING DEEP REINFORCEMENT LEARNING", "authors": ["Giovanny Espitia", "Yui Tik Pang", "James C. Gumbart"], "abstract": "We address protein structure prediction in the 3D Hydrophobic-Polar lattice model through two novel deep learning architectures. For proteins under 36 residues, our hybrid reservoir-based model combines fixed random projections with trainable deep layers, achieving optimal conformations with 25% fewer training episodes. For longer sequences, we employ a long short-term memory network with multi-headed attention, matching best-known energy values. Both architectures leverage a stabilized Deep Q-Learning framework with experience replay and target networks, demonstrating consistent achievement of optimal conformations while significantly improving training efficiency compared to existing methods.", "sections": [{"title": "Introduction and Related Work", "content": "Simulating protein folding is a fundamental challenge in biophysics and computational biology, yet it is crucial for understanding protein structure, function, and dynamics, with significant implications for drug discovery and disease diagnosis. The Hydrophobic-Polar (HP) model serves as a simplified yet powerful framework for studying protein folding, classifying amino acids as either hydrophobic (H) or polar (P) on a lattice structure. Despite its apparent simplicity, finding optimal conformations in the HP model remains NP-complete, making it particularly challenging for larger proteins. Early approaches to this problem employed various computational methods, including genetic algorithms [Unger and Moult, 1993], Monte Carlo simulations with evolutionary algorithms [Patton et al., 1995], and memetic algorithms with self-adaptive local search [Krasnogor, 2010]. Additional methodologies encompassed ant colony optimization [Shmygelska and Hoos, 2005], core-directed chain growth [Beutler and Dill, 1996], and the pruned-enriched Rosenbluth method (PERM) [Grassberger, 1997]. Recent advances in deep reinforcement learning (DRL) have opened new avenues for addressing the protein folding challenge. Notable contributions include Q-learning approaches [Czibula et al., 2011], hybrid methods combining Q-learning with ant colony optimization [Do\u011fan and \u00d6lmez, 2015], and FoldingZero [Li et al., 2018], which integrates Monte Carlo tree search with convolutional neural networks. Building upon these foundations, we propose two novel architectures: (1) a reservoir computing-based hybrid architecture [Jaeger, 2007] that captures temporal dependencies in the protein folding process and (2) an LSTM network enhanced with multi-head attention layers that effectively models long-range interactions between amino acids [Bahdanau et al., 2015]. These architectural approaches have individually demonstrated success in various domains, including time series prediction [Subramoney et al., 2021], speech recognition [Araujo et al., 2020], and robot control [Antonelo and Schrauwen, 2015].Our work represents the first application of reservoir computing to the protein folding problem in the 3D HP model, while also introducing an attention-enhanced LSTM architecture specifically designed for longer protein sequences. The reservoir-based approach leverages the computational efficiency of fixed, randomly initialized recurrent neural networks to project input data into high-dimensional space, while the attention mechanism in the LSTM architecture enables effective modeling of interactions between distant amino acids. Both architectures consistently achieve optimal conformations matching the best known energy values while demonstrating improved training efficiency compared to traditional approaches. The remainder of this paper is organized as follows: Section 2"}, {"title": "Methodology", "content": "In this section, we describe the methods used to model the problem as Markov Decision Process (MDP) and introduce the details of the architectures."}, {"title": "Modeling the problem in a cubic lattice", "content": "Let G be a 3D cubic lattice with lattice points (x, y, z) \u2208 Z\u00b3. Similar to [Yang et al., 2023], the protein folding process is modeled as a self-avoiding walk (SAW) on G, where each amino acid in the sequence occupies a single lattice point, and no two amino acids can occupy the same point simultaneously. The SAW begins by placing the first two amino acids at positions ro = (0,0,0) and r\u2081 = (0,1,0) in the cubic lattice. The following constraints are imposed on the placement of subsequent amino acids:\n\u2022 Distance constraint: The distance between consecutive amino acids in the sequence must be exactly one lattice unit. Let ri = (xi, Yi, zi) and ri+1 = (Xi+1, Yi+1, Zi+1) be the positions of two consecutive amino acids. Then, the distance constraint can be expressed as:\n$||F_{i+1} - r_i|| = ((x_{i+1} - x_i)^2 + (y_{i+1} - y_i)^2$\n$+(z_{i+1} - z_i)^2)^{\\frac{1}{2}} = 1$                                                     (1)\n\u2022 Bond angle constraint: The bond angles formed by three consecutive amino acids must be restricted to 90\u00b0 or 180\u00b0 to ensure rotational invariance. Let ri, ri+1, and ri+2 be the positions of three consecutive amino acids. The bond angle constraint can be expressed as:\n$(i+1 - ri) (ri+2 - ri+1) = 0$ (for a 90\u00b0 angle)\n$(i+1 - ri) \u00d7 (ri+2 - ri+1) = 0$ (for a 180\u00b0 angle) (2)\nwhere denotes the dot product and \u00d7 denotes the cross product.\n\u2022 Self-avoidance constraint: The chain must not intersect with itself, i.e., no two amino acids can occupy the same lattice point. This can be expressed as:\nrir; \u2200i, j\u2208 0, 1, . . ., N \u2212 1, i \u2260 . j (3)\nwhere N is the total number of amino acids in the sequence.\n\u2022 Translational invariance: Translational invariance is ensured by the presence of primitive translation vectors (a1, a2, a3) that map the lattice onto itself. The translation vectors are defined as:\nT = n171 + n2a2 + n3a3 (4)\nwhere n1, n2, N3 \u2208 Z.\nIn this problem, the objective is to find the optimal conformation that maximizes the number of hydrophobic-hydrophobic (H-H) contacts in the folded protein. An H-H contact occurs when two non-consecutive hydrophobic amino acids are placed adjacent to each other in the lattice. The energy of a given fold is defined as the negative of the total number of valid H-H contacts:\nE = -(number of valid H-H contacts) (5)\nBy minimizing the energy function, the most stable conformation with the maximum number of H-H contacts can be found."}, {"title": "DRL Setup", "content": "By treating the protein folding process as a SAW, the RL agent places the amino acids sequentially. In our setup, the protein is considered the agent, and the cubic lattice represents the environment. The length of the path corresponds to the number of amino acids in the protein sequence. At the end of each episode, the environment provides the agent with a reward based on the energy of the achieved fold."}, {"title": "Markov Decision Process Formulation", "content": "A MDP is defined by a tuple (S, A, P, R, \u03b3), where S is the state space, A is the action space, P is the transition probability function, R is the reward function, and \u03b3\u2208 [0,1] is the discount factor. At each discrete time step t, the agent interacts with the environment by observing the current state st \u2208 S and taking an action at \u2208 A. The environment then transitions to a new state st+1 \u2208 S according to the transition probability function P(st+1|st, at) and provides a reward rt+1 \u2208 R to the agent. The objective of the agent is to learn a policy \u03c0 : S \u2192 A that maximizes the expected cumulative reward over an episode, defined as E [\u2211t=0\u03b3trt+1], where T is the length of the episode."}, {"title": "Deep Q-Learning with Stabilization Techniques", "content": "Deep Q-learning (DQN) is a reinforcement learning algorithm that combines Q-learning with deep neural networks to optimize the conformations achieved by the agent in the protein folding problem. Q-learning is a value-based method that learns the optimal action-value function, or Q-function, which represents the expected future reward for taking a particular action in a given state. The Q-function, denoted as Q(s, a), satisfies the Bellman optimality equation:\n$Q(s, a) = E [rt+1 + y \\max_{at+1} Q(st+1, at+1)]$                                                                                                                                                                                                                                                                                                                                                                                       (6)\nwhere rt+1 is the reward received at time step t + 1 for carrying out action at at state st, \u03b3 \u2208 [0, 1] is the discount factor that determines the importance of future rewards, and st+1 is the state at time step t + 1. The expectation E is taken over all possible next states and actions, reflecting the average outcome based on the agent's policy and the environment's dynamics. The optimal Q-function, denoted as Q* (s, a), satisfies the Bellman optimality equation and provides the maximum expected future reward for taking action a in state s. DQN addresses the limitations of traditional Q-learning, which becomes intractable for problems with large state spaces, by approximating the Q-function using a deep neural network. The neural network, parameterized by 0, takes the state s as input and outputs the Q-values for each action a. The training objective is to minimize the mean-squared error loss function:\n$L(\u03b8) = E[(r + y \\max_{at+1} Q(st+1, at+1; \u03b8\u00af) \u2013 Q(s, a; \u03b8))^2]$                                                                                                                                                                                                                                                                                                                                                                     (7)\nwhere st+1 is the next state, at+1 is the next action, and 0\u00af represents the parameters of a target network. The target network is a separate neural network that is periodically updated with the parameters of the main network, denoted as 0. The use of a target network is a key stabilization technique introduced in the DQN paper by [Mnih et al., 2015], which helps to mitigate the issue of divergence in the learning process. During training, the agent interacts with the environment and stores the experienced transitions et = (st, at, rt+1, St+1) in a replay buffer D. The replay buffer is a fixed-size cache that stores the most recent transitions experienced by the agent. At each training step, a minibatch of transitions e1, 2, ..., en is sampled uniformly from the replay buffer to update the parameters of the main network 0. The use of a replay buffer helps to break the correlation between consecutive samples and stabilizes the learning process by providing a diverse set of experiences for training.\nThe training process is displayed in Figure 1 and proceeds as follows:\n1. Sample a minibatch of transitions e1, 2, . . ., en from the replay buffer D.\n2. For each transition et = (st, at, rt+1, St+1), compute the target:\n$Y_t = \\begin{cases}  rt+1 & \\text{if st+1 is terminal} \\\\  rt+1 + y \\max_{a'} Q(s_{t+1},a';\\bar{0}) & \\text{otherwise} \\end{cases}$                                                                                                                                                                                                                                                                                            (8)\n3. Update the parameters @ of the main network by minimizing the loss function:\n$L(0) = \\frac{1}{N} \\sum_{t=1}^N (Y_t \u2013 Q(st, at; 0))^2$                                                                                                                                                                                                                                                                       (9)\n4. Every C steps, update the parameters 0\u00af of the target network by copying the parameters @ of the main network:\n00 (10)\n5. Select an action at based on the current state st using an e-greedy policy derived from the Q-values:\n$at = \\begin{cases} \\argmax_a Q(st, a; \u03b8) & \\text{with probability 1 - } \\epsilon \\\\ \\text{random action} & \\text{with probability } \\epsilon \\end{cases}$                                                                                                                                                 (11)\nwhere e is the exploration rate that determines the probability of selecting a random action instead of the greedy action with the highest Q-value."}, {"title": "State Representation", "content": "We represent the states using one-hot encoded vectors. Each state vector consists of two parts: the first part represents the available actions (forward (F), backward (B), right (R), left (L), up (U), and down (D)), and the second part encodes the type of amino acid (H or P) at the current position. Specifically, the state is an 8-dimensional vector, where the first six elements correspond to the possible actions the agent can take: No Decision (ND), F, L, R, U, D. The last two elements represent the type of amino acid (H or P). For each training episode, we obtain a 3D shape tensor (N,8,1), where N is the length of the protein sequence. The first dimension corresponds to the time steps, that is, the positions in the protein sequence, while the second dimension represents the state features (actions and amino acid type). The third dimension is a singleton dimension to facilitate input to the neural network. This state representation allows the agent to make informed decisions based on the available actions and the type of amino acid at each position. By concatenating action and amino acid information, the neural network can learn the dependencies between amino acid placements and the resulting energy conformations. It is important to note that the neural network does not directly observe amino acid placements; instead, it deduces this information from actions taken thus far. The \"No Decision\" (ND) indicates a state where no further moves are possible, leading to episode termination. The backward action (B) is excluded because once an amino acid is placed at a position, revisiting that position is not feasible; thus, it cannot be performed in this context."}, {"title": "Q-Network Architectures", "content": "In this subsection, we describe the two architectures proposed in this paper."}, {"title": "Hybrid - Reservoir", "content": "The architecture consists of a reservoir layer followed by several fully connected layers, as illustrated in Figure 2.\nThe input to the reservoir neural network is a flattened vector representation of the state, denoted as x \u2208 RN\u00d7d, where N is the sequence length and d is the dimensionality of the one-hot encoded vector described in 2.2.3. The reservoir layer applies a fixed random projection of the input into a high-dimensional space. Mathematically, the reservoir layer can be described as follows:\n$r(t) = f(W_{in}x(t) + W r(t - 1))$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 (12)\nwhere r(t) \u2208 RNr is the reservoir state at time step t, x(t) \u2208 Rd is the input state, Win \u2208 RNr\u00d7d is the trainable input weight matrix, W \u2208 RN\u30f6\u00d7Nr is the reservoir weight matrix, and f(\u00b7) is the activation function (e.g., hyperbolic tangent). The reservoir weight matrix W is randomly initialized and remains fixed during training. It follows a specific connectivity pattern, such as the Erd\u0151s-R\u00e9nyi topology, which promotes a limited number of active connections among neurons. The sparsity and connectivity of the reservoir are important factors in determining its computational capacity and ability to capture complex dynamics. The size of the reservoir, denoted as Nr, is a hyperparameter that depends on the length and complexity of the protein sequence. Empirically, we found that a reservoir size of 1000 works well for sequences of length N < 36, while for longer sequences (N = 48, 50), a larger reservoir size of around 3000 is used to capture the increased complexity. The reservoir layer applies a hyperbolic tangent (tanh) activation function to introduce nonlinearity. The output of the reservoir layer is then passed through a series of fully connected layers with Rectified Linear Unit (ReLU) activation functions. The sizes of the fully connected layers are 512, 256, 128, and 84, respectively. These layers learn to extract meaningful features from the reservoir representation and progressively reduce the dimensionality of the feature space. The final output layer is a fully connected layer with a size equal to the number of actions, which generates the Q-values for each action."}, {"title": "LSTM with Multi-Head-Attention", "content": "For long proteins, we employ an LSTM architecture with multi-head attention [Vaswani et al., 2017]. The architecture consists of an LSTM layer, a multi-head attention mechanism, and a fully connected output layer, as illustrated in Figure 3.\nThe multi-head attention mechanism enhances the network's ability to focus on different aspects of the input sequence simultaneously. Given an input sequence processed by the LSTM layers producing hidden states H \u2208 RN\u00d7d, where N is the sequence length and d is the hidden dimension, the attention mechanism computes attention patterns across four different representation subspaces. For each attention head i, the mechanism computes:\n$Attention (Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{dk}})V$                                                                                                                                                                                                                                                                                                                                                                                           (13)\nOur implementation uses a hidden size of d = 512 with four attention heads. The LSTM layer processes the input sequence with batch-first ordering, and the attention mechanism operates on the full sequence of LSTM outputs. The final output is obtained by selecting the last time step of the attention output, followed by a fully connected layer that maps to the action space dimension."}, {"title": "Experiments and Results", "content": "To evaluate our approach, we utilized sequences with best-known energy values published in Table 1 from [Boumedine and Bouroubi, 2022]. Figure 4 illustrates the optimal folds for various sequences, providing a benchmark for our experiments. We began our investigation with the baseline Fully Connected Feedforward Neural Network (FFNN), which employs a four-layer architecture (512\u2192256\u2192output-size) with ReLU activations. This model was effective for shorter sequences (N \u2264 36), demonstrating reasonable performance in achieving optimal energy states. However, it struggled with longer sequences, often failing to converge to the best-known values for more complex protein structures. This limitation highlighted the need for enhancements that could better capture the intricate dynamics of protein folding. To address these shortcomings, we introduced the Reservoir-based Fully Connected Feedforward Neural Network (FFNN-R). By incorporating a reservoir layer initialized using Xavier uniform initialization and tanh activation, FFNN-R enhanced the network's temporal memory capabilities while maintaining training efficiency. The reservoir's sparsity and specific connectivity patterns allowed for improved processing of input data. Empirical results showed that FFNN-R outperformed the vanilla FFNN, converging to optimal energy states faster, requiring approximately 25% fewer training episodes to reach these conformations. Particularly for shorter sequences, FFNN-R consistently achieved BKVs for sequences A1-A5 and A8-A10, showcasing its superior convergence speed. Building on the improvements seen with FFNN-R, we explored LSTM architectures to further enhance performance on longer sequences. The LSTM-OLH architecture utilized traditional LSTM cells while relying solely on the last hidden state for decision-making. While this approach provided some improvements over FFNN, it still struggled to effectively capture long-range dependencies critical in protein folding. To maximize performance further, we developed the LSTM-A architecture, which incorporated a multi-head attention mechanism. This enhancement allowed the model to dynamically weigh different temporal aspects of the state sequence, effectively capturing long-range interactions between amino acids. The results indicated that LSTM-A significantly outperformed both LSTM-OLH and vanilla FFNN implementations for longer sequences (N > 36). It consistently achieved optimal or near-optimal energy states that matched BKVs for sequences 3d1-3d5 and approached BKVs for more challenging sequences like 3d6-3d9. Throughout our experiments, we employed a stabilized Deep Q-Learning framework with experience replay and an epsilon-greedy strategy for action selection. The training utilized a single NVIDIA H100 GPU with architectures"}, {"title": "Efficiency", "content": "The LSTM-A and FFNN-R architectures demonstrate distinct computational profiles in memory usage and training efficiency. LSTM-A requires 2.4GB memory for sequences \u2264 36 residues (3d1-3d4), with memory distributed across LSTM layers (512 units, 1.8GB), multi-head attention mechanism (0.4GB), and auxiliary layers (0.2GB). FFNN-R"}, {"title": "Discussion", "content": "Our analysis reveals several key insights into both architectures' effectiveness. The LSTM-A's 4-head attention mechanism proves optimal for capturing long-range dependencies in protein folding, as evidenced by the attention weight patterns shown in Figure 7. The visualization demonstrates how different attention heads specialize over training iterations \u2013 initially showing uniform weights (yellow matrices) that evolve into distinct patterns capturing both local and global protein structure interactions. Network depth beyond 5 layers shows a decrease in performance, while batch size of 32 provides optimal balance between memory usage and training stability.\nCompared to traditional genetic algorithms and Monte Carlo methods, the LSTM-A demonstrates superior performance on longer sequences while requiring significantly less computational time to reach optimal conformations. The evolution"}, {"title": "Limitations and Future Directions", "content": "Our implementation achieves state-of-the-art results for the HP model, though important limitations remain. The reservoir-based approach shows decreased performance for proteins exceeding 36 residues, while the LSTM-A architecture, though more robust for longer sequences, demands substantial computational resources and training time. Future work should address several key challenges: extending the architectures to handle realistic protein force fields, incorporating additional physical constraints, optimizing the attention mechanism's efficiency for longer sequences, and exploring hybrid approaches that leverage the strengths of both architectures. The linear scaling relationship between sequence length and reservoir size also merits investigation for improved efficiency. The HP model provides an ideal benchmark system, capturing fundamental protein folding aspects while remaining computationally tractable. Its NP-complete nature in finding optimal conformations makes it particularly valuable for testing new algorithms. Notably, our methods successfully achieve optimal conformations that align with known energy minima."}, {"title": "Conclusion", "content": "This study advances protein structure prediction through two novel architectures. The LSTM-A achieves good performance on longer sequences (N > 36), while the FFNN-R provides efficient solutions for shorter sequences. Both consistently match best-known energy values across benchmark sequences. The success of these architectures demonstrates the viability of deep learning approaches in protein structure prediction, while establishing new benchmarks for"}]}