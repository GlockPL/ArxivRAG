{"title": "LMS-AUTOTSF: LEARNABLE MULTI-SCALE DECOMPOSITION AND INTEGRATED AUTOCORRELATION FOR TIME SERIES FORECASTING", "authors": ["Ibrahim Delibasoglu", "Sanjay Chakraborty", "Fredrik Heintz"], "abstract": "Time series forecasting is an important challenge with significant applications in areas such as weather prediction, stock market analysis, and scientific simulations. In this work, we introduce LMS-AutoTSF, a novel time series forecasting architecture that incorporates autocorrelation while leveraging dual encoders operating at multiple scales. Unlike traditional models that rely on predefined trend and seasonal components, LMS-AutoTSF employs two separate encoders per scale: one focusing on low-pass filtering to capture trends and the other utilizing high-pass filtering to model seasonal variations. These filters are learnable, allowing the model to dynamically adapt and isolate trend and seasonal components directly in the frequency domain. A key innovation in our approach is the integration of autocorrelation, achieved by computing lagged differences in time steps, which enables the model to capture dependencies across time more effectively. Each encoder processes the input through fully connected layers to handle temporal and channel interactions. By combining frequency-domain filtering, autocorrelation-based temporal modeling, and channel-wise transformations, LMS-AutoTSF not only accurately captures long-term dependencies and fine-grained patterns but also operates more efficiently compared to other state-of-the-art methods. Its lightweight design ensures faster processing while maintaining high precision in forecasting across diverse time horizons.", "sections": [{"title": "Introduction", "content": "Time series data typically comprises an ordered sequence of observed or measured outcomes from a process at fixed time intervals. These datasets are designed to capture relevant information and activities within a specific subject matter. The primary goal of time series applications is to estimate future values by the identification of underlying patterns in historical data. Time series forecasting (TSF) is a frequently used technique in fields like traffic congestion anticipation, weather forecasting, and stock market prediction. Making more informed and efficient decisions is made possible by accurate forecasting, which gives decision-makers the ability to recognise and reduce risks [1]. Due to the complex and ever-changing nature of real-world systems, observed time series often exhibit intricate temporal patterns. These patterns can involve a mix of increasing, decreasing, and fluctuating trends, making forecasting highly challenging. Multivariate time series forecasting involves predicting future values of multiple interrelated time-dependent variables based on their historical data. Unlike univariate forecasting, which deals with a single variable, multivariate forecasting considers the interactions and dependencies between multiple variables, allowing for more complex and potentially more accurate predictions across several dimensions [2]. In time series data, the trend component represents the long-term directional movement, showing consistent upward or downward patterns, while the seasonal component captures regular, repeating fluctuations over fixed intervals, such as daily, monthly, or yearly cycles. Variations in trends can be linear or nonlinear and may shift or reverse over time, whereas seasonal variations are typically periodic but can vary in amplitude or timing due to external factors. Both components often overlap, making the task of isolating and forecasting them more complex [3]. In time series analysis, frequency domain filtering has two types. A low-pass filtering is commonly used to capture seasonal variations by allowing low-frequency components of the data to pass through while attenuating high-frequency noise. This process helps isolate recurring seasonal patterns, such as annual or monthly cycles, which are important in fields like finance and climatology [4]. Conversely, high-pass filtering is employed to model trends by passing high-frequency components and filtering out lower-frequency signals, which represent long-term, smooth variations. This technique is particularly useful for identifying short-term fluctuations and trends, removing the impact of long-term cyclical patterns [5]. Both methods provide a clearer decomposition of time series data, enabling analysts to focus on specific components of interest. Transformer is emerging in time series forecasting, driven by the tremendous success in the natural language processing field. Transformer has the ability to extract multi-level representations from sequences and illustrate pairwise connections [6]. In time series data analysis, Transformers that combine frequency-domain filtering with temporal and channel-wise transformations offer a powerful approach to capturing complex dependencies and patterns. By leveraging frequency-domain filtering, these models can efficiently separate and analyze the signal's various frequency components, isolating key seasonal or trend-related elements [2]. Additionally, temporal and channel-wise transformations allow the model to process both the sequential nature of time series data and the relationships across multiple channels, making it particularly adept at handling multivariate time series and discovering intricate dependencies between variables [7]. This dual capability enables the model to address both local and global patterns, improving performance in tasks such as forecasting, anomaly detection, and classification. Our main contributions are as follows:\n\u2022 The dynamic decomposition enhances the model's ability to capture intricate patterns and the model can learn the trend and seasonality features of each dataset without depending on fixed assumptions.\n\u2022 By leveraging the concept of autocorrelation in the time domain, the architecture effectively incorporates temporal dependencies into the forecasting process. This integration allows the model to recognize and utilize historical relationships within the data, improving forecasting accuracy.\n\u2022 LMS-AutoTSF achieves state-of-the-art performance in time series forecasting tasks by integrating learnable decomposition, autocorrelation, and multi-scale processing across a wide range of benchmarks.\n\u2022 LMS-AutoTSF is a lightweight and computationally efficient architecture, that significantly reduces processing time compared to existing state-of-the-art methods, while maintaining or even improving forecasting accuracy.\nWe do comprehensive tests on different time-series forecasting benchmarks to support our motivation and hypothesis. When compared to various forecasting techniques, the performance of our proposed model is at the cutting edge. Extensive ablation investigations and analysis trials support the viability of suggested designs and their consequent benefit over earlier methods.\nThis paper is organized as follows. Section 2 explains a set of typical times series forecasting works from the literature which sets the notion and motivation of this paper. Section 3 briefly explains the problem statement of this paper. In Section 4, we provide a detailed discussion of our proposed architecture called LMS-AutoTSF which is suitable for multivariate time series forecasting. In section 5, we discuss the result analysis of the proposed methodology and describe a detailed comparison with the state-of-the-art time series forecasting models. A conclusion of the work's findings is provided in Section 6."}, {"title": "Literature Review", "content": "In long-term and short-term time series forecasting, transformers have demonstrated exceptional performance and significant potential [2, 8, 9, 10]. Informer [11], the first renowned transformer for time-series forecasting, uses a generative style decoder and ProbSparse self-attention to solve problems like quadratic time complexity. A number of models were introduced after Informer[11], including Autoformer [12], Pyraformer [13], iTransformer [6], Reformer [14] and FEDFormer [15]. While Pyraformer[13] concentrates on multiresolution attention for signal processing efficiency. Autoformer[12] employs auto-correlation and decomposition for performance, while FEDFormer [15] combines frequency analysis with Transformers for improved time series representation. By emphasising the value of patches, the PatchTST [9] improves the model's capacity to identify both local and global relationships in data. To leverage the cross-dimension dependency, a Crossformer [16] model is presented for multivariate time-series forecasting. The Dimension-Segment-Wise (DSW) embedding approach is used in Crossformer to embed the input time-series data into a 2D vector array while preserving time and dimension information. Thus, the Two-Stage Attention (TSA) layer successfully captures the cross-time and cross-dimension dependency. Using the DSW embedding and TSA layer, Crossformer builds a Hierarchical Encoder-Decoder (HED) that utilises the data at different scales for the ultimate prediction. A multilayer perceptron (MLP) architecture is used by the time series forecasting model TSMixer [17] to blend and process temporal information. Token-mixing layers, which are used by TSMixer in place of conventional recurrent or convolutional neural networks, capture dependencies between several time steps and feature dimensions. TSMixer is able to forecast across a variety of time series datasets with efficiency and effectiveness by concentrating on these relationships. Its straightforward yet effective architecture enables parallel processing, which reduces training and inference times while preserving competitive performance when compared to more intricate models like Transformers and LSTMs. TimeMixer [18], a complete multi-layer perceptron-based architecture, takes the entire benefit of disentangled multiscale time-series in both the past extraction and future prediction stages. The Past-Decomposable-Mixing (PDM) and Future-Multipredictor-Mixing (FMM) components make up this system. Specifically, PDM uses the decomposition to multiscale series and then independently combines the seasonal and trend patterns that have been decomposed in fine-to-coarse and coarse-to-fine directions, therefore aggregating the macroscopic trend information and the microscopic seasonal information in turn. FMM additionally groups several predictors to take use of their complimentary predicting skills in multiscale observations. For time series forecasting, the DLinear model [10] is a simple and effective method that breaks down a time series into trend and seasonal components. By modelling each component independently using linear layers, DLinear streamlines the forecasting process in contrast to intricate deep learning models. The model is able to extract from the data both short-term seasonal patterns and long-term trends thanks to this decomposition. Besides transformer models, a novel norm-bounded graph attention network (GAT) is generated for multivariate time-series forecasting by upper-bounding the Frobenius norm of weights in each layer of the GAT model to improve performance and address the basic over-smoothing issue in deep graph neural networks-based models [8]. The problem is made more complex by GAT's over-smoothing, which is more complex because it multiplies several attention matrices at different moments. GAT's over-smoothing is less obvious than other methods. The trainable adaptive parameters in weight-bound control may be able to reduce over-smoothing by distributing the attention matrices throughout layers and nodes. Autoformer and FEDFormer are transformer-based methods that rely on attention mechanisms to capture long-term dependencies and decompose time series into trend and seasonal components. Autoformer employs progressive attention and learnable decomposition, while FEDFormer integrates frequency domain features and federated attention for improved efficiency. TimeMixer, in contrast, adopts a non-transformer-based approach similar to ours, utilizing mixing components for representation learning. These methods share some conceptual similarities with our architecture, such as leveraging decomposition techniques and multi-scale processing. However, our lightweight, fully connected architecture relies on autocorrelation as a core feature in skip connection and FFT-based learnable filters to extract trend and seasonal components effectively. This streamlined design achieves superior performance while remaining computationally efficient, as evidenced by our experimental results."}, {"title": "Problem Statement", "content": "This paper addresses the challenge of long-term forecasting for multivariate time series, using historical data. We define a multivariate time series at time t as X\u2081 = [Xt,1,Xt,2,..., Xt,N], where xt,n represents the value of the n-th variable at time t, for n = 1, 2, . . ., N. We use the notation Xt:t+h to represent the X-values from the time point t tot + H, inclusive. The aim is to develop a model for forecasting the future values of the series over the next T time steps, based on the most recent L time steps. The parameters Land H are referred to as the look-back window and the prediction horizon, respectively. Specifically, for a given initial time to, the model takes as input the sequence Xto-L:to, corresponding to the past L time steps, and outputs the predicted sequence Xto:to+H, representing the forecasted values for the next H time steps. The predicted value of Xt at time t is denoted by Xt. In brief, the goal of multivariate time series forecasting is to predict future values Xt:t+H given past observations Xt\u2212L:t:\n$X_{t:t+H} = f(X_{t-L:t})$\n(1)\nThe forecasting performance of the model is assessed by computing the mean squared error (MSE) and the mean"}, {"title": "Methodology", "content": "In this work, we introduce a novel approach for time series forecasting that leverages a combination of multi-scale input processing, frequency domain filtering [19], and autocorrelation. The general overview of the proposed architecture is represented in Figure 1. To capture both local and global patterns in the time series, we apply multi-scale down-sampling of the input data. The input sequence is progressively down-sampled using an average pooling layer, with each scale indexed by k representing different temporal resolutions. This enables the model to capture variations across multiple time horizons. In the rest of the article, we use the abbreviation X\u013bB to represent Xt\u2212L:t, where \"LB\" stands for \"loopback\".\n$X^{(k)}_{t:t+H} = Downsampler(X_{t:t+H}^{(k)})$\n(2)\nFor each scale k, downsampled time series is decomposed into trend (T) and seasonality (S) components with learnable (differentiable) layers in the frequency domain.\nThe decomposition is achieved by applying a frequency-domain transformation (via FFT), followed by learnable low-pass and high-pass filters. The cutoff frequency and steepness of the filters are trainable parameters, and different filters are learned for each feature of the time series. The filtering process is defined as:\n\u2022 Frequency transform: Fast Fourier Transform (FFT) efficiently transforms a time series Xt from the time domain into its frequency domain representation X (f), and vice versa, by decomposing the series into a sum of sinusoidal components.\n$FFT(X_{L}^{LB}) = \\sum_{i=0}^{L-1}X_{t-L+i} \\cdot e^{-j \\frac{2\\pi i}{L}}$\n(3)\n$X_{freq} = FFT(X_{L}^{LB})$\n(4)\n\u2022 Low-pass filter: Captures the trend (T) by retaining lower frequencies, defined as:\n$T = X_{low} = FFT^{-1}(X_{freq} \\cdot \\sigma(-(f - f_{cutoff}) \\cdot s))$\n(5)\nwhere f is the frequency, fcutoff is the learnable cutoff frequency, s is the steepness, and o is the sigmoid function.\n\u2022 High-pass filter: Extracts the seasonal component (S) by retaining higher frequencies, defined"}, {"title": "Experiments", "content": "In this section, we perform in-depth experiments to assess how well our proposed LMS-AutoTSF model forecasts in conjunction with state-of-the-art time-series forecasting architectures. An ablation work is also applied to measure the effect of the proposed modules of learnable decomposition and autocorrelation. All the experiments are implemented in PyTorch, CUDA version 12.2 and conducted on a single NVIDIA-GeForce RTX 3090 with 24GB GPU. We have replicated all of the compared baseline models and implemented them using the benchmark Time-Series Library (TSLib) [20] repository, which is based on the configurations provided by the official code or actual article for each model. We train the proposed model on all datasets using a batch size of 32, a learning rate of 0.0001 except PEMS, and the ADAM optimizer with L2 loss. The model operates with K = 4 scales."}, {"title": "Datasets", "content": "We evaluate the performance of our proposed architecture on eight widely-used public benchmark datasets, including Weather, Electricity, Traffic, Exchange and four ETT datasets (ETTh1, ETTh2, ETTm1, ETTm2) for long-term forecasting as detailed in supplemmentary file. The number of features in these datasets varies significantly, with Electricity containing 321 features and Traffic comprising 862 features, making them notably larger and more complex compared to others. In contrast, Weather has 21 features, Exchange has 8 features, and the four ETT datasets each have 7 features, allowing for a diverse evaluation of our architecture across datasets with varying feature scales. The PEMS dataset [21], which is used for short-term traffic flow forecasting, includes four public traffic network datasets: PEMS03, PEMS04, PEMS07, and PEMS08. It provides an additional challenge with its highly dynamic and rapidly changing time series. Additionally, the M4 dataset includes 100,000 time series spanning various domains such as finance, industry, and"}, {"title": "Forecasting Results", "content": "Table 1 provides insights into the contributions of various components of the LMS-AutoTSF architecture. Each row represents a different configuration of the model, showcasing the impact of using fixed decompomposition, learnable decomposition, and autocorrelation on forecasting performance. The inclusion of autocorrelation further optimizes the model's performance, as seen in the third row, where both learnable decomposition and autocorrelation yield the lowest MSE and MAE values across most datasets. This suggests that integrating autocorrelation helps the model capture temporal dependencies more effectively. Comprehensive forecasting results are listed in Tables 2, 3 and 4. A lower MSE/MAE reflects more accurate predictions, and our proposed LMS-AutoTSF consistently achieves the best performance across most datasets for both long-term and short-term forecasting issue. It outperforms state-of-the-art models such as TimeMixer, iTransformer, and PatchTST, particularly excelling in high-dimensional time series forecasting. We have used sMAPE, MAPE, OWA, and MASE to evaluate the performance of our model on the M4 dataset. SMAPE and MAPE measure percentage-based forecast accuracy, with sMAPE normalizing by the sum of actual and forecast values. OWA combines SMAPE and MASE, benchmarking results against the na\u00efve seasonal model, while MASE assesses error scaled by the in-sample mean absolute error. These metrics provide a comprehensive evaluation across diverse time series in the M4 dataset. Figure 3 and 4 show sample long-term predictions for best four architectures: LMS-AutoTSF, iTransformer, TimeMixer, and PatchTST. In addition to the results presented in Table 2, further comparison of forecasting methods, including models not listed in the table, is provided in Figure 2. It represents a broader analysis of the methods' performance across various datasets, highlighting the advantages of our proposed LMS-AutoTSF model. Figure 5 shows the sample prediction on PEMS03 dataset, and it is seen that proposed LMS-AutoTSF has closer predictions to the ground truth. More details for the PEMS prediction comparison are represented in the appendix. For short-term forecasting performance, proposed method has competitive performance with TimeMixer, and outperforms iTransformer and PatchTST as shown in Table 4. Table 5 presents a comparison of multivariate short-term forecasting models in terms of execution time (in seconds) and the number of FLOPs (floating-point operations) across four PEMS datasets. LMS-AutoTSF consistently demonstrates the fastest execution time, outperforming models like TimeMixer, iTransformer, and PatchTST. Additionally, LMS-AutoTSF also achieves significantly fewer FLOPs, highlighting its computational efficiency. The number of FLOPs was calculated using the torchprofile library, emphasizing the model's optimized performance with reduced computational overhead compared to other methods."}, {"title": "Conclusion", "content": "In this work, we have introduced LMS-AutoTSF, a novel time series forecasting architecture that combines learnable decomposition with integrated autocorrelation and multi-scale processing. By leveraging filtering through learnable parameters, the model dynamically isolates trend and seasonal components directly in the frequency domain. Integrated autocorrelation further enhances the model's ability to capture temporal dependencies, improving forecasting accuracy across various time horizons. LMS-AutoTSF consistently achieves state-of-the-art performance across a wide range of benchmarks, showcasing its generality and robustness for time series forecasting tasks."}]}