{"title": "Multi-Modality Collaborative Learning for Sentiment Analysis", "authors": ["Shanmin Wang", "Chengguang Liu", "Qingshan Liu"], "abstract": "Multimodal sentiment analysis (MSA) identifies individuals' sentiment states in videos by integrating visual, audio, and text modalities. Despite progress in existing methods, the inherent modality heterogeneity limits the effective capture of interactive sentiment features across modalities. In this paper, by introducing a Multi-Modality Collaborative Learning (MMCL) framework, we facilitate cross-modal interactions and capture enhanced and complementary features from modality-common and modality-specific representations, respectively. Specifically, we design a parameter-free decoupling module and separate uni-modality into modality-common and modality-specific components through semantics assessment of cross-modal elements. For modality-specific representations, inspired by the act-reward mechanism in reinforcement learning, we design policy models to adaptively mine complementary sentiment features under the guidance of a joint reward. For modality-common representations, intra-modal attention is employed to highlight crucial components, playing enhanced roles among modalities. Experimental results, including superiority evaluations on four databases, effectiveness verification of each module, and assessment of complementary features, demonstrate that MMCL successfully learns collaborative features across modalities and significantly improves performance. The code can be available at https://github.com/smwanghhh/MMCL.", "sections": [{"title": "I. INTRODUCTION", "content": "Multimodal sentiment analysis (MSA) aims to infer individuals' sentiment states from signals they emit, and has wide applications in human-computer interactions, intelligent healthcare, intelligent driving, etc [1]\u2013[4]. Typically, individuals convey their inner states through a combination of facial expressions, spoken language, and acoustic behaviors, which can be captured from video clips [5]\u2013[8]. Since these signals are driven by the same sentiment states, they definitely exhibit collaborative characteristics [9]. Facilitating interactions between these signals and capturing collaborative features can enhance and compensate sentiment expressions, making it one of the keys for MSA [10]\u2013[12].\nModality fusion [13], [14] and representation disentanglement [15], [16] are popular MSA methods. Modality fusion leverages tensor-based [17], attention-based [18], or transfer-based [19] techniques to integrate representations from all modalities. Nevertheless, representations from various modalities often differ significantly in constituent elements, elements' distributions, noise levels, task relevance, etc, which are termed modality heterogeneity [20]. The heterogeneity of modalities hinders methods that directly integrate semantically unaligned representations from extracting interactive sentiment features across modalities seriously [16], [21], [22].\nRepresentation disentanglement methods suggest that cross-modal interactive features include modality-common and modality-specific components [15], [16]. Modality-common features have shared semantics among modalities, such as rhythm, which can be expressed simultaneously through lip movements and sound waves. Modality-specific features are exclusively inherent to a particular modality, such as visual color properties. Based on this insight, these methods propose a new multimodal data processing paradigm that prioritizes disentanglement before fusion. Specifically, techniques such as subspace learning [15], [23], adversarial learning [16], [21], [24], and transfer learning [25]\u2013[27] are employed to separate unimodal representations into modality-common and modality-specific components, with the intention of providing cross-modal enhancement and complementarity, respectively [21], [25], [28]. Subsequently, these disentangled components from all modalities are combined into a joint multimodal representation for prediction. However, two critical issues remain. First of all, modality-specific representations often fail to fully serve complementary roles due to their inclusion of task-irrelevant semantics, resulting in fragile multimodal representations. Besides, these disentanglement approaches rely heavily on the synergistic use of complex models paired with rigorous loss functions, leading to limited adaptability.\nTo alleviate the above issues, in this paper, we propose a Multi-Modality Collaborative Learning (MMCL) framework, capturing interactive sentiment features that enhance and complement across modalities. MMCL begins with a parameter-free decoupling module. Due to the inherent temporal asynchrony nature between modalities, the decoupling module assesses semantic correlations between temporal elements and divides unimodal representation into common and specific components, avoiding complex learning prerequisites. For decoupled specific representations, MMCL learns from the act-reward mechanism in reinforcement learning and captures complementary features adaptively. Specifically, it assigns a policy model for each specific representation for feature learning, and rewards learned features across modalities unitedly. The reward is incorporated into a centralized critic model"}, {"title": "II. RELATED WORK", "content": "Multimodal sentiment analysis (MSA) aims to determine individuals' sentiment states or emotional categories [29], [30]. In the early stages, researchers shift from unimodal to multimodal sentiment analysis [13], [14], [31], [32], leading to modality-fusion techniques becoming the focus.\nMultimodal Fusion Strategies. Early and late fusion are two paradigms to integrate multiple modalities. Early-fusion approaches merge multiple representations into a joint one before making decisions. Late-fusion methods independently predict from uni-modalities and combine predicted vectors to make final decisions [33], [34]. Compared to late fusion, early fusion is more widely used. For example, TFN combined bi- or tri-modal data through tensor operations [32]. MFN and MARN integrated multiple signals over time via gated units and attention mechanisms [13], [31], respectively. MCTN performs cyclic translations among modalities and treats intermediate results as the joint multimodal representation [19]. MulT exploited multiple transformers to combine pairwise modalities by establishing long-time dependencies [14]. However, the gaps between heterogeneous signals pose significant challenges for these models in effectively capturing features for cross-modal interactions [16], [21], [22].\nDisentangled Representation Learning. To efficiently facilitate cross-model interactions, recent studies have divided unimodal representations into modality-common and specific components before modality fusion, aiming to provide enhanced and complementary sentimental clues, respectively [15], [25], [28]. To learn common and specific features, a variety of disentanglement techniques have emerged [35], [36]. Subspace learning, adversarial learning, and transfer learning are leading techniques for disentanglement. Subspace learning approaches employ distinct encoders paired with strict constraints to map raw inputs into common and specific subspaces [23]. For instance, MISA integrates similarity loss, orthogonal loss, and reconstruction loss to guide the learning of features in each branch [15]. Additionally, some research leverages adversarial learning based on encoders to further increase discrepancies between disentangled common and specific features. [16], [21], [24]. Several studies further conclude from previous research that the text modality provides higher-order semantic information compared to acoustic and visual modalities [37]. Based on this premise, they use the text modality as an anchor and divide the visual and acoustic data into common and specific features. For example, encoders, seq2seq, and transformer models are utilized in CRNet [27], TCSP [25], and CJTF [26], respectively, to transfer acoustic and visual data to the text modality.\nThe aforementioned models impose stringent constraints on common feature learning to ensure a high level of similarity. The residuals between inputs and common features are treated as specific features. Inheriting the semantic similarity requirement for common features from the above works and considering the temporal asynchronicity across modalities, we design a parameter-free decoupling module. It assesses the semantic similarities of each paired cross-modal element along the time dimension, which avoids complex parameter learning and structural design, getting better adaptability.\nAfter disentanglement, common and specific from all modalities are combined through concatenation [23], [25]\u2013[27], distribution alignment [16], or popular cross-modal attention [14], [15], [18], [24], [38]\u2013[40]. The premise behind the above operations is that disentangled common and specific features perfectly play the role of cross-modal enhancement and complementarity, respectively. Nevertheless, according to the disentanglement scheme, specific features also convey task-irrelevant semantics. In this paper, inspired by adaptive policy learning under the rewarding mechanism in reinforcement learning [41]\u2013[43], we design a specialized policy model for each modality-specific representation to mine complementary features."}, {"title": "III. MULTI-MODALITY COLLABORATIVE LEARNING (MMCL)", "content": "In this section, we elaborate on the proposed MMCL method. The overall structure is illustrated in Figure 1. For the three input modalities {X^v, X^a, X^t}, we represent and map them into the same subspace as {Z^v, Z^a, Z^t \u2208 R^{L\u00d7d_k} }, where L is the sequence length, and d_k is representations' dimension. Our objective is to coordinate multi-modalities to make accurate predictions. The collaboration between modalities contains enhanced and complementary features, which can be captured from common and specific representations, respectively. To this end, we first employ the Common-Specific Feature Decoupling (CSD) module to separate unimodal representations. Subsequently, crucial elements in decoupled common features are emphasized to promote mutual enhancement"}, {"title": "B. Common-Specific Representation Decoupling", "content": "Previous decoupling models, which rely on complex structural design and parameter learning, exhibit poor adaptability when incorporated with further collaborative feature learning. Building on the core principles of these decoupling models and accounting for temporal asynchrony across modalities, MMCL introduces a parameter-free approach based on the semantic similarities between cross-modal elements. For each unimodal representation, the CSD module decouples common features by calculating the semantic similarity between each of its temporal elements and those from other modalities. Features with low semantic similarity are then identified as modality-specific. Take the visual representation as an example ('1' of the CSD module in Figure 1 signifies 'v', '2' and '3' denote 'a' and 't', respectively.), its similarities with the text and audio modalities are calculated as follows.\nSim^{va} = \\frac{Z^v Z^a}{|| Z^v|| || Z^a||},\nSim^{vt} = \\frac{Z^v Z^t}{|| Z^v|| || Z^t||},\nwhere Sim^{va}, Sim^{vt} \u2208 R^{L\u00d7L}. Elements in Sim^{va} and Sim^{vt} represent the cosine similarities between corresponding cross-modal elements. Elements exhibiting high similarity suggest a strong likelihood of being common across both modalities. To extend the similarities from two to three or more modalities, the CSD module incorporates a function F_{us} to integrate similarity matrices.\n(W^v)_{ij} = F_{us}(Sim^{va}, Sim^{vt}); \u2200i \u2208 [0, L-1], \u2200j \u2208 [0, L-1];\nwhere F_{us} is the comparison function that takes the smaller similarity score and can be expressed as follows.\nF_{us}(a,b) = \n\\begin{cases}\na, & \\text{if } a \u2264 b;\\\\\nb, & \\text{otherwise;}\n\\end{cases}\nIn the CSD module, representations that show significant relevance across all modalities are treated as common through F_{us}. Otherwise, they are marked as specific. In the experimental section, we also examine other comparison functions to explore broader common and specific features. Based on the similarity matrix W^v, the common features Z^v_c from the visual modality can be obtained as follows.\nZ^v_c = W^v Z^v.\nThe matrix reflecting the discrepancy between cross-modal elements is calculated as W = 1 - W^v. Based on the difference matrix W, the specific features can be obtained.\nZ^v_s = W Z^v.\nDuring common and specific feature calculations, the sum of elements per row in W^v and W is limited to 1. The CSD module is also performed on text and audio modalities to obtain the decoupled representations, respectively."}, {"title": "C. Crucial Common Feature Enhancement", "content": "The common representations are shared across all modalities. For example, rhythm can be simultaneously depicted through visual cues like lip movements and acoustic signals such as sound waves. Common representations may express repeated content between modalities, impacting the efficiency of information processing. To mitigate this issue, we highlight"}, {"title": "D. Complementary Specific Feature Mining", "content": "Individuals' sentiments are time-varying, and the semantics conveyed by specific features from various modalities change at different moments. Dynamically adjusting the importance of these features enables effective utilization of the complementary strengths across modalities. For instance, audio features may dominate during moments of subdued tone, while visual features often provide richer cues during intense segments. Unfortunately, current modality disentanglement methods directly integrate specific features into a joint multimodal representation, presuming equal importance for all modalities at all time [16], [25]\u2013[27]. This assumption risks obscuring critical emotional signals and limits the effective exploitation of modalities' complementary nature. To better utilize complementary features, we propose collaboratively learning the temporal importance of specific features but lack efficient supervised signals. In reinforcement learning, adaptive policy adjustments are facilitated by feedback from professional rewards without prior knowledge of optimal policies [43]. Inspired by this act-reward mechanism in reinforcement learning, we design modality-specific policy models to explore the temporal importance of features. By incorporating shared rewards, our method jointly optimizes policies to ensure each learned feature contributes valuable information during fusion.\nAs shown in Figure 1, we assign an independent policy model to each specific representation Z_s^v, taking actions in mine features Z_s^v from it. To ensure that the mined features are complementary across modalities, we facilitate collaborative interaction among multiple specific representations Z using a centralized critic model. This centralized critic model gathers observed representations and actions from all modalities to evaluate the policy models from a global perspective. Based on this global evaluation, the policy models are optimized jointly, enabling the adaptive capture of complementary features.\nFormally, each policy model takes observed specific representation Z_s^v as input and outputs action A^v according to current policy \u03c0^v.\nA^v = F_{\u03c0^v} (Z_s^v; \u03b8_{\u03c0^v})\nwhere \u03b8_{\u03c0^v} is the parameter of the policy model F_{\u03c0^v}. A^v \u2208 R^L identifies complementary clues within Z_s^v. In practice, the policy model is implemented as a fully connected layer. After all policy models execute Eq. 7, the actions and observed representations are sent to the centralized critic model to obtain a global evaluation Q.\nQ = F_{critic} (Z^v_s, A^v, Z^a_s, A^a, Z^t_s, A^t; \u03b8_{critic})\nThe centralized critic model evaluates the rationality of policies by referring to the observed specific representations. It also takes into account the long-term impact of actions on the task, resulting in Q being the cumulative reward. Specifically, the critic model is implemented as an 8-head transformer that takes the concatenation of representations and actions as input. The critic model is optimized with the Temporal-Difference (TD) Error algorithm [43], [44]: L_{critic} = Q - Q'. Q' represents the ground truth of the cumulative reward and is calculated as Q' = R + \u03b3Q'', Q'' is the cumulative reward in the next stage. R denotes the immediate reward, and \u03b3 is the discount factor.\nJoint Policy Adjustment. Benefitting from the centralized critic model, each policy model strives to take action A^v in order to jointly maximize the cumulative reward. Thus, the objective function for policy models is L_{policy} = -Q. Take the vision modality as an example, its policy \u03c0^v is adjusted as follows.\n\u2207_{\u03b8_{\u03c0^v}} L_{policy} = -\u2207_{A^v} Q \u22c5 \u2207_{\u03b8_{\u03c0^v}} A^v\nwhere Q and A^v are obtained from Eq. 8 and Eq. 7, respectively. Clearly, observed specific representations and actions from audio and text modalities have explicitly contributed to adjusting the visual policy through Q. Both audio and text modalities update their policies in similar ways, suggesting that these modalities provide references for learning complementary features within each modality. The joint policy adjustment mechanism allows learned features to play compensating roles across modalities. Notably, the centralized critic model will be removed after training, leaving only the policy models for inference.\nReward. At each stage, we get complementary features by combining observed representations and actions: Z^v = Z^v_s \u00d7 A^v. Complementary features are further integrated with enhanced ones to form the joint representation: Z = [Z_c^v, Z^v] * [Z_c^a, Z^a] * [Z_c^t, Z^t], where [,] denotes concatenation, and * signifies the modality fusion operation. The multimodal representation Z is then applied to downstream tasks to feed back a joint reward R for optimizing policy and critic models. R varies with tasks. For regression tasks, R = \u2212 |Y' \u2013 Y|, where Y' and Y are true and predicted sentiment states, respectively. For classification tasks, R = \\sum_{i=1}^{C} Y_{Y_i}, where Y_{Y_i} is the predicted probability for the true class, and C is the total number of categories. The calculation of rewards relies on both policy-critic and prediction modules. So, except for the loss from policy and critic modules, MMCL also involves a prediction loss L_P. The objective function of MMCL can be expressed as follows.\nL = \u03b1_1 L_P + \u03b1_2(L_{policy} + L_{critic})\nwhere \u03b1_1 and \u03b1_2 are weights for the prediction and policy-critic modules, respectively."}, {"title": "IV. EXPERIMENT", "content": "We conduct extensive experiments on two Multimodal Sentiment Analysis (MSA), a Multimodal Emotion Recognition (MER), and a Multimodal Depression Assessment (MDA) benchmarks.\nCMU-MOSI [17] and CMU-MOSEI [45] are widely used for MSA, collected from online sharing websites. The MOSI database contains 1281, 229, and 685 utterance-level sequences for training, validation, and testing, respectively. Each sentence is annotated with the sentiment scores ranging from -3(strongly negative) to +3(strongly positive). The MO-SEI dataset comprises 16,265 utterances for training, 1,869 for validation, and 4,643 for testing.\nIEMOCAP [46] is a laboratory-collected MER database. It has about 10K utterances labeled with nine emotions: angry, happy, sad, neutral, surprised, fearful, excited, frustrated, and other. Following the popular work [14], we take the first four emotions for experiments.\nCMDC [47] is a popular MDA database that includes 78 subjects, each responding to 12 questions. 45 subjects are both audio and video recorded. The subjects' depression levels are evaluated using PHQ-9 scores [48], which range from 0 to 27 in total. We perform five-fold cross-validation on the CMDC database..\nFor the MSA task, we evaluate MMCL with (1) MAE: mean absolute error; (2) Corr: correlation between predictions and ground truth; (3) Acc2: binary accuracy, samples to be positive if its sentiment value is greater than 0, and vice"}, {"title": "B. Implemented Details", "content": "Following popular works [14], [47], [49]\u2013[52], the pre-trained BERT model [53] is used for extracting 768-dimensional text features for MOSI and MOSEI databases. For the IEMOCAP database, we utilize 768-dimensional Glove features to represent text data. For audio data, COVAREP is used to extract 12-Mel frequency cepstral coefficients, pitch tracking, spectral envelope, and so on, resulting in 74-dimensional representations. Visual data is processed with Facet to capture action units, facial landmarks, head pose, etc., forming 47-dimensional, 35-dimensional, and 35-dimensional features for the MOSI, MOSEI, and IEMOCAP databases, respectively. For the CMDC database, we use the provided 768-dimensional BERT, 768-dimensional TimesFormer [54], and 128-dimensional Viggish features [55] for three modalities. We map three modalities into a 256-dimensional subspace. Without loss of generality, we merge three modalities through weighted summation and concatenation, respectively. The discounted factor \u03b3 is set as 0.5. For MSA and MDA"}, {"title": "C. Ablation Study", "content": "We conduct comprehensive experiments on the MOSI and IEMOCAP benchmarks to provide a thorough analysis of MMCL. These experiments include the ablation study on each module, analysis of various comparison functions F_{us}, examination of the role of enhanced and complementary features, as well as assessment of the importance of each modality. Besides, we also present the performance under different hyper-parameters and visualized representation comparisons.\nAblation studies on Hyper-parameters. MMCL contains objective functions for the prediction and policy-critic modules, weighted by \u03b1_1 and \u03b1_2, respectively. To create a more rigorous experimental setup, we conduct extensive experiments across a wide range of values for two weights. Figure 2 shows the weights and their corresponding performance. MMCL achieves the optimal MAE for the MOSI database when \u03b1\u2081 = 15, \u03b1\u2082 = 5. Weights for the MOSEI and CMDC databases align with those used for MOSI. For the IEMOCAP database, MMCL reaches the best accuracy with \u03b1\u2081 = 7, \u03b1\u2082 = 13.\nImportance of Modules. In Table I, we analyze the importance of each module by removing them one at a time. First, we eliminate the decoupling module (CSD) and proceed with enhanced and complementary feature learning on the raw input. Without the decoupling module, the subsequent operations fail to perform effectively, resulting in a notable drop in performance compared to MMCL. Next, we remove the enhanced feature learning module (CCE) and directly integrate the decoupled common features into the joint representation, which leads to a slight decrease in performance. Finally, we"}, {"title": "Evaluation on Complementary Features", "content": "We calculate the information gain rate g [61] for specific and complementary features to investigate cross-modal complementarity. First,"}, {"title": "D. Comparison With State-of-the-Art Methods", "content": "We compare MMCL with some state-of-the-art models, including TFN [32], LMF [57], MulT [14], MAG [49], MISA [15], TFR-Net [58], HyCon [59], MCL [51], MHE [60], CMHFM [36], CRNet [27], TMBL [23], UniMF [62], TLRF [63], DTN [16]. For fair comparisons, we compare BERT-based models for MSA and MDA, Glove-based models for MER, respectively.\nMultimodal Sentiment Analysis. Comparative results on the MOSI and MOSEI benchmarks are reported in Table II. Compared to the recent DTN and popular MISA, which disentangles representations via adversarial learning and subspace learning, respectively, MMCL obtains common and specific features through the simple decoupling process and further"}, {"title": "V. CONCLUSION", "content": "In this paper, we capture enhanced and complementary collaborative features among modalities to analyze sentiment states. For collaborative sentimental feature learning, the proposed MMCL model first decouples unimodal representations into common and specific components, and then learns enhanced and complementary properties from these decoupled features. Our main contributions include highlighting the distinction between collaborative attributes and decoupled representations, as well as implementing adaptive complementary feature mining using the rewarding mechanism in reinforcement learning. Experimental evaluations of the information gain rate demonstrate that MMCL successfully mines complementary properties from specific representations."}]}