{"title": "A Layered Architecture for Developing and Enhancing Capabilities in Large Language Model-based Software Systems", "authors": ["Dawen Zhang", "Xiwei Xu", "Chen Wang", "Zhenchang Xing", "Robert Mao"], "abstract": "Significant efforts has been made to expand the use of Large Language Models (LLMs) beyond basic language tasks. While the generalizability and versatility of LLMs have enabled widespread adoption, evolving demands in application development often exceed their native capabilities. Meeting these demands may involve a diverse set of methods, such as enhancing creativity through either inference temperature adjustments or creativity-provoking prompts. Selecting the right approach is critical, as different methods lead to trade-offs in engineering complexity, scalability, and operational costs. This paper introduces a layered architecture that organizes LLM software system development into distinct layers, each characterized by specific attributes. By aligning capabilities with these layers, the framework encourages the systematic implementation of capabilities in effective and efficient ways that ultimately supports desired functionalities and qualities. Through practical case studies, we illustrate the utility of the framework. This work offers developers actionable insights for selecting suitable technologies in LLM-based software system development, promoting robustness and scalability.", "sections": [{"title": "1 Introduction", "content": "The use of Large Language Models (LLMs) has expanded beyond traditional language-related tasks such as translation and question answering. This widespread adoption is largely driven by the generalizability and versatility of LLMs, which stem from being trained on vast amounts of diverse data sourced from the internet and human annotations, allowing them to capture patterns across many domains. Furthermore, LLMs use text as a flexible input/output interface, which makes interacting with them intuitive and adaptable to various contexts. Combined with advances in techniques that improve their ability to follow instructions and align with specific needs, LLMs have been increasingly applied to a variety of domain applications requiring flexibility and scalability, including software development [1-3], process automation [4, 5], financial analysis [6, 7], manufacturing [8,9], education [10,11], and scientific research [12-14].\nHowever, despite their remarkable strengths, LLMs have clear limitations. For instance, even when trained on vast datasets, they often struggle with domain-specific knowledge and lack the specialized expertise needed for certain tasks [15]. Additionally, LLMs are prone to generating plausible-sounding but factually incorrect outputs [16], commonly referred to as \u201challucination.\" Since LLMs are primarily trained on the NLP task of next word prediction based on statistical probabilities [17], they may be less reliable and efficient than simpler but specialized tools for tasks that require a deeper understanding or internal task representation [18]. Furthermore, due to their language processing nature, they also interact with the external world exclusively through the interface of text, limiting effective access to the diverse interfaces of external tools or systems [19]."}, {"title": "2 Motivating Scenarios", "content": "In this section, we present two motivating scenarios in which LLM-based application developers are likely to encounter the challenges addressed by our approach. While established solutions may exist for these scenarios, we aim to highlight the lack of a structured design process and conceptual framework. These hurdles, though partially addressed in some cases, continue to create gaps in other emerging scenarios that require careful design considerations."}, {"title": "2.1 Application Scenario: Integrating Additional Data", "content": "A key scenario in developing LLM-based applications is deciding how to integrate additional data, such as domain-specific knowledge. In general, this can be achieved through two main approaches: integrating the information as parameterized knowledge or integrating it as non-parameterized knowledge. Parameterized knowledge involves embedding the information directly into the model through training or fine-tuning, allowing the model to internalize the information. Non-parameterized knowledge supplies external information dynamically, typically through prompts, during inference, or post-generation, and is commonly referred to as Retrieval Augmented Generation (RAG) [15].\nWhen determining between parameterized knowledge and non-parameterized knowledge, while there are various factors that need to be considered, the primary difference between these two methods is whether the information is parameterized. In the case of parameterized knowledge, the model learns this information into its internal representations, making it readily available during inference. This brings a number of characteristics, for instance:\n\u2022 Minimal latency - As the information is already learned into parameters of LLMs, using these knowledge does not trigger retrieval from separate sources, and does not need information to be explicitly contained and processed as part of the input. This introduces minimal latency at inference time.\n\u2022 Internal representations of information - Information is learned into the model's parameters, allowing it to form and store complex associations, relations and patterns within its internal representation. This integration enables the model to utilize the knowledge cohesively, drawing connections across tokens and adapting responses based on these embedded information.\n\u2022 Resource-intensive at training - Integrating knowledge through training or fine-tuning can be resource-intensive, requiring significant labour, computational and time investment upfront.\nIn contrast, non-parameterized knowledge allows for flexible information integration at the inference stage by embedding relevant external information as context within the input provided to the large language model. This approach brings its own set of characteristics, such as:\n\u2022 Flexibility - Integrating information through non-parameterized approaches allows knowledge to be continuously added or updated independently of specific models and to be seamlessly transferred and utilized by any model without being tightly coupled to one.\n\u2022 Additional input overheads - To supply external information, non-parameterized methods provide additional context to models within the input prompt, which consumes token space and may introduce computational overheads at inference.\n\u2022 Surface utilization of information - Since the information is provided solely through prompts as external context and is not embedded within the model's parameters, it is processed based on"}, {"title": "2.2 Application Scenario: Constraining Output Structure", "content": "Another typical scenario is constraining the output structure of the language model. There are already relatively mature solutions, such as structured output based on constrained decoding, widely offered by commercial LLM vendors and the open-source community. However, these solutions were not available at the initial onset of demand, but instead, they went through an evolution process.\nAs LLMs gained popularity, users began to use them not only as chatbots but also to explore their potential as software components for executing tasks. These LLM-based components are connected with other components primarily through their text-based interfaces. However, since majority of non-LLM-based components may not be capable of processing natural language inputs, it becomes essential for LLMs to generate structured outputs that can be parsed and processed by other components. However, LLMs acquire their generalist capabilities during pre-training by learning the representations of vast unstructured textual knowledge from diverse sources in a probabilistic manner, which makes them natively incapable to produce stable structured outputs.\nIn order to connect LLMs with other components, developers attempted the challenge from various perspectives. Initially, the most intuitive method of trial and error was leveraged. Such attempt has led to sophisticated parsers in companion with carefully crafted prompts, but a structured output from LLMs was still not guaranteed. For instance, in the widely used LLM application framework LangChain, several structured output parsers, including StructuredOutputParser,\nListOutputParser and CommaSeparatedListOutputParser, were introduced\u00b9, to parse the outputs generated by the LLMs. The LLMs were expected to generate outputs based on text prompts that specify the desired output formats. A while after that, a new commit2 introduced additional trial-and-error-based output parsers, including RetryOutputParser, RetryWithErrorOutputParser and OutputFixingParser, to complement the structured output parsers. Since then, sophisticated retry, reflection, and fixing mechanisms have been integrated into the codebase, expanding it from initial 80 lines of code (LoC) to over a thousand LoC.\nFrom a language model training perspective, generating outputs in specific formats has long been a traditional focus in the field. Fine-tuning has been an effective method for enforcing specific output formats in LLMs [23] by adapting the models with the datasets that exemplify the desired structures. As the demand for structured outputs increased, fine-tuning was offered by the commercial LLM vendors and the open source community explicitly as a means to reliable structured outputs, such as in OpenAI's GPT-3.5 Turbo fine-tuning API release. 3. However, one significant challenge in this scenario is retaining the generalist capabilities of LLMs from the risk of catastrophic forgetting [24]. Furthermore, maintaining the flexibility of not requiring exhaustive fine-tuning across all possible output formats makes it more complex.\nAnother crucial stage where the output format can be enforced is the inference time. From a text generation perspective, the LLM decoding process selects a sequence of tokens based on the probability distribution for each subsequent token. By interpreting the output format schema as context-free grammars [25], the candidate token space can be constrained through logits manipulation, assigning the lowest probabilities to tokens that would violate the specified format [26], a technique known as constrained decoding. Since the probability of generated tokens is directly controlled by the formal representation of the expected output format, this approach ensures that only outputs strictly aligned with the specified format are generated. This method has been integrated into commercial models such as Claude 34 and GPT-405, and open-source libraries like HuggingFace Transformers6 and llama.cpp7. The core implementation of this method used in the Hugging Face Transformers library takes around 200 LoC8.\nAt the current stage of this evolution, LLM-based applications utilize a combination of three technologies, with inference-time constrained decoding positioned at the center for consistency and reliability. Training and fine-tuning, along with format-instructive prompts, assist the model in better understanding the schemas, ensuring that the outputs are both syntactically correct and semantically meaningful. Additionally, the retry and fixing parsers provides an extra layer of assurance that the outputs are of expected formats."}, {"title": "2.3 Motivation and Scope", "content": "From these two scenarios, we observe that implementing a specific capability within LLM-based applications can lead to various technological paths, with the suitability of each technology dependent on the specific requirements of the scenario. Identifying these technological paths may require significant engineering efforts, and incur time and resources cost on it. Moreover, these technological paths may operate at different layers. For example, in the constraining output structure scenario, fine-tuning targets the knowledge learned into the model parameters, while constrained decoding focuses on the token selection process, and prompt-based approaches address non-parameterized inputs to the model. Although these different approaches can be employed to implement the same capability, they may occur at distinct decoupled layers of a LLM-based application, leading to significantly different expectations, outcomes, and implications for developers. Consequently, it is essential for developers to possess a systematic understanding of the software architecture of LLM-based applications and to reinforce software engineering principles to make informed architectural decisions.\nTherefore, the motivation behind this work is to propose an approach which serves to map anticipated capabilities to a potential technology stack for implementation, even in the absence of specific existing or known solutions."}, {"title": "3 The Approach", "content": null}, {"title": "3.1 Glossary", "content": "To enhance clarity for the reader, we define two key terms used in this approach."}, {"title": "3.2 Principles of the Approach", "content": "\"It is the mapping of a system's functionality onto software structures that determines the architecture's support for qualities.\u201d\nSoftware Architecture in Practice [31]\nWe extend this principle to the software architecture for LLM-based application by mapping capabilities onto the appropriate layers and components with corresponding attributes.\nLayers and components are defined following the principle of separation of concerns. We present the layers, components and their attributes with additional properties in Section 3.3. Using this architecture, we illustrate the process of how to map desired capabilities onto the appropriate structures in Section 3.4."}, {"title": "3.3 Layers", "content": "Inspired by layered software architectures such as n-tier architecture [32] and layered Internet protocol stack [33], we break an LLM-based application into layers based on the nature of relevant development activities, including Model Layer, Inference Layer, and Application Layer. For each layer in the architecture, we examine the nature of the components and their roles in supporting specific attributes. The overall structure is shown in Fig 1."}, {"title": "3.3.1 Model Layer", "content": "The Model Layer involves the foundational aspects of building an LLM, from the data collection to the architecture design and training process. The output of this layer is the large language model, which is a machine learning model typically built upon transformer or a variant architecture trained using data on natural language processing (NLP) tasks such as causal language modeling (next-token prediction). Through this process, the model learns a statistical representation of language from the data, and embeds in its parameters. We divide the Model Layer into three core components, i.e. Data, Model Architecture, and Training.\nData Data used for training and fine-tuning LLMs goes through a process of selection, collection, and preprocessing, serving as the source from which LLMs learn the representations. In state-of-the-art (SOTA) LLMs, training data is typically collected from diverse sources, including publicly available internet content, inputs from human data workers, knowledge obtained from domain experts, user interactions and feedback, and distilled data from the output of LLMs [34-36].\nThe quality and scope of the training data directly determine what information is embedded within the internal representation of LLMs and thus subsequently set the boundaries of model's knowledge and capabilities. Beyond learning factual knowledge from the training data, LLMs also capture the patterns, relationships, and biases present in the training data, all of which contribute to shaping the model's behaviors and responses. Moreover, these boundaries, behaviors, and responses are not fixed, as LLMs can be further adapted and refined through continual pre-training or fine-tuning, where additional data is used to update the model's internal representations.\nModel Architecture The Model Architecture defines the structure and underlying mechanisms of LLMs. The vast majority of LLMs are built on the transformer architecture, which leverage a self-attention mechanism that allows the model to capture long-range dependencies and contextual relationships between tokens in a sequence [37]. This makes transformers effective for a wide range of NLP tasks, from translation and summarization to text generation.\nThe capability of model is largely tied to the scale of model, often measured by the number of parameters. The scale of model is a critical factor that may lead to the emergence of model abilities [28]. Larger models have shown emergent abilities that are absent in smaller models, such as few-shot learning. These abilities are likely a result of the higher number of parameters leading to increased capacity to capture complex patterns and relationships from the training data [22].\nHowever, this comes with trade-offs in terms of computational cost and resource requirements. Larger models demand significant computational resources at training and inference, including more performant computing units and larger memory, impacting the latency and cost of the model. To address these challenges, some LLMs utilize a Mixture of Experts (MoE) architecture [38, 39], which offers a balance between the scale and efficiency. MoE models consist of multiple expert sub-networks, with only a subset of experts activated for each input, reducing the computational overhead of the inference. In contrast, smaller models, though less capable, are faster and more efficient, making them more feasible to be used for real-time applications or deployed on edge devices.\nTraining Training is the process where model learns from the data and embeds what it learns into parameters. While the dominant approach remains next-token prediction, various training objectives and fine-tuning techniques have been developed to address specific needs and enhance performance across different applications.\nNext-token prediction is a widely adopted training objective for LLMs, where the model learns to predict the next token in a sequence by estimating the probability distribution of tokens based on the preceding context. This task, also known as causal language modeling, aligns well with autoregressive models including GPT [40], allowing model to generate fluent and coherent text. Another text generation task is masked language modeling, adopted by BERT [41], trains model to fill in the random blanks within a sequence of tokens. This bidirectional context, where the model makes use of both preceding and following tokens, enables its ability to understand the full sentence structure and semantic relationships but at the same time also makes it less fit for sequential text generation.\nIf certain information or patterns are absent in the pre-training dataset, the model can be fine-tuned on task-specific data to refine its knowledge base and behavior. As the demand for using LLMS for specific tasks has increased [42], methods like instruction tuning, direct preference optimization (DPO), and reinforcement learning from human feedback (RLHF) have become popular to align models with human preferences and improve their ability to follow instructions [43-45]. These models are fine-tuned on crafted dataset or reward functions so that the patterns and human preferences are captured into parameters of models. These fune-tuning methods can also be used for specialized purposes, such as setting rules for LLMs [46] or enabling models to autonomously call tools [47]. The objective of next-token prediction can be further adapted by combining with techniques like zero-shot learning, chain-of-thought (CoT) [48], Monte-Carlo tree search (MCTS) [49] and Reinforcement Learning. For instance, Quiet-STaR [50] enhances reasoning by guiding models to \"think before speaking\" using intermediate rationale tokens before predicting next token to improve capabilities for reasoning tasks."}, {"title": "3.3.2 Inference Layer", "content": "The Inference Layer utilizes the model's learned representations to generate responses, a process known as decoding. In transformer architecture, the attention mechanism is involved to determine the most relevant context when forming the probability distribution of the next token [37]. In each decoding step, the model produces a probability distribution over all tokens. This distribution is generated by applying the softmax function to the logits derived from the model's final hidden state, and the resulting probability distribution serves as the basis for selecting the next token in the sequence.\nThe key objective in inference layer is deciding how tokens are chosen from the logits. The na\u00efve way is to select the token with the highest probability, known as greedy sampling. While efficient, greedy sampling makes the generated sequence too rigid and deterministic. To make the output diverse, methods like top-k [52] and top-p [53] samplings consider a subset of candidate tokens based on their ranks or cumulative probabilities. Beam search-based decoding uses a heuristic approach of sampling multiple candidate sequences and select the tokens based on the overall probability of the sequence [54]. Applied on the logits before produce probability distribution with softmax, temperature scaling manipulate the logits by scaling it with a temperature value, so that lower temperatures make the probability distribution sharper, leading to more deterministic outputs, while higher temperatures flatten the distribution, making the selection more stochastic. These strategies represent macro-level approaches to token selection.\nAs the inference layer controls the overall process of token generation, it can also exercise micro-level control, managing the generation in a fine-grained manner. This involves influencing not just the global token selection strategies, but also making more crafted adjustments during the decoding process to steer the output. As decoding involves processing the logits, logits manipulation makes it possible to directly boost or suppress certain tokens, enforce specific semantic patterns across the sequence, or promote or eliminate the generation of particular tokens or sequences. This also enables constrained decoding [26], where the probability of tokens violating predefined constraints is set to zero, ensuring that the generated output strictly adheres to the desired structure or format.\nIn addition to controlling the tokens generated, a primary concern in the Inference Layer is how to generate tokens efficiently. Efficient decoding focuses on minimizing latency, reducing computational overhead, and optimizing resource usage while maintaining the quality of generated outputs. One way to achieve that is speculative decoding, which uses a smaller assistant model alongside the main LLM during inference [55-57]. The assistant model quickly drafts preliminary candidate tokens, which the primary model can then accept or correct based on the logits from a single forward pass. Other methods may involve lower level customization, including paging the K-V cache [58], efficient attention mechanism [59], precision reduction [60], and parallelism [61]."}, {"title": "3.3.3 Application Layer", "content": "The Application Layer translates LLM's text generation power into functionalities, bridging the gap between the raw input/output text generation paradigm of LLMs and fully functional applications. From a conceptual viewpoint, the Application Layer centers on four key components: Prompt Engineering, Mechanism Engineering, Tooling, and Orchestration. While additional components exist in LLM-based applications, such as user interface (UI) and application hosting, these aspects closely align with traditional software development practices and are not unique to LLM-based applications.\nPrompt Engineering Prompt Engineering focuses on designing prompts that guide the LLM to produce the desired outputs. The practice of Prompt Engineering is rooted in the foundations established by the Model Layer, where the model learns its internal representations from training data and is often fine-tuned to follow instructions. However, the probabilistic nature of LLMs remains unchanged. The generated responses are inherently influenced by the learned statistical patterns and the underlying probability distributions. The core principle of Prompt Engineering is to steer the model's probability distribution, amplifying the likelihood of generating the intended response [62].\nOne effective strategy in Prompt Engineering is to reduce the ambiguity of the query. Vague query descriptions tend to produce inconsistent outputs, as they are less likely to align with the specific patterns the model has learned during training. By providing clear and explicit instructions, the model's focus can be narrowed, guiding it toward a more concentrated area of its learned representation.\nAnother effective strategy is to supply additional guiding tokens directly within the prompt or to trigger the model to generate them before providing the final response. This helps steer the probability distribution towards the desired outcome by establishing a clearer context or guiding the model's reasoning process [22]. The approach leverages patterns learned during training, where the model has encountered sequences involving intermediate reasoning steps, explanations, and structured processes. Since the inference process considers all previously generated tokens, this method enhances the likelihood of producing structurally coherent, contextually relevant, and logically consistent outputs, better aligning with specific task requirements.\nExamples of these methods include zero-shot learning methods such as role playing [63], step-by-step prompting [64], and explain-then-answer prompting [65], and few-shot learning methods such as the vanilla few-shot prompting [66] and chain-of-thought prompting [48]. Additionally, prompt optimization can be achieved through manual or automated tuning methods [62].\nMechanism Engineering Mechanism Engineering [5] involves integrating LLM's text generation into modular mechanisms, that employ structured approaches that guide the module's outputs towards achieving broader objectives or solving complex problems. Mechanism Engineering designs abstract processes that leverage the model's generative abilities in a systematic way. These mechanisms typically consist of multiple individual text generation processes, each serving orthogonal or complementary objectives aimed at reaching a specific goal. Different generation processes are often connected through symbolic or rule-based programs, represented using certain structures, such as directed acyclic graph (DAG).\nWhile LLMs generate tokens in an autoregressive and linear manner, this generation process inherently lacks explicit representations for iteration, recursion, branching, and conditionals. Mechanism Engineering addresses this limitation by providing an external representation that helps maintain state across different stages of text generation. By integrating symbolic structures and logical controls, it enables the LLM to incorporate external information or runtime state, and perform sophisticated workflows.\nRetrieval-augmented generation (RAG) [15] inject external knowledge into the prompt to provide additional context for knowledge-intensive tasks. The mechanism has evolved by allowing LLM to actively query external information, such as Agentic RAG [67] and Retrieval-Interleaved Generation (RIG) [68].\nThe trial-and-error method involves repeatedly incorporating error information into the input to the LLM, enabling it to improve its responses until the task is successfully accomplished. [5]. The ReAct framework [69] implements an interleaved reasoning and acting mechanism, allowing the model to incrementally execute tasks by alternating between thought processes and actions. Reflexion [70] introduces more sophisticated mechanisms that enable the model to iteratively work on a task, incorporating information from both its trajectory and experience to enhance performance over time. Self-consistency [71] and tree of thoughts (ToT) [72] both adopt multi-path methods to explore the solution space of the task. In self-consistency, multiple paths are generated for a given prompt, and the final solution is determined by the voting mechanism, which selects the most consistent or frequent answer among the generated responses from all paths. In contrast, Tree of Thoughts (ToT) employs the LLM to evaluate intermediate steps along the path.\nTooling Without tools, LLMs are only generators of text, lacking the capacity to perform automation tasks, validate and iteratively refine proposed solutions, or access external information. Since LLMs can only interact with the environment via text-based input and output, they require tools to be integrated and adapted for use through natural language. The LLM itself cannot directly interact with tool interfaces, so it instead relies on intermediary components that interpret the generated text to invoke external tools.\nIn passive tool calling, such as in vanilla Retrieval-Augmented Generation (RAG) [15], the decision to call a tool (i.e., retriever for the case of RAG) is made externally, not by the LLM itself. The result of the tool call is then supplied back to the LLM as part of the prompt context. This approach does not require the LLM to be aware of the existence of tools or understand the interfaces for using them.\nIn active tool calling, the LLM is given a degree of agency, allowing it to decide when and which tools to invoke based on the context of the task. The model can generate structured text (e.g., JSON) that follows specific conventions designed to trigger tool calls directly, or it can describe the tool invocation in natural language. In scenarios where the LLM uses unstructured natural language to specify the tool call, the responsibility for interpreting and executing the tool request is delegated to a specifically fine-tuned LLM, for instance the ToolFormer [47] and DataGemma [68]. This setup allows the primary LLM to maintain flexibility in how it interacts with tools while offloading the responsibility of explicitly conducting tool calls to a specialized component.\nOrchestration The orchestration component manages the chaining of LLMs, integrates tools into the application, and maintains the states and overall process of the application.\nLLM chaining involves linking together multiple LLM calls to form a cohesive process, tackling tasks in a divide-and-conquer manner. Each call builds upon the responses from previous ones, allowing the system to break down complex tasks into smaller, manageable sub-tasks. This enables the application to handle multi-step problems effectively.\nThe orchestration component also determines which tools are included in the application, how they are utilized, and how they are shared across multiple LLM calls. Different tools may have distinct execution behaviors, such as asynchronous processing or requiring specific input formats, which the orchestration component must accommodate.\nStates of the application as well as both short-term and long-term memory are maintained as part of orchestration. It tracks session data, user configurations, previous interactions, and the sequence of LLM calls, ensuring that context is preserved throughout the entire task execution. Additionally, the orchestration component handles errors and exceptions, particularly when the LLM generates unexpected outputs. It includes mechanisms to detect issues, trigger corrective actions, or adjust the process, ensuring a smooth and consistent execution."}, {"title": "3.3.4 Intra-layer and Inter-layer Dependencies", "content": "The layers and components of the framework are not isolated. There are dependencies across and within layers, such that achieving the desired capability may require support from other components within the same layer, i.e., Intra-layer Dependencies, or from components across different layers, i.e., Inter-layer Dependencies.\nIntra-layer Dependencies In the Model Layer, components often depend on one another to work effectively. For instance, certain fine-tuning methods rely heavily on the availability and format of specific data. To perform instruction tuning, the data must be curated in the form of instruction-output pairs. Without this structured data format, the fine-tuning process cannot align the model effectively tune the model to follow instructions.\nIn the Application Layer, Mechanism Engineering has dependencies on both Prompt Engineering and Tooling components. Mechanism Engineering often requires carefully crafted prompts to guide the LLM towards desired responses. Additionally, certain mechanisms rely on the integration of external tools to complete the task.\nInter-layer Dependencies The Inference Layer often relies on components from the Model Layer to achieve optimal performance and efficiency. For example, efficient inference methods, such as running the model in reduced precision, may depend on specific training methods like quantization-aware training conducted in the Model Layer.\nThe Application Layer depends heavily on both the Model and Inference Layers. For instance, Prompt Engineering is influenced by the model architecture, particularly the scale of the model, which determines its ability to perform tasks like few-shot learning. Additionally, the presence of specific representations learned from data is crucial, as effective prompts rely on amplifying the model's probability distribution to guide responses toward desired outputs. The effectiveness of certain prompts or mechanisms, especially those relying on instruction-following, often requires instruction tuning at the Model Layer to align the model's behavior. Moreover, the transferability of prompts across different models depends on characteristics of models from the Model Layer [62].\nIn Application Layer, to build tools effectively, especially when employing active and direct tool calling, the support for structured output generation in the Inference Layer is beneficial, as it helps the LLM produce responses in the correct format required by the tool interfaces."}, {"title": "3.4 Mapping Capabilities onto Layers", "content": "The Capability Mapping aims at aligning specific system capabilities with the attributes and components across the layers of the architecture. We will use the capability of generating JSON output as an example to illustrate this process.\nAttributes Identification The first step in implementing a capability is to identify its relevant attributes across the layers. As demonstrated above, different layers are responsible for distinct attributes of LLM-based applications. Each capability may correspond to one or more layers, depending on its characteristics and requirements. Additionally, some attributes may become depreciating in necessity due to the presence of other attributes.\nSolution Architecture Based on the identified attributes, we can design a solution architecture that aligns solutions with the appropriate components in each layer. It is important to account for Intra-layer and Inter-layer Dependencies. A simplified description for the JSON generation example is shown in Table 5.\nAccess Resolution In cases where certain components in the architecture are gated or restricted, alternative solutions need to be considered.\nIn the JSON output generation example, if the Inference Layer is gated, developers can consider utilizing open-source alternatives. Alternatively, developers can compensate the gated components by enabling fallback mechanisms, such as the exception handling previously considered depreciating, which can be enabled to validate and correct the output format with mechanisms such as trial-and-error.\nHowever, developers must also consider the cost-effectiveness of implementing such sophisticated and depreciating workarounds, especially if there is a strong likelihood that these capabilities will be centrally provided by vendors in future updates, driven by high demand. In these cases, it is important for developers to evaluate their own priorities to decide whether to implement these solutions or wait for vendor support, based on their demands and resource constraints.\nEvaluation While the capability mapping provides a useful framework for guiding the implementation, it cannot replace the need for thorough evaluation of the solutions. It is crucial to conduct comprehensive testing, including ablation studies and continuous monitoring, to assess the effectiveness of each component. By dynamically adjusting the sophistication of each component in response to evaluation feedback, developers can ensure both the effectiveness and efficiency of the implementation."}, {"title": "4 Use Case Evaluation", "content": "In addition to the JSON output generation example, we evaluate the usefulness of the framework with a variety of use cases.\nCreativity Creativity refers to the capability of generating original ideas. While LLMs cannot truly generalize beyond the data they have been trained on, they can produce responses that appear creative by leveraging different aspects across the layers, as demonstrated in Table 6.\nDepend on the type of creativity needed, different solutions across layers can be selected or combined for desired effect. For example, Microsoft Copilot uses the temperature parameter to control the level of creativity in responses13. In contrast, for more specialized applications like scientific research agents, sophisticated workflows are employed for controlled creativity in creative tasks [73]. However, such complex workflows may be overly resource-intensive for general applications, where users only require a degree of semantic variation, rather than exhaustive creative exploration.\nCall Caching Call Caching refers to the capability of reusing results from previous LLM calls to reduce computational costs and overheads. The system can solutions to achieve it, as shown in Table 7.\nLong Context ChatGPT was only accepting 4096 tokens at the initial released and was insufficient for advanced usage such as codebase analysis. A larger context window enables the model to capture rich context or handle long documents. The solution at each layer is shown in Table 8."}, {"title": "5 Related Work", "content": "Several studies have examined the design and architectural considerations of LLM-based systems. Zhou et al. [77] present a decision model for foundation model-based agents, analyzing trade-offs between various options for each type of architectural element. Lu et al. [78] propose a layered reference architecture for foundation model-based systems, emphasizing a pattern-oriented approach that focuses on component orchestration. Liu et al. [79] compile a comprehensive design patterns catalogue for foundation model-based agents.\nIn specific areas, Shamsujjoha et al. [80] provide a taxonomy focused on runtime guardrails for LLM agents, exploring a set of guardrail options at each level of system. Wang et al. [5] introduce the concept of Mechanism Engineering, surveying relevant methods from the perspective of design mechanisms and applications of autonomous agent. Liu et al. [62] offer an extensive survey on various prompting strategies for LLMs as well as their relevant enabling methods required during pre-training and inference. Welleck et al. [81] explore inference-time algorithms, highlighting various objectives of decoding strategies. Zhang et al. [43] provide a detailed survey on instruction tuning, discussing aspects including dataset preparation, tuning methods, and efficient tuning techniques.\nWhile these works provide valuable insights into specific aspects, problems, or a particular layer of LLM-based applications, they address isolated components without a unified perspective. In contrast, our work fills this gap by offering a holistic layered approach that systematically examines the characteristics of each layer within LLM-based applications and guide the development of capabilities across these layers."}, {"title": "6 Conclusion", "content": "This paper presents a layered approach for implementing capabilities in LLM-based applications, decoupling the system into three distinct layers: Model Layer, Inference Layer, and Application Layer. Each layer is assigned specific attributes based on the characteristics of layers and their components. By aligning capabilities with relevant attributes across layers, this approach enables developers to systematically identify implementation strategies.\nThe proposed framework was evaluated against several typical use cases, demonstrating its usefulness and feasibility in guiding the design of LLM-based applications. Our work bridges the gap between high-level capability requirements and the underlying architectural components, offering a holistic strategy accommodating the development of capabilities in LLM-based applications."}]}