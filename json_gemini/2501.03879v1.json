{"title": "CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds Ratio on High-Resolution Point Clouds", "authors": ["Keonwoo Kim", "Yeongjae Cho", "Taebaek Hwang", "Minsoo Jo", "Sangdo Han"], "abstract": "Recent research has demonstrated that Large Language Models (LLMs) are not limited to text-only tasks but can also function as multimodal models across various modalities, including audio, images, and videos. In particular, research on 3D Large Multimodal Models (3D LMMs) is making notable strides, driven by the potential of processing higher-dimensional data like point clouds. However, upon closer examination, we find that the visual and textual content within each sample of existing training datasets lacks both high informational granularity and clarity, which serve as a bottleneck for precise cross-modal understanding. To address these issues, we propose CL3DOR, Contrastive Learning for 3D large multimodal models via Odds ratio on high-Resolution point clouds, designed to ensure greater specificity and clarity in both visual and textual content. Specifically, we increase the density of point clouds per object and construct informative hard negative responses in the training dataset to penalize unwanted responses. To leverage hard negative responses, we incorporate the odds ratio as an auxiliary term for contrastive learning into the conventional language modeling loss. CL3DOR achieves state-of-the-art performance in 3D scene understanding and reasoning benchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key components through extensive experiments.", "sections": [{"title": "1. Introduction", "content": "Humans primarily understand and reason about the world based on vision and language. To emulate the human approach, recent research has focused on Large Multimodal Models (LMMs), particularly 2D LMMs (Bai et al., 2023; Liu et al., 2024c; Jian et al., 2024; Reid et al., 2024), which inject 2D vision features into the language model space to create general-purpose assistants. However, relying solely on 2D visual information poses significant limitations in accurately interpreting the 3D world. Since reasoning and understanding based on 3D data offer a higher-dimensional perspective, recent research has increasingly focused on 3D LMMs (Xu et al., 2023; Zhou et al., 2024b; Huang et al., 2023; 2024; Qi et al., 2024) that integrate 3D vision features into the language model space to improve the comprehension of 3D data and language instructions.\nRecently, most 3D LMMs have been developed with a point encoder for processing point clouds, a pre-trained large language model (LLM) (Jiang et al., 2023; Touvron et al., 2023; Team et al., 2024) for handling instructions, and projection layers that bridge the feature spaces of the point encoder and the LLM. In 3D LMM-related tasks, ensuring high performance is crucial, as performance degradation can lead to significant risks, including physical accidents when such models are deployed in real-world applications involving devices that exert physical force, such as robots (Patil et al., 2023; Sharkawy & Koustoumpardis, 2022). However, we find that existing 3D LMMs often fall short in spatial understanding due to the quality of the training data. In line with recent studies (Chen et al., 2024a; Zhou et al., 2024a), we define high-quality training data as follows: high informational granularity and clarity within each sample, rather than merely the quantity of data, both of which are essential for effective instruction tuning.\nFrom a visual feature perspective, the clarity of objects is often limited by the low-resolution of the data used in training. In 2D LMMs, a significant amount of research has been conducted on high-resolution images to enhance model performance by providing more detailed visual information (Liu et al., 2024a;b; Zhang et al., 2024). However, previous studies on 3D LMMs have predominantly used low-resolution point clouds (Huang et al., 2024; Zhu et al., 2023), which can lack sufficient detail, leading to ambiguous spatial representations. On the other hand, from a textual perspective, the data in existing 3D scene-text datasets often lack variety and detail, both critical for enhancing informational granularity and clarity, due to their construction using rule-based methods (Azuma et al., 2022; Chen et al., 2021). To mitigate the issue, we have focused on recent studies (Zheng et al., 2023; Yan et al., 2024; Sarkar et al., 2024) showing that negative samples help models distinguish more clearly between correct and incorrect responses.\nIn this work, we propose CL3DOR, Contrastive Learning for 3D large multimodal models via Odds ratio on high-Resolution point clouds, a novel approach that addresses quality issues in both visual and textual content data. As illustrated in Figure 1, we ensure greater specificity and clarity through two approaches. For visual features, we enhance the resolution by increasing the density of point clouds per object. For textual features, as shown in Figure 2 (b), we use GPT-40 (OpenAI, 2024) to augment each response in the instruction tuning dataset with plausible hard negatives that the model could easily confuse. To utilize an augmented dataset, we reinterpret the objective function used in preference optimization, ORPO (Hong et al., 2024), from the perspective of contrastive learning by incorporating the odds ratio as an auxiliary component of the negative log-likelihood (NLL) loss, a process we refer to as spatial contrastive instruction tuning.\nApplying spatial contrastive instruction tuning directly to CL3DOR, which uses an LLM as its backbone, poses challenges due to unaligned feature spaces with point clouds. To address the issue, we first conduct two alignment tuning stages for 3D-object and 3D-scene awareness before spatial contrastive instruction tuning. Consequently, as depicted in Figure 2 (a), CL3DOR undergoes a three-stage training paradigm. Such a structured approach enables CL3DOR to fully exploit learnable information and significantly enhances cross-modal understanding by capturing fine-grained distinctions between positive and negative responses.\nTo validate the effectiveness of CL3DOR, we conduct extensive experiments across various benchmarks (Azuma et al., 2022; Ma et al., 2022; Chen et al., 2021), assessing its spatial understanding and reasoning capabilities. The results demonstrate that CL3DOR achieves state-of-the-art performance on most metrics compared to task-adaptive"}, {"title": "2. Related work", "content": "3D Large Multimodal Model With the ongoing evolution of LLMs and 2D LMMs, research on 3D LMMs has significantly increased (Wang et al., 2023; Chen et al., 2024b; Qi et al., 2024; Zhou et al., 2024b; Fu et al., 2024; Guo et al., 2023). 3D-LLM (Hong et al., 2023) extracts 3D features from multi-view images and projects them into a pre-trained 2D LMM space, facilitating tasks such as 3D captioning, and question answering. However, it is constrained by limitations in semantic richness due to the inherent challenges of reconstructing 2D images into 3D features. In response, 3D-VisTA (Zhu et al., 2023) uses self-attention layers for both single-modal modeling and multimodal fusion. Chat-3D v2 (Huang et al., 2023) employs unique object identifiers to enhance object referencing and scene understanding in 3D environments. LEO (Huang et al., 2024) presents a two-stage training model for an embodied multimodal generalist agent, achieving multi-task capabilities without additional fine-tuning. Despite these advancements, previous research has overlooked issues related to dataset quality, including the use of low-resolution point clouds and the inability to fully harness the rich information available in 3D scene data. These shortcomings restrict the models' ability to achieve a comprehensive spatial representation.\nContrastive Learning with Language Generation Contrastive learning focuses on improving feature representation by decreasing the distance between positive pairs and increasing the distance between negative pairs. It has significantly influenced the field of natural language generation, with CoNT (An et al., 2022) aligning encoder and decoder representations for non-open-ended language generation, and BRIO (Liu et al., 2022) using a contrastive loss to ensure the likelihood of sequences aligns with reference text similarity. Additionally, the exploration of contrastive learning related to sequence likelihood has progressed (Jain et al., 2022). Specifically, CLICK (Zheng et al., 2023) applies contrastive learning to sequence likelihood to avoid generating undesirable text, such as toxic language and unnatural repetition, whereas FDPO (Gunjal et al., 2024) and HALVA (Sarkar et al., 2024) use it on sub-sentence level likelihood to mitigate hallucinations in 2D LMMs. To the best of our knowledge, CL3DOR is the first approach to apply contrastive learning to instruction tuning in the 3D LMMs."}, {"title": "3. Methodology", "content": "3.1. Overview\nAs depicted in Figure 3, CL3DOR primarily consists of three components: the 3D scene encoder, the point-language connector, and the pre-trained Large Language Model (LLM). The 3D scene encoder, which includes a pre-trained point cloud encoder and a spatial transformer (Chen et al., 2022), processes segmented 3D scene point clouds with 8,192 points per object for high-resolution. The extracted object-centric features are concatenated, transformed, and passed through the point-language connector, which uses MLP layers to generate object-specific tokens. These tokens, combined with tokenized language instructions, are input into the LLM to generate responses in an auto-regressive manner. Further details are provided in the Appendix."}, {"title": "3.2. Training Paradigm in CL3DOR", "content": "The training process of CL3DOR follows a three-stage paradigm, as outlined in (Huang et al., 2023). We include two alignment tuning stages, beginning with 3D-object awareness to learn about individual objects, and then expanding to 3D-scene awareness to grasp relationships among them. The final stage focuses on spatial contrastive instruction tuning to optimize performance in various downstream tasks. Detailed information about the training datasets used in each stage is provided in Table 1. The likelihood of the output sequence y given the input x can be factorized autoregressively as follows:\n$P_o(y | x) = \\prod_{i=1}^{y} P_o(y^i | x,y^{<i})$,\nwhere $y^{<i}$ represents all tokens preceding $y^{i}$. This factorization forms the basis for the Negative Log-Likelihood (NLL) loss, which is defined as:\n$L_{NLL} = -\\frac{1}{y} \\sum_{i=1}^{y} log P_o (y^i | X_p, x_t, y^{<i}),$\nwhere y, xp, and xt denote the label, 3D scene point clouds, and language instruction, respectively. The first and second stages only use NLL loss, while the third stage incorporates an additional term for contrastive learning.\nStage 1: 3D-Object Alignment Tuning The objective of the first stage is to enhance CL3DOR's understanding of 3D objects represented by point clouds. It is achieved by training the model on the Cap3D dataset (Luo et al., 2024), which is derived from Objaverse (Deitke et al., 2023), and focuses on generating captions for 3D objects. In this stage, only the parameters of the spatial transformer and the point-language connector are updated.\nStage 2: Spatial Alignment Tuning In the second stage, CL3DOR extends its comprehension from individual objects to entire 3D scenes composed of multiple objects, thereby acquiring spatial awareness. The training involves the ReferIt3D dataset (Achlioptas et al., 2020), which emphasizes the description of referred objects. Also, a custom-built scene captioning dataset is used, generated by GPT-40 to produce captions based on multi-view screenshots of 3D scenes from various top-view angles. The trainable parameters remain consistent with those in the first stage.\nStage 3: Spatial Contrastive Instruction Tuning The final stage focuses on general downstream tasks such as 3D-object-in-the-scene captioning, question answering (QA), and grounded QA for object existence within 3D scenes. Unlike the previous stages, the final stage uses a triplet structure in the training dataset. To effectively leverage negative responses in triplet text data, we incorporate an auxiliary term $L_{OR}$, based on the odds calculated from the likelihood of generating the output sequence y given an input sequence x, as shown in Eq. (1). The process involves applying a logarithm to the ratio of the odds for positive (y+) and negative (y-) responses, followed by the sigmoid function \u03c3, as detailed in Eq. (2). Notably, NLL loss is calculated only with the positive response y+. The final objective function is defined as Eq. (3), with \u03bb as a hyper-parameter for the weighted term. During this phase, the trainable parameters include those of the spatial transformer, point-language connector, and LLM.\n$odds_e (y | x) = \\frac{P_o(y | x)}{1 - P_o(y | x)}$                                                        (1)"}, {"title": "3.3. Dataset Refinement for CL3DOR", "content": "High-Resolution Point Clouds Previous studies (Zhu et al., 2023; Huang et al., 2024) typically sample 1,024 points per object for extracting features from 3D scenes. However, as illustrated in Figure 4, a resolution of 1,024 points results in significant information loss, making it challenging even for humans to discern object characteristics. Drawing on the importance of high-resolution visual content in 2D LMMs (Liu et al., 2024a;b; Zhang et al., 2024), we hypothesize that high-resolution point cloud sampling is crucial for accurate visual feature extraction.\nTo address the issue, we employ PointBERT (Yu et al., 2022), a pre-trained point cloud encoder that processes 8,192 points per object to extract detailed 3D object features. The high sampling rate ensures the extraction of higher informational granularity of 3D object features, capturing even the finest details to enhance clarity and provide abundant information. We further conduct an ablation analysis comparing high-resolution and low-resolution sampling. The results, detailed in the Discussion section, highlight the importance of maintaining high point cloud density to reduce information loss and improve object representation fidelity.\nHard Negative Response Generation For effective contrastive learning during the spatial contrastive instruction tuning of CL3DOR, it is essential to use triplet data comprising a question, a positive response, and a negative response. Inspired by prior studies (Robinson et al., 2020; Byun et al., 2022), we incorporate hard negatives that are plausible yet incorrect, enabling the model to learn fine-grained features from a limited dataset efficiently. We devise pipelines that augment existing datasets with informative negatives generated by GPT-40, excluding the 3D object existence task. Examples of the triplet data are shown in Figure 2, and detailed prompts for GPT-40 can be found in the Appendix.\nFor the 3D question-answering task, we generate hard negative responses for the ScanQA (Azuma et al., 2022) and SQA3D (Ma et al., 2022) datasets. These datasets include questions about the presence, location, and attributes of specific objects in a 3D indoor scene. By leveraging a top-view image and a list of scene objects as inputs to GPT-40, we craft contextually rich negatives. It ensures the granularity of the data, as hard negatives force the model to distinguish finer details and reduce ambiguity in responses.\nMoreover, the 3D-object-in-the-scene captioning task generates concise descriptions of specified objects within a 3D scene, using the Scan2Cap (Chen et al., 2021) training dataset. To support contrastive learning, we augment the dataset with hard negative captions crafted by GPT-40. Positive captions accurately describe the relative positions and attributes of the objects, while negative captions maintain the original structure but deliberately alter key elements such as location or attributes. This approach aims to sharpen the model's ability to distinguish subtle yet crucial differences.\nLastly, in the 3D-object existence task, we use ScanNet (Dai et al., 2017) data to create binary questions about object presence in a scene, with responses limited to yes or no. Unlike the tasks that use hard negatives to increase difficulty, the binary answer format limits the complexity of negative responses. To increase the difficulty of the questions, we generate no responses by considering the frequency and co-occurrence of objects in the entire dataset."}, {"title": "4. Experimental Setup", "content": "Implementation details During the spatial contrastive instruction tuning, we set key hyperparameters as follows: one epoch, a learning rate of 6e-5 with a cosine annealing schedule, and a batch size of 64. The X for the OR loss is linearly ramped up from 0 to 3e-1 to ensure stable training, similar to the distillation weight a in ALBEF (Li et al., 2021). We employ LLaMA3-8B-Instruct (Dubey et al., 2024) as the LLM. All datasets used for training CL3DOR will be made publicly available. Further details can be found in the Appendix.\nBenchmarks We evaluate CL3DOR on various benchmarks, focusing on 3D scene understanding, reasoning, and object hallucination. We assess performance on four major benchmarks: 3D captioning on Scan2Cap (Chen et al., 2021), 3D QA on ScanQA (Azuma et al., 2022), 3D embodied reasoning on SQA3D (Ma et al., 2022), and 3D object hallucination on 3D-POPE (Yang et al., 2024). The"}, {"title": "5. Experimental Results", "content": "5.1. Main Results\nAs shown in Table 2, CL3DOR achieves state-of-the-art performance across most metrics in all 3D scene understanding and reasoning datasets. Despite single-task models and task-adaptive models being specifically fine-tuned for particular tasks, CL3DOR, a general-purpose model, delivers the highest performance without any task-specific fine-tuning. While the previous state-of-the-art general-purpose model, LEO, also performs well, CL3DOR surpasses it significantly. Specifically, in ScanQA, CL3DOR outperforms LEO across all metrics, with notable improvements of 9 points in CIDEr and 5.3%p in EM@1-refined. Similarly, in the Scan2Cap dataset, CL3DOR exceeds LEO by 21 points in CIDEr and by 11.8%p in sentence similarity scores. While CL3DOR shows lower BLEU-4 and METEOR scores on Scan2Cap, these metrics heavily emphasize lexical similarity and thus fail to fully capture the model's performance. As noted in Chat-3D v2, the higher CIDEr and sentence similarity scores suggest that our model produces more diverse and semantically rich outputs, potentially capturing the essence of the scene more effectively.\nFurthermore, we evaluate CL3DOR on the 3D object hallucination task. Table 3 shows CL3DOR's performance on the 3D-POPE benchmark under Random, Popular, and Adversarial settings. We do not directly compare our method with 3D-GRAND (Yang et al., 2024), as it is trained on a substantially larger dataset containing 6.8M 3D scene-text samples; even for the existence task alone, it uses 532K samples, further highlighting the disparity in dataset sizes. Therefore, rather than comparing absolute performance metrics, we focus on comparisons with models trained on a relatively similar number of samples to ensure a fair assessment of CL3DOR's object hallucination capabilities. The baselines 3D-LLM, 3D-VisTA, and LEO exhibit similar accuracy levels to the random baseline. Notably, LEO and 3D-LLM show high Yes rates, indicating a severe bias towards the existence of objects. In contrast, CL3DOR maintains a more stable Yes rate while achieving higher F1 scores and accuracy compared to the baselines. These findings indicate that CL3DOR not only excels in general 3D scene understanding and reasoning tasks but also demonstrates robust performance in 3D object hallucination tasks."}, {"title": "5.2. Impact of High-Quality Visual and Textual Data", "content": "Superior Results with Higher Object Resolution. Table 4 presents the performance of CL3DOR trained under both high-resolution and low-resolution settings, sampling 8,192 and 1,024 point clouds per object, respectively, on the ScanQA, Scan2Cap, and 3D-POPE benchmarks. In all cases, the high-resolution settings result in superior performance across all datasets and metrics compared to the low-resolution settings. Particularly in the Scan2Cap dataset, the most significant improvement is observed in the CIDEr metric, indicating that higher resolution substantially enhances the model's capability to generate precise captions for specific objects. Furthermore, the increase in computational cost due to the larger size of the point cloud is not significant because the point cloud encoder within the 3D scene encoder accounts for only 0.27% of CL3DOR's total parameters. Most of the computational load is handled by the LLM, so the increase in computation cost does not significantly impact the system's efficiency.\nThe Crucial Role of Hard Negatives in Spatial Contrastive Instruction Tuning. To verify the effectiveness of the hard negative dataset constructed during spatial contrastive instruction tuning, we create an additional easy negative dataset and conduct an ablation study based on the difficulty of the negative datasets. The easy negative settings expand the original instruction-response pairs into triplets by incorporating responses randomly selected from different instructions within the dataset. As shown in Table 4, the experimental results consistently demonstrate that the hard negative settings yield higher performance in most benchmarks compared to the easy negative settings.\nMoreover, we analyze the log odds ratio represented as $log \\frac{oddse (y+ | x)}{odds (y- | x)}$ and the reward margin, defined as the difference between the log probabilities of positive and negative responses, for both easy and hard negatives during contrastive learning. The analyses, depicted in Figure 6, reveal that CL3DOR trained with hard negatives (green line) consistently exhibits lower log odds ratios and reward margins compared to those trained with easy negatives (blue line) throughout most training steps. These metrics suggest that hard negatives pose a greater challenge to the model, thereby driving the development of a more nuanced and robust understanding in context. Consequently, the hard negative setting fosters deeper learning and better equips the model to fully harness the information in the data for improving spatial understanding, as evidenced by the experimental results."}, {"title": "5.3. Impact of Odds Ratio in Objective Function", "content": "Stability and Performance: Why Odds Ratio Excels? Following previous work (Hong et al., 2024), we compare two objective functions for spatial contrastive instruction"}, {"title": "6. Conclusion", "content": "In this work, we propose CL3DOR, a 3D Large Multimodal Model designed to effectively harness high informational granularity and clarity in both visual and textual content. We enhance visual features by increasing the density of point clouds per object, thereby reducing information loss and achieving high-fidelity visual representations. For textual features, we augment the instruction tuning dataset with plausible hard negative responses to provide more meaningful information. To fully leverage the expanded dataset with hard negatives, we apply contrastive learning by incorporating an odds ratio term into the NLL loss, reinterpreting the objective function used in preference optimization. Consequently, CL3DOR achieves state-of-the-art performance on 3D scene understanding and reasoning benchmarks, surpassing existing baselines."}, {"title": "Limitation", "content": "Despite these promising results, our evaluation is constrained by limited resources, allowing only few epochs of training and precluding the use of million-scale 3D scene-text samples. These limitations prevent us from fully exploring potential performance improvements with extended training. We will address these constraints in future research."}, {"title": "A.1. Architecture", "content": "CL3DOR is a generative model designed to produce language responses by integrating both point cloud data and textual inputs. The primary components of CL3DOR are the 3D scene encoder, the point-language connector, and a pre-trained LLM.\n3D Scene Encoder The 3D scene encoder comprises a pre-trained point cloud encoder and a spatial transformer. In our experiments, we utilize PointBERT (Yu et al., 2022), a Transformer-based model (Vaswani et al., 2017) pre-trained on the ShapeNet dataset (Chang et al., 2015), which contains over 50,000 3D models. Each model in ShapeNet is represented by 8,192 point clouds, a resolution we maintain in CL3DOR to ensure high fidelity in point cloud data. This approach allows us to fully leverage PointBERT's robust feature embedding capabilities. Importantly, PointBERT remains frozen throughout all three training stages of CL3DOR, meaning its weights are not updated, thus preserving its pre-trained knowledge.\nThe spatial transformer (Chen et al., 2022), widely used in 3D Large Multimodal Models (3D LMMs) (Zhu et al., 2023; Huang et al., 2023; 2024), is crucial for learning spatial relationships between multiple objects within a 3D scene. This module explicitly calculates pairwise spatial relations and integrates them with standard self-attention to enhance spatial reasoning capabilities. We employ a three-layer spatial transformer with 8 heads to process the object-centric features produced by PointBERT, ultimately generating object tokens for the LLM. For other configurations, we maintain the default settings as outlined in (Chen et al., 2022).\nPoint-Language Connector To align the output vector space of the 3D scene encoder with the input vector space of the large language model, we use projection layers consisting of two layers activated by the GeLU (Hendrycks & Gimpel, 2016). The parameters of this module are updated throughout all training stages.\nLarge Language Model We select a decoder-only Transformer-based Large Language Model (LLM) as our backbone, specifically LLaMA3-8B-Instruct\u2020 (Dubey et al., 2024). The LLM's input includes the system message, 3D scene point clouds, and language instructions. Before being fed into the LLM, the 3D scene point clouds are transformed into special tokens.\nTo seamlessly integrate the 3D point cloud data, we expand the LLM's vocabulary with three special tokens: <point_start>, <point_patch>, and <point_end>, and train their corresponding embeddings. The <point_patch> token represents each object in the 3D scene, so if a 3D scene comprises N objects, there will be N <point_patch> tokens. The <point_start> token is placed at the beginning of the point cloud sequence, and the <point_end> token is appended at the end, encapsulating the <point_patch> tokens within the LLM's input sequence."}, {"title": "A.2. Implementation details", "content": "CL3DOR is trained through a three-stage paradigm, with the hyperparameters for each stage consolidated into Table 5. It provides a comprehensive overview of the hyperparameters used for 3D-object alignment tuning, spatial alignment tuning, and spatial contrastive instruction tuning, facilitating direct comparison across the stages. Due to limited available resources, all evaluations are conducted using a single inference run, rather than multiple runs to generate statistical values."}, {"title": "A.3. Details in Dataset", "content": "Coordinate Alignment When training with ReferIt3D, Scan2Cap, or SQA3D, the positions of objects and instructions are incorporated. Therefore, it is necessary to align the coordinates of the captions and the 3D scene within a unified world coordinate system. To achieve this, we translate the origin of each scene to the mean of the points that compose the scene, thereby aligning it with the actual intended position.\nInstruction in SQA3D In SQA3D, the captions include both the instructions (e.g., \"Which direction should I go if I want to throw litter?\") and the instructor's current situation (e.g., \"I am sitting on the chair with a bag behind me.\"). During training, the situation and the instruction are combined into a single instruction."}]}