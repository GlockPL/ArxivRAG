{"title": "Quality Assured: Rethinking Annotation Strategies in Imaging AI", "authors": ["Tim R\u00e4dsch", "Annika Reinke", "Vivienn Weru", "Minu D. Tizabi", "Nicholas Heller", "Fabian Isensee", "Annette Kopp-Schneider", "Lena Maier-Hein"], "abstract": "This paper does not describe a novel method. Instead, it studies an essential foundation for reliable benchmarking and ultimately real-world application of AI-based image analysis: generating high-quality reference annotations. Previous research has focused on crowdsourcing as a means of outsourcing annotations. However, little attention has so far been given to annotation companies, specifically regarding their internal quality assurance (QA) processes. Therefore, our aim is to evaluate the influence of QA employed by annotation companies on annotation quality and devise methodologies for maximizing data annotation efficacy. Based on a total of 57,648 instance segmented images obtained from a total of 924 annotators and 34 QA workers from four annotation companies and Amazon Mechanical Turk (MTurk), we derived the following insights: (1) Annotation companies perform better both in terms of quantity and quality compared to the widely used platform MTurk. (2) Annotation companies' internal QA only provides marginal improvements, if any. However, improving labeling instructions instead of investing in QA can substantially boost annotation performance. (3) The benefit of internal QA depends on specific image characteristics. Our work could enable researchers to derive substantially more value from a fixed annotation budget and change the way annotation companies conduct internal QA.", "sections": [{"title": "1 Introduction", "content": "Following a long period of extensive focus on pure method development, proper benchmarking and validation have recently been recognized as a major challenge in AI-based image analysis [33, 43]. In this context, high-quality annotation of reference datasets is a fundamental requirement to ensure the meaningful validation of image analysis algorithms [15]. As annotation tasks may be cumbersome, crowdsourcing is often used to partially outsource image annotation. Traditionally, such outsourcing is conducted via crowdsourcing platforms [32], with Amazon Mechanical Turk (MTurk) as the predominant choice [9,49]."}, {"title": "2 Related Work", "content": "With respect to the quality of annotations obtained from varying annotation providers, the closest work conducted are comparisons between crowdsourcing platforms and domain experts. Irshad et al. [23] and Petrovi\u0107 et al. [38], among others, have shown that crowdsourced annotations can reach the annotation quality of domain experts in safety-critical applications. In recent literature, Duggan et al. [13] and Kentley et al. [25] compared domain-specialized crowds as a new type of annotation provider and found comparable performance to domain experts. Furthermore, R\u00e4dsch et al. [42] provided a comparison of annotation companies to crowdsourcing, but focused solely on raw annotation quality without QA and without relating it to annotation budgets.\nInterestingly, researchers report concerns about the annotation quality obtained from MTurk [31], such as obtaining unusable annotations in up to 67.8% of cases [6]. Other research fields, such as psychology, even report a \"substantial decrease in data quality\" [7]. However, we are not aware of work that has systematically compared the performance between annotation companies and crowdsourcing platforms such as MTurk while considering annotation budgets."}, {"title": "2.2 Quality assurance", "content": "The main goal of QA is to enhance and ensure the quality of annotations. A comprehensive overview on QA in crowdsourcing platforms is provided by Daniel et al. [10]. However, their research does not include annotation companies. During the annotation process, quality control techniques generally fall into three categories [31]: (1) comparing to a gold standard with pre-defined reference annotations (e.g., [27]), (2) analyzing combined outputs from multiple annotators (e.g., [21]), and (3) investigating annotator behaviors (e.g., [17, 19]). Other approaches to enhancing annotation quality on crowdsourcing platforms target training of crowdworkers [12] and iterative refinement of the annotation environment [3,4,14]. It should be noted that annotation byproducts, such as mouse movement during annotation generation [16, 17] or inter-rater agreement [48], can be used to boost model performance, by incorporating this information in the training.\nThe closest related work focusing on the impact of QA in annotation companies was introduced by Wang et al. [47] and Lu et al. [31]. However, their work focuses on external QA conducted by experienced personnel from the requester rather than internal QA, and features a substantially smaller number of annotators from annotation companies. To our knowledge, a systematic investigation into the effects of internal QA processes within annotation companies is thus, so far, lacking."}, {"title": "2.3 Impact of real-world image characteristics on annotation quality", "content": "Related research analyzing the impact of real-world image characteristics highlights their effect on annotations created by machine learning models and annotators alike. Michaelis et al. [37] demonstrated that models easily encounter failures with increasingly challenging real-world image characteristics, such as added snow or poor lighting characteristics. Similarly, annotators generate less reliable annotations in the presence of challenging real-world image characteristics. This effect can be observed both within [1,24,31] and across domains [18,26]. However, a systematic analysis of the impact of such characteristics on internal QA employed by annotation companies is lacking."}, {"title": "3 Methods", "content": "In the following, we describe the selected data, the labeling instructions, annotation providers and annotators, the QA process, and the experimental design of our work."}, {"title": "3.1 Data", "content": "We conducted our experiments on the instance segmentation data from the Heidelberg Colorectal (HeiCo) dataset for surgical data science in the sensor operating room [34], representing a safety-critical application that crowdsourcing platforms can still handle. The task involved segmenting surgical instruments from laparoscopic videos, resulting in 57,648 instance segmentation masks on 4,050 unique frames. Furthermore, we used 57,636 metadata annotations on the unique images to quantify the effect of varying real-world image characteristics (see RQ3). Characteristics included motion blur, underexposure, object occlusion, or overlapping objects, among others, and are illustrated in Suppl. A."}, {"title": "3.2 Labeling instructions", "content": "To investigate the benefit of QA compared to an investment in detailed labeling instructions, we created three distinct types of labeling instructions with varying levels of information density, namely, (1) minimal text, (2) extended text, and (3) extended text including exemplary pictures. Each subsequent labeling instruction type was structured to contain more detailed information than the preceding. To ensure accurate labeling of the imaging data, we established a collection of design principles applicable to all labeling instructions:\n(1) Labeling information is presented in a slide format to enhance the processing of information by humans.\n(2) Each slide contains a distinct 'chunk' of information, which is a self-contained piece of content. This approach of 'chunking' information lessens the cognitive load on the working memory.\n(3) Related information chunks are placed in close proximity to one another.\n(4) A uniform design template is used throughout, featuring specific fonts, symbols, and color schemes.\nThe three labeling instructions differ as follows:\nMinimal text labeling instructions: These instructions provide a concise textual outline, offering examples of typical annotations and highlighting the most frequent annotation types (see Suppl. B.1). They reflect a scenario with minimal effort invested in developing the instructions. The basic framework includes seven slides totaling 168 words.\nExtended text labeling instructions: Building upon the simplified version, the extended text instructions offer a thorough textual explanation, supplemented by both positive and negative examples (refer to Suppl. B.2). They cover both regular and some rare annotation scenarios. The enhanced version comprises ten slides with a total of 446 words.\nExtended text including pictures labeling instructions: These instructions augment the enhanced textual guidelines with images (detailed in Suppl. B.3). The visual aids feature descriptions, symbols, annotations, and color coding to effectively communicate the details on the slides. They also include illustrations of rare annotation cases, indicating a deep and well-documented understanding of the labeling process. This set contains 16 slides with a total of 961 words. We refer the reader to Suppl. B for full details of each labeling instruction type."}, {"title": "3.3 Annotation providers", "content": "The study was conducted based on 216 annotators and 34 QA workers from four annotation companies and 708 crowdworkers from the crowdsourcing platform MTurk. Our goal was to provide the best representation for both annotation provider types.\nConsequently, we established higher standards for our MTurk crowdworkers compared to the usual research benchmarks, which generally require a 95% acceptance rate of Human Intelligence Tasks (HITs) and at least 100 completed tasks [3, 6, 17]. Our criteria included a 98% acceptance rate with over 5,000 completed HITs. We spread out the HITs over 40 days at all times to obtain a representative MTurk sample and compensated workers fairly, as recommended by Litman et al. [30]. The annotation companies we selected have a solid background in handling large-scale annotation tasks in a variety of domains and operate internationally. The annotators were allocated by the companies themselves to replicate the typical workflow of a project carried out by an annotation company. Each annotator was linked to a specific annotation provider that had been predetermined. No selection process from our side took place for either the annotators, QA workers, or the MTurk crowdworkers.\nTo prevent information leakage, we conducted the labeling process with increasingly detailed labeling instructions (minimal text instruction, extended text instruction, and extended text including pictures instruction). Each annotator worked exclusively with one of the three instruction types as introduced in Sec. 3.2. To enhance security, we included a mandatory minimum gap of ten days between consecutive labeling instructions."}, {"title": "3.4 Quality assurance process", "content": "In the internal QA process, annotators typically report to a small number of experienced QA workers, who are often responsible for training their team and, in our case, responsible for the QA of the annotations.\nThe general workflow for all companies consisted of an annotation stage, in which images were assigned randomly to each annotator, and a QA stage. During the annotation stage, annotators received raw images with the task of annotating them until they considered the quality of annotations satisfactory. Annotators were aware that their work would be checked by QA. Once submitted, the QA worker gained access to the generated annotations, allowing them to make modifications until they were satisfied before submitting the final annotations for delivery to the requester. Throughout this process, both annotators and QA workers retained access to the respective labeling instructions at any given moment. We obtained access to the annotations both before and after QA through a software company handling a substantial number of annotation projects annually. The experiments were seamlessly integrated into the software company's pipeline of annotation projects, reducing the risk of exposing our experiments. Pricing was established to be representative of other projects of similar size.\nTo maintain fairness and avoid giving any annotation provider a potential informational advantage that may have affected our statistical analysis, no specific queries from the annotation companies about the content of the labeling instructions were addressed."}, {"title": "3.5 Experimental design", "content": "RQ 1: Each of the five labeling providers annotated the same images with each set of labeling instructions, employing separate annotators for each task. We analyzed the number of spam images, images with severe errors, i.e., images with at least one false positive and/or false negative, and the overall annotation quality with the Dice Similarity Coefficient (DSC) [11] as the overlap-based metric and the Normalized Surface Distance (NSD) [39] as the distance-based metric. We define a spam annotation as a purposefully poor-quality annotation with the intent to obtain the payment without properly conducting the annotation task. Annotation duration was used as an auxiliary measure, where accessible. For instance, if an annotator completed the task in only 17 seconds, they could not have read the instructions and properly performed annotations on several images. Of note, a spammer could wait a relevant number of seconds and then enter a poor-quality annotation to bypass the annotation duration check. Thus, each image was additionally visually inspected by an engineer with extensive labeling experience. Spam annotation examples are displayed in Suppl. D.\nRQ 2: The annotation companies conducted their internal QA processes directly after their annotators completed the annotation job, as explained in Sec. 3.4 and Fig. 2a. The experiments began with only providing the minimal text labeling instructions, followed by the extended text labeling instructions, and finally, the extended text instructions with pictures, to prevent information leakage.\nFor each annotated image, we calculated the DSC and NSD per instance, aggregated the scores per annotated image, and aggregated the scores over annotation company and labeling instruction. We then compared the annotation quality without and with QA across the three types of labeling instructions for the four annotation companies by computing both absolute and relative changes in DSC/NSD aggregates, the percentage of modified images, and the change in the percentage of severe errors, among other parameters.\nRQ 3: In contrast to MTurk, the four annotation companies conducted an additional annotation round, including QA, with the extended text including pictures labeling instructions to quantify the impact of different real-world image characteristics, represented by 57,636 metadata annotations, on the probability of improvement. The data was analyzed with a logistic mixed model [35] which included random effects for the image, the annotator, and the annotation company. Further method details are provided in Suppl. C."}, {"title": "4 Results", "content": null}, {"title": "4.1 How does the choice of annotation provider influence the quality and cost of annotations?", "content": "Annotated images generated by the annotation companies were of higher quality, comprising no spam (MTurk: ~20%) and a much higher proportion of remaining images without severe annotation errors (factor of > 2 improvement for all companies compared to MTurk), as shown in Fig. 3. In addition, the annotation companies were 61.1% cheaper in the median, despite their additional employment of QA workers. More specifically, the costs ranged from 24.5% to 81.5% of the MTurk costs for the same images and number of annotations. We have not been given permission to reveal the identities and exact absolute budgets used for the annotation companies, hence we report costs relative to the MTurk budget. Examples of spam annotations are provided in Suppl. D.\nRegardless of the annotation scenario, the annotation companies generated higher quality annotations than MTurk with higher DSC scores including smaller interquartile ranges (IQRs) (see Fig. 7a), higher NSD scores including smaller IQRS (see Fig. 7b), and lower numbers of severe errors (see Fig. 7c)."}, {"title": "4.2 Does annotation companies' quality assurance improve annotation quality?", "content": "As shown in Fig. 4, the internal QA conducted by annotation companies does not substantially improve annotation quality if the QA workers only use minimal text instructions. The impact of QA when having access to extended text instructions or extended text including pictures instructions depended highly on the annotation company, while the effect was still minimal compared to not performing QA: Two of the four annotation companies show no change in the median DSC when comparing QA to non-QA outputs for extended text or extended text and picture instructions. The remaining two companies only show a very small relative improvement of below 2%.\nWe obtained even smaller improvements with the NSD scores, as depicted in Suppl. E. In certain scenarios, QA was able to remove a small proportion of severe errors, however, the vast majority remains (see Suppl. F). Notably, the percentage of modified images by QA workers depended highly on the company, as depicted in Suppl. G. Fig. 5 further illustrates that providing annotators with high-quality labeling instructions including pictures (green) leads them to vastly outperform annotators working with lower-quality labeling instructions, even if these are followed by additional internal QA (dark blue)."}, {"title": "4.3 Should quality assurance focus on specific subsets of images?", "content": "As shown in Fig. 6a, QA significantly increases the odds of improving the annotation quality for images with generalizable image characteristics such as underexposed objects, intersection of objects, objects occluded by background in comparison to the absence of this characteristic (vertical black line). The only exceptions are motion artifacts present in the background and object(s) overexposure.\nIn contrast to the general increase of images with generalizable image characteristics, none of the domain-specific image characteristics yield a significant change in the odds of improving the annotation quality over a regular image, see Fig. 6b. The only exception for domain-specific characteristics is object(s) covered by smoke, which could be interpreted as a form of object occlusion."}, {"title": "5 Discussion", "content": "The data annotation market is estimated to reach a volume of more than USD 4.1 billion by the end of 2024 [8]. As the annotation process becomes increasingly automated and foundation models evolve, the market is undergoing drastic shifts. With lower entry barriers for annotation, more efficient data annotation tools, and a rising impact of QA, it is thought to result in an estimated USD 8.2 billion market by 2028 [40]. As a research community, we need to understand how to ensure high annotation quality throughout this evolution, as reliable reference annotations are an indispensable foundation for the benchmarking of image analysis algorithms, and thus their ultimate translation into real-world practice.\nTo our knowledge, this study is the first to quantitatively and critically examine the impact of internal QA conducted by annotation companies in the context of safety-critical applications. The most important insights can be summarized as follows:\n1. Annotation companies perform better both in terms of quantity and quality compared to the widely used platform MTurk (Fig. 3). This implies that requesters working with crowdsourcing platforms should seriously consider annotation companies as an alternative. The organizational structure of these companies, which enables a single point of contact for requesters and eliminates spam annotations, appears to be a contributing factor to their efficiency. In light of poor working conditions being a major problem in annotation generation [22, 36], annotation companies, which can more easily be inspected than distributed crowdsourcing platforms, are thought to offer better working contracts, enabling fairer working conditions for annotators. It should be noted that not every annotation company automatically ensures adequate working conditions. The annotation companies employed in this study underwent thorough scrutiny to guarantee compliance with good working condition guidelines. With annotators working at a shared location instead of in a distributed crowd, verifying annotators' working conditions has become substantially easier. We thus view our work as an initial step towards greater accountability on the part of annotation requesters.\n2. In contrast to common belief, internal QA mechanisms do not necessarily improve annotation quality (Fig. 4). Furthermore, we observe no systematic QA quality improvement difference between annotation companies depending on their region of origin, size, or price, meaning more expensive annotation companies do not translate to better QA. However, improving labeling instructions instead of investing in QA can substantially boost the annotation performance (Fig. 5). Consequently, in order to obtain high-quality annotations, annotation requesters should prioritize refining their labeling instructions, followed by implementing QA or even a different external QA process as a secondary step. Compared to QA, better labeling instructions can be generated with relatively few resources.\n3. The benefit of QA depends on the specific image characteristics (Fig. 6). In fact, QA did improve the quality of images with certain challenging real-world image characteristics, see Fig. 6a. Consequently, it might be useful for requesters and annotation companies alike to allocate resources to identifying challenging image characteristics during a project, train classifiers to identify images containing these characteristics and focus the internal QA effort on these images.\nOne could contend that the recent rise of foundation models marginalizes the role of annotation providers. We argue, however, that it only changes their role. In this context, it is necessary to differentiate between the data quality demands for the training and testing of models. With a recent trend towards fine-tuning pre-trained models, smaller high-quality datasets can achieve similar performance with less computation [5,29], highlighting the importance of the quality of unlabeled data in unsupervised training and its role in reducing carbon emissions [45]. In testing, especially for safety-critical applications such as autonomous driving and clinical medicine, ensuring high-quality annotated test data is crucial for accurate model evaluation and real-world applicability [43]."}, {"title": "6 Conclusion and Future Work", "content": "In conclusion, our study underscores the pivotal role of enhancing the initial annotation quality, for instance via allocating resources to the improvement of labeling instructions, over QA in obtaining higher reference annotation quality in high-stakes AI [43]. Future research should delve into the development and implementation of comprehensive training programs for annotators from annotation companies to further enhance initial annotation quality. Exploring the role of annotation companies in the context of increasingly automated annotation processes driven by foundation models presents another avenue for future research."}, {"title": "I \\sim (1|Image\\_name) + (1|Company/Annotator) + \\sum_{i=1}^{7}G_i + \\sum_{i=1}^{10} D_i", "content": "Software: R version 4.0.2\n(package Ime4 version 1.1.33).\n(1)"}]}