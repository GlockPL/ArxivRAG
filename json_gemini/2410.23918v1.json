{"title": "BITSTACK: FINE-GRAINED SIZE CONTROL FOR COMPRESSED LARGE LANGUAGE MODELS IN VARIABLE MEMORY ENVIRONMENTS", "authors": ["Xinghao Wang", "Pengyu Wang", "Bo Wang", "Dong Zhang", "Yunhua Zhou", "Xipeng Qiu"], "abstract": "Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from capability to availability, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce BitStack, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) have demonstrated superior performance on various benchmarks and are increasingly serving as practical assistants in people's daily lives, such as general language assistants, search engines , and code assistants.\nWith the blessing of scaling laws , LLMs are becoming more powerful as their sizes expand, and the main bottleneck for deploying task-capable LLMs has shifted from their capability to their availability. For example, loading only the weights of the Llama 3.1 8B model requires approximately 14.96 GB of RAM in FP16, not including the activations, which also consume significant memory during inference, especially for long-context tasks.\nTo adapt to various memory and device constraints, numerous methods have been proposed for LLM compression, such as quantization, pruning, and distillation . These methods often compress models to a predefined compression ratio (e.g., specifying numerical precision, defining target structures for pruned models or student models) and require running the compression procedure from scratch for every compression setting. Another line of research for compressing LLMs is weight decomposition . These methods compress the model weights via low-rank decomposition but often suffer from severe performance degradation at high compression ratios.\nDeploying large language models locally(e.g. on personal computers or smartphones) is a common practice, as it safeguards private data and enables offline functionality. However, the available RAM on these devices is often limited and variable, as the total memory capacity is generally small and memory usage by other applications can fluctuate(Figure 1a). This variability in available memory poses a challenge for deploying LLMs, as they require consistent and substantial RAM resources. For example, when more memory becomes available from other applications, users may want to use a 4-bit quantized model instead of a 3-bit one for better performance. However, this requires reloading the entire model, which may cause significant delays due to limited transmission bandwidth. Additionally, multiple versions of the model at different compression ratios need to be stored on the device, and each version requires running a separate compression process in advance, which increases the storage burden on the device and requires additional computational resources to run separate compression processes. Therefore, a compression strategy that enables dynamic trade-offs between memory usage and performance is highly desirable.\nAs discussed earlier, achieving these trade-offs requires avoiding compressing towards a fixed ratio. Instead, we aim to compress the model once, allowing it to be dynamically loaded within any arbitrary memory budget, which leads us to weight decomposition. However, previous studies on weight decomposition for LLMs failed to match the performance with practical methods like quantization. To tackle this challenge, we propose a novel training-free, decomposition-based weight compression approach called BitStack, where we decompose the original weight matrices and iteratively decompose the residuals from the previous approximation. In the decomposition process, we account for the unequal importance of weights (stemming from the high variance in activation channel magnitudes) by scaling the weights before decomposition. We then iteratively apply singular value decomposition (SVD) to decompose the magnitude of the matrices (or residuals) into vectors while preserving their sign matrix, yielding an approximately 1 bit of memory per parameter residual block in each iteration. Subsequently, the residual blocks for different weights across various layers are universally sorted and stacked based on their importance to overall performance at the current memory level, stored as basic transmission units in storage. Weight matrices are also treated as stacks, progressively approaching the original matrices as more blocks are added. In this way, BitStack enables a memory-performance trade-off for LLMs by dynamically loading or offloading residual blocks between running memory and storage devices, making LLM deployment feasible in variable memory environments. We conduct extensive evaluations on BitStack across a wide range of tasks, demonstrating that, despite its capability to deploy in variable memory environments, BitStack consistently matches or surpasses the performance of widely adopted compression methods like GPTQ and AWQ , especially at extreme compression ratios(Figure 1b). To the best of"}, {"title": "BITSTACK", "content": "An overview of BitStack is illustrated in Figure 2. BitStack is able to dynamically adjust the size of each weight matrix based on the available memory capacity at the time. When more RAM is freed by other applications, we can retrieve additional residual blocks from a pre-sorted stack and load them into RAM. Conversely, when memory becomes limited, we can offload residual blocks from the model weights (also stored as stacks) back to storage devices in reverse order, ensuring the system remains functional. In the following subsections, we first introduce the decomposition procedure for each weight (or residual) matrix in Section 2.1, and then explain how we sort the residual blocks to be pushed into the universal stack in Section 2.2. A comprehensive overview of BitStack is provided in Algorithm 1."}, {"title": "DECOMPOSING WEIGHTS IN LLMS", "content": "In weight decomposition, the objective is to break down weight matrices into sub-matrices to reduce the total number of parameters, with the ability to reconstruct the full matrices during inference. Singular value decomposition (SVD), in particular, is a widely-used and effective method for matrix decomposition due to its ability to capture the most significant components of the weight matrices. Formally, let $W \\in \\mathbb{R}^{m\\times n}$ be a weight matrix in a linear layer, we can decompose $W$ via SVD by:\n$W = U \\Sigma V^T = \\sum_{i=1}^{d} \\sigma_i u_i v_i^T$\nwhere $d = min\\{m, n\\}$, $\\sigma_1 \\geq \\sigma_2 \\geq \\cdot > \\sigma_d$ are the singular values of $W$, and $u_i$ and $v_i$ are the corresponding left and right singular vectors, respectively. We then obtain a rank-k approximation"}, {"title": "ACTIVATION-AWARE DECOMPOSITION", "content": "Large language models are known to exhibit outliers in their activations, i.e., the channel variance in $X$ can be high, leading to outputs dominated by these outliers. Fortunately, prior research has demonstrated that these outliers are often systematically distributed across the activation channels, underscoring the importance of accurately restoring the corresponding weight rows. Lin et al. first proposed scaling the weight matrix using a row-wise scaling vector s, which is precomputed with a calibration set to reduce the quantization error of salient weights. Yuan et al. further adopted this method, scaling the weights before applying SVD. In BitStack, we also adopt this methodology to preserve the restoration accuracy of the salient weights. To simplify the process, we do not incorporate any additional searching procedures or hyperparameters to obtain the scaling factors as in previous studies ; instead, we compute the scaling factor using the channel-wise l2 norm of $X$. Formally, let $X \\in \\mathbb{R}^{P \\times m}$ represent the input activations for a linear layer, computed using a calibration set, and $W \\in \\mathbb{R}^{m \\times n}$ be the corresponding weight matrix, we compute the scaling factor as follows:\n$S = [||X_1 ||_2, ||X_2||_2,, ||X_n||_2]$\nThe inference computation can then be transformed to:\n$XW = Xdiag(1/s)diag(s)W = Xdiag(1/s)W_{scaled}$\nAnd we use $W_{scaled}$ for the subsequent decomposition."}, {"title": "ITERATIVE ABSOLUTE VALUE DECOMPOSITION", "content": "To reduce the approximation error in each decomposition process, we propose to use absolute value decomposition. In this approach, we first decompose each (scaled) weight matrix into its sign matrix and absolute value matrix; for a weight matrix $W \\in \\mathbb{R}^{m \\times n}$ this is expressed as $W = W_{sign} |W|$. We then apply SVD on $|W|$ while retaining $W_{sign}$. This method enables us to store more information than directly applying SVD on $W$, since we save an additional matrix $W_{sign}$ which is typically large in LLMs. Since $W_{sign}$ consists solely $\\pm 1's$, we can pack $W_{sign}$ to GPU-supported data types for storage and unpack it for use during inference computation. We store the singular vectors in FP16, resulting in an overall memory occupation of approximately 1 bit per parameter when $k < min\\{m, n\\}$ in each decomposition process. A similar technique was employed in recent quantization-aware training research to initialize the weights for 1-bit LLM training.\nFormally, for matrix $W = W_{sign} |W|$, the approximation of $W$ after absolute value decomposition would be:\n$W_{avd} = W_{sign} |W|_{svd} = W_{sign} (A'B'^T)$\nwhere $|W|_{svd} = U' \\Sigma V'^T$, $A' = [\\sqrt{\\sigma_1} u'_1,..., \\sqrt{\\sigma_k} u'_k]$ and $B' = [\\sqrt{\\sigma_1} v'_1, ..., \\sqrt{\\sigma_k} v'_k]$."}, {"title": "SORTING RESIDUAL BLOCKS", "content": "Having universally decomposed each weight matrix in every layer, it is essential to determine the order in which these residual blocks are loaded from storage into memory to optimize model performance within a given memory budget. To this end, we utilize a small calibration set to calculate perplexity, assessing how much each residual block influences the overall performance. However, solving this sorting problem remains non-trivial, even with this comparison criterion, since the search space is large. For instance, in a model with L layers, each containing M linear layers, and with each weight matrix decomposed over n iterations, there are $n^{LM}$ possible combinations of settings across the various linear layers.\nTo reduce the search space, we constrain the difference in the number of residual blocks across all weight stacks to no more than 1. This approach facilitates a smooth memory-performance trade-off and promotes effective load balancing when the model is distributed across multiple devices, resulting in a significant reduction of the search space to $n^{LM}$. More specifically, no stack loads the i + 1th block until all stacks have loaded the ith block. We then sort the relative order of all the i + 1th blocks based on their importance, which is measured by the perplexity score after loading this single residual block while keeping all other i + 1th blocks for other stacks unloaded. The residual blocks are then placed into a universal stack, ensuring: 1) for all ith blocks, blocks with lower measured perplexity scores are on top of those with higher scores; 2) all ith blocks are on top of any i + 1th ones. This allows a relatively more important block to be loaded when additional memory becomes available. We provide the pseudocode of the sorting process from Line 25 to Line 44 in Algorithm 1."}, {"title": "EXPERIMENTS", "content": null}, {"title": "EVALUATION ON BASE MODELS", "content": null}, {"title": "SETTINGS", "content": "Baselines. Since our method is training-free, we compare it with two other strong, widely adopted training-free model compression baselines: GPTQ and AWQ, both of which also require only a small calibration set as in our approach. Note that we do not include other decomposition-based methods, as they suffer from severe performance degradation under high compression ratios ($1 - \\frac{compressed\\ model\\ memory}{original\\ model\\ memory}$), and their reported highest compression ratios are significantly lower than those in our study. For example, the highest compression ratios are 30%, 25%, and 60% for FWSVD, ASVD, and SVD-LLM, respectively. Furthermore, for the state-of-the-art decomposition-based method, SVD-LLM, the perplexity score increased by 745% and the average performance on zero-shot tasks dropped by 40% at a compression ratio of 60% compared to the original FP16 model,"}, {"title": "RESULTS", "content": "Evaluation results of both the perplexity scores and zero-shot performance of Llama 3.1/Llama 2/Llama 3 models are presented in Table 1, 2 and 3, respectively. Since BitStack allows for megabyte-level size control, we align the model sizes of the BitStack-compressed models with those of the different baselines for a fair comparison. Specifically, we utilize the largest size that does not exceed the baselines' sizes.\nBitStack performs better at extreme compression ratios. As shown in the tables, BitStack delivers superior or comparable performance with strong quantization baselines across different compression ratios, despite having the advantage that it only needs to compress and store once and can dynamically adjust its memory consumption at a megabyte level. More specifically, BitStack models constantly outperform the baselines at extremely high compression ratios. For 7/8B models, BitStack constantly outperforms GPTQ models below 4-bit-level and AWQ models below 3-bit-level. For 7/8B models, BitStack outperforms the best 2-bit baselines with group quantization by an absolute margin of 12.1(Llama 3.1), 22.3(Llama 2), 10.4(Llama 3) on average performance in zero-shot tasks. This advantage is even more pronounced in larger models; for example, on the Llama 3.1 70B, BitStack retains 89%of the performance of the original FP16 models, surpassing the best baseline by a substantial margin of 41.3 on zero-shot tasks.\nBitStack maintains strong performance at lower compression ratios. While quantization baselines excel at lower compression ratios, BitStack maintains comparable effectiveness, even with group quantization, which significantly enhances the performance of these quantization methods. For instance, at the lowest compression ratio (64%) in our experiments, BitStack Llama 3.1 8B and 70B models can recover 96% and 98% of the zero-shot performance of the original FP16 model, respectively. Although they exhibit slightly higher perplexity scores, they only fall short of the best baselines by a negligible 1.7 and 0.8 absolute average score on zero-shot tasks. As shown in the tables, the gap consistently narrows as the model size increases. For instance, when compared to the best baseline with group quantization, the gap in zero-shot tasks decreases from 1.7 to 1.1 to 0.2 for Llama 2 models with 7B, 13B, and 70B parameters, respectively(Table. 2). It can be seen that BitStack demonstrates particularly strong performance with larger models. For 70B models, it consistently outperforms the baselines without group quantization across all compression ratios in our experiments."}, {"title": "EVALUATION ON INSTRUCTION-TUNED MODELS", "content": null}, {"title": "SETTINGS", "content": "To assess the generalization capability of our method, we conduct further experiments on instruction-tuned models. Specifically, we apply compression to the Llama 3.1 Instruct 8B and 70B models using both our approach and AWQ, which has been shown to be a stronger baseline in the previous section. We follow the procedure in Zheng et al. (2023) and evaluate the compressed models on MT-Bench , which consists of 80 multi-turn common user prompts, covering writing, roleplay, extraction, reasoning, math, coding, knowledge I (STEM), and knowledge II (humanities/social science). We use OpenAI gpt-4o as the judging model to evaluate the model answers."}, {"title": "RESULTS", "content": "Figure 4a illustrates the evaluation results on BitStack compressed Llama-3.1-Instruct-8B model with {4000, 5000, 6000, 7000, 8000} megabytes. The results show a clear trend across all domains: increasing the model size (by loading more residual blocks from storage) consistently improves performance. This underscores that while BitStack facilitates fine-grained memory-performance trade-offs, the performance improvement spans all domains comprehensively. When compared to AWQ, BitStack demonstrates a similar trend at various compression ratios as seen with the base models. As shown in Figure 4b, at extremely high compression ratios\u2014approximately at the 2-bit level\u2014BitStack models can occasionally generate reasonable answers, whereas the AWQ compressed model fails to produce coherent text. This distinction becomes even more pronounced at the 3-bit level, where the BitStack model consistently generates high-quality responses, while the AWQ model still outputs gibberish. At lower compression ratios (4-bit level), where quantization-based methods excel, BitStack outperforms or matches the baseline on about \u00bd of the samples for the 8B model and about \u2153 for the 70B model. We provide extra qualitative results in Section A.3."}, {"title": "ABLATION STUDY AND ANALYSIS", "content": "In this section, we conduct an ablation study to evaluate the impact of each component within our proposed approach. We assess performance by plotting perplexity and average zero-shot accuracy curves to measure the model's effectiveness at different memory footprints. For these experiments, we use the BitStack Llama 3.1 8B model and evaluate performance with a memory stride of 500MB. Additionally, we provide further discussion on the minimal transmission units in BitStack in Section A.4 and analyze the inference overhead in BitStack in Section A.5."}, {"title": "Impact of each component.", "content": "As shown in Figure 5, activation-aware scaling consistently improves model performance across all compression ratios, with particularly strong effects at higher compression ratios. For instance, it leads to an 8-point improvement in average zero-shot performance at a"}, {"title": "Impact of the sorting algorithm for residual blocks.", "content": "To reduce the search space for sorting residual blocks, we propose constraining the length difference between weight stacks to no more than 1(as detailed in Section 2.2, referred to as Average). We compare this approach to two alternatives:: 1) Random, which randomly shuffles the universal residual stack without any search process; 2) Greedy, which evaluates each weight stack at each level (number of residual blocks) while freezing all other weight stacks at a level of, and utilize the current perplexity as the importance score for corresponding stack at that level, which also has a search space of nLM. We provide visualization of resulting weight stacks of the three sorting approaches in Section A.6. As shown in Figure. 6, as the memory footprint goes up, all three approaches converge as most residual blocks are loaded into the model. However, at lower memory footprints(< 8000MB), Average significantly outperforms both baselines, surpassing the best baseline by 16, 16, and 7 points in absolute zero-shot performance at 4000MB, 4500MB, and 5000MB, respectively. In addition to excelling at high compression ratios, Average also provides better load balancing, as the memory footprint of each block varies minimally, making it easier to deploy in distributed scenarios."}, {"title": "Ablation on calibration set size n.", "content": "We compute the scaling vector s using various sizes of the calibration set, as shown in Figure. 7a. The figure demonstrates that BitStack is robust to changes in calibration set size, as the curves align almost perfectly across different sizes, particularly as the memory footprint increases. Interestingly, BitStack even performs slightly better with a smaller calibration set size of 64 in extreme compression scenarios, such as with a memory footprint of 4000MB."}, {"title": "Ablation on number of kept singular vectors k in each decomposition process.", "content": "Generally, a larger k in SVD indicates a better approximation in each decomposition process, but at the cost of increased memory usage, as the singular vectors are stored in FP16. Figure. 7b illustrates the performance when setting k to {1, 4, 8, 16, 32}. As shown in the figure, keeping only the largest singular value and its corresponding vectors is insufficient for a good approximation, leading to performance degradation. On the other hand, increasing k results in fewer residual blocks being loaded at the same memory footprint, limiting model performance. This is evident from the figure,"}, {"title": "RELATED WORK", "content": "Fixed ratio weight compression. As discussed in Section 1, we categorize weight compression approaches such as quantization, pruning, and distillation under fixed ratio weight compression. Quantization-based methods compress weights by reducing precision, pruning techniques compress by directly modifying the model structure (e.g., reducing the number of layers or hidden dimensions), and distillation methods involve training a smaller model on the outputs of the original model. The latter two approaches, as well as quantization methods for higher compression ratios, typically require extensive training , which can be computationally expensive when compressing models for multiple compression ratios. Furthermore, models compressed by these methods are poorly suited for variable memory environments due to their fixed memory usage, preventing efficient utilization of available capacity.\nAdaptive ratio weight compression. Weight decomposition methods are more suitable for adaptive ratio weight compression due to their forward-compatible nature, as the approximation improves with the inclusion of more singular vectors in SVD. However, current decomposition-based weight compression approaches for LLMs tend to collapse at high compression ratios, rendering them impractical for real-world deployment. In this work, we bridge the performance gap between decomposition-based methods and practical quantization-based approaches, making LLM deployment in variable memory environments feasible."}, {"title": "CONCLUSION", "content": "In this paper, we highlight the challenge of deploying compressed large language models in variable memory environments and propose BitStack, a decomposition-based compression approach designed to address this issue. BitStack enables megabyte-level memory-performance trade-offs in a training-free manner, requiring only a small calibration set. Additionally, BitStack is simple to implement, with the decomposition of 70B models being achievable on a single GPU. Despite its flexibility in memory footprint, BitStack consistently matches or surpasses the performance of practical baselines, making it a viable solution for real-world applications. We believe that BitStack represents a new paradigm for LLM deployment on local devices, providing not only efficient memory management but also strong performance within the given memory budget."}, {"title": "APPENDIX", "content": "We provide further details of our approach in this appendix as follows:\n\u2022 Section A.1, the overall algorithm of BitStack.\n\u2022 Section A.2, evaluation results of Llama 2 and Llama 3 models.\n\u2022 Section A.3, qualitative results of BitStack Llama 3.1 Instruct 8B and 70B models.\n\u2022 Section A.4, discussion on minimal transmission units in BitStack.\n\u2022 Section A.5, analysis of inference overhead in BitStack\n\u2022 Section A.6, visualizations of weight stack in different sorting approaches."}, {"title": "OVERALL ALGORITHM OF BITSTACK", "content": "Algorithm 1 illustrates the pseudocode for the overall algorithm of BitStack. Specifically, Lines 1 to 8 describe the activation-aware weight scaling process, as introduced in Section 2.1.1. This scaling is applied only once before the iterative absolute decomposition. Lines 9 to 23 detail the iterative absolute decomposition process, as explained in Section 2.1.1 where each scaled weight matrix is decomposed into n residual blocks and stored as a stack. Finally, Lines 24 to 44 demonstrate the"}, {"title": "EVALUATIONS OF LLAMA2 AND LLAMA3 MODELS", "content": null}, {"title": "QUALITATIVE RESULTS", "content": "In Table 4 and 5, we compare BitStack and AWQ across different compression ratios on the Llama 3.1 Instruct 8B and 70B models. As shown in Table 4, which illustrates a math reasoning task, at a relatively lower compression ratio (5338MB), both compressed models produce correct answers with several reasoning steps. As the compression ratio increases (e.g., 4506MB), the AWQ-compressed model starts with a correct reasoning path but collapses midway through inference, whereas the BitStack-compressed model still manages to reach the correct answer. At extreme compression ratios, such as 76% (3674MB), neither model produces a correct answer; the AWQ model outputs gibberish from the beginning, while the BitStack model is able to generate coherent text but omits the reasoning steps, ultimately arriving at an incorrect conclusion. For the 70B models in As shown in Table 5, where the models are asked to write rhyming proofs, the trend is similar to the 8B models. As compression ratios increase, AWQ models begin to output gibberish, while BitStack models continue to produce fluent text, albeit with lower answer quality."}, {"title": "DISCUSSION ON MINIMAL TRANSMISSION UNITS IN BITSTACK", "content": "In this section, we discuss the minimal transmission units, i.e., residual blocks, in BitStack. As detailed in Section 2.1.2, we decompose the approximation residuals in each iteration into their sign"}, {"title": "ANALYSIS OF INFERENCE OVERHEAD OF BITSTACK", "content": "In our experiments, we did not include any inference optimization, as this is not the primary focus of this research. However, we provide an analysis here to support future deployment of BitStack models in real-world scenarios. Similar to other weight-only compression methods (e.g., weight-only quantization), the restoration of weights is performed on the fly, introducing an inference overhead. As illustrated in Figure 9, we roughly divide the total inference overhead into two parts: Overhead 1 and Overhead 2.\nOverhead 1 represents the residual block restoration time, including unpacking the sign matrix and the multiplication time as in Eq. 5. This can be significantly reduced by utilizing efficient unpacking kernels and fusing the residual block restoration operations.\nOverhead 2 refers to the additional time required to restore more residual blocks for weight stacks that load more than one block. As shown in the figure, Overhead 2 increases linearly as more residual blocks are loaded. This occurs because, in our implementation, the residual blocks are restored sequentially when computing Eq. 8. In practice, however, all residual blocks in the stack can be computed in parallel, as they are independent of one another, making Overhead 2 fully eliminable."}, {"title": "VISUALIZATIONS OF WEIGHT STACKS", "content": "In Figure 10, we provide the visualization of the weight stacks in BitStack for three different sorting approaches, as detailed in Section 2.2. The Average approach, which we adopt in BitStack, exhibits minimal variance in the memory consumption of different stacks, benefiting load balancing in distributed deployment. Moreover, it demonstrates excellent performance in our experiments, particularly at extreme compression ratios."}]}