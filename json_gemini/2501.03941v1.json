{"title": "Synthetic Data Privacy Metrics", "authors": ["Amy Steier", "Lipika Ramaswamy", "Andre Manoel", "Alexa Haushalter"], "abstract": "Recent advancements in generative AI have made it possible to create synthetic\ndatasets that can be as accurate as real-world data for training AI models, power-ing statistical insights, and fostering collaboration with sensitive datasets while\noffering strong privacy guarantees. Effectively measuring the empirical privacy\nof synthetic data is an important step in the process. However, while there is a\nmultitude of new privacy metrics being published every day, there currently is no\nstandardization. In this paper, we review the pros and cons of popular metrics that\ninclude simulations of adversarial attacks. We also review current best practices\nfor amending generative models to enhance the privacy of the data they create (e.g.\ndifferential privacy).", "sections": [{"title": "Introduction", "content": "Fundamentally, synthetic data is annotated information that computer simulations or algorithms\ngenerate as an alternative to real-world data [1]. It is generated using a class of machine learning\nmodels that learn the underlying relationships in a real-world dataset and can generate new data\ninstances. High-quality synthetic data will contain the same statistical relationship between variables\n(e.g., if real-world data reveals that weight and height are positively correlated, then this observation\nwill hold true in the synthetic data). However, records of a synthetic dataset will not have a clear link\nto records of the original, real-world data on which it is based. Synthetic data should be one step\nremoved from real-world data. This means that synthetic data generated with the right level of privacy\nprotection can provide safeguards against adversarial attacks, something traditional anonymization\ntechniques like masking or tokenization cannot promise.\nSynthetic data has been hailed as a solution to the difficult problem of sharing sensitive data while\nprotecting privacy. However, the fact that synthetic records are \u201cartificial\u201d does not, per se, guarantee\nthat the privacy of all individuals in the original dataset is protected. There is abundant evidence that\nsynthetic generative models can suffer from unintended memorization [2] and can leak information\nassociated with training samples [3, 4, 5, 6, 7]. Thus effective privacy metrics quantify such leakage\nand are a critical part of the use of synthetic data.\nThere is currently a lack of standardization of empirical privacy metrics for synthetic data. In this\npaper, we review popular metrics as well as techniques for strengthening the privacy of synthetic\ndata."}, {"title": "Background", "content": ""}, {"title": "Data types Under Consideration", "content": "The dominant type of synthetic data is tabular, but it is also common to see synthetic text, image,\ngraph, audio and video data. This paper focuses on tabular synthetic data. The privacy metrics that\nexploit adversarial attacks have also been used with synthetic text and image data. These are typically"}, {"title": "The Privacy-Utility Trade-off", "content": "The privacy-utility trade-off in synthetic data refers to the balance between ensuring the privacy of\nindividuals' information and maintaining the usefulness of the generated data for analysis or training\na downstream model. Techniques to enhance the privacy of synthetic data can sometimes come at the\ncost of decreasing utility. The measurement of the quality and utility of synthetic data has been well\nstudied [8, 9, 10]. This paper focuses specifically on privacy metrics."}, {"title": "Privacy Metrics", "content": "In this section we review the popular privacy metrics of k-anonymity, personally identifiable informa-\ntion (PII) replay, exact match counts, distance to closest record (DCR), nearest neighbor distance\nratio (NNDR), nearest neighbor adversarial accuracy (NNAA), membership inference attacks (MIAs)\nand attribute inference attacks (AIAs)."}, {"title": "K-Anonymity", "content": "K-anonymity is a popular privacy metric that dates back to 1986 [11] but the term was coined in a\n1998 paper [12]. Data is said to have the k-anonymity property if the information for each person\ncontained in the data cannot be distinguished from at least k - 1 individuals whose information also\nappears in the data. K-anonymity protects against hackers or malicious parties using \u2018re-identification,'\nor the practice of tracing data's origins back to the individual it is connected to in the real world.\nVariants of k-anonymity include l-diversity and t-closeness [13].\nTo assess k-anonymity one must first determine whether each field is a direct identifier, non-identifier\nor quasi-identifiers. Examples of direct identifiers are name or address. In k-anonymity, it is presumed\nthat the direct identifiers will be suppressed and it's the quasi-identifiers that are used by an adversary\nfor re-identification. Quasi-identifiers are semi-identifying fields that somehow an adversary knows.\nFor example, a date of birth is often a quasi-identifier because it is information about an individual\nthat is known or relatively easy for an adversary to find out. More generally, an adversary may know\nthe quasi-identifiers about an individual because that individual is an acquaintance of the adversary\nor because the adversary has access to a population database or registry of identifiable information.\nResearch shows that the combination of quasi-identifiers 'gender', 'birth date', and 'postal code'\nreidentify between 63 and 87% of the U.S. population [14].\nAlthough k-anonymity is a popular and easy to understand metric, pragmatically it can be difficult to\nknow which fields are quasi-identifiers. By definition, a quasi-identifier is a field that when linked to\nexternal information could be used to identify an individual. To some extent, almost any field could\nbe a quasi-identifier making it difficult for practitioners to know which fields to use. This measure\nalso tends to not scale well with really high dimensional data [15]."}, {"title": "Personally Identifiable Information (PII) Replay", "content": "Personally identifiable information (PII) in natural language is data that can re-identify an individual.\nPII can be a direct identifier or quasi-identifier. The hope in creating synthetic data is that PII in\ntext won't be replayed in the synthetic data. Measuring PII replay as a metric is important because\nwithout added precautions, it can be a common occurrence [16, 17, 18]. It should be noted, however,\nthat different types of PII carry different risks. If a common first name or last name is replayed there's\nlittle privacy risk, but if a full name is replayed the risk is higher.\nTypical approaches for reducing PII replay are Differential Privacy [19], PII scrubbing [16] or\npseudonymization [20]. Read more about such recourse measures in Section 4."}, {"title": "Exact Match Counts", "content": "This metric, sometimes referred to as Identical Match Share (IMS), measures the number of training\nrecords that are exactly replicated in the synthetic data. Theoretically, you're shooting for no exact\nmatches. In the ideal privacy/utility tradeoff scenario, the goal is to have the generated data be in the\n\"Goldilocks\" zone: not too similar to the training data, but also not too dissimilar.\nIMS captures the proportion of identical copies between train (original) and synthetic records. The\ntest passes if that proportion is smaller or equal to the one between train and test datasets. A test\ndataset is the portion of the original data put aside before a synthetic model is trained. Assessing\nexact matches is a common practice in industry [21, 22, 23, 24]."}, {"title": "Distance to Closest Record (DCR)", "content": "Distance to closest record (DCR) is a popular privacy metric that has been in use for quite some\ntime [25]. There are many variations of it. In its most basic form it measures the Euclidean distance\nbetween any synthetic record and its closest corresponding real neighbor. Sometimes the median\ndistance is reported [26] and sometimes distance at the 5th percentile is used [27]. Ideally, the higher\nthe DCR the lesser the risk of privacy breach. DCR of 0 means it replicated training examples in\nthe synthetic data. DCR is supposed to protect against settings where the train data was just slightly\nperturbed or noised and presented as synthetic.\nOne variation of DCR is to compare the train-train DCR with the train-synth DCR [22, 26]. Privacy\nis preserved if train-train DCR is less than train-synth DCR. This means the synthetic data is further\nfrom the real data than the real data is to itself, meaning that the synthetic data is more private than a\ncopy or a simple perturbation of the real data.\nAnother variation is to look at DCR within real data and DCR within synthetic data. The former\nmetric is the Euclidean distance between each record in the real data and its closest corresponding\nrecord in the real data, whereas the latter measures the same but on the synthetic dataset. If the DCR\nwithin the synthetic data is less than that in the real data, this can be indicative of a model collapse\nproblem [28].\nFinally, yet another variation is to compute the train-synth DCR and the holdout-synth DCR. The\nholdout dataset is from the same source as the training, but it was not used in the training. If the\nsynthetic data is significantly closer to the training data than the holdout data, this means that some\ninformation specific to the training data has leaked into the synthetic dataset. If the synthetic data\nis significantly farther from the training data than the holdout data, this means that we have lost\ninformation in terms of accuracy or fidelity. Sometimes the share of synthetic records that are closer\nto a training than to a holdout record is computed [29]. A share of 50% or less indicates that the\nsynthetic dataset does not provide any information that could lead an attacker to assume whether a\ncertain individual was actually present in the training set.\nDCR is a quantitative and easily interpretable privacy metric. It is widely used both in academia\n[27, 30, 31, 32, 33, 34, 35, 36] and industry [21, 22, 24, 29, 37, 23]."}, {"title": "Nearest Neighbor Distance Ratio (NNDR)", "content": "Nearest neighbor distance ratio (NNDR) measures the ratio of each synthetic record's distance to the\nnearest training neighbor compared to the distance to the second nearest training neighbor. This ratio\nis within [0, 1]. Higher values indicate better privacy. Low values may reveal sensitive information\nfrom the closest real data record. A value close to 0 implies that the point is likely to be close to an\noutlier. Thus, NNDR serves as a measure of proximity to outliers in the original data set and outliers\nare known to be more vulnerable to adversarial attacks [38, 39, 40, 38].\nThere are many similarities to DCR. Both median distance [41] and 5th percentile [27] are commonly\nused. Further, sometimes the train-synth NNDR is compared to the holdout-synth NNDR [41]. The\nholdout dataset is from the same source as the training, but it was not used in the training. If the\ntrain-synth NNDR is significantly higher than the holdout-synth NNDR this is an indication that\ninformation specific to the training set was leaked into the synthetic set. If the train-synth NNDR is\nsignificantly lower than the holdout-synth NNDR, this suggests that information has been lost and"}, {"title": "Nearest Neighbor Adversarial Accuracy (NNAA)", "content": "Nearest neighbor adversarial accuracy (NNAA) is a privacy metric that directly measures the extent\nto which a generative model overfits the real dataset [35, 45]. This type of measurement is important\nas adversarial attacks often exploit model overfitting [46, 47]. The equation for adversarial accuracy\nis shown below.\n$AATS = \\frac{1}{2n} \\sum_{i=1}^{n} 1(d_{rs}(i) > d_{rr}(i)) + \\frac{1}{n} \\sum_{i=1}^{n} 1(d_{st}(i) > d_{ss}(i))$\nIn this equation, the distance from one point in a target distribution R is compared to the nearest point\nin a source distribution S as well as the distance to the next nearest point in the target distribution.\nThese distances are defined as $d_{rs}(i) = min_{j} || x_{i}^{r} - x_{j}^{s} ||$ and $d_{rr}(i) = min_{j,j\\neq i} || x_{i}^{r} - x_{j}^{r} ||$\nPrivacy loss is then defined by the difference between the adversarial accuracy on the test set and the\nadversarial accuracy on the training set as shown in the below equations:\n$(Train AdversarialAcc.) = E[AA_{Train}]$\n$(Test AdversarialAcc.) = E[AA_{Test}]$\n$PrivacyLoss = TestAA \u2013 TrainAA$\nNNAA essentially measures the effectiveness of an adversary in distinguishing between in-distribution\nand out-of-distribution data points based on their nearest neighbors. Yale et al [45] explain in their\npaper that if the generative model does a good job, then the adversarial classifier can't distinguish\nbetween generated data and real data; train and test adversarial accuracy should both be 0.5, and the\nprivacy loss will be 0. If the generator overfits the training data, the train adversarial accuracy will\nbe near 0 (good training resemblance), but the test adversarial accuracy will be around 0.5 (poor\ntest resemblance). Thus the privacy loss will be high (near 0.5). If the generative model does a poor\njob and underfits, it will serve generated data that does not resemble real data. Thus the adversarial\nclassifier will have no problem classifying real vs. synthetic so the train and test adversarial accuracy\nwill both be high (>0.5) and similar, and the privacy loss will also be near 0. In this last case, privacy\nis preserved but the utility of the data may be low.\nNNAA is an increasingly referenced privacy metric [48, 9] that can give a user confidence that their\ndata is safe from adversarial attacks."}, {"title": "Membership Inference Attacks (MIAS)", "content": "Membership inference attacks (MIAs) have become an increasingly common privacy metric. In a\nsynthetic data context, an MIA is when an attacker tries to identify if certain real records have been\nused to train the synthetic data generation algorithm. This is a privacy risk since, for example, if the\nsynthetic dataset is about breast cancer, then the attacker can deduce if the person they found has\nbreast cancer.\nThe first MIA against machine learning models was proposed by Shokri et al [46]. In this attack, the\nadversary trains shadow models on datasets sampled from the same data distribution as the target\nmodel. The adversary then calculates the difference between the perplexity of the target record w.r.t.\nthe target and shadow models, and uses this as a score to decide if the record was a training member\nor not. The main intuition behind this design is that models behave differently when they see data\nthat do not belong to the training dataset. This difference is captured in the model outputs as well as\nin their internal representations. The use of shadow models remains the most common form of black\nbox MIAs [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]. LiRA (Likelihood Ratio Attack) by Carlini\net al [59] is a popular shadow-based MIA. Shadow models are sometimes referred to as reference"}, {"title": "Attribute Inference Attacks (AIAs)", "content": "In an attribute inference attack (AIA) the adversary tries to use knowledge of some of the fields\n(the quasi-identifiers) to determine values for other sensitive fields. Quasi-identifiers are the semi-\nidentifying fields that somehow an adversary has access. See the section on k-anonymity above for\nmore information on quasi-identifiers.\nThe most common AIA is based on the k-nearest neighbors (KNN) algorithm [8, 10, 66, 72, 7, 73, 74].\nAn overview of this AIA is shown in figure 2. In this AIA, it is assumed the attacker has access to the\nsynthetic data as well as the quasi-identifiers for some portion of the real data. For each record in this\nattack dataset, the quasi-identifiers are used to find the k-nearest synthetic neighbors.\nFrom these k-nearest neighbors a mean record is computed. This is done by taking the mode for\ncategorical fields and the mean for numeric. Then all the fields that are not quasi-identifiers are\ncompared between the attack record and this mean nearest neighbor synthetic record. If the fields\nmatch, this is accumulated as a correct prediction for that column and if they don't match it's\naccumulated as an incorrect prediction. From these predictions you can get an AIA accuracy for each\ncolumn. Sometimes column accuracy is weighted by the column's entropy as a way to take inherent\npredictability into account [9, 66]. An overall score is often computed by taking the average of the\ncolumn accuracies.\nOne variation of this common attack is, instead of KNN, a machine-learning model is trained (such\nas a random forest or gradient-boosted trees model) using the synthetic dataset S as a training set\n(where the quasi-identifiers Q are used as predictors and the sensitive variable t is used as the target)\nand then predict the value of t using this model [58, 22]. Another variation is to train the distance"}, {"title": "Privacy Enhancing Techniques", "content": "It is often assumed that since synthetic data typically has no one-to-one correspondence with the orig-\ninal data, it will be sufficiently private. This assumption is not always valid, which has motivated the\ndevelopment of privacy metrics described in section 3. There are, however, a variety of mechanisms\nfor strengthening the privacy of your synthetic data. Below are some of the best practices."}, {"title": "Differential Privacy", "content": "The most widely accepted approach to privacy is the use of differential privacy (DP) for training\nsynthetic data generation algorithms [39, 64, 81, 82]. Differential privacy is a mathematical definition\nof privacy. It provides the guarantee that an algorithm's behavior hardly changes when a single record\nor entity joins or leaves the dataset. DP provides protection against adversarial attacks with high\nprobability. A formal definition is as follows:\nDefinition 1. Differential Privacy: A randomized mechanism M with domain D (e.g. possible\ntraining datasets) and range R (e.g. all possible trained models) satisfies (\u03f5,\u03b4)-differential privacy if\nfor any two adjacent datasets d, d\u2032 \u2208 D and for any subset of outputs S \u2286 R it holds that\n$Pr[M(d) \\in S] \\leq e^{\\epsilon} Pr[M(d') \\in S] + \\delta$\nTwo datasets are considered adjacent if they differ by the existence of one training record. Hence the\nabove definition is saying that the output of a model trained with DP (e.g. the synthetic data) should\nbe roughly the same regardless of the existence of some specific user's record in the training set.\nIn deep learning, DP is often achieved by using a differentially private stochastic gradient descent (DP-\nSGD) algorithm. DP-SGD achieves differential privacy by clipping gradients and adding noise during\nthe optimization process. Other DP algorithms include Private Aggregation of Teacher Ensembles\n(PATE) [83] and algorithms for categorical data, such as Private-PGM (Probabilistic Graphtical\nModels) and AIM (Adaptive and Iterative Mechanisms) [84].\nOne of the challenges of using DP is maintaining reasonable utility of the data. This must be closely\nmonitored when picking an \u03f5 to use. There is an interesting relationship between membershp inference\nattacks (MIAs) and DP. It has been shown that DP provides an upper bound on the effectiveness of\nMIAs and MIAs can be used to create a lower bound or effective \u03f5 [85, 86, 87, 88, 89]."}, {"title": "Pseudonymization", "content": "Another privacy strengthening approach is to use pseudonymization of PII before training the synthetic\ngeneration model. Pseudonymization is a \u201cdata management and de-identification procedure by\nwhich personally identifiable information fields within a data record are replaced by one or more\nartificial identifiers, or pseudonyms\" [90]. Named-entity recognition (NER) is used to detect and\nreplace PII present in both structured and unstructured text fields [91, 92]. Sometimes PII is masked\ninstead of being replaced with pseudonyms. This is commonly referred to as \"PII scrubbing.\" PII\nscrubbing can mitigate membership inference but can lower utility much more than DP does [16]."}, {"title": "Privacy Filters", "content": "Privacy filters can be used to eliminate synthetic records that are vulnerable to adversarial attacks. A\nsimilarity filter is a post-processing checkpoint that actively removes any synthetic data record that\nis overly similar to a training record, ensuring that no such record slips through the cracks, even in\nthe case of accidental overfitting [93, 94, 95]. An outlier filter is a post-processing checkpoint that\nactively removes any synthetic record that is an outlier with respect to the training data. Outliers\nrevealed in the synthetic dataset can be exploited by membership inference attacks (MIAs), attribute\ninference attacks (AIAs), and a wide variety of other adversarial attacks [93].\nWhile removing outliers from the synthetic data can improve privacy, this does not hold true for\nremoving outliers from the original training data. Carlini et al [94] describe a phenomenon called the\n\"Privacy Onion Effect\" where removing the \"layer\" of outlier points that are most vulnerable to a\nprivacy attack exposes a new layer of previously-safe points to the same attack.\nPrivacy filters can provide strong protection against data linkage, membership inference, and re-\nidentification attacks with only a minimal loss in data accuracy or utility. However, they are not\nsuitable for use when a DP guarantee is desired from the synthetic data as they utilize information\nfrom training records."}, {"title": "Overfitting Prevention", "content": "Many adversarial attacks exploit the fact that a generative model is overfit [46, 47]. This means efforts\nto prevent overfitting will likely improve the privacy of synthetic data. Common practices to prevent\noverfitting include using batch-based optimization (e.g., stochastic gradient descent), regularization\n(e.g., dropout layers in deep neural networks), and early stopping."}, {"title": "Remove Duplication", "content": "The success of MIAs has been shown to correlate with the presence of duplicated training data.\nRemoving duplicates prior to training can improve the privacy of synthetic data [96, 97, 98, 99, 100].\nAnother approach is to preprocess the data to limit the number of records containing information\nabout any one individual. The more records there are for an individual, the higher the likelihood of\nexposure to an adversarial attack. One must be careful, though, when deduplicating training data\nbefore applying differentially-private training as this has been shown to lead to adaptive privacy\nattacks [101]."}, {"title": "Data Augmentation", "content": "Data augmentation involves creating new training data by modifying existing data, which can help\nreduce the reliance on sensitive data and minimize privacy risks. As the size of the training set\nincreases, the success of MIAs decreases monotonically [67, 102, 103]. This suggests that increasing\nthe number of distinct samples in the training set can help to alleviate overfitting which then reduces\nthe success of adversarial attacks."}, {"title": "Use a Smaller Model", "content": "Larger models are more prone to memorization. Hence an effective privacy technique is to reduce\nthe size of the model you use, especially if it's an LLM. The performance of most types of MIAs\nincreases as model size increases [67, 97, 99, 104]. The exception is when Parameter-Efficient\nFine-Tuning (PEFT) is used. This method enables efficient adaptation of large pretrained models\nto various downstream applications by only fine-tuning a small number of extra model parameters\ninstead of all the model's parameters. Thus privacy is correlated with the number of parameters tuned\nas opposed to the overall model size. Note that using a smaller model could impact the utility of\nsynthetic data."}, {"title": "Enhance Architecture", "content": "More sophisticated strategies have been further developed for different generative models. For\nexample, privGAN [105], RoCGAN [106] and PATE-GAN [83] equip GANs with privacy protection"}, {"title": "Conclusion", "content": "Recent advancements in generative AI have made it possible to create synthetic data with accuracy\nthat can be as effective as real-world data for training AI models, powering statistical insights, and\nfostering collaboration with sensitive datasets while offering strong privacy guarantees. Effectively\nmeasuring the privacy of synthetic data is an important step in the process. In this paper, we explored\na wide variety of privacy metrics as well as different techniques for improving privacy."}]}