{"title": "Experiments with truth using Machine Learning:\nSpectral analysis and explainable classification of\nsynthetic, false, and genuine information", "authors": ["Vishnu S. Pendyala", "Madhulika Dutta"], "abstract": "Misinformation is still a major societal problem and\nthe arrival of Large Language Models (LLMs) only added to it.\nThis paper analyzes synthetic, false, and genuine information in\nthe form of text from spectral analysis, visualization, and explain-\nability perspectives to find the answer to why the problem is still\nunsolved despite multiple years of research and a plethora of solu-\ntions in the literature. Various embedding techniques on multiple\ndatasets are used to represent information for the purpose. The\ndiverse spectral and non-spectral methods used on these em-\nbeddings include t-distributed Stochastic Neighbor Embedding\n(t-SNE), Principal Component Analysis (PCA), and Variational\nAutoencoders (VAEs). Classification is done using multiple ma-\nchine learning algorithms. Local Interpretable Model-Agnostic\nExplanations (LIME), SHapley Additive explanations (SHAP),\nand Integrated Gradients are used for the explanation of the\nclassification. The analysis and the explanations generated show\nthat misinformation is quite closely intertwined with genuine\ninformation and the machine learning algorithms are not as\neffective in separating the two despite the claims in the literature.", "sections": [{"title": "I. INTRODUCTION", "content": "Misinformation, fake news, and lies have been adversely\nimpacting society in many significant ways. The advent of\ngenerative Al applications such as ChatGPT has only ex-\nasperated the problem [1]. Misinformation can be multi-\nmodal. Generative AI is capable of producing misinformation\nin multiple modalities. However, while the difficulties in\ndetecting fake images are well documented in the literature\n[2], the same is not true for misinformation in the form of\ntext. On the other hand, some studies have reported 100%\naccuracy in detecting AI-generated text using simple language\nmodels such as BOW [3]. There is also abundant literature on\nsolving the misinformation containment problem with human-\ngenerated text but it is a well-known fact that the problem is\nstill largely unsolved [4]. This work expands on the previous\nanalysis of the problem [5].\nGiven the current gap in the literature in sufficiently identi-\nfying the reasons why misinformation containment is still an\nunsolved problem, there is a need to focus on why machine\nlearning is unable to solve the problem despite the tall claims\nto the contrary in the literature, which is mostly based on\nthe currently available embedding techniques. Embeddings are\nessentially representations of the input data in the hidden\nlayers of neural networks. This work is an attempt to determine\nwhat makes it so difficult to identify misinformation based\non the embeddings and the limitations of the current Neural\nNatural Language Processing (NNLP) and Machine Learning\n(ML) techniques in doing so using a variety of experiments\nto visualize, classify, and explain."}, {"title": "II. RELATED WORK", "content": "Misinformation containment is proven in the literature to\nbe NP-hard [6]. Misinformation detection can be addressed\nusing diverse approaches, including algorithms such as the\nKalman Filter [7], statistical techniques and first-order logic\n[8]. However, it is established in the literature that machine\nlearning is a good alternative to heuristic algorithms to solve\nNP-hard problems [9]. A further literature survey naturally\nshows a comprehensive use of machine learning and deep\nlearning in conjunction with NLP techniques to address the\nproblem. There are multiple surveys [10] [11] describing the\nliterature in this regard. Large language models (LLMs) have\nalso been used to detect misinformation and the reasoning\nbehind the classifications has been discussed qualitatively and\nquantitatively using explainability techniques [12].\nThe explainability perspective of misinformation detection\nhas also been explored in the literature. Liu et al. [13] propose\na logic-based neural model for multimodal misinformation\ndetection, integrating interpretable logic clauses to enhance\nexplainability and reliability in detecting misinformation on\nonline social platforms. In another research [14], a method\ncombining two approaches is proposed: (a) Domain Adver-\nsarial Neural Network (DANN) for generalizability across\nplatforms and (b) an explainable AI technique, LIME to\nunderstand the model's reasoning. The authors test this method\non COVID-19 misinformation and show that it significantly\nimproves detection accuracy while providing explanations for\nthe results. Explainable AI and visualization techniques to\nenhance understanding of disinformation detection models are\nproposed to aid in interpreting and presenting results effec-"}, {"title": "A. Contribution", "content": "Most of the current literature detects misinformation using\nprevalent embedding frameworks. This paper investigates the\nfollowing research question: RQ: How effective are the current\nembedding techniques in separating truthful information from\nfalse information? The answer to the question is approached\nfrom visualization first followed by generating insights into\nhow the model arrived at the classification. The work is\nunique in framing the research question and investigating it\nfrom diverse perspectives. In the literature survey, a thorough\nquantitative analysis of the determinants found that the ho-\nmogeneity of the communities in terms of their information\nconsumption pattern is the primary driver for misinformation\nspread [18]. Other than that, to the best of our knowledge,\nthis is the first work that investigates using spectral and\nnon-spectral methods of visualization and explainability, why\nmisinformation containment is mostly an unsolved problem\nusing diverse datasets containing synthetic, genuine, and false\ninformation."}, {"title": "III. DATASETS AND PREPROCESSING", "content": "For a comprehensive analysis, a variety of datasets are used\nfor the investigation."}, {"title": "A. LIAR Dataset", "content": "The LIAR dataset [19] contains around 12,800 short state-\nments collected from various sources such as political debates,\nFacebook posts, news releases, and tweets that are labeled\nmanually. There are six fine-grained labels for each of the\n12,800 statements: true, mostly-true, half-true, barely-true,\nfalse, and pants-fire, indicating the degrees of truthfulness. For\nthis project, the labels were encoded into three categories, the\nfirst class consists of all true labels, the second class contains\nfalse, and the third class contains pants-fire statements. After\nencoding the labels, the dataset has 1047 'pants-fire' state-\nments, 2507 'false' statements, and 9237 'true' statements.\nAfter splitting the data into train, test, and validation sets, the\ntrue and false statements in the train set were downsampled\nto 839 statements in each of the three classes to get a\nbalanced dataset. Since language models such as BERT are\npre-trained, not having a huge dataset is not an issue. The usual\npreprocessing of the dataset was done to make it ready for the\nclassification task. For instance, to rule out any non-English\nocurrences in the statements, all statements containing non-\nASCII characters were removed."}, {"title": "B. Human ChatGPT Comparison Corpus", "content": "The dataset used for the second set of experiments is\na Question Answering Dataset based on the public Human\nChatGPT Comparison Corpus (HC3) data. This dataset's main\nobjective is to compare and examine the variations between\nresponses produced by ChatGPT and human respondents in\na variety of disciplines, such as open-domain, finance, health,\nlaw, and psychology. The dataset contains various question\nprompts, and answers given by ChatGPT and humans. For\nthe experiments, the questions that were asked from Chat-\nGPT were not used. Only the answers given were used.\nThe dataset contains 24,321 question-answer prompts, but for\nexperimentation, only the first 15,000 answers from humans\nand ChatGPT were used. The dataset was already clean and\nbalanced, hence pre-processing was not needed. To train the\nclassification model, 15,000 answers each for both classes\nwere selected. Then the data was shuffled and split into the\ntrain, test, and validation sets in the ratio of 8:1:1. The data\nwas split in such a way that the train, test, and validation sets\nhad an equal distribution of classes."}, {"title": "C. Artificial Intelligence Generated Abstracts", "content": "The Artificial Intelligence Generated Abstracts (AI-GA)\ndataset sourced from GitHub consists of 28,662 entries for\nscientific paper titles, extracted from the COVID-19 Open\nResearch Dataset Challenge (CORD-19) dataset, and their\nabstracts. For half of the paper titles, the abstracts were\ngenerated using GPT-3 and are thus not original. The dataset is\nprelabeled, with label 0 signifying real human-written abstracts\nand label 1 for artificially generated ones. Each class has\n14,331 instances belonging to it, making the dataset balanced,\nand no additional pre-processing was needed to be performed."}, {"title": "IV. METHODOLOGY", "content": "For carrying out spectral analysis of misinformation in text,\nthe statements in the dataset are converted into vectors using\nembeddings techniques based on BERT [20], S-BERT [21],\nwhich build upon the transformer architecture [22] and also\non the Doc2VecC framework [23]. The embeddings project\nthe statements into the latent feature vector space. Spectral\nanalysis and visualization of the statement vectors are then\nperformed using two substantially different techniques - the\nt-SNE [24] and PCA [25] algorithms depicting the actual\nlabels. PCA is a linear approach, while t-SNE is a non-linear\nvisualization technique. Hence the two are fundamentally\ndifferent in their approach. The diversity in the representation\nlearning, classification algorithms, and spectral techniques is\nto ensure that as much as possible, no bias in our conclusions\nis attributable to the technique.\nThe dimensionality is reduced to two so that the latent\nfeature space can be visualized better. The analysis shows\nthat the data is highly non-linear. The statements are then\nclassified using two machine learning algorithms, Support\nVector Machines (SVM) using the Radial Basis Function\n(RBF) kernel and K-Nearest Neighbors (KNN), which are\nknown to perform well on non-linear data. These classification\nalgorithms work in fundamentally different ways as well. The\nhyperparameters such as the kernel function used for SVM and\nthe value of k for KNN are determined through the process"}, {"title": "V. EXPERIMENTS", "content": "Three experiments were conducted on the LIAR dataset to\ndetect the latent patterns in the data. To be able to apply\ndifferent methods to capture the pattern of true, false, or lie\nstatements it was needed to represent the statements as vectors\nconcerning their context. These experiments were done as\nfollows:\n1) Statement Embeddings using BERT\nThe pre-trained model called bert-base-uncased from the\nHuggingface open-source library is used to vectorize the\nstatements. The dataset is downsampled to make it balanced.\nThe final generated vectors are present in a 2D array with 2517\nrows and 768 embedding dimensions. The results of PCA and\nt-SNE are visualized in Fig. 1 and Fig. 2. Both graphs are\nhighly scattered, showing the features are highly non-linear\nand cannot be clustered in a 2D space.\n2) Statement Embeddings Using Doc2vecC\nAs the t-SNE and PCA results of the embeddings using the\nBERT model showed no specific pattern, the doc2vecC model\nwas applied to the LIAR dataset. The Doc2VecC framework\nrepresents a document as an average of the word embed-\ndings of randomly sampled words in that document. Word em-\nbeddings generated by Doc2vecC with respect to the context\nare reported to be significantly better than those generated by\nWord2Vec [23]. An additional corruption model is included\nin the algorithm that gives more importance to informative\nwords and suppresses common words by using data-dependent\nregularization. The original Doc2VecC algorithm produces a\nvector consisting of 100 dimensions. For this project, it is\nchanged to 256 dimensions to capture more features from each\nstatement. The visualizations using PCA and t-SNE are shown\nin Fig. 3 and Fig. 4. The resultant graphs are again highly\nscattered with no specific pattern.\n3) Statement Embeddings Using Sentence-BERT\nSentence-BERT (SBERT) [21] is a modification of the pre-\ntrained BERT algorithm. SBERT embeds sentences faster and\nmore accurately in comparison to BERT and its optimized\nvariant, RoBERTa. This model maps sentences and paragraphs\nto a 384-dimensional vector and can be used for tasks like\nclustering or semantic search. To vectorize the statements of\nthe LIAR dataset, a pre-trained model \u201call-MiniLM-L6-v2\u201d\nwas applied. Post this, PCA and t-SNE were applied to the\nstatement embeddings to visualize the dataset. The results of\nthese two methods are shown in Fig. 5 and Fig. 6. The obtained\ngraphs are highly scattered with no specific pattern."}, {"title": "A. Spectral Analysis of Misinformation", "content": "Three experiments were conducted on the LIAR dataset to\ndetect the latent patterns in the data. To be able to apply\ndifferent methods to capture the pattern of true, false, or lie\nstatements it was needed to represent the statements as vectors\nconcerning their context. These experiments were done as\nfollows:\n1) Statement Embeddings using BERT\nThe pre-trained model called bert-base-uncased from the\nHuggingface open-source library is used to vectorize the\nstatements. The dataset is downsampled to make it balanced.\nThe final generated vectors are present in a 2D array with 2517\nrows and 768 embedding dimensions. The results of PCA and\nt-SNE are visualized in Fig. 1 and Fig. 2. Both graphs are\nhighly scattered, showing the features are highly non-linear\nand cannot be clustered in a 2D space.\n2) Statement Embeddings Using Doc2vecC\nAs the t-SNE and PCA results of the embeddings using the\nBERT model showed no specific pattern, the doc2vecC model\nwas applied to the LIAR dataset. The Doc2VecC framework\nrepresents a document as an average of the word embed-\ndings of randomly sampled words in that document. Word em-\nbeddings generated by Doc2vecC with respect to the context\nare reported to be significantly better than those generated by\nWord2Vec [23]. An additional corruption model is included\nin the algorithm that gives more importance to informative\nwords and suppresses common words by using data-dependent\nregularization. The original Doc2VecC algorithm produces a\nvector consisting of 100 dimensions. For this project, it is\nchanged to 256 dimensions to capture more features from each\nstatement. The visualizations using PCA and t-SNE are shown\nin Fig. 3 and Fig. 4. The resultant graphs are again highly\nscattered with no specific pattern.\n3) Statement Embeddings Using Sentence-BERT\nSentence-BERT (SBERT) [21] is a modification of the pre-\ntrained BERT algorithm. SBERT embeds sentences faster and\nmore accurately in comparison to BERT and its optimized\nvariant, RoBERTa. This model maps sentences and paragraphs\nto a 384-dimensional vector and can be used for tasks like\nclustering or semantic search. To vectorize the statements of\nthe LIAR dataset, a pre-trained model \u201call-MiniLM-L6-v2\u201d\nwas applied. Post this, PCA and t-SNE were applied to the\nstatement embeddings to visualize the dataset. The results of\nthese two methods are shown in Fig. 5 and Fig. 6. The obtained\ngraphs are highly scattered with no specific pattern."}, {"title": "B. AI/Human Generated Text Detection", "content": "To study different techniques that could help distinguish\nbetween text written by humans and synthetically generated\ntext, the following experiments were carried out:\n1) ChatGPT versus Human Answered Questions\nTo classify a text if it has been generated by ChatGPT\nor Human, the pre-trained \"roberta-base\" model from the\ntransformers library was used. The RobertaForSequenceClas-"}, {"title": "VI. RESULTS", "content": "For comprehensiveness, the performance of the SVM and\nKNN models was evaluated using different metrics. Accuracy\nshows how well the models predicted the true-false and true-\nnegatives. Precision emphasizes true positives and the F-1\nscore shows a harmonic mean of precision and recall. In\naddition to accuracy, F-1 score, and precision, ROC accuracy\nwas calculated. In a multi-class classification experiment with\na highly imbalanced dataset, ROC micro-averaging is prefer-\nable over macro-averaging. Micro-averaging will aggregate the\ncontributions of all classes to calculate the average. However,\nin macro-averaging, the ROC is calculated independently of\neach class, and then an average is applied to it that causes it\nto treat each class equally. Thus, for the balanced dataset, it\nis preferable to use ROC macro-averaging.\nThe performance results of classifications using SVM and\nKNN models for BERT, doc2vecC, and SBERT embeddings\nare shown in Table I.\nIn Fig. 15 the ROC accuracy is shown for true vs the rest\nof the classes. The area under the curve gave a 65% accuracy\nbut as shown in Fig. 15 the focus changed to capture whether\na lie was found in a statement or not. Therefore, the ROC\ncurve lies vs other classes and is depicted with an accuracy"}, {"title": "VII. DISCUSSION", "content": "As shown in the spectral analysis and visualization with\ndimensionality reduced to the two most important latent fea-\ntures in Figures 1, 2, 3 4, and 5, 6 and three latent features in\nFig. 7 and Fig. 8, there are no natural discernible clusters\nof the various classes. Each class of statements is spread\nacross the feature space and the classes are not significantly\ndistinguishable. None of the diverse embedding schemes used\nplace the data points in natural clusters. Instead, they place\nthe statement vectors from different classes next to or even\ncoinciding with each other so much so that even classifiers like\nSVM with RBF kernel and K-NN that can classify highly non-\nlinear data also cannot perform well. The numbers in Table I\nare indicative of this fact.\nIt must be noted that the visualizations are highly scaled-\ndown versions of the original feature space but are quite\nrepresentative of the relative positions of the feature vectors.\nBecause of the compacted feature space, many feature vectors\noverlap with each other making it appear as if there are clusters\nin some combinations of the embedding scheme and spectral\nmethod. Adding a dimension and visualizing the embeddings\nin 3D as in Fig. 7 and Fig. 8 does not help much. However,\ncorrelating such figures with the other figures, the dataset\ncharacteristics, and also the accuracy metrics in Table I, it\ncan be concluded that in terms of the embeddings computed,\nthere is not a substantial difference in the embeddings for the\ntrue and other classes of textual information.\nUsing the LIME, SHAP, and Integrated Gradients explain-\nability modules to improve the interpretability of the model's\nresults for the other two sets of experiments also generated\nsimilar results. Based on the top contributing factors, in this\ncase, words, returned by the algorithm, there are no definitive\nwords that characterize fake and real text. Most words are\ngeneric in nature and could have been part of the corpus for\neither classes of text. The use of variational autoencoders to\nvisualize the distribution of fake versus real text samples also\ndid not yield any clear patterns for differentiating between the\ntwo classes.\nThis is true across the diverse embedding schemes that we\nused. The plots, which are of different shapes and distributions,\nindicate that the embeddings generated by the three frame-"}, {"title": "VIII. CONCLUSION", "content": "The complexity of classifying truth from lies is detailed\nin this work in an attempt to answer why the problem\nof misinformation containment is unsolved. The work used\nmultiple schemes of embeddings, datasets, spectral and non-\nspectral analysis, multiple supervised learning algorithms,\nexplainability techniques, and evaluation metrics to analyze the\nreasons. Current representation learning does not significantly\ndistinguish between true, false, and the \"pants on fire\u201d kind of\ninformation with varying degrees of truthfulness. The post-\nhoc explanations generated using LIME, SHAP, and Inte-\ngrated Gradients do not convincingly bring out the distinction\nbetween synthetic, false, and genuine information either. A\npossible research direction is to invent a new embedding\nscheme for text that is capable of capturing the veracity of\nits information content."}]}