{"title": "Towards Making Flowchart Images\nMachine Interpretable", "authors": ["Shreya Shukla", "Prajwal Gatti", "Yogesh Kumar", "Vikash Yadav", "Anand Mishra"], "abstract": "Computer programming textbooks and software documenta-\ntions often contain flowcharts to illustrate the flow of an algorithm or pro-\ncedure. Modern OCR engines often tag these flowcharts as graphics and\nignore them in further processing. In this paper, we work towards making\nflowchart images machine-interpretable by converting them to executable\nPython codes. To this end, inspired by the recent success in natural lan-\nguage to code generation literature, we present a novel transformer-based\nframework, namely FLOCO-T5. Our model is well-suited for this task,\nas it can effectively learn semantics, structure, and patterns of program-\nming languages, which it leverages to generate syntactically correct code.\nWe also used a task-specific pre-training objective to pre-train FLOCO-\nT5 using a large number of logic-preserving augmented code samples.\nFurther, to perform a rigorous study of this problem, we introduce the\nFLOCO dataset that contains 11,884 flowchart images and their corre-\nsponding Python codes. Our experiments show promising results, and\nFLOCO-T5 clearly outperforms related competitive baselines on code\ngeneration metrics. We make our dataset and implementation publicly\navailable\u00b9.", "sections": [{"title": "1 Introduction", "content": "Flowcharts are widely used across documents to represent algorithms, processes,\nor workflows in a graphical manner and provide a clear and concise understand-\ning of complex processes. They contain short textual commands or conditions\ninside various intent-specific shapes, e.g., diamond for decision-making block,\nrhomboid for input, and output. These shapes are connected with directed or\nundirected arrows to define a sequential flow of information and processing. In\ncomputer programming textbooks and software documentation, flowcharts are\nmore often used as a program-planning tool to communicate the complex logic"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Flowchart Understanding", "content": "There have been several attempts to build software for flowchart-to-code conver-\nsion, such as authors in [11], and [30] introduced interactive user interfaces to\nconvert flowcharts to codes on-the-fly in various programming languages. These\nrule-based approaches, however, impose restrictions and do not support the con-\nversion for offline flowchart images like ours. In [35], a platform was designed to\nrecognize flowcharts and convert them to ANSI-C code using structure identifica-\ntion. In [16], a method was proposed for handwritten flowcharts, using rule-based\ntechniques for preprocessing and generating pseudo code. In [8], improved results\nwere achieved in flowchart recognition by combining statistical and structural\ninformation. In [28], the Faster RCNN object detection system was extended\nwith an arrow keypoint predictor to recognize handwritten flowcharts. In [13],\nDrawnNet was proposed, a keypoint-based detector for handwritten diagram\nrecognition.\nA recent work [31] introduced a novel benchmark and dataset for question-\nanswering over flowcharts. However, their flowchart images are unsuited for pro-\ngramming tasks and can not be used for our problem. The work closest to our"}, {"title": "2.2 Large-scale pre-trained Language Models", "content": "The introduction of the transformer [32] architecture has brought a remarkable\nrevolution in natural language processing. Further, to deal with the scarcity\nof labeled data and build a general-purpose model for a wide range of NLP\napplications, Radford et al. [25] proposed GPT, which is based on a transformer-\ndecoder and pre-trained with an unlabeled pool of data in a self-supervised\nfashion. However, it follows a unidirectional autoregressive approach and is not\nsuitable for tasks utilizing information from the entire sequence. Kenton et al.\nintroduced BERT [17], a transformer-encoder-based method trained in a similar\nself-supervised fashion. BERT [17] follows a bidirectional autoencoder nature and\nis unsuitable for generation tasks that utilize information from the previously\ngenerated tokens in the sequence. To deal with the shortcomings of GPT [25]\nand BERT [17], Lewis et al. introduced BART [19], a denoising autoencoder\nthat uses a bidirectional encoder and an auto-regressive decoder. These large-\nscale language models are often fine-tuned with a small set of labeled data for\nthe supervised downstream task. In general, there are other well-explored pre-\ntrained transformer-based methods such as T5 [26], MASS [29], ELECTRA [10],\nand ROBERTa [21]. In this work, we utilize CodeT5 [33], which adopts the\nencoder-decoder-based transformer model viz. T5 [26], and is pre-trained on\nprogramming language data."}, {"title": "2.3 Language Modeling for Code Generation", "content": "A significant amount of effort has been invested in automating software engi-\nneering using deep learning. Recent work has focused on transferable represen-\ntations rather than task-specific ones. Pre-trained NLP models like BERT [17],\nGPT [25], and BART [19] have demonstrated transferability to programming\nlanguages, yielding positive results for a range of code-related tasks.\nFeng et al. [14] introduced CodeBERT, which utilized BERT architecture\npre-trained on programming language and natural language used in the software\ndevelopment domain, with masked language modeling objective. Guo et al. [15]\nproposed GraphCodeBERT as an improvement upon CodeBert by leveraging"}, {"title": "3 FloCo: A novel dataset for Flowchart image to python\nCode conversion", "content": "We introduce a novel large-scale dataset for Flowchart images to python Code\nconversion. We refer to this dataset as FLOCO. It contains 11,884 flowchart\nimages and corresponding python codes. A selection of representative examples\nfrom the FLOCO dataset is depicted in Figure 2. We make FLOCO publicly\navailable for download 2.\nFlowchart-related research has been under-explored in the literature. How-\never, there exist some related datasets such as (a) OHFCD dataset [4] has 419\nhandwritten flowcharts; however, it does not contain the corresponding codes as\ntheir focus is reading handwritten flowchart images and not code generation, (b)\na more recent dataset namely FlowchartQA [31] introduces a synthetic dataset\nfor question answering and reasoning on flowcharts. (c) in [23], authors intro-\nduced a collection of 775 handwritten flowchart images and corresponding C\nprogramming languages. However, codes for this dataset are not publicly avail-"}, {"title": "4 Proposed Approach", "content": "The goal of FLOW2CODE is to generate code from a given flowchart image. We\napproach this task as a sequence-to-sequence generation problem involving two\ndifferent modalities: image (flowchart) and text (code) and propose a framework,\nnamely FLOCO-T5 (Flowchart-to-Code T5 Model) that involves: (i) reading and\nconverting the flowchart image into a sequence encoding, and then (ii) autore-\ngressively generating code using the flowchart encoding. Figure 3 illustrates the\nproposed framework. We describe the two steps in the following subsections:"}, {"title": "4.1 Flowchart Encoding Generation", "content": "In this step, we encode flowchart images into intermediate sequence encodings\nin the form of text. Given the flowchart image, we first detect and recognize the\nflowchart blocks, namely process (rectangle), decision (diamond), input/output\n(rhomboid), and terminal (oval), using the Hough transform-based shape detec-\ntors [6]. We further employ an off-the-shelf OCR method viz. easyOCR [12] to\nrecognize the text within the boxes and on arrowheads for digitized flowchart\nimages. We then match the recognized shapes and text using their respective\ncoordinates, i.e., a text is paired with the name of a block only if the text co-\nordinates lie within the shape coordinates. The final flowchart encoding is a"}, {"title": "4.2 Code Generation", "content": "Inspired by the recent success of large-scale pre-trained code generation models,\nwe adapt Code-T5 [33] a transformer-based baseline trained for code gener-\nation, to our task. To this end, we initially pre-train it on a large number of\nlogic-preserving augmented codes on the masked modeling objective in a self-\nsupervised setting. The pre-training process adds knowledge of flowchart struc-"}, {"title": "Data Augmentation:", "content": "In order to increase the size of the dataset while keeping\nthe logic of codes intact, we explored data augmentation. This has been achieved\nby changing function names and variable names. We augmented the training sub-\nset of the FLOCO dataset. Replacing all functions and variables with a specific\nset of characters would make the dataset biased. Therefore, the function and\nvariable names were constructed randomly using uppercase/lowercase letters,\nunderscore, and/or digits while keeping the naming conventions for the Python\nprogramming language in mind. The length of the function names was chosen\nrandomly from the range of 4 - 13; for variable names, the range was 1-3. Thus,\neach program was augmented in three different ways: changing the function or\nvariable names or changing both function and variable names together. Figure 4\ndepicts all the augmentations corresponding to a sample code. After augmen-\ntation, the train dataset size increased from 10, 102 to 40,408. These 30,306\naugmented codes have been utilized at the pre-training stage of our method."}, {"title": "Masked Modeling Objective:", "content": "Inspired by the success of the Masked Lan-\nguage Modeling (MLM) pre-training objective in BERT [17], we propose an\nanalogous objective specific to our problem. We adopted the pre-trained CodeT5\nmodel and trained it on the augmented codes and flowchart encodings of the\ntrain set of FLOCO. Tokens in the pre-training dataset are masked randomly\nat a probability of 0.15, and we aim to optimize the loss associated with the\nreconstruction of the original sample, as shown below:\n$$L_{mml}(E, \\overline{E}) = - \\sum_{t=1}^{N} log(p(e_t|e_{0:t-1}, \\overline{E})).$$\nwhere $E =< e_1,e_2...,e_n >$ and $\\overline{E} =< f_r(e_1), f_r(e_2), ..., f_r(e_n) >$ represent\nthe ground truth and masked encodings/code, respectively. E is obtained by\napplying the function $f_r(e_i)$ to the ground truth encoding, which randomly re-\nplaces token $e_i$ with the mask token [MASK] with a probability of 0.15. N,\n$e_0$ denotes the length of the flowchart encoding and start token, respectively.\nFigure 5 shows examples of masked modeling implemented for encoding and a\ncode sample. For the encoding input, if we mask the shape of a block (PARAL-\nLELOGRAM in the given example), the model must be able to infer the correct\nshape based on the context and the pattern it has learned during training."}, {"title": "Fine-tuning:", "content": "After pre-training FLOCO-T5 on augmented data, we further\nfine-tuned it on the training data of FloCo, for FLOW2CODE task. Figure 3 (a)\nshows the training pipeline; the given flowchart image is first converted into\nsequence encoding by detecting shapes and the text inside the shapes using\nan off-the-shelf OCR technique. Positional encodings are added to the flowchart\nencodings before feeding them to the encoder. The decoder has access to the out-\nput of the encoder. It starts with a start token, and auto-regressively generates"}, {"title": "5 Experiments and Results", "content": "In this section, we present an extensive experimental analysis on the FLOCO\nbenchmark to verify the efficacy of our proposed model."}, {"title": "5.1 Evaluation metrics", "content": "Following the code generation literature [1], we evaluated the performance of our\nbaselines and proposed model using the following three metrics:\ni. BLEU [24]: a widely used word-overlap metric for assessing the quality of\nmachine-translated text by comparing the n-grams of the generated code to\nthe reference (ground truth) code and counting the number of matches.\nii. CodeBLEU [27]: a specialized metric that evaluates the quality of gener-\nated code, taking into account syntactical and logical correctness and the\ncode's structure as reflected in the abstract syntax tree and data flow, in\naddition to comparing n-grams.\niii. Exact Match (EM): a binary metric that checks if the generated code\nsequence is exactly the same as the ground-truth code."}, {"title": "5.2 Baseline Models", "content": "To evaluate the effectiveness of the proposed method, we compared it against\nthe following four competitive baselines:\nVanilla Transformer [32] is the attention-based encoder-decoder architecture\nupon which the transformer-based pre-trained models are built. By comparing\nthe proposed method with this baseline, we can observe the specific advantages\nof pre-training.\nBART [19] is a pre-trained, bidirectional, autoregressive encoder-decoder archi-\ntecture that was pre-trained on unlabelled natural language data and optimized\nusing reconstruction loss. The noising techniques used were token masking, to-\nken deletion, text infilling, sentence permutation, and document rotation.\nPLBART [1] is an extension of BART and was pre-trained on a large-scale\ndataset containing unlabelled natural language and programming language data.\nThe pre-training objective was denoising autoencoding, and the noising strate-\ngies used were token masking, deletion, and infilling.\nCodeT5 [33] adopted the T5 (pre-trained on natural language) architecture\nand was pre-trained on natural language and programming language data. The\npre-training objectives were span prediction, identifier tagging, masked identifier\nprediction, and bimodal dual generation.\nBy comparing our proposed method with these baselines, we can observe how\nour method outperforms them and understand how it leverages the pre-training."}, {"title": "5.3 Implementation details for Reproducibility", "content": "FLOCO-T5 is implemented using the Huggingface library [34] and utilizes the\nimplementation of CodeT5 [33], using the 'Salesforce/codet5-base' pre-trained\nmodel available on Huggingface. The model contains 222.9 million trainable pa-\nrameters. It consists of 12 encoder, 12 decoder layers, and 12 attention heads in\neach layer. The input encodings are truncated or padded to a maximum length\nof 512 tokens. We optimize training using the Adam [18] optimizer with a learn-\ning rate of le 5, a warmup for 2450 steps, and a batch size of 16. We use the\nsame training configuration in both the pre-training and fine-tuning stages. All\nthe baselines were trained on a single NVIDIA A6000 GPU with 48 GB VRAM."}, {"title": "5.4 Results and discussions", "content": "We evaluate our method on the proposed FLOCO dataset and compare it against\ncompetitive baselines, namely vanilla transformer [32], BART [19], PLBART [1],\nand CodeT5 [33]. Table 3 shows the performance of the implemented baselines\nand proposed FLOCO-T5 on three evaluation metrics. Vanilla Transformer [32] is\ntrained from scratch in contrast to other baselines, pre-trained on large-scale un-\nlabelled data with different self-supervised pre-training objectives. Hence, Vanilla\nTransformer lacks the understanding of language and programming semantics\nand structure, resulting in the lowest performance for all the metrics. BART [19]\nis pre-trained on natural language text and thus, has a better understanding of\nthe semantics and structure of the sequential data, as natural text also has rules,\nstructure, and other syntactical properties. It results in better performance as\ncompared to the Vanilla Transformer for all of the metrics. PLBART is pre-\ntrained on the natural text and programming language, which means it has a\nbetter understanding of code structure and semantics, resulting in better perfor-\nmance compared to BART and Vanilla Transformer on all metrics. CodeT5 [33]\nis pre-trained with programming-language-specific, fine-grained identifier-aware\ndenoising tasks, which help in exploiting code semantics and structure in a more\nexquisite way, resulting in significant improvement over other baselines. In the\nproposed FLOCO-T5, we adopted a pre-trained CodeT5 model, which has task-\nspecific knowledge, and further pre-trained it on augmented training samples for\nthe mask token generation task. As expected, FLOCO-T5 outperforms all base-\nlines for all the metrics used for evaluation, showing the efficacy of the proposed\ncode augmentation and pre-training strategy.\nFigure 6 shows the generated codes for two flowchart samples. We compare\nthe ground truth codes with the ones generated from PLBART, CodeT5, and\nour method. FLOCO-T5 is able to generate codes syntactically correct codes,\nwhich are similar to the ground truth codes, while other baselines fall short in\ngenerating correct codes. This observation is same across other test samples as\nnumerically summarized by Table 3.\nWe further conducted an experiment with three flowchart image encoding\nmethods, shown in Table 2, and results presented in Table 4. The modified string\nencoding method utilized a [SEP] token to separate each step of the flowchart,"}, {"title": "Can the proposed approach work for hand-drawn Flowchart Images?", "content": "We evaluated FLOCO-T5 on hand-drawn flowchart images using 40 samples\ncreated by three human annotators. Flowchart block detection and recognition\nwere performed with OpenCV [6]. For handwritten text recognition, we em-\nployed CRAFT text detection [5] and TrOCR text recognition [20]. FLOCO-T5\nachieved a BLEU score of 21.4% and a CodeBLEU score of 34.6% on these\nhand-drawn flowcharts. Fig. 7 displays Python codes generated for two sam-\nple hand-drawn flowcharts. These results indicate our approach's suitability for\nhand-drawn flowcharts, and performance can be significantly enhanced with ad-\nvances in handwritten text recognition."}, {"title": "Limitations:", "content": "We observed that code generation performance of our model is\nhigher for shorter programs (<\u2248 12 lines) but drops for longer programs (>\u2248 15\nlines) due to the dataset's bias towards shorter programs (average length: 4.6\nlines) as shown in Figure 8. To address this issue, we propose increasing the num-\nber of training samples for longer programs. Future work will focus on expanding\nthe FLOCO dataset to include longer and more complex code and researching\ninformation flow in state and block diagrams."}, {"title": "6 Conclusion", "content": "We introduced the FLOCO-T5 framework for generating Python code from\nflowchart images and presented the FLOCO dataset to benchmark the FLOW2CODE\ntask. FLOW2CODE is modeled as a sequence-to-sequence problem, where flowcharts\nare first encoded by detecting the shapes of blocks and reading the text within,\nand further transformed into code using competitive transformer baselines. FLOCO-\nT5's task-specific pre-training results in significant improvements over related\nbaselines. The recent advancements in Large Language Models (LLMs), such\nas ChatGPT, have revolutionized the field of code generation, and they can be\nadapted to solve our task. However, ensuring that these massive models have not\nseen our test data is not a trivial task. Furthermore, despite these advancements,\nwe firmly believe that our dataset can be used to study open problems such as\ndevelopment of lightweight and interpretable models for generating code from\nflowchart images. We leave these as future directions to work on."}]}