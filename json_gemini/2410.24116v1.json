{"title": "AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization", "authors": ["Amir Kazemi", "Qurat ul ain Fatima", "Volodymyr Kindratenko", "Christopher Tessum"], "abstract": "Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages out-painting to address the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Augmentation with outpainted vehicles improves overall performance metrics by up to 8% and enhances prediction of underrepresented classes by up to 20%. This approach, exemplifying outpainting as a self-annotating paradigm, presents a solution that enhances dataset versatility across multiple domains of machine learning. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of computer vision has undergone significant expansion, driving transformative changes across various domains such as autonomous driving, urban planning, and environmental monitoring. These advanced technologies are set to revolutionize transportation by reducing traffic accidents, easing congestion, and mitigating urban air pollution through improved vehicle operations. Central to these advancements is the robustness and accuracy of object classification and localization algorithms, which depend critically on the quality and diversity of the training datasets available. However, the development of these technologies is hampered by a crucial challenge: the lack of diverse, eye-level vehicle images in desired classes.\nVision-based localization methods, despite their reliance on optimal lighting conditions and susceptibility to obstructions, stand out due to their robust performance in familiar settings. These methods excel by recognizing visual features to enhance navigation and obstacle detection, providing rich contextual data that not only enhances system functionality but also ensures a comprehensive environmental understanding. Nonetheless, the efficacy of these classification and localization models hinges on access to varied and extensive datasets. Public datasets frequently lack adequate eye-level vehicle representations essential for autonomous driving and roadside surveillance applications. Furthermore, these datasets often do not include detailed or desired vehicle categorizations, thereby limiting their practical utility. This is further exacerbated by the presence of smaller or partial vehicles in the background which are not annotated. All these shortcomings can adversely affect the performance of vision-based localization machine learning models tasked with operating in dynamic, real-world settings.\nOur dataset introduces an novel self-annotating approach to generating a high-quality collection of AI-generated, eye-level vehicle images; see Figure 1. The methodology commences with the detection of vehicles within manually-selected existing images using a pre-trained model. This strategic selection process directly influences the quality and usability of the final dataset, ensuring a comprehensive representation of different vehicle classes. Upon detection, we crop these images to create \"seed images\" which can manually be classified as desired. To further augment the dataset and introduce variability, we employ generative AI for outpainting using the seed images. This step involves recoloring and placing the cropped vehicle images onto larger canvases at random coordinates and scales, thereby enhancing the diversity of the dataset and simulating real-world contexts.\nThe contributions of this dataset are manifold, significantly enhancing the adaptability and performance of machine learning models. The use of outpainting not only increases the diversity of the dataset but also simulates real-world conditions where vehicles appear at various sizes in various parts of an image. This simulation is crucial for developing robust algorithms capable of accurate vehicle classification and localization under diverse operational scenarios. Each image within the dataset is automatically annotated with detailed bounding box coordinates, providing valuable ground truth data for training and evaluation purposes. Also, seed images can readily be categorized into desired classes manually before populating them by outpainting. We may use AIDOVECL as a method to augment real vehicle image datasets which suffer from data scarcity or class imblanace. Augmentation with outpainted vehicles improves overall performance metrics by up to 8% and enhances"}, {"title": "2 Background", "content": "Vehicle datasets are essential for improving model accuracy in vehicle recognition across different environments. Despite their importance, these datasets frequently encounter challenges such as limited context around vehicles, broad classifications that obscure finer distinctions, and suboptimal capture angles. These limitations can severely affect tasks requiring detailed environmental information, precise vehicle identification, or particular viewing angles. We briefly review major vehicle datasets to highlight these shortcomings and identify the need for more customizable datasets.\nThe Stanford Cars dataset contains over 16,000 images of 196 classes of cars , covering a wide range of makes, models, and manufacturing years. This dataset is particularly valued for its high-quality images and detailed annotations, making it a standard benchmark in vehicle recognition and classification tasks. However, many images include unlabeled vehicles in the background, and in some instances, the images do not provide enough context, being either too close to the bounding box or surrounded by blank spaces, which is not ideal for training highly accurate localization models. Besides, the dataset does not include heavy trucks and buses.\nMiovision Traffic Camera Dataset (MIO-TCD) dataset  is another significant resource utilized in vehicle recognition and traffic scene understanding. It comprises a diverse set of traffic-related images designed to facilitate the training and evaluation of various algorithms on mobile platforms. While the dataset offers a broad variety of vehicle types (10 vehicle classes) and traffic scenarios, its major drawback is the lack of near eye-level perspectives. Most images are captured from elevated angles, which can limit the dataset's applicability for applications requiring a pedestrian's, driver's, or roadside point of view.\nThe Common Objects in Context (COCO) dataset  is a cornerstone in computer vision, known for its robust features in object detection, segmentation,\nand captioning across diverse scenes. It excels due to extensive annotations, including precise segmentation masks essential for object localization. However, its vehicle categorization is limited to just three classes: car, bus, and truck. This broad classification can be inadequate for specialized applications needing detailed vehicle recognition, such as advanced safety and surveillance systems.\nIn summary, datasets like Stanford Cars, MIO-TCD, and COCO advance vehicle recognition but have limitations such as insufficient contextual details, limited vehicle classifications, and non-ideal viewing perspectives, highlighting the need for customizable vehicle datasets. Addressing these gaps is essential for developing application-specific models for evolving vehicle recognition demands."}, {"title": "2.2 Inpainting Methods", "content": "Image inpainting is a computational technique used to restore and manipulate images, primarily for removing unwanted objects or artifacts. It has applications across various domains, including security, where it plays a critical role in ensuring the integrity of visual information by eliminating potentially compromising elements from shared images. Originally employed for repairing old or damaged images, it has evolved to address a range of distortions such as text, noise, scratches, and lines. While serving as a valuable tool for enhancing image fidelity, it also presents challenges in forgery detection due to the potential for sophisticated manipulation techniques to conceal traces of editing.\nA variety of strategies are employed to reconstruct missing or corrupted image areas, ranging from progressive  and structural information-guided  to attention-based  approaches. These strategies leverage iterative filling processes, structural and spatial data, and controlled information propagation via masks to mitigate color inconsistencies. Pluralistic approaches address the inherent complexity of inpainting by generating diverse possible reconstructions for each image segment . Extending these foundational strategies, various deep learning methods utilize frameworks such as Generative Adversarial Networks (GAN)  and Variational Autoencoder (VAE) integrated with Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN)  to advance inpainting for a diverse set of applications .\nThe recent introduction of Latent Diffusion Models (LDM) represents a significant advancement for the inpainting task . This method leverages latent diffusion processes to synthesize high-resolution images, focusing on intricate details and structural coherence that outpace traditional models. Unlike previous techniques that often concentrated on replicating textures and patterns, LDMs excel in generating and processing contextual and structural details crucial for complex inpainting tasks. The diffusion process iteratively refines images through transitions across latent representations, enabling effective handling of various distortions without extensive manual parameter tuning. This breakthrough offers a powerful, scalable solution for high-fidelity image inpainting, setting"}, {"title": "2.3 Related Data Augmentation Methods", "content": "Data augmentation is a cornerstone technique in machine learning, used to enhance the size and diversity of training datasets, thereby improving the generalizability of models. Traditional methods like flipping and rotation have long been staples, but more sophisticated techniques are necessary to tackle the complex demands of modern AI systems. While augmentation methods such as mosiac and mixup have proved to be promising, the advent of outpainting using diffusion models for object augmentation in diverse environments remains fairly unexplored.\nMosaic data augmentation, introduced in the YOLOv4 paper , represents a significant advancement in training deep learning models for object detection. This technique involves combining four or more different training images into a single composite image. By doing so, it exposes the model to a more diverse array of object scales, positions, and contexts within a single training example. This approach not only helps in enhancing the detection capabilities of the model, especially for smaller objects, but also significantly boosts its ability to generalize to various real-world scenarios. The inclusion of multiple scenes in one image helps in simulating a more complex and varied visual environment, which is crucial for improving the robustness and accuracy of object detection systems. This method has proven to be especially effective in handling datasets where objects appear in dense configurations and diverse backgrounds.\nMixup data augmentation , significantly enhances the training of neural networks for image classification and related tasks. This method operates by blending two images and their corresponding labels, creating a new, synthetic training example. Mixup achieves this blending by a convex mixture of two images and their corresponding labels, with weights that are randomly determined yet constrained to sum to one. By doing this, mixup encourages the model to favor simpler linear behavior in-between training examples, mitigating issues of overfitting and improving model generalization. This linear interpolation between examples provides smoother estimates of uncertainty, which is particularly beneficial in scenarios where models must handle novel or ambiguous inputs. The inherent noise introduced through this process also helps to robustify the model against small perturbations in input space, thus making it more adaptable and robust in practical applications.\nWhile Mosaic and Mixup augmentation techniques have made significant strides in enhancing neural network robustness, they also present notable limitations. Mosaic can sometimes result in overly complex images that confuse the model rather than help it learn, especially when objects from different images do not naturally coexist. Similarly, mixup might lead to ambiguous labels if the images combined are too distinct, potentially misleading the model during training. Furthermore, methods like copy-paste  and x-paste , which rely on precise segmentation masks and may not be"}, {"title": "3 Methods and Setup", "content": "as useful with just bounding box annotations, often face challenges in integrating objects seamlessly into new scenes, leading to unnatural edges or misplaced objects. In contrast, outpainting using diffusion models emerges as a more promising avenue. These models can generate highly realistic and contextually varied synthetic data by extending the canvas of existing images, creating entirely new scenarios that maintain the integrity and realism of the original content. Additionally, the availability of pretrained prompt-guided diffusion models allows for efficient deployment, significantly speeding up the development cycle and utilizing resources merely for inference. This method not only addresses the limitations of conventional augmentation techniques but also provides a scalable solution to train more robust and adaptable AI systems."}, {"title": "3.1 Creating Seed Vehicle Images", "content": "The initial step involves gathering a diverse collection of images of vehicles across the desired categories. This process is manual and entails selecting images where vehicles are presented in a near-eye-level perspective, ensuring that each vehicle is well-separated from smaller background vehicles, if present. Additionally, it is crucial that vehicles are fully visible within the frame, as partial representations result in information loss. For this analysis, our collection comprises over 15000 vehicle images from various sources (including Stanford Cars) which are utilized exclusively for academic and non-commercial research purposes.\nIn the next step, images selected during data collection are fed into a selected pretrained vehicle detection model which is able to detect three classes of car, bus, and truck. Note that we use this model to detect the vehicle bounding boxes but not to determine the vehicle class, as we aim to detect different and more classes of vehicles than are available in pretrained models. We elaborate on selecting the detection model in the next paragraph. The largest bounding box detected in each image is then cropped, including a 15% buffer on all sides to enhance the outpainting process. This buffer is crucial as it helps create a smoother transition between the masked area and the newly outpainted regions. Images with vehicle dimensions smaller than 32 pixels in any direction are excluded to ensure high quality and detail in the seed images. Additionally, we retain the uncropped versions of the selected seed images (about 6000 images) for ablation studies, serving as a baseline.\nWe select the aforementioned detection model by a consensus voting approach, where each model in a predefined ensemble-FCOS , RetinaNet , SSD , MaskRCNN , and FasterRCNN \u2014detects the largest bounding box for vehicles in a given image. These detections are then compared pairwise using the Intersection over Union (IoU) metric, where each bounding box gains a vote for every other box it significantly overlaps with (as defined by a threshold of 0.95). The model with the highest votes (i.e. FCOS) is selected as the primary detector, with the order indicating backup priorities in case the"}, {"title": "3.2 Synthetic Data Generation", "content": "Seed images are randomly scaled and positioned on a 512 x 512-pixel blank canvas. The channels of these seed images are also permuted to diversify the colorization of vehicles. However, this does not alter black and white colors, which is partly advantageous as it maintains the black color of tires. Corresponding annotations are calculated based on the dimensions of the original bounding box (i.e., dividing the buffered seed dimensions by 1.15 to remove the 15% buffer), and the scale and position of the seed on the canvas. We also create a mask image to aid the outpainting modules. This mask maintains the buffer zone unlike the annotation which accounts for the actual bounding box of the vehicle\u2014and will be blurred on borders to ensure a smooth transition into the outpainted areas.\nFor the outpainting process, we employ the stable diffusion inpainting model from Hugging Face  which is based on LDM . The model utilizes both positive and negative prompts. The positive prompt is constructed dynamically according to the vehicle class and follows the pattern: \"A {location} during {time} with no vehicle.\" The {location} parameter can take values from {highway, road, street, downtown, plaza}, tailored to the vehicle type. For instance, TRUCK might use highway, road, street}, while BUS is specifically linked to {street, downtown, plaza}.\nThe {time} variable encompasses {spring, summer, fall, winter, a sunny day, a cloudy day, a rainy day, evening, sunset, sunrise}, providing a temporal context to complement the location. This structured prompt design ensures a contextually appropriate generation of the scene, enhancing the realism of the outpainted areas. To avoid introducing unwanted vehicles, which are not annotated in the previous step, the negative prompt explicitly excludes traffic, train, car, truck, bus, van}. This selection relies on the fact that the masked area remains largely intact during outpainting, allowing us to maintain focus on our vehicle of the desired class. Additionally, we may add {billboard, text, advertisement} to the negative prompt to avoid generating images with billboards includ"}, {"title": "Image Quality Assessment", "content": "We employ outpainting techniques to extend the boundaries of original vehicle images, creating visually plausible extended scenes. The quality of these outpainted images is rigorously evaluated using three no-reference image quality assessment scores: BRISQUE , CLIP-IQA , and total variation (TV) loss . BRISQUE is designed to assess natural scene statistics and quantify potential distortions, typically ranging from 0 to 100, with lower scores indicating better quality. Similarly, CLIP-IQA leverages the CLIP model's ability to assess perceptual quality through deep learning, with scores between 0 and 1 where higher scores suggest superior perceptual quality. Furthermore, we calculate the TV loss for downscaled 32x32 images, a measure that promotes spatial smoothness by minimizing the intensity variation between adjacent pixels. This step is crucial for reducing the likelihood of selecting collage-like images. Utilizing low resolution for this calculation efficiently highlights global discontinuities often found in collage images. This makes it a practical approach for identifying abrupt changes that compromise the continuity of outpainted scenes. We set three criteria: a BRISQUE score of 15 or lower, a CLIP-IQA score of 0.9 or higher, and a maximum threshold of 15 for TV loss. Images that do not meet these thresholds are excluded from further consideration, ensuring that only high-quality, visually realistic extensions are pursued."}, {"title": "Data Splitting", "content": "When generating outpainted images for training, validation, and test sets, we ensure that each set is derived from distinct seed images, eliminating any overlap. Specifically, seed images are allocated into separate training, validation, and test groups, ensuring that vehicles in the validation and test sets are not present in the training set. This approach enhances the robustness of model training by preventing data leakage. Table 2 is presented for further reference throughout the work. To ensure a sufficient and reliable test dataset for evaluation, we allocate 50% of the data to the test split across all studied cases. Meanwhile, the training and validation sets comprise 40% and 10% of the data, respectively."}, {"title": "Background Generation", "content": "To generate background images for our dataset, we utilize the text-to-image stable diffusion model from Hugging Face  based on LDM . This process involves selecting from predefined descriptions that vividly depict various urban scenes devoid of any vehicles, such as empty streets and parks at different times and under various atmospheric conditions. These textual prompts guide the generation of detailed and realistic background images while using negative prompts to"}, {"title": "3.3 Vehicle Classification and Localization", "content": "In this study, we utilize the YOLO (You Only Look Once) object detection framework , specifically employing the YOLOv8 model . We begin by loading a pretrained YOLOv8 model, which provides a robust starting point due to its weights being trained on a comprehensive dataset. The training process is conducted over 1000 epochs, using a batch size of 32, an initial learning rate of 0.01, and an image size of 512x512 pixels. An early stopping patience of 20 is set to prevent overfitting to the training data. For the augmented dataset, which is double the size, we double the batch size to 64 and initial learning rate to 0.02, maintaining the same number of model weight updates and ensuring effective learning progress. This approach leverages the efficiency and accuracy of pretrained models while adapting them to the unique characteristics of our dataset, assessing the object detection capabilities of AIDOVECL as a method to augment real vehicle images.\nIn this work, we compare the performance of trained models using several key metrics: Precision, Recall, mAP50, mAP50-95, Fitness, and F1 Score. Precision measures the accuracy of positive predictions, indicating the proportion of correct positive identifications out of all positive predictions made. Recall evaluates the model's ability to identify all relevant instances, representing the fraction of true positives detected among the actual positives. The mAP50 evaluates the model's accuracy at a 50% Intersection over Union (IoU) threshold, assessing how well the predicted bounding boxes align with the ground truth. The mAP50-95 extends this evaluation across IoU thresholds from 50% to 95%, providing a robust measure of model performance under varying levels of detection stringency. Fitness is specifically defined in YOLOv8 and calculated as a weighted sum of metrics: 10% from mAP@0.5 and 90% from mAP@0.5:0.95, omitting weights for Precision and Recall by default. Finally, the F1 Score combines Precision and Recall into a single metric that quantifies the model's overall accuracy and completeness in detection tasks."}, {"title": "3.4 Compute Resources", "content": "Our experiments were conducted on the Delta advanced computing and data resource at the National Center for Supercomputing Application (NCSA). The specific node utilized features 4 CPU cores, each allocated with 16 GB of memory, totaling 64 GB, and is equipped with an NVIDIA A100-SXM4-40GB GPU. The compute time for outpainting each 512 \u00d7 512-pixel image is approximately five seconds, and generating an image satisfying quality scores requires about 30 seconds on average."}, {"title": "4 Results and Discussion", "content": "Samples of outpainted vehicles, as generated using the methodology described in the previous section, are illustrated in Figure 2. The diverse set of vehicle images results from varying the prompts for location and time of the scene, alongside the inherent randomness in generative inpainting models, and the seed's color, scaling, and positioning on the canvas. Additionally, employing no-reference image quality assessments, such as TV loss, BRISQUE, and CLIP-IQA scores, ensures a degree of realism and natural appearance within the generated images. During attempts per every seed image, the method finds an outpainted image whose location and timing is more relevant to the type and lighting of the vehicle by meeting the quality criteria.\nThe scene integrity accross outpainted images underscores the capability of the inpainting model to generate relevant context around a vehicle, viewed at street level or generally from an eye-level perspective. Notably, the use of negative prompts to exclude unwanted vehicles has proven successful in almost all cases. This approach suggests that explicitly mentioning the masked object as a positive prompt is unnecessary, as the entire image generation is somehow conditioned upon it. Although it might seem counterintuitive, given that guidelines for inpainting models usually suggest including the masked object in the positive prompt, this approach helps us avoid background vehicles that are not self-annotated."}, {"title": "4.2 Object Classification and Localization", "content": "In an ablation study examining AIDOVECL's capability to augment real datasets, we train YOLOv8 models on real images augmented by AIDOVECL, generated solely for the training split and doubling its size (one outpainted image per real image). The validation and test splits consist only of real images. This method assesses how augmentation with outpainted vehicles influences model performance on unseen real images, relative to using only the real dataset. In addition, we evaluate AIDOVECL affect on metrics in the presence of two other augmentation techniques, i.e. mixup and mosaic, as shown in Table 3. The confusion matrices in Figure 3 complements the results by demonstrating improvements over different classes.\nAs Table 3 shows, the augmented dataset outperforms the real one in almost all cases. The improvement in precision is more pronounced than in recall, likely because AIDOVECL does not alter the brand and model of the car, but changes the color, size, and location of it in the scene. Therefore, recall, which measures the diversity of correctly identified classes, may benefit less from the outpainting method compared with precision. However, the F1 score, which incorporates both precision and recall, shows significant improvement in all studied cases. The bounding box prediction accuracies improve in all cases as indicated by mAP50, mAP50-95, and fitness. This can be attributed to the random scaling and positioning of objects in AIDOVECL. Overall, while the Augmented case may not show significant standalone improvement over Real (mosaic) and Real (mixup), it excels when combined with these techniques because the introduction of outpainted vehicles adds unique contextual variations that blend effectively with the randomized environments created by"}, {"title": "5 Conclusion", "content": "The AIDOVECL dataset represents a significant advancement in the field of machine learning for vehicle classification and localization. It addresses the critical issue of annotated data scarcity in desired classes and perspectives, and mitigates the problem of unlabeled background objects in public datasets. By incorporating generative AI techniques\u2014specifically prompt-guided outpainting\u2014AIDOVECL not only enriches the diversity of eye-level vehicle images but also effectively simulates realistic urban traffic scenarios. This approach is crucial for training algorithms capable of performing accurately in dynamic real-world environments. AIDOVECL's innovative self-annotating method, which potentially forgoes the need for manual annotation, offers a substantial improvement over traditional dataset curation methods. It can augment real datasets with class imbalances, thus enhancing the adaptability and performance of machine learning models. This dataset meets immediate"}, {"title": "Limitations", "content": "A primary limitation is that we currently cannot outpaint multiple vehicles within a single image, as our employed inpainting model fails to generate reasonable and coherent scenes with two or more desired vehicles. This challenge stems not only from limitations of the inpainting model but also from the difficulty in selecting correlated seed images that realistically might be observed together. This limitation restricts our dataset's ability to provide rich occlusion cases for objects, which are crucial for training robust detection systems. Additionally, our reliance on pretrained detection and inpainting models for generating seed images and outpainting them presents another limitation. If the objects deviate significantly from those used during model training, they may not be detected, leading to their omission from the pool of seed images, or the outpainting may appear unrealistic. Moreover, there is a need for more images, as seed images must be selected from clean images that meet specific criteria. Therefore, we still need more real data to generate more diverse outpainted images. Addressing these limitations will be essential for enhancing the functionality and applicability of our dataset, ultimately leading to more accurate and versatile object detection models."}, {"title": "Future Works", "content": "To address the existing limitations, enhancing inpainting models with spatial transformer networks or advanced attention mechanisms can significantly improve scene coherence, particularly in handling complex scenarios involving multiple objects. These technologies enable the model to focus selectively on different parts of an image and adjust the spatial relationships between objects, leading to more realistic and dynamically composed scenes. Additionally, advancing data curation methods is crucial. Employing machine learning algorithms that can analyze and predict plausible object-background pairings based on contextual and spatial data could revolutionize how seed images are selected and combined. This approach not only enhances the realism of the generated images but also ensures that the synthetic data better mimics real-world variability, thereby improving the training and performance of object detection models."}, {"title": "Broader Impact Statement", "content": "In this research, we introduce a method for generating training datasets for vehicle detection and classification using outpainting techniques. The potential benefits of our work include enhanced accuracy in automated vehicle detection, which could improve urban planning, traffic management, and autonomous driving, contributing to safer and more efficient urban environments. While our dataset is carefully curated to focus on vehicle features that support these applications, we acknowledge that any form of data capture involving vehicles may contain elements that could raise privacy concerns. We emphasize the importance of ethical guidelines and responsible use of technologies to mitigate potential risks associated with data misuse."}]}