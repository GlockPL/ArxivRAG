{"title": "Continuous K-space Recovery Network with Image Guidance for Fast MRI Reconstruction", "authors": ["Yucong Meng", "Zhiwei Yang", "Minghong Duan", "Yonghong Shi", "Zhijian Song"], "abstract": "Magnetic resonance imaging (MRI) is a crucial tool for clinical diagnosis while facing the challenge of long scanning time. To reduce the acquisition time, fast MRI reconstruction aims to restore high-quality images from the undersampled k-space. Existing methods typically train deep learning models to map the undersampled data to artifact-free MRI images. However, these studies often overlook the unique properties of k-space and directly apply general networks designed for image processing to k-space recovery, leaving the precise learning of k-space largely underexplored. In this work, we propose a continuous k-space recovery network from a new perspective of implicit neural representation with image domain guidance, which boosts the performance of MRI reconstruction. Specifically, (1) an implicit neural representation based encoder-decoder structure is customized to continuously query unsampled k-values. (2) an image guidance module is designed to mine the semantic information from the low-quality MRI images to further guide the k-space recovery. (3) a multi-stage training strategy is proposed to recover dense k-space progressively. Extensive experiments conducted on CC359, fastMRI, and IXI datasets demonstrate the effectiveness of our method and its superiority over other competitors.", "sections": [{"title": "I. INTRODUCTION", "content": "MAGNETIC resonance imaging (MRI) is widely used for clinical diagnosis because of its advantages of no radiation, high resolution, and satisfactory soft-tissue contrast. However, the MRI acquisition process is inherently slow due to both physiological and hardware limitations, as well as the sequential nature of obtaining measurements in the frequency domain (k-space) [1], [2]. The extended scan time required to obtain fully sampled MRI images causes patient discomfort and reduces accessibility to the modality. To speed up this process, accelerated MRI aims to reconstruct MRI images from undersampled k-space [3]\u2013[5]. However, the aliasing artifacts caused by such insufficient sampling often affect the clinical diagnosis. Therefore, it becomes a significant challenge how to reduce the amount of k-space acquisition while maintaining or even improving MRI image quality.\nEssentially, the aliasing artifacts arise because undersampling destroys the k-space\u2019s integrity. Many methods ignore the importance of k-space recovery and focus on reconstruction only from low-quality MRI images [6]\u2013[10]. To this end, several works consider recovering k-space via an external network as shown in Fig.1 (a) [11]\u2013[18]. Despite some progress, they neglect the unique properties inherent to the k-space and directly adopt general CNNs which are not suitable for k-space recovery. CNNs have an inductive bias that their kernels are shared across spatial positions. However, in k-space, spatial positions represent frequency components of sine and cosine functions, and identical patterns at different positions can represent entirely different information [19]\u2013[22]. Previous works overlook this specific characteristic of k-space and adopt conventional image processing networks for processing k-space, resulting in inferior results. Hence, designing a customized network for undersampled k-space recovery is the key to reconstructing satisfactory MRI images.\nIntuitively, a high-quality MRI image is generated from a continuous k-space where each coordinate has a specific signal. Therefore, the ideal reconstruction of k-space should be modeled as a function where arbitrary coordinates can be mapped into a k-value. Such a concept of continuous representation is consistent with implicit neural representation (INR), which emerged as a new paradigm for image super-resolution [23], [24]. Specifically, INR uses a neural network"}, {"title": "II. RELATED WORK", "content": "Traditional MRI techniques involve the acquisition of dense k-space data, resulting in extended scan time. In recent years, a variety of deep neural networks have been explored for accelerated MRI. The pipeline for these methods usually consists of three steps, i.e., roughly recovering the complete k-space from undersampled data by employing a zero-filling strategy, then generating low-quality MRI images via inverse fast Fourier transform (IFFT), and finally learning the mapping relationship between such low-quality MRI images and their corresponding high-quality ones by adapting advanced CNN architectures, including UNet [26], deep residual network [27], generative adversarial network [28], and deep cascaded network [29].\nBesides, benefiting from the capability of learning global information, transformers [30]\u2013[33] have recently achieved satisfactory performance for accelerated MRI. Feng et al. [34], [35] pioneered the introduction of the transformer in the field of MRI reconstruction, integrating MRI reconstruction with super-resolution in a multitask fashion. Huang et al. [36] proposed a parallel imaging coupled swin transformer-based model. To sum up, the data-driven nature of deep learning allows these methods to learn the mapping between low-quality input and artifact-free images. However, such networks operate only in the image domain, making it challenging to ensure the consistency of the k-space.\nTo this end, several attempts have been made to incorporate physics information in this line of work, including enforcing k-space consistency directly after image enhancement or adding k-space consistency as an additional cost function term during training. For example, Hyun et al. [11] enhanced k-space consistency by directly replacing the generated k-values with the corresponding original ones. Yang et al. [12] considered incorporating an additional k-space consistency term into the loss function. Though effective at reducing artifacts, they fundamentally only focus on image-domain restoration, with k-space information used for coarse correction or consistency supervision. The images relied upon by such methods are severely compromised, and this damage is irreversible [37], [38]. Indeed, the continuity of the k-space is crucial for the recovery of high-quality MRI images. However, these approaches neglect this and result in inferior results.\nRecently, some studies began to recover k-space. Taejoon Eo et al. [39] proposed KIKINet, using a combination of 4 different CNNs to operate on k-space, image, k-space, and image sequentially. Osvald Nitski et al. [40] proposed CDF-Net, using two UNet to operate on the image and k-space domain respectively, and combining them into an end-to-end framework. Wang et al. [41] introduced a compressed sensing equivariant imaging prior framework that combines a data preparation strategy for generalizable MRI reconstruction. Zhao et al. [42] proposed SwinGAN, consisting of a k-space generator and an image-domain generator that utilizes swin transformer as the backbone. Such approaches aimed to preserve and leverage the k-space information, achieving more"}, {"title": "III. METHODOLOGY", "content": "1) MRI Reconstrucion: Let K represent the complex-valued, fully sampled k-space acquired from the MRI scanner, accelerated MRI usually employs undersampling to acquire a reduced set of k-space, i.e., Ks, while a substantial portion of the k-space, i.e., Kus, is unsampled. Here, we simulate this undersampling process by the element-wise multiplication (\u2297) of K with a two-dimensional mask M:\n$K\u2083 = M \u2297 K, Kus = (1 \u2013 M) \u2297 K$\nCorrespondingly, low-quality MRI image Is can be obtained, i.e., $I = IFFT(K_s)$. To recover high-quality MRI image 1, deep learning methods typically leverage extensive training data to establish a mapping relationship between \u00ce and the sampled data (Is, Ks), which can be formulated as:\n$\u00ce = f_\u03b8(I_s, K_s)$\nHere, \u03b8 is the parameters set of the deep neural network. In contrast to these methods, our IGKR-Net focuses on recovering Kus, which is the fundamental cause of MRI image blurring and is crucial for high-quality MRI reconstruction."}, {"title": "B. Network Architecture", "content": "As shown in Fig.2, the overall architecture of our IGKR-Net can be roughly divided into two parts, i.e., the primary part directly processing in k-space domain (denoted as red lines), and the auxiliary part operating on the data of image domain\n1) Low Resolution Implicit Transformer (LRIT): LRIT and HRIT share the same structure but differ in their inputs. Here, we take LRIT as an example to explain its structure in detail. As depicted in Fig.2 (a), LRIT is an encoder-decoder architecture aiming to recover LR k-space K\u2081 and its corresponding \u00ce\u2081 by querying the LR coordinates.\nSpecifically, given a set of sampled k-values Ks and its coordinates Cs, a tokenization procedure is firstly applied to convert them into a vector sequence V:\n$V = MLP(K_s) + PE(C_s)$\nwhere a multilayer perceptrons (MLPs) layer is applied to Ks, and PE refers to positional encodings with sine and cosine functions. Then we send V into the encoder of LRIT which consists of multi-head self-attention (MSA) and feed-forward network (FFN) to obtain the latent space S as follows:\n$S = FFN(MSA(V))$\nSubsequently, we design a decoder that employs standard transformer decoder layers consisting of multi-head cross attention (MCA), MSA, and FFN, to recover complete while low-resolution k-space, i.e., K\u2081. In detail, for MCA, we employ encoded LR coordinates $C_{lr} \u2208 R^{2\u00d7H\u00d7W}$ to serve as the query (Q) and apply two different linear projects to latent space S to serve as the key (K) and value (V). After that, we apply MSA to the weighted feature again and decode it via FFN. Thus, we obtain a low-resolution while complete k-space $K_1 \u2208 R^{2\u00d7H\u00d7W}$, as well as its corresponding MRI image $\u00ce_1 \u2208 R^{1\u00d7H\u00d7W}$. Which can be formulated as:\n$K\u2081 = FFN(MSA(MCA(Q, K, V))$\n$\u00ce\u2081 = IFFT(K1)$"}, {"title": "2) Image Domain Guidance Module (IDGM):", "content": "As shown in Fig.2 (b), our IDGM fuses the low-quality image Is and the output of LRIT, i.e., \u00ce\u2081, to fully utilize the original image information and guide the process of k-space recovery.\nFig.3 (a) illustrates the detailed structure of the IDGM, which can be divided into two stages: shallow stage and deep stage. Specifically, during the shallow stage, we firstly perform two convolutions with different strides to extract shallow features from Is and I\u2081 and unify them to the same size, i.e., \u00ce\u2081\u2208 $R^{1\u00d7H\u00d7W} \u2192 R^{h\u00d7H\u00d7W}$, $I\u2208 R^{1\u00d7H\u00d7W} \u2192 R^{h\u00d7H\u00d7W}$, where h denotes the channel number. Subsequently, the features are fused via element-wise summation and restored to Iup \u2208 $R^{1\u00d7H\u00d7W}$ via another convolution and upsampling up(\u00b7), \u0456.\u0435., $I_{up} = up(conv(conv(\u00ce\u2081) + conv(I_s)))$.\nDuring the deep fusion, we first employ two convolution heads to extract image-specific shallow features from Is and Iup, respectively. After element-wise summation, the fused feature is sent into two attention branches, generating local channel attention At without any pooling and global channel attention Ag by using global average pooling, respectively. Given Al and Ag, the final attention weights A can be obtained by the broadcasted addition (+), i.e., $A = sigmoid(A_l+A_g)$. Finally, we obtain 12, as well as its corresponding k-space K2:\n$\u00ce\u2082 = conv(A \u2297 conv(I_{up}))$\n$K2 = FFT(\u00ce\u2082)$"}, {"title": "3) High Resolution Implicit Transformer (HRIT):", "content": "As shown in Fig.2 (c), to further reconstruct the dense k-space with high resolution, we apply the implicit transformer again. Different from the above LRIT, here we use the K' and C' from restored K2 to form vector sequence V', i.e., $V' = MLP(K_2') + PE(C')$. Then we get latent space S':\n$S' = FFN(MSA(V'))$\nFinally, we obtain the high quality k-space K3 and its corresponding MRI image 13 by encoding HR coordinates $C_{hr} \u2208 R^{H\u00d7W}$ as query (Q') and projecting S' as key (K') and value (V'). Which can be formulated as:\n$K3 = FFN(MSA(MCA(Q', K', V'))$\n$13 = IFFT(K3)$"}, {"title": "4) Tri-Attention Refinement Module (TARM):", "content": "As shown in Fig.2 (d), our TARM further refines the reconstruction result 13 in the image domain, providing it with richer image details.\nFig.3 (b) illustrates the detailed structure of the TARM, which comprises a convolution head and three attention branches, i.e., spatial attention (SA), channel attention (CA), and pixel attention (PA). Specifically, for an image that needs to be refined, i.e., I3, the TARM firstly extracts shallow feature f by a convolution. Then, f is sent into three attention branches. To detail, PA generates a 3D attention mask M1 without any pooling or sampling, which means the output features have local information. CA uses global average pooling (GAP) to generate the 1D attention mask, i.e., M2, to obtain features that have global information. SA uses GAP or max average pooling (MAP) to generate a 2D attention mask, i.e., M3, to get features with global information. Finally,\n$\u00ce\u2081 = conv(M\u2081 \u2297 f + M2 \u2297 f + M3 \u2297 f)$\n$K4 = FFT(14)$"}, {"title": "5) Loss Functions and Training Strategy:", "content": "Following [42], [60], we evaluate the intermediate outputs of various modules by using the L2 loss, which consists of two terms. To detail, the first term is used to constrain the consistency between the reconstructed k-space K\u2081 and corresponding fully sampled k-space, i.e., Kir or K, while the second term is used to constrain the consistency between the reconstructed image I and the corresponding fully sampled image, i.e., Ilr or I:\n$L_i = {||K_{ir} - K_{i}||^2 + ||I_{lr} - I_{i}||^2  if i = 1, ||K-K_{i}||^2 + ||I - I_{i}||^2  if i = 2,3,4}$\nwhere I, Ilr, and Kir is obtained via $I = IFFT(K)$, $I_{lr} = downsample(I)$, and $k_{ir} = FFT(I_{lr})$, respsectively. Furthermore, to mitigate the issues of over-smoothing and distortion and obtain finer reconstruction results step-by-step, we design a multi-stage training strategy, as illustrated by Algorithm 1. Specifically, we divide the training process into four stages, i.e., $stage = stage_i$, when $epoch \u2208 [E_{i-1}, E_i]|_{i=1}^4$. In summary, our IGIT-Net recovers the precise MRI images progressively by optimizing the loss at various stages:\n$L_{stage_j} = \\sum_{i=1}^j L_i$"}, {"title": "IV. EXPERIMENTS", "content": "We use two real k-space datasets, CC359 and fastMRI, and one simulated dataset, IXI, as described below.\nThe public brain MR raw data set\u2013the Calgary-Campinas dataset, which comes from a clinical MR\nscanner (Discovery MR750; GE Healthcare, Waukesha, WI, USA) is used to train and test our proposed model. In total, 4129 slices from 25 volumes are randomly selected to form the training set, and the testing set is composed of 1650 slices from 10 other volumes.\nThe fastMRI dataset is currently the"}, {"title": "C. Ablation Studies and Analysis", "content": "To validate the effec- tiveness of our proposed Image Domain Guidance Module (IDGM) and Tri-Attention Refinement Module (TARM), ablation experiments are conducted on these two modules. Specifically, we design two network variants: (1) w/o DIFM, in which the DIFM module is removed from the network and the k-space spectrum outputted from the LRIT is directly used for HRIT; (2) w/o TARM, the TARM module is removed from the network and the MRI image outputted from the HRIT is regarded as final reconstruction result."}, {"title": "V. CONCLUSION", "content": "In this paper, we focused on the problem of image-domain guided k-space recovery for MRI reconstruction. We proposed a novel method, called IGKR-Net, which tackled the key problem of current deep learning based MRI reconstruction methods, namely the insufficient and unsuitable k-space recovery. Specifically, an implicit transformer based k-space reconstruction is proposed to efficiently learn continuous feature space and recover high-quality k-space in the Fourier domain. This mechanism facilitates more precise and effective learning of continuous representations of k-space information, resulting in precise k-space spectrum that are more favorable for MRI reconstruction. Furthermore, an image domain enhancement branch consists of a fusion module and a refinement module is introduced to enable accurate dual-domain feature aggregation and image refinement, thereby guiding MRI reconstruction. Our method demonstrates superior performance on publicly available datasets, confirming its effectiveness."}]}