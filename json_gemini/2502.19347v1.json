{"title": "Controlled Diversity: Length-optimized Natural Language Generation", "authors": ["Diana Marie Schenke", "Timo Baumann"], "abstract": "LLMs are not generally able to adjust the length of their outputs based on strict length requirements, a capability that would improve their usefulness in applications that require adherence to diverse user and system requirements. We present an approach to train LLMs to acquire this capability by augmenting existing data and applying existing fine-tuning techniques, which we compare based on the trained models' adherence to the length requirement and overall response quality relative to the baseline model. Our results demonstrate that these techniques can be successfully applied to train LLMs to adhere to length requirements, with the trained models generating texts which better align to the length requirements. Our results indicate that our method may change the response quality when using training data that was not generated by the baseline model. This allows simultaneous alignment to another training objective in certain scenarios, but is undesirable otherwise. Training on a dataset containing the model's own responses eliminates this issue.", "sections": [{"title": "Introduction", "content": "Generating texts while under strict length restrictions is a complex task that is relevant both in content creation and human-machine communication applications. Modern Large Language Models (LLMs) have been shown to possess the ability to create a wide range of content, which makes them useful in many different fields (Zhao et al., 2024, pp. 70-77). The nature of LLMs however makes the exact length of the texts they generate unpredictable, making them ill-suited for applications with strict length requirements. In this paper, we present a method that enables LLMs to be useful for these kinds of tasks, a problem that has not been addressed in any published research paper.\nThe broader goal of fine-tuning LLMs to better adhere to a set of preferences, on the other hand, is a well researched field and has brought forth a whole range of different fine-tuning approaches (Zhao et al., 2024, pp. 36-43). Besides utilizing the fundamental approach of supervised fine-tuning (SFT), we adapt three reinforcement learning methods, namely Proximal Policy Optimization (PPO; Schulman et al., 2017), Direct Preference Optimization (DPO; Rafailov et al., 2023) and Odds Ratio Preference Optimization (ORPO; Hong et al., 2024) to achieve our objective. These methods are originally combined with the reinforcement learning with human feedback (RLHF) (Ziegler et al., 2020) approach, in which the model learns from human evaluations. However, they have also previously been used to optimize automatically measured performance metrics (e. g. Penzkofer and Baumann, 2024). The difference between the generated and required length of output can also be automatically measured. Thus, we likewise perform fully automated RL.\nImportantly, we generate the necessary training data through a data augmentation process that can easily be performed on most datasets. This makes it possible to add the length objective as a side objective, while the LLM also learns its central tasks (e. g. from human feedback). One example application of the integration of our approach into LLM fine-tuning is text simplification (Feng et al., 2023), where simplified summaries of a user-specified length might further improve accessibility to complex content.\nOther tasks have diversity requirements based on strict outside factors, creating length requirements that are user or situation specific, like a time limit based on user attention span or their commute time, or a space limit based on user eyesight, device settings or hardware properties. Spoken communication between a software assistant and a car driver is one example of a task that has to account for complex environmental factors, as the assistant's responses should never distract the driver. Kennington et al. (2014) have demonstrated how non LLM"}, {"title": "Background", "content": "LLMs are already being applied to a wide range of applications: they excel in classic NLP tasks like text generation and information extraction, are capable of performing information retrieval, act as recommender systems and evaluate both human and LLM generated content (Zhao et al., 2024, pp. 70-77). A key element to their success is their ability to be adapted to very specific tasks, which is achieved by fine-tuning a pretrained LLM using various approaches. In the following sections, we will discuss some of these approaches and explore how they can be applied to enable LLMs to adhere to length requirements.\nThe simplest approach to adapt a pretrained LLM to a specific task is supervised fine-tuning (SFT), which relies on a relatively small dataset of demonstrations, that is often created by humans specifically for this purpose and is generally of high quality. The model learns by generating completions of partial data samples, using the actual completions in the dataset to generate a loss based on their difference. While models adapted using supervised fine-tuning generally perform better at these specific tasks than baseline models, supervised fine-tuning is often just the first step in a process called reinforcement learning with human feedback (RLHF), which is a widely used approach for adapting pretrained LLMs to new tasks.\nOuyang et al. (2024) summarize the process of aligning an LLM to human feedback using reinforcement learning in the following three-step procedure:\n1. Supervised Fine-Tuning: This step consists of fine-tuning a pretrained LLM on a set of human curated demonstration data of prompts and desired responses.\n2. Reward Model Training: In this step, a reward model is trained to evaluate the outputs of the trained model from step one based on human provided preference data.\n3. Policy Optimization with Reinforcement Learning: In the last step, the LLM is trained using a reinforcement learning algorithm, usually PPO (Schulman et al., 2017). The reward model is used to generate training rewards in this step.\nDuring the third step, the expected reward for the responses generated by a policy \u03c0 based on a set of prompts D are simply the predictions of the reward model r(x, y) (Ziegler et al., 2020) for the response y given prompt x:\n$E_{x\\sim D,y\\sim\\pi(\\cdot|x)} [r(x, y)]$\nBased on this, the following PPO objective can be formulated (Ziegler et al., 2020; Rafailov et al., 2023):\n$\\underset{\\theta}{maximize}  E_{x\\sim D,y\\sim\\pi_{\\theta^{SFT}}(\\cdot|x)} [r(x, y)] - \\beta K L [\\pi_{\\theta^{SFT}}(\\cdot|S_t), \\pi_{\\theta}(\\cdot|S_t)]$\nwhere $\\theta_{SFT}$ refers to the model parameters after the supervised fine-tuning step, meaning that the model derived after this step is used as reference.\nOur objective is to minimize the difference between the actual length of the generated text and the target length, which can be measured automatically. Thus, some objective measure based on the length difference can be used as a reward, like squared difference, rendering the training of a reward model obsolete.\nWe thus define our reward function r(y) for a given response y as the squared difference between the length of the response len(y) and the length target specified in the prompt $len_{target}$:\n$r(y) = (len(y) \u2013 len_{target})^2$\nThe Direct Preference Optimization approach to LLM alignment, which was introduced by Rafailov et al. (2023), eliminates the need for a reward model by instead using preference data to align the LLM. Preference data refers to a dataset containing two (or more) possible responses to a prompt, where one of the responses is labelled as \u201cpreferred\". The goal of DPO is, on a high level, to train the LLM to generate outputs that are closer to the preferred responses in the data set and less like the responses that are not preferred.\nRafailov et al. (2023) reparameterize the PPO objective and apply the Bradley-Terry model to produce a reward based on the model's likelihood to produce the preferred and not preferred responses $y_w$ and $y_l$. They derive the following objective:\n$E_{(x,y_w,y_l)\\sim D}  \\beta log \\left[\\frac{\\pi_{\\theta^{SFT}}(y_w|x)}{\\pi_{\\theta}(y_l|x)} + \\beta log \\left[\\frac{\\pi_{\\theta^{SFT}}(y_l|x)}{\\pi_{\\theta}(y_l|x)}\\right]$"}, {"title": "Methods", "content": "We derive a first dataset from the UltraChat dataset\u00b9, which was collected and published by Ding et al. (2023). This dataset is composed of around one million chat-style conversations between two instances of ChatGPT.\nWe use the first question-response pair of each conversation only and we augment the question by adding a short sentence stating the length requirement to its end. The length target in the augmented prompt is set to match the length of the response that is contained in UltraChat, making them the desired response. This method of data augmentation can be applied to any data set that is formatted in a prompt-response style, making it easy to integrate into an existing training setup for a given task.\nWe also create a second dataset containing the same questions as in UltraChat but responses generated by baseline Llama 3.1. This dataset has the disadvantage of being more time-consuming to\nWe choose the UltraChat dataset for its large size and diverse content, which allows us to evaluate the fine-tuning process for multiple tasks and to reduce the risk of underfitting. A difference in data quality between synthetic (LLM-generated) and human-made data was not a primary concern, as we were not interested in improving the quality of the models responses. We specifically use samples from the \"questions about the world\" portion of UltraChat.\nWe use the Llama 3.1 8b model for our experiments, which is part of a project by Meta AI to provide large state-of-the-art pretrained LLMs to the research community free of charge (Touvron et al., 2023). Their newest models, which were released in 2024 and are referred to as the \"Llama 3 herd of models\", are a range of LLMs of different size and capability, which were developed by AI@Meta (2024). We additionally use Quantized Low Rank Adaption (QLora, Dettmers et al., 2023) during model adaptation to shorten the training time and reduce the memory usage of the model.\nTo measure if the response quality of the model degrades during training, we employ the following measures: To estimate response quality, we use the semantic similarity of the model's responses to high quality reference responses. We measure semantic similarity using the F1 measure of Sem-Score (Aynetdinov and Akbik, 2024) and use the ChatGPT-generated responses contained in the UltraChat dataset alongside the responses of the untrained baseline model as references.\nAs an additional precaution, we use the language tool utility (Myint et al., 2012) to check for grammar, spelling and syntax errors.\nWe train Llama 3.1 using supervised fine-tuning for 10 epochs, each consisting of 128,000 samples from our first dataset, saving a copy of the model after every epoch. These copies are then compared on an evaluation dataset consisting of 1,280 new samples, with the model that performs best given its training time being chosen for further fine-tuning.\nChoosing such a high number of epochs allows us to explore whether training for multiple epochs could be a useful strategy in a data limited scenario. We are specifically interested in examining whether there would be an inverse relation between the models' adherence to length requirements and the quality of its outputs, making such a strategy potentially disadvantageous.\nWe then further train the best model from the previous experiment using PPO, DPO and ORPO2\nWhile ORPO is usually applied without any prior SFT training, doing so would have required the creation of another large preference data set containing baseline Llama 3.1 responses. We decided against creating this dataset due to resource constraints, and instead apply ORPO similarly to"}, {"title": "Results", "content": "After training with supervised fine-tuning, the models responses align significantly closer to the length requirements. The mean relative deviation from the length requirement was reduced from 108% to 7.61% after one epoch (\u2248 92% decrease) and then to 6.05% after the third (another \u2248 21% decrease). After the third epoch, the improvement became smaller and less stable, decreasing to 5.35% after the tenth epoch (another \u2248 12% decrease), but also increasing intermittently.\nWe decide to pick the model that was trained for three epochs as a viable starting point for further experiments and comparisons as we felt improvements afterwards were too marginal to justify the\nadditional training time.\nThe results of further fine-tuning the SFT model using reinforcement learning are mixed. The ORPO-trained model is the only one to consistently outperform the SFT model it is based on across all length requirements (3.12% mean relative deviation from length requirement, \u2248 48.4% decrease), even if the gain is only marginal on the character count length requirement. The DPO model's performance is inconsistent, outperforming the SFT model overall (4.64% mean relative deviation from length requirement, \u2248 23.3% decrease), but performs significantly worse on the character count and speech length requirements than both the ORPO model and the SFT baseline model. The results of the PPO model are worse than those of the baseline SFT model (7.16% mean relative deviation from length requirement, \u2248 18.3% increase).\nWe therefore decide to base our further experiments on the ORPO model, as we deem its improvement big enough to be worth the additional training time. This model will be referred to as Model 1 below. We train another model (Model 2) using the dataset that contains responses generated by the baseline Llama 3.1 model, using the same approach of three epochs of SFT followed by ORPO for comparability.\nOf the two final models, Model 1 outperforms Model 2, deviating less from the length requirement (see Figure 4 for details). Importantly, however, Model 2's responses share the same semantic similarity to the reference responses as the baseline model, indicating no deterioration in response quality after optimizing for length.\nIn contrast, Model 1 responses are more similar to ChatGPT-generated responses, which is hardly surprising, given that the ChatGPT style was part of its training. Overall, we do not find relevant changes in SemScore or gramaticality ratings, indicating that optimizing for length requirements does not lead to response deterioration.4\nWith respect to the four kinds of length requirements, we find that these are, of course, highly correlated. Our trained models' performance (relative error of length) does not significantly differ between the length requirements, indicating that they all work similarly well. (However, models fail to generalize to another requirement, number of words in the response.)"}, {"title": "Discussion", "content": "We presented a method for adapting LLMs to adhere to length requirements, demonstrating that existing fine-tuning techniques are sufficient for this task. Specifically, SFT and ORPO proved to be the most effective among the methods tested.\nThe reason for this might lie in their design: SFT trains the model by adjusting its output directly based on the difference of individual output tokens compared to tokens in reference data, while PPO and DPO use a reinforcement learning-based approach where they propagate backwards from a reward that evaluates the entire output text. ORPO combines both approaches. On a more abstract level, SFT-based approaches train the model to \"imitate\" the reference texts, while reinforcement learning approaches instead train it to produce texts that are evaluated highly based on an objective (Zhao et al., 2024, p. 41).\nIt has been demonstrated that this allows reinforcement learning-based approaches to be more successful than SFT when aligning LLMs to abstract objectives that center around learning from human feedback (Ouyang et al., 2024; Stiennon et al., 2020; Ziegler et al., 2020). If adhering to length requirements is, however, a fundamentally simpler objective than other common fine-tuning objectives, then using SFT-based approaches might be sufficient, explaining the success of SFT in our experiments.\nThe comparative under-performance of some reinforcement learning-based approaches could be explained by their inherent instability (Zhao et al., 2024, p. 43), which might lead to longer convergence times. ORPO on the other hand, combining both approaches, might be able to utilize their respective advantages for further gains.\nOverall, we find the following data augmentation approaches useful: when training for adherence to length requirements as a side objective, or when strict adherence to them is the most important concern, modifying the prompts in an existing prompt-response style dataset to include length requirements is both effective and efficient. However, note that this approach changes the response behaviour and therefore potentially the response quality of the LLM. In situations where this is unacceptable, using prompts from a dataset to generate responses using the baseline model, then augmenting the prompts with fitting length requirements, could be a better approach, but produces a model that tends to deviate more from the requirement.\nLimitations to our findings are the fact that we only tested one model that was trained on augmented data taken from a single dataset, oriented towards a single task. The length requirements we used were also quite long (see Figure 2) on average, making it hard to generalize our findings to tasks that require very short responses. In particular, we find that our models are unable to deal with length requirements outside the range that they were trained with, as is exemplified in Appendix A."}]}