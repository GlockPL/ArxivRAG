{"title": "HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes", "authors": ["Xuanyu Su", "Yansong Li", "Diana Inkpen", "Nathalie Japkowicz"], "abstract": "Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within \"Confounder Memes\". To address this, we introduce HATESIEVE, a new framework designed to enhance the detection and segmentation of hateful elements in memes. HATESIEVE features a novel Contrastive Meme Generator that creates semantically paired memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments on the Hateful Meme Dataset show that HATESIEVE not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. Caution: Contains academic discussions of hate speech; viewer discretion advised.", "sections": [{"title": "Introduction", "content": "The emergence of Large Multimodal Models (LMMs), such as GPT-4V (Achiam et al., 2023), Stable Diffusion (Rombach et al., 2022), and DALL-E (Ramesh et al., 2022), has ushered in a new era where people increasingly rely on these models for generating and interpreting visual and textual information. While these services simplify access to information, they also introduce the risk of unregulated content that could distort public perception and cause harm to social groups (Su et al., 2023; Qu et al., 2023; Chin et al., 2023; Qu et al., 2024; Meng Huat Tiong et al., 2024; Lin et al., 2024). To address this risk, current LMM platforms implement safety filters, incorporating Alignment (Ghafouri et al., 2023), Inference Guidance (Chiang et al., 2023), and Input&Output Filter (Alon and Kamfonas, 2023) to detect and eliminate offensive or inappropriate components in both images and text.\nHowever, these safety filters face challenges in identifying \"Confounder Memes\" (Kiela et al., 2020; Mei et al., 2023), which deliberately combine visual and textual elements to convey biased and discriminatory messages. Such memes may not have obvious offensive content in their individual components, instead delivering their harmful messages through their combined presentation.\nA straightforward solution involves supervised fine-tuning (SFT) of LMMs (Lin et al., 2024) to recognize hateful semantics in memes. Nevertheless, this approach encounters obstacles: (1) the scarcity of detailed annotations in existing hateful meme datasets, making it difficult for models to accurately distinguish between hateful and non-hateful memes, especially when the differences are subtle. (2) Deploying LMMs as safety filters alongside their regular online service\u00b9 usage is computationally intensive and non-trival (Lin et al., 2024). Alternatively, a lightweight classifier (Kumar and Nandakumar, 2022a; Mei et al., 2023) could be trained from scratch using a specialized hateful meme dataset, but this method suffers from limited interpretability and cannot provide a clear rationale for its classifications.\nTo address these challenges, we introduce HATESIEVE, a new framework for detecting hateful memes, detailed in Figure 2. HATESIEVE features an Image-Text Alignment (ITA) module that generates context-aware embeddings using contrastive learning to differentiate between semantically similar memes with varying sentiments. Owing to the absence of suitable annotations in existing datasets, we developed a contrastive meme generator that produces related triplet pairs for training. Pre-training the ITA on this dataset enables it to segment hateful content across image and text modalities effectively, with subsequent fine-tuning on classification tasks demonstrating enhanced performance over current models.\n1.  We propose a unique contrastive meme generator that synthesizes semantically similar triplet pairs, addressing the gap where specific pairwise annotations are lacking in existing datasets.\n2.  Our ITA module efficiently produces context-aware embeddings for images and texts, improving the detection and segmentation of hateful content.\nEmpirical experiments conducted on the Hateful Meme Dataset (Kiela et al., 2020) validate that HATESIEVE not only surpasses existing large multimodal models (LMMs) in performance\u2014with"}, {"title": "Related Work", "content": "Safety Filter: Existing safety filters for Large Language Models (LLMs) and LMMs typically comprise Alignment (Ghafouri et al., 2023; Touvron et al., 2023; Rafailov et al., 2024; Wu et al., 2024), Inference Guidance (Bai et al., 2022; Chiang et al., 2023; Zhang et al., 2023), and Input/Output Filter components (Alon and Kamfonas, 2023; Hu et al., 2023). Alignment involves fine-tuning LLMs to meet safety objectives using methods such as reinforcement learning from human feedback (RLHF) that optimize models based on safety data and human preferences. Inference guidance steers models towards generating safer responses through system prompts and token selection adjustments during generation. Input&Output filters detect and manage harmful content. However, these methods are primarily designed for unimodal content and struggle to adapt to multimodal content, such as confounder memes.\nAlignment necessitates retraining LLMs and massive annotated preference dataset, which is inefficient for online services. Inference guidance depends on LMMs correctly identifying hateful content in memes, which is not always applicable. Additionally, current Input&Output filters generally cater to single modalities, such as the IMSyPP text classification model (Kralj Novak et al., 2022) for text and NSFW filters (Rando et al., 2022) for images in diffusion models. Our HATESIEVE framework addresses these limitations by functioning as an Input&Output filter specifically designed for the meme. It allows to identify and segment both the visual and the textual elements within memes.\nHateful Meme Detection: Current methods for detecting hateful memes generally fall into two categories. The first category, reasoning-based, uses LMMs like LLaVA (Liu et al., 2024) and InstructBLIP (Dai et al., 2024) that generate visual prompts (Li et al., 2023b) based on images. These prompts are concatenated with text data for comprehensive analysis, allowing the LMMs to offer detailed classifications and explanations (Lin et al., 2024). This enables users to assess biases and gain deeper insights into hateful content. However, this approach relies heavily on carefully tailored prompts specifically designed for hate speech detection, making it difficult to create a universal prompt that fits all hateful contexts (Lin et al., 2024). Even minor changes can cause LMMs to misinterpret or overlook hateful memes (Rizwan et al., 2024). While SFT can make LMMs less dependent on prompt design, it is time-consuming and computationally intensive, posing challenges for deployment as safety filters in online services.\nAnother category of methods uses representation learning and includes lightweight methods such as MOMENTA (Pramanick et al., 2021), PromptHate (Cao et al., 2022), and HateClipper (Kumar and Nandakumar, 2022b). MOMENTA constructs intra-modality attention by integrating external facial recognition data and background knowledge with the CLIP model. PromptHate converts images into text and then classifies them using a language model. HateClipper creates an image-text interaction matrix to fuse multimodal information. These methods enable straightforward classification with fewer parameters, but they offer limited interpretability of their classifications.\nIn contrast, our HATESIEVE framework generates context-aware image and text embeddings that enable effective meme segmentation and provide visual interpretation, while delivering classification performance comparable to existing methods."}, {"title": "Methodology", "content": "The HATESIEVE workflow involves: 1) Generating a triplet dataset with the Contrastive Meme Generator. 2) Pre-training the ITA module using the triplet dataset. 3) Extracting attention maps and performing segmentation with the pre-trained ITA. 4) Fine-tuning the ITA for hate speech classification."}, {"title": "Contrastive Meme Generator", "content": "Our Contrastive Meme Generator is designed to produce both non-hateful and hateful versions of any given meme {(Ii, Mi, Ti)}_1^N, where I\u2081 \u2208 RH\u00d7W\u00d7C is the image pixels of the meme, Mi \u2208 RH\u00d7W_is the caption mask, and Ti is the caption overlaid on the meme. These non-hateful and hateful versions are then used for subsequent contrastive learning. The first step in our Contrastive Meme Generator (CMG) is modality separation. By isolating the caption from the meme, we remove text borders and artifacts that may interfere with the image information, ensuring clean image content. Specifically, we employ the LaMA image (Suvorov et al., 2021) inpainting pipeline to extract the pure image content I = fLaMA(Ii, Mi) from the meme.\nTo generate the non-hateful version meme (I+, T+), we utilize InstructBLIP (Dai et al., 2024) to create a positive caption T+ = fInstructBLIP(I) of the image content, our prompt is written as follows: \"Please generate a positive and descriptive caption for the provided image {I}.\" Then, we utilize SDXL with SDEdit (Meng et al., 2021) to produce a high resolution non-hateful image I+ = fSDXL(T+).\nConstructing a hateful version of a meme presents significant challenges due to the absence of direct annotations regarding ethnic groups, religious affiliations, social groups, or cultural identities in the meme (Ii, Ti). This lack of explicit metadata complicates the generation of semantically similar hateful memes. To address this, we selected the largest available multimodal hate speech dataset, MMHS150k (Gomez et al., 2020), focusing specifically on its \u201chateful\" category for our reference dataset. We then employed the CLIP image encoder (Radford et al., 2021) to derive embeddings for all image representations I\u00af \u2208 RN\u00d7d, where N represents the total number of memes in the dataset, and d is the dimension of the embeddings. These embeddings serve as our reference embedding database. For each image Ii, we apply FAISS (Douze et al., 2024) to find the most similar"}, {"title": "Triplet Dataset Generation", "content": "Our study constructs triplets of pairs for contrastive learning, each composed of an original meme (Ii, Ti) and its two variations:\n{(Ii, Ti), (INon-Hate, TNon-Hate), (IHate, THate)}\nTo ensure the differentiation between hateful and non-hateful memes while preserving semantic coherence, each component within a meme\u2014-either an image Ii or text Ti--is subjected to a pre-filtering process to identify potentially offensive or controversial content. Initially, each meme undergoes a dual filtering process as follows:\n\u2022 Text Filtering: Employing the IMSyPP Filter, the textual content Ti is evaluated for offensive or violent/controversial material, yielding a classification y, where 1 indicates offensive (controversial) and 0 indicates non-offensive (safe).\n\u2022 Image Filtering: Using the NSFW filter from Stable Diffusion, images Ii are assessed for inappropriate content like nudity or violence, with y being 1 for NSFW (controversial) content and 0 for safe content.\nAs shown in Figure 4, based on the filtering results, we construct the triplet dataset with the following process. For non-hateful pairs, designated as (INon-Hate, TNon-Hate), we sample from several combinations:\n\u2022 (I+, T+) comprises both image and text generated by the CMG, ensuring that both elements are safe.\n\u2022 (I\u2081, T+) includes the original image deemed safe by the NSFW filter, paired with text that has been generated by the CMG as safe text.\n\u2022 (I+, T\u2081) features an image generated by the CMG, paired with the original text that has been classified as safe.\nConversely, for hateful pairs (IHate, THate), we sample from the following scenarios:\n\u2022 (I, T\u2081) where both the image and text are generated by the CMG as controversial, embodying content flagged for potential offensiveness.\n\u2022 (Ii, T\u2081) utilizes the original image classified as controversial, paired with controversially generated text by the CMG.\n\u2022 (I, T\u2081) combines a controversially generated image by the CMG with the original text classified as controversial."}, {"title": "Image-Text Alignment Module", "content": "For each meme (Ii, Ti), our ITA module is designed to derive a token-level, context-aware representation that integrates both the image and the text components, as illustrated in Figure 5. The process unfolds as follows:\nFirst, we leverage a pre-trained CLIP encoder to extract initial embeddings for each modality. Specifically, we derive pooled embeddings for text, Tool \u2208 Rd, and for images, IPool \u2208 Rd, using fCLIP (Ii, Ti). Additionally, we further extract Ti and I\u00bf, where Ti \u2208 Rlxd and I\u00bf \u2208 Roxdi, using CLIP's text and image encoders, respectively. Here, I represents the text sequence length, o the image patch size, di the dimension of the image embedding, and d the dimension of the text embedding.\nThen the combined image-text embedding is constructed as X\u2081 = [W1I;,Ti], where Xi E R(o+l)\u00d7d and W\u2081 is a projection layer designed to map I\u00bf into the same dimensional space as Ti.\nTo achieve an aligned token-level representation between image and text, we introduce a text-image intra self-attention mechanism, defined as:\nAttn = Softmax( (XWq)l(XWk)l)T XWvl / \u221adk  \nwhere dk is the key dimension, l denotes the layer number, and Wql, Wkl, Wvl are the weight matrices for the query, key, and value components in the self-attention layers. The image-text representation is obtained through:\nX = fAlign(Attnx-1)\nwhere fAlign fl represents the l-th self-attention block within an L-layer Image-Text Alignment module.\nAfter processing through L layers, the output image-text representation X is split and subsequently pooled using the original pooling layer from the CLIP model to form I Align and TAlign. The final image-text representation is then constructed as follows:\nH\u2081 = foc(LN ([Align, Align] [IPool, Tool]))\nwhere \u2295 denotes the operation for residual connection, LN represents layer normalization, and fDC is the decoder, which is constructed with a multi-layer perception (MLP)."}, {"title": "Training Objective", "content": "Our ITA training regimen is organized into two distinct phases: 1) Pre-training through contrastive learning, which equips the ITA module with the ability to effectively segment image and text components within hateful memes, and 2) Fine-tuning for classification tasks, enhancing its ability for specific applications.\nGiven the generated triplet dataset D = {(Ii, Ti), (INon-Hate, TNon-Hate), (IHate, THate)}P1, where P denotes the total number of triplets, we extract the image-text representations for each element in the set as {H, HNon-Hate, H Hate}. For each triplet, where Yi= 1 indicates a hateful meme, we identify HHate as the positive pair H and Non-Hate as the negative pair H. The reverse holds for non-hateful memes with y\u2081 = 0. The contrastive learning objective is formulated as follows:\nLtri = \u2211 max (0, d(H\u2081, H+) \u2013 d(H\u2081, H\u00a1\u00af) + \u0454)\ni=1\nwhere d represents the Euclidean distance and e is a predefined margin that ensures a minimum discernible difference between the distances of similar and dissimilar pairs.\nTo adapt the ITA module to the hateful meme classification task, we introduce an additional classification layer fe, parameterized by 0, and fine-tune it using the following loss function:\nLcls= -log P(Yi|Hi; 0)\ni=1^N\nwhere N is the number of examples in the original Hateful Meme dataset."}, {"title": "Hate Component Segmentation", "content": "Our hate component segmentation is structured as follows: After the ITA module is pre-trained via contrastive learning, it can process any given meme (Ii, Ti) to extract a series of self-attention maps {Attn}_1^L from all layers. We begin by averaging these self-attention maps across layers to obtain Attn. We then isolate the image attention map Attn\u00ed, and the text attention map Attn,1t, where 1 < lj < L\u2081 + 12 and L\u2081 + 1 < lt < Lv. Here, lj denotes the j-th image patch among a total of L\u2081 patches, and lt indicates the t-th text token within a maximum of Ly text tokens.\nSubsequently, we compute the text-aware image attention for each patch:\nAttn_{lj}^{l}= \u2211 Attn_{lj,lt} / LT\nl=0 ^ LT\nand the image-aware text attention for each text token:\nAttn_{lt}^{i}= \u2211 Attn_{lt,lj} / LI\nl=0 ^ LI\nTo construct an image segmentation map, we employ bilinear interpolation to upscale the L\u2081 \u00d7 LI patch-level attention maps to H \u00d7 Wpixel-level resolution, facilitating detailed visual analysis of the meme components. As for the text segmentation, we select the Top-k tokens based on the attention scores per token, which allows for precise identification and analysis of the most contextually significant textual elements within the meme.\nTo enhance detailed object segmentation, we developed an object highlighting pipeline illustrated in Figure 6. Initially, we extracted the attention map, Attn\u00ed, using HATESIEVE and subsequently employed SegmentAnything (Kirillov et al., 2023) to detect and segment objects within the meme. This process produced a series of segmented objects, represented as O = [01,...,On]. We assessed the importance of each object, (or), by integrating the attention map with the object mask using RoIAlign (He et al., 2017). To isolate only the most relevant objects, we implemented a threshold criterion, \u03a6(0) > \u03bb, where X is the pre-established significance threshold."}, {"title": "Experiments", "content": "Dataset Our dataset generation for the triplet model leverages the HatefulMemes (Kiela et al., 2020) and MMHS150k (Gomez et al., 2020) datasets. We evaluated our framework by validating the classification and segmentation effects using the HatefulMemes test-unseen dataset. This dataset, provided primarily by Facebook Research, comprises 10,000 annotated meme images. Each meme integrates text and an image to challenge the model's proficiency in detecting hate speech within memes. The MMHS150k dataset contains 150,000 tweets, each featuring an image and text pair, compiled using 51 terms from Hatebase. These tweets were collected between September 2018 and February 2019, specifically to analyze hate speech expressions on social media. Our training dataset for contrastive learning consists of 8,500 entries from the HatefulMemes training dataset and 33,844 hateful data samples from MMHS150k by our contrastive meme generator.\nBaselines In the classification evaluation for our HATESIEVE, our baseline models that we compare to are as follows:\n\u2022 LMMs: The set includes GPT-4V (Achiam et al., 2023), CogVLM (Wang et al., 2023), LLava-1.5 (Liu et al., 2023), InstructBLIP (Dai et al., 2024), MiniGPT-4 (Zhu et al., 2023), Qwen-VL (Bai et al., 2023), OpenFlamingo (Awadalla et al., 2023), MMGPT (Gong et al., 2023), and MinGPT-v2 (Chen et al., 2023) for zero-shot inference, along with LLava-1.5, InstructBLIP, and BLIP2 employing Supervised Fine-Tuning with QLora (Dettmers et al., 2024).\n\u2022 CLIP-based method: It encompasses the original CLIP model along with its variants, HateCLIPer and MOMENTA.\nWe provide detailed descriptions of the methods we implemented for these experiments in Appendix A. Evaluating the segmentation capabilities of HATESIEVE is challenging due to the absence of pixel-level and token-level annotations. Consequently, we sampled 100 memes from the HatefulMemes dataset and manually compared the segmentation performance of our HATESIEVE model against LMMs equipped with vision prompt generators such as InstructBLIP. We involved three human annotators in the process, and decisions were made based on a majority vote.\nImplementation Details Using the Contrastive Meme Generator, we produced a total of 42,344 triplet pairs. During the pre-training and fine-tuning phases, we employed the CLIP-VIT-BASE-PATCH32 as our backbone for the image-text encoder and froze all the CLIP parameters. Our newly introduced Image-Text Alignment module comprises 6 layers of self-attention blocks. Additionally, we incorporated a two-layer MLP as a decoder for classification fine-tuning.\nIn the contrastive learning pre-training stage, we used a learning rate of le-4 and trained the model over 4 epochs, which took approximately 4 hours on an NVIDIA 4090 GPU. For the fine-tuning stage in the classification task, we fine-tuned the model with a learning rate of le-5 for 4 epochs, completing in just 10 minutes. Throughout these stages, the Adam optimizer was utilized, with \u03b2 = (0.9, 0.999)."}, {"title": "Results on HatefulMeme Classification", "content": "As shown in table 1, we assessed the efficacy of various LMMs and CLIP-based methods in zero-shot inference and SFT settings. Overall, except for GPT-4V, other open-source models demonstrated limited capabilities under zero-shot conditions, revealing that while current LMMs exhibit multimodal recognition and understanding abilities from extensive pre-training, they still fall short in effectively recognizing hateful memes that demand more complex semantic reasoning. In the supervised fine-tuning setting, the performance improvements of LMMs, even for those fine-tuned with QLora, were minimal. For a detailed visual analysis, please see section 4.3.\nIn contrast, CLIP-based methods significantly outperformed LMMs in the SFT setting, achieving higher accuracy and F1 scores. Notably, HateCLIPer recorded the highest accuracy, 74.46% among these, but its introduction of a d \u00d7 d image-text feature interaction matrix led to an exponential increase in the number of model parameters, resulting in 1.1 billion trainable parameters with the CLIPLarge encoder, which compromises its efficiency as a safety filter. Conversely, our HateSieve, which incurs lower parameter costs, achieved the highest F1 score, surpassing that of GPT-4V, and its accuracy also exceeded those of CLIP, MOMENTA, and HateCLIPerBase. This underscores the effectiveness of our contrastive learning pre-training and ITA module."}, {"title": "Results on Hate Component Segmentation", "content": "We compare InstructBLIP after SFT on the HatefulMeme dataset, CLIP+ITA which is directly fine-tuned with HatefulMeme, and our HATESIEVE model. For InstructBLIP, we prompt the model as follows: \"Please examine the provided meme, which includes an [image] and accompanying [text]. Determine if the content can be considered hateful. If you conclude that the meme is hateful, identify and list the specific keywords or phrases in the text.\u201d This allows us to identify the text tokens that InstructBLIP considers ambiguous. Regarding image segmentation, we adhere to Li et al. approach, mapping the query corresponding to the Q-Former in InstructBLIP with the image's cross-attention map using bilinear interpolation.\nAs illustrated in the figure 7, the attention heatmap generated by InstructBLIP converges to a very small area, resulting in ineffective subsequent segmentation. Although InstructBLIP classifies this meme as hateful, it indicates that the model does not fully utilize the image information in its analysis. In contrast, the attention map of CLIP+ITA, which is solely fine-tuned on Hateful-Meme, is overly dispersed across the image and fails to concentrate on the relevant characters or scenes. On the other hand, our contrastively pre-trained HATESIEVE effectively focuses on specific characters in the image and correlates them with the keywords in the text. This demonstrates that our contrastive learning pre-training enables HATESIEVE to correctly link specific components of the image and of the text within the meme.\nDue to the lack of precise image and text annotations, we randomly sampled 100 groups of memes from HatefulMeme and manually evaluated the accuracy of the images rendered by the attention heatmap and the corresponding text. As shown in Table 2, our HATESIEVE significantly outperforms InstructBLIP and CLIP+ITA in segmentation effectiveness, validating the positive correlation of contrastive learning with image-text associations."}, {"title": "Ablation Study", "content": "ITA Parameter Scale We investigated the influence of self attention layers within the ITA on the classification efficacy of HATESIEVE. As depicted in Figure 8, there is a clear trend where the classification performance of HATESIEVE improves when increasing the number of layers. However, this improvement plateaus and subsequently declines once the layer count surpasses 6, as evidenced by a noticeable drop in the F1 score.\nTriplet Data Scale We investigated the impact of the triplet dataset scale used during the pre-training stage with contrastive learning. As illustrated in Figure 8, we analyzed the effects on the classification performance of HATESIEVE when pre-trained with 0% (no pre-training), 25%, 50%, 75%, and 100% of the triplet dataset. The results clearly show that an increase in pre-training data enhances the classification capabilities of HATESIEVE. This underscores the critical role of our triplet dataset in the effectiveness of contrastive learning during pre-training.\nPre-training Strategy We also investigated the impact of different pre-training strategies on the classification performance of HateSieve. We compared the following approaches:\n\u2022 In-domain pre-training, which involves using the HatefulMeme training set to directly sample similar hateful version memes, rather than sampling {I-,T-} from the MMHS150k dataset during the triplet dataset generation process.\n\u2022 CLS (Classification-only Strategy): In this approach, we substituted the contrastive learning pre-training with direct classification. We utilized the same volume of training samples, comprising 8,500 entries from the Hateful-Meme dataset and 33,844 sets of multimodal hate speech data from MMHS150k.\nAs shown in Figure 9, the model pre-trained in-domain outperformed the CLS pre-training, confirming the superior effectiveness of contrastive learning. However, it underperformed compared to other HATESIEVE resources, We speculate that this lower performance is due to the limited variety in the HatefulMeme dataset and unseen test data. Sole reliance on the in-domain dataset didn't substantially improve generalization.\nMeanwhile, although the CLS-only pre-trained model improved upon the baseline HATESIEVE, as shown in Table 1, the significant computational expense did not match the modest performance gains, making it inefficient."}, {"title": "Segmentation Results", "content": "Additional segmentation results are illustrated in Figure 10. The results demonstrate HATESIEVE'S capability to correlate hateful text with objects within images, underscoring the effectiveness of the proposed pre-training with contrastive learning and ITA module."}, {"title": "Conclusion", "content": "We developed HATESIEVE, a framework for classifying and segmenting hateful memes. Our experiments show that contrastive learning on a custom triplet dataset improves classification and achieves effective segmentation."}, {"title": "Limitations", "content": "Our current work faces several limitations, including the broad semantic scope of our contrastive meme generator. We plan to refine this to enhance contrastive learning in future iterations. Additionally, achieving high accuracy for image segmentation in HATESIEVE is challenging. The attention maps are restricted to the image-patch level, and attempts to refine these to pixel-level detail via linear interpolation introduce biases and do not inherently optimize for segmentation accuracy. We aim to expand these capabilities and also to conduct further experiments on a more extensive array of multimodal content related to hate speech in future work."}, {"title": "Ethics Statement", "content": "Our research with the Contrastive Meme Generator, which generates both hateful and non-hateful memes, may involve sensitive content. However, all materials are sourced from open-source datasets and confined to academic research, ensuring privacy protection. We adhere to high ethical standards, actively mitigating biases and misuse, and advocate for the responsible use of LMMs."}, {"title": "Appendix", "content": "When implementing supervised fine-tuning on LMMs, we consistently utilized the QLora framework by integrating a set of trainable parameters (d=64) into both the query and key components of the Q-Former. This adjustment was applied to joint LLM architectures such as OPT-6.7b for BLIP2 and Vicuna-7b for InstructBLIP, while the original parameters were kept frozen. We established a constant dropout rate of 0.05 and set the hyperparameter \u03b1 at 256. The fine-tuning was conducted with a learning rate of 5e - 5 and a batch size of 8."}]}