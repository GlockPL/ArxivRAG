{"title": "Foreign-Object Detection in High-Voltage Transmission Line Based on Improved YOLOv8m", "authors": ["Zhenyue Wang", "Guowu Yuan", "Hao Zhou", "Yi Ma", "Yutang Ma"], "abstract": "The safe operation of high-voltage transmission lines ensures the power grid's security. Various foreign objects attached to the transmission lines, such as balloons, kites and nesting birds, can significantly affect the safe and stable operation of high-voltage transmission lines. With the advancement of computer vision technology, periodic automatic inspection of foreign objects is efficient and necessary. Existing detection methods have low accuracy because foreign objects attached to the transmission lines are complex, including occlusions, diverse object types, significant scale variations, and complex backgrounds. In response to the practical needs of the Yunnan Branch of China Southern Power Grid Co., Ltd., this paper proposes an improved YOLOv8m-based model for detecting foreign objects on transmission lines. Experiments are conducted on a dataset collected from Yunnan Power Grid. The proposed model enhances the original YOLOv8m by incorporating a Global Attention Module (GAM) into the backbone to focus on occluded foreign objects, replacing the SPPF module with the SPPCSPC module to augment the model's multiscale feature extraction capability, and introducing the Focal-EIoU loss function to address the issue of high- and low-quality sample imbalances. These improvements accelerate model convergence and enhance detection accuracy. The experimental results demonstrate that our proposed model achieves a 2.7% increase in mAP_0.5, a 4% increase in mAP_0.5:0.95, and a 6% increase in recall.", "sections": [{"title": "1. Introduction", "content": "High-voltage transmission lines play a crucial role in ensuring the safety and stability of electricity transmission [1]. With the rapid development of the Chinese economy, China's power generation accounted for approximately 30.34% of the world's total in 2022, making its high-voltage transmission line network the largest globally [2]. These transmission lines traverse diverse and dynamic environments, often passing through residential and commercial areas, making them susceptible to the attachment of various foreign objects such as balloons, kites, fabrics and plastic waste [3]. Additionally, in natural settings like forests, birds nesting and twigs can challenge the safety of the transmission lines [4]. Suppose foreign objects on the transmission lines are not promptly detected and cleared: in that case, they can lead to accidents such as single-phase grounding and inter-phase short circuits, severely affecting the regular operation of the transmission lines, jeopardizing human safety, and resulting in casualties like accidental electrocution and fatalities [5].\nEarly detection of foreign objects on transmission lines relied predominantly on manual inspections, a method characterized by high labor intensity and inefficiency. With the development of computer vision technology and the widespread application of unmanned aerial vehicles (UAVs), UAVs can capture the transmission lines' images. Then, the images can be analyzed using computer vision techniques to automatically identify anomalies, presenting a significant advancement in line inspection methodologies [6,7].\nCurrently, target detection research on transmission lines has mainly focused on target detection of principal components, foreign-object detection, smoke detection, insulator defect detection, etc. Among them, foreign-object detection mainly focuses on four types of foreign objects, namely balloons, bird nests, kites, and plastic trash. However, in the plateau mountainous environment of Yunnan Province, China, slender twigs and large birds also seriously interfere with the regular operation of high-voltage transmission lines. Therefore, slender twigs and birds are added to our detection database. The existing methods cannot effectively solve the problems faced by the Yunnan Branch of China Southern Power Grid Co., Ltd., so our research has specific value.\nThis paper's contributions are as follows:\n(1) Based on the data provided by the Yunnan Power Science Research Institute in the Yunnan Branch of China Southern Power Grid Co., Ltd., a foreign object dataset of power transmission lines was established. This dataset includes six types of objects: trash, twigs, nests, kites, birds, and balloons. In this database, foreign objects are occluded, with multiple types, large-scale changes, and complex backgrounds. The detection accuracy of existing methods is not high.\n(2) We propose a foreign-object-detection model for high-voltage transmission lines based on improved YOLOv8m. The experimental results show that our model has achieved high accuracy and can meet the needs of practical applications."}, {"title": "2. Related Work", "content": "2.1. Object-Detection Model\nIn 2014, Girshick et al. [8] used the \"region proposal + convolutional neural network\" to replace the traditional method of \"sliding window + manually designed features\" in object detection, and designed the R-CNN framework, which made a huge breakthrough in object-detection technology. In 2015, Girshick et al. [9] proposed the Fast RCNN algorithm, which further improved RCNN and innovatively proposed multi task loss. They also trained classifiers and bounding box regressors, achieving end-to-end training in the detection stage, greatly improving accuracy and speed. Ren et al. [10] proposed the Faster RCNN algorithm shortly after, introducing an RPN network, making candidate box generation almost cost free. In 2016, Redmon et al. [11] proposed the YOLO algorithm, which is the first single-stage object-detection algorithm in the era of deep learning, and its speed is very fast. This network divides the image into grids and predicts the bounding box and classification probability of each grid area. A single neural network can obtain results from the complete image through one operation, which is conducive to end-to-end optimization of detection performance. In the same year, combining the anchor mechanism of RCNN and the regression idea of YOLO, Liu et al. [12] proposed the SSD algorithm, introducing multi-scale detection methods, and performing detection on feature maps extracted at each scale. Lin et al. [13] proposed RetinaNet in 2017 to investigate the accuracy of single-stage detection methods that lag behind two-stage detection methods. They believe that the imbalance of categories during the training process leads to a disadvantage in the accuracy of single-stage methods. Therefore, they propose Focal Loss to replace traditional cross entropy, improve the weight of background samples, and make the model more inclined towards target samples that are difficult to detect during the training process.\nIn 2017 and 2018, Redmon et al. launched subsequent versions of YOLO: YOLOv2 [14] and YOLOv3 [15]. The YOLOv2 algorithm introduces BN (batch normalization), multi-scale training, anchor box mechanism, and fine-grained features to improve the YOLOvl algorithm. On the basis of YOLOv2, the YOLOv3 algorithm adopts better backbone network, multi-scale prediction, and nine anchor boxes for detection, which improves the accuracy of the detection algorithm while ensuring real-time performance. In 2020, Bochkovskiy A et al. [16] proposed the YOLOv4 model. In the same year, Glenn et al. [17] proposed YOLOv5, which greatly reduced the number of parameters in the YOLO model, but its accuracy was comparable to YOLOv4. In 2021, Ge Zheng et al. [18] proposed the YOLOX model. In the same year, Wang et al. [19] proposed YOLOR. In 2022, Meituan [20] proposed YOLOv6, which was inspired by the design concept of hardware-aware neural networks. Based on the RepVGG style [21], it designed the reconfigurable and more efficient backbone networks, EfficientRep Backbone and Rep-PAN Neck; it optimized and designed a more concise and effective Efficient Decoupled Head, which further reduces the additional delay overhead caused by general decoupling heads while maintaining accuracy; and the anchor-free mode was adopted, supplemented by the SimOTA [22] label allocation strategy and SIoU [23] boundary box regression loss, to further improve detection accuracy. In the same year, Wang, C, et al. [24] proposed YOLOv7, proposing different network models for different devices. The concept of a gradient propagation path was used to analyze the reparameterization strategies applicable to different layers of networks, and a planned model structure reparameterization was proposed; the author proposes a new label allocation method called the Coarse to Fine (Coarse to Fine) guided label allocation strategy, which allows Aux Head to generate more grids as positive samples and reduce information loss. In terms of model design architecture, E-ELAN was proposed, which enhances the network feature's learning ability through amplification, confusion, and merging without changing the original gradient path of ELAN [25]. RepVGG style can improve performance through multiple branches during the training process, and inference speed can be accelerated through structural reparameterization. In 2023, Ultratics [26] released YOLOv8, which may have referenced the design concept of YOLOv7 ELAN in its backbone network and Neck section. YOLOv5's C3 structure was replaced with a C2f structure with richer gradient flow, and different channel numbers were adjusted for different scale models, significantly improving model performance; the Head section has been replaced with the current mainstream decoupling head structure, separating classification and detection heads, and also replaced with Anchor Free from Anchor Based. In terms of loss calculation, the TaskAligned Assigner positive sample allocation strategy was adopted, and the Distribution Focal Loss was introduced. The data augmentation part of the training introduces the last 10 epochs in YOLOX to turn off Mosiac augmentation, which can effectively improve accuracy.\nYOLOv8m is one of the latest models in object detection, exhibiting advantages in detection accuracy and speed. Because the foreign-object-detection model for high-voltage transmission lines will be integrated into the hardware of the drone platform in the later stage, we are considering using the YoloV8m lightweight model and modifying it to make it more suitable for our application.\n2.2. Research on Target Detection for Transmission Lines\nIn the early stages of employing computer vision for detecting foreign objects on transmission lines, traditional digital-image processing techniques were often used, employing non-neural network-based methods to extract defect features from transmission line images [27]. For instance, Cai et al. [28] employed contour detection of the transmission conductor's shape to determine the presence of foreign objects. Jin et al. [29] proposed an Otsu adaptive threshold segmentation algorithm, improved with morphology, to remove background noise, and applied a gradient method to identify power line edge positions, utilizing the Hough transform to analyze the number of lines to achieve foreign object recognition. Wang et al. [30] introduced a detection method for foreign objects on transmission lines based on line structure perception, segmenting and analyzing the grayscale and line width of acquired line segments in horizontal and vertical directions, to identify foreign objects on the transmission lines within complex backgrounds. However, the rich and diverse details in images of foreign objects on transmission lines make it challenging to describe target features using manually designed single features comprehensively. These methods exhibit relatively low accuracy in identifying foreign objects and struggle to differentiate between anomalies.\nWith the rapid advancement of deep learning, methodologies for detecting foreign objects on power transmission lines based on deep learning have been proposed. Shi et al. [31] initially employed the Selective Search method to extract candidate regions from pole images. Subsequently, training was conducted using the CaffeNet network model, adjusting and optimizing samples and network parameters through pre-training and re-training, resulting in a detection accuracy of 92.46% for nests. Zhu et al. [32] processed images using the scale histogram-matching method and employed the DFB-NN model, which effectively integrates low-resolution information and high-semantic information, to detect various objects, including vehicle-mounted cranes, tower cranes, forklifts, winding foreign objects, and wildfires, achieving an accuracy of 88.1%. Yang et al. [33] utilized the DenseNet network to replace the penultimate layer network in YOLO v3, achieving average detection accuracies of 94.7% for kites, nests, and trash. Yu et al. [34] used Otsu adaptive threshold segmentation, morphological processing and other methods to extract regions of interest. Then, DenseNet201 was used to extract the features of the regions of interest. Finally, the ECOC-SVM model was trained, and the average accuracy of balloons, kites, plastics and nests reached 93.3%. Yu et al. [35] employed a concatenated fusion strategy to merge features extracted by different networks. They utilized a random-forest classification model to achieve a detection accuracy of 95.8% for balloons, kites, nests, and plastics. Zou et al. [36] replaced the FPN structure of YOLOv5 with the BiFPN structure, integrated the CA attention mechanism into the CSP2_X module, and added a small-object detection layer, resulting in an average detection accuracy of 98.3% for nests, plastics, and kites. Qiu et al. [37] proposed the YOLOv4-EDAM model, utilizing the DnCNN image denoising network to reduce image noise and employing SENet-MobileNetV2 to extract features. Additionally, the model enhanced the network's feature extraction capability using CBAM-SPP and CBAM-PANet, and utilized NMS to eliminate overlapping boxes, achieving a detection accuracy of 96.71% for nests, kites, balloons and trash. Wu et al. [38] used the ASPP module in YOLOX, incorporated the CBAM attention mechanism, employed the GIOU loss function, and achieved a detection accuracy of 86.57% for nests, balloons, trash and kites. Yu et al. [39] introduced SPD convolution into the YOLOv7 model, optimizing hyperparameters to achieve a detection accuracy of 92.2% for cranes, excavators, bulldozers, tower cranes, trucks and nests. Zhang et al. [40] made improvements based on the YOLOv4 model, generating anchor boxes suitable for this dataset using k-means clustering, replacing the network's SPP module with an SPPF module, and substituting the SiLU activation function for the Leaky ReLU activation function, achieving an average detection accuracy of 97.57% for balloons, nests, kites, and trash. Tang et al. [41] combined Transformer V2 with YOLOX, utilizing the STCSP feature extraction layer in the backbone network and employing the Hybrid Spatial Pyramid Pooling (HSPP) module and the RepVGGBlock. This approach achieved a detection accuracy of 96.7% for nests, kites, and balloons. In research on transmission line components and environment, Chen et al. [42] improved a YOLOv5s model for key-component detection of power transmission lines. Cheng et al. [43] proposed a detection algorithm called P2E-YOLOv5 to detect a small target. Chen et al. [44] proposed an optimized YOLOv7-tiny model for smoke detection in power transmission lines.\nThe existing foreign-object detection mainly focuses on four types of foreign objects, namely balloons, bird nests, kites, and plastic trash. However, in the plateau mountainous environment of Yunnan Province, China, slender twigs and large birds also seriously interfere with the regular operation of high-voltage transmission lines. Therefore, slender twigs and birds are added to our detection database. The existing methods cannot effectively solve the problems faced by the Yunnan Branch of China Southern Power Grid Co., Ltd."}, {"title": "3. Datasets", "content": "The datasets of foreign objects on power transmission lines utilized in this study were provided by the Electric Power Science Research Institute in Yunnan Branch of China Southern Power Grid Co., Ltd. The image data was collected using various drones, resulting in significant variations in pixel dimensions, ranging from 640 \u00d7 640, 856 \u00d7 486, 1280 \u00d7 720 to 2738 \u00d7 2270. The sample images of foreign objects on power transmission lines were categorized into six classes: trash, twig, nest, kite, bird and balloon. Figure 1a represents trash, Figure 1b depicts twig, Figure 1c displays nest, Figure 1d exhibits kite, Figure 1e portrays bird, and Figure 1f illustrates balloon. It is essential to note that the images shown in Figure 1 are not original. Because the foreign objects are small, proportionally, in some original images, we extracted the partial images around foreign objects for a more explicit representation.\nDue to detailed latitude and longitude information in some sample images, we have covered this information with black blocks according to data security requirements."}, {"title": "4. Proposed Method", "content": "Addressing the challenges posed by occlusions, diverse object categories, significant variations in object scales, and complex backgrounds in detecting foreign objects on power transmission lines, this paper proposes an improved model for foreign-object detection based on the YOLOv8m framework. A GAM attention mechanism is incorporated into the backbone to enhance the model's focus on the foreign object, to mitigate their occlusion. In response to the challenge of multiple object categories and intricate backgrounds, the SPPF module is replaced with the SPPCSPC module to augment the model's multi-scale feature-extraction capability. A Focal-EIoU loss function is introduced to address the imbalance in high- and low-quality samples, thereby expediting model convergence and elevating detection precision. These enhancements are elaborated sequentially in Sections 3.2 to 3.4.\n4.1. Enhanced YOLOv8m Model\nThe improved YOLOv8m model is illustrated in Figure 3, and the enhancements are outlined within the red boxes, elucidated as follows.\nIn Figure 3, the GAM attention mechanism is introduced into the backbone, a feature detailed in Section 4.2. The SPPF module is replaced by the SPPCSPC module, a modification discussed in Section 4.3. The integration of the Focal-EIoU loss function is presented in Section 4.4."}, {"title": "4.2. GAM (Global Attention Mechanism)", "content": "Specifically addressing the issue of occlusion in detecting foreign objects on power transmission lines, we integrate the GAM attention mechanism into the backbone to enhance the model's focus on the foreign object.\nThe Global Attention Mechanism (GAM) represents a technique that enriches inter-dimensional interactions across channel and spatial dimensions by preserving channel and spatial information. This augmentation is pivotal in obtaining critical target information.\nGAM adopts the channel-spatial sequence of CBAM [45] and entails a redesign of both the channel-attention submodule and the spatial-attention submodule, depicted in Figure 4. In Formula (1), $M_c$ represents the handling of the input vector $F_1$ by the channel-attention submodule, where $\\otimes$ is the tensor product of the processed vector and the input vector $F_1$. In Formula (2), $M_s$ signifies the spatial-attention submodule's processing of the vector $F_2$, where $\\otimes$ is the tensor product of the processed vector and the input vector $F_2$.\nIn Figure 5, we delineate the specific processing procedure of the channel-attention submodule depicted in Figure 4. The channel-attention submodule employs a 3D permutation (dimensional transformation) to retain three-dimensional information. Subsequently, a two-layer Multi-Layer Perceptron (MLP) enhances inter-dimensional dependency relationships spanning channel and spatial dimensions. An MLP structure is an encoder-decoder architecture with a reducing factor of $r$.\nIn Figure 6, we delineate the specific processing procedure of the spatial-attention submodule depicted in Figure 4. The spatial-attention submodule employs two convolutions to fuse spatial information, to heighten focus on spatial information. Moreover, a reducing factor $r$ is used within the channel-attention submodule to augment attention towards spatial features, as depicted in Figure 6."}, {"title": "4.3. SPPCSPC Module", "content": "In response to significant scale changes, multiple types and complex backgrounds of foreign objects, we have replaced the SPPF module with the SPPCSPC module, to augment the model's ability to extract multi-scale features. The SPPCSPC module uses a multi-scale pyramid pooling (SPP) structure and conventional convolutional operations. This structure amplifies the model's receptive field and elevates its capacity to extract features across multiple scales, enhancing overall detection capabilities.\nThe SPPCSPC module, shown in Figure 7, segregates features into two segments. One segment undergoes processing utilizing the SPP structure, employing four max-pooling operations at distinct scales, to capture diverse receptive fields. Consequently, this step amalgamates spatial feature information of varying dimensions and generates fixed-size feature vectors. In parallel, the other segment undergoes standard convolutional operations. Finally, the features obtained from these two segments are concatenated to form the comprehensive feature vector."}, {"title": "4.4. Focal-EIoU Loss Function", "content": "To address the imbalance between high- and low-quality samples (the intersection size of different regions on the union of predicted-truth boxes and ground-truth boxes), we add a Focal-EIoU loss function. The Focal-EIoU loss function enhances the optimization contributions for high-quality samples with lower quantities when regressing the predicted bounding boxes towards the ground-truth boxes, while simultaneously diminishing the optimization contributions for low-quality samples with higher quantities in this regression process. The Focal-EIoU loss function can effectively address the imbalance between high- and low-quality samples, expediting model convergence and augmenting detection precision.\nThe EIoU loss function, shown in Figure 8, is represented by Equation (3). The EIoU is a metric for evaluating the dissimilarity between predicted boxes and ground-truth boxes. The Focal-EIoU loss function utilizes this metric to guide the training process, emphasizing high-quality samples for more effective model learning and convergence."}, {"title": "5. Experiment and Analysis", "content": "5.1. Experimental Setup\nThe hardware configuration for our experiment included an Intel(R) Core(TM) i7-10700K CPU, 32 GB of RAM, and an NVIDIA GeForce RTX 2080 Ti GPU. The software environment comprised Windows 10, PyTorch 1.11.0, CUDA 11.3, and PyCharm Community Edition 2021.3.\nThe momentum was initialized to 0.937, the batch size was 24, the weight decay rate was 0.0005, the initial learning rate was 0.01, and the training epoch was 100. Given the varying sizes of the image samples, all input images were transformed to 640 \u00d7 640 pixels.\n5.2. Evaluation Metrics\nIn object detection, Intersection over Union (IoU) represents the area ratio of the intersection to the union between the predicted box and the ground-truth box. TP (True Positive) denotes the sample count where the real category is positive and the model's prediction is also positive; TN (True Negative) indicates the sample count where the real category is negative, and the model predicts them as negative; FP (False Positive) is the sample count where the real category is negative, but the model predicts them as positive; and FN (False Negative) represents the sample count where the real category is positive, but the model predicts them as negative.\nOur ablation experiments used seven metrics: mAP_0.5, mAP_0.5:0.95, precision, recall, parameters, GFloats, and speed. Our comparative experiments used five metrics: mAP_0.5, mAP_0.5:0.95, precision, recall, and speed. The definitions of these metrics are as follows:\n1. Precision: the proportion of predicted positive samples that are actually positive (Precision = TP/(TP + FP)).\n2. Recall: the proportion of actual positive samples that are predicted as positive (Recall = TP/(TP + FN)).\n3. mAP_0.5: precision is calculated for each class when the IoU threshold is 0.5, and the precision of each class is averaged to obtain mAP_0.5.\n4. mAP_0.5:0.95: average mAP across different IoU thresholds from 0.5 to 0.95 (with a step of 0.05).\n5. Parameter: the model's parameter count.\n6. GFLOPs: model computational complexity. One GFLOPs means that the model requires billions of floating-point operations.\n7. Speed: the model's detection speed. It indicates the milliseconds required for detecting a single image.\n5.3. Experimental Results and Analysis\n5.3.1. Model Training Comparison\nIn this experiment, the training epoch was 100. Figure 9 illustrates the comparison of mAP_0.5 during training, Figure 10 presents the comparison of mAP_0.5:0.95 during training, Figure 11 exhibits the comparison of precision during training, Figure 12 showcases the comparison of recall during training, and Figure 13 displays the comparison of cls_loss during training. From these figures, it is evident that our model outperforms the original model across various metrics.\nThe four curves at the beginning of \"Original\" are trained using the original dataset, and their models are the original model, incorporating the GAM attention mechanism, continuing to add the SPPCSCP module, and continuing to add the Focal -EIoU loss function, respectively. The other two curves, starting with \u201cdataAugmentation\", are trained using the augmented dataset, and their models are the original model and our improved model, respectively."}, {"title": "6. Conclusions", "content": "This paper proposes a foreign-object-detection model for transmission lines based on improved YOLOv8m. Our model autonomously detects anomalies in aerial images of power transmission lines, reducing the workload associated with anomaly detection and enhancing inspection efficiency.\nAddressing the challenges of foreign-object detection along power transmission lines, we use three key improvements: firstly, to mitigate issues related to occlusion, a Global Attention Mechanism (GAM) was introduced, enhancing focus on obscured targets. Secondly, in response to the diversity of foreign-object categories and complex backgrounds, the SPPCSPC module was integrated, boosting the model's multi-scale feature-extraction capabilities. Thirdly, the Focal-EIoU loss function was introduced to address imbalances between high- and low-quality samples, expediting model convergence and augmenting detection accuracy. The refined YOLOv8m model exhibited significant enhancements, elevating mAP_0.5 from 92.8% to 95.5%, mAP_0.5:0.95 from 76.4% to 80.4%, and recall from 85.9% to 91.9%.\nNo publicly available datasets for power transmission line foreign-object detection have been provided in previously published literature, thus hindering experimentation with our enhanced model on alternative datasets. The dataset utilized in this study originates from the Electric Power Research Institute, Yunnan Power Grid Co., Ltd., featuring geolocation data in certain images. Upon removing location information and obtaining approval from the aforementioned organization, we plan to release this dataset publicly, facilitating future research in related domains.\nAlthough the upgraded model demonstrates the potential to be applied in practical detection scenarios for foreign objects on power transmission lines, its detection speed is presently insufficient for direct application on terminal CPU devices. Subsequent work will focus on adopting lighter models, such as YOLOv8n, to enhance model detection speed while maintaining detection accuracy."}]}