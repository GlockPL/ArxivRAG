{"title": "SuperCode: Sustainability PER AI-driven CO-DEsign", "authors": ["P. Chris Broekema", "Rob V. van Nieuwpoort"], "abstract": "Currently, data-intensive scientific applications require vast amounts of compute resources to deliver world-leading science. The climate emergency has made it clear that unlimited use of resources (e.g., energy) for scientific discovery is no longer acceptable. Future computing hardware promises to be much more energy efficient, but without better optimized software this cannot reach its full potential. In this vision paper, we propose a generic AI-driven co-design methodology, using specialized Large Language Models (like ChatGPT), to effectively generate efficient code for emerging computing hardware. We describe how we will validate our methodology with two radio astronomy applications, with sustainability as the key performance indicator.\nThis paper is a modified version of our accepted SuperCode project proposal. We present it here in this form to introduce the vision behind this project and to disseminate the work in the spirit of Open Science and transparency. An additional aim is to collect feedback, invite potential collaboration partners and use-cases to join the project.", "sections": [{"title": "1 Introduction", "content": "Data-intensive science, such as radio astronomy and high-energy physics, requires vast amounts of compute resources to deliver world-leading science. The current energy and environmental crises drive a strong desire to do science in a manner that minimizes the environmental impact we make while maximizing the science we can deliver. Modern special-purpose compute architectures promise to be much more power efficient than general-purpose systems. However, leveraging these novel architectures is time-consuming and thus expensive due to the effort it takes to port existing code to a new architecture and the increasing complexity and specialization of hardware components.\nIn this vision paper, we present SuperCode (Sustainability PER AI-driven CO-DEsign), a novel approach on how to improve effective co-design of hardware and software, since this is essential to ensure that the resulting hardware and software combination is fit for purpose and able to run efficiently. We hypothesize that recent advances in code generation with AI-based Large Language Models (LLMs, e.g., ChatGPT) can be a catalyst for this process. We propose a systematic AI-driven co-design methodology that can drastically reduce the turn-around time to evaluate emerging technologies for data-intensive science, with sustainability as Key Performance Indicator (KPI). To validate our novel approach, we will explore two radio astronomy science cases and investigate their most optimal and sustainable emerging technology platform. With the partners in our project, we will explore opportunities in other domains like climate research, remote sensing and earth observation.\nThe primary contributions in this paper can be summarized as follows:"}, {"title": "3 Problem statement", "content": "Data- and compute-intensive science, or eScience,\nis now firmly established as the fourth science\nparadigm [22], as introduced by Tony Hey and Jim\nGray. While this opens exciting new abilities to ex-\nplore new science in vast amounts of data, this comes\nat very significant processing and energy cost [64, 39].\nThis cost is often overlooked and poorly understood\nor appreciated. In the current climate crisis we can\nno longer accept that world-leading science has an\nunknown, and more importantly, potentially uncon-\nstrained environmental impact [39]. We argue that a\ncareful and deliberate consideration needs to be made\nwhether the scientific impact outweighs the poten-\ntial environmental impact by the processing required.\nOptimization and a better mapping of software to\nhardware can shift this balance in our favour.\nA key part in the optimization process is the abil-\nity to efficiently co-design software, hardware, and\nscience. However, such co-design is time-consuming\nand complex due to required manual optimisation"}, {"title": "3.1 Illustrative Example", "content": "In the appendix 9 to this paper we show an example of the current state-of-the-art, which demonstrates our vision using existing, not fine-tuned, models. In this case we use QWEN 2.5 Coder 14B Instruct12. The result, shown with the context removed and only the generated code reproduced, demonstrates that the concept has merit but that current models are not suitable for the task at hand. The following prompt was used:\nWrite me an efficient polyphase\nfilter for the RISC-V architecture\nwith V vector extensions in\nassembly.\nThe result demonstrates that this model is aware of the RISC-V architecture and uses the correct assembly vector instructions. However, it does not understand what a polyphase filter (essentially a bank of Finite Impulse Response (FIR) filters feeding into a Fast Fourier Transform (FFT)) is. The result seems to use V-extension calls, but the result does not make sense. This illustrates that the current general purpose models clearly needs to be fine-tuned for our applications and that a human-in-the-loop approach is essential for the application we have in mind."}, {"title": "4 Vision and methodology", "content": "The primary transverse goal of SuperCode is to reduce the environmental impact of data-intensive applications, through a novel approach of AI-driven co-design. In contrast to normal co-design, where computational performance or efficiency is used as Key Performance Indicator (KPI), we introduce a sustainability score instead. This approach not only benefits"}, {"title": "4.1 Sustainability", "content": "Scientific discovery is moving at an unprecedented pace, and many areas of research are limited in their potential by the lack of sufficient signal and data processing capacity [6]. However, the age of data-driven scientific discovery potentially comes at a very significant environmental impact. Signal processing for the LOFAR telescope, excluding science processing done by the astronomer, exceeds 500 MWh per year [28]. With increased capabilities this is expected to increase over the next couple of years. Future telescopes, like the Square Kilometre Array (SKA), are scaled such that procuring sufficient compute capacity is initially infeasible [18]. Even the partial system in that design will likely require MW-scale power. Efforts are underway to gain insights into the environmental impact that groundbreaking research infrastructures have [28, 37, 31]. However, there is currently no measure for the environmental impact of a scientific discovery.\nPart of our vision is to visualize the resources required to make science possible. While energy consumption is the most obvious parameter in this context, it is by no means the only one. This is offset by the science output and/or economic impact, this is the societal value created. The latter was studied for e.g. LOFAR by the Rathenau institute [52]. While the definition of science value is bound to be controversial, this may be defined in terms of peer-reviewed publications, scientific discoveries, or prestigious prizes. We optimize for relative science value, which we define as the total value created (total value of ownership, TVO) divided by the resources consumed (total cost of ownership, TCO), shown in equation 1.\n$M_s = \\frac{TVO}{TCO}$ (1)\nMore detail about this method and its application in science can be found in our earlier work [10]. Using this relation, we will design hardware and software combinations that maximize science per unit of environmental impact (the used unit is flexible, and can be energy, CO2, water, etc., or combinations of those). While some resources will inevitably be consumed, we need to be conscious of both the cost of those resources, the value that can be created, and be responsible enough to maximize the science we do with those. Using the proposed methodology, we aim to create a process that provides tailored advice for different science cases."}, {"title": "4.2 Sustainability as Key Performance Indicator", "content": "In this project we distinguish two separate classes of key performance indicators: at macro and micro level. At the micro level, we evaluate the effectiveness of our AI-models by measuring the effort required to evaluate a particular emerging technology for our two use-cases. This involves an estimate of the quality of the produced prototype code, i.e., does it work as is, what performance is achieved, and does it produce accurate results, as well as the accuracy of the sustainability estimate produced by the AI model. Furthermore, we track the effort required and time needed to evaluate complex emerging technologies. We will thus use concrete measurements to validate our hypothesis that AI-driven co-design is able to significantly reduce the effort and time required to adapt and test hardware and software combinations for data-intensive science applications. At macro level our co-design KPIs focus on sustainability. We aim to minimize the environmental impact of data-intensive science by leveraging emerging technologies to optimize the efficiency of signal processing needed to turn instrument data into scientific data products. Traditionally we would optimize for computational performance or efficiency, in this project we rather focus on sustainability, the exact definition and metric of which is to be defined in the first"}, {"title": "4.3 AI-driven co-design", "content": "Co-design generally uses an existing algorithm and tailors the hardware and software to make sure the implementation is efficient. This process is complex and time consuming, requiring hand-optimizing computational hotspots and a vast amount of domain- and platform-specific knowledge [63, 47, 58, 60, 46, 49, 51, 48]. Therefore, it is currently unfeasible to quickly react to changing scientific requirements, nor to discuss unforeseen opportunities with the researchers. Essentially, the scientist currently is not an integral part of the co-design loop. Exploiting the latest AI developments, we aim to turn this around.\nGenerative AI, and in particular large language models like ChatGPT, have shown a remarkable ability to solve straightforward coding tasks. Products like GitHub co-pilot5, Codeium and BlackBox7 are changing the way software is developed [68, 21, 38, 4, 29], adding a tireless companion to the programmer that helps to avoid common pitfalls and write the generic code that is often needed. This allows the programmer to focus on functional correctness, performance and scalability [4]. Our hypothesis is that the use of AI in co-design can drastically reduce the effort needed to evaluate emerging technologies, making their use far more viable. This in turn leads to more sustainable science through more efficient use of energy and/or other resources. While we will test closed models, our approach is generic and we will favour open models like Llama2 [53] to ensure reproducibility. We acknowledge that these models do not perform as well as closed models. Llama2 was optimized for dialog applications using reinforcement learning.\nA well-known property of LLMs is that they can hallucinate [67, 71], and thus generate unrelated or incorrect output. This is commonly seen as a bug. In this project, we see this as a feature instead, and aim to tap into this creativity to generate novel solutions. I.e., we want to use this emerging knowledge for emerging technologies. By combining AI with the concept of the human in the co-design loop, the creativity exhibited by large language models is both constrained and leveraged.\nBy evaluating existing generative models and tailoring and testing our own models based on existing foundation models, we will build an AI co-design companion that will assist both programmer and system architect to design a sustainable hardware and software combination. Initially we will prompt an existing AI large language model with a combination of software and a detailed technical description of an emerging technology under investigation. Figure 2 shows a high-level overview of our AI-driven co-design concept. The methodology we envision works as follows.\nWe will add more information to the foundation models through matrix factorization techniques like low-rank adaptation (LoRA [24], LOHA [25], LoKr [17]) or other emerging compute-efficient training/fine-tuning techniques (DyLoRA [54], GloRA [15] or (IA)\u00b3 [33]). This way, we tailor the AI for our needs. We will train these for every emerging technology we investigate, providing the knowledge needed to generate code and estimate the sustainability of the platform, and the two use-cases at the heart of the project. Recent successful work in using LLMs for Chip design indicates that LLMs are indeed capable of grasping new architectures [34]. Additionally, we will investigate prompt tailoring and retrieval-augmented generation on modern models [69, 20, 32], where we take advantage of longer context lengths achieved by newer LLMs that allow ever larger prompts that would ultimately allow us to provide both the entire architecture description as well as the reference code. We will compare these two approaches. These techniques will form the heart of our AI-accelerated co-design process (RQ 2.1, PhD-1). The trained specializations will be publicly released.\nWe will first train generative foundation models with reference implementations of the scientific data algorithms we use (in our case signal processing algorithms like FIR filter banks [55], FFTs, beam forming [47], correlation [9, 58, 60, 41], dedispersion [51, 48], etc.). These reference implementations are not optimized for performance, but for explainability and correctness. Using the existing base of open-source radio astronomy code, which is most of the code running current telescopes, we will train our own model. To validate our approach, as a first step we will use LLMs to translate existing Nvidia GPU code to use AMD GPUs. Resulting code and performance will be compared to those generated by the"}, {"title": "4.4 Emerging Technologies", "content": "To reduce the environmental impact of data-intensive science, we turn to emerging technologies. The inevitable demise of Moore's law scaling has given rise to a host of alternative technologies that aim to offer improved performance and efficiency at lower cost. One of the earliest examples and one that is now firmly in the mainstream, is General Purpose computing on Graphics Processing Units (GP-GPU) [36, 35]. Many techniques leverage specialization, where special purpose components perform"}, {"title": "5 Use-cases", "content": "We will apply our methodology to two use-cases: one terrestrial and one space-based. We have selected these because they are both challenging but put vastly different constraints and requirements on the processing platform. Moreover, compared to data-"}, {"title": "5.1 Use-case 1: Terrestrial large-scale distributed radio telescopes", "content": "Our first use-case focuses on terrestrial large-scale distributed radio aperture synthesis arrays. These are at the forefront of low- and mid-frequency radio astronomy. Specifically, we will use The LOw Frequency ARray (LOFAR) [56], designed and built by ASTRON (See also Figure 3), and Square Kilometre Array (SKA) [45, 8] telescopes, currently under construction by a multi-national consortium including ASTRON. Most partners in our user committee are also involved in the construction of the software pipelines of the SKA.\nAperture synthesis arrays create a virtual telescope by combining multiple geographically separated sensors. These all sample the electromagnetic spectrum that, according to the Van Cittert-Zernike theorem [70] can be considered to come from the same distant source. Correlating many combinations of sensors gives us a sparse set of points that have a Fourier-relation with the sky image. We generally take advantage of the earth's rotation as well as the coherent nature of the signal of interest over incoherent noise to fill in the sparse image and amplify the weak astronomical signals. Radio astronomy requires a lot of signal processing to turn what is essentially noise into a scientific data product. The computational requirements for such instruments scale dramatically (O(n\u00b3) - O(n4)) with the size of the telescope, both due to increased data volumes and more processing required per unit of data, which is also a key driving factor in sensitivity and resolution.\nLOFAR, a state-of-the-art low-frequency array, requires processing at tera-scale. The SKA, which is about an order of magnitude larger in terms of receivers, is expected to require peta-scale processing [8]. The initial procurement of compute infrastructure will not cover that requirement, mostly due to budget and energy constraints. The scientific potential of current and future radio telescopes is limited by the availability of sufficient affordable data processing capacity and software. In use-case 1, we will evaluate the sustainability aspects of signal processing algorithms for terrestrial telescopes, like FIR filter banks [55], FFTs, beam forming [47], correlation [9, 58, 60, 41], dedispersion [51, 48], etc."}, {"title": "5.2 Use-case 2: Space-based radio telescopes", "content": "Use-case 2 is more speculative. The promise of higher sensitivity thanks to less ionospheric disturbance and Radio Frequency Interference (RFI) [57] and an unexplored low-frequency band due to the opaqueness of the atmosphere to frequencies around 10MHz, is generating interest for space-based interferometers. Such instruments include both orbiting swarms (OLFAR [61, 5], ALO [27] and DSL [16], as well as lunar surface instruments (Farside [12] and LuSEE'Night' [3]). The OLFAR roadmap is presented in Figure 4. The hostile extra-terrestrial environment and limited available energy, volume, mass and ability to dissipate heat, lead to unique and quite different concepts and challenges [40] compared to terrestrial telescopes, making this an interesting use-case.\nWhere terrestrial radio telescopes favour the transport of as much data as feasible to a central location for processing, this is not the case for space-based telescopes. Here, due to excessive bandwidth constraints and costs, edge processing is key. Furthermore, in space, energy is not abundant and heat dissipation of signal processing systems may be challenging. Cosmic radiation requires space-hardened systems, which are based on older (i.e., with larger gate sizes) production technologies, whereas from a sustainability viewpoint we would favour the newest processes. Note that these considerations are immediately applicable to space-based earth observation, where similar constraints are encountered. Project partners Sioux and S[&]T have extensive experience in earth observation and have expressed a keen interest in this similarity.\nThe signal processing is similar for space-based radio interferometry. That said, sustainability will be very different, considering energy in space is sustainably generated. Thus, production and launch will be much more dominant in the equation. Furthermore, we need to consider the environmental impact after the useful life of the instrument. If in orbit, this means de-orbiting to avoid creating space junk. In use-case 2, we will evaluate the which emerging architectures are most suitable and sustainable for space-based telescopes."}, {"title": "6 Scientific and Societal Impact", "content": "We firmly believe that the use of LLMs will fundamentally and disruptively change the way both"}, {"title": "7 Related work", "content": "AI-supported code generation has recently become possible [1, 66] and has seen only very limited application in high-performance computing. So far, no research has been done on generating and porting code to emerging architectures. This is challenging since there are no concrete code examples on the emerging architecture, so the LLMs must effectively do transfer learning to port existing codes to new architectures. LLMs have not been used with the specific focus of generating code for exploring more energy efficient combinations of hardware architecture and software implementation.\nGodoy et al. [21] evaluate AI-assisted generative capabilities on several numerical HPC kernels. They generated codes for a variety of programming models and languages, including C++/OpenMP, OpenACC, Kokkos, SyCL, CUDA, Fortran and Julia. However, all algorithms tested were well known (e.g., the models were trained on many examples), and they did not generated code for emerging architectures. Nevertheless, this does indicate that our proposed approach is feasible.\nCurrently, LLMs do not understand program semantics, and offer no guarantees about quality of the generated code. Jain et al. [26] demonstrated"}, {"title": "8 Conclusions", "content": "In this paper we have presented a novel AI-driven co-design concept. We argue that Large Language Models can significantly accelerate the evaluation and adoption of efficient new and emerging technologies for data-intensive science. By adopting a human-in-the-loop approach we aim to leverage the creativity exhibited by modern LLMs and turn a conceived weakness (commonly referred to as hallucination) into a strength. Our concept will be applied to some of the most challenging data-intensive science use-cases in radio astronomy, validating and exercising the approach to its limit.\nThe vision presented in this paper will be further explored in the SuperCode project, starting in January 2025."}, {"title": "9 Appendix A: Generated (incorrect) Code for a Polyphase Filter.", "content": "This appendix shows an example of generated (incorrect!) code for a polyphase filter in vectorized RISC-V assembly using the QWEN 2.5 Coder 14B Instruct model. Note that only the generated code is shown, not the generated explanation and context.\n'''assembly\n.section .data\ninput: word 1, 2, 3, 4, 5,6,7,8 # Example input data\ncoeffl: word 0.5, 0.5, 0.5, 0.5 # Coefficients for subfilter 1\ncoeff2: word 0.5, 0.5, 0.5, 0.5 # Coefficients for subfilter 2\noutput: .space 8 # Space for output data\n.section .text\n.globl _start\n_start:\n#Load base addresses and lengths\nla t0, input # Load input base address\nla t1, coeff1 #Load coefficient 1 base address\nla t2, coeff2 #Load coefficient 2 base address\nla t3, output # Load output base address\nli t4, 4 # Length of input (number of elements)\n# Initialize pointers\nadd t5, t0, t4, slliw #2 t5 = input + length * 4 (end of input)\nadd t6, t3, t4, slliw #2 t6 = output + length * 4 (end of output)\n#Loop over input data\nloop:\n# Check if we've reached the end of input\nbeq t0, t5, end\n#Load 4 elements from input\nvlw.vv v0, (t0)\n#Load 4 coefficients from coeffl and coeff2\nvlw.vv v1, (t1)\nvlw.vv v2, (t2)\n# Multiply and add for subfilter 1\nvfma.vv v3, v0, v1\n# Multiply and add for subfilter 2\nvfma.vv v4, v0, v2\n#Store results to output\nvsw.vv v3, (t3)\nvsw.vv v4, (t3)\n#Move to next set of coefficients\naddi t1, t1, 16\naddi t2, t2, 16\n#Move to next set of input and output\naddi t0, t0, 16\naddi t3, t3, 8\n# Repeat loop\nj loop\n# Exit program (system call for Linux)\nli a7, 93\necall\n...'''"}]}