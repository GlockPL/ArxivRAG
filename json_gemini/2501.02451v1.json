{"title": "ENHANCING CONTRASTIVE LEARNING FOR RETINAL IMAGING VIA ADJUSTED\nAUGMENTATION SCALES", "authors": ["Zijie Cheng", "Boxuan Li", "Andr\u00e9 Altmann", "Pearse A Keane", "Yukun Zhou"], "abstract": "Contrastive learning, a prominent approach within self-\nsupervised learning, has demonstrated significant effective-\nness in developing generalizable models for various appli-\ncations involving natural images. However, recent research\nindicates that these successes do not necessarily extend to the\nmedical imaging domain. In this paper, we investigate the\nreasons for this suboptimal performance and hypothesize that\nthe dense distribution of medical images poses challenges\nto the pretext tasks in contrastive learning, particularly in\nconstructing positive and negative pairs. We explore model\nperformance under different augmentation strategies and\ncompare the results to those achieved with strong augmenta-\ntions. Our study includes six publicly available datasets cov-\nering multiple clinically relevant tasks. We further assess the\nmodel's generalizability through external evaluations. The\nmodel pre-trained with weak augmentation outperform those\nwith strong augmentation, improving AUROC from 0.838\nto 0.848 and AUPR from 0.523 to 0.597 on MESSIDOR-2,\nand showing similar enhancements across other datasets. Our\nfindings suggest that optimizing the scale of augmentation is\ncritical for enhancing the efficacy of contrastive learning in\nmedical imaging.\nIndex Terms- contrastive learning, augmentation scales,\ndata distribution, retinal imaging", "sections": [{"title": "1. INTRODUCTION", "content": "Contrastive learning is a machine learning paradigm that\ntrains models to distinguish between similar and dissimilar\ndata points without relying on explicit labels. Despite being\npre-trained only on unlabeled data, contrastive learning drives\ncompetitive pre-trained models compared to those pre-trained\nwith supervised learning-based methods. In the natural im-\nage domain, it has demonstrated promising results in diverse\ntasks such as object detection [1], image classification [2],\nand video analysis [3]. Compared to generative learning,\ncontrastive learning has shown greater effectiveness in var-\nious applications involving natural images [4, 5]. However,\nwhether this observation extends to medical images remains\nunderexplored.\nRecent research has begun comparing contrastive learn-\ning and generative learning in medical AI. For instance, RET-\nFound [6], a foundation model for retinal images, employed\na generative learning strategy using the Masked Autoencoder\n[7] for model development and demonstrated superior perfor-\nmance compared to contrastive learning methods in retinal\ndisease classification. Understanding the reasons behind this\ninconsistency and developing a simple yet efficient solution to\nimprove contrastive learning for medical imaging is crucial.\nThe suboptimal performance of contrastive learning in\nmedical imaging is likely due to inherent differences between\nthe distributions of natural and medical images [8]. Natu-\nral images are colorful with varying pixel intensities, while\nmedical images are usually grayscale and structurally simi-\nlar, especially within the same organ or tissue type [9, 10].\nThis characteristic results in a denser distribution of medical\nimages within the latent space compared to natural images\n[11]. We hypothesize that such a dense distribution degrades\nperformance when applying contrastive learning methods to\nmedical images. As shown in Figures 1(a) and 1(b), natural\nimages under strong augmentations in contrastive learning\nare sparsely distributed in latent space, while different med-\nical images tend to overlap heavily. Since the pretext of\ncontrastive learning is to differentiate between positive pairs\n(augmented views of the same image) and negative pairs\n(augmented views of different images), the severe overlap in\nmedical images makes the pretext task highly challenging,\nmaking it difficult for the model to converge effectively.\nIn this work, we propose a simple yet effective solution\nto enhance contrastive learning performance by reducing\naugmentation scales. The project pipeline is illustrated in\nFigure 1(c). We employ Distillation with No Labels (DINO)\n[4], and validated our solution on glaucoma and diabetic\nretinopathy diagnosis, using both internal and external evalu-\nations. Our approach not only enhances feature clustering but\nalso demonstrates improved diagnostic accuracy compared to\nmodels using stronger augmentation strategies."}, {"title": "2. METHODS", "content": "2.1. Problem Definition\nFor contrastive learning, given a set of unlabeled retinal im-\nages $D = \\{x_i\\}_{i=1}^L$, we create positive pairs $P^+$ by randomly\nselecting an image $x_i \\in D$ and apply twice augmentation $I_{t,s}$\nrespectively to get augmented data $x_i^1$ and $x_i^2$, where t indi-\ncates the augmentation type and s the scale range. While for\nnegative pairs, we sample two images $x_i$ and $x_j \\in D$ (with\n$i \\neq j$) and apply the augmentation to each image, forming the\nnegative pair $P^- = (x_i^1, x_j^3)$. The distance between positive\npairs and negative pairs can be measured by $Dis(\\cdot)$:\n$Dis(P^+) = |x_i^1 - x_i^2|$,\n$Dis(P^-) = |x_i^1 - x_j^3|$.\nThe general training objective of contrastive learning is to\ntrain the model f to maximize the distance between negative\npairs and to minimize that for positive pairs,\n$f = \\underset{f}{argmax} (Dis(P^-) \u2013 Dis(P^+)) $.   (3)\nWhen $Dis(P^+)$ approximates $Dis(P^-)$, it is challeng-\ning to train the model f to converge well. This issue is\nprominent in medical imaging due to less variation compared\nto natural images. For example, retinal images depict the\nanatomical tissue of retina, often showing similar structure\nand orientation [12], as shown in Figure 1(b). With strong\naugmentations strong (e.g., cropping the images into small\npatches), $Dis(P^-)$ decrease while $Dis(P^+)$ increases, which\nbrings further challenges in achieving objects of equation 3\nand may result in suboptimal model pre-training with con-\ntrastive learning, showing the poor performance in classifying\nthe positive and negative pairs.\nSuch suboptimal model performance extends to down-\nstream applications, where models are fine-tuned with labeled\ndata $D_l = \\{x_i, Y_i\\}_{i=1}^L$ for diverse tasks like disease diagno-\nsis, where x represents the data and y indicates the label. To\nimprove the model's capability in clinically meaningful appli-\ncations, we aim to optimize $\\underset{x_i,y_i\\in D_l}{arg max} T (E(x_i), Y_i)$,\nwhere $T(\\cdot)$ is the score function and $E(\\cdot)$ is the encoder of\nthe model f.\nOur strategy involves enhancing the contrastive learn-\ning performance by specifically decreasing $Dis(P^+)$ while\nincreasing $Dis(P^-)$.\n2.2. Scattering Data Distribution with Weak Augmenta-\ntions\nTo achieve such a goal for retinal images, a straightforward\nstrategy is to scale down the augmentation. An extreme case"}, {"title": "3. EXPERIMENT", "content": "We evaluate the efficacy of different augmentation strategies\nusing clinically meaningful tasks, including diabetic retinopa-\nthy (DR) diagnosis, glaucoma detection, and multi-class reti-\nnal disease classification.\nFor DR diagnosis, we include MESSIDOR-2 [13], IDRID\n[14], and APTOS2019 [15]. The labels for DR are based\non the International Clinical DR Severity Scale, covering five\nstages from no DR to proliferative DR. For glaucoma diagno-\nsis, we use the PAPILA dataset [16], which has three categor-\nical labels: non-glaucoma, early glaucoma (suspected glau-\ncoma), and advanced glaucoma. For multi-class disease clas-\nsification tasks, we use two datasets, JSIEC [17] containing\n1,000 images with 39 categories of common retinal diseases\nand conditions, and Retina dataset [18] with labels for nor-\nmal, glaucoma, cataract, and retinal disease. Data splitting\ndetails are shown in Table 1. The pre-training data are from\nMoorfields Eye Hospital [6].\n3.2. Pre-training details\nDINO [4], a representative and commonly used contrastive\nlearning strategy, was used in the experiment. We first initial-\nized the model with ImageNet weights and then pre-trained it\nusing 1.4 million retinal images from Moorfields Eye Hospi-\ntal. The data preprocessing, data quality control, model archi-\ntecture, and hyperparameters (except for those related to aug-\nmentations) were standardized to ensure a fair comparison.\nThe model was pre-trained using an NVIDIA A100 (80G).\nThe details of strong, weak, and weak+med are listed in\nTable 2. Local crop indicates the range of cropping local\npatches and global crop represents that for cropping global\npatches. Color jittering involves random adjustments to im-\nage brightness and contrast to simulate variations in imaging\nconditions. Gaussian blur applies a Gaussian filter to smooth\nimages, mimicking motion blur or out-of-focus effects. Ran-\ndom noise adds Gaussian noise to simulate sensor or acqui-\nsition noise. Random bias field introduces smooth, spatially\nvarying intensity variations to mimic changes in light illumi-\nnation direction.\nWe then compared these models by adapting them to\ndownstream tasks, i.e., disease diagnosis. We evaluated the\nmodel performance with the Area Under the Receiver Oper-\nating Characteristic curve (AUROC) and the Area Under the\nPrecision-Recall curve (AUPR). Each experiment is run five\ntimes with random seeds to obtain performance statistics.\n3.3. Experiment Result\nWe first observed the clustering performance, i.e., how posi-\ntive and negative pairs distribute, before and after reducing the\naugmentation scale. The results are shown in Figure 2. The\nmodel pre-trained with weak better separated these pairs.\nWe also repeatedly augmented each image multiple times to\ncreate image groups, where positive pairs consist of images\nwithin the same group, and negative pairs are images from\ndifferent groups. We found that positive pairs cluster more\nclosely under weak augmentation in the t-SNE map [19].\nIn internal evaluation, as shown in Table 4, DINO with\nweak outperforms other augmentation strategies in most\nretinal disease classification tasks. On MESSIDOR-2, IDRID,"}, {"title": "4. CONCLUSION", "content": "In this study, we aimed to improve the contrastive learning\nperformance in the medical image domain. We proposed a hy-\npothesis that the dense distribution of medical images might\ncause the suboptimal performance of contrastive learning, and\nvalidated in our experiments and validation. Our findings sug-\ngest that simply reducing augmentation scales to an appropri-\nate level can improve the clustering performance and there-\nfore enhance model performance. Additionally, when incor-\nporating medical-specific augmentation med to weak, the\ncollective augmentation again decreases $Dis(P^-)$, while in-\ncrease $Dis(P^+)$, generating adverse effects on model perfor-\nmance. These offer key guidance into the model pre-training\nwith contrastive learning for medical images.\nAlthough bringing insights, we acknowledge several lim-\nitations in this work that should be studied in future work.\nFirst, we only validated our hypothesis and solution on DINO;\nmore contrastive learning strategies, such as DINOv2 [5],\ncould be investigated. Second, some quantitative metrics\ndescribing the clustering performance have not been inves-\ntigated, which will be proposed in future work to guide the\naugmentation scaling. Finally, some techniques like tailored\nloss functions adjusting the weights on positive and negative\npairs will be studied. This work pioneered the optimization\nof contrastive learning in the medical domain and encouraged\nthe tailored model training settings for medical images."}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "Only de-identified retrospective data were used in this re-\nsearch, without the active involvement of patients. All\ndatasets used in the downstream applications are publicly\navailable."}, {"title": "6. ACKNOWLEDGMENT", "content": "No funding was received for conducting this study. The au-\nthors have no relevant financial or non-financial interests to\ndisclose."}]}