{"title": "Explore-Construct-Filter: An Automated Framework for Rich and Reliable API Knowledge Graphs", "authors": ["Yanbang Sun", "Qing Huang", "Xiaoxue Ren", "Zhenchang Xing", "Xiaohong Li", "Junjie Wang"], "abstract": "The API Knowledge Graph (API KG) is a structured network that models API entities and their relations, providing essential semantic insights for tasks such as API recommendation, code generation, and API misuse detection. However, constructing a knowledge-rich and reliable API KG presents several challenges. Existing schema-based methods rely heavily on manual annotations to design KG schemas, leading to excessive manual overhead. On the other hand, schema-free methods, due to the lack of schema guidance, are prone to introducing noise, reducing the KG's reliability. To address these issues, we propose the Explore-Construct-Filter framework, an automated approach for API KG construction based on large language models (LLMs). This framework consists of three key modules: 1) KG exploration: LLMs simulate the workflow of annotators to automatically design a schema with comprehensive type triples, minimizing human intervention; 2) KG construction: Guided by the schema, LLMs extract instance triples to construct a rich yet unreliable API KG; 3) KG filtering: Removing invalid type triples and suspicious instance triples to construct a rich and reliable API KG. Experimental results demonstrate that our method surpasses the state-of-the-art method, achieving a 25.2% improvement in F1 score. Moreover, the Explore-Construct-Filter framework proves effective, with the KG exploration module increasing KG richness by 133.6% and the KG filtering module improving reliability by 26.6%. Finally, cross-model experiments confirm the generalizability of our framework.", "sections": [{"title": "I. INTRODUCTION", "content": "Application Programming Interfaces (APIs) are essential in modern software development, enabling seamless interactions between system components [1]\u2013[3]. Even simple programs, such as \"Hello World,\" rely on at least one API. However, the vast number of available APIs and their complex interconnections pose significant challenges for developers. For example, the Java standard library contains over 30,000 APIs, and any two APIs may exhibit as many as 11 distinct types of semantic relations [4], [5]. In such a complex ecosystem, developers often struggle to choose and use the right APIs. Take HashMap and Hashtable as an example: both store key-value pairs, but HashMap is non-thread-safe, while Hashtable is thread-safe. Ignoring these subtle differences can lead developers to choose the wrong API, resulting in inefficient or incorrect implementations. Therefore, organizing the interconnections between APIs (such as similar usage and different behaviors) into structured API knowledge will enhance developers' ability to make informed API choices.\nPrevious studies [6]\u2013[8] introduce the concept of an API Knowledge Graph (KG), which organizes and represents the rich semantic information among APIs. API KGS offer a valuable approach for understanding and leveraging API knowledge in various scenarios. For example, in API recommendation tasks [7], [9], [10], when the order of set elements is not required, an API KG can recommend the more efficient java.util.HashSet over java.util.TreeSet. This is because the KG not only captures the usage of APIs but also reveals subtle differences between them, enabling more informed and context-aware recommendations. However, despite their great potential, constructing a knowledge-rich and reliable API KG still faces many challenges\nExisting methods for constructing API KGs are predominantly schema-based [5], [11], [12]. These methods rely on predefined schemas, including entity types, relation types, and type triples (combinations of entity types and relation types), to guide the extraction of API entities and relations. For example, Huang et al. [11] design a schema that includes 3 entity types, 9 relation types, and 9 type triples by annotating and summarizing the entity types and relation types in API and tutorial documentation, and construct an API KG based on this schema. However, developing such schemas requires annotators to invest significant time, resulting in high labor costs. In particular, the more relation types are needed for constructing a rich KG, the more annotated documentation there will be, which further exacerbates the labor costs [13].\nIn the field of natural language processing, schema-free methods [14]\u2013[17] are another mainstream approach for constructing KGs. Unlike schema-based methods, schema-free methods extract instance triples directly from text, thereby reducing labor costs. For example, Zhang et al. [15] propose EDC, which utilizes large language models (LLMs) through few-shot prompting to identify and extract instance triples ([subject, relation, object]) from input texts, independent of any specific schema. However, when these methods are applied to the construction of API KGs, they often introduce noise and reduce the reliability of the KG. Due to the lack of schema guidance, these methods may extract inaccurate instance triples. For example, the instance triple (SortedSet, is a type of, Set) extracted from the text \"You can add elements to a SortedSet just like you would to a regular Set...\" fails to accurately express the original semantic meaning. Furthermore, the constructed KG lacks type information, which limits its practicality. For example, it is impossible to retrieve classes"}, {"title": "II. BACKGROUND", "content": "In this section, we first introduce API relations and API KG construction, and then discuss how to use LLMs to enhance the API KG construction process."}, {"title": "A. API Relation", "content": "API relations reflect the semantic connections and interactions between different APIs. Liu et al. [8] studied the subtle differences between APIs, comparing them from three aspects: categorization, functionality, and characteristics. Based on this, Huang et al. [5] summarized nine types of API semantic relations, including behavior-difference, function-replace, and efficiency-comparison. For example, the behavior difference relation describes how two similar APIs behave differently when performing the same task, whereas the efficiency comparison relation specifies the variance in efficiency between two APIs under particular conditions. These API semantic relations are widely present in natural language texts (e.g., API documentation and Q&A forums) and understanding them is crucial for the correct and efficient use of APIs. In this paper, we focus more on the semantic relations between APIs rather than their structural relations. The former requires careful mining from large amounts of textual data, while the latter can be easily obtained from development documentation."}, {"title": "B. API KG Construction", "content": "The API KG is a complex network structure designed to represent APIs and their interrelationships. In an API KG, each node corresponds to an API and includes essential attributes such as name, description, and functionality. This information enables developers to quickly understand the API's purpose and usage. The edges in the graph represent semantic relations between APIs, such as constraint and collaboration. By analyzing these relations, developers can gain a clear understanding of how APIs interact, optimizing system integration.\nAPI KG construction begins with the design of the KG schema, which defines the entity and relation types within the KG. This schema uses type triples, such as (ETA, RTR, ETB), to represent connections between entity types A and B through relation type R. Guided by this schema, instances are extracted from diverse data sources to form the API KG, represented by instance triples like (ea, r, eb), where entity a is related to entity b via relation r. Therefore, an API KG with a rich set of instances requires a complete KG schema that includes as many type triples as possible.\nMost API KG schemas are manually designed by annotators [5], [7], [24]. These annotators determine the necessary entity and relation types based on domain knowledge and summarize type triples to construct the schema. However, developing these schemas requires significant time from annotators, resulting in high labor costs. In this paper, we propose an LLM-based automated method that simulates annotators in summarizing entity and relation types from a small set of texts to generate a complete schema. Based on it, we further construct a knowledge-rich API KG."}, {"title": "C. LLM for KG Construction", "content": "LLMs, such as GPT-4 [25] and Claude [26], are deep learning models trained on vast amounts of text data. Recently, LLMs are widely applied to downstream tasks. For instance, some researchers [11], [13] attempt to utilize LLMs to extract API entities and relations, thereby constructing an API KG. However, no work has yet attempted to use LLMs to design a KG schema. In the schema design phase, LLMs need to reason about various components of the KG schema, such as how to categorize entities and how to define relations between them. To improve reasoning accuracy, we use the Chain-of-Thought (CoT) method, which breaks down complex tasks into multiple simple tasks, and leverages the LLM to accomplish these simple tasks step by step.\nTo further enhance the effectiveness of task execution, we introduce the in-context learning method [27], [28]. By providing task descriptions and examples, this method helps LLMs capture patterns and rules within tasks. However, research [29], [30] shows that the effectiveness of in-context learning largely depends on the design of prompts, including prompt style, example content, and example order. To address it, we adopt structured prompts [31] to enhance the LLM's performance across various tasks. By combining these strategies, LLMs can better simulate manual annotators to construct API KGs."}, {"title": "III. MOTIVATION", "content": "In this section, we will provide a detailed explanation of the limitations of existing methods. As shown in Fig. 1, six texts are randomly selected from Stack Overflow posts, each describing the semantic relations between Java APIs. Both methods are applied to construct KGs from these texts.\nFig. 1-A illustrates the workflow of the representative schema-based method, short for MKC [5]. First, this method relies on annotators to summarize entity types, relation types, and type triples to design the KG schema. Based on this schema, preset rules (e.g., API_1 be similar to API_2) are then used to extract instance triples from the texts. Finally, the method constructs an API KG containing instance triples with their type information. For example, the entity type of Collection.sort() is method, and the relation type of \"relies on\" is dependency. Note that no instance triples are extracted from the fourth and sixth texts, as they do not match any type triples in the schema. However, this method heavily depends on annotators, leading to high labor costs.\nOne representative schema-free method is GraphRAG [14]. As shown in Fig. 1-B, GraphRAG requires users to specify the entity type according to the task. Based on these entity types, an LLM is used to extract instance triples from the text, constructing a KG containing five instance triples. However, due to the lack of guidance on relation types, LLMs fail to accurately analyze the relation between entities. Therefore, the extracted instance triples are inconsistent with the text semantics, introducing noise and thus reducing the reliability of the KG. For example, this method extracts the incorrect instance triple (SortedSet, is a type of, Set) from the fifth text. Furthermore, these instance triples lack relation types, which limits the utility of the KG. For example, the instance triple (collection.sort(), relies on, Array.aslist()) belongs to the type triple (method, null, method).\nZhang et al. [15] propose another schema-free method, EDC, with its workflow illustrated in Fig. 1-C. First, the method uses an LLM to extract instance triples, forming an"}, {"title": "IV. APPROACH", "content": "In this paper, we propose an LLM-based automated method for API KG construction. The overall framework of our method is shown in Fig. 2, which consists of three modules: KG exploration, KG construction, and KG filtering. The KG exploration module thoroughly analyzes entity types and relation types from seed texts. It then combines entity types and relation types comprehensively using a fully connected strategy to form a KG schema containing all potential type triples. The KG construction module extracts instance triples based on this schema to construct a rich but unreliable KG, which may contain some suspicious instance triples. The KG filtering module then removes these suspicious instances using frequency-based statistics, constructing a rich and reliable KG. Next, we will introduce each module in detail."}, {"title": "A. KG Exploration", "content": "To construct a richness API KG, the primary task is to design a KG schema that contains diverse entity types and relation types. To achieve this, we design the KG exploration module, which follows the principle of \"abstracting high-dimensional types from low-dimensional facts\" to automatically design the KG schema in a bottom-up manner. As shown in Fig. 3, the KG Exploration module consists of seven functional units: entity extraction, relation extraction, entity type labeling, relation type labeling, entity type fusion, relation type fusion, and fully connected KG schema generation. Among them, except for the relation type labeling and the fully connected KG schema generation unit, the other units are all AI units, which implement specific functions by calling LLMs. The input of the KG exploration module is a set of seed texts, and the output is a potential KG schema. Next, we will provide a detailed description of the functional units of this module to help understand its workflow.\n1) Entity Extraction: This unit is used to extract API entity instances of any type from the given text. As shown in Fig. 3, the input of this unit is natural language text, and the output is the API entities contained in the text. To improve the performance of LLMs, based on the extensive prompting pattern proposed by Xing et al. [31], we format the natural language prompt into a controllable structured prompt. For each Al unit of our framework, we provide a detailed description of its prompt design, including role descriptions, task commands, and considerations, as well as four illustrative examples. For more details, please refer to the Appendix IX. The structured prompt design for this unit is shown in Fig. 8.\n2) Relation Extraction: To extract the relations between API entities more accurately, similar to previous works [13], we combine the extracted entities into API pairs. For example, e1, e2, and e3 can be combined into (e1, e2), (e2, e3), and (e1, e3). As shown in Fig. 3, we input the text and API entity pairs into the relation extraction unit at the same time, it outputs the instance triples in the form of (ehead, r, etail). The structured prompt design for this unit is shown in Fig. 9.\n3) Entity Type Labeling: This unit is used to label the entity types of API entities. As shown in Fig. 3, its input is the text and the extracted API entities, and its output is the entity type to which each entity belongs. In order to achieve the goal of abstracting high-dimensional types from low-dimensional facts, this unit should output specific and low-dimensional entity types, such as concrete class, utility class, etc. Please refer to Fig. 10 for more detailed information.\n4) Relation Type Labeling: The relation type labeling unit is used to label the relation types of the relation. Since the relation instances in the instance triples are concise enough, they can be regarded as low-dimensional relation types. In short, the relation types here are exactly the relation instances in the instance triples, so this unit is a non-AI unit and does not require the participation of the LLM.\n5) Entity Type Fusion: As shown in Fig. 3, this unit can only start after the entity types in all the seed texts have been labeled. It takes all low-dimensional entity types as input and outputs new high-dimensional entity types. For example, concrete class and utility class are fused into the class category. To improve the accuracy of subsequent schema-guided entity extraction, this unit also generates definitions for each fused entity type. Moreover, it outputs the mapping between the new entity types and the original types, such as \"class: [concrete class, utility class]\", for performance evaluation. The prompt design for this unit can be seen in Fig. 11.\n6) Relation Type Fusion: The relation type fusion unit abstracts low-dimensional relation types into high-dimensional ones. It takes all low-dimensional relation types as input and outputs new high-dimensional relation types. For example, \u201crelies on\u201d and \u201cdepends on\u201d can be fused into new relation type \"dependency\" This unit can only start after all relation types have been extracted from the seed texts. It also outputs the definition of the new relation type and the mapping between the new relation type and the low-dimensional relation types. More details of this unit can be found in Fig. 12.\n7) Fully Connected KG Schema Generation: This unit is used to construct the KG schema, which guides the subsequent"}, {"title": "B. KG Construction", "content": "To construct API KG based on KG schema, we design the KG construction module. As shown in Fig. 4, this module consists of three functional units: schema-guided entity extraction, schema-guided relation extraction, and entity-relation collection. The first two are Al units while entity-relation collection is a non-AI unit. Next, we will describe these units.\n1) Schema-guided Entity Extraction: This unit is used to extract API entities of a given type from the text. Its input is the text, entity types and their definitions, and its output is API entity instances and the entity types to which they belong. The prompt design of this unit can be seen in Fig. 13.\n2) Schema-guided Relation Extraction: To improve the accuracy of relation extraction, we combine the extracted API entities into API pairs. Then, we input the text, API entity pairs, and the relation types into this unit, which output are the extracted API relations and the relation types to which they belong. Please refer to Fig. 14 for more details.\n3) Entity-Relation Collection: After extracting entities and relations from all the texts, this unit will collect all the instance triples and their type information to construct the API KG."}, {"title": "C. KG Filtering", "content": "Since the KG exploration module is fully automated and lacks manual verification of the KG schema, there may be many invalid type triples. As a result, the KG constructed based on this schema may contain suspicious instance triples. To remove these suspicious instance triples, we design a KG filtering module. As shown in Fig. 5, this module consists of two non-AI units: KG schema update and KG update.\n1) KG Schema Update: This unit is used to remove invalid type triples in the KG schema. Its inputs are the unreliable KG and the potential KG schema, and its output is the validated KG schema. In this module, we adopt a frequency-based method to access the validity of type triples.\nIn the field of data mining, the association rule is often used to measure the association strength between different items [32]\u2013[34]. Inspired by this, we apply the association rule to evaluate the association strength between entity types and relation types in type triples, that is, the validity of type triples. However, there are various ways to construct association rules. For example, the relation type can be inferred from the entity type pair, that is, (ET1, ET2) -> RT1; another entity type can also be inferred from a certain entity type and the relation type, that is, (ET1, RT1) -> ET2. Since this paper mainly focuses on the potential relation types between entity types, the Pattern (ET1, ET2) -> RT1 is adopted. For this Pattern, the method includes three metrics:\n* Support: It refers to the proportion of the number of type triples (ET1, RT1, ET2) in the KG, which can be used to measure the universality of the type triple. Its calculation formula is as follows, where all refers to the total number of type triples.\nSupport (Pattern) =  $\\frac{num (ET1, RT1, ET2)}{all}$\n* Confidence: It refers to the conditional probability of the appearance of the relation type RT1 under the condition that the entity types ET1 and ET2 already exist, which can be used to measure the reliability of the type triple. Its calculation formula is as follows:\nConfidence (Pattern) = $\\frac{Support(Pattern)}{Support(ET1, ET2)}$\n* Lift: It is the ratio of the confidence of the Pattern (ET1, ET2) -> RT1 to the probability of the relation type RT1 appearing independently in the KG, which is used to measure whether there is dependence between (ET1, ET2) and RT1. Its calculation formula is as follows:\nLift (Pattern) = $\\frac{Confidence (Pattern)}{Support (RT1)}$\nIn summary, these three metrics evaluate the validity of type triples from different perspectives. The support reflects the universality, the confidence reflects the reliability of the association, and the lift reflects the interdependence. However, when the thresholds are set too high, some valid type triples will be missed; conversely, when the thresholds are set too low, some invalid triples will be retained. To address this, we design an experiment to explore the optimal choice of thresholds (see Section VI-A for details). Finally, Support, Confidence, and Lift are set to 0.005, 0.02, and 1.0, respectively. In this unit, a type triple can be considered valid only when the values of these three metrics of the type triple are all higher than their respective thresholds. Based on this, we remove the invalid type triples and obtain a validated KG schema."}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "This section delves into four RQs to evaluate and discuss our method's performance."}, {"title": "A. What is the optimal threshold in the KG filtering module?", "content": "1) Motivation: In the KG Filtering module, we apply the association rule to filter out invalid type triples. The association rule involves three key metrics: support, confidence, and lift. Setting appropriate thresholds for these metrics is crucial for ensuring the effectiveness of the KG filtering module and the reliability of the KG. This RQ aims to explore the optimal thresholds for these metrics to balance the reliability of the KG with the richness of the API knowledge it contains."}, {"title": "B. How well does our method perform in KG construction?", "content": "1) Motivation: In this paper, we design three main modules to achieve the automated construction of the API KG. These three modules contain various Al units, such as entity extraction, entity type annotation, and so on. This RQ aims to explore whether these Al units are effective and to investigate the performance of our method in constructing the API KG.\nAnswer: The Al units we designed can efficiently complete various tasks, ensuring the construction of the KG. Our method overcomes the limitations of existing method, discovering a rich variety of entity and relation types, thereby constructing a practical and reliable KG."}, {"title": "C. Is the Explore-Construct-Filter framework effective?", "content": "1) Motivation: To enhance the richness and reliability of the API KG, we propose the exploration-construct-filter framework and design three core modules: KG exploration, KG construction, and KG filtering. This RQ aims to verify whether this strategy can enhance the effectiveness of the KG.\nAnswer: The explore-construct-filter framework is effective and indispensable. This framework can significantly improve the richness and reliability of the API KG."}, {"title": "D. How generalizable is our method across different LLMs?", "content": "1) Motivation: In this paper, we propose an automated method based on the LLM (GPT-4) to construct API KGs. For this RQ, our goal is to verify whether different LLMs impact our method.\nAnswer: Our method is universal across different models, and the more capable the model is, the better the performance of this method will be."}, {"title": "VII. DISCUSSION", "content": "This section includes two parts: one is the threat to validity of our method, and the other is the advantages of this method."}, {"title": "A. Threats to Validity", "content": "The validity of this paper faces three main threats. The first is the manual annotation of experimental results, which may be influenced by the annotator's subjective judgment. To minimize this bias, we assign two annotators to each dataset and calculate the kappa coefficient to assess their agreement. All kappa values exceed 0.75, indicating a high level of consistency in the annotation results, thereby ensuring the reliability of the experimental results.\nAnother threat comes from the threshold setting in the KG filtering module. If the threshold is set too high, it may filter out some correct and valuable type triples; if set too low, it may retain too many low-confidence type triples, reducing the reliability of the KG. Since exhaustively testing all threshold combinations is impractical, we test five sets of thresholds and select the optimal one. However, the chosen threshold may still not be the best, and a few valuable type triples still be filtered out, such as (method, replacement, method).\nThe last threat comes from the choice of seed text. To ensure a fair comparison, we treat the text used for designing the KG schema in the baseline [6] as the seed text and construct a KG schema with a more diverse entity types and relation types. However, we do not test the method's performance on other seed texts. Nevertheless, we find that the KG schema constructed by our method already covers the majority of type information, including 4 entity types and 13 relation types. In the future, we will apply this method to more API texts to explore whether additional relation types can be discovered."}, {"title": "B. Advantages of Our Method", "content": "Although this method is currently applied to API data, theoretically, with minor adjustments to the prompts, it could be extended to other data domains, thus becoming a universal method for automatic KG construction. However, achieving true universality presents some challenges. The data structures and semantic characteristics differ significantly across different fields, which may limit the adaptability of this method to other domains. In the future, we will focus on adjusting this method to adapt to the specific data structures and semantic requirements of different fields.\nIntegrating our method with existing KG retrieval tech-nologies, such as GraphRAG [14], can form a comprehensive tool for knowledge extraction, analysis, and utilization. This integration leverages GraphRAG's expertise in KG retrieval and optimization, complementing the intelligent construction capabilities of the proposed method, thereby enabling a one-stop solution for KG construction, updating, and application."}, {"title": "VIII. RELATED WORK", "content": "An API KG is a complex network that represents API entities and their relations using a graph structure. In an API KG, nodes typically represent API entities (e.g., classes, methods, etc.), while edges represent relations between en-tities (e.g., invocation, constraint, etc.). The construction of an API KG aims to extract structured knowledge from API documentation, codebases, and other resources, helping de-velopers better understand and utilize APIs. For example, an API KG can recommend suitable APIs for specific tasks to developers [45]. Additionally, API KGs can support tasks such as code generation [46], [47] and misuse detection [12], [48].\nEarly API KG construction methods [8], [24], [49] rely on manually designed KG schemas. These methods typically follow a \"pipeline\" approach, including sub-tasks such as entity identification, entity classification, and relation classi-fication. For instance, entity identification may use regular expressions [50], island parsing [51], or heuristic rules [7], while relation classification relies on syntactic analysis and annotation techniques [8], [52]. Additionally, some studies attempt to use machine learning methods to automate the extraction of entities and relations. For example, Ye et al. [53] propose APIReal, which uses CRF to identify API entities. Huo et al. [54] design ARCLIN, which uses BI-LSTM as encoder and CRF as decoder to identify API entities, rather than just CRF. To further improve the accuracy of knowledge extraction, Huang et al. propose AERJE [11], which achieves joint extraction of API entities and relations by fine-tuning a T5 model. Although these methods perform well in specific scenarios, they also have significant limitations. On one hand, rule-based methods have limited generalization ability and struggle to adapt to the diversity of different API documents. On the other hand, machine learning methods typically require large amounts of labeled data, which is particularly challeng-ing in low-resource environments.\nWith the advancement of LLM technologies, LLM-based KG construction methods have gradually become mainstream."}, {"title": "IX. CONCLUSION AND FUTURE WORK", "content": "This paper proposes an LLM-based automated KG construction method to address the issues of high manual cost, and noise in existing method. The method introduces three key im-provements: automation to enhance efficiency, comprehensive type discover to improve richness, and the \u201cexplore-construct-filter\" strategy to ensure reliability. Specifically, this method includes three core modules: the KG exploration module, the KG construction module, and the KG filtering module. The KG exploration module generates a complete schema through diverse type combinations based on the \"fully con-nected graph\" concept, ensuring the schema's completeness and comprehensiveness. Next, the KG construction module leverages LLMs to extract schema-compliant instances from large-scale text corpora, forming a preliminary KG. Finally, the KG filtering module enhances reliability by filtering out suspicious triples using probabilistic methods. Experimental results show that this method can explore a schema that includes comprehensive entity types and relation types, and based on this, construct a rich and reliable KG. While effective with API data, the method's adaptability suggests broader applications across various fields and data structures. Looking ahead, we aim to integrate this method with KG retrieval tools like GraphRAG to create a comprehensive knowledge extraction, analysis, and utilization toolkit."}, {"title": "APPENDIX", "content": "This section provides supplementary materials, including detailed prompt designs, parameter settings for our method, and the KG schemas generated by different methods."}, {"title": "A. Prompt Design for Our Method", "content": "All prompts in this paper use a structured design [31]. Taking Fig. 8 as an example, it has three top-level parts: @Persona (which defines the identity and function of LLM), @ContextControl (which sets behavior constraints for LLM), and @Instruction (which provides operation instructions for LLM).\nAmong them, Persona contains two sub-parts:\n* @Description: describes the task objective: (such as \"You are an intelligent API entity extractor...\");\n* @Terminology: describes technical terms: (such as \"Terms API entity...\").\n@ContextControl contains several @Rules that limit the behavior in the context, e.g., \u201cEnsure your output is concise...", "sub-parts": "n* @InputVariable: describes the input of prompt (such as \"text\" here);\n* @Commands: clarifies the execution steps of the LLM, such as \"Based on the definition of API entity terminology, extract the API entities...\";\n* @OutputVariable describes the input of prompt (such as 'entities", "Rules": "emphasizes the notices when LLM executes the command, such as \"The part of speech for API entities...", "Example": "It is used to help understand the requirements of the task and clarify the output specifications."}, {"title": "B. Parameter Setting", "content": "In this paper, we implement our method and baselines by calling GPT-40. It is the latest model of OpenAI, which has outstanding text understanding capabilities and can perform relatively complex inference tasks [25]. When calling the LLM, some parameters usually need to be set, including temperature, max_tokens, n, frequency_penalty, and pres-ence_penalty. Among them, temperature is used to control the randomness of the generated text. To ensure the stability of our method, we set it to 0 so that the LLM can generate more deterministic results. Max_tokens is used to specify the maximum length of the generated result. Since the result lengths output by different units are different, max_tokens has no fixed value. For example, the max_tokens of the entity extraction unit is set to 128; while for the entity type fusion unit and the relation type fusion unit, the max_token is set to 4096. The parameter n represents the number of generated results and is set to 1. In addition, frequency_penalty and presence_penalty are used to control the coherence of the generated text, and they are kept as the default values (0)."}, {"title": "C. The KG Schema Generated by Each Method", "content": "In this section, we will introduce the KG schema generated by each method. First, Table X presents the 13 relation types generated by our method, 8 of which overlap with the relation types in the existing MKC method [6]. For example, equivalence and function similarity both indicate that entities are similar or equal in functionality. Since the occurrence frequency of the relation type \u201cfunction opposite\" in MKC is low, our method filters it out. However, we also discover five unique relation types, including:\n* Containment: It indicates that one entity contains an-other entity within it. For example, SortedMap contains headMap().\n* Modification: It means that One entity alters or modifies another entity. For instance, remove() can modify the elements in a SortedSet.\n* Execution: It represents that one entity initiates or carries out the operation of another entity. For example, execute lock() to close the Lock instance.\n* Access: It implies that one entity retrieves or acquires data from another entity. For example, readInt() reads data from a DataInputStream.\n* Limitation: It signifies that one entity imposes constraints on another entity's behavior or functionality. For exam-ple, the output of add() is limited by the state of the BlockingQueue.\nThese new relations types form the foundation for constructing a comprehensive API KG.\nTable XI compares the differences in type information be-tween existing methods and our method. GraphRAG [14] and EDC [15] are schema-free methods, with the former lacking relations types (e.g., the type triple (class, null, class)) and the latter lacking entity types (e.g., the type triple (null, check, null)). In contrast, MKC defines 3 entity types (package, class, method) and 9 relations types, while our method defines 4 entity types (package, class, method, interface) and 13 relation types, resulting in 34 type triples (including 26 correct type triples). This ensures the comprehensiveness and richness of the KG. Although the EDC method can refine relation types, there is still redundancy in the final relation types, which can be further optimized. For example, the relation types such as \"checks\", \"precedes\", and \"test\" have similar semantics and can be further merged. In contrast, our method can abstract low-dimensional relation types into high-dimensional ones, avoiding such semantic redundancy."}]}