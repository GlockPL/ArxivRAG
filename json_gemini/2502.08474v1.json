{"title": "Training-Free Restoration of Pruned Neural Networks", "authors": ["Keonho Lee", "Minsoo Kim", "Dong-Wan Choi"], "abstract": "Although network pruning has been highly popularized to compress deep neural networks, its resulting accuracy heavily depends on a fine-tuning process that is often computationally expensive and requires the original data. However, this may not be the case in real-world scenarios, and hence a few recent works attempt to restore pruned networks without any expensive retraining process. Their strong assumption is that every neuron being pruned can be replaced with another one quite similar to it, but unfortunately this does not hold in many neural networks, where the similarity between neurons is extremely low in some layers. In this article, we propose a more rigorous and robust method of restoring pruned networks in a fine-tuning free and data-free manner, called LBYL (Leave Before You Leave). LBYL significantly relaxes the aforementioned assumption in a way that each pruned neuron leaves its pieces of information to as many preserved neurons as possible and thereby multiple neurons together obtain a more robust approximation to the original output of the neuron who just left. Our method is based on a theoretical analysis on how to formulate the reconstruction error between the original network and its approximation, which nicely leads to a closed form solution for our derived loss function. Through the extensive experiments, LBYL is confirmed to be indeed more effective to approximate the original network and consequently able to achieve higher accuracy for restored networks, compared to the recent approaches exploiting the similarity between two neurons. The very first version of this work, which contains major technical and theoretical components, was submitted to NeurIPS 2021 and ICML 2022.", "sections": [{"title": "I. INTRODUCTION", "content": "Network pruning is one of the most well-studied model compression technique for an overparameterized deep neural network, and many different pruning strategies have been introduced with the goal of removing less important parameters from the original network. Due to its simplicity in terms of both implementation and methodology, network pruning has been pretty popular to the point that it is provided as a default model compression function in standard libraries like TensorFlow\u00b9 and PyTorch\u00b2, along with the other compression options such as quantization [1]. Among two major pruning schemes, namely weight pruning (a.k.a. unstructured pruning) [2]\u2013[7] and filter pruning (a.k.a. structured pruning), filter pruning is more actively studied [8]\u2013[13] due to its strength of reducing the actual computation cost at inference time without any special hardware support. In this sense, this article also focuses on filter pruning.\nEven though many of the existing pruning methods achieve a reasonable level of the final accuracy, it mostly comes at an expensive cost of retraining at the end of (or in the middle of) pruning phase. For instance, iterative filter pruning methods [8], [14], [15] alternately perform pruning phase and retraining phase so that the original model can gradually be compressed as well as recovered over the hundreds of epochs. Another major approach is to conduct one-shot pruning and then fine-tune the pruned network for its original performance to be recovered [10], [13], [16]\u2013[18]. In most one-shot pruning techniques, the fine-tuning phase does not take longer than a few dozens of epochs probably because there is no big difference in accuracy among pruning criteria beyond that point. In fact, it is reported that even a randomly pruned network can reach comparable performance after fine-tuning of hundreds or thousands of epochs [11]. Thus, if we do not have any constraints on time, computing resources, or training data for a long fine-tuning process, one-shot pruning methods would be less meaningful. Somewhat fortunately, however, this is not the case in many practical scenarios where we cannot access or re-collect the original data due to its large volume or some commercial issues [19]. Even though some recent works [16], [20] attempt to use only a small amount of training data instead, but they still suffer from a time-consuming process of fine-tuning to achieve satisfactory performance. Furthermore, it would not be always possible to collect samples in the same domain as the original data, regardless of whether they are labeled or not. Only quite a few works [21], [22] have been introduced to recover pruned networks without using any data and fine-tuning process. Their common approach is what we call one-to-one compensation, where the most similar unpruned neuron takes the role of a pruned neuron. This approach is based on the strong assumption that there exists such a neuron quite similar to each one being pruned at the same layer. Not surprisingly, this assumption cannot be guaranteed in deep neural networks, which is even reported in the result of [21] showing that the pairwise similarity on filters gets extremely low particularly in deeper layers. In addition, according to our theoretical analysis on the reconstruction error, a single neuron can only lead to a very rough approximation to the original output even if the neuron is similar enough to its pruned counterpart.\nIn this article, we propose a theoretically more rigorous and robust method, called Leave Before You Leave (LBYL), to effectively compensate for pruned filters, which is also free of any data and fine-tuning. As depicted in Figure 1, we significantly relax the strong assumption by observing the fact that a pruned filter is better to be represented by a linear combination of as many preserved filters as possible than it is done by a single filter. We first mathematically analyze how the reconstruction error can be formulated between the original network and its pruned network, and thereby discover three"}, {"title": "II. RELATED WORKS", "content": "Many filter pruning methods have been studied without considering such a case where we cannot use any data, whether real or synthetic, to restore a pruned network. They mostly go through an expensive retraining process either iteratively [8], [14], [15], [25] or lastly as a fine-tuning phase [17], [18], [26]. In this article, we focus on the opposite case, which is more challenging in practice, where fine-tuning with any data is not available.\nPruning with less fine-tuning. Most of the existing filter pruning methods have tried to avoid an expensive fine- tuning process by means of a carefully designed criterion for identifying unimportant filters such that the loss of information is minimized when they are pruned. To this end, they often make the best use of data-dependent values like channels and gradients that can be obtained by making inferences with some data. [12] proposes a greedy algorithm that prunes the filters whose corresponding channels minimize the layer-wise reconstruction error. Similarly, [9] formulates this problem of channel selection as lasso regression and least square reconstruction. [27] globally chooses the redundant filters in the pre-trained model by utilizing feature-discrimination- based filter importance. [28] suggests uninformative channel selection method via class-aware trace ratio optimization, which measures joint impact across channels in a convolutional layer. [29] introduces data-independent pruning criterion based on coreset together with an intermediate recovery method without training in order to reduce the overhead of fine-tuning. Despite their proposed methods on effective channel or neuron selection, a few epochs of fine-tuning process as well as some training data is inevitable for the pruned network to be sufficiently recovered. To speedup filter pruning, [30] utilizes a finetuning structure based on constrastive knowledge transfer, thereby proposing a coarse-to-fine neural architecture search algorithm.\nPruning with less fine-tuning and less data. For the case of pruning with limited data, several methods [16], [20] utilize knowledge distillation [31] not to use the entire original data in the recovery process. [16] recently proposes CURL that globally prunes filters using a KL-divergence based criterion and perform knowledge distillation with a small dataset as a fine-tuning process. [20] tackles the similar problem and devise a way of transforming all the filters in a layer into compact ones, called reborn filters, using only the limited data. These methods do not use the entire original data, but they still require some kinds of retraining process with a small amount of data.\nPruning with no fine-tuning and no data. In the literature of filter pruning, there are only a few works [21], [22], [32] that adopt the same problem setting as ours, that is, pruning filters without any fine-tuning and data. It is worth mentioning that our work focuses on this pure recovery process with respect to pruned networks without any support of extra memory and hardware, as in [21], [22]. Both methods are based on the strong assumption that we can always find a pair of filters that are quite similar to each other and therefore a pruned filter can be compensated by its similar unpruned one. However, this assumption does not hold in modern convolutional neural networks (CNNs) having a number of filters in a fairly deep architecture. As even presented in [21], the cosine similarity between each filter and its nearest one gets extremely low"}, {"title": "III. PROBLEM FORMULATION", "content": "This section formally defines the problem of restoring a given pruned network with only using its original pretrained CNN in a way free of data and fine-tuning."}, {"title": "A. Filter Pruning in a CNN", "content": "Consider a given CNN to be pruned with L layers, where each l-th layer starts with a convolution operation on its input channels, which are the output of the previous (l - 1)-th layer $A^{(l-1)}$, with the group of convolution filters $W^{(l)}$ and thereby obtain the set of feature maps $Z^{(l)}$ as follows:\n$Z^{(l)} = A^{(l-1)} \\ast W^{(l)}$,\nwhere $\\ast$ represents the convolution operation. Then, this convolution process is normally followed by a batch normalization (BN) process and an activation function such as ReLU, and the l-th layer finally outputs an activation map $A^{(l)}$ to be sent to the (l + 1)-th layer through this sequence of procedures as:\n$A^{(l)} = F(N(Z^{(l)}))$,\nwhere $F(\u00b7)$ is an activation function and $N(\u00b7)$ is a BN procedure.\nNote that all of $W^{(l)}$, $Z^{(l)}$, and $A^{(l)}$ are tensors such that: $W^{(l)} \\in \\mathbb{R}^{m \\times n \\times k \\times k}$ and $Z^{(l)}, A^{(l)} \\in \\mathbb{R}^{m \\times w \\times h}$, where (1) m is the number of filters, which also equals the number of output activation maps, (2) n is the number of input activation maps resulting from the (l \u2013 1)-th layer, (3) k \u00d7 k is the size of each filter, and (4) w \u00d7 h is the size of each output channel for the l-th layer.\nFilter pruning as n-mode product. When filter pruning is performed at the l-th layer, all three tensors above are consequently modified to their damaged versions, namely $\\widehat{W}^{(l)}$, $\\widetilde{Z}^{(l)}$, and $\\widetilde{A}^{(l)}$, respectively, in a way that: $\\widehat{W}^{(l)} \\in \\mathbb{R}^{t \\times n \\times k \\times k}$ and $\\widetilde{Z}^{(l)}, \\widetilde{A}^{(l)} \\in \\mathbb{R}^{t \\times w \\times h}$, where t is the number of remaining filters after pruning and therefore t < m. Mathematically, the tensor of remaining filters, i.e., $\\widehat{W}^{(l)}$, is obtained by the 1-mode product [39]\u00b3 of the tensor of the original filters $W^{(l)}$ with a pruning matrix $S \\in \\mathbb{R}^{m \\times t}$ as follows:\n$\\widehat{W}^{(l)} = W^{(l)} \\times_1 S^T$, where $S_{i,k} = \\begin{cases} 1 & \\text{if } i = i_k \\\\ 0 & \\text{otherwise} \\end{cases}$\ns.t. $i, i_k \\in [1, m]$ and $k \\in [1,t]$.\nBy Eq. (1), each $i_k$-th filter is not pruned and the other (m - t) filters are completely removed from $W^{(l)}$ to be $\\widehat{W}^{(l)}$. This reduction at the l-th layer causes another reduction for each filter of the (l + 1)-th layer so that $W^{(l+1)}$ is now modified to $\\widehat{W}^{(l+1)} \\in \\mathbb{R}^{m' \\times t \\times k' \\times k'}$, where m' is the number of filters of size k' \u00d7 k' in the (l + 1)-th layer. Due to this series of information losses, the resulting feature map (i.e., $Z^{(l+1)}$) would severely be damaged to be $\\widetilde{Z}^{(l+1)}$ as shown below:\n$\\widetilde{Z}^{(l+1)} = \\widetilde{A}^{(l)} \\ast \\widehat{W}^{(l+1)}$\nThe shape of $\\widetilde{Z}^{(l+1)}$ remains the same unless we also prune filters for the (l + 1)-th layer. If we do so as well, the loss of information will be accumulated and further propagated to the next layers. Note that $\\widehat{W}^{(l+1)}$ can also be represented by the 2-mode product [39] of $W^{(l+1)}$ with the transpose of the same matrix S as:\n$\\widehat{W}^{(l+1)} = W^{(l+1)} \\times_2 S^T$"}, {"title": "B. Problem of Restoring a Pruned Network without Data and Fine-Tuning", "content": "As mentioned earlier, our goal is to restore a pruned and thus damaged CNN without using any data and re-training process, which implies the following two facts. First, we have to use a pruning criterion exploiting only the values of filters themselves such as L1-norm. In this sense, this article does not focus on proposing a sophisticated pruning criterion but intends to recover a network somehow pruned by such a simple criterion. Secondly, since we cannot make appropriate changes in the remaining filters by fine-tuning, we should make the best use of the original network and identify how the information carried by a pruned filter can be delivered to the remaining filters.\nDelivery matrix. In order to represent the information to be delivered to the preserved filters, let us first think of what the pruning matrix S means. As defined in Eq. (1) and shown in Figure 2(a), each row is either a zero vector (for filters being pruned) or a one-hot vector (for remaining filters), which is intended only to remove filters without delivering any information. Intuitively, we can transform this pruning matrix into a delivery matrix that carries information for filters being pruned by replacing some meaningful values with some of the zero values therein. Once we find such an ideal S*, we can plug it into S of Eq. (2) to deliver missing information propagated from the l-th layer to the filters at the (l + 1)-th layer, which will hopefully generate an approximation $\\widetilde{Z}^{(l+1)}$ close to the original feature map as follows:\n$\\widetilde{Z}^{(l+1)} = \\widetilde{A}^{(l)} \\ast (W^{(l+1)} \\times_2 S^{*T}) \\approx Z^{(l+1)}$\nThus, using the delivery matrix S*, the information loss caused by pruning at each layer is recovered at the feature map of the next layer. It is noteworthy that here both pruning matrix and delivery matrix are theoretical concepts to see the insight on what is happening when pruning filters from a pretrained CNN, rather than things that actually exist in neural networks.\nProblem statement. Given a pretrained CNN, our problem aims to find the best delivery matrix S* for each layer without any data and training process such that the following reconstruction error is minimized:\n$\\sum_{i=1}^{m'} ||Z_i^{(l+1)} - \\widetilde{Z}_i^{(l+1)}||_1$,\nwhere $Z_i^{(l+1)}$ and $\\widetilde{Z}_i^{(l+1)}$ indicate the i-th original feature map and its corresponding approximation, respectively, out of m' filters in the (l + 1)-th layer. Note that what is challenging here is that we cannot obtain the activation maps in $A^{(l)}$ and $\\widetilde{A}^{(l)}$ without data as they are data-dependent values."}, {"title": "IV. PROPOSED METHOD OF RESTORING PRUNED FILTERS", "content": "In this section, we first theoretically examine how the reconstruction error of Eq. (4) can be formulated in a way free of using data-dependent parameters. Based on this analysis, we present our train-free recovery method that has a closed form solution for minimizing the formulated reconstruction loss."}, {"title": "A. Data-Independent Reconstruction Loss", "content": "For brevity, we deal with a simple case of pruning only a single filter, which can trivially be extended to cover multiple filters. Without loss of generality, let the j-th filter of the l-th layer be pruned. Thus, here after, j is a fixed value that represents a particular filter being pruned.\nLeave Before You Leave (LBYL) assumption. In order to transform Eq. (4) to a data-free version, our proposed assumption is that each filter being pruned can leave its pieces of information for the remaining ones by adding its corresponding pruned channel to each of the other channels in the next layer. Since the quantity of information depends on how much the pruned filter is related to a particular remaining filter, we need to multiply the pruned channel with a different value before addition. Based on this assumption, for the j-th filter being pruned, the delivery matrix $S^* \\in \\mathbb{R}^{m \\times (m-1)}$ is defined to be in the following form:\n$S^*_{i,k} = \\begin{cases} 1 & \\text{if } i = i_k \\\\ s_i & \\text{if } i = j \\\\ 0 & \\text{otherwise} \\end{cases}$\ns.t. $i,i' \\in [1,m]$ and $k \\in [1,m - 1]$, where $s_i$ is a scalar value that quantifies to what extent the i-th filter is related to the j-th filter being pruned. Figure 2(b) shows an example of the delivery matrix for our LBYL method.\nWithout BN and an activation function. Starting with Eq. (4), we first derive its data-free version without considering both a BN procedure and an activation function, and then make an extension for them. With the proposed form of the delivery matrix in Eq. (5) together with Eq. (3), the i-th channel of $\\widetilde{Z}^{(l+1)}$ is now represented as:\n$\\widetilde{Z}_i^{(l+1)} = \\sum_{k=1, k\\neq j}^{m} A_k^{(l)} (W_{i,k}^{(l+1)} + s_{i,k} W_{i,j}^{(l+1)})$.\nThen, the reconstruction error for the i-th channel of the feature map at the (l + 1)-th layer can be derived as:\n$Z_i^{(l+1)} - \\widetilde{Z}_i^{(l+1)} = (A_j^{(l)} - \\sum_{k=1, k\\neq j}^{m} s_{i,k} A_k^{(l)}) W_{i,j}^{(l+1)}$."}, {"title": "V. EXPERIMENTS", "content": "In this section, we empirically validate the performance and effectiveness of our proposed LBYL method, compared to a one-to-one compensation method, called Neuron Merging (NM) [21] as well as a baseline called Prune that does not perform any recovery process after pruning. For a fair comparison, we"}, {"title": "VI. CONCLUSION", "content": "This paper proposed the problem of restoring a pruned CNN in a way free of training data and fine-tuning. We mathematically formulated how filter pruning can make a damage to the output of the pruned network, and focused on how to model the information carried by each pruned filter to be delivered to the other remaining filters. Our proposed assumption is inspired by the fact that the more the remaining filters participate in the recovery process, the better the approximation we can obtain for the original output. With this assumption, we successfully decomposed the reconstruction error into the three different components, and thereby designed a data-free loss function along with its closed form solution. Our future work would be to extend the proposed loss function so as to cover different activation functions other than ReLU."}]}