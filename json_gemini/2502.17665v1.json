{"title": "Effective Field Neural Network", "authors": ["Xi Liu", "Yujun Zhao", "Chun Yu Wan", "Yang Zhang", "Junwei Liu"], "abstract": "In recent years, with the rapid development of machine learning, physicists have been exploring its new applications in solving or alleviating the curse of dimensionality in many-body problems. In order to accurately reflect the underlying physics of the problem, domain knowledge must be encoded into the machine learning algorithms. In this work, inspired by field theory, we propose a new set of machine learning models called effective field neural networks (EFNNs) that can automatically and efficiently capture important many-body interactions through multiple self-refining processes. Taking the classical 3-spin infinite-range model and the quantum double exchange model as case studies, we explicitly demonstrate that EFNNs significantly outperform fully-connected deep neural networks (DNNs) and the effective model. Furthermore, with the help of convolution operations, the EFNNs learned in a small system can be seamlessly used in a larger system without additional training and the relative errors even decrease, which further demonstrates the efficacy of EFNNs in representing core physical behaviors.", "sections": [{"title": "Introduction", "content": "The collective behaviors of many interacting particles such as spins, molecules, and atoms give rise to some of the most intriguing phenomena in condensed matter physics. However, theoretical and computational studies of many-body problems often confront the curse of dimensionality and one must devise strategies to circumvent it. Machine learning has emerged as a powerful tool for extracting effective features from high-dimensional datasets, garnering significant attention for its ability to tackle long-standing problems in condensed matter physics. Numerous machine learning techniques have been applied to a wide range of classical and quantum many-body systems, including classifying phases of matter with supervised [1-3] and unsupervised machine learning [4-8], representing many-body quantum states [9-13], accelerating Monte-Carlo simulations [14-17], fitting high-dimensional potential energy surfaces [18-22] and searching for spin glass ground states [23].\nStandard deep neural networks (DNNs) have been shown to be ineffective in solving many-body problems unless augmented with physical knowledge. For example, mapping the spin configuration to energy in an 8 \u00d7 8 classical two-dimensional Ising model requires highly complex DNN structures [24]. Consequently, extensive research has focused on encoding physics into neural network architectures to enhance their effectiveness in physical applications, such as incorporating physical laws [25-28] and physical constraints [29, 30], mimicking physical processes [31], accounting for symmetry [32] and integrating graph structure in atomic bonding [33-39], employing renormalization group procedures [40]. These physics-encoded neural networks have found applications in computing effective Hamiltonians [41-43], property prediction [36, 39, 41, 43-49], structure search [37, 38], etc.\nThese methods, although often outperforming standard DNNs, tend to rely on intuitive and simplified imitations of physical principles, and still remain elusive in revealing the underlying the many-body interactions.\nIn this work, inspired by field theory, we propose a new class of machine learning models, called effective field neural networks (EFNNs). The core motivation is to decompose many-body interactions into single quasi-particle representations governed by an emergent effective field. Different from standard DNNs, where an adjacent layer relies only on the previous layer, EFNNs incorporate the initial feature values at every layer to form the self-similarity structure with parameters trains via a recursive self-refining process, in the spirit of renormalization group theory, and hence can effectively capture the underlying physics of many-body interactions. Unlike approaches such as FermiNet [50], which directly parameterizes high-dimensional wave-functions without explicit separation of quasi-particles and fields, our method does not presuppose a fixed ansatz for the wavefunction. Instead, the quasi-particle and effective field representations are learned through an recursive self-refining process, mirroring the renormalization group (RG) framework in field theory, while leveraging the expressive power of deep neural networks. This recursive approach progressively refines the accuracy of both the quasi-particle and effective field descriptions.\nWe first illustrate the EFNN architecture by reformulating the classical 2D Ising model within the EFNN framework. The Hamiltonian of a classical 2D Ising model is defined as H(S) = -J\u2211(ij) Sisj, where (ij) denotes a summation over pairs of nearest-neighbor sites, si = \u00b11 represents the spin at site i, Jis the interaction strength between two spins, and S is the collection of all spins. First, we define the effective field on spin si as (S), which is equal to the sum of the nearest neigh-"}, {"title": null, "content": "bor spins of si multiplied by -3. Then, we define the independent spin (quasi-particle) as sidi(S), hence the total energy reads H(S) = \u2211isidi(S). As illustrated schematically in Fig. 1(a), the interacting spins are first mapped to an effective field, then the effective field combining with the interacting spins is mapped to independent spins. Finally, a summation is performed over the independent spins, and the total energy is obtained. We reformulate the computational procedure within a DNN-like structure, depicted in Fig. 1(b). Here, So = S is the input layer, representing the interacting spins. The effective field layer F\u2081 and the quasi-particle layer S\u2081 constitute a single field-particle (FP) layer. The evaluation of S\u2081 relies on both the effective layer F\u2081 and the interacting spin layer So, therefore a connection is required from So to S1, as well as from F\u2081 to S1, distinguishing this architecture from a standard DNN. Finally, a summation is performed over S\u2081 to obtain the energy E.\nIn the classical 2D Ising model, both the effective field and quasi-particles can be exactly calculated, which, however, becomes infeasible for more complex many-body interactions. Following the spirit of renormalization from field theory, we can recursively refine the evaluations of the effective field and the quasi-particles. As shown in Fig. 2(a), we can extend the FP layer number in Fig. 1(b) to obtain the deep effective field neural networks. The evaluation procedure is detailed as\nFi = fi-1(Si-1), Si = gi(So) Fi, E = q(Sn),\nwhere the function fi-1 maps the previous quasi-particle layer Si-1 (for i > 2) or the initial layer So (for i = 1) to the effective field layer Fi. The input layer So is first processed by gi, then multiplied in elements by Fi to produce the quasi-particle layer Si. Each FP layer consists of Fi and Si. The final quasi-particle layer Sn generates the output energy E through the function q, which sums all elements of the last layer. Typically, fi and g; are nonlinear to enhance expressiveness, though linear functions may suffice for simpler models like the classical 2D Ising model with nearest-neighbor interactions.\nWe present two case studies to demonstrate the capability of EFNNs to capture many-body interactions. The first case study focuses on the classical 3-spin infinite range model in 1D, with Hamiltonian:\nH(S) = - \\sum_{i<j<k} J_{ijk} S_i S_j S_k,\nwhere the spins take binary values 0,1, Jijk are constants, and the system comprises 15 lattice sites. We train EFNNs to evaluate H(S) in Eq. (1), and compare its performance with that of DNNs.\nWe generate 16000 training and 4000 test samples using Eq. (1). Utilizing the EFNN architecture in Fig. 2(a), the nonlinear mappings fi and g; are constructed by sequential linear layers with tanh activations. The network is trained with the Adam optimizer and a decaying learning rate. Performance is evaluated by the relative error, defined as the square root of the MSE on the test set divided by the absolute mean of H(S). We compare EFNNs to standard DNNs, using 18 neurons per layer and varying the number of FP layers (n = 1,2,3). With one FP layer, the DNN achieves approximately a 7\u00d710-2"}, {"title": null, "content": "relative error, while the EFNN attains around 4 \u00d7 10-2. As the number of FP layers increases, the DNN's error rapidly worsens, whereas the EFNN's error decreases to about 1 \u00d7 10-2 with three FP layers (Fig. 2(b)). Additionally, Fig. 2(c) shows that increasing the number of neurons in the EFNN from 15 to 150 leads to an algebraic decrease in relative error, reaching a minimum around 150 neurons for all tested FP layers, and optimal performance is achieved with three FP layers. These results demonstrate that EFNNs effectively capture many-body interactions through multiple feed-forward paths, improving accuracy as layers and neurons increase. In contrast, DNNs lose their ability to fit the energy accurately with additional layers. Both the number of FP layers and the internal neuron count are crucial for the EFNNs' high performance.\nWe proceed to our second, more challenging case study: evaluating the Monte-Carlo energy of the quantum double exchange model [51-53] at finite temperature. The Hamiltonian on an NX N lattice reads\nH(S) = -t \\sum_{(i,j),\\alpha} (c_{i \\alpha}^{\\dagger} c_{j \\alpha} + h.c.) - \\frac{J}{2} \\sum_{i,\\alpha,\\beta} \\vec{\\sigma}_i \\cdot \\vec{S}_i \\sigma_{\\alpha\\beta} c^{\\dagger}_{i \\alpha} c_{i\\beta},\nwhere (ij) denotes nearest neighbors, \u0109ia is the fermion annihilation operator with spin \u03b1 at site i, \u03c3 are the Pauli matrices, and si \u2208 S\u00b2 is a 3D classical unit vector representing the local spin at site i, the classical spin interacts with the fermions through an on-site coupling, and S is the collection of all classical spins si. We set N = 10, J = 16t, \u03bc = \u22128.3t, T = 0.1t, t = 1.\nAt finite temperature T, the Monte-Carlo energy is defined as EMC(S) = -\u03a4\u03a3 log(1+e\u00af+En(S)), where {En(S)} are the eigen-energies of H(S) in Eq. (2) for a given spin configuration S. This quantity proves to be useful in Monte-Carlo simulations of the double exchange model, as it can replace exact diagonalization in Determinant Quantum Monte Carlo (DQMC) computations [54]. Diagonalizing a 2N2 \u00d7 2N2 Hermitian matrix for Eq. (2) has a computational complexity of O(N6) for each spin update. Both effective models and neural networks can significantly reduce this complexity. We compare the performance of an effective model incorporating 4-body RKKY-type interactions [55-57] with that of EFNNs adopting symmetrization. The effective model is expressed as\nE_{eff}(S) \\sim J_0 + \\sum_{ij} g_{ij} \\vec{S}_i \\cdot \\vec{S}_j + G_{ijkl} (\\vec{S}_i \\cdot \\vec{S}_j)(\\vec{S}_k \\cdot \\vec{S}_l).\nHere, the dot products include all pairs of spins on the lattice, including self-interactions. Clearly, the number of parameters factorially increases with N and the effective model cannot be solved exactly for a large N. To simplify,"}, {"title": null, "content": "we divide the 10\u00d710 lattice into four 5\u00d75 tiles, restricting two- and four-body interactions within each tile.\nThe spin interactions in Eq. (3) are exclusively expressed as dot products to satisfy the O(3) symmetry of Eq. (2). Inspired by this symmetry, we incorporate symmetrization layers into the EFNN architecture, depicted in Fig. 3(a). The symmetrization is achieved by summing dot products of each channel of So after convolution:\nT_{j}(S_0) = \\sum_{k=x,y,z} \\text{conv}(S_{0,k}) \\text{conv}(S_{0,k}), j = 0,1,...,n.\nHere, each Sok \u2208 RN\u00d7N is convolved into RC\u00d7N\u00d7N Supplementary Material [58] details the convolution process and proves that T; remains invariant under O(3) transformations. As shown in Fig. 4(a), in the first FP layer, F\u2081 and S\u2081 are initialized as\nF_1 = f_0(T_0(S_0)), S_1 = g_1(T_1(S_0)) \\odot F_1.\nFor subsequent layers j = 2,..., n, the effective field and quasi-particle layers are\nF_j = f_{j-1}(S_{j-1}), S_j = g_j(T_j(S_0)) \\odot F_j.\nThe mappings f; and gk consist of a convolutional layer, batch normalization, a tanh activation, and another convolutional layer. Fig. 3 illustrates the computational workflow: (a) symmetrization, (b) generation of quasi-particle layers, (c) transformation to effective field layers for subsequent FP layers, and (d) evaluation of energy. For the energy function q, after passing through the aforementioned layers, an extra summation layer generates a scalar value as the predicted energy. Specifically, the final quasi-particle layer Sn is transformed into a single-channel matrix, and an element-wise summation is performed to produce the scalar energy prediction E, as shown in Fig. 3(d). The layer dimensions are as follows: input layer So \u2208 R3\u00d7N\u00d7N, intermediate layers Fj, S; \u2208 RC\u00d7N\u00d7N, for j = 1,...,n, and output E = q(Sn) \u2208 R.\nWe generate a total of 2 \u00d7 106 data points and split them into 80% for training and 20% for testing. These data are obtained by exact diagonalization of the Hamiltonian in Eq. (2). For the effective model in Eq. (3), we perform a linear regression to obtain the coefficients. The EFNNs with symmetrization are trained using networks with C = 10, 15, 20, 25 channels and n = 1,2,3 FP layers. As shown in Fig. 4(b), the effective model achieves a relative error of approximately 2 \u00d7 10-2. In contrast, the EFNNs with symmetrization consistently reduce the relative error below 3\u00d710-3 as Cincreases from 10 to 25, for n = 1, 2, 3. Notably, the EFNNs outperform the effective model by a factor of six while utilizing significantly fewer parameters, underscoring their capability to capture high-order interactions in quantum many-body systems. Their parameter count scales as O(nC2), resulting in 26250, 49560, and 72870 parameters for n = 1,2,3 at C = 10. In stark contrast, the perturbative expansion in"}, {"title": null, "content": "Eq. (3) scales factorially as O(N^{2+2+k-1}), with k denoting the interaction order a scaling that can lead to divergence at higher orders. The full effective model in Eq. (3) contains 12758826 parameters, which reduces to 213201 when interactions are restricted within each tile. Furthermore, EFNNs effectively select the most significant interactions automatically, an essential aspect of renormalization. By renormalizing the interactions and prioritizing key terms, EFNNs avoid both divergence and explosive growth in the number of parameters.\nWe further assess the extrapolation performance of EFNNs with symmetrization with the model trained on a 10 x 10 lattice and tested on systems of N = 15, 20, 25, 30, 35, and 40. Using the relative error of energy as the performance metric, Fig. 4(c) illustrates that for N = 10, all EFNNs accurately predict the energy, consistent with Fig. 4(b). As the lattice size increases, the relative error slightly decreases for all FP layers with n = 1,2,3. This occurs because the absolute error increases only marginally with N, while the absolute value of the average energy grows significantly, thereby reducing the relative error. Moreover, EFNNs with n = 2 and n = 3 FP layers outperform those with a single FP layer, demonstrating enhanced renormalization capability with additional layers. Notably, even for larger systems, all models maintain relative errors below 3 \u00d7 10-3, with the n = 2 configuration achieving a minimum relative error of 1 x 10-3. These results underscore the EFNN's efficacy: the convolution operations with padding under boundary condition PBC effectively capture interactions"}, {"title": null, "content": "between localized and neighboring spins, and the renormalization through multiple FP layers ensures robust energy predictions across varying lattice sizes.\nWe give some discussions and conclude this paper. In both case studies, we have demonstrated that increasing the number of layers and neurons (or channels in convolutional layers) enhances the neural network's accuracy, aligning with well-established findings in fields like computer vision. EFNN's self-similar structure mirrors the renormalization process\u2014a non-perturbative method for evaluating complicated functions, such as the Yang-Mills \u1e9e function [59] or Becke's correction to the LDA functional in DFT [60]. This approximation framework stems from Pad\u00e9 approximants [61] using rational functions, which can also be expressed as continued fractions-an important and specific example of continued functions [62], known for superior convergence with much fewer parameters [63-66]. Actually, EFNNs possess exactly the structure of continued functions, as shown in Fig. 5(a). In an EFNN, the initial feature layer So connects recursively to every quasi-particle layer via mappings gj; each nonlinear mapping fj\u22121 is multiplied with gj(So). In addition, EFNNs employ the tanh activa-"}, {"title": null, "content": "tion function within their renormalization-like structure, making them compatible with standard deep learning frameworks and enabling training via stochastic gradient descent with back-propagation. This contrasts with Pad\u00e9 approximants, which typically use the inverse function and hence usually determine parameters through perturbative series matching due to potential singularity in the gradient descent. As shown in Fig. 5(b), ResNets [67-70] adopt a similar but fundamentally different skip-like connections from EFNNs. In a ResNet, So is skip-connected to later layers only once, with later skip connections starting in the middle, and its summation-based aggregation fails to meet the continued function formulation\u2014resulting in suboptimal performance to characterize the many-body interactions. Standard DNNs (Fig. 5(c)) lack skip connections entirely, and So only appears in the beginning of iterations, resulting in a very limited expressive capability. Moreover, by partitioning intermediate layers into effective field and quasi-particle layers, EFNNs provide a clear physical interpretation: quasi-particles emerge from the dot product of interacting particles (via a mapping) and effective fields, which is absent in ResNets and standard DNNs.\nIn conclusion, inspired by field theory, we have proposed a novel deep neural network architecture called EFNN, which effectively captures high-order interactions in both classical and quantum models. EFNN's high accuracy and efficiency arise from the recursive refinement of effective fields and the construction of quasi-particles that encapsulate complex many-body effects. As discussed earlier, the connections from the true particle layer So to the quasi-particle layers (or the preceding symmetrization layers) establish a structure of continued functions, and have renormalization power analogous to Pad\u00e9 approximants. Once the effective fields and quasi-particles are properly renormalized, the trained model can accurately predict the energy for a system with complicated many-body interactions. Furthermore, EFNNS maintain high accuracy even when applied to significantly larger systems. In future work, we aim to integrate Pad\u00e9 approximation and continued function theories more rigorously into the EFNN framework and derive error estimates for the neural network's predictions."}]}