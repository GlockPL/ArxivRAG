{"title": "LLM AGENTS IMPROVE SEMANTIC CODE SEARCH", "authors": ["Sarthak Jain", "Aditya Dora", "Ka Seng Sam", "Prabhat Singh"], "abstract": "Code Search is a key task that many programmers often have to perform while developing solutions\nto problems. Current methodologies suffer from an inability to perform accurately on prompts that\ncontain some ambiguity or ones that require additional context relative to a code-base. We introduce\nthe approach of using Retrieval Augmented Generation (RAG) powered agents to inject information\ninto user prompts allowing for better inputs into embedding models. By utilizing RAG, agents\nenhance user queries with relevant details from GitHub repositories, making them more informative\nand contextually aligned. Additionally, we introduce a multi-stream ensemble approach which when\npaired with agentic workflow can obtain improved retrieval accuracy, which we deploy on application\ncalled repo-rift.com. Experimental results on the CodeSearchNet dataset demonstrate that RepoRift\nsignificantly outperforms existing methods, achieving an 78.2% success rate at Success@10 and a\n34.6% success rate at Success@1. This research presents a substantial advancement in semantic code\nsearch, highlighting the potential of agentic LLMs and RAG to enhance code retrieval systems.", "sections": [{"title": "Introduction", "content": "A key task that many programmers often perform is searching through codebases to find snippets that can solve specific\nproblems. This practice coined as code search is essential for facilitating code reuse [1]. While traditional code\nsearch involves the usage of keyword matching, code search has evolved to learn and predict on the semantics behind\nqueries and snippets allowing programmers to more accurately retrieve code that aligns with their intent. Recent\nadvances in deep learning have been at the center of current methodologies. Through training large language models\n(LLM) on large corpora of text and code, LLMs have obtained strong natural language to code generation capabilities\nwhich has extended to better semantic code search. Notable research in this domain includes \"Deep Code Search\"\n[2], which utilizes recurrent neural networks, to learn sequential information behind code and their descriptions and\nconsequently map them into a unified vector space. Building upon DeepCS, other architectures like Carl-CS which\nexploits co-attentive representation learning [3] and PSCS [4] which focuses on using code flow obtained from Abstract\nSyntax Trees also improved code search capabilities. Other significant work in this domain is \"CodeBERT: A Pre-\nTrained Model for Programming and Natural Languages\" [5], which leverages a bi-modal transformer to jointly model\nprogramming and natural languages, significantly enhancing the model's ability to form accurate embeddings based on\nsemantic content. Building on this approach, the paper \"Text and Code Embeddings by Contrastive Pre-Training\" by\nOpenAI [6] introduces a contrastive learning technique paired with unprecedented large training data to generate state\nof the art embeddings for both text and code, further enhancing the ability (even above CodeBERT and variations like\nGraphCodeBERT [7]) to match natural language queries with relevant code snippets by distinguishing subtle differences\nin meaning across various contexts."}, {"title": "Methodology", "content": ""}, {"title": "Information Injection via Agentic Large Language Models (LLMs) and Retrieval Augmented Generation (RAG)", "content": "Given a natural language query Q and a GitHub repository database D, as seen in Figure 1 we enhance the query using\nan agent with internet access. The agent's primary objectives are to contextualize Q relative to D and to enrich Q with\nadditional details, thereby improving the match between the user input and the correct code snippet.\nOur agent architecture, built using the CrewAI framework on top of OpenAI's GPT-4 model, functions as a \"Technical\nResearch Writer.\" The agent augments the query based on the prompt:\n\"Given an input text prompt: [Q]. Add more technical details about some of the topics in this text prompt in the general\ncontext of the following github repo: [D]. If you can't find how it is implemented in the repository, then provide\ninformation on how it is implemented generally. Ensure that you are not given more info than necessary and only give"}, {"title": "Ensemble Architecture with Multi-Stream Comparisons", "content": "The purpose of the ensemble architecture is through many different comparisons, a more accurate final set of likely\nsnippets can be formed. Additionally, during code generation, classes can be created with many different functions, and\na multi-stream architecture that breaks down the generated code is needed. Once A is created, our methodology forwards\nit to multi-stream processes that work together to produce a small set of targeted snippets. In our implementation, all\ncode (exclusively in Python) is divided into a set of functions Y and a set of classes Z. The initial step involves creating\nan embedding for A. Then the first stream compares the embedding of Q with the embeddings of each element in Y\nwhere the top 3 elements in Y with the largest cosine similarity to Q are added to the final target set:\nEmbedding(A) = $e_A$\n$e_Y = \\{e_{Y_1}, e_{Y_2},..., e_{Y_n}\\}$\nCosine Similarity($e_Q, e_Y$) = $\\frac{e_Qe_Y}{||e_Q||||e_Y||}$\nTop 3 in Y = {$Y_{i1}, Y_{i2}, Y_{i3}$}\nThe second stream processes involves generation of code A through two chains of OpenAI's GPT-3.5-turbo to generate\ncode C and then evaluate its quality.\nGPT-3.5-turbo(A) \u2192 C\nC is then converted to an embedding and compared with the embeddings of each element in Y and Z\nEmbedding(C) = $e_C$\n$e_Y = \\{e_{Y_1}, e_{Y_2},..., e_{Y_m}\\}$\n$e_Z= \\{e_{Z_1},e_{Z_2},..., e_{Z_k}\\}$\nCosine Similarity($e_C, e_Y$) = $\\frac{e_Ce_Y}{||e_C||||e_Y||}$"}, {"title": "Experimental Setup", "content": ""}, {"title": "Dataset", "content": "Following [4], we leverage the CodeSearchNet dataset [13], which features numerous pairs of natural language queries\nand corresponding code snippets, all associated with specific GitHub repositories. To conduct our study, we manually\nprocessed each natural language query via repo-rift.com, randomly selecting 101 rows from the Python evaluation set\nof CodeSearchNet. To maintain fairness and ensure broad applicability, we included queries of varying lengths and\nonly altered the natural language query if it detailed parameters or return types. Furthermore, some queries were left\nunmodified, even those containing parameter and return information, to uphold generalizability.\nWe excluded and replaced only those rows where the code snippet had been removed from the current main branch\nof the repository or when the repository size exceeded the upload capacity of our repo-rift.com application. The\nazure-sdk-python repo was the sole instance of the latter issue. We opted to exclude snippets not present in the main\nbranch because our repo-rift.com application could not effectively upload files from previous branches, thus making\nreplacement a more straightforward solution."}, {"title": "Implementation Details", "content": "For the backend of the RepoRift application, we employ third-party packages and OpenAI APIs. The agent is constructed\nusing the CrewAI framework. The website is built with the Vue JavaScript framework and SQL, and it is deployed on a\nstandard AWS plan. To evaluate our software, we manually input 101 rows of data into our website and observe the\nresults displayed as panels on the right."}, {"title": "Evaluation Metrics", "content": "Following previous codesearch research from DeepCS [2], CARLCS [3], and PSCS [4], we utilize the same Success@10\nand Success@1 metrics to compare accuracy. While the aforementioned methods have been translated to evaluate\nthe Java Dataset of CodeSearchNet, we test on the Python Dataset. Success@k is a metric that determines whether a\ndetected code snippet from a system is in the top k results. Therefore, to be positively labeled in the Success@1 metric,\nthe result must be the highest rank."}, {"title": "Results", "content": "We compare our methods to other baselines with the most similar evaluation setups. The evaluation for PSCS, CARLCS,\nand DeepCS have all been translated to CodeSearchNet, where they are given thousands of snippets and expected to\nfind the correct snippet according to a provided natural language query. While these three methods have been built\nfor Java, all we do differently is test on Python. And while the previous baselines, as per [4], conduct a search over\nall test snippets in CodeSearchNet, we conduct a search over all snippets in a GitHub repository as that is what our\nuse case is specifically designed for. We directly take the success rates from [4] and compare them with the success\nrates calculated through our evaluation, making the judgment that the difference is trivial. Additionally we remove\ncomments from all code snippets to ensure an obvious fair evaluation.\nWe chose methods that most closely mimic the real-world use of a tool across different GitHub repositories. This\napproach involves a constantly dynamic set and size of distractor codes that have tighter relationships to the correct\nsnippet. The models chosen for comparison, such as DeepCS, CARLCS, and PSCS, are highly cited. While we couldn't\nfind a specific well-cited piece of research that used a dynamic set of distractor codes, we selected methods with large\nstatic distractor sets. The methods we compared do not inherently use a dynamic set of distractor codes. However, their\ndistractor sets are substantial, with 19k snippets, providing a robust benchmark for evaluation. When ranking from 1 to\n10, we make the sound conclusion that the distractor snippets from 10 to 999 would be significantly different each time\na new natural language query is processed unlike the fixed distractor codes present in CodeBERT [5]. This variation\nclosely simulates a dynamic distractor set, making our comparisons relevant and comprehensive.\nTable 1 provides the evaluation results, comparing our method, RepoRift, against the baselines. The success rates are\nmeasured at two levels: Success@10 and Success @1, which indicate the percentage of correct snippets found within\nthe top 10 and the top 1 results, respectively. Despite not being optimized for Success@1 due to its ensemble approach,\nRepoRift significantly outperforms all other methods. Specifically, RepoRift achieves an 78.2% \u00b18.1 success rate\nat Success@10, which has a lower bound accuracy that is approximately 22.5% better than the highest-performing\nbaseline (PSCS at 47.6%). For Success@1, RepoRift achieves a 34.6% \u00b19.3 success rate, which has a lower bound\naccuracy that is approximately 2.4% better than the highest-performing baseline (PSCS at 22.9%).\nRepoRift achieves high accuracy with minimal preprocessing of the evaluation set. It effectively handles queries in\nvarious forms, including those written in Russian, raw URLs, and vague conceptual information. This versatility\nshowcases RepoRift's capability to understand and process a wide range of input types without requiring extensive\npreprocessing. These results demonstrate that RepoRift not only outperforms other methods in both Success@10 and\nSuccess@1 metrics but also does so while maintaining a high level of flexibility and minimal preprocessing. The\nimprovement in success rates highlights the effectiveness of our approach in searching and identifying relevant code\nsnippets in a larger and more diverse dataset."}, {"title": "Conclusion", "content": "In this paper, we presented the use of information injection as a methodology to improve code search. The reasoning\nbehind such a use case was to add vital details to alleviate the vagueness and ambiguity present in a user prompt for\na code search application. By leveraging agentic LLM models and RAG, our system was able to perform internet\nsearches relevant to a prompt and github repository, consequently addressing the Vocubulary Mismatch Problem and\nallowing for context-aware searches.\nWe provide three main contributions. Firstly, we demonstrate how agentic LLMs in combination with RAG allow for\nfurther contextualization of queries, a methodology we coin as information injection. Secondly, by pairing this process\nwith a multi-stream ensemble approach we achieve state-of-the-art accuracy for semantic code search. By translating\nthe query to code and then utilizing many comparison to generate a final set, a larger variation of snippets are able to\nbe captured. Finally for our third contribution, we deployed our advanced techniques onto a website called RepoRift\n(www.repo-rift.com). RepoRift allows users to perform semantic code searches within specific GitHub repositories.\nThe platform's practical utility and performance in real-world scenarios underscore the effectiveness of our approach.\nOur experimental results, conducted on the CodeSearchNet dataset, show that RepoRift significantly outperforms\nexisting methods such as DeepCS, CARLCS, and PSCS. Specifically, RepoRift achieved an 78.2% success rate at\nSuccess@10 and a 34.6% success rate at Success@1, demonstrating superior performance in both metrics. These\nresults highlight the potential of our method to enhance the accuracy and relevance of semantic code searches. In\nconclusion, our research presents a significant advancement in the field of semantic code search. By integrating agentic\nLLMs and RAG, we have addressed critical challenges and improved the overall effectiveness of code retrieval systems."}, {"title": "Future Work", "content": "Further analyzing the full evaluation breakdown in section 6, we were able to discern several weaknesses in our approach\nthat lays down a better idea for future work. While utilizing code generation before embeddings is helpful for code\nsearch [2], it struggles to account for snippets that are almost primarily constructed from other functions and classes\nwithin a codebase. Therefore, while sometimes through naming conventions in generated code embeddings can still\nretrieve the right snippet, the code search results for this case are significantly weaker. For instance, below are two\nexample of snippets that RepoRift was unable to identify:\nUser Prompt: \"convert a field's content into some valid HTML\""}]}