{"title": "Task-oriented Prompt Enhancement via Script Generation", "authors": ["CHUNG-YU WANG", "ALIREZA DAGHIGHFARSOODEH", "HUNG VIET PHAM"], "abstract": "Large language Models (LLMs) have demonstrated remarkable abilities across various tasks, leveraging\nadvanced reasoning. Yet, they struggle with task-oriented prompts due to a lack of specific prior knowledge of\nthe task answers. The current state-of-the-art approach, PAL, utilizes code generation to address this issue.\nHowever, PAL depends on manually crafted prompt templates and examples while still producing inaccurate\nresults.\nIn this work, we present TITAN-a novel strategy designed to enhance LLMs' performance on task-oriented\nprompts. TITAN achieves this by generating scripts using a universal approach and zero-shot learning. Unlike\nexisting methods, TITAN eliminates the need for detailed task-specific instructions and extensive manual\nefforts. TITAN enhances LLMs' performance on various tasks by utilizing their analytical and code-generation\ncapabilities in a streamlined process. TITAN employs two key techniques: (1) step-back prompting to extract\nthe task's input specifications and (2) chain-of-thought prompting to identify required procedural steps. This\ninformation is used to improve the LLMs' code-generation process. TITAN further refines the generated script\nthrough post-processing and the script is executed to retrieve the final answer.\nOur comprehensive evaluation demonstrates TITAN's effectiveness in a diverse set of tasks. On average,\nTITAN outperforms the state-of-the-art zero-shot approach by 7.6% and 3.9% when paired with GPT-3.5\nand GPT-4. Overall, without human annotation, TITAN achieves state-of-the-art performance in 8 out of\n11 cases while only marginally losing to few-shot approaches (which needed human intervention) on three\noccasions by small margins. This work represents a significant advancement in addressing task-oriented\nprompts, offering a novel solution for effectively utilizing LLMs in everyday life tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) like GPT have significantly advanced the field of Natural Language\nProcessing (NLP) through their proficiency in a wide range of tasks, including text generation [1, 30],\ntranslation [16, 42], summarization [19], and answering questions [21, 25]. These models are\ntrained on vast datasets, enabling them to produce text that is both coherent and contextually\nappropriate [47]. However, when it comes to handling basic, task-oriented problems that involve\nnumerical calculations or step executions, LLMs often fall short [24, 52]. This is an inherent\nweakness due to the way LLMs are designed and constructed. LLMs are trained on large text\ncorpora and finely tuned for linguistic content creation and processing [3]. LLMs construct answers\nfrom text corpora and often face difficulties when the answers are not present directly in the\ntraining dataset but require precise numerical operations or step executions [15]. For instance,\nLLMs' limitation in counting tasks is evident in a simple test. If GPT-4 [2] using greedy decoding\n(0.0 temperature), is asked: \u201cEd had 22 more marbles than Doug. Doug lost 8 of his marbles at the\nplayground. How many more marbles did Ed have than Doug then?\u201d, the answer would be: \u201cEd still\nhad 22 more marbles than Doug\u201d. LLMs have a tendency to provide approximations or incorrect\ncounts. This highlights the necessity for specialized prompt improvement to address these kinds of\ntask-oriented challenges.\nOne approach is to utilize prompt engineering [27, 32, 50, 57, 60] to improve LLMs' performance\non specific tasks. This process involves crafting well-defined, strategically structured prompts\nto guide models towards specific outcomes. For instance, Chain-of-Thought (CoT) [45], one of\nthe prompting techniques, breaks down problems into intermediate, textual steps to facilitate\nproblem understanding and reveal steps toward potential solutions. CoT could be applied to the\nabove problems to help LLM derive steps to complete the task. However, CoT still struggles with\ntask-oriented problems as it still inherits the LLMs' weakness of not having direct access to the\nnumerical answers [15]. To address this, prior work proposes Program-Aided Language Models\n(PAL)[13]. PAL employs the generation of code as an intermediate step in the reasoning process to\nbridge the task execution gap.\nNevertheless, these prompt techniques perform optimally when used with few-shot prompting, in\nwhich hand-picked problem examples and answers are provided to help LLMs correctly comprehend\nand solve problems. However, crafting these examples for few-shot prompting is non-trivial and is\nan extra burden for the users without any related experience [10, 28, 35]. Using wrong examples [37]\nor wrongly ordering examples [31] for few-shot prompting will affect performance dramatically.\nTo address the drawbacks of crafting few-shot prompts for task-oriented problems, we introduce\nTITAN, a novel prompting framework to address the challenges of task-oriented problems while\nrequiring no user effort.\nDistinct from PAL, which directly generates scripts, TITAN incorporates two additional inter-\nmediate reasoning stages: input extraction and step extraction. These additional reasoning stages\nfacilitate the generation of accurate scripts without the need for labeled examples. Specifically,\nTITAN applies step-back prompting [57] to extract the inputs and their specifications for each\ntask, making it the first framework to incorporate step-back prompting into code generation tasks.\nInput extraction helps LLMs overcome their tendency to perform poorly when given meaningless\nvariables or when there is a misunderstanding of the problem's inputs. In parallel, TITAN extracts\nprocedure steps to complete the task by utilizing CoT prompting, which has been shown to aid\nLLMs in accurately comprehending problems by breaking them down into steps [22, 45]. Finally,\nTITAN combines the information from these two additional reasoning stages to generate the most\naccurate scripts that are capable of producing precise answers.\nTo assess the effectiveness of TITAN, we evaluate it across seven distinct prior datasets [9, 13, 23,\n33, 34, 39] containing mathematical and symbolic reasoning tasks and four additional task-oriented\nbenchmarks we constructed in this work. The results demonstrate that TITAN, when paired with\nGPT-4, consistently outperforms PAL zero-shot variant by an average accuracy improvement of\n3.9%. When compared to few-shot approaches, TITAN performs better or comparable in most cases\n(8 out of 11 datasets). Furthermore, TITAN remains effective even with less advanced LLMs, such as\nGPT-3.5. TITAN improves state-of-the-art zero-shot approaches on GPT-3.5 by an average of 7.6%.\nOverall, when using GPT-4, TITAN achieves state-of-the-art performance on 8 out of 11 evaluated\ndatasets while only marginally losing to few-shot approaches (which needed human intervention)"}, {"title": "2 APPROACH", "content": "In this work, we introduce TITAN, a novel approach to improve LLMs responses to task-oriented\nprompts. This is achieved by generating code through the use of a universal prompt template\ncoupled with zero-shot learning techniques. TITAN significantly differs from prior prompt-specific\nstrategies [6, 7, 13, 29, 53, 55, 59] since it provides a general and effective way to boost the perfor-\nmance of LLMs in performing various tasks without the need for individual prompt adjustments.\nTITAN enhances LLMs in two key ways: firstly, by utilizing the inherent abilities of LLMs to dissect\nand analyze complex queries; and secondly, by employing the code generation ability of LLMs to\ncreate executable scripts that produce the answers."}, {"title": "2.1 Challenges", "content": "As discussed in the Introduction, task-oriented prompts that involve numerical numbers (e.g., Ed\nhad 22 more marbles than Doug. Doug lost 8 of his marbles at the playground. How many more\nmarbles did Ed have than Doug then?) are not a direct fit for LLMs [15]. LLMs excel at analyzing and\nunderstanding complex queries by learning prior knowledge from a large corpus of text data [4, 44].\nHowever, for LLMs to correctly respond to a query, the answer must be composed of such prior\nknowledge [14]. In the case of task-oriented prompts, the answer often is the result of an execution\nof some procedure described by the prompt. Hence in most cases, the result does not directly exist\nin the prior knowledge.\nConversely, LLMs have shown proficiency in generating code from natural language descrip-\ntions [11, 26, 38, 40, 51], as their training datasets contain numerous coding instances that the LLMs\ncan utilize. By combining the analysis ability of LLMs with the task execution function of computer\nprograms, we can create a system that is capable of precisely performing complex task-oriented\nprompts.\nPrior approaches improve task-oriented prompts with code generation by relying heavily on\ntask-specific prompts, requiring extensive manual effort and domain expertise. Such approaches [7,\n13, 53, 55] employ uniquely crafted prompt templates tailored to individual tasks, along with\nfew-shot learning techniques, to create scripts that can produce the desired response. Thus, for a\nspecific task, the user would need to design a prompt template and provide some learning examples\nto get the most accurate responses from LLMs.\nFigure 1 shows an example of a step-by-step execution of TITAN. Specifically, given the input\nprompt \"Ed had 22 more marbles than Doug. Doug lost 8 of his marbles at the playground. How\nmany more marbles did Ed have than Doug then?\u201d, which asks TITAN to perform a subtraction task\nof \u201cMarbles Ed had\u201d and \u201cMarbles Doug had after he lost some of them at the playground\u201d. TITAN\nperforms step-back analysis to extract the specification of the inputs (i.e., the initial difference in\nmarbles and the amount that Doug had after the accident). At the same time, TITAN performs\nchain-of-thought (CoT) analysis to extract the procedure (i.e., Understanding the situation and the\nchanges after the accident) as well as the expected output (i.e., calculating the new difference). By\ncombining these two pieces of information, TITAN was able to generate the solution script that\ncorrectly outputs the expected result."}, {"title": "2.2 Script generation with step-back and zero-shot chain-of-thought prompting", "content": "To reduce the reliance on human effort in utilizing LLMs for task-oriented prompts, TITAN leverages\ntwo recently proposed prompt engineering techniques: step-back prompting [57] and zero-shot\nchain-of-thought prompting [22]. Step-back prompting helps TITAN analyze the query to identify\nthe relevant inputs and their requirements. Additionally, zero-shot chain-of-thought prompting\nanalyzes the query to extract the relevant steps and procedures to perform the task while requiring\nno additional input from the user. By integrating these two processes, TITAN could produce a\nscript that faithfully represents the original task, which in turn yields a precise response.\nExtracting inputs and specification phase: TITAN employs step-back prompting to identify the\ninput requirements from the initial prompt. This process involves querying the LLM to identify and\noutline the specific inputs needed for each task. By taking a step back, TITAN enables the model to\nfocus on the essential inputs required for successful code execution in the later stage.\nStep-back prompting consists of two stages: abstraction and reasoning. Instead of asking the\nquestion directly, the abstraction stage asks the LLM a step-back question about a related higher-\nlevel concept or principle [57]. In the reasoning stage, the LLM is asked to reason about the\nhigh-level concept or principle facts found after the step-back question. In this work, TITAN utilizes\nthe abstraction stage to extract inputs and their specifications from the original prompt. It then\nperforms the reasoning stage when generating code based on the extracted inputs.\nAs demonstrated in Figure 1, TITAN performs the abstraction stage by asking a step-back\nquestion \"... Take a step back and extract all the inputs mentioned in the client's query ...\u201d to extract\nthe inputs from the original prompt. Specifically, by directing the LLM to focus on a higher concept\n(i.e., inputs and their specifications) without directly addressing the original prompt, TITAN can\nextract accurate inputs that help the LLM in the later code generation phase. In this example, TITAN\nextracts the two inputs (i.e., \u201c22\u201d and \u201c8\u201d) and their specifications (i.e., \u201cinitial difference in marbles\"\nand \"Doug's loss\u201d). These are then integrated into the generated code later as initial_difference\n= 22 and doug_lost = 8.\nExtracting the goal and procedure steps phase: To improve the precision of the code generation\nstep, TITAN applies zero-shot chain-of-thought prompting to delve deeper into the goal of the task\nand the logical steps needed to achieve it while requiring no additional examples. TITAN prompts\nthe LLM to articulate a step-by-step reasoning process, effectively mapping out the pathway from\nproblem statement to solution. As shown in Figure 1, TITAN uses the prompt \u201c... should guide\nus step by step on how to solve this problem ...", "Calculate the New Difference": "Additionally,\nchain-of-thought prompting helps TITAN uncover logical thinking to solve the problem \u201cIt only\nchanges how many more marbles Ed has compared to Doug\u201d. This phase guides the LLM to better\nunderstand the final goal and process during the code generation phase. This is demonstrated in\nthe final generated code new_difference = initial_difference + doug_lost which matches\nthe extracted steps.\nCode generation phase: Combining the information extracted in the two previous phases, TITAN\nprompts the LLM to generate code based on clearly defined inputs, the articulated goal, and\nwell-reasoned steps. This code generation phase combines the inputs of step-back prompting and\nthe steps from chain-of-though prompting to synthesize a coherent and functional code output.\nSpecifically, TITAN employs the prompt \u201cGenerate a general Python function to solve the following\nquestion for general purpose: {question}\u201d to ask LLMs to generate a Python function for the question.\nFollowed up by the prompts \"This is an example to show you how to think about it and how to\nbreak it into smaller steps: \u201c{The Output from Goal and Steps Extraction}\u201d\u201d and \u201c For the inputs,\nuse", "Ed had 22 more marbles than Doug. Doug lost 8 of his marbles at the\nplayground. How many more marbles did Ed have than Doug then?\" This example demonstrates\nthe effectiveness of step-back and chain-of-thought prompting in helping LLM generate code with\nthe correct inputs initial_difference = 22 and doug_lost = 8, and precise representation of\nthe task new_difference = initial_difference + doug_lost.\nPost-processing and code execution: Since LLMs return free-form responses, TITAN employ\na rigorous extraction and validation process. Specifically, TITAN utilizes regular expressions to\nextract the generated code from the responses. The regular expressions target consistent formatting\"\n    },\n    {\n      \"title\": \"3 EXPERIMENTAL SETUP\",\n      \"content\": \"3.1\nTask-oriented datasets\nDrawing from the prior study [15], when faced with a more challenging question on prime numbers\nthat were sufficiently common to have representation on the Internet, the system performed\nadequately at that time. Hence, we create four task-oriented datasets from scratch to thoroughly\nevaluate TITAN performance. These include simple task-oriented prompts which we found to be\nparticularly difficult for LLMs to directly address. This new dataset will benefit future research,\ngiven the current trend in prompt construction towards decomposition techniques, such as Least-\nto-Most Prompting [60] and Decomposed Prompting [20]. Our dataset includes decomposition\ntasks such as finding, counting, true/false questions, and generative tasks.\nFinding Dataset: Given that LLMs have been shown to be strong in natural language tasks, it\nwas a surprise to see LLMs (including GPT-4), perform poorly on simple Finding tasks where the\nprompt asks the models to identify specific patterns and letters within some given text. This Finding\ndataset includes 1100 queries that ask for a pattern or a word as a response, we demonstrate prompt\ntemplates for generating the Finding dataset in Table 2. These types of templates evaluate LLMs\u2019\ncapacity to discern patterns amidst varying textual contexts.\nCounting Dataset: Counting tasks is not a natural fit for LLMs. As demonstrated in the Introduction,\nGPT models can make mistakes when performing counting tasks. In this dataset, we include 1100\nqueries that require numerical responses. We form the dataset with various prompt templates stated\"\n    },\n    {\n      \"title\": \"3.2 Mathematical and symbolic reasoning datasets\",\n      \"content\": \"Following prior work [13, 55], we evaluate TITAN on mathematical and symbolic reasoning datasets\nsuch as GSM8K, GSMHard [9], SVAMP [34], MAWPS [23], PENGUINS [13, 39], and ASDIv[33].\nGSM8K and GSMHard are expansive collections of high-quality, linguistically varied grade\nschool mathematics word problems, meticulously curated by adept problem composers. Each\nproblem within the dataset necessitates a solution process that spans between two to eight steps,\npredominantly involving a series of fundamental arithmetic operations (i.e., addition, subtraction,\nmultiplication, and division) to deduce the conclusive answer.\nSVAMP (i.e., Simple Variations on Arithmetic Math Word Problems) dataset presents itself as a\nchallenge set specifically designed for elementary-level Math Word Problems (MWP). An MWP is\ndefined by a concise narrative in Natural Language, delineating a scenario or state of the world,\nwhich culminates in posing a query regarding one or more unknown quantities.\nMAWPS is an online compendium dedicated to Math Word Problems (MWP) and serves as a\ncomprehensive dataset for the evaluation of diverse algorithms.\nPENGUINS delineates a task framework that integrates a tabular dataset of penguins, augmented\nwith supplementary descriptors in natural language. The primary objective within this framework\nis to deduce answers to queries concerning the attributes of the penguins based on the provided\ndataset and descriptions.\nASDIv corpus (i.e., Academia Sinica Diverse MWP Dataset) is another MWP dataset which\nconsists diverse language patterns and problem types, constituting an English MWP collection.\nThis corpus is designed for the assessment of the proficiency of various MWP solvers.\"\n    },\n    {\n      \"title\": \"3.3 Baselines\",\n      \"content\": \"PAL [13] is the state-of-the-art approach that focuses on task-oriented prompts. PAL employs a\ncode interpreter for problem reasoning with few-shot prompting while TITAN utilizes zero-shot\nlearning. To gain a deeper understanding of how our approach compares to the state-of-the-art we\nalso include PAL\u2019s zero-shot prompting technique (PAL ZS) as our second baseline. PAL ZS utilizes\nthe same prompt template as PAL but without the use of examples.\"\n    },\n    {\n      \"title\": \"3.4 Experiment Details\",\n      \"content\": \"We replicate PAL's result by executing PAL code from their GitHub using the same version of\nGPT-3.5 and GPT-4 as stated in the original paper. To run PAL on our task-oriented datasets, we\nform the PAL few-shot prompt with four examples by randomly selecting one example from each\ntask-oriented dataset, otherwise, we keep all PAL's templates the same.\nThe zero-shot version of PAL uses the same templates but without any examples. Since PAL didn't\nprovide a zero-shot prompt, we developed the zero-shot version of PAL inspired by another related\nstudy, PoT [7]. We adopt their prompt format by first posing the problem, followed by a request for\nthe LLMs to complete a Python function named \u201csolution()": "ithout adding any examples. The\nfunction name and the return type are the same as the PAL few-shot version. For a fair comparison,\nwe utilize the same metric used by PAL, which adopts exact match scores for evaluation. Unless\nspecified differently, all experiments by default employ greedy decoding, adjusting the temperature\nof the language models to 0.0."}, {"title": "4 RESULT AND DISCUSSION", "content": "In this section, we compare TITAN against state-of-the-art code generation with zero-shot (RQ1) or\nfew-shot (RQ2). In RQ3, we evaluate if self-consistency could help improve TITAN at a significant\ncost. Finally, we perform an ablation study (RQ4) to evaluate the contribution of each component\nin TITAN (i.e., input extraction and step extraction)."}, {"title": "4.1 RQ1: How does TITAN compare to the state-of-the-art zero-shot prompting\napproaches with code generation?", "content": "Similar to prior work, TITAN employs script generation to solve mathematical and task-oriented\nproblems. Hence, we first assess the effectiveness of TITAN when comparing state-of-the-art\napproaches with code generation in the zero-shot scenario. Existing code generation methods such\nas PAL, Model Selection (MS), and X-of-Thought (XoT), all utilize few-shot prompting. To evaluate\nhow TITAN is compared to the prior work in the zero-shot scenario, we create a baseline PAL\nZS (PAL zero-shot version) by incorporating a zero-shot prompts template [7] into PAL to be our\nzero-shot baseline."}, {"title": "4.2 RQ2: How does TITAN compare to the state-of-the-art few-shot prompting\napproaches with code generation?", "content": "In this RQ, we assess the broader effectiveness of TITAN when comparing state-of-the-art ap-\nproaches with code generation that also utilizes few-shot prompting such as PAL few-shot version,\nModel Selection (MS), and X-of-Thought (XoT). The details of each baseline are discussed in Sec 3.3.\nWe replicate PAL on the original and our task-oriented datasets, we employ the code provided in\nthe original study."}, {"title": "4.3 RQ3: Can self-consistency help improve TITAN?", "content": "Self-consistency [43] is an agnostic strategy that can be applied to CoT prompting approaches\nto improve the underlying approach performance. One downsize of self-consistency is that it\nrequires many duplicated queries to be sent to the LLM which can have diminishing returns [18].\nSince TITAN incorporates the CoT we hypothesize that incorporating self-consistency could\nfurther improve TITAN's performance. However, due to the elevated cost, we do not incorporate\nself-consistency in TITAN by default and instead use the temperature of 0.\nIn this RQ3, we want to test if self-consistency can help improve TITAN further and if the benefit\nwarrants the cost. Specifically, we apply self-consistency on TITAN for three datasets (GSM8K,\nMultiarith, and True/False) to see if self-consistency can help TITAN achieve state-of-the-art"}, {"title": "4.4 RQ4: How each component contribute to TITAN's overall performance?", "content": "Input and step extraction are two important phases that help TITAN generate precise scripts that\ncan be used to generate correct responses. In this RQ, we perform an ablation study with GPT-4 to\nsee how each component contributes to TITAN's overall performance."}, {"title": "5 RELATED WORK", "content": "5.1\nPrompt engineering with code generation\nThere has been prior work that utilizes code generation to address the gap in LLMs' execution\nability when it comes to task-oriented prompts. PAL[13] introduces an innovative method for\ntackling mathematical problems through script generation combined with few-shot prompting.\nFurther improvement is made by a Model Selection technique (MS) [55] which employs both PAL\nand CoT (Chain-of-Thought) in tandem by selecting the best response between them.\nX-of-Thoughts (XoT) [29] represents another code generation strategy, focusing on resolving\nmathematical and algebraic equations by dynamically switching among different prompting meth-\nods. Another distinct method, CSV [59], approaches the resolution of mathematical challenges\nthrough coding, which heavily relies on the GPT-4 Code Interpreter. Its efficacy is evaluated exclu-\nsively on the MATH dataset [17], indicating a specialized focus on coding solutions for mathematical\nissues.\nIn contrast, TITAN diverges significantly from these methods by adopting a zero-shot learning\ntechnique, which enhances its capacity for generalization across diverse problem sets. TITAN\naims to solve reasoning questions without relying on any hand-crafted data, few-shot learning\ntechniques, or human annotators."}, {"title": "5.2 Traditional prompt engineering without code generation", "content": "The exploration of concepts such as Chain-of-Thought [45] and Step-Back [57] has revealed the\ncapacity of LLMs for zero-shot learning [22] primarily through their ability to process information\nin a step-by-step manner. Prior work such as PHP [56], Self-Contrast [53], and Boosting-of-Thought\n(BoT) [6] has been developed to uncover the reasoning paths through post-hoc strategies of prompt\nengineering, aiming to enhance the models' problem-solving capabilities by mimicking human-\nlike [16] reasoning processes. These techniques use self-verification methods. For example, BoT\niteratively generates, assesses, and refines thoughts using the model's self-evaluation. Self-Contrast\nexploits diverse solutions to enrich reasoning, while PHP refines reasoning paths with LLM outputs\ntoward the correct answer.\nBecause the mechanisms and effectiveness of self-correction in LLMs are not well-understood [18],\nwe do not include such iterative mechanism in TITAN. Recent research indicates that LLMs are not\nyet capable of self-correction [18]. Distinct from such interactive methods which require human\nannotations or many more query rounds, TITAN does not require manual effort to incorporate\nevaluation information into the reasoning process."}, {"title": "5.3 Code generation with LLMs", "content": "Recently, research in using LLMs for code generation [5, 12, 36, 49] has made significant advance-\nments. These advancements often come from training these models with more specific examples\nof code, which makes them better at specific coding tasks [5]. Another line of research is guided\ncode generation [54, 58] where LLMs are utilized to efficiently create code. Specifically, Willard\net al. [46] introduce a system that guides LLMs to produce text using rules and structures from\nprogramming languages, reducing unnecessary steps in creating code sequences. Another recent\nwork, SynCode [41], is a framework that improves how LLMs understand and generate code by\nfocusing on the rules of programming languages. This approach helps create more accurate code\nby filtering out mistakes and focusing on the correct coding syntax. TITAN differs from these code\ngeneration research in the general nature of TITAN's input (i.e., prompt) where TITAN is designed\nto improve the overall question/answer capability of LLMs by utilizing script generation and not to\nsolve general software engineering problems related to code generation."}, {"title": "6 CONCLUSION", "content": "In this work, we introduced TITAN, a novel approach for natural language reasoning that leverages\nscript generation through the extraction of inputs and steps by utilizing Step-Back and Chain-\nof-Thought prompting respectively. We evaluate TITAN on 11 datasets with a comprehensive\ncomparison with prior state-of-the-art. Unlike preceding approaches that predominantly rely on few-\nshot prompting techniques, TITAN employs a zero-shot prompting strategy, thereby eliminating\nthe requirement for hand-crafted data. Our findings demonstrate that TITAN exhibits superior\nperformance in a diverse set of tasks. Furthermore, the integration of TITAN with self-consistency\nfurther enhances its efficacy (with some additional cost), thereby underscoring the potential of\nTITAN as a robust solution for advanced natural language reasoning challenges."}, {"title": "7 DATA AVAILABILITY", "content": "We release our code and data through the following link: https://anonymous.4open.science/r/TITAN-\nTask-oriented-Prompt-Improvement-with-Script-Generation-3BE4."}]}