{"title": "TRANSFORMER\u00b2: SELF-ADAPTIVE LLMS", "authors": ["Qi Sun", "Edoardo Cetin", "Yujin Tang"], "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer2, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer\u00b2 employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \"expert\u201d vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Transformer\u00b2 demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer\u00b2 represents a significant leap forward, offering a scalable, ef-ficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems. Our code is available at https://github.com/SakanaAI/self-adaptive-llms", "sections": [{"title": "INTRODUCTION", "content": "Self-adaptive large language models (LLMs) would represent a significant advancement in artificial intelligence, providing a framework where mod-els can adjust to varied tasks and dy-namic contexts in real time. While compositionality and scalability are crucial for effective adaptation, cur-rent LLM training methodologies fall short of achieving both these prop-erties simultaneously. Our research aims to present a pioneering solu-tion to realize this vision and address these gaps.\nTraditionally, LLM post-training has sought to optimize a model for a wide range of capabilities in a single, ex-tensive training session. While this \"one-shot\" fine-tuning framework is ideal from a simplicity perspective, it is also difficult to achieve in practice.\nFor instance, post-training is still highly resource-intensive, leading to significant computational costs and training times. Additionally, there tends to be notable performance trade-offs when in-troducing additional breadth to the data, making it challenging to overcome overfitting and task interference at the same time.\nIn contrast, self-adaptive models offer a more flexible and efficient approach. Rather than attempting to train an LLM for all tasks in one step, expert modules can be developed offline and augmented"}, {"title": "RELATED WORKS", "content": "Self-adaptive LLMs We define self-adaptive LLMs as a group of LLMs or a standalone LLM that can evaluate and modify its behavior in response to changes in its operating environment or internal state, without external intervention. This adaptation can be explored from two perspectives: a macroview, where multiple LLMs collaborate and/or compete, and a microview, where internal adaptations allow a single LLM to specialize in different tasks.\nMacroview: From this perspective, the system directs queries to LLMs with domain specific exper-tise, prioritizing outputs from expert models, thereby achieving higher accuracy and task-specific optimization. Such task-specific ensembles can be realized through various mechanisms: multiple LLMs playing distinct roles and coordinate toward a shared goal (Zhuge et al., 2023), engaging in mutual listening and debate (Du et al., 2023), or using meticulously crafted prompt construc-tions (Zhang et al., 2024) to integrate knowledge library and skill planning. Naturally, the improve-ment in the specialization and adaptive capabilities of individual LLMs in the ensemble enhances the collective performance. Thus, in this paper, we focus on the microview of self-adaptive LLMs.\nMicroview: MoE in LLMs plays a critical role in this perspective (Tianlong et al., 2024). In MoE systems, inputs are dynamically routed to a subset of specialized modules or layers (e.g., MLPs) containing domain-specific knowledge (Rajbhandari et al., 2022; Fedus et al., 2022). To reduce inference time, researchers introduce sparsely activated MoE where only a subset of the experts are selected per token Jiang et al. (2024); Qwen Team (2024). While it is possible to view Transformer2 loosely as a type of MoE, there are two major differences. In the aforementioned systems, self-adaptation is achieved through token-level routing, whereas Transformer\u00b2 employs a sample-level module selection strategy. The second difference lies in the construction of expert modules. In traditional MoE systems, expert modules are either trained from scratch (Fedus et al., 2022; Jiang et al., 2024) or dense models (e.g., upcycling) (Qwen Team, 2024; Zhu et al., 2024), without an auxiliary loss to ensure module specialization. In contrast, Transformer\u00b2 specifically trains expert vectors with RL to acquire domain specific-knowledge, making them true experts.\nLow-rank adaptation PEFT methods such as LoRA (Hu et al., 2021) works by freezing the original model's parameters and introducing small trainable low-rank matrices for task-specific updates. It significantly lowers the computational and memory costs while providing performance comparable to full fine-tuning. Inspired by LoRA's design, various modifications have been proposed (Zhang et al., 2023; Kopiczko et al., 2023; Liu et al., 2024; Ba\u0142azy et al., 2024; Cetoli, 2024). Transformer2 does not rely on low-rank matrices, and instead scales the singular vectors of the original parameter matrix that span the full rank space.\nSVD for LLM Fine-tuning SVD is increasingly being used as an inductive bias for PEFT in LLMs. For example, Wang et al. (2024) decompose a weight matrix and use the minor singular components, associated with noisy or long-tail information, to initialize low-rank matrices for LoRA fine-tuning. In a similar vein, SVD is employed to approximate an original weight matrix with the top r singular vectors, corresponding to the highest singular values. A small trainable matrix is then introduced on top of the truncated singular value matrix to adjust the magnitude and orientations within this top-r subspace (Ba\u0142azy et al., 2024; Cetoli, 2024). However, the drawback of this approach is that retaining only the top singular components can result in the loss of important information, particularly when the singular values distribution is less skewed. The work most similar to ours is a concurrent effort by Lingam et al. (2024), where they introduce various sparsification methods that utilize the SVD of the weights. However, it is not for self-adaptive LLMs and does not use RL to enhance learning efficiency."}, {"title": "METHODS", "content": ""}, {"title": "PRELIMINARIES", "content": "Singular value decomposition (SVD) offers a fundamental view of matrix multiplications. In the context of neural networks, each weight matrix $W \\in \\mathbb{R}^{n \\times m}$ can be decomposed into three compo-nents $W = U \\Sigma V^T$, yielding semi-orthogonal matrices $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ together with an ordered vector of r singular values (in descending order) arranged in the diagonal matrix $\\Sigma \\in \\mathbb{R}^{r \\times r}$. The linear operation defined by applying W onto x, can be then decomposed into a sum of indepen-"}, {"title": "TRANSFORMER\u00b2", "content": "The construction of Transformer\u00b2 comprises two main steps, for which we provide an illustrative overview in Figure 2. First, we introduce Singular Value Fine-tuning (SVF), a method to learn with RL compact and compositional expert vectors based on the SVD of the base model's weights. Then, we describe three different adaptation strategies within Transformer\u00b2, inspired by three or-thogonal principles, which adaptively combine the SVF-trained expert vectors during inference. We motivate how the properties of SVF are highly complementary to our adaptation strategies, making Transformer\u00b2 an effective and scalable framework for the design of new self-adaptive LLMs.\nSingular value fine-tuning is a key building block in Transformer\u00b2. It offers an extremely efficient parameterization for fine-tuning and provides inherent compositionality for adaptation. Conven-tional fine-tuning techniques often aim to augment pre-trained models with new capabilities by mod-ifying their weight matrices. However, in large-scale transformers, these weights are already rich repositories of abstracted knowledge, thanks to the breadth of the pre-training data and expansive architectural design. In fact, as evidenced in much of the prior literature, the requisite capabilities for solving many downstream tasks appear to already exist within these pre-trained models (Sharma et al., 2023). Therefore, instead of seeking to add new features, an efficient fine-tuning approach should focus on making these latent capabilities more expressible. Motivated by these considera-"}, {"title": "EXPERIMENTS", "content": "We extensively evaluate Transformer\u00b2 on multiple tasks and models with the purpose of: (1) as-sessing the efficiency and effectiveness of SVF; (2) demonstrating self-adaptiveness through the three proposed adaptation strategies; (3) conducting in-depth analysis and ablation studies aimed at understanding and interpreting the properties of our new framework."}, {"title": "EXPERIMENTAL SETUPS", "content": "To validate the generality of Transformer\u00b2 we consider three pre-trained LLMs ranging across dif-ferent model families and architecture sizes: LLAMA3-8B-INSTRUCT, MISTRAL-7B-INSTRUCT-V0.3, and LLAMA3-70B-INSTRUCT. For each model, we obtain three sets of SVF-trained z vec-"}, {"title": "EXPERIMENTAL RESULTS", "content": "SVF performance We provide results after training on each considered task with the LLAMA3-8B-INSTRUCT, MISTRAL-7B-INSTRUCT-V0.3, and LLAMA3-70B-INSTRUCT base models in Ta-ble 1. Remarkably, we find that SVF provides considerable and consistent performance gains across nearly all tasks and base models. Instead, LoRA experts yield smaller gains and even sporadic per-formance degradation. (These LoRA experts are trained with next token prediction. While we also have LoRA experts trained with RL in Table 4, RL seems work less well with LoRA than with SVF.) This observed trend extends also to the vision-language domain, as fine-tuning LLAMA3-LLAVA-NEXT-8B with SVF bolsters the base model's performance by over 39% (see Figure 5). To ensure a fair comparison, we provide extensive ablations to both our model and the LoRA baseline considering different architecture and optimization objectives in Appendix 4.3). Due to its essential parameterization, we would like to note that training SVF requires considerably fewer resources, with less than 10% of the training parameters of our LoRA implementation.\nAdaptation performance With the SVF trained z vectors, we assess the self-adaptation capability of Transformer2 on unseen tasks. For a fair comparison with LoRA, we record the performance of this baseline using all checkpoints from the considered training tasks and report only its high-est performance for each of the test tasks. As shown in Table 2, all of our Transformer\u00b2 adapta-tion strategies demonstrate improvements across all tasks for LLAMA3-8B-INSTRUCT base models, and in at least two out of three tasks for both MISTRAL-7B-INSTRUCT-V0.3 and LLAMA3-70B-INSTRUCT. In contrast, even the best training LoRAs only provide marginal improvements on the"}, {"title": "ANALYSIS", "content": "Lastly, we analyze and discuss the properties of our adaptation strategies for which we provide extensions and further discussion Appendix B."}, {"title": "CONCLUSION", "content": "In this paper, we introduced Transformer\u00b2, providing a novel blueprint toward realizing self-adaptive LLMs. Within this framework, we first proposed SVF, offering superior performance than prior fine-tuning recipes, together with reduced costs, high compositionality, and overfitting regularization\u2014all crucial properties to achieve scalable self-adaptation. Leveraging a set of SVF experts as building blocks, we developed three effective strategies for self-adaptation, each offering unique benefits and monotonic performance benefits with increasing access to the test-time conditions.\nWhile Transformer\u00b2 demonstrates promising results, there remain exciting opportunities for future work. One limitation is that the capabilities of SVF experts are tied to the latent components of the base model. To address this, model merging offers a promising direction (Yu et al., 2024; Goddard et al., 2024; Akiba et al., 2024), enabling specialized models to be combined into a single, more capable model. Additionally, while our CEM-based adaptation effectively balances performance and efficiency, scaling to a large number of specialized domains may introduce increased one-time computational costs. However, this trade-off is offset by the benefits of improved performance and enhanced self-adaptation capabilities. Advances in model merging and efficient adaptation tech-niques have produced models dominating open leaderboards, making them strong candidates as base models for Transformer\u00b2 and opening new possibilities for adaptive LLMs."}, {"title": "AUTHOR CONTRIBUTIONS", "content": "Yujin Tang initiated the project. Qi Sun proposed the prompted-based method, developed the evalu-ation framework, conducted the experiments, and provided contributions to writing. Edoardo Cetin designed the few-shot CEM adaptation strategy, performed the experiment, and made major con-tributions to manuscript writing. Yujin Tang proposed the core algorithm, conducted initial experi-ments, made major contributions to the manuscript, and managed the project."}]}