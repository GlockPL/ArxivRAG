{"title": "A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models", "authors": ["Geonhee Kim", "Marco Valentino", "Andr\u00e9 Freitas"], "abstract": "Recent studies on logical reasoning in auto-regressive Language Models (LMs) have sparked a debate on whether such models can learn systematic reasoning principles during pre-training or merely exploit superficial patterns in the training data. This paper presents a mechanistic interpretation of syllogistic reasoning in LMs to further enhance our understanding of internal dynamics. Specifically, we present a methodology for circuit discovery aimed at disentangling content-independent reasoning mechanisms from world knowledge acquired during pre-training. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic reasoning, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes and model sizes, finding that the identified circuit is sufficient and necessary for all the schemes on which the model achieves high downstream accuracy (\u2265 60%). Overall, our findings suggest that LMs indeed learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalisable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.", "sections": [{"title": "Introduction", "content": "Transformer-based Language Models (LMs) have led to remarkable results across various natural language processing tasks (Radford et al. 2018, 2019; Brown et al. 2020; OpenAI et al. 2024; Jason et al. 2022; Bubeck et al. 2023). This success has catalyzed research interest in systematically exploring the reasoning capabilities emerging during pre-training (Clark, Tafjord, and Richardson 2020). Recent findings suggest that logical reasoning abilities may emerge in large-scale models (Rae et al. 2022; Kojima et al. 2022; Wei et al. 2022) or through transfer learning on specialized datasets (Betz, Voigt, and Richardson 2021). However, ongoing debates question whether these models apply systematic inference rules or reuse superficial patterns learned during pre-training (Talmor et al. 2020; Kassner, Krojer, and Sch\u00fctze 2020; Wu et al. 2024). This controversy underscores the need for a deeper understanding of the low-level logical inference mechanisms in LMs (Rozanova, Valentino, and Freitas 2024; Rozanova et al. 2023, 2022).\nTo improve our understanding of the internal mechanisms, this paper focuses on Mechanistic Interpretability (Olah et al. 2020; Nanda et al. 2023), aiming to discover the core circuit responsible for content-independent syllogistic reasoning. In particular, we focus on categorical syllogisms with universal affirmative quantifiers (i.e., AAA-1, Barbara) motivated by two key factors. First, as observed in natural logic studies (MacCartney and Manning 2007), this form of syllogistic reasoning is prevalent in everyday language. Therefore, it is likely that LMs are exposed to such reasoning schema during pre-training. Second, AAA-1 is a form of unconditionally valid syllogism independent of the premises' truth condition (Holyoak and Morrison 2005). This characteristic offers a deterministic and scalable task design, other than allowing us to investigate the disentanglement between reasoning and knowledge representation.\nThrough the adoption of Activation Patching (Meng et al. 2022) and embedding space analysis (i.e., Logit Lens) (Nostalgebraist 2020; Geva et al. 2022; Dar et al. 2023), we investigate the following main research questions: RQ1: What is the content-independent syllogistic reasoning mechanism internalized in LMs during pre-training?; RQ2: Are content-independent mechanisms disentangled from specific world knowledge and belief biases?; RQ3: Does the core reasoning mechanism generalize across model sizes and different syllogistic schemes?\nTo answer these questions, we present a methodology that consists of 3 main stages. First, we define a syllogistic completion task designed to assess the model's ability to predict valid conclusions from premises and facilitate the construction of test sets for circuit analysis. Second, we implement a circuit discovery pipeline on the syllogistic schema instantiated only with symbolic variables (Table 1, Symbolic) to identify the core sub-components responsible for content-independent reasoning. We conduct this analysis under two intervention methods: middle-term corruption and all-term corruption, aiming to identify latent transitive reasoning mechanisms and term-related information flow. Third, we investigate the generalization of the identified circuit on concrete schemes instantiated with commonsense knowledge to identify potential belief biases and explore how the internal behavior varies with different schemes and model sizes.\nWe present the following overall conclusions:\n1. The circuit analysis reveals that auto-regressive LMs develop specific inference mechanisms during pre-training, finding evidence supporting a three-stage mechanism for syllogistic reasoning: (1) naive recitation of the first premise; (2) suppression of duplicated middle-term information; and (3) mediation towards the correct output through the interplay of attention heads.\n2. Further experiments on circuit transferability demonstrate that the identified mechanism is still necessary for reasoning on syllogistic schemes instantiated with commonsense knowledge. However, a deeper analysis suggests that specific belief biases acquired during pre-training might play an important role, contaminating the content-independent circuit mechanism with additional attention heads responsible for encoding general and contextualized world knowledge.\n3. We found that the identified circuit is sufficient and necessary for all the syllogistic schemes in which the model achieves high downstream accuracy (\u2265 60%). This result suggests that LMs learn content-independent reasoning"}, {"title": "Methodology", "content": "Our main research objective is to discover and interpret the core mechanisms adopted by auto-regressive Language Models (LMs) when performing content-independent syllogistic reasoning. To this end, we present a methodology that consists of 3 main stages. First, we define a syllogistic completion task that can be instrumental for our analysis. Second, we leverage the syllogistic completion task to implement a circuit discovery pipeline on a syllogistic schema instantiated only with symbolic variables (Table 1, Symbolic). Third, we investigate the generalization of the identified circuit on concrete schemes instantiated with commonsense knowledge and explore how the internal behavior varies with different schemes and model sizes."}, {"title": "Syllogism Completion Task", "content": "We design a syllogism completion task for assessing the reasoning abilities of LMs, building upon established approaches in the literature (Betz, Voigt, and Richardson 2021; Wu et al. 2023). In particular, we focus on categorical syllogisms with universal affirmative quantifiers (i.e., AAA-1, Barbara) because of their frequency in natural language and their content-independent reasoning property."}, {"title": "Circuit Discovery", "content": "Our main objective is to find a circuit $C$ for syllogistic reasoning in a Language Model $M$. A circuit $C$ can be defined as a subset of the original model $M$ that is both sufficient and necessary for achieving the original model performance on the syllogistic completion task. In order to identify a circuit, we employ Activation Patching together with circuit ablation methods (Meng et al. 2022; Vig et al. 2020).\nActivation Patching. This technique involves modifying the activation of targeted components and observing the resulting changes. Our study primarily examines the activation of residual streams and multi-head self-attention at the sequence level to trace the term information flow. Given a masked syllogistic input $x = [P_1;P_2;C \\setminus \\{p\\}]$ as $(s, m_1, m_2, p)$, which can be read as \u201cAll $s$ are $m_1$. All $m_2$ are $p$. Therefore, all $s$ are [mask]\", and its correct completion $y = p$, the Activation Patching technique consists of the following steps:\n(1) Clean Run: For the target input $(s, m_1, m_2,p)$, we record the baseline logit difference, $\\delta^+(p, m)$ from the forward pass output of the model (Figure 2(1)).\n(2) Corrupted Run: We re-run the model on a corrupted input after applying a specific intervention (e.g., changing the middle term $m_2$) and record the logit difference $\\delta^-(p, m)$ (Figure 2(3)).\n(3) Intervened Run: We run the model with the corrupted inputs again and replace the corrupted activations with those from the clean runs to compute the response from the remaining components and measure the causal impact of the intervention. Here, we measure the adjusted logit difference $\\delta_p(p, m)$ (Figure 2(2)).\n(4) Quantification: We quantify the causal impact of each intervention using a patching score $S$ following (Heimersheim and Nanda 2024) as shown in Figure 2(4). This score is further normalized to $[-1,1]$.\nWe complement Activation Patching with known methods for analysing hidden activations in transformers, including Logit Lens (Nostalgebraist 2020) with an input-agnostic approach (Dar et al. 2023; Hanna, Liu, and Variengien 2024). Additional technical details can be found in Appendix C.\nInterventions. The choice of tokens to corrupt is a critical aspect of the experimental design, and it is essential to establish an appropriate type of intervention that aligns with the hypothesis being tested. In this study, we employ two distinct interventions to isolate the mechanisms related to the reasoning schema from the propagation of specific token information:\n(1) Middle-Term Corruption: To investigate the reasoning mechanism employed in the syllogism completion task, we primarily focus on the transitive property of the middle term, $(m_1 \\rightarrow m_2) \\land (m_1 = m_2)$. We hypothesize that disrupting the transitive property will localize the component responsible for syllogistic reasoning. Therefore, our intervention method replaces the second middle term $m_2$ with an unseen symbol $m'_2$, breaking the equality and effectively corrupting the validity of the reasoning, pivoting the correct answer towards $m$: $(s,m_1, m_2,p) \\rightarrow (s, m_1, m'_2, p)$.\n(2) All-Term Corruption: We examine how term-related information flows to the last position for the final prediction to identify potential mover heads. To this end, we replace the original terms $(s, m_1, m_2,p)$ with different terms $(s', m'_1, m'_2, p')$ while keeping the completion answers unchanged. We hypothesize that if the mover heads carry the $p$ related information over $m$ related information for a valid prediction, the logit difference will increase; otherwise, it will decrease.\""}, {"title": "Circuit Ablation", "content": "Once we discover a relevant circuit $C$, we evaluate its necessity and sufficiency using mean ablation. Specifically, we define a circuit $C$ as necessary if ablating the identified heads $H$ in $C$ and preserving the remaining components in $M$ decreases the original performance. Conversely, we define a circuit $C$ as sufficient if preserving only the identified heads $H$ in $C$ and ablating all the remaining heads in the original model $M$ is sufficient to obtain the performance of $M$. To perform mean ablation, we gradually average the target attention heads from downstream to upstream layers and measure the average logit difference $\\delta(p, m)$ to assess the impact of the ablation on $M$. Additional details are included in Appendix D."}, {"title": "Empirical Evaluation", "content": "Model Selection. We select the LM for the circuit analysis based on a trade-off between model size and performance. To this end, we measure the accuracy of GPT-2 (Radford et al. 2019) with different sizes (117M, 345M, 762M, and 1.5B) on the syllogism completion task. Specifically, we aim to identify potential phase transition points where significant performance changes emerge (Jason et al. 2022; Kaplan et al. 2020). As shown in Figure 3, we observe a notable phase transition from small to medium sizes, where average performance increases by 139.97% in the symbolic setting and 119.21% in the non-symbolic setting. Given these results, we select GPT-2 Medium for circuit discovery and subsequently test the compatibility with different model sizes (See Appendix E for the model architecture). For all experiments, we use 90 samples each for symbolic and non-symbolic arguments to balance the depth of analysis with computational efficiency."}, {"title": "Localization of Transitive Reasoning Mechanisms", "content": "Middle-term corruption reveals information flow relevant to the transitive property. Figure 4(a-b) presents the results of the middle-term intervention targeting attention heads and residual stream. Figure 4(a), in particular, reveals the positive role of heads $h_{11.10}$ and $h_{19.1}$ (where $h_{l.k}$ denotes the $k$th head in layer $l$). Moreover, we observe an information propagation pattern from the $[m_2]$ position to the $[p]$ position (Figure 4(b)). These observations allow us to hypothesize that information from $[m_2]$ is conveyed to $[p]$ on the residual stream by attention head $h_{11.10}$, which exhibits a strong patching score around the layer responsible for the propagation. To verify this hypothesis, we further investigate the attention weights between $[p]$ and $[m_2]$. As expected, $[m_2]$ is the position most highly attended by $[p]$, with an average attention weight of 0.15$\\pm$0.07 (Figure 4(e)). These results suggest that information is indeed moved from $[m_2]$ to $[p]$ on the residual stream subspace, with $h_{11.10}$ playing a crucial role in this mechanism.\nDuplicate middle-term information is aggregated via induction heads. We further investigate how information from $[m_2]$ is moved to $[p]$, positing that this relates to the model's internal reasoning mechanism. We notice that at position $[p]$, the model can observe the complete AAA-1 syllogistic structure (Figure 2(a)) with middle-term duplication. We hypothesize that this structural information for reasoning is collected at one position, given that information refinement occurs at the specific position such as last token (Hanna, Liu, and Variengien 2024; Stolfo, Belinkov, and Sachan 2023). To verify this, we trace the information flow of head $h_{11.10}$ using path patching (Wang et al. 2023). Our results show that $h_{11.10}$ operates based on several induction heads (Elhage et al. 2021) ($h_{5.8}$, $h_{6.1}$, $h_{6.15}$ and $h_{7.2}$) formed at $[m_2]$. These heads attend to the [[m1] + 1] token due to the $m_1 = m_2$ conditioned matching operation, likely containing $m_1$-related information from previous token heads. We conclude, therefore, that $m_1$ and $m_2$ information are aggregated at position $[p]$. Additional details of the path patching results are available in the Appendix F.\nSuppressive mechanism is revealed through Logit Lens. To better understand the internal mechanism occurring at the $[p]$ position, we investigate attention head $h_{11.10}$. Having previously examined the attention pattern (composed of attention weights), we now focus on the attention value and output by analyzing the OV circuit ($W_vW_o$) (Elhage et al. 2021) using the logit lens method (Nostalgebraist 2020). Interestingly, the result in Figure 4(c) shows a clear negative diagonal pattern suggesting that $h_{11.10}$ strongly suppresses the logit when attending to the same token as the corresponding output. Given our previous findings that $h_{11.10}$ reads information from the subspace of token $[m_2]$, we conclude that it applies a suppressive mechanism to $m$-related information and writes it back to the residual stream's subspace at $[p]$. If later heads carry this information to the last token, this mechanism becomes crucial for the model to arrive at the correct answer. For simplicity, we name head $h_{11.10}$ as $m$-suppression head.\nLocalization of Term-Related Information Flow\nKey information is moved from term-specific positions to the last position. Now, we use the all-term intervention method to localize mover heads that carry term-related information. In the residual stream patching results (Figure 4(d)), we observe the highest positive score at the $[p]$ position, while negative scores are most prominent at the $[m_1]$ and $[m_2]$ positions, indicating that the information residing in these positions indeed contributes to their corresponding token prediction. This observation aligns with the importance of token embedding information at their respective positions given iterative refinement on the residual stream (Simoulin and Crabb\u00e9 2021). A closer examination of the last token position reveals that the negative effect propagates from relatively early layers (approximately layer 10 onwards), yet positive effects are from later layers (approximately layer 14 onwards), incurring a positive shift of the model's prediction from $m$ to $p$.\nInformation is carried by later positive and negative mover heads. Given the findings that the information for prediction resides in term-specific positions, we trace which attention heads transfer the information from each term position ([p], [m1] and [m2]) to the last token. We call these heads mover heads as the existence of \"positive or negative mover heads\" that carry or suppress information from the specific token position to the last token position (Wang et al. 2023; Garc\u00eda-Carrasco, Mat\u00e9, and Carlos Trujillo 2024). To discover the source of information flow, we investigate the heads with high attention value weight ($W_v$) patching score and identify 9 notable mover heads (see Appendix G for details of mover heads localization)."}, {"title": "Summary", "content": "Overall, we found that the internal mechanisms to arrive at the correct conclusion involve the following phases:\n1. Long Induction: Early layers exhibit biases towards the wrong conclusion due to long-range repetition of the first premise \"All A are B\".\n2. Duplication: Induction heads aggregate information about duplicated middle terms in the premises.\n3. Suppression: The model aggregates and inhibits middle-term information (i.e., \u201cB\u201d) suppressing the long induction mechanism.\n4. Mover: Token-specific information is propagated to the last token position. The process culminates in the prediction shift from B' to the correct token 'C'.\nTherefore, the discovered circuit is mainly characterized by an internal error correction mechanism. Interestingly, this mechanism is different from the way humans would reason on syllogistic arguments through the use of abstract logical primitives and inference rules."}, {"title": "Circuit Evaluation", "content": "To evaluate the comprehensive correctness of the circuit, we assess necessity and sufficiency via the ablation method described in section 2. The ablation study in Figure 5(a) shows that the identified circuit is both necessary and sufficient, demonstrating a consistent performance degradation when removing circuit components and revealing a complete restoration of the original model's performance when considering only the circuit's subcomponents. We further verify the robustness of the symbolic circuit to superficial and semantic-preserving perturbations. In particular, we modify the letters into numbers (Figure 5(b)) and adopt semantically equivalent quantifiers and related verbs (e.g., \"All ... are\" is converted into \"Each ... is\"). The ablation result demonstrates that both types of perturbations do not undermine the sufficiency and necessity property of the circuit."}, {"title": "Circuit Transferability", "content": "The circuit is necessary for non-symbolic syllogistic arguments, yet not sufficient for belief-inconsistent ones. We present ablation results for the two generated non-symbolic datasets: belief-consistent (Figure 6(a)) and belief-inconsistent (Figure 6(b)) (See Appendix H for the intervention results). Notably, the symbolic circuit proves necessary for both types of non-symbolic inputs, suggesting that the logic derived from symbolic syllogisms remains essential even when natural words are substituted. Regarding sufficiency, while belief-consistent data show significant performance recovery, we observe an inability to restore original performance on the belief-inconsistent set despite an increasing trend. These results indicate that belief biases encoded in different attention heads may play an important role."}, {"title": "Belief biases corrupt reasoning mechanisms.", "content": "To further investigate the impact of belief biases, we again conduct intervention experiments. Specifically, we observe that in the AAA-1 syllogism schema (s, m1, m2, p), the subject token s should be irrelevant for deriving the correct answer p over m. Therefore, we leverage this property to verify whether word-specific biases are introduced when instantiating the schema with concrete knowledge. To this end, we perform Activation Patching by corrupting the subject term, transforming $(s,m_1, m_2,p)$ to $(s', m_1, m_2,p)$, and measuring the degradation in logit difference as $\\delta_+(p, m) - \\delta_-(p, m)$. Notably, the non-symbolic setting exhibits a significant performance degradation of 0.66 \u00b1 1.27, representing a 299.96% change from the baseline. In contrast, the symbolic setting shows a minimal degradation of 0.00 \u00b1 0.64, a mere 0.35% drop. This drastic difference supports our hypothesis that the knowledge acquired during pre-training corrupts the content-independent reasoning circuit identified on the symbolic set with additional attention heads."}, {"title": "Generalization to Different Syllogistic Schemes", "content": "We further aim to understand whether the symbolic circuit is specific to the AAA-1 syllogism (Barbara). To this end, we extend our experiment to encompass all 15 unconditionally valid syllogisms (see Appendix A and I for details about the syllogistic schemes and more experimental results). To evaluate the transferability to all 15 schemes, we verify three main conditions, reported in Table 2: (C1) Necessity, (2) Sufficiency, and (C3) Positive Logit Difference. We also measure the accuracy of the completion task for each syllogism. From Table 2, we observe that the circuit is sufficient and necessary for all the syllogistic schemes in which the model achieves high downstream accuracy (\u2265 60%), supporting the conclusion that the circuit includes components that are crucial for the emergence of syllogistic reasoning in general. Notably, all three conditions are satisfied for the affirmative syllogisms (AII-3, IAI-3, IAI-4), for which the model achieves an accuracy above 0.6."}, {"title": "Generalization to Different Model Sizes", "content": "Finally, we expand our analysis to different sizes of GPT-2 (small, large and XL), assuming core mechanisms would transfer across models trained on the same dataset. Here, the intervention results show similar suppression mechanism patterns and information flow across all models. However, the residual stream of GPT-2 small in the all-term corruption setup is reversed due to its low downstream accuracy (see Appendix J). Moreover, as the model size increases, we found evidence that the contribution of attention heads becomes more complex. We hypothesize this might be caused by a stronger impact of world knowledge with increasing size, as suggested by the decrease in accuracy on the symbolic dataset and the increase on the non-symbolic set observed for the XL model (See Figure 3)."}, {"title": "Related Work", "content": "Mechanistic circuit analysis has emerged as a promising approach to interpreting the internal mechanisms of Transformers (Olah et al. 2020; Nanda et al. 2023; Olsson et al. 2022; Wang et al. 2023; Garc\u00eda-Carrasco, Mat\u00e9, and Carlos Trujillo 2024; Hanna, Liu, and Variengien 2024). Existing approaches investigating internal reasoning mechanisms mainly focus on math-related tasks, elucidating the information flow for answering mathematical questions (Stolfo, Belinkov, and Sachan 2023) and examining arithmetic operations (Quirke and Barez 2024). Most pertinent to our work, Wiegreffe et al. (2024) provides a mechanistic interpretation of multiple-choice question answering, investigating attention head-level patterns. In general, our mechanistic analysis complements recent work investigating the challenges in processing reasoning arguments that contradict established beliefs and whether the reasoning in LMs stems from internalized rules or memorized content (Ando et al. 2023; Yu, Merullo, and Pavlick 2023; Wu et al. 2024; Talmor et al. 2020; Wu et al. 2024; Kassner, Krojer, and Sch\u00fctze 2020; Haviv et al. 2023; Feldman 2020; Monea et al. 2024; Yu, Merullo, and Pavlick 2023; Wallat, Singh, and Anand 2020; Eisape et al. 2024). To address this question, different mechanistic approaches have been adopted to localizing factual associations (Meng et al. 2022; Geva et al. 2023; Dai et al. 2022) or assessing conditions for generalisation (Wang et al. 2024). However, to the best of our knowledge, we are the first to investigate content-independent mechanisms for syllogistic reasoning."}, {"title": "Conclusion", "content": "In this study, we have provided a comprehensive mechanistic interpretation of syllogistic reasoning within auto-regressive language models. Overall, our findings suggest that LMs indeed learn transferable content-independent reasoning mechanisms but that, at the same time, such mechanisms might be contaminated and suppressed by the same specific world knowledge acquired during pre-training.\nIt is important to acknowledge some of the limitations in our study: (1) our analysis is conducted predominantly on the transitive property and term-specific information and does not consider the full spectrum of the reasoning dynamics that might appear in more complex scenarios. (2) Our experimentation is confined to a specific reasoning template that lacks the complexity and noise of real-world problems due to the intricate level of granularity and the computational complexity required in circuit analysis. Despite these limitations, we believe our findings could offer valuable insights into the reasoning mechanisms adopted by auto-regressive language models and lay a solid foundation for future research in this field."}, {"title": "Appendix", "content": "This document serves as a technical appendix to the paper titled A Mechanistic Interpretation of Syllogistic Reasoning in Auto-Regressive Language Models. It provides additional theoretical background, experimental details, and supplementary results that support the main findings presented in the original paper."}, {"title": "A. Unconditionally Valid Syllogism Schemes", "content": "We list all 15 unconditionally valid syllogisms (Table 3)."}, {"title": "B. Dataset Generation", "content": "B.1. Symbolic\nThe symbolic dataset comprises sentences where all terms in the premises are represented by abstract symbols (uppercase alphabet letters). From the set of all 26 uppercase alphabets, three-letter triples (e.g., A, B and C) are randomly sampled. For each triple, six permutated prompts and label pairs are generated following a template designed to minimise latent semantic interference among alphabet symbol tokens.\nB.2. Non-symbolic\nThe non-symbolic datasets are constructed based on GenericsKB (Bhakthavatsalam, Anastasiades, and Clark 2020), a resource that provides a foundation for evaluation of sentence veracity with associated truthfulness scores (0-1). The dataset construction process involves the following steps:\n\u2022 Extraction: Select generic sentences with a truthfulness score of 1 based on GenericsKB (Bhakthavatsalam, Anastasiades, and Clark 2020), specifically those in the form A are B, using regular expression.\n\u2022 Syllogism Construction: We form universal affirmative syllogism arguments (Barbara) based on a template by chaining sentences where the predicate of one sentence logically matches the subject of another.\n\u2022 Constraints: We exclude terms tokenised into multiple tokens to maintain consistency in comparison with the symbolic dataset, which is essential for coherent circuit analysis.\n\u2022 Classification: To address issues of partial inclusion and syntactic ambiguity in constructed syllogism arguments, we employ GPT-4 (OpenAI et al. 2024) to classify whether arguments contain only truthful premises and whether the middle-terms are syntactically and semantically equivalent. This step helps us classify a belief-consistent dataset and a belief-inconsistent dataset.\n\u2022 Validation: We manually evaluate the alignment of classified arguments with human belief-consistency (i.e., premises are true and logic is valid).\nThis process ensures that our non-symbolic dataset maintains logical equivalence with the symbolic dataset while incorporating meaningful semantic real-word concepts.\nB.3. Data Statistics\nWe organise data statistics for generated datasets (Table 4)."}, {"title": "C. Embedding Space Analysis Details", "content": "One well-known method for analysing hidden activation in transformer language models is projecting activation into embedding space, the so-called logit lens (Nostalgebraist 2020; Elhage et al. 2021; Geva et al. 2022). In particular, we focus on an input-agnostic approach (Dar et al. 2023; Hanna, Liu, and Variengien 2024), utilising both unembedding ($\\textbf{W}_U$) and embedding ($\\textbf{W}_E$) matrices to construct a $\\mathbb{R}^{|V| \\times |V|}$ matrix, where $|V|$ represents the model's vocabulary size. In this study, we employ the OV circuit (Elhage et al. 2021) of attention heads, formed by attention value and output weights ($\\textbf{W}_v\\textbf{W}_o$), to understand the general behaviour of how source tokens influence output logits. The OV circuit-based logit lens for attention head h is formalised as $\\textbf{W}_E \\textbf{W}_U \\textbf{W}_v \\textbf{W}_o \\in \\mathbb{R}^{|V| \\times |V|}$. For simplicity and in line with established conventions (Elhage et al. 2021; Dar et al. 2023), this formulation ignores layer normalisation."}, {"title": "D. Circuit Ablation Method Details", "content": "To measure the necessity and sufficiency of the circuit C in the model M, we knock out attention heads in H from the model M and measure the average logit difference $d(p, m)$ along the batch. We denote the logit difference in circuit state C as $d(p, m, C)$ and every subset of heads set as $H \\subset H$.\nIn order to verify the head's necessity in the model, we conduct a cumulative ablation of C from total circuit M, progressing from downstream to upstream layers. At each ablation step k, where $C = M\\backslash H_k$ and $H_k$ denotes the set of ablated attention heads up to step k as:\n$E_{x \\sim x}[d_k(p, m, C)]$ where $C = M \\backslash H_k$\nConversely, we perform a cumulative addition of attention heads for evaluating the sufficiency of the circuit, starting from earlier layers and progressing to later ones, while maintaining all other attention heads in a mean-ablated state. At each addition step j, where $C = M \\backslash (H \\backslash H_j)$ and $H_j$ denotes the set of added attention heads up to step j as:\n$E_{x \\sim x}[d_j(p, m, C)]$ where $C = M \\backslash (H \\backslash H_j)$"}, {"title": "E. GPT-2 Model Architecture", "content": "We provide a self-contained concise overview of the GPT-2 model architecture, highlighting the main components and their mathematical relationships. Bias terms are not presented for the simplicity. We refer the conventions of notation in Elhage et al. (2021) and (Geva et al. 2023)."}, {"title": "F. Path Patching Details", "content": "Path patching, an generalised version of activation patching, aims to compute direct effects among model components, rather than indirect effects diffused from intervened components (Wang et al. 2023). This method involves freezing non-targeted activations during an initial forward pass, storing the corrupted targeted activation state, and then executing a subsequent forward pass with the targeted activation state substituted by the corrupted one. This approach isolates targeted component-to-component effects, eliminating non-relevant interactions. In our implementation, we employ a noising method where clean activations are replaced with corrupted ones, resulting in negative scores indicating positive contributions from corresponding components. Our findings, illustrated in Figure 7, reveal that $h_{11.10}$ strongly operates based on several heads: $h_{5.8}$, $h_{6.1}$, $h_{6.15}$, and $h_{7.2}$. Manual investigation of attention patterns subsequently confirmed these as induction heads (Elhage et al. 2021)."}, {"title": "G. Localising Mover Heads", "content": "To efficiently classify the numerous attention heads (384 in GPT-2 medium) within the model", "S[p": "S[m1", "S[m2": "on the x-axis and the patching score for all sequence positions (S) on the y-axis.\nBased on this PPD score quadrant layout analysis", "roles": "n(1) First quadrant (Positive Copy Candidates): Heads with S > 0 and positive PPD", "p": "based information.\n(2) Second quadrant (Positive Suppression Candidates): Heads with S > 0 but negative PPD", "m": "based information.\n(3) Third quadrant (Negative Copy Candidates): Heads with S < 0 and negative PPD"}, {"m": "based information.\n(4) Fourth quadrant (Negative Suppression Candidates): Heads with S < 0 but positive PPD, predominantly suppressing [p"}]}