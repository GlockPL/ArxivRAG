{"title": "Training and Evaluating Language Models with Template-based Data Generation", "authors": ["Yifan Zhang"], "abstract": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, these models often struggle with tasks requiring complex reasoning, particularly in mathematical problem-solving, due in part to the scarcity of large-scale, high-quality, domain-specific datasets necessary for training sophisticated reasoning abilities. To address this limitation, we introduce Template-based Data Generation (TDG), a novel approach that leverages LLMs (GPT-4) to automatically generate parameterized meta-templates, which are then used to synthesize a vast array of high-quality problems and solutions. Leveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset comprising over 7 million synthetically generated grade school math problems each accompanied by code-based and natural language solutions\u2014with the potential to generate an effectively unlimited number more. This dataset alleviates the scarcity of large-scale mathematical datasets and serves as a valuable resource for pre-training, fine-tuning, and evaluating LLMs in mathematical reasoning. Our method not only enables the generation of virtually infinite data but also elevates data augmentation to a new level by using GPT-4 for meta-template generation, ensuring diverse and high-quality problem structures. The TemplateMath Part I: TemplateGSM dataset is publicly available at https://huggingface.co/datasets/math-ai/TemplateGSM.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have revolutionized natural language processing (NLP), exhibiting unprecedented capabilities in language understanding and generation. Models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and Llama (Touvron et al., 2023) have achieved remarkable success across various NLP tasks, including machine translation, summarization, and question answering.\nDespite these advancements, LLMs often struggle with tasks requiring complex reasoning and precise problem-solving skills, particularly in mathematical domains (Hendrycks et al., 2021; Cobbe et al., 2021). Mathematical reasoning poses unique challenges due to its reliance on rigorous logic, structured methodologies, and the necessity for exact solutions. Existing mathematical datasets are limited in both size and diversity, hindering models' ability to generalize across a wide range of problems (Paster et al., 2023; Azerbayev et al., 2023). The scarcity of large-scale, high-quality mathematical datasets is a significant barrier to developing LLMs capable of sophisticated mathematical reasoning."}, {"title": "Template-based Data Generation", "content": "Template-based Data Generation (TDG) is a method designed to systematically produce a vast array of mathematical problems along with their corresponding solutions by leveraging parameterized templates. To elevate data augmentation to a new level, we employ GPT-4 to generate these meta-templates, capturing a wide variety of problem structures and linguistic styles. By varying parameters within these GPT-4-generated templates, TDG ensures both scalability and quality in the generated data. This approach enables the creation of diverse and complex problem sets, which are essential for training and evaluating large language models in mathematical reasoning tasks."}, {"title": "Methodology", "content": "The TDG process involves several key components that work together to generate high-quality mathematical datasets:"}, {"title": "Generation of Meta-Templates with LLMs", "content": "We begin by utilizing large language models (LLMs), such as GPT-4, to generate meta-templates (Zhang et al., 2023) that capture the underlying structures of various mathematical problem types. GPT-4's advanced language generation capabilities allow us to produce a diverse set of templates encompassing a wide range of mathematical concepts and problem types. These templates include placeholders for variable components such as names, items, quantities, dates, and locations. By harnessing GPT-4, we ensure that the templates are linguistically diverse and contextually rich, contributing to the overall quality and diversity of the dataset."}, {"title": "Simultaneous Q&A Generation and Verification", "content": "In a single integrated step, we generate specific problems and their corresponding solutions by substituting parameters into the GPT-4-generated meta-templates. Parameters are carefully selected to satisfy specific conditions, ensuring the solvability and validity of the problems. This simultaneous generation of questions and answers maintains consistency between the problem statements and their solutions."}, {"title": "Process Flowchart", "content": "An illustrative overview of our TDG method is presented in Figure 1. The flowchart demonstrates the process starting from the LLM (e.g., GPT-4) generating meta-templates to the final dataset creation. After the meta-templates are generated, parameters are substituted to instantiate problems and solutions simultaneously. The generated Q&A pairs undergo verification using code execution and LLM-based checks. This loop continues, discarding any invalid pairs, until the dataset is populated with verified, high-quality data."}, {"title": "Code Implementation Example", "content": "An illustrative example of our TDG method is presented in Figure 2. The code snippet demonstrates how we generate problems involving sales over two consecutive months. The meta-template for this problem type was generated using GPT-4, capturing a realistic scenario that can be varied through parameter substitution. We include lists of random terms such as names, items, months, and locations to create diverse and realistic problem contexts. This randomness introduces variability and prevents the dataset from becoming repetitive, which helps in training models to generalize better."}, {"title": "Generated Problem and Solution Example", "content": "To illustrate the output of our TDG method, consider the following example generated using a GPT-4-produced template:\nProblem:\nEmily has 15 apples. She buys 3 times more apples and then gives away 5 apples to her friend. How many apples does Emily have now?\nSolution:\nEmily initially has 15 apples. She buys 3 times more, so she buys 15 \u00d7 3 = 45 apples. Now she has 15 + 45 = 60 apples. She gives away 5 apples, so she is left with 60 - 5 = 55 apples. Therefore, Emily has 55 apples now."}, {"title": "Advantages of TDG", "content": "The TDG method offers several significant advantages that make it particularly effective for generating large-scale mathematical datasets:\n\u2022 Scalability: TDG enables the generation of an effectively infinite amount of data by varying parameters within GPT-4-generated templates. This scalability is crucial for training large language models that require vast amounts of data.\n\u2022 Quality Assurance: By integrating generation and verification into a single step with reject-sampling and utilizing code execution and LLM verification, we ensure that each problem-solution pair is correct and reliable. This precise supervision enhances the quality of the dataset and the performance of models trained on it.\n\u2022 Efficiency: The iterative process of generating and verifying Q&A pairs streamlines data creation, allowing for efficient accumulation of high-quality data.\n\u2022 Diversity: The use of GPT-4 to generate meta-templates introduces a wide variety of problem structures and linguistic styles, enhancing the diversity of the dataset. This diversity helps models generalize better to new and unseen problems.\n\u2022 Elevated Data Augmentation: By incorporating GPT-4 into the template generation process, we elevate data augmentation to a new level, enabling the synthesis of data that is both varied and high-quality."}, {"title": "TemplateMath Part I: TemplateGSM Dataset", "content": "Building upon the TDG method, we have developed TemplateGSM, a dataset consisting of over 7 million grade school math problems. Each problem is paired with both a code-based solution and a natural language explanation. The problems cover a wide range of mathematical topics suitable for grade school levels, including arithmetic operations, fractions, percentages, and basic algebra. The meta-templates used to generate these problems were created using GPT-4, ensuring a rich diversity in problem structures and linguistic expressions. This comprehensive coverage ensures that the dataset can be used to train models on various problem types and difficulty levels."}, {"title": "Dataset Statistics", "content": "The key statistics of the TemplateGSM dataset are presented in Table 1. With 7,473,000 problems generated from 7,473 unique GPT-4-generated templates, the dataset offers extensive diversity in both problem structures and content.\nThe average lengths of problems and solutions indicate that the dataset provides substantial context and detailed explanations, which are beneficial for training language models to understand and solve complex reasoning tasks."}, {"title": "Dataset Availability", "content": "TemplateGSM is publicly available and can be accessed at https://huggingface.co/datasets/math-ai/TemplateGSM. The code used for data generation is also provided at https://github.com/iiis-ai/TemplateMath. By sharing both the dataset and the generation code, we enable researchers and practitioners to reproduce our results, extend the dataset, and apply the TDG method to other domains or problem types.\nThe TemplateGSM dataset serves as a valuable resource for pre-training, fine-tuning, and evaluating large language models in mathematical reasoning tasks. By addressing the data scarcity problem, it facilitates the development of models capable of sophisticated reasoning, such as IBM's Granite Language Model (Granite Team, 2024). The inclusion of GPT-4-generated templates introduces a level of diversity and naturalness in problem statements that closely mimic human-crafted problems. We anticipate that TemplateGSM and TemplateMath will contribute to advancements in AI research focused on reasoning and problem-solving."}, {"title": "Related Work", "content": "Mathematical Datasets. The development of mathematical datasets has been pivotal in advancing AI research, particularly in mathematical reasoning and problem-solving. Early datasets like AQUA-RAT (Ling et al., 2017) provided annotated question-answer pairs for arithmetic word problems. The MATH dataset (Hendrycks et al., 2021) comprises over 12,500 challenging competition-level problems, serving as a benchmark for evaluating mathematical reasoning abilities. However, its limited size restricts its utility for training large models.\nTo expand available resources, Paster et al. (2023) introduced OPENWEBMATH, which filters web data to collect mathematical content. While it offers a larger dataset, quality control remains challenging due to the noisy nature of web data. PROOF-PILE (Azerbayev et al., 2023) aggregates informal mathematical texts but lacks the structured problem-solution pairs necessary for supervised learning. Similarly, Zhang et al. (2024) proposed using language models as generative verifiers for data selection and released the AutoMathText dataset, but the data is still in unstructured text format.\nOur work builds upon these efforts by providing a significantly larger and more diverse dataset of mathematical problems with verified solutions, addressing the need for high-quality training data in mathematical reasoning.\nTraining LLMs on Mathematical Tasks. Base LLMs trained on vast corpora have demonstrated impressive language capabilities (Brown et al., 2020; Touvron et al., 2023). However, their performance on mathematical tasks is limited due to the scarcity of domain-specific training data. Recent studies have explored fine-tuning LLMs on mathematical datasets through continual pre-training (Lewkowycz et al., 2022; Azerbayev et al., 2023) or supervised fine-tuning (SFT) (Yu et al., 2023; Yue et al., 2023; Weng et al., 2023).\nContinual pre-training involves further training on mathematical texts, enhancing models' familiarity with mathematical language but not necessarily improving problem-solving skills. SFT approaches fine-tune models on curated question-answer pairs but are constrained by the availability of high-quality datasets."}, {"title": "Conclusion", "content": "We have introduced Template-based Data Generation (TDG), a novel approach for generating large-scale, high-quality mathematical datasets through parameterized templates generated by GPT-4. Utilizing TDG, we created TemplateGSM, a dataset of over 7 million synthetically generated grade school math problems with verified solutions in both code and natural language formats.\nOur extensive experiments demonstrate that TemplateGSM significantly enhances the mathematical reasoning capabilities of LLMs when used for pre-training and fine-tuning. The precise supervision offered by code execution and verification ensures the reliability of the data, fostering the development of models with improved understanding and problem-solving abilities.\nBy leveraging GPT-4 to generate meta-templates, we have elevated data augmentation to a new level, introducing greater diversity and richness in the generated data. We believe that TDG and the TemplateGSM dataset will contribute substantially to advancing research in mathematical reasoning with LLMs. By addressing the data scarcity problem, our work opens new avenues for developing models capable of complex reasoning tasks.\nLimitations. While TDG and TemplateGSM offer substantial benefits, there are limitations. One limitation is template bias, where models may become biased toward the structures present in the GPT-4-generated templates. Additionally, the generated problems are primarily at the grade school level, so extending TDG to higher-level mathematics requires careful template design, reflecting challenges in addressing complexity levels. Moreover, although GPT-4 introduces linguistic diversity, the style may still differ from human-authored educational materials, indicating a limitation in authenticity.\nFuture Work. Future research directions include expanding template coverage by developing templates for more advanced mathematical topics, potentially leveraging even more sophisticated LLMs or collaborative human-AI template generation. Integrating augmentation techniques, such as using language models to rephrase and compose problems (Liu et al., 2024), can increase linguistic diversity further. Extending TDG to generate problems in multiple languages would create multilingual datasets. Additionally, exploring methods to mitigate template bias and enhance the authenticity of problem statements could improve the utility of the dataset. Conducting studies to assess the quality and educational value of the generated problems through human evaluation would provide valuable insights."}]}