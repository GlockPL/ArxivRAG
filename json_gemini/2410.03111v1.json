{"title": "LORC: Low-Rank Compression for LLMs KV Cache\nwith a Progressive Compression Strategy", "authors": ["Rongzhi Zhang", "Kuang Wang", "Liyuan Liu", "Shuohang Wang", "Hao Cheng", "Chao Zhang", "Yelong Shen"], "abstract": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by storing\npreviously computed KV vectors. However, its memory consumption scales lin-\nearly with sequence length and batch size, posing a significant bottleneck in LLM\ndeployment. Existing approaches to mitigate this issue include: (1) efficient atten-\ntion variants integrated in upcycling stages, which requires extensive parameter\ntuning thus unsuitable for pre-trained LLMs; (2) KV cache compression at test\ntime, primarily through token eviction policies, which often overlook inter-layer\ndependencies and can be task-specific.\nThis paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-\nin integration with existing transformer-based LLMs without model retraining.\nTo effectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is supported by\nour theoretical analysis on how compression errors accumulate in deep networks.\nOur method is designed to function without model tuning in upcycling stages or\ntask-specific profiling in test stages. Extensive experiments with LLaMA models\nranging from 8B to 70B parameters across various tasks show that our approach\nsignificantly reduces the GPU memory footprint while maintaining performance.", "sections": [{"title": "1 Introduction", "content": "Autoregressive large language models (LLMs) such as GPT (Achiam et al., 2023), PaLM (Chowdhery\net al., 2023), and LLaMA (Touvron et al., 2023), built upon transformer architectures (Vaswani\net al., 2017), have shown remarkable capabilities across a wide range of tasks. However, the\nattention mechanism underpinning those models poses significant challenges to the efficiency of\ntheir deployment, particularly the management of the Key-Value (KV) cache. The KV cache is\noriginally designed to accelerate the generation process by storing intermediate attention KV vectors,\nthus avoiding recomputation of shared prefixes for each autoregressively generated token. Despite\nreducing computational overhead, the KV cache significantly increases memory footprints, as its\nsize scales linearly with both sequence length and batch size. This drives the need for KV cache\ncompression to enable cost-effective deployment of LLMs across various devices and platforms.\nTo address the overhead of the original attention mechanism, one prominent line of work aims\nto design more efficient attention variants, such as multi-query attention (MQA) (Shazeer, 2019)\nand group-query attention (GQA) (Ainslie et al., 2023), which inherently reduce the corresponding\nKV cache. Nevertheless, those techniques typically require upcycling existing models. Without\nproper training, their direct application often results in degraded performance (Ribar et al., 2023;"}, {"title": null, "content": "Ainslie et al., 2023; Liu et al., 2024b), thereby making them unsuitable for deployment in resource-\nconstrained environments. Recently, Liu et al. (2024a) design a multi-head latent attention (MLA) for\nefficient inference, utilizing low-rank key-value union compression to reduce KV cache. However,\nsimilar to MQA and GQA, MLA is also integrated during the model's training cycle, thus not directly\napplicable to pre-trained LLMs.\nIn contrast, another line of work focuses on KV cache compression at test time, primarily achieved\nby dropping tokens while leaving the backbone model intact. Several works design the token eviction\npolicy based on accumulated attention scores (Sheng et al., 2023; Zhang et al., 2024b; Liu et al.,\n2024b), or heuristics such as special tokens or and relative distance between tokens (Ge et al., 2023)\nHowever, these methods either ignore inter-layer dependencies or require attention pattern analysis,\nand the resulting eviction policy can be task-specific.\nIn this paper, we propose to compress KV cache from an orthogonal perspective, i.e., the KV weight\nmatrices. As the KV weight matrices are typically characterized by low-rank properties, we perform\na low-rank approximation to reduce their dimension and thus compress the resulting KV cache.\nRecognizing that compressed KV caches inevitably introduce information loss to subsequent layers,\nand that sensitivity to input changes varies across layers, we introduce a progressive compression\nstrategy. This approach is grounded in the calculation of cumulative condition numbers for KV weight\nmatrices across different layers, reflecting their sensitivity and guiding the compression strategy.\nTheoretically, we derive error bounds for both individual layer compression and error propagation\nthrough the network. These theoretical results reveal that errors introduced in earlier (shallower)\nlayers are amplified more significantly than those in deeper layers, and informs our progressive\ncompression strategy.\nOur method is designed for straightforward implementation, requiring neither model profiling nor\ndetailed inspection of the attention structure. It can be directly applied to pre-trained LLMs by\nextracting weight matrices and leveraging their inherent properties to swiftly determine optimal\nlayer-wise compression. This approach offers a practical and efficient solution for enhancing LLM\ninference performance in memory-constrained deployment scenarios, without the need for model\nretraining or complex eviction strategy composition.\nWe evaluate our method on 8B, 13B, and 70B LLaMA models that built upon multi-query attention\nor group-query attention. Experiments across tasks such as commonsense reasoning, reading compre-\nhension, text summarization, and mathematical reasoning, demonstrate that our approach can reduce\nsubstantial GPU memory footprint while maintaining minimal impact on performance."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Attention Mechanism", "content": "Attention mechanisms in Transformer models have evolved to enhance efficiency and effectiveness\n(Vaswani et al., 2017). Multi-Query Attention (MQA) (Shazeer, 2019) reduces memory requirements\nduring decoding, while Grouped-Query Attention (GQA) (Ainslie et al., 2023) balances efficiency\nand performance by sharing key and value heads among query groups. Recently, Liu et al. (2024a)\nintroduced Multi-head Latent Attention (MLA), using low-rank key-value union compression to\noptimize inference. However, these approaches are typically integrated during model training, limiting\ntheir applicability to pre-trained LLMs. Parallel research efforts have targeted inference efficiency\nimprovements. For example, Pope et al. (2023) developed multi-dimensional partitioning techniques,\nand de Jong et al. (2022) optimized the Fusion-in-Decoder (FiD) approach (Izacard & Grave, 2020)\nfor more efficient inference. Holmes et al. (2024) introduces SplitFuse which leverages dynamic\nprompt and generation decomposition and unification to further improve continuous batching and\nsystem throughput. In this paper, we contribute to this line of research by improving inference\nefficiency through the compression of KV cache. Our approach leverages the low-rank property of\nthe attention weight matrices, offering a plug-and-play method to reduce the memory footprint of\nLLMs during inference without requiring model retraining."}, {"title": "2.2 KV Cache Compression", "content": "As Large Language Models (LLMs) continue to grow in size and complexity, efficient management\nof their memory usage during inference has become a critical challenge. Early efforts to compress"}, {"title": null, "content": "token hidden states (Guan et al., 2022; Sun et al., 2022; Zhou et al., 2020) are limited to non-\nautoregressive models and require retraining, thus motivating research into pruning tokens in the\nKV cache of auto-regressive LLMs. For instance, Mu et al. (2024) learns to compress prompts into\na few special tokens to reduce memory pressure during caching, but this token prediction requires\nmodel retraining and could be an expensive overhead during inference. Several methods design token\neviction policies based on accumulated attention scores (Sheng et al., 2023; Zhang et al., 2024b; Liu\net al., 2024b), or heuristics such as special tokens and relative distance between tokens (Ge et al.,\n2023). However, these approaches often overlook inter-layer dependencies, potentially resulting in\ntask-specific eviction policies that may not generalize well across different applications. In contrast\nto token-dropping methods, our study takes a different tack. We focus on compressing the KV cache\nfrom the perspective of weight matrix dimension reduction. Importantly, our progressive compression\nstrategy carefully addresses the issue of error propagation across compressed layers, a consideration\noften ignored in previous methods.\nA few studies have explored customized cache budgets across different layers in the context of token\ndropping, yet no definitive consensus has been reached on the most effective strategies. Zhang\net al. (2024a) suggest increasing compression intensity in higher layers based on the assumption that\nthese layers contain less critical information. Conversely, Liu et al. (2024b) argue that significant\ntokens exhibit greater variability at higher layers, thus larger caches are required to reduce cache\nmisses. While these approaches demonstrate understanding of layer-specific requirements, they\ndepend heavily on task-specific attention patterns. Our approach diverges fundamentally by adopting\nan orthogonal perspective to compression, focusing on weight matrix dimension reduction rather than\ntoken eviction. This approach enables us to establish error propagation bounds across the network and\nto guide our progressive compression strategy effectively. It eliminates the need to analyze attention\npatterns for eviction policy design, simplifying implementation and enhancing general applicability\nacross different LLMs.\nConcurrently, Liu et al. (2024a) and Yu et al. (2024) modify attention mechanisms to manage KV\ncaches more efficiently during inference. While these methods align with our philosophy of altering\nattention dynamics, they require either pretraining adjustments or extensive model finetuning to\naccommodate the modified attention schemas, limiting their practicality in deployed systems. In\ncontrast, our method requires no such training or fine-tuning, offering a plug-and-play solution that\nseamlessly integrates with pre-trained models to deliver efficient compression without compromising\nthe model's integrity or performance."}, {"title": "3 Preliminary: Attention Mechanism and KV Cache", "content": "Transformer-based language models use self-attention to weigh the importance of different tokens,\nthus allowing for the model to focus on different parts of the input sequence. Given an input\nX\u2208 \u211d^(N\u00d7D), where N is the sequence length and D is the dimensionality of each token's embedding,\nwe compute the Query (Q), Key (K), and Value (V) matrices by multiplying X with their respective\nweight matrices: Q = XWq, K = XWk, V = XWv.\nThen the attention mechanism is as follows:\nAttention(Q, K, V) = softmax(QKT /\u221adk)V.  (1)\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions\nMultiHead(Q, K, V) = Concat(head\u00b9,...,head)W\uff61, (2)\nwhere head\u00b2 = Attention(X(W)T, X(W)T, X(W)T). (3)\nHere, W, W, and We are the weight matrices for the i-th attention head, and Wo is the weight\nmatrix for the output linear transformation."}, {"title": "4 Method", "content": "We structure this section as follows. In Section 4.1, we detail the process of compressing the KV\ncache for a single layer using Singular Value Decomposition (SVD) on weight matrices. Section 4.2\nintroduces our progressive compression strategy, which determines adaptive compression dimensions\nfor each layer. Finally, Section 4.3 covers additional considerations for handling various attention\nmechanisms, and Section C addresses the implementation details specific to the rotary position\nembedding. Figure 1 presents an overview of our method, illustrating the low-rank approximation of\nthe weight matrix and the progressive compression strategy across layers."}, {"title": "4.1 KV Cache Compression via Low-rank Approximation of Weight Matrices", "content": "Unlike previous approaches that focus on token-level eviction strategies or require model retraining,\nwe propose a novel method that operates at the weight matrix level in the attention mechanism. This\napproach leverages the inherent low-rank properties of these matrices (as shown in Appendix B),\nallowing for significant compression without the need for complex token selection algorithms or\ntime-consuming model tuning. By applying a low-rank approximation to the weight matrices, we\neffectively reduce the dimensionality of the KV cache while preserving the essential information flow\nthrough the network.\nKey Matrix Compression: Figure 1 presents how we implement SVD on the key weight matrices.\nSpecifically, for the i-th head in the MHA attention, we decompose its key matrix W \u2208 R^(D\u00d7d) to:\nSVD(W)Dxd = UDxd\u00a2\u00a3dcxdcVdxd = UDxdc(CVT)dcxd. (5)\nFor MHA, there are h attention heads, then the decomposition becomes:"}, {"title": null, "content": "SVD(W)Dxhd = UDxdc(\u2211VT)dcxhd = UDxdc [(A1)dcxd (A2)dcxd ... (Ah)dexd], (6)\nwhere (A\u00b2)dxd is the i-th block in the matrix (\u2211VT)dcxhd\u00b7\nWe have now decomposed the key matrix W to the multiplication of UD\u00d7de and (\u2211VT)dcxhd. We\nwill multiply X with (DVT) hdxde as the compressed key, which is stored in the KV cache. Through\nthis implementation, we effectively update the size of key cache from hd to de, where de is smaller\nthan hd, reducing the memory footprint while keeping the essential information intact.\nFor UDxd, we incorporate it to the query calculation by updating the original query matrix WHE\nRDxhd as follows:\nWH = (WH)DxhdUDxdc (7)\nNote that the embedding dimension D is equal to the product of the number of attention heads h and\nthe dimension per head d, i.e., D = hd. Consequently, the updated query matrix WH \u2208 RDxdc.\nValue Matrix Compression: The decomposition for the value matrix follows a similar structure\nto that of the key matrix, with the difference that we integrate its left singular vectors to the output\nmatrix W. Specifically, the value matrix is decomposed as:\nSVD(WH)Dxhd = UDxdc(CVT)dcxhd = UDxdc [ (B\u00b9)dcxd (B2)dcxd ... (Bh)dcxd] (8)\nwhere (B\u00b2)dxd is the i-th block in the matrix (DVT)dexhd\u00b7 After multiplication with X, the\ndimension of the value cache shrinks from hd to de, thus reducing memory consumption.\nIn contrast to the key matrix operation, we incorporate UDxde to the output matrix. To achieve this,\nwe update the output matrix W\u3002\u2208 RD\u00d7D as follows:\nWo = (UT)de\u00d7D(Wo)D\u00d7D, (9)\nresulting in an updated output matrix Wo \u2208 Rde\u00d7D.\nCompression Ratio: The compression strategy effectively reduces the dimensions from N \u00d7 d \u00d7h\nfor both keys and values to N \u00d7 dc, ensuring data integrity and minimizing overhead. This results in\na layer compression ratio p = de/hxd, which quantifies the extent of the reduction."}, {"title": "4.2 Progressive Compression Strategy", "content": "Algorithm 1 LORC Algorithm\nRequire: Pre-trained LLM with L layers\n1: Initialize cumulative condition numbers \u03ba\u03b9\n2: forl L to 1 do\n3: Compute \u03ba(W) and \u03ba(W)\n4: \u03ba\u03b9 \u2190 \u03a0\u03b9\u03ba(W)\u00b7\u03ba(W)\n5: end for\n6: forl 1 to L do\n7: Calculate de by Eq. 13\n8: if > threshold then\n9: Skip compression for layer l\n10: continue\n11: end if\n12: Key Matrix Compression:\n13: Perform SVD: W = Ukk(VF)\n14: W\u2190 Uk[:, : dc](\u03a3kVF)[: de, :]\n15: W\u2190 WUk[:, :d]\n16: Value Matrix Compression:\n17: Perform SVD: W = U\u03a3(VT)\n18: W\u2190 Uv [:, : dc] (\u03a3VT)[: de, :]\n19: W!, \u2190 Uv [:, : dc]TW!\n20: Update KV cache size for layer l\n21: end for\nHaving established low-rank approximation for\ncompressing weight matrices, we now address its\ndynamic application across network layers. This ap-\nproach is necessary due to the varying sensitivity of\ndifferent layers, which significantly affects overall\nmodel efficacy and efficiency.\nTo tackle this challenge, we propose a progressive\ncompression strategy for our low-rank approxima-\ntion of KV weight matrices. Our intuition is that the\ncompressed shallow layers could lead to cascading\nerrors that propagate and amplify through the net-\nwork. Therefore, we measure the layer sensitivity\nby the condition numbers of KV matrices to de-\ntermine layer-wise compression dimensions. This\napproach accounts for each layer's sensitivity to per-\nturbations caused by previously compressed layers,\nensuring output variations remain within acceptable\nranges. This progressive nature allows for more\nconservative compression in shallow layers and\nmore aggressive compression in deeper layers, min-\nimizing the risk of error accumulation throughout\nthe network. By carefully balancing compression\nacross layers, we maintain model integrity while\nachieving significant memory savings."}, {"title": "Condition Number and Sensitivity Analysis", "content": "To ensure that the change in the output b\u2081 = A[x\u012b\nremains within a specified range when the input x\u2081 changes due to compression in previous layers, we\nneed to consider the sensitivity of the output to such changes. Given a weight matrix A1, its condition\nnumber plays a crucial role in determining the allowable change in x\u2081. The condition number K(A\u0391\u03b9)\nis defined as:\n\u03ba(\u0391\u03b9) = |\u0391\u03b9|2. |\u039171|2 = \u03c3max (\u0391\u03b9)/\u03c3min (\u0391\u03b9), (10)\nwhere omax (Al) and omin(A1) are the largest and smallest singular values of A\u2081, respectively. To\nkeep the relative change in the output by within a tolerance e, we utilize the standard definition of the\ncondition number to relate it to the allowable relative change in the input x1:\nAbi/bi < \u03ba(\u0391\u03b9)\u00b7|\u0394\u03a7\u03b9/X1| <\u03b5. (11)\nSolving for the allowable relative change in x\u2081, we obtain: |\u0394\u03a7\u03b9/X1| \u2264 \u03b5/\u03ba(\u0391\u03b9). This inequality indicates\nthat the acceptable change in the input x\u012b is inversely proportional to the condition number \u03ba(\u0391\u03b9)\nof the layer's weight matrix. Layers with higher condition numbers are more sensitive to input\nperturbations, requiring smaller changes in x\u012b to maintain the output within the desired range. Given\nthe multi-layer structure of transformers, it is essential to consider not just the condition number of a\nsingle layer but the cumulative effect of condition numbers from all preceding layers. This cumulative\nmeasure gives a more holistic view of how perturbations might propagate and amplify as data passes\nthrough successive layers.\nCumulative Condition Number: To effectively manage this across the network, we calculate the\ncumulative condition number as an estimated layer sensitivity, which we then use to derive the\ncompression dimension. For a model with L layers, we calculate the cumulative condition number\nfor each layer l by multiplying the condition numbers of the current layer and all subsequent layers:\n\u03ba\u03b9 = \u220f(j=l to L)\u03ba(W)\u00b7 \u03ba(W), (12)\nwhere W and W denote the key and value weight matrices of the j-th layer, respectively. This\ncumulative condition number k\u012b reflects the total amplification of input perturbations from current\nlayer to the final output layer, encompassing the effects of layers from l to L.\nCompression Dimension: Based on the cumulative condition number, we then adjust the compression\ndimensions for each layer to balance the fidelity and compression rate. More sensitive layers\n(those with higher cumulative condition numbers) will have less aggressive compression to preserve\ninformation, whereas layers with lower sensitivity can be compressed more substantially without\nsignificantly affecting the overall network performance. We compute the compressed dimension d\nfor each layer by scaling \u012e using the following function:\nde = dmax \u00d7 [1-(max(i\u2208[1:L]) log(ki) \u2013 log(\u03ba\u03b9))/(max(i\u2208[1:L]) log(ki) \u2013 min(i\u2208[1:L]) log(ki)) \u00d7 (1-dmin/dmax)] (13)\nwhere dmax is the maximum allowable compressed dimension, and dmin is the minimum one.\nThe logarithmic scale mitigates the effect of large variations in the cumulative condition numbers,\nproviding a more balanced sensitivity metric across layers. This equation ensures that layers with\nhigher sensitivity (larger k\u2081) retain more dimensions (larger d\u0131), while less sensitive layers can be\ncompressed more aggressively."}, {"title": "4.3 Multi-head Attention and Group-query Attention", "content": "The above derivation in Section 4.1 holds for standard MHA, where the model dimension D equals\nto the multiplication of number of head and head dimension h \u00d7 d. For GQA, the number of KV\nheads is reduced as shown in Table 3. To adapt such implementation, we can still follow the above\nprocedure for cache compression. After fetching the key and value from cache, we just need to repeat\nthem according to the number of the total attention heads."}, {"title": "5 Error Bounds for KV Cache Compression", "content": "In this section, we derive error bounds for our KV cache compression method, considering both\nindividual layer errors and their propagation through a deep network. These theoretical results provide\ninsights into how the matrix decomposition-based compression affects the network's performance\nand guide the progressive compression strategy to balance model efficiency and performance."}, {"title": "5.1 Error Bound for Key/Value Matrix Approximation", "content": "Theorem 1 Let W \u2208 Rm\u00d7n be a weight matrix (either key or value), and let W \u2208 Rm\u00d7n be its\nrank-k approximation obtained via truncated singular value decomposition (SVD). For any input\nvector x \u2208 Rn, the error introduced by the approximation is bounded by:\n||Wx \u2013 Wx||2 \u2264 0k+1||x||2, (14)\nwhere ok+1 is the (k + 1)-th singular value of W.\nThe proof is provided in Appendix A.1. This theorem quantifies the error introduced at a single layer\ndue to compressing the weight matrix. The bound indicates that the error is directly proportional\nto the (k + 1)-th singular value of W and the norm of the input vector x. Larger singular values\ncorrespond to directions of significant variance in the data, so truncating smaller singular values\n(which represent less significant features) minimizes the error introduced by compression."}, {"title": "5.2 Single Layer Error Bound Including Nonlinearities", "content": "We now extend the analysis to include the effect of nonlinearities within a single layer. We derive an\nerror bound that accounts for both the approximation of the weight matrix and the layer's nonlinear\nactivation function. For simplicity, we analyze the error introduced by compressing each weight\nmatrix (key or value) individually.\nTheorem 2 Consider a single layer applying a linear transformation W followed by a nonlinearity\n\u222e with Lipschitz constant L\u00f3. Let W be the compressed version of W obtained via truncated SVD\nwith rank k. For any input vector x \u2208 R, the error at the output of the layer is bounded by:\n|$(Wx) \u2013 $(\u00d1x)| \u2264 Lok+1||2||2. (15)\nThe proof is straightforward by using Theorem 1 and the Lipschitz property of $, we present it as the\nbase case in the proof of Theorem 3, which is detailed in Appendix A.2.\nThis theorem shows that the error introduced by the compressed weight matrix propagates through\nthe nonlinearity, scaled by the Lipschitz constant of the activation function. While considering\nboth matrices simultaneously complicates the bounds due to their interactions within the attention\nmechanism, it is still feasible to derive combined error bounds because the attention mechanism\nallows us to mathematically bound these interactions. The total error due to simultaneous compression\ncan be bounded by the sum of their individual approximation errors, scaled by a constant. However,\nfor simplicity and clarity in the following derivation, we use the simplified version that considers\neach matrix individually."}, {"title": "5.3 Error Propagation Bound", "content": "Theorem 3 Consider an L-layer network where each layer i applies a linear transformation Wi\nfollowed by a nonlinearity & with Lipschitz constant L. Let Wi be the compressed version of Wi\nobtained via truncated SVD with rank ki. The error at the output of the network is bounded by:\n||XL-XL||2\u2264 ( \u2211(i=1 to L) (\u03c3(i)k(i)+1/(\u041f(j=i+1 to L))||Wj||2) L(\u03d5)(L\u2212i)), (16)\nwhere XL and XL are the outputs of the original and compressed networks, respectively; \u03c3 k(i)+1 is the\n(ki + 1)-th singular value of Wi; ||Wj ||2 denotes the spectral norm of Wj; and L\u00f8 is the Lipschitz\nconstant of the activation function \u03c6.\nWe detail the proof in Appendix A.2. Until now, we have established an upper bound on the\ncumulative error at the network's output due to compression of weight matrices across multiple\nlayers. It is important to note that the nonlinearities characterized by the Lipschitz constant L\nrepresent a simplification. In practice, transformer models like LLaMA incorporate complex nonlinear\ncomponents, so the exact error propagation may deviate from this simplified bound due to intricate\nnonlinearities. Despite these complexities, the theorem still offers insights into how compression\nerrors may accumulate in deep networks. Specifically, it reveals that errors introduced in earlier\n(shallower) layers are amplified more significantly than those in deeper layers because they pass\nthrough more subsequent transformations and nonlinearities.\nThis understanding supports our design of a progressive compression strategy, where we compress\nshallow layers less aggressively than deeper ones. By preserving more information in the early\nlayers (i.e., retaining more singular values), we minimize the initial errors that could be significantly\namplified throughout the network. This approach helps maintain overall model performance while\nstill achieving substantial compression in deeper layers, where the impact on the final output is less\npronounced due to reduced error amplification."}, {"title": "6 Experiment", "content": null}, {"title": "6.1 Models", "content": "We conduct experiments using two attention mechanisms, Multi-Head Attention (MHA) (Vaswani\net al., 2017) and Graph Query Attention (GQA) (Ainslie et al., 2023), across three models: LLaMA-\n2-13B, LLaMA-3-Instruct-8B, and LLaMA-3-Instruct-70B. The LLaMA-2 family incorporates the\nMHA mechanism, while the LLaMA-3 family is based on the GQA framework. We list the model\nspecifications in Table 3. Note that for the models based on MHA, the number of KV heads is equal\nto the number of attention heads, so the weight matrices of KV are square matrices. The models\nbased on GQA use an intermediate number of key-value heads to group the query heads, with an\nadjustment on the shape of KV weight matrices."}, {"title": "6.2 Implementation Details", "content": "In practice, we set thresholds to exclude compression on layers with high cumulative condition\nnumbers: 30 for LLaMA-3-Instruct-8B, and 90 for LLaMA-2-13B and LLaMA-3-Instruct-70B. The\ndmax equals to the original head dimension, while dmin varies based on the target compression ratio.\nFor baseline methods, we have the same refrained layers while applying the uniform compression\nratios across compressed layers instead of using a progressive compression strategy."}, {"title": "6.3 Dataset", "content": "We follow Touvron et al. (2023) to evaluate our methods on the following tasks: BoolQ (Clark et al.,\n2019) for reading comprehension, XSum (Narayan et al., 2018) for text summarization. Openbook\nQA (Mihaylov et al., 2018) for commonsense reasoning, and GSM8K (Cobbe et al., 2021) for\nmathematical reasoning. We use ROUGE score (Lin, 2004) as the evaluation metric for XSum and\naccuracy for the other tasks. We report 2-shot results for LLaMA-2 models on BoolQ, and 0-shot\nresults for other settings."}, {"title": "6.4 Main Results", "content": "Figure 2 presents our main results on four datasets with different KV cache budgets. Compared to\nthe full-cache model, LORC achieves on-par performance with a significant compression ratio, and\nthe performance degradation is still nearly negligible with a 60% compression ratio on most datasets.\nWhen slightly compressed, LoRC could even enhance model performance in some cases. Note that\nour method requires no model training or model profiling, the only efforts are SVD on weight matrices\nwhich requires minimal computational cost compared to the LLM inference. Such plug-and-play"}, {"title": "6.5 Single Layer Profiling", "content": "To investigate the impact of compres-\nsion at different layers, we conduct\nexperiments on single-layer compres-\nsion as shown in Fig. 3. We use\nLLaMA-3-Instruct-8B on OpenBook\nQA for this experiment. The original\ndimension of the KV head is 1024,\nand we select compression dimen-\nsions from [256,384,512] to com-\npress each single layer while keeping\nall other layers untouched.\nFigure 3 shows clear layer-specific variability, indicating that some layers are more susceptible to\ncompression than others, particularly in the shallow layers. It is observed that the deep layers (i.e.,\nlayers 15-31 of the 32-layer LLaMA-3-Instruct 8B model), despite the reduction in dimensions,\nmaintain performance closely approaching the full KV Cache baseline. This suggests that these layers\ncan sustain robust performance even when subjected to significant parameter reduction. This finding\nsupports our progressive compression strategy for optimizing model efficiency without significantly\ncompromising the model's effectiveness."}, {"title": "6.6 Curse of shallow layers", "content": "To validate the intuition of the progressive compression strategy that the noise caused by shallow\ncompressed layers will be amplified more after propagation, we compare it to compressing the first\nlayer and the shallow blocks (i.e., the first 1/8 layers in a model) on 3 LLaMA models.\nTable 1 shows how the compressed shallow layers impact the model performance, taking the baseline\nfull-cache model and our method as reference. The results indicate that compressing only the first\nlayer can lead to a performance decline, with reductions ranging from minimal to moderate. For\ninstance, the LLaMA-3-70B gives a 7.0% decrease, while the LLaMA-3-Instruct-8b shows a more\nsubstantial drop of 10.8%. When compressing the shallow blocks, the impact is more pronounced.\nThe LLaMA-3-Instruct-8B suffers a 16.6% reduction. Notably, the LLaMA-3-Instruct-70b model\nshows a drastic 68.0% decline, highlighting a significant sensitivity to shallow layer compression.\nThese findings underscore the importance of careful layer selection in compression strategies and\nvalidate the effectiveness of our progressive compression method, as the choice of layer to compress\ncan have a substantial impact on model performance, particularly in larger or more complex models."}, {"title": "6.7 Memory footprint reduction analysis", "content": "Table 2: Summary of Model Sizes, KV cache usage and performance drop. Experiments were\nconducted with a batch size of 64 and a sequence length of 2048 for all models.\nWe report the memory footprint reduction in Table 2. By controlling the performance drop averaged\non the four tasks less than 1%, we can achieve a considerable compression ratio from 55%-60%.\nFor the LLaMA-3 models in which the GQA has already been employed to save the KV cache, we\nfurther achieve a significant compression ratio. Note that we have excluded the GSM8k results for\nthe performance drop calculation for a fair comparison."}, {"title": "7 Conclusions", "content": "In conclusion, we proposed LORC, a novel approach to KV cache compression that capitalizes\non the inherent low-rank properties of weight matrices. Our method employs a progressive layer-\nwise compression strategy, implementing a post-hoc low-rank approximation to circumvent the\ncomplexities and limitations associated with token-level eviction strategies and model retraining.\nMoreover, we provide theoretical analysis, deriving error bounds for layer compression and error\npropagation in deep networks, supporting our design of progressive compression strategy. This\ntheoretically grounded and universally applicable approach preserves model integrity and performance\nacross diverse tasks, attention mechanisms, and model scales. Our comprehensive experimental\nresults demonstrate that LORC significantly reduces GPU memory requirements while minimally\nimpacting performance. This approach offers a robust and efficient solution of KV cache compression,\nwithout requiring attention pattern analysis or model tuning."}, {"title": "A Detailed Proofs", "content": null}, {"title": "A.1 Proof of Theorem 1", "content": "Proof.\nLet W = UDVT be the full SVD of W", "by": "nW = Uk2kV", "have": "n||Wx \u2013 Wx||2 = ||(W \u2013 W)x||2\n= ||U(\u03a3 \u2013 \u03a3\u03ba)Vx||2\n= ||(\u03a3 \u2013 \u03a3\u03ba)Vx||2, since U is orthogonal\n= || diag(0, ..., 0, 0k+1,...,on)V\u0f0bx||2\n< \u03c3\u03ba+1||VTx||2\n= k+1||"}]}