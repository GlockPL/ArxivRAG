{"title": "VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks", "authors": ["Shiduo Zhang", "Zhe Xu", "Peiju Liu", "Xiaopeng Yu", "Yuan Li", "Qinghui Gao", "Zhaoye Fei", "Zhangyue Yin", "Zuxuan Wu", "Yu-Gang Jiang", "Xipeng Qiu"], "abstract": "General-purposed embodied agents are designed to understand the users' natural instructions or intentions and act precisely to complete universal tasks. Recently, methods based on foundation models especially Vision-Language-Action models (VLAs) have shown a substantial potential to solve language-conditioned manipulation (LCM) tasks well. However, existing benchmarks do not adequately meet the needs of VLAs and relative algorithms. To better define such general-purpose tasks in the context of LLMs and advance the research in VLAs, we present VLABench, an open-source benchmark for evaluating universal LCM task learning. VLABench provides 100 carefully designed categories of tasks, with strong randomization in each category of task and a total of 2000+ objects. VLABench stands out from previous benchmarks in four key aspects: 1) tasks requiring world knowledge and common sense transfer, 2) natural language instructions with implicit human intentions rather than templates, 3) long-horizon tasks demanding multi-step reasoning, and 4) evaluation of both action policies and language model capabilities. The benchmark assesses multiple competencies including understanding of mesh&texture, spatial relationship, semantic instruction, physical laws, knowledge transfer and reasoning, etc. To support the downstream finetuning, we provide high-quality", "sections": [{"title": "1. Introduction", "content": "Language-conditioned manipulation represents a fundamental challenge in embodied AI and a stepping stone toward Artificial General Intelligence [1, 4, 13]. Such tasks require agents to master multiple capabilities: interpreting natural language instructions, understanding complex environments, making decisions, formulating plans, and executing precise actions. The rapid advancement of Large Language Models (LLMs) and Vision-Language Models (VLMs) [1, 14] has revolutionized the field with their impressive general abilities in semantic understanding, coding, planning, and reasoning. The strong generalization capabilities has inspired two main approaches in language-conditioned manipulation: pre-training vision-language-action models using large-scale robotics data, as demonstrated by RT-2 and Palm-E [4, 13, 46], and integrating foundation models into agent workflows, like Vox-Poser and Copa [22, 23], which combine LLM/VLM outputs with grasp prediction [15, 16] and motion planning algorithms [25].\nWhile real-world robotics experiments provide valuable insights, their complexity and environmental variability often challenge reproducibility. Simulation-based evaluation has emerged as a fair and practical alternative. Existing benchmarks like RLBench, Calvin, and LIBERO [24, 35, 41] offer diverse task sets but fall short in addressing the unique requirements of foundation model-based methods. Tasks designed to align with the capabilities of foundation model-based algorithms should encompass nuanced semantic understanding of user intent, the integration of common-sense knowledge, and a robust ability to interpret diverse visual scenes, as well as require sophisticated multi-step reasoning. Such tasks demand a sophisticated integration of multimodal understanding to effectively interpret and respond to complex, real-world contexts. For example, one of the tasks in RT-2 [4] is \"move the Coke can to Taylor Swift\", while another task in CoPA [22] is \"make me a cup of pour-over coffee\u201d. The first task challenges the robot to use common sense to identify Taylor, a capability of knowledge transfer that previous policies struggled to achieve. The second task further intensifies the difficulty, requiring the robot to decompose the task into sub-tasks and execute the steps to operate a coffee machine-a long-horizon challenge that has previously been difficult for a single policy to accomplish.\nTo better define the types of language-conditioned ma-"}, {"title": "nipulation tasks suited for foundation models and provide a standardized evaluation suite to advance robotics re- search, we introduce VLABench. VLABench is an open-source benchmark specifically designed for methods utilizing foundation models. The tasks in VLABench are carefully divided into several dimensions to evaluate models across various aspects, including 1) Mastery of common sense and world knowledge, 2) Understanding of mesh and texture, 3) Comprehension of semantically rich instructions, 4) Spatial understanding, 5) Grasp of physical rules, and 6) Reasoning ability. For benchmarking pur- poses, VLABench offers 100 task categories with comprehensive evaluations across various methods. With a diverse collection of over 2,000 3D objects and scenes, VLABench creates a wide range of visual contexts and tasks. It enables the assessment of generalization capabilities through learning across multiple skills, providing thorough evaluations spanning visual, linguistic, planning, knowledge transfer, and action dimensions.\nTo ensure fair comparison and evaluation, we develop an automated data collection framework to construct standardized datasets for each task, supporting model training and fine-tuning. Using this dataset, we conduct extensive experiments to evaluate and benchmark three distinct types of approaches: pre-trained VLA, workflows integrating foundation models, and vision-language models (VLMs). The experimental results indicate that existing VLA methods perform poorly on our tasks and don't exhibit the level of generalization abilities or the \u201cemergent\u201d phenomena observed in large models [58]. We summarize contributions as follows:", "content": null}, {"title": "2. Related Works", "content": "Benchmarks and Datasets. Numerous benchmarks such as RLBench and LIBERO [24, 32, 35, 41, 63] have been proposed to evaluate language-conditioned manipulation policies in realistic physical settings. A comparison of these benchmarks is provided in Table 1. However, most of these focus on skill learning and fail to sufficiently address long-horizon planning capabilities. Meanwhile, some benchmarks [49, 52, 60] address room-scale mobile manipulation tasks that require long-term memory or reasoning. Yet, these interactions typically occur through interfaces, rather than direct physical manipulation, limiting the transferability of learned policies to real-world scenarios. Additionally, while efforts [19, 29, 35, 43] have made strides in task format, difficulty, and scale, these benchmarks have largely overlooked the guiding role of language in tasks, often relying on template instructions that explicitly specify the robot's actions. VLABench is the first to introduce features such as natural human interaction, implicit goal-oriented semantics, and requirements based on common sense into robot manipulation tasks, as shown in Figure 2. In terms of generalization evaluation, previous works [24, 35, 41] typically assess models at the instance level within the same category, which limits their ability to evaluate generalization across diverse object categories or different tasks within the same skill set. In contrast, VLABench is the first benchmark to evaluate generalization capabilities across a wide range of tasks, object types, and task categories, providing a more comprehensive assessment of model versatility.\nLarge-scale datasets have been built in both real and simulation [5, 43, 46, 57] for large-scale imitation learning for manipulation. However, real-world data faces challenges related to scalability, making it difficult to gather sufficient data at scale [3]. Simulated datasets, while more scalable, often suffer from limited diversity in scenarios and tasks [24, 41], and still require teleoperation [18, 35] for data collection. VLABench addresses these limitations by offering a broader range of tasks that are more closely aligned with real-world conditions, covering diverse aspects of vision, language, tasks, and skills. Furthermore, it introduces an efficient and robust process for the automated generation of simulated data, significantly enhancing task diversity and scalability.\nPretrained Vision-Language-Action Models. The recent rise of multimodal models [1, 12, 37, 62] and the collection and organization of operational datasets [45, 57], have led to the integration of vision-language-action models (VLAs) [3, 4, 13, 27] into language-conditioned manipulation tasks. While the term VLA generally refers to models that combine visual and language inputs for policy learning, we focus specifically on approaches leveraging pre-trained models. Several works [3, 13, 27] have applied further training to pre-trained vision-language models (VLMs) for language-conditioned manipulation. These models demonstrate impressive generalization to unseen objects and tasks, yet their control precision is somewhat limited by the discretization of actions [47]. To address this limitation, some approaches have explored using diffusion models [8, 47] as policy networks or using diffusion decoders [31, 59]. Pre-trained models based on diffusion models [34, 40] have shown promising advancements in improving continuous space distribution learning. VLABench includes a selection of these representative methods for comprehensive evaluation.\nFramework Utilizing Foundation Models. Pre-trained language models [5, 14, 50] and vision-language models[1, 39] have demonstrated strong generalization and versatility. Some researchers [22, 23, 36] combine the general perception and cognitive abilities of these pre-trained models with traditional planning and control algorithms to create agent workflows. These frameworks allow robots to perform complex zero-shot manipulation tasks without requiring additional training. To harness the capabilities of foundation models for manipulation, some works [23, 33] utilize the code comprehension and generation abilities of large"}, {"title": "3. VLABench", "content": null}, {"title": "3.1. Task Description", "content": "VLABench is composed of 60 primitive and 40 composite tasks, categorized by task difficulty and required timesteps. These tasks are designed to encompass a rich variety of skills while covering ample visual and language semantic information. For skill learning, 100 tasks in VLABench cover a wide range including 1) Pick&place, 2) Open&close door, 3) Open&close drawer, 4) Hang objects on the wall, 5) Use tool e.g. Hammer nail, 6) Press button, 7) Insert, 8) Pour, 9) Twist, and 10) Explore. In addition, VLABench places greater emphasis on real-life scenarios and essential daily tasks, representing more interactive language instructions, a wider variety of task settings, the integration of common sense and societal knowledge, and long-horizon tasks requiring logical planning, as shown in Figure 3. Notably, VLABench adopts a stricter definition of task generalization, which will be elaborated on in the Section 3.2. The whole task list can be found in supplementary material.\nPrimitive Tasks. Primitive tasks are divided into five dimensions, each corresponding to the assessment of a specific ability dimension.\nMesh&Texture Understanding. This type of task requires the model to recognize different meshes and understand various texture features. Take the SelectToy task shown in Figure 3 (a) as an example, the robot is directly required to place a specific toy e.g. Aquaman into a receptacle. The model must possess strong visual capabilities to accurately recognize such complex meshes and textures.\nSpatial Understanding. Spatial understanding tasks involve various spatial relationships, such as the nth left-/right position, inside/outside of a receptacle, the mth row and nth column, near/far, and beside a specific object, representing relative positional relationships. Figure 3 (b) shows one case in task in PullBook. Such a complex relative positional relationship imposes extremely high demands on the model's multimodal understanding [7].\nCommon Sense & World Knowledge. Tasks relative to common sense/world knowledge require the agent to transfer the knowledge gained in the pre-train stage to solve the problem. The task shown in Figure 3 (c) requires the agent not only to recognize different types of flowers from visual information but also to leverage world knowledge to determine that \"the tulip is the national flower of the Netherlands\".\nSemantic Understanding. This type of task emphasizes the complexity, subtlety, and natural interactivity of language instructions. Task objectives are often implicitly conveyed through a natural conversation. To perform well in GetDrink task as Figure 3 (d) shown, the agent must capture the implied request from a lengthy instruction: to take out a chilled cola from the refrigerator.\nPhysical Law. This type of task expects the robot to integrate visual information and take correct actions based on physical principles and real-time observation. In UseSeesaw task in Figure 3 (e), the robot is commanded to grasp an object that can not be achieved directly. The agent must recognize the need to apply the principle of leverage by using sufficient weight to lift the target object on the other end.\nComposite Tasks. Composite tasks in VLABench involve the combination of multiple skills, long-term task planning, and multi-step logical reasoning from instructions, scenes, and even game rules. Figure 3 (f) showcases a variety of challenging complex tasks. Composite tasks have a significantly longer trajectory horizon, with an average episode length exceeding 500 timesteps-considerably more than the average of 120 timesteps for primitive tasks. In Figure 3 (f1), The agent must not only correctly identify all poker cards from visual information and use world knowledge of poker rules to select the best hand, but also flip face-down cards to acquire complete information. This type of task,"}, {"title": "3.2. Benchmark", "content": "Evaluation. VLABench organizes evaluations into three main categories: assessments of pretrained or fine-tuned vision-language-action (VLA) models, heuristic workflows that integrate foundation models with various algorithms, and multi-dimensional evaluations of vision-language models (VLMs).\nGeneralization Ability of VLAs. For trained vision-language-action (VLA) models, the evaluation in VLABench includes two settings: seen objects and unseen objects. The seen objects evaluation closely aligns with the data distribution of the training set, primarily testing the model's skill acquisition. Meanwhile, the unseen objects evaluation presents a greater challenge, requiring the model to exhibit strong generalization capabilities. Unlike previous benchmarks [24, 41], VLABench defines unseen objects as entirely different categories. For instance, in the PickFruit task, target objects for seen evaluation include apples, bananas, pears, and oranges, while unseen objects include kiwis, mangos, strawberries, lemons, and other distinct fruits. This setup requires the model to demonstrate not only strong visual generalization capabilities but also to handle the vastly differing common-sense knowledge associated with different categories of objects, as well as the challenge of processing lengthy instructions with unfamiliar tokens.\nZero-shot Transfer Ability of Heuristic Workflow. Training-free workflow methods are evaluated under a single setting but in many ability dimensions. Apart from the capability points for primitive tasks mentioned in Section 3.1, we extend the evaluation to cover various skills and long-horizon tasks to assess the overall capability and execution robustness of the workflow.\nComprehensive Evaluation of VLMs' Capabilities. Similar to heuristic workflows, the evaluation of VLMs is also comprehensive. Since VLMs lack intrinsic action capabilities, we organized a skill library and integrated it into a domain-specific language (DSL) [44, 51], leveraging annotated asset information as prior knowledge. This DSL functions as a straightforward API that VLMs can call, enabling efficient interaction. The whole evaluation pipeline will be discussed in Section 4.3.\nMetric. Our evaluation focuses on generalization capabilities, but the task success metric, limited to a 0/1 score, is better suited for assessing straightforward skill learning."}, {"title": "3.3. Simulation", "content": "Simulator. VLABench is built based on Mujoco[55] and its control suite dm_control[56]. We selected Mujoco as the core simulation platform for our benchmark due to its lightweight design, high performance, and exceptional physical realism. These advances enable convenient, rapid evaluation of diverse algorithms. The VLABench framework is highly modular, meaning various object entities can be flexibly combined to create large-scale and diverse tasks and scenarios.\nAssets. To meet the requirements of diverse tasks and capability assessments, we built an asset library centered around multiple task themes. We inherited some annotated assets from Robocasa [43] and retrieved numerous 3D models from Objaverse [11]. For novel tasks, such as the series of tasks we created around the toy theme, we carefully gathered a variety of high-quality character models from online 3D model sites. These models were then converted to MJCF format using the obj2mjcf [61] tool. Similarly to previous work [29, 43], we expanded the dataset of common simple objects using generative AI models. Specifically, we utilized Tripo.al's text-to-3D and image-to-3D features to construct additional 3D objects, and Runaway.ai to generate multiple material textures. Ultimately, the asset library we constructed contains 163 categories of objects, totaling 2164 items. Most of the assets are listed in Section 7.3.\nRobots. To ensure versatility and broad applicability, we integrated a range of embodiment types. These include, but are not limited to, various models of 6-axis and 7-axis robotic arms, dual-arm robots, and humanoid robots. In the standard evaluation process, VLABench employs a 7-DoF Franka Emika Panda manipulator equipped with a parallel gripper. We represent the position and orientation of the robot's end-effector in Euclidean space R\u00b3 using 3D coordinates for position and quaternions for orientation. Using"}, {"title": "inverse kinematics, we then resolve these end-effector poses into the corresponding rotational angles for the seven joints.", "content": null}, {"title": "3.4. Dataset Construction", "content": "Domain Randomization. To ensure data diversity and richness, we implemented various types of domain randomization. These randomizations include object position and orientation, mesh scale, scene layout, background and object textures (such as walls, floors, and tabletops), as well as lighting parameters. Details can be found in Section 6.3.\nTrajectory Generation. As human teleoperation is time-consuming and not scalable [35, 43], we developed an efficient, scalable automated data collection pipeline based on our custom skill library. Inspired by [18], our data collection framework leverages the prior information including point clouds of the environment, entities' grasp-points, target entity at the current step, etc. The data collection framework includes multiple task-specific motion planners. These motion planners call upon the skills in the skill library based on the current task progress and determine parameters by incorporating prior information. Subsequently, the selected skills generate trajectories using RRT [26], with quaternion interpolation achieved through Spherical Linear Interpolation (SLERP). The final trajectory is smoothed using a Bezier curve to optimize path quality. To enhance sample efficiency during data collection, reject sampling and failure-triggered early termination are applied.\nInstruction Augmentation. We use GPT-4 [1] to generate descriptions that incorporate target-specific characteristics and interactive instructions that encompass a variety of contexts and intentions. The supplementary material provides details on the generation process and the complete prompts."}, {"title": "4. Experiments", "content": "Following Section 3.2, we conducted experiments centered on pre-trained VLA models, workflows incorporating multiple algorithmic modules, and various VLMs. The remainder of this section provides a detailed description of the experimental setup."}, {"title": "4.1. Generalization Ability of VLAS", "content": "Pretrained VLAs are expected to possess robust generalization and versatility similar to LLMs. Experiments about are set to address the following research questions:\nQ1: Do pre-trained VLAs exhibit stronger general abilities with unseen categories of objects?\nQ2: Can pre-trained VLAs transfer their general knowledge and behavioral abilities to similar but unseen tasks?\nQ3: Can pre-trained VLAs understand natural user interactions and implicit goal requirements?\nQ4: Do pre-trained VLAs have the potential to transfer their world knowledge to related tasks?"}, {"title": "Thus, we introduce Progress Score (PS) as a graduated metric for more nuanced assessment. The computation equa- tion of PS is:", "content": null}, {"title": "PS = \\alpha\\cdot\\frac{N_{\\text{correct}}}{N} + (1 - \\alpha)\\cdot\\frac{M_{\\text{done}}}{M}", "content": null}, {"title": "Q5: Can existing VLA architectures accurately support the completion of long-horizon tasks?", "content": "Experiment Setup. To investigate the questions outlined above, we fine-tuned various pre-trained VLA architectures, including OpenVLA, Octo, and RDT-1B [27, 40, 54], on our high-quality dataset. Our composite tasks demand generalization across language, vision, common sense, and long-horizon reasoning, requiring the integration of multiple skills. To assess generalization ability, we selected primitive tasks as the foundation for evaluation. Within each category of primitive tasks, the Mesh&Texture (base) tasks, Common sense & World knowledge tasks, and Semantic tasks share similar task setups and trajectories. Therefore, we opt for joint training on base and common sense data across each task category and evaluate in different settings. During the fine-tuning stage, we sample 100 trajectories from each task category, resulting in a total of 1,600 trajectories to ensure balanced representation across tasks. For complex tasks, we perform fine-tuning separately within the domain of each task and conduct evaluations independently.\nResult and Analysis. In the evaluation stage, different task settings are applied to cover multiple generalization abilities. In Table 2a, we present experimental results comparing the generalization capabilities of vision and common sense by evaluating seen and unseen categories of objects. The experimental results indicate that the current large-scale pre-trained VLAs did not exhibit the expected rapid adaptation to downstream tasks. The fine-tuned models performed poorly in primitive tasks especially involving the Pick&Place skill, the findings are similar to [43]. Limited by its discretization process and single-frame input architecture, OpenVLA's skill-learning capability is lower than that of RDT-1B. However, benefiting from pre-trained VLMs, OpenVLA achieves higher scores than RDT-1B on common-sense tasks involving unseen objects. Our analysis suggests that although OpenVLA only fits trajectory data during pre-training, its foundation on Llama2-7B provides it with greater generalization potential.\nIn Table 2b, 2c, and 2d, evaluations were conducted on out-of-domain semantically rich language, unseen but similar tasks, and composite tasks respectively. These experimental results indicate that current architectures and pre-training approaches are insufficient for equipping VLA models with stronger semantic understanding, skill transfer, and long-horizon planning capabilities. Analogous to the classic paradigm of pretraining-finetuning in large language models during the GPT-3 era [48], it is still difficult to determine how much gain VLA has achieved from pre-training on the scarce, quality-varying dataset of only a few million samples. Moreover, this becomes even more challenging if the backbone has already undergone large-scale vision-language training. Drawing an analogy to the development trajectory of large language models, the present state of VLAs is still far from reaching a level comparable to GPT-2. Further ablation studies and analysis are presented in Section 10.1."}, {"title": "4.2. Performance of Workflow Utilizing Foundation Model", "content": "For our evaluation of foundation model-based algorithms, we reviewed two state-of-the-art frameworks, Vox-poser [23] and CoPA [22], and the comparison results are shown in Figure 4. Given Voxposer's dependence on large language models (LLMs), we assessed Voxposer's performance with and without visual perception capabilities. While Voxposer performed adequately on basic tasks and achieved the Progress Scores of 30-40, its reliance on LLM-driven motion planning often led to grasping failures due to limited information for effective grasp planning, especially when interpreting rotation in non-visual contexts, resulting in low overall scores.\nInterestingly, the foundational LLM alone maintained"}, {"title": "4.3. Comprehensive Ability of VLMs", "content": "We referred to the evaluation results of multiple series of Vision-Language Models (VLMs) provided by Open-Compass [10] and selected several models from different families with strong overall performance. These models include: GPT-4-turbo-2024-0409, GPT-40-2024-08-06 [1], GLM-4V-9B [17], MiniCPM-V2.6 [20], Qwen2-VL-7B [2], InterVL2-8B [6], and LLaVA-NeXT [38]. We evaluate the comprehensive performance of these models with the dataset derived from naturally self-contained information within a simulated environment. This dataset consists"}, {"title": "Thus, we introduce Progress Score (PS) as a graduated metric for more nuanced assessment. The computation equa- tion of PS is:", "content": null}, {"title": "PS = \\alpha \\cdot \\frac{Ncorrect}{N} + (1 - \\alpha) \\cdot \\frac{Mdone}{M}", "content": null}, {"title": "The computation equation of PS is:", "content": null}, {"title": "PM = \\frac{|Nodematched |}{Nodettotal}", "content": null}, {"title": "Score = w1\\cdot SR +w2PR+w3\\cdot SPR+w4\\cdot PM", "content": null}, {"title": "10. Detailed Analysis and Case Study", "content": null}, {"title": "10.1. Ablations and Analysis for VLAs", "content": "Experimental results show that the current open-source VLAs perform poorly on our tasks. On one hand, this can be attributed to the high difficulty of VLABench tasks, which impose stringent requirements on the generalization capabilities of the models. More importantly, the limitations and deficiencies in both the architecture and pretraining process of current VLAs make it challenging for them to adapt effectively to downstream tasks after large-scale pretraining, especially under fine-tuning scenarios with diverse data distributions. This stands in stark contrast to LLMs, which"}, {"title": "Skill Recall Rate (SR). We use SR as the coarsest-grained metric to evaluate the model's capability to identify and in- voke the correct skills.", "content": null}, {"title": "SR = \\frac{|SLgt \\cap SLpred|}{|SLgt|}", "content": null}, {"title": "10.2. Further Analysis for Workflows", "content": "From the experimental results, we observe that while the framework algorithm based on the foundation model demonstrates some degree of robustness in handling complex semantic settings, the overall success rate and PS score remain relatively low. A comprehensive analysis of the failure cases reveals that the underlying issues can be broadly categorized into the following groups.\nPerception. One of the primary challenges lies in the model's image and spatial perception capabilities. As Vox-poser is implemented as a purely text-based framework, its perception module relies directly on the ground-truth labels of all items, which are provided as input for selection. While this leverages the comprehension and generalization capabilities of large language models to understand tasks, it exposes significant limitations in scenarios that require spatial perception and image-based reasoning. Specifically, Voxposer demonstrates clear incompetence in handling tasks involving spatial awareness or detailed image descriptions.\nTo address this, we augmented our experimental setup by incorporating an image perception module into Voxposer. Although this adjustment improved success rates on spatial perception tasks, the overall performance deteriorated due to errors introduced by the visual perception module. A"}, {"title": "10.3. Ablations and Analysis for VLMs", "content": "In our evaluation of VLMs, we conducted two key ex-periments to explore the impact of Chain-of-Thought (CoT) prompting and few-shot learning on model performance.\nEffect of CoT Prompting. Our investigation into the use of CoT prompting revealed a notable improvement in over- all performance for the InternVL2 model, as shown in Fig- ure 16. Similarly, LLaVA-NeXT and Qwen2-VL demon- strated enhanced performance in challenging tasks, particu- larly those requiring reasoning about complex scenarios and physics laws. However, their performance on semantically common-sense tasks remained stagnant or experienced mi- nor degradation. In contrast, the MiniCPM model exhib- ited significant limitations: it failed to output answers at the conclusion of the reasoning process when CoT was applied, resulting in all scores dropping to 0.0.\nEffect of Few-Shot Learning. As shown in Figure 17 our"}]}