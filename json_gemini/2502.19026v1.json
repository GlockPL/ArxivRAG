{"title": "InternVQA: Advancing Compressed Video Quality Assessment with Distilling Large Foundation Model", "authors": ["Fengbin Guan", "Zihao Yu", "Yiting Lu", "Xin Li", "Zhibo Chen"], "abstract": "Video quality assessment tasks rely heavily on the rich features required for video understanding, such as semantic information, texture, and temporal motion. The existing video foundational model, InternVideo2, has demonstrated strong potential in video understanding tasks due to its large parameter size and large-scale multimodal data pertaining. Building on this, we explored the transferability of InternVideo2 to video quality assessment under compression scenarios. To design a lightweight model suitable for this task, we proposed a distillation method to equip the smaller model with rich compression quality priors. Additionally, we examined the performance of different backbones during the distillation process. The results showed that, compared to other methods, our lightweight model distilled from InternVideo2 achieved excellent performance in compression video quality assessment.", "sections": [{"title": "I. INTRODUCTION", "content": "In video understanding tasks, critical information such as semantic content, texture details, and temporal motion features is equally essential for video quality assessment(VQA). For example, human perception of distortions is significantly influenced by semantic information[Guan et al., 2024], motion blur is closely tied to the temporal motion in videos[Wu et al., 2023, Yuan et al., 2023], and many distortions are reflected in the texture details [Zhang et al., 2023]. Therefore, we can leverage the strong representational power of foundational video models to enhance video quality assessment tasks.\nInternVideo2[Wang et al., 2024], a new family of video foundation models, has demonstrated exceptional capabilities across various video understanding tasks, including action recognition/detection, video-text tasks, and video-centric dialogue. This success is attributed to its massive parameter size, large-scale data support, and the application of masked video learning, which endows InternVideo2 with powerful representation abilities. These rich features effectively support various downstream video tasks.\nIn this study, we explore the potential of InternVideo2 for compressed video quality assessment. However, its substantial parameter count results in excessive resource consumption for these tasks. To address this, we developed a distillation method that retains the rich compression quality priors from the large model, creating a more efficient and lightweight model. This approach improves efficiency and reduces resource requirements without sacrificing performance. Additionally, we explored distilling the robust representations of InternVideo2 into various backbones, comparing homologous and heterogeneous distillation methods. Our experimental results demonstrate that, through homologous distillation, the smaller model not only surpasses existing methods on two compression quality assessment datasets but also matches or exceeds the performance of the original large model, achieving both high efficiency and superior performance.\nOur contributions are as follows:\n\u2022 We explored the transferability of the strong video representation capabilities of InternVideo2 to video quality assessment tasks. The experimental results indicate that these representations are well-suited for quality assessment, delivering exceptional performance.\n\u2022 To design a lightweight model tailored for compressed video quality assessment, we proposed a distillation method that extracts knowledge from InternVideo2, enabling a small model to maintain high performance while significantly reducing model size.\n\u2022 We investigated different backbones for distillation and found that homologous distillation outperforms heterogeneous distillation, exceeding the performance of existing video quality assessment methods and achieving an optimal balance between efficiency and performance."}, {"title": "II. RELATED WORK", "content": "In the field of video quality assessment, there are currently two main approaches: traditional methods based on handcrafted feature extraction and modern methods based on deep learning. Traditional methods[Mittal et al., 2015, Liao et al., 2022, Korhonen, 2019] rely on manually designed features to assess video quality; however, due to the limitations of handcrafted features, these methods typically only capture shallow quality representations and fail to account for the complex factors that influence video quality.\nWith the rapid advancement of deep learning, models[Sun et al., 2022, Wu et al., 2022, Lu et al., 2024b, Yu et al., 2024b,a, Lu et al., 2024a] based on this technology have demonstrated superior feature extraction capabilities in video quality assessment tasks. Notably, networks such as 3D-CNN[Tran et al., 2015] and Video Swin Transformers [Liu et al., 2022] are able to capture deep spatiotemporal features from video data. However, while these models have improved video quality assessment capabilities, they still struggle to fully capture the complex characteristics that affect video quality, making it difficult to meet the demands of comprehensive video quality assessment tasks.\nVideo foundation models are commonly used for video understanding tasks such as action recognition, typically leveraging convolutional [Tran et al., 2015] and attention mechanisms[Liu et al., 2022] to extract video features. UniFormer[Li et al., 2023] combines convolution and attention to reduce spatiotemporal redundancy and mitigate global dependency issues. Building on this, UniFormer2[Li et al., 2022] incorporates pre-trained ViTs to further capture rich image priors. InternVideo[Wang et al., 2022] enhances the understanding of complex video content by integrating UniFormer2 with video-language contrastive learning and video-masked modeling. InternVideo2[Wang et al., 2024] adopts a progressive training strategy, leveraging its massive parameter size and extensive data through masked distillation training, achieving superior performance."}, {"title": "III. METHOD", "content": "InternVideo2 is selected as the video foundational model for our video quality assessment tasks due to its exceptional capability in capturing rich and intricate features required for comprehensive video understanding. As a new family of video foundation models, InternVideo2 has demonstrated strong performance across a variety of video-level tasks, including action recognition, detection, video-text matching, and video-centric dialogue. These tasks require deep semantic understanding, fine-grained texture analysis, and temporal motion tracking, all of which are crucial for both video comprehension and quality assessment.\nThe strength of InternVideo2 lies in its ability to process vast video data and use masked video modeling to learn highly informative representations. These representations are particularly beneficial for downstream tasks, encapsulating essential content, structure, and dynamics of the video. The model's large parameter size, combined with extensive training on diverse datasets, provides strong generalization across various tasks and scenarios, making it ideal for quality assessment.\nFor video quality assessment, the perceptual impact of distortions often hinges on certain key features. Semantic information plays a critical role in human perception of video quality, as viewers are generally more sensitive to distortions in areas containing meaningful content (e.g., faces, objects) compared to less significant regions. The ability of InternVideo2 to capture semantic content ensures that such crucial areas are well-represented, enabling the model to detect and assess quality degradations that are more noticeable to human viewers.\nIn addition to semantic content, texture features are essential for evaluating video quality, particularly in the context of compression artifacts and other distortions. Various forms of degradation, such as blocking, ringing, or blurring, directly impact the fine details of texture. InternVideo2 excels at capturing texture information, making it ideally suited for identifying these subtle distortions and thereby contributing to more accurate quality predictions.\nFurthermore, temporal motion features are essential in video understanding and play a critical role in evaluating quality distortions. Motion-related artifacts, such as motion blur or frame drops, substantially impact video quality by disrupting temporal continuity. InternVideo2 demonstrates a strong capacity to capture temporal dynamics, enabling accurate representation and detection of these motion-related distortions, which are frequently disregarded by models focused exclusively on spatial quality.\nLeveraging the capacity of InternVideo2 to capture rich features-semantic, texture, and temporal motion-this approach addresses the unique challenges of video quality assessment. The powerful representations of InternVideo2 provide a strong foundation for building a robust VQA model, ensuring it generalizes across various distortions while remaining sensitive to the nuanced ways these distortions impact perceived quality."}, {"title": "B. Knowledge Distillation Framework for Compression Video Quality Assessment", "content": "In this study, we apply knowledge distillation[Gou et al., 2021] to transfer the powerful representation capabilities of the teacher model, InternVideo2-1B, to a lightweight student model to address the resource consumption challenges in compression video quality assessment. The core idea of knowledge distillation is to leverage the large teacher model to guide the learning process of the smaller student model, enabling the student to maintain efficiency while acquiring similar feature representation capabilities, especially those critical for compression quality assessment.\nTo ensure that the student model effectively learns the rich features related to compression quality from the teacher model, we designed a dual-loss mechanism for the distillation process. The total loss consists of three components: (1) L2 Loss for the teacher model, which supervises the error between the predicted output of the teacher model and the ground truth; (2) L2 Loss for the student model, which ensures that the predictions of the student model align with the ground truth; and (3) Smooth L1 Loss, which aligns the feature representations between the teacher model and the student model. The overall loss function is:\n\n\\(L_{total} = L_{teacher} + L_{student} + L_{Smooth L1}\\).\n\nThis dual-loss design enables the student model to learn from both the ground truth and the feature space of the teacher model, making it more robust in handling compression distortions in video quality assessment.\nBuilding on the design of the above distillation method, we implemented distillation in two ways:\n1) Homologous Distillation: In homologous distillation, we selected a student model with a structure similar to that of the teacher model, maintaining architectural consistency between the two. Specifically, the teacher model is InternVideo2-1B, while the student model is the lightweight InternVideo2-Small/Base, both of which are based on the ViT (Vision Transformer) architecture. By reducing the number of parameters and layers in the student model, we effectively lower the model complexity while preserving the key feature extraction capabilities of the original architecture.\nAs shown in Figure 1, in the homologous distillation process, we train only the last ViT block and head of the teacher model, freezing the rest of the model. In contrast, the entire student model is trainable. During distillation, the feature outputs of the final layer from both the teacher and student models are aligned using Smooth L1 Loss, ensuring that the student model can effectively learn the high-level representations from the teacher model. Additionally, we use ground truth to supervise the final outputs of both models to ensure prediction accuracy. Under this homologous distillation framework, the student model successfully inherits the compression quality priors from the teacher model, particularly excelling in capturing the semantic, texture, and temporal motion features essential for video quality perception.\n2) Heterogeneous Distillation: In heterogeneous distillation, we adopted a student model with a different architecture from that of the teacher model. Specifically, the teacher model remains InternVideo2-1B, while the student model is FastVQA, with a backbone based on 3D Swin Transformer. Although FastVQA is a lightweight model that performs well in video quality assessment tasks, the significant architectural differences between it and the teacher model make the feature transfer process more complex.\nSimilar to homologous distillation, we use Smooth L1 Loss to align the final-layer feature outputs between the teacher and student models, ensuring that the student model inherits important compression quality priors from the teacher model. However, due to the substantial architectural differences, the student model faces greater challenges in feature learning and transfer, especially when dealing with high-frequency texture details and temporal motion features related to compression distortions.\nThrough these two distillation strategies, we explore the performance of homologous and heterogeneous architectures in the knowledge distillation process, analyzing how different distillation approaches influence the representation capabilities of the student model without altering the teacher model."}, {"title": "IV. EXPERIMENTS", "content": "In this study, we utilized two compression video quality assessment datasets: the BVI-HD dataset [Zhang et al., 2018] and the Waterloo IVC 4K Video Quality Database [Li et al., 2019].\nThe BVI-HD dataset includes 32 reference videos and 384 distorted sequences, covering 12 types of distortions generated using HEVC and HEVC with synthesis mode (HEVC-SYNTH), offering a foundation for studying video quality under these compression methods.\nThe Waterloo IVC 4K Video Quality Database consists of 20 pristine 4K videos encoded using five different encoders (HEVC, H264, VP9, AV1, AVS2) at three resolutions (960x540, 1920x1080, 3840x2160). Each resolution has four distortion levels, yielding 1200 videos in total, providing a rich resource for evaluating compression performance across different encoders and resolutions.\nThese two datasets offer diverse types of compression distortions and various resolution settings, enabling a comprehensive evaluation of the video quality assessment performance of our model across different compression scenarios.\nIn this study, we use Pearson Linear Correlation Coefficient (PLCC)[Sedgwick, 2012] and Spearman's Rank Correlation Coefficient (SRCC)[Sedgwick, 2014] as evaluation metrics, both of which have a range between 0 and 1. A value nearer to 1 indicates a higher correlation between the predicted outcomes and the actual ground truth.\nOur experimental approach closely follows the training strategy outlined in InternVideo2[Wang et al., 2024]. The input videos are randomly cropped into patches of 8\u00d7224\u00d7224 resolution. We employed InternVideo2-1B as the teacher model, which has an embedding dimension of 1408 and a depth of 40 ViT blocks. For the student models, we used InternVideo2-Small/Base, where the depth of their ViT blocks is 12. Specifically, the embedding dimensions for the Small model and Base model are 384 and 768, respectively. The training process utilized pre-trained weights from K400, with a batch size of 8. All experiments were conducted using DeepSpeed on two A100 GPUs."}, {"title": "B. Experimental Performance Comparison and Analysis", "content": "To evaluate the capability of InternVideo2 in compression video quality assessment, we conducted experiments on the two aforementioned datasets, with the results presented in TableI. InternVideo2-1B, leveraging its strong video quality representation ability, outperformed other existing VQA methods, achieving the best performance. In contrast, the performance of the InternVideo2-Small/Base models was not as competitive. To develop a lightweight and high-performance model for compression quality assessment, we applied knowledge distillation to transfer the representation capabilities of InternVideo2-1B to the smaller models.\nThe experimental results demonstrate that the designed distillation method significantly enhances the performance of the student models, leading to an efficient and high-performing compression quality assessment model. Specifically, under homologous distillation, the student models (InternVideo2-Small/Base), which share the same ViT architecture as the teacher model (InternVideo2-1B), were better able to inherit the feature extraction abilities of the teacher model. This was particularly evident in handling complex video distortions, where the student models performed comparably to, and in some cases even surpassed, the teacher model. The student models effectively balanced computational efficiency with performance.\nIn contrast, although heterogeneous distillation enabled a certain degree of model lightweighting, the structural differences between the 3D Swin backbone of FastVQA and the ViT architecture of InternVideo2-1B presented challenges for the student model to fully replicate the feature extraction capabilities of the teacher model. This limitation was particularly evident in the processing of temporal motion information and high-frequency texture details, where the performance of FastVQA was noticeably inferior to that of the student models trained through homologous distillation. The performance gap indicates that, while heterogeneous distillation offers some architectural flexibility, it proves less effective than homologous distillation for the task of compressed video quality assessment."}, {"title": "V. CONCLUSION", "content": "In this study, we demonstrated the substantial potential of InternVideo2 in the field of compression video quality assessment. By leveraging its rich video representation capabilities, we designed a distillation method to transfer knowledge from the large model to a smaller, more efficient model. Our experiments reveal that the lightweight model achieved exceptional performance, closely approaching or even surpassing the results of the original large model, especially through homologous distillation. This method effectively balances high performance with resource efficiency, offering a significant reduction in computational requirements while maintaining strong predictive accuracy. These findings underscore the effectiveness of distilling large foundational models for lightweight, high-performance compression video quality assessment."}]}