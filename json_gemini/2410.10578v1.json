{"title": "Burning RED: Unlocking Subtask-Driven Reinforcement Learning and Risk-Awareness in Average-Reward Markov Decision Processes", "authors": ["Juan Sebastian Rojas", "Chi-Guhn Lee"], "abstract": "Average-reward Markov decision processes (MDPs) provide a foundational framework for sequential decision-making under uncertainty. However, average-reward MDPs have remained largely unexplored in reinforcement learning (RL) settings, with the majority of RL-based efforts having been allocated to episodic and discounted MDPs. In this work, we study a unique structural property of average-reward MDPs and utilize it to introduce Reward-Extended Differential (or RED) reinforcement learning: a novel RL framework that can be used to effectively and efficiently solve various subtasks simultaneously in the average-reward setting. We introduce a family of RED learning algorithms for prediction and control, including proven-convergent algorithms for the tabular case. We then showcase the power of these algorithms by demonstrating how they can be used to learn a policy that optimizes, for the first time, the well-known conditional value-at-risk (CVaR) risk measure in a fully-online manner, without the use of an explicit bi-level optimization scheme or an augmented state-space.", "sections": [{"title": "1 Introduction", "content": "Markov decision processes (MDPs) [1] are a long-established framework for sequential decision-making under uncertainty. Episodic and discounted MDPs, which aim to optimize a sum of rewards over time, have enjoyed success in recent years when utilizing reinforcement learning (RL) solution methods [2] to tackle certain problems of interest in various domains. Despite this success however, these MDP-based methods have yet to be fully embraced in real-world applications due to the various intricacies and implications of real-world operation that often trump the ability of current state-of-the-art methods [3]. We therefore turn to the less-explored average-reward MDP, which aims to optimize the reward received per time-step, to see how its unique structural properties can be leveraged to tackle challenging problems that have evaded its episodic and discounted counterparts.\nIn particular, we focus our attention on one such problem where the average-reward MDP may offer structural advantages over episodic and discounted MDPs: risk-aware decision-making. More formally, a risk-aware (or risk-sensitive) MDP is an MDP which aims to optimize a risk-based measure instead of the typical (risk-neutral) expectation term. In this work, we show how leveraging the average-reward MDP allows us to overcome many of the computational challenges and non-trivialities that arise when performing risk-based optimization in episodic and discounted MDPs. In doing so, we arrive at a general-purpose, theoretically-sound framework that allows for a more subtask-driven approach to reinforcement learning, where various learning problems, or subtasks, are solved simultaneously to help solve a larger, central learning problem.\nMore formally, we introduce Reward-Extended Differential (or RED) reinforcement learning: a first-of-its-kind RL framework that makes it possible to effectively and efficiently solve various subtasks (or subgoals) simultaneously in the average-reward setting. At the heart of this framework"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Average-reward reinforcement learning", "content": "Average-reward (or average-cost) MDPs were first studied in works such as [1]. Since then, there have been various, albeit limited, works that have explored average-reward MDPs in the context of RL (see [6, 7] for detailed literature reviews on average-reward RL). Recently, Wan et al. [8] provided a rigorous theoretical treatment of average-reward MDPs in the context of RL, and proposed the proven-convergent \u2018Differential Q-learning' and (off-policy) 'Differential TD-learning' algorithms for the tabular case. Our work primarily builds off of Wan et al., and we utilize their proof technique when formulating our own proofs for the convergence of our algorithms. To the best of our knowledge, our work is the first to explore solving subtasks simultaneously in the average-reward setting."}, {"title": "2.2 Risk-aware learning and optimization in MDPs", "content": "The notion of risk-aware learning and optimization in MDP-based settings has been long-studied, from the well-established expected utility framework [9], to the more contemporary framework of coherent risk measures [10]. To date, these risk-based efforts have almost exclusively focused on the episodic and discounted settings. This includes notable works such as [11] and [12], which, similar to the work presented in this paper, aim to optimize the CVaR risk measure (see [13, 14] for detailed literature reviews). In the average-reward setting, [15] recently proposed a set of algorithms for optimizing the CVaR risk measure, however their methods require the use of an augmented state-space and a sensitivity-based bi-level optimization. By contrast, our work, to the best of our knowledge, is the first to optimize the CVaR risk measure in an MDP-based setting without the use of an explicit bi-level optimization scheme or an augmented state-space."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Average-reward reinforcement learning", "content": "A finite average-reward MDP is the tuple M = (S, A, R, p), where S is a finite set of states, A is a finite set of actions, R \u2282 R is a finite set of rewards, and p : S\u00d7A\u00d7R\u00d7 S \u2192 [0, 1] is a probabilistic transition function that describes the dynamics of the environment. At each discreet"}, {"title": "3.2 Conditional value-at-risk (CVaR)", "content": "Consider a random variable X with a finite mean on a probability space (\u03a9, F, P), and with a cumulative distribution function F(x) = P(X \u2264 x). The (left-tail) value-at-risk (VaR) of X with parameter \u03c4 \u2208 (0,1) represents the 7-quantile of X, such that VaR_\u03c4(X) = max{x | F(x) \u2264 \u03c4}.\nThe (left-tail) conditional value-at-risk (CVaR) of X with parameter 7 is defined as follows:\nCVaR_\u03c4(X) = 1/\u03c4 \u222b_0^\u03c4 VaR_u(X)du\nWhen F(X) is continuous at x = VaR_\u03c4(X), the conditional value-at-risk can be written as follows:\nCVaR_\u03c4(X) = E[X | X \u2264 VaR_\u03c4(X)].\nHence, Equation (9) allows for the interpretation of CVaR as the expected value of the 7 left quantile of the distribution of X. In this work, X represents the stationary reward distribution that we are trying to optimize. In recent years, CVaR has emerged as a popular risk measure, in-part because it is a 'coherent' risk measure [10], meaning that it satisfies key mathematical properties which can be meaningful in safety-critical and risk-related applications.\nAn important, well-known property of CVaR, which we will use in our analysis, is that it can be represented as follows [5]:\nCVaR_\u03c4(X) = max_b E[b - 1/\u03c4 (b - X)^+] = E[VaR_\u03c4 (X) - 1/\u03c4 (VaR_\u03c4(X) \u2013 X)^+].\nwhere, (y)+ = max(y, 0). Existing methods typically formulate the CVaR optimization problem as the following bi-level optimization with an augmented state-space that includes an estimate of VaR(X) (in this case, b):\nmax CVaR_\u03c4(X) = max_\u03c0 max_b E[b - 1/\u03c4 (b - X)^+] = max_b (b - max_\u03c0 E[(b \u2013 X)+]).\nwhere the 'inner' optimization problem can be solved using standard MDP solution methods."}, {"title": "4 Reward-extended differential (RED) reinforcement learning", "content": "We now present our primary contribution: a framework for solving various subtasks simultaneously in the average-reward setting. We call this framework reward-extended differential (or RED) reinforcement learning. The 'differential' part of the name comes from the use of the differential algorithms from average-reward MDPs. The 'reward-extended' part of the name comes from the use of the reward-extended TD error, a novel concept that we will introduce shortly. We will show how combining the reward-extended TD error with a unique structural property of average-reward MDPs allows us to solve various subtasks simultaneously in an effective and efficient manner. We first derive the overall framework, then present a family of RL algorithms that utilize the framework. In the subsequent section, we will utilize this framework to tackle the CVaR optimization problem."}, {"title": "4.1 The framework", "content": "We begin our discussion by first describing what is meant by a 'subtask'. Consider our goal of finding a policy that induces a stationary reward distribution with an optimal reward CVaR. Here, we are interested in maximizing the scalar objective, CVaR_\u03c4(R), however our MDP only has access to the (typical) reward signal, R. We know that the two are related as specified in Equation (10), where the equality only holds for the optimal value of the scalar b. Unfortunately, we do not know this optimal value, b* (which corresponds to VaR). In this scenario, b is the subtask that we are interested in solving because if we optimize b, in addition to CVaR_\u03c4(R), we will have our desired optimal solution (as per Equation (10)). More generally, we can define a subtask as follows:\nDefinition 4.1 (Subtask). A subtask, z_i, is any scalar prediction or control objective belonging to a corresponding finite set Z_i \u2282 R, such that:\ni) there exists a linear (or piecewise linear) function, f : R \u00d7 Z_1 \u00d7 Z_2 \u00d7 \u00b7\u00b7\u00b7 \u00d7 Z_i \u00d7 \u2026\u2026\u2026 \u00d7 Z_n \u2192 R, that is invertible with respect to each input given all other inputs, where R is the finite set of observed per-step rewards from the MDP M, R \u2282 R is a modified, finite set of per-step rewards whose long-run average is the primary prediction or control objective of the modified MDP, M = (S, A, R, p), and Z = {z_1 \u2208 Z_1, z_2 \u2208 Z_2, ..., z_n \u2208 Z_n} is the set of n subtasks that we wish to solve; and\nii) z_i is independent of the states and actions, and hence independent of the observed per-step reward, r \u2208 R, such that E[f (r, z_1, z_2, ..., z_n)] = f (E[r], z_1, z_2, ..., z_n).\nWith this definition in mind, we now proceed by providing the basic intuition behind our framework by using the average-reward itself, r, as a blueprint of sorts for how we will derive the update rules in our learning algorithms for our subtasks. In particular, we will show how the process for deriving the update rule for the average-reward estimate, R_t, in Equations (6) and (7) can be adapted to derive equivalent update rules for estimates corresponding to any subtask that satisfies Definition 4.1.\nConsider the Bellman equation (4). We begin by pointing out that the average-reward satisfies many of the key properties of a subtask. In particular, we can see that satisfies E[r \u2013 r_\u03c0 + \u03c5_\u03c0(s')] = E[r + \u03c5_\u03c0(s')] \u2013 r_\u03c0. This allows us to rewrite the Bellman equation (4) as follows:"}, {"title": "4.2 The algorithms", "content": "We now present our family of RED RL algorithms. The full set of algorithms, including algorithms that utilize function approximation, are included in Appendix A. The convergence proofs for the tabular algorithms are included in Appendix B. The convergence of non-tabular, off-policy RL algorithms in the average-reward setting that directly use the TD error to estimate the average-reward is still an open research question (e.g. see [16]).\nRED TD-learning algorithm (tabular): We update a table of estimates, V_t: S \u2192 R as follows:\nRED Q-learning algorithm (tabular): We update a table of estimates, Q_t : S \u00d7 A \u2192 R as follows:"}, {"title": "5 Case study: RED RL for CVaR optimization", "content": "We now illustrate the usefulness of the RED RL framework by showing how it can be used to learn a policy that optimizes the CVaR risk measure without the use of an explicit bi-level optimization scheme (as in Equation (11)), or an augmented state-space.\nAs previously mentioned in Section 4.1, our goal is to learn a policy that induces a stationary reward distribution with an optimal reward CVaR (instead of the regular average-reward). Here, the reward CVaR is our primary control objective (i.e., the r that we want to optimize), and the value-at-risk, VaR (b in Equation (10)), is our subtask. When applying the RED RL algorithms to a modified version of Equation (10) (see Appendix C for more details), we arrive at the RED CVaR algorithms (see Appendix C for the full algorithms), which have the following update for our subtask, VaR:\nwhere na is the step size, T is the CVaR parameter, and dt is the regular TD error. As such, we can see that a subtask update, which utilizes the reward-extended TD error, ultimately amounts to an 'extended' version of the regular TD update. Consequently, this update rule allows us to optimize our subtask, VaR, without the use of an explicit bi-level optimization scheme or an augmented state-space.\nWe now present empirical results when applying the RED CVaR algorithms on two learning tasks. The first task is a simple two-state environment that we created for the purposes of testing our algorithms. It is called the red-pill blue-pill environment (see Appendix D), where at every time step an agent can take either a red pill, which takes them to the \u2018red world' state, or a blue pill, which takes them to the 'blue world' state. Each state has its own characteristic reward distribution, and in this case, the red world state has a reward distribution with a lower (worse) mean but higher (better) CVaR compared to the blue world state. Hence, we would expect the regular Differential Q-learning algorithm to learn a policy that prefers to stay in the blue world, and that the RED CVaR Q-learning algorithm learns a policy that prefers to stay in the red world. This task is illustrated in Fig. 3a)."}, {"title": "6 Discussion, limitations, and future work", "content": "In this work, we introduced reward-extended differential (or RED) reinforcement learning: a novel reinforcement learning framework that can be used to solve various subtasks simultaneously in the average-reward setting. We introduced a family of RED RL algorithms for prediction and control, and then showcased how these algorithms could be adopted to effectively and efficiently tackle the CVaR optimization problem. More specifically, we were able to use the RED RL framework to successfully learn a policy that optimized the CVaR risk measure without using an explicit bi-level optimization scheme or an augmented state-space, thereby alleviating some of the computational challenges and non-trivialities that arise when performing risk-based optimization in the episodic and discounted settings. Empirically, we showed that the RED-based CVaR algorithms fared well both in tabular and linear function approximation settings. Moreover, our experiments suggest that these algorithms are robust to the initial guesses for the subtasks and primary learning objective.\nMore broadly, our work has introduced a theoretically-sound framework that allows for a subtask-driven approach to reinforcement learning, where various learning problems (or subtasks) are solved simultaneously to help solve a larger, central learning problem. In this work, we showed (both theoretically and empirically) how this framework can be utilized to predict and/or optimize any arbitrary number of subtasks simultaneously in the average-reward setting. Central to this result is the novel concept of the reward-extended TD error, which is utilized in our framework to develop learning rules for the subtasks, and satisfies key theoretical properties that make it possible to solve any given subtask in a fully-online manner by minimizing the regular TD error. Moreover, we built-upon existing results from Wan et al. [8] to show the almost sure convergence of tabular algorithms derived from our framework. While we have only begun to grasp the implications of our framework, we have already seen some promising indications in the CVaR case study: the ability to turn explicit bi-level optimization problems into implicit bi-level optimizations that can be solved in a fully-online manner, as well as the potential to turn certain states (that meet certain conditions) into subtasks, thereby reducing the size of the state-space.\nNonetheless, while these results are encouraging, they are subject to a number of limitations. Firstly, by nature of operating in the average-reward setting, we are subject to the somewhat-strict assumptions made about the Markov chain induced by the policy (e.g. unichain or communicating). These assumptions could restrict the applicability of our framework, as they may not always hold in practice. Similarly, our definition for a subtask requires that the associated subtask function be linear, which may also limit the applicability of our framework to simpler functions. Finally, it remains to be seen empirically how our framework performs when dealing with multiple subtasks, when taking on more complex tasks, and/or when utilizing nonlinear function approximation.\nIn future work, we hope to address many of these limitations, as well as explore how these promising results can be extended to other domains, beyond the risk-awareness problem. In particular, we believe that the ability to optimize various subtasks simultaneously, as well as the potential to reduce the size of the state-space, by converting certain states to subtasks (where appropriate), could help alleviate significant computational challenges in other areas moving forward."}]}