{"title": "A Refreshed Similarity-based Upsampler for Direct High-Ratio Feature Upsampling", "authors": ["Minghao Zhou", "Hong Wang", "Yefeng Zheng", "Deyu Meng"], "abstract": "Feature upsampling is a fundamental and indispensable ingredient of almost all current network structures for image segmentation tasks. Very recently, a popular similarity-based feature upsampling pipeline has been proposed, which utilizes a high-resolution (HR) feature as guidance to help upsample the low-resolution (LR) deep feature based on their local similarity. Albeit achieving promising performance, this pipeline has specific limitations in methodological designs: 1) HR query and LR key features are not well aligned in a controllable manner; 2) the similarity between query-key features is computed based on the fixed inner product form, lacking flexibility; 3) neighbor selection is coarsely operated on LR features, resulting in mosaic artifacts. These shortcomings make the existing methods along this pipeline primarily applicable to hierarchical network architectures with iterative features as guidance and they are not readily extended to a broader range of structures, especially for a direct high-ratio upsampling. Against these issues, we thoroughly refresh this pipeline and meticulously optimize every methodological design. Specifically, we firstly propose an explicitly controllable query-key feature alignment from both semantic-aware and detail-aware perspectives, and then construct a parameterized paired central difference convolution block for flexibly calculating the similarity between the well-aligned query-key features. Besides, we develop a fine-grained neighbor selection strategy on HR features, which is simple yet effective for alleviating mosaic artifacts. Based on these careful designs, we systematically construct a refreshed similarity-based feature upsampling framework named ReSFU. Comprehensive experiments substantiate that our proposed ReSFU is finely applicable to various types of architectures in a direct high-ratio upsampling manner, and consistently achieves satisfactory performance on different applications, including semantic segmentation, medical image segmentation, instance segmentation, and panoptic segmentation, showing superior generality and ease of deployment beyond the existing upsamplers.", "sections": [{"title": "1 INTRODUCTION", "content": "As a fundamental ingredient in deep network architectures, feature upsampling aims to restore the spatial resolution of low-resolution (LR) features, which is widely used in image segmentation tasks, such as semantic segmentation [4], instance segmentation [5], and panoptic segmentation [6].\nDuring the feature upsampling process, each high-resolution (HR) feature element is typically estimated by weighting its neighboring elements from the input LR feature. To generate the weights (also known as the upsampling kernels [2], [7]), in recent years, various upsampling research lines have been proposed. Specifically, bilinear and nearest-neighbor interpolation are the most widely-adopted feature upsampling methods, which are implemented based on hand-crafted weighting rules and often cause blurry effects. To enhance the flexibility of feature upsampling, some studies have developed different content-aware dynamic upsamplers [7]\u2013[11]. For example, in [7], the authors devised a dynamic network to generate the upsampling kernel from LR features. Despite promising performance, they usually struggle to restore clear object boundaries due to the loss of fine-grained details in LR features.\nAgainst this issue, a novel similarity-based feature upsampling pipeline SAPA [2] has been proposed, which utilizes an HR feature y as the guidance to help accomplish"}, {"title": "2 RELATED WORK", "content": "2.1 Feature Upsampling\nGuidance-Free Upsampling Methods. Nearest neighbor and bilinear interpolation are two widely adopted feature"}, {"title": "3 REVISITING SIMILARITY-BASED FEATURE UP- SAMPLING PIPELINE", "content": "In this section, we carefully reformulate and delve into the current similarity-based feature upsampling pipeline.\nGiven an LR deep feature x \u2208 Rhw\u00d7C and an HR guidance feature y \u2208 RHWxc,1 the existing similarity-based feature upsampling framework SAPA [2], [21] aims to upsample x to an HR deep feature x \u2208 RHW\u00d7C under the guidance of y. Specifically, for each HR pixel i \u2208 {1,...,HW}, the HR deep feature element \u017e\u1d62 \u2208 RC is generally estimated by weighting its neighboring LR deep feature elements based on an upsampling kernel Softmax(si) as:\nxi = Softmax(si)XN(i),\n(1)\nwhere XN(i) \u2208 RK\u00b2\u00d7C denotes the neighboring LR feature elements of pixel i; |N(i)| = K\u00b2 is the number of neighboring LR pixels with a pre-defined kernel size K; si \u2208 RK\u00b2 is the similarity scores assigned to the K2 neighbors of pixel i. In the representative base version of SAPA [2], si is computed as:\nsi sim(qi, kN(i)) qik(i),\n(2)\nwhere q and k are linear projections of y and \u00e6, respectively, with D channels; qi \u2208 RD is the i-th feature element of q \u2208 RHW\u00d7D; kN(i) \u2208 RK2\u00d7D represents the K2 neighboring feature elements of pixel i in k \u2208 Rhw\u00d7D; sim (\u00b7,\u00b7) is a general similarity function, designed as the inner product between qi and kN(i) in SAPA. We regard q, k, and x as the HR query, LR key, and LR value features, respectively.\nAs seen, the entire similarity-based feature upsampling pipeline is strongly associated with the design of three main parts: query-key feature acquisition for facilitating the computation of similarity si; the similarity function sim (, ); neighbor selection N (i). For better understanding, based on the backbone Segmenter-S with a direct \u00d74 upsampling for the last-layer feature under the guidance of a shallow feature (see Fig. 10), we experimentally visualize SAPA in Fig. 2 (a) and identify the following issues:\n1) Uncontrollable Query-Key Alignment. In such a direct high-ratio upsampling case, the HR guidance feature y is relatively shallow and it generally contains low-level information, such as texture patterns, while the LR deep feature x usually contains high-level semantics [40], [54]. Since the linear projection operation with fixed weights is not sensitive to the content of either y or x, the linearly-projected query and key cannot be well-aligned in the feature space without extra control, which would seriously mislead the subsequent similarity computation.\n2) Inflexible Similarity Computation. The conventional inner product operation in Eq. (3) is non-parametric and lacks flexibility in capturing the relations between the query and key. Directly adopting this inner product form would cause that within a small local region N(i), the computed K\u00b2 scores qik (i) are close to each other (see Fig. 6 (a)). This would consequently generate an overly smooth upsampling"}, {"title": "4 METHOD", "content": "Motivated by the analysis in Sec. 3, in this section, we aim to refresh the current similarity-based feature upsampling framework by carefully optimizing every involved component in Eqs. (2) and (3), including query-key alignment, similarity computation, and neighbor selection. Then we correspondingly construct a more flexible and universal up-sampling framework, called ReSFU, which has the potential to adapt to various network structures.\nSpecifically, as shown in Fig. 2 (b), we first propose a controllable query-key feature alignment method from both semantic-aware and detail-aware perspectives. Then, we design a specific paired central difference convolution (PCDC) block for flexibly calculating the similarity between the aligned query-key pairs. Finally, we devise a fine-grained neighbor selection (FNS) strategy to alleviate mosaic artifacts. For each part, the detailed designs are given below."}, {"title": "4.1 Explicitly Controllable Query-Key Alignment", "content": "From [40], [54], an HR guidance feature y from a shallow layer usually captures more detail-related information while the LR deep feature x encodes more semantic-related information. Thus, we informally refer to y (and its linear projection, the query feature q) and \u00e6 (and its linear projection, the key feature k) as residing in the detail space and the semantic space, respectively, as presented in Fig. 3.\nConfronted with the representational discrepancy between the detail and semantic spaces, it is inaccurate to directly calculate the similarity score between the original query-key feature pair, i.e., q and k, across different spaces. To fully capture and exploit the relations among different neighboring pixels that share similar information both at the detail and semantic levels for guiding the upsampling process, we propose a two-pronged approach to explicitly transform the original features and generate two query-key pairs aligned in detail space and semantic space, respectively, to facilitate accurate similarity computation. Please refer to the lower row of Fig. 3 for the overall design.\n4.1.1 Semantic-Aware Mutual-Alignment\nFirstly, our goal is to project the original query q into the semantic space so that it can better align with the key feature k in the semantic space, while also preserving the structural details in the detail space. To this end, inspired by the guided filter (GF) [14], [55], we propose to linearly transform q in every local window for detail preservation while minimizing the distance between the transformed query and the key k for the mutual-alignment in the semantic space. In this manner, the structural information of q can be efficiently integrated with the semantic information of k into the transformed query [14].\nMathematically, let qGF \u2208 RHW\u00d7D be the transformed query and each of its element (qGF)id can be estimated via solving the following optimization problem [14]:\nminmjddjd\u03a3i\u2208Ij(((qGF)id\u2212kid)2+ema2jd),\ns.t.(qGF)id=mjdqid+njd,\u2200i\u2208Ij,\n(3)\nwhere (qGF) id, qid, and kid are the feature elements of qGF, q, and k at pixel i and channel d, respectively; k \u2208 RHW\u00d7D is the bilinearly upsampled HR result of the original LR key k; d \u2208 {1, ..., D}; mjd and njd\u0105 are linear coefficients for pixel j at channel d; I; is a square window with radius r centered at pixel j\u2208 {1,...,HW}; and \u0454 is a small regularization weight. In experiments, the radius r is experimentally set to 8 and \u0454 is set to 0.001. Please note that for element-wise minimization computation in Eq. (4), we adopt the upsampled key feature k for keeping the same size with qGF, which also belongs to the semantic space.\nFrom [56], the explicit solution of Eq. (4) can be easily derived as:\nmjd=\u03a3ieljLidkid\u2212\u03bc\u03b1\u03bc\u03b1\u03c3\u03c4\u03b1\u03c4\u03b5jd,njd=pd\u2212mjda\u03bc\u03b1\u03c3\u03c4\u03b1jd,\n(4)\nwhere \u03bckj=1j\u2211i\u2208ljkid,\u03c32kj=1j\u2211i\u2208ljk2id\u2212\u03bc2kjand|Ij|is the number of pixels\n\u03bc\u03b1jd=1j\u2211i\u2208ljqid\nwhere \u03bckj=1j\u2211i\u2208ljkid,\u03c32kj=1j\u2211i\u2208ljk2id\u2212\u03bc2kj and |Ij| is the number of pixels contained in the local window Ij, which is r\u00b2.\nSince a pixel i is involved in different overlapping window Ij that covers the pixel i, (qGF)id would change with these local windows. Following [14], by averaging all these overlapping local windows Ij, we can get the final transformed query as:\n(qGF)id=midqid+nid,(5)\nwhere mid=1\u03a3je1;mjd and nid=1\u03a3je1;njd.\n(6)\nAs seen, the optimized query qGF is explicitly derived on the basis of the original query q and key k, achieving the controllable alignment with key in the semantic space. From Eq. (3), for the explicitly optimized query-key pair qGF and k, the similarity (ss); for pixel i is calculated as:\n(ss);sim((qGF)i,kN(i)).\n(7)\nWe call ss \u2208 RHW \u00d7K\u00b2 as semantic-aware mutual similarity.\n4.1.2 Detail-Aware Self-Alignment\nIn addition to the semantic space, we also seek to fully exploit the structural information in the detail space and generate the aligned query-key pair in this space for better guiding the upsampling process. Unfortunately, unlike the"}, {"title": "4.2 PCDC for Flexible Similarity Calculation", "content": "To compute ss and sd in Eq. (9), instead of adopting the fixed inner product-based manner for sim(,) in [2], [21], here we aim to specifically design a parameterized convolution operation to flexibly model the inherent relevance between every query-key pair, i.e., qGF and k, q and qGs, for more accurate similarity calculation.\n4.2.1 Paired Central Difference Convolution\nFor a vanilla convolution layer with G groups, given an arbitrary input x \u2208 RHW\u00d7D with D input channels, each element vil in the convolution result v\u2208 RHW\u00d7L with L output channels is calculated by:\nvil=g+1D/G\u22121K2\u2211d=gD/G\u2211n=1wndlXind+bl, jn \u2208 N(i),(8)\nwhere i is the pixel index; the output channel index le {0, . . ., L \u2212 1}; g = [DlG] is the group index ranging from 0 to G \u2013 1; d is the input channel index; jn \u2208 N(i) indexes a neighbor of the pixel i, with n = 1,\u2026\u2026, K\u00b2; wndt is an element of w \u2208 RK2\u00d7D/G\u00d7L representing the grouped convolution weights; the index d = d%(DG), where % is the modulo operation; b \u2208 RL is the bias.\nInspired by the ability of central difference convolution [28] in combining the flexibility of learnable convolution and the awareness of local gradient information, here we propose a paired central difference convolution (PCDC) to capture the relations between the query and key features. Specifically, following the framework of the vanilla grouped convolution in Eq. (10), we replace its input with the \u201cpaired central difference\u201d between any aligned query-key pair, q and k, and then obtain the PCDC output v as:\nvil=g+1D/G\u22121K2\u2211d=gD/G\u2211n=1wndl(kind\u2212qid)+bi, jn \u2208 N(i).(9)\n4.2.2 PCDC-Block for Similarity Calculation\nBased on the capability of the proposed PCDC in capturing the local similarity between the query-key pair, here we construct a PCDC-Block to finally implement the function sim(,) for similarity computation as given in Fig. 5.\nConcretely, for each aligned query-key pair, i.e., qGF and k, q and qgs, they are first separately input to a group normalization layer with shared affine parameters to get the normalized query-key pair \u012b and k. Then the normalized pair is passed through a PCDC computation layer, i.e., Eq. (12), to obtain the intermediate result v. Then by feeding v to a channel compressor to transform the channel number from L to K2, we can get the corresponding similarity scores for every query-key pair, i.e., ss and sq. Here the channel compressor is sequentially composed of a 1 \u00d7 1 convolution layer, a ReLU layer, a group normalization layer, and a 1\u00d71 convolution layer, where these two convolution layers are both with 4 groups and the intermediate channel size is 128."}, {"title": "4.3 Fine-grained Neighbor Selection", "content": "From Sec. 3, for the similarity-based feature upsampling pipeline, the neighborhood selection N (i) exists in two procedures, i.e., 1) the selection on key feature for query-key similarity calculation and 2) the selection on value feature for the weight-value computation in Eq. (2). For the existing approaches implemented based on Eqs. (2)(3), they select the neighbors N (i) directly on LR key feature k and LR value feature x in a grid-wise manner, respectively. This would possibly result in mosaic artifacts as analyzed in Sec. 3. To address this problem, for our proposed refreshed pipeline mainly implemented based on Eqs. (2) (9) (12),"}, {"title": "5 EXPERIMENTS", "content": "In this section, we comprehensively substantiate the applicability of our proposed ReSFU by conducting extensive experiments on various network structures for the semantic segmentation task. Then, we provide detailed model verification and ablation studies to clearly show the working mechanism of ReSFU and validate the rationality of every involved component. Finally, in order to better demonstrate the universality of the proposed ReSFU, we extend it to more application scenarios with a variety of datasets, including medical image segmentation, instance segmentation and panoptic segmentation.\n5.1 Experimental Setup\nImplementation details. For similarity calculation in Eq. (12), the number of output channels L and groups G are 32 and 4, respectively. The projection dimension D is 32 and the kernel size K is 3. For all the involved networks, we modify the default upsampling method (nearest neighbor for FPN neck [60] or bilinear for other networks) by substituting it with alternative upsamplers (replacement details are described in the experimental comparisons below), while keeping other training settings constant for end-to-end training on NVIDIA V100 GPUs. Please refer to the supplementary material for more details.\nComparison methods. Consistent to the latest works [13], [22], we compare the proposed ReSFU with the following feature upsamplers along this research field, including:"}, {"title": "5.2 Semantic Segmentation on Various Architectures", "content": "In this section, for the semantic segmentation task, we verify the effectiveness of our proposed ReSFU by conducting comprehensive experiments on various network structures with different types of popular backbones including ResNet [61], Vision TransFormer (ViT) [40], Mix Transformer (MiT) [1], and Swin Transformer [62]. For the selection of network structures, we consider not only the hierarchical upsampling manner adopted by the existing baselines, e.g., SegFormer [1], but also the non-hierarchical upsampling"}, {"title": "5.3 Model Analysis", "content": "To better understand the working mechanism underlying the proposed ReSFU and validate the rationality of each methodological design, in this section, we conduct a comprehensive model analysis experiment, including model visualization and a series of ablation studies.\n5.3.1 Model Visualization\nBased on the backbone Segmenter-ViT-S in Fig. 10 (b), we first visualize each component learned by ReSFU.\nVisualization of Query-Key Features. Fig. 16 (a) presents each component, including original features y and \u00e6, two aligned query-key pairs (qGF and k, q and qgs), and the upsampled feature \u017e. It is clearly observed that the GF-based optimization query qGF semantically resembles k while retaining structural information in q, e.g., the edges of the tower. The smoothed query qas effectively removes"}, {"title": "5.4 Application to More Tasks", "content": "In this section, we extend ReSFU to more application scenarios including medical image segmentation, instance segmentation, and panoptic segmentation.\n5.4.1 Medical Image Segmentation\nUpsampling Details. For the medical image segmentation task, we choose the prevalent network, TransUNet [48], with a U-shape architecture different from those presented in Sec. 5.2. As illustrated in Fig. 18, this network contains three \u00d72 upsampling procedures. For the guidance, other upsamplers sequentially adopt C3, C2, and C1, which is a more suitable setting for them, while our ReSFU only uses the shallow C1 for guiding the upsampling at different levels of features.\nPerformance Comparison. The experiments are executed based on two widely-adopted datasets, i.e., Synapse for multi-organ segmentation, and the automated cardiac diagnosis challenge (ACDC).4 Specifically, Synapse is annotated with eight abdominal organs, i.e., aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas, spleen, and stomach. Following [48], we employ 18 computed tomography (CT) scans with 2,212 axial slices for training and 12 CT scans for validation. For ACDC, the involved magnetic resonance imaging (MRI) scans are annotated with left ventricle (LV), right ventricle (RV), and myocardium (MYO). Consistent to [48], we utilize 70 scans with 1,930 axial slices for training and 40 scans for validation. Quantitative evaluation is conducted based on the two typical metrics, including Dice (the higher the better) and the 95-th percentile of the Hausdorff Distance (HD95, the lower the better)."}, {"title": "5.4.2 Instance Segmentation and Panoptic Segmentation", "content": "Here we further verify the universality of our proposed ReSFU through two other segmentation tasks, including instance segmentation and panoptic segmentation.\nSpecifically, for instance segmentation, we employ the classic Mask R-CNN [42] with ResNet50 as the backbone and replace the upsampling stages in FPN with ReSFU, as shown in Fig. 15 (b). All the results are quantitatively evaluated based on metrics of the average precision (AP) series, including mask AP, AP50, AP75, APS, APM, and APL. For panoptic segmentation, following [13], we choose Panoptic FPN [6] with ResNet50 as the backbone and adjust the three upsamplers in FPN, as presented in Fig. 15 (b). We report panoptic quality (PQ), PQ on things (PQth), PQ on stuff (PQst), segmentation quality (SQ), and recognition quality (RQ) [6]. The MS COCO dataset [67] is used for training and evaluating the two tasks.\nTable 12 reports the quantitative results on the two additional segmentation tasks. As seen, our proposed ReSFU performs more competitively over almost all the evaluation metrics, showing favorable universality. Fig. 20 and Fig. 21 provide the visual results achieved by different upsampling methods for instance segmentation and panoptic segmentation, respectively. It is easily observed that the proposed ReSFU attains better segmentation results with more details and more consistent semantics, such as the horns and ears of the giraffes in Fig. 20, and the floor in Fig. 21."}, {"title": "5.5 More Discussions on Direct/Iterative Upsampling", "content": "As explained previously, other comparing upsamplers are inserted in an iterative \u00d72 upsampling manner by default, while our proposed ReSFU is always simply executed in a direct upsampling process without iterative guidance features. Here we provide more discussions to further evaluate different methods under different upsampling manners.\nTaking PSPNet as an example, for the comparing methods, e.g., CARAFE, FADE, SAPA, DySample, and ReSFU,"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this paper, for the fundamental upsampling design included in almost all current network architectures, we carefully reformulated the current similarity-based feature upsampling framework and thoroughly analyzed the limitations in methodological designs together with experimental visualizations. For each component involved in this pipeline, we meticulously proposed specific optimization designs and correspondingly constructed a refreshed similarity-based feature upsampling framework, called ReSFU. Through comprehensive model verification and ablation studies, we fully validated the role of each component and clearly shown the working mechanism underlying our proposed ReSFU. Based on different types of network architectures, extensive experiments substantiated the superiority of our proposed ReSFU beyond the existing baselines and demonstrated that our ReSFU can be adapted not only to different upsampling structures with flexible guidance manners but also to different applications, including medical image segmentation, instance segmentation, and panoptic segmentation.\nThanks to the favorable generality of our proposed ReSFU, apart from segmentation tasks, there is great potential to extend it to more applications, such as, the pansharpening task [68] or the multispectral and hyperspectral image fusion task [69] with the available HR input images as guidance. However, for the applications without HR reference images, e.g., image super-resoultion [70] and image generation [71], these guidance-based feature upsampling methods, e.g., FADE, SAPA, and our ReSFU, are difficult to apply. This is beyond our focus in this paper and deserves further investigation in our future work."}]}