{"title": "Can Large Language Models Capture Video Game Engagement?", "authors": ["David Melhart", "Matthew Barthet", "Georgios N. Yannakakis"], "abstract": "Can out-of-the-box pretrained Large Language Models (LLMs) detect human affect successfully when observing a video? To address this question, for the first time, we evaluate comprehensively the capacity of popular LLMs to annotate and successfully predict continuous affect annotations of videos when prompted by a sequence of text and video frames in a multimodal fashion. Particularly in this paper, we test LLMs' ability to correctly label changes of in-game engagement in 80 minutes of annotated videogame footage from 20 first-person shooter games of the GameVibe corpus. We run over 2,400 experiments to investigate the impact of LLM architecture, model size, input modality, prompting strategy, and ground truth processing method on engagement prediction. Our findings suggest that while LLMs rightfully claim human-like performance across multiple domains, they generally fall behind capturing continuous experience annotations provided by humans. We examine some of the underlying causes for the relatively poor overall performance, highlight the cases where LLMs exceed expectations, and draw a roadmap for the further exploration of automated emotion labelling via LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "The use of autoregressive modelling and large pretrained models such as Large Language Models (LLMs) is currently dominating AI research. LLMs have demonstrated unprecedented advances in language translation, code generation, problem solving, and AI-based assistance among many other downstream tasks [1]. Given their versatility and efficiency compared to earlier autoregressive models, one might even argue that the current capabilities of LLMs are endless as long as a problem and its corresponding solution(s) are represented as text. Meanwhile, the recent applications of LLMs within affective computing largely consider text-based affect modelling tasks such as LLM-based sentiment analysis [2], [3], [4]. The automatic labelling of affect based on time-continuous visual input remains largely unexplored [4], however, as the handful of studies available rely on still images [5], [6].\nMotivated by the aforementioned lack of studies this paper introduces the first comprehensive evaluation of LLMs tasked to predict time-continuous affect labels from videos. In this initial evaluation we let LLMs observe gameplay videos as we prompt them with textual information of what they observe, and ask them to label the viewer engagement on those videos. We chose games as the domain of our study since they can act as rich elicitors of emotions and can offer a wide range of dynamic scenes and stimuli, varying from intense player actions to less intense game-world exploration. Even though LLMs have been used in a series of diverse tasks within the domain of videogames\u2014both in academic studies [7], [8] and industrial applications such as Al Dungeon (Latitude, 2019), AI People (GoodAI, 2025) and Infinite Craft\u00b9 the capacity of these foundation models as predictors of player experience has not been investigated yet.\nWe employ LLMs as autonomous player experience annotators and present a thorough evaluation of their capacity to predict player experience in one-shot and few-shot fashions. Specifically, we compare state of the art foundation models from the LLaVA and GPT families against human annotated data of player engagement of the GameVibe dataset [9] (see Fig. 1). The dataset contains continuous engagement labels of gameplay videos across a variety of first-person shooter (FPS) games. We present selected results out of 2,440 experimental settings in which we vary and test LLM model types, model sizes, prompting strategies, input types, and"}, {"title": "II. RELATED WORK", "content": "This study investigates the capacity of LMMs to accurately annotate subjectively-defined aspects of gameplay. We leverage the existing knowledge-priors of these algorithms, without fine-tuning or using complex retrieval augmented strategies. We thus hypothesise that the algorithm's prior knowledge is sufficient to approximate the ground truth of engagement (as provided via human feedback) in a set of gameplay scenarios. This section covers related work in affect modelling using LLMs, the use of LLMs in games, and it ends with a focus on modelling aspects of players and their games."}, {"title": "A. LLMs for Affect Modelling", "content": "Given the resounding success of LLMs in several domains, several recent research efforts naturally focus on their direct application in affect detection tasks. The vast majority of research on LLMs related to human affect have focused on predicting manifestations of affect from text as this plays to the strengths of their architecture. Unsurprisingly, sentiment analysis has been the most common research application of LLMs in affective computing and has given us some impressive results already [10]. Indicatively, Broekens et al. [3] highlighted how GPT-3.5 can accurately perform sentiment analysis on the ANET corpus [11] for valence, arousal and dominance. Similarly, M\u00fcller et al. [12] used fine-tuned Llama2-7b [13] and Gemma [14] models to classify shame in the DEEP corpus [15], achieving 84% accuracy. Whilst LLMs have been extensively tested for sentiment analysis on existing text-based corpora, research on using LLMs as predictors of experience by observing multimodal content such as games remains unexplored.\nDespite their promise, some critical challenges have emerged when working with pre-trained LLMs for prediction tasks such as affect modelling. A recent study by Chochlakis et al. [16] has found that LLMs struggle to perform meaningful in-context learning from new examples and remain fixed to their knowledge priors, with larger models exaggerating this issue. This problem is even more pressing in closed-source models such as GPT-40 because researchers lack important details which can help them assess the level of data contamination. Balloccu et al. [17] conducted a study across 255 academic papers and found that LLMs have been exposed to a significant number of samples from existing ML benchmarks, potentially painting a misleading picture about their predictive performance in such tasks. While the dataset we use in this paper covers a novel domain, it is possible that some of the videos in the GameVibe dataset have been exposed to some of the models we use. However, because the dataset was published after the models used here\u00b3, we are confident that the engagement prediction task specifically does not suffer from any significant data contamination.\nBeyond contamination, we also have to face the inherent biases encoded in LLMs. Mao et al. [10] have conducted a study on such biases in BERT-like models [18] on affective computing tasks. In our study we use what Mao et al. call \u201ccoarse-grain\u201d tasks\u2014a binary decision with symmetrical labels (here increase and decrease of engagement). When evaluating these types of tasks, LLMs have been shown to exhibit less bias [10] than on \"fine-grained\u201d tasks with multiple asymmetrical labels. This gives us confidence on the feasibility of our task\u2014which is formulated as a binary classification problem.\nAmin et al. [19] have also conducted a study on the capabilities of GPT [20] on affective computing tasks. They have put forth a comprehensive series of experiments which included a similar pairwise preference classification task for engagement prediction to what we use in this paper. They showed that when it comes to subjective tasks with a high potential for disagreement between annotators, out-of-box LLMs, such as GPT struggle compared to architectures leveraging specialized supervised networks. In those experiments-focusing on a simple one-shot prompting strategy on text input-GPT barely surpassed the baseline. In contrast [19], we investigate multimodal, chain-of-thought, and few-shot strategies in visual-based engagement prediction tasks across multiple games, analysing where LLMs either struggle or flourish compared to baseline approaches."}, {"title": "B. LLMs in Games", "content": "The recent developments in LLM methods and technology brought unprecedented wide adoption of AI across multiple domains including law [21], healthcare [22], and education [23]. Advancements in transformer architectures [24], coupled with a rapid increase in dataset and parameter sizes [25] led to a new wave of algorithms with previously unseen capabilities to generate high-quality text. Starting with Bidirectional Encoder Representations from Transformers (BERT) [18] but eventually popularized with the release of Generative Pre-trained Transformers (GPT) [1], [26], [20], LLMs have largely been characterized as transformer-based models, using large amounts of parameters (in the 100 millions and billions), built on large amounts of data, generating text in an autoregressive manner that is predicting future tokens based on prior data. More recently, LLMs have been expanded to handle new modalities beyond text, such as audio and images [13], making them a candidate for applications using multimodal content such as gameplay videos.\nIn the context of games, LLMs have been used to create game-playing agents [27], [28], commentators [29] game analytics [30], [31], AI directors and game masters [32], [33], content generators [34], and design assistants [35]. Beyond the academic setting, we are seeing considerable interest from industrial players as well, such as NVIDIA's recent ACE small language models \u2074 for autonomously generating the behaviour and animation of NPCs. Gallotta et al. [7] offer a recent and thorough overview on how LLMs can be utilised in games. In their roadmap, they identify player modelling as one of the most promising, yet unexplored avenues for future research into LLMs and games. Whilst affect modelling research has demonstrated that LLMs can be effective predictors in tasks such as sentiment analysis [10], they are yet to be widely evaluated to modelling player experience in the context of games."}, {"title": "C. Player Affect Modelling", "content": "Player modelling is an active field within AI and games research [8] with a particular focus on methods that capture emotional and behavioural aspects of gameplay such as engagement [36], toxicity [37] and motivation [38]. Traditionally, the field has focused heavily on data aggregation [39] and pattern discovery [40], [38] of playing behaviours, but there has been a recent shift towards moment-to-moment predictive"}, {"title": "III. THE GAMEVIBE CORPUS", "content": "This section gives a general overview of the GameVibe corpus used throughout all experiments presented in this paper followed by an outline of the preprocessing approach we adopted for the engagement labels in this study. While the dataset is introduced thoroughly in [9] in this section we highlight the main aspects of the dataset that are relevant to our experiments here."}, {"title": "A. Corpus Overview", "content": "The GameVibe corpus [9] consists of a set of 120 au- diovisual clips and human annotations for engagement as viewers of first-person shooter games. This corpus presents a significant challenge for affect modelling research as its stimuli encompass a wide variety of graphical styles (e.g."}, {"title": "B. Engagement Data Pre-Processing", "content": "Our data preprocessing method closely follows common practices in affective computing and methods introduced in previous studies with GameVibe [54]. Thus, each annotation trace was resampled into three-second non-overlapping time windows using simple averaging. The videos were sampled at a similar rate to align the stimuli to the engagement traces provided by the participants. These traces were then processed into discrete ordinal signals by comparing pairs of consecutive time windows to determine whether engagement increased (1),"}, {"title": "IV. METHODOLOGY", "content": "In this section we detail our chosen algorithms and the different prompting strategies we employ throughout our experiments. In the presented studies we evaluate the capacity of LLMs to correctly evaluate changes of engagement in gameplay videos. In particular we picked LLaVA and GPT-4o as our base LLMs under investigation (see Section IV-A). In all reported experiments the downstream task of the employed LLM is to label a change in engagement (increase or decrease) given two consecutive frames of a video. We evaluate the algorithm's performance against the human labelled engagement data of GameVibe that we treat as our ground truth.\nTo explore how different experimental setups affect LLM engagement predictability, we ran experiments both with Multimodal and Text Input. Figure 2 illustrates the overall strategy and the different experimental setting employed. In the Multimodal Input setting, the input for the algorithm is one or two images accompanied by a text-prompt describing the task. We detail the format of the multimodal input in Section IV-B. In the Text Input setting, instead, we provide text-based descriptions of two video frames as part of the text prompt. We describe the format of the text input in Section IV-C. Finally, we also study few-shot prompting, using multimodal input and we detail this process in Section IV-D along with our general prompting strategy."}, {"title": "A. Employed LLMs", "content": "As mentioned earlier, we employ the Large Language and Vision Assistant (LLaVA) [56] and the Generative Pre-trained Transformer (GPT) models for all reported experiments. This section outlines the reasons we select these two LMMs and details the specific algorithmic properties we used for each model.\n1) LLaVA: LLaVA [57], [56] is an ensemble model connecting a vision encoder with an LLM. LLaVA uses Contrastive Language-Image Pre-training (CLIP) [58] as a vision encoder and Vicuna [59] as a language decoder. To train LLaVA, Liu et al. leveraged GPT4 to generate data on instruction following examples and trained their framework end-to-end to fuse vision and language input. The result is a robust model which is able to output text-descriptions and solve reasoning tasks based on image and text prompts combined. We have selected LLaVA because a) it is an open-source model with multimodal capabilities; and b) it is easily deployed in local environments. We run experiments with the 7 billion (7b), 13 billion (13b), and 34 billion (34b) parameter version of the algorithm using the Ollama API\u2075.\n2) GPT-4o: GPT4 is, at the time of writing, the most recent of a series of Generative Pre-trained Transformer (GPT) models developed by OpenAI. GPT4 is a closed source model. While a technical report about GPT4 has been published [20], the exact architecture and training data is unknown. What is known is that GPT4 uses a transformer architecture for both vision and language tasks, relies on reinforcement learning from human feedback and makes use of rule-based reward models based on hidden policy models and human-written rubrics to steer the algorithm in a direction that is considered \"safe\" by OpenAI. In this paper we use the GPT-4o (Omni) 2024-08-06 model variant. At the time of writing this is considered the flagship model of OpenAI. Unlike previous iterations, GPT-4o is trained end-to-end to incorporate text, audio, image, and video in both its input and output space [60]. We have selected this model because it is one of the most popular [61], state-of-art, closed-source LLMs as an alternative to the open-source LLaVA. We leverage the Open AI API for all reported experiments with GPT-4o."}, {"title": "B. Multimodal Input", "content": "In our experiments with Multimodal Input, we feed the models with both visual input and a corresponding text prompt. To provide the visual input we first extract single frames from GameVibe videos at a given interval. Then each frame is cropped to a square and downscaled to a fixed size. Particularly, in our experiments using one image we downscale our images to 336 \u00d7 336 pixels to be able to achieve the highest resolution input possible when combining two images in LLaVA models. In our early experiments with Multimodal Input, we use a single image as the model's input due to a limitation of the LLaVA models, which can only consider one image at a time. To circumvent this limitation we stitch the two video frames together vertically (i.e. a top and a bottom image), leaving a white band of 50 pixels between them. We call this experimental setting Multimodal Input - 1 Image (Stitched). This type of image stitching performs well on LLaVA models compared to other approaches\u2014such as concatenating the visual tokens [62]. For consistency we follow the same processing method with our GPT-4o models when it comes to experiments using a single image. We show an example of this prompting strategy and the output it produces in the Appendix (see Fig. 10).\nIn experiments involving few-shot prompting, we use two separate images per prompt. This experimental setting, named Multimodal Input - 2 Images, is only applicable to GPT-4o. This choice is partly informed by the aforementioned technical limitation of LLaVA since the few-shot experiments require"}, {"title": "C. Text Input", "content": "In our experiments using Text Input, we feed the models with text descriptions of two video frames as part of the prompt. We obtain these descriptions using the same LLM we use to generate the engagement evaluation. Similarly to the Multimodal Input - 1 Image setup, we downsample the obtained video frames to 336 \u00d7 336 pixels. Contrary to the previous setup, here we use these images one-by-one and generate descriptions in two different ways. We call these Basic and Advanced Descriptions based on the amount of context given to the model. For the former, we instruct the model to give a brief description, capturing only essential details without subjective commentary based on the setting and layout, enemies, and player action. For obtaining Advanced Descriptions, we instruct the model to also take player engagement into account and generate a description that captures how it might engage the player or viewer. We illustrate this process in the Appendix; see Figs. 11 and 12 respectively. For the engagement prediction task, we feed these descriptions to the models in pairs as part of their text prompt. We show an example of this prompting strategy and the output it produces in the Appendix (see Fig. 13)."}, {"title": "D. Prompting Methods", "content": "All prompting strategies we use for the engagement evaluation task follow a Chain-of-Thought (CoT) paradigm [63], [64]. We ask the models to provide a comparison between the given input frames, reasoning its analysis of engagement, and finally offering a one-word decision (i.e., engagement increase or decrease). Additionally, for the Multimodal Input experiments we also generate a description of the visual input before the comparison. In the Multimodal Input - 1 Image and Text Input experiments the decision is to pick the most engaging frame (see Fig. 10 in the Appendix). In the Multimodal Input - 2 Images experiments, instead, we refine the prompt and ask the model to explicitly output increasing and decreasing labels. We instruct the model to output its answers in a JSON format, which we parse and extract the final decision from; see also Fig. 14 in the Appendix.\nFor our few-shot experiments in the Multimodal Input - 2 Images setup, we generate artificial reasoning samples for a positive and negative example for each task. We use the same CoT prompt for this process as for the one-shot Multimodal Input - 2 Images experiments. We will call this prompt \"COT prompt\" in the remainder of this section. To generate these samples we take the following steps (see also Multimodal Input 2 Images, Few-Shot in the middle of Fig 2):\n1) We take a random example from the same game as presented in the task from an unseen session.\n2) We use the same CoT prompt as for the final engagement evaluation task but modify the prompt leaving only the correct option for the decision.\n3) We amend the prompt with the correct evaluation based on the ground truth (see Ground Truth Engagement on Fig. 2).\n4) We add a Reasoning Prompt to instruct the model to provide reasoning for the ground truth evaluation. By removing incorrect options but using the same CoT prompt when generating positive and negative examples, we ensure that the algorithm's output is formatted the same way as for the downstream task, including the description, comparison, reasoning, and decision. We use these outputs to construct an artificial history of positive and negative examples, which are added to the final prompt for the engagement evaluation task. For this final step we provide the CoT prompt with the example images as a question, and the example output as an answer; then finally we provide a set of unseen images with the CoT prompt and instruct the LLM to evaluate engagement the same way it would for a one-shot experiment. Figures 15 and 16 in the Appendix detail the process starting from example generation all the way to engagement prediction."}, {"title": "V. RESULTS", "content": "This section presents the main results of the experiments performed as follows. In Section V-A we outline the setup of the experiments reported and in Section V-B we discuss our exploratory findings. In Section V-C we examine LLM performance across different input modalities for the engagement evaluation task. Section V-D presents the results of our few-shot prompting experiments, and finally Section V-E takes a qualitative lens in our attempt to explain and justify our core findings."}, {"title": "A. Experimental Setup", "content": "We compare the engagement labels generated by LLMs to an engagement ground truth calculated from 3-second time windows of GameVibe annotation traces as outlined in Section III-B. We introduce and vary two hyperparameters in this process:\n1) A temporal shift compared to the observed video frame (\u0394t). This is similar to what the literature often refers to as input lag [42]. While this correction is generally used to account for reaction time, here we use it to control the temporal difference between the observed frames and the ground truth (see Fig. 4).\n2) A preference threshold (\u03b8), taking values between 0 and 1, that determines whether a difference between the ground truth value of two consecutive time windows is considered a change (increase or decrease) in engagement; e.g. \u03b8 = 0.05 considers windows which have a difference of more than 5% when evaluating engagement change."}, {"title": "B. Sensitivity Analysis", "content": "We experiment with the temporal shift \u0394t \u2208 {0, -0.5, -1, -1.5, -2, -2.5, -3} and preference threshold \u03b8 \u2208 {0,0.01, 0.05,0.1} parameters\u2014introduced in the previous section using the 7, 13, and 34 billion parameter version of LLaVA, and GPT-4o. The combinations of these parameters, however, result in 112 experimental setups for each game. Due to space considerations we only present the best performing subset of these hyperparameters (\u0394t\u2208 {\u22121,-2} and \u03b8 \u2208 {0.01,0.05}). We run these experiments with the Multimodal Input - 1 Image strategy as described in Section IV-B. We chose this setup for the initial parameter tuning because this is the most straightforward setup involving only one image and one text prompt.\nFigure 5 presents the \u0394A performance across two \u0394t and \u03b8 values. We can observe that larger \u0394t and \u03b8 values tend to yield higher performance; it also appears that the model size and architecture have a higher impact on \u0394A. While LLaVA- 7b and LLaVA-34b consistently perform significantly worse than the baseline-measured with Student's t-Test at significance level \u03b1 < 0.05 corrected with the Bonferroni method, accounting for repeated measurements\u2014GPT-4o shows performance comparable to the baseline. Interestingly, LLaVA-13b outperforms the larger LLaVA model and is not significantly worse than the baseline performance.\nThe best performing hyperparameter set is \u0394t = -2 and \u03b8 = 0.05 both in terms of average and single-game performance. The best performances are as follows: LLaVa- 34b improves the baseline by 37% on Blitz Brigade; and GPT-4o by 33%, 29%, and 26% on Doom (1993), Wolfram (2012), and Blitz, Brigade, respectively. Interestingly, we can see comparable performances with other models and configurations on single games. The most indicative of these is the LLaVA-7b model reaching 37% higher performance than the baseline on Wolfram with \u0394t = \u22121 and \u03b8 = 0.01. The average performance of the aforementioned setup, however, is lower that the performance of models tuned to \u0394t = -2 and \u03b8 = 0.05. This indicates that the models are sensitive to the games themselves and can't perform uniformly well across the whole dataset. Two striking examples are LLaVA- 13b, consistently outperforming every other model on Void Bastards (2019) and LLaVA-7b, consistently underperforming on CS:GO - Dust2 (2012).\nSome games are easier to predict than others, regardless of experimental setup. For example, Wolfram, Blitz Brigade, and PUBG are constantly listed within the top performing games in terms of \u0394A, whereas Heretic, Counter Strike 1.6, Overwatch 2 (2022), and HROT (2023) yield among the lowest \u0394A. It is important to note that games where engagement changes are predicted well by LLMs tend to have lower baselines (i.e. Wolfram: 57%; Blitz Brigade 52%; PUBG: 60%) whereas games where engagement is not predicted as well tend to have high baselines (i.e. Heretic: 73%; Counter Strike 1.6: 78%; Overwatch 2: 69%; and HROT: 61%). This indicates that engagement prediction is easier in game videos that feature more dynamic gameplay footage and a more uniform distribution of increasing vs. decreasing engagement labels.\nConsidering the overall performance of LLM engagement prediction across games, we fix our parameters for processing the ground truth at \u0394t = -2s and \u03b8 = 0.05 for the remaining experiments presented in this paper. As we observed high levels of performance across different LLM models, we continue our investigations experimenting with both LLaVA and GPT-4o models."}, {"title": "C. Text-based Engagement Prediction", "content": "In this section we examine the impact of text-based vs. multimodal prompting strategies on LLM performance. While in the former case we provide solely a text prompt to the model, in the latter case we feed both a text prompt and a corresponding image. Because the performance of LLMs can be affected even by small prompt variations [10], we experiment with both Basic and Advanced prompts. The prompting procedure for the text-based experiments are detailed in Section IV-C. Figure 6 shows the \u0394A performance of Text Input experiments compared to the best Multimodal Input - 1 Image models discussed in the previous section.\nIn this section our analysis focuses on the Text Input compared to the Multimodal 1 Image (Stitched) results presented in the previous section. This focus on text allows us to compare the Text Input method to a simple multimodal approach across different models. Our hypothesis is that the strategy of generating text-descriptions of frames first and then using these descriptions as part of the Text Input will improve model performance, because it essentially encodes the images in terms of action and player involvement. We thus assume that using this type of Text Input will present a better representation by discarding surface-level differences between frames and emphasising the structural differences.\nOverall, we can note that LLaVA-34b models perform significantly worse than the baseline across all modalities (Multimodal and Text) except when the text-only input is combined with Advanced Descriptions, but the performance still remains on the lower end of the spectrum. LLaVA-13b models yield performance values that are significantly below the baseline interdependently of the description setup. Finally, LLaVA-7b underperforms significantly on the multimodal task. It is somewhat surprising that while the larger LLaVA models generally perform better on multimodal tasks, the smallest model (7b) marginally outperforms the other two larger models of the LLaVA family when fed with text-only input. We hypothesise that this is due to the larger models' stronger tendency to fall into what Chochlakis et al. [16] call \u201cgravity wells of knowledge priors\u201d. This hypothesis is reinforced when we look at the best performing LLaVA models of Fig. 6. The better performing LLMs are usually fed with Basic instead of Advanced Descriptions. The added context seems to confuse the LLM or fails to orient the models to make accurate predictions. The same issue doesn't seem to affect the GPT- 4o model which performs consistently close to the baseline and better than the LLaVA family overall. While the GPT- 4o model performs marginally better on the text-input task using the Advanced Descriptions, the biggest improvement can be observed with Basic Descriptions on Doom with a 39% relative gain in accuracy.\nWith regards to the different prompting strategies we observe no significant difference in performance between Basic and Advanced Descriptions for Text Input, among the models tested. While some prompting techniques appear to help certain models to perform well in certain games, there is no apparent overarching pattern we can analyse. It also seems that any performance outliers can mostly be explained through the particularities of the data and the chosen algorithm. Some indicative examples of this observation are the games CS:GO - Dust2, CS:GO - Office, and Doom where the discrepancy between the best and worst performing models is the largest. Conversely HROT, Apex Legends, and Medal of Honor 2010 have the least amount of performance variation across models and prompting strategies. It is worth noting that the models are only successful in predicting Apex Legends\u2014with GPT-4o reaching 31% \u2206A using Text Input - Basic Description. In general, LLaVA models appear to be more sensitive than GPT models to the input modality and prompting strategy, often"}, {"title": "D. Multi-Image One-Shot and Few-Shot Prompting", "content": "In this section we present experiments using Multimodal Input - 2 Image, One-Shot and Few-Shot strategies (see Fig. 2 and Section IV for more details on these approaches). In these experiments we opt to employ the GPT-4o model only; the reason for doing so is two-fold. First, GPT models have been observed to be more consistent and perform better across all games in experiments presented in the previous sections. Second, models of the LLaVA family are limited in how they can process images as input. As mentioned in Section IV-B LLaVA models can only take single images in their input space, while GPT-4o uses a tile-based input tokenizer that is able to handle multiple images.\nFigure 7 presents the results of our Multimodal Input - 2 Image, One-Shot and Few-Shot experiments compared to the best overall Text Input and Multimodal Input - 1 Image results obtained using GPT-4o. We can see that the best overall performance is achieved when using Few-Shot prompting. While the relative improvement over the baseline is not significant across all games, there is a clear pattern of improvement compared to other models. Comparing results between Fig. 6 and Fig. 7, can see that GPT-4o models significantly outperform LLaVA models on several experimental setups. These setups include LLaVA-7b on the Multimodal Input - 1 Image (Stitched) task; LLaVA-13b models on both Text Input setups; and LLaVA- 34b on the Text Input - Basic Description and Multimodal Input - 1 Image (Stitched) tasks. While there is no significant difference between one-shot and few-shot prompting, the latter strategy improves the performance in 13 out of 20 experimental settings. We note only 6 out of 20 settings where the introduction of few-shot prompting decreased the performance. Our findings are aligned with results reported in the literature [63], [66], [64] suggesting that a few-show, multimodal, chain-of-thought prompting method can significantly improve LLM performance. However even with this performance boost, the observed models barely surpass the majority baseline, on average, across games.\nLooking at the best and worst performances of GPT-4o across games we observe a familiar pattern. Once again, the games whose engagement is easier to predict are Wolfram (38%), Apex Legends (27%), and Doom (23%) when we look at the average performance across both the GPT-4o One-Shot and Few-Shot settings. Similarly, the games where the LLM models performed worst on average are Counter Strike 1.6 (-40%), Corridor 7 (-25%), and Heretic (-17%). These findings are in line with our previous experiments."}, {"title": "E. Qualitative Analysis", "content": "In this section we outline the reasons for the observed poor performance of the tested LLMs and analyse why certain games are easier to predict. For our analysis we are looking at the highest performing model, the GPT-4o with Multimodal Input - 2 Images using Few-Shot prompting. Employing this model we list 5 games where the \u0394\u0391 exceeds 25%: Doom, Wolfram, PlayerUnknown's Battlegrounds (PUBG) (2018), Apex Legends, and Borderlands 3 (2019). Conversely, the five games, where the performance was well-below the baseline are as follows: Corridor 7, Heretic, Counter Strike 1.6, Medal of Honor (2010), and HROT; see Fig. 8.\nA qualitative analysis of the games where LLMs perform best (vs. those where they perform worst) reveals some possible underlying reasons that could influence these models. The five games where LLMs perform best are fast paced, with short bursts of action separated by similarly short navigation sequences. The game scenes are well-lit or stylized in a way that is easy to read. In contrast, the five games where LLMs fail to assign engagement labels feature repetitive sections of navigation with limited gameplaying action such as shooting, reloading, collecting items, or dodging fire. These games also tend to feature dark backgrounds and enemies with silhouettes that are difficult to distinguish, or they take place in drab environments where the ground, background, and often non- player characters blend together. A representative example that highlights these performance differences are the Counter Strike game variants existent in the dataset; see Fig. 9. Compared to the best performance of the multimodal few-shot GPT-4o on Counter Strike 1.6 (33% worse than baseline), the same model on CS:GO - Dust2 has a performance comparable to baseline levels. Even though these two games use essentially the same level, the visuals of CS:GO - Dust2 are much clearer; in Counter Strike 1.6 the background and foreground are harder to separate visually. In CS:GO - Office\u2014where the visuals are arguably even more readable\u2014the model showcases much higher predictive capacity (i.e. 19% higher than the baseline).\nAnother way to explain the fluctuation in LLM performance is the familiarity of the model with the games per se. We observe that more popular games (such as Counter Strike, Apex Legends, and PUBG, with a peak viewership\u00b9\u2070 of 1,914,861, 674,070, and 597,663, respectively on Twitch\u00b9\u00b9 yield generally better engagement predictions compared to less popular games (such as HROT, Heretic, and Corridor 7, with a peak viewership of 24, 721, 2,280, and 195 on Twitch\u00b9\u00b9), although we should be careful with naive over-generalizations\n\u0394\u0391 = (A_{LLM} - A_b)/A_b"}, {"title": "VI. DISCUSSION", "content": "The evaluation experiments presented in this paper are the first of its kind for LLM-based engagement prediction in games. While collectively we tried 2, 440 combinations of experimental settings-varying the LLM model type, model size, prompting strategy, input type, and ground truth processing\u2014 there are still many aspects that we did not explore in this initial study. We argue, however, that we set out to lay ground works for future research by approaching the problem of automating gameplay annotation in a relatively straightforward way. While, for instance, we experimented with several out- of-box LLM models and prompting strategies, we kept the granularity of the vision input constant which potentially poses a core limitation to this initial study. Since we sample the videos in question at a 3-second interval, the model loses a lot of information between these frames. Although we briefly experimented with different time intervals (i.e. between 1 and 5 seconds), simply increasing the sampling rate did not yield a performance increase. It is likely, however, that by either providing more frames per query or using video input directly would lead to a significant performance improvement that remains to be tested in future studies. These investigations were purposefully left out of the scope of the current study, mainly because (at the time of writing) there were no widely available video models which could have have fit into the experimental protocol presented here.\nWhile video input could feed more information"}]}