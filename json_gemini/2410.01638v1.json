{"title": "DATA EXTRAPOLATION FOR TEXT-TO-IMAGE GENERATION ON SMALL DATASETS", "authors": ["Senmao Ye", "Fei Liu"], "abstract": "Text-to-image generation requires large amount of training data to synthesizing high-quality images. For augmenting training data, previous methods rely on data interpolations like cropping, flipping, and mixing up, which fail to introduce new information and yield only marginal improvements. In this paper, we propose a new data augmentation method for text-to-image generation using linear extrapolation. Specifically, we apply linear extrapolation only on text feature, and new image data are retrieved from the internet by search engines. For the reliability of new text-image pairs, we design two outlier detectors to purify retrieved images. Based on extrapolation, we construct training samples dozens of times larger than the original dataset, resulting in a significant improvement in text-to-image performance. Moreover, we propose a NULL-guidance to refine score estimation, and apply recurrent affine transformation to fuse text information. Our model achieves FID scores of 7.91, 9.52 and 5.00 on the CUB, Oxford and COCO datasets. The code and data will be available on GitHub.", "sections": [{"title": "INTRODUCTION", "content": "Text-to-image generation aims to synthesize images according to textual descriptions. As the bridge between human language and generative models, text-to-image generation (Reed et al., 2016b; Ye et al., 2023; Sauer et al., 2023; Rombach et al., 2022; Ramesh et al., 2022)is applied to more and more application domains, such as digital human (Yin & Li, 2023), image editing (Brack et al., 2024), and computer-aided design (Liu et al., 2023). The diversity of applications leads to a large number of small datasets, where existing data are not sufficient to train high-quality generative models, and generative large models cannot overcome the long-tail effect of diverse applications.\nTo augment training data, existing methods typically rely on data interpolation techniques such as cropping, flipping, and mixing up images (Zhang et al., 2017). While these methods leverage human knowledge to create new perspectives on existing images or features, they do not introduce new information and yield only marginal improvements. Additionally, Retrieval-base models (Chen et al., 2022; Sheynin et al., 2022; Li et al., 2022) employs retrieval methods to gather relevant training data from external databases like WikiImages. However, these external databases often contain very few images for specific entries, and their description styles differ significantly from those in text-to-image datasets. Furthermore, VQ-diffusion (Gu et al., 2022) pre-trains its text-to-image model on the Conceptual Caption dataset with 15 million images, but the resulting improvements are not obvious.\nIn this paper, we explore data linear extrapolation to augment training data. Linear extrapolation can be risky, as similar text-image pairs may not be nearby in Euclidean space. For information"}, {"title": "RELATED WORK", "content": "GAN-based text-to-image models. Text-to-image synthesis is a key task within conditional image synthesis (Feng et al., 2022; Tan et al., 2022; Peng et al., 2021; Hou et al., 2022). The pioneering work of (Reed et al., 2016b) first tackled this task using conditional GANS (Mirza & Osindero, 2014). To better integrate text information into the synthesis process, DF-GAN (Tao et al., 2022) introduced a deep fusion method featuring multiple affine layers within a single block. Unlike previous approaches, DF-GAN eliminated the normalization operation without sacrificing performance, thus reducing computational demands and alleviating limitations associated with large batch sizes. Building on DF-GAN, RAT-GAN employed a recurrent neural network to progressively incorporate text information into the synthesized images. GALIP (Tao et al., 2023) and StyleGAN-T (Sauer et al., 2023) explore the potential of combining GAN models with transformers for large-scale text-to-image synthesis. However, the aforementioned GAN-based models often struggle to produce high-quality images.\nDiffusion-based text-to-image models. Recently, diffusion models (Ho et al., 2020; Song & Ermon, 2019; Song et al., 2021; Hyv\u00e4rinen, 2005) have demonstrated impressive generation performance across various tasks. Building on this success, Imagen (Saharia et al., 2022) and DALL\u00b7E 2 (Ramesh et al., 2022) can synthesize images that are sufficiently realistic for real-world applications. To alleviate computational burdens, they first generate 64\u00d764 images and then upsample them to high-resolution using another diffusion model. Additionally, the Latent Diffusion Model (Rombach et al., 2022) encodes high-resolution images into low-resolution latent codes, avoiding the exponential computation costs associated with increased resolution. DiT (Peebles & Xie, 2023)"}, {"title": "LINEAR EXTRAPOLATION FOR TEXT-TO-IMAGE GENERATION", "content": "In this section, we begin by collecting similar images from the internet. Next, we explain how to extrapolate text descriptions. Following that, we use the extrapolated text-image pairs to train a diffusion model with RAT blocks. Finally, we sample images using NULL-condition guidance."}, {"title": "COLLECTING SIMILAR AND CLEAN IMAGES", "content": "Linear extrapolation requires the images to be sufficiently close in semantic space. Hence, we automatically retrieve similar images by searching for their classification labels. However, search engines return both similar images and outliers. To eliminate unwanted outliers, we employ a cluster detector for irrelevant outliers and a classification detector for similar outliers. For the cluster detector, each image is encoded into a vector using the CLIP image encoder. Images retrieved with the same keyword are then clustered using K-means. If the distance from the cluster center to dataset images exceeds a threshold, this cluster is excluded. For the classification detector, we train a fine-grained classification model on the original dataset, which assigns a label to each web image. If the label does not match with the search keyword, corresponding image is then excluded."}, {"title": "LINEAR EXTRAPOLATION ON TEXT FEATURE SPACE", "content": "Here we introduce how to extrapolates text descriptions for web images. Assuming that web images are sufficiently close to dataset images in semantic space, each web image can be represented by nearest k images:\n$\\arg \\min |f - F \\times w|^2$,\nwhere $w = [w_1, w_2, ..., w_k]$are the reconstruction weights and $F = [f_1, f_2, ..., f_k]$ are the image features of dataset images produced by CLIP image encoder. Since the above equation is a super-determined problem, we solve this coefficient using least squares:\n$w = (F^T F)^{-1} F^T f$.\nWe assume that the image feature space and text feature space share the same local manifold. Hence, the image reconstruction efficient w can be used to compute the text feature of web images:\n$s = S \\times w$,\nwhere $S = [S_1, S_2, ..., S_k]$ is the fake sentence features for nearest k dataset images, and s is the sentence feature for a web image."}, {"title": "RECURRENT DIFFUSION TRANSFORMER ON LATENT SPACE", "content": "The training objective of the diffusion model is the squared error loss proposed by DDPM (Ho et al., 2020):\n$L(\\theta) = |\\epsilon - \\epsilon_{\\theta} (\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon) ||^2$,\nwhere $ \\epsilon \\in N (0, 1)$ is the score noise injected at every diffusion step, and $\\epsilon_{\\theta}$ is the predicted noise by a diffusion network consisted of 12 transformer layers. $\\bar{\\alpha}_t$ and ${\\alpha}_t$ are hyper-parameters controlling the speed of diffusion. The work of score mismatching (Ye & Liu, 2024) shows that predicting the score noise leads to an unbiased estimation.\nNetwork architecture. As depicted in Fig 8, the diffusion network consists of transformer blocks. Recurrent affine transformation is used to enhance the consistency between transformer blocks. To avoid directly mixing text embedding and time embedding, we stack four transformer blocks as a RAT block and text embedding is fed into the top of each RAT block. Each RAT block applies a channel-wise shifting operation on a image feature map:\n$c' = c + \\beta$,\nwhere c is the image feature vector and $ \\beta $ is shifting parameters predicted by a one-hidden-layer multi-layer perception (MLP) conditioned on recurrent neural network hidden state $h_t$.\nIn each transformer block, we inject time embedding by a channel-wise scaling operation and a channel-wise shifting operation on c. At last, the image feature c is multiplied by a scaling parameter $\\alpha$. This process can be formally expressed as:\n$c' = Transformer((1 + \\gamma) \\cdot c + \\beta) \\cdot \\alpha$,\nwhere $ \\alpha, \\gamma, \\beta$ are parameters predicted by two one-hidden layer MLPs conditioned on time embedding.\nWhen applied to an image feature map composed of w \u00d7 h feature vectors, the same affine transformation is repeated for every feature vector.\nEarly stop of fine-tuning. Extrapolation may produces training data very close to the original dataset, which makes fine-tuning saturate very quickly. Excessive fine-tuning epochs would forget knowledge gained from the extrapolated data and overfit small datasets. As a result, the training loss of the diffusion model becomes unreliable. Therefore, fine-tuning should be stopped when the FID score begins to increase."}, {"title": "SYNTHESIZING FAKE IMAGES", "content": "Finally, we introduces how to synthesizing images from scratch. As depicted in Figure 8, the synthesis begins with sampling a random vector z from standard Gaussian distribution. And then, this"}, {"title": "EXPERIMENTS", "content": "NULL guidance. A sentence with no new information is able to boost text-to-image performance obviously. This guidance is inspired by Classifier-free diffusion guidance (Ho & Salimans, 2022) which uses a dummy class label to boost label-to-image performance. Similarly, we design CLIP prompt without obvious visual meaning and embed them into the diffusion model. Specifically, we denote the original score estimation based on text description as $\\epsilon_{text}$ and score estimation based on null description as $\\epsilon_{null}$. Then we mix these two estimations for a more accurate estimation $\\epsilon'$:\n$\\epsilon' = (\\epsilon_{text} - \\epsilon_{null}) \\times \\eta + \\epsilon_{null}$,\nwhere, $ \\eta$ is the guidance ration controlling the balance of two estimations. When $ \\eta $ = 1, NULL Guidance falls back to an ordinary score estimation. Usually a NULL prompt with the average meaning of the dataset achieve the best performance."}, {"title": "COMPARISONS WITH OTHERS", "content": "Quantitative results. We present results for the CUB dataset of bird images, the Oxford-102 dataset of flower images, and the MS COCO dataset of common objects, as shown in Table 1. On the CUB dataset, our model achieve an IS score of 6.56 and an FID score of 6.36, outperforming all the previous models. For the Oxford dataset, we achieve an IS score of 4.35 and an FID score of 6.36, outperforming all the previous models. On the COCO dataset, our model achieves an FID score of 5.00 that is competitive with previous best result.Compared with VQ-Diffusion, our model uses less training data and achieve much better performance. This comparison reveals that pre-training on large datasets can be inefficient and lead to suboptimal results. Moreover, results in Table 1 reveal that Inception model pre-trained on ImageNet is less sensitive than fine-tuned on small datasets. Additionally, the Inception score on the Oxford dataset exceeds that of real images (4.10). Extensive results demonstrate the effectiveness and generalization ability of the proposed data extrapolation method.\nQualitative results. We present qualitative results for the CUB dataset of bird images and the Oxford-102 dataset of flower images. In Figure 3, we compare the visualization results of DF-GAN, RAT-GAN, and our model. DF-GAN and RAT-GAN are previous state-of-the-art methods for text-to-image synthesis. On the CUB dataset, with more clear details such as feathers, eyes, and"}, {"title": "ABLATION STUDIES", "content": "Analysis of outlier detectors. In Table 2, we present text-to-image results without cluster detector or classification detector. According to ID 0,1 and 2, the FID score without outlier detectors degrade severely because noisy images force the diffusion model to generate irrelevant objects. Although fine-tuning on small datasets could alleviate noise pollution but parameters also forget general knowledge at the same time. According to ID 2 and 3, classification detector performs better than cluster detector because it has utilized fine-grained classification labels.\nAnalysis of extrapolation quantity. More images generally lead to improved text-to-image results. however, this trend saturates around 100,000 images, after which the improvement in FID becomes less significant with more training samples. This phenomenon aligns with that diffusion models perform much better than GANs on the COCO dataset (84K images) but exhibit similar performance to GANs on the CUB and Oxford datasets( 10K images). Furthermore, with transformers as core building blocks, GALIP performs similarly to previous models on the CUB dataset. This suggests that transformer architectures exacerbate the need for larger training datasets.\nAnalysis of NULL guidance. The performance of NULL guidance is influenced by both the NULL prompt and the guidance ratio. The results in Table 4 indicate that a NULL prompt reflecting the average meaning of the dataset achieves the best performance. Additionally, a suitable guidance ratio is crucial for optimal results, and we find that a ratio around 1.5 yields the best performance on the CUB and COCO datasets. However, on the Oxford dataset, NULL guidance improves the Inception Score from 4.10 to 4.35 but degrades the FID score from 9.52 to 11.07.\nAnalysis of text injection. Text injection is crucial for text-to-image generation. As shown in ID 4 and 5 of Table 2, RAT significantly improves the FID score. Further experiments indicate that directly mixing text feature with time embedding results in an FID score of 25.41, which is much worse than 16.74 achieved by RAT. This suggests that time embedding provides information very different to text embedding. Additionally, incorporat-ing a scaling operator into RAT can lead to model collapse, as information becomes highly compressed in latent space. Consequently, the mean value of the latent code becomes sensitive, and the scaling operation disrupts the information structure."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose a new data augmentation method for text-to-image generation using linear extrapolation. Specifically, we apply linear extrapolation only on text data, and new image data are retrieved from the internet by search engines. For the reliability of new text-image pairs, we design two outlier detectors to purify retrieved images. Based on extrapolation, we construct training samples dozens of times larger than the original dataset, resulting in a significant improvement in text-to-image performance. Moreover, we propose a NULL-condition guidance to refine the score estimation for text-to-image generation. This guidance is also applicable to existing text-to-image models without further training. In the future, linear extrapolation and NULL-condition guidance could be applied to tasks beyond text-to-image generation."}]}