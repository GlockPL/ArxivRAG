{"title": "TRAINING ULTRA LONG CONTEXT LANGUAGE MODEL WITH FULLY\nPIPELINED DISTRIBUTED TRANSFORMER", "authors": ["Jinghan Yao", "Sam Ade Jacobs", "Masahiro Tanaka", "Olatunji Ruwase", "Aamir Shafi", "Hari Subramoni", "Dhabaleswar K. (DK) Panda"], "abstract": "Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language\nprocessing and computational biology, such as text generation and protein sequence analysis. However, training\nLLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading\nhigher costs and greater complexity. Alternative approaches that introduce long context capabilities via\ndownstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully\nPipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware\nefficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the\nsame hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design,\nwe can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of\nMFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across\ndifferent LLM models. The code is available here.", "sections": [{"title": "1 INTRODUCTION", "content": "The rapid advancement of large language models (LLMs)\nhas significantly impacted natural language processing\n(NLP), driving improvements across a wide range of applica-\ntions. As LLMs like GPT-4, Claude, and Gemini become in-\ncreasingly capable of processing regular prompts, there is a\ngrowing demand to extend their context windows to accom-\nmodate longer input sequences. This capability is crucial\nfor a variety of applications, including comprehensive docu-\nment analysis, where models must process entire legal doc-\numents or scientific papers (Peng et al., 2023; Xiong et al.,\n2023); long-form content generation, such as writing books\nor detailed reports; maintaining coherent and contextually\nrelevant long-term dialogues in conversational AI (Beltagy\net al., 2020; MosaicML, 2023; Munkhdalai et al., 2024;\nTouvron et al., 2023); and handling complex multi-step rea-\nsoning tasks in fields like healthcare (Gao et al., 2021; Li\net al., 2022; Zvyagin et al., 2023), climate (Nguyen et al.,\n2023), and finance (Eisfeldt et al., 2023; Kim et al., 2023;\n2024; Li et al., 2023; Yang et al., 2023).\nHowever, LLM training is typically constrained to relatively\nshort context lengths, such as 8K or 32K tokens. Currently,\nmost LLMs utilize rotary position embedding (Su et al.,\n2024) (ROPE) to encode input tokens. Despite its improved\nefficiency and extrapolation capability compared to regu-\nlar position embeddings (Vaswani, 2017), to accommodate\nmuch longer prompts during inference, RoPE often requires\naggressive rescale and map. These adjustments struggle in\nproperly adapt models to longer context before the model's\nperformance deteriorates, leading to a collapse in the qual-\nity of its outputs. This presents a critical challenge: the\nnecessity to train LLMs originally on the desired long con-\ntext lengths to ensure robust performance across varying\napplications.\nThere are multiple difficulties in training long-context LLMs.\nFor a given-sized LLM model, as we increase the sequence"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Memory-efficient Transformer", "content": "The substantial memory demands of Transformers have\nspurred extensive research into memory-efficient attention\nmechanisms to facilitate their application to longer se-\nquences. FlashAttention (Dao, 2023; Dao et al., 2022) has\nemerged as the de facto standard due to its robust and versa-\ntile performance. FlashAttention employs an online softmax\ntechnique to avoid the materialization of the attention matrix,\nthereby reducing memory overhead from $O(N^2)$ to $O(N)$\nwhile preserving accuracy. Other notable strategies include\nlow-rank approximations (Katharopoulos et al., 2020; Wang\net al., 2020), kernel-based methods (Kitaev et al., 2020;"}, {"title": "2.2 Long context training", "content": "Recent advancements in Transformer architectures have\nsignificantly enhanced their capability to process long se-\nquences, which is crucial for tasks that require extensive\ncontextual understanding. This section reviews pivotal con-\ntributions in this domain, each addressing the inherent mem-\nory limitations of standard Transformer models, while also\npointing out some of their practical challenges.\nMegatron-SP(Korthikanti et al., 2023) adopts a sequence\nparallelism technique which is tightly integrated with its\ntensor parallelism. In this approach, sequences are parti-\ntioned along the sequence dimension, and all-gather and\nreduce-scatter collectives are employed to aggregate the\nQKV (query, key, value) projections for attention computa-\ntion. The communication complexity analysis indicates that,\nin contrast to our approach, the communication volume in\nMegatron-SP's sequence parallelism increases linearly with\nthe message size (M) regardless of the number of compute\ndevices.\nThe Blockwise Parallel Transformer (BPT) (Liu & Abbeel,\n2024) employs a blockwise computation strategy for both\nself-attention and feedforward layers, optimizing memory\nusage and allowing the processing of sequences much longer\nthan traditional Transformers. However, despite its effi-\nciency, BPT requires careful tuning of block sizes and mem-\nory management to avoid diminishing returns on perfor-\nmance when scaling to extremely long sequences.\nRing Attention (Liu et al., 2023) enhances Transformer's\nscalability by distributing long sequences across multiple\ndevices. This innovative approach overlaps the communica-\ntion of key-value pairs with the computation of blockwise\nattention, effectively increasing the feasible sequence length\nproportionally to the number of available devices. However,\nreliance on device count for scaling and multi-step commu-\nnications introduces potential issues in environments with\nsub-optimal hardware, where performance can be unpre-\ndictably affected by network latency and bandwidth con-\nstraints.\nDeepSpeed Ulysses (Jacobs et al., 2023) tackles the chal-\nlenges of sequence parallelism by partitioning input data\nalong the sequence dimension and utilizing an efficient all-\nto-all collective communication strategy for attention com-\nputations. Although this method maintains a constant com-\nmunication volume regardless of the increase in sequence\nlengths and device counts, achieving significant speedups\nand scalability, it may still encounter practical hurdles in\ndeployment related to large-scale clusters and the optimiza-\ntion of collective communication patterns across diverse\ncomputing environments.\nEach of these methods uniquely contributes to the field of\nlong-sequence Transformer training, offering solutions fo-\ncused on memory efficiency, communication overhead, and\ncomputational speed. However, the common requirement\nfor substantial GPU resources in all these approaches under-\nscores a critical barrier to broader adoption and scalability.\nThis challenge highlights the need for future research to\ndevelop more resource-efficient methods that can deliver\nsimilar benefits with a modest hardware budget."}, {"title": "3 PRELIMINARY", "content": ""}, {"title": "3.1 GPU memory requirements in distributed\nTransformer", "content": "As Transformer becomes the de facto most powerful and\nubiquitous architecture in today's generative models, its\noverall pipeline also becomes homogeneous. We use the"}, {"title": "3.2 Combining DeepSpeed sequence parallel and\nZERO", "content": "Among sequence parallel strategies, DeepSpeed-Ulysses\nexcels with its highly efficient communication pattern and is\ncomplementary to the most advanced model-based training\nschemes such as DeepSpeed ZeRO. We first recap the key\nfeature of DeepSpeed Ulysses, and how it can work with\nZeRO-3."}, {"title": "4 DESIGN OF FULLY PIPELINED\nDISTRIBUTED TRANSFORMER", "content": "In this section, we start by introducing how to design a seam-\nless and efficient pipelined distributed attention with offload-\ning. Since different hardware hierarchies are involved in the\ndesign, e.g. Tensor cores, NVLINK, PCIe, etc, each with\ndifferent data throughput and latency, they are required to\ncoordinate carefully."}, {"title": "4.1 Pipelining and scheduling", "content": "As QKV projection, Alltoall communication, attention, and\nFFN will create multiple intermediate buffers, leading to\nsevere memory spikes, especially during the backward pass,\nto make the sequence computation in the Transformer block\nfully pipelined and memory efficient, our chunk and offload-\ning design will start with the initial input tensor (i.e. hidden\nstate). Firstly, we use the following notations throughout\nthe paper for ease of explanation. For operations other than\nthe distributed attention, each GPU holds and processes\na $[b, S_{local}, h_{global}, d]$ tensor, where $s_{local}$ denotes the lo-\ncal sequence length, $h_{global}$ denotes the total number of\nheads. For the distributed attention, each GPU will pro-\ncess the entire sequence, but with specific heads, which is a\n$[b, S_{global}, h_{local}, d]$ tensor.\nFor the first QKV projection, since tokens are pro-\ncessed elementwise, we directly slice the local sequence\ntensor $[b, s_{local}, h_{global}, d]$ into $u$ chunks, each as a"}, {"title": "4.2 Double buffering", "content": "Though the idea of using host memory to hold unused se-\nquences is intuitive, the unmatched hardware transfer band-\nwidth poses a significant challenge in fully exploiting com-\nputing power. For a typical HPC node, GPUs are connected\nthrough high-bandwidth NVLink, which can reach more\nthan 100 GB/s of peer-to-peer bandwidth. However, the\ncommon PCIe Gen-4 link with 16 lanes only provides 32\nGB/s of unidirectional bandwidth, which also requires the\nhost memory and GPU to be in the same NUMA domain."}, {"title": "5 EVALUATION", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Models: We conduct our main experiments using the GPT\nand Llama models, with model sizes ranging from 2.7B\nto 70B. By default, we enable activation checkpoint with\nCPU offloading. To fully exploit the potential of FPDT, we\nuse DeepSpeed ZeRO-3 to partition the model parameters\nacross the sequence parallel group (refer to 3.2). We also\nset the batch size to 1, as this allows us to test the maximum\nsequence length we can reach.\nExperimental Environment: We use multiple GPU nodes,\neach with four A100 80 GB, connected via 3rd-Gen NVLink.\nThere are two CPU sockets. The PCIe between host and\ndevice has a theoretical uni-directional bandwidth of 32\nGB/s. Each node is equipped with 1 TB of host memory.\nFor the internode connection, we use NVIDIA 200 Gbps\nHDR InfiniBand."}, {"title": "5.2 Overall Performance", "content": "There are several widely used solutions for training long-\ncontext language models. Megatron-SP (Korthikanti et al.,\n2023) partitions sequence activations and leverages tensor\nparallel. DeepSpeed Ulysses (Jacobs et al., 2023) adopts a\none-step Alltoall to gather tokens and scatter heads among\nall GPUs. In this section, We compare our proposed design\nwith these state-of-the-art solutions.\nWe choose six widely used LLM models with different sizes.\nFor GPT-like ones, we have 2.7B, 6.7B, 13B, and 30B. For\nLlama, we use the 8B and 70B models. For Megatron-SP,\nwe follow its default parallel setting, which leverages tensor"}, {"title": "5.3 Tradeoff on Sequence Chunk Size", "content": "As discussed in 4.2, choosing a proper chunk size can not\nonly exploit the computing power of the hardware but also\nallow the data moving from host to device and from device\nto host to overlap by computation. In Table 11, we use a\ndefault chunk size of 64K for all our FPDT-based methods.\nIn this part, we will demonstrate why this is our preference.\nTable 12 shows the usage of GPU HBM at different chunk\nlengths. We conduct the profiling on a single node with 4\nGPUs, with a fixed global sequence length of 256K. We\nchange the length of each sequence chunk and compare the\ncorresponding memory footprint and MFU. The chunk size\nof 256K means that we do not use chunk, but just run the\nbaseline Ulysses. 8K, 16K, 32K, 64K, and 128K of chunk\nsize corresponds to 32, 16, 8, 4, and 2 chunks in total in our\nFPDT pipeline scheme. As shown in Figure 12a, 12b, 12d,\nour FPDT can significantly reduce the activation memory\nfootprint; for example, in the 2.7B model, we reduce the\nactivation memory from 27GB to 18GB by splitting the\nsequence into two chunks. Despite that increasing the num-\nber of chunks can further reduce the memory footprint, we"}, {"title": "5.4 Chunk Granularity", "content": "As we analyzed in table 2, in forward and backward passes,\nattention operation and FFN can incur different amounts of\nintermediate buffers, therefore, different chunking strategies\nneed to be applied. The chunking and offloading strategies\nof the attention part have been introduced in 4.2. For FFN,\nhowever, we cannot easily leverage offloading to reduce\nGPU memory consumption without significantly sacrificing\nhardware efficiency. For token-wise operations such FFN,\nthe complexity of computation increases linearly, i.e. $O(N)$.\nIn this case, $F(N) = \u0398(G(N))$, where F and G are the com-\nplexities of compute and memory, respectively. Considering\nthe high throughput of GPU compute cores, the latency of\noffloading and prefetching can never be overlapped by com-\nputation, thus, for FFN, we don't use offloading. Figure 13\nare the memory profiles created by the PyTorch profiler\n(note that PyTorch randomly picks colors in each profile),\nwe found that for the GPT and Llama models, setting the\nnumber of chunks in the FFN to be twice that of the attention\nis sufficient to ensure that the attention part strictly binds\nthe memory footprint.\nAlso noteworthy is that since we do not offload chunks in\nFFN, as long as the size of each chunk is not too small,\nthe overall training throughput would not be significantly\naffected. We also want to emphasize that another memory\nspike during training is found in the final calculation of\nsoftmax and cross-entropy loss. As the vocabulary size\nis much larger than the model's hidden dimension, and\nthe operation usually requires a Float32 data type, the\nlast linear project would lead to the out-of-memory issue.\nHowever, this part can be solved by chunking as well, and\nsince it is at the end of the forward pass, the number of\nchunks used in this part is trivial to the overall performance.\nThus, we suggest that setting it to $\\frac{hidden\\_size}{vocab\\_size} x 2$, would\nsolve the memory spike."}, {"title": "5.5 Training strategies in long-context LLM", "content": "Though we discussed our experiments' setup in 5.2, we\nwould like to know how each strategy contributes to the\nlong-context LLM training. Tensor parallel is widely used\nin distributed model training. It allows each GPU to only\nkeep a slice of the tensor along the hidden dimension, hence"}, {"title": "5.6 Convergence evaluation", "content": "Figure 14 shows the convergence of the baseline GPT model\nthat leverages tensor parallel on 4 GPUs, with a batch size\nof 256 and ZeRO-1 enabled, and our FPDT w/ and w/o of-\nloading. Our proposed FPDT is a pure system optimization\ntechnique that enables the training of ultra-long sequence\nTransformer model, thus there is no (negative) on the quality\nof trained models."}, {"title": "6 FUTURE WORK", "content": "This paper focuses on alleviating the memory constraints\nthat are brought by the activations and intermediate buffers\nin long-sequence LLM training. We conduct experiments\nwith the ZeRO-3 technique. However, we noticed that Py-\nTorch here can also incur a high memory spike when it\nreduces the gradients across all GPUs. In certain cases, this\nmemory spike can be more significant than the activation's\nmemory spikes, which becomes a bottleneck in keeping\nincreasing sequence length. We will investigate this and\nwelcome researchers in this field to advance long-sequence\nLLM training together."}, {"title": "7 CONCLUSION", "content": "In this paper, we present the Fully Pipelined Distributed\nTransformer (FPDT), for efficiently training long-sequence\nLLMs within resource-constrained environment. Our pro-\nposed method leverages advanced sequence parallelism and\nZeRO-3, largely reducing the GPU resource required for\nmillion-level sequence training. With our elaborately de-\nsigned overlapping scheme, training 2.7B to 70B LLMs\non up to 4M token sequence with FPDT reaches over\n55% MFU. Our method can also be applied to LLMs with\nTransformer-like blocks with little re-configuration. We be-\nlieve our work can benefit the large community in exploring\nLLMs' capability in long context scenarios."}]}