{"title": "Swin-BERT: A Feature Fusion System designed for Speech-based Alzheimer's Dementia Detection", "authors": ["Yilin Pan", "Yanpei Shi", "Yijia Zhang", "Mingyu Lu"], "abstract": "Abstract-Speech is usually used for constructing an automatic Alzheimer's dementia (AD) detection system, as the acoustic and linguistic abilities show a decline in people living with AD at the early stages. However, speech includes not only AD-related local and global information but also other information unrelated to cognitive status, such as age and gender. In this paper, we propose a speech-based system named Swin-BERT for automatic dementia detection. For the acoustic part, the shifted windows multi-head attention that proposed to extract local and global information from images, is used for designing our acoustic-based system. To decouple the effect of age and gender on acoustic feature extraction, they are used as an extra input of the designed acoustic system. For the linguistic part, the rhythm-related information, which varies significantly between people living with and without AD, is removed while transcribing the audio recordings into transcripts. To compensate for the removed rhythm-related information, the character-level transcripts are proposed to be used as the extra input of a word-level BERT-style system. Finally, the Swin-BERT combines the acoustic features learned from our proposed acoustic-based system with our linguistic-based system. The experiments are based on the two datasets provided by the international dementia detection challenges: the ADRESS and ADRESSo. The results show that both the proposed acoustic and linguistic systems can be better or comparable with previous research on the two datasets. Superior results are achieved by the proposed Swin-BERT system on the ADRESS and ADRESSo datasets, which are 85.58% F-score and 87.32% F-score respectively.\nIndex Terms: Alzheimer's dementia, character-level tran-scripts, shifted windows attention, rhythm-related information", "sections": [{"title": "I. INTRODUCTION", "content": "Automatic detection of Alzheimer's dementia (AD) from spontaneous speech has the potential to be applied as a cheap, easy-to-use, and objective clinical assistance tool for dementia detection. Speech-based automatic AD detection has shown its efficiency in enabling early treatment of people living with AD, which can slow down AD deterioration. Although the changes of a person's speech can often be seen many years before diagnosis [1], the audio recordings include multiple information, such as cognitive status, age [2], [3] and gender [4]. Lack of consideration of age and gender can affect the efficiency of the extracted acoustic features for dementia detection [5], [6]. When learning the cognitive status related acoustic information in dementia detection, both\nthe global and local acoustic related information have shown distinctive patterns throughout previous research clinically [7] or automatically [8], [9]. Swin Transformer [10] was designed to leverage the global and local information embedded in the image and speech [11] based shifted windows multi-head attention (SW-MHA), which is expected to be used for acoustic-based dementia detection in our paper.\nAs dementia progresses, almost all aspects of language can be affected [12]. While constructing a linguistic-based system, the automatic word-level transcripts generated by an automatic speech recognition (ASR) system are generally used. In this process, the rhythm-related information, such as hesitations resulting from AD, is removed, although the rhythm-related information varies significantly between the healthy controls (HCs) and people living with AD [13]\u2013[17]. A popular method for extracting rhythm-related information is by using the time alignment information estimated by the ASR system [16], [18]. As discussed in [18], modelling the rhythm-related information as a sequential series can embed more information compared to the statistical analysis. In [14], the sequential rhythm-related information is preserved by learning the embedding matrix of the character-level transcripts generated by an ASR system. In this paper, the sequential information embedded in the character-level transcripts and word-level transcripts is extracted with a BERT-style system for constructing a linguistic-based system.\nOur contributions are as follows. Firstly, a SW-MHA in-spired acoustic-based system is proposed to extract the global and local information related to dementia embedded within the audio recordings. Using the age and gender information as the extra input can improve the system's performance fur-ther. Secondly, using the character-level transcripts and word-level transcripts together is demonstrated to be informative for constructing a linguistic-based dementia detection system. Finally, a fusion system named Swin-BERT is designed to take advantage of the acoustic-based and linguistic-based systems to reach superior results on both the ADRESS datasets (released for the first international shared-task challenge [19]) and ADReSSo (released for the second international shared-task challenge [20])."}, {"title": "II. DESIGNED SYSTEMS", "content": "This section describes the proposed systems for dementia detection, including the acoustic-based system, and linguistic-based system. The proposed feature fusion system named Swin-BERT is designed to leverage the extracted acoustic information and linguistic information, as is shown in Figure 1"}, {"title": "A. Acoustic-based System", "content": "The acoustic-based system is composed of a hierarchical SW-MHA structure inspired by the Swin transformer, as shown in Figure 1 (a). Suppose \\(X \\in \\mathbb{R}^{T \\times F \\times 1}\\) denotes the input spectrogram of the model, where T denotes the number of time frames, F denotes the number of frequency bins, 1 denotes the number of channels. To utilise the age and gender information, the age and gender information is first embedded into vectors, and the input spectrogram X is concatenated with the provision of the age and gender embedding, and denoted as \\(X_{\\text{concat}}\\). A SW-MHA block, together with a patch merging layer, is called a SW-MHA group. In our designed acoustic-based system, the processed spectrogram \\(X_{\\text{concat}}\\) is processed by five SW-MHA groups for learning the global and local acoustic information related to dementia detection. The output of the first group is denoted as \\(X^1 \\in \\mathbb{R}^{\\frac{T}{2} \\times \\frac{F}{2} \\times 2D}\\), and the output of the final SW-MHA group is denoted as \\(X^5 \\in \\mathbb{R}^{\\frac{T}{32} \\times \\frac{F}{32} \\times 8D}\\). An average pooling operation is used on both time and frequency dimensions to output \\(X_P \\in \\mathbb{R}^{1 \\times 8D}\\) before feeding into two linear layers for doing classification."}, {"title": "B. Linguistic-based System", "content": "The designed linguistic-based system with word-level and character-level transcripts as the input of the BERT-based system is shown in Figure 1 (b). To extract the distinguishing information between the HCs and people living with AD from the word-level and character-level transcripts. First of all, the generated character-level transcripts are processed with a character dictionary to arrive at a character-level feature matrix. The character dictionary is composed of 32 characters. Each character-level transcript is transmitted into a one-hot matrix \\(M \\in \\mathbb{R}^{l \\times 32}\\), where l equals to the character length. The length l of each transcript depends on the input audio. To obtain a fixed length matrix M', the character-level embedding matrix M is chunked or padded with zero into the same length. Then, both the character-level embedding matrix and the word-level transcript have been used as the system's\ninput. To process the character-level embedding matrix, two convolutional layers a max-pooling layer and a linear layer are designed. One embedding layer and twelve transformer layers are used to extract word-level information from the tran-scripts. The processed character-level and word-level features are concatenated before being input into the linear layer for classification."}, {"title": "C. Feature-fusion System", "content": "Swin-BERT is designed to leverage the extracted acoustic information and linguistic information, as is shown in Figure 1 (c). In our designed feature fusion system, the learned acoustic feature \\(M_{\\text{acoustic}} \\in \\mathbb{R}^{32 \\times 1024}\\) is the statistical matrix generated by the acoustic feature \\(X_a \\in \\mathbb{R}^{1 \\times 1024}\\) extracted from the final linear layer of the acoustic-based system. While designing the feature fusion system, whether to include the character-level embedding matrix as the extra input for the linguistic-based system is also explored. The results show that when fusing with the learned acoustic features, only word-level transcripts can get a better performance on the two datasets. More discussion are shown in Section IV."}, {"title": "III. EXPERIMENTAL SETTINGS", "content": "In this section, the information on the ADRESS and ADRESSo datasets, as well as the parameters of the proposed systems, are presented in detail."}, {"title": "A. Dataset Information", "content": "The picture description task is a constrained task that relies less on episodic memory and more on semantic knowledge and retrieval ability [21]. A picture is presented as a prompt in the test, with individuals then being asked to describe what they have seen in this picture. During this process, answers are often recorded to be subsequently used by the neuropsy-chologist when scoring their tests. The most commonly used picture is that of a line drawing called the \"Cookie Theft\", which originates from a test for aphasia [22]. The datasets used in this paper are both based upon the cookie theft picture description task and are presented in more detail in Table I and Table II, respectively."}, {"title": "B. System Parameters", "content": "1) Parameters of the acoustic-based System: The input waveform is first transformed into Mel-spectrograms for the acoustic-based system, with the dimension F set to 64. In the patch embedding stage, the patch window size P is set to 4, allowing for a patch window size of 4 \u00d7 4. Subsequent to this, four Swin Transformer blocks are used, with the number of attention heads in the SW-MSA being 4, 8, 16 and 32 for each Swin Transformer block. The latent size D is set to 96 for all layers. For training purposes, the model is firstly pre-trained on the AudioSet dataset [23], then fine-tuned on ADRESS and ADRESSo datasets. Adam optimiser is used for all experiments\nwith \u03b21 = 0.95, \u03b22 = 0.999. The initial learning rate is 10-4. These parameters are selected according to the parameters shown in [24].\n2) Parameters of the ASR System: Wav2vec2.0 is used as the ASR system, which is pre-trained and fine-tuned on 960 hours of Librispeech using 16kHz sampled speech audio. The pre-trained system is then used to generate both word-level and character-level transcripts. No manual transcripts from the ADRESS or ADRESSo datasets are used for fine-tuning the pre-trained wav2vec2.0 system.\n3) Parameters of the Linguistic-based System: The param-eter of each layer of the linguistic-based system is shown in Figure 1. The input and output dimensions of the first convolution layer are 1024 and 512, respectively, followed by the second convolution layer with 128 output dimensions. BERT-base-uncase is used as the pre-trained model to initialise the structure used for extracting the word-level feature. The output of BERT's last layer is used to represent the word-level features. The batch size is set to 8, and the epoch is equal to 12. The maximum word limit for the word-level transcripts is set to 512, and the maximum character limit for the character-level transcripts is set to 3000, according to the average length of both the word-level and sentence-level transcripts. Each of these parameters is selected according to the quality of the results attained during the training set.\n4) Parameters of Swin-BERT System: The acoustic feature matrix \\(M_{\\text{acoustic}}\\) is padded to [32*1024]. Feature matrices that are shorter than 32 are padded with zero, whilst those longer than 32 are chunked. Swin-BERT is similar to the linguistic-based system, but we used learned acoustic features instead of character-level transcripts as the input. The input and output dimensions of the first convolution layer are 32 and 16, respectively, followed by the second convolution layer with 16 output dimensions. The batch size is set to 8, and the epoch is equal to 12. The maximum word limit of the word-level transcripts is set to 512, and the maximum character limit of the character-level transcripts is set to 1024, which are selected according to the quality of the results attained during the training set."}, {"title": "IV. RESULTS", "content": "This section shows the performance of the acoustic-based, the linguistic-based, and the fusion systems evaluated using the ADRESS and ADRESSo datasets. Results are analysed to demonstrate the efficiency of our proposed methods, and the related results on the two datasets proposed previously are summarised to compare with our proposed systems."}, {"title": "A. Linguistic-based Results", "content": "To explore how the character-level embedding matrix can affect the performance of the linguistic-based dementia detec-tion system, the results of the systems with and without the character-level embedding matrix are shown in Table III. As shown, by including the character-level embedding matrix as the extra input, the system's performance can be improved on both the ADRESS and ADRESSo datasets. Specifically, for the ADRESSo dataset, the F-score can be improved from 84.55% to 86.07%."}, {"title": "B. Acoustic-based Results", "content": "Table IV shows the baseline systems and results obtained using our designed acoustic-based system. As shown, our proposed acoustic-based system can ensure superior perfor-mance on both the ADRESS and ADRESSo datasets. For the ADRESSo dataset, the proposed acoustic-based system reached an F-score of 76.04%. For the ADRESS test set, the result was improved from 73.25% F-score to 79.14% F-score after using age and gender as the extra input of our designed system. Compared to previous research, our proposed system can ensure comparable or better performance on the two datasets."}, {"title": "C. Feature Fusion Results", "content": "Table V shows the feature fusion results based on Swin-BERT on the ADRESS and ADRESSo datasets by using the acoustic features corresponding to the Table IV. Compared to the acoustic-based or linguistic-based results, the results shown in Table V demonstrate that our proposed Swin-BERT system can be improved after doing feature fusion, and ensure state-of-the-art results on the two datasets than previous re-search. We also explored how age, gender and character-level embedding matrix can affect the performance of the Swin-BERT. The result shows that Swin-BERT can perform better with age and gender information but without the character-level embedding matrix. It is inferred that the learned acoustic features already include rhythm-related information; age and gender information are critical for dementia detection."}, {"title": "V. CONCLUSION", "content": "This paper proposed a system named Swin-BERT for con-structing a speech-based dementia detection system. The pro-posed system contained two parts: the acoustic-based system and the linguistic-based system. For the acoustic-based system, the SW-MHA is used to extract both local and global features for AD classifications while leveraging the age and gender information. For the linguistic-based system, the character-level embedding matrix is used as the extra input of BERT designed for the word-level transcripts. The proposed feature fusion system named Swin-BERT leverages the local and global information learned from the acoustic-based system together with the linguistic-based system. The result shows that our proposed Swin-BERT can ensure superior results on the ADRESS and ADRESSo datasets."}]}