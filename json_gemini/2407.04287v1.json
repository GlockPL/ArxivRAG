{"title": "MARS: Paying more attention to visual attributes for text-based person search", "authors": ["ALEX ERGASTI", "TOMASO FONTANINI", "CLAUDIO FERRARI", "MASSIMO BERTOZZI", "ANDREA PRATI"], "abstract": "Text-based person search (TBPS) is a problem that gained significant interest within the research community. The task is that of retrieving one or more images of a specific individual based on a textual description. The multi-modal nature of the task requires learning representations that bridge text and image data within a shared latent space. Existing TBPS systems face two major challenges. One is defined as inter-identity noise that is due to the inherent vagueness and imprecision of text descriptions and it indicates how descriptions of visual attributes can be generally associated to different people; the other is the intra-identity variations, which are all those nuisances e.g. pose, illumination, that can alter the visual appearance of the same textual attributes for a given subject. To address these issues, this paper presents a novel TBPS architecture named MARS (Mae-Attribute-Relation-Sensitive), which enhances current state-of-the-art models by introducing two key components: a Visual Reconstruction Loss and an Attribute Loss. The former employs a Masked AutoEncoder trained to reconstruct randomly masked image patches with the aid of the textual description. In doing so the model is encouraged to learn more expressive representations and textual-visual relations in the latent space. The Attribute Loss, instead, balances the contribution of different types of attributes, defined as adjective-noun chunks of text. This loss ensures that every attribute is taken into consideration in the person retrieval process. Extensive experiments on three commonly used datasets, namely CUHK-PEDES, ICFG-PEDES, and RSTPReid, report performance improvements, with significant gains in the mean Average Precision (mAP) metric w.r.t. the current state of the art. Code will be available at https://github.com/ErgastiAlex/MARS.", "sections": [{"title": "1 INTRODUCTION", "content": "The integration of text prompts in the re-identification task, called text-based person search (TBPS), has gained lots of interest in the research community lately [1, 15]. In TBPS, textual descriptions"}, {"title": "2 RELATED WORKS", "content": "Joining together text and images for the task of text-based image retrieval and tracking was first\nexplored by Shuang, et al. [12], who also introduced the CUHK-PEDES dataset. This dataset is\ncomposed of a set of pedestrian images paired with a text description which serves as query to\nretrieve the correct subject. This new dataset and problem to be solved garnered a lot of attention,\nand several methods were proposed to address it. Zheng et al. [28] proposed a novel hierarchical\nGumbel attention network to boost cross-modal alignment, while Wang et al. [20] introduced a\nnovel multi-granularity embedding learning model. On the other side, [27] proposed a cross-modal\nprojection matching (CMPM) loss and a cross-modal projection classification (CMPC) loss. Later,\nShao et al. [18] introduced an end-to-end framework based on transformers to learn, for both text\nand images, granularity-unified representations. In addition, a set of methods experimented with\nusing additional data such as segmentation, pose estimation or attribute prediction to boost the\nretrieval performance [21, 29].\nIn addition, Wu et al. [22] introduced two sub-tasks, image colorization and text completion.\nThe first one helps learning rich text information to colorize gray images, while, in the second\none, the model is requested to complete color word vacancies in the captions. Then, Zeng et al.\n[26] proposed a Relation-aware Aggregation Network (RAN) exploiting the relationship between\nthe person and the local objects. Additionally, three auxiliary tasks are introduced: identifying the\ngender of the pedestrian, discerning the images of the similar pedestrian, and aligning the semantic\ninformation between caption and image. Also, a common problem in text-to-image search is the\npresence of weak positive pairs. This was first tackled by Ding et al. [5] that assigned different\nmargins in the triplet loss.\nUp until this point, the vision encoder and the text encoder necessary to align the embeddings of\nthe different modalities were trained from scratch. Recently, the use of pretrained vision-language\nmodels has caught attention, e.g. in [3, 19, 23, 24]. Cao et al. [23] perform an empirical study\nabout using CLIP [16] as backbone for TBPS. Among these, IRRA [9], which was pretrained on\nCLIP, introduced an Implicit Relation Reasoning module and aims to minimize the KL divergence\nbetween distributions of image-text similarity and normalized label matching. Also, IRRA proposed\na masked language modelling (MLM) in which a masked set of image embeddings is reconstructed\nwith the aid of text tokens. Additionally, RaSa [1] designed two novel strategies: Relation-Aware\nlearning (RA) and Sensitivity-Aware learning (SA). A concurrent work with RaSa is represented by\nCADA [13] which focuses of building bidirectional image-text associations. More in detail, it tries\nto associate text tokens with image patches and image regions with text attributes. The latter is\ndone by modifying the MLM into masking specific attributes and not random words."}, {"title": "3 PROPOSED METHOD", "content": "In this section, the proposed model architecture will be presented as well as the training losses."}, {"title": "3.1 The MARS Architecture", "content": "In this paper we propose MARS (Mae-Attribute-Relation-Sensitive), a novel architecture for TBPS.\nWhen building the system, we decided to use RaSa [1] as starting point since currently is one of\nthe best TBPS models and we initialized the architecture weights on ALBEF [11].\nMARS is composed by four main components (Fig. 2): (a) an Image Encoder $\\mathcal{E}_{v}$ which encodes a\nsequence of image patches, (b) a Text Encoder $\\mathcal{E}_{t}$ which produces the text embeddings from the\ncaptions, (c) a MAE Decoder $\\mathcal{D}_{mae}$ which is tasked to reconstruct masked images and, finally, (d) a\nCross-Modal Encoder $\\mathcal{E}_{cross}$ which computes our proposed attribute loss along with the baseline\nRaSa [1] losses: Sensitive-Aware and Relation-Aware losses.\nMore in detail, the Image Encoder is a Vision Transformer (ViT) [6] composed by 12 transformer\nblocks consisting in Self-Attention layers and Feed Forward Layers. The Text Encoder and the\nCross-Modal Encoder are based on BERT [4] which is a 12 blocks transformer-based architecture\nfor language understanding. The first 6 blocks of BERT are used as Text Encoder. On the other\nhand, the Cross-Modal Encoder is composed by all the 12 blocks of BERT, but, differently than\nprevious methods like [1, 11], we equip all its blocks with cross-attention layers instead of only the\nlast 6. By doing so, we can perform the cross-modal encoding using the whole BERT architecture,\nwhich helps boosting the matching accuracy as it will be shown in the experiments. Finally, the\nMAE Decoder is composed by 4 transformer blocks equipped with cross attentions. Additionally,\na momentum model is initialized. The momentum model is a slower version of the online model\nwhose weights are obtained using Exponential Moving Average (EMA):\n$\\theta = m\\hat{\\theta} + (1 - m)\\theta\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(1)$\nwhere $\\hat{\\theta}$ are the weights of the momentum models, while $\\theta$ are the weights of the online model, and\n$m$ is the momentum coefficient. This model will be crucial when calculating the losses as explained\nin Section 3.2.\nDuring training, starting from a image-text pair ($I, T$), $\\mathcal{E}_{v}$ produces a sequence of image embed-\ndings $v = {v_{cls}, v_{1}, \\cdots, v_{M}}$ for each of the $M$ image patches, while a tokenized text is fed to $\\mathcal{E}_{t}$\nproducing a sequence of text embeddings $t = {t_{cls}, t_{1}, \\ldots, t_{N} }$, being $N$ the number of word. In\nboth $v$ and $t$ the first embedding is the class token [CLS]. Additionally, a masked version of the\nimage patches of length $L < M$ is embedded using $\\mathcal{E}_{v}$. Then, a set of $K = M \u2013 L$ mask embeddings\nare inserted in the obtained sequence at the masked positions and the whole sequence is fed to\n$\\mathcal{D}_{mae}$ which reconstructs the original image also with the aid of text embeddings $t$ that are fed\nin $\\mathcal{D}_{mae}$ via cross attention mechanism. Finally, text $T$ is used as input to $\\mathcal{E}_{cross}$ while image\nembeddings $v$ are injected in $\\mathcal{E}_{cross}$ cross attention layers producing the cross-modal embeddings\n$f = {f_{cls}, f_{1},\u00b7\u00b7\u00b7,f_{N}}$. The [CLS] token of the cross-modal embeddings will be used to perform an\nadditional matching between images and captions.\nThe evaluation phase is composed of two steps: first, all the image and text embeddings are\ncalculated using the image and text encoder and, for each text embedding, an ordered list of the\nclosest image embedding is obtained by calculating the similarity between the [CLS] token of\nthe text and the images. Then, the first $k$ candidates for each text are selected and an additional\nre-ranking phase is performed considering the matching results of the Cross-Modal Encoder. This\nadditional step allows to further boost the ranking results."}, {"title": "3.2 Baseline Losses", "content": "As a baseline training objective for our model, we employ the loss set used in RaSa [1]. Additionaly,\nour final proposed architecture also introduces two novel losses: an Attribute Loss and a Masked\nAutoencoder Loss."}, {"title": "Relation-Aware Loss", "content": "The Relation-Aware (RA) loss is a modification to the conventional Image-\nText Matching (ITM) loss commonly employed in various models [10, 11, 25]. In particular, ITM\nperforms a binary classification between positive and negative image-text pairs. Instead of selecting\nhard-negative samples at random, the ITM variation, denoted as p-ITM, creates a negative pair\nset by evaluating embedding similarity and employing this value as the probability of drawing\na negative pair. This similarity is quantified using the [CLS] token representations from the\nunimodal encoders (Text and Image Encoder in Fig. 2). The probability of choosing a negative\npair is proportional to the similarity of the corresponding image-text [CLS] tokens. Consequently,\nnegative pairs exhibiting higher similarity are more likely to be selected, enhancing the robustness\nof the model in distinguishing between truly-related and unrelated image-text pairs. The loss\n$\\mathcal{L}_{p-ITM}$ is a Cross-Entropy Loss that distinguishes if input pairs ($I, T$) are positive or negative.\nLet $l_{itm}(f_{cls})$ be a fully connected layer applied on the [CLS] token of $\\mathcal{E}_{cross}(T, \\mathcal{E}_{v}(I))$ which\npredicts the logit for a given class $c$. The loss can be calculated as:\n$\\mathcal{L}_{p-ITM} = \\frac{1}{3 \\cdot N_{B}} \\sum_{(I,T) \\in P} \\sum_{c \\in C} y_{c} \\log \\frac{exp(l_{itm}(f_{cls}))}{\\sum_{nec} exp(l_{itm}(f_{cls}))} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(2)$\nwhere $C$ is the set of possible classes, which includes two categories: positive and negative pairs.\nThe variable $y_{c}$ represents the ground-truth, where $y_{c} = 1$ if the pair ($I, T$) belongs to the class $c$. The\nset $P$ is built as the union of three subsets, hence the division by 3, each of size $N_{B}$, $P^{++}$, $P^{-+}$, $P^{+-}$:\n$\\bullet$ $P^{++}$ consists of the input batch, where all pairs ($I, T$) are positive.\n$\\bullet$ $P^{-+}$ is composed of a negative image $I$ for each text $T$, sampled randomly with a probability\ndetermined by the similarity between $t_{cls}$ obtained from $\\mathcal{E}_{t}(T)$ and $v_{cls}$ obtained from $\\mathcal{E}(I)$.\n$\\bullet$ $P^{+-}$ is composed of a negative text $T$ for each image $I$, sampled randomly with a probability\ndetermined by the similarity between $v_{cls}$ obtained from $\\mathcal{E}(I)$ and $t_{cls}$ obtained from $\\mathcal{E}_{t}(T)$.\nFurthermore, the p-ITM loss is expanded by adding a Positive Relation Detection (PRD), formulated\nas a Cross Entropy Loss, which aims to detect weak positive pairs. During training, the weak\npositive pairs are built by randomly switching the caption of an image with a caption of a different\nimage having the same identity. Viceversa, we define strong positive pairs as the original pairs\ncoming from the dataset. Let $l_{hrd}(f_{cls})$ be a fully connected layer applied on the [CLS] token of\n$\\mathcal{E}_{cross}(T, \\mathcal{E}_{v}(I))$ which predict the logit for a given class $c$, then:\n$\\mathcal{L}_{prd} = - \\frac{1}{N_{B}} \\sum_{(I,T) \\in P^{++}} \\sum_{c \\in C} y_{c} \\log \\frac{exp(l_{hrd}(f_{cls}))}{\\sum_{nec} exp(l_{prd}(f_{cls}))} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(3)$\nwhere $P^{++}$ are only positive pairs that can be both weak or strong and $C$ is the number of classes\n(two in this case), corresponding to strong positive pairs and weak positive pairs. The final RA loss\nis then computed as:\n$\\mathcal{L}_{RA} = \\mathcal{L}_{p-ITM} + \\lambda_{1} \\mathcal{L}_{prd}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(4)$\nwhere $\\lambda_{1}$ is an hyperparameter used to balance the contribution of $\\mathcal{L}_{prd}$."}, {"title": "Sensitive-Aware Loss", "content": "Similar to RA loss, Sensitive-Aware (SA) loss is an expansion of the basic\nMasked Language Modeling (MLM) introduced in [9] that adds a Momentum-based Replace Token\nDetection (m-RTD). Given a strongly positive pair ($I, T$), the MLM loss is expressed as a Cross\nEntropy Loss. Given a masked text $T_{mask}$, where each word has a probability $p$ of being masked out,\nthe model is trained to predict the correct missing word. Let $V$ represent the set of all possible words\nin the vocabulary and $l_{v}(f_{mask})$ be a fully connected layer applied on each embedding obtained\nfrom $\\mathcal{E}_{cross}(T_{mask}, \\mathcal{E}_{v}(I))$ which predicts the logit for the vocabulary $v$. The MLM loss is formulated\nas:\n$\\mathcal{L}_{MLM} = \\frac{1}{N_{B}} \\sum_{(I,T) \\in P^{++}} \\frac{1}{N_{t}} \\sum_{w \\in t} \\sum_{m_{w}} \\sum_{v \\in V} y_{v} \\log \\frac{exp(l_{v}(f_{mask}))}{\\sum_{nev} exp(l_{v}(f_{mask}))} \\quad\\quad\\quad\\quad\\quad(5)$\nwhere $N_{B}$ is the batch size, $N_{t}$ is the number of masked words for a given text $t$, $m_{w}$ is 1 if the\nword is masked, otherwise 0 (i.e. $N_{t} = \\sum_{w \\in t} m_{w}$) and $y_{v}$ is a one-hot value on the ground-truth\nvocabulary. On the other hand, in m-RTD, the focus is on detecting words that have been replaced.\nTo replace the masked word, the momentum model of the MLM is employed, which converges\nslowly providing less accurate word predictions. The MLM momentum model predicts a word for\neach masked word, by effectively replacing the masked words with its predictions, and the task of\nthe online model is to identify which of these words have been replaced. The m-RTD loss is based\non a Cross-Entropy Loss which teaches the model to distinguish between replaced and non-replaced\nwords. Let $C$ be the set of possible predictions for each word, where a prediction can be either\n\"replaced\" or \"not replaced\", and $l_{c}(f_{repl})$ be a fully connected layer applied on each embedding\nobtained from $\\mathcal{E}_{cross}(T_{repl}, \\mathcal{E}_{v}(I))$ which predicts the logit for the class $c$. The loss function can be\nexpressed as:\n$\\mathcal{L}_{m-RTD} = \\frac{1}{N_{B}} \\sum_{(I,T) \\in P^{++}} \\frac{1}{N_{t}} \\sum_{w \\in t} \\sum_{c \\in C} y_{c} \\log \\frac{exp(l_{c}(f_{repl}))}{\\sum_{nec} exp(l_{c}(f_{repl}))}\\quad\\quad\\quad\\quad(6)$\nwhere $N_{B}$ is the batch size, $N_{t}$ is the number of words in a given text $t$ and $y_{c}$ is the ground-truth.\nThe final $\\mathcal{L}_{SA}$ is then:\n$\\mathcal{L}_{SA} = \\mathcal{L}_{MLM} + \\lambda_{2}\\mathcal{L}_{m-RTD}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(7)$\nwhere $\\lambda_{2}$ is an hyperparameter used to balance the contribution of $\\mathcal{L}_{m-RTD}$."}, {"title": "Contrastive Loss", "content": "Contrastive Loss (CL) is the last baseline model loss. As shown by Fig. 2, the\ncontrastive loss is calculated using only the [CLS] token of the two encoders, the Image Encoder\nand the Text Encoder, after passing them into a linear layer to project in a lower dimension space.\nGiven an Image-Text pair ($I, T$), we obtain $v_{cls}$ from $\\mathcal{E}(I)$ and $t_{cls}$ from $\\mathcal{E}_{t}(T)$. Then, the two\nembeddings are fed into the linear layer, obtaining $t'_{s}$ and $v'_{s}$. The same process is replicated also\nfor the momentum model, obtaining $t'_{cls}$ and $v'_{cls}$. Also, an image queue $Q_{I}$ and a text queue $Q_{t}$\nare stored to implicitly enlarge the batch size. The CL is then formulated as:\n$\\mathcal{L}_{NCE}(x_{1}, x_{2}, Q) = - \\frac{1}{|Q|} \\sum_{(x,x_{+}) \\in (X_{1},X_{2})} \\log \\frac{exp(s(x, x_{+})/\\tau)}{\\sum_{x_{i}e} exp(s(x, x_{i})/\\tau)} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(8)$\nwhere $\\tau$ is a learnable temperature parameters, $Q$ is the queue and $s(x, x_{+}) = \\frac{x^{T}x_{+}}{||x|| \\cdot ||x_{+}||}$. The\nimage-text constrative loss (ITC) [11, 16] is formulated as:\n$\\mathcal{L}_{ITC} = [\\mathcal{L}_{NCE}(v'_{cls}, t'_{cls}, Q_{t}) + \\mathcal{L}_{NCE} (t'_{els}, v'_{cls}, Q_{u})]/2\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(9)$\nOther than $\\mathcal{L}_{ITC}$, in RaSa also a intra-modal constrative loss (IMC) is added, which focuses on\nkeeping close the image and text embedding of the same people with respect to the other people.\n$\\mathcal{L}_{IMC} = [\\mathcal{L}_{NCE}(v'_{cls}, v'_{cls}, Q_{u}) + \\mathcal{L}_{NCE} (t'_{els}, t'_{cls}, Q_{t})]/2\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(10)$\nThe final loss then becomes:\n$\\mathcal{L}_{CL} = (\\mathcal{L}_{IMC} + \\mathcal{L}_{ITC})/2\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(11)$"}, {"title": "3.3 Attribute Loss", "content": "Our attribute loss is designed to enhance the model capability to distinguish between matching\nand non-matching text-image pairs. In particular, we define an attribute in a caption as a chunk of\nwords composed by a noun and its corresponding adjectives (e.g. \u201cwhite long shirt\u201d). To extract\nthese chunks, SpaCy [8] was employed. The idea behind this loss is that in captions composed\nby several attributes the model is not able to give the right importance to each attributes and\npotentially could ignore the most discriminative ones. Limiting this effect is crucial since often,\ndue to the vague nature of text description, two people with different identities could be described\nby very similar texts, differing only for a single attribute. In this case, if most distinctive attributes\nare neglected, the correct matching between a text description and the correct person could fail,\nhindering the model accuracy. For this reason, the proposed attribute loss has the objective of\nlimiting these cases, ultimately making the whole system more robust.\nIn order to do so, given the output of the cross-modal encoder $\\mathcal{E}_{cross}$, which takes as input the\ntext $T$ and the image embedding $v = \\mathcal{E}(I)$, for each attribute i.e. chunk $ch$ of noun-adjective words\nin a given text $T$, the average of the corresponding embeddings is calculated as follows:\n$\\overline{ch(T, v, ch)} = \\frac{1}{N_{ch}} \\sum_{w \\in ch} \\mathcal{E}_{cross}(T, v)[w^{l}]\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(12)$\nwhere $N_{ch}$ is the number of words in a given chunk $ch$ and $w^{l}$ is the position of the word $w$ in the\noutput of $\\mathcal{E}_{cross}$.\nHaving this information, is now possible to calculate the proposed Attribute Loss $\\mathcal{L}_{AL}$ for each\nchunk. More in detail, $\\mathcal{L}_{AL}$ is tasked to perform a matching between each attribute chunk in the\ncaption and the real image. Let $N_{B}$ be the batch size, $N_{ch}$ the number of chunks in a text $T$ associated\nwith an image $I$ and $l_{attr}(\\overline{ch(t, i, ch)})$ be the same fully connected layer as the Eq. 2 which predict\nif the image-text pair ($I, T$) matches or not. The loss function becomes:"}, {"title": "LAL", "content": "$\\mathcal{L}_{AL} = - \\frac{1}{3 \\cdot N_{B}} \\sum_{(I,T) \\in P} \\sum_{c \\in \\overline{ch}et} \\sum_{c \\in C} y_{c} \\log \\frac{exp(l_{attr}(\\overline{ch(T, \\mathcal{E}(I), ch)}))}{\\sum_{nec} exp(l_{attr}(\\overline{ch(T, \\mathcal{E}(I), ch)}))}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(13)$\nHere, $C$, $y_{c}$ and $P$ are built as in Eq. 2.\nFurthermore, we explored a weighted variant of the loss function. The results of this experiment\nare presented in Table 2 later in the paper. Specifically, we selected the top 25 most common nouns\nand adjectives in the CUHK-PEDES corpus (Fig. 4) and calculated the frequency values normalized\nbetween 0 and 1. Let $a_{w}$ denote the frequency of a given word $w$. If the word is not among the top\n25 most common words, we set $a_{w}$ to 0. We then define the importance weight $\\omega_{ch}$ for the chunk\n$ch$ as follows:\n$\\omega_{ch} = 1 - \\frac{\\sum_{w \\in ch} a_{w}}{N_{ch}}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(14)$\nwhere $N_{ch}$ is the total number of words contained in the chunk. Finally, the final weighted attribute\nloss is formulated as:\n$\\mathcal{L}_{weighted-AL} = - \\frac{1}{3 \\cdot N_{B}} \\sum_{(I,T) \\in P} \\sum_{c \\in \\overline{ch}et} \\sum_{c \\in C} \\omega_{ch} y_{c} \\log \\frac{exp(l_{attr}(\\overline{ch(T, \\mathcal{E}(I), ch)}))}{\\sum_{nec} exp(l_{attr}(\\overline{ch(T, \\mathcal{E}_{v}(I), ch)}))}\\quad\\quad\\quad\\quad\\quad(15)$\nAs described in Eq. 14, lower importance weights ($\\omega_{ch} \\rightarrow 0$) are assigned to chunks with very\ncommon words and higher importance weights ($\\omega_{ch} \\rightarrow 1$) are assigned to chunks with uncommon\nwords. This approach is used to downweigh the contribution of very common attributes that match\nwith several different images and therefore identities.\nIn summary, attribute loss is used to pay attention on the subtle details of a single sentence,\nimproving matching performance using fine-grained details contained in the text that describe\nan image (i.e. \u201cA pink headset\u201d can be a very uncommon attribute that, if properly considered,\nimproves the model accuracy). As a result, attribute loss helps the model to use the entire given\ntext without losing details. In other words, by distributing the attention evenly, it encourages a\nmore comprehensive understanding of the input data."}, {"title": "3.4 Masked AutoEncoder Loss", "content": "Inspired by the masked language model, we have developed a novel loss function based on the\nMasked AutoEncoder [7] (MAE). MAE was originally used as a self-supervised training technique\nfor transformers. The goal is to reconstruct a sequence of masked image patches back into the\noriginal unmasked one. In our case, we customized this technique integrating also text embeddings.\nMore in detail, we inject the text embeddings in the MAE decoder via cross attention layers. The\naim is to use the textual information to help the decoder reconstruct the image patches, hence\nstrongly linking together words and visual information.\nGiven an image-text pair ($I, T$), we randomly sample patches from the image $I$ with a probability\n$P_{mae}$ and discard the remaining patches. The selected patches are processed through the Image\nEncoder $\\mathcal{E}_{v}$ to obtain their corresponding embeddings ${v_{cls}, v_{1}, . . ., v_{L} }$, with $L < M$. Prior to feeding\nthese embeddings into the MAE decoder $\\mathcal{D}_{mae}$, the embeddings for the removed $K = M \u2013 L$ patches\nare replaced with a learnable mask embedding, thus obtaining a set $v_{masked} = {v'_{cls}, v_{1}, ..., v_{M}}$\nof dimension $M$. The set $v_{masked}$ is then fed into the MAE decoder $\\mathcal{D}_{mae}$, where it is fused with\nthe text embeddings ${t_{cls}, t_{1}, ..., t_{N} } = \\mathcal{E}_{t}(T)$ corresponding to the text $T$ using cross-attention\nmechanism to reconstruct the original image. The MAE loss is a reconstruction loss, which is\ncalculated using the mean squared error (MSE) of the removed patches:\n$\\mathcal{L}_{MAE} = \\frac{1}{N_{B}} \\sum_{i=0}^{N_{B}} \\frac{1}{1-K} \\sum_{j=0}^{M} m | x_{j}^{i} - \\hat{x_{j}^{i}} |^{2} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(16)$\nwhere $m$ is an indicator variable that equals 1 if the patch was originally removed and thus needs\nto be reconstructed, and 0 otherwise. Let $x$ be the original image patch and $\\hat{x}$ be the reconstructed\none, then, $x_{i} = \\mathcal{D}_{mae}(v_{masked}, \\mathcal{E}_{t}(T))$.\nIn our case the proposed MAE is trained end-to-end along with all the other components of the\nmodel bridging the gap between textual and image information."}, {"title": "3.5 Full Objective and Reranking", "content": "Finally, the complete model loss is:\n$\\mathcal{L} = \\mathcal{L}_{p-ITM} + \\lambda_{1}\\mathcal{L}_{prd} + \\mathcal{L}_{MLM} + \\lambda_{2}\\mathcal{L}_{m-RTD} +\\lambda_{3}\\mathcal{L}_{CL} + \\lambda_{4}\\mathcal{L}_{MAE} + \\lambda_{5}\\mathcal{L}_{AL}\\quad\\quad\\quad(17)$\nwhere each $\\lambda_{*}$ is a weight assigned to a specific loss.\nDuring inference, referring to both ALBEF [11] and RaSa [1], considering the high inefficiency\nof the quadratic interaction operation, we employ a sampling strategy, where we select a subset\nof $k$ image-text pairs and apply the ITM rank to this reduced set. Specifically, given a text input\n$T$, we identify the top-k, with $k = 128$, images by computing the similarity scores $s(t_{cls}, v_{cls})$ and\nselecting the images with the highest scores. An analysis of how changing this parameter affects\nboth efficiency and accuracy is provided in Section 5.1."}, {"title": "4 EXPERIMENTAL RESULTS", "content": ""}, {"title": "4.1 Experimental Settings", "content": "We train our model on a single NVIDIA 4090 GPU for a total of 30 epochs using a batch size of\n8. We employ the AdamW optimizer [14] with a weight decay of 0.02 decay. Initial values of the\nlearning rate are 1e \u2013 4 for PRD and m-RTD parameters, and 1e \u2013 5 for other parameters. Images\nare resized to 384 \u00d7 384 (dataset image size is 128 \u00d7 384), with also the possibility of horizontal\nrandom flip. We set the maximum number of words in BERT to 70. Momentum coefficient $m$ is"}, {"title": "4.2 Metrics", "content": "To evaluate our model, we adopt widely-used metrics in TBPS. Firstly, we evaluate our model with\nRank@K, with K=1, 5 and 10. Rank@K evaluates how many times a model is able to predict at\nleast an image corresponding to a given text in the first K proposed images. Lastly, we calculate the\nmean Average Precision (mAP). Let $N_{T}$ be the number of text in the test set, we calculate the mAP\nas the mean of each average precision for each text t ($AP_{t}$).\n$mAP = \\frac{1}{N_{T}} \\sum_{t \\in T}AP_{t} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(18)$\nAP expresses how well the model is able to retrieve correct images in the early positions. It can be\ncalculated as:\n$AP = \\frac{1}{N_{id}} \\sum_{k}P(k) \\cdot rel(k)\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad(19)$\nwhere $N_{id}$ is the number of the correct identities, $P(k)$ is the precision at the position k, calculated\nas $P(k) = \\frac{\\sum_{i=1}^{k} m_{i}}{k}$, with $m_{i} = 1$ if it is a correct match, 0 otherwise and rel(k) is the indicator function which\nis 1 if the position k contains a positive match, 0 otherwise.\nWe argue that mAP is a crucial metric to express the quality of a retrieval model since it\nencapsulates better the capability of the model to propose positive match in top positions. This is\nespecially true for TBPS where we want to be able to find all the different identities corresponding\nto a specific caption."}, {"title": "4.3"}]}