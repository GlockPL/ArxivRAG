{"title": "Gradient-Free Neural Network Training on the Edge", "authors": ["Dotan Di Castro", "Omkar Joglekar", "Shir Kozlovsky", "Vladimir Tchuiev", "Michal Moshkovitz"], "abstract": "Training neural networks is computationally heavy and energy-intensive. Many methodologies were developed to save computational requirements and energy by reducing the precision of network weights at inference time and introducing techniques such as rounding, stochastic rounding, and quantization. However, most of these techniques still require full gradient precision at training time, which makes training such models prohibitive on edge devices. This work presents a novel technique for training neural networks without needing gradients. This enables a training process where all the weights are one or two bits, without any hidden full precision computations. We show that it is possible to train models without gradient-based optimization techniques by identifying erroneous contributions of each neuron towards the expected classification and flipping the relevant bits using logical operations. We tested our method on several standard datasets and achieved performance comparable to corresponding gradient-based baselines with a fraction of the compute power.", "sections": [{"title": "Introduction", "content": "Deep Neural Networks are crucial components in many tasks such as image recognition (Deng et al., 2009), speech recognition (Mehrish et al., 2023), chat-bots (Radford et al., 2019; Brown et al., 2020), control systems (Fujimoto and Gu, 2021), games (Berner et al., 2019; Van Hasselt et al., 2016), algorithmic trading (Wang and Yan, 2021), to name a few. While they are deployed everywhere, neural networks are trained exclusively in the cloud servers or desktop computers that have access to a lot of power, computing, and memory. Typically, the influx of new data requires the activation of complex training pipelines to update these models (Kreuzberger et al., 2023). These pipelines upload the data from the embedded hardware to the computationally powerful servers, retrain the model with the old and new data (or in a k-shot manner as shown by Mosbach et al., 2023), and then deploy them back on to the embedded hardware. This update protocol is high latency and requires a stable network connection (Shaheen et al., 2022). While this process is not necessarily inconvenient, many times in critical safety applications, it is desirable to train on edge devices. However, edge devices are equipped with limited battery capacity, memory and compute to execute the entire training loop efficiently and in real-time.\nIn this work, we introduce a novel method that enables both training and inference on the edge device. This method offers a major divergence from existing techniques that primarily train networks using full-precision gradients and then distill and quantize the model weights to be power and memory efficient to run on edge devices (Courbariaux et al., 2016b; Liang et al., 2021; He et al., 2017). Some other techniques that are currently used include weight compression (Courbariaux et al., 2016b), channel compression (He et al., 2017), etc.\nWe propose a new optimization technique, called Gradient Free Training (GFT) that uses stochasticity with heuristics in order to do training. Through this technique, we showcase a technique that can effectively leverage quantized weights, activations and inputs to each hidden layer, by training them using our novel optimization technique. We show that this enables effective training of models on edge devices that are battery limited. In this method, we replace gradient computation with the computation of a low-precision contribution tensor that measures how many errors a particular weight causes, in the computation of a given output. This contribution determines whether that"}, {"title": "Related Work", "content": "Various studies have explored techniques for model optimization through quantization and discretization of weights and/or activation functions. Notable works, such as Han et al. (2016); Iandola et al. (2016); Han et al. (2015); Venkataramani et al. (2014); Zhu et al. (2016); Pan et al. (2017), propose methods that accelerate computation by reducing network parameters and connections. However, despite these advancements, these approaches still rely on full precision multiplications and accumulations during training.\nIn contrast, other approaches replace traditional multiplications altogether. Research such as Lin et al. (2016); Courbariaux et al. (2016a); Li et al. (2022); Zhu et al. (2017); Courbariaux et al. (2016b); Rastegari et al. (2016); Deng et al. (2018) substitute original computations with binary logic operations and accumulations. For instance, Binary Weight Networks (BWN) and Ternary Weight Networks (TWN) limit the network weights to binary ({-1,+1}) and ternary ({-1,0, +1}) spaces, respectively, thus eliminating multiplication operations entirely. These methods focus on discretizing weight matrices, significantly reducing computational requirements.\nFurther advancements, like those in Rastegari et al. (2016), extend these methods by restricting both weights and activation functions to binary values ({-1,+1}), allowing multiply-accumulate operations to be replaced with binary logic operations such as XNOR. Enhancements such as Gated XNOR Net-works (GXNOR-Nets) Deng et al. (2018) introduce more sophisticated discretization frameworks, achieving performance comparable to full-precision methods with significantly lower memory and computational costs. Recent innovations include BitNet, introduced by Wang et al. (2023), a transformer-based architecture utilizing binary weights to reduce energy consumption, and a modification by Ma et al. (2024), which employs ternary weights while achieving performance close to that of 16-bit transformers.\nAlthough these methods are pivotal in advancing low-compute, energy-efficient neural networks, they still rely on gradient-based optimization methods like Stochastic Gradient Descent (SGD), often involving rounding or quantization steps. Our work diverges from this by presenting a novel training method that eliminates the dependency on gradient-based approaches altogether. This offers an end-to-end solution for low-compute, memory-efficient, and energy-efficient training. Additionally, our method can complement the aforementioned techniques to further improve the performance and efficiency of low-compute neural networks."}, {"title": "Background", "content": "For simplicity, we focus on the classical problem of feature-based supervised machine learning, but we note that the presented method can be extended to any neural network problem. To ensure clarity and due to space constraints, we present all formulations in tensor form, akin to implementations found in frameworks such as PyTorch (Paszke et al., 2019). Specifically, we consider a neural network with L layers. We begin with a labeled dataset comprising N data points, denoted as D = {x\u2081, ..., xN}. Each sample is a vector containing d\u2080 features, expressed as x\u2099 = [x\u2099,\u2080, ..., x\u2099,d\u2080\u208b\u2081] \u2208 R\u1d48\u2070, where n = 1, ..., N.\nThroughout this paper, we maintain a consistent notation where subscripts refer to sample indices, and superscripts denote layer indices. During training, we sample batches from D, each of size B. We denote such a batch b as Xb.\nAssume that the ground truth labels are denoted by y\u2081, ..., yN \u2208 R\u1d48\u1d38. The main objective in this setup is to learn the parameter matrices W\u00b9, ..., WL, where W\u02e1 \u2208 R\u1d48\u02e1\u208b\u2081\u00d7d\u02e1 for l = 1, ..., L. Here, d\u2081 represents both the dimension of the network output and the"}, {"title": "Full precision Back Propagation", "content": "For completeness, we first recall the backpropagation algorithm using the \u03b4 rule, similarly to Haykin (1998) exposition. In this formulation, \u03b4\u207d\u02e1\u207e \u2208 R\u1d2e\u00d7d\u02e1 for l = 1, ..., L represents the backpropagated gradient in batch form. Here, Lb denotes the network error for batch b, and e\u266d is defined as the error with respect to the output of the L-th layer. For further details, we refer the reader to Chapter 4 of Haykin (1998).\nNetwork Error and \u03b4:\n\u03b4^(L) = eb \u25cb \u03c3\u2032(v^(L)), Lb = L\u2080 Tr(e\u22a4\u266d \u25cb e\u266d)   (4)\nFor layers l = L, ..., 1, the gradients are\n\u2207W^(l) = (X^(l-1))\u1d40 \u03b4^(l), W^(l) \u2208 R^(d_{l-1} x d_{l}).  (5)\nOne step in the Gradient Descent algorithm is W^(l) = W^(l) \u2212 \u2207W^(l). Back propagate the \u03b4 with \u03b4^(l-1) \u2208 R^(d_{l-1} x B).\n\u03b4^(l-1) = \u03c3\u2032(v^(l-1)) \u2299 [\u03b4^(l) \u00b7 (W^(l))\u1d40],\nwhere \u2299 is the standard Hadamard product."}, {"title": "Methods", "content": "In this section, we define the notation and present our algorithm. For convenience, we put in Figure ?? scheme of the architecture, sizes, variables, etc."}, {"title": "The Contribution Tensor and Optimization Principle", "content": "In this section, we present our GFT optimization technique for a binary or ternary network. In order to build intuition of our method, we begin by examining a single layer, and later, we generalize this principle to a deep architecture.\nObserve a linear layer as in (1). We define C^(l) \u2208 R^(B x d_{l-1} x d_{l}) to be the contribution tensor as an intermediate tensor, which signifies the contribution of each (data point, input feature, output feature) towards the final estimation. Formally, for b \u2208 {1, ..., B},i \u2208 {1,...,d_{l\u22121}}, o \u2208 {1,...,d_l}, the element C_{bio}^(l) of the matrix C^(l) is defined as\nC_{bio}^(l) = X_{bi}^(l-1) W_{io}^(l)   (6)\nwhere, X_{bi}^(l-1) is an element in the batch matrix X^(l-1) and W_{io}^(l) is an element in weight matrix W^(l). We note that the element C_{bio}^(l) represents the contribution of the ith feature of the bth data point towards the oth output, in the lth layer of the network. Note that W_{io}^(l) \u2208 {\u22121, +1} in the binary network case and  W_{io}^(l) may be in {-1,+1}, {-1,0,+1}, or R, depending on the network precision. We note that if we sum the tensor C^(l) along the input dimension, we recover the matrix v^(l), i.e., for b = 1, ..., B and o = 1, . . ., d_{l}\nv_{bo}^(l) = \u03a3\u1d62 C_{bio}^(l) = \u03a3\u1d62 X_{bi}^(l-1) W_{io}^(l).  (7)\nLet us define the positive and negative indices of contributions in (7) to be\nC_P^(bo) = {i | C_{bio}^(l) > 0}, C_N^(bo) = {i | C_{bio}^(l) < 0},   (8)\nSo we can rewrite (7) as\nv_{bo}^(l) = \u03a3_{i \u2208 C_P^(bo)} C_{bio}^(l) + \u03a3_{i \u2208 C_N^(bo)} C_{bio}^(l)  (9)\ni.e., we can divide the output into positive contributions and negative contributions.\nNow assume that the output of this layer:\nConsider the output of this layer l, v_{bo}^(l) < 0, but the input of layer l + 1, x_{bo}^(l) is required to be positive in order to be aligned with the label, then we need to"}, {"title": "Back Propagation-like Update Rule using Combinatorial Search for Hyper-Quantized Networks", "content": "Determining the optimal weight change that is consistent with all samples in D or even B is not always possible\u00b9. Furthermore, it can be shown that the combinatorial problem of identifying \"which weights to change?\" can scale exponentially with L, d\u2097\u208b\u2081, d\u2097, and B.\nTo address this issue, we define a tensor B^(l) \u2208 Z^(d_{l-1} x d_{l}), where Z is the set of integers. Each element of this tensor B_{io}^(l), aggregates the contributions {C_{bio}^(l)}_(b=1)^B for each weight W_{io}^(l), based on the number of samples in X^(l-1) that are negatively affected by this weight. If there is a mismatch between the sign of the output (or the backpropagated \u03b4^(l)) and the sign of {C_{bio}^(l)}_(b=1)^B, then an error exists. In this case B_{io}^(l) is determined by:\nB_{io}^(l) = \u2211_(b=1)^B sign(X_{bi}^(l-1)) (10)\nIntuitively, the sign of B_{io}^(l) denotes which direction W_{io}^(l) needs to be updated and the magnitude signifies how many elements of the batch need this weight changed. Refer to lines 13-17 in Alg. 1.\nOnce B^(l) is created, we choose the top k^(l) most destructive and change the corresponding weights W_{io}^(l) with probability P_{change}, i.e., if we define with K^(l) = {(i, o) | |B_{io}^(l)| belongs to k largest values}, then,\n\u2200(i, o) \u2208 d_{l-1} \u00d7 d_{l}:\nW_{io}^(l) = W_{io}^(l) - sign(B_{io}^(l))   (i, o) \u2208 K^(l) and w.p. P_{change}\nW_{io}^(l) else  W_{io}^(l) (11)\nRemark 1: Although the above equation is expressed as additions and subtractions, this is merely for clarity of expression. In practice, the whole operation can be implemented using bitwise operators to further improve efficiency. Remark 2: We also note that the variable (k^(l))_(l=1)^L can be used later for further optimizations or scheduling like in ADAM Kingma and Ba (2014), Momentum Sutskever et al. (2013), or other optimizers."}, {"title": "Generalization to Deep Networks", "content": "Inspired by the \u03b4 back-propagation update rule in (4), we extend our update rule to multiple layers using a similar technique of using a \u03b4^(l) as a surrogate signal for the inner gradient errors. Specifically, we define L exactly as defined in (4) and \u03b4^(l-1) as follows:\n\u03b4^(l-1) = \u03b4^(l) \u00b7 (W^(l))\u1d40 (12)\nWe wish to highlight that if \u03b4^(l) is an integer \u2200l = L, ..., l, then \u03b4^(l-1) will be an integer as well, since W^(l) is ternary.\nFurthermore, by defining our algorithm this way, allows us to train hybrid networks, where selective layers are ternary. For example, if in a 3 layer Dense Neural Network (DNN), if layers 1 and 3 are full-precision (FP32) and layer 2 is ternary, then the precision of \u03b4^(l)\u2200l = 1,2,3 in such a case will be FP32, allowing us to optimize layers 1,3 using standard optimizers such as AdamW, while optimizing layer 2 using our technique. Note that our optimization still uses integers due to the discretization step in line 15 of Alg. 1 The full algorithm is summarized in Algorithm 1."}, {"title": "Energy consumption estimation", "content": "In previous methods such as BitNet Wang et al. (2023) and PokeBNN Zhang et al. (2022), the authors introduce a method to estimate the energy consumed by addition and multiplication operations on 45nm and 7nm technologies. Using their protocol, we attempt to estimate the amount of energy our optimization technique uses, as compared to a default implementation of AdamW in PyTorch, which we compare with.\nFirst we estimate the energy use of a single AdamW optimization step. We refer to the algorithm described on the official PyTorch documentation\u00b2, assuming default parameters. Assume that the whole model has"}, {"title": "Dense Neural Network experiments", "content": "We test the effectiveness of our optimization technique for DNNs, using the MNIST dataset. In these experiments, we compare 3 types of architectures:\n1. Full-Precision: Standard FP32 model parameters trained using AdamW.\n2. Hybrid: The input and output layers of the model are FP32 and all the hidden layers are ternary. The FP32 layers are optimized using AdamW and the ternary layers are optimized using our top-k gradient-free method.\n3. Full-Ternary: All the layers of the model are ternary and optimized using our gradient-free method.\nWe train a 5-layer model, with input dimension 784, output dimension 10 and all hidden dimensions 4096. The total number of trainable parameters is 53.6M. We train all these models for 10 epochs, with a batch size of 256.\nLooking at 1, we observe that the full-precision model showcases the best performance of 98.33% on the MNIST dataset in terms of accuracy, as expected. However, it consumes ~ 14\u00d7 more energy in training, as compared to the Full-Ternary model, that uses our optimization technique. The Hybrid model presents a valid trade-off between the two methods, that achieves a comparable 97.15% accuracy, while consuming only ~ 1.8\u00d7 more energy than the full-ternary. Furthermore, the full-precision model also uses 20.23\u00d7 more memory as compared to the full-ternary model and requires 18.58\u00d7 more parameter updates."}, {"title": "NanoGPT Experiments", "content": "We train nanoGPT as described in the Tutorial, on the Tiny Shakespeare dataset, using character tokenization, for 2000 iterations, with a batch size of 128 and learning rate warmup for 10 iterations. We train 3 versions of the model by selective replacement of FP32 layers with ternary ones, as described below:\n1. Full-Precision: All transformer parameters are FP32, trained using AdamW.\n2. Hybrid-1: Only the MLP in each attention block is ternary; the K, Q, V, and O projection matrices, the embedding lookup, positional encoding, and classification head are FP32.\n3. Hybrid-2: The K, Q, V, O projection matrices and MLP in each attention block are ternary, but the embedding lookup, positional encoding, and classification head are FP32. The ternary parameters are trained using our optimization technique."}, {"title": "Vision Transformer Experiments", "content": "We train a standard ViT-Lx16 model Dosovitskiy et al. (2021), with a total of 61.3M model parameters, with an image size of 224, without using any image augmentations in training. We tested the performance of our model on the ImageNette and ImageNet datasets for the classification task. The ImageNette experiments were trained for 2000 iterations with a warm-up of 20 and a batch size of 256, while the ImageNet experiments were trained for 200k iterations, with a warm-up of 1000 iterations and a batch size of 384. Based on our findings in Section 5.2, we trained only two versions of the models:\n1. Full-Precision: All parameters are FP32, trained using AdamW.\n2. Hybrid: Only the MLP in each attention block is ternary, the K, Q, V, and O projection matrices, the patch embedding, positional encoding, and classification head are FP32.\nWe report the results in Table 3, where Full-Precision-10 and Hybrid-10 are trained on ImageNette, while Full-Precision-1000 and Hybrid-1000 are trained on ImageNet.\nWe observe comparable performance for the smaller ImageNette dataset on both the hybrid and full-precision models, with around 38% less energy consumption in training and a 59.2% smaller memory footprint. However, there is still a major performance drop in the full ImageNet dataset, which highlights the limitations of our current implementation in handling datasets with numerous classes. Future work will consist of improving the quantization techniques we use to quantize the underlying parameters, as well as optimizing the performance on high-class classification datasets."}, {"title": "Conclusion and Future Work", "content": "In this work we presented a gradient-free update rule for hyper-quantized networks such as those having binary and ternary weights and biases. We also implemented this optimization using default PyTorch's Autograd functionality, allowing users to use FP32 layers interleaved with hyper-quantized ones while running optimization in a hybrid manner. We showcase the effectiveness of our introduced technique on various architectures, such as DNNs, Transformers and Vision Transformers, and various data modalities such as language and image. We also show the benefits and trade-offs with using our optimization technique. Potential implications of our optimization technique are in fine-tuning, linear probing and model personalizing on edge devices, with limited network connectivity, memory and energy. Our method could also be applied to current Parameter-Efficient Fine-Tuning approaches such as QLORA Dettmers et al. (2023), by using hyper-quantized adapter layers instead of NF-4 ones.\nIn the current state, our method faces major performance degradation when the dataset has a lot of classes, such as the language-modeling task with 56 classes and ImageNet with 1000. We believe these problems can be fixed by modifying our underlying quantization method, such as by using windowed quantization to better handle outliers. This will be topic of immediate future research. Further research in this method should involve optimized implementations for bitwise and integer operations in deep learning frameworks to better use the full capabilities of the hardware, to improve latency."}]}