{"title": "Explaining Datasets in Words:\nStatistical Models with Natural Language Parameters", "authors": ["Ruiqi Zhong", "Heng Wang", "Dan Klein", "Jacob Steinhardt"], "abstract": "To make sense of massive data, we often first fit simplified models and then interpret the parameters; for example, we cluster the text embeddings and then interpret the mean parameters of each cluster. However, these parameters are often high-dimensional and hard to interpret. To make model parameters directly interpretable, we introduce a family of statistical models\u2014including clustering, time series, and classification models\u2014parameterized by natural language predicates. For example, a cluster of text about COVID could be parameterized by the predicate \u201cdiscusses COVID\". To learn these statistical models effectively, we develop a model-agnostic algorithm that optimizes continuous relaxations of predicate parameters with gradient descent and discretizes them by prompting language models (LMs). Finally, we apply our framework to a wide range of problems: taxonomizing user chat dialogues, characterizing how they evolve across time, finding categories where one language model is better than the other, clustering math problems based on subareas, and explaining visual features in memorable images. Our framework is highly versatile, applicable to both textual and visual domains, can be easily steered to focus on specific properties (e.g. subareas), and explains sophisticated concepts that classical methods (e.g. n-gram analysis) struggle to produce.", "sections": [{"title": "1 Introduction", "content": "To analyze massive datasets, we often fit simplified statistical models and interpret the learned parameters. For example, to categorize a set of user queries, we might cluster their embeddings, look at samples from each cluster, and hopefully each cluster corresponds to an explainable category, e.g. \u201casks about COVID symptoms\u201d or \u201cdiscusses the U.S. Election\u201d. Unfortunately, each cluster might contain an uninterpretable group of queries, thus failing to explain the categories.\nSuch a failure is not an isolated incident: many models explain datasets by learning high dimensional parameters, but these parameters might require significant human effort to interpret. For example, BERTopic [18] learns uninterpretable cluster centers over high-dimensional neural embeddings. LDA [7], Dynamic Topic Modeling [6] (time series), and Naive Bayes (classification) learn weights over a large set of words/phrases, which do not directly explain abstract concepts [9, 52, 63]. We want model parameters that are more interpretable, since explaining datasets is important in machine learning [60], business [4], political discussion [47], and science [17, 34].\nTo make model parameters directly interpretable, we introduce a family of models where some of their parameters are represented as natural language predicates, which are inherently interpretable. Our core insight is that we can use a predicate to extract a 0/1 feature by checking whether it is true on a sample. For instance, given the predicate $ = \u201cdiscusses the U.S. Election\u201d, its denotation [] is a binary function that evaluates to 1 on texts x discussing the U.S. Election and 0 otherwise:\n[ : \"discusses the U.S. Election\u201d](x : \u201cIs Georgia a swinging state this year?\u201d) = 1."}, {"title": "2 Related Work", "content": "Statistical Modeling in Text. Statistical models based on n-gram features or neural embeddings are broadly used to analyze text datasets. For example, logistic regression or na\u00efve Bayes models are frequently used to explain differences between text distributions [51]; Gaussian mixture models on pre-trained embeddings can create text clusters [2]; topic models can mine major topics across a large collection of documents [7] and across time [6]. However, since these models usually rely on high-dimensional parameters, they are difficult to interpret: for example, human studies from [9] show that the most probable words for a topic might not form a semantically coherent category. To interpret these models, prior works proposed to explain each topic or cluster by extracting candidate phrases either from the corpus or from Wikipedia [8, 49, 57]. Our work complements these approaches to explain models with natural language predicates, which are potentially more flexible."}, {"title": "Prompting Language Model to Explain Dataset Patterns", "content": "Our algorithm heavily relies on the ability of LLMs to explain distributional patterns in data when prompted with datasets [39, 46]. [61, 62, 15] have prompted LLMs to explain differences between two text distributions; [53, 38, 50, 27] prompted LLMs to generate topic descriptions over unstructured texts; [44, 22, 64] prompted LLMs to explain the function that maps from an input to an output; [45, 5] prompted LLMs to explain what inputs activate a direction in the neural embedding space. However, these works focused on individual applications or models in isolation; in contrast, our work creates a unifying framework to define and learn more complex models (e.g. time series) with natural language parameters."}, {"title": "Concept Bottleneck Models (CBM)", "content": "CBMs aim to achieve explainability by learning a simple model over a set of interpretable features [26], and recent works have proposed to extract these features using natural language phrases/predicates [3, 55, 30, 12, 41]. While most of these works focus on classification tasks, our work formalizes a broad family of models-including clustering and time series and proposes a model-agnostic algorithm to learn them. Additionally, these prior works focus on downstream task performance (e.g. classification accuracy), thus implicitly assuming that the model grounds the feature explanations in the same way as humans; in contrast, since our focus is on explanations, we focus on our algorithm's ability to recover ground truth explainable features.\nWe discuss more related work on discrete prompt optimization, exploratory analysis, and learning with latent language in Appendix A."}, {"title": "3 Mathematical Formulation", "content": ""}, {"title": "3.1 Predicate-Conditioned Distribution", "content": "In order to model text distributions with natural language parameters, we introduce a new family of distributions, predicate-conditioned distributions; these distributions will serve as building blocks for the models introduced later, just like normal distributions are building blocks for many classical models like Gaussian Mixture or Kalman Filter. Predicate-conditioned distributions p are supported on the set X of all the text samples we observe from the dataset, and they are parameterized by (1) a list of K predicates 6 \u2208 \u03a6\u039a, and (2) real-valued weights w \u2208 RK on those predicates. Formally,\n$p(x | \\phi, w) \\propto e^{w^T [\\phi](x)}$.\nWe now explain how to (1) extract a feature vector from x using 6, (2) linearly combined by re-weighting with w, and (3) use the reweighted values to define p(x | w, \u03c6).\nNatural Language Parameters 6. Each predicate \u03c6\u2208 \u03a6 is a natural language string and its denotation [] : X \u2192 {0,1} maps samples to their value under the predicate. For example, if $ = \u201cis sports-related\u201d, then [$](\u201cI love soccer.\u201d)= 1. Since a model typically requires multiple features to explain the data, we consider vectors \u00fe\u2208 \u03a6\u03ba of K predicates, where now [6] maps X to {0,1}K:\n$[\\phi](x) := ([\\phi_1](x), [\\phi_2](x),..., [\\phi_K](x))$.\nTo instantiate [] computationally, we prompt a language model to check whether o is true on the input x, following the practice from prior works [61, 62]. See Figure 2 (left) for the prompt we used.\nReweighting with w. Consider the following example:\nw = [\u22125,3]; $ = [\u201cis in English\u201d, \u201cis sports-related\u201d].\nThen w\u00b9 [6] has a value of -5 \u00b7 1 + 3\u00b70 = -5 for an English, non-sports related sample x. More generally, wT [](x) is larger for non-English sports-related samples.\nDefining p(x | \u03a6, w). According to Equation 1, p(x | 6,w) is a distribution over X, all the text samples we observe, but it puts more weights on x with higher values of w\u00b9[6](x). Using the example w and above, p(x | 6, w) has higher probability on non-English sports-related texts.\nFinally, we define U(x) as the uniform distribution over X for later use."}, {"title": "3.2 Example Models Parameterized by Natural Language Predicates", "content": "We introduce three models parameterized by predicates: clustering, time series, and multi-label classification. For each model, we explain its input, the learned parameters & and w, the log-likelihood loss L, and its relation to classical models.\nClustering. This model aims to help humans explore a large corpus by creating clusters, each explained by a predicate. Such a model may help humans obtain a quick overview for a large set of machine learning inputs [60], policy discussions [47], or business reviews [4]. Given a set of text X, our model produces a set of K clusters, each parameterized by a learned predicate $k; for example, if the predicate is \u201cdiscusses the U.S. Election\", then the corresponding cluster is a uniform distribution over all samples in X that discuss the U.S. Election.\nSimilar to K-means clustering, each sample x is assigned to a unique cluster. We use a one-hot basis vector bx \u2208 RK to indicate the cluster assignment of x, and set Wx = \u03c4\u00b7bx, where \u2020 has a large value (e.g. 10). We maximize the total log-likelihood:\n$\\mathcal{L}(\\phi, w) = - \\sum_{x} log(p(x | \\phi, w_x)); w_x = \\tau \\cdot b_x, \text{ where } \\tau \\rightarrow \\infty \\text{ and } b_x \text{ is a basis vector}.$\nHowever, some samples might not belong to any cluster and thus have 0 probability; to prevent infinite loss, we add another \u201cbackground cluster\u201d U(x) that is uniform over all samples in X; therefore, each sample x can back off to this cluster and incur a loss of at most log U(x) = log(|X|).\nTime Series Modeling. This model aims to explain latent variations in texts that change across time; for example, finding that an increasing number of people \u201csearch about flu symptons\" ($) can help us forecast a potential outbreak [16]. Formally, the input is a sequence of T text samples X = {xt}=1 Our model produces K predicates k that capture the principle axes of variation in x across time. We model w\u2081 .. wT as being drawn from a Brownian motion, i.e.,\n$p(x_t | \\phi, w_t) \\propto exp(w_t^T [\\phi](x_t)); w_t := w_{t-1} + \\mathcal{N}(0, \\lambda^{-1} I),$\nwhere X is a real-valued hyper-parameter that regularizes how fast w can change. The loss L is hence\n$\\mathcal{L}(\\phi, w) = \\sum_{t=1}^T - log(p(x_t | \\phi, w_t)) + \\sum_{t=1}^{T-1} ||w_t - w_{t+1}||_2.$\nMulticlass Classification with Learned Feature Predicates. This model aims to explain the decision boundary between groups of texts, e.g. explaining what features are more correlated with the fake news class [35] compared to other news, or explaining what activates a neuron [5]. Suppose there are C classes in total; the dataset is a set of samples xi each associated with a class yi \u2208 [C]. Our model is hence a linear logistic regression model on the feature vectors extracted by 6, i.e.\n$logits(x_i) = W\\cdot [\\phi](x_i); \\mathcal{L}(\\phi, W) = - \\sum_i log(\\frac{e^{logits(x_i)_{y_i}}}{\\sum_{c=1}^C e^{logits(x_i)_c}}),$\nwhere W \u2208 RC\u00d7K is the weight matrix for logistic regression.\""}, {"title": "4 Method", "content": "We can now learn the parameters for each model above by minimizing the loss function L. Formally,\n$\\phi, w = argmin_{\\phi \\in \\Phi^K, \\omega} \\mathcal{L}(\\phi,W)$.\nHowever, optimizing & is challenging, since it is discrete and therefore cannot be directly optimized by gradient-based methods. To address this challenge, we develop a general optimization method, which we describe at a high level in Section 4.1, introduce its individual components in Section 4.2, and explain our full algorithm in Section 4.3."}, {"title": "4.1 High-Level Overview", "content": "Our framework pieces together three core functions that require minimal model-specific design:\n1. OptW, which optimizes w.\n2. OptRelaxedPhi, which optimizes a continuous relaxation \u00d8k for each predicate \u222ek.\n3. Discretize, which maps from continuous predicate k to a list of candidate predicates.\nUsing these three components, our overall method initializes the set of predicates by first optimizing w and using OptW and OptRelaxedPhi and then discretizing \u1fa7 with Discretize. To further improve the loss, it then iteratively removes the least useful predicate, re-optimizes its continuous representation, and discretizes it back to a natural language predicate.\nTo provide more intuition for these three components, we explain what they should achieve in the context of clustering. OptW should optimize the 1-hot choice vectors wr by assigning each text sample to the cluster with maximum likelihood. OptRelaxedPhi should find a continuous cluster representation k similar to the sample embeddings assigned to this cluster, and Discretize generates candidate predicates that explain which samples' embeddings are similar to k. Next, we introduce these three components formally for general models with predicate parameters."}, {"title": "4.2 Three Components of our framework", "content": "OptW optimizes w while fixing the values of 6. Formally, OptW(6) := argminwL(6, w).\nThis function needs to be designed by the user for every new model, but it is generally straightforward: in the clustering model, it corresponds to finding the cluster that assigns the highest probability for each sample; in classification, it corresponds to learning a logistic regression model; in the time series model, the loss is convex with respect to w and hence can be optimized via gradient descent.\nFor later use, we define the fitness of a list of predicates 6 as the negative loss after w is optimized:\nFitness(6) := -L(, OptW()).\nNext, we discuss OptRelaxedPhi. The parameters 6 are discrete strings, so the loss function is not differentiable with respect to 6. To address this, we approximate [6](x) with the dot product of two continuous vectors, Okex, where ex \u2208 Rd is a feature embedding of x normalized to unit length (e.g. the last-layer activations of some neural network), and \u00d8k \u2208 Rd is a unit-length, continuous relaxation of k. Intuitively, if the optimal $ = \u201cis sports-related", "w": "nOptRelaxedPhi(w) = argmin\u03c61...\u03c6KL(\u03c6 | w).\nWe sometimes also use it to optimize a single continuous predicate \u017fk given a fixed w and all discrete predicate variables other than k (denoted as \u2212k):\nOptRelaxedPhi(\u03a6\u2212k, w) = argmin\u012bk L(\u03a6k|\u03a6\u2212k, w)."}, {"title": "4.3 Piecing the Three Components Together", "content": "Our algorithm has two stages: we first initialize all the predicate variables and then iteratively refine each of them. During initialization, we\n1. randomly initialize continuous predicates \u017f to be the embedding of random samples from X\n2. optimize L(, w) by alternatively optimizing w and all the continuous predicates \u1fa7 with OptW and OptRelaxedPhi, and\n3. set \u00d8k as the first candidate from Discretize(k)\nDuring refinement, we repeat the following steps for S iterations:\n1. find the least useful predicate $k; we define the usefulness of k as how much the fitness would decrease if we zero it out, i.e. -Fitness(6\u2212k, 0).\n2. optimize k using OptRelaxedPhi and choose the fittest predicate from Discretize(k)\nWe include a formal description of our algorithm in Appendix Algorithm G."}, {"title": "5 Experiments", "content": "In this section, we benchmark our algorithm from Section 4; we later apply it to open-ended applications in Section 6. We run our algorithm on datasets where we know the ground truth predicates 6 and evaluate whether it can recover them. On five datasets and three statistical models, continuous relaxation and iterative refinement consistently improve performance. Our general method also matches a previous specialized method for explainable clustering [53]."}, {"title": "5.1 Datasets", "content": "We design a suite of datasets for each of the three statistical models mentioned from Section 3.2. Each dataset has a set of reference predicates, and we evaluate our algorithm's ability to recover them.\nClustering. We consider five datasets, AGNews, DBPedia, NYT, Bills, and Wiki [40, 58, 32, 23]. The datasets have 4/14/9/21/15 topic classes each described in a predicate, and we sample 2048 examples from each for evaluation.\nMulticlass Classification. We design a classification dataset with 5,000 articles and 20 classes; its goal is to evaluate a method's ability to recover the latent interpretable features useful for classification. Therefore, we design each class to be a set of articles that satisfy three predicates about its topic, location, and language; for example, one of the classes can be described by the predicates \"has a topic of sports\", \"is in Japan\", and \"is written in English\". We create this dataset by adapting the New York Times Articles dataset [40], where each article is associated with a topic and a location predicate; we then translate them into Spanish, French, and Deutsch. We consider in total 4 + 4 + 4 = 12 different predicates for each of the topic/location/language attributes and subsample 20 classes from all 4x4x4 = 64 combinations.\nTime Series modeling. We synthesize a time series problem by further adapting the translated NYT dataset above. We set the total time T = 2048 and sample x1... xT according to the time series model in Section 3.2 to create the benchmark. We set & to be the 12 predicates mentioned above and the weight w.k for each predicate k to be a cosine function with a period of T to simulate how each attribute evolves throughout time. In addition, we included three simpler datasets where there is only variation on one attribute (i.e. varies only on one of topic/location/language). We name these four time series modeling all, topic, locat, and lang, respectively. See Appendix B for a more detailed explanation."}, {"title": "5.2 Metrics", "content": "To evaluate our algorithm, we match each learned predicate \u017fk with a reference 4,, compute the F1-score and surface similarity for each pair, and then report the average across all pairs. To create the matching, we match k to the *, with the highest overlap (number of samples where both are true); formally, we define a bi-partite matching problem to match each predicate in 4 with one in 4*, define the weight of matching and to be their overlap, and then find the maximum weight matching via the Hungarian algorithm. We now explain the F1-score and surface similarity metric.\nF1-score Similarity. We compute the F1-score of using f(x) to predict $*(x) on X, the set of samples we observe. This is similar to the standard protocol for evaluating cluster quality [28].\nSurface Form Similiarity. We can also directly evaluate the similarity between two predicates based on their string values, e.g. \u201cis about sports\u201d is similar in meaning to \u201chas a topic of sports\", a metric previously used by [62]. For a pair of predicates, we ask gpt-4 to evaluate whether they are similar in meaning, related, or irrelevant, with each option associated with a surface-similarity score of 1/0.5/0. We display the prompt in Figure 7 and example ratings in Table 1.\""}, {"title": "5.3 Experiments on Our Benchmark", "content": "We now use these metrics and datasets to evaluate the optimization algorithm proposed in Section 4 and run ablations to investigate whether continuous relaxation and iterative refinement are effective. We will first introduce the overall experimental setup, and then discuss individual takeaways supported by experimental results in each paragraph.\nExperimental Setup. When running the algorithm, we generate candidate predicates in Discretize with gpt-3.5-turbo [37]; to perform the denotation operation [[$](x), we use flan-t5-x1 [13]; we create the embedding for each sample x with the Instructor-xl model [48] and then normalize it with 12 norm. We set the number of candidates M returned by Discretize to be 5 and the number of optimization iteration S to be 10. To reduce noises due to randomness, we average the performance of five random seeds for each experiment.\nTable 2 reports the results of clustering and Table 3 reports other results. For each dataset, we perform several ablation experiments and present the takeaways from these results."}, {"title": "Takeaway 0", "content": "Is our method better than na\u00efvely prompting language model to generate predicates? How does our approach compare to a na\u00efve baseline approach, which directly prompts the language model to generate predicates based on dataset samples? For this baseline, we repeatedly prompt a language model to generate more predicates until we obtain K predicates, compute their denotation, evaluate them using the metrics in Section 5.2, and report the performance in Table 2 and 5, the Prompting row. Across all entries, our approach significantly outperforms this baseline."}, {"title": "Takeaway 1", "content": "Relax + discretize is better than exploring randomly generated predicates. Our optimization algorithm explores the top-5 LLM-generated predicates that have the highest correlations with k.ex. Would choosing a random predicate be equally effective? To investigate this question, we experimented with a variant of our algorithm that randomly chooses five predicates without utilizing the continuous representation \u00d8k (No-Relax). In Table 2 and 3, No-Relax underperforms our full algorithm (Ours) in all cases. In Appendix Figure 8, we plot the loss after each iteration averaged across all tasks, and we find that Ours converges much faster than No-Relax."}, {"title": "Takeaway 2", "content": "Iterative refinement improves the performance. We considered a variant of our algorithm that only discretizes the initial continuous representations and does not iteratively refine the predicates (No-Refine). In Table 2 and 3, No-Refine underperforms the full algorithm in all cases."}, {"title": "Takeaway 3", "content": "Our model-agnostic method is competitive with previous methods specialized for explainable clustering. We compare our method to GoalEx from [53], which designs a specialized method for explainable clustering based on integer linear programming. Even though our method is model-agnostic, it matches or outperforms GoalEx on four out of five datasets and improves F1 by 0.02 on average."}, {"title": "Takeaway 4", "content": "Our method accounts for information beyond the set of text samples (e.g. temporal correlations in the time series). We investigate this claim using the time series datasets, where we shuffle the text order and hence destroy the time-dependent information a model could use to extract informative predicates (Shuffled). Table 3 finds that Ours is better than Shuffled in all cases, indicating that our method does make use of temporal correlations."}, {"title": "6 Open-Ended Applications", "content": "We apply our framework to a broad range of applications to show that it is highly versatile. Our framework can monitor data streams (Section 6.1), apply to the visual domain (Section 6.2), and be easily steered to explain specific abstract properties (Section 6.3). Across all applications, our framework is able to explain complex concepts that classical methods struggle to produce."}, {"title": "6.1 Running Our Models Out of the Box", "content": "Monitoring Complex Data Streams of LLM Usages\nWe apply our models from Section 3.2 to monitor complex data streams of LLM usages. In particular, we recursively apply our clustering model to taxonomize user queries into application categories, apply our time series model to characterize trends in use cases across time, and apply our classification model to find categories where one LLM is better than the other. Due to space constraints, we present the key results in the main paper and the full results in Appendix F."}, {"title": "Taxonomizing User Applications via Clustering", "content": "LLMs are general-purpose systems, and users might apply LLMs in ways unanticipated by the developers. If the developers can better understand how the LLMs are used, they could collect training data correspondingly, ban unforeseen harmful applications, or develop application-specific methods. However, the amount of user queries is too large for individual developers to process, so an automatically constructed taxonomy could be helpful.\nWe recursively apply our clustering model to user queries to the ChatGPT language model. We obtain the queries by extracting the first turns from the dialogues in WildChat [59], a corpus of 1M real-world user-ChatGPT dialogues. We use gpt-40 [36] to discretize and claude-3.5-sonnet [1] to compute denotations. We first generate K = 6 clusters on a subset of 2048 queries; then we generate K = 4 subclusters for each cluster with > 32 samples.\nWe present part of the taxonomy in Figure 3 (left) and contrast it with the taxonomy constructed by directly applying LDA recursively (right). Although some LDA topics are plausibly related to certain applications, they are still ambiguous; for example, it is unclear what topic 1 \"ar prompt description detailed\" means. After manually inspecting the samples associated with this topic, we found that they were related to the application of writing prompts for an image-generation model. In contrast, our framework can explain complicated concepts that are difficult to infer from individual words; for example, it generates \u201crequesting graphic design prompts\" for the above application, which is much clearer in its meaning when explained in natural language.\""}, {"title": "Characterizing Temporal Trends via Time Series Modeling", "content": "Understanding temporal trends in user queries can help forecast flu outbreaks [16], prevent self-reinforcing trends [19], or identify new application opportunities. We run our time series model on 1000 queries from WildChat with K = 4 to identify temporal trends in user applications, and report part of the results in Figure 4. Based on the blue curve, we find that an increasing number of users \u201crequests writing or content creation \u2013 creating stories based on given prompts.'. This helps motivate systems like Coauthor [29] to assist with this use case."}, {"title": "Finding Categories where One Language Model is Better than the Other", "content": "One popular method to evaluate LLMs is crowd-sourcing: an evaluation platform (e.g. ChatBotArena [11]) / or a company (e.g. OpenAI) accepts prompts from users, shows users responses from two different LLM systems, and the users indicate which one they like better. The ranking among the LLM systems is then determined by Elo-rating, i.e. how often they win against each other.\nHowever, aggregate Elo-rating omits subtle differences between LLM systems. For example, LLama-3-70B achieved a similar rating as Claude-3-Opus, and the LLM community was excited that open-weight models were catching up. However, is LLama-3-70B similarly capable across all categories, or is it significantly better/worse under some categories? Such information is important for downstream developers, since some capabilities are more commercially valuable than others: e.g. a programmer usually does not care about LLM's capability to write jokes. We need a more fine-grained comparison.\nWe directly apply the classification model from our framework to solve this task. To understand the categories where LLama-3-70B is better/worse than Claude-3-Opus, we gather user queries x from the ChatBotArena maintainers (personal communication), set y = 1 if the LLama-3-70B's response to x is preferred and y = 0 otherwise. We set K = 3.\nOur model finds that LLama-3-70B is better when the query \u201casks an open-ended or thought-provoking question\u201d but worse when it \u201cpresents a technical question\u201d or \u201ccontains code snippets\". These findings are corroborated by manual analysis by the ChatBotArena maintainers, who also found that Llama-3 is better at open-ended and creative tasks while worse at technical problems. We hope that our model can automatically generate similar analysis in the future when a new LLM is released, thus saving researchers' efforts.\nTo summarize, our framework 1) enables us to define a time series model to explain temporal trends in natural language, and 2) outputs sophisticated explanations that LDA fails to generate. However, it is far from perfect: it is slow to compute denotations for all pairs of x and candidates & since it involves many LLM API calls, and the predicates themselves are sometimes redundant. We describe these limitations and potential ways to improve them in Appendix F.\""}, {"title": "6.2 Applying Our Classification Model to Images", "content": "Explaining Memorable Visual Features\nOur framework is applicable to the vision domain since a natural language predicate & can extract binary values from an image x. For example, for the rightmost image x in Figure 5 right, the predicate \u201cportrays a person\u201d evaluates to 1, i.e. [4](x) = 1, while \u201ccontains texts\u201d evaluates to 0.\nWe present an application of our classification model from Section 3.2 to images, which learns linear weights over a set of visual features described by natural language predicates. This model has also appeared in prior works: our model is equivalent to the language-based concept bottleneck model proposed by [55, 41]; additionally, when K = 1 and C = 2, our model is equivalent to the VisDiff framework [15], which finds a single predicate to discriminate samples from two classes of images.\nWe apply our classification model to the LaMem dataset [24] to understand what visual features make an image more memorable, an interesting cognitive science question. We now define the samples xi"}, {"title": "6.3 Explaining Abstract Properties via Easy Steering", "content": "Clustering Problems Based on Subarea\nCan our framework explain more abstract aspects of a sample x: e.g. subarea, the type of knowledge required to solve a math problem x? We show this is feasible by applying our model from Section 3.2 to cluster math problems and steering it to focus on explaining subareas. Meanwhile, classical methods struggle to explain abstract aspects.\nWe apply our clustering model from Section 3.2 to cluster the MATH dataset [21] based on subareas. The MATH dataset contains five labeled subareas and we hope our model can recover all of them: Algebra, counting_and_probability, geometry, number_theory, and precalculus. To steer our clustering model to explain subareas, we simply prompt the discretizer LLM \u201cI want to cluster these math problems based on the type of skills required to solve them.\" We set K = 5, using gpt-40 to discretize and gpt-40-mini to compute denotation.\nWe present the outputs of our model on the left of Figure 6. With simple prompting, our model is successfully steered to cluster based on subareas and recovers all five labeled subareas from the MATH dataset. Note that our explanations can explain abstract properties that have no word overlap with the samples that match them: for example, the math problems that \u201crequires geometric reasoning\u201c (Figure 6 left 3) usually contain neither of the word \u201cgeometric\u201d or \u201creasoning\u201d."}, {"title": "7 Conclusion", "content": "In this work, we formalize a broad family of models parameterized by natural language predicates. We design a learning algorithm based on continuous relaxation and iterative refinement, both of them effective based on our ablation studies. Finally, we apply our framework to a wide range of applications, showing that it is highly versatile, practically useful, applicable to both text and vision domains, and explains sophisticated concepts that classical methods struggle to produce. We hope future works can make our method more computationally efficient and apply it to more realistic applications, thus assisting humans to discover and understand complex patterns in the world."}, {"title": "A More Related Work", "content": "LLM for Exploratory Analysis. Due to its code generation capability [10], large language models have been used to automatically generate programs to analyze a dataset and generate reports from them [31, 20]. In comparison, our work focuses on generating natural language parameters to extract real-valued features from structured data.\nDiscrete Prompt Optimization. Many prior works optimized discrete prompts to improve the predictive performance [43, 14], and some recent works demonstrated that LLMs can optimize prompts to reach state-of-the-art accuracy [64, 56]. In comparison, we focus on optimizing discrete predicates to explain patterns rather than improve task performance.\nLearning with Latent Language. [3] first proposed to learn in a hypothesis space of natural language strings to improve generalization, and later works in this area have focused on using natural language to guide the learning process to improve downstream task performance [33, 25, 42, 54]. In contrast, our work focuses on explaining datasets with natural language, rather than improving downstream task performance."}, {"title": "B Time Series Dataset", "content": "To sample texts from the All time series problem, we sample from the time series model described in Section 3.2; we set & to be all the 12 predicates, sort them first by attributes (e.g. topic/location/language) then alphabets, and we set the weight for the kth predicate to be a sin function with period T and evenly spaced phases, i.e.\n$w_{k,t} = sin(2\\pi(\\frac{t}{T} + \\frac{k}{K}))$\nAs a result, the weight for each predicate has evenly spaced phases and would peak at different time period."}, {"title": "C Surface form similarity prompt", "content": "We include our prompt used to evaluate the surface form similarity between the predicted predicate $k and the reference predicate $k* in Figure 7."}, {"title": "D Additional Results on Our Benchmark", "content": "Our method is similar or better than classical methods such as topic modeling or K-means. We report the performance of K-means clustering and topic modeling under the clustering benchmark"}, {"title": "E Detecting Self-Reinforcing Trends in Machine Learning System", "content": "Machine learning systems sometimes have unintended side effects and reinforcement themselves. [19"}]}