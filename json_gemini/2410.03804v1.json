{"title": "MIXTURE OF ATTENTIONS\nFOR SPECULATIVE DECODING", "authors": ["Matthieu Zimmer", "Milan Gritta", "Gerasimos Lampouras", "Haitham Bou Ammar", "Jun Wang"], "abstract": "The growth in the number of parameters of Large Language Models (LLMs) has\nled to a significant surge in computational requirements, making them challeng-\ning and costly to deploy. Speculative decoding (SD) leverages smaller models\nto efficiently propose future tokens, which are then verified by the LLM in par-\nallel. Small models that utilise activations from the LLM currently achieve the\nfastest decoding speeds. However, we identify several limitations of SD models\nincluding the lack of on-policyness during training and partial observability. To\naddress these shortcomings, we propose a more grounded architecture for small\nmodels by introducing a Mixture of Attentions for SD. Our novel architecture can\nbe applied in two scenarios: a conventional single device deployment and a novel\nclient-server deployment where the small model is hosted on a consumer device\nand the LLM on a server. In a single-device scenario, we demonstrate state-of-\nthe-art speedups improving EAGLE-2 by 9.5% and its acceptance length by 25%.\nIn a client-server setting, our experiments demonstrate: 1) state-of-the-art laten-\ncies with minimal calls to the server for different network conditions, and 2) in\nthe event of a complete disconnection, our approach can maintain higher accuracy\ncompared to other SD methods and demonstrates advantages over API calls to\nLLMs, which would otherwise be unable to continue the generation process.", "sections": [{"title": "1 INTRODUCTION", "content": "Auto-regressive inference with LLMs has become quite cost-prohibitive due to the increasing pa-\nrameter count of recent transformer-based LLMs (Vaswani, 2017). Different types of (usually or-\nthogonal) solutions have been proposed to address this challenge, e.g. Mixture of Experts (Jacobs\net al., 1991), Flash Attention (Dao et al., 2022), Model Quantization and Distillation (Polino et al.,\n2018), Linear/Sparse Self-Attention (Zhang et al., 2021), Tensor Parallelism (Shoeybi et al., 2019)\nand others. In this work, we focus on a recent LLM acceleration technique called Speculative De-\ncoding, which leverages efficient models (smaller but less capable) to draft future tokens, which are\nverified by the LLM (more capable but much less efficient) in parallel (Leviathan et al., 2023).\nThe most recent state-of-the-art SD methods, like EAGLE (Li et al., 2024b) and MEDUSA (Cai\net al., 2024a), leverage activations from the LLM. However, those methods have some architectural\nlimitations including partial observability and the lack of on-policyness. Partial observability occurs\nwhen the small (draft) model lacks complete information about the state of the LLM, leading to\nsuboptimal predictions. The lack of on-policyness during training arises because the small model is\noften trained under ideal conditions, assuming perfect inputs. This does not reflect the real-world\nscenario where the small model generates some inputs. The longer we draft new tokens using only\nthe small model, the bigger the distribution shift from the training setting. These limitations can\ndegrade the performance and reliability of speculative decoding."}, {"title": "2 BACKGROUND", "content": "We first present the background knowledge required for the remainder of the paper, i.e. the decoding\nmechanisms of LLMs as well as the drafting + verification techniques that ensure correct generation."}, {"title": "2.1 LLM DECODING", "content": "Decoding refers to the process by which LLMs generate tokens in response to input queries. This\ngeneration is typically done auto-regressively, where each new token $y_t$ is sampled from the LLM's\ndistribution, conditioned on both the query and the preceding tokens $y_{<t}$. We explore decoding from\nthe perspective of dynamic systems, providing a foundation for developing new decoding mecha-\nnisms that combine large and small models (Kong et al., 2024). The internal workings of LLMs\ncan be best understood from a dynamic system perspective, which evolves as tokens are generated.\nGiven a large model $M_{Large}$, we can describe the state transition model of vanilla decoding as:\n$h_{<t+1}, o_{t+1} = fLarge(h_{<t}, token_embed(y_t)),\\qquad Y_{t+1} \\sim Softmax(LM\\_head(o_{t+1})),$ (1)\nwhere $h_{<t}$ represents the key and value tensors of every layer until the current time-step t, $Y_t$ is\nthe most recent token and $Y_{t+1}$ is the next token, which is sampled from a softmax distribution.\nFurthermore, token_embed is a lookup table, it assigns an embedding to a particular token of the\nvocabulary V. LM-head is a projection from the embedding size to the vocabulary size |V|. Finally,\n$f_{Large}()$ is the function aggregating all decoder layers of $M_{Large}$ and $o_{t+1}$ is the activation of the\nfinal decoder layer. With this, the state of the dynamic system is composed of $(h_{<t}, Y_t)$, the minimal\ninformation needed to sample the next token from $M_{Large}$."}, {"title": "2.2 SPECULATIVE DECODING", "content": "Some of the earliest work on speculative decoding was introduced by Stern et al. (2018), later ex-\ntended to non-greedy sampling (Leviathan et al., 2023). These methods are motivated by the pro-\nhibitive cost of auto-regressive generation with $M_{Large}$ that could be alleviated by using a draft\nmodel $M_{Small}$ that can more efficiently generate tokens that do not require the full capability of"}, {"title": "2.3 ARCHITECTURE OF $M_{SMALL}$", "content": "Speculative decoding architectures broadly fall into two categories, independent and self-drafting.\nIndependent drafters are typically smaller versions of $M_{Large}$ from the same model family (Li et al.,\n2024a; Zhao et al., 2024; He et al., 2023) while self-drafting methods leverage either a subset of\n$M_{Large}$ and/or newly initialised parameters (Ankner et al., 2024; Cai et al., 2024a).\nOur contribution is built on EAGLE (Li et al., 2024b), a self-drafting architecture which has shown\nthe best results on the Spec-Bench (Xia et al., 2024) leaderboard so far. The drafter reuses the\ntoken_embed and LM-head parameters of $M_{Large}$ (1). It takes as input the ground-truth activations\nof the last decoder layer of $M_{Large}$, $o_{1},\u00b7\u00b7\u00b7, o_{t}$ and the tokens of the sequence $Y_{1},\u2026, Y_{t}$ to predict\nthe next activations $\\hat{o}_{t+1}$, which is passed to the LM-head to predict the next token distribution:\n$\\hat{o}_{t+1} = M_{EAGLE}((o_{1},\u2026\u2026, o_{t}), token\\_embed(y_{1},\u2026\u2026, y_{t})), \\qquad \\hat{y}_{t+1} \\sim Softmax(LM\\_head(\\hat{o}_{t+1}))$\nThe process is repeated by appending $\\hat{o}_{t+1}, \\hat{y}_{t+1}$ to the inputs to auto-regressively draft tokens $\\hat{y}_{t+2}$."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 MIXTURE OF ATTENTIONS", "content": "We begin by defining important properties of $M_{Small}$ followed by detailed architectural choices."}, {"title": "3.1.1 PARTIAL OBSERVABILITY", "content": "In Markov Decision Processes (Kaelbling et al., 1998), partial observability is a common challenge\nwhere the agent does not have enough information about the true underlying state to take optimal de-\ncisions. This limitation can significantly degrade the agent's performance. Several approaches have\nbeen proposed to mitigate this, e.g., adding additional previous observations (Mnih et al., 2015). In"}, {"title": "3.1.2 LACK OF ON-POLICYNESS", "content": "Discrepancies between training and testing scenarios arise because, during training, transformer\nmodels are typically conditioned on ground-truth sequences, assuming that all previous inputs are\ncorrect. If this assumption seems unproblematic for the standard training of transformers, assuming\nit for training $M_{Small}$ in speculative-decoding scenarios is much more delicate. It is known that some\nof the previous inputs are generated directly from $M_{Small}$, therefore much less accurate. The more\ntokens we predict with $M_{Small}$ only, the more error accumulation we can expect. To alleviate this,\nEAGLE adds uniform noise into its observations $(o_{1},\u2026\u2026, o_{t})$ at training time, but this is not ideal.\nIn order to train $M_{Small}$ optimally, we need to ensure that its training and inference conditions\nare closely matched. Specifically, this means training $M_{Small}$ as if some of the previous tokens\nwere generated by itself. Additionally, we should account for situations where the activations from\n$M_{Large}$ are not available, i.e. during the drafting cycle. This approach is called on-policy training.\nIn on-policy training, the data used for training is generated by the same policy (or model) that is\ncurrently being trained. For example, when we train a transformer using next-token prediction on\na static dataset, this is considered off-policy because the data doesn't change based on the model's\ndecisions. However, if we mix this static dataset with data generated by the model itself during\ntraining, we move towards a more on-policy approach. Similarly, if the model won't have access to\ncertain information, e.g. the activations of $M_{Large}$ during generation, then always training $M_{Small}$\nwith that information would also be considered off-policy.\nHowever, on-policy training is very costly because we would need to generate from the model during\ntraining. To formalise this limitation, we introduce the concept of T-step boundedness:\nProperty 3.2 (T-step bounded). A drafter f is said to be T-step bounded if, in a single forward pass,\nit can predict up to T future tokens without additional input from $M_{Large}$, i.e., $f(Y_{1}, Y_{2},..., Y_{t}) \u2192\n(\\hat{y}_{t+1}, \\hat{y}_{t+2},..., \\hat{y}_{t+T})$."}, {"title": "3.2 TARGET LAYER INFERENCE", "content": "Previous work assumed that the final hidden layer before LM-head was the most appropriate tar-\nget (activations) $M_{Small}$ should predict. However, we challenge that assumption by hypothesising\nthat targeting a deeper $M_{Large}$ layer may be more advantageous in terms of draft quality. We thus\ndecompose the dynamic system (1) layer-by-layer by introducing l as the (superscript) layer index:\n$o_{t+1}^{l+1} = token\\_embed(y_{t}),\\qquad h_{<t+1}^{l+1}, o_{t+1}^{l+1} = f_{decoder}^{l}(h_{t}^{l}, o_{t+1}^{l}),\\qquad Y_{t+1} \\sim Softmax(LM\\_head(o_{t+1}^{L+1})) \\qquad l = 1,..., L$\nwhere $f_{decoder}$ is the decoder layer of $M_{Large}$ at layer l. The state of this new dynamic system\nis composed of $( (o_{t+1}^{l+1},h_{t+1}^{l+1},h^{L}_{t})$. We observe that to perfectly predict $o_{t+1}^{L+1}$, it is sufficient to\nperfectly predict $o_{t+1}^{L+1}$ and reuse the $f_{decoder}$ of $M_{Large}$ and the already computed KV cache $h^{L}_{1}$ of\nthe layer L at time t. The same recursive reasoning can be made to predict $o_{t,1}$ from $o_{t}^{l}, O_{t+1}$, etc.\nWe assume (and later show) that predicting $o_{t+1}^{l+1}$ is always easier than predicting $o_{t+1}^{L+1}$ for l < k due\nto $o_{t+1}$ undergoing fewer layer transformations. Hence, we introduce a new hyperparameter N to\nrefer to the target layer $o_{L+1-N}$ that the $M_{Small}$ should predict. When N > 0, the N last layers\nof $M_{Large}$ (kept frozen during training) and their KV cache are used to output $o_{L+1}$. Henceforth,\nwe use notation (N = l) where l is an integer, to denote the target layer for inference. We can now\nprovide the equation describing our $M_{Ou}$ for a given N assuming t was the last time we verified\nwith $M_{Large}$:\n$\\hat{o}_{T+1}^{L+1-N} = M_{0}(h_{t}, token\\_embed(y_{1},\u2026, Y_{t}, \\hat{y}_{t+1},\u2026, \\hat{y}_{T})),\\qquad \\hat{h}_{<T+1}^{l}, \\hat{o}_{T+1}^{l}= f_{decoder}((h_{t}, h_{t,<=T}), \\hat{o}_{T+1}^{l+1}), \\qquad \\hat{y}_{T+1} \\sim Softmax(LM\\_head(\\hat{o}_{T+1}^{L+1})).\\qquad l = L - N, . . ., L,$"}, {"title": "3.3 Loss", "content": "Let $M_{Small}$ be parameterised by $\\theta$, we use a similar training loss as EAGLE, i.e. a reverse-KL loss,\nshown to be more suitable for distillation (Gu et al., 2024), with a Smooth-L1 loss $L$ between the\npredicted activations of the $M_{Small}$ $\\hat{O}_{L+1-N}$ and the target one obtained from $M_{Large}$:\n$arg \\min_{\\theta} L_{AKL}[M_{Small}(\\theta)||M_{Large}] + \\lambda_{1}L (\\hat{o}^{L+1-N}, o^{L+1-N}).$ (2)\nTo keep the training lightweight, we do not generate from $M_{Large}$ or $M_{Small}$ during training. This\nloss is only defined over the response part of the prompt of a fixed training dataset."}, {"title": "4 EXPERIMENTS", "content": "In all experiments, we use LLama3-8B-Instruct (Dubey et al., 2024) as $M_{Large}$. We train $M_{Small}$ on\nthe Ultrachat dataset (Ding et al., 2023) without a system prompt and we do not assume that we know\nthe system prompt at test time. $M_{Large}$ is trained with the standard Llama3-Instruct chat template.\nUltrachat is composed of around 200k prompts with around 240M tokens using the LLama3 tok-\nenizer. We use multiple test datasets for generation including various tasks such as reasoning, code\ngeneration, multi-turn conversation and summarisation. We notably relied on the SpecBench bench-\nmark (Xia et al., 2024) and the following datasets: MT-Bench (Zheng et al., 2023), HumanEval\n(Chen et al., 2021), GSM8K (Cobbe et al., 2021), Alpaca (Taori et al., 2023), CNN/Daily Mail\n(Nallapati et al., 2016) and Natural Questions (Kwiatkowski et al., 2019). We describe additional\nhyperparameters and experimental settings in Appendix A.1.\nWe compare our method to EAGLE-2 and an independent distilled $M_{Small}$ of similar size (denoted\n\"Independent\"). In order to train the EAGLE model, we assume N = 0 in the distillation loss (2).\nThe independent $M_{Small}$ leverages the token\\_embed and LM-head parameters of $M_{Large}$ with only\nthe decoder layers trained using an identical distillation loss (2) and $\\lambda_{1}$ = 0. We do not compare\nto Medusa as EAGLE has consistently demonstrated superior speedups on various benchmarks (Xia\net al., 2024). We also compare the performance of the official EAGLE-2 weights shared by Li et al.\n(2024b). We refer to this as \"EAGLE-2 off.\". Note that this model was trained on different data and"}, {"title": "4.1 SINGLE DEVICE", "content": "We now present the main single-device experiments using the SpecBench Xia et al. (2024) bench-\nmark without a system-prompt to ensure a fair comparison between models."}, {"title": "4.2 CLIENT-SERVER", "content": "In this study, we investigate how self-drafting with our method performs in a client-server scenario.\nTo do so, we place $M_{Small}$ on a client device and host $M_{Large}$ on a server (see Appendix A.2 for an\nillustration). The server is performing verification and sends the relevant $M_{Large}$ activations to the\nclient, which in turn is proposing new tokens. The server has 3 times more float16 tflops than the\nclient. The devices are located in two different cities, separated by ~300 km. The ping between the\ndevices is around 9 ms and the bandwidth ~50 Mbits/sec. In order to simulate a realistic client-server\nscenario, we are using 5G and 4G network profiles. In 4G, we assume a maximum of 20 Mbits/sec\nwith a normally distributed delay of 21 ms \u00b1 19 ms and a 0.1% chance of dropping packets. In 5G,\nwe assume a normally distributed delay of 10 ms \u00b1 10 ms with a 0.1% chance of dropping packets.\nTo do so, we rely on the Linux traffic control subsystem.\nIn this scenario, the token-per-second performance also depends on the size of the messages. To this\nend, we analyse the length of the messages sent between the client and the server (see Table 7). There\nis a clear distinction between self-drafting methods that need to send/receive activation tensors and\nindependent methods that only exchange text (e.g. token ids). Therefore, we shall analyse whether\nthe improvement in drafting quality can offset the increase in message lengths. On the client, we\nencode each node in the draft tree using 3 bytes for the token id and 1 byte for its position in the\ntree. The server answers with the accepted tokens encoded using 3 bytes each plus the associated\nactivations, if required. For Llama3-8B-Instruct and N \u2264 1, our architecture's payload is less than\nor equal to EAGLE message lengths. In order to further reduce message sizes, we quantise the E\nand Eku tensors to 8 bits. For both EAGLE and Mixture of Attentions, the initial message sent by\nthe server (before the first token is drafted) is typically the biggest as it represents the activations of\nthe entire prompt. Therefore, we additionally gzip-compress this message after quantisation.\nIn Table 3, we can observe that \"Ours (N=0)\" achieves the fastest decoding speeds. Interestingly,\nit is even faster than independent small models that do not exchange any activation tensors. As\nexpected, our Mixture of Attentions is not as fast as in the single device setting, but it can recover\nthe speed of vanilla decoding in a single device setup (33 tokens-per-second, see Appendix A.4)."}, {"title": "4.3 ABLATION STUDY", "content": "We now present important ablation results for different components of our Mixture of Attentions\narchitecture. Since multiple models were required to be fine-tuned for this study, we have limited\neach run to 10 epochs. For this ablation, we introduce the \"Ours (N=l, -LSA)\" variant that does not\nrely on LSA and takes as input $o_{1},..., o_{t}$ as the keys and values of the CA layer. We also include\ntwo more EAGLE baselines, one with additional trainable parameters \"EAGLE (more params)\" and\nanother with additional decoder layers \"EAGLE (more layers)\" but an equal number of trainable\nparameters. This is to ensure that the benefit of our architecture does not come from simply adding\ndecoder layers or parameters. In this experiment, we use the HumanEval dataset with strict stopping\ncriteria, exiting decoding as soon as the model no longer generates source code.\nDoes the on-policyness (brought with the CA layer) and the T-step bounded property have\na positive impact on the quality of the drafts? In Table 5, we compare EAGLE with \"Ours\n(N=0, -LSA)\" for an answer to this question. We can see that these components provide a major\nimprovement of 26% in tokens-per-second as well as improved acceptance length of 33%."}, {"title": "5 RELATED WORK", "content": "Medusa (Cai et al., 2024a) is one of the earliest works leveraging the activations of $M_{Large}$ as inputs\nto $M_{Small}$ for the purpose of SD. Thanks to their work, speculative decoding can be applied to any\nLLM by distilling an $M_{Small}$. Medusa generates K future tokens in parallel by training K new\nLM_heads where each head predicts a token at position $k \\in K$. EAGLE (Li et al., 2024c) and\nHydra (Ankner et al., 2024) are auto-regressive extensions of Medusa. They observe that non-auto-\nregressive generation limits the acceptance length as $M_{Small}$ is not aware of previous tokens. We do\nnot compare to Medusa or Hydra as EAGLE is ranked higher on the SpecBench leaderboard.\nTandem Transformers (Nair et al., 2024) propose an effective integration of $M_{Large}$ and $M_{Small}$ by\nletting $M_{Small}$ attend to the down-projected hidden states of $M_{Large}$. These rich contextualised rep-\nresentations enable $M_{Small}$ to draft hypotheses with a higher acceptance rate as the two models are\naligned on shared hidden states. We were not able to compare with them because of the lack of open-\nsource implementation, the use of closed-source LLMs and an undisclosed amount of data/compute\nto reproduce the work. Moreover, tandem transformers appear to have a high communication over-\nhead between big and small models, making it unrealistic for a client/server setting.\nOrthogonal to our work, researchers have recently proposed training-free SD methods. Lookahead\nDecoding (Fu et al., 2024) generates new tokens with a single $M_{Large}$ using Jacobi iterations, ex-\ntended by CLLM Kou et al. (2024) and Ouroborous (Zhao et al., 2024). We evaluated the latter in\nour settings, however, it was shown to be less efficient than the EAGLE-2 tree decoding strategy, see\nAppendix A.4. For additional related and orthogonal work in the extended SD landscape, we refer\nthe reader to Xia et al. (2024) for a detailed and highly informative speculative decoding survey."}, {"title": "6 CONCLUSION", "content": "We have introduced a Mixture of Attentions architecture for Speculative Decoding to effectively\naddress several limitations of existing state-of-the-art methods. In order to enhance drafting accu-\nracy of $M_{Small}$, we proposed a mixture of attention layers: Layer Self-Attention to mitigate partial\nobservability and Self-Attention followed by Cross-Attention to train more on-policy. We have"}, {"title": "A APPENDIX", "content": "Upon acceptance, we will make the code publicly available at www.github.com."}, {"title": "A.1 HYPERPARAMETERS", "content": ""}, {"title": "A.3 ALGORITHM", "content": ""}, {"title": "A.4 ADDITIONAL EXPERIMENTS", "content": "To perform this experiment, we reuse the same full HumanEval dataset with a strict stopping criteria\nas done in the ablation study in the single device setting."}, {"title": "A.5 PRIVACY APPLICATION", "content": "Another advantage of the client-server setup is that we can selectively ensure privacy for the client\nby only sending the non-sensitive part of the prompt to the server. Essentially, the client can split\ntheir input into a consecutive \"safe\" text and a \"private\" text. The server processes only the \"safe\"\ntext, which could be general context or non-sensitive information. The client keeps the \"private\"\ntext, such as confidential data or sensitive instructions, and handles this part locally with $M_{Small}$.\nFor instance, the client might send the server some Python code along with a general description.\nHowever, any sensitive information, such as the login and password to inject into the code, remains\non the client side and is not transmitted to the server. It is only passed to $M_{Small}$. This approach\nleverages the activations of $M_{Large}$ to increase the accuracy of $M_{Small}$ for parts of the task while\nensuring that sensitive information is never exposed outside the client's environment."}]}