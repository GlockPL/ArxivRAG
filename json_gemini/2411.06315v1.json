{"title": "NeuReg: Domain-invariant 3D Image Registration on Human and Mouse Brains", "authors": ["Taha Razzaq", "Asim Iqbal"], "abstract": "Medical brain imaging relies heavily on image registration to accurately curate structural boundaries of brain features for various healthcare applications. Deep learning models have shown remarkable performance in image registration in recent years. Still, they often struggle to handle the diversity of 3D brain volumes, challenged by their structural and contrastive variations and their imaging domains. In this work, we present NeuReg, a Neuro-inspired 3D image registration architecture with the feature of domain invariance. NeuReg generates domain-agnostic representations of imaging features and incorporates a shifting window-based Swin Transformer block as the encoder. This enables our model to capture the variations across brain imaging modalities and species. We demonstrate a new benchmark in multi-domain publicly available datasets comprising human and mouse 3D brain volumes. Extensive experiments reveal that our model (NeuReg) outperforms the existing baseline deep learning-based image registration models and provides a high-performance boost on cross-domain datasets, where models are trained on \u201csource-only\" domain and tested on completely \"unseen\" target domains. Our work establishes a new state-of-the-art for domain-agnostic 3D brain image registration, underpinned by Neuro-inspired Transformer-based architecture.", "sections": [{"title": "Introduction", "content": "Image registration is an integral part of medical imaging analysis which by learning spatial deformation is able to reveal significant differences between a pair of images or volumes. Traditional image registration approaches including ANTS (Avants et al. [2008]), FFD (Rueckert et al. [2006]) and ADDMM (Thorley et al. [2021]) have achieved considerable performance and hence have been employed in various registration-based tools and techniques in real-world medical imaging applications, however, their time-consuming nature proves to be a bottleneck.\nRecently, Deep Neural Networks (DNNs) have shown exponential success in computer vision tasks including object detection (Tan et al. [2020]) and image classification (Chen et al. [2021]), hence they are actively being used to replace the traditional image segmentation (Iqbal et al. [2019]) and 3D registration models (Mok and Chung [2022], Chen and Frey [2022]), Shi et al. [2022], Mahmood et al. [2020], Mahmood et al. [2023], Mahmood et al. [2024a], Mahmood et al. [2024b]). Although, Convolutional Neural Networks (CNNs) efficiently encode and decode the visual differences between the input image pairs and provide an effective image registration pipeline (Huang et al. [2021]), (Hu et al. [2018]), there still exists a lot of limitations in these architectures in practice (Shen et al. [2019]), hence efforts are being directed towards designing scalable deep learning architectures. In this direction, transformer-based deformable image registration models (Chen and Frey [2022]), (Shi et al. [2022]) have achieved state-of-the-art (SOTA) performance on different imaging datasets. However, they require extensive training which is resource- and time-consuming. To mitigate the extensive training, FourierNet (Jia et al. [2023]), proposed a band-limited deformation which converts the deformation field to a Fourier domain representation, and a model-driven decoder is used. We propose a novel architecture that integrates a Swin Transformer (Liu et al. [2021]) encoder with a model-driven decoder, enabling high performance in the 3D registration task while minimizing the computational resources required for model training."}, {"title": "Results and Discussion", "content": "Furthermore, a significant drawback of using DNNs and transformer-based architectures is their dependency on the training data domain. In case of a covariate shift between the training and testing data, these models experience performance deterioration. Recent models like SynthMorph (Hoffmann and Billot [2022]) mitigate the dependency on the training data domain by drawing training samples from a random distribution. While this approach leads to a contrast invariant model yielding significant results, it requires prolonged training and fails to capture scale and structure variation in complex datasets.\nWe design a neuro-inspired pre-processing layer in our architecture that generates a domain-agnostic representation of the 3D brain volumes. To combine this domain-agnostic representation feature with the Swin Transformer encoder and model-driven decoder, we propose, NeuReg, a domain-agnostic transformer-based model architecture which is inspired by the mammalian visual system, and ensures that our 3D image registration model performs well on 'unseen' target domains for human and mouse datasets.\nPerformance on cross-domain human datasets:\nTo quantify the performance of our model on human brain imaging, we consider two open-sourced, commonly used Magnetic Resonance Imaging (MRI) datasets. iSeg-2017 dataset contains multi-modal MRI data covering T1-weighted and T2-weighted imaging domains. While T1-weighted MRI enhances the intensity of the fatty tissue and suppresses the signal of the Cerebrospinal Fluid (CSF) in the brain, T2-weighted MRI is completely opposite in terms of intensity since it enhances the CSF signal instead. Models trained on either domain (T1 or T2) are expected to perform well on MRI scans generated from the same domain but may perform poorly in case of a domain shift however a domain-agnostic framework should generalize and have high performance even across domains. To gauge the extent to which our model architecture generalizes over different domains, we compare its performance with the baseline models for both T1 and T2 domains in the iSeg-2017 dataset. We report the DICE and SSIM score for the iSeg-2017 dataset in Table 1 along with qualitatively comparing our model's results with FourierNet and SynthMorph, as shown in Figure 2. Both qualitative and quantitative results demonstrate that our model outperforms the existing 3D registration models. Although the baseline architectures have benchmarking performance for 3D registration on large open-sourced single-domain human MRI datasets, they are unable to generalize over different domains. Since our model, while creating a domain-agnostic representation of the data, mitigates the intensity shift present in the T1 and T2 scans, it cannot only outperform SOTA models but also achieve significantly high SSIM scores.\nFigure 2 qualitatively compares our model with SynthMorph and FourierNet while highlighting the CSF region. The zoomed in snippets of the normal brain (Figure 2, 1st and 3rd columns) clearly show the intensity difference between T1 and T2. While T1 has a low-intensity signal for CSF, the region has a high intensity in the case of T2. Moreover, the snippets showing our version of the fixed and moving samples (Figure 2, 2nd and 4th columns), clearly highlight that our model removes the intensity shift, and CSF in domain generalized T1 and T2 has attained similar intensity profile. The registration results show that while SynthMorph has an overall decent performance, it falters in specific regions leading to a marginally low score, as compared to our models. In the case of FourierNet, although the model tries to align the moving segmentation to the ground truth it fails in regions where intensity shift is encountered, including CSF. Our models, on the other hand, are able to come close to the actual segmentation ground-truth, and hence achieve high SSIM and DICE scores. This trend is consistently observed in cases where the model is trained on T1 and tested on T2 and vice versa.\nSince iSeg-2017 is a relatively small dataset, in order to show the utility of our model across a large dataset, we test our framework on larger and more complex OASIS-3 brain imaging data. We focus on T1-weighted and T2-weighted MRI scans and report results on both domains. Since SynthMorph and FourierNet are the current state-of-the-art in 3D registration, we compare the performance of our model with them for the OASIS-3 dataset.\nIn Table 2 we report the SSIM score computed between the fixed and aligned moving brains as the metric to compare the performance of our model with SynthMorph and FourierNet. We observe that both SynthMorph and FourierNet have a very low performance on data sampled from unseen domains which indicates that the model is unable to capture the varying complexity in different domains and generalizes poorly. Our model architecture, however, outperforms them by a large margin, indicating its ability to generalize across different domains. OASIS-3 dataset not only spans different domains but also contains scans from participants at different stages of cognitive decline, hence leading to large structural variation. Since we do not differentiate between the patients' cognitive health, the dataset also encapsulates structural changes. This adds to the overall complexity of the dataset and hence it requires extensive training. Although, our model outperforms SynthMorph and FourierNet and can capture the dataset complexity, however, we anticipate that our model results can be further improved with extensive training. We show additional results and comparison of our model with SynthMorph and FourierNet in Supplementary Figure 1\nPerformance on cross-domain mouse dataset:\nTo show the generalizability of our approach across species and complex imaging domains, we also repeat our experiments on the DevCCF dataset. Each model is trained on a single domain and tested on the remaining 5 domains, for which the average SSIM and DICE score is reported in Table 3.\nThe qualitative comparison between our best performing model and FourierNet is also shown in Figure 3. The first row in Figure 3 shows a clear intensity-based shift in the original samples from all the domains along with their scale variations.\nSince DevCCF encapsulates different developmental (Postnatal day 4, 14, and 56) stages in the mice brain, the dataset has significant variation in terms of scale and structure. While the domains vary in intensity and scale factors, they also have structural dissimilarities since the dataset spans different postnatal mouse ages. Since the brain undergoes delineation during development and each brain region grows in a non-uniform fashion, the dataset contains significant structural variation. A domain-agnostic registration model trained on DevCCF is expected to overcome scale as well as structural variation in 3D brain volumes and perform well across ages and domains. Due to the excessive structural dissimilarities in this dataset, our model trained with MSE as the image similarity loss performs the best since it is able to capture the existing structural differences between the input images (fixed and moving) during training. Moreover, since we aim to evaluate the models' capability to register across scale and structure variation, we use SSIM as the evaluation metric as well. Both Table 3 and Figure 3 show that our model performs better than the baselines and is able to capture the variation and complexity of the dataset in an apt manner. Additional results on different postnatal mouse ages is shown in Supplementary Figure 2. We show that our model outperforms FourierNet on all ages.\nThrough qualitative and quantitative results, we show that our model (NeuReg) outperforms existing registration frameworks across modalities and species. We report benchmarking performance for publicly available human and mouse datasets and show our model's capability to generalize across domains."}, {"title": "Conclusion", "content": "In this study, we propose NeuReg, a neuro-inspired domain agnostic transformer-based architecture for 3D image registration and demonstrate its benchmarking performance on cross-modal publicly available human and mouse brain 3D imaging datasets. Unlike the existing baselines and SOTA models for registration, our approach generalizes over unseen domains and performs exceptionally well on multi-domain datasets. Our domain generalization pre-processing layer in the proposed NeuReg architecture generates an invariant representation of the imaging volumes that captures the key common features present in cross-modal datasets. We demonstrate the effectiveness of our approach on commonly used human brain cross-modal datasets, for instance, training on T1 MRI modality and testing on T2 MRI modality (using iSeg-2017 and OASIS-3 datasets) and vice versa. We further extend this on the mouse brain 3D imaging dataset and demonstrate the benchmarking performance across all six cross-domains (by training on a single source domain and testing on the remaining 5 domains for each category). Our Swin-transformer-based architecture not only captures the intensity-based shifts present in the cross-modal datasets but also captures the scale and non-linear delineations present in longitudinal datasets (by training on a single developmental age and testing on unseen developmental ages with varying domains). This shows the promising performance of our proposed architecture on multiple fronts i.e. domain-shift due to imaging as well as age. In the future, we aim to extend NeuReg to handle more complex domains and even diverse datasets as well as apply it to explore neurodevelopmental alternations in healthy and non-healthy brains."}, {"title": "Methods", "content": "NeuReg architecture\nWe follow the conventional paradigm of image registration where a moving brain $I_m$ is mapped to the fixed brain, $I_f$. We propose a novel domain-agnostic transformer-based architecture, summarized in Figure 1, which consists of a domain generalized pre-processing layer, Swin transformer encoder, and a model-driven decoder.\nDomain generalization layer: We take inspiration from the feature processing procedure by the excitatory and inhibitory neurons in the mammalian visual cortex and design a domain-agnostic representation of image. In 3D registration, we process the input 3D volume $I \\in \\mathbb{R}^{W \\times H \\times D}$, where $W$, $H$, and $D$ represent its width, height, and depth, respectively. Our goal is to form a domain-independent representation, denoted as $I(n)$. The image $I$ is segmented into patches of size $x$, represented as $T = \\{t_1, t_2, ..., t_n\\}$, where each patch $t_i$ encircles a pixel $m$ at coordinates $(i,j,k)$. For each patch, its mean $\\mu_{t_i}$ and standard deviation $\\sigma_{t_i}$ are calculated to construct $I_n$:\n$\\mu_{t_i} = \\frac{1}{x^2} \\sum_{i,j,k \\in t_i} M_{ijk}$\n$\\sigma_{t_i} = \\frac{1}{x^2} \\sum_{i,j,k \\in t_i} (M_{ijk} - \\mu_{t_i})^2\\right)^{1/2}$\nThe corresponding domain-generalized representation, $I_n$ is derived by updating pixel values within each patch according to the mean and standard deviation of the patch, as governed by the following equations\n$z = max\\{\\sigma_{t_1}, \\sigma_{t_2},...,\\sigma_{t_n}\\}$\n$I(n) = \\sigma_{t_i}/z$\nExisting pre-processing contrast normalization techniques, including local contrast normalization (LCN) (Jarrett et al. [2009]) and local response normalization (LRN) (Krizhevsky et al. [2012]), often result in information loss. These methods focus on enhancing local contrast within an image, which can lead to the suppression or enhancement of specific pixels, thus losing important pixel-level information (Huang et al. [2020]). Similar issues arise with edge detection filters such as Laplacian (Gong et al. [2021]) and Sobel (Vincent and Folorunso [2009]) filters, which primarily detect edge maps and tend to overlook low-intensity pixel information.\nIn contrast, our technique preserves pixel-level information while ensuring normalization across the entire image without selectively enhancing or suppressing any pixels. This results in a consistent and comprehensive contrast normalization. Additionally, our domain generalization technique can serve as a pre-processing layer atop any existing 3D registration model, enhancing performance for multi-domain registration tasks (Supplementary Table 1).\nThe domain agnostic representation of $I_m$ and $I_f$ is fed to the Swin Transformer for registration.\nSwin transformer encoder: For our encoder block, we use rectangular parallelepiped windows, a variation of the original Swin transformer, to accommodate the varying shapes of the 3D volumes $(M_x, M_y, M_z)$. The global and local differences between the domain invariant image input pairs $(I_m$ and $I_f)$ are successfully captured by the self-attention mechanism of the Swin transformer and are used to produce a deformation field $(\\phi)$. We apply general Discrete Fourier transform (DFT) on the deformation field and convert it from a spatial domain to a low pass Fourier domain.\nModel-driven decoder: The decoder used in our framework comprises a zero padding layer followed by an inverse DFT (iDFT). The low dimensional deformation fields produced by the encoder are passed through a zero padding layer and a standard inverse Discrete Fourier Transform is applied to restore the original dimensions of the deformation field $(\\phi)$. Since both zero padding and iDFT are differentiable, our decoder does not require training and is able to manipulate the encoder's output through knowledge-based learning only. The resultant deformation fields are warped with $I_m$ through spatial transform to produce the aligned moving brain $I_w$."}, {"title": "Loss functions", "content": "$I_w = I_m$\nThe loss function used in our model training, similar to the traditional frameworks, consists of an image similarity loss which is computed between the fixed and the aligned moving image along with a regularizer which ensures the smoothness of the generated deformation fields.\n$L(I_f,I_m, \\phi) = L_{sim} (I_f, I_m, \\phi) + \\lambda R(\\phi)$\nwhere $L_{sim}$ represents the image similarity measure and $R$ is the deformation field regularizer. $\\lambda$ is the hyper-parameter for balancing both terms."}, {"title": "Experiments", "content": "Datasets and pre-processing\nWe set up our experiments to analyze the performance of our domain-agnostic transformer-based architecture for 3D image registration. We train the existing state-of-the-art (Jia et al. [2023],Chen and Frey [2022],Balakrishnan et al. [2019]) and our model on a specific domain and test their performance on remaining, completely unseen domains. To gauge SynthMorph's performance, we utilized its publicly available brain image registration model checkpoint and conducted experiments on human and mouse brain 3D volumes.\nHuman datasets: For our experiments we use two open-sourced Human datasets - iSeg-2017 dataset (Wang et al. [2019a]) and OASIS-3 (LaMontagne et al. [2019]). iSeg-2017 dataset comprises of brain tissues from T1 and T2-weighted MRI imaging data collected from 6-month-old infant brains. The brain scans are accompanied by their segmentation which partitions the brain into 3 distinct regions - White matter (WM), Grey matter (GM), and Cerebrospinal Fluid (CSF). Since the iSeg-2017 dataset is publicly available and contains multi-domain brain imaging data, it perfectly aligns with our study. This dataset is commonly used by the community for 3D segmentation frameworks (Wang et al. [2019b], Nguyen et al. [2021],Zhuang et al. [2021]).\nAnother publicly accessible human dataset, OASIS-3, is used in our experimentations. OASIS-3 is a retrospective compilation of data collected by WUSTL Knight ADRC over the course of 30 years. The dataset contains MR sessions from normal participants along with individuals undergoing cognitive decline. The MRI imaging data spans T1-weighted, T2-weighted, FLAIR, ASL, SWI, time of flight, resting-state BOLD, and DTI domains. For our experimentation, we shortlisted T1-weighted and T2-weighted MRI scans and remained agnostic to the participants' cognitive health. OASIS-3 is also actively used for cognitive health prediction frameworks, especially for Alzheimer's disease (Bourgeat et al. [2022], Qiu et al. [2022]). Standard pre-processing of skull stripping and normalization was performed by the provider and all the brain scans were affinely registered using a common scan before being cropped to (160 \u00d7 192 \u00d7 224) dimensions.\nMouse dataset: In order to demonstrate the generalizability of our proposed framework across species, we also perform all the experiments on the DevCCF mouse imaging dataset (Kronman et al. [2023]). DevCCF is an open-sourced 3D mouse brain imaging cross-domain dataset that contains multi-modal mouse brain volumes collected at 7 different developmental timestamps. While this dataset also covers mouse embryonic days (E11.5, E13.5, E15.5, E18.5), we only focus on the Postnatal (P) days (P4, P14 and P56). All the domains present in DevCCF (LSFM, FA, DWI, ADC, MTR, and T2-weighted) are used in our experiments. We trained ours as well as baseline models on a single domain tested the performance on all the remaining unseen domains and repeated the same process for each domain.\nWe focus on inter-subject registration, which is a core and challenging problem due to spatial complexity. During training, we pair samples from different subjects within the same domain while for testing, we combine samples from different subjects and domains. All models were trained and tested on a consistent dataset. We trained our model for 500 epochs with early stopping (patience = 30). The hyper-parameters were optimized and learning rate lr = 5e-4 and $\\lambda$ = 1.0 yielded the best results. When MSE is used as the image similarity loss, $\\lambda$ is set to 0.2, yielding the best results across all models."}, {"title": "Baseline architectures", "content": "To gauge the performance of our proposed architecture, NeuReg, we compare our results with the SOTA 3D registration model, FourierNet (Jia et al."}]}