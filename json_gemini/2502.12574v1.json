{"title": "HEADINFER: Memory-Efficient LLM Inference by Head-wise Offloading", "authors": ["Cheng Luo", "Zefan Cai", "Hanshi Sun", "Jinqi Xiao", "Bo Yuan", "Wen Xiao", "Junjie Hu", "Jiawei Zhao", "Beidi Chen", "Anima Anandkumar"], "abstract": "Transformer-based large language models (LLMs) demonstrate impressive performance in long context generation. Extending the context length has disproportionately shifted the memory footprint of LLMs during inference to the key-value cache (KV cache). In this paper, we propose HEADINFER, which offloads the KV cache to CPU RAM while avoiding the need to fully store the KV cache for any transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise offloading strategy, maintaining only selective attention heads' KV cache on the GPU while computing attention output dynamically. Through roofline analysis, we demonstrate that HEADINFER maintains computational efficiency while significantly reducing memory footprint. We evaluate HEADINFER on the Llama-3-8B model with a 1-million-token sequence, reducing the GPU memory footprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage from 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline inference. Notably, HEADINFER enables 4-million-token inference with an 8B model on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without approximation methods.", "sections": [{"title": "1. Introduction", "content": "Modern Large Language Models (LLMs) increasingly support extremely long inputs: Llama-3 (Dubey et al., 2024) handles up to 128K tokens, Claude (Anthropic, 2024) supports up to 1 million tokens, while Gradient AI (Pekelis et al., 2024) extends Llama-3 to 4 million tokens. These extended context lengths improve performance on tasks such as book summarization (Pal et al., 2023) and video generation (Liu et al., 2024b), requiring millions of tokens."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Generative Inference and KV Caching", "content": "Generative LLM inference typically involves two main stages: the prefill stage and the decoding stage. In the prefill stage, the model processes the initial input prompt by computing attention scores for all tokens in the input sequence. For long-context input, it is common to adopt chunked prefill (Agrawal et al., 2024), which divides the prompt into fixed-length chunks to incrementally build the KV cache. This strategy significantly reduces peak memory usage by lowering linear layers' peak intermediate activation size from the entire sequence to just the smaller chunk. In the subsequent decoding stage, each newly generated token from the prefill stage is fed back into the model, creating an autoregressive generation process. The LLM produces one new token at a time, and each token attends to all previous KV cache."}, {"title": "2.2. Lossy KV Cache Management", "content": "Evit KV cache can reduce memory usage and computational complexity. One direction is to identify and retain only the most 'valuable' tokens within the KV cache. Representative methods include Sliding Window Attention (Beltagy et al., 2020), Heavy Hitter (Zhang et al., 2023), and StreamingLLM (Xiao et al., 2024b). Another direction is to identify and retain the attention heads. Wu et al. (Wu et al., 2024) find a way to evaluate the importance of attention heads. Head-wise sparsity such as duo-attention (Xiao et al., 2024a), HeadKV (Fu et al., 2024), and Razorattention (Tang et al., 2024) start to divide up KV cache budgets based on the importance of each head, which is usually determined by the need for retrieval or reasoning. Minference (Jiang et al., 2024a) takes this idea further by applying distinct sparse patterns to different heads."}, {"title": "2.3. Offloading KV Cache", "content": "Offloading the KV cache from the GPU memory to CPU DRAM is another memory-efficient strategy. For instance, LayerKV (Xiong et al., 2024) implements efficient layer-wise KV offloading and overlapping data transfers to improve the context length. FastDecode (He & Zhai, 2024) and NEO (Jiang et al., 2024b) also offload parts of the KV cache to the CPU and perform attention computations on the CPU. ShadowKV (Sun et al., 2024) combines SVD decomposition with offloading to reduce communication overhead. FlexInfer (Xu et al., 2024) introduces the vTensor abstraction to better manage heterogeneous memory resources. Infinigen (Lee et al., 2024) introduces dynamic KV cache management with offloading systems."}, {"title": "3. HEADINFER: Head-wise Offload", "content": ""}, {"title": "3.1. Background:", "content": "Regular Inference with KV cache Generation. At each time step t, the input token $x_t$ is transformed into an embedding vector $E(x_t)$ by embedding the token. Then, a linear projection generates the key $K_t$ and the value $V_t$, which can be written as follows:\n$K_t = W_KE(x_t), V_t = W_vE(x_t)$  (1)\nHere $W_K$ and $V_e$ are projection weights, and the current $K_t$ and $V_t$ are appended to the existing key cache $K_{cache}$ and value cache $V_{cache}$.\n$K_{cache} = [K_1, K_2, ..., K_t], V_{cache} = [V_1, V_2, ..., V_t]$ (2)\nThe state of the cache can be memory-intensive. Together with sequence length $S$ and hidden length $D$, $K_t, V_t \\in \\mathbb{R}^{S \\times H}$. This takes $2 \\times S \\times D$ memory."}, {"title": "Attention with KV Cache.", "content": "During the computation of self-attention at time step t, the model utilizes the entire sequence of keys and values from time steps 1 to t (stored in the KV cache) alongside the new query $Q_t$:\n$A_t = Softmax(\\frac{Q_tK^T_{cache}}{\\sqrt{d_k}}) V_{cache}$ (3)\nwhere $d_k$ is the dimensionality of the keys."}, {"title": "Offload KV Cache.", "content": "When the context length S grows on a million scale, or the GPU's on-device High-Bandwidth Memory (HBM, or $M_{HBM}$) is small on the consumer GPU, it may become insufficient to store the entire key-value cache. In such scenarios, KV cache offloading provides a practical approach to handling memory limitations. Offloading can temporarily move parts of the KV cache to CPU RAM or other external resources (e.g., NVMe storage and CPU disk). However, each offloading strategy introduces potential communication overheads that must be carefully weighed against the gains in usable context length.\nFor a batch of size B, a transformer with L layers, and the KV cache with the bytes per element sizeof(datatype), the total KV cache size is:\n$Size_{KVcache} = 2 \\times B \\times L \\times S \\times D \\times sizeof(datatype)$ (4)\nIf $Size_{KVcache} > M_{HBM}$, the system can offload some portion of the KV cache to external memory to avoid out-of-memory errors. Let $\\alpha(0 < \\alpha < 1)$ be the fraction of the KV cache that remains on the GPU, and 1 - $\\alpha$ be the fraction offloaded to external memory. The memory footprint on the GPU $Size_{on-GPU}$ can be expressed as follows:\n$Size_{on-GPU} = \\alpha \\times Size_{KVcache}$ (5)\nTherefore, we require:\n$Size_{on-GPU} \\leq M_{HBM}, \\alpha \\leq \\frac{M_{HBM}}{Size_{KVcache}}$ (6)"}, {"title": "3.2. Head-wise Offload (HEADINFER)", "content": "Head-wise KV Cache Generation. In transformer architectures, each self-attention layer is split into multiple attention heads H. Each head has its own set of key-value (KV) tensors:\n$K^{(h)} \\in \\mathbb{R}^{S \\times D_h}, V^{(h)} \\in \\mathbb{R}^{S \\times D_h}$ (7)\nwhere D is divided by H so that each head handles a sub-dimension $D_h = D/H$. Therefore, instead of a single large KV cache, a head-wise approach organizes cache in H separate memory spaces. Formally, at time step t, we have:\n$K^{(h)}_{Cache} = [K^{(h)}_1, ..., K^{(h)}_t], V^{(h)}_{cache} = [V^{(h)}_1, ..., V^{(h)}_t]$ (8)"}, {"title": "Attention", "content": "As a result, each head stores its keys and values in a contiguous memory space, enabling selective offloading of certain heads' cache when memory constraints emerge.\nDuring self-attention at the time step t, we calculate the attention output for each head h independently, using:\n$A^{(h)}_t = Softmax(\\frac{Q^{(h)}_t K^T_{Cache(h)}}{\\sqrt{d_k}}) V^{(h)}_{Cache}$ (9)\nFinally, the outputs $A^{(h)}_{h=1...H}$ are concatenated to form the final output of attention for that layer."}, {"title": "Head-wise Offload.", "content": "Since the attention computation has a head-wise independence, if we can keep the KV cache of a single head rather than the entire layer, then the memory consumption can be reduced substantially. This leads to our proposed head-wise offload (HEADINFER) strategy.\nHEADINFER is designed to minimize the fraction of on-GPU data $\\alpha$ (the fraction of the total KV cache stored on GPU). Using Llama-3-8b as an example, we define $H_{all}$ as the total number of attention heads of a given model, which equals the number of heads per layer times the number of layers $H \\times L$. $H_{on}$ is the number of heads h retained on the GPU, and $H_{off}$ is the number of heads offloaded to external memory (CPU); obviously, we have $H_{on} + H_{off} = H_{all}$ \nDefine $\\alpha$ as the fraction of the KV cache that remains on the GPU. We can store all KV cache on the GPU if $\\alpha = 1$ or layer-wise offload if $\\alpha = 1/L$. In our head-wise scheme:\n$\\alpha = \\frac{H_{on}}{H_{all}} = \\frac{1}{L \\times H}$ (10)\nHere we keep only a single head on the GPU, and the fraction of the total KV cache that occupies GPU memory is reduced by $\\alpha = 1/(L \\times H)$, with a total size of:\n$S_{on-GPU} = 2 \\times B \\times S \\times D_H \\times sizeof(datatype)$ (11)\nBy reducing $\\alpha$, we can preserve GPU memory capacity for extended context inference and make the million-level inferences on consumer GPUs possible."}, {"title": "3.3. Granularity: Sequences, Layers, and Heads", "content": "HEADINFER differs from traditional offload in terms of granularity. When deploying large language models, each dimension of the model can become a potential bottleneck in GPU memory. Naively offloading the entire KV cache or entire layers can be too coarse, leading to suboptimal memory usage.\nAs shown in Figure 3, HeadInfer addresses this challenge by introducing a hierarchical set of techniques, including chunked-prefill, layer-wise offload, and head-wise offload that operate at increasingly fine-grained levels of sequence (S), layers (L), and heads (H).\n* Chunked-Prefill (S): Rather than processing all sequences of input tokens at once, HEADINFER divides the sequence into smaller chunks, each processed separately during the prefill stage. This partition helps reduce the activation memory usage.\n* Layer-Wise Offload (L): Instead of storing the entire KV cache on the GPU, HEADINFER offloads it to the GPU and fetches the KV cache of specific layers on demand. Consequently, only the relevant portion of the KV cache resides on the GPU at any given time.\n* Head-Wise Offload (H): Within each layer, HEADINFER can selectively offload the KV cache for all attention heads, fetching certain attention heads on demand. This offers the finest granularity by focusing on a subset of heads within each layer.\nBy combining these techniques, HEADINFER allows precise control over which parts of the activation and KV cache remain on the GPU. Because these dimensions nest naturally as chunks are subsets of the sequence, layers are subsets of the model, and heads are subsets of the attention layer; HEADINFER can flexibly adapt to a wide range of hardware constraints, such as available GPU memory, while meeting performance requirements."}, {"title": "4. HEADINFER for Memory-Efficient Inference", "content": "For the offloading strategy, the potential communication overheads must be carefully considered against the gains in context length extension. This is because the offloading communication cost grows along with the context length. Fortunately, for this, HEADINFER can overlap the communication cost with evening growing attention computation. This is achieved by a ping-pong memory design (Figure 4) for asynchronous offloading and prefetch. We present the implementation on algorithm 1."}, {"title": "Overlapping Head-wise KV Cache Movement and GPU Compute.", "content": "The overlapping mechanism is critical for offload performance, especially for long-context inference when the computation time can completely overlap the offload communication. While the GPU computes attention for the current head $A^{(h)}$, it concurrently prefetches the next head's KV cache from the CPU and offloads the current head's cache back to the CPU. Ping-pong memory enables non-blocking PCIe transfers to synchronize memory movement with computation. This mechanism is presented in Algorithm 1 lines 10-11 and 25-26, as async prefetch and async update."}, {"title": "Efficient Management of KV Cache.", "content": "Effective KV cache management guarantees the feasibility of long context head-wise offload. Key strategies here include head-wise partitioning and pre-allocation. Head-wise partitioning makes sure that each head has its own KV cache $K^{(h)}, V^{(h)}$, and allows selective offloading and retention based on memory availability and head importance. HEADINFER pre-allocates the CPU's KV cache memory and the GPU's ping-pong memory to avoid runtime memory allocation overheads."}, {"title": "Adaptive Head-wise Offloading.", "content": "Adaptive head-wise offloading reduces kernel launch overheads caused by head-wise partitioning, especially for small context sizes. This involves fusing multiple attention heads into a single kernel, reducing the number of kernel launches at the cost of"}, {"title": "5. Analysis", "content": "Although KV cache offload is proposed to reduce memory usage, it remains an open question whether offloading harms overall performance, especially when the context length S is large and works with chunked-prefill. This section analyzes the theoretical peak performance for a given GPU under constrained high-bandwidth memory (HBM) and peripheral component interconnect express (PCIe).\nPerformance Model. We consider a GPU characterized by HBM capacity $M_{HBM}$, memory bandwidth $B_{mem}$, and compute throughput $F_{GPU}$ (measured in FLOPS). We also incorporate the slower PCIe bandwidth $B_{pcie}$ into the performance model to account for the offload.\nMemory Bound vs. Compute Bound. GPU operators can be classified as compute bound or memory bound, which is determined by the time spent in arithmetic operations and the time spent accessing HBM. Two primary bottlenecks define the system's performance regime:\n* Memory-Bound: When $B_{mem}$ (memory bandwidth) is insufficient to transfer the KV cache quickly enough, inference operates below the GPU's peak FLOPS capacity.\n* Compute-Bound: When $B_{mem}$ is sufficient, high compute efficiency is achieved and the throughput is determined by the GPU's peak computation rate $F_{GPU}$.\nPeak Performance. Assume, for simplicity, that we can approximate the per-token inference time T as follows:\n$T \\approx max(T_{comp}, T_{mem})$ (12)\nDuring the prefill stage, $T_{comp} \\approx \\frac{D \\times S^2 \\times L \\times H}{F_{GPU}}$ captures the compute part, and $T_{mem} \\approx \\frac{D \\times S \\times L \\times H}{B_{mem}}$ captures the memory part. When scaling S, $T_{comp}$ grows faster than $T_{mem}$, associated with the quadratic growth in sequence length, which makes compute throughput the limiting factor."}, {"title": "6. Performance Evaluation", "content": ""}, {"title": "6.1. Experimental Setup", "content": "We conduct our experiments on RTX-4090 GPUs, which are configured with 24.5 GB of HBM3, 4\u00d7 AMD EPYC 7B13 with 64 CPUs each (a total of 256 cores), and 2x Gen4 NVMe of 512 GB each and 1 TB of DDR5 RAM in total. The GPUs are independently connected to the host with 16 PCIe 4.0 interfaces, providing 25.0 GB/s unidirectional D2H and H2D throughput for pinned host memory."}, {"title": "6.2. Long-Context Benchmarks", "content": "We evaluate HEADINFER using the LongBench v2 (Bai et al., 2024) and SCBench (Li et al., 2024) benchmarks, and other long-context benchmark results such as Needle-in-a-Haystack (NIAH) (Kamradt, 2023) and Ruler (Hsieh et al., 2024) are shown in the Appendix A.2. We use the Llama-3-8B-Instruct-Gradient-1024k model, which supports up to 1 million context lengths. All lossless methods, including standard inference, chunked prefill, layer-wise offload, and HEADINFER, are running on a single RTX-4090 GPU, with a maximal context length achieved within 24 GB. The long-context benchmark results aim to reveal the performance gain with HEADINFER's context length extensions.\nLongBench v2 is a comprehensive suite of long-context datasets designed to assess long-context problems that require deep understanding and reasoning. It comprises 503 difficult multiple-choice questions within two difficulties, including \"Easy/Hard\" and word lengths, including \"Short\" of 0-32k words, \"Medium\" of 32k-128k words, and \"Long\" of 128k-2M words."}, {"title": "6.3. Efficiency Results of Memory and Throughput", "content": "We evaluate the maximum context length supported under RTX-4090 memory constraints, as well as prefill/decoding throughput. Our experiments use the Llama-3-8B, Llama-3-70B, Mistral (Jiang et al., 2023), Qwen (Bai et al., 2023a), and Gemma-2 (Team et al., 2024) models. The default number format for weights and activations is BFloat16, and 4-bit KV-quant is deployed with KIVI (Liu et al., 2024c). The chunk size is set to 10K based on our roofline analysis.\nLLM Inference on Consumer GPUs with 24G memory. We measure the GPU memory consumption of Llama3-8B inference with HEADINFER and 1 million context length. HEADINFER uses 17GB during prefill and 16.4GB during decoding; in contrast, other methods are unable to run at this context scale using 24GB RTX-4090. Accordingly, we measure the maximum achievable context length to assess memory efficiency. As shown in Table 3, HeadINFER outperforms other system optimization approaches, scaling from thousand-level contexts (10K-70K) to million-level contexts (1,024K\u20134,200K). Compared to layer-wise offload, HEADINFER can maintain stable activation memory with chunked prefill and minimize KV cache GPU memory with head-wise offload. Note that with HEADINFER, the maximum context lengths for Llama3, Llama2, and Mistral are constrained by CPU RAM (512GB for KV cache), while the other methods are limited by GPU memory. This means we can use larger CPU RAM or offload to disk for a more extended context. We leave this exploration for future work."}, {"title": "7. Ablation Study", "content": "How do different levels of granularity affect memory? To evaluate the impact of different levels of granularity on memory, we conducted a detailed ablation study using the Llama-3-8B model. From the memory perspective, three"}, {"title": "8. Conclusion", "content": "In this paper, we introduced HEADINFER, a novel head-wise KV cache management framework designed to enable efficient long-context large language model (LLM) inference on consumer GPUs. HEADINFER dynamically offloads KV cache components to CPU memory, employing head-wise and asynchronous offloading strategies. Our Roofline analysis highlights HEADINFER's ability to preserve GPU computational efficiency while reducing memory footprint, making long-context inference feasible on consumer-grade GPUs. Our evaluations demonstrated HEADINFER's ability to achieve over 1-4 million context tokens on a single consumer GPU with mathematical equivalency.\nWe hope that our work will inspire future research in memory-efficient inference from the perspective of head-wise offload. We believe that HEADINFER will be a valuable tool for the community, enabling the inference of long context on consumer-grade hardware with limited resources."}, {"title": "Impact Statement: Democratizing Access to Advanced AI", "content": "Artificial Intelligence (AI) has the potential to transform industries, revolutionize education, and empower individuals. However, the deployment of cutting-edge models, particularly large language models (LLMs), is often hindered by significant resource requirements, creating barriers to entry for smaller organizations and underserved communities.\nIn this work, we introduce HEADINFER, a memory-efficient inference framework designed to bridge this gap. By leveraging head-wise offloading strategies, HEADINFER enables resource-constrained devices to process unprecedentedly long contexts, achieving capabilities typically reserved for high-performance systems. For instance, HEADINFER allows a consumer-grade GPU to handle over 1 million context tokens, democratizing access to advanced LLM functionalities.\nUltimately, HEADINFER aligns with the broader vision of democratizing AI, ensuring that technological advancements benefit humanity as a whole rather than a select few. Through innovations like HEADINFER, we hope to inspire a future where AI is universally accessible, fostering creativity, knowledge sharing, and inclusive progress."}, {"title": "A. Experiment Details", "content": ""}, {"title": "A.1. Prefill and Decoding Overhead of HEADINFER", "content": "We evaluate the computational overhead of HEADINFER compared to baseline approaches across different context lengths using the Llama-3-8B model. Our analysis focuses on two key phases: prefill and decoding."}, {"title": "A.2. Long-Context Benchmarks Details", "content": "Needle-in-a-Haystack (Kamradt, 2023) is a challenging pressure test designed to assess the ability of models to accurately identify and retrieve relevant information from a lengthy context. HEADINFER can accurately recall the information queried within the context in question.\nRuler (Hsieh et al., 2024). Designed for a comprehensive evaluation of long context, the Ruler benchmark is a recent synthetic benchmark suite that includes 13 complex tasks in four main categories. Each context length variation includes 2,600 examples, with tests conducted at 4K, 8K, 16K, 32K, 64K, and 128K tokens. The benchmark comprises four key categories. The retrieval category includes various Needle-in-a-Haystack tasks: Single (S-NIAH) for finding individual key-value pairs in noisy text, Multi-keys (MK-NIAH) for retrieving specific values among hard distractors, Multi-values (MV-NIAH) for finding all values linked to a single key, and Multi-queries (MQ-NIAH) for retrieving values across multiple keys. The Multi-hop Tracing category features Variable Tracking (VT), requiring models to trace and return all variable names pointing to the same value through variable bindings. For aggregation, the benchmark includes Common Words Extraction (CWE) for identifying top-K common words from mixed sets and Frequent Words Extraction (FWE) for finding the most frequent words from a Zeta distribution. The Question Answering category extends traditional QA datasets by adding distracting paragraphs, testing models' ability to locate and utilize relevant information amid distractors."}, {"title": "A.3. Memory Analysis of Llama-3-8B Inference", "content": "In this work, we compare several inference configurations, each targeting different trade-offs between memory efficiency and inference speed. While Figure 1 gives the theoretical benefit of HEADINFER compared to other methods in terms of memory usage, we also give how the theoretical memory is calculated here. Table 10 provides a comparative breakdown of memory usage across different inference strategies. These methods balance GPU memory consumption for weights, KV-cache, and activations while addressing the challenges of scaling to long-context inputs. Each strategy is outlined below:\n* Standard: The standard inference method keeps all weights, KV-cache, and activations entirely in GPU memory. With a context length of S, the KV-cache scales as S \u00d7 L \u00d7 Dh \u00d7 H, where L is the number of layers, Dh is the hidden dimension per head, and H is the number of attention heads. Activations require additional memory of S \u00d7 D+2\u00d7S \u00d7 I, where I is the intermediate MLP dimension. While this approach achieves baseline performance, it is constrained by GPU memory limits (e.g., 128 GB for the KV-cache and 207GB for toal).\n* Chunked-Prefill: By dividing the input into smaller chunks of size chunk, this method reduces activation memory from S to chunk. The memory footprint for activations is chunk\u00d7D+2\u00d7chunk\u00d7I, where only part of the sequence resides in GPU memory during processing. Although the KV-cache size remains S \u00d7 L \u00d7 D \u00d7 H, this technique significantly lowers activation memory requirements, reduce total memory from 207GB to 143GB.\n* 4-bit KV-Quant: This method compresses the KV-cache from fp16/bf16 (16-bit floating point) to a 4-bit representation, reducing its size by a factor of 4. The memory usage becomes S\u00d7L\u00d7 Dh\u00d7H/4, while activations remain S\u00d7D+2\u00d7S\u00d7I."}, {"title": "B. Motivation and Additional Related Work", "content": "In this section, we first explain that the KV cache size becomes a critical issue for long-text generation in LLM inference, and it becomes more problematic when deploying modern offloading-based inference systems. We then discuss why the existing KV cache management methods cannot fundamentally address the problem in an offloading-based system."}, {"title": "B.1. Memory Requirements for Inference", "content": "This section characterizes the memory requirements for transformer inference. It can be categorized into two components: i) Model Parameters and ii) Activation memory primarily referring to KV cache. The memory requirement for model parameters primarily depends on the hidden dimension D, the number of attention heads, and the number of Transformer layers L. Nearly all the parameters in a Transformer block come from linear layers within L attention blocks, L multilayer perceptron (MLP) blocks, and one language modeling head (LM-head) block. Take Llama-3-8B as an example; the total parameters in a transformer-based model can be approximated as 14 \u00d7 L \u00d7 HD\u00b2 with 15GB of memory. The memory required for activation memory primarily consists of the KV cache, which depends on the model architecture, batch size B, and sequence length S, and it can be pretty significant. The memory can be estimated as 2 \u00d7 B \u00d7 S \u00d7 H \u00d7 D. For instance, in the Llama-3-8B (Dubey et al., 2024) model architecture, serving with FP16 KV cache for 1 million tokens would require at least 207 GB of memory-exceeding the capacity of a single 80GB GPU."}, {"title": "B.2. KV Cache in LLM Inference Systems", "content": "As discussed in the previous section, today's LLM serving systems exploit KV caching to avoid redundant computation of key and value projections during the chunked-prefill decoding stage. While this is an effective solution for short sequence generation with a single client request, the KV cache quickly becomes a key memory consumer when we generate long sequences or employ modern request batching techniques (Sheng et al., 2023).\nIn the Llama-3-8B (Dubey et al., 2024) model architecture, serving with FP16 KV cache for 1 million tokens would require at least 256 GB of memory-exceeding the capacity of a single 80GB GPU. Additionally, the latencies of pre-filling and decoding with such large contexts are significant, posing substantial challenges to the effective use of LLMs in long-context scenarios.\nThe rapidly expanding KV cache leads to an urgent need and numerous efforts for KV cache compression, particularly in scenarios with limited GPU memory. Architectural modifications, such as Grouped-Query Attention (Ainslie et al., 2023), Pose (Zhu et al., 2023), Rope (Su et al., 2024), PI (Chen et al., 2023), LongNet (Ding et al., 2023), MST (Luo et al., 2024), LOQT (Loeschcke et al., 2024), Lora (Hu et al., 2021) and Galore (Zhao et al., 2024) require expensive model pre-training. One direction is non-Transformer architecture design, such as Mamba (Gu & Dao, 2023), Linear Attention (Katharopoulos et al., 2020), RWKV (Peng et al., 2023), Griffin (De et al., 2024). However, the transformer is still the most widely used model structure, and in this paper, we focus on KV cache reduction for typical transformers. KV cache token-drop methods, such as H2O (Zhang et al., 2023), StreamingLLM (Xiao et al., 2024b), InfiniGen (Lee et al., 2024), ClusterKV (Liu et al., 2024a) often compromise accuracy in long-context applications and are incompatible with essential KV cache optimization"}, {"title": "B.3. Challenges in KV Cache Management", "content": "In this context, several recent works propose reducing the KV cache size through retrieval head evictions. However, all the prior works assume the persistence of attention patterns across layers, that is, if a head is deemed a retrieval head.\nKV token eviction affects long-context performance. Figure 7 shows significant performance degradation since the actual information required by the query might be discarded if considered unimportant, which uses the KV cache of all prior tokens for computing attention results, and the KV cache management method of H2O with a KV cache budget of 2000 tokens. H2O (Zhang et al., 2023) is a state-of-the-art technique that retains only a small percentage of important tokens in the KV cache to reduce its size. It assesses the importance of each token in every iteration and removes unimportant ones before the next iteration to keep the KV cache size in check.\nThe figure indicates that this is not the practice case, despite H2O-like approaches assuming that the attention pattern does"}, {"title": "C. Roofline Model for head-wise flash attention", "content": "The Roofline model (Williams et al., 2009) serves as an effective theoretical framework to assess the potential performance of deploying a model on particular hardware. Here we evaluate hardware performance of memory access and processing unit capabilities."}, {"title": "D. Extension: HEADINFER Implementation with Head-wise Sparsity", "content": "HEADINFER, which enables offloading the head-wise KV cache with head-wise sparsity. The key design principle behind HEADINFER is to exploit the redundancy of CPU memory capacity to increase the context size after identifying the important heads in the KV cache. As such, most of the heads for the KV cache are kept in the CPU memory as we generate new tokens, not discarding them like previous work. However, we do not"}, {"title": "D.1. Design Principles", "content": "Interleaved KV cache Updates Across GPU and CPU. The uneven memory consumption and low PCIe link utilization (studied in section 3) during different attention head generation provide an opportunity to exploit the idle GPU memory and PCIe link during the KV cache update phase. To exploit this opportunity, during the attention processing, a head of the attention KV cache can be dynamically fetched on the GPU to compute the attention weight output in parallel while the CPU prefetches the next head. A key requirement to generate the attention output for a given attention head is to stage its parameters (p), query (q), key (k), and value (v), and the attention weight generation is scheduled for each head. In case the key (k) and value (v) of the head are not present on the GPU, the generation operation will trigger a prefetch read from the CPU memory where the head is offloaded, causing I/O operations in the critical execution path of updates. By leveraging the fact that multiple head attention, such as MHA (Vaswani et al., 2017) and GQA (Ainslie et al., 2023), are"}, {"title": "D.2. Extension Experiment with Head-wise Sparsity", "content": "We evaluate our head-wise extension of HEADINFER using 50% sparsity, which means half of the attention heads are sparse, on Llama3-8B models and compare against HEADINFER. For the prefill and decoding latency, the extension achieves close to 2x speedup. The results of this evaluation are shown in Table 14."}, {"title": "E. Easy-to-use and Portable Implementation", "content": "HEADINFER is designed for straightforward integration with existing inference or training frameworks, such as Hugging Face Transformers, requiring only minimal modifications. Below, we illustrate how one can adapt a standard Llama attention module to enable head-wise KV offloading with minimal impact on the rest"}]}