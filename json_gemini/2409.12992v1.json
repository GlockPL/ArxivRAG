{"title": "DiffEditor: Enhancing Speech Editing with Semantic Enrichment and Acoustic Consistency", "authors": ["Yang Chen", "Yuhang Jia", "Shiwan Zhao", "Ziyue Jiang", "Haoran Li", "Jiarong Kang", "Yong Qin"], "abstract": "As text-based speech editing becomes increasingly prevalent, the demand for unrestricted free-text editing continues to grow. However, existing speech editing techniques encounter significant challenges, particularly in maintaining intelligibility and acoustic consistency when dealing with out-of-domain (OOD) text. In this paper, we introduce DiffEditor, a novel speech editing model designed to enhance performance in OOD text scenarios through semantic enrichment and acoustic consistency. To improve the intelligibility of the edited speech, we enrich the semantic information of phoneme embeddings by integrating word embeddings extracted from a pretrained language model. Furthermore, we emphasize that inter-frame smoothing properties are critical for modeling acoustic consistency, and thus we propose a first-order loss function to promote smoother transitions at editing boundaries and enhance the overall fluency of the edited speech. Experimental results demonstrate that our model achieves state-of-the-art performance in both in-domain and OOD text scenarios. The code and demo are available at https://github.com/NKU-HLT/DiffEditor.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of digital media and the rapid expansion of internet technologies have significantly transformed how we create and share content. Speech editing, a technique that modifies audio by altering its corresponding text, has become increasingly relevant in this context. Speech editing [1]\u2013[5] enables users to edit the transcript rather than the audio itself, streamlining the process of generating and refining spoken content. This approach has become particularly valuable in diverse applications such as video production for social media, online course development, and film dubbing.\nSpeech editing models primarily focus on enhancing both the intelligibility of the synthesized speech and the acoustic consistency of the output. Many studies have been dedicated to improving these two metrics. For example, A3T [6] proposes an alignment-aware approach that reconstructs masked acoustic signals during training by using text inputs and acoustic-text alignments, significantly improving the intelligibility through correct phoneme duration. Moreover, FluentSpeech [7] employs a context-aware diffusion model [8]\u2013[10] and a pretrained MFA model [11] to further improve the alignment and adds a variation adaptor [12] to model the pitch, thereby remarkably enhancing both the intelligibility and acoustic consistency. Based on this, FluentEditor [13] incorporates one loss based on GST [14] and another loss that only considers the two adjacent frames at the edited boundaries to enhance acoustic consistency.\nHowever, existing methods predominantly follow in-domain text testing and training paradigms, focusing on continuous spectrograms derived from high-quality datasets [15], [16]. These datasets feature standardized text corpora, which limit their applicability to real-world scenarios where users often provide more diverse and flexible inputs, thus frequently resulting in speech outputs of OOD text with unnatural prosody and incorrect pronunciation [17].\nTo address the OOD text problem in speech editing, we propose DiffEditor, a novel approach consisting of two key components. The first component enhances the model\u2019s intelligibility for OOD text by incorporating prior knowledge with rich text coverage, mitigating issues such as unclear pronunciation and errors [17]. The second component optimizes both the editing boundaries and overall fluency by leveraging inter-frame smoothing properties, ensuring acoustic consistency throughout the entire utterance.\nBuilding on this foundation, we enhance intelligibility by integrating word embeddings [18] derived from a pretrained BERT [19] model. These embeddings, trained on extensive text corpora, effectively capture the semantic and contextual nuances of OOD text, thereby improving the intelligibility of the edited speech. Additionally, we recognize that inter-frame smoothing properties play a vital role in achieving acoustic consistency. To this end, we introduce a first-order difference loss function during model training. The unique characteristics of the first-order difference enable our DiffEditor to focus not only on the transitions between the reserved and generated audio at the boundaries but also on the pattern of inter-frame changes within the edited speech, ultimately enhancing the overall acoustic consistency of the entire utterance.\nOur subjective and objective experiments indicate that DiffEditor achieves state-of-the-art performance in both in domain and OOD text scenarios."}, {"title": "II. METHODOLOGY", "content": "We will first provide an overview of the DiffEditor workflow, followed by a detailed discussion of the specific implementations of both semantic enrichment and acoustic consistency."}, {"title": "A. Overall Workflow", "content": "As shown in 1, DiffEditor utilizes the Conditional Diffusion Model, which includes the condition generation module and the Spectrogram Denoiser module.\nAssuming the edited input text is represented as $X = (X_1, X_2, X_3, ..., X_{|x|}),$ it is processed through the G2P [20] module to obtain the phoneme sequence denoted as $P = (P_1, P_2, ..., P_{|p|})$. After passing through the phoneme encoder, phoneme embeddings $e_{phoneme}$ are generated. Meanwhile, the text $X$ is processed through the BERT model to obtain word embeddings $e_{word}$. These word embeddings are subsequently upsampled to align with the phoneme embeddings. Then, $e_{word}$ is concatenated with the $e_{phoneme}$ to produce $C_{hybrid}$.\nThe hybrid embedding $C_{hybrid}$ are upsampled to frame-level features by utilizing a length regulator, which leverages the temporal information produced by the masked duration predictor. These frame-level features, along with additional components such as pitch $e_{pitch}$, speaker embedding $e_{speaker}$, and the masked mel-spectrogram embeddings $e_{masked\\_mel}$, are combined with the hybrid embedding to form the condition $C$. The generation of other embedding follows the configurations established in [7].\nThe ground truth spectrogram is represented as $Y = (Y_1, Y_2, Y_3, ..., Y_{|y|})$, while its masked counterpart is denoted as $Y_{mask} = mask(Y)$. The masking function follows the specifications outlined in [7].\nThe Spectrogram Denoiser processes the condition features $C$, which are generated from both $Y_{mask}$ and the input $X$ and the forwards-processed spectrogram $Y$ as inputs to produce $\\hat{Y}$, where $\\hat{Y}$ maintains the unmasked part of $Y_{mask}$ while reconstructing the masked portions of the spectrogram."}, {"title": "B. Semantic Enrichment", "content": "Given that word embeddings derived from BERT effectively capture the semantic and contextual nuances of OOD text, thereby enhancing the intelligibility of edited speech, we propose concatenating the BERT-generated word embeddings with phoneme embeddings to enrich the semantic information. This approach is illustrated in the Senmantic Enrichment section of 1.\nThe BERT model is a transformer-based model designed to understand the context of words in a sentence by looking at the words that come before and after them, which is a significant advancement over previous models that read text in a unidirectional manner. This training approach is very similar to the masked prediction method used in speech editing. Therefore, this model addresses the semantic information deficiency of phoneme embedding and enhances the understanding of contextual semantic information.\nFollowing the processing of text through BERT, we upsample the word embeddings to align with the structure depicted by the dotted line in the Semantic Enrichment section of Figure 1. This ensures that the upsampled word embeddings can be seamlessly concatenated with the phoneme embeddings. Moreover, the timing of concatenation between $E_{word}$ and $e_{phoneme}$ is critical; Random concatenation is insufficient. We outline three distinct concatenation strategies below, along with an analysis of their advantages and disadvantages.\nFirst, we concatenate the phoneme embeddings and word embeddings to generate hybrid embeddings and use the hybrid embeddings as the input of the masked dur predictor and the masked pitch predictor. This approch will increase the predictors' parameters due to the expansion of embedding dimension and lead to the overfitting problem during training. Second, we directly concatenate the word embeddings with the condition $c$. In this approach, all embeddings except the word embedding share the same dimension size of 192. However, global embeddings such as speaker embedding needs to function across the entire text representation, which is a concatenation of phoneme embeddings and word embeddings. Consequently, this type of model has lower performance.\nLast but not least, we choose to concatenate $e_{phoneme}$ and $e_{word}$ immediately after the $e_{phoneme}$ has passed through the masked duration predictor and masked pitch predictor. We then add the $e_{masked\\_mel}$ and $e_{speaker}$ to the $C_{hybrid}$, as illustrated in the Condition C section of Figure 1. This strategy achieves the best performance in terms of semantic enrichment because it avoids the mistakes of the former two strategies."}, {"title": "C. Acoustic Consistency", "content": "We realize that the inter-frame smooth properties play an important role in achieving acoustic consistency. To this end, we introduce the first-order difference loss during model training.\nThe first-order difference $\\Delta Y$ for the ground-truth acoustic features $Y$ is computed as:\n$\\Delta Y = Y^{(i)} - Y^{(i-1)}, \\quad \\text{for } i \\geq 2$\nwhere $i$ denotes the index of the frame. This calculation captures the rate of change between adjacent frames, particularly at the editing boundaries. To enforce temporal consistency and minimize abrupt changes in the predicted $\\hat{Y}$, we introduce the first-order difference loss $L_{FD}$. This loss compares the first-order differences of the predicted $\\hat{Y}$ with those of the ground-truth speech $Y$ and is defined as:\n$L_{FD} = MAE(\\Delta Y, \\Delta \\hat{Y})$\nHere, MAE denotes the Mean Absolute Error. This loss is crucial for editing the boundaries of synthesized speech during speech generation. By enforcing $L_{FD}$, the model ensures that transitions between edited and unedited segments are natural and fluent, while preserving the overall fluency and naturalness of the generated audio."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "We trained DiffEditor using the VCTK dataset [15] and evaluated its performance employing both the VCTK and LJspeech test sets [16]. Both the VCTK and LJspeech are high-quality English Speech datasets. The VCTK Corpus consists of speech data spoken by 109 English speakers with various accents, and each speaker reads about 400 sentences from the same newspaper. As a result, we randomly chose 10 speakers as the test set for in domain text evaluation, and the remaining 99 speakers made up the training set. According to the summary provided by the large language model, the VCTK dataset encompassed eight topics: Daily Life, Sports, Interpersonal Relationships, Work, Politics, Education, Art, and Natural Phenomena, whereas the first 1000 utterances of the LJspeech dataset mainly describe the Newgate Prison in London and the related situations. Thus, for the OOD evaluation, we utilized the first 1000 utterances from the LJspeech dataset, which belongs to a completely different text domain."}, {"title": "B. Experimental Setup", "content": "The configuration of the phoneme encoder, variance adaptor [12] (which includes the masked duration and pitch predictors), and the spectrogram denoiser are consistent with those described in [7]. The pretrained word encoder in our study is the BERT model, specifically the bert-base-multilingual-uncased\u00b9 version released by Google on Hugging Face. In our methodology, we assign a weight of 4 to the loss function associated with the first-order difference. The batch size is configured to 90, and DiffEditor is trained for 200,000 steps using a single NVIDIA 3090 GPU."}, {"title": "C. Evaluation Metrics", "content": "For subjective evaluation, we utilize Mean Opinion Score (MOS) [21] to assess speech quality, FMOS [13] to evaluate fluency, and IMOS to measure intelligibility. These metrics provide a detailed understanding of the model's improvements in these aspects. Additionally, we conduct a corresponding ablation study using CMOS (comparative mean opinion score) [21], CFMOS, and CIMOS. For objective evaluation, we employ Mel-Cepstral Distortion (MCD) [22], Short-Time Objective Intelligibility (STOI) [23], and Perceptual Evaluation of Speech Quality (PESQ) [24]."}, {"title": "D. Comparative Study", "content": "We have conducted extensive comparative experiments, which include five models. they are: 1) CampNet [2] presents a context-aware mask prediction network to mimic the process of text-based speech editing. 2) EditSpeech [1] trains two traditional autoregressive text-to-speech (TTS) models, one of them is trained in a left-to-right manner, and the other is trained in a right-to-left way. 3) A\u00b3T [6] proposes an alignment-aware approach that reconstructs masked acoustic signals during training by leveraging text inputs and acoustic-text alignments. 4) FluentSpeech [7] advances A3T [6] further with a context-aware diffusion model that iteratively refines the modified mel-spectrogram, guided by contextual features. 5) DiffEditor integrates multi-level feature reinforcement and first-order difference loss."}, {"title": "E. Main Results", "content": "Objective results: In order to objectively assess the performance of the out-of-domain (OOD) text and in domain text in the comparative study, we randomly mask a continuous span of the audio segment, which corresponds to a continuous phoneme sequence based on the alignment file generated by MFA [11]. Subsequently, we utilize the five models to generate the masked part of the audio and calculate MCD, PESQ, and STOI to fill the table I. According to the evaluation, our DiffEditor achieves state-of-the-art performance in both in domain and OOD text scenarios for all three objective metrics."}, {"title": "F. Ablation Study", "content": "To further explain the influence of each component, we have conducted ablation experiments on the two methods employed by DiffEditor, namely \"w/o LFD\" and \"w/o eword\". The \"w/o LFD\" means that the DiffEditor subtracts the first-order difference loss function. Table III indicates that the subtraction of the first-order difference loss function results in a decline in fluency and intelligibility, with the decrease in fluency being more significant than that when subtracting the word embeddings. This implies that the first-order difference enhances the overall acoustic consistency of the entire utterance. On the other hand, the subtraction of the word embeddings leads to a major decrease in intelligibility, and the decrease in intelligibility is more significant than that when subtracting"}, {"title": "G. Editing Boundaries Analysis", "content": "As depicted in Figures 2, we visualize the mel-spectrograms generated by the FluentSpeech baseline and our DiffEditor. The text for both spectrograms is edited from \u201csix other people were injure\u201d to \u201cmozart and five other individuals were injure\u201d. The red box regions in the figures represent the precise editing boundary of the sentence, which is the generated \u201cindividual\u201d. As can be observed from the Figures 2, when DiffEditor generates the boundaries, the spectrogram is more abundant and contains more detailed information, which demonstrates that the model learns the inter-frame transition pattern at the boundaries."}, {"title": "IV. CONCLUSION", "content": "In this paper, we present DiffEditor, a novel speech editing framework that achieves state-of-the-art performance across both in domain and OOD text scenarios. By integrating word embeddings from a pretrained BERT model, we effectively incorporate prior knowledge with rich text coverage, significantly enhancing the model's intelligibility. Furthermore, by employing a first-order difference loss function, we ensure acoustic consistency at the editing boundaries and throughout the entire utterance."}]}