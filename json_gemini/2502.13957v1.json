{"title": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision", "authors": ["Guangzhi Xiong", "Qiao Jin", "Xiao Wang", "Yin Fang", "Haolin Liu", "Yifan Yang", "Fangyuan Chen", "Zhixing Song", "Dengyu Wang", "Minjia Zhang", "Zhiyong Lu", "Aidong Zhang"], "abstract": "Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose Re-Search, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG. The project homepage is available at https://rag-gym.github.io/.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have revolutionized online information-seeking by providing direct responses to user needs. Such capabilities are often evaluated by knowledge-intensive tasks such as multi-hop question answering (QA), which require sequential reasoning over relevant knowledge. Pre-trained on trillions of tokens from diverse sources, LLMs have demonstrated strong reasoning capabilities and acquired considerable general knowledge. As such, they are often used as standalone systems that process user queries and generate responses without relying on external utilities. Despite these impressive capabilities, LLMs often struggle with questions where they may lack sufficient and up-to-date domain knowledge. This may lead to inaccurate responses or hallucinations, which can be particularly problematic in high-stakes fields.Retrieval-augmented generation (RAG) has emerged as a promising solution to address knowledge-intensive tasks. By grounding the LLM outputs in relevant information returned by information retrieval (IR) systems, RAG improves both accuracy and verifiability. Expanding on this paradigm, information-seeking agents such as ReAct iteratively interact with IR systems to tackle complex questions more effectively. However, most existing methods rely heavily on prompting techniques, which require substantial efforts for the prompt design and might not generalize well across different tasks.To improve such agents in solving knowledge-intensive and multi-hop QA tasks, we propose RAG-Gym, a unified framework for optimizing agentic RAG through process supervision. RAG-Gym formulates the knowledge-intensive QA as a nested Markov Decision Process (MDP), where the outer MDP action quality is annotated with process rewards, which in turn guide the inner MDP token generation. By supporting diverse agent architectures and process supervision methods, RAG-Gym provides a comprehensive platform for improving information-seeking agents. In addition to RAG-Gym, we propose ReSearch, a novel agent architecture that unifies answer reasoning and search query generation in a single action. This design explicitly aligns the generated query with missing information in answer construction. Through extensive experiments, we demonstrate that RAG-Gym effectively enhances agentic RAG performance on knowledge-intensive tasks, with ReSearch"}, {"title": "2. RAG-Gymnasium (RAG-Gym)", "content": "RAG-Gym models knowledge-intensive QA as a nested Markov Decision Process (MDP), where the outer MDP governs high-level action generation through interactions with the information retrieval (IR) environment, while the inner MDP controls token generation within LLMs. This formulation is compatible with various agentic RAG architecture that leverages IR for augmented answer generation. By assigning process rewards from high-level actions to sequential token generation, RAG-Gym enables effective tuning of language agents, which can be further utilized for optimizing agent parameters or training process verifiers."}, {"title": "2.1. Knowledge-intensive QA as Nested MDP", "content": "Figure 1 presents the architecture of our RAG-Gym framework, which formulates a knowledge-intensive question answering task as a nested Markov decision process (MDP). With LLMs as the core engine of the reasoning and search agents, the sequential token generation is the inner MDP, where each action is to generate the next token given all existing ones. Below we explain in detail how the outer MDP is constructed to generalize various language agent designs on knowledge-intensive questions.\nState Space S. At each time step t, the state $s_t \\in S$ comprises the original question Q and the information-seeking"}, {"title": "2.2. Improving Search Agents with Process Supervision", "content": "While outcome-based rewards, such as answer correctness, provide a clear optimization objective, they offer limited insight into the quality of intermediate reasoning and search steps. In RAG-Gym, a poorly formulated query may still lead to a correct answer if the language agent compensates with internal knowledge, making it difficult to assess the effectiveness of the search query. To address this, process supervision is essential. Recent advancements in process reward modeling show that guiding intermediate steps enhances both performance and robustness in language agents. Since outer MDP actions in RAG-Gym are discrete, optimizing agents solely through outcome rewards is challenging. By incorporating process rewards, RAG-Gym enables more effective LLM tuning, aligning token generation with high-quality search behavior. Section 2.2.1 details our process reward data collection pipeline, and Section 2.2.2 describes the algorithms for tuning language agents with process supervision."}, {"title": "2.2.1. COLLECTION OF PROCESS REWARD DATA", "content": "To evaluate intermediate reasoning and search steps in RAG-Gym, we design a process reward function that assesses queries based on three key criteria:\n\u2022 Sufficiency: A query must first be necessary. If the retrieval history already contains sufficient information, answering should be the preferred action instead of searching further.\n\u2022 Utility: Queries should also be precise, actionable, and foundational to solving the question while avoiding unnecessary details.\n\u2022 Redundancy: Queries should introduce new, useful information rather than repeating past searches.\nThese criteria ensure that queries are efficient, targeted, and contribute meaningfully to constructing the final answer."}, {"title": "2.2.2. TUNING AGENTS WITH PROCESS SUPERVISION", "content": "The process reward data collected serves as a key resource for improving language agents in RAG-Gym through three distinct methods: supervised fine-tuning, direct preference optimization, and process reward modeling. Each method leverages the data to address specific aspects of the training paradigm and task requirements, enabling robust optimization of reasoning and action generation.\nIn supervised fine-tuning (SFT), selected actions from the process rewards are used to train the language agent. Formally, the goal of SFT is to minimize the negative log-likelihood of the selected actions given their states:\n$L_{SFT}(\\theta) = -E_{(s_t, a) \\sim D} [log \\pi_{f(\\theta)}(a | s_t)],$ (5)\nwhere D is the dataset of process reward-labeled state-action pairs. This method provides a straightforward way to incorporate process supervision but does not explicitly account for unselected actions, potentially limiting its ability to distinguish between subtle preferences.\nDirect preference optimization (DPO) introduces a contrastive learning framework that incorporates both selected and unselected actions. The process reward data is reformulated into preference pairs $(a_t^+, a_t^-)$, where $a_t^+$ is the preferred action and $a_t^-$ is the less-preferred alternative for $s_t$. The DPO objective minimizes the following loss:\n$L_{DPO}(\\theta) = -E_{(s_t, a_t^+, a_t^-) \\sim D} [log \\sigma(\\beta (log \\frac{\\pi_{f(\\theta)}(a_t^+|s_t)}{\\pi_{ref}(a_t^+|s_t)} - log \\frac{\\pi_{f(\\theta)}(a_t^-|s_t)}{\\pi_{ref}(a_t^-|s_t)}))]$ (6)\nwhere $\\pi_{f(\\theta)}$ is the policy being optimized, $\\pi_{ref}$ is the reference policy, $\\beta$ is a temperature parameter controlling the strength of the preference weighting, and $\\sigma(\\cdot)$ is the sigmoid function. By explicitly comparing actions, DPO captures nuanced preferences and enables the agent to learn from both positive and negative feedback.\nProcess reward modeling (PRM) takes a different approach by training a separate reward model $r_{\\phi}(s_t, a_t)$ to predict process rewards based on the collected data. The objective is to minimize a contrastive loss that evaluates the quality of preferred actions relative to less-preferred actions:\n$L_{PRM}(\\phi) = -E_{(s_t, a_t^+, a_t^-) \\sim D} [log \\sigma(r_{\\phi}(s_t, a_t^+) - r_{\\phi}(s_t, a_t^-))].$ (7)\nUnlike SFT and DPO, PRM does not directly tune the policy $\\pi_{f(\\theta)}$ but instead trains the reward model $r_{\\phi}$ parameterized by $\\phi$ to estimate the quality of intermediate reasoning and actions. The reward model can then guide decision-making by selecting high-quality actions during inference, eliminating the need for agent fine-tuning. This makes PRM especially useful for large-scale or proprietary models, offering a flexible and scalable approach to improving reasoning and search. Algorithm 1 details how trained process reward models are applied during inference."}, {"title": "3. Reasoning and Search (ReSearch) Agent", "content": "Existing agent architectures, such as ReAct, exhibit inherent limitations when applied to knowledge-intensive tasks. They can only reason about either the answer generation or the query generation at each step. This may lead to queries that cannot directly contribute to answer construction, as these two tasks are not necessarily aligned during reasoning. As illustrated in Appendix E, existing designs may generate queries that fail to retrieve the most relevant evidence, leading to suboptimal information acquisition and degrading answer quality. This limitation underscores the necessity of an architecture that explicitly aligns search queries with answer construction."}, {"title": "4. Results", "content": ""}, {"title": "4.1. Experimental Settings", "content": "To evaluate how different agents perform on knowledge-intensive QA tasks and how they can benefit from various process supervision methods implemented in RAG-"}, {"title": "4.2. Comparison of Process Supervision Methods", "content": "Table 1 shows the performance of various agents and their tuned versions using different process supervision methods in RAG-Gym. Process supervision consistently improves performance across all agents compared to the zero-shot learning (ZSL) baseline, demonstrating its effectiveness in enhancing intermediate reasoning and query generation. Among the three process supervision algorithms, PRM achieves the best results overall, outperforming ZSL baselines by up to 25.6% (ReAct; Average F1).\nWhile PRM outperforms the other methods, both DPO and SFT show significant improvements over the ZSL baseline. Interestingly, SFT slightly outperforms DPO on the Direct,"}, {"title": "4.3. Comparison of ReSearch and other Agents", "content": "The results also show that ReSearch consistently outperforms other agents, both in the ZSL setting and in settings with process supervision. Without tuning, ReSearch achieves strong zero-shot performance, demonstrating the effectiveness of explicitly aligning answer reasoning with query generation. Using process reward models, ReSearch achieves state-of-the-art performance, with an average EM score of 54.31% and an average F1 score of 62.41% across different datasets. Furthermore, ReSearch exhibits superior generalization, achieving top scores on 2WikiMultihopQA and Bamboogle without task-specific fine-tuning. These results validate the ability of ReSearch's unified reasoning and search framework to effectively leverage process supervision for both task-specific improvements and broader generalization across knowledge-intensive QA tasks."}, {"title": "4.4. Reward Model Transferability", "content": "Figure 3 highlights the performance improvements of the ReSearch agent with GPT-4o-mini using Llama-3.1-8B-based process reward models. The action selection with reward models leads to consistent gains across all tasks, demonstrating the transferability of PRM to effectively select high-quality actions in different LLMs. This result also highlights the potential of using process reward models as a plug-and-play module to enhance the reasoning and search capabilities of proprietary LLMs, where direct fine-tuning is not feasible due to restrictions on model access."}, {"title": "5. Analysis and Discussion", "content": ""}, {"title": "5.1. Comparison of Different Reward Sources", "content": "To evaluate the effectiveness of different process reward sources in training reward models, we conducted experiments on MedQA and compared their alignments with domain expert preferences as well as their impact on downstream accuracy. Specifically, we examined whether using LLMs like GPT-40 for process data annotation is an effective approach and how its preferences align with human annotations. Four domain experts annotated 200 MedQA questions. A reward model was trained on the remaining 800 training questions annotated with GPT-40, and its pref-"}, {"title": "5.2. Training Time Scaling", "content": "For the evaluation of training sample size and its impacts on the performance of ReSearch agents, we conducted experiments using process reward models trained on varying"}, {"title": "5.3. Inference Time Scaling", "content": "Since process reward models optimize action-taking by identifying high-quality actions from the generated candidates during inference, we explored how the agent performance changes with the increasing number of sampled actions at each time step. Figure 5 displays the results of our inference time scaling study, with ReSearch as the tested agent. We observe a consistent trend across multiple benchmarks, where increasing the number of sampled actions generally improves performance. Specifically, for HotpotQA and Bamboogle, the F1 score continues to rise as more actions are sampled, demonstrating the benefits of expanding the candidate set to enable better action selection at each step. However, performance gains gradually diminish, indicating that the agent reaches a point where additional sampled actions contribute less to improvement. This suggests that while action sampling is beneficial, there is a limit to how much additional sampling enhances decision-making."}, {"title": "6. Related Work", "content": ""}, {"title": "6.1. Retrieval-Augmented Generation (RAG)", "content": "RAG has emerged as a powerful paradigm for enhancing language models in knowledge-intensive tasks. By integrating external knowledge sources into generative models, RAG enables the production of more accurate and informative responses. This approach has seen success across various domains, such as open-domain question answering and conversational agents. Researchers have aimed to improve RAG by optimizing retrieval components or enhancing the generative model's ability to leverage retrieved content. Applications of RAG extend to specialized fields, like biomedical question answering, showcasing its versatility. Despite these advancements, vanilla RAG architectures typically rely on a single round of retrieval, limiting their effectiveness in scenarios requiring iterative reasoning and complex information integration."}, {"title": "6.2. Multi-hop Question Answering (QA)", "content": "Multi-hop QA tasks require systems to perform reasoning across multiple pieces of information, often from disparate sources, to derive a correct answer . These tasks are inherently challenging for traditional RAG architectures due to their single retrieval pass, which restricts the ability to gather all necessary information for complex questions. Early attempts to address multi-hop QA in RAG systems include agent designs such as ReAct, which integrate reasoning processes into retrieval steps. Nevertheless, these strategies commonly rely on finely tuned heuristics or hand-crafted prompts, lacking robustness when facing diverse question formulations."}, {"title": "6.3. Process Reward Modeling (PRM)", "content": "PRM provides a structured framework for guiding LLM reasoning by incorporating intermediate feedback during the reasoning process. This approach has been shown to enhance the reasoning capabilities and accuracy of models in complex tasks. Researchers have explored various methods to implement process supervision, including constructing intermediate supervision signals through rollouts, human feedback, or LLM feedback. By shifting the focus from outcome-based rewards to"}, {"title": "7. Conclusion", "content": "We presented RAG-Gym, a framework for optimizing reasoning and search agents through process supervision, and introduced ReSearch, an agent architecture that unifies answer reasoning with search query generation. Experiments show that RAG-Gym improves search agents on knowledge-intensive tasks, with ReSearch consistently outperforming baselines. We also demonstrated the effectiveness of using LLMs as process reward judges, the transferability of trained reward models on different LLMs, and the scaling patterns of ReSearch during training and inference."}, {"title": "8. Acknowledgment", "content": "This research was supported by the Division of Intramural Research (DIR) of the National Library of Medicine (NLM), National Institutes of Health (NIH), and other grants from NIH and National Science Foundation (NSF)."}, {"title": "Impact Statement", "content": "This paper presents RAG-Gym, a framework designed to advance the field of language modeling by optimizing reasoning and search agents through process supervision. Our work contributes to improving retrieval-augmented generation (RAG) by refining how language models seek, retrieve, and reason about information. The techniques proposed in this paper enhance the capabilities of language agents to handle knowledge-intensive tasks, making them more effective across various domains.\nThere are potential societal implications of this work, including its applications in improving AI-driven decision-making systems in areas such as research, education, and healthcare. However, our research primarily focuses on the technical advancement of process supervision in search and reasoning agents, and we do not foresee any immediate ethical concerns requiring further discussion."}, {"title": "C. Baseline Descriptions", "content": "Here are the detailed descriptions of various baseline agents that we implemented in the experiments.\n\u2022 Direct: The agent directly outputs the predicted answer without reasoning at the first iteration.\n\u2022 CoT: The agent outputs both the reasoning and the predicted answer at the first iteration.\n\u2022 RAG: The agent outputs the original question as the search query at the first iteration. It reasons about the updated state and generates a predicted answer at the second iteration.\n\u2022 ReAct: The agent reasons about the given state and generates either a search action or an answer action.\n\u2022 Search-01: Before the state reasoning, the agent summarizes the retrieved documents in the information-seeking history as a direct answer to the corresponding search query. The agent uses query-answer pairs instead of query-documents pairs for the construction of LLM input as our ReSearch does. It can be considered a special version of ReAct with RAG instead of information retrieval (IR) as the tool."}, {"title": "D. Implementation Details", "content": "For the implementation of the IR environment, we select Wikipedia as the supporting corpus for the retrieval of relevant information for questions from HotpotQA, 2WikiMultihopQA, and Bamboogle. For the environment of solving MedQA questions, we use a combination of medical textbooks and StatPearls which were pre-processed by . For all tasks, we used both lexical and semantic retrievers whose results were merged with Reciprocal Rank Fusion. BM25 and BGE-Base were used for HotpotQA, 2WikiMultihopQA, and Bamboogle, while in MedQA, we selected BM25 and MedCPT.\nFor all LLM tuning in our paper, we employed the LoRA fine-tuning with r = 256 and alpha = 512 on all attention components in the transformers architecture. The process supervision methods are implemented using the TRL package. We used the instruction-tuned version of Llama-3.1-8B as the base model for implementing various agents, which also served as the base model for process reward modeling (PRM). For the tuning of Search-o1 and ReSearch agents, only the LLM for action reasoning is trained while the one for history knowledge summarization remains unturned.\nAll results of zero-short learning (ZSL), supervised fine-tuning (SFT), and direct preference optimization (DPO) are generated with a temperature of 0.0. For the evaluation of PRM, we employed a temperature of 1.0 with 10 different actions sampled for each step in the information-seeking trajectory. Algorithm 1 presents our algorithm of using the trained process reward model to guide the action selection during inference."}, {"title": "E. Case Studies", "content": ""}, {"title": "E.1. Comparison of Agent Architectures on Bamboogle", "content": "We analyze the reasoning and search behaviors of RAG, ReAct, Search-01, and ReSearch using an example from the Bamboogle dataset. As shown in Figure 6, given the question \u201cWhat was the father of the last surviving Canadian father of Confederation?\", the three agents show distinct behaviors when generating the first action.\nThe RAG agent directly passes the question as a search query without decomposition, relying entirely on retrieval to infer the answer. This often leads to ineffective searches that fail to retrieve necessary intermediate facts. ReAct and Search-01 improve upon this by engaging in stepwise query reasoning, first identifying the need to determine the last surviving Canadian father of Confederation before issuing a search query. However, the generated query, \u201cList of Canadian fathers of Confederation\u201d, retrieves broad information rather than directly resolving the missing knowledge.\nIn contrast, ReSearch explicitly integrates answer reasoning with search. It first constructs a potential answer, identifying an unverified claim that William Lyon Mackenzie King is among the last surviving Canadian fathers of Confederation. Recognizing the missing evidence, it formulates a targeted query, \u201cWho is the last surviving Canadian father of Confederation?\u201d, to resolve the uncertainty. This approach ensures that retrieval is aligned with answer construction, minimizing unnecessary\""}, {"title": "E.2. PRM-Guided Action Selection in MedQA", "content": "To illustrate how the Process Reward Modeling (PRM) improves decision-making, we examine a case from the MedQA dataset (Table 4). The model is tasked with identifying the mechanism of action of the most likely anti-diabetic medication for a 60-year-old patient presenting with symptoms indicative of fluid overload.\nFor the generation of the first action, the agent initially generates a broad and less actionable query about heart failure, which, while relevant, does not directly contribute to identifying the correct medication. Meanwhile, it also provides another candidate action with the query \"What are common classes of oral anti-diabetic medications?\", which leads to retrieving"}, {"title": "F. Prompt Templates", "content": "We provide structured prompt templates for history knowledge summarization and action generation in our proposed ReSearch agent. The template in Figure 7 ensures that retrieved documents are summarized concisely and factually for follow-up queries. Figure 8 shows the prompt template used by ReSearch to align the answer construction with query formulation. The prompt of using GPT-40 for process reward data annotation is presented in Figure 9."}]}