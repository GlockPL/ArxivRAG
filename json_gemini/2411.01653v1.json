{"title": "Diagnosing Medical Datasets with Training Dynamics", "authors": ["Laura Wenderoth"], "abstract": "This study explores the potential of using training dynamics as an automated alternative to human annotation for evaluating the quality of training data. The framework used is Data Maps, which classifies data points into categories such as easy-to-learn, hard-to-learn, and ambiguous (Swayamdipta et al., 2020). Swayamdipta et al. (2020) highlight that difficult-to-learn examples often contain errors, and ambiguous cases have a significant impact on model training. To confirm the reliability of these findings, we replicated the experiments using a challenging dataset, with a focus on medical question answering. In addition to text comprehension, this field requires the acquisition of detailed medical knowledge, which further complicates the task. A comprehensive evaluation was conducted to assess the feasibility and transferability of the Data Maps framework to the medical domain. The evaluation indicates that the framework is not suitable for addressing the unique challenges of datasets in answering medical questions.", "sections": [{"title": "Introduction", "content": "Significant advancements have been achieved in the realm of natural language processing (NLP), and language generation models have become essential tools for various daily tasks (George and George, 2023; Chang et al., 2023; Huo et al., 2023). Nevertheless, a persistent challenge is that these models often generate hallucinated or incorrect responses (Rawte et al., 2023; Dhuliawala et al., 2023; Huang et al., 2023; Huo et al., 2023). In fields such as medicine, where precision is paramount, there is little tolerance for inaccuracies in language model (LLM) results. Therefore, there are ongoing efforts to fine-tune LLMs for medical datasets and enhance them with domain-specific expertise (Lee et al., 2020; Singhal et al., 2023a,b; Luo et al., 2023).\nThe accuracy of these models is often assessed using existing multiple choice datasets such as USMLE MedQA (Pal et al., 2022). However, the accuracy of these models is heavily reliant on the quality of the training data (Liang et al., 2022; Bernhardt et al., 2022; Chklovski et al., 2023; Johnson et al., 2023). Generating and evaluating high-quality medical training data is a challenging task, as it must be done by experts, which is expensive and time-consuming. Several researchers have investigated training dynamics to evaluate data points during the training process (Xing et al., 2018; Toneva et al., 2018; Le Bras et al., 2020; Swayamdipta et al., 2020). Swayamdipta et al. (2020) propose a promising solution to evaluate datasets based on training dynamics using a framework called Data Maps. This approach divides the training instances in a dataset into easy-to-learn, hard-to-learn and ambiguous classes. It was discovered that training on only 33% of the dataset, but with examples solely from the ambiguous class, produces comparable results on the out-of-distribution (OOD) dataset as training on the entire dataset. Additionally, it has been demonstrated that data points that are hard-to-learn have a higher prevalence of mislabelled instances than their easy-to-learn counterparts. This finding suggests that it is feasible to identify valuable and accurate instances of adversarial datasets. Once created, hard-to-learn examples can be filtered out and particularly ambiguous examples can be used for further training to reduce the risk of error and increase generalisability.\nThis approach is particularly valuable in the medical field, where dataset creation requires expert collaboration, making it expensive and labour-intensive. The Data Maps framework can be a cost-effective tool for quality assessment in medical datasets, contributing to a more robust and reliable application of LLMs in medical contexts. Therefore, this paper makes the following contributions:\n\u2022 Replication: Repeating the experiment on a new dataset to demonstrate overall effectiveness.\n\u2022 Feasability Evaluation: Assessing the practicability of the Data Maps framework on datasets in general. This entails a meticulous examination of the method's adaptability and efficacy.\n\u2022 Transferability to Medical Domain: Analyse method's inherent strengths and weaknesses, particularly within the intricate domain of medical question answering."}, {"title": "Data", "content": "This study uses two multiple-choice datasets. The primary dataset used for training is the MedQA dataset, which contains a comprehensive set of medical knowledge questions. To evaluate the out-of-distribution (OOD) model's performance, we also incorporated questions from the Applied Knowledge Test of the Royal College of General Practitioners general practitioners (GP) examination in the UK. This dataset is referred to as GP-UK.\nMedQA is derived from the United States Medical Licence Exams (USMLE), providing a solid foundation based on professional medical exams (Pal et al., 2022). The dataset covers 2.4k healthcare topics and spans 21 different medical subjects. The dataset contains historical exam questions from official websites such as AIIMS and NEET PG, covering the period from 1991 to the present. An exam question comprises a single query with four potential solutions, and the most correct solution has to be found. It is divided into training (182,822 samples), validation (4,183 samples) and test data (6,150 samples). The dataset has been used for benchmarking since 2019. Figure 1 shows the ranking created by Papers with code (2024). The Med-PaLM 2 model ranks highest with an exceptional accuracy of 85.4%, followed closely by alternative configurations of the same model labelled CoT + SC (83.7%) and 5-shot (79.7%). Passing these exams typically requires a score of 60%. However, it is important to note that prior to 2022, language models such as BioLinkBERT base and large, and GPT-Neo (2.7 B) only achieved accuracies of around or less than 40%. This demonstrates the historical difficulty in obtaining accurate answers to medical questions using language models.\nGP-UK comprises sample questions from two Applied Knowledge Tests created by the Royal College of General Practitioners (2019, 2023). Questions accompanied by pictures were excluded, resulting in a total of 59 questions. Furthermore, the answer options for the questions in the GP-UK dataset were restricted to four, which aligns with the standard multiple-choice format of medical exams in the US. This curated dataset serves as an OOD dataset for our investigation of the effectiveness and generalisability of the proposed methodology in the context of answering medical questions."}, {"title": "Mapping Datasets with Training Dynamics", "content": "In the field of dataset evaluation, several approaches have been explored, each offering unique perspectives on evaluating the quality and relevance of data. One possibility is to look at various parameters that change during training. For example, Toneva et al. (2018) analyse the training dynamics, focusing in particular on instances that are often misclassified in later training epochs, even though they were previously classified correctly.\nWe are examining a comparable technique developed by Swayamdipta et al. (2020), known as Data Maps. The main aim of Data Maps is to visually represent how a model interacts with a dataset over time, allowing for a detailed understanding of the contributions made by individual instances to the model's learning process. To achieve this, three measures are used: confidence, variability, and correctness. The metrics are calculated by making a full pass over the training set after each epoch, without gradient updates, and saving the probabilities for the true label of each sample. These measures capture key aspects of the model's behaviour during training, revealing its level of certainty, prediction consistency, and classification accuracy across different instances.\nConfidence $\u00fb_i$ quantifies the model's reliability in assigning the correct label to an observation. It is formally expressed as the average model probability assigned to the true label y over the entire training process of E epochs, as denoted by the Equation 1:\n$\\mu_{i} = \\frac{1}{E} \\sum_{e=1}^{E} P_{\\theta^{(e)}}(y|x_{i})$ (1)\nwhere $p_{\\theta^{(e)}}$ is the probability of the model with parameters $\\theta (e)$ at the end of the $e^{th}$ epoch.\nVariability $i measures the distribution of the model's label assignment probabilities $P_{\\theta^{(e)}}$ for a given instance y across epochs E. The standard deviation, as shown in Equation 2, quantifies it:\n$\\hat{\\sigma}_{i} = \\sqrt{\\frac{\\sum_{e=1}^{E} (P_{\\theta^{(e)}} (y | x_{i}) - \\mu_{i})^{2}}{E}}$ (2)\nThe variability value indicates the consistency of the model's predictions for a specific instance. Low variability suggests stable label assignment, while high variability suggests indecision during training.\nCorrectness is a metric that reflects how accurately the model labels observations over epochs. It has possible values of 1 + E and indicates the accuracy of the model's predictions over the entire training period, normalised by the maximum number of correct classifications on a dataset."}, {"title": "Experiments", "content": "The methodology of the study involves training a RoBERTa-large model on the entire training dataset, selecting appropriate instances, and then retraining the model using only those instances. We utilize the pre-trained version of RoBERTa in English, trained using a masked language modelling objective, to finetune it on the medical question answering dataset. Additionally, we test this pre-trained version without fine-tuning of RoBERTa on both the MedQA and OOD dataset. We choose exclusivly the RoBERTa model because it was identified as optimal in the original paper. Each training phase consists of 20 epochs, with an early stopping after 10 epochs at constant validation accuracy and a batch size of 96 epochs. This is in line with the specifications of the original paper. The training process utilises an RTX A6000 GPU.\nFirst, the model was trained using the entire training dataset. The Data Maps framework was applied to compute the three training dynamics."}, {"title": "Results", "content": "The results are divided into three different subsections: replication, feasibility, and transferability to the medical domain. Replication examines the performance of the training dynamics calculated using the Data Maps framework on the MedQA dataset, comparing the results with those of the original work. The feasibility section assesses the technical implementation by evaluating the ease of replication and efficiency of creating Data Maps. In the final section, transferability to the medical domain, we evaluate the potential value and applicability of these training dynamics in a medical context."}, {"title": "Replication", "content": "The replication section is divided into data selection and detection of mislabelled instances, two tasks from the original paper replicated on the MedQA dataset. Data selection aims to filter instances that are valuable for training to enhance generalizability. The detection of mislabelled instances is based on findings by Swayamdipta et al. (2020), which suggest a higher prevalence of mislabelled samples in the hard-to-learn category compared to the easy-to-learn category.\nData Selection Table 1 provides an overview of the model's performance in various training scenarios. The best results are achieved when training on the full MedQA dataset, which shows superior performance on both the MedQA testset and the OOD dataset. When using the 33% training subset, random selection appears to perform better on the ID dataset, although this difference could be due to chance with only one trained seed. Therefore it is very likely that random and ambiguous selection perform similarly well on 33% of the ID dataset, in contrast to the original paper which found a significant difference between these two selections. It is worth noting that the ambiguous selection of 33% in the original paper approaches the same training accuracy as the the 100%. However, a clear improvement can be seen compared to the pre-trained model.\nThe random selection in the OOD dataset performed worse that the ambiguous selection just by 2 percentage points which correlates to one misclassified instance. From this it can be deduced that both selections perform equally well on the OOD dataset, given the restriction to one seed and the small sample size. Notably, both selections perform worse than chance and consequently worse than the pretrained model, which performs as well as chance. This discrepancy can be attributed to the uneven distribution of response options in the OOD dataset. In contrast to the original work, the model trained on the entire dataset generalizes better than the model trained on the ambiguous selection. But this is one of the significant contributions of Data Maps. However, this result could not be replicated for MedQA."}, {"title": "Detection of Mislabelled Instances", "content": "When investigating mislabelled examples, we manually checked the 100 most hard-to-learn examples. We comprehensively reviewed each example to ensure it was correct and matched the provided label. We did not identify any incorrectly labelled examples during this rigorous review. As the questions in our dataset are sourced from official exam questions, the probability of common errors is low. This is in contrast to other open-source datasets, such as SNLI (Bowman et al., 2015), which were used in the original paper (Swayamdipta et al., 2020). The absence of mislabelled examples among the hard-to-learn instances highlights the dataset's reliability and robustness.\nIn summary, we were unable to replicate the results presented in the original paper. We found no significant difference in the performance of models trained on randomly selected or ambiguous subsets of the dataset. Additionally, our investigation of potentially mislabelled examples revealed no inaccuracies."}, {"title": "Feasibility", "content": "The implementation of the provided code was technically challenging due to its lack of modularity and inadequate documentation. Critical information, such as requirements and Python version compatibility, was absent, making it difficult to adapt the codebase. The code's inflexibility compounded the difficulty of applying it to new datasets, requiring significant effort. Apart from creating Data Maps plots, which were relatively straightforward once the data had been computed, other aspects of the codebase were less reproducible.\nIn terms of efficiency, training the model for a single epoch on an RTX A6000 GPU with a batch size of 96 took an average of 47 minutes and 37 seconds. The training was conducted over 20 epochs, resulting in a training time of 16 hours without the calculation of training dynamics. Overall, the calculation took approximately 18 hours. The results are illustrated in Figure 2. These findings illuminate the computational demands of the training process to be able to replicate the training dynamics using the Data Maps framework."}, {"title": "Transferability to Medical Domain", "content": "Training a model to answer medical questions is a challenging task, as demonstrated by the performance chart on the MedQA dataset in Figure 1. BERT-based models have difficulty achieving an accuracy of 40%. Therefore, our achievement of 36% accuracy on the testset is commendable. However, it also indicates that the model must heavily overfit on the training data in order to achieve separation between data points calculated using traning dynamics. Figure 4 shows the training and validation accuracy over the epochs. The training and validation processes began with an initial accuracy of 31.7%. The accuracy quickly increased during training, reaching a peak of 87.7% by Epoch 20. In contrast, the validation accuracy initially increased up to Epoch 5, but then fluctuated with intermittent declines and recoveries. Finally, the validation accuracy stabilised and experienced a slight increase, reaching its maximum of 35.739% in epoch 19. Most importantly Figure 4 illustrates that the model overfitts on the training data without any improvement on the evaluation accurary.\nHowever, not performing this step results in crowded scores, as ilustrated in Figure 3. In this case, the model was trained for only 5 epochs before calculating the training dynamics like it was done in the original paper. Medical datasets require significantly longer training than standard NLP datasets like SNLI. To address this issue, more complex models can be used to calculate training dynamics. LLMs like Med-PaLM2 achieve an accuracy rate of over 80%, which is comparable to the standard NLP datasets used in the original paper. However, these models require even longer training times, which makes it unrealistic to calculate training dynamics.\nThis phenomenon illustrates the challenge of transferring knowledge to the medical field. Medical questions require not only lexical understanding but also domain-specific knowledge. The results highlight that the medical dataset is too complex, making it impossible to calculate the meaningful training dynamics. It appears that the significance of the training dynamics is only meaningful if the model learns from the training data and and does not simply overfit to it. Therefore, the transferability of the Data Maps framework to medical data has not been successful."}, {"title": "Conclusion", "content": "In conclusion, Data Maps provide an automated method for visualising and diagnosing large datasets through training dynamics. Our attempt to replicate the original paper's results was challenging, as we found no significant disparities in model performance between randomly selected and ambiguous subsets of the dataset. Furthermore, our examination of potentially mislabelled examples revealed no inaccuracies.\nFeasibility assessments highlighted the resource-intensive nature of the approach. For example, when calculating training dynamics, which includes training on a 50 GB GPU, using the ROBERTa model required approximately 18 hours, resulting in significant computational and energy costs.\nOur findings suggest limited success in transferring to the medical domain, especially for smaller models. Data Maps classes, such as hard-to-learn, easy-to-learn, and ambiguous, could only be computed through overfitting. The ambiguous subset selected for the study did not perform better than randomly selected subset, which challenges the original paper's results. Large language models (LLMs) achieved similar accuracies on medical datasets to RoBERTa on the datasets used in the original paper. However, applying Data Maps to LLMs and retraining appeared to be cost prohibitive.\nFuture research could explore the potential of using LLMs' hallucination capabilities to calculate the training dynamics confidence, variablity and correctness during inference. Developing a more cost-effective variant of Data Maps for LLMs remains a challenge, leaving room for further exploration in subsequent studies."}]}