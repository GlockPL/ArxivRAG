{"title": "Is Depth All You Need?\nAn Exploration of Iterative Reasoning in LLMs", "authors": ["Zongqian Wu", "Tianyu Li", "Baoduo Xu", "Jiaying Yang", "Mengmeng Zhan", "Xiaofeng Zhu", "Lei Feng"], "abstract": "Deep iterative chain-of-thought (CoT) reasoning enables LLMs to tackle complex tasks by progressively activating relevant pre-trained knowledge. However, it faces challenges in ensuring continual improvement and determining a stopping criterion. In this paper, we investigate whether the relevant knowledge that contributes directly to solving the given question can be activated from the initial reasoning path, thus circumventing the need for iterative refinement. Our experiments reveal that increasing the diversity of initial reasoning paths can achieve comparable or superior performance, a concept we term breadth reasoning. However, existing breadth reasoning approaches, such as self-consistency, offer limited diversity. To address this limitation, we propose a simple yet effective method that enhances reasoning breadth by integrating contextual exploration with reduced sampling randomness. Extensive experiments demonstrate that our approach significantly outperforms deep iterative reasoning.", "sections": [{"title": "1 Introduction", "content": "Chain-of-thought (CoT) reasoning improves the performance of large language models (LLMs) on complex tasks by guiding them to construct intermediate steps before producing final answers (Wei et al., 2022; Kojima et al., 2022). Recently, the OpenAI o1 model (OpenAI, 2024) extended CoT by refeeding both the reasoning process and prediction as new inputs into LLMs and constructing multiple iterative rounds. This approach, which further enhances the ability of LLMs to tackle more challenging tasks, is termed deep iterative reasoning, as shown in the upper part of Figure 1.\nHowever, deep iterative reasoning faces two key challenges: (i) ensuring that the reasoning process"}, {"title": "2 Deep Iterative Reasoning", "content": "This section provides a detailed analysis of deep iterative reasoning. Specifically, Section 2.1 outlines its complete process, while Section 2.2 examines its role in activating prior knowledge. Finally, Section 2.3 investigates whether this knowledge can be directly activated from the initial reasoning path."}, {"title": "2.1 From Standard CoT to Depth Reasoning", "content": "Given the i-th question qi of dataset and the prompt \u011d (e.g., \"Let's think step by step\"), the CoT reasoning steps can be generated by the LLM:\nr = LLM(Concat(qi, \u011d)), (1)\nwhere Concat(\u00b7) function refers to the sequential concatenation of the specified texts. Next, the reasoning steps r\u00fd obtained from Eq. (1) is concatenated with the question qi and the prompt \u011d. The resulting concatenated texts are then fed into the LLM to generate prediction:\np = LLM(Concat(qi, \u011d, ri)). (2)\nFor further deep iterative reasoning mechanism, the current reasoning steps r and prediction pi are used as feedback and fed back into the LLM, generating a new reasoning steps:\nr = LLM(Concat(qi, \u011d, r'\u00ed, p\u00ed, g*)), (3)\nwhere g* is a prompt used to guide the new reasoning. It can either be a reuse of \u011d or be manually constructed (Wu et al., 2024). Next, the LLM discards the earlier reasoning rf and the prediction p', generating new predictions based solely on r\":\np = LLM(Concat(qi, \u011d, ri\")). (4)\nThe process can be repeated (Eqs. (3) - (4)) until the pre-defined stopping criteria are met, or the maximum iteration count is reached.\""}, {"title": "2.2 Deep Iterative Reasoning as a Gradual Activation of Prior Knowledge", "content": "Although deep iterative reasoning, as described in Section 2.1, has demonstrated strong performance (OpenAI, 2024; Wu et al., 2024), its effectiveness is not universally applicable across all tasks.\nAs shown in Figure 3, the performance of LLMs on arithmetic tasks (i.e., AQuA and AddSub datasets) improves significantly from 69.9% to 80.8% as the number of iterations increases. In contrast, the average performance on commonsense tasks (i.e., StrategyQA and CommonsenseQA datasets) remains stable, showing no noticeable improvement. The discrepancy arises from the fundamental nature of these tasks. Arithmetic questions inherently require logical reasoning, where each reasoning steps and its prediction serve as the basis for the subsequent reasoning, forming a progressive deduction process. As the iterations advance, prior logical knowledge related to question-solving is gradually activated, enabling LLMs to derive increasingly reliable answers. An example of this is shown in the upper part of Figure 2.\nHowever, the lack of performance gains in commonsense tasks can be attributed to their core dependence on information retrieval rather than iterative reasoning. For instance, the concept of \"normal basketball\" can be linked to facts such as \"popular in America\" and \"full of air\". If LLMs have not encountered similar commonsense knowledge during the pre-training phase, deeper iterative reasoning alone is unlikely to resolve these challenges. The lower part of Figure 2 provides an illustrative example of this limitation.\nBuilding on these experimental findings, we draw the following important conclusion: Deep iterative reasoning does not generate new knowledge beyond what is embedded in pre-trained LLMs. Instead, its effectiveness stems from LLMs leveraging model-generated information as self-reminders, progressively activating existing pre-trained knowledge relevant to the given test question."}, {"title": "2.3 Can Depth Reasoning be Replaced?", "content": "Based on the conclusion in Section 2.2, we pose a problem: Can relevant pre-trained knowledge that directly contributes to solving the given question be activated from the initial reasoning path? If so, this would eliminate the need for iterative reasoning, thereby bypassing its associated challenges, e.g., progressive refinement and stopping criterion.\nTo investigate this issue, we first selected samples from the arithmetic experiment (i.e., AQUA and AddSub datasets) constructed in Section 2.2, where standard CoT reasoning produced incorrect predictions, while deep iterative reasoning made correct predictions. Subsequently, we applied the self-consistency approach, which generates diverse reasoning paths by sampling from LLMs and aggregates the corresponding multiple predictions through majority voting, to re-test these samples using only a single round of iteration. The experimental results are shown in Figure 4. For the AddSub dataset, accuracy exhibits a steady upward trend with the increase in the number of reasoning paths, reaching a peak at five paths (84.8%), after which it slightly decreases at six paths (77.3%). Similarly, in the AQuA dataset, accuracy improves from 38.4% with a single reasoning path to 61.6% at three paths. However, beyond three paths, the improvement plateaus, with accuracy stabilizing between four and six paths, peaking at 58.9%.\nThese experimental results can be distilled into two key findings: (i) with only one reasoning path, approximately half of the samples are correctly re-classified; and (ii) as the number of reasoning paths increases, the performance initially improves and then stabilizes. This indicates that generating diverse initial reasoning paths has the potential to activate relevant pre-trained knowledge that contributes directly to solving the given question. Subsequently, the correct answer can be determined through major voting across predictions from different reasoning paths. The underlying rationale is that a given question may have multiple solutions. We refer this alternative approach as breadth reasoning, in contrast to deep iterative reasoning.\nHowever, the diversity of reasoning paths generated through self-consistency is limited, restricting the breadth of reasoning and thus failing to fully leverage the potential of LLMs. As shown in Figure 4, self-consistency alone does not completely replace the advantages of deep iterative reasoning. We will address this limitation in Section 3."}, {"title": "3 Breadth Reasoning", "content": "In this section, we begin by reviewing the entire CoT reasoning process, identifying factors that may contribute to the generation of diverse reasoning paths, as discussed in Section 3.1. Then, we investigate the impact of these factors in Section 3.2. Finally, Section 3.3 introduces a simple yet effective method to expand the reasoning breadth."}, {"title": "3.1 What Influences Reasoning Diversity?", "content": "We review the complete CoT reasoning process in the upper part of Figure 5. As the CoT process progresses, we identify four critical factors that may generate diverse initial reasoning paths.\nSpecifically, the CoT process begins with the given question and pre-defined prompt. Modifying the expression of these elements, while preserving their original meaning, can lead to different reasoning paths, as illustrated in and \u2461 of Figure 5. Next, the fixed question and prompt are input into the LLMs, where perturbations to the LLMs themselves can also influence the resulting reasoning paths, as shown in \u2462 of Figure 5. Finally, LLMs generate reasoning paths based on the question and prompt. By performing multiple samplings, different reasoning paths can be explored, similar to self-consistency, as depicted in \u2463 of Figure 5."}, {"title": "3.2 Impact Analysis of Factors", "content": "We assess the impact of these factors on the diversity of reasoning paths using a set of questions that LLMs cannot solve. This evaluation involves generating multiple predictions through diverse reasoning paths shaped by these factors. To quantify diversity, we compute the entropy of these predictions, where a higher entropy value signifies that the factor fosters a broader exploration of reasoning paths. This analysis strategy is grounded in the observation that, for simple questions, the different reasoning paths generated by LLMs typically lead to the same prediction. However, for more complex questions, the predictions derived from different reasoning paths tend to exhibit greater variability.\nThe evaluation results shown in Table 3 in Appendix indicate that, apart from the perturbation directly applied to LLMs, the other three factors significantly influence the reasoning path diversity. Among them, the diversity induced by self-consistency is smaller compared to the other two factors. The fundamental reason for this difference lies in LLM's strong dependence on context. Effectively modifying the expression of a question or prompt is equivalent to creating a new context, prompting the model to reassess the question and generate reasoning paths distinct from the original expression. In contrast, merely sampling reasoning paths from LLMs remains within the same contextual framework, leading to limited variation and failing to overcome cognitive inertia effectively.\nBased on the above analysis, we draw two important insights: (i) perturbing LLMs themselves has little impact on the diversity of initial reasoning path, as LLMs possess strong robustness; (ii) the diversity of reasoning paths guided by factors later in the CoT reasoning process decreases, as the input context becomes progressively fixed, reducing the degrees of freedom available to LLMs."}, {"title": "3.3 Proposed Method", "content": "Building on the insights in Section 3.2, we propose a sample yet effective method to extend the reasoning breadth. Specifically, we modify the expression of the given question or pre-defined prompt while preserving its original meaning. These input context modifications encourages LLMs to explore different hypotheses, consider diverse premises, or initiate reasoning from different sequences.\nTo achieve this, we first construct an instruction: # Instruction: Rephrase the following sentence to change its wording and structure while maintaining the same meaning. Ensure the core sentence remains unchanged. Next, we concatenate this instruction with the question or prompt and feed it into LLMs to generate multiple reformulated versions. Each reformulation serves as the foundation for an independent CoT reasoning process.\nDuring each CoT process, we generate multiple reasoning paths, thereby reducing sampling randomness. By integrating contextual exploration with self-consistency, we extend the reasoning breadth and fully leverage the potential of LLMs. For instance, if we generate three distinct reformulations of a given question and apply self-consistency sampling twice per reformulation, we obtain a total of \u201c3\u00d72=6\" diverse reasoning paths. The final answer is then selected using voting based on the six predictions from these reasoning paths."}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nWe evaluate our method on ten reasoning datasets, including six arithmetic datasets (i.e., MultiArith (Roy and Roth, 2016), GSM8K (Cobbe et al., 2021), SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014), AQuA (Ling et al., 2017), and SVAMP (Patel et al., 2021)), two commonsense reasoning datasets (i.e., StrategyQA (Geva et al., 2021) and CommonsenseQA (Talmor et al., 2018)), and two symbolic reasoning datasets (i.e., LastLetter and CoinFlip (Wei et al., 2022)).\nFor all experiments, we utilize GPT-3.5-turbo-0125 as the foundation model, chosen for its accessibility and cost-effectiveness. Our comparative study examines three categories of methods:\n\u2022 Standard CoT methods: Directly feed the question into LLMs without prompts for zero-shot inference, then introduce a generic prompt for reasoning with greedy decoding (i.e., Zero-Shot CoT (Kojima et al., 2022)).\n\u2022 Deep iterative reasoning methods: Iteratively refine reasoning by using previous steps and predictions as new inputs, terminating when reaching a pre-defined limit or a semantic entropy-based stopping criterion (i.e., Deep-CoT and ARI (Wu et al., 2024)).\n\u2022 Breadth reasoning methods: Improve robustness by generating diverse predictions and selecting the answer via voting. This includes sampling multiple reasoning paths (i.e., self-consistency) and modifying the question or prompt (i.e., question-consistency and prompt-consistency) while preserving semantics."}, {"title": "4.2 Evaluating Different Breadth Methods", "content": "Rows 5-9 of Table 1 present the performance of various breadth reasoning methods across ten datasets, including self-consistency (SC), question-consistency (QuestionC), and prompt-consistency (PromptC), as well as our proposed methods, QuestionC-SC and PromptC-SC. The results show that SC performs particularly well on symbolic reasoning tasks (i.e., LastLetter and CoinFlip), confirming its effectiveness in maintaining consistency across multiple reasoning paths. Meanwhile, QuestionC and PromptC enhance the diversity of reasoning paths by modifying the expressions of the question and prompt, respectively, leading to further improvements in breadth reasoning performance, with notable gains in arithmetic tasks.\nCompared to other breadth reasoning methods, our QuestionC-SC and PromptC-SC achieve superior performance. These two methods further explore the contextual space while reducing sampling randomness based on SC. Notably, PromptC-SC attains the highest average accuracy of 85.1% across all datasets. The 4.3% improvement of PromptC-SC over PromptC underscores the effectiveness of integrating SC with contextual prompt space exploration in enhancing reasoning diversity."}, {"title": "4.3 Breadth v.s. Depth", "content": "We compare breadth reasoning methods with deep iterative reasoning methods (i.e., Deep-CoT and ARI) to analyze the respective advantages of these two reasoning paradigms. Experimental results indicate that Deep-CoT performs particularly well on arithmetic tasks such as GSM8K and AQUA datasets, demonstrating its capability to optimize the reasoning process through multiple iterations. Building on this, ARI further introduces an adaptive iterative reasoning mechanism, achieving the best performance (87.1%) on the AddSub dataset.\nHowever, in symbolic and commonsense reasoning tasks, breadth reasoning methods generally outperform deep iterative reasoning methods. For instance, PromptC-SC surpasses ARI by 3.6% and 1.4% on the LastLetter and CoinFlip datasets, respectively. Moreover, PromptC-SC also achieves a higher overall average accuracy (85.1%) compared to ARI (83.3%), with an improvement of 1.8%. These results suggest that breadth reasoning exhibits significant advantages in tasks requiring diverse and structured reasoning paths.\nNotably, QuestionC-SC and PromptC-SC also demonstrate strong competitiveness in tasks traditionally favoring deep iterative reasoning methods, such as GSM8K and AQuA datasets. This observation indicates that while deep reasoning excels in logical tasks, breadth reasoning methods can achieve comparable or even superior performance by integrating self-consistency with contextual space exploration. These findings further highlight the significant potential of breadth reasoning methods in handling diverse reasoning tasks."}, {"title": "4.4 Breadth Analysis", "content": "While our method has shown significant improvements, several issues remain to be explored: (i) Does our method truly extend the reasoning breadth? and (ii) Is our method broader than self-consistency, or does it simply gain improvements by increasing the number of reasoning paths?"}, {"title": "5 Why Our Method Works?", "content": "Our proposed method significantly outperforms existing breadth reasoning and deep iterative reasoning approaches. In this section, we analyze the underlying reasons for these improvements.\nIn breadth reasoning, the diversity of reasoning paths obtained solely through self-consistency (SC) is limited. This reason is that the input context space is fixed, constraining the degrees of freedom in LLMs. As illustrated on the right side of Figure 7, reasoning paths obtained through SC sampling may be confined to the same plane, where different points on the plane represent distinct reasoning paths leading to the same predicted outcome.\nFor simple questions, SC-guided reasoning paths typically cluster within the correct plane, meaning that even a single sampled reasoning path is often sufficient to reach the correct answer. In such cases, while SC is effective, it is not essential. However, for complex questions, if the fixed context primarily guides reasoning paths toward an incorrect plane, the SC approach fails. In contrast, our method enhances the diversity of reasoning paths by exploring the context space, thereby covering a broader range of reasoning strategies. Meanwhile, we integrate SC to improve the reasoning stability under the same context. As a result, our approach increases the probability that the reasoning paths fall within the correct reasoning plane.\nOn the other hand, deep iterative reasoning progressively refines reasoning process through multiple iterations to achieve the correct prediction. However, it face two critical challenges: (1) how can we ensure that each newly generated reasoning path improves upon the previous iteration? (2) how can we effectively determine the optimal stopping criterion for iteration? As illustrated on the left side of Figure 7, when a newly generated reasoning path fails to introduce meaningful updates, the iteration becomes ineffective, potentially preventing the discovery of the correct reasoning path within the pre-defined maximum number of iterations. Furthermore, even if the reasoning path has already reached the correct plane, continued iterations may introduce unnecessary extensions, increasing the risk of errors in the final prediction.\nIn contrast, our method employs a single round of reasoning strategy, effectively circumventing the limitations of iterative refinement and thereby substantially improving reasoning performance."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we analyzed deep iterative reasoning and identified its reliance on progressively activating pre-trained knowledge. To overcome its challenges, such as ensuring refinement across iterations and determining a stopping criterion, we proposed a breadth reasoning method, which enhances reasoning diversity by modifying input expressions and leveraging self-consistency. Experiments showed that our method effectively expands the reasoning space and outperforms deep iterative reasoning without multiple iterations, highlighting reasoning diversity as a viable alternative to iterative refinement for improving CoT in LLMs.\nSeveral promising directions remain for future work. While modifying pre-defined prompts led to significant performance gains, these modifications remain within the same semantic space. Integrating more effective semantic exploration could further enhance reasoning breadth. Additionally, although breadth reasoning successfully circumvents the challenges of deep iterative reasoning, its computational cost cannot be ignored. Future CoT research should focus on efficiently combining depth and breadth reasoning to maximize performance while minimizing computational overhead."}, {"title": "Limitations", "content": "Although our proposed breadth reasoning approach excels in enhancing reasoning diversity and outperforms deep iterative reasoning in this regard, it still has certain limitations. First, while it successfully enhances reasoning diversity and outperforms deep iterative reasoning, its reliance on multiple generated reasoning paths increases computational cost, which may not be practical for real-time or resource-constrained applications.\nSecond, our approach primarily explores reasoning diversity within a fixed semantic space by modifying input expressions. However, this modification may not fully capture deeper semantic variations that could further improve reasoning breadth. Developing techniques to explore broader semantic spaces remains an open challenge.\nLastly, although breadth reasoning circumvents the challenges of iterative refinement, it does not explicitly incorporate mechanisms to ensure reasoning consistency across diverse paths. Future work could investigate hybrid approaches that balance depth and breadth reasoning to further enhance the effectiveness of CoT reasoning in LLMs."}, {"title": "A Appendix", "content": "A.1 Related work\nA.1.1 Chain-of-Thought\nChain-of-thought reasoning (CoT) (Kojima et al., 2022; Wei et al., 2022), a prompting-based approach that guides LLMs to reason step by step, has emerged as a powerful paradigm for enhancing their decision-making capabilities. CoT has been widely applied to various tasks, including mathematical problem-solving (Mishra et al., 2022) and multi-modal reasoning (Chen et al., 2023; Lu et al., 2022), due to its improved reasoning performance, interpretability, transparency (Wang et al., 2023), and collaborative capabilities (Le et al., 2024).\nTraditional CoT methods primarily focus on prompt construction. Manual prompting techniques, such as PAL (Gao et al., 2023), achieve high performance but are costly and difficult to generalize. In contrast, automatic prompting methods, such as Auto-CoT (Zhang et al., 2022), offer low-cost, easily transferable solutions but are more error-prone. To balance performance and efficiency, semi-automatic prompting methods, including AutoMate CoT (Shum et al., 2023) and BoostedPrompt (Pitis et al., 2023), have been introduced, making them suitable for real-world applications.\nBeyond prompt design, several different approaches address challenges in CoT reasoning. Self-Refine (Madaan et al., 2024) and Reflexion (Shinn et al., 2024) enhance LLM reliability by effectively mitigating hallucinations and factual inaccuracies through iterative verification and refinement. To better overcome vanilla CoT's limitations in handling complex questions, L2M (Zhou et al., 2022) decompose problems into simpler sub-tasks. In highly knowledge-sensitive tasks, KD-CoT (Wang et al., 2023) integrate external knowledge bases to improve accuracy and reduce factual errors. Additionally, RankPrompt (Hu et al., 2024) leverages inherent uncertainty through self-ensemble techniques, allowing LLMs to rank predictions and enhance accuracy.\nDriven by the structural limitations of sequential CoT reasoning, recent advancements have introduced more expressive architectures, such as tree- and graph-based structures. Tree structures (Yao et al., 2024b; Chen et al., 2024) facilitate broader exploration and enable backtracking, enhancing the model's ability to refine its reasoning process. Meanwhile, graph structures (Lei et al., 2023; Besta et al., 2024) improve sub-problem aggregation and enable self-verification, further strengthening the robustness and coherence of CoT reasoning."}, {"title": "A.1.2 Long-Chain Thought Reasoning", "content": "The foundational concept of CoT reasoning has been significantly extended through the development of long-chain thought reasoning. Pioneering models such as OpenAI ol series (OpenAI, 2024) have introduced inference-time scaling by lengthening the reasoning process, enabling more sophisticated deliberation. The key advantage of long-chain CoT lies in its ability to break down complex problems into finer-grained steps, fostering deeper analytical reasoning and ultimately leading to more precise and comprehensive solutions.\nBeyond the OpenAI 01, several notable models have embraced the long-chain reasoning paradigm. For instance, DeepSeek-R1 (DeepSeek, 2024), QwQ (Qwen, 2024), and Marco-o1 (Zhao et al., 2024) have demonstrated the effectiveness of this approach. These models iteratively refine their reasoning by identifying and correcting errors, simplifying intricate steps, and exploring alternative strategies when necessary, thereby enhancing both the robustness and adaptability of their inference.\nMoreover, some models have incorporated verifiable reward mechanisms to refine long-chain CoT generation while addressing challenges such as reward hacking in large-scale reinforcement learning. Specifically, methods that leverage accuracy-driven rewards from ground-truth answers (Team et al., 2025; Pan et al., 2025) help ensure the reliability and consistency of the generated reasoning paths.\nThe impact of long-chain CoT extends beyond text-based reasoning. For example, Mulberry (Yao et al., 2024a) has shown that o1-like reasoning principles can be effectively applied to multimodal contexts, expanding the applicability of long-chain CoT across diverse domains. Additionally, recent advancements have focused on improving stepwise coherence and integrating diverse reasoning strategies (Team et al., 2025), leading to enhanced model performance and training efficiency.\nThese developments underscore the transformative potential of long-chain CoT in enabling LLMs to engage in more structured, rigorous, and adaptable problem-solving, paving the way for more advanced and reliable AI reasoning systems."}, {"title": "A.2 Algorithmic Pseudo-Code", "content": "We provide the pseudo-code of our proposed method (e.g., QuestionC-SC) in Algorithm 1."}, {"title": "A.3 Experimental Results of Section 3.2", "content": "A.3.1 Experimental Setting\nThis section outlines the experimental setup, detailing the models, datasets, and the variations introduced under different experimental conditions. Specifically, we design four types of experiments: (i) modify the expression of the given question, (ii) modify the expression of the pre-defined prompt, (iii) perturbations applied to LLMs themselves, and (iv) sample initial reasoning paths from LLMs.\nModel: Our experiments using the open-source model Llama-2-7b-chat, released by Meta. Llama-2-7b-chat is an optimized version of Llama-2-7b for conversational use cases. The model weights can be accessed from https://huggingface.co/meta-llama/Llama-2-7b-chat.\nDatasets: Rather than evaluating the entire dataset, we construct a challenging subset comprising problems that LLMs struggle to solve. Specifically, we select 20 questions from the AddSub dataset and another 20 from the AQuA dataset, forming two separate test sets.\nComputational Setup: All experiments are conducted on 8 NVIDIA GeForce RTX 3090 GPUs. The top-k parameter is set to 10. Unless otherwise stated, we use \"Let's think step by step.\" as the CoT prompt trigger for all experiments except for those involving variations in prompt expressions. Similarly, except for the variations introduced by LLM sampling, the temperature parameter is set to 1.0 for all other experiments.\nVariations in Question Expressions: To generate different question formulations while preserving their meaning, we use the following instruction: # Instruction: Significantly rephrase the following question to change its wording and structure while maintaining the same meaning and intent. Ensure that the core problem remains unchanged. For each question, this prompt generates 10 reworded versions, including the original, which are used as input for the CoT prompting mechanism.\nVariations in Prompt Expressions: To evaluate the impact of different prompt formulations on reasoning, we use the following 10 alternative prompts to induce LLMs to generate reasoning chains: (1) Here are the steps we can follow to achieve our goal. (2) How about we break it down into smaller parts? (3) How would you like to approach this situation in a methodical manner? (4) Let's break down the problem into smaller parts and tackle each one separately. (5) Here are some thoughtful steps to consider. (6) Let's take it one step at a time. (7) Let's take a thoughtful approach. (8) In a systematic approach, let's work through the following stages. (9) In a systematic manner, let us consider each detail. (10) Let's think step by step. Each of these prompts generates a distinct reasoning path, allowing us to analyze the sensitivity of the model to different phrasing styles.\nVariations Introduced by Perturbations to LLMS: To introduce controlled perturbations into the LLM, we replace its final linear layer with a noisy linear layer, where both the weights and biases are perturbed. The noise is sampled from a normal distribution, with a standard deviation ranging from 0.010 to 0.020 in increments of 0.001, resulting in 10 distinct perturbed models. Each perturbed model generates a unique reasoning path, which is subsequently processed by the standard LLaMA-2-7B-Chat to obtain the final results.\nVariations Introduced by LLM Sampling: To assess the impact of sampling variability, we set the temperature parameter to 0.8 and the number"}, {"title": "A.3.2 Main Results", "content": "Table 3 presents the information entropy values of predictions generated under different influencing factors, including variations in the question phrasing, prompt formulation, model-intrinsic perturbations, and sampling strategies. The entropy metric quantifies the diversity of reasoning paths, where a higher entropy value indicates a greater exploration of different reasoning trajectories.\nNotably, LLM-intrinsic perturbations yield the least entropy, suggesting that the model's inherent generation process alone is insufficient to significantly diversify reasoning pathways. This result highlights LLMs' strong robustness, where minor perturbations fail to introduce substantial variations in reasoning paths, leading to consistent predictions even under slight modifications. Sampling strategies exhibit moderate entropy values, indicating that while sampling can introduce some variations in the reasoning process, it remains constrained by the overarching context and prior model biases.\nAdditionally, we observe that the information entropy values are relatively consistent across datasets, with AQuA and AddSub showing similar trends. The average entropy values reinforce the conclusion that prompt modifications are the most effective in enhancing reasoning diversity, while LLM perturbations alone are the least impactful.\nThese findings provide empirical evidence for the crucial role of context reconfiguration in diversifying reasoning paths, suggesting that future work on enhancing reasoning diversity should prioritize context manipulation strategies over simple perturbations or sampling techniques."}]}