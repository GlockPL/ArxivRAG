{"title": "Star Attention: Efficient LLM Inference over Long Sequences", "authors": ["Shantanu Acharya", "Fei Jia", "Boris Ginsburg"], "abstract": "Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 95-100% of accuracy.", "sections": [{"title": "INTRODUCTION", "content": "Recent Large Language Models (LLMs) can support contexts up to millions of tokens in length (Gemini-Team, 2024; Anthropic, 2024; Meta-AI, 2024), unlocking applications such as repository-level code analysis, multi-document summarization, and large corpus retrieval. However, processing such long sequences with LLMs requires substantial computational and memory resources due to the quadratic complexity of the self-attention mechanism.\nTo address these challenges, various techniques have been proposed to reduce memory usage and increase inference speed. For example, Flash Attention introduces an efficient GPU block-wise implementation of the global attention, achieving significant reductions in memory overhead and runtime (Dao et al., 2022; Dao, 2024). Ring Attention further extends this idea by distributing the computation of self-attention and feed-forward modules across multiple devices, cleverly overlapping communication with shard-local attention computations to enhance scalability (Liu et al., 2024a; Beltagy et al., 2020). More broadly, distributed strategies such as tensor, pipeline, sequence, and data parallelism have been proposed to divide compute effectively across multiple machines (Shoeybi et al., 2019; Huang et al., 2019; Li et al., 2023; Meta-AI, 2021).\nSeveral prior works have shown that the attention matrix can be approximated with sparse attention mechanisms reducing the algorithmic complexity from quadratic to linear or log-linear. Child et al. (2019) significantly reduces the complexity of attention by leveraging sparse factorizations and (Choromanski et al., 2021) approximates attention using kernel-based methods. (Beltagy et al., 2020) employs sliding window attention and global tokens for efficient long-sequence processing while Xiao et al. (2024) adapts it for real-time long-sequence generation utilizing attention sinks. Complementing these approaches, memory-efficient techniques have also emerged. Key-value (KV) cache compression (Dai et al., 2019; Ge et al., 2024; Munkhdalai et al., 2024; Sun et al., 2024; Liu et al., 2024b) and low-rank approximations (Srebro & Jaakkola, 2003) trade precision for reduced memory usage.\nWe introduce Star Attention, a novel algorithm for efficient LLM long-context inference 1. This method is based on the observation that LLM inference usually has two stages: (1) prompt encoding, where the model processes input and stores KV vectors in the cache and (2) token generation, where model attends to the KV cache and autoregressively generates new tokens while updating the"}, {"title": "STAR ATTENTION ALGORITHM", "content": "Star Attention operates in two phases: (1) Context Encoding, where the long context is divided into contiguous blocks and is processed with local blockwise attention, and (2) Query Encoding and Token Generation, where the query is processed, and answer tokens are generated using global attention. Below, we detail each phase of the algorithm."}, {"title": "PHASE 1: CONTEXT ENCODING", "content": "Given an input sequence comprising a context c followed by a query q, the context c is divided into n contiguous blocks: c = [C1, C2, ..., Cn], where each block ci contains b tokens. We introduce an anchor block mechanism, in which, each block except the first\u2014is prefixed with the first block C1 of the sequence, referred to as the anchor block. This concatenation forms an augmented context c':\nc' = [C1, (C1 C2), (C1 C3), . . ., (C1 Cn)]\nwhere each augmented block c\u00f3 contains 2b tokens: b tokens from the anchor block c\u2081 followed by b tokens from the current block ci. The positional indices of c\u2081 are preserved, ensuring that its tokens retain their original position indices [0, 1, . . ., b\u22121]. The augmented blocks are distributed across compute hosts, where each host computes attention over the 2b tokens from its assigned block c and generates the corresponding key-value (KV) vectors. While KVs for the anchor block c\u2081 are discarded, the KVs for the current block ci are retained in the cache.\nWe observe that, without anchor blocks-i.e., applying blockwise attention only to the original context c-the model fails to generate correct outputs. We conjecture this failure is due to the incorrect approximation to the attention patterns observed during phase 2, where multiple attention spikes, known as attention sinks (Xiao et al., 2024), are distributed across the sequence. These spikes occur because each block is processed independently, creating an attention sink at the start of each block. As a result, the model struggles to effectively focus on relevant parts of the context. To address this issue, we prefix the blocks with the anchor block c\u2081, shifting the"}, {"title": "PHASE 2: QUERY ENCODING AND TOKEN GENERATION", "content": "In phase 2, global attention is employed to encode the query and generate output tokens by using a distributed softmax algorithm that eliminates the need to transfer KV cache between hosts. A designated query-host hq coordinates this computation. The query is broadcast to all hosts and transformed into the sequence Q \u2208 R^{lq\u00d7d}, where lq is the query length, and d is the attention head dimension. Each host h computes the local attention output An for the query Q using its local key-value pairs Kh, Vh \u2208 R^{lk\u00d7d}, where lk is the sequence length of the KV cache. The local attention is computed as:\n\\begin{equation}\nA_h = \\frac{\\exp \\left( \\frac{QK_h^T}{\\sqrt{d}} \\right) V_h}{\\sum_{k=1}^{l_k} \\exp \\left( \\frac{QK_{h,k}^T}{\\sqrt{d}} \\right)} \\tag{1}\n\\end{equation}\nIn addition to Ah, each host also stores the sum of the exponents sh from the the local softmax operation (the denominator from Equation 1):\n\\begin{equation}\nS_h = \\sum_{k=1}^{l_k} \\exp \\left(\\frac{QK_{h,k}^T}{\\sqrt{d}}\\right) \\tag{2}\n\\end{equation}\nThe query-host hq gathers the local attention An and the sums of exponents sh from all hosts:\nA = [A1, A2,..., AH]\nS = [s1,s2,...,sH]\nThe global softmax denominator, Sglobal, is then computed as the sum of all local exponents:\n\\begin{equation}\nS_{global} = \\sum_{h=1}^{H} S_h \\tag{3}\n\\end{equation}\nThe query-host uses sglobal to aggregate the local attentions to compute the global attention:\n\\begin{equation}\nA_{global} = \\sum_{h=1}^{H} \\frac{S_h}{S_{global}} A_h \\tag{4}\n\\end{equation}\nThis method ensures that the global attention scores are normalized correctly across all hosts. It requires the communication of only a single scalar sh (the local sum of exponents) and a vector Ah (the local attention) per token. In practice, the log-sum-exp method from online softmax (Milakov & Gimelshein, 2018) can be used to maintain the numerical stability during global attention aggregation. This distributed approach enables efficient computation by minimal data transfers between hosts.\nOutput generation and cache update. After computing the global attention output, the query-host hq generates the next token and its KV cache is updated with the key and value vectors of the new token. This process is repeated for each generated token.\nThis two-phase mechanism-local context encoding with anchor blocks in Phase 1 followed by global query encoding with token generation in Phase 2-gives significant improvements in inference speed, while keeping the accuracy close to the global attention."}, {"title": "EXPERIMENTS", "content": "We evaluate Star Attention on several Llama-based models with sequence lengths ranging from 16K to 1M tokens on RULER (Hsieh et al., 2024) and BABILong (Kuratov et al., 2024) benchmarks. We begin by comparing accuracy and the speed achieved by Star Attention versus baseline - Ring attention. Further, we investigate the impact of varying block sizes on accuracy, illustrating the trade-off between accuracy and speedup. Finally, we conduct a detailed analysis of challenging and favorable cases for Star Attention by examining distinct RULER task categories."}, {"title": "SETUP", "content": "Models. We benchmark the base and instruct variants of the Llama-3.1 8B model which support context lengths up to 128K tokens (Meta-AI, 2024). In addition, we evaluate two Gradient.ai models that extend Llama-3-8B's context up to 1M tokens Gradient.ai (2024). To access the scalability of our method, we also evaluate the Llama-3.1-70B-Instruct model. We observe that large LMs achieve even greater speedups with Star Attention on long context tasks.\nBaseline. We compare Star Attention with Ring Attention (Liu et al., 2024a). Ring Attention computes global block-wise attention by having each host communicate its respective KV cache in a ring pattern across all the hosts . More details regarding our baseline selection in Appendix C.1.\nConfiguration. We implement Star Attention using HuggingFace Transformers library (Wolf et al., 2020). All experiments are done on A100 GPUs with bfloat16 precision. More details on the experiment configuration are in Appendix C.\nEvaluation Benchmarks. We use the RULER benchmark for evaluation. It consists of 13 tasks categorized into 4 domains: Needle-in-a-Haystack (Retrieval), Multi-Hop Tracing, Aggregation, and Question Answering (Hsieh et al., 2024). Additionally, we report results on the BABILong benchmark, which encompass tasks where multiple supporting facts encoded in the context are required to generate accurate answers (Kuratov et al., 2024). Further details on the benchmarks and specific tasks can be found in Appendix B."}, {"title": "RESULTS", "content": "Table 1 provides relative speedup and accuracy achieved by Star Attention versus Global (Ring) Attention from 16K to 128K tokens on the RULER benchmark. In each setting, the context block size and anchor block size are set to one-quarter of the total sequence length. Star Attention achieves similar accuracy to full global attention, with relative accuracy degradation limited to 0-3% while also giving upto 5x inference speedup. This demonstrates that Star Attention effectively preserves the model's ability to retrieve relevant information, even with a significantly reduced context window. In case of larger models, such as Llama-3.1-70B Instruct, we find that these models achieves even greater speedups at any given sequence length while maintaining similar levels of accuracy degradation. We discuss Star Attention's strengths and limitations based on RULER subtasks in Section 3.4. Full RULER scores for all models can be found in Appendix E."}, {"title": "TRADE-OFF BETWEEN ACCURACY AND SPEED", "content": "Figure 5, illustrates the effect of varying block sizes during context encoding with sequence length of 128K tokens. Larger block sizes correlate to higher accuracy with Star Attention. This trend is consistent across all sequence lengths in our experiments.\nFrom our experiments, we observe that setting the block size to approximately one-quarter of the total sequence length strikes an optimal trade-off between accuracy and speed. However, for sequence lengths exceeding 128K, as shown in Table 2, we fix the block size at 32K tokens to prioritize speedup, allowing for some acceptable accuracy degradation. Similarly, for larger models such as the Llama-3.1-70B-Instruct, we limit the block size to 16K tokens. The choice of block size is dependent on the user on how much accuracy can be traded for improved speed. As the block size increases, Star Attention's performance approaches that of full global attention, providing users with flexibility in balancing computational efficiency with accuracy. Additional details regarding the experimental setup are provided in Appendix C.2."}, {"title": "IN-DEPTH ANALYSIS ON RULER TASK CATEGORIES", "content": "In this section we investigate the strengths and limitations of Star Attention, using different categories of tasks within RULER. The benchmark has five primary categories: Single-NIAH, Multi-NIAH, Multi-Hop Tracing, Aggregation, and Question Answering (QA). Figure 6 presents categorical results of RULER for the Llama-3.1-8B-Instruct model on sequence length of 32K. The trend is consistent across all sequence lengths (16K, 32K, 64K, and 128K), as detailed in Appendix E (Figure 8). Notably, Star Attention achieves scores nearly identical to global attention in Single-NIAH tasks. However, in more complex tasks such as Multi-NIAH and QA, it shows slight decline in performance, with reductions ranging from 1.6% to 4.9% in Multi-NIAH and 0.9% to 6.8% in QA tasks. Despite these challenges, Star Attention consistently retains overall 95-100% accuracy of global attention."}, {"title": "ABLATION STUDY", "content": "The ablation experiments focus on the Needle-in-a-Haystack (NIAH) task, which tests a model's ability to answer queries based on a small, relevant piece of information (\u201cneedle\") embedded within a large context (\"haystack\"). To increase the task's complexity, we explore three variations from the RULER benchmark (Hsieh et al., 2024): Single-NIAH, Multi-key NIAH, and Multi-query NIAH."}, {"title": "POSITION AND CONTENT OF ANCHOR BLOCK", "content": "In this section, we explore the role of anchor blocks during Phase 1 that enables Star Attention to approximate global attention behavior. As outlined in Section 2.1, anchor blocks are crucial in managing the attention spikes generated at the start of each context block, helping Star Attention approximate global attention (see Table 3) Drawing from the hypotheses on sink tokens in Xiao et al. (2024), we consider two potential explanations for the effectiveness of anchor blocks: (1) the model may develop a bias toward the absolute position of the anchor block, or (2) the semantic content of the anchor block is essential for maintaining performance. To better understand how anchor blocks enable Star Attention to approximate global attention distribution, we test both the hypotheses. We conduct experiments on the Llama-3.1-8B-Instruct model, varying both the position and content of the anchor block. We evaluate two configurations: a block size of 16K for sequences of length 64K, and a block size of 32K for sequences of length 128K, in both the cases, with anchor block size matching the context block size.\nPosition of anchor block: Here, we fix the content of the anchor block to the first context block and vary its position IDs. We test three scenarios : (1) the position IDs are randomly sampled from the range [0, starting position of the current block] (e.g., for a block starting at position 32K, position IDs are sampled from [0, 32K] ); (2) the position IDs are derived from the previous block (e.g., for a block of size 16K starting at position 32K, position IDs are sampled from [16K, 32K] ); (3) the position IDs are fixed to the first block (our proposed approach). As shown in Table 3, varying the position of the anchor block has minimal impact on accuracy.\nContent of anchor block: We fix the position IDs of the anchor block to that of the first block but vary its content. We explore several configurations (as shown in Table 3): (i) a single repeated token (e.g.,'','the', or '.'); (ii) random tokens; (iii) shuffling the tokens of the first block; and (iv) using the original first block content (the proposed approach). Our results show that the content of the anchor block significantly impacts performance, with the original first block content yielding the best results. This outcome suggests that since global attention is performed during Phase 2, it is important for the local context blocks to attend to anchor blocks whose content reflects what the model would see during global attention.\nPrevious block as anchor block: To examine the roles of both position and content, we experiment with using the previous block as the anchor block. For example, for a block of size 16K starting at position 32K, the anchor block would be the block with position IDs from 16K to 32K. This configuration has lower accuracy comparing to using the first block as the anchor.\nIn summary, we found that while the positional placement of the anchor block is not important, its content is critical for optimal performance."}, {"title": "SIZE OF ANCHOR BLOCK", "content": "As discussed in Section 3.3, larger block sizes improve the accuracy of Star Attention. In this section, we analyze the impact of varying anchor block size while maintaining a fixed block size of 32K for a sequence length of 128K. As illustrated in Figure 7, increasing the anchor block size enhances model accuracy, with the best performance observed when the anchor block size equals the context block size. Although Figure 3b demonstrates that attention spikes predominantly occur in the first few tokens, reducing the number of tokens in the anchor block leads to a substantial drop in performance. This suggests that a larger anchor block is critical for maintaining model accuracy, despite attention spikes being concentrated at the beginning of the sequence. This observation implies that the anchor block's effectiveness is not solely due to its role in managing attention sinks but may involve other underlying factors. These findings remain consistent across both base and instruct models, as well as for all sequence lengths. Further investigation into why the anchor block size must be equivalent to the context block size is left for future work."}, {"title": "CONCLUSION", "content": "In this paper, we introduced Star Attention, a novel block-sparse attention mechanism designed to enable efficient inference on long sequences in transformer-based LLMs. The method operates in two phases: (1) context tokens are processed using blockwise-local attention, with the context segmented into blocks where each block is prefixed with an anchor block; and (2) then the query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention delivers up to 11x speedup over Ring Attention while maintaining 95-100% accuracy, significantly enhancing both memory efficiency and inference speed. Scaling Star Attention to longer sequences (up to 1M) and larger models, we observe even greater speedups while preserving similar levels of accuracy. Despite these advances, several open questions remain. The role and optimal size of anchor blocks relative to context blocks require further exploration. Additionally, while Star Attention performs effectively with block sizes set to one-quarter of the sequence length, accuracy degrades when using smaller blocks on longer sequences. Future work will focus on refining the anchor block mechanism and improving performance on more complex long-context tasks to enhance the scalability and robustness of Star Attention."}]}