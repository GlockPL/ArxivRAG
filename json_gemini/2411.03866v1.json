{"title": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the Way Forward", "authors": ["Shashi Kumar", "Iuliia Thorbecke", "Sergio Burdisso", "Esa\u00fa Villatoro-Tello", "Manjunath K E", "Kadri Hacio\u011flu", "Pradeep Rangappa", "Petr Motlicek", "Aravind Ganapathiraju", "Andreas Stolcke"], "abstract": "Recent research has demonstrated that training a linear connector between speech foundation encoders and large language models (LLMs) enables this architecture to achieve strong ASR capabilities. Despite the impressive results, it remains unclear whether these simple approaches are robust enough across different scenarios and speech conditions, such as domain shifts and different speech perturbations. In this paper, we address these questions by conducting various ablation experiments using a recent and widely adopted approach called SLAM-ASR. We present novel empirical findings that offer insights on how to effectively utilize the SLAM-ASR architecture across a wide range of settings. Our main findings indicate that the SLAM-ASR exhibits poor performance in cross-domain evaluation settings. Additionally, speech perturbations within in-domain data, such as changes in speed or the presence of additive noise, can significantly impact performance. Our findings offer critical insights for fine-tuning and configuring robust LLM-based ASR models, tailored to different data characteristics and computational resources.", "sections": [{"title": "I. INTRODUCTION", "content": "Enabling large language models (LLMs) to \"comprehend\" non-textual modalities has received substantial attention recently. For instance, in [1] the authors trained a projection layer to align the outputs of a visual encoder with an LLM. In the context of automatic speech recognition (ASR), some early methods utilize a cascaded approach, where speech is first transcribed using an automated ASR system, followed by processing the resulting text with an LLM to enhance the transcription accuracy [2]\u2013[5]. However, cascaded approaches have several limitations, including error propagation and a lack of access to valuable paralinguistic acoustic information, such as prosody, speaker characteristics, and emotional valence.\nRecently, systems that integrate robust speech encoders with instruction-tuned LLMs through a connector/projector layer have been proposed as end-to-end ASR solutions [6]\u2013[10]; henceforth referred as LLM-based ASR systems. Intuitively, the main task of the connector/projector is to learn how to transform acoustic embeddings from speech encoders into speech representations (tokens) that are meaningful within the LLM's embedding space. These representa-tions are then combined with text instructions (i.e., prompts) and fed into an LLM to generate various predictions, such as transcription, emotion classification, language identification, and named entity recognition. Three immediate advantages of such architectures are: (a) high compute efficiency, as the entire system can be adapted to new tasks by adopting parameter-efficient approaches, and/or by only training the projector layer; (b) data efficiency due to the vast corpora used in the pre-training of foundational models; and (c) enhanced generalization capability, as LLMs can leverage prompts for zero-shot or in-context learning to handle unseen tasks effectively [7]."}, {"title": "II. METHODS", "content": "The SLAM-ASR architecture consists of a speech encoder, fol-lowed by a fixed downsampler, then a trainable linear projector, with the final network being an instruction-tuned LLM accepting text embedding (prompt) and audio outputs coming from the projector (Figure 1). One advantage of this implementation is its flexibility, which makes it possible to use different existing pre-trained speech encoders, as well as a variety of LLMs. According to the original work [10], and confirmed by our experiments, the combination of WavLM-large [15] as a speech encoder and Vicuna-7B as an LLM achieves the best performance. Thus, in the experiments reported here, we always use the same WavLM-large+Vicuna-7B combination as the original work. The key element of SLAM architecture is that the only learnable element is the linear projector, a single hidden layer followed by a ReLU activation and a regression layer:\n$E_i = Linear(ReLU (Linear(Z_i)))$\nwhere $Z_i$ is the i-th down-sampled audio feature, consisting of the concatenation of k consecutive frames (output from WavLM) in the feature dimension, and $E_i$ has the same dimension as the LLM input embedding. We refer to $E_i$ as the speech token embedding of the i-th audio feature. Finally, an input audio consisting of the n down-sampled audio features $Z_0 ... Z_n$ is given to the instruction-tuned LLM using the following prompt:\n$E_0 ... E_n < s >USER: Transcribe speech to text.  ASSISTANT:{transcript}</s>$\nwhere {transcript} is the text corresponding to the audio, which is either provided during training or forced to be generated by the LLM at inference time."}, {"title": "B. Baseline model", "content": "For the sake of a fair comparison, we will use the WavLM-large [15] based model as our baseline ASR system since, as in the original work, is also used as the speech encoder in the SLAM-ASR model. More precisely, the baseline simply consists of adding a linear layer on top of the WavLM-large, trained using Connectionist Temporal Classification (CTC) [16] loss, as a typical End-to-End (E2E) ASR model. We refer to this model simply as \"ASR\" in contrast to \"SLAM-ASR\"."}, {"title": "III. EXPERIMENTAL SETUP", "content": "We selected three well-known benchmark datasets\u2014LibriSpeech, CallHome, and CommonVoice\u2014due to their distinct characteristics and the unique challenges they present. Additionally, we experi-mented with a private dataset, ContactCenter, composed of multido-main contact center conversations.\nLibriSpeech corpus consists of read English speech derived from audiobooks with 1000 hours of speech sampled at 16 kHz [17]. We trained our models using the 960h training partition, while all the evaluations were performed in the LibriSpeech test-clean partition.\nCallHome English dataset (LDC97S42) contains spontaneous tele-phone conversations between multiple speakers, comprising 12.5h of training and 1.5h of test transcribed speech data. This dataset poses challenges due to its conversational nature, known to be difficult for ASR, with a large number of short segments.\nCommon Voice comprises several thousand hours of crowdsourced audio in more than 100 languages [18], featuring significant variabil-ity in speakers, accents, speaking styles, background noise, among others. We used the English test partition from CommonVoice-v11, containing 27h, but only as test set, to evaluate robustness.\nContactCenter contains 48h of training and 6h of test stereo-audio/transcript data from contact centre conversations between agents and customers. We upsampled audio from 8 kHz to 16 kHz to align with the SLAM-ASR model's requirements."}, {"title": "B. Training and technical details", "content": "For the experiments on LibriSpeech, we use the publicly available checkpoint from SLAM-ASR while we trained the projectors on CallHome and ContactCenter datasets. To ensure that our results are reproducible and comparable to SLAM-ASR results, in all our experiments, we follow setup from the original work [10]: the projector was trained for 3 epochs, with early stopping based on cross entropy loss on dev set, using AdamW [19] with a learning rate \u03b3 = 1e-4 and a batch size of 4; the output of the speech encoder is at 50 Hz and the downsampling rate is set to k = 5, leading to the down-sampled audio features $Z_i$ being at 10 Hz, or equivalently, 10 speech token embeddings $E_i$ per second; the projector hidden layer dimension is set to 2048 and beam search (beam size = 4) is used for decoding."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "Since the projector is the only trainable component in SLAM-ASR, we hypothesize it may have a tendency to overfit the domain seen during training. This could limit its generalization power towards unseen domains or tasks when compared to standard ASR models, as highlighted in [20]. Thus, the goal of this experiment is to evaluate the robustness of SLAM-ASR under cross-domain scenarios to test our hypothesis. More precisely, we assess the performance degradation of SLAM-ASR when trained on one dataset but evaluated on different datasets (Section III-A). We compare this degradation to the performance drop observed in its standard ASR counterpart under the same conditions.\nFrom the results, shown in Table I, it is clear that SLAM-ASR shows significantly greater performance degradation than its ASR counterpart when decoding audio from datasets different from those used in training, consistently across all datasets. Additionally, SLAM-ASR exhibits exceptionally high WER (denoted by \u221e) for certain dataset combinations. Upon manual analysis, we identified that this behavior was primarily due to a substantial increase in insertion errors, caused by LLM hallucinations spiraling out of control."}, {"title": "B. Speech perturbation ablations", "content": "In the previous subsection, SLAM-ASR demonstrated limited cross-dataset generalization capabilities. This observation suggests that the model's claimed strong speech recognition performance may be partially due to its reliance on dataset-specific acoustic and speaker characteristics, which it exploits as shortcuts to map audio features $Z_i$ to speech token embeddings $E_i$. To further investigate this hypothesis, we designed a series of ablation experiments incorporating various speech perturbation techniques applied to the original evaluation audio. Specifically, we evaluate how different levels of perturbations impact the performance of SLAM-ASR. For these experiments, we used the SLAM-ASR model trained on LibriSpeech, enabling a com-parison of the results reported in the original SLAM-ASR paper [10] with those obtained in this section, when the test waveforms are altered.\n1) Tempo perturbation: In SLAM-ASR, the projector must learn to map each audio feature $Z_i$ to the LLM's input space, specifically the (sub)word embedding space. We hypothesize that the learned mapping could be influenced by the speaking rates present in the training data. Intuitively, the average speaking rate affects the average number of frames into which words are split and, consequently, the average number of audio feature $Z_i$s to be mapped to the words' corresponding $E_i$s. Therefore, we assess the robustness of SLAM-ASR to time-scale modifications on the test set. In particular, the time-scale was modified by simply manipulating the hop length using the well-established PSOLA method [21], [22] which modifies the prosody of natural speech while retaining a high level of naturalness. Figure 2a shows the change in WER performance across time-scale ratios ranging from 0.5 to 1.5, with increments of 0.1. As the speaking pace increases (i.e., ratio > 1), both models exhibit a similar decline in performance. However, when the pace slows down (i.e., ratio < 1), SLAM-ASR's WER rises sharply, reaching nearly 12% at a ratio of 0.5 (half the normal speed), in contrast to its ASR counterpart. These results suggest that SLAM-ASR struggles to handle an increase in the number of frames that must be mapped to the same number of (sub)words. This phenomenon may occur because a greater mismatch between the number of frames and the LLM's input wordpieces makes the mapping from Z to the corresponding E significantly more challenging."}, {"title": "C. Speech-to-text alignment analysis", "content": "Previous research has suggested that the LLM-based ASR task can be viewed as a regurgitation activity, where the language model is responsible for refining and reproducing information in the same order as it appears in the audio encoder's output sequence [6]. Thus, if the projector in SLAM-ASR can provide a sequence of embeddings monotonically aligned with the text embeddings, the LLM-based ASR problem reduces to a repetition task, which should not require the full capacity of an LLM.\nNevertheless, a key aspect of the SLAM-ASR architecture is that both the speech encoder and the LLM are frozen, which contrasts with the current trend in LLM-based ASR research [6]\u2013[10], where adapters (e.g., LoRA [13]) are used to fine-tune the LLM. Hence, we hypothesize that SLAM-ASR will face more difficulties in learning the alignment of speech tokens and text tokens compared to methods that also fine-tune the LLM. To validate our hypothesis, we conducted an additional experiment on the ContactCenter dataset using the LORA adapter to fine-tune the LLM alongside training the projector of SLAM-ASR. We then computed the cosine similarity between each possible pair of speech tokens and text tokens embedding of the LLM. Figure 4 shows the resultant alignment plots for two randomly selected samples. As can be inferred from the similarity plots, the task of aligning audio to text is harder for the original SLAM-ASR setup (left-side plots), i.e., without LoRA.\nAdditionally, to assess what the projector is learning, we mapped the learned speech tokens to their closest token from the LLM's vocabulary. The results of this exploration are shown in Figure 5. Note that the output from SLAM-ASR is mostly gibberish, while the tokens retrieved by SLAM-ASR+LORA are much more aligned with the reference text. Overall, the results from this ablation raise the question of whether freezing the LLM is advisable, given the clear advantage of using LoRA for better alignments and the improvement in performance for in-domain and cross-domain scenarios in terms of WER."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we address the question of what is and what is not advisable when working with a recent, widely adopted LLM-based ASR solution, i.e., SLAM-ASR. By ablation, we were able to identify the good, the bad, and the ugly aspects of this ASR paradigm in which both the speech foundation model and the LLM are frozen, and connected through a trainable linear projector.\nThe Good: SLAM-ASR is efficient, both in computation and data usage, while delivering competitive in-domain performance. Its flexibility allows easy integration of various speech encoders and LLMs, the only trainable component being a small linear projector, a great advantage when computational resources are scarce.\nThe Bad: Like many other LLM-based ASR systems, SLAM-ASR tends to strongly overfit the training domain. This is a clear disadvantage compared to traditional ASR models, which have better generalization capabilities and are less sensitive to domain shifts and speech perturbation. Our experiments show that SLAM-ASR can easily go off the rails and hallucinate (e.g., repetitions or unrelated text) when simple perturbations are applied to the speech signal.\nThe Ugly: Unlike LLM-based ASR approaches that use LoRA for fine-tuning the LLM, the SLAM-ASR architecture lacks clear evidence that the projector is learning an alignment between speech and text, rather than some other spurious correlation. Our analysis shows that the alignment is less evident without LoRA during training, resulting in gibberish output when speech tokens are mapped to text tokens.\nThe Way Forward: Overall, our ablations suggest that SLAM-ASR should be trained and used with in-domain data for inference. However, if the data is very noisy, such as in CallHome, a traditional ASR model is the better choice. Similarly, our experimental analysis suggests that LoRA adapters should be considered for improving the alignment of speech and text tokens, as well as for better performance of SLAM-ASR in both in-domain and cross-domain scenarios."}]}