{"title": "Identifying High Consideration E-Commerce Search Queries", "authors": ["Zhiyu Chen", "Jason Choi", "Besnik Fetahu", "Shervin Malmasi"], "abstract": "In e-commerce, high consideration search missions typically require careful and elaborate decision making, and involve a substantial research investment from customers. We consider the task of automatically identifying such High Consideration (HC) queries. Detecting such missions or searches enables e-commerce sites to better serve user needs through targeted experiences such as curated QA widgets that help users reach purchase decisions. We explore the task by proposing an Engagement-based Query Ranking (EQR) approach, focusing on query ranking to indicate potential engagement levels with query-related shopping knowledge content during product search. Unlike previous studies on predicting trends, EQR prioritizes query-level features related to customer behavior, financial indicators, and catalog information rather than popularity signals. We introduce an accurate and scalable method for EQR and present experimental results demonstrating its effectiveness. Offline experiments show strong ranking performance. Human evaluation shows a precision of 96% for HC queries identified by our model. The model was commercially deployed, and shown to outperform human-selected queries in terms of downstream customer impact, as measured through engagement.", "sections": [{"title": "Introduction", "content": "The integration of information content in online shopping is increasingly gaining importance (Vedula et al., 2024; Kuzi and Malmasi, 2024), but such content may be more useful for certain search missions than others. The effective identification of specific subsets of keywords (Zhao et al., 2019; Yuan et al., 2022; Ryali et al., 2023) is essential not only for driving organic traffic and revenue in e-commerce, but also for enhancing the overall customer experience. Related to the tasks of keyword selection and targeting (Shu et al., 2020; Zheng et al., 2020), creating or serving customized content for specific queries helps provide relevant content for the right population. For example, when customers search for \"prime day deals\" on Amazon, a dedicated widget will appear above product search results. Clicking on this widget will direct customers to a specialized page with customized information, thus enhancing their shopping experience for specific deals. Identifying such queries is usually the initial step prior to content creation and targeting. Rather than directing customers to a dedicated web page, such targeted content could also be co-displayed with product search results when customers are looking for products. For example, a Question-Answer (QA) widget with relevant shopping knowledge could be rendered to match the customer's query, as shown in Figure 1a.\nSuch tailored content is generally most useful for purchases requiring exploration, comparison, and deep decision making (Sondhi et al., 2018). We refer to such searches as High-Consideration (HC) Queries since consumers require additional information to consider their decision, or refine their search (Branco et al., 2012). However, curating customized content is an expensive manual process, and not feasible for all shopping queries as the query space is in the hundreds of millions. In Figure 1b, we illustrate the end-to-end process of creating curated content. Initially, HC queries are selected, typically through a manual process guided by heuristics. This often involves human annotators reviewing the top queries ranked by search frequency and identifying potential HC queries based on subjective criteria. Next, customized content is manually curated for each selected query. To serve a QA widget, corresponding question-answer pairs are then created. Finally, these customized QA pairs are ingested into a database and retrieved for specific queries (Chen et al., 2023) as in Figure 1a. Manual keyword selection by humans is the most straightforward approach. However, even with frequency-ranked query lists this is an expensive and low-yield process as most queries are judged to be of low consideration (e.g., consumables and minor purchases). Therefore, a lot of human efforts can be wasted using the conventional method. To address this, our work focuses on the task of automatically identifying the small subset of such HC queries in this large space. Identifying the most valuable queries (i.e., step 1 in Figure 1b) is crucial for maximizing the Return on Investment (ROI) of human efforts dedicated to content curation (i.e., step 2 in Figure 1b). As discussed later, the cost associated with content creation (and other factors), is an important consideration in framing this as a ranking task rather than a classification task.\nWe hypothesize that HC queries can be identified by a combination of behavioral cues, financial signals, and catalog features. To identify HC queries, we propose the novel task of Engagement-based Query Ranking (EQR) to train a model leveraging these signals. To learn this ranking function, our approach relies on engagement with informational shopping content in search results (e.g. a QA element) as a proxy target for HC query labels (which are expensive to define). This engagement can be measured as the Click-Through Rate (CTR) of the content displayed for a set of seed queries. As we will show, these targets can then be used to create a generalizable model to predict HC queries across all search traffic. This approach also allows for continuous learning by using engagement from new content created for queries selected by our model.\nOur key contributions are summarized below:\n\u2022 We introduce a novel task called Engagement-based Query Ranking (EQR), aimed at ranking HC queries based on their engagement metrics.\n\u2022 We propose a simple and effective method for EQR, which could effective identify novel queries that may result in prospective future engagement.\n\u2022 Our offline experiments show our proposed method outperforms all baselines for EQR across all metrics. And a human evaluation measured the model's precision at 96% in terms of HC queries selection.\n\u2022 Finally, commercial deployment of the model showed that the downstream customer impact from its selected HC queries is higher than those selected by human annotators."}, {"title": "Related Work", "content": "Query Understanding in E-commerce Query understanding is important to optimize search results for e-commerce platforms. To improve the relevance of search results while preserving the recall, embedding-based methods (Lin et al., 2018) have been proposed to map a query into a target product category. The first empirical study on e-commerce queries was conducted by Sondhi et al. (2018) and they categorized e-commerce queries into five categories based on different search behaviors. Chen et al. (2023) introduced an intent classifier to determine whether to display an FAQ entry for a given query. Our work can be considered as an extension to Chen et al. (2023) and focuses on identifying new queries where customers could benefit from the associated content from a QA component. Once those queries are identified, we could expand the QA database accordingly to increase its coverage.\nQuery Performance Prediction In information retrieval, the task of query performance prediction (QPP) (Carmel and Kurland, 2012) aims to predict the effectiveness of a query given a retrieval system without using human-labeled relevance judgments. QPP methods can generally be categorized into pre-retrieval and post-retrieval methods (Hauff et al., 2008). Pre-retrieval methods are designed to estimate query performance before the retrieval stage is reached, and they utilize various features such as query term characteristics and collection statistics to make their predictions (Mothe and Tanguy, 2005). On the other hand, post-retrieval techniques (Cronen-Townsend et al., 2002; Roitman and Kurland, 2019) focus on deriving predictions from the ranked list of results obtained through the retrieval process. Unlike pre-retrieval methods, post-retrieval predictors have access to the actual search outcomes, which can provide valuable additional information for analysis. For example, Query Clarity (Cronen-Townsend et al., 2002) evaluates the quality of search results by measuring the KL divergence between language models derived from the search results and those from the corpus. For the first time, Kumar et al. (2018) performed query performance prediction in e-commerce domain.\nOur method for identifying high consideration queries shares commonalities with QPP methods, which provide insights for a query without relying on human judgments. Instead of focusing on retrieval quality for a given query, we care about the downstream business impact (e.g., click-through rate) of curated content for a selected query. We also model the task as a regression task to predict a target measure of queries (Zamani et al., 2018; Hashemi et al., 2019; Arabzadeh et al., 2021; Khodabakhsh and Bagheri, 2023).\nTrending Queries Detection Our work is also related to detection of trending queries. Giummol\u00e8 et al. (2013) analyzed on real data and showed that a topic trending on Twitter may subsequently emerge as a popular search query on Google. Lee et al. (2014) proposed to predict trending queries with a classifier trained on features derived from the historical frequencies of queries. More recently, trending prediction has also been explored in e-commerce scenario. Yuan et al. (2022) introduced a method to mine fashion trends represented by product attributes on e-commerce platforms. TrendSpotter was proposed by Ryali et al. (2023) to forecast trending products. Different from prior work, our goal is to identify high-consideration queries that could encourage user engagement with related content, rather than focusing on the queries themselves. A trending query may not necessarily fall into the high-consideration category due to specific business considerations. Additionally, our method does not rely on surface features of queries and we aim to discover new HC queries."}, {"title": "Method", "content": "Since we aim to predict a subset of queries according to our criteria, this task could be framed as either classification or ranking. However, the classification approach is overly simplistic, and we model the task of HC queries identification as a ranking task instead of classification task for the following considerations.\nShortcomings of Binary Classification Ideally, we want to create tailored content for every targeted query. However, not all queries will have content engagement. User needs for informational content are subjective and depend on factors such as knowledge level. By taking a ranking approach we can identify HC queries with higher engagement and prioritize them, which is an important aspect given the constrained human resources required for content creation and validation. A simple classification approach would not allow us to maximize the impact of our efforts in production by addressing the most promising queries first. Additionally, using a classification approach would require additional steps for threshold selection, as well as ensuring that probability outputs from the model are well calibrated, both of which can be avoided with a ranking approach.\nRanking Approach Since our primary objective is scoring queries independently, rather than relative ranking, we choose to model our task as a pointwise ranking problem, similar to the work on query performance prediction (Zamani et al., 2018; Hashemi et al., 2019; Arabzadeh et al., 2021; Khodabakhsh and Bagheri, 2023; Meng et al., 2024). This allows us to train a model using data with absolute engagement scores, rather than collecting pairwise comparisons. A pointwise approach is also preferred from a model complexity perspective. It allows us to use standard regression approaches, rather than more sophisticated pairwise or listwise ranking models. This is preferred as the lower computational cost leads to faster training and inference. A pointwise approach is also more interpretable, and makes it easy to perform inference on new queries as they appear. Since it is trained on engagement data, it is also the most suitable for incremental or online learning, allowing the pointwise model to easily adapt to changing user behavior. Accordingly, a pointwise approach is the most scalable and practical solution."}, {"title": "Pointwise Ranking Model", "content": "Our goal is to create a supervised ranking model for selecting HC queries. We first define our proxy target, the query (q) level engagement score (e), as following:\n$e(q) = \\frac{freq_c(q)}{freq(q)}$\nwhere $freq_c(q)$ is the user click count for an informational component (e.g. QA widget like in Figure 1a) displayed in the search results, and $freq(q)$ is the total frequency for the query. Note that the definition of the engagement score can vary among different businesses. In this work, we consider the overall query-level engagement instead of content-level engagement like the CTR of an individual QA pair generated for a query. The scope of this paper is on the selection of HC queries and we leave the study of how to guide the content generation for optimizing CTR as future work.\nFor a query $q \\in Q$, we have its user interaction features $x \\in R^d$ where d is the total number of features. We aim to learn a function $f(\u00b7)$ that could predict the engagement score of a query given its features. Once we obtain the predicted engagement scores for a list of n queries $Q = {q_1, ..., q_i,..., q_n}$, then we re-rank them in descending order of their engagement scores.\nWe do not rely on query embeddings for several reasons. First, the query space is large, and differences between queries can be nuanced (airpods vs. airpods case), possibly leading to poor generalization. As we will demonstrate in Section 5, embedding-based methods perform inferior compared to our proposed method, which does not rely on query surface text features. Second, text encoders are slower in both training and inference. Finally, by using behavioral features our model is language agnostic without using a multilingual encoder."}, {"title": "Training Data Acquisition", "content": "In order to bootstrap the model training, a key requirement is to have a small but representative set of seed queries with engagement scores to train a model that can generalize to previously untargeted queries. In general this seed query set should be manually chosen by experts, and be stratified over product categories for coverage.\nNext, we summarize the features we used (Section 3.3) and then describe how we train the ranking model (Section 3.4)."}, {"title": "Query Features", "content": "We have three feature groups, and all features are listed in Table 1.\nBehavioral Features (B1-B8) characterize user interactions with a search system for a query. After a query is submitted, we observe subsequent interactions such as clicking on a search result, or adding a product to the cart. We hypothesize that how users interact with the results (e.g. number of item clicks, going deeper into the results, etc.) can help identify HC queries.\nFinancial Features (F1-F5) relate to the purchases associated with a query. We hypothesize that financial signals (e.g. order volumes, prices, temporal patterns) can help distinguish HC queries.\nCatalog Features (C1-C3) focus on features of the products served in the search results, as derived from the product catalog. Such features serve as feedback from the product search system and are inspired by post-retrieval methods for predicting query performance (Cronen-Townsend et al., 2002; Roitman and Kurland, 2019; Butman et al., 2013)."}, {"title": "Training", "content": "We approach the EQR task as pointwise regression and train Gradient Boosted Decision Trees (GBDT) to predict query engagement scores. During training we minimize the Mean Squared Error (MSE):\n$L = \\frac{1}{2} \\Sigma (f(x_i) - e(q_i))^2$\nwhere $x_i$ is the feature vector of query $q_i$. In this paper, we adopt the XGBoost (Chen and Guestrin, 2016) implementation to train our models."}, {"title": "Experimental Setup", "content": "Dataset As illustrated in Figure 1a, we collected the data associated with a QA component from Amazon spanning a one-year period. During this time, 11,273 queries were manually selected to trigger the QA component and display curated content. We obtained the corresponding query-level user engagement data as our proxy targets. The data was divided into training, validation, and test sets, with respective proportions of 70%, 15%, and 15%."}, {"title": "Evaluation Metrics", "content": "To evaluate the performance of different methods, we use the following evaluation metrics:\n\u2022 HIT@k: Considering k queries with the highest ground-truth engagement scores as positives, this is the ratio of positive queries in top-k predicted results by the model.\n\u2022 Kendall's Tau: An ordinal rank correlation coefficient for two lists (our predictions and ground truth). Values lie in [-1, 1], and larger values indicate greater similarity (Kendall, 1938). This metric is also commonly used in query performance prediction (Hauff and Azzopardi, 2009).\n\u2022 MSE: Mean Square Error between the ground-truth engagement and predictions. It is a more challenging metric as it requires capturing the precise engagement levels, which may fluctuate due to seasonal variations."}, {"title": "Baselines", "content": "We compare our GBDT ranker with the following methods.\n\u2022 Frequency: Queries are ranked by frequency. This baseline measures whether popularity is a predictor for detecting HC queries.\n\u2022 Regression methods: We use the features outlined in Section 3.3 for both Random Forest and linear models, which include Lasso, Ridge, Elastic Net, and linear regression.\n\u2022 ROBERTa: A 300M parameter encoder pre-trained on internal shopping data. We fine-tune this with our training data."}, {"title": "High Consideration Query Prediction", "content": "Decision Tree Ensembles yield best overall performance. The gradient-boosted XGboost ensemble achieves the best performance across all metrics, with Random Forest achieving similar results. These ensemble-based models outperform all other single models, due to robustness and ability to capture diverse patterns within the data. Among linear methods, Lasso regression exhibits the best performance, although the difference is not statistically significant when compared to Elastic Net. We observe that the Hit@k results of the frequency-based ranking baseline are notably poor. This verifies our claim in Section 1 that query frequency information does not necessarily correlate with customer consideration.\nWe achieve high recall for the top queries. When measuring recall of the top 500 HC queries with Hit@500, around 70% of them were identified by XGBoost and approximately 60% were identified by all other models trained with our proposed features. The robust performance across models highlights the reliability and consistency of our approach in accurately identifying HC queries.\nRanking over the entire test set is accurate. When measuring ranking performance on the entire test set with Kendall's Tau, XGBoost and Random Forest both show a strong correlation with the ground truth rankings. Rankings from the regression models show moderate correlations. We also observe that in general MSE follows a similar trend with Kendall's Tau. This is expected since both metrics consider the entire test set. However, when Kendall's Tau indicates low correlation, the MSE difference between two methods can be large (comparing zero-shot and few-shot GPT baselines).\nText-based methods underperform feature-based models. This is unsurprising given that those methods are typically pre-trained to capture semantic similarities in text. Even GPT-3.5 performs poorly in all metrics, with little or no improvement even with few-shot in-context learning. GPT-40 achieves better results than GPT-3.5 but still cannot beat all other feature-based models. The task of EQR requires predicting on unseen queries, which may lead to challenges in generalization when facing queries that are dissimilar in semantics to those encountered during (pre-)training. This shows that query semantics alone are not sufficient for this task; behavioral features provide stronger cues."}, {"title": "Ablation Study and Feature Importance", "content": "We conduct an ablation study on the feature groups from Section 3.3. Specifically, we examine how our best XGBoost model's performance changes when using each feature group independently. The results are shown in Table 2 (row 3 to row 5). We observe a significant drop in performance across metrics when utilizing only one group of features. This drop indicates that behavioral features are of the most importance, followed by financial features."}, {"title": "Human Evaluation", "content": "To validate the precision of our method in identifying HC queries, we used our model to make predictions on a large sample of unseen traffic in terms of our QA widget. We then created a set of 1,500 queries by selecting evenly from both the highest and lowest ranked queries in terms of predicted engagement scores. Expert annotators judged the shuffled set and classified each query as HC or not. The human labels were used as ground truth, and the model's top ranked queries were considered HC. We computed the precision of our model as 96%, indicating its ability to generalize to unseen queries and potentially replace human annotators."}, {"title": "Commercial Deployment", "content": "Our model has been deployed in a production setting to identify HC queries for more than one year. As an initial step towards commercial deployment, we performed a head-to-head comparison between queries chosen by human experts and our model. Each group selected 500 HC queries, for which we curated and deployed high-quality QA content. As our metric, real engagement metrics (eq. (1)) were measured over a 30-day period. Results showed that the model-chosen queries outperformed the human-selected set with a relative increase of 6%.\nHaving validated the precision of our predictions (\u00a75.3) and their downstream impact on customers, the model was moved to full commercial deployment. Removing the need for human annotators enables scaling HC query selection applications from an order of thousands to millions with relative ease. This, in turn, enables rapid model improvements by building an engagement-based feedback loop for the model to learn from its own predictions."}, {"title": "Discussion and Conclusion", "content": "We introduce the task of Engagement-based Query Ranking in order to select High Consideration queries. We proposed three categories of features to train pointwise rankers to address this task. Our experimental results show that our proposed method achieves better performance than the baselines. The human evaluation indicates that our method could serve as an effective tool to save resources spent on error-prone human annotations.\nOne limitation of our work is the difficulty in accurately measuring the true recall of our model. Future work could consider the combination of product category predictions for queries and conduct the selection of high consideration queries for each product category. Another limitation of our work is that we do not consider optimizing the curated content for selected queries and rely on human experts to decide what content to be create. Furthermore, we did not address the removal of nearly duplicated queries, which would require a separate processing pipeline. Future work could focus on personalized content selection for HC queries, and leverage active learning techniques to optimize the content-level engagement metrics. Another promising direction to explore is the integration of behavioral and financial features, along with an innovative LLM-based optimization approach (Senel et al., 2024) to improve overall performance."}, {"title": "Appendix", "content": "The only difference between zero-shot and few-shot prompts are the presence of 20 additional pairs of (query, eq) few-shot example. However due to legal and privacy reasons, we only include our zero-shot prompt below for generating engagement scores with reasons."}]}