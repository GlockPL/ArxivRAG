{"title": "When End-to-End is Overkill: Rethinking Cascaded Speech-to-Text Translation", "authors": ["Anna Min", "Chenxu Hu", "Yi Ren", "Hang Zhao"], "abstract": "Though end-to-end speech-to-text translation has been a great success, we argue that the cascaded speech-to-text translation model still has its place, which is usually criticized for the error propagation between automatic speech recognition (ASR) and machine translation (MT) models. In this paper, we explore the benefits of incorporating multiple candidates from ASR and self-supervised speech features into MT. Our analysis reveals that the primary cause of cascading errors stems from the increased divergence between similar samples in the speech domain when mapped to the text domain. By including multiple candidates and self-supervised speech features, our approach allows the machine translation model to choose the right words and ensure precise translation using various speech samples. This strategy minimizes error spread and takes advantage of large ASR and MT datasets, along with pre-trained ASR/MT models, while addressing associated issues.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the academic community has been intrigued by the rapid advancement of end-to-end speech-to-text translation models [1]. These efficient encoder-decoder architectures provide a direct avenue for translating speech, bypassing the need for complex intermediate symbolic representations. However, the arduous task of assembling and curating end-to-end data poses a significant challenge, entailing considerable costs and extensive efforts. These end-to-end methods need the careful selection of high-quality data or argumentation [2]\u2013[6], encompassing both speech and translated transcripts, and the scrupulous exclusion of erroneous examples.\nNonetheless, cascaded speech-to-text translation models have encountered substantial criticism due to an intrinsic shortcoming called \"cascaded loss\" or \"error propagation\". Studies on ASR+MT systems have explored various meth-ods to enhance the integration of ASR output lattices into MT models [7]-[9]. To mitigate error propagation, several approaches [10]\u2013[17] have been proposed to integrate ASR and MT models for end-to-end models, which necessitate the addition of supplementary modules and substantial additional training. In contrast, the method proposed in our research employs an n-best strategy that does not require additional parameters and can significantly enhance performance with minimal fine-tuning.\nRecent studies [18], [19] have demonstrated the perfor-mance improvements achieved by scaling up pre-trained mod-els for downstream natural language processing tasks. To fully exploit the potential of pre-trained MT and ASR models, we present a novel perspective on error propagation and the preservation of essential speech information. We propose the idea of utilizing multiple ASR candidates for machine transla-tion, integrated with self-supervised speech representations, to enhance the accuracy of the translation. Our comprehensive analysis reveals the primary causes of error propagation in cascaded systems, which originate from the misalignment be-tween the acoustic and semantic dimensions of speech. Factors such as homophones with different meanings and word elisions contribute to inaccuracies in ASR results, which consequently propagate to the machine translation model. Furthermore, we explore the use of self-supervised language representations to preserve fine-grained linguistic information in speech.\nOur model has the following advantages:\n1) Our model achieves the best performance among cas-caded models in the Speech-to-Text(S2T) translation task.\n2) Our method can leverage variously known ASR and ma-chine translation (MT) pre-trained models. Specifically, it can effortlessly adapt to different model architectures"}, {"title": "II. ANALYSIS", "content": "To see where the error propagation lies in the cascaded system and how ASR errors propagate to MT, we pose two questions: 1) Can the top-ranked ASR candidate cover all the lexicons? 2) Is the top-ranked ASR candidate always the best translation result?\nPreliminary experiments\nIn the cascaded system, we extract the top 20 results based on the scores from the ASR system and then select the top n candidates. We calculate the lexical overlap between these candidates {C1,C2...Cn} and the ground truth text {gt} of the ASR, which means the length of set of all words in candidates {w/w \u2208 Ck,1 \u2264 k \u2264 n} intersected with {w/w \u2208 gt} divided by the length of the latter. In Table 1, \"average\" refers to the average lexical overlap between each candidate and the ground truth. At the same time \"cumulative\" represents the lexical overlap when considering a combination of n candidates with the ground truth. We observe that as n increases, although the average lexical overlap decreases, the cumulative overlap improves. This indicates that apart from the top-ranked candidate, the ASR system fails to include some vocabulary, which remains in the lower-ranked candidates.\nTHE PROPORTION OF THE ASR RESULT INDEX OF THE BEST CANDIDATE\nBASED ON THE BLEU SCORE OF THE TRANSLATION RESULT\nbe a point where cascaded errors occur. Therefore, we delve deeper into analyzing these cascaded errors and make the hypothesis that the source of cascaded loss mainly arises from discrepancies between the pronunciation space and semantic space and that ASR models encounter difficulties in selecting results that most accurately match the language patterns.\nASR is trained using paired speech and text data, but the language patterns it captures are not as rich as those in translation models. Therefore, when the recognition results are similar, it is difficult for the ASR model to select the candidate that best aligns with human common sense and grammar based on scores alone.\nAs an example, consider the phrase \"has put the race on the top.\" The highest-scoring candidate in ASR recognizes \"race\" as \"rays,\" which contradicts common sense. However, subsequent candidates include various results with similar pronunciations, such as \"race,\u201d \u201craised,\" and \u201craise.\" An-other example is \"Recording the transaction in an immutable distributed ledger,\" while the highest-scoring result in ASR is \"Recording the transaction in an immutable distributed lecture,\" which is not a common expression. The subsequent candidates include \"legend,\u201d \u201cliterature,\u201d \u201cletter,\u201d \u201cledger,\" and other results.\""}, {"title": "III. METHOD", "content": "To leverage the powerful capability of machine translation in capturing semantic patterns, we propose utilizing multi-candidate ASR inputs and averaging attention computation in the MT model. Furthermore, to address the issue of error propagation in ASR caused by homophones, we employ self-supervised speech representations to enhance accuracy. The combination of methods we propose is represented by the model depicted in Figure 2."}, {"title": "A. Multi-candidate from ASR", "content": "The correspondence between the speech domain and the semantic domain is not always perfect. However, compared to machine translation models, ASR models have limited ability to capture semantic patterns effectively. Consequently, ASR is prone to mapping speech samples that are acoustically similar to semantically distant outputs. Moreover, ASR cannot selectively choose samples that align more closely with human language conventions and patterns. For a given speech input, the correct vocabulary might be dispersed among multiple can-didates generated during beam search. Unfortunately, previous cascaded systems only consider the top-scoring candidate and pass it to the machine translation model, resulting in error propagation within the cascaded system. Our cascaded model first uses Wenet [20] to perform ASR on GigaST [21]; we store the top 20 ASR result sentences based on the cumulative log probability value from the end of beam search.\n1) Aligned by common substrings: We follow the following steps to align the candidates:\n1) The top n ranked texts \\(t_k\\(1 \\le k \\le n)\\) consisting of nk words (W1, W2....Wnk) are selected (in our following experiments, we set n to be 5).\n2) The dynamic programming algorithm for finding the longest common substrings [22] is used for n-1 times, in the preprocessing stage before putting into the model. There are n - 1 processes in total. We denote \\(t^{n+1}\\) as the aligned result of ti after the mth\\(1 \\le m \\le n - 1)\\) process. t\u00b9 is equal to t\u2081.\n3) During the mth process, (1) we calculate the longest common subsequences of {t, tm+1}, and get {tm+1, tm+1} by connecting the substrings and the largest length of uncommon substrings. (2) The common substrings are aligned, and the remaining parts are padded with \"unk\" tokens. (3) When 2 < m \u2264 n \u2212 1, the {t...tm+1} are padded with \u201cunk\u201d tokens at the same indexes where {tr} is padded to {tm+1}. Then, one process ends.\n4) After n - 1 processes, the aligned and padded texts of the same length are used as input to the attention-based machine translation model.\n2) Average attention among candidates: Thus, we pro-pose an innovative approach, incorporating multiple ASR candidates into a single attention-based machine translation"}, {"title": "B. Acoustic and linguistic features fusion", "content": "The process of converting audio recognition to text and then to machine translation further results in the loss of a large amount of acoustic information in speech. We propose a multi-candidate self-supervised learning speech machine translation model(shown in Figure 2) to enhance the accuracy of filtering correct results by leveraging language patterns from machine translation and fusing acoustic and linguistic features, following the approach utilized in previous works [23] and utilizing HuBERT [24] for generating target self-supervised discrete units. This choice was influenced by the superior performance demonstrated by HuBERT in various tasks such as ASR, spoken language modeling, and speech synthesis, as shown in by [25]-[27]. It outperforms other unsupervised rep-resentations, including VAE-based representations employed in [28], [29].\nWe follow [23] to use the 11th layer of HuBERT as input, which contains richer linguistic information, and transform them into tokens with a word list of 1000 for the number of speech units using the K-means model trained on English speech. Due to the high length of the original unit, we reduced"}, {"title": "IV. EXPERIMENTS & RESULTS", "content": "For ST datasets, we use GigaST [21], a large-scale pseudo speech translation (ST) corpus containing 7.5M en-zh pairs. It is created by translating the text in GigaSpeech [30], an English ASR corpus, into German and Chinese. The training set is translated by a robust machine translation system, and the test set is translated by humans."}, {"title": "B. Model setup", "content": "1) Cascaded system: Our cascaded model first uses Wenet [20] to perform ASR on Gigaspeech with 7.4M valid en-zh pairs, and we store the speech recognition texts cor-responding to the top 20 cumulative log probability value rankings at the end of beam search. The top n ranked texts are input as encoders into the mBART-based machine translation model. We do not use any self-supervised speech models to initialize the ASR encoder. The encoder is trained from scratch following the WeNet paper. Taking inspiration from the approach proposed by [23], We use the multilingual HuBERT (mHuBERT) model and K-means model to encode source speech into a vocabulary of 1000 units. The mHuBERT and the K-means models are learned from the combination of English, Spanish, and French unlabeled speech data from VoxPopuli [31], while we use them to encode English speech only.\n2) Multi-candidate mBART: The multi-candidate mBART is built upon the mBART model [32]. We fine-tune the mBART model on 8 A100 GPUs. Our model configuration follows\u00b9to use the same parameters for fine-tuning the mBART model.\nOn the GigaST dataset, we compare our method with the original cascaded system and two end-to-end ST systems, including SSL-Transformer and Speech-Transformer [21]."}, {"title": "C. Results", "content": "1) Comparison with baselines: Comparing settings (6) and (4), it shows that the Alignment and Multi-candidate methods enhance the performance of the conventional cascaded system in setting (4). When comparing settings (7), (6), and (3), it shows that fusing acoustic and linguistic features improves the performance of the conventional cascaded system in setting (4), making it comparable to the end-to-end model.\nMoreover, while the transition from high-quality machine translation models to improved multi-candidate translation\nmodels requires no parameter changes, in the era of emerging and popular large language models, it is evident that our approach holds more excellent practical value and prospects than the End-to-End model. Furthermore, our model achieves a BLEU score of 37.3 after just one epoch of training, requiring only one hour on our settings. With a concise duration of fine-tuning, it surpasses the performance of former cascaded models, demonstrating the effectiveness of our approach.\n2) Ablation: Comparing settings (5) and (6) shows that the Alignment method is crucial to make the multi-candidate method effective. Without lexical alignment, the sentence-level Multi-candidate average attention struggles to select the correct candidate words.\n3) Case study: By employing the multi-candidate strategy, we can observe that among the top five candidates based on their scores, there exist samples that deviate from conventional human language expressions. Nevertheless, in the attention mechanism of the machine translation process, words that align more closely with human expression and convey correct semantic meaning receive greater attention. As for the example of \"Where the Golgi apparatus, sometimes called the Golgi body, receives them.\" The first candidate from ASR misrecog-nizes the pronunciation of the \u201cgolgi apparatus\u201d as two non-existent words, \u201cgolgy\u201d and \u201cgolji\u201d. However, the candidate with a BLEU score of 100 is ranked fifth. The machine translation model leverages rich text patterns through multi-candidates, allocates more attention to the correct candidate \"Golgi apparatus\" and effortlessly selects the proper trans-lation, regarding the example of \"Recording the transaction in an immutable distributed ledger\" while the subsequent candidates include \"legend\u201d, \u201cliterature\u201d, \u201cletter", "ledger": "nd other alternatives."}, {"title": "V. CONCLUSION", "content": "Our analysis pinpoints factors contributing to error propaga-tion in cascaded systems, such as pronunciation disparities and semantic differences. Our multi-candidate approach notably enhances speech-to-text (S2T) translation, bridging the S2T-T2T gap without altering model parameters. With enhanced ASR and MT resources, our multi-candidate method narrows the S2T-T2T divide, providing increased accuracy and effi-ciency, all without additional parameters or modules."}]}