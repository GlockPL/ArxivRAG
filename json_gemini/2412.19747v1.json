{"title": "Enhancing Adversarial Robustness of Deep Neural Networks Through Supervised Contrastive Learning", "authors": ["Longwei Wang", "Navid Nayyem", "Abdullah Rakin"], "abstract": "Adversarial attacks exploit the vulnerabilities of convolutional neural networks by introducing imperceptible perturbations that lead to misclassifications, exposing weaknesses in feature representations and decision boundaries. This paper presents a novel framework combining supervised contrastive learning and margin-based contrastive loss to enhance adversarial robustness. Supervised contrastive learning improves the structure of the feature space by clustering embeddings of samples within the same class and separating those from different classes. Margin-based contrastive loss, inspired by support vector machines, enforces explicit constraints to create robust decision boundaries with well-defined margins. Experiments on the CIFAR-100 dataset with a ResNet-18 backbone demonstrate robustness performance improvements in adversarial accuracy under Fast Gradient Sign Method attacks.", "sections": [{"title": "I. INTRODUCTION", "content": "Convolutional Neural Networks have revolutionized the field of computer vision by achieving remarkable performance on tasks such as image classification, object detection, semantic segmentation, and more. Their ability to learn hierarchical feature representations from data has been instrumental in solving complex problems that were once considered intractable. However, alongside their success, CNNs have been found to exhibit a critical vulnerability: they are highly sensitive to adversarial attacks. Adversarial attacks are small, imperceptible perturbations added to input data that can lead to drastic changes in model predictions. For instance, a correctly classified image of a bird can be misclassified as an airplane with the addition of a carefully crafted perturbation. This phenomenon raises serious concerns about the reliability and robustness of CNNs, particularly in real-world, safety-critical applications such as autonomous driving, medical imaging, and security systems.\nA primary cause of this adversarial vulnerability lies in the feature extraction process of CNNs. During training, the convolutional kernels in CNNs often fail to learn robust and meaningful features. Instead, they tend to rely on spurious patterns or shortcuts present in the training data. These fragile patterns can easily be manipulated by adversarial perturbations, causing the model to misclassify inputs. This inadequacy stems from the standard training objective, which typically optimizes for task-specific accuracy (e.g., cross-entropy loss) without explicitly encouraging the model to learn invariant and robust feature representations. As a result, the learned kernels are insufficiently generalized and prone to failure under even minor deviations from the training distribution.\nNumerous approaches have been proposed to improve the adversarial robustness of CNNs. These include:\n\u2022 Adversarial Training: Incorporating adversarial examples during training to make the model robust against specific types of attacks. While effective, this method is computationally expensive and often fails to generalize to unseen attacks.\n\u2022 Regularization Techniques: Methods such as weight decay, dropout, and input noise injection aim to improve generalization. However, these are often insufficient to counteract strong adversarial perturbations.\n\u2022 Defensive Distillation: A technique that modifies the model training process to reduce sensitivity to small input changes. While promising, it can be circumvented by advanced attack strategies.\n\u2022 Gradient Regularization: Penalizing large gradients in the loss function with respect to input can reduce sensitivity to perturbations but may also hinder model performance on clean data.\nDespite their contributions, these methods share common shortcomings. Most are tailored to specific types of adversarial attacks, making them less effective against new or unseen threats. Furthermore, they often fail to address the underlying issue: the extraction of robust and meaningful features by the CNN kernels.\nTo address these challenges, we propose a novel approach leveraging Supervised Contrastive Learning to improve the adversarial robustness of CNNs. Contrastive learning is a self-supervised learning paradigm that aims to learn feature representations by comparing similar (positive) and dissimilar (negative) samples. Supervised Contrastive Learning extends this framework by incorporating label information, allowing the model to learn class-specific relationships in the feature space. Specifically, it encourages samples from the same class to cluster tightly together while maintaining clear separation from samples of other classes. This explicit alignment of features makes the model's decision boundaries more robust to adversarial perturbations.\nOur approach combines the supervised contrastive loss with the traditional cross-entropy loss to achieve two objectives simultaneously:\n\u2022 Robust Feature Learning: The supervised contrastive"}, {"title": "II. RELATED WORK", "content": "The vulnerability of Convolutional Neural Networks (CNNs) to adversarial attacks has been a critical area of research since Szegedy et al. (2014) revealed that neural networks exhibit unexpected fragility under adversarial perturbations [1]. These perturbations, often imperceptible to humans, exploit the complex and non-linear decision boundaries of CNNs, resulting in misclassification of inputs.\nAdversarial Attacks: Goodfellow et al. (2015) introduced the Fast Gradient Sign Method (FGSM) as one of the earliest efficient methods to generate adversarial examples [2]. This was followed by stronger iterative attacks like Projected Gradient Descent (PGD) [3] and optimization-based methods such as the Carlini and Wagner (CW) attack [4]. These attacks highlighted how easily CNNs could be deceived, even under minimal perturbations.\nAdversarial Defenses: To counter adversarial attacks, researchers have explored multiple defense strategies:\n\u2022 Adversarial Training: This approach, formalized by Madry et al. (2018), involves training models on adversarially perturbed examples, effectively hardening them against specific attacks [3]. While adversarial training is widely regarded as one of the most robust defenses, it incurs significant computational overhead and often struggles to generalize across unseen attacks [5].\n\u2022 Gradient Regularization: Ross and Doshi-Velez (2018) proposed input gradient regularization as a way to smooth the model's decision boundaries and reduce sensitivity to input perturbations [6]. However, this method is limited in handling high-dimensional perturbations.\n\u2022 Certified Robustness: Approaches such as randomized smoothing [7] and interval bound propagation (IBP) [8] provide theoretical guarantees of robustness under certain perturbation bounds. Despite their theoretical appeal, these methods often struggle with scalability and general applicability.\n\u2022 Feature Space Regularization: Zhang et al. (2019) introduced TRADES, which explicitly balances robustness and accuracy by penalizing feature space discrepancies during adversarial training [9]. Such methods have gained attention for providing a principled way to address robustness without overly compromising clean accuracy.\nWhile these methods represent significant progress, they often rely on either computationally expensive procedures or attack-specific adaptations. Furthermore, many fail to address the root cause of adversarial vulnerability: the inadequacy of CNN kernels in extracting robust, meaningful features that generalize well beyond clean data.\nContrastive learning has emerged as a cornerstone of self-supervised learning, offering a mechanism to learn high-quality representations without labeled data. The contrastive paradigm works by maximizing agreement between positive pairs (augmented views of the same sample) and minimizing agreement with negative pairs (different samples). SimCLR [10] and MoCo [13] have been pivotal in demonstrating the efficacy of contrastive learning for unsupervised representation learning.\nSupervised Contrastive Learning: Khosla et al. (2020) extended the contrastive framework to the supervised setting by incorporating label information to create positive and negative pairs. In supervised contrastive learning, embeddings of samples belonging to the same class are brought closer in the latent space, while embeddings of different classes are pushed apart [15]. This additional supervision significantly improves intra-class clustering and inter-class separation, making the learned representations more discriminative and robust.\nContrastive Learning for Adversarial Robustness: While contrastive learning has primarily been explored for unsupervised and semi-supervised learning, its potential for improving adversarial robustness is gaining attention. Jiang et al. (2020) proposed adversarial contrastive learning, which combines contrastive objectives with adversarial training [18]. However, the lack of label supervision in their approach limits the discriminative power of the learned features. Similarly, work by Fan et al. (2021) explored self-supervised adversarial learning but struggled to match the robustness of supervised methods [21]."}, {"title": "III. METHODOLOGY", "content": "Adversarial attacks exploit weaknesses in convolutional neural networks (CNNs) by introducing imperceptible perturbations that lead to incorrect predictions. These vulnerabilities arise primarily due to: (1) Weak feature representations, where CNNs rely on non-generalizable patterns susceptible to adversarial perturbations, and (2) poorly defined decision boundaries, allowing minor perturbations to push inputs across these boundaries.\nExisting defenses, such as adversarial training, address these issues but suffer from high computational cost and limited generalization to unseen attack types. In contrast, Supervised Contrastive Learning (SCL) provides a scalable and generalizable solution by structuring the feature space through alignment of embeddings. By clustering embeddings of samples from the same class and separating embeddings from different classes, SCL creates robust and well-separated decision boundaries. These structured embeddings enhance resilience to adversarial perturbations while maintaining computational efficiency, motivating its use for refining adversarial robustness.\nSupervised Contrastive Learning leverages label information to guide the structuring of the feature space. For a batch of samples ${x}_i$ with corresponding labels ${y}_i$, the Supervised Contrastive Loss is defined as:\n$$L_{SCL} = \\frac{1}{N} \\sum_{i=1}^{N} - \\frac{1}{|P(i)|} \\log \\frac{\\exp(\\text{sim}(z_i, z_p) / \\tau)}{\\sum_{a \\in A(i)} \\exp(\\text{sim}(z_i, z_a) / \\tau)},$$ \nwhere N is the batch size, $z_i = f_{proj}(f_{backbone}(x_i))$ represents the normalized embedding of the i-th sample obtained from the projection head applied to the backbone features, $P(i)$ contains positive samples (samples with the same label as $x_i$), and $A(i)$ includes all samples in the batch except $x_i$. The cosine similarity $\\text{sim}(z_i, z_j)$ is computed as:\n$$\\text{sim}(z_i, z_j) = \\frac{z_i \\cdot z_j}{\\|z_i\\| \\cdot \\|z_j\\|},$$ \nThe temperature hyperparameter $\\tau$ controls the sharpness of the similarity distribution. By minimizing $L_{SCL}$, the model aligns embeddings of positive samples while separating embeddings of negative samples, resulting in a structured feature space.\nTo refine a pre-trained baseline model, the supervised contrastive loss is combined with the cross-entropy loss:\n$$L_{refined} = \\alpha L_{SCL} + \\beta L_{CE},$$\nwhere $L_{CE}$ is the cross-entropy loss, and $\\alpha$ and $\\beta$ are weighting factors that balance the contributions of the contrastive and task-specific objectives. This combination ensures robust feature learning and strong classification performance.\nSupervised contrastive learning lacks explicit mechanisms to enforce well-defined boundaries between classes. To address this limitation, we propose the Margin-Based Contrastive Loss, inspired by the margin maximization principle in Support Vector Machines (SVMs). This loss introduces explicit constraints to enhance intra-class compactness and inter-class separation.\nThe margin-based contrastive loss is defined as:\n$$L_{Margin} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\frac{1}{|P(i)|} \\sum_{p \\in P(i)} \\max(0, m_p - \\text{sim}(z_i, z_p)) + \\frac{1}{|N(i)|} \\sum_{n \\in N(i)} \\max(0, \\text{sim}(z_i, z_n) - m_n) \\right],$$ \nwhere $P(i)$ and $N(i)$ represent the sets of positive and negative samples for anchor $i$, respectively, and $m_p > 0$ and $m_n > 0$ are the positive and negative margins.\nThe first term penalizes positive pairs with similarity scores below $m_p$, ensuring tight clustering within classes. The second term penalizes negative pairs with similarity scores above $m_n$,"}, {"title": "IV. EXPERIMENTS", "content": "This section provides a comprehensive evaluation of the proposed framework, which leverages supervised contrastive learning (SCL) and margin-based contrastive loss to enhance adversarial robustness. The evaluation includes training a baseline ResNet-18 model, refining it using SCL and margin-based constraints, and assessing its robustness under adversarial attacks. Detailed analyses of results and ablation studies are presented to highlight the contributions of each component.\nThe experiments are conducted on the CIFAR-100 dataset, a widely used benchmark for image classification tasks, consisting of 60,000 images across 100 classes, with 50,000 training samples and 10,000 test samples. Each image has a resolution of 32 \u00d7 32 pixels, and the dataset is split into non-overlapping training and test sets.\nData Preprocessing: The dataset is normalized to the range [-1,1] using the mean and standard deviation of the CIFAR-100 dataset. No data augmentation techniques are applied to maintain a focus on the proposed robustness enhancements.\nNetwork Architecture: ResNet-18 is employed as the baseline model due to its simplicity and effectiveness. The final fully connected layer is modified to output 100 logits, corresponding to the 100 classes. For the refined model, a two-layer projection head is added, producing 64-dimensional embeddings suitable for contrastive learning.\nTraining Configurations: The experiments are conducted in two stages:\n1. Model Training: The baseline ResNet-18 model is trained for 10 epochs using cross-entropy loss with a batch size of 128. The Adam optimizer is used with a learning rate of 0.001.\n2. Model Refinement: The trained baseline model is refined for 10 additional epochs using supervised contrastive learning combined with cross-entropy loss. The weighting factors for the combined loss are set to $\\alpha = 0.5$ and $\\beta = 0.5$.\nMargin-Based Refinement: For experiments incorporating margin-based constraints, the margin-based contrastive loss is"}, {"title": "V. CONCLUSION", "content": "The proposed framework combines supervised contrastive learning (SCL) with margin-based contrastive loss to enhance the adversarial robustness of convolutional neural networks (CNNs). By aligning embeddings within the same class and separating those from different classes, SCL creates a structured feature space, while margin-based constraints stabilize decision boundaries through explicit intra-class compactness and inter-class separation. Experiments on CIFAR-100 demonstrate that the proposed approach achieves performance increase in adversarial accuracy under FGSM attacks, without compromising clean data performance. These results establish the proposed framework as an efficient solution for robust deep learning, with potential for broader applications and integration with other adversarial defense mechanisms."}]}