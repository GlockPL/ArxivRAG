{"title": "PROREASON: MULTI-MODAL PROACTIVE REASONING WITH DECOUPLED EYESIGHT AND WISDOM", "authors": ["Jingqi Zhou", "Sheng Wang", "Jingwei Dong", "Lei Li", "Jiahui Gao", "Lingpeng Kong", "Chuan Wu"], "abstract": "Large vision-language models (LVLMs) have witnessed significant progress on visual understanding tasks. However, they often prioritize language knowledge over image information on visual reasoning tasks, incurring performance degradation. To tackle this issue, we first identify the drawbacks of existing solutions (i.e., insufficient and irrelevant visual descriptions, and limited multi-modal capacities). We then decompose visual reasoning process into two stages: visual perception (i.e., eyesight) and textual reasoning (i.e., wisdom), and introduce a novel visual reasoning framework named PROREASON. This framework features multi-run proactive perception and decoupled vision-reasoning capabilities. Briefly, given a multi-modal question, PROREASON iterates proactive information collection and reasoning until the answer can be concluded with necessary and sufficient visual descriptions. Notably, the disassociation of capabilities allows seamless integration of existing large language models (LLMs) to compensate for the reasoning deficits of LVLMs. Our extensive experiments demonstrate that PROREASON outperforms both existing multi-step reasoning frameworks and passive peer methods on a wide range of benchmarks for both open-source and closed-source models. In addition, with the assistance of LLMS, PROREASON achieves a performance improvement of up to 15% on MMMU benchmark. Our insights into existing solutions and the decoupled perspective for feasible integration of LLMs illuminate future research on visual reasoning techniques, especially LLM-assisted ones.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, large language models (LLMs) (Dubey et al., 2024; Team et al., 2023; Jiang et al., 2023) have experienced explosive growth in their capabilities, driving significant advancements across various fields (Shao et al., 2023; Guo et al., 2024; Shao et al., 2024). This progress has also sparked interest in developing large vision-language models (LVLMs) (Chen et al., 2024a; Bai et al., 2023), which, like LLaVA (Li et al., 2024b), have achieved remarkable performance in multi-modal understanding tasks. However, state-of-the-art (SOTA) LVLMs still struggle to integrate visual understanding with textual reasoning simultaneously due to inherent modality differences and deficient training data. For example, Ghosh et al. (2024) demonstrate that LVLMs often rely more on their prior language knowledge, neglecting visual information in multi-modal reasoning tasks such as visual chart understanding and math reasoning, resulting in performance degradation. Figure 2 illustrates a typical case of this issue, where the reasoning process remains irrelevant to the image.\nTo address the challenges, a promising solution is to extract visual information from images into textual form to assist LVLMs in reasoning (Yin et al., 2023b; Mitra et al., 2024). For example, Ghosh et al. (2024) instruct LVLMs to generate fine-grained captions to facilitate the subsequent reasoning process. However, these existing methods exhibit two primary limitations: (i) The visual extraction process is question-agnostic and reasoning-free, that the image description is not targeted for a given question, and reasoning is not involved to deduce extra information for better descriptions. This drawback, termed as \u201cpassive\", results in irrelevant or insufficient information, and subsequent"}, {"title": "2 PRELIMINARY OBSERVATIONS", "content": "We first analyze the behaviors of several existing methods on the MMMU (Yue et al., 2023) dataset, a challenging multi-modal benchmark requiring comprehensive college-level knowledge and fine-grained reasoning abilities. All experiments are conducted with three recent LVLMs for robustness: Llama3-LLAVA-NeXT-8B (Li et al., 2024a), LLAVA-OneVision-Qwen2-7B-OV (Li et al., 2024b) and Qwen-VL-Chat (Bai et al., 2023). Further details are provided in Sec. 4.1."}, {"title": "2.1 CHALLENGES IN VISUAL REASONING: LIMITATIONS OF LVLMS", "content": "Chain-of-Thought(CoT) (Wei et al., 2022) has been extensively verified to enhance the performance of LLMs. Here, we explore its impact on the reasoning performance of LVLMs. Similarly, we require the models to \"think step by step\" before generating final answers. As shown in Table 1, a counter-intuitive phenomenon is observed: compared to \"Direct\u201d answering method, the introduction of CoT consistently incurs slight performance degradation across all three models."}, {"title": "2.2 DRAWBACKS OF PASSIVE INFORMATION EXTRACTION", "content": "Passive visual reasoning techniques suffer insufficient and irrelevant visual information, despite mitigating the oversight of images by converting them into detailed captions, like Visual Description Grounded Decoding (VDGD) (Ghosh et al., 2024). To support this claim, we generate"}, {"title": "3 METHOD", "content": "As illustrated in Figure 1, PROREASON consists of five functionally distinct yet inter-cooperative sub-agents, along with a Memory component. The entire workflow comprises three steps: Action,"}, {"title": "3.1 ACTION STEP", "content": "Action step is the core of question-oriented visual information extraction, driven by three sub-agents: Dispatcher, Vision Expert, and Reasoning Expert. The Dispatcher orchestrates the workflow, selectively directing the Vision Expert to capture specific visual information, or instructing the Reasoning Expert to analyze known information to derive more. The responses from both the Vision Expert and the Reasoning Expert are stored in a textual Memory component.\nFormally, given an image \\(I\\) and its corresponding textual question \\(Q\\), the Dispatcher decides to consult the Vision Expert or Reasoning Expert, based on the analysis of \\(Q\\) and the known information in the Memory (if not empty). The Dispatcher then generates a query \\(q\\) for the chosen expert. If the Vision Expert is selected, it takes the image \\(I\\) and query \\(q\\) as input, and generates an answer \\(A_v\\), which is then stored in the Memory. When the Reasoning Expert is selected, it provides a response consisting of the reasoning process and the final answer \\(A_r\\), based on the query \\(q\\) and known information in the Memory, before only \\(A_r\\) is stored in Memory. Notably, the Memory component allows PROREASON to keep compact information, and avoids lengthy reasoning traces like CoT and ReAct, thereby suffering less from redundant information."}, {"title": "3.2 JUDGMENT STEP", "content": "Judgment step is conducted by a sub-agent called Referee, which evaluates whether the information stored in Memory is sufficient to answer the question \\(Q\\). The input to the Referee includes the question \\(Q\\) and available information in the Memory. If the Memory contains adequate information to answer the question \\(Q\\), the Referee outputs the identifier \u201cSOLVABLE\"; otherwise, it outputs \u201cUNSOLVABLE\". If the Referee's output is SOLVABLE, the workflow precedes to the Summary phase. Conversely, if the output is UNSOLVABLE, the Action phase is re-executed to gather more necessary information. This allows the Referee to collaborate closely with the three sub-agents in the Action step, enabling the framework to proactively acquire the necessary information and prevent omissions, thereby overcoming the drawbacks of passive methods."}, {"title": "3.3 SUMMARY STEP", "content": "The Summary step focuses on integrating the available information in the Memory, and providing the final answer to the question \\(Q\\). This step is mainly powered by a sub-agent called Summarizer. Once the Referee determines that the information in the Memory is sufficient to address the question \\(Q\\), and outputs the identifier SOLVABLE, the Summarizer will be called to draw a conclusion, based on the Memory information. This conclusion represents the answer to the question of the whole framework, and will be evaluated by the performance metrics."}, {"title": "3.4 ADVANTAGES OF PROREASON", "content": "Reduced information Mission or Redundancy. With the close collaboration of the Dispatcher, Vision Expert, Reasoning Expert, and Referee agents, PROREASON can proactively (i.e., question-orientedly and reasoning-involvedly) extract the necessary and sufficient visual details from images, effectively avoiding information omission or redundancy. Meanwhile, the Memory component retains only the image captions from the Vision Expert and the reasoning results from the Reasoning Expert, providing a compact textual descriptions. This contributes to minimize the interference of irrelevant information on the subsequent Summarizer.\nLLM-assisted Multi-modal Reasoning. In PROREASON, the complete multi-modal reasoning process is decomposed into visual perception and textual reasoning stages, each executed by separate agents. These agents are then effectively organized through a designate pipeline to complete the tasks. This decomposition-before-integration approach not only circumvents the inherent differences between modalities, but also allows visually irrelevant sub-agents to be performed by text-only LLMs. Consequently, the extensively verified powerful reasoning capabilities of LLMs can be seamlessly synergized to power multi-modal reasoning process for performance enhancement."}, {"title": "4 EXPERIMENTS", "content": "In this section, we first evaluate the performance of our PROREASON framework against recent baselines on multiple benchmarks, followed by an in-depth ablation analysis of different components."}, {"title": "4.1 GENERAL SETUP", "content": "Datasets. To comprehensively validate the performance of our framework, we conduct experiments across four benchmarks: Multi-modal Large Language Model Evaluation (MME) (Yin et al., 2023a), Massive Multi-discipline Multi-modal Understanding and Reasoning (MMMU) (Yue et al., 2023), MathVista (Wang et al., 2024), and HallusionBench (Liu et al., 2023a). All of them require visual reasoning capabilities to complete the tasks correctly, and are introduced briefly as follows:\n\u2022 MME is an inclusive benchmark that encompasses 14 subtasks, designed to evaluate perceptual and cognitive abilities. Due to our emphasis on visual reasoning, we select the cognition-relevant tasks, including Commonsense Reasoning, Numerical Calculation, Text Translation, and Code Reasoning.\n\u2022 MMMU evaluates multi-modal models with multidisciplinary tasks that require college-level domain-specific knowledge and detailed reasoning. It comprises 11,500 questions across 30 disciplines and 183 sub-fields, emphasizing advanced perception and domain-specific reasoning.\n\u2022 MathVista focuses on more challenging mathematical reasoning tasks that demand precise visual recognition and compositional reasoning. It includes 6,141 examples from 31 multimodal mathematics datasets.\n\u2022 HallusionBench evaluates models' ability to reason with images such as statistical charts, emphasizing nuanced visual understanding. It consists of 346 images paired with 1,129 questions, meticulously crafted by experts.\nBase Models. We employ GPT-40-mini, Llama3-LLAVA-NeXT-8B and Qwen2.5-72B-Instruct (Team, 2024) as our base models, due to their excellent performance, accessibility, and representativeness of model sources. As one of the most performant LVLMs, GPT-40-mini demonstrates significant advancements in visual reasoning capabilities, and provides cheap and fast API. In contrast, Llama3-LLaVA-NeXT-8B is a fully open-sourced LVLM developed by the LLaVA team based on the Llama-3-8B LLM (Dubey et al., 2024) and CLIP vision encoder (Radford et al., 2021). Qwen2.5-72B-Instruct is a robust LLM designed to deliver high-quality instruction following and handle complex tasks. It will be utilized for achieving LLM-assisted multi-modal reasoning.\nBaselines. Besides the most basic method where models are instructed to answer questions directly, we compare PROREASON with two categories of peer methods. To determine the benefits of proactive information extraction in PROREASON, we first consider two SOTA passive visual reasoning methods, VDGD (Ghosh et al., 2024) and CCoT (Mitra et al., 2024). Additionally, we choose two multi-step reasoning frameworks of LLMs, Chain of Thought (CoT) (Wei et al., 2022) and ReAct (Yao et al., 2022), to demonstrate the effects of directly migrating LLM solutions to LVLMs.\n\u2022 Direct. As indicated by the name, models are required to answer questions directly without dedicated prompts. This baseline is set to evaluate the initial performance of base models.\n\u2022 VDGD. With image caption prefixed to text instruction, VDGD selects tokens that deviate the least from the description using a formula based on Kullback-Leibler divergence.\n\u2022 CCoT. Given an image and the question, CCOT first generates a scene graph of the image with LVLMs, and then extracts the answer by prompting the LVLMs with the graph.\n\u2022 CoT. CoT is an advanced prompting method that encourages LLMs to break complex tasks down into a series of easy steps, which has been applied broadly and verified to boost the reasoning performance remarkably (Chu et al., 2023)."}, {"title": "4.2 MAIN RESULTS", "content": "PROREASON exhibits significant and consistent performance enhancement over baselines across all the benchmarks. As listed in Table 4, despite better performance than the direct method on MME dataset, VDGD and CCoT fail to demonstrate consistent improvements on the other datasets. In contrast, PROREASON consistently outperforms all the other baselines for both Llama3-LLAVA-NeXT-8B and GPT-40-mini model across all benchmarks, with a peak improvement as 13.2%, demonstrating the superiority and task robustness of PROREASON.\nProactive information acquisition surpasses SOTA passive methods, especially in complex visual reasoning tasks. Specifically, compared to MME, MathVista and HallusionBench present higher image complexity and question difficulty, and thus require stronger visual understanding and textual reasoning capabilities. This leads to performance degradation of passive methods (i.e., VDGD and CCoT), highlighting their limited applicability to complex visual reasoning tasks. In contrast, PROREASON achieves notable performance improvements, up to 5.1%, by proactively acquiring visual information from images rather than generating question-agnostic captions. This aligns with out previous obervations in Sec. 2.2 that passive methods introduce substantial information redundancy or omission, misleading subsequent reasoning processes.\nDecoupling the visual perception and textual reasoning capabilities of an LVLM outperforms their simultaneous inherent usage. As listed in Table 4, when both capabilities are utilized concurrently, CoT consistently degrades performance compared to the \"Direct\u201d method across all benchmarks with Llama3-LLAVA-NeXT-8B model, consistent with the findings in Sec. 2.1. In contrast, despite the same models, PROREASON alternates between visual information acquisition and textual reasoning processes, allowing to leverage each capability more effectively. This enables PROREASON to consistently outperform CoT with both Llama3-LLAVA-NeXT-8B and GPT-40-mini across all benchmarks, demonstrating the effectiveness of capability decoupling.\nPROREASON outperforms ReAct with even less token consumption. Given the similarity in multi-step reasoning, we compare PROREASON and ReAct in terms of performance and cost. Specifically, as an LLM-specific multi-step reasoning framework, ReAct only outperforms the \u201cDirect\" method on MME and MMMU, but underperforms on MathVista and HallusionBench, showing inferior performance compared to the consistent improvements of PROREASON. Furthermore, we compare their average token consumption of GPT-40-mini. As shown in Table 8 of Appendix A, PROREASON consumes significantly fewer tokens than ReAct on both MME and MathVista tasks, indicating its higher token efficiency and the importance of a compact Memory in reducing token usage. Coupled with better performance, this suggests the superiority of PROREASON over LLM-specific ReAct framework.\nText-only LLMs can be effectively integrated into PROREASON for dramatically enhanced performance. As mentioned in Sec. 3.4, the decoupled visual perception and textual reasoning capabilities facilitate the seamless integration of text-only LLMs. To demonstrate the utility of this advantage, we fix the Vision Expert as Llama3-LLaVA-NeXT-8B, and replace other agents with text-only LLMs. As listed in Table 5, with the assistance of either Qwen2.5-72B-Instruct or GPT-40-mini, the Llama3-LLAVA-NeXT-8B Vision Expert receives remarkable performance boost across all benchmarks, particularly by 15% on MMMU and 11.7% on MathVista, compared to directly providing answers. In contrast, ReAct gains a much smaller improvement. This highlights the unique advantage of PROREASON in leveraging existing text-only LLMs for enhanced performance. Notably, this advantage may open new avenues for continuously pushing the performance limits of LVLMs with the assistance of existing powerful LLMs."}, {"title": "4.3 RELATIVE IMPORTANCE OF SUB-AGENTS", "content": "To assess the importance of each sub-agent within the PROREASON framework for visual reasoning tasks, we design five scenarios where Llama3-LLaVA-NeXT-8B acts as Dispatcher, Vision Expert, Reasoning Expert, Referee, or Summarizer, respectively, while the other sub-agents are powered by GPT-40-mini. Given that Llama3-LaVA-NeXT-8B exhibits weaker visual understanding and textual reasoning capabilities than GPT-40-mini, the more significant the performance drop incurred by replacing a sub-agent with Llama3-LaVA-NeXT-8B is, the more important that sub-agent is. Here we primarily consider the MME and MMMU benchmarks due to their comprehensive question coverage. The experimental results are presented in Table 6."}, {"title": "4.4 WHICH ONE IS MORE CRUCIAL: VISUAL UNDERSTANDING OR TEXTUAL REASONING?", "content": "PROREASON effectively decouples the visual understanding and textual reasoning capabilities of LVLMs. However, it remains unclear which of these two capacities is more critical for visual reasoning tasks. To answer this question, we conduct comparative experiments of the following three scenarios:\n\u2022 Llama3-LLAVA-NeXT-8B as All Sub-Agents. All sub-agents within PROREASON framework are performed by Llama3-LLaVA-NeXT-8B model.\n\u2022 GPT-4o-mini as Vision Expert. Based on the above scenario, we implement the Vision Expert with GPT-40-mini, while keep the other textual sub-agents unchanged.\n\u2022 GPT-40-mini as Textual Sub-Agents. Reversely, we utilize Llama3-LLaVA-NeXT-8B as the Vision Expert, and GPT-40-mini for the other vision-irrelevant sub-agents.\nTextual reasoning capabilities outweigh visual understanding for multi-modal reasoning tasks, although both are important. As shown in Table 7, replacing either the Vision Expert or the"}, {"title": "5 RELATED WORK", "content": "Large Visual-Language Model. Recently, large vision-language models (LVLMs) (Bai et al., 2023; Chen et al., 2023; Liu et al., 2024b) have garnered widespread attention and demonstrated remarkable advancements in understanding and generating multi-modal contents. In the open-source domain, numerous LVLMs, like LLaVA (Liu et al., 2023c;b; 2024a; Li et al., 2024a;b) and InternVL (Chen et al., 2024b) families, have been extensively developed. In the closed-source domain, proprietary models such as GPT-4 (Achiam et al., 2023) and Gemini Pro 1.5 (Reid et al., 2024) have also achieved significantly success. Despite these advancements, existing LVLMs still encounter challenges in effectively integrating visual understanding with textual reasoning capabilities simultaneously. This limitation is particularly evident in their diminished attention to image content during visual reasoning process, such as chart interpretation and visual math reasoning, leading to degraded performance (Ghosh et al., 2024) and motivating more effective solutions.\nPassive Visual Reasoning. Extracting information from images into text can effectively assist LVLM in performing visual reasoning tasks. Visual Description Grounded Decoding (VDGD) (Ghosh et al., 2024) first describes the image before prefixing this description to the prompt, assisting LVLMs on visual reasoning tasks. Furthermore, Compositional Chain-of-Thought (CCoT) (Mitra et al., 2024) directs LVLMs to generate scene graphs (SGs) that serve as a bridge between the visual and textual domains, aiding LVLMs in subsequent tasks. However, most of these methods employ a question-agnostic and reasoning-free visual extraction process, where image descriptions are not tailored to specific questions, and no reasoning is applied to infer additional information for improved descriptions. These \u201cpassive\u201d approaches lead to the inclusion of irrelevant or redundant information, ultimately degrading performance. In contrast, PROREASON adopts question-oriented agents to collect necessary and sufficient information, effectively circumventing these drawbacks.\nMulti-step Reasoning Framework. Multi-step reasoning frameworks have been developed for LLMs to achieve better performance by breaking down complex questions into easier ones (Pan et al., 2024). As a representative method, Chain-of-Thought (CoT) (Wei et al., 2022) enhances the arithmetic and commonsense reasoning capabilities by explicitly generating intermediate reasoning steps before concluding the final answers. Tree-of-Thoughts (ToT) (Yao et al., 2024) further refines the CoT mechanism by allowing LLMs to consider multiple reasoning paths and do self-assessment before making decisions. Considering that the inherent knowledge of LLMs may not be sufficient to complete tasks, ReAct (Yao et al., 2022) integrates information retrieval into the reasoning chains, enabling models to pause to verify results and determine whether additional information is needed before proceeding. Nevertheless, multi-step reasoning frameworks designed for text-only LLMs are not fully applicable to visual reasoning tasks, and may even impair the performance of LVLMS (Ghosh et al., 2024)."}, {"title": "6 CONCLUSION", "content": "In this paper, we first validate that existing multi-modal reasoning approaches still suffer insufficient and irrelevant visual descriptions, as well as limited multi-modal capacities. To address these issues, we decompose the visual reasoning process into visual perception and textual reasoning stages, and introduce a novel visual reasoning framework named PROREASON, featuring multi-run proactive perception and decoupled vision-reasoning capabilities. Empirically, extensive experiments demonstrate the superiority of PROREASON over both passive image information acquisition methods and multi-step reasoning frameworks for text-only LLMs across multiple visual reasoning benchmarks with both open-source and closed-source models. Notably, our method showcases the remarkable"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 DEMONSTRATIVE EXAMPLES", "content": ""}, {"title": "A.2 PROMPTS", "content": ""}, {"title": "A.3 SUPPLEMENTARY RESULTS", "content": ""}]}