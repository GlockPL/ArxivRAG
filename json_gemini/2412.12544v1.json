{"title": "Seed-CTS: Unleashing the Power of Tree Search for Superior Performance in Competitive Coding Tasks", "authors": ["Hao Wang", "Boyi Liu", "Yufeng Zhang", "Jie Chen"], "abstract": "Competition-level code generation tasks pose significant challenges for current state-of-the-art large language models (LLMs). For example, on the LiveCodeBench-Hard dataset, models such as O1-Mini and O1-Preview achieve pass@1 rates of only 0.366 and 0.143, respectively. While tree search techniques have proven effective in domains like mathematics and general coding, their potential in competition-level code generation remains under-explored. In this work, we propose a novel token-level tree search method specifically designed for code generation. Leveraging Qwen2.5-Coder-32B-Instruct, our approach achieves a pass rate of 0.305 on LiveCodeBench-Hard, surpassing the pass@100 performance of GPT40-0513 (0.245). Furthermore, by integrating Chain-of-Thought (CoT) prompting, we improve our method's performance to 0.351, approaching O1-Mini's pass@1 rate. To ensure reproducibility, we report the average number of generations required per problem by our tree search method on the test set. Our findings underscore the potential of tree search to significantly enhance performance on competition-level code generation tasks. This opens up new possibilities for large-scale synthesis of challenging code problems supervised fine-tuning (SFT) data, advancing competition-level code generation tasks.", "sections": [{"title": "Introduction", "content": "Competition-level code generation tasks present a unique set of challenges for large language models (LLMs). These tasks require models to not only comprehend complex problem statements but also generate executable code that adheres to logical and syntactical constraints. While existing state-of-the-art LLMs have achieved remarkable success in general-purpose programming benchmarks, their performance on competitive programming datasets, such as LiveCodeBench-Hard [Naman et al., 2024], remains far from satisfactory. For example, recent models like O1-Mini and O1-Preview exhibit pass@1 rates of only 0.366 and 0.143, respectively. This performance gap highlights the need for novel methodologies to enhance model capabilities in solving these challenging tasks.\nRecent research has demonstrated the potential of tree search techniques in reasoning tasks like mathematics and general programming. However, their application to competition-level code generation remains under-explored. Existing approaches primarily rely on large-scale proprietary LLMs within tree search frameworks, overlooking the possibility that smaller, open-source models\u2014when paired with an effective search strategy\u2014could achieve superior results. Moreover, while data augmentation"}, {"title": "Related Work", "content": ""}, {"title": "LLMs for Code Generation", "content": "Large language models (LLMs), with their powerful reasoning capabilities, have been widely adopted in code-related research and applications. The primary approach to building code LLMs involves pre-training or fine-tuning them on large code datasets, such as CodeX [Chen et al., 2021], AlphaCode [Li et al., 2022], WizardCoder [Luo et al., 2023], CodeGeeX [Zheng et al., 2023], Starcoder [Li et al., 2023] and Code LLama [Roziere et al., 2023]. Foundation models, like GPT-4 [Achiam et al., 2023] and Claude\u00b2, exhibit remarkable code generation capabilities despite lacking additional fine-tuning on code-specific data. Additionally, building upon the robust planning capabilities [Yao et al., 2022] and reflection mechanisms [Shinn et al., 2024] of LLMs, LLM-powered autonomous agents have shown significant potential in advancing automated code generation [Huang et al., 2023, Hong et al., 2023, Wang et al., 2024, Zhang et al., 2024]. For example, Agentcoder [Huang et al., 2023] proposes a multi-agent framework that includes programmer agents, test designer agents, and test execution agents to collaboratively generate and test code, MetaGPT [Hong et al., 2023] imitates the main roles in software companies in the real world, using different AI agents to play and ultimately produce a project."}, {"title": "Prompt Engineering", "content": "Designing effective prompts to seamlessly communicate with LLMs to fully harness their full potential can significantly improve LLMs performance without additional training. Some representative technologies of prompt engineering include Chain-of-Thought (CoT) [Wei et al., 2022], Self-Consistency [Wang et al., 2022], Tree-of-Thought (ToT) [Yao et al., 2024], Reasoning via Planning (RAP) [Hao et al., 2023] and Self-Refine [Madaan et al., 2024]. This technique can be directly applied in LLM for iterative and self improving (refining) code generation. For instance, CodeCoT [Huang et al., 2023] integrates chain-of-thought reasoning with a self-examination process, iteratively refining code based on execution feedback to ensure both logical correctness and syntactic validity. Self-planning [Jiang et al., 2024] enhances code generation by using LLMs to first plan solution steps from intent and then generate code step-by-step. Self-debugging [Chen et al., 2023], an LLM is prompted to iteratively refine code predictions by utilizing feedback from explanations and execution results to identify and fix errors."}, {"title": "Monte Carlo Tree Search (MCTS) for Reasoning", "content": "Chen et al. [2021] showed that repeated sampling can produce correct code solutions, suggesting the answer lies within the LLMs' output space with notable probability, motivating the use of tree search for efficient exploration [Li et al., 2024, Qi et al., 2024, Wang et al., 2024, Hui et al., 2024]. PG-TD [Zhang et al., 2023] introduces a planning-guided Transformer decoding algorithm that uses MCTS and test-case evaluation to iteratively refine code generation. Zhang et al. [2024] proposed ReST-MCTS*, a method that integrates process reward guidance with tree search to infer high-quality reasoning traces and per-step rewards, enabling more effective self-training of policy. Another common method is LATS [Zhou et al., 2023], which leverages LLMs as agents, value functions, and optimizers, incorporating MCTS to enhance decision-making through external feedback and experience. PlanSearch [Wang et al., 2024] improves code generation by searching over diverse natural language plans instead of directly over code."}, {"title": "Method", "content": ""}, {"title": "Preliminary", "content": "Neural code generation aims to automatically transform natural language descriptions into executable source code through large language models (LLMs). Here we provide a formal definition of the code generation task.\nLet $D$ represent a natural language description of a programming task, which may include problem statements, requirements, and additional programming context such as function signatures or assertions. The code generation task can be formalized as learning a model $\\pi_\\theta$ parameterized by $\\theta$ that generates code solution $C$ given description $D$:\n$C \\sim \\pi_\\theta(D)$.\nTo evaluate the correctness of generated code, we define a test suite $T = \\{(x_i, y_i)\\}_{i=1}^{|T|}$ where each test case consists of an input $x_i$ and its expected output $y_i$. The test suite is typically divided into two subsets\n$T = T_{pub} \\cup T_{priv}$,\nwhere $T_{pub}$ represents public test cases visible during development, and $T_{priv}$ represents private test cases held out for evaluation.\nGiven a code solution $C$, we define an execution function $Exec(C, x)$ that returns the output of running $C$ on input $x$. The correctness of $C$ can then be measured by comparing its outputs against the expected outputs across all test cases:\n$Correct(C | D, T) = \\frac{1}{|T|} \\sum_{(x,y) \\in T} \\mathbb{1}(Exec(C, x) = y)$,\nwhere $\\mathbb{1}(\u00b7)$ is the indicator function.\nThe objective of code generation is to find model parameters $\\theta^*$ that maximize the expected correctness across a distribution of programming tasks:\n$\\theta^* = arg \\max_\\theta \\mathbb{E}_{D,T} [Correct(\\pi_\\theta(D))]$."}, {"title": "MCTS with CoT Prompting", "content": "The proposed method is motivated by the desire to integrate Chain-of-Thought (CoT) reasoning with Monte Carlo Tree Search (MCTS). Specifically, the approach enables LLMs to first generate intermediate reasoning steps, followed by code generation. Through iterative refinement and optimization of both the reasoning and code components via MCTS, the method aims to enhance the model's performance on challenging competition-level code generation tasks. Next, we will provide a detailed description of our methods in each key component of MCTS.\nCoT Prompting. To improve the performance of LLMs on challenging competition-level code generation tasks, we introduce a structured CoT prompting methodology, which guides the model through a two-step reasoning process planning and coding to ensure logical and syntactically correct outputs. MCTS iteratively refines and optimizes both the planning and coding stages, improving performance in complex code generation tasks. The prompt explicitly instructs the model to:\n\u2022 Solution Planning: Analyze the problem specification and create a detailed step-by-step plan. This step includes outlining the problem-solving logic, choosing appropriate data structures, and determining the functions required for implementation.\n\u2022 Code Generation: Based on the detailed plan, write Python code adhering to coding standards and ensuring proper syntax.\nHere is an example:"}, {"title": "Selection.", "content": "The selection phase in MCTS strives to balance exploration and exploitation by selecting actions that are most likely to yield beneficial results. At the selection stage, the algorithm starts from the root node s0 and traverses the tree until it reaches a leaf node. Our method use a token-level MCTS so that each state s represents a candidate token. At each node s, the action a \u2208 A(s), where A(s) denotes the set of available actions in state s taken by the LLM \u03c0, is chosen by maximizing the P-UCB score:\nP-UCB(s, a) = Q(s, a) + \\beta(s) \\cdot p(a | s) \\cdot \\sqrt{\\frac{ln N(s)}{1+ N(s,a)}},\n\\beta(s) = log \\bigg(\\frac{N(s) + C_{base} + 1}{C_{base}}\\bigg) + c.\nHere:\n\u2022 Q(s, a) represents the average reward (defined in Simulation) of action a at state s.\n\u2022 N(s) is the total number of visits to state s.\n\u2022 N(s, a) is the number of times action a has been taken from state s.\n\u2022 p(as) is the prior probability of action a at s, proposed by the LLM \u03c0.\n\u2022 Cbase and c are hyperparameters that balance exploration and exploitation."}, {"title": "Expansion.", "content": "When the selection process reaches a node s in the search tree, the expansion phase creates new child nodes by considering potential next tokens. Unlike standard MCTS which might randomly sample actions, we leverage the LLM's predictions to guide expansion:\nGiven the current state s, we obtain the k most probable next tokens using the TOP-K function:\nT_k(s) = TOP\\_K(s, k),\nwhere k is a hyperparameter that limits the maximum number of children per node, and Tr(s) returns the set of k most likely next tokens according to the LLM's probability distribution.\nFor the new child node s', the visit count N(s', a') and the average reward Q(s', a') for all a' \u2208 A(s') are initialized to zero:\nN(s', a') = 0, Q(s', a') = 0, \\forall a' \\in A(s').\nThe use of priors p(s' | a') derived from the policy \u03c0 enables the tree to bias future expansions toward promising regions of the search space."}, {"title": "Simulation.", "content": "Once a new node s' is added to the tree, the algorithm estimates its value through a simulation, also called a rollout. Starting from s', actions are sampled according to the policy \u03c0 until a terminal state st is reached or a predefined depth limit dmax is exceeded. In this paper, we use two methods to estimate the quality for the state s', we call it as hard reward (HR) and partial reward (PR). Normally, we use all public test cases to validate the generated code. HR supposes that if everything passes, the code is considered correct, and if there exist errors, it is incorrect. Assume that T is the set of all test cases, the HR can be formalized as:\nR^{HR} = \\begin{cases}1, & \\text{if } \\mathbb{1}(Exec(C, x) = y) = 1,\\forall (x, y) \\in T;\\\\0, & \\text{otherwise}.\\end{cases}\nHowever, when addressing challenging coding problems, the distinction between partial success and complete failure is critical. Accordingly, our method leverages the pass rate on the test set as the reward signal, denoted as PR:\n$PPR_{s'} = \\frac{1}{|T|} \\sum_{(x,y) \\in T} \\mathbb{1}(Exec(C, x) = y)$."}, {"title": "Backpropagation.", "content": "The backpropagation stage updates the statistics of all nodes along the path from the newly expanded node s' back to the root s0. For each node-action pair (s, a) on the path, the visit count N(s, a) and the average reward Q(s, a) are updated as follows:\nN(s,a) \\leftarrow N(s, a) + 1,\nQ(s, a) \\leftarrow \\frac{(N(s, a) \u2013 1) \\cdot Q(s,a) + R(s')}{N(s,a)}\nThese updates propagate the simulation result R(s') upward, refining the action-value estimates Q(s, a) and balancing the contributions of exploration and exploitation."}, {"title": "Experiment", "content": "We use LiveCodeBench [Naman et al., 2024] as the test dataset, which comprises 659 problems collected between May 1, 2023, and September 1, 2024. The dataset categorizes problems into three difficulty levels: easy, medium, and hard, with 245 medium-level problems and 151 hard-level problems. Given that state-of-the-art models, such as Claude-3.5-Sonnet-20240620, have achieved a pass@1 rate of 0.869 on the easy subset, evaluating the impact of alternative inference techniques on these problems would likely be uninformative. Therefore, our study focuses on the medium and hard subsets, where there is greater scope for improvement and more meaningful differences in inference performance.\nTo validate the model-agnostic nature of the proposed MCTS approach, we employ four generative models, DeepSeekCoder-6.7B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, and Qwen2.5- 32B-Instruct, in our experiment."}, {"title": "MCTS on LiveCodeBench", "content": "In this section, we evaluate the performance of MCTS across several different generating models on LiveCodeBench-Medium and LiveCodeBench-Hard. For better readability, we present plots in what follows and defer detailed tables to Appendix A.\nMedium Level. We begin by comparing the performance of MCTS against the pass@k rates for the same generating models. Figure 2(a) illustrates the performance of MCTS with DeepSeekCoder- 6.7B-Instruct and Qwen2.5-32B-Instruct as generating models, compared against their pass@k rates on LiveCodeBench-Medium. In addition, Figure 2(b) illustrates the performance variation of MCTS and Best-of-N (or pass@k) as the number of number of generations increases. Across all tested configurations, MCTS consistently outperforms the Best-of-N baselines, which demonstrates its effectiveness in leveraging the same underlying model to achieve superior results."}, {"title": "Direct Prompting vs CoT Prompting", "content": "Chain-of-Thought (CoT) prompting has proven to be an effective technique for enhancing model performance in reasoning tasks. Throughout our experiments, we also explore whether integrating CoT prompting with MCTS can further improve overall performance.\nOn LiveCodeBench-Medium, Figure 2 demonstrates that MCTS with CoT prompting consistently outperforms the baseline. For example, with max_rollouts = 64 and Qwen2.5-Coder-32B-Instruct, MCTS with CoT prompting achieves a pass rate of 0.810, compared to 0.770 with direct prompting. A similar trend is observed on LiveCodeBench-Hard, as shown in Figure 3: with max_rollouts = 64 and Qwen2.5-32B-Instruct, MCTS with CoT prompting achieves a pass rate of 0.351, outperforming the pass rate of 0.305 from direct prompting.\nFigure 5 further highlights the efficiency of CoT prompting, requiring fewer generations on both datasets when combined with MCTS. Additionally, Figure 4 demonstrates consistent performance improvements across different models when CoT prompting is applied. As model capabilities increase, the combined approach of MCTS with CoT prompting becomes even more effective. To"}, {"title": "Deep Insight into MCTS's Selection Phase", "content": "In the context of MCTS, the final generated response can be decomposed into three key components: (1) the path identified during the selection phase via P-UCB search, (2) the actions sampled by the model during the expansion phase, and (3) the content generated through autoregressive decoding in the simulation phase. Since the sampling methods used in the simulation phase are indistinguishable from techniques like Best-of-N, we are particularly interested in the specific effects of the paths identified during the selection and expansion phases. To explore this, we present an interpretive experiment in the sequel.\nUsing the LiveCodeBench-Hard dataset with Qwen2.5-14B-Instruct, we fix max_rollouts = 16 and record the best paths discovered by MCTS. We the modify the prompt for each problem to include the format prompt + best path and employ standard autoregressive decoding methods to sample and compute pass@k rates. Figure 6 compares the pass@k rates of Qwen2.5-14B-Instruct on LiveCodeBench-Hard when using the original prompt versus the modified prompt.\nThe results demonstrate significant improvements with the modified prompt across all evaluated k-values (K = 1,5,10,50, and 100). For instance, at K = 1, the pass rate increases by 25.5% compared to the baseline, while at K=100, the relative improvement is 19%. These findings highlight the substantial contribution of the paths identified during the selection and expansion phases to the overall performance of MCTS."}, {"title": "CodeContest-Test", "content": "CodeContest-Test is a widely used benchmark set for code competition tasks, complementing LiveCodeBench. To evaluate the generalizability and effectiveness of the methods proposed in this paper across multiple code competition benchmark sets, we also compare our methods against several baseline models on CodeContest-Test.\nTo control experimental costs, we utilize the Qwen2.5-Coder-32B-Instruct model and fix the MCTS max_rollouts at 32. As shown in Figure 7, when using direct prompting, MCTS achieves a pass rate of 0.582, surpassing the performance of Claude-3.5-Sonnet. Notably, the average number of generations for MCTS in this setup is only 79.055.\nWhen employing Chain-of-Thought (CoT) prompting, the performance of MCTS improves further, achieving a pass@100 rate of 0.618, while reducing the average number of generations to 75.962. These results demonstrate the effectiveness of combining MCTS with CoT prompting, achieving better performance with fewer sampling attempts."}, {"title": "Conclusion", "content": "In this paper, we proposed a novel token-level Monte Carlo Tree Search (MCTS) framework combined with Chain-of-Thought (CoT) prompting, tailored for competition-level code generation tasks. Using the open-source Qwen2.5-Coder-32B-Instruct model, our approach demonstrates its effectiveness by achieving a pass rate of 0.351 on LiveCodeBench-Hard, nearing the pass@1 performance of O1-Mini. The results highlight the capability of our framework to significantly improve the problem- solving efficiency and accuracy of open-source models, thereby reducing the reliance on large-scale proprietary black-box LLMs. Moreover, our method's ability to generate consistent and high- quality solutions making it possible to synthesize supervised fine-tuning (SFT) data for large-scale competition level code problems from open-source LLMs. By synthesizing robust datasets directly from the target model, our approach paves the way for more effective and intrinsically aligned post-training strategies.\nIn the future, our framework can be further enhanced by integrating with techniques like rejection sampling and self-consistent reasoning. These techniques could complement our MCTS framework, further enhancing the LLMs' reasoning capabilities and improving their performance on competition- level code generation tasks. By enabling more robust and diverse exploration of potential solutions, and minimizing the generation of incorrect or incomplete code, these enhancements have the potential to advance the state of the art in solving complex coding problems."}]}