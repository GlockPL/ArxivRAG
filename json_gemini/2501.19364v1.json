{"title": "COSTI: Consistency Models for (a faster) Spatio-Temporal Imputation", "authors": ["Javier Sol\u00eds-Garc\u00eda", "Bel\u00e9n Vega-M\u00e1rquez", "Juan A. Nepomuceno", "Isabel A. Nepomuceno-Chamorro"], "abstract": "Multivariate Time Series Imputation (MTSI) is crucial for many applications, such as healthcare monitoring and traffic management, where incomplete data can compromise decision-making. Existing state-of-the-art methods, like Denoising Diffusion Probabilistic Models (DDPMs), achieve high imputation accuracy; however, they suffer from significant computational costs and are notably time-consuming due to their iterative nature. In this work, we propose CoSTI, an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI employs Consistency Training to achieve comparable imputation quality to DDPMs while drastically reducing inference times, making it more suitable for real-time applications. We evaluate CoSTI across multiple datasets and missing data scenarios, demonstrating up to a 98% reduction in imputation time with performance on par with diffusion-based models. This work bridges the gap between efficiency and accuracy in generative imputation tasks, providing a scalable solution for handling missing data in critical spatio-temporal systems.", "sections": [{"title": "1. Introduction", "content": "One of the most pervasive challenges when working with Multivariate Time Series (MTS) is handling missing values. This issue, known as MTSI, is critical because inaccurate imputations can distort the original data distribution (Sol\u00eds-Garc\u00eda et al., 2023). Missing values arise from various situations, which differ across domains. For instance, they can result from sensor malfunctions in Internet of Things (IoT) applications, where this issue is common (Ahmed et al., 2022; Wu et al., 2020), or in clinical settings (Moor et al., 2020), where patient data may be irregularly recorded or omitted entirely.\nMTSI has been extensively studied using traditional Ma-"}, {"title": "2. Related Works", "content": "Multivariate time series imputation The challenge of MTSI has been extensively studied through a variety of approaches. Traditional methods, such as mean imputation, zero imputation, or linear trends (Moor et al., 2020), are simple to implement but often compromise data integrity by introducing bias. ML techniques, including k-nearest neighbors (Beretta & Santaniello, 2016), matrix factoriza-tion (Cichocki & Phan, 2009), and scalable Gaussian pro-cesses (Li & Marlin, 2016), offer more adaptive and robust solutions. DL further transformed the field with Recurrent Neural Networks (RNNs), which excel at capturing sequen-tial dependencies critical for imputation tasks (Suo et al., 2019; Lipton et al., 2016). Bidirectional RNNs (BiRNNs), such as BRITS (Cao et al., 2018), enhance this capability by processing sequences in both forward and reverse tempo-ral directions, allowing for more comprehensive temporal modeling.\nTransformers, with their self-attention mechanisms, repre-sent a significant step forward by addressing limitations of RNNs, such as error propagation, and efficiently capturing long-range dependencies in time series data (Tashiro et al., 2021; Liu et al., 2023). More recently, advances in genera-tive models and State-Space Models (SSMs) have expanded the potential of MTSI methods. Structured State Space"}, {"title": "Diffusion models for MTSI", "content": "DDPMs have emerged as a robust solution for MTSI, leveraging iterative denoising pro-cesses to reconstruct missing data with high accuracy. Con-ditioning mechanisms, as demonstrated by CSDI (Tashiro et al., 2021), are critical to their success, allowing DDPMs to outperform earlier generative methods. Conversely, un-conditional DDPMs fail to deliver accurate imputations (Yun et al., 2023), underscoring the importance of carefully crafted conditional architectures. Recent advancements, such as PriSTI (Liu et al., 2023), have refined this approach by introducing specialized layers to enhance the generation of conditional information, achieving state-of-the-art results. Furthermore, the integration of S4 blocks into DDPMs has shown promise in improving temporal modeling efficiency (Alcaraz & Strodthoff, 2023).\nRecent advancements have further demonstrated the flexi-bility of DDPM architectures. TIMBA (Sol\u00eds-Garc\u00eda et al., 2024), for instance, replaces Transformer-based temporal blocks with bidirectional Mamba blocks, achieving com-petitive performance and showcasing the effectiveness of structured attention mechanisms in diffusion models. This approach highlights how architectural adaptations can fur-ther enhance the robustness and efficiency of DDPMs for MTSI, reinforcing their status as a leading framework for accurate and reliable time series imputation."}, {"title": "Consistency models", "content": "Consistency Models (CMs) are a recent advancement in generative modeling, closely related to DDPMs, but designed to optimize for single-step genera-tion. This approach prioritizes faster inference, albeit with a"}, {"title": "3. Background", "content": null}, {"title": "3.1. Multivariate Time Series Imputation", "content": "A MTS consists of recordings at different time steps, de-noted by t, across multiple variables or channels, denoted by Nt. Formally, an MTS can be represented as $X_t \\in \\mathbb{R}^{N_t \\times d}$, where each row i contains the d-dimensional vector $x \\in \\mathbb{R}^{d}$, corresponding to the i-th variable at time t. Addition-ally, a binary mask matrix $M_t \\in \\{0,1\\}^{N_t \\times d}$ is used to indicate the presence of missing values: $M_t[i, j] = 0$ when the corresponding value in $X_t$ is missing, and $M_t[i, j] = 1$ when the value is observed.\nThe imputed time series generated by a model is represented as $\\hat{X}_t$, while $X_t$ and $X_t$ denote the data with missing values and ground-truth imputation we aim to approximate, and $X_t$ refers to the time series imputed using a linear interpolation technique.\nSince our approach leverages graph-based models, follow-ing Cini et al. (2022), we model the MTS as a sequence of graphs. At each time step t, a graph $G_t = <X_t, A_t>$ is defined, where $A_t$ is the adjacency matrix for that time step. In this work, we assume a constant graph topology over time; thus, A remains fixed for all t."}, {"title": "3.2. Consistency Models", "content": "CMs are built on the probability flow ordinary differential equation (PF-ODE), which describes the evolution of data corrupted by Gaussian noise. Starting from a data distribu-tion $P_{data}(x)$, the diffusion process applies noise perturba-tions $N(0, \\sigma^2 I)$, transforming the data into pure noise. This process is expressed as (Song & Dhariwal, 2024):\n$p_\\sigma(x) = \\int P_{data}(y)N(x|y, \\sigma^2 I)dy$   (1)\nThe PF-ODE is given by (Karras et al., 2022):\n$\\frac{dx}{d\\sigma} = -\\sigma\\nabla_x \\log p_\\sigma(x) \\sigma \\in [\\sigma_{min}, \\sigma_{max}]$   (2)\nwhere $\\nabla_x \\log p_\\sigma(x)$, referred to as the score function, mea-sures the gradient of the log-likelihood of the data under $p_\\sigma(x)$. Diffusion models are a type of score-based gen-erative model because they estimate this score function at different noise levels during the corruption process (Song et al., 2021). For practical implementation, $\\sigma_{min}$ is set to a small value such that $p_{\\sigma_{min}}(x) \\approx P_{data}(x)$, and $\\sigma_{max}$ is large enough to approximate $p_{\\sigma_{max}}(x) \\sim N(0, \\sigma_{max}I)$. Following Karras et al. (2022), we set $\\sigma_{min} = 0.002$ and $\\sigma_{max} = 80$.\nThe PF-ODE allows transitioning between noise levels. For example, $x_{\\sigma_{min}}$ can be recovered from $x_{\\sigma}$, by solving the PF-ODE, defining a consistency function $f^* : (x_\\sigma, \\sigma) \\rightarrow x_{\\sigma_{min}}$. This function must satisfy self-consistency: $f^*(x_{\\sigma}, \\sigma) = f^*(x_{\\sigma'}, \\sigma')$ for any $\\sigma, \\sigma' \\in [\\sigma_{min}, \\sigma_{max}]$. Additionally, it must respect the boundary condition $f^*(x, \\sigma_{min}) = x$, where it acts as an identity function (Song et al., 2023).\nThe consistency function is parameterized using a neural network called the consistency model, $f_c$, which is designed to approximate $f^*$ while adhering to the boundary condition. The parameterization used in this work, following Song et al. (2023), is:\n$f_c(x, \\sigma) = C_{skip}(\\sigma)x + C_{out}(\\sigma) F_c(x, \\sigma)$   (3)\nWith differentiable functions $C_{skip}(\\sigma)$ and $C_{out}(\\sigma)$ such that $C_{skip}(\\sigma_{min}) = 1$ and $C_{out}(\\sigma_{min}) = 0$, defined as:\n$C_{skip} (\\sigma) = \\frac{\\sigma_{data}^2}{(\\sigma - \\sigma_{min}) + \\sigma_{data}},  C_{out}(\\sigma) = \\frac{\\sigma_{data} (\\sigma - \\sigma_{min})}{\\sqrt{\\sigma_{data} + \\sigma^2}}$   (4)\nTraining involves discretizing the PF-ODE into noise levels $\\sigma_{min} = \\sigma_1 < \\dots < \\sigma_N = \\sigma_{max}$, parameterized as $\\sigma_i = (\\sigma_{min}^{1/\\rho} + \\frac{i-1}{N-1} (\\sigma_{max}^{1/\\rho} - \\sigma_{min}^{1/\\rho}))^\\rho$ with $\\rho = 7$ (Karras et al., 2022)."}, {"title": "4. COSTI", "content": null}, {"title": "4.1. Consistency Training for imputation", "content": "To train a Consistency Model for MTSI, we introduced sev-eral modifications to improve the results. Below, we detail the adjustments made and describe the final architecture used to implement the Consistency Model. More compre-hensive information on the training and sampling procedures can be found in Algorithms 1 and 2 in the Appendix A.1."}, {"title": "4.1.1. CONDITIONAL INFORMATION", "content": "Consistency models share many similarities with diffusion models, as they are heavily inspired by the underlying theo-retical framework. Diffusion models have been successfully applied in the field of MTSI, often emphasizing the impor-tance of incorporating enhanced conditional information to guide the model towards better imputations. Indeed, Yun et al. (2023) demonstrated that without this conditional infor-mation, these models fail to produce adequate imputations.\nIn light of these findings and building on the successes of diffusion-based models in this domain, we incorporated con-ditional information following a philosophy similar to Liu et al. (2023). Specifically, we included $X_t, A Mt$. Conse-quently, our Consistency Model is parameterized as follows: $F_c: (X_{\\tau}, \\sigma_{\\epsilon}, X, A, M_t, \\sigma_i) \\rightarrow X_t$."}, {"title": "4.1.2. LosS FUNCTION AND REGULARIZATION", "content": "In MTSI, the task requires reconstructing information often entirely absent in the original dataset. To simulate this sce-nario during training, synthetic missing values are dynami-cally generated in each batch. The error is then computed solely at positions where ground-truth information exists. This process yields $\\check{X}_t$, a data matrix with synthetic missing"}, {"title": "4.1.3. \u039f\u03a1\u03a4\u0399MIZATION STRATEGY AND CURRICULUM LEARNING", "content": "Optimization strategies in prior work have varied. Song et al. (2023) and Song & Dhariwal (2024) used RAdam without schedulers or weight decay, while Geng et al. (2024) in-corporated learning rate decay in their experiments with RAdam and Adam. In our approach, we adopted the Scheduler-Free optimizer (Defazio et al., 2024), specifically the AdamW variant, which eliminates the need for a learn-ing rate scheduler while achieving state-of-the-art results. To ensure stable convergence, we introduced weight decay, further emphasizing the role of regularization in training Consistency Models.\nFor curriculum learning, we progressively increased N dur-ing training to refine consistency across different $\\sigma$ values. This strategy allows the model to start by comparing points farther apart along the PF-FLOW and gradually focus on smaller differences between closer $\\sigma$ values (Song & Dhari-wal, 2024). In this work, we used a linear scheduler, incre-menting N from $s_0 = 10$ to $s_1 = 200$, achieving a balance between simplicity and effective training."}, {"title": "4.1.4. DETERMINISTIC IMPUTATION", "content": "Like diffusion models, Consistency Models generate proba-bilistic imputations. To produce a deterministic result, we performed 100 imputations, estimated the distribution of predicted values, and used the median as the final output. This approach, as described in (Tashiro et al., 2021), en-sures a robust and consistent result suitable for practical applications."}, {"title": "4.2. Model architecture", "content": "Our model's architecture is primarily inspired by the U-Net structure (Ronneberger et al., 2015), a design exten-sively adopted in various works exploring consistency mod-els (Song et al., 2023; Song & Dhariwal, 2024; Luo et al., 2023). However, recent advancements in the application of DDPMS to MTSI have introduced specialized components tailored to the problem (Tashiro et al., 2021; Liu et al., 2023; Sol\u00eds-Garc\u00eda et al., 2024). In this work, we propose a hybrid approach, combining the foundational principles of U-Net with adaptations informed by recent developments in the literature. This architecture is both validated by prior re-search and designed to leverage the strengths of consistency models. A high-level overview of the proposed architecture is presented in Figure 2."}, {"title": "4.2.1. SPATIO-TEMPORAL FEATURE EXTRACTION MODULES (STFEM)", "content": "An essential component of our model is the Spatio-Temporal Feature Extraction Module (STFEM), which extends the design principles of the Conditional Feature Extraction Mod-ules proposed in prior work (Liu et al., 2023). STFEMs are specifically crafted to extract spatio-temporal representa-tions by combining bidirectional Mamba blocks, transform-ers with self-attention for spatial dimensions, and a Message Passing Neural Network inspired by Wu et al. (2019b). The architecture incorporates two STFEMS corresponding to the model's dual input streams: the primary input channel (Xt, A), located in the upper branch of Figure 2, and the conditional information channel, positioned in the lower branch, which processes inputs (Xt, Mt, A)."}, {"title": "4.2.2. U-NET COMPRESSION AND CROSS-ATTENTION", "content": "Following STFEM, both branches of the model proceed through the compression stage of the U-Net architecture. Temporal and spatial dimensions are sequentially reduced by factors $f_t$ and $f_s$, respectively. This follows the \"time-then-graph\" approach (Gao & Ribeiro, 2022), utilizing blocks specifically designed for each dimension and comprising transformers and convolutional layers.\nIn the conditional branch, information is compressed using self-attention, generating representations at three stages to serve as conditional inputs. In the primary branch, com-pression employs cross-attention mechanisms to incorpo-rate conditional information. Specifically, in each atten-tion block, the primary sequence $Z \\in \\mathbb{R}^{n \\times d}$ and the con-ditional sequence $H \\in \\mathbb{R}^{n \\times d}$ are combined using the attention mechanism defined as Attention(Q,K,V) =\n$softmax(\\frac{QK^\\top}{\\sqrt{d}}) \\cdot V$, where $Q = W_Q \\cdot Z$, $K = W_K \\cdot H$, and $V = W_V^{(H)} W_V \\cdot H$, with $W_Q^{(i)}, W_K^{(i)}, W_V^{(i)} \\in \\mathbb{R}^{d \\times d}$ (Rombach et al., 2022)."}, {"title": "4.2.3. NOISE EXTRACTION MODULES (NEM)", "content": "At the bottleneck of the U-Net, Noise Extraction Mod-ules (NEM) are employed, following their introduction by Tashiro et al. (2021) and subsequent enhancements by Liu et al. (2023) and Sol\u00eds-Garc\u00eda et al. (2024). Originally de-signed for estimating noise in diffusion models, we adapt NEMs for consistency models, leveraging the conditional information extracted earlier.\nThese modules integrate bidirectional Mamba blocks in place of temporal transformers, as in (Sol\u00eds-Garc\u00eda et al., 2024). However, given Mamba blocks' lack of native sup-port for cross-attention, we reintroduce transformers for this purpose. Each NEM comprises the following sequence: a temporal transformer with cross-attention for conditional information, a bidirectional Mamba block for sequence anal-ysis, a spatial transformer with cross-attention, and a spatial transformer with self-attention. This design ensures that both temporal and spatial dimensions are analyzed for con-ditional and intrinsic sequence data.\nFinally, the outputs are processed through a Multi-Layer Per-ceptron (MLP) with gated activation. Each NEM produces two outputs: one as input for the next NEM and another as a noise estimate, $H_{NEM_i}$. Each block also incorporates an embedding for $\\sigma$, constructed using positional embeddings (Song & Dhariwal, 2024)."}, {"title": "4.2.4. FINAL RECONSTRUCTION", "content": "After summing the noise estimates ($H_{NEM_i}$), the data passes through the reconstruction stage of the U-Net, which re-stores the original dimensionality of the inputs. This pro-cess leverages the conditional information and utilizes skip connections to integrate features from the initial stages of the architecture."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Datasets", "content": "To evaluate our approach, we use datasets that are well-established and frequently cited in the literature, but at the same time serve to represent the importance of obtaining faster imputations.\nWe used datasets identical to those in Cini et al. (2022), which established benchmarks subsequently adopted as com-parison metrics. Specifically, the AQI-36 dataset (Yi et al., 2016) contains hourly air quality readings from 36 stations in Beijing over one year, with 13.24% missing data. Addi-tionally, we used the METR-LA and PEMS-BAY datasets (Li et al., 2018), which document traffic conditions in Los Angeles and San Francisco Bay Area, respectively. METR-LA includes records from 207 sensors over four months with an 8.10% missing rate, while PEMS-BAY comprises"}, {"title": "5.2. Experimental settings", "content": "All experiments were conducted five times with different random seeds to ensure robustness. Four synthetic miss-ing data generation strategies were employed during the training:\n1) Point Strategy: Randomly masks [0, 100]% of data per batch. 2) Block Strategy: Generates sequences of missing values of length [L/2, L] with a [0, 15]% probability, along-side an additional 5% random missingness. 3) Historical Strategy: Uses original imputation masks from the dataset. 4) Hybrid Strategy: Combines the point strategy (primary) with either the block or historical strategy (secondary) with a 50% probability.\nFor \"Point missing\u201d scenarios, the point strategy was ap-plied. \"Block missing\" scenarios utilized the hybrid strategy with the block strategy as secondary. For AQI-36, the hy-brid strategy was employed with the historical strategy as secondary. Finally, the PhysioNet Challenge 2019 dataset was trained using the point strategy.\nTraining epochs varied by dataset: 200 for AQI-36, 300"}, {"title": "5.3. Results and Discussion", "content": null}, {"title": "5.3.1. PERFORMANCE ANALYSIS: IMPUTATION RESULTS", "content": "Table 1 reports the inference times (in hours) required to impute the test sets of the evaluated datasets for all consid-ered diffusion models and CoSTI using a single sampling step. Additionally, Tables 2 and 3 present the imputation performance of CoSTI with 1 and 2 sampling steps (denoted as CoSTI and CoSTI-2, respectively) compared to other dif-fusion models across all datasets and experimental scenarios described in Section 5.2."}, {"title": "Inference Speed Analysis:", "content": "From Table 1, it is evident that COSTI significantly outperforms diffusion models in terms of inference speed. Unlike CSDI, PriSTI, and TIMBA, which require $T$ steps to generate a sample from noise, COSTI achieves sampling with just one step. This character-istic makes CoSTI approximately 1/$T$ times faster than the other models. For AQI-36, diffusion models use $T = 100$, while for other datasets, $T = 50$, which aligns with the observed results: CoSTI requires only 1.64% of TIMBA's"}, {"title": "Imputation Accuracy Analysis:", "content": "Tables 2 and 3 high-light that although CoSTI does not consistently achieve the best results, its performance is comparable to that of CSDI, PriSTI, and TIMBA. Notably, CoSTI excels in the Phys-ionet Challenge 2019 dataset. Furthermore, increasing the number of sampling steps enhances CoSTI's accuracy, as shown by the improvements observed with CoSTI-2. This demonstrates a trade-off between speed and performance, offering flexibility depending on application needs.\nFor a more exhaustive analysis of the models, Appendix B includes ablation studies, modifications to CoSTI's training, and a comparison against a well-established benchmark in the literature. These additional experiments provide deeper insights into the model's behavior under various configura-tions and further contextualize its performance."}, {"title": "5.3.2. SENSITIVITY ANALYSIS", "content": "To evaluate sensitivity to varying missing value ratios, we tested COSTI, CSDI, PriSTI, and TIMBA on the METR-LA dataset under the Point-missing scenario with different lev-els of missing data. Leveraging the best-performing model weights from Table 3, the results are visualized in Figure 3. CoSTI exhibits performance comparable to PriSTI and TIMBA, particularly under higher missing rates, underscor-ing its robustness and reliability in challenging imputation scenarios."}, {"title": "5.3.3. DOWNSTREAM TASK EVALUATION", "content": "Assessing imputation models solely on reconstruction met-rics may not reflect their practical utility. Thus, we evaluate their effectiveness on a downstream task: node value predic-tion. This task, consistent with Sol\u00eds-Garc\u00eda et al. (2024), involves predicting a node's value at time $t$ using values from other nodes at the same timestamp.\nUsing the AQI-36 dataset, missing values were imputed with the best-performing model weights from Table 2. Im-puted data were split into a new training (80%) and test set (20%), normalized with a MinMax scaler. We trained an MLP with one hidden layer (100 neurons) for 500 epochs, minimizing MSE. Final test metrics excluded imputed target values, focusing on actual values only. As seen in Table 4, COSTI achieves comparable performance to diffusion-based models, outperforming them in node 31 prediction."}, {"title": "6. Conclusions", "content": "This work introduced CoSTI, a novel model for MTSI that leverages Consistency Models to achieve results comparable to state-of-the-art diffusion-based methods, while drastically reducing computational costs. By incorporating Consis-tency Training, CoSTI demonstrated up to a 98% reduction in inference time across diverse datasets and missing data scenarios.\nThese results position CoSTI as an efficient and scalable solution for real-time applications in critical domains like healthcare and traffic management. Future work could ex-plore adapting CoSTI for time series forecasting and further optimizing training efficiency through Latent Consistency Models."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Implementation details", "content": null}, {"title": "A.1. Detailed algorithms description", "content": "As an additional detail to the discussion in Section 3.2, the noise scheduler used in this study is based on the Karras scheduler (Karras et al., 2022), with noise levels $\\sigma_\\epsilon \\in \\{0.002, 80\\}$. The magnitude of $\\sigma_i$ and the perturbed input $X_{\\tau,\\sigma_\\epsilon}$ can vary significantly when passed to the model. Therefore, even though it is not always explicitly mentioned in other studies or equations, these inputs are typically scaled for training the neural network responsible for learning the consistency function. Consequently, the inputs are modulated as follows:\n$F_\\Theta (Cin(\\sigma_\\epsilon) \\cdot X_{\\tau,\\sigma_\\epsilon}, X_t, A, M_t, C_{noise} (\\sigma_i))$   (7)\nwhich, following prior work (Karras et al., 2022; Song et al., 2023; Geng et al., 2024), is implemented in this paper as:\n$Cin(\\sigma) = \\frac{1}{\\sqrt{2 + \\sigma_{data}}}$  $Cin(\\sigma) = \\frac{ln (\\sigma)}{4}$   (8)\nTo implement the proposed model, we employ two distinct algorithms: one for training (Algorithm 1) and another for sampling or imputation (Algorithm 2)."}, {"title": "A.1.1. TRAINING ALGORITHM", "content": "The training algorithm has a noteworthy feature: it is optimized to skip a forward pass when $\\sigma = \\sigma_{min}$. This optimization slightly accelerates execution for small values of $N$, although its impact diminishes as training progresses and $N$ increases."}, {"title": "A.1.2. SAMPLING ALGORITHM", "content": "The sampling algorithm supports sampling in as many steps as desired, however in this article we have focused on 1-step and 2-step sampling. By default, sampling begins from the highest noise level allowed, $\\sigma_{i_1} = \\sigma_{max} = 80$. For two-step sampling, a dataset-specific noise level $\\sigma_{i_2}$ is experimentally determined (details in Appendix A.2)."}, {"title": "A.2. Hyperparameters", "content": "This section provides a detailed overview of the hyperparameters used in CoSTI. First, we list the hyperparameters that were kept constant throughout all experiments conducted in this study:"}, {"title": "A.3. Additional Dataset Information", "content": "Table 6 provides detailed information about the datasets used in this study, including the topology of their adjacency matrices, the number of nodes and edges, and the proportions of original and additionally injected missing values.\nThe datasets employed in this study are publicly available and free to use. Specifically, the Torch SpatioTemporal library (Cini & Marisca, 2022) offers tools to download and preprocess the AQI-36, METR-LA, and PEMS-BAY datasets. The Physionet Challenge 2019 dataset is accessible at: https://physionet.org/content/challenge-2019/1.0.0/."}, {"title": "A.4. Code Reproducibility", "content": "This study emphasizes code reproducibility and accessibility to the datasets employed. The implementation has been made publicly available on GitHub: https://github.com/javiersgjavi/CoSTI.\nThe codebase is written in Python (Van Rossum & Drake, 2009) and leverages several widely used open-source libraries, including:\n\u2022 Pytorch (Paszke et al., 2017).\n\u2022 Pytorch Lightning (Falcon & The PyTorch Lightning team, 2019)\n\u2022 Numpy (Harris et al., 2020).\n\u2022 Torch spatio-temporal (Cini & Marisca, 2022).\n\u2022 Pandas (pandas development team, 2020; Wes McKinney, 2010).\n\u2022 Hydra (Yadan, 2019).\nTo streamline the execution of experiments, a Docker image and corresponding container (Merkel, 2014) were created, along with scripts to facilitate their setup and use. All experimental results were obtained using a system configured as follows: Ubuntu 22.04.2 LTS operating system, an AMD Ryzen Threadripper PRO 3955WX CPU with 16 cores, an NVIDIA RTX A5000 GPU with 24 GB of memory, and 128 GB of DDR4 RAM (8x16 GB modules). Table 7 summarizes the training and inference times achieved with this setup."}, {"title": "B. Additional Results", "content": null}, {"title": "B.1. Benchmark analysis", "content": "To provide a broader context for the results obtained with CoSTI compared to other imputation techniques, we evaluated it using the comprehensive benchmark introduced by Cini et al. (2022). This benchmark includes a diverse range of methods, encompassing both traditional statistical approaches and more advanced generative models. Specifically, it evaluates the following categories of techniques:"}, {"title": "B.2. Sensitivity analysis", "content": "The outcomes of the missing rate experiment described in Section 5.3.2 are summarized for the four models in Tables 9 and 10, reporting performance in terms of MAE and MSE, respectively."}, {"title": "B.3. Ablation Study and Component Analysis", "content": "This section provides a comprehensive evaluation of our model through ablation studies and component analysis. We investigate the contribution of the techniques introduced in this paper, examining the impact of architectural components, curriculum learning strategies, and optimizers."}, {"title": "B.3.1. ARCHITECTURAL COMPONENT ABLATIONS", "content": "We conducted the following experiments to analyze the impact of architectural components:\n\u2022 w/o Cond: This experiment evaluates the performance of CoSTI when the second model head, responsible for extracting conditional information, is removed.\n\u2022 w/o SFTE: This test assesses the model's performance when the STFEM modules, which extract spatio-temporal representations of the input sequence at the network's entry, are omitted.\n\u2022 w/o NEM: This experiment examines the impact of removing the NEM blocks, located at the core of the network, that are designed to estimate noise in the input sequence.\n\u2022 w/o self-attention: This test evaluates the contribution of the Bi-Mamba and Spatial Self-Attention layers within the NEM blocks to assess whether their inclusion enhances performance."}, {"title": "B.3.2. CURRICULUM LEARNING STRATEGY EVALUATION", "content": "In this section", "Scheduler": "This is the scheduler implemented in this paper. It is a linear scheduler with $s_0 = 10$ and $s_1 = 200$. In Table 12 and Figure 4", "Linear $s_0 = 10$.\\\u201d\n\u2022 Linear $s_0 = 2$": "This scheduler is similar to the one implemented in this paper but uses $s_0 = 2$.\n\u2022 Linear $s_1 = 1280$: This scheduler also follows a linear approach"}, {"Scheduler": "This scheduler is the one originally proposed in Song et al. (2023)", "Constant": "This is the simplest scheduler", "Exponential": "Proposed in Song & Dhariwal (2024)"}, {"Exponential": "This is a custom scheduler that introduces a pretraining phase with $N = 2$. Inspired by Geng et al. (2024)", "techniques": "a progressive improvement in model performance as $N$ increases. While larger $N$ values reduce approximation bias and improve imputation quality, they"}]}