{"title": "Fruit Fly Classification (Diptera: Tephritidae) in Images, Applying Transfer Learning", "authors": ["Erick Andrew Bustamante Flores", "Harley Vera-Olivera", "Ivan Cesar Medrano Valencia", "Carlos Fernando Montoya Cubas"], "abstract": "This study develops a transfer learning model for the automated classification of two species of fruit flies, Anastrepha fraterculus and Ceratitis capitata, in a controlled laboratory environment. The research addresses the need to optimize identification and classification, which are currently performed manually by experts, being affected by human factors and facing time challenges. The methodological process of this study includes the capture of high-quality images using a mobile phone camera and a stereo microscope, followed by segmentation to reduce size and focus on relevant morphological areas. The images were carefully labeled and preprocessed to ensure the quality and consistency of the dataset used to train the pre-trained convolutional neural network models VGG16, VGG19, and Inception-v3. The results were evaluated using the F1-score, achieving 82% for VGG16 and VGG19, while Inception-v3 reached an F1-score of 93%. Inception-v3's reliability was verified through model testing in uncontrolled environments, with positive results, complemented by the Grad-CAM technique, demonstrating its ability to capture essential morphological features. These findings indicate that Inception-v3 is an effective and replicable approach for classifying A. fraterculus and C. capitata, with potential for implementation in automated monitoring systems.", "sections": [{"title": "1. Introduction", "content": "Fruit flies, belonging to the Order Diptera and the Family Tephritidae, are one of the main pests affecting fruit production worldwide. There are approximately 4,000 species of fruit flies, distributed in genera of great eco-nomic importance such as Anastrepha, Bactrocera, Ceratitis, Dacus, Rhagoletis, and Toxotrypana [16, 2]. In Peru, several genera are present, including Anastrepha (with 44 species), Ceratitis (1 species), Rhagoletis (5 species), and Toxotrypana (1 species) [7]. The species of greatest eco-nomic relevance, such as Anastrepha distincta, Anastrepha fraterculus, Ceratitis capitata, and Anastrepha striata, are found in various regions of the country, from the south and central areas to the north [26].\nThe biological cycle of these pests causes significant damage to fruit production. Females deposit their eggs un-der the skin of the fruit, where the larvae develop and con-sume the pulp, accelerating ripening, decomposition, and falling, leading to substantial economic losses. In Latin America, fruit flies are estimated to cause annual losses of around 35 million dollars, and in Andean Group countries (Bolivia, Colombia, Ecuador, and Peru), these losses ex-ceed 30% of the value of fruit and vegetable production [14]. In Peru, the damage caused by these pests reaches up to 60% of the production [22].\nTo mitigate these losses, \"El Servicio Nacional de Sanidad Agraria del Per\u00fa\" (SENASA) has implemented fruit fly de-tection and monitoring projects. This system relies on the use of McPhail traps with food attractants, which allow for the collection of specimens for laboratory analysis. Specif-ically, in the province of \"La Convenci\u00f3n\" in the Cusco re-gion, specialists identify and classify species by analyzing specific morphological characteristics, and weekly reports are generated to evaluate the population of these species in the monitored areas.\nCurrently, species identification is performed manually by experts, which requires speed and precision. However, both factors are compromised: the average identification time per fly is around 10 seconds, and the accuracy of the analysis can be affected by human factors such as fatigue and subjectivity. This manual process, although necessary, is not optimal during peak fly population periods, resulting in delays in decision-making.\nRecent studies have shown that computational systems based on artificial intelligence (AI), particularly those us-ing deep learning techniques, are capable of improving ac-curacy in similar classification processes [1, 6]. The liter-"}, {"title": "2. Antecedentes", "content": null}, {"title": "2.1. Machine Learning Techniques for Fruit Fly Classification", "content": "In the study by [3], the classification of the fruit fly species Anastrepha fraterculus, Anastrepha obliqua, and Anastrepha sororcula was investigated using a set of wing and ovipositor images. During preprocessing, Otsu's thresh-olding and morphological dilation were employed to seg-ment the images, extracting color and texture features to train multiple classifiers. Accuracy was optimized by com-bining up to 33 individual classifiers with a Support Vector Machines (SVM) meta-classifier and merging the datasets, achieving a final accuracy of 98.8%.\n[18] proposed the development of a classification sys-tem designed to identify fruit fly species using smart traps, utilizing two datasets: one with four classes (Anastrepha fraterculus, Ceratitis capitata, \"Other insects,\" and \"Residues\" and another with three classes, consolidating the last two into \"Others.\" The images were converted to grayscale, adaptive thresholding and morphological opening were ap-plied to remove unwanted areas. They used Bag of Visual Words (BOVW) for feature extraction and evaluated var-ious classification models, with SVM being the most effec-tive, achieving accuracies of 84.56% and 86.38% in each dataset."}, {"title": "2.2. Deep Learning Techniques for Fruit Fly Classification", "content": "The classification of Ceratitis capitata and Grapholita molesta in smart traps was addressed by [12] using pre-trained models, with ResNet18 standing out, initially achiev-ing an accuracy of 84.28%. The accuracy improved to 93.55% and 91.28%, respectively, after applying data aug-mentation techniques (flipping, rotation, and random eras-ing). Given the goal of implementing a model on the Raspberry Pi v2, SqueezeNet was selected, achieving an accuracy of 88.56% for Ceratitis capitata and 90.60% for Grapholita molesta with the same data augmentation tech-niques.\n[9] focused on the identification of fruit fly species, us-ing a dataset containing wing images of Anastrepha frater-culus, Anastrepha obliqua, and Anastrepha sororcula.\nFeature descriptors and detectors were employed to train nine machine learning techniques. The best perfor-mance was achieved with the Multilayer Perceptron (MLP), which reached an average accuracy of 88.9% in classifying all species.\nThe research by [4] addressed the determination of the age of fruit fly pupae with the goal of making them ster-ile. The dataset was prepared by cleaning a section of the pupae to expose the fly's eyes.\nThe Multi-Template Matching technique was applied alongside an Inception-v1 model. This combination of image preprocessing with templates and the Inception-v1 model achieved an accuracy of 72.22% for Anastrepha lu-dens and 83.17% for Ceratitis capitata."}, {"title": "2.3. Deep Learning Techniques Combined with Machine Learning for Fruit Fly Classification", "content": "The research by [17] addressed the classification of Bac-trocera dorsalis, Bactrocera cucurbitae, Bactrocera tau, and Bactrocera scutellata in images with complex backgrounds. Initially, they tested different hyperparameter configura-tions in a CNN but faced overfitting issues. To resolve this, they replaced the final Softmax layer of the CNN with machine learning models: SVM, KNN, AdaBoost, and Random Forest. The CNN-SVM combination was the most effective, achieving an accuracy of 92.4%.\nIn their work, [8] successfully classified Anastrepha species: A. fraterculus, A. obliqua, and A. sororcula, using pre-trained models (ResNet, VGG16, VGG19, Xception, In-ception) to extract features. These features were evalu-ated with machine learning models such as Decision Tree, )KNN, MLP, Naive Bayes, SGD, and SVM. Using cross-validation, VGG16-SVM achieved the highest accuracy, reaching 95.68%.\nThe proposal by [13] focused on the development of a monitoring system using sticky traps, with two datasets: one distinguishing \"Olive Fly\" and \"Others\" (including debris), and another differentiating \"Fruit Fly\" and debris. They used models such as Random Forest, SVM, Decision Trees, and deep networks like VGG16, MobileNet, and Xception. For the first dataset, RF and SVM achieved"}, {"title": "3. Morphological Characteristics.", "content": "Understanding the morphological characteristics of dif-ferent species of fruit flies is crucial for appreciating the work of experts in the identification and classification of these pests in the laboratory. The accuracy in species iden-tification depends on a thorough analysis of their morpho-logical traits, among which the wings, thorax, and ovipos-itor are key elements. These characteristics allow differen-tiation of species that, although visually similar, exhibit subtle variations in their anatomical structures, essential for their taxonomic classification."}, {"title": "3.1. Sections and Venations of the Fruit Fly Wing.", "content": "The wings of the fruit fly provide key information through their sections and venations, which are crucial for the clas-sification of different species. This information is repre-sented in specific patterns and structural differences that allow effective identification and differentiation between species.\nIn Figure 1, the distribution of the venations and sec-tions present in the wing is shown. Some venations pro-vide relevant information for species differentiation, such as the R4+5, bm-cu, dm-cu veins, among others. Simi-larly, specific sections such as dm, r4+5, bm, among others, are identified. For the two species studied in this research, the venations and sections that allow their differentiation will be described in detail."}, {"title": "3.2. Parts of the Fruit Fly Thorax.", "content": "The thorax of the fruit fly constitutes another signifi-cant source of information due to its structure and the col-oration patterns it exhibits. These characteristics are es-sential for distinguishing between various species, enabling precise classification.\nIn Figure 2, the distribution of the parts that make up the thorax is presented. Some anatomical structures"}, {"title": "4. Materials and Methods", "content": null}, {"title": "4.1. Image Acquisition", "content": "Two main categories are considered for the image cap-ture setup: laboratory environments and field environ-ments [11].\nSince SENASA is the competent authority in the iden-tification of fruit fly species, access was requested to one of their facilities. This allowed image capture in a labo-ratory environment, which follows a standard protocol for capturing, identifying, classifying, and reporting species. The laboratory protocol is as follows:\n\u2022 Official SENASA McPhail traps (Figure 5) are placed in different districts the province of \"La Convenci\u00f3n\".\n\u2022 The fruit fly species are attracted to the traps us-ing a food attractant made from Torula yeast. The flies that land on this attractant become stuck and trapped.\n\u2022 To collect the flies, the food attractant, along with the trapped flies, is poured into jars labeled accord-ing to the district, zone, and specific trap where they were found.\n\u2022 The jars with the samples are handed over to the expert responsible for the identification and classifi-cation of the species.\n\u2022 The information about the identified and classified species is recorded in an Official Trap Record (ROT) format, which allows tracking of the traps installed nationwide [21]."}, {"title": "4.2. Preprocessing of the dataset.", "content": "To standardize the image size and facilitate analysis, they were transformed to a resolution of 800 x 600 pixels. This resizing process is crucial to ensure the uniformity of the dataset, and this resizing is shown in Figure 8."}, {"title": "5. Experiments and Results", "content": "This section details the experiments conducted and the results obtained in our work. For this, three selected mod-els were employed: VGG16, VGG19, and Inception-V3. The specific results of each model are presented below, along with a comparative analysis of their performance, evaluated in terms of accuracy, recall, F1-score, and clas-sification time."}, {"title": "5.1. Segmentation.", "content": "The dataset was generated from the manual capture of images using the eyepieces of a stereomicroscope, which resulted in variations in contrast, lighting, and brightness of the images. During the segmentation process, it was observed that some of these images were not correctly seg-mented. In Figure 14.b, which shows the segmentation of Figure 14.a, it can be seen that the image retains parts of the background, which are considered noise. This noise could affect the predictions of the models, preventing them from being properly trained. On the other hand, Fig-ure 14.d shows correct segmentation, without background remnants or noise, ensuring higher prediction accuracy.\nTo ensure proper training of the models, images that were not correctly segmented were manually separated. These included 50 discarded images per class, which re-duced the dataset size to 236 images per class."}, {"title": "5.2. Training.", "content": "As described in [23], the VGG16 and VGG19 models were originally trained with images of 224 x 224 pixels. On the other hand, [27] mentions that the Inception-v3 model was trained with images of 299 x 299 pixels, although it can also process images of lower resolution. While train-ing with lower-resolution images may increase the required"}, {"title": "5.3. Results.", "content": "After running experiments with each selected model, the following results were obtained:\nVGG16 Model. The best performance for this model was achieved in epoch 91, with a total training time of 4750.1 seconds. For the test set, consisting of 148 images, the classification time was 28.88 seconds for Anastrepha fraterculus samples and 16.25 seconds for Ceratitis cap-itata samples, with a total classification time of 45.13 seconds.\nVGG19 Model. The VGG19 model achieved its best performance in epoch 79, with a total training time of 5831.29 seconds. In the test set of 148 images, the classifi-cation time was 16.81 seconds for Anastrepha fraterculus samples and 24.02 seconds for Ceratitis capitata samples, with a total classification time of 40.83 seconds.\nInception-v3 Model. The best performance for the Inception-v3 model was achieved in epoch 41, with a to-tal training time of 806.56 seconds. In the test set of 148 images, the classification time was 7.68 seconds for Anas-trepha fraterculus samples and 7.36 seconds for Ceratitis capitata samples, with a total classification time of 15.04 seconds."}, {"title": "6. Discusi\u00f3n", "content": null}, {"title": "6.1. Segmentation", "content": "During the segmentation process, it was identified that some images were affected by various factors associated with the capture process, resulting in a reduction of the original dataset by approximately 17.5%.\nFactors such as brightness, contrast, and lighting con-ditions were found to have a significant impact on the ability of a segmentation algorithm to generalize correctly. Although manual capture with mobile devices facilitates quick data collection and instant storage, these uncon-trolled conditions introduce additional challenges in later stages, such as preprocessing, thereby affecting the quality and consistency of the segmentation.\nHowever, the images discarded due to poor segmenta-tion were not completely removed. They were included in the test set, playing an important role in evaluating the models' ability to generalize and classify images with varied characteristics, including those with imperfect seg-mentations."}, {"title": "6.2. Results", "content": "The classification task in this research does not align with a typical binary classification scenario, where classes are usually defined as \"normal\" and \"abnormal.\" Instead, we work with two specific classes: Anastrepha fraterculus and Ceratitis capitata, whose correct classification is cru-cial in the context of our study."}, {"title": "6.3. Analysis of Model Explainability.", "content": "To better understand which visual aspects the models are considering when making classifications, we employed the Grad-CAM technique. This technique allows us to gen-erate a visual representation of the areas of the images that the models consider most relevant when assigning them to a specific class. For this, we used two images from our test set, as shown in (Figure 16)."}, {"title": "6.4. Criteria for Model Selection.", "content": "The criteria considered for model selection include clas-sification time, performance metrics, areas of interest iden-tified by the models, and their performance in uncontrolled scenarios. These criteria are based on the results obtained from evaluating the models with test images, consisting of 74 images per class under study."}, {"title": "6.4.1. Evaluation of Classification Time.", "content": "When comparing classification times, the Inception-V3 model proved to be the most efficient, with a total time of 15.04 seconds. It was followed by the VGG19 model, with a time of 40.83 seconds, while VGG16 recorded a time of 45.13 seconds. These results position Inception-V3 as the most efficient model in terms of classification speed."}, {"title": "6.4.2. Evaluation of Performance Metrics.", "content": "In terms of performance metrics, Inception-V3 excelled with an F1-score of 93%, reflecting its superior ability to learn and generalize the patterns associated with the stud-ied species. In contrast, VGG16 and VGG19 achieved F1-scores of 0.82%. These findings suggest that Inception-V3 offers higher precision and robustness in classification."}, {"title": "6.4.3. Evaluation Using Grad-CAM.", "content": "In Section 6.3, the graphical representations of the im-age regions used by the models to make their predictions are presented, using the Grad-CAM technique. This tech-nique highlights the areas of the image that are relevant for each prediction and is shown in the third column of Fig-ures 17, 18, and 19, where Grad-CAM is overlaid on the original image, facilitating the visualization of the areas on which each model focuses its attention.\nUpon analyzing the results, it is observed that Inception-V3 covers a greater amount of information, including re-gions with key morphological characteristics. This demon-strates that Inception-V3 is the model that best utilizes these features to make its classifications.\nThe goal of this work is to develop a model capable of replicating the criteria used by experts in species classifi-cation. The results obtained confirm that Inception-V3 is the most suitable model for identifying relevant morpho-logical areas, validating its suitability for this task."}, {"title": "6.4.4. Evaluation in Uncontrolled Scenarios.", "content": "For the evaluation of uncontrolled scenarios, images were selected from the internet with the goal of more thor-oughly assessing the robustness and generalization capa-bility of our models. We used a set of images manually"}, {"title": "7. Conclusions.", "content": "In this study, various classification models were eval-uated with the goal of classifying fruit fly species in seg-mented images. Among the models tested, Inception-V3 demonstrated the highest efficiency, achieving a total clas-sification time of 15.04 seconds for 148 samples and an F1-score of 93%, indicating its suitability for species clas-sification in terms of both accuracy and speed.\nAlthough Inception-V3 showed strong performance in controlled conditions and with poorly segmented images, its generalization capability on a set of randomly selected images from the internet was limited, yielding an F1-score of 69%. This result emphasizes the importance of incorpo-rating more diverse conditions in future studies to enhance the model's applicability in real-world environments.\nThe visual analysis conducted with Grad-Cam revealed that Inception-V3 was able to identify key morphological features associated with the species, further supporting its ability to extract relevant characteristics for classification. This, combined with its performance on well-segmented data and additional tests, justifies its selection as the most appropriate model for the task.\nIn conclusion, Inception-V3 emerges as a promising model for species classification due to its balance of ac-curacy and speed, along with its partial adaptability to diverse scenarios. However, the findings also highlight the necessity of continuing to explore strategies to improve the model's generalization in uncontrolled conditions, expand-ing its potential for real-world applications."}]}