{"title": "Keyword-Aware ASR Error Augmentation for Robust Dialogue State Tracking", "authors": ["Jihyun Lee", "Solee Im", "Wonjun Lee", "Gary Geunbae Lee"], "abstract": "Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic Speech Recognition (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.", "sections": [{"title": "1 Introduction", "content": "Task-oriented dialogue systems (TODs) assist users in achieving specific objectives through conversations and are used in various sectors, including customer service and hotel reservations. A crucial component of these systems is Dialogue State Tracking (DST), which extracts vital information from conversations in a slot-value format (e.g., hotel-name: Claire Hotel). This information is essential for querying databases and generating responses.\nHowever, DST models face significant challenges in spoken dialogue environments, where user utterances are converted into text by automatic speech recognition (ASR). Notably, Soltau et al. (2022) observed a drastic reduction in model accuracy from 41.6% to 23.6% in such environments\u00b9. This decline is primarily due to ASR errors, which frequently misrecognize named entities a key target in DST.\nTo address ASR inaccuracies, data augmentation has emerged as a viable, cost-efficient strategy. Existing text augmentation methods, such as word swapping and back translation , do not maintain audio similarity with the original text, leading to discrepancies with ASR error patterns. To bridge this gap, Sharma et al. (2020) and Jacqmin et al. (2023) synthesized audio from text with text-to-speech (TTS) model and processed it through ASR, while Hrinchuk et al. (2020) and Zhang et al. (2021) employed translation model structure to introduce ASR-like errors directly into texts.\nDespite these advancements, prior methods often fail to provide sufficient error patterns for DST model training. Accurately identifying key terms is vital for DST performance; thus, models need to be trained on a broad spectrum of ASR-errored keywords. Unfortunately, many current strategies do not ensure that errors are positioned within critical keywords, often generating trivial examples by altering non-essential words such as random words or sentence structure. This oversight results in sub-optimal DST performance against ASR errors.\nTo address these limitations, we introduce Error Positioning Augmentation (EPA), a straightforward yet effective method that ensures sufficient errors in keywords. Our method leverages large language models (LLMs), which have demonstrated impressive capabilities in semantic augmentation and precise text generation control. Despite their strengths, LLMs' potential for phonetic augmentation remains largely unexplored.\nIn our method, we utilize in-context learning to guide the augmentation process. By providing examples that highlight the key parts, we enable the model to position errors in keywords. Surprisingly, without col-"}, {"title": "2 Method", "content": "Before detailing each step, we first clarify the notation. Dialogue context from turn 1 to t is denoted as Dt={($1, U\u2081), ..., (st, ut)} where s denotes for system and u for user utterance. DST model predicts the dialogue state (also called belief state) Bt given Dt. Bt is composed with slot sl and value v pairs, denoted as Bt = {(sl\u0131, v\u2081), ..., (slj, vj)} where slj and vj is j-th slot name and value. J is the total number of slots."}, {"title": "2.2 Step 1: ASR Error for Overall Utterance", "content": "In this step, we augmented the overall utterance by introducing general ASR errors. We began by constructing example sets for in-context learning, utilizing an open-source audio dataset. From this dataset, we randomly se-"}, {"title": "2.3 Step 2: ASR Error for Keywords", "content": "While Step 1 introduces general ASR errors into u, it does not ensure that these errors enhance the diversity of keywords. To create a more effective training dataset, we generated keyword-"}, {"title": "3 Augmentation Results of EPA", "content": "We first assessed the quality of ASR errors simulated by the EPA method to guarantee they closely resembled genuine ASR errors. demonstrates that with overall utterance and keyword error augmentation, EPA effectively generates sentences that align with our objectives: maintaining phonetic similarity to the original text while confirming errors in keywords."}, {"title": "4 Experiments", "content": "The DSTC11 dataset , an audio version of MultiWOZ 2.1 , comprises 8,000 dialogues for training, 1,000 for validation, and 1,000 for testing. To enhance generalization, we conducted experiments across four distinct ASR environments, characterized by word error rate (WER)\u00b3 and noise levels: (1) a low accuracy ASR model (WER > 0.03), (2) a caf\u00e9 and traffic noised audio, (3) a paraphrased setting where users naturally paraphrased the transcriptions, and (4) a high accuracy ASR model (WER < 0.03) enviroment.\nMetrics. For overall performance evaluation, we used joint goal accuracy (JGA), which requires all slot-value pairs to match the gold label. We also reported named entity accuracy (N-acc), the average accuracy across named entity slots.\nLLMs. we primarily used GPT-3.5. Additionally, as this is the first attempt to augment pronunciation-related errors with LLMs, we also included Llama2-7B and OPT-6.7B.\nCompared methods. We compared our method with two established approaches: text-based augmentations, AEDA, EDA, and Back Translation, and audio-aware augmentation methods, using synthesized audio (TTS-ASR) and translation model structure (ASR-translation). Lastly, we included Olisia , the top-ranked method in the DSTC11 competition."}, {"title": "4.2 Robustness Improvement through EPA", "content": "The results in demonstrate the effectiveness of EPA in enhancing robustness to ASR errors in the DST task. Remarkably, EPA outperformed existing text-based and audio-based augmentation methods across all environments, showing substantial improvement in N-Acc with gains of over 4%, compared to the modest 1-2% improvements of other methods. This trend is particularly pronounced in challenging environments, such as low-accuracy ASR and noisy audio conditions, and is also observed in LLaMA-7B and OPT 6.7B. The ablation study revealed that keyword error augmentation consistently boosted performance across all LLMs, capitalizing on its utility to optimize DST performance against ASR errors."}, {"title": "4.3 Error Analysis", "content": "To examine the effects of keyword augmentation in more detail, we conducted ablation studies and analyzed error reduction rates by categorizing errors into three types. presents the percentage reduction in error rates compared to the baseline. The results demonstrate that EPA is effective in \"Wrong\" and \"Ignore\" error types, and keyword augmentation highly contributed to this improvement by decreasing the error rate from 5.29% to 8.19%. Interestingly, while keyword augmentation led to substantial reductions in the \"Wrong\" type error, it increased the \"Spurious\" error type. This suggests that the model may become overly cautious, occasionally overestimating the presence of named entities."}, {"title": "4.4 Qualitative Assessment of EPA Method", "content": "To evaluate the word diversity and audio similarity of the augmented text, we performed a qualitative assessment in Table 4. This assessment includes metrics such as the unique word increase rate, named entity change rate, and audio similarity \u2074. The results demonstrate that EPA achieves remarkable diversity in unique words (1.81x) and the highest rate of named entity augmentation (95.47%), while maintaining considerable audio similarity (91.57%). Furthermore, keyword augmentation significantly contributes to the improvement, particularly in the named entity change rate (from 68.81% to 91.57%), ensuring that the errors exhibit sufficiently diverse patterns in keywords."}, {"title": "5 Conclusion", "content": "We propose a novel data augmentation method tailored for DST tasks that ensures sufficient error patterns in both key phrases and overall text. By leveraging LLMs for their controlled text generation capabilities, we strategically place errors within key phrases. Our method demonstrates substantially improved robustness in DST by generating diverse, plausible keyword errors. Error case analysis reveals that keyword augmentation significantly enhances robustness against ASR errors. As the pioneering research in leveraging LLMs for generating ASR errors, we hope this work lays a strong foundation for future phonetic-based augmentation research."}, {"title": "Limitations", "content": "First, the introduction of keyword augmentation, while reducing \"Wrong\" and \"Ignore\" errors, resulted in an increase in \"Spurious\" errors, indicating a potential trade-off in error types. Additionally, our experiments were conducted using only the T5-base model. However, we believe that our method will work effectively with other models as well. Furthermore, we did not explore the application of our method to other similar datasets, such as those used in spoken language understanding (SLU) or named entity recognition (NER) tasks. Future work should consider these areas to validate and extend the applicability of our approach."}, {"title": "A Baseline Performance Comparison with Clean Text", "content": "For comparison, we report the baseline performance on an error-free, clean test dataset. Please note that DSTC11  does not provide a text script for the test dataset, so we are manually cleaning 50 dialogues to ensure they are error-free. In the experiment, the baseline model achieved a JGA score of 45.2% and an N-ACC score of 86.5% in an ASR error-free environment. Compared to the JGA, which is 34.8%, and N-ACC, which is 52.07%, in the ASR-errored environment (High-acc ASR model environment), this discrepancy highlights the significant impact of ASR errors on performance degradation."}, {"title": "B Training Details", "content": "In training models, we used T5-base as the backbone model and instructed the model to generate the Bt by given Dt in sequence to sequence manner, as in  and the loss function is\n$$L = -\\sum_{t=1}^{T} log P(B_t | Inst, D_t).$$\nWe set the learning rate as 4e-5 and used the AdamW  optimizer. One GeForce RTX 3090 is used for training and the batch size is 16. Trained until reaching the max patient, which is 3."}, {"title": "C Experiment Details"}, {"title": "C.1 Dataset", "content": "\u2022 Low-acc ASR environment: Whisper-base model (74M) is used for transcription. WER on LibriSpeech.test-clean is 0.05.\n\u2022 Noisy audio environment: Incorporated authentic cafe and traffic noise from with a 10 to 20 Signal-to-Noise Ratio (SNR) and transcribed it using the Whisper large model.\n\u2022 Paraphrased environment: When recording the audio, the text was paraphrased to more closely resemble spoken language.\n\u2022 High-acc ASR environment: Whisper-large model (1550M) is used for transcription. WER on LibriSpeech.test-clean is 0.027."}, {"title": "C.2 Comparison Methods", "content": "\u2022 AEDA : We randomly inserted punctuation marks, effectively maintaining the original word order.\n\u2022 EDA : We augmented data by applying edit-based technique that implements four rule-based modifications-synonym replacement, random insertion, swapping, and deletion.\n\u2022 Back Translation : We translated original texts to error texts and then back to the original texts for generating syntactic variations during the process. We use English to German \u2075 and German to English \u2076 models as translator.\n\u2022 TTS-ASR: We used Tacotron2  for the TTS model to synthesize the audio and use Whisper-base as an ASR model to simulate the ASR errors.\n\u2022 ASR translation: We employed a sequence-to-sequence structure to translate clean text into ASR-errored text. Our training set comprised 300 hours of paired clean and ASR-errored text, as detailed in Section 2.2. We fine-tuned the model based on the T5-base architecture , using the loss function defined in equation 4. The loss function is as follows:\n$$L = -\\sum_{i=1}^{I} log P(e_i | g_i).$$\n\u2022 Generate ASR error augmented text with similar pronunciation but different words based on the given gold text examples.Apply character and word substitutions, additions,or deletions while maintaining the overallpronunciation and context.Error rate should be high"}, {"title": "D Prompt Example", "content": ""}]}