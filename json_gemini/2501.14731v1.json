{"title": "From Critique to Clarity: A Pathway to Faithful and Personalized Code Explanations with Large Language Models", "authors": ["Luo Zhang", "Zexing Xu", "Yichuan Li", "Seyed Rasoul Etesami", "Kyumin Lee"], "abstract": "In the realm of software development, providing accurate and per-sonalized code explanations is crucial for both technical profes-sionals and business stakeholders. Technical professionals benefitfrom enhanced understanding and improved problem-solving skills,while business stakeholders gain insights into project alignmentsand transparency. Despite the potential, generating such explana-tions is often time-consuming and challenging. This paper presentsan innovative approach that leverages the advanced capabilities oflarge language models (LLMs) to generate faithful and personalizedcode explanations. Our methodology integrates prompt enhance-ment, self-correction mechanisms, personalized content customiza-tion, and interaction with external tools, facilitated by collaborationamong multiple LLM agents. We evaluate our approach using bothautomatic and human assessments, demonstrating that our methodnot only produces accurate explanations but also tailors them toindividual user preferences. Our findings suggest that this approachsignificantly improves the quality and relevance of code explana-tions, offering a valuable tool for developers and stakeholders alike.", "sections": [{"title": "1 Introduction", "content": "Code explanations are crucial in the digital landscape, serving asessential learning tools for tech professionals and aligning tech-nical projects with business goals for stakeholders [13, 15, 19]. Ef-forts to address these code understanding challenges have includedtasks such as code summarization and code comment generation.Code summarization provides high-level overviews but often lacksdetailed insights, making it primarily useful for documentationpurposes [3, 21]. Conversely, code comment generation involvesline-by-line commenting, offering detailed explanations but po-tentially overwhelming users seeking a broader understanding[18, 27]. The diversity of user needs-ranging from data scientistsrequiring domain-specific insights to software engineers focusingon architectural details-necessitates a more tailored approach topersonalized code explanation. Personalized code explanations tai-lored to users' backgrounds and knowledge levels are essentialfor effectively conveying complex information [2, 12]. However,creating comprehensive, in-depth, and personalized explanationsis time-consuming and resource-intensive [31, 40], limiting theiravailability and posing challenges for both learners and developers.Besides, Faithfulness which is also very important in code expla-nation. It ensures that the generated text accurately reflects thecode's functionality and logic, avoiding any misinterpretation oroversimplification [4, 16]. Recent advancements in large languagemodels (LLMs) offer a promising solution to the challenges of codeexplanation. LLMs have demonstrated exceptional performanceacross a diverse array of tasks, primarily due to their enhanced rea-soning capabilities [6, 23]. The efficacy of LLMs in tackling complextasks heavily relies on advanced prompt engineering techniques,such as chain-of-thought (CoT) prompting [55]. This method, alongwith iterative prompting and question decomposition, enhancesthe logical flow and clarity of explanations [14, 17, 36, 46, 47, 56].These approaches collectively contribute to the substantial promisein improving the accuracy and effectiveness of LLM-generatedresponses in solving complex problems. However, generating per-sonalized and faithful code explanations requires more than a singleprompt [5, 34]; it necessitates complex and sequential prompt de-sign.\nCurrent methods for code explanation face significant challengesin providing faithful, personalized explanations, and balancing var-ious requirements. Many approaches struggle with faithfulness,often producing explanations that are syntactically correct but se-mantically incorrect, leading to misunderstandings or errors inin code interpretation [29]. For example, an LLM might describe asorting algorithm correctly in terms of its steps but fail to explainits actual complexity or edge cases. Ensuring personalization isanother challenge, as generic explanations that ignore the user'sbackground, expertise, or specific needs result in less effective com-munication [1]. For instance, novice programmers might requirestep-by-step explanations, while experienced developers mightprefer summaries. Additionally, balancing accuracy, completeness,"}, {"title": "2 Problem Definition", "content": "Our research is based on a formalized code problem dataset, con-sisting of n individual problems. Each problem, denoted as p, islinked to a single human-generated oracle solution, s. To generate afaithful explanation e for a problem-solution pair (p, s), we samplefrom the model's distribution PM conditioned on the prompt &,problem p, and solution s:\n$e \\sim P_M(\\phi \\oplus p \\oplus s)$"}, {"title": "3 Method", "content": "Figure 1 provides an overview of the method. Inspired by the itera-tive approach humans adopt in writing, our approach introduces aniterative explanation refinement process, which is adapted into twospecific loops: the faithfulness loop and the personalization loop.These loops work in tandem to generate a code explanation that isboth technically faithful and personalized to the user's backgroundand programming skills. It is worth noting that we observed a po-tential trade-off between pursuing personalization and maintainingfaithfulness simultaneously, as optimizing both may lead to com-promises. Mili\u010dka et al. [41] also found that LLMs can downplaytheir cognitive abilities to fit the personas they simulate. Therefore,we designed these loops as independent components to maximizeeach objective.\n3.1 Iterative Explanation Refinement\nDrawing inspiration from the iterative refinement employed byhumans in writing, our proposed methodology for generating high-quality code explanations consists of a three-stage process: reflec-tion, iterative explanation, and verification and analysis. This sys-tematic approach ensures continuous improvement by detectingand rectifying errors that arise from real-world interactions. Al-though both the faithfulness and personalization loops leveragethese shared stages, they adapt them to meet their distinct objec-tives, reflecting the tailored nature of each loop.\nReflection. In the reflecting stage, the method leverages the sum-marization capabilities of LLMs Jin et al. [20] to efficiently extractkey information from a given context, such as a problem descrip-tion or a user's historical inquiries on Stack Overflow. The outputfrom the reflection stage serves as input for more complex tasks, acrucial component of the subsequent stages' requirements. Thus,the reflection stage plays a crucial role in knowledge accumulationand progression, providing the necessary insights and informationto support subsequent, more challenging steps Ridnik et al. [50].\nInitialization and Refinement. This stage, pivotal for enhancingcode explanations, entails two key tasks: initial setup and iterativerefinement. To commence, we provide the LLM with the code so-lution, context, and knowledge from the Reflection stage as input.And utilize the chain-of-thought (CoT) methodology described byWei et al. [55] to initiate code explanations. For example, in the caseof the faithful loop, the LLM first generates a detailed, sequentialexplanation and then provides a high-level understanding of thecode. If the initial explanation fails to meet certain criteria, a revi-sion process ensues. Here, the LLM is fed with the code, context,the previously generated explanation, and knowledge from the Ver-ification and Analysis stage, which is instrumental in pinpointingerrors and offering actionable suggestions through external toolinteractions. This iterative process continues until predeterminedstopping conditions are met, ensuring continuous improvement inthe explanation quality.\nVerification and Analysis. Recent studies have demonstrated thecapacity of LLMs to interact with external tools Paranjape et al.[45], Wu et al. [57], Yuan et al. [58], enhancing their ability toscrutinize and refine their initial responses. The central conceptof this stage involves the LLM engaging with external utilities,such as a Python executor or another LLM, to verify the previouslygenerated explanation. If the external tool output indicates thatthe previous generated explanation satisfies specific criteria, therefining loop terminate. Otherwise, the LLM is required to analyzethe error and provide some revision suggestions for the followingexplanation improvement.\n3.2 Faithfulness Loop\nThe 3-stage refinement is suitable for this process, but some ad-justments are necessary. In the Reflection stage, given the complexand intricate code problem p, we ask the LLM to extract the prob-lem goals, inputs, outputs, conditions and other relevant details,represented as pr.\nIn the Iterative Explanation stage, the initialization step generatesan initial step-by-step description and high-level explanation e0based on the problem p, the accepted code solution s, and the prob-lem reflection pr generated by the reflecting stage. The revisionstep generates an improved explanation ei+1 based on the problemp, the code solution s, the problem reflection pr, the previous ex-planation ei, the verification code solution vsi, the executor outputeoi, and the failure analysis ai.\nIn the Verification and Analysis stage, to verify if the code ex-planation is faithful, we test how much it can aid in solving theproblem by utilizing the code generating ability of LLMs Li et al.[26], Ni et al. [42], Ridnik et al. [50]. Given the problem p and theprevious explanation ei, a verification code solution usi is generated.Then verification code solution is executed to obtain the outputeoi, which is compared against the public test cases. If the outputis incorrect, the LLM analyzes the error and generates an analysisai based on the problem p, the code solution s, the problem reflec-tion pr, the verification solution code vsi, and the executor outputeoi. We found that LLMs excel more in finding code-related issuescompared to textual problems. Therefore, during error analysis, wetask the LLM to analyze errors in the verification solution code,and based on this analysis, we modify the code explanation.\n3.3 Personalization Loop\nIn addition to faithfulness, the acceptance of code explanationsby the audience is crucial. People with different backgrounds andprogramming skills have varying requirements for code explana-tions. Therefore, we need this step to produce personalized codeexplanations. Different from existing studies [9, 24, 52, 54] on role-playing LLMs, which focus on using demographic tags and conver-sation history data to simulate personas, our approach leveragesusers' actions-specifically their inquiry history about Python, datastructures, and algorithms on Stack Overflow-to infer their pro-gramming profile. This method allows the LLM to represent their"}, {"title": "4 Experiment Setup", "content": "Code Problem Dataset. This study utilizes the CodeContests dataset[26], sourced from competitive programming platforms such asCodeforces, to ensure the robustness and validity of our findings.To mitigate data leakage, we exclusively rely on the validationand test sets as our primary data sources. We rigorously filter outproblems with image-based descriptions and those lacking oraclePython solutions. The validation set comprises 67 authentic contestproblems collected from various online platforms, while the testset consists of 102 instances. Moreover, each problem in the datasetincludes multiple oracle solutions. Given the constraints on thecontext window size of LLMs [60], we adopt the shortest solutionfor each problem.\nInquiry History Dataset. To collect the real user coding pref-erence, we collected user profiles from Stack Overflow\u00b2 using ananonymized dump of all user-contributed content on the website[48], which includes questions, answers, comments, tags, and re-lated data. Specifically, we choose 10 users from the dataset andfor each user, we sampled their five most recent inquiries relatedto Python, data structures, and algorithms, which include the title,tags, and body of each inquiry.\nBaseline. Several studies have investigated the application ofLLMs in generating code explanations Brusilovsky et al. [7], Chenet al. [8], Leinonen et al. [22], Li et al. [25], MacNeil et al. [33, 35], Oliet al. [43], Sarsa [51], yet their approaches often center on thefeasibility of LLMs with simple prompts. Li et al. [25]'s work stands"}, {"title": "5 Results and Analysis", "content": "In this section, we present and analyze the results from two keyangles: faithfulness and personalization. These angles provide in-sight into how well the generated explanations assist in solvingcode problems and how effectively they cater to individual users'profiles."}, {"title": "6 Conclusion", "content": "In this paper, we propose explaining competitive-level program-ming solutions using LLMs with a iterative methodology that com-bines prompt enhancement, self-correction capabilities, person-alized content customization according to user preferences, andefficient integration with external resources, as well as facilitationof collaboration among various LLM agents. Our evaluation demon-strates that the method can generate more faithful code explana-tions which can guide another LLM to better solve the problem.Also, the method can generate personalized code explanations thatalign better with individual preferences, no matter evaluated bythe automatic evaluation or the human evaluation.\nOur explanation method can potentially be applied to annotatelarge-scale data (e.g., the full CodeContests training set), yieldingthousands of silver explanations that can be used to fine-tune areasoning model for competitive-level programming problems. Thisapproach could help bridge the long-standing reasoning gap be-tween problem and program for complex programming problems.Moving forward, we aim to further address solving such problemsby focusing on enhancing reasoning for programming problems."}]}