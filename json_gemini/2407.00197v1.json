{"title": "Tradeoffs When Considering Deep Reinforcement Learning for Contingency Management in Advanced Air Mobility", "authors": ["Luis E. Alvarez", "Marc W. Brittain", "Steven D. Young"], "abstract": "Air transportation is undergoing a rapid evolution globally with the introduction of Advanced Air Mobility (AAM) and with it comes novel challenges and opportunities for transforming aviation. As AAM operations introduce increasing heterogeneity in vehicle capabilities and density, increased levels of automation are likely necessary to achieve operational safety and efficiency goals. This paper focuses on one example where increased automation has been suggested. Autonomous operations will need contingency management systems that can monitor evolving risk across a span of interrelated (or interdependent) hazards and, if necessary, execute appropriate control interventions via supervised or automated decision making. Accommodating this complex environment may require automated functions (autonomy) that apply artificial intelligence (AI) techniques that can adapt and respond to a quickly changing environment. This paper explores the use of Deep Reinforcement Learning (DRL) which has shown promising performance in complex and high-dimensional environments where the objective can be constructed as a sequential decision-making problem. An extension of a prior formulation of the contingency management problem as a Markov Decision Process (MDP) is presented and uses a DRL framework to train agents that mitigate hazards present in the simulation environment. A comparison of these learning-based agents and classical techniques is presented in terms of their performance, verification difficulties, and development process.", "sections": [{"title": "I. Introduction", "content": "THE air transportation system is currently undergoing a rapid evolution with the introduction of novel concepts and the demand for an increase in operational efficiency. One of those novel concepts is Advanced Air Mobility (AAM), which foresees the transport of cargo and passengers across cities, as well as communities currently under-served by aviation. These AAM operations may include electric vertical take-off and landing (eVTOL) aircraft, high-density operations, and a combination of piloted (i.e., remote or on-board) and autonomous flights. Across the world, organizations such as the FAA, NASA, and EASA expect these operations to increase in density from a single operation per hour to over 100 simultaneous operations per hour over a local region [1\u20134]. The FAA and NASA refer to the increasing phases of complexity for AAM operations as Urban Air Mobility (UAM) Maturity Levels (UML). Low-density human piloted airspace operations are classified as UML 1, while highly autonomous and dense operations are described as UML 4 and beyond. Evident in NASA's and the FAA's concepts of operations, is an expectation that some types of operations will transition from human piloted to fully autonomous operations. Artificial Intelligence (AI) techniques are viewed as one way to enable higher levels of autonomy because the operations will need to make safety critical"}, {"title": "II. Background", "content": "Prior research has demonstrated Deep Reinforcement Learning (DRL) systems can outperform classical approaches in a variety of complex decision-making tasks. Unlike heuristic-based systems, which use predefined rules and expert knowledge, DRL systems learn and adapt to new situations through experience and interaction with the environment. As an example, to support traffic separation assurance in commercial aviation, researchers have implemented a variety of approaches to train AI agents to resolve traffic conflicts. Each implementation models uncertainty as an MDP where"}, {"title": "III. Problem Formulation", "content": "This section explains the approach to establish the algorithms, models, scenarios, and metrics used for the testing of the selected CM agents."}, {"title": "A. DRL Algorithms", "content": "Basing the DRL contingency management framework on prior AAM-Gym developments [17, 33] allowed the research team to quickly adapt and test algorithms that adopt the OpenAI [34] interface. Two algorithms, D2MAV [35]"}, {"title": "B. Baseline Simulation and Heuristic Algorithm", "content": "As baselines, this paper uses a heuristic based agent inspired by [39] and nominal simulations where no agent interacts with the aircraft. The heuristic based agent has equivalent state space information as the DRL with the additional knowledge of the nodes that define the corridor network. The heuristic based agent uses estimates of the energy reserves during flight and constantly monitors the energy required to continue the flight plan as well as the probability of loss-of-control. The criteria for taking control of the aircraft is defined by Algorithm 1 in the Appendix and is summarized as follows:\n1) Calculate the remaining flight time available and required flight time to continue route, based on energy reserves.\n2) Reroute aircraft if the energy required to complete flight plan is greater than energy reserves.\n3) Reroute aircraft if it experiences or encounters any hazard along the flight path.\n4) Reroute aircraft if there is a hazardous situation at the destination that would preclude landing (i.e., $P_{H_{dest}}$ is greater than zero).\n5) Reroute utilizing corridor network if aircraft can sustain flight, and there is no hazard found along the path.\n6) If flight path through corridor network is not possible, reroute directly to closest vertiport whose direct path does not cross a hazardous area or has the lowest total risk."}, {"title": "C. Hazard Modeling", "content": "The framework allows for the introduction of a variety of hazard types that may impact aircraft dynamics and state. In this paper the hazards of interest are 1) wind, 2) no fly zones, 3) regions of high-density population (modeled as risk fields) 4) increased energy consumption due to the charge cycle of the battery, and 5) navigation system performance where aircraft experience a loss-of-control. 3D wind fields are generated using a back-end simulator (BlueSky [38]). For training initial tasks, a constant zero-wind is used. Then, as tasks and training get more complex, randomly generated winds fields are used. This is further discussed in Sec IV.D. A New York City population density model is extracted from the Oak Ridge National Library population database and merged with the temporal component of the New York City Taxi and Limousine Trip Record Data [40, 41]. This population data is used with a ballistic trajectory model, defined by [39], to estimate the probability of casualties when aircraft reaches zero energy."}, {"title": "D. Aircraft Energy Modeling", "content": "The aircraft available energy is modeled by the linear energy model defined in [17]. The model is modified to directly initialize the aircraft with an energy level defined by a uniform distribution [20, 250] KWh. The energy model uses three properties to calculate the energy consumed for a given time step: 1) the battery life cycle for the battery assigned to the aircraft, 2) a constant energy consumption rate, and 3) a battery failure probability assigned to each aircraft. The probability of battery failure is an additional feature added to the energy model to further introduce uncertainty in the likelihood of aircraft reaching their desired destination even with high energy levels assigned at the beginning of flight. During agent training these factors on average lead to total flight times between 2\u201341 minutes. This range in flight time allows the DRL algorithms to experience complete energy failure of the aircraft and learn to divert aircraft to alternative vertiports or return to the departure vertiport. During evaluation of the algorithms, the range of initial energy is modified to explore the behavior learned."}, {"title": "E. Scenario Generation", "content": "Training and evaluation of DRL agents requires an accrual of experience by interacting with a simulation environment to learn the optimal policy. For the testing reported in this paper, the environment is initialized by randomly sampling from 40 days of synthetic AAM traffic generated with UAMToolKit, an event driven AAM traffic capacity and demand modeling tool [42]. The toolkit assumes a total fleet size of 100 aircraft. Since the focus of this research is on the cruise/enroute phase of the flight, the UAMToolKit parameters for capacity at vertiports, passenger capacity, turn-around times, and wind data are selected to maximize the number of airborne aircraft. Thus, the constraint on vertiport and"}, {"title": "F. Metrics", "content": "All aircraft state and environment information was recorded for post processing and evaluation. In addition, the following metrics were calculated during run time: total reward experienced by each aircraft, aircraft reaching each terminal state (i.e., reaching destination vertiport, rerouted to alternate vertiport, return to departure vertiport, experience loss-of-control, energy depleted), time spent in flight route, contingency actions taken, latitudes and longitudes traversed, and the projected probability of casualty when reaching the terminal state."}, {"title": "IV. Algorithm Design", "content": "This section discusses design and training of the CM agents using two previously developed DRL algorithms. We extend a previous DRL framework [17] by introducing additional state information, rewards, and an implementation of a heuristic based approach."}, {"title": "A. Curriculum Training", "content": "Undertaking the development of a DRL agent requires understanding the use case objectives and fine tuning each reward parameter to achieve the desired behavior. Inspired by the human learning process, we adopt a curriculum learning strategy where the agent is sequentially trained through a set of increasingly complex tasks supportive of CM for UAM operations. Each task builds upon the previous one, allowing the agents to learn robust contingency management strategies, as shown in Table 2.\nEach agent is pre-loaded with a set of possible vertiport landing sites and the planned destination. At each time step in a simulated flight, the agent must decide whether to intervene and reroute the aircraft to an alternate vertiport, return to the departure vertiport, or continue to its original destination. In this paper, the factors that influence an agent's CM decisions are available power, wind, navigation system performance, no-fly zones, and underlying population density."}, {"title": "B. State Space", "content": "Each aircraft, referred to as ownship, in the simulation environment is controlled by an independent instance of the contingency management agent. The ownship's state s at time t is defined by Eq 1.\n$St = Ot + Vt + ht$   (1)\n$Ot = (\\psi, z, \\dot{v}_{xy}, v_{xy}, E, \\dot{E}, P_c, W_y, W_x, (x_{wpt}^{(j)}-x, y_{wpt}^{(j)} - y, P_H^{(j)}, P_{dest}^{(j)} ) \\forall j \\in [1, N_{wpt}] $   (2)\nwhere $O_t$ represents ownship's heading ($\\psi$), altitude (z), horizontal acceleration ($\\dot{v}_{xy}$), horizontal true air speed ($v_{xy}$), remaining energy (E), and energy used within a time step ($\\dot{E}$). $P_c$ is the probability of casualties in the event of loss of aircraft control due to complete battery failure or a loss-of-control field. $W_y$ and $W_x$ represent the north and east wind speed respectively. Additionally, $N_{wpt}$ number of waypoints along the original flight plan are provided with relative east (X) and north (Y) location to ownship $(x_{wpt}^{(j)}-x, y_{wpt}^{(j)} - y)$, hazard function output at waypoint $(P_H^{(j)})$, and distance to destination from waypoint ($P_{dest}^{(j)}$).\n$Vt = (\\Theta_{dest}, \\rho_{dest}, P_{H_{dest}}, (P_H^{(j)}) ) \\forall j \\in [1, N_\\Theta + 1]$,   (3)\n$V_t$ contains the ownship relative bearing to destination ($\\Theta_{dest}$), distance to destination ($\\rho_{dest}$), hazard function output at destination ($P_{H_{dest}}$), and $P_H^{(j)}$, the hazard function output of $N_\\Theta$ points within a radius $R_{field}$ from the ownship, such that these points are evenly distributed in a circle around ownship and an additional point at the current location of ownship. The angle between each of these adjacent points is given by:\n$\\Delta \\Theta = \\frac{360^\\circ}{N_\\Theta}$ (4)\n$h_t = (\\rho^{(i)}, \\Theta^{(i)}, P_H^{(i)} ) \\forall i \\in [1, N_{vertiports}]$\nThe vertiport state information $h^{(i)}$ at time t includes the relative distance from ownship to the vertiport ($\\rho^{(i)}$), relative bearing to ownship ($\\Theta^{(i)}$) and probability of loss of aircraft control at the vertiport ($P_H^{(i)}$), defined by a hazard function."}, {"title": "C. Reward Model", "content": "In prior research we presented a reward model formulated for contingency management DRL agents [17]. This section discusses extensions of the reward model and the observed or expected behavior that led to changes in the reward model. Due to the curriculum training approach undertaken, not all rewards are active in each curriculum task, and their specific use will be highlighted in the results. The action space is extended from prior research to include two additional heading change actions of \u00b11\u00b0 for the agent to fine tune the desired heading towards a vertiport, see Table 3."}, {"title": "D. Experiment Setup", "content": "A span of tests were designed to help understand and advance the overarching framework (i.e., Objective 1) as well as to evaluate the set of CM agents previously described (Objective 2). The Lincoln Laboratory Supercomputer Center (LLSC) was utilized in this study to meet the substantial data requirements for training and evaluating the DRL agents. The compute architecture comprised 16 Intel Xeon Gold 6248 2.5 GHz compute nodes, each equipped with two NVIDIA Tesla V100 graphics processing units (GPUs) [43]. Each curriculum task listed in Table 2 necessitated the careful introduction of rewards and state inputs to achieve satisfactory performance. This performance was attained by training a DRL agent with 40 distinct parallel simulations, each independently running 3-hour windows of operations. Training at each curriculum task involved 20,000 iterations, with each iteration consisting of 64 training steps (equivalent to 5 simulation seconds) per 40 distinct parallel simulation. Consequently, each curriculum task required an average of 68 hours of wall-clock time.\nIn curriculum task T3, the centers of the loss-of-control and no-fly zone hazard regions are randomly initialized to align with a node in the network as designed in [42]. Wind speed and direction are randomly sampled from independent uniform distributions ranging in [0, 10] knots and [0, 360] degrees. The velocity of aircraft is initialized using a uniform distribution with velocities ranging between [5, 65] knots. A total of 100 simulated days of operations are used for"}, {"title": "V. Results", "content": "In this section we discuss the observed performance of each agent versus nominal operations (i.e. unequipped). Figures 2 to 5 summarize collected data; which includes, for each agent, 100 evaluations repeatedly simulated with a unique random seed used for all agents. Over 798 million operations providing over 266 million hours of flight data were collected for the DRL agents to achieve the performance through the curriculum learning. Overall when the aircraft fleet is initialized with energy reserves sampled from a uniform distribution equivalent to the training scenarios, 20-250 KWh, the DRL based agent successfully reroutes an average of 95.2% ($\\sigma_2$ = \u00b1 1.08%) and 91.6% ($\\sigma_2$ = \u00b1 1.29%) of aircraft to vertiports, for SACD-A and D2MAV-A respectively. The heuristic agent outperforms the nominal (unequipped) operations, however it only successfully reroutes 76.1% ($\\sigma_2$ = \u00b1 6.07%) to a vertiport as compared to (46.2%) for the nominal operations. Closer inspection of Fig. 2a reveals that each agent maintains a comparable number of aircraft reaching their destination as in the nominal case with SACD-A rerouting more aircraft to their destination if the original flight plan is unfeasible. Since all agents only have $N_{wpt}$ of flight path information they must utilize state information near ownship to sense regions of loss-of-control (i.e., $P_H^{(j)}$, $P_{H_{dest}}$ as defined in Section IV). While the heuristic agent allows for tracks to use routes with a risk of loss-of-control below $P_{H_{threshold}}$ to reduce situations where no route is possible, more aircraft are found to experience loss-of-control than $P_{H_{threshold}}$ and reducing this value leads to a tradeoff in more aircraft experiencing energy depletion due to the longer flight paths selected. This tradeoff is due to a lack of global information resulting in sub-optimal routing causing 21.8% ($\\sigma_2$ = \u00b1 5.612%) of aircraft to experience loss-of-control. The SACD-A nearly eliminates loss-of-control risk with only 0.07% ($\\sigma_2$ = \u00b1 0.119%) of aircraft encountering it. This is achieved by SACD-A exploiting the state space information in training, Fig. 3, and rerouting aircraft around the hazard region, as seen in Fig. 4. Even with intensive training, Fig. 4 shows the flight trajectories"}, {"title": "VI. Conclusion", "content": "A set of learning-based in-time decision-making agents were trained and evaluated as supportive of automation CM for AAM operations in the presence of hazards. The learning-based agents show DRL can meet complex safety objectives and are robust to uncertainty in the environment. When compared to a heuristic based approach, the DRL agents have a lower standard deviation in decision-making variability; with results within two standard deviations varying by less than 2% as compared to 6% indicating a higher level of understanding of the uncertainties in the environment. These results show promise for agents that can achieve both safety and operational efficiency objectives. To achieve the performance shown, over 798 million operations providing over 266 million hours of flight experience were collected for the DRL agents throughout the entire curriculum. This large amount of training experience highlights a tradeoff with expert knowledge-based heuristic systems. Careful tuning is also required to avoid catastrophic forgetting during training. Additionally, a notable tradeoff of using learning-based approaches is the challenge of achieving safety-critical assurance levels for ML-based agents. Rigorous verification and validation processes are needed, both for the agents themselves and the tools used to test them (e.g., the simulation environment). To help with this challenge, this work presents a framework to design, test, and evaluate a variety of algorithm approaches at scales previously not possible; and collect evidence such as suggested by [32]. As aviation moves towards increasingly autonomous systems, the development of comprehensive validation frameworks is essential to ensure safety and reliability."}, {"title": "VII. Future Work", "content": "Future research will focus on enhancing the robustness and efficacy of learning-based approaches in contingency management systems for AAM. Firstly, we will integrate probability of casualty and the real-time risk assessment method suggested by [39] to the heuristic agent and compare the performance versus the DRL agents. Additionally, we intend to extend the range of hazards to include dynamic hazards and multiple hazards within the environment. Lastly, we aim to evaluate the interoperability of the agents with collision avoidance agents. This is needed to ensure seamless and appropriate coordination when independent autonomous agents may affect flight."}]}