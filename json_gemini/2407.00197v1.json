{"title": "Tradeoffs When Considering Deep Reinforcement Learning for Contingency Management in Advanced Air Mobility", "authors": ["Luis E. Alvarez", "Marc W. Brittain", "Steven D. Young"], "abstract": "Air transportation is undergoing a rapid evolution globally with the introduction of Advanced Air Mobility (AAM) and with it comes novel challenges and opportunities for transforming aviation. As AAM operations introduce increasing heterogeneity in vehicle capabilities and density, increased levels of automation are likely necessary to achieve operational safety and efficiency goals. This paper focuses on one example where increased automation has been suggested. Autonomous operations will need contingency management systems that can monitor evolving risk across a span of interrelated (or interdependent) hazards and, if necessary, execute appropriate control interventions via supervised or automated decision making. Accommodating this complex environment may require automated functions (autonomy) that apply artificial intelligence (AI) techniques that can adapt and respond to a quickly changing environment. This paper explores the use of Deep Reinforcement Learning (DRL) which has shown promising performance in complex and high-dimensional environments where the objective can be constructed as a sequential decision-making problem. An extension of a prior formulation of the contingency management problem as a Markov Decision Process (MDP) is presented and uses a DRL framework to train agents that mitigate hazards present in the simulation environment. A comparison of these learning-based agents and classical techniques is presented in terms of their performance, verification difficulties, and development process.", "sections": [{"title": "I. Introduction", "content": "HE air transportation system is currently undergoing a rapid evolution with the introduction of novel concepts and\nthe demand for an increase in operational efficiency. One of those novel concepts is Advanced Air Mobility (AAM),\nwhich foresees the transport of cargo and passengers across cities, as well as communities currently under-served by\naviation. These AAM operations may include electric vertical take-off and landing (eVTOL) aircraft, high-density\noperations, and a combination of piloted (i.e., remote or on-board) and autonomous flights. Across the world,\norganizations such as the FAA, NASA, and EASA expect these operations to increase in density from a single operation\nper hour to over 100 simultaneous operations per hour over a local region [1\u20134]. The FAA and NASA refer to the\nincreasing phases of complexity for AAM operations as Urban Air Mobility (UAM) Maturity Levels (UML). Low-density\nhuman piloted airspace operations are classified as UML 1, while highly autonomous and dense operations are described\nas UML 4 and beyond. Evident in NASA's and the FAA's concepts of operations, is an expectation that some types of\noperations will transition from human piloted to fully autonomous operations. Artificial Intelligence (AI) techniques\nare viewed as one way to enable higher levels of autonomy because the operations will need to make safety critical"}, {"title": "II. Background", "content": "Prior research has demonstrated Deep Reinforcement Learning (DRL) systems can outperform classical approaches\nin a variety of complex decision-making tasks. Unlike heuristic-based systems, which use predefined rules and expert\nknowledge, DRL systems learn and adapt to new situations through experience and interaction with the environment.\nAs an example, to support traffic separation assurance in commercial aviation, researchers have implemented a variety\nof approaches to train AI agents to resolve traffic conflicts. Each implementation models uncertainty as an MDP where"}, {"title": "III. Problem Formulation", "content": "This section explains the approach to establish the algorithms, models, scenarios, and metrics used for the testing of\nthe selected CM agents."}, {"title": "A. DRL Algorithms", "content": "Basing the DRL contingency management framework on prior AAM-Gym developments [17, 33] allowed the\nresearch team to quickly adapt and test algorithms that adopt the OpenAI [34] interface. Two algorithms, D2MAV [35]"}, {"title": "B. Baseline Simulation and Heuristic Algorithm", "content": "As baselines, this paper uses a heuristic based agent inspired by [39] and nominal simulations where no agent\ninteracts with the aircraft. The heuristic based agent has equivalent state space information as the DRL with the\nadditional knowledge of the nodes that define the corridor network. The heuristic based agent uses estimates of the\nenergy reserves during flight and constantly monitors the energy required to continue the flight plan as well as the\nprobability of loss-of-control. The criteria for taking control of the aircraft is defined by Algorithm 1 in the Appendix\nand is summarized as follows:\n1) Calculate the remaining flight time available and required flight time to continue route, based on energy reserves.\n2) Reroute aircraft if the energy required to complete flight plan is greater than energy reserves.\n3) Reroute aircraft if it experiences or encounters any hazard along the flight path.\n4) Reroute aircraft if there is a hazardous situation at the destination that would preclude landing (i.e., $P_{Hdest}$ is\ngreater than zero).\n5) Reroute utilizing corridor network if aircraft can sustain flight, and there is no hazard found along the path.\n6) If flight path through corridor network is not possible, reroute directly to closest vertiport whose direct path does\nnot cross a hazardous area or has the lowest total risk."}, {"title": "C. Hazard Modeling", "content": "The framework allows for the introduction of a variety of hazard types that may impact aircraft dynamics and state.\nIn this paper the hazards of interest are 1) wind, 2) no fly zones, 3) regions of high-density population (modeled as risk\nfields) 4) increased energy consumption due to the charge cycle of the battery, and 5) navigation system performance\nwhere aircraft experience a loss-of-control. 3D wind fields are generated using a back-end simulator (BlueSky [38]). For\ntraining initial tasks, a constant zero-wind is used. Then, as tasks and training get more complex, randomly generated\nwinds fields are used. This is further discussed in Sec IV.D. A New York City population density model is extracted\nfrom the Oak Ridge National Library population database and merged with the temporal component of the New York\nCity Taxi and Limousine Trip Record Data [40, 41]. This population data is used with a ballistic trajectory model,\ndefined by [39], to estimate the probability of casualties when aircraft reaches zero energy."}, {"title": "D. Aircraft Energy Modeling", "content": "The aircraft available energy is modeled by the linear energy model defined in [17]. The model is modified to\ndirectly initialize the aircraft with an energy level defined by a uniform distribution [20, 250] KWh. The energy model\nuses three properties to calculate the energy consumed for a given time step: 1) the battery life cycle for the battery\nassigned to the aircraft, 2) a constant energy consumption rate, and 3) a battery failure probability assigned to each\naircraft. The probability of battery failure is an additional feature added to the energy model to further introduce\nuncertainty in the likelihood of aircraft reaching their desired destination even with high energy levels assigned at the\nbeginning of flight. During agent training these factors on average lead to total flight times between 2\u201341 minutes. This\nrange in flight time allows the DRL algorithms to experience complete energy failure of the aircraft and learn to divert\naircraft to alternative vertiports or return to the departure vertiport. During evaluation of the algorithms, the range of\ninitial energy is modified to explore the behavior learned."}, {"title": "E. Scenario Generation", "content": "Training and evaluation of DRL agents requires an accrual of experience by interacting with a simulation environment\nto learn the optimal policy. For the testing reported in this paper, the environment is initialized by randomly sampling\nfrom 40 days of synthetic AAM traffic generated with UAMToolKit, an event driven AAM traffic capacity and demand\nmodeling tool [42]. The toolkit assumes a total fleet size of 100 aircraft. Since the focus of this research is on the\ncruise/enroute phase of the flight, the UAMToolKit parameters for capacity at vertiports, passenger capacity, turn-around\ntimes, and wind data are selected to maximize the number of airborne aircraft. Thus, the constraint on vertiport and"}, {"title": "F. Metrics", "content": "All aircraft state and environment information was recorded for post processing and evaluation. In addition, the\nfollowing metrics were calculated during run time: total reward experienced by each aircraft, aircraft reaching each\nterminal state (i.e., reaching destination vertiport, rerouted to alternate vertiport, return to departure vertiport, experience\nloss-of-control, energy depleted), time spent in flight route, contingency actions taken, latitudes and longitudes traversed,\nand the projected probability of casualty when reaching the terminal state."}, {"title": "IV. Algorithm Design", "content": "This section discusses design and training of the CM agents using two previously developed DRL algorithms. We\nextend a previous DRL framework [17] by introducing additional state information, rewards, and an implementation of\na heuristic based approach."}, {"title": "A. Curriculum Training", "content": "Undertaking the development of a DRL agent requires understanding the use case objectives and fine tuning each\nreward parameter to achieve the desired behavior. Inspired by the human learning process, we adopt a curriculum\nlearning strategy where the agent is sequentially trained through a set of increasingly complex tasks supportive of\nCM for UAM operations. Each task builds upon the previous one, allowing the agents to learn robust contingency\nmanagement strategies, as shown in Table 2.\nEach agent is pre-loaded with a set of possible vertiport landing sites and the planned destination. At each time step\nin a simulated flight, the agent must decide whether to intervene and reroute the aircraft to an alternate vertiport, return\nto the departure vertiport, or continue to its original destination. In this paper, the factors that influence an agent's CM\ndecisions are available power, wind, navigation system performance, no-fly zones, and underlying population density."}, {"title": "B. State Space", "content": "Each aircraft, referred to as ownship, in the simulation environment is controlled by an independent instance of the\ncontingency management agent. The ownship's state $s$ at time $t$ is defined by Eq 1.\n$s_{t} = O_{t} + V_{t} + h_{t}$   (1)\n$O_{t} = (\\psi, z, \\dot{v_{xy}}, v_{xy}, E, \\dot{E}, P_{c}, W_{y}, W_{x}, (x^{(j)}_{wpt}- x, y^{(j)}_{wpt}- y, P^{(j)}_{H}, P^{(j)}_{dest}) \\forall j \\in [1, N_{wpt}]$   (2)\nwhere $O_t$ represents ownship's heading $(\\psi)$, altitude $(z)$, horizontal acceleration $(\\dot{v_{xy}})$, horizontal true air speed\n$(v_{xy})$, remaining energy $(E)$, and energy used within a time step $(\\dot{E})$. $P_c$ is the probability of casualties in the event of\nloss of aircraft control due to complete battery failure or a loss-of-control field. $W_y$ and $W_x$ represent the north and\neast wind speed respectively. Additionally, $N_{wpt}$ number of waypoints along the original flight plan are provided with\nrelative east (X) and north (Y) location to ownship $(x^{(j)}_{wpt}- x, y^{(j)}_{wpt}- y)$, hazard function output at waypoint $(P^{(j)}_{H})$, and\ndistance to destination from waypoint $(P^{(j)}_{dest})$.\n$V_{t} = (\\Theta_{dest}, \\rho_{dest}, P^{H}_{Hdest}, (P^{y}_{H} )) \\forall j \\in [1, N_{\\Theta} + 1]$,  (3)\n$V_t$ contains the ownship relative bearing to destination $(\\Theta_{dest})$, distance to destination $(\\rho_{dest})$, hazard function output\nat destination $(P^{H}_{Hdest})$, and $P^{y}_{H}$), the hazard function output of $N_{\\Theta}$ points within a radius $R_{field}$ from the ownship, such\nthat these points are evenly distributed in a circle around ownship and an additional point at the current location of\nownship. The angle between each of these adjacent points is given by:\n$\\Delta \\Theta = \\frac{360^\\circ}{N_{\\Theta}}$\n$h_{t} = (\\rho^{(i)}, \\theta^{(i)}, P^{(i)}_{H}) \\forall i \\in [1, N_{vertiports}]$\n(4)\nThe vertiport state information $h^{(i)}$ at time $t$ includes the relative distance from ownship to the vertiport $(\\rho^{(i)})$,,\nrelative bearing to ownship $(\\theta^{(i)})$ and probability of loss of aircraft control at the vertiport $(P^{(i)}_{H})$, defined by a hazard\nfunction."}, {"title": "C. Reward Model", "content": "In prior research we presented a reward model formulated for contingency management DRL agents [17]. This\nsection discusses extensions of the reward model and the observed or expected behavior that led to changes in the\nreward model. Due to the curriculum training approach undertaken, not all rewards are active in each curriculum task,\nand their specific use will be highlighted in the results. The action space is extended from prior research to include\ntwo additional heading change actions of \u00b11\u00b0 for the agent to fine tune the desired heading towards a vertiport, see Table 3."}, {"title": "D. Experiment Setup", "content": "A span of tests were designed to help understand and advance the overarching framework (i.e., Objective 1) as\nwell as to evaluate the set of CM agents previously described (Objective 2). The Lincoln Laboratory Supercomputer\nCenter (LLSC) was utilized in this study to meet the substantial data requirements for training and evaluating the DRL\nagents. The compute architecture comprised 16 Intel Xeon Gold 6248 2.5 GHz compute nodes, each equipped with two\nNVIDIA Tesla V100 graphics processing units (GPUs) [43]. Each curriculum task listed in Table 2 necessitated the\ncareful introduction of rewards and state inputs to achieve satisfactory performance. This performance was attained by\ntraining a DRL agent with 40 distinct parallel simulations, each independently running 3-hour windows of operations.\nTraining at each curriculum task involved 20,000 iterations, with each iteration consisting of 64 training steps (equivalent\nto 5 simulation seconds) per 40 distinct parallel simulation. Consequently, each curriculum task required an average of\n68 hours of wall-clock time.\nIn curriculum task T3, the centers of the loss-of-control and no-fly zone hazard regions are randomly initialized to\nalign with a node in the network as designed in [42]. Wind speed and direction are randomly sampled from independent\nuniform distributions ranging in [0, 10] knots and [0, 360] degrees. The velocity of aircraft is initialized using a uniform\ndistribution with velocities ranging between [5, 65] knots. A total of 100 simulated days of operations are used for"}, {"title": "V. Results", "content": "In this section we discuss the observed performance of each agent versus nominal operations (i.e. unequipped).\nFigures 2 to 5 summarize collected data; which includes, for each agent, 100 evaluations repeatedly simulated with a\nunique random seed used for all agents. Over 798 million operations providing over 266 million hours of flight data were\ncollected for the DRL agents to achieve the performance through the curriculum learning. Overall when the aircraft fleet\nis initialized with energy reserves sampled from a uniform distribution equivalent to the training scenarios, 20-250 KWh,\nthe DRL based agent successfully reroutes an average of 95.2% ($\\sigma_2 = \\pm 1.08%$) and 91.6% ($\\sigma_2 = \\pm 1.29%$) of aircraft\nto vertiports, for SACD-A and D2MAV-A respectively. The heuristic agent outperforms the nominal (unequipped)\noperations, however it only successfully reroutes 76.1% ($\\sigma_2 = \\pm 6.07%$) to a vertiport as compared to (46.2%) for the\nnominal operations. Closer inspection of Fig. 2a reveals that each agent maintains a comparable number of aircraft\nreaching their destination as in the nominal case with SACD-A rerouting more aircraft to their destination if the original\nflight plan is unfeasible. Since all agents only have $N_{wpt}$ of flight path information they must utilize state information\nnear ownship to sense regions of loss-of-control (i.e., $P^{(j)}_{H}, P^{y}_{Hdest}$ as defined in Section IV). While the heuristic agent\nallows for tracks to use routes with a risk of loss-of-control below $P_{Hthreshold}$ to reduce situations where no route\nis possible, more aircraft are found to experience loss-of-control than $P_{Hthreshold}$ and reducing this value leads to a\ntradeoff in more aircraft experiencing energy depletion due to the longer flight paths selected. This tradeoff is due to a\nlack of global information resulting in sub-optimal routing causing 21.8% ($\\sigma_2 = \\pm 5.612%$) of aircraft to experience\nloss-of-control. The SACD-A nearly eliminates loss-of-control risk with only 0.07% ($\\sigma_2 = \\pm 0.119%$) of aircraft\nencountering it. This is achieved by SACD-A exploiting the state space information in training, Fig. 3, and rerouting\naircraft around the hazard region, as seen in Fig. 4. Even with intensive training, Fig. 4 shows the flight trajectories"}, {"title": "VI. Conclusion", "content": "A set of learning-based in-time decision-making agents were trained and evaluated as supportive of automation\nCM for AAM operations in the presence of hazards. The learning-based agents show DRL can meet complex safety\nobjectives and are robust to uncertainty in the environment. When compared to a heuristic based approach, the DRL\nagents have a lower standard deviation in decision-making variability; with results within two standard deviations\nvarying by less than 2% as compared to 6% indicating a higher level of understanding of the uncertainties in the\nenvironment. These results show promise for agents that can achieve both safety and operational efficiency objectives.\nTo achieve the performance shown, over 798 million operations providing over 266 million hours of flight experience\nwere collected for the DRL agents throughout the entire curriculum. This large amount of training experience highlights\na tradeoff with expert knowledge-based heuristic systems. Careful tuning is also required to avoid catastrophic forgetting\nduring training. Additionally, a notable tradeoff of using learning-based approaches is the challenge of achieving\nsafety-critical assurance levels for ML-based agents. Rigorous verification and validation processes are needed, both for\nthe agents themselves and the tools used to test them (e.g., the simulation environment). To help with this challenge,\nthis work presents a framework to design, test, and evaluate a variety of algorithm approaches at scales previously not\npossible; and collect evidence such as suggested by [32]. As aviation moves towards increasingly autonomous systems,\nthe development of comprehensive validation frameworks is essential to ensure safety and reliability."}, {"title": "VII. Future Work", "content": "Future research will focus on enhancing the robustness and efficacy of learning-based approaches in contingency\nmanagement systems for AAM. Firstly, we will integrate probability of casualty and the real-time risk assessment\nmethod suggested by [39] to the heuristic agent and compare the performance versus the DRL agents. Additionally, we\nintend to extend the range of hazards to include dynamic hazards and multiple hazards within the environment. Lastly, we\naim to evaluate the interoperability of the agents with collision avoidance agents. This is needed to ensure seamless\nand appropriate coordination when independent autonomous agents may affect flight."}, {"title": "Appendix", "content": "Algorithm 1 Heuristic Agent Algorithm\n1: Input: Environment State Space\n2: Input: Corridor Network Nodes (latitude, longitude, altitudes)\n3: Output: Reroute or Continue Flight Path\n4: Environment State Space: $S_{t}$\n5: Flight Time Remaining: $T_{remaining} \\leftarrow E/\\dot{E}$\n6: Flight Time Required: $T_{required} \\leftarrow \\rho_{dest}/v_{xy}$\n7: Probability of Loss of Control: $P_{H} \\leftarrow any (P_{C} > P_{Hthreshold}, P_{Hdest} > P_{Hthreshold})$\n8: IsReRouteRequired($S_{t}$,network,$T_{remaining}$,$T_{required}$, $P_{H}$)\n9: function REROUTEINNETWORK($S_{t}$, network)\n$P_{min} \\leftarrow R_{\\rho}$\n$P_{H} \\leftarrow R_{PH}$\n$r_{selected} \\leftarrow None$\n13: for $vpt \\in Veriports$ do\n14:     $R, R_{\\rho}, R_{PH} \\leftarrow Dijkstra(closest node to aircraft location, vpt, network)$\n15:     if $R_{PH} \\leq P_{Hthreshold}$ and $R_{\\rho} < P_{min}$ then\n16:     $P_{min} \\leftarrow R_{\\rho}$\n17:     $P_{H} \\leftarrow R_{PH}$\n18:     $r_{selected} \\leftarrow R$\n19: function STRAIGHTREROUTE($S_{t}$, network)\n20:   $P_{min} \\leftarrow \\infty$\n21:   $P_{H} \\leftarrow \\infty$\n22:   $vpt_{selected} \\leftarrow None$\n23:   for $vpt \\in Veriports$ do\n24:     $R, R_{\\rho}, R_{PH} \\leftarrow Distance(aircraft location, vpt)$\n25:     if $R_{PH} \\leq P_{Hthreshold}$ and $R_{\\rho} < P_{min}$ then\n26:       $P_{min} \\leftarrow R_{\\rho}$\n27:       $P_{H} \\leftarrow R_{PH}$\n28:       $r_{selected} \\leftarrow R$\n29: function ISREROUTEREQUIRED($S_{t}$, network, $T_{remaining}$, $T_{required}$, $P_{H}$)\n30:   if $T_{required} > T_{remaining}$ or $P_{H}$ then\n31:     $r_{selected} \\leftarrow RerouteInNetwork(S_{t}, network)$\n32:     if $r_{selected} is None$ then\n33:        $r_{selected} \\leftarrow StraightReroute(S_{t}, network)$\n34:     Reroute using $r_{selected}$\n35:   else\n36:     Don't Take Control"}]}