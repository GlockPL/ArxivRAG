{"title": "A Benchmark Environment for Offline Reinforcement Learning in Racing Games", "authors": ["Girolamo Macaluso", "Alessandro Sestini", "Andrew D. Bagdanov"], "abstract": "Offline Reinforcement Learning (ORL) is a promis-ing approach to reduce the high sample complexity of traditionalReinforcement Learning (RL) by eliminating the need for contin-uous environmental interactions. ORL exploits a dataset of pre-collected transitions and thus expands the range of application ofRL to tasks in which the excessive environment queries increase training time and decrease efficiency, such as in modern AAAgames. This paper introduces OfflineMania a novel environmentfor ORL research. It is inspired by the iconic TrackMania seriesand developed using the Unity 3D game engine. The environmentsimulates a single-agent racing game in which the objective is tocomplete the track through optimal navigation. We provide avariety of datasets to assess ORL performance. These datasets,created from policies of varying ability and in different sizes,aim to offer a challenging testbed for algorithm development andevaluation. We further establish a set of baselines for a range ofOnline RL, ORL, and hybrid Offline to Online RL approachesusing our environment.", "sections": [{"title": "I. INTRODUCTION", "content": "Reinforcement Learning (RL) has become increasingly pop-ular in the gaming industry as it offers a promising way ofcreating immersive gaming experiences. From training AI-controlled non-player characters [1, 2] to automated gametesting [3, 4].\nHowever the widespread use of RL is often limited byits sample complexity which makes training in complex en-vironments, such as modern AAA games, slow and ineffi-cient. Offline RL (ORL) has recently garnered interest as aframework aimed at improving the sample efficiency of RLagents [5]. With ORL, one can completely eliminate the needfor interaction with the environment and instead rely on apreviously collected dataset of experiences. Such datasets canbe made readily accessible to game developers; for instance,they could use samples obtained from playtesting sessions ordata extracted from previously released games.\nIn this paper we introduce OfflineMania a novel gameenvironment for Online RL and ORL, centered around asingle-agent racing game inspired by the iconic TrackMa-nia [6] game series. Our environment, built using the Unity3D game engine [7], provides a track in which the agentmust complete the race through optimal navigation. Moreover,we provide datasets of agent experiences tailored specificallyfor benchmarking ORL techniques. These datasets are ofvarying quality, ranging from those generated using randompolicies to those crafted by expert agents. We additionally offersmaller and mixed versions of these datasets. These variantsare designed to test algorithmic performance under complexscenarios that challenge the robustness of learning methods.Our work focuses on providing a gaming testbed environmentand multiple datasets tailored for game AI research in ORL.To the best of our knowledge such a combination is notcurrently available in the existing literature. We further providea study assessing the performance of different Online RL andORL algorithms using our new datasets. We also investigatethe performance of fine-tuning policies trained offline usingOnline RL. This last framework is a more natural approach ingame development, as it allows developers to take advantage ofdatasets to create a policy that then can be effectively improvedwith fewer game interactions.\nThe key contributions of this work are:\n\u2022 we introduce OfflineMania, a new environment inspiredby TrackMania developed using the Unity 3D engine;\n\u2022 we provide diverse datasets of varying sizes, collectedusing policies of different expertise levels; and\n\u2022 we present results for a variety of baseline algorithmsincluding both Online and ORL approaches, as well ashybrid methods that combine Offline training with Onlinefine-tuning."}, {"title": "II. RELATED WORK", "content": "Offline Reinforcement Learning (ORL). Offline Rein-forcement Learning is a promising way to address the samplecomplexity challenges faced by Online RL. The core objectiveof ORL is to create a robust policy from a fixed dataset of pre-collected environment transitions, without requiring furtheronline interactions with the environment [8, 9, 10]. ORL hasemerged as a promising approach to training game agents [11].Modern AAA games are often computationally demanding,slow to simulate, and inherently unstable, all of which lead toincreased need for extensive interactions with the environment.\nORL has significantly benefited from the development ofbenchmarks like D4RL [12], which provides a wide range ofdatasets across various domains such as locomotion, roboticmanipulation, and vision-based autonomous driving. However,to the best of our knowledge, there is no ORL dataset availablein the literature specifically tailored for studying these tech-niques in gaming environments, particularly in the context ofracing games. This paper aims to bridge this gap.\nOffline to Online RL. The Offline to Online approachfocuses on how to effectively improve policies trained withORL in a online setting which allows further environmentinteractions. It is a promising approach for training gameagents [13] since a game in development can change ona daily basis, potentially rendering the datasets collectedin an environment iteration insufficient for subsequent it-erations. Offline to Online RL can allow game developersto seamlessly transition to new environment with minimalonline interactions. However, this transition poses significantchallenges [14, 15], including dealing with distribution shiftsbetween the offline data and the new online interactions. Forthese reasons, with this paper we aim to provide a benchmarksuite for investigating the impact of Offline to Online trainingin gaming environments."}, {"title": "III. ENVIRONMENT AND DATASETS", "content": "A. Environment\nWe developed OfflineMania, shown in Figure 1, using theUnity 3D game engine leveraging the ML-Agents package [7].It features a Gymnasium-compatible interface [16], ensuringstraightforward integration into existing experimental setups.The game is computationally efficient, with the game speedeasily adjustable to speed up online training process. Addition-ally, the environment supports rendering capabilities, offeringa birds-eye view of the car. This visualization facilitatesqualitative assessment of agent behavior and simplifies theevaluation process.\nWe now describe the main elements of the environment: thestate and action-space, the reward function, and episode loop.\nState Space. The state space is a vector in $\\mathbb{R}^{33}$. It iscomposed by 15 raycasts covering a 180-degree field of viewin front of the car, with each ray are associated two values:one indicating the presence of an object within its path,and the other specifying the distance to the detected object.Additionally, the components of the velocity of the car areincluded as part of the state representation.\nAction Space. The agent action space consists of twocontinuous values. The first value controls the steering angleof the car which ranges from -1 (indicating a left turn) to 1(indicating a right turn). The second value controls the accel-eration or braking of the car, with a value of 1 correspondingto full acceleration and a value of -1 representing braking orreversing when the car is stationary.\nReward Signal. Our reward function draws inspiration fromprior work [2]. We denote with $p_t$ the position of the carprojected onto the track centerline at timestep t, and by $p_{best}$the most advanced position achieved thus far in the episode.Our reward is then:\n$r_t = \\begin{cases} r_{prog} - \\lambda \\Vert U_{car} \\Vert & \\text{if in contact with wall} \\\\ 0 & \\text{otherwise} \\end{cases}$, (1)\nwhere $r_{prog}$ quantifies the progress of position $p_t$ along thecenterline relative to $p_{best}$, as shown in Figure 1. We set $r_{prog}$to 0 if $p_t$ does not advance beyond $p_{best}$. $U_{car}$ is the magnitudeof the velocity at the moment of impact, and $\\lambda$ is a fixedcoefficient penalizing collisions. In our environment, $\\lambda = 50$.\nEpisode. During each episode the vehicle starts with itsposition dynamically chosen within a designated square areabefore the fist turn in the track. We also randomize theorientation of the car, between -30 and 30 degrees from thecenterline, which ensures that it is always facing the correctdirection. Every episode has a fixed length of 2,000 steps. Inthis many steps an expert agent can complete at most 5 laps.\nB. Datasets\nIn order to support comprehensive research in ORL, wegenerated a diverse series datasets. First, we train three dis-tinct policies using Proximal Policy Optimization (PPO) [17],stopping the training after 1,000, 5,000, and 12,500 networkupdates, respectively. Each of the trained policy representsvarying degrees of ability in navigating the race track. Thepolicies obtain mean cumulative reward of -360, 327, and1183 respectively, over five episodes. The first policy struggleswith the initial corner. The second policy, while capableof occasionally completing the track, exhibits inconsistentperformance. In contrast, the third policy consistently achievedhigh performance by efficiently navigating the track, includingcorner-cutting strategies, successfully completing 5 laps ineach episode.\nUsing these three policies, we collected three distinctdatasets: basic, medium, and expert, each consisting of100,000 transitions. Additionally, we created mixed datasets,one consisting of 200,000 transitions in total and anotherof only 5,000. We refer to these datasets with mix largeand mix small, respectively. The mixed datasets consist of90% transitions sampled from the basic policy, 7% from themedium policy, and only 3% from the expert policy. Thedistribution of transitions in the mixed datasets was chosento simulate a complex scenario in which ORL algorithmsmust stitch together different behaviors in order to correctlylearn an optimal policy. The smaller version of the mixeddataset is useful to understand the behavior of ORL approacheswhen dealing with small datasets. Following this idea, we buil"}, {"title": "IV. BENCHMARK STUDY", "content": "OfflineMania aims to be a testbed for developing newtraining techniques. We provide results of a set of baselines forwidely recognized methods for Online RL, ORL, and Offlineto Online RL. For all approaches we present the mean resultsover five different seeds. All experiments were conductedusing a system equipped with a Nvidia RTX 2070 and anAMD Ryzen 3600X processor.\nFor Online RL baselines, we opted for two state-of-the-art methods: Proximal Policy Optimization (PPO) [17] andSoft Actor Critic (SAC) [18]. PPO, known for its efficacy androbustness to hyperparameter selection, represents a widelyused policy-based approach. Similarly, we selected SAC, anactor-critic algorithm, for its efficiency and for its importancein the RL landscape. We trained our PPO agent over 12,500network updates, for a total of 15 million environment interac-tions and approximately 10 hours of training time. In contrast,training with SAC spanned 3 million network updates, withan equivalent number of environment interactions, totalingapproximately 20 hours of training time.\nFor ORL approaches, we chose Conservative Q-Learning(CQL) [10], Twin Delayed Deep Deterministic policy gradi-ent with Behavioral cloning (TD3BC) [9], and Implicit Q-Learning (IQL) [8]. For all algorithms we present results after300,000 network updates, corresponding to about one hour oftraining time for all algorithms.\nFor Offline to Online approaches, we compare variousmethods. These include: an approach combining TD3BC [9]for offline training and TD3 [19] for online fine tuning;IQL [8], following the fine-tuning process outlined in the orig-inal paper; Jump Start Reinforcement Learning (JSRL) [20],which utilizes offline policies as guides for online training;Policy EXpainsion (PEX) [21], a method combining offlinepolicies with online training to enhance exploration; and thework of Macaluso et al. [22] that we will refer as SDBG,designed specifically for small offline datasets, utilizing aworld model based augmentation to improve offline training.For each approach we present results after 300,000 offlinenetwork updates and 1 million online fine-tuning updates, fora total of about 4 hours of training across all algorithms. SincePEX, SDGB, and JSRL are agnostic to the algorithm used, wedecided to show results using IQL.\nA. Online RL Results\nWe use online RL for training a policy from scratch withenvironment interactions. The resulting mean reward achievedby PPO at the end of the training was 1183, indicative ofconsistent high-quality behaviors on the track. The policy isproficient at navigating the track, and is also able to cut cornerseffectively. It can complete a lap in 385 steps, for a total of 5laps in a single episode. PPO succeeds at the cost of a largenumber of environment interactions (about 15 million).\nConversely, despite training for 20 hours and 3 millionenvironment interactions, the SAC outcomes are considerablyless promising. The SAC policy achieves a total mean re-ward of only 215 and demonstrates suboptimal performancecharacterized by slower and less stable navigation of thetrack compared to the PPO-trained policy. This highlights thelimitations of SAC, despite its better sample efficiency, inproducing effective policies even after extensive training.\nThese results quantify the efficacy of Online RL approaches.However, they also highlight the challenges posed by the slowtraining speed and significant sample inefficiency.\nB. Offline RL Results\nIn Table I we present the results for ORL baselines onthe datasets described in Section III-B. For all algorithms thetraining time for 300,000 network updates was under one hour.Remarkably, IQL consistently outperforms TD3BC and CQLacross nearly all datasets. Of particular note is the ability ofIQL to learn policies from the expert dataset that even surpassthe performance of the policy used to generate it. On all otherdatasets, IQL produces policies that are capable of navigatingthe track without major collisions.\nConversely, TD3BC and CQL unexpectedly fall short onall datasets, most notably on the expert dataset. This may beattributed to sensitivity to the hyperparameters that is a wellknown problem in ORL [5].\nC. Offline to Online RL Results\nIn this section we report baseline performance for thecombination of ORL pre-training and Online RL fine-tuning.Such approaches are useful when an offline policy fails to meetdeployment standards. In these cases, we would like to im-prove the offline-trained policy by allowing some interactionswith the environment, while still minimizing such interactions.\nWhile this process might appear straightforward, it iscomplex primarily due to the distributional shift problem.Distributional shift occurs during the first training iterationswhen moving to online training, as the agent navigates intounexplored state-action spaces. In this setting the values ofthe Q-function trained during the offline phase may becomehighly inaccurate. This inaccuracy can lead to incorrect policyevaluations and arbitrary policy updates in these unseen states,which undermines the policy learned through ORL [15]."}, {"title": "V. DISCUSSION", "content": "ORL is a promising approach to fostering more widespreaduse of RL in modern video games, however there is a lackof environments with datasets that can be used to test thecapabilities of ORL methods in challenging conditions. Thispaper introduces a novel game environment which serves as abenchmark for ORL research. We provide datasets of varyingdata quality and quantity which facilitates the simulation ofcomplex training scenarios\u00b9. Additionally, we present resultsfrom state-of-the-art methods for Online RL, ORL, and hybridOffline to Online RL approaches. This benchmark aims tofacilitate future investigation into the use of offline data ingaming environments. By leveraging offline datasets, we caneffectively mitigate the challenges in applying RL techniquesinto modern games, thereby contributing the integration of RLinto modern game development workflows."}]}