{"title": "Evaluating Alternative Training Interventions Using Personalized Computational Models of Learning", "authors": ["Christopher James MacLellan", "Kimberly Stowers", "Lisa Brady"], "abstract": "Evaluating different training interventions to determine which produce the best learning outcomes is one of the main challenges faced by instructional designers. Typically, these designers use A/B experiments to evaluate each intervention; however, it is costly and time consuming to run such studies. To address this issue, we explore how computational models of learning might support designers in reasoning causally about alternative interventions within a fractions tutor. We present an approach for automatically tuning models to specific individuals and show that personalized models make better predictions of students' behavior than generic ones. Next, we conduct simulations to generate counterfactual predictions of performance and learning for two students (high and low performing) in different versions of the fractions tutor. Our approach makes predictions that align with previous human findings, as well as testable predictions that might be evaluated with future human experiments.", "sections": [{"title": "1. Introduction", "content": "The primary goal of a training intervention designer is to improve the performance of an individual or team along some desirable dimension. Interventions might target capabilities for a wide range of tasks across the physical, cognitive, or social domains. They might also target performance across time scales, from seconds and minutes to weeks and months. For example, soldiers might undergo specialized fitness training to increase the weight they can lift, K12 students might complete practice problems to improve their ability to correctly add fractions, and astronauts might practice working in different roles to foster their ability to perform well as team.\nRegardless of the task, domain, or time scale, identifying the interventions that best achieve the desired performance goals and evaluating their effectiveness in a cost-effective way is a central challenge for intervention designers. Koedinger et al. (2013) sketch out the design space for educational interventions and claim that there are over 200 trillion possible combinations, even when considering just a small design space with 15 possible instructional techniques, three dosage lev-"}, {"title": "2. Background", "content": "As a starting point, we should look to the substantial prior work on student modeling within intelligent tutoring systems. For example, early research by Conati et al. (1997) explored the use of Bayesian models to infer students' mental plans and knowledge state based on their observable actions. One key shortcoming of their work was that it did not account for learning (i.e., how a student's knowledge state changes as a result of practice), effectively treating each practice opportunity as an assessment rather than a learning event. Another approach by Corbett and Anderson (1994), Bayesian Knowledge Tracing, overcomes this issue by using a hidden Markov model to track how a student's knowledge changes in response to practice. Over the past two decades, researchers have built on this early work, exploring different statistical accounts of student learning (MacLellan et al., 2015). These approaches, often referred to as knowledge tracing, have been successfully applied within tutoring systems to estimate students' knowledge and predict their performance.\nDespite this success in guiding personalization and adaptation within tutors, they have shortcomings for our current objective. First, they require preexisting student data for parameter estimation before they can generate predictions about performance. This limits their ability to generalize to novel training interventions for which no previously collected data are available. Additionally, these approaches only model learning and performance abstractly. For example, Bayesian knowl-"}, {"title": "3. Fraction Arithmetic Tutor", "content": "To investigate the use of computational models to support design of training interventions, we chose to investigate decision making and learning within a tutoring system for fraction arithmetic. Patel et al. (2016) created this tutor to teach students how to solve three types of problems: fraction addition with same denominators, fraction addition with different denominators, and fraction multiplication. Following the standard design for intelligent tutoring systems (Vanlehn, 2006), it provides immediate correctness feedback on each step and students can only proceed once they have performed all steps correctly. Additionally, if students get stuck, then they can request a \u201chint\u201d and the tutor provides an example of how to perform the next step.\nThe system scaffolds students in solving these problems in a particular fashion. For all three problem types, they must decide whether to convert the fractions before solving. If they elect to convert and the tutor determines it is appropriate, then it presents them with additional input fields to support conversion. When adding fractions with same denominator and multiplying them, students can input numerators and denominators in any order and can only mark the problem as done once both fields have correct inputs. When students are solving addition problems with different denominators, they must convert the fraction to common denominators before proceeding. In this case, the tutor requires students to use the butterfly method\u2013 the two denominators are multiplied to get a common denominator and the opposing numerators and denominators are multiplied to get new numerators. Additionally, students must input the converted fraction values in a particular order. First they must input the lower left denominator and then they can input the right denominator and the left numerator. Finally, students can enter the right numerator. Once they have converted the fraction, they can input the answers for numerators and denominators in any order. The students can proceed once both answer fields have correct inputs.\nFor our analysis, we used the \"Study 2\" data from the publicly available \u201cFraction Addition and Multiplication\u201d data set accessed via DataShop (Koedinger et al., 2010). These data come from an experiment conducted by Patel et al. (2016) to investigate whether it is better to block or interleave students' fractions practice. For this study, 118 sixth graders were randomly assigned to receive 48 practice problems in either a blocked or interleaved order. Half of the students in"}, {"title": "4. Apprentice Learner Architecture", "content": "To account for human learning in the fractions tutor, we constructed agents using the the Apprentice Learner Architecture (Maclellan et al., 2016; MacLellan, 2017), which provides a framework for modeling human learning and decision making within tutors. Our architecture has two memories: a long-term store that contains skills (hand authored or learned) and a short-term store with working memory elements that are manipulated through skill execution. Skills have three parts: conditions that constrain when the skill applies, effects that update the working memory when it is executed, and a function that predicts the expected future reward of executing the skill in a given state.\nThe architecture has two performance components that support interaction with and learning from tutoring systems. First, the skill matching and execution component matches skills from long-term memory against current working memory elements. When multiple skills match, it uses the expected value function associated with each skill to predict the future reward that will be achieved by executing the respective skill in the current state. The skill with the highest expected reward is selected for execution. When an agent fires a skill that produces only internal changes in working memory, it gets a small penalty (e.g., -0.01) to discourage unnecessary action. When it executes a skill that generates an external step within the tutoring system, it receives feedback on its action"}, {"title": "5. Tuning a Model to Account for Individual Student Differences", "content": "In prior work (Maclellan et al., 2016; MacLellan, 2017), we explored the use of Apprentice Learner models for predicting which fractions interventions yield the best learning. These studies successfully predict students will have lower error during tutoring in the blocked condition and lower error on a posttest in the interleaved condition. However, when we compared the learning curves generated by the humans and agents, we found a large discrepancy. Our previous work assumed that all students were identical learners that came to the fraction arithmetic task without any prior fraction arithmetic knowledge, so agents made an error on every first opportunity, which we counted as an error. In contrast, the data show that human students make mistakes on their first skill opportunities less than half the time, suggesting that students enter the tutoring setting knowing how to do most of the fraction steps. Beyond differences in prior knowledge, we also anticipate that students have cognitive differences that impact their learning. For example, students might have varying thresholds for their willingness to guess an answer, which may affect how quickly they learn. Our previous approach did not account for these individual-level differences."}, {"title": "5.1 Our Model Tuning Approach", "content": "To address this gap, we devised an approach to automatically tailoring models of specific students to better account for their differences. We use the HyperOpt tookit (Bergstra et al., 2013) to create personalized agents representing specific students. Each agent is parameterized by its initial skills (prior knowledge), as well as cognitive parameters (e.g., its disposition towards exploration). We provide HyperOpt with the space of all skills and the range of values for each parameter. To tune to a specific student, HyperOpt iteratively samples skills and parameters. It creates an Apprentice Learner agent with these sampled values, simulates the target student and corresponding intervention, and evaluates how accurately the agent emulates the observed behavior on each step. The error is fed back to the HyperOpt toolkit, which uses Bayesian inference to update its sampling distribution for selecting skills and parameters. With each iteration, HyperOpt converges towards a configuration that minimizes the differences with the student.\nOur tuning process bears some resemblance to knowledge tracing. For example, Conati et al. (1997) explore the use of sampling to update a Bayesian model of each student's knowledge state. Also, Corbett and Anderson (1994) used a Bayesian sequence model (a hidden Markov model) to estimate each student's knowledge at each time point. One central difference from our approach is how they represent expertise. Knowledge tracing has only an abstract representation of skills (e.g., the probability that a student knows a given skill and would uses it correctly). In contrast, our approach explicitly represents each skill as a rule-like structure within the Apprentice Learner architecture (see Section 4). Additionally, the other approaches maintain a best estimate of each student's knowledge state over the course of problem solving, whereas ours focuses on generating a best estimate of a student's initial knowledge state. Finally, our approach uses a mechanistic model of learning, embodied in the Apprentice Learner agents, that the others lack."}, {"title": "5.2 Evaluation of Model Tuning", "content": "We conducted two evaluations of this approach. First, we examined whether HyperOpt could identify skills and parameters that improved the alignment between each agent and its target human. We tuned 24 Apprentice Learner agents to fit 24 students using 20 iterations of HyperOpt optimization in each case. For the performance evaluation step of the tuning process (Figure 3), we simulated each student's behavior on their first ten tutor problems and analyzed how frequently the simulation correctly predicted the correctness of the respective student's first attempt on each step.\nAs an example, imagine we applied our approach a high-performing student that already knows all fraction arithmetic skills. Initially, HyperOpt might select an agent configuration that has expertise in whole number arithmetic but not fractions. The system would construct an Apprentice agent using this configuration and apply it to simulate problem solving and learning on the first ten problems the human received from the tutor. HyperOpt's sampling distribution would then be updated based on the similarity between the agent and human performance. In this case, the agent would perform worse than the human, so HyperOpt's sampling distribution would be updated to"}, {"title": "6. Generating Counterfactual Prediction with Personalized Models", "content": "Although Patel et al. (2016) argue for a one-size-fits-all approach to problem ordering\u2014that interleaved practice yields better learning than blocked practice\u2014we take a more nuanced view that different kinds of practice are better for different students, depending on their prior knowledge and other differences. Unfortunately, this view complicates the intervention design problem because it means the designer must find the best intervention for each student, rather than a single intervention that works best for everyone. Having demonstrated that personalized models generate improved predictions for their target students, we explored how they might inform the selection of which tutoring interventions will be best for each recipient.\nTo support instructional design, our approach can generate counterfactual predictions of a student's performance. If previously collected intervention data for a student are available, then a designer can create a personalized Apprentice Learner agent with these data and apply it to predict what the student's performance would have been in the counterfactual condition. The model could also be tuned with other information, such as pretest results. By leveraging such data, the designer can generate counterfactual predictions about how a particular student would respond to an intervention prior to administering it.\nIn this paper, we examined such counterfactual predictions. In particular, we explored how our Apprentice Learner agents might be applied to answer four counterfactual questions regarding students who used the fraction tutor:"}, {"title": "6.1 Evaluating the Use of Personalized Models for Counterfactual Prediction", "content": "To evaluate our approach, we tested its ability to answer the four counterfactual questions. For each query, we started by tuning models to target students using performance data from the interventions they received, as described in Section 5. We then simulated each student's behavior in counterfactual interventions using the respective personalized agent to determine what each of them would have done had they received different interventions.\nWe focused our simulation efforts on modeling two specific students from the human data. We will refer to them as the 'high-performing student' and the \u2018low-performing student.' The first had the highest tutor performance of those in the blocked condition and the second had the lowest tutor performance of those in the interleaved condition. For each one, we constructed a personalized agent using their performance data. We chose to generate counterfactual predictions for these two"}, {"title": "6.1.1 High-Performing Student Predictions", "content": "shows the observed and simulated learning curves for the high performer in each of the three interventions (blocked, interleaved, and faded), reporting average performance across all fraction arithmetic skills. The opportunity count represents how many prior opportunities the learner had to exercise each skill. Thus, the error at opportunity zero corresponds to the average performance the first time the student applied each skill within the tutor. For these plots, we treated each input field for each problem type as exercising a unique skill; for example, filling in the numera-"}, {"title": "6.1.2 Low-Performing Student Predictions", "content": "As signified by a low error rate (less than 10% when applying skills for the first time), the high performer starts training with ample prior fraction knowledge. To explore behavior at the other end of the spectrum, we constructed a personalized model of the low performer, who we hypothesize has more limited expertise. shows the learning curves (simulated and observed) for this student in each possible intervention. As expected, this learner makes many more errors than the other one, but still gets approximately half of their first steps correct (initial error rate around 48%), suggesting a fair amount of prior fractions knowledge.\nFigure 7(a) compares the student's observed performance on the interleaved condition with the model's predictions on its variations. The graphs show that the model generates reasonable predictions that are in alignment with the observations. There is only a minor difference on the first opportunity, with the model predicting a slightly higher error. As mentioned previously, this discrepancy is not as large as that observed in previous Apprentice Learner work, where the model predicts 100% error on the first step. Additionally, the observed performance is generally within the predicted confidence intervals generated by the model. If we compare the predicted interleaved performance in Figure 7(a) to the predicted blocked performance in Figure 7(b), we see that the"}, {"title": "6.2 Discussion", "content": "These results demonstrate how intervention designers might use personalized models to predict how students would respond to counterfactual interventions. This capability is powerful because it should let designers conduct low-cost simulations to evaluate many alternative interventions. They can then run human studies to evaluate the interventions that the simulations suggest will be most promising. Before designers can trust predictions from Apprentice Learner models, they need evidence that the counterfactual predictions approximate human performance. Unfortunately, evaluating such predictions using previously collected performance data is difficult because, by definition, no ground truth data are available. However, future work can and should explore the design of experiments to test our predictions.\nWe claim that our findings provide evidence that our models make reasonable counterfactual predictions about how students will respond to alternative interventions. For both the high and low performers, they generated understandable predictions. When we compared the observed performance to the predictions for different training regiments, we found a close agreement. Further, for both students they predicted lower error rates in the blocked than interleaved condition, which has been observed in prior studies (Patel et al., 2016).\nIt is harder to evaluate the model predictions for the faded condition because no human data are available. However, our approach makes a reasonable, but not entirely obvious, prediction that students in the faded setting will have lower overall error and that it will decrease more quickly than either in the blocked or interleaved conditions. Additionally, it predicts that error in the tail of the learning curve will look more like that in the blocked condition (less spiky) than in the interleaved condition (more spiky), even though practice at higher opportunities in the faded condition is essentially interleaved. We have not yet evaluated these predictions with human experiments, but we argue they constitute reasonable counterfactual predictions that are consistent with prior research on blocking versus interleaving (Carvalho & Goldstone, 2015). A good test of our approach would be to run a human study comparing faded problem ordering to the blocked and interleaved conditions.\nFinally, these simulated counterfactual data provide some answers to our four counterfactual questions. They give a picture of how students' learning curves might differ if they were in the blocked or interleaved condition (Q1) and they show how students' learning curves might differ if they were in a novel faded condition (Q2). They also provide a picture of how low and high performers would respond differently to these interventions (Q3 and Q4). In general, our study suggests that which intervention the high performer receives matters little due to their ample prior fractions knowledge. However, the low performer improves in all three conditions, but seems to improve the most in the faded condition."}, {"title": "7. Related Research", "content": "Our work is not the first to explore the application of computational models to guide the design of interactive systems. Card (1981) proposed using a Model Human Processor that encapsulates cognitive theory into a computational approach to evaluate the usability of interface designs in lieu of more costly human experiments. More recently, John et al. (2004) have worked to realize this vision through the development of the CogTool system, which supports designers in building usable interfaces. One key limitation of these efforts, with respect to the current work, is their emphasis on expert rather than novice performance. Additionally, they have not been applied to learning environments, as they possess no models of the learning process.\nSeveral lines of research aim to model students within tutors. A large portion has focused on knowledge tracing (Conati et al., 1997; Corbett & Anderson, 1994; MacLellan et al., 2015), which only abstractly (not mechanistically) models the learning and decision-making process. As a result, it requires previously collected data to fit parameters before it can generate predictions\u2014it cannot generalize to interventions for which no data are available. In contrast, computational models of learning, such as Cascade (VanLehn et al., 1992), STEPS (Ur & VanLehn, 1995), SimStudent (Li et al., 2012), and Apprentice Learner (MacLellan, 2017), possess mechanistic models of learning and decision making\u2014they model how knowledge is applied to generate behavior and how it is updated in response to examples and feedback. As a result, they can generate purely theory-driven predictions, even for interventions that lack student data. Our work builds directly on these projects to explore how they can support instructional designers in evaluating the effectiveness of different"}, {"title": "8. Conclusions and Future Work", "content": "This paper has presented evidence to support three high-level claims: (1) that computational models of learning can support causal prediction of human behavior in response to training interventions; (2) that one can tune these models to better predict performance for individuals by adjusting prior knowledge and parameters; and (3) that these personalized models generate plausible counterfactual predictions for target students. To support these claims, we extended the Apprentice Learner Architecture and demonstrated its use for causally reasoning about which fraction arithmetic interventions would produce better learning in particular students. We described how to tune agents to individuals using performance data and showed that they predict student performance better than generic agents. Finally, we constructed personalized models for high and low performers, then used them to counterfactually predict learning curves for three different interventions. The results showed that personalization yields reasonable predictions that agree with the available human data. The approach also generate plausible predictions for a novel intervention for which no data are available but that might be tested with future experiments.\nThere are many possible directions for future research. We are particularly interested in pre-registering our predictions for the \u2018faded blocked to interleaved' intervention and running a study with humans to test them. Specifically, our work suggests that the faded condition will yield faster learning and less spiky error rates in the learning curve's tail. Future investigations should also administer quizzes to agents at the end of training to evaluate final performance across all skills. Patel et al. (2016) showed that learners have lower error on a posttest in the interleaved condition, even though they did better during tutoring in the blocked condition. This additional evaluation will let us investigate these effects using simulated agents.\nIn previous research (MacLellan & Koedinger, 2022), we emphasized the application of computational learning models to support tutor development in psychology, chemistry, math, language learning, and engineering. However, most prior work has focused on mathematical tasks. Future efforts should explore use of our approach on additional domains to showcase its generality. Additionally, past research has focused almost exclusively on learning within intelligent tutoring systems and we need more investigations of how the approach applies to other settings, such as educational games. To support these environments, our framework will likely require substantial extensions. For example, our agents currently only support step-based interactions and expect immediate feedback, but most games require continuous-time interaction and provide only delayed feedback.\nWe should also expand the framework to account for additional learning phenomena, such as the testing effect, where students' performance improves after taking tests, even though they receive no feedback or instruction. Modeling this effect would require extending agents to engage in learning even in the absence of such information. We hypothesize that agents will need mechanisms to generate their own internal feedback when taking tests, which they can use to refine their skills during test taking. Future work should explore variations of this idea to account for how students learn when engaging in problem solving without external feedback or guidance.\nFinally, although there have been some efforts to make our approach accessible to training intervention developers, such as teachers or instructional designers (e.g., Weitekamp et al., 2020), we need more work in this area. Future research should collaborate with potential end users to design interfaces that are usable and do not require technical expertise to automate the steps currently being handled manually on our team. We should explore approaches that support instructional designers by automatically searching the space of training interventions, say by searching the space of problem orderings to find ones that predict the best learning."}]}