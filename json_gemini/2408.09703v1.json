{"title": "Partial-Multivariate Model for Forecasting", "authors": ["Jaehoon Lee", "Hankook Lee", "Sungik Choi", "Sungjun Cho", "Moontae Lee"], "abstract": "When solving forecasting problems including multiple time-series features, ex- existing approaches often fall into two extreme categories, depending on whether to utilize inter-feature information: univariate and complete-multivariate models. Unlike univariate cases which ignore the information, complete-multivariate mod- els compute relationships among a complete set of features. However, despite the potential advantage of leveraging the additional information, complete-multivariate models sometimes underperform univariate ones. Therefore, our research aims to explore a middle ground between these two by introducing what we term Partial- Multivariate models where a neural network captures only partial relationships, that is, dependencies within subsets of all features. To this end, we propose PM- former, a Transformer-based partial-multivariate model, with its training algorithm. We demonstrate that PMformer outperforms various univariate and complete- multivariate models, providing a theoretical rationale and empirical analysis for its superiority. Additionally, by proposing an inference technique for PMformer, the forecasting accuracy is further enhanced. Finally, we highlight other advantages of PMformer: efficiency and robustness under missing features.", "sections": [{"title": "1 Introduction", "content": "Time-series forecasting is a fundamental machine learning task that aims to predict future events based on past observations, requiring to capture temporal dynamics. A forecasting problem often includes interrelated multiple variables (e.g., multiple market values in stock price forecasting). For decades, the forecasting task with multiple time-series features has been of great importance in various applications such as health care (Nguyen et al., 2021; Jones et al., 2009), meteorology (Sanhudo et al., 2021; Angryk et al., 2020), and finance (Qiu et al., 2020; Mehtab, Sen, 2021).\nFor this problem, there have been developed a number of deep-learning models, including linear models (Chen et al., 2023a; Zeng et al., 2022), state-space models (Liang et al., 2024; Gu et al., 2022), recurrent neural networks (RNNs) (Lin et al., 2023b; Du et al., 2021), convolution neural networks (CNNs) (Wang et al., 2023; Liu et al., 2022a), and Transformers (Zhou et al., 2021; Liu et al., 2022b). These models are typically categorized by the existence of modules that capture inter-feature information, falling into two extremes: (i) univariate and (ii) complete-multivariate models\u00b9. While univariate models only capture temporal dependencies, complete-multivariate ones incorporate additional modules that account for complete dependencies among all given features.\nHowever, although complete-multivariate models have potential advantages over univariate ones by additionally utilizing inter-feature information, some studies have demonstrated that complete- multivariate models are sometimes inferior to univariate ones in time-series forecasting. (Zeng et al., 2022; Zhang et al., 2023a; Xu et al., 2024) For example, in Table 1, PatchTST (a univariate"}, {"title": "2 Related Works", "content": "To solve the forecasting problem with multiple features, it is important to discover not only temporal but also inter-feature relationships. As for inter-feature relationships, existing studies often aim to capture full dependencies among a complete set of features, which we call complete-multivariate models. For example, some approaches encode all features into a single hidden vector, which is then decoded back into feature spaces after some processes. This technique has been applied to various architectures, including RNNs (Che et al., 2016), CNNs (Bai et al., 2018), state-space models (Gu et al., 2022), and Transformers (Wu et al., 2022). Conversely, other complete-multivariate studies have developed modules to explicitly capture these relationships. For instance, Zhang, Yan (2023) computes $D \\times D$ attention matrices among $D$ features by encoding each feature into a separate token, while Wu et al. (2020) utilizes graph neural networks with graphs of inter-feature relationships. Additionally, Chen et al. (2023a) parameterizes a weight matrix $W \\in \\mathbb{R}^{D \\times D}$, where each element in the i-th row and j-th column represents the relationship between the i-th and j-th features.\nUnlike complete-multivariate models which fully employ inter-feature information, new models have recently been developed: univariate models. (Zeng et al., 2022; Xu et al., 2024; Nie et al., 2023; Wang et al., 2024; Lee et al., 2024) These models capture temporal dynamics but ignore the inter-feature information by processing each of $D$ features separately as independent inputs. It is worth noting that univariate models usually include a single neural network shared by all features. Existing models typically either utilize inter-feature information among all features or ignore it entirely. In contrast, we propose a completely different approach to using inter-feature information, where a shared model captures relationships only within subsets of features, named partial-multivariate models."}, {"title": "3 Method", "content": "In this section, we first formulate partial-multivariate forecasting models. Subsequently, we introduce Partial-Multivariate Transformer (PMformer) with its training algorithm and inference technique. Finally, we provide a theoretical analysis to explain the superiority of PMformer."}, {"title": "3.1 Partial-Multivariate Forecasting Model", "content": "In this section, we provide the formulation of the partial-multivariate forecasting model. To simplify a notation, we denote the set of integers from $N$ to $M$ (inclusive of $N$ and exclusive of $M$) as $[N:M]$ (i.e., $[N:M] := \\{N, N+1, ..., M-1\\}$). Also, when the collection of numbers is given as indices for vectors or matrices, it indicates selecting all indices within the collection. (e.g., $X_{[0:T],d=[0:D]} := \\{\\{x_{t,d}\\}_{t\\in[0:T]}\\}_{d\\in[0:D]}$). Let $x_{t,d} \\in \\mathbb{R}$ the t-th observation of the d-th feature, and $X_{[0:T],d}$ and $X_{[T:T+\\tau],d}$ the d-th feature's historical inputs and ground truth of future outputs with $T$ and $\\tau$ indicating the length of past and future time steps, respectively. Assuming $D$ denotes the number of features, then a partial-multivariate forecasting model $f$ is formulated as follows:\n$X_{[T:T+\\tau],F} = f(X_{[0:T],F}, F), \\quad F \\in F_{\\text{all}} = \\{F | F \\subset [0:D], |F| = S\\}.$         (1)\nAfter sampling a subset $F$ of size $S$ from a complete set of features $[0:D]$, a model $f$ takes feature indices in $F$ and their historical observations $X_{[0:T],F}$ as input, forecasting the future of the selected features $X_{[T:T+\\tau],F}$. In this formulation, a single model $f$ is shared by any $F \\in F_{\\text{all}}$. In Transformer- based models, $F$ is typically encoded using positional encoding schemes. It is worth noting that from this formulation, univariate and complete-multivariate can be represented by extreme cases of $S$ where $S = 1$ and $S = D$, respectively. Our research goal is to explore the middle ground with partial-multivariate models where $1 < S < D$."}, {"title": "3.2 PMformer", "content": "For complete-multivariate cases, the architectures are required to capture perpetually unchanging (i.e., static) relationships among a complete set of features. In other words, $F$ in equation (1) is always the same as $[0:D]$. However, for partial-multivariate cases, $F$ can vary when re-sampled, requiring to ability to deal with dynamic relationships. Therefore, inspired by recent Transformer-based models using segmentation (Nie et al., 2023; Zhang, Yan, 2023), we devise PMformer which addresses this problem by encoding each feature into individual tokens and calculating attention maps only with the feature tokens in $F$. The overall architecture is illustrated in Figure 2.\nAfter sampling $F$ in equation (1), the historical observations of selected features $X_{[0:T],F} \\in \\mathbb{R}^{T\\times S}$ are encoded into latent tokens $h^{(0)} \\in \\mathbb{R}^{N_s\\times S\\times d_h}$ via a segmentation process where $N_s$ is the number of segments and $d_h$ is hidden size. The segmentation process is formulated as follows:\n$h^{(0)}_b = \\text{Linear}(x_{[\\frac{T}{N_s}b:\\frac{T}{N_s}(b+1)],F_i) + e_\\text{time} + e_\\text{feat}, \\quad b \\in [0, N_s], i \\in [0, S],$        (2)\nwhere $F_i$ denotes the i-th element in F. A single linear layer maps observations into latent space with learnable time-wise and feature-wise positional embeddings, $e^{\\text{Time}} \\in \\mathbb{R}^{N_s\\times d_h}$ and $e^{\\text{Feat}} \\in \\mathbb{R}^{D\\times d_h}$. In most scenarios, we can reasonably assume the input time span $T$ to be divisible by $N_s$ by adjusting $T$ during data pre-processing or padding with zeros as in Zhang, Yan (2023) and Nie et al. (2023).\nSubsequently, $h^{(0)}$ is processed through L PMformer blocks. Each block is formulated as follows:\n$\\hat{h}^{(l-1)} = h^{(l-1)} + \\text{Feature-Attention}(h^{(l-1)}, \\text{Temporal-Attention}(h^{(l-1)})),$         (3)\n$h^{(l)} = h^{(l-1)} + \\text{MLP}(\\hat{h}^{(l-1)}), \\quad l = 1, ..., L.$         (4)\nMLP in equation (4) operates both feature-wise and time-wise, resembling the feed-forward networks found in the original Transformer (Vaswani et al., 2017). As shown in equation (3), there are two types of attention modules:\n$\\forall i \\in [0:S], \\text{Temporal-Attention}(h)_{[0:N_s],i} = \\text{MHSA}(h_{[0:N_s],i}, h_{[0:N_s],i}, h_{[0:N_s],i}, h_{[0:N_s],i}),$         (5)\n$\\forall b \\in [0:N_s], \\text{Feature-Attention}(h,v)_{b,[0:S]} = \\text{MHSA}(h_{b,[0:S]}, h_{b,[0:S]}, v_{b,[0:S]}).$         (6)\nMHSA(Q, K, V) denotes the multi-head self-attention layer like in Vaswani et al. (2017) where Q, K, and V are queries, keys and values. While temporal attention is responsible for capturing temporal dependencies, feature attention mixes representations among features in $F$.\nStarting with initial representations $h^{(0)}$, PMformer encoder with L blocks generates final representa- tions $h^{(L)}$. These representations are then passed through a decoder to forecast future observations. Similar to Nie et al. (2023), the concatenated representations $h_{i,[0:N_s]}^{(L)}$ are mapped to future observa- tions $X_{[T,T+\\tau],F}$; via a single linear layer."}, {"title": "3.3 Training Algorithm for PMformer", "content": "To train PMformer, the process to sample F from a complete set of features is necessary. Ideally, the sampling process would select mutually correlated features. However, prior knowledge about the re- lationships between features is usually unavailable. Therefore, we use random sampling where F is sam- pled fully at random, leading to training Algorithm 1 with use_random_partition = False where $N_{\\upsilon}$ is the number of sampled subsets per iteration note that for-loop in while-loop can be dealt with parallelly with attention masking techniques. How- ever, this training algorithm may result in redun- dancy or omission in each iteration, as some fea- tures might be selected multiple times while others might never be chosen across the $N_{\\upsilon}$ trials. To address this issue, we propose a training algorithm based on random partitioning (see Algo- rithm 1 with use_random_partition = True). In this algorithm, $D$ features are partitioned into"}, {"title": "3.4 Inference Technique for PMformer", "content": "After training PMformer, we can measure inference score using Algorithm 1 without line 11 where use_random_partition = True. During inference time, it is important to group mutually sig- nificant features together. Attention scores among all features can provide information on feature relationships, but evaluating these scores results in a high computational cost, $O(D^2)$. To avoid this, we propose a simple inference technique that doesn't require such expensive computations. We evaluate future predictions by repeating the inference process based on random partitioning $N_\\mathcal{I}$ times and averaging $N_\\mathcal{I}$ outputs to obtain the final outcomes note that the inference process based on random partitioning achieves efficient costs because of computing attention within subsets of size $S$.\nThis technique relies on the principle that the probability of a specific event occurring at least once out of $N_\\mathcal{I}$ trials increases as $N_\\mathcal{I}$ grows. Let $P(F^*) = p$ be the probability that we sample a specific subset $F^*$ from all possible cases. The probability of sampling $F^*$ at least once out of $N_\\mathcal{I}$ trials is $1 - (1 - p)^{N_\\mathcal{I}}$. Given that $0 < p < 1$, $1 - (1 - p)^{N_\\mathcal{I}}$ increases as $N_\\mathcal{I}$ increases. By treating a specific subset $F^*$ as one that includes mutually significant features, our inference technique with a large $N_\\mathcal{I}$ increases the likelihood of selecting a subset including highly correlated features at least once, thereby improving forecasting performance. This is supported by our empirical results in Section 4.3."}, {"title": "3.5 Theoretical Analysis on PMformer", "content": "In this section, we provide theoretical reasons for superiority of our partial-multivariate models over complete-multivariate and univariate ones, based on PAC-Bayes framework, similar to other works (Woo et al., 2023; Amit, Meir, 2019; Valle-P\u00e9rez, Louis, 2020). Let a neural network $f$ be a partial-multivariate model which samples subsets $F$ of $S$ size from a complete set of $D$ features as defined in equation (1). Also, $\\mathcal{T}$ is a training dataset which consists of $m$ instances sampled from the true data distribution. $\\mathcal{H}$ denotes the hypothesis class of $f$ with $P(h)$ and $Q(h)$ representing the prior and posterior distributions over the hypotheses $h$, respectively. Then, based on McAllester (1999), the generalization bound of a partial-multivariate model $f$ is given by:\nTheorem 1. Under some assumptions, with probability at least $1 - \\delta$ over the selection of the sample $\\mathcal{T}$, we have the following for generalized loss $l(Q)$ under posterior distributions $Q$.\n$l(Q) \\leq \\widehat{l}(Q) + \\sqrt{\\frac{\\text{KL}(Q||P) + \\ln \\frac{1}{\\delta} + \\ln m}{2m-1}},$        (7)\nwhere $H(Q)$ is the entropy of $Q$, (i.e., $H(Q) = \\mathbb{E}_{h \\sim Q}[-\\log Q(h)]$ ) and $C$ is a constant.\nIn equation (7), the upper bound depends on $m$ and $-H(Q)$, both of which are related to $S$. Selecting subsets of $S$ size from $D$ features leads to $|F_{\\text{all}}| = \\binom{D}{S}$ possible cases, affecting $m$ (i.e., $m \\propto |F_{\\text{all}}|$) , where $F_{\\text{all}}$ is a pool including all possible $F$. This is because each subset is regarded as a separate instance as in Figure 1. Also, the following theorem reveals relationships between $S$ and $-H(Q)$:\nTheorem 2. Let $H(Q_{S})$ be the entropy of a posterior distribution $Q_S$ with subset size $S$. For $S_+$ and $S_-$ satisfying $S_+ > S_-$, $H(Q_{S_+}) \\leq H(Q_{S_-})$.\nTheorem 2 is intuitively connected to the fact that capturing dependencies within large subsets of size $S_+$ is usually harder tasks than the case of small $S_-$, because more relationships are captured in the case of $S_-$. As such, the region of hypotheses that satisfies conditions for such hard tasks would be smaller than the one that meets the conditions for a simple task. In other words, probabilities of a posterior distribution $Q_{S_+}$ might be centered on a smaller region of hypotheses than $Q_{S_-}$, leading to decreasing the entropy of $Q_{S_+}$. We refer the readers to Appendix A for full proofs and assumptions for the theorems."}, {"title": "4 Experiments", "content": "We mainly follow the experiment protocol of previous forecasting tasks with multiple features, (Zhou et al., 2021). A detailed description of datasets, baselines, and hyperparameters is in Appendix C.\nDatasets. We evaluate PMformer and other methods on the seven real-world datasets with $D > 1$: (i- iv) ETTh1, ETTh2, ETTm1, and ETTm2 (D = 7), (v) Weather (D = 21), (vi) Electricity (D = 321), and (vii) Traffic (D = 862). For each dataset, four settings are constructed with different forecasting lengths $\\tau$, which is in $\\{96, 192, 336, 720\\}$.\nBaselines. Various complete-multivariate and univariate models are included in our baselines. For complete-multivariate baselines, we use Crossformer (Zhang, Yan, 2023), FEDformer (Zhou et al., 2022), Pyraformer (Liu et al., 2022b), Informer (Zhou et al., 2021), TSMixer (Chen et al., 2023a), MICN (Wang et al., 2023), TimesNet (Wu et al., 2023), and DeepTime (Woo et al., 2023). On the other hand, univariate ones include NLinear, DLinear (Zeng et al., 2022), and PatchTST (Nie et al., 2023). Furthermore, we compare PMformer against concurrent complete-multivariate (Mod- ernTCN (donghao, xue wang, 2024), JTFT (Chen et al., 2023b), GCformer (Zhao et al., 2023), CARD (Xue et al., 2023), Client (Gao et al., 2023), and PETformer (Lin et al., 2023a)) and univari- ate models (FITS (Xu et al., 2024), PITS (Lee et al., 2024), and TimeMixer (Wang et al., 2024)). According to code accessibility or fair experimental setting with ours, we select these concurrent models.\nOther settings. PMformer is trained with mean squared error (MSE) between ground truth and forecasting outputs. Also, we use MSE and mean absolute error (MAE) as evaluation metrics, and mainly report MSE. The MAE scores of experimental results are available in Appendix G.1. After training each method with five different random seeds, we measure the scores of evaluation metrics in each case and report the average scores. For the subset size $S$, we use $S = 3$ for ETT datasets, $S = 7$ for Weather, $S = 30$ for Electricity, and $S = 20$ for Traffic, satisfying $1 < S < D/2$. Also, for the inference technique of PMformer, we set $N_\\mathcal{I}$ to 3."}, {"title": "4.2 Forecasting Result", "content": "Table 1 shows the test MSE of representative baselines along with the PMformer. PMformer outperforms univariate and complete-multivariate baselines in 27 out of 28 tasks and achieves the second place in the remaining one. We also provide visualizations of forecasting results of PMformer and some baselines in Appendix G.2, which shows the superiority of PMformer. On top of that, PMformer is compared to the nine concurrent baselines in Table 2. PMformer shows top-1 performance in 10 cases and top-2 in 12 cases out of 12 cases, attaining a 1.167 average rank. The scores in Table 1 and 2 are measured with $N_\\mathcal{I}$ = 3, and in Appendix F, we provide another forecasting result which shows that our PMformer still outperforms other baselines even with $N_\\mathcal{I}$ = 1."}, {"title": "4.3 Analysis", "content": "In this section, we provide some analysis on our PMformer. We refer the readers to Appendix G for additional experimental results."}, {"title": "5 Conclusion", "content": "Various models have been developed to address the forecasting problem with multiple variables. However, most studies focus on two extremes: univariate or complete-multivariate models. To explore the middle ground between them, our research introduces a novel concept, partial-multivariate models, devising PMformer. PMformer captures dependencies only within subsets of a complete feature set using a single inter-feature attention module shared by all subsets. To train PMformer under usual situations without prior knowledge on subset selection, we propose a training algorithm based on random sampling or partitioning. Extensive experiments show that PMformer outperforms 20 baseline models. To explain PMformer's superior performance, we theoretically analyze the upper-bound on generalization errors of PMformer compared to complete-multivariate and univariate ones and provide empirical results supporting the results of the theoretical analysis. Additionally, we enhance forecasting accuracy by introducing a simple inference technique for PMformer. Finally, we highlight PMformer's useful characteristics in terms of the efficiency of inter-feature attention and robustness under missing features against complete-multivariate models.\nLimitation. Further theoretical analysis is needed to more accurately explain partial-multivariate models, such as precisely calculating the entropy of posterior distributions and relaxing certain assumptions. Despite these limitations, our research still remains significant as it introduces the concept of partial-multivariate models for the first time and provides theoretical analyses that align with empirical results.\nBroader Impacts. Our work might have positive effects by benefiting those who devise foundation models for times series because different time series vary in the number of features and our feature sampling scheme where the sampled subset size is always S can overcome this heterogeneity. As for the negative ones, we think the negative effects of forecasting well are still under-explored."}, {"title": "A Proof", "content": ""}, {"title": "A.1 Proof for Theorem 1", "content": "Starting from McAllester's bound on generalization errors (McAllester, 1999), we derive generaliza- tion bound in Theorem 1. Before getting into the main part, we define some notations. Let a neural network $f$ be a partial-multivariate model which samples subsets $F$ consisting of $S$ features from a complete set of $D$ features as defined in equation (1). $\\mathcal{H}$ denotes hypothesis class of $f$, and $P(h)$ and $Q(h)$ are a prior and posterior distribution over the hypotheses $h$, respectively. Also, $(x, y)$ is a input-output pair in an entire dataset and $(x, y)$ is a pair in a training dataset $\\mathcal{T}$ with $m$ instances sampled from the entire dataset. At last, $\\hat{y} = f(x)$ is the output value of a neural network $f$, and $l(Q)$ and $\\widehat{l}(Q, \\mathcal{T})$ are generalized and empirical training loss under posterior distributions $Q$ and training datasets $\\mathcal{T}$.\nSubsequently, we list assumptions for proof:\nAssumption 1. The maximum and minimum values of $y$ are known and min-max normalization is applied to $y$ (i.\u0435., $0 \\leq y \\leq 1$).\nAssumption 2. The output values of a neural network are assumed to be between 0 and 1, (i.e., $0 \\leq \\hat{y} \\leq 1$).\nAssumption 3. For posterior distributions $Q$, $Q$ is pruned. In other words, we set $Q(h) = 0$ for hypotheses $h$ where $Q(h) < P(h)$ and renormalize it.\nAssumption 4. For any hypothesis $h$, $P(h) > \\omega$ where $\\omega$ is the minimum probabilities in $P(h)$ and $\\omega > 0$.\nAssumption 5. For posterior distributions $Q$ and training datasets $\\mathcal{T}$, $\\widehat{l}(Q, \\mathcal{T}) \\approx 0$.\nGiven that min-max normalization has been often used in time-series domains with empirical minimum and maximum values (Bhanja, Das, 2019), Assumption 1 can be regarded as a reasonable one. Also, by equipping the last layer with some activation functions such as Sigmoid or Tanh (hyperbolic tangent) like Xu et al. (2019) and adequate post-processing, Assumption 2 can be satisfied. As for Assumption 3, according to (McAllester, 1999), it might have very little effects on $Q$. Finally, because Transformers can universally approximate any continuous sequence-to-sequence function (Yun et al., 2020), (possibly, extended to general deep neural networks with the universal approximation theorem (Cybenko, 1989)), any hypothesis $h$ can be approximated with proper parameters in $f$. Thus, we can assume $P(h) > \\omega > 0$ for any $h$ when sampling the initial parameters of $f$ from the whole real-number space (Assmuption 4). Also with proper training process and this universal approximation theorem, $\\widehat{l}(Q, \\mathcal{T})$ might approximate to zero (Assumption 5). With these assumptions, the proof for Theorem 1 is as follows:\nProof. Let MSE be a loss function $l$. Then, according to Assumption 1 and 2, $0 \\leq l(h, (x, y)) \\leq 1$ for any data instance $(x, y)$ and hypothesis $h$. Then, with probability at least $1 - \\delta$ over the selection of the sample $\\mathcal{T}$ of size $m$, we have the following for $Q$ (McAllester, 1999):\n$l(Q) \\leq \\widehat{l}(Q, \\mathcal{T}) + \\sqrt{\\frac{\\text{D}(Q||P) + \\ln \\frac{1}{\\delta} + \\ln m}{2m-1}},$        (8)\nwhere D(Q||P) denotes Kullback-Leibler divergence from distribution Q to P. Due to Assumption 5, $\\widehat{l}(Q, \\mathcal{T}) \\approx 0$. Also, because $\\mathbb{E}[\\log \\frac{Q(h)}{P(h)}] < \\log \\mathbb{E}[\\frac{Q(h)}{P(h)}] < \\log \\frac{1}{\\omega} = C$ with Jensen's inequality and Assumption 4, $\\text{D}(Q||P) = \\mathbb{E}_{h \\sim Q}[\\log \\frac{Q(h)}{P(h)}] = \\mathbb{E}[\\log Q(h)] + \\mathbb{E}[\\log \\frac{1}{P(h)}] < \\mathbb{E}[\\log Q(h)] + C$.\nTherefore, we can derive Theorem 1 by substituting $\\widehat{l}(Q, \\mathcal{T})$ and $\\text{D}(Q||P)$ with 0 and $\\mathbb{E}[\\log Q(h)] + C$, respectively:\n$l(Q) \\leq \\mathbb{E}_{h \\sim Q}[\\log Q(h)] + \\sqrt{\\frac{-\\mathbb{E}_{h \\sim Q}[\\log Q(h)] + \\log \\frac{1}{\\omega} + \\log m + \\delta + C}{2m-1}},$        (9)"}, {"title": "A.2 Proof for Theorem 2", "content": "To provide a proof for Theorem 2, we first prove Lemma 1. For Lemma 1, we need the following assumption:\nAssumption 7. A neural network $f$ models models $p(y|x)$ where $(x, y)$ is an input-output pair.\nBy regarding the output of a neural network $\\hat{y}$ as mean of normal or Student's t-distribution like in Rasul et al. (2024), Assumption 7 can be satisfied. Then, Lemma 1 and a proof are as follows:\nLemma 1. Let $\\widehat{l}(Q_{S}, \\mathcal{T}_S)$ be a training loss with posterior distributions $Q_S$ and a training dataset $\\mathcal{T}_S$ when a subset size is $S$. Accordingly, $\\widehat{l}(Q_{S}, \\mathcal{T}_S) < \\epsilon$ with small $\\epsilon$ is a training objective. Then, for $S_+$ and $S_-$ where $S_+ > S_-$, $Q_{S_+}$ satisfies both $\\widehat{l}(Q_{S_+}, \\mathcal{T}_{S_+}) < \\epsilon$ and $\\widehat{l}(Q_{S_+}, \\mathcal{T}_{S_-}) < \\epsilon$. (On the other hands, $Q_{S_-}$ is required to satisfy only $\\widehat{l}(Q_{S_-}, \\mathcal{T}_{S_-}) < \\epsilon$.)\nProof. Let $S_+$ and $S_-$ be subset size where $S_+ > S_-$. $F_{S_-}$ be any subset of $S_-$ size sampled from a complete set of features, and $F_{S_+}$ is any subset of $S_+$ size among ones that satisfy $F_{S_-} \\subset F_{S_+}$. $F_R$ is the set of elements that are in $F_{S_+}$ but not in $F_{S_-}$ (i.e., $F_R = F_{S_+} - F_{S_-}$). $\\widehat{l}(Q_S, \\mathcal{T}_S)$ is a training loss value with posterior distributions $Q_S$ and a training dataset $\\mathcal{T}_S$ when a subset size is $S$. Then, after training process satisfying $\\widehat{l}(Q_{S_+}, \\mathcal{T}_{S_+}) < \\epsilon$ where $\\epsilon$ is a small value, we can say that $f$ under $Q_{S_+}$ outputs the true value of $p(y_{F_{S_+}}|x_{F_{S_+}})$, according to Assumption 7. In the following process, we demonstrate that $p(y_{F_{S_-}}|x_{F_{S_-}})$ can be derived from $p(y_{F_{S_+}}|x_{F_{S_+}}) = p(y_{F_{S_-}, y_{F_R}}|x_{F_{S_-}, x_{F_R}})$:\n$\\begin{aligned} \\int_{y_{F_R}} p(y_{F_{S_-}, y_{F_R}}|x_{F_{S_-}, x_{F_R}}) dy_{F_R} & = \\int_{y_{F_R}} p(y_{F_{S_-}, y_{F_R}}|x_{F_{S_-}, x_{F_R}}) p(x_{F_R}|x_{F_{S_-}}) dx_{F_R}dy_{F_R}, \\\\ & = p(y_{F_{S_-}}|x_{F_{S_-}}), \\end{aligned}$        (10)\n(11)\n(12)\nIn that expectation can be approximated by an empirical mean with sufficient data and integral can be addressed with discretization, we can think that $p(y_{F_{S_-}}|x_{F_{S_-}})$ can be derived from $p(y_{F_{S_+}}|x_{F_{S_+}})$. According to this fact, $f$ under $Q_+$ should be able to output not only true $p(y_{F_{S_+}}|x_{F_{S_+}})$ but also true $p(y_{F_{S_-}}|x_{F_{S_-}})$. Therefore, we conclude that $Q_+$ have to satisfy both $\\widehat{l}(Q_{S_+}, \\mathcal{T}_{S_+}) < \\epsilon$ and $\\widehat{l}(Q_{S_+}, \\mathcal{T}_{S_-}) < \\epsilon$.\nWith Lemma 1, we provide a proof for Theorem 2:"}, {"title": "A.3 Proof for Theorem 3", "content": "Proof. Let $h$ be a hypothesis on a space defined when a subset size is $S$. Then", "follows": "n$Q(h_S) = p(h_S|c_S = 1)", "2": "nAssumption 8. hypotheses $h_{S_+}$ and $h_{S_-}$ have similar distributions after training with $\\mathcal{T}_{S_-}$ (i.\u0435., $p(h_{S_+}|c_{S_-} = 1) \\approx p(h_{S_-}|c_{S_-} = 1)$).\nAssumption 9. Prior distributions are nearly non-informative (i.e."}]}