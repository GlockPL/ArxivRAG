{"title": "Explainable Human-AI Interaction: A Planning Perspective", "authors": ["Sarath Sreedharan", "Anagha Kulkarni", "Subbarao Kambhampati"], "abstract": "From its inception, AI has had a rather ambivalent relationship with humans\u2014swinging between their augmentation and replacement. Now, as AI technologies enter our everyday lives at an ever increasing pace, there is a greater need for AI systems to work synergistically with humans. One critical requirement for such synergistic human-AI interaction is that the AI systems be explainable to the humans in the loop. To do this effectively, AI agents need to go beyond planning with their own models of the world, and take into account the mental model of the human in the loop. Drawing from several years of research in our lab, we will discuss how the AI agent can use these mental models to either conform to human expectations, or change those expectations through explanatory communication. While the main focus of the book is on cooperative scenarios, we will point out how the same mental models can be used for obfuscation and deception. Although the book is primarily driven by our own research in these areas, in every chapter, we will provide ample connections to relevant research from other groups.", "sections": [{"title": "Introduction", "content": "Artificial Intelligence, the discipline many of us call our intellectual home, is suddenly having a rather huge cultural moment. It is hard to turn anywhere without running into mentions of AI technology and hype about its expected positive and negative societal impacts. AI has been compared to fire and electricity, and commercial interest in the AI technologies has sky rocketed. Universities even high schools are rushing to start new degree programs or colleges dedicated to AI. Civil society organizations are scrambling to understand the impact of AI technology on humanity, and governments are competing to encourage or regulate AI research and deployment.\nThere is considerable hand-wringing by pundits of all stripes on whether in the future, AI agents will get along with us or turn on us. Much is being written about the need to make AI technologies safe and delay the \"doomsday\". We believe that as AI researchers, we are not (and cannot be) passive observers. It is our responsibility to design agents that can and will get along with us. Making such human-aware AI agents, however poses several foundational research challenges that go beyond simply adding user interfaces post facto. In particular, human-aware AI systems need to be designed such that their behavior is explainable to the humans interacting with them. This book describes some of the state-of-the-art approaches in making AI systems explainable."}, {"title": "Humans & AI Agents: An Ambivalent Relationship", "content": "In this book we focus on human-aware AI systems-goal directed autonomous systems that are capable of effectively interacting, collaborating and teaming with humans. Although developing such systems seems like a rather self-evidently fruitful enterprise, and popular imaginations of AI, dating back to HAL, almost always assume we already do have human-aware AI systems technology, little of the actual energies of the AI research community have gone in this direction.\nFrom its inception, AI has had a rather ambivalent relationship to humans\u2014swinging between their augmentation and replacement. Most high profile achievements of AI have either been far away from the humans\u2014think Spirit and Opportunity exploring Mars; or in a decidedly adversarial stance with humans, be it Deep Blue, AlphaGo or Libratus. Research into effective ways of making AI systems interact, team and collaborate with humans has received significantly less attention. It is perhaps no wonder that many lay people have fears about AI technology!\nThis state of affairs is a bit puzzling given the rich history of early connections between AI and psychology. Part of the initial reluctance to work on these issues had to do with the worry that focusing on AI systems working with human might somehow dilute the grand goals of the AI enterprise, and might even lead to temptations of \"cheating,\" with most of the intelligent work being done by the humans in the loop. After all, prestidigitation"}, {"title": "Explanations in Humans", "content": "Since our books is about explainable AI systems, it is useful to start with a broad overview of explainability and explanations in humans."}, {"title": "When and Why do Humans expect explanations from each other?", "content": "To understand the different use cases for explanations offered by AI systems, it is useful to survey the different scenarios where humans ask for explanations from each other:\nWhen they are confused and or surprised by the behavior of the other person\nPeople expect explanations when the behavior from the other person is not what they expected and is thus inexplicable. It is worth noting that this confusion at the other person's behavior is not predicated on that person's behavior being incorrect or in-optimal. We may well be confused/surprised when a toddler, for example, makes an optimal chess move. In other words, the need for explanations arises when the other person's behavior is not consistent with the model we have of the other person. Explanations are thus meant to reconcile these misaligned expectations.\nWhen they want to teach the other person We offer explanations either to make the other person understand the real rationale behind a decision or to convince the other person that our decision in this case is not a fluke. Explanatory dialog allows either party to correct the mental model of the other party. The explanations become useful in localizing the fault, if any, in the other person's understanding our decision.\nNote that the need for explanation is thus dependent on one person's model of the other person's capabilities/reasoning. Mental models thus play a crucial part in offering customized explanations. Indeed, a doctor explains her diagnostic decision to her patient"}, {"title": "How do Humans Exchange Explanations?", "content": "We now turn to a qualitative understanding of the ways in which humans exchange ex-planations, with a view to gleaning lessons for explanations in the human-AI interaction. While explanations might occur in multiple modalities, we differentiate two broad types: Pointing explanations and Symbolic Explanations. A given explanatory interaction might be interspersed with both types of explanations.\nPointing (Tacit) Explanations These are the type of explanations where the main explanation consists of pointing to some specific features of the object that both the explainer and explainee can see. This type of explanations may well be the only feasible one to exchange when the agents share little beyond what they perceive in their immediate environment. Pointing explanations can get quite unwieldy (both in terms of the communication bandwidth and the cognitive load for processing them)- especially when explaining sequential decisions (such as explaining why you took an earlier flight than the one the other person expected)-as they will involve pointing to the relevant regions of the shared \u201cvideo history\" between the agents (or, more generally, space time signal tubes).\nSymbolic (Explicit) Explanations These involve exchanging explanations in a symbolic vocabulary. Clearly, these require that the explainer and explainee share a symbolic vocabulary to begin with.\nTypically, pointing explanations are used for tacit knowledge tasks, and symbolic ex-planations are used for explicit knowledge tasks. Interestingly, over time, people tend to develop symbolic vocabulary even for exchanging explanations over tacit knowledge tasks. Consider, for example, terms such as pick and roll in basket ball, that are used as a shorthand for a complex space time tube-even though the overall task is largely a tacit knowledge one.\nThe preference for symbolic explanations is not merely because of their compactness,\""}, {"title": "(Why) Should AI Systems be Explainable?", "content": "Before we delve into explainability in AI systems, we have to address the view point that explainability from AI systems is largely unnecessary. Some have said, for example, that AI systems-such as those underlying Facebook-routinely make millions of decisions/rec-ommendations (of the \u201cyou might be interested in seeing these pages\u201d variety) and no users ask for explanations. Some even take the view that since AI systems might eventually be \"more intelligent\" than any mere mortal human, requiring them to provide explanations for their (obviously correct) decisions unnecessarily hobbles them. Despite this, there are multiple reasons why we want AI systems to be explainable\n\u2022\tSince humans do expect explanations from each other, a naturalistic human-AI interaction requires that AI systems be explainable\n\u2022\tContestability of decisions is an important part of ensuring that the decisions are seen to be fair and transparent, thus engendering trust in humans. If AI systems make high stakes decisions, they too need to be contestable.\n\u2022\tSince there is always an insurmountable gap between the true preferences of humans and the AI system's estimate of those preferences, an explanatory dialog allows for humans to \"teach\" the AI systems their true preferences in a demand-driven fashion.\n\u2022\tGiven that AI systems-especially those that are trained purely from raw data-may have unanticipated failure modes, explanations for their decisions often help the humans get a better sense of these failure modes. As a case in point, recently, it has been reported that when some Facebook users saw a video of an African male in a quotidian situation, the system solicitously asked the users Do you like to see more primate videos? As egregious as this \u201cheads-up\" of the failure mode of the underlying system sounds, it can be more insidious when the system just silently acts on the decisions arrived at through those failure modes, such as, for example, inexplicably filling the user's feeds with a slight uptick of primate videos. Explanations are thus a way for us to catch failure modes of these alien intelligences that we are increasingly surrounded by."}, {"title": "Dimensions of Explainable AI systems", "content": "Now that we have reviewed how explanations and explainability play a part in human-human interactions, we turn to explainability in human-AI interactions. We will specif-ically look at the use cases for explanations in human-AI interaction, some desirable requirements on explanations, and an overview of the ongoing research on explainable AI systems."}, {"title": "Use cases for explanations in Human-AI Interaction", "content": "When humans are in the loop with AI systems, they might be playing a variety of different roles. Being interpretable to one human in one specific role may or may not translate to interpretability for other humans in other roles. Perhaps the most popular role considered in the explainable AI literature to-date is humans as debuggers trying to flag and correct an AI system's behavior. In fact, much of the explainable machine learning research has been focused on this type of debugging role for humans. Given that the humans in the loop here have invested themselves into debugging the system, they are willing to go into the land of the AI agents, rather than expect them to come into theirs. This lowers the premium on comprehensibility of explanations.\nNext we can have humans as observers of the robot's behavior \u2013 either in a peer to peer, or student/teacher setting. The observer might be a lay person who doesn't have access to the robot's model of the task; or an expert one who does.\nFinally, the human might be a collaborator-who actively takes part in completing a joint task with the robot.\nNo matter the role of the human, one important issue is whether the interaction between the human and the robot is a one-off one (i.e., they only interact once in the context of that class of tasks) or a longitudinal one (where the human interacts with the same robot over extended periods). In this latter case, the robot can engender trust in the human through its behavior, which, in turn reduces the need for interpretability. In particular, the premium on interpretability of the behavior itself is reduced when the humans develop trust over the capabilities and general beneficence of the robot."}, {"title": "Requirements on Explanations", "content": "There are a variety of requirements that can be placed on explanations that an AI agent gives the human in the loop:\nComprehensibility: The explanation should be comprehensible to the human in the loop. This not only means that it should be in terms that the human can under-stand, but should not pose undue cognitive load (i.e., expect unreasonable inferential capabilities).\nCustomization: The explanations should be in a form and at a level that is accessible to the receiving party (explainee).\nCommunicability: The explanation should be easy to exchange. For example, symbolic explanations are much easier than pointing explanations (especially in sequential decision problems when they have to point to space time signal tubes).\nSoundness: This is the guarantee from the AI agent that this explanation is really the reason behind its decision. Such a guarantee also implicitly implies that the agent will stand behind the explanation-and that the decision will change if the condi-tions underlying the explanation are falsified. For example, if the explanation for a"}, {"title": "Explainable AI: The Landscape & The Tribes", "content": "The work in explainable AI systems can be classified into multiple dimensions. The first is whether what needs to be explained is a single-decision (e.g. classification) task or a behavior resulting from a sequence of decisions. Second dimension is whether the task is guided by explicit (verbalizable) knowledge or is a tacit task. The third is whether the interaction between the human and the AI agent is one-shot or iterative/longitudi-nal. Armed with these dimensions, we can discern several research \"tribes\" focusing on explainable AI systems:\nExplainable Classification Most work on the so-called \"explainable ML\" focused on explainable classification. The classification tasks may be \"tacit\" in that the machine learns its own potentially inscrutable features/representations. A prominent subclass here is image recognition/classification tasks based on deep learning approaches. In these cases, the default communication between the AI agents and humans will be over the shared substrate of the (spatial) image itself. Explanations here thus amount to \"saliency annotations\" over the image-showing which pixels/regions in the image have played a significant part in the final classification decision. Of course, there are other classification tasks such as loan approval or fraudulent transaction detection-where human-specified features, rather than pixels/signals are used as the input to the classifier. Here explana-tions can be in terms of the relative importance of various features (e.g. shapley values).\nExplainable Behavior Here we are interested in sequential decision settings, such as human-robot or human-AI interactions, where humans and AI agents work in tandem to achieve certain goals. Depending on the setting, the human might be a passive ob-server/monitor, or an active collaborator. The objective here is for the AI agent (e.g. the robot) to exhibit behavior that is interpretable to the human in the loop. A large part of this work focuses on tacit interactions between the humans and robots (e.g. robots moving in ways that avoid colliding with the human, robots signaling which point they plan to go to etc.). The interactions are tacit in that the human has no shared vocab-ulary with the robot other than the observed behavior of the robot. Here explainability or interpretability typically depend on the robot's ability to exhibit a behavior that helps the human understand its \u201cintentions\". Concepts that have been explored in this context include explicability\u2014the behavior being in conformance with human's expectation of the robot, predictability\u2014the behavior being predictable over small time periods (e.g. next few actions), and legibility the behavior signaling the goals of the robot.\""}, {"title": "Our Perspective on Human-Aware and Explainable AI Agents", "content": "In this section, we give a brief summary of the the broad perspective taken in this book in designing human-aware and explainable AI systems. This will be followed in the next section by the overview of the book itself."}, {"title": "How do we make AI agents Human-Aware?", "content": "When two humans collaborate to solve a task, both of them will develop approximate models of the goals and capabilities of each other (the so called \"theory of mind\"), and use them to support fluid team performance. AI agents interacting with humans - be they embodied or virtual \u2013 will also need to take this implicit mental modeling into account. This certainly poses several research challenges. Indeed, it can be argued that acquiring and reasoning with such models changes almost every aspect of the architecture of an intelligent agent. As an illustration, consider the architecture of an intelligent agent that takes human mental models into account shown in Figure 1.1. Clearly most parts of the agent architecture \u2013 including state estimation, estimation of the evolution of the world, projection of its own actions, as well as the task of using all this knowledge to decide what course of action the agent should take are all critically impacted by the need to take human mental models into account. This in turn gives rise to many fundamental research challenges. In this book, we will use the research in our lab to illustrate some of these challenges as well as our attempts to address them. Our work has focused on the challenges of human-aware AI in the context of human-robot interaction scenarios as well as human decision support scenarios Figure 1.2 shows some of the test beds and micro-worlds we have used in our ongoing work."}, {"title": "Mental Models in Explainable AI Systems", "content": "In our research, we address the following central question in designing human-aware AI systems: What does it take for an AI agent to show explainable behavior in the presence of humans? Broadly put, our answer is this: To synthesize explainable behavior, AI agents need to go beyond planning with their own models of the world, and take into account the mental model of the human in the loop. The mental model here is not just the goals and capabilities of the human in the loop, but includes the human's model of the AI agent's goals/capabilities."}, {"title": "Overview of this Book", "content": "Through the rest of the book, we will look at some of the central challenges related to human-aware planning that arise due to and can be addressed through working with the human's mental model. In particular, we will ground our discussions of the topics within the context of using such models to generate either interpretable or deceptive behavior. The book is structured as follows:\nChapter 2 In this chapter, we will focus on formally defining the goal-directed deter-ministic planning formalisms and the associated notations that we will be using to study and ground the technical discussions throughout this book. We will also define the three main interpretability metrics; namely, Explicability, Legibility, and Predictability. We will be revisiting these three measures in the following chapters. We will see how the different methods discussed throughout the book relate to these measures, and in various cases could be understood as being designed to optimize, at the very least a variant of these measures."}, {"title": "Planning Models", "content": "We will be using goal-oriented STRIPS planning models to represent the planning prob-lems used in our discussions. However, the ideas discussed in this book apply to any of the popular planning representations. Under this representation scheme, a planning model (sometimes also referred to as a planning problem) can be represented by the tuple \n$M = (F, A, I, G, C)$,\n where the elements correspond to\n\u2022\t$F$ - A set of propositional fluents that define the space of possible task states. Each state corresponds to a specific instantiation of the propositions. We will denote the set of states by $S$. When required we will uniquely identify each state by the subset of fluents which are true in the given state. This representation scheme implicitly encodes the fact that any proposition from $F$ not present in the set representation of the state is false in the underlying state.\n\u2022\t$A$ - The set of actions that are available to the agent. Under this representation scheme, each action $a_i \\in A$ is described by a tuple of the form\n$a_i = (pre(a_i), adds(a_i), dels(a_i))$,\nwhere\n\t$pre(a_i)$ - The preconditions for executing the action. For most of the discus-sion, we will follow the STRIPS execution semantics, wherein an action is only allowed to execute in a state where the preconditions are 'met'. In general, pre-conditions can be any logical formula over the propositional fluents provided in $F$. Moreover, we would say an action precondition is met in a given state if the logical formula holds in that state (remember a state here correspond to a specific instantiation of fluents) We will mostly consider cases where the precondition is captured as a conjunctive formula over a subset of state fluents, which we can equivalently represent as a set over these fluents. This represen-tation allows us to test whether a precondition holds by checking if the set of fluents that are part of the precondition is a subset of the fluents that are true in the given state. Thus the action $a_i$ is executable in a state $s_k$, if $pre(a_i) \\subseteq S_k$."}, {"title": "Measures of Interpretability", "content": "adds($a_i$)/dels($a_i$) - The add and delete effects of the action $a_i$ together captures the effects of executing the action in a state. The add effects represent the set of state fluents that will be made true by the action and the delete effects capture the state fluents that will be turned false. Thus executing an action $a_i$ in state $s_j$ results in a state\n\t$s_k = (s_j \\setminus dels(a_i)) \\cup adds(a_i)$\n\u2022\t$I$ - The initial state from which the agent starts.\n\u2022\t$G$ - The goal that the agent is trying to achieve. Usually the goal is considered to be a partially specified state. That is, the agent is particularly interested in achieving specific values for certain state fluents and that the values of other state fluents do not matter. Thus any state where the specified goal fluent values are met are considered to be valid goal states.\n\u2022\t$C$ - The cost function ($C : A \\rightarrow R_{>0}$) that specifies the cost of executing a particular action.\nGiven such a planning model, the solution takes the form of a plan, which is a sequence of actions. A plan\n$\\pi = (a_1, ..., a_k)$ is said to be a valid plan for a planning model $M$, if executing the sequence of the plan results in a state that satisfies the goal. Given the cost function, we can also define the cost of a plan, $C'(\\pi) = \\Sigma_{a_i\\in \\pi}C(a_i)$. A plan, $\\pi$, is said to be optimal if there exists no valid plan that costs less than $\\pi$. We will use the term behavior to refer to the observed state action sequence generated by the execution of a given plan. For models where all actions have unit cost, we will generally skip $C$ from the model definition. We will use the superscript \u2018*' to refer to optimal plans. Further, we will use the modifier \u2018\u2019 to refer to prefixes of a plan and '+' operator to refer to concatenation of action sequences. In cases where we are comparing the cost of the same plan over different models, we will overload the cost functions $C$ to take the model as an argument, i.e., we will use $C(\\pi, M_1)$ to denote the cost of plan $\\pi$ in the model $M_1$, while $C(\\pi, M_2)$ denotes its cost in $M_1$. Additionally, we will use the notation $C_M^*$ to denote the cost of an optimal plan in the model $M$.\nSince most of the discussion in this book will rely on reasoning about the properties of such plan in different models, we will use a transition function $\\delta$ to capture the effect of executing the plan under a given model, such that the execution of an action $a_i$ in state $s_j$ for a model $M$, where $\\delta(s_j, a, M)$, gives the state that results from the execution of action $a$ in accordance with the model $M$.\nIncomplete Models In this book, we will also be dealing with scenarios where the model may not be completely known. In particular, we will consider cases where the specification may be incomplete insofar as we do not know every part of the model with absolute certainty, instead we will allow information on parts of the model that may be possible. To represent such models, we can follow the conventions of an annotated model, wherein the model definition is quite similar to STRIPS one, except that each action $a_i$ is now defined as $a_i$ = (pre($a_i$), poss\\_pre$a_i$, adds($a_i$), pre($a_i$), dels($a_i$), dels($a_i$)), where 'pre', 'adds' and 'dels' represent the possible preconditions, adds and deletes of an action. If a fluent $f$ is part of such a possible list, say a possible precondition, then we are, in effect, saying that we need to consider two possible versions of action $a_i$; one where it has a precondition $f$ and one where it does not. If the model in total contains k possible model"}, {"title": "Planning Models", "content": "components (including preconditions and effects), then it is in fact representing $2^k$ possible models (this set of possible models are sometimes referred to as the completion set of an annotated model).\nSensor Models Most of the settings discussed in the book contain multiple agents and require one of the agents to observe and make sense of the actions performed by the other. We will refer to the former agent as the observer and the other as the actor (though these roles need not be fixed in a given problem). In many cases, the observer may not have perfect visibility of the actor's activities. In particular, we will consider cases where the observer's sensor model may not be able to distinguish between multiple different activities performed by the actor. To capture cases like this, we will use the tuple, Obs = ($\\Omega, O$) to specify the sensor model of the observer, where the elements correspond to:\n\u2022\t$\\Omega$ - A set of observation tokens that are distinguishable by the observer. We will use $\\omega$ to denote an observation token, and $\\langle \\omega \\rangle$ to denote a sequence of tokens.\n\u2022\t$O$ - An observation function ($O : A \\times S \\rightarrow \\Omega$) maps an action performed and the next state reached by the actor to an observation token in $\\Omega$. This function captures any limitations present in the observer's sensor model. If the agent cannot distinguish between multiple different activities, we say that the agent has partial observability. Given a sequence of tokens, $\\langle \\omega \\rangle$, a plan $\\pi$ is consistent with this sequence if and only if the observation at any step could have been generated by the corresponding plan step (denoted as $\\langle \\omega \\rangle | = \\pi$)."}, {"title": "Models in Play in Human-Aware Planning Problems", "content": "Throughout most of this book, we will use the tuple $M^R = (F^R, A^R, I^R, G^R, C^R)$ to capture the robot's plan-ning model that it uses to plan its actions, $M^H = (F^H, A^H, I^H, G^H, C^H)$ the model human may use to capture their own capabilities and plan their behavior and\n$\\hat{M} = (\\hat{F}^R, \\hat{A}^R, \\hat{I}^R, \\hat{G}^R, \\hat{C}^R)$ the mental model the human maintains of the robot. In cases where we want to differentiate between the different definitions of the same action under different models, we will use the specific model name in the superscript to differentiate them. For example, we will refer to the definition of action $a_i$ in the robot's original model as $a_i^{M^R}$, while the human's expectation of this model will be captured as $\\hat{a}_i^{\\hat{M}}$. In cases where the human is maintaining an explicit set of possible robot models then we will use $\\theta$ to represent this set. In this book, we will be using the term robot to refer to the autonomous agent that will be acting in the world and interacting with the human. Though, by no means are the methods discussed in the book are limited to physically embodied agents. In fact, in Chapter 10, we will see many examples of software agents capable of using the same methods. Much of the discussions in this book will be focused on cases where the human teammate is merely an observer, and thus in terms of human models we will be focusing on $\\hat{M}$. For the sensor models, since the human will be the observer, we will denote the human sensor model by the tuple, $Obs^H = (\\Omega^H, O^H)$."}, {"title": "Modes of Interpretable Behavior", "content": "The term interpretable behavior has been used to cover a wide variety of behaviors. However, one unifying thread that runs throughout the different views is the fact that they are all strategies designed to handle the asymmetry between what the human knows about the robot (in terms of its model or the plan it is following) and the robot's model or plans. A robot's actions may be uninterpretable when it does not conform to the expectations or predictions engendered by the human's model of the agent. In this chapter, we will look at three notions of interpretability; namely explicability, legibility, and predictability. Each of these interpretability types captures a different aspect of the asymmetry. For each type, we will define a scoring function, that maps a given behavior and the human's beliefs about the robot to a score. Thus a robot that desires to exhibit a certain type of interpretable behavior can do so by selecting behaviors that maximize the given score. Figure 2.1, present some sample behavior illustrating the various interpretability measures."}, {"title": "Explicability", "content": "The notion of explicability is related to the human's understanding of the robot's model and how well it explains or aligns with the behavior generated by the robot."}, {"title": "Legibility", "content": "The robot may be capable of performing multiple different tasks in a given environment. In such environments, the human observer may sometimes have ambiguity over the current task being pursued by the robot. In such cases, the robot can make its behavior inter-pretable to the human by implicitly communicating its task through its behavior. This brings us to another type of interpretable behavior, namely, legible behavior. In general, legible behavior which allows the robot to convey some information implicitly. In this setting, the inherent assumption is that the human observer maintains a set of possible hypotheses about the robot model but is unaware of the true model. In a nutshell,\nLegibility reduces observer's ambiguity over possible models the robot is using.\nLet $M$ be the set of possible models that the human thinks the robot may have. Here the robot's objective is to execute a plan that reduces the human observer's ambiguity over the possible tasks under consideration. Therefore, the legibility score of a plan $\\pi$ can be defined as follows:"}, {"title": "Predictability", "content": "In the case of explicability, as long as the robot's behavior conforms to one of the human's expected plans, the behavior is explicable to the human. However, predictability goes beyond explicability, in that, the behavior has to reduce the number of possible plan completions in the human's mental model given the robot's task. In a nutshell,\nPlan predictability reduces ambiguity over possible plan completions, given a plan prefix.\nThat is, a predictable behavior is one that allows the human observer to guess or anticipate the robot's actions towards its goal. We can define the predictability score, i.e., predictability score of a plan prefix, $\\overline{\\pi}$, given the human's mental model, $\\hat{M}$, as follows:\n$P(\\overline{\\pi}, \\hat{M}) \\propto \\frac{1}{|{\\hat{\\pi} | \\hat{\\pi} \\in compl(\\overline{\\pi})} |}$"}, {"title": "Communication to Improve Interpretability", "content": "One of the core reasons for the uninterpretability of any agent behavior is the asymmetry between the agent's knowledge and what the human observer knows about the agent. Explicability and legibility stem from the human's misunderstanding or lack of knowledge about the agent's model and predictability deals with the human's ignorance about the specific plan being followed by the agent. This means that in addition to adjusting the agent behavior, another strategy the agent could adopt is to inform the human about its model or the current plan."}, {"title": "Communicating Model Information", "content": "Among the communication strategies, we will focus primarily on communicating model information. Such communication requires the ability to decompose the complete model into meaningful components and be able to reason about the effect of receiving information about that component in the human model. We will do this by assuming that the planning models can be represented a set of model features or parameters that can be meaningfully communicated to the human. STRIPS like models gives us a natural parameterization scheme of the form\n$\\Gamma : M \\rightarrow s$ represents any planning problem $M = (F, A, I, G, C)$ as a state s $\\subseteq$ F as follows -"}, {"title": "Other Considerations in Interpretable Planning", "content": "In this section, we will look at some of the other factors that have been studied in the context of interpretable behavior generation and human-aware planning in general.\nModes of Operation One additional factor we can take into account while describing interpretable behavior is what part of the plan is being analyzed by the human observer. Are they watching the plan being executed on the fly or are they trying to analyze the entire plan? We will refer to the former as online setting and the latter as offline or post-hoc setting. In the previous sections, while we defined predictability in an online setting both legibility and explicability were defined for an offline setting. This is not to imply"}, {"title": "Generalizing Interpretability Measures", "content": "In this chapter and in the rest of the book, we focus one three interpretability measures that were identified by previous works as capturing some desirable behaviors in human-robot interaction settings. Note that these are specific instances of larger behavioral patterns the robot could exhibit in such scenarios. Another possible way to categorize agent behaviors would be to organize them based on how they influence or use the human mental models. In the case of settings with humans as observers, this gives rise to two broad categories, namely;\nModel-Communication Behaviors: Model-communication involves molding the human's mental models through implicit (i.e tacit) or explicit communication, to allow the agent to achieve their objectives. Examples of model-communication behaviors in-clude legible behavior generation where the agent is trying to implicitly communicate some model information and the model-reconciliation explanation where the agent is di-rectly communicating some model information so that the robot behavior appears more explicable.\nModel-Following Behaviors: This strategy involves taking the current models of the human and generating behavior that conforms to current human expectations or ex-ploits it in the robot's favor. The examples of this strategy we have seen so far include explicability and predictability.\nOne could see the agent engaging cyclically in model-communication and model-following behaviors, wherein the agent may choose to mold the user's expectations to a point where its behavior may be better received by the observer. We could go one step further and get away from behavior categorization altogether, and simply define a single human-aware planning problem that generates these individual interpretable behaviors in different scenarios. In fact, Sreedharan et al. [2021b] defines a generalized human-aware planning problem as follows"}, {"title": "Explicable Behavior Generation", "content": "In chapter 2, among other things we defined the notion of explicability of a plan and laid out an informal description of explicable planning. In this chapter, we will take a closer look at explicability and discuss some practical methods to facilitate explicable planning. This would include discussion on both planning algorithms specifically designed for generating explicable behavior and how one could design/update the task to make the generation of explicable plans easier.\nIn the earlier chapter, we discussed how the generation of explicable plans requires the robot to simultaneously reason with both models $\\hat{M}^R$ and $\\hat{M}^H$. This is because the robot needs to select feasible and possibly low-cost plans from $M^R$, that align with or are close to plans expected by the human (as computed from $M^h$). This implies the robot needs to have access to $M^H$ (at least an approximation of it in some form). An immediate question the reader could ask is if $M^H$ is available to the robot, why is $"}]}