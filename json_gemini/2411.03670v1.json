{"title": "Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?", "authors": ["Pedro R. A. S. Bassi", "Wenxuan Li", "Yucheng Tang", "Fabian Isensee", "Zifu Wang", "Jieneng Chen", "Yu-Cheng Chou", "Yannick Kirchhoff", "Maximilian Rokuss", "Ziyan Huang", "Jin Ye", "Junjun He", "Tassilo Wald", "Constantin Ulrich", "Michael Baumgartner", "Saikat Roy", "Klaus H. Maier-Hein", "Paul Jaeger", "Yiwen Ye", "Jianpeng Zhang", "Yutong Xie", "Ziyang Chen", "Yong Xia", "Zhaohu Xing", "Lei Zhu", "Afshin Bozorgpour", "Pratibha Kumari", "Yousef Sadegheih", "Dorit Merhof", "Reza Azad", "Pengcheng Shi", "Ting Ma", "Yuxin Du", "Fan Bai", "Tiejun Huang", "Bo Zhao", "Haonan Wang", "Xiaomeng Li", "Hanxue Gu", "Haoyu Dong", "Jichen Yang", "Maciej A. Mazurowski", "Saumya Gupta", "Linshan Wu", "Jiaxin Zhuang", "Hao Chen", "Holger Roth", "Daguang Xu", "Matthew B. Blaschko", "Sergio Decherchi", "Andrea Cavalli", "Alan L. Yuille", "Zongwei Zhou"], "abstract": "How can we test AI performance? This question seems trivial, but it isn't. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone, a large-scale collaborative segmentation benchmark of 9 types of abdominal organs. This benchmark is based on 5,195 training CT scans from 76 hospitals around the world and 5,903 testing CT scans from 11 additional hospitals. This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across out-of-distribution scenarios. We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms. In addition, we also evaluated pre-existing AI frameworks-which, differing from algorithms, are more flexible and can support different algorithms\u2014including MONAI from NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are committed to expanding this benchmark to encourage more innovation of AI algorithms for the medical domain.", "sections": [{"title": "1 Introduction", "content": "The development of AI algorithms has led to enormous progress in medical segmentation, but few algorithms are reliable enough for clinical use [3, 35, 10]. Most AI algorithms fall short of expert radiologists, who are much more reliable and consistent when dealing with medical images from multiple hospitals, varied in different scanners, clinical protocols, patient demographics, or disease prevalences [66, 45, 33, 87]. Therefore, the question remains: How can we test medical AI in the diverse scenarios that are encountered by radiologists? Establishing a trustworthy AI benchmark is important but exceptionally challenging, and seldom achieved in the medical domain. Tougher tests, like out-of-distribution evaluation on large, varied datasets, are needed.\nStandard benchmarks have underlying problems that cause confusion in algorithm comparisons and delay progress. First, in-distribution test sets. In the medical domain, CT scans in the test set often share sources, scanners, and populations with the training set. As a result, AI algorithms may perform well on the test set but generalize poorly to out-of-distribution (OOD) scenarios [21, 7, 8, 45, 33]. For example, Xia et al. [78] found that AI algorithms trained on data from Johns Hopkins Hospital (Baltimore, USA) lose accuracy in pancreatic tumor detection when evaluated on CT scans from Heidelberg Medical School (Heidelberg, Germany). Second, small-size test sets. Annotating medical data is expensive and time-consuming, but training AI requires substantial annotated data [57, 58]. Therefore, most annotated data is used for training, leaving very little assigned for testing. Recent CT datasets such as TotalSegmentator [75], WORD [51], and MSD [2], offered fewer than 100 CT scans for testing. Even a single success or failure can skew results, reducing the statistical power and potentially misleading conclusions. Third, over-simplified metrics. Most standard benchmarks only compare average performance, failing to identify each AI algorithm's strengths and weaknesses in different scenarios. For instance, one algorithm might excel at segmenting small, circular structures (like the gall bladder) while another performs better on long, tubular ones (such as the aorta). Average performance across many classes can hide these nuances. Fourth, unfair comparisons. Almost every paper reports that the newly 'proposed AI' outperforms existing 'alternative AIs.' The improvement becomes more significant if alternative AIs are reproduced and evaluated on an unknown training/test split. There are biases in comparison due to asymmetric efforts made in optimizing the proposed and alternative AIs. Many independent studies have reported these comparison biases over the years [35, 37] but remain unresolved. There is a need to have more widely adopted benchmarks (e.g., challenges) where all AI algorithms are trained by their inventors and evaluated by third parties. Fifth, short-term outcome pressure. Standard benchmarks are often in short-term and non-recurring, requiring a final solution within several months. For example, RSNA 2024 Abdominal Trauma Detection [15] only opened for three months for data access and AI development & evaluation. The short-term outcome pressure can discourage new classes of AI algorithms that need considerable time and computational resources for a thorough investigation, as their vanilla versions (e.g., Mamba [22, 83] in early 2024 and Transformers [16] in early 2021) might not outperform all the alternatives judged. The benchmark must have long-term commitment and allowance.\nTo address this AI mismeasurement issue, we present the Touchstone benchmark, an effort towards the objective of creating a fair, large-scale, and widely-adopted medical AI benchmark. Its scale is large, featuring a training set of 5,195 publicly available CT scans from 76 hospitals and a test set of 5,903 CT scans from additional 11 hospitals. Test sets were unknown to the participants of the benchmark. All 11,098 scans are annotated per voxel for 9 anatomical structures. The training set annotations were created by collaboration between AI specialists and radiologists followed by manual revision [58], 5,160 out of 5,903 test scans are proprietary and manually annotated, and the remaining test datasets are publicly available, annotated by AI-radiologist collaboration. As of May 2024, 14 global teams from eight countries have contributed to our benchmark. These teams are known for inventing novel AI algorithms for medical segmentation. In summary, the Touchstone benchmark explores an evaluation philosophy defined by the following five contributions:\n1. Evaluating on out-of-distribution data: The JHH test set (Sec. 2.1) presents 5,160 CT scans from an hospital never seen during training, introducing a new scale of external validation for abdominal CT benchmarks. The test data distribution varies in contrast enhancement (pre, venous, arterial, post-phases), disease condition (30% containing abdominal tumors at varied stages), demographics (age, gender, race), image quality (e.g., slice thickness of 0.5-1.5 mm), and scanner types. We have collected metadata information for 72% of the test set (N=5,160) and reported AI performance in each sub-group."}, {"title": "2 Touchstone Benchmark", "content": "2.1 Datasets \u2013 Annotations, Statistics, Distribution, & Characteristics\nWe used one training dataset and two test datasets to perform a comprehensive out-of-distribution benchmark. The training and test datasets were collected from many hospitals worldwide. Figure 1 shows the demographics of the two test datasets, JHH and TotalSegmentator; Appendix Figures 3-4 provide examples of CT scans and per-voxel annotations for various demographic groups across all datasets. The JHH dataset is proprietary and used for third-party evaluation; participants do not have access to the CT scans or their annotations. TotalSegmentator is a publicly available dataset; we did not inform the inventors beforehand of its use in our evaluation and confirmed that their AI algorithms had not been trained on this dataset. We included this public dataset to enable future participants to easily compare their algorithms with our benchmark.\nAbdomenAtlas 1.0\u2014N=5,195; publicly available for training purposes\u2014is the largest multi-organ fully-annotated CT dataset to date, encompassing 76 hospitals in 8 countries [58]. It leveraged a"}, {"title": "4 Conclusion & Discussion", "content": "Conclusion. Are we on the right way for evaluating AI algorithms for medical segmentation? This paper outlines five properties of an ideal benchmark: (I) diverse data distribution in both training and test datasets, (II) a large number of test samples, (III) varied evaluation perspectives, (IV) equitably optimized AI algorithms, and (V) a long-term commitment. Touchstone sets itself apart from previous benchmarks in these criteria, enabling us to share unique insights that often missing in standard benchmarks. Our findings indicate: (1) AI performance can vary significantly across different datasets, with per-class differences of 10\u201320% common, and up to 80% observed (SAM-Adapter in kidney); thus, out-of-distribution evaluation across multiple datasets is crucial for ensuring AI's reliability and clinical adoption. (2) Larger test datasets reveal more significant differences between AI algorithms, allowing for meaningful rankings and nuanced analyses. (3) Average rankings can obscure AI's specific strengths; per-organ and metadata analysis is crucial in highlighting the benefits of innovative vision-language algorithms and the first diffusion-based 3D medical segmentation model. (4) By evaluating diverse AI architectures trained by their inventors, we establish a fair reference point for future development, which Touchstone will continually support with a long-term commitment.\nLabel Noise in Training Set. There is no perfect ground truth in segmentation datasets (except for synthetic data [32, 42, 13, 17, 14, 40]), especially in the abdominal region where anatomical boundaries can be blurry due to disease or age (examples in Appendix A.3). Identifying these boundaries is challenging for both human annotators and AI algorithms. Many recent datasets, including TotalSegmentator [75] and AbdomenAtlas 1.0 [58], use human-in-the-loop strategies, combining AI-predicted annotations and manual annotations by radiologists, which inevitably contain label errors. The errors in AbdomenAtlas 1.0 arise from poor CT image quality (e.g., BDMAP_00000339, BDMAP_00001044, BDMAP_00003725), mistakes in AI predictions but not revised by humans, and inconsistency in label standards across the public datasets incorporated into AbdomenAtlas 1.0 [43]. With the feedback from our benchmark participants, we can partially detect these label errors, primarily in the aorta (32.4%), a structure with high annotation standard inconsistency in public data (e.g., in BTCV and FLARE) [46, 47], and in the L&R kidneys (2.6%). We revised AbdomenAtlas 1.0 by reducing label errors in the aorta to 5.4% and in the L&R kidneys to 0.6%. A ResEncL trained on the revised AbdomenAtlas 1.0 showed statistically significant performance gains in the aorta, but gains for kidneys were small and not always statistically significant (see Tables 2-3). These results highlight that current AI may be resistant to moderate levels of label noise (2.6%), but not to high levels (32.4%), as we detail in Appendix E. As future work, an improved label error detector will be a valuable tool for automatically assessing the quality of publicly available datasets and quickly improving quality through human annotation based on detected errors.\nHigh-Quality, Proprietary Test Set. Having JHH (N=5,160) available for third-party evaluation is a big plus for OOD benchmarks. It was completely annotated by radiologists, manually and following a well-defined annotation standard, for several years [57]. Thus, it can serve as a gold standard for our benchmark. The fact that JHH is a private dataset has both problems and benefits. It can significantly increases feedback time for AI performance evaluation, as it requires additional procedures to submit the AI to a third party, set it up, and run it on over 5,000 CT scans. If a benchmark takes too much work to run, it will not gain wide traction. But making test set (either images or annotations) publicly available can cause more problems\u2014including completely destroying the OOD benchmark. For"}]}