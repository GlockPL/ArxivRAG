{"title": "Uncertainty-aware segmentation for rainfall prediction post processing", "authors": ["Simone Monaco", "Luca Monaco", "Daniele Apiletti"], "abstract": "Accurate precipitation forecasts are crucial for applications such as flood management, agricultural planning, water resource allocation, and weather warnings. Despite advances in numerical weather prediction (NWP) models, they still exhibit significant biases and uncertainties, especially at high spatial and temporal resolutions. To address these limitations, we explore uncertainty-aware deep learning models for post-processing daily cumulative quantitative precipitation forecasts to obtain forecast uncertainties that lead to a better trade-off between accuracy and reliability. Our study compares different state-of-the-art models, and we propose a variant of the well-known SDE-Net, called SDE U-Net, tailored to segmentation problems like ours. We evaluate its performance for both typical and intense precipitation events.\nOur results show that all deep learning models significantly outperform the average baseline NWP solution, with our implementation of the SDE U-Net showing the best trade-off between accuracy and reliability. Integrating these models, which account for uncertainty, into operational forecasting systems can improve decision-making and preparedness for weather-related events.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate precipitation forecasts are essential for flood management, agricultural planning, water resource allocation, and weather warnings. Despite significant advancements in numerical weather prediction (NWP) models, these models still exhibit biases and uncertainties, especially at high spatial and temporal resolutions. This is due to the complex, nonlinear nature of atmospheric processes and inherent approximations in NWP models [4, 25]. The direct model output (DMO) of NWPs is highly sensitive to initial conditions, boundary conditions, and parameterization schemes (e.g., orography). Consequently, predictions are incomplete without a characterization of the associated uncertainty [7, 8]. For instance, forecast uncertainty is crucial for the Italian Civil Protection in issuing localized severe weather warnings.\nPost-processing techniques have been developed to mitigate NWP limitations and improve prediction reliability. Traditional statistical methods like model output statistics (MOS) and ensemble model output statistics (EMOS) have been somewhat successful, but often fail to capture the complexity of precipitation patterns and uncertainties [14, 27].\nRecently, machine learning (ML) has shown remarkable results in improving weather forecasts by processing large datasets and recognizing complex patterns that conventional methods struggle with [2, 28].\nOur contributions focus on enhancing the reliability and consistency of rainfall forecast uncertainty estimates through post-processing daily cumulative quantitative precipitation forecasts (QPF) from NWPs in northwestern Italy. We aim to improve prediction accuracy while ensuring reliable uncertainty estimates in precipitation forecasts. By reinterpreting rainfall estimation as an image segmentation task, we explore the application of various deep-learning approaches to develop a post-processing tool that integrates forecasts from multiple NWP models. Alongside state-of-the-art solutions, we introduced SDE U-Net, a variant of SDE-Net. [17], specifically designed for segmentation tasks.\nThis multi-model approach leverages the strengths of individual numerical models, combining them to enhance overall forecast accuracy and reliability [12]. We then comprehensively evaluated the proposed algorithms, particularly focusing on uncertainty estimation for typical and intense weather events. Our analysis addresses the accuracy-reliability tradeoff, balancing confidence in model predictions with the risk of forecasts missing the physical outcomes.\nThe post-processing systems investigated in this work can be integrated into operational forecasting systems, leading to more informed decision-making and better preparation for weather-related events."}, {"title": "2 BACKGROUND", "content": "Uncertainty can have different sources. In a machine learning context, the definitions of aleatoric and epistemic uncertainties help us understand and manage the limitations and reliability of our models' predictions. Aleatoric uncertainty is due to inherent noise in the data: this type of uncertainty is present in the observations and cannot be reduced even if we collect more data. It arises from the natural variability in the data generation process. Epistemic uncertainty reflects the model's uncertainty about its predictions due to insufficient training data or limited model capacity [18]."}, {"title": "2.1 Related Works", "content": "One of the most popular research directions for quantifying uncertainty in neural networks involves Bayesian neural networks (BNNs) [20, 21], which quantify prediction uncertainty by imposing probability distributions over model parameters instead of using point estimates. While BNNs provide a principled method for quantifying uncertainty, the exact derivation of parameter posteriors is often computationally difficult, especially for large input data sets, such as in computer vision tasks.\nAmong the non-Bayesian approaches, a prominent method in this category is model ensembling [15], in which multiple deep neural networks (DNNs) with different initialization are trained and statistics on their predictions are generated for uncertainty estimation. However, training an ensemble of DNNs can be prohibitively expensive.\nOther non-Bayesian methods [10] often mix aleatory uncertainty with epistemic uncertainty. Separating these two sources of uncertainty is crucial for many tasks [1]. SDE-Net [17] addresses this problem by introducing a Brownian motion term into the network architecture to capture epistemic uncertainty and view DNN transformations as state evolution in a stochastic dynamical system. However, this architecture is demonstrated on simple classification and regression tasks with tabular data and cannot be directly applied to segmentation tasks and rainfall prediction without modifications.\nSeveral studies have used Monte Carlo (MC) dropout to estimate uncertainty. Wang et al. [31] analyzed the epistemic and aleatory uncertainty for CNN-based medical image segmentation at both pixel and structural levels.\nTo our knowledge, only a few works have provided estimates of uncertainties in QPF post-processing. Moosavi et al. [22] have recently applied machine learning strategies to estimate and predict NWP errors in precipitation forecasting. Unfortunately, it is specific to the Weather Research and Forecasting (WRF) model and may not generalize well to other weather models."}, {"title": "3 METHODS", "content": "We can formulate our task below with a double interpretation. In the deterministic interpretation, given a true precipitation map P for a given event and a set of n imperfect predictions {Pi}i=1,...n which are results of as many different NWP models, our deep learning algorithm - represented as a parametric function of weights \u03b8 - must produce an output P of the form\n\\(P = f(\\{P_i\\}; \\theta)\\)   (1)\nsuch that the distance function\n\\(L = ||P - \\hat{P}||^2\\)  (2)\nis minimized. This equation presents an L2 loss function, but other alternatives can be employed as needed.\nAlternatively, from a probabilistic point of view, we can think of the NWP outcomes Pi as different i.i.d. samples from a distribution of a stochastic process of the form\n\\(P_i = P + dp_i\\)  (3)\nWhere the dpi represents the epistemic error provided by each numerical model. In this framework, we expect an uncertainty in the model prediction P due to the type of input it was trained on. At the same time, we expect some aleatoric uncertainty due to inherent measure errors in observational data. In this work, we will not directly distinguish between the two and will just provide overall forecast uncertainty estimates where we consider both sources of error.\nConventional deep learning models are generally used deterministically, providing no access to prediction uncertainty. To address this limitation, we propose to reformulate the problem by replacing the parametric model f with a variant that can produce a distribution of outcomes instead of a single value. In other words, the model prediction can be represented as a sample from this distribution:\n\\(P \\sim f(\\{P_i\\}; \\theta)\\)  (4)\nWhere f represents the variational model. Given \\(\\tilde{Y} = \\{P_i\\}^n\\) a set of n samples from the predictive distribution, we can define the prediction intervals (PIs) with a confidence level of \\(\u03b3 \u2208 [0, 1)\\) as the range \\([l(\\tilde{Y}), u(\\tilde{Y})]\\) such that the probability \\(P (l(\\tilde{Y}) < P_{n+1} < u(\\tilde{Y})) = \u03b3\\), which indicate the expected error between the prediction and the actual targets. A large PI indicates greater uncertainty in the model's predictions. While the actual precipitation value is likely to be within the specified interval, the predictions may not be very accurate. Essentially, a large PI indicates that the model has less confidence in its predictions, reflecting greater variability in the input data or inherent challenges in the prediction process.\nConversely, a small PI indicates a higher confidence in the model's predictions, which suggests that the actual precipitation value is likely to be very close to the predicted value. However, this also increases the risk that the actual values will lie outside these PIs. The optimal PI range, therefore, depends heavily on the practical application and is a tradeoff between accuracy and reliability.\nIn the context of rainfall prediction, NWP simulations Pi typically exhibit a large PI due to varying mathematical assumptions in the different models. While this broad PI is beneficial for capturing intense meteorological events, it can also lead to excessive uncertainty. Ideally, a refined model should reduce this range while maintaining sufficient width to capture significant weather events effectively."}, {"title": "3.1 Case study", "content": "Our study aims to estimate forecast uncertainties in daily cumulative QPF over northwestern Italy, specifically focusing on the Piemonte and Valle d'Aosta regions over 24 hours. These areas present a particular challenge for precipitation forecasting due to their varied topography, significantly influencing local precipitation patterns.\nTo address this task, we compiled a dataset of gridded daily cumulative precipitation observations from ground stations provided by ARPA Piemonte, covering the Area of Interest (AoI) with a spatial resolution of approximately 12 km. These observations, namely the Pis, are interpreted as images of size L \u00d7 W, with each pixel being the precipitation within the associated land area. For each real observation, n NWP outcomes are collected and gridded to match the shape of the ground truth, producing an image of size L \u00d7 W \u00d7 n when stacked together along the channel axis."}, {"title": "3.2 Deep learning architectures", "content": "Within this framework, the problem can be phrased as a segmentation task. We chose a U-Net architecture [26] as our deterministic backbone network. Despite the availability of many newer alternatives, U-Net remains extremely popular in fields such as medical imaging, remote sensor analysis, and diffusion models [23, 29]. Its skip-connected encoder-decoder structure is particularly well suited for capturing both local and global contexts, making it ideal for our task, for which we have experimentally found that other more complex architectures do not yield remarkable results. However, it is worth noting that our choice of U-Net is not crucial for the subsequent analysis. All the model changes we will present can also be applied to other segmentation backbones.\nBased on this, we have developed several models for segmentation under uncertainty that incorporate the best-known strategies from the literature to achieve this property for different tasks. In the following sections, we briefly introduce these models and highlight our contributions to the development of some of them."}, {"title": "3.2.1 Monte Carlo Dropout U-Net", "content": "Henceforth MCD U-Net, this approach enhances the backbone model with a Monte Carlo Dropout (MCD) strategy [24]. Dropout, originally introduced as a regularization procedure, involves randomly discarding a subset of neurons during training to prevent overfitting and improve the generalization of the model. In MCD, this concept is extended to the testing phase to estimate uncertainty, as the dropout at the time of inference can be considered as a Bayesian approximation. Dropout layers are applied during inference, and multiple forward passes are performed to generate a distribution of predictions. The variance of these predictions is then a measure of the uncertainty of the model."}, {"title": "3.2.2 Deep Ensemble U-Net", "content": "Henceforth Ens U-Net, in this technique, several U-Net models are trained independently of each other with different initializations [15]. As with the previous method, this approach also leads to variability in the model results, although the number of trained models limits the possible different output patterns."}, {"title": "3.2.3 SDE U-Net", "content": "SDE-Net was recently proposed by Kong et al. [17] to integrate Stochastic Differential Equations (SDEs) into deep learning models for capturing uncertainty. Neural networks can be viewed as continuous-time transformations of input dynamics, with model epistemic uncertainty accessed by viewing this process as a stochastic dynamical system governed by the following stochastic differential equation:\n\\(dx_t = f(x_t, t; \\theta_f)dt + g(x_0; \\theta_g)dW_t\\)  (5)\nHere, the diffusion term g modulates the Brownian motion dWt, representing the stochastic component of the process. The parametric functions f(-; \u03b8f) and g(-; \u03b8g) are two neural networks trained to model aleatoric and epistemic uncertainty, respectively. The training strategy ensures that g provides a small variance for data within the training distribution and a large variance for data outside it. This is obtained by addressing the following objective function:\n\\(\\min_{\\theta_f} E [L(x_T)] + \\min_{\\theta_g} E_{x_0} [g(x_0; \\theta_g)] + \\max_{\\theta_g} E_{x_0} [g(x_0; \\theta_g)]\\),  (6)\nwhere L is the task-dependent loss function enforcing stochastic process' terminal outcome xT to approach the target prediction and x0 is an out-of-distribution sample obtained by adding Gaussian noise to the initial state x0 sampled from the training data.\nThe original implementation of SDE-Net develops the input-output system over the time interval [0, T] using an Euler-Maruyama scheme, where the two components of the equation 5 are added iteratively with a fixed step size. This allows using the same networks at each time step, reducing the overall number of weights.\nExtending this strategy to the U-Net architecture is a challenge because the main advantage of U-Net lies in its networked encoder-decoder structure. In U-Net, the input signal going into each encoder block generates an output that serves as the input for the next encoder block and is also passed to the corresponding decoder block via skip connections. These blocks have different input and output channels, which makes it impossible to share weights between them. To incorporate the SDE-Net strategy, we set the number of time splits to match the number of encoder blocks. For each encoder block, we construct a diffusion block placed at each skip connection and simulate an integration step at each encoder-decoder exchange.\nThis network is trained using the strategy proposed in [17] to assign higher uncertainty to out-of-distribution inputs, enabling effective uncertainty quantification in segmentation tasks while preserving the essential U-Net structure."}, {"title": "3.3 Experimental design and Validation metrics", "content": "To measure the benefits of uncertainty-aware architectures in rainfall prediction tasks, we train the models to reconstruct precipitation maps from different typical events. In contrast, we also collect a set of events labelled as intensive and separated from the training data. Intensive events are all those where the maximum recorded precipitation within the RoI (Region of Interest) exceeds the 99th percentile for the corresponding season. Further insights are given in the following section. Based on this separation, we expect a deep learning model to perform better when evaluated on typical events, while the performance degrades for intense events. We compare the uncertainty provided by a PoorMan's Ensemble (average of NWP forecasts), which is our benchmark, with the forecast uncertainty from each considered machine learning model. To get a basic uncertainty estimate, we use normalized rMSE, while to quantify the trade-off between accuracy and reliability, we introduce a coverage-length-based criterion (CLC) as defined in [19]\n\\(CLC = \\frac{NMPIL}{\u03c3(PICP, \u03b7, \u03bc)}\\)  (7)\nwhere \u03c3 is a sigmoid function with scaling parameter \u03b7 and translation parameter \u03bc:\n\\(\u03c3(PICP, \u03b7, \u03bc) = \\frac{1}{1 + e^{-\u03b7(PICP-\u03bc)}}\\)  (8)\nWe aim to achieve low values of Normalized Mean Prediction Interval Length (NMPIL), as it indicates a narrower spread in ensemble predictions, which we seek to minimize for more meaningful and useful predictions. However, reducing NMPIL negatively affects the coverage of Prediction Intervals (PIs), resulting in an undesirable number of predictions falling outside the PIs. To address this issue, we aim for high PI Coverage Probability (PICP) values, which measure the proportion of target values within the prediction interval. Consequently, we strive for the smallest possible values of CLC. The parameter \u03b7 controls the penalty when PICP falls below the minimum acceptable level \u03bc.\nIdeally, the threshold value of acceptability \u03bc should be as close as possible to 1, so we set it to a reasonably high value, namely \u03bc = \u03b3 = 0.95. In [5] the parameter \u03b7 is explored in the context of neural networks training in order to study learning sensitivity and dynamics, leading to a useful range 1 < \u03b7 < 10: this contribution can be applied also in other contexts such as CLC, so we examine the behaviour of CLC as a function of \u03b7 ranging from 0 to 12, which slightly extends the suggested range. Of course, predictions falling outside a PI with \u03bc = 0.95 should be strongly penalized, so we are particularly interested in the CLC values for high \u03b7 (i.e. around 10). Accordingly, we provide tabular values for rMSE, PICP, NMPIL, and CLC with \u03b7 = 12, the maximum value considered in our analysis.\nMCD U-Net and SDE U-Net use 20 sampled predictions, while Ens U-Net (3.2.2) is based on 5 repetitions, i.e., as many ensemble models. This approach estimates forecast uncertainty for each model, reflecting epistemic error. We then repeat the process in a 9-fold cross-validation to ensure statistical significance, accessing aleatoric uncertainty. This involves training the models on nine different training-validation-test subsets, derived using weather physics considerations detailed in subsection 4.1. Uncertainty estimates for each validation metric VM are provided as:\n\\(VM = VM_{repetitions} \u00b1 VM_{9-fold CV}\\)  (9)"}, {"title": "4 EXPERIMENTS", "content": "As previously mentioned, the dataset includes observations from ground stations provided by ARPA Piemonte, preprocessed using optimum interpolation [13] to generate images on a fixed grid. These observations span from 1957 to the present, providing a continuous and comprehensive record of precipitation across various meteorological conditions. The dataset also includes precipitation forecasts from four NWP models: BOLAM-CNR [6], ECMWF-IFS [11], COSMO-2I [3], and COSMO-5M [9].\nEvents were classified as \"intense\" if their spatial maximum precipitation exceeded the seasonal 99th percentile, with thresholds of 64.58mm in winter, 95.71mm in spring, 93.26mm in summer, and 140.40mm in autumn. This classification resulted in 436 events, 40 of which were marked as intense and set aside during the training phase.\nFor typical events, we applied K-means clustering based on the variability-average plane to categorize events into convective, stratiform, and intermediate types. These types are characterized by high spatial variability and low spatial average precipitation, low spatial variability and high spatial average precipitation, and intermediate characteristics, respectively [16, 30]. The dataset was then divided into nine distinct training-validation-test subsets, ensuring uniform event type distribution across all subsets. The code for our experiments is available online \u00b9, while the dataset can be shared upon request."}, {"title": "4.2 Results", "content": "Figure 1 displays the behaviour of CLC for \u03bc = 0.95 and \u03b7 values ranging from 0 to 12 for typical and intense events, comparing all deep learning models to the average of the weather models (PoorMan's Ensemble, PME). Error bars are omitted for readability. Smaller CLC values indicate a better trade-off between accuracy and reliability, achieved through low NMPIL and high PCIP values, particularly at higher \u03b7 values. Table 1 summarizes the results of our analysis in terms of CLC with \u03bc = 0.95 and \u03b7 = 12, alongside rMSE, PICP, and NMPIL for all uncertainty-aware models compared to PME.\nAs expected, rMSE is higher for intense events than for typical events, with all deep learning models significantly outperforming PME. The low uncertainty in rMSE estimates highlights the model with the best rMSE. Ens U-Net achieves the lowest rMSE for typical events (8.15\u00b710-3), while SDE U-Net achieves the lowest for intense events (2.63\u00b710-2), indicating higher prediction accuracy.\nThe PICP column shows that PME has better percentage coverage than the learning models by 10 to 15%, but at the cost of much wider prediction intervals, as indicated by the NMPIL column. PME has NMPIL values of 1.483\u00b710-2 for typical events and 3.605\u00b710-2 for intense events, compared to the deep learning models' range of 2\u00b710-3 to 8\u00b710-3 for both event types. This suggests that PME predictions are reliable but lack accuracy.\nHowever, the trade-off between accuracy and reliability differs for typical and intense events when considering CLC at \u03b7 = 12. For typical events, CLC indicates no substantial advantage for deep learning models over PME (0.09 < CLC < 0.11 for PME, Ens U-Net, and SDE U-Net), although MCD U-Net shows a slight improvement (0.065). For intense events, SDE U-Net has the lowest CLC value (0.229), with the differences between PME, MCD U-Net, and Ens U-Net being negligible. While SDE-UNet does not have the highest PICP, its primary advantage is the small prediction intervals, as reflected in NMPIL, especially for intense events (3.45\u00b710-3). This results in a highly favourable accuracy-reliability trade-off, as shown by CLC.\nFor 8 < \u03b7 < 12, PME consistently shows higher CLC values for typical events compared to deep learning models, though the difference is minor. SDE U-Net and Ens U-Net perform comparably or slightly better than MCD U-Net around \u03b7 = 9. For intense events, PME consistently underperforms against the deep learning models. MCD U-Net and Ens U-Net have similar CLC values, but SDE U-Net demonstrates the best accuracy-reliability tradeoff.\nOverall, these results highlight the effectiveness of deep learning models, particularly the SDE U-Net, in providing accurate and precise rainfall predictions while maintaining a reasonable level of uncertainty quantification."}, {"title": "5 CONCLUSIONS", "content": "To summarise, our study demonstrates the effectiveness of deep learning solutions to improve the accuracy and reliability of NWP post-processing systems for precipitation forecasts. By evaluating both typical and intense precipitation events, we found that all deep learning models significantly outperformed the average baseline NWP solution, with our implementation of SDE-UNet showing the best trade-off between accuracy and reliability.\nIntegrating these models, which account for uncertainty, into operational forecasting systems can improve decision-making and better preparation for weather-related events. Future work will focus on refining these models and exploring alternatives to achieve more comprehensive results in predicting precipitation while accounting for uncertainty."}]}