{"title": "Social and Ethical Risks Posed by General-Purpose LLMs for Settling Newcomers in Canada", "authors": ["Isar Nejadgholi", "Maryam Molamohammadi"], "abstract": "The non-profit settlement sector in Canada supports newcomers in achieving successful integration. This sector faces increasing operational pressures amidst rising immigration targets, which highlights a need for enhanced efficiency and innovation, potentially through reliable AI solutions. The ad-hoc use of general-purpose generative AI, such as ChatGPT, might become a common practice among newcomers and service providers to address this need. However, these tools are not tailored for the settlement domain and can have detrimental implications for immigrants and refugees. We explore the risks that these tools might pose on newcomers to first, warn against the unguarded use of generative AI, and second, to incentivize further research and development in creating AI literacy programs as well as customized LLMs that are aligned with the preferences of the impacted communities. Crucially, such technologies should be designed to integrate seamlessly into the existing workflow of the settlement sector, ensuring human oversight, trustworthiness, and accountability.", "sections": [{"title": "1- Introduction", "content": "In Canada, government-sponsored settlement programs assist immigrants and refugees in tackling their unique challenges. The goals of the settlement sector are to facilitate short-term settlement and long-term social, cultural and civic integration [1]. The benefits of a robust settlement sector extend beyond the experiences of newcomers themselves and contribute to the prosperity of society at large. According to a report from Immigration, Refugees and Citizenship Canada (IRCC), over"}, {"title": "2- Harms of General-Purpose LLMs", "content": "Following the taxonomy introduced by Weidinger et al. [13], we explore the specific vulnerabilities of newcomers when using LLMs in the host environment. These harms can be categorized into four distinct groups: discrimination and exclusion harms, misinformation harms, information hazards, and malicious use. We omit the discussion of two other categories identified in the taxonomy: human-AI interaction and environmental and socioeconomic harms, which can only be evaluated following a systemic and at-scale deployment of LLMs within a sector. We first discuss the four categories of harm in the context of settlement and then, where possible, present examples of how the unsafe use of LLMs can impact newcomers."}, {"title": "2-1 Discrimination and Exclusion Harm", "content": "Generative models exhibit biases in their output, including gender and racial biases in regard to professional roles and recommendation letters [14, 15]. Also, these models tend to generate lower-quality output when prompted in non-English or less common dialects of English [16, 17] or flag texts written by minorities as problematic content [18]. Potential scenarios where newcomers might face discrimination when using generative AI include: 1) A language model may make stereotypical job suggestions to a newcomer with a distinct English dialect, 2) A language model could ignore specific concerns of a newcomer family, such as language support programs, when providing information about the local school system, possibly making them feel neglected and excluded, and 3) A language model might not recognize sensitive topics related to the newcomers' specific backgrounds and refuse to reply, associating the question with offensive language and potentially alienating the user from seeking further assistance. In the next section, we present examples of this category of harm observed in testing the ChatGPT models in settlement-related scenarios."}, {"title": "2-2 Misinformation Harms", "content": "LLMs generate false, misleading or low-quality information, referred to as hallucinations [19]. Although factually incorrect, the hallucinated text is grammatically correct, fluent, and seems authentic, often leading to uncritical trust and contributing to misguided decision-making and a cascade of unintended consequences [20]. Users tend to trust technology when they find it reliable in most cases [21]; thus, arguably, an LLM that mostly generates factually correct predictions may pose a greater hazard than one that often generates incorrect results. Several potential scenarios under this category of harm include 1) An LLM provides inaccurate legal advice when used to understand the rights and obligations under the law, and 2) An LLM provides inaccurate advice when used to navigate the healthcare system in the new country about products and treatments that vary significantly by region resulting in adverse health outcomes or even legal issues. The next section explores examples of inaccurate information observed when testing the ChatGPT system in the context of settlement."}, {"title": "2-3 Information Hazard", "content": "Literature shows the potential dissemination of information can cause harm to an individual or a group even if there is no malicious intent by a technology designer and no mistake by the user. This harm can result from the revelation of private information stemming from memorization of training data [22,23], inferences drawn from correlated data [24], or providing access to typically inaccessible types of data [25]. Individuals may enter sensitive information such as their Social Insurance Number, residential address, or insurance card numbers into the continuously collected data flow, which is then further fed to LLMs, increasing the possibility of their sensitive data being exposed. In interviews with service providers, we confirmed that newcomers mostly use ChatGPT to fill out forms. This widespread use highlights the vulnerability of newcomers, who must navigate an overwhelming number of forms and administrative procedures, often without adequate guidance or support.\nDespite the importance of this category of risks from LLMs, we are not able to provide examples of such hazards in the next section. Creating controlled experimental conditions that accurately reflect real-world scenarios where such sensitive information might be shared is a challenging task. Moreover, the potential long-term impacts of data breaches introduced by Al systems cannot be easily captured in short-term experiments, and more comprehensive and longitudinal studies are required to fully understand and mitigate these risks."}, {"title": "2-4 Malicious Uses", "content": "LLMs can harm newcomers when used by malicious parties by enhancing the perpetrators' capacity in the execution of intentional harm. Particularly, LLMs are"}, {"title": "3- Case Studies of Harmful Uses of ChatGPT in Settlement", "content": "In this section, we present a set of experiments that showcase the potential harms of the ad-hoc use of general-purpose LLMs during the settlement journey. Although most of these experiments are designed with simplified prompts, they provide evidence for harms that might appear in more realistic and complex scenarios. Besides raising awareness about the unsafe use of general-purpose LLMs, the following experiments demonstrate the harms that need to be accounted for and mitigated when creating customized AI tools for this sector. For all experiments where the ChatGPT is assumed to be used by a newcomer, we used GPT-3.5, and for those where ChatGPT is used by other people in scenarios related to newcomers, we used GPT-40. In all cases, we used the OpenAI interface to simulate real user experiences. In order to keep the experiments independent of each other, the memory of the ChatGPT account we used was turned off."}, {"title": "3-1 Bias in Employment Suggestions", "content": "Background: One of the most crucial steps to newcomers' settlement is finding employment. This can be particularly challenging due to potential language barriers and unrefined skill-matching systems in the host country. Aside from facilitating settlement, finding employment is critical to newcomers' successful economic, social and cultural integration. Participating in the workforce gives individuals the ability to familiarize themselves with the people, culture, and institutions of their host country. Moreover, a newcomer's first job in their host country significantly impacts their career. It establishes the baseline of compensation and experience from which they can build up. To support newcomer integration and ensure effective skill-matching, employment recommendation systems should be fair. Notably, the newcomer's race, gender, background, sexuality, and other aspects of their identity should not perversely impact their employment prospects. Given the"}, {"title": "3-2 Performance Disparity by Language", "content": "Background: In the short-term settlement stage, newcomers to Canada are often adapting to life in a new country by familiarizing themselves with its culture, social services, and institutions. For instance, newcomer families may consider updating their children's health records to match Canadian standards. Notably, newcomers often experience language barriers in the medical domain and may have difficulty accessing information and services in a language they are comfortable with. In these instances, conversational agents like ChatGPT may be used by newcomers so they can access information in their preferred language. We consider a scenario where newcomers use ChatGPT to access information about vaccine requirements for their children."}, {"title": "3-3 Performance Disparity in Official Languages", "content": "Background: In a bilingual country like Canada, all government services are offered in the official languages of English and French. To support bilingualism and language rights, the use of conversational LLMs should reflect similar performance across these official languages."}, {"title": "3-4 Stereotypical Representation", "content": "Background: While AI tools can be used by newcomers to support their settlement journey, they may also be used by other stakeholders to access information about newcomers. We consider a scenario where ChatGPT is used to represent newcomers, including immigrants and refugees. While specific demographics may be more prevalent among immigrants and refugees to Canada, it is crucial to consider the effects of encoded demographic overgeneralizations in LLM tools. The perpetuation of stereotypical representations has the potential to be harmful, divisive, and exclusionary. This is especially true given the advanced multimodal capabilities of emerging tools. Biases and stereotypical representations can now appear in text descriptions, images, and videos."}, {"title": "3-5 Hallucinations", "content": "Background: Throughout their settlement journey, newcomers often require reliable information regarding social services and institutions. This includes schools or universities, medical clinics, and SPOs to facilitate their settlement. Given the"}, {"title": "3-6 Misinformation", "content": "Background: Upon arrival, newcomers may have difficulty accessing context-specific common knowledge. For example, when searching for a job, they need to gather information about wage rates, maximum hours they are allowed to work, and other relevant information. We consider a scenario where newcomers use ChatGPT to ask about minimum wage and maximum working hours. In Canada, the minimum wage is established by the government of the specific jurisdiction or by the federal government for federally regulated businesses. Minimum wage adjustments typically occur annually or semi-annually and often reflect changes in inflation and the Consumer Price Index. Overtime rates represent the minimum compensation required for salaried or hourly employees who work beyond a legally defined threshold of hours per week (or, in some places, per day). Depending on the jurisdiction, certain industries may have exemptions from these regulations. In most cases, the maximum hours of work allowable in a week is 48."}, {"title": "3-7 Multimodal Malicious Use", "content": "Background: While previous scenarios focused on potential harms that arise from the use of generative models by newcomers, tools like ChatGPT might also be used against newcomers. We consider a scenario where a malicious user takes advantage of ChatGPT to target newcomers with scams or fraud. Newcomers to Canada may not be used to how governments, companies, and other institutions operate. As such, they are prone to being disproportionately targeted by malicious users who take advantage of their status and lack of familiarity. Moreover, technological advances often facilitate cheaper and more effective malicious uses due to a lack of regulation or safeguards. This is especially true in the case of generative AI tools which are becoming increasingly proficient at producing multimodal outputs (ex. text, image, audio, video,...). These capabilities can be used to make calls, texts, emails, and advertisements that seem credible and legitimate, especially if users lack the resources and literacy to differentiate between real and malicious AI-enabled inquiries."}, {"title": "4- The Way Forward", "content": ""}, {"title": "4-1 The Need to Customize AI Tools", "content": "There is a critical need to empower the settlement sector with customized and tailored Al tools for information delivery and other established services [28]. To-day's AI technologies are built on top of LLMs as foundational models, making it crucial to consider their potential harms when designing task-specific AI tools. Specifically, the mitigation of biases needs to be a primary consideration. Although"}, {"title": "4-2 Potential AI Applications", "content": "The settlement sector in Canada can greatly benefit from the integration of customized AI technologies, enhancing the support provided to newcomers and refugees. These tools need to be designed and deployed within the existing service structures of SPOs and an expert-in-the-loop paradigm. emphasizes the importance of designing and adopting AI technologies within existing frameworks to ensure accountability, transparency, and human oversight. This approach enhances service delivery and ensures that Al integration is aligned with the sector's core values and objectives.\nThe figure showcases several examples of AI applications across different service areas. For instance, in Language Training and Assessment, AI can offer personalized study plans and language training, helping individuals improve their language skills more efficiently. In Information & Orientation, AI can deliver personalized information on health, finances, law, and employment and provide digital skill training. These AI-driven solutions can ensure newcomers receive tailored and relevant information, significantly improving their ability to integrate into Canadian society.\nAdditionally, AI can play a crucial role in Needs and Assets Assessment and Referral Services by processing assets data and predicting needs, which enables better"}, {"title": "5- Conclusion", "content": "Although AI has been used and studied in immigration, this use has predominantly been in border security and screening processes, which often raised ethical concerns [47, 48]. To the best of our knowledge, this is one of the first attempts to explore the potential and limitations of AI in the settlement sector. We highlighted how new immigrants and refugees might become overly dependent on and vulnerable to the extensive use of generic chatbots and raised awareness about the challenges and implications of over-reliance on such technologies. Our experiments included examples of performance discrepancies across Canada's official languages, biases in employment-related suggestions, performance discrepancies across languages in accessing health information, as well as examples of hallucinations, misinformation, stereotypical representations, and potential misuse.\nOur work serves as a call to action for improving these technologies to support the successful integration of newcomers into their new societies. For such technologies to be reliable in settlement services, they require careful alignment and customization to address the potential harms of generative models. We provided recommendations and encouraged further research on developing Al literacy programs and designing aligned LLMs for the newcomer community in Canada. This also includes establishing guidelines for fairness in AI, promoting transparency in AI operations, and fostering an environment where users from all backgrounds can trust and benefit equally from Al technologies. This work will be inherently multi-disciplinary, participatory, and human-centred, involving collaboration between policymakers, technologists, stakeholders, and settlement service providers. It is crucial to note that settlement service providers must be at the center of the design and adoption of such technologies. This approach ensures that newcomers' unique needs and contexts are accurately addressed through human expert oversight and within the established and accountable structures."}]}