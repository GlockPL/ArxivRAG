{"title": "Social and Ethical Risks Posed by General-Purpose LLMs for Settling Newcomers in Canada", "authors": ["Isar Nejadgholi", "Maryam Molamohammadi"], "abstract": "The non-profit settlement sector in Canada supports newcomers in achieving successful integration. This sector faces increasing operational pressures amidst rising immigration targets, which highlights a need for enhanced efficiency and innovation, potentially through reliable AI solutions. The ad-hoc use of general-purpose generative AI, such as ChatGPT, might become a common practice among newcomers and service providers to address this need. However, these tools are not tailored for the settlement domain and can have detrimental implications for immigrants and refugees. We explore the risks that these tools might pose on newcomers to first, warn against the unguarded use of generative AI, and second, to incentivize further research and development in creating AI literacy programs as well as customized LLMs that are aligned with the preferences of the impacted communities. Crucially, such technologies should be designed to integrate seamlessly into the existing workflow of the settlement sector, ensuring human oversight, trustworthiness, and accountability.", "sections": [{"title": "1- Introduction", "content": "In Canada, government-sponsored settlement programs assist immigrants and refugees in tackling their unique challenges. The goals of the settlement sector are to facilitate short-term settlement and long-term social, cultural and civic integration [1]. The benefits of a robust settlement sector extend beyond the experiences of newcomers themselves and contribute to the prosperity of society at large. According to a report from Immigration, Refugees and Citizenship Canada (IRCC), over 0.5M clients accessed settlement services in 2022, which demonstrates a significant demand for these services [2]. Information and Orientation (I&O) services receive the second-highest funding allocations at $313M, representing 19% of the total funding. Despite this investment, clients reported difficulty accessing I&O services in the Newcomer Outcomes Survey 2020-2021 [2]. They cited the following challenges: not knowing how or where to access services, the inadequacy of services themselves, capacity limits or inconvenient hours. Some were even unaware of these services and would have accessed them otherwise, particularly women, resettled refugees, and protected groups.\nThe Canadian settlement sector will likely face increasing operational bottlenecks as the government's immigration targets grow by 25% in three years [3]. With a larger client base, the Canadian settlement sector is evidently stretched and needs enhanced efficiency to manage increasing demands effectively. Reliable Al solutions tailored to the needs of newcomers might lift the pressure on the sector by streamlining access to information. These solutions can be integrated into online platforms, which have shown stronger knowledge outcomes and are preferred by most I&O clients [4]. Some cities like Calgary have already developed and piloted centralized online initiatives to enhance cooperation between Service Provider Organizations (SPOs) and reduce service duplication in order to increase service delivery efficiency across the city. However, there are currently no automation aspects in this initiative.\nIn the absence of reliable online information delivery platforms, the ad hoc use of generative AI, such as ChatGPT, might become a common practice among newcomers and service providers. The Office of the United Nations High Commissioner for Human Rights' latest report on Digital Border Governance notes the growing use of generative AI in the migration sector internationally [5]. In Canada, an article from CIC News [6] shared a ChatGPT guide for Canadian newcomers, which has reached over 30,500 shares in a few months. The guide's content, the article's context, and the total number of shares are early signs of an accrued interest in generative Al by newcomers. Also, several plugins have recently been introduced to help newcomers in their immigration journey.\u00b9 There is, however, an evident lack of data on this usage among newcomers and SPO staff workers that must be further investigated.\nThis chapter warns against the ad-hoc use of general-purpose generative Al for information access during settlement. These tools can have detrimental implications on marginalized communities, specifically immigrants and refugees [7]. In the context of settlement, newcomers might not be well-equipped to fact-check potentially inaccurate outputs. They can be specifically vulnerable as they undergo a challenging transitory phase of life, are unfamiliar with the specifics of the host country, experience language barriers, and might be at the intersection of underserved demographics. Besides factually incorrect outputs, other harms might occur since generic Large Language Models (LLMs) are not well-aligned with the needs of newcomers. Although these models undergo training procedures to align with human"}, {"title": "2- Harms of General-Purpose LLMs", "content": "Following the taxonomy introduced by Weidinger et al. [13], we explore the specific vulnerabilities of newcomers when using LLMs in the host environment. These harms can be categorized into four distinct groups: discrimination and exclusion harms, misinformation harms, information hazards, and malicious use. We omit the discussion of two other categories identified in the taxonomy: human-AI interaction and environmental and socioeconomic harms, which can only be evaluated following a systemic and at-scale deployment of LLMs within a sector. We first discuss the four categories of harm in the context of settlement and then, where possible, present examples of how the unsafe use of LLMs can impact newcomers."}, {"title": "2-1 Discrimination and Exclusion Harm", "content": "Generative models exhibit biases in their output, including gender and racial biases in regard to professional roles and recommendation letters [14, 15]. Also, these models tend to generate lower-quality output when prompted in non-English or less common dialects of English [16, 17] or flag texts written by minorities as problematic content [18]. Potential scenarios where newcomers might face discrimination when using generative AI include: 1) A language model may make stereotypical job suggestions to a newcomer with a distinct English dialect, 2) A language model could ignore specific concerns of a newcomer family, such as language support programs, when providing information about the local school system, possibly making them feel neglected and excluded, and 3) A language model might not recognize sensitive topics related to the newcomers' specific backgrounds and refuse to reply, associating the question with offensive language and potentially alienating the user from seeking further assistance. In the next section, we present examples of this category of harm observed in testing the ChatGPT models in settlement-related scenarios."}, {"title": "2-2 Misinformation Harms", "content": "LLMs generate false, misleading or low-quality information, referred to as hallucinations [19]. Although factually incorrect, the hallucinated text is grammatically correct, fluent, and seems authentic, often leading to uncritical trust and contributing to misguided decision-making and a cascade of unintended consequences [20]. Users tend to trust technology when they find it reliable in most cases [21]; thus, arguably, an LLM that mostly generates factually correct predictions may pose a greater hazard than one that often generates incorrect results. Several potential scenarios under this category of harm include 1) An LLM provides inaccurate legal advice when used to understand the rights and obligations under the law, and 2) An LLM provides inaccurate advice when used to navigate the healthcare system in the new country about products and treatments that vary significantly by region resulting in adverse health outcomes or even legal issues. The next section explores examples of inaccurate information observed when testing the ChatGPT system in the context of settlement."}, {"title": "2-3 Information Hazard", "content": "Literature shows the potential dissemination of information can cause harm to an individual or a group even if there is no malicious intent by a technology designer and no mistake by the user. This harm can result from the revelation of private information stemming from memorization of training data [22,23], inferences drawn from correlated data [24], or providing access to typically inaccessible types of data [25]. Individuals may enter sensitive information such as their Social Insurance Number, residential address, or insurance card numbers into the continuously collected data flow, which is then further fed to LLMs, increasing the possibility of their sensitive data being exposed. In interviews with service providers, we confirmed that newcomers mostly use ChatGPT to fill out forms. This widespread use highlights the vulnerability of newcomers, who must navigate an overwhelming number of forms and administrative procedures, often without adequate guidance or support.\nDespite the importance of this category of risks from LLMs, we are not able to provide examples of such hazards in the next section. Creating controlled experimental conditions that accurately reflect real-world scenarios where such sensitive information might be shared is a challenging task. Moreover, the potential long-term impacts of data breaches introduced by Al systems cannot be easily captured in short-term experiments, and more comprehensive and longitudinal studies are required to fully understand and mitigate these risks."}, {"title": "2-4 Malicious Uses", "content": "LLMs can harm newcomers when used by malicious parties by enhancing the perpetrators' capacity in the execution of intentional harm. Particularly, LLMs are shown to make disinformation cheaper and more systematic, facilitate fraud and targeted manipulation, and assist cyber-attacks, illegitimate surveillance, and censorship [26]. While settling into a new environment, finding trustworthy sources can be challenging, and newcomers often have various needs, which makes them more susceptible to being targeted by malicious actors [27]. In a likely scenario, newcomers from a specific age group might respond to scam ads designed based on their settlement needs. This could cause both material and psychological. Disinformation campaigns can also target specific communities to manipulate their perspective toward desired results. In the next section, we show examples of how the ChatGPT system might be misused by malicious actors and highlight that the settlement sector needs technologies to detect and combat such misuse."}, {"title": "3- Case Studies of Harmful Uses of ChatGPT in Settlement", "content": "In this section, we present a set of experiments that showcase the potential harms of the ad-hoc use of general-purpose LLMs during the settlement journey. Although most of these experiments are designed with simplified prompts, they provide evidence for harms that might appear in more realistic and complex scenarios. Besides raising awareness about the unsafe use of general-purpose LLMs, the following experiments demonstrate the harms that need to be accounted for and mitigated when creating customized AI tools for this sector. For all experiments where the ChatGPT is assumed to be used by a newcomer, we used GPT-3.5, and for those where ChatGPT is used by other people in scenarios related to newcomers, we used GPT-40. In all cases, we used the OpenAI interface to simulate real user experiences. In order to keep the experiments independent of each other, the memory of the ChatGPT account we used was turned off."}, {"title": "3-1 Bias in Employment Suggestions", "content": "Background: One of the most crucial steps to newcomers' settlement is finding employment. This can be particularly challenging due to potential language barriers and unrefined skill-matching systems in the host country. Aside from facilitating settlement, finding employment is critical to newcomers' successful economic, social and cultural integration. Participating in the workforce gives individuals the ability to familiarize themselves with the people, culture, and institutions of their host country. Moreover, a newcomer's first job in their host country significantly impacts their career. It establishes the baseline of compensation and experience from which they can build up. To support newcomer integration and ensure effective skill-matching, employment recommendation systems should be fair. Notably, the newcomer's race, gender, background, sexuality, and other aspects of their identity should not perversely impact their employment prospects. Given the"}, {"title": "3-2 Performance Disparity by Language", "content": "Background: In the short-term settlement stage, newcomers to Canada are often adapting to life in a new country by familiarizing themselves with its culture, social services, and institutions. For instance, newcomer families may consider updating their children's health records to match Canadian standards. Notably, newcomers often experience language barriers in the medical domain and may have difficulty accessing information and services in a language they are comfortable with. In these instances, conversational agents like ChatGPT may be used by newcomers so they can access information in their preferred language. We consider a scenario where newcomers use ChatGPT to access information about vaccine requirements for their children."}, {"title": "3-3 Performance Disparity in Official Languages", "content": "Background: In a bilingual country like Canada, all government services are offered in the official languages of English and French. To support bilingualism and language rights, the use of conversational LLMs should reflect similar performance across these official languages."}, {"title": "3-4 Stereotypical Representation", "content": "Background: While AI tools can be used by newcomers to support their settlement journey, they may also be used by other stakeholders to access information about newcomers. We consider a scenario where ChatGPT is used to represent newcomers, including immigrants and refugees. While specific demographics may be more prevalent among immigrants and refugees to Canada, it is crucial to consider the effects of encoded demographic overgeneralizations in LLM tools. The perpetuation of stereotypical representations has the potential to be harmful, divisive, and exclusionary. This is especially true given the advanced multimodal capabilities of emerging tools. Biases and stereotypical representations can now appear in text descriptions, images, and videos."}, {"title": "3-5 Hallucinations", "content": "Background: Throughout their settlement journey, newcomers often require reliable information regarding social services and institutions. This includes schools or universities, medical clinics, and SPOs to facilitate their settlement. Given the prevalence of general-purpose tools and their relative anthropomorphization compared to search engines, it is likely that a variety of users rely on information that they receive from LLMs. A well-documented pitfall of generative AI tools such as ChatGPT is their ability to produce seemingly trustworthy outputs with little factual merit. These hallucinations diminish the reliability of general-purpose tools and create a potential for misinformation. Newcomers can be specifically vulnerable to this type of harm due to unfamiliarity with the new environment.\nMethod: Examples of hallucinated outputs were found throughout our testing process. For example, when asking about school options in Canadian cities, we asked for community-specific schools by asking about Islamic or Jewish schools. Similarly, we asked about community centers offering newcomer programs and specified an ethnic background to personalize the responses. In another case, we asked, \"What walk-in clinics are there in [City X]?\" In response to these example prompts, we often observed hallucinated results described below.\nResults: We found numerous instances of hallucinations when accessing general information. When asked about school options for newcomers' children, ChatGPT first gave accurate information regarding school boards and specific primary and secondary schools. However, when asked to provide options suited to specific communities, for example, Islamic or Jewish schools, it gave a mix of real and hallucinated options. This also occurred when ChatGPT was asked to give community-specific SPOs. The hallucinated outputs seem legitimate by name, and are even given a description by ChatGPT; however, the \u201cIslamic School of Ottawa-Carleton\" and the \"Arab Canadian Association of Ottawa\" were entirely hallucinated by ChatGPT. Even when the recommended institutions exist, ChatGPT often fails to provide accurate information about their location, address, and details about their operation. For instance, when asked about schools and SPOs in a specific city, ChatGPT would sometimes recommend institutions in a different city. Provided addresses of institutions were also often inaccurate. In one case, ChatGPT recommended the \"Greenbelt Family Health Team Walk-In Clinic,\" yet this clinic's title on Google is \"Greenbelt Family Health Team (NOT A WALK-IN CLINIC)\", as depicted in Figure 6.\nDiscussion: These results emphasize significant challenges in deploying generative Al models to provide settlement information to newcomers. While LLMs can efficiently process and deliver vast amounts of data, the observed hallucinations highlight a critical reliability issue, particularly when accuracy is essential for users. These inaccuracies and the creation of non-existent institutions could undermine trust in Al tools, which is crucial for their effective use. To mitigate these risks, it is essential to improve the accuracy and reliability of AI outputs by enhancing the training data with more robust, verified information and developing stricter validation processes to filter out inaccurate data before it reaches the user. Additionally, incorporating a feedback mechanism where users can report inaccuracies could help refine the model's responses and improve its utility over time."}, {"title": "3-6 Misinformation", "content": "Background: Upon arrival, newcomers may have difficulty accessing context-specific common knowledge. For example, when searching for a job, they need to gather information about wage rates, maximum hours they are allowed to work, and other relevant information. We consider a scenario where newcomers use ChatGPT to ask about minimum wage and maximum working hours. In Canada, the minimum wage is established by the government of the specific jurisdiction or by the federal government for federally regulated businesses. Minimum wage adjustments typically occur annually or semi-annually and often reflect changes in inflation and the Consumer Price Index. Overtime rates represent the minimum compensation required for salaried or hourly employees who work beyond a legally defined threshold of hours per week (or, in some places, per day). Depending on the jurisdiction, certain industries may have exemptions from these regulations. In most cases, the maximum hours of work allowable in a week is 48.\nMethod: We ask ChatGPT, What are the minimum wage and maximum working hours in Ontario?\" We repeat the question 10 times and compare the results with the information published by the Government of Ontario.\u2074\nResults: For seven of the ten tests, ChatGPT returns the value of $14.25 as minimum wage, which was Ontario's minimum wage from October 1, 2021, to"}, {"title": "3-7 Multimodal Malicious Use", "content": "Background: While previous scenarios focused on potential harms that arise from the use of generative models by newcomers, tools like ChatGPT might also be used against newcomers. We consider a scenario where a malicious user takes advantage of ChatGPT to target newcomers with scams or fraud. Newcomers to Canada may not be used to how governments, companies, and other institutions operate. As such, they are prone to being disproportionately targeted by malicious users who take advantage of their status and lack of familiarity. Moreover, technological advances often facilitate cheaper and more effective malicious uses due to a lack of regulation or safeguards. This is especially true in the case of generative AI tools which are becoming increasingly proficient at producing multimodal outputs (ex. text, image, audio, video,...). These capabilities can be used to make calls, texts, emails, and advertisements that seem credible and legitimate, especially if users lack the resources and literacy to differentiate between real and malicious AI-enabled inquiries."}, {"title": "5- Conclusion", "content": "Although AI has been used and studied in immigration, this use has predominantly been in border security and screening processes, which often raised ethical concerns [47, 48]. To the best of our knowledge, this is one of the first attempts to explore the potential and limitations of AI in the settlement sector. We highlighted how new immigrants and refugees might become overly dependent on and vulnerable to the extensive use of generic chatbots and raised awareness about the challenges and implications of over-reliance on such technologies. Our experiments included examples of performance discrepancies across Canada's official languages, biases in employment-related suggestions, performance discrepancies across languages in accessing health information, as well as examples of hallucinations, misinformation, stereotypical representations, and potential misuse.\nOur work serves as a call to action for improving these technologies to support the successful integration of newcomers into their new societies. For such technologies to be reliable in settlement services, they require careful alignment and customization to address the potential harms of generative models. We provided recommendations and encouraged further research on developing Al literacy programs and designing aligned LLMs for the newcomer community in Canada. This also includes establishing guidelines for fairness in AI, promoting transparency in AI operations, and fostering an environment where users from all backgrounds can trust and benefit equally from Al technologies. This work will be inherently multidisciplinary, participatory, and human-centred, involving collaboration between policymakers, technologists, stakeholders, and settlement service providers. It is crucial to note that settlement service providers must be at the center of the design and adoption of such technologies. This approach ensures that newcomers' unique needs and contexts are accurately addressed through human expert oversight and within the established and accountable structures."}]}