{"title": "BiasGuard: Guardrailing Fairness in Machine Learning Production Systems", "authors": ["Nurit Cohen-Inger", "Seffi Cohen", "Neomi Rabaev", "Lior Rokach", "Bracha Shapira"], "abstract": "As machine learning (ML) systems increasingly impact critical sectors such as hiring, financial risk assessments, and criminal justice, the imperative to ensure fairness has intensified due to potential negative implications. While much ML fairness research has focused on enhancing training data and processes, addressing the outputs of already deployed systems has received less attention. This paper introduces 'BiasGuard', a novel approach designed to act as a fairness guardrail in production ML systems. BiasGuard leverages Test-Time Augmentation (TTA) powered by Conditional Generative Adversarial Network (CTGAN), a cutting-edge generative AI model, to synthesize data samples conditioned on inverted protected attribute values, thereby promoting equitable outcomes across diverse groups. This method aims to provide equal opportunities for both privileged and unprivileged groups while significantly enhancing the fairness metrics of deployed systems without the need for retraining. Our comprehensive experimental analysis across diverse datasets reveals that BiasGuard enhances fairness by 31% while only reducing accuracy by 0.09% compared to non-mitigated benchmarks. Additionally, BiasGuard outperforms existing post-processing methods in improving fairness, positioning it as an effective tool to safeguard against biases when retraining the model is impractical.", "sections": [{"title": "1 Introduction", "content": "In the evolving landscape of machine learning applications, the imperative to ensure fairness has intensified, particularly given their substantial impact on society [1]. The pursuit of fairness aims to achieve equitable outcomes for all individuals and groups, a goal often compromised by inherent biases that manifest themselves as disparate impacts based on protected attributes such as race or gender [2]. These biases risk perpetuating discrimination in essential areas including employment, legal sentencing, and educational admissions [3], highlighting the urgent need for effective interventions to address these disparities and promote fairness [4]. Protected attributes are characteristics that are legally or socially protected from discrimination, such as race, gender, age, religion, sexual orientation, disability status, and national origin. Therefore, it is essential that machine learning models make decisions that are not only transparent but also consistently fair, ensuring that both privileged and marginalized groups are safeguarded from biased outcomes.\nTraditional bias mitigation strategies include pre-processing, in-processing, and post-processing techniques [5]. However, in many real-world applications, especially those involving pre-trained Software as a Service (SaaS) models"}, {"title": "2 Background and Related Work", "content": "This section covers the background and relevant work to our research, focusing on detecting and mitigating bias in ML systems, with particular emphasis on post-processing mitigation methods that are relevant for production scenarios. Additionally, since our method employs Test-Time-Augmentation using the CTGAN technique to create synthetic test data, we will review this background as well."}, {"title": "2.1 Bias Detection Metrics", "content": "Canton et al.'s survey [1] identifies two primary categories of bias metrics: those based on outcome probabilities, such as Statistical Parity [8] and Disparate Impact [9], and those derived from the confusion matrix, such as Equalized Odds and Opportunity [10], and Accuracy Rate Difference [11]. A single scenario can be evaluated differently on various metrics, some considered fair and others unfair [12].\nOur selection of the Equalized Odds metric, as the leading metric, was guided by its ability to comprehensively evaluate correct and incorrect classifications between groups, ensuring comparable True Positive Rates (TPR) and False Positive Rates (FPR). This approach provides a more nuanced examination of fairness compared to metrics that focus on a single type of error. It avoids the pitfalls of statistical metrics like Disparate Impact, which only considers outcome proportions and could mislead by promoting uniform denials [10]. Additionally, it steers clear of the over correction risks associated with Disparate Impact, which might favor underrepresented groups without genuinely addressing fairness [13]. Confusion-matrix based metrics, especially suitable for post-processing, do not require manipulation of the training dataset and account for 'real-life' differences between groups. Within this category, Equalized Odds offers a broader perspective by addressing both false positives and false negatives, directly tackling the key issues of biases that may occur."}, {"title": "2.2 Bias Mitigation Methods", "content": "The field of bias mitigation has seen significant research growth, with approximately 350 related studies [5] categorized into pre-processing, in-processing, and post-processing approaches:\nPre-processing methods aim to eliminate bias from the training data by transforming the protected attribute or balancing groups [9, 14, 15], learning fair representation [16], or changing protected attribute distribution [17]. FairUS method [18] recently used CTGAN technique to upsample the training set.\nIn-processing methods incorporate fairness constraints during training. A relevant technique uses GAN technology to debias the model's outcome [19]. However, our approach differs as we employ GANs to generate synthetic data for TTA, not during model training."}, {"title": "2.3 Test-Time Augmentation", "content": "Test-time augmentation is an enhancement method applied during the testing phase instead of training. This process, which generates multiple augmented versions of each test sample and then amalgamates these outcomes with those of the unaltered sample, is particularly beneficial in scenarios where model retraining or modification is impractical. The adoption of TTA has been notable in fields such as computer vision, natural language processing [25, 26], and anomaly detection [27, 28], demonstrating its versatility and effectiveness in improving model performance without structural changes."}, {"title": "2.3.1 GAN-based Augmentation for Tabular Data", "content": "The Conditional Generative Adversarial Network (CTGAN) introduced by Xu et al. [29] represents a significant advancement in generating synthetic tabular data, adept at mimicking complex distributions. CTGAN, as an advanced generative Al technique, plays a pivotal role in BiasGuard, synthesizing data that preserves original feature distributions while conditioning on inverse protected attributes to address fairness, thus facilitating a nuanced adjustment of biases at test time, ensuring the model remains a black box while evaluated under fairer conditions."}, {"title": "3 Method", "content": "BiasGuard provides a systematic approach to fair classification in production, by leveraging augmented test (inference) samples with opposite protected attributes (e.g., \"male\" replaced by \"female\"). This method averages the probabilities of several samples, regardless of the protected value (privileged and unprivileged), to achieve fairer predictions. BiasGuard is designed to actively monitor and adjust the fairness of predictions in production, ensuring ongoing compliance with fairness standards. Figure 2 illustrates the BiasGuard method applied during the test phase. For each instance in the test set, BiasGuard generates a balanced subset of augmentations with the inverse protected attribute value, then classifies both the augmentations and the original instance using a black-box model."}, {"title": "3.1 Formulation", "content": "Consider a test set $X_{test} \\in \\mathbb{R}^{m \\times n}$, where m denotes the number of instances and n the number of attributes. Let $y \\in [0, 1]^m$ represent the label vector for $X_{test}$. We define $M : \\mathbb{R}^{m \\times n} \\rightarrow [0,1]^m$ as a pre-trained black-box binary classifier that generates classification probabilities $\\hat{y} \\in [0, 1]^m$ for the test set $X_{test}$.\nOur focus is on enhancing fairness in binary classification models across a range of protected attributes. For instance, in tasks such as predicting criminal recidivism, let $y \\in Y = \\{0, 1\\}$ be the target label, and $PA \\in \\{0, 1\\}$ the protected"}, {"title": "3.2 Why BiasGuard Improves Fairness", "content": "Consider two nearly identical individuals who differ only by their protected attribute (Privileged vs. Unprivileged). If the predicted outcome changes from favorable (e.g., \"selected\") for the Privileged individual to unfavorable (e.g., \"not selected\") for the Unprivileged one, this suggests potential bias because the difference in the decision is driven solely by the protected attribute rather than relevant competencies or other features."}, {"title": "3.3 Time Complexity", "content": "The computational efficiency of BiasGuard is a critical factor for real-world deployment, particularly in production settings with strict latency requirements. BiasGuard's complexity arises mainly from synthetic data generation and subsequent prediction on augmented samples. First, for each test instance, BiasGuard uses the CTGAN generator (with L layers and input dimensionality D) to produce T synthetic augmentations; each synthetic instance generation costs $O(L \\cdot D^2)$. Thus, for m instances, total synthetic data generation takes $O(mT \\cdot L \\cdot D^2)$. Although this overhead surpasses that of simpler baselines, it can be mitigated using GPUs or lightweight generative models. Next, predictions for the original and augmented samples require $O(mT)$ time since each instance and its synthetic variants are fed into the black-box model, and their outputs are aggregated (e.g., via weighted averaging). Sensitivity analyses show that reducing T decreases latency while still achieving substantial fairness gains, making BiasGuard amenable to large-scale, real-time applications."}, {"title": "4 Experiments", "content": "The experiments section evaluates the BiasGuard method's performance in minimizing Equalized Odds while main-taining a satisfactory level of accuracy. This section describes the experimental setup, including methods, classifier, configurations, and datasets used. We conducted five experiments to assess BiasGuard's ability to minimize Equalized Odds with only a minor effect on the model's performance (accuracy). Each experiment yields 7 results, which include the measurements of Equalized Odds and accuracy for three compared baselines and our method in several configurations."}, {"title": "4.1 Evaluation Metrics", "content": "For our method evaluation, we utilize two primary categories of fairness metrics: confusion-matrix-based measures and outcome-based measures, along with accuracy as follows:\n\u2022 Equalized Odds (EOD). As discussed in the background section, EOD [10] belongs to the confusion-matrix category, capturing discrepancies in both false positive and true positive rates. Let PA be the protected attribute with values (Privileged, Unprivileged). Then:\n$EOD = \\frac{1}{2}(\\left|TPR_{PA=Privileged} - TPR_{PA=Unprivileged}\\right| + \\left|FPR_{PA=Privileged} \u2013 FPR_{PA=Unprivileged}\\right|)$.\nBy emphasizing both TPR and FPR, EOD offers a nuanced view of bias that goes beyond assessing only one type of error."}, {"title": "\u2022 Disparate Impact (DI)", "content": "In contrast, Disparate Impact falls under the outcome-based (or \"statistical\") category, as discussed in the background section [1]. It measures the ratio of positive decisions received by unprivileged and privileged groups:\n$DI = \\frac{Pr(\\hat{Y} = 1 \\vert PA = Unprivileged)}{Pr(\\hat{Y} = 1 \\vert PA = Privileged)}$\nA value of DI close to 1 indicates similar treatment across the two groups, whereas values significantly different from 1 may indicate disparate treatment.\n\u2022 Accuracy. Since bias mitigation can sometimes reduce predictive performance, we also track the model's overall accuracy. This provides insight into how fairness interventions interact with predictive effectiveness.\nTo ensure the stability of our results, we compute the standard deviation (STD) for each metric. By analyzing EOD, DI, and accuracy side by side, we capture both the effectiveness of our BiasGuard method in mitigating bias and its impact on model performance."}, {"title": "4.2 Compared Methods", "content": "The experiments are performed on five benchmark datasets, using the proposed BiasGuard method with different number of augmentations (named BiasGuard-2, BiasGuard-4, BiasGuard-6 and BiasGuard-8) against three compared methods, namely:\n(1) Baseline - The original model without any mitigation technique as a baseline.\n(2) Reject Option - The original model with the Reject Option post-processing method.\n(3) Threshold Opt - The original model with Threshold Optimizer post-processing method.\nWe selected the Reject Option and the Threshold Optimizer methods for our baseline comparison due to their well-established foundation in fairness literature and proven state-of-the-art performance in recent studies, such as [30]. Their suitability for black-box models and alignment with our focus on post-processing methods that change the predictions where uncertainty exists, make them ideal benchmarks to assess the incremental benefits of our BiasGuard method in enhancing fairness without compromising accuracy."}, {"title": "4.3 Implementation Details", "content": "To demonstrate BiasGuard we employed a Random Forest model from Sklearn [31] library, which aimed to evaluate the impact of the mitigation on fairness and accuracy metrics. Hort et al. [5] show that Random Forest is the common classifier used in bias mitigation studies. It should be noted that our method is model agnostic and we used the model for empirical proof of our concept.\nWe used the \"Synthetic Data Vault Project\" (SDV) CTGAN implementation\u00b9 with the following tuned hyper-parameters: 500 epochs, embedding_dim set to 32, generator_dim and discriminator_dim set to (256,128,64,32), discriminator and generator learning rate set to 5e-6."}, {"title": "4.4 Datasets", "content": "We selected a variety of datasets [32] from multiple domains (legal, medical, recruitment, and census) to showcase BiasGuard's adaptability and efficacy across different protected attributes. Each dataset poses distinct fairness challenges, involving attributes such as sex, age, and race, illustrating the broad applicability of BiasGuard. By evaluating our"}, {"title": "5 Results", "content": "The performance of the BiasGuard method was evaluated across several metrics, including EOD, accuracy, and DI, and compared against a baseline and two related work methods: Threshold Optimizer and Reject Option. Table 1 summarizes the results of these experiments."}, {"title": "5.1 Equalized Odds (EOD)", "content": "BiasGuard demonstrated a superior performance in terms of EOD in 5 out of the 5 experiments when compared to the baseline, in at least one of the configurations of BiasGuard. In 4 of the experiments, BiasGuard outperformed both the baseline and the related work methods. On average, BiasGuard improved EOD by 31% compared to the baseline, highlighting its effectiveness in reducing disparity in outcomes across different groups."}, {"title": "5.2 Accuracy", "content": "BiasGuard's accuracy was slightly lower than the baseline, with a small average drop of only 0.09% across experiments. However, in 3 experiments, BiasGuard's accuracy outperformed both the baseline and the related work methods. This suggests that while BiasGuard prioritizes fairness, the trade-off in accuracy is minimal and less than related work methods."}, {"title": "5.3 Disparate Impact (DI)", "content": "BiasGuard also demonstrated improvements in DI in 3 experiments out of 5, when compared to both the baseline and related work methods. This suggests that BiasGuard is effective not only in enhancing fairness (EOD) but also in reducing disparities in treatment between groups, further supporting its suitability for bias mitigation."}, {"title": "5.4 Comparison to Threshold Optimizer and Reject Option", "content": "For both Threshold Optimizer and Reject Option, EOD performance worsened compared to the baseline, with average decreases of 16% and 10%, respectively. This deterioration in fairness was accompanied by accuracy drops of 0.63% for Threshold Optimizer and 0.21% for Reject Option, indicating that both methods may introduce a significant trade-off between fairness and accuracy.\nAs shown in Figure 3 that summarizes the results, BiasGuard outperforms the baseline in terms of fairness, with a minimal trade-off in accuracy, and demonstrates improvements in DI."}, {"title": "5.5 Sensitivity Analysis of BiasGuard Augmentation Levels", "content": "The number of augmentations significantly influences both the performance and operational efficiency of our BiasGuard method, especially in production environments where running time is critical. We conducted a sensitivity analysis over four augmentation levels (2, 4, 6, 8} to understand the trade-offs between accuracy and fairness (measured via Equalized Odds). The results in Table 1 reveal that, in some datasets, different augmentation levels yield nearly identical outcomes when they involve the same \"critical flips\". In other cases, one augmentation setting achieves slightly better fairness and accuracy trade-offs, highlighting that the optimal number of augmentations varies by dataset and scenario. This choice also affects processing time, a key factor in real-time applications. Consequently, our method provides flexibility to configure the number of augmentations, enabling practitioners to choose the setting that best suits their specific constraints and objectives."}, {"title": "5.6 Time Complexity Analysis", "content": "Table 2 presents the running time in seconds for each of the experiments, comparing all the methods.\nWe compared BiasGuard's inference time against several baselines over 5-fold evaluation and different augmentation configurations using CTGAN-based synthetic data. Our findings indicate an approximate 11-31 times increase in inference time compared to no-test-time-augmentation (TTA) or simpler baselines (Threshold Optimizer, Reject Option). However, this overhead remains practically feasible, particularly for large datasets. For instance, with the ADULT"}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Trade-Off Between Accuracy and Fairness", "content": "In our experiments, we observed a trade-off between accuracy and fairness, a common challenge in implementing post-processing bias mitigation methods. BiasGuard achieved a 31% improvement in fairness metrics, as measured by reductions in EOD, at the cost of a minimal 0.09% decrease in accuracy. This trade-off reflects inherent tensions in ML models between enhancing fairness and maintaining high predictive performance, discussed thoroughly in literature [37, 38], demonstrating that trade-offs are typically present across various bias mitigation strategies, especially post-processing ones. When comparing the trade-offs to the related work methods, such as Threshold Optimizer and Reject Option, we observe a more pronounced drop in accuracy. Threshold Optimizer and Reject Option showed accuracy decreases of 0.63% and 0.21%, respectively. These methods not only exhibit worsened fairness but also incur larger accuracy drops compared to BiasGuard, highlighting the relatively better balance achieved by BiasGuard in maintaining predictive performance.\nFor some scenarios, the trade-off observed with BiasGuard is justified by its minimal drop in accuracy, making it a more viable choice in settings where slight sacrifices in accuracy are acceptable in favor of fairness improvements."}, {"title": "6.2 Flips Count", "content": "In four out of five experiments, BiasGuard produced the fewest flips relative to other post-processing methods, yet still achieved the best EOD performance. This indicates that BiasGuard efficiently targets only those instances requiring adjustment, rather than broadly modifying predictions. Consequently, BiasGuard appears to be a more stable and reliable approach that introduces less noise while preserving most of the model's original decisions. Its ability to minimize flips without sacrificing fairness further highlights its efficiency in bias mitigation."}, {"title": "6.3 Robustness and Agnosticism of BiasGuard", "content": "Our experiments have demonstrated BiasGuard's compatibility with a variety of scenarios, datasets, and protected attributes, indicating its broad applicability across different domains. Additionally, BiasGuard operates on the predictions provided by a trained classifier, rather than modifying the model's architecture or retraining it. As a result, BiasGuard is model-agnostic: it can be seamlessly integrated with any underlying ML algorithm, from logistic regression or decision trees to deep neural networks. By leveraging only the output of the classifier, BiasGuard effectively decouples bias mitigation from model design, offering a lightweight yet powerful approach to enforcing fairness."}, {"title": "6.4 Ethical Implications", "content": "BiasGuard's ethical considerations are critical to its practical application in real-world systems. The results demonstrate consistent reductions in Equalized Odds across all groups, confirming that BiasGuard effectively addresses complex bias patterns. Additionally, its performance remained stable in datasets with imbalanced class distributions, such as the ADULT dataset (for the race protected attribute), highlighting its adaptability to dynamic real-world data scenarios.\nBiasGuard's reliance on synthetic data generated by Conditional GANS (CTGAN) raises ethical considerations, particularly the risk of introducing unintended biases or over-correction. To address these concerns, we implemented rigorous monitoring of the synthetic data to ensure it accurately reflects the statistical properties of the original data, including diversity across protected and non-protected groups. This approach ensures fairness improvements for unprivileged groups without adversely affecting the outcomes for non-protected groups. Statistical analyses of prediction outcomes confirmed that BiasGuard achieves fairness without significant performance degradation or unintended disparities.\nAnother potential ethical challenge is over-correction, where fairness adjustments for unprivileged groups inadver-tently create disadvantageous outcomes for privileged groups. BiasGuard mitigates this by balancing original predictions with those derived from synthetic augmentations rather than replacing the original predictions outright. This balance preserves the integrity of predictions for all groups while improving fairness metrics. Moreover, BiasGuard presented the least amount of flips, in comparison to other post-processing methods. Furthermore, BiasGuard's model-agnostic nature makes it particularly suited for deployment in high-stakes applications, such as healthcare or criminal justice, where retraining the model is often infeasible. By functioning as a fairness guardrail during inference, BiasGuard enables ethical decision-making without altering the underlying model."}, {"title": "7 Conclusions", "content": "In this study, we introduced BiasGuard, an innovative method that enhances fairness in machine learning models through Test-Time Augmentation. Utilizing CTGAN to generate synthetic data with inverse protected attributes, BiasGuard addresses bias effectively without altering existing model architectures. Validated across four real-world datasets, it significantly reduces the Equalized Odds fairness metric while maintaining a reasonable accuracy trade-off.\nBiasGuard acts as a dynamic fairness monitor in production environments, allowing ongoing adjustments to fairness standards in situations where model retraining is impractical. This is crucial in sectors like healthcare and finance, where decisions have significant societal impacts. By integrating synthetic instances that represent opposite protected attribute values, BiasGuard mitigates biases effectively, maintaining the model's black box integrity.\nLimitations and Future Directions: BiasGuard introduces additional computational overhead during inference because it generates synthetic data to balance the predictions. However, this extra processing time is minimal on a"}, {"title": "8 Code Availability", "content": "The benchmark datasets and our method's reproducible source code are available at https://github.com/nuritci/BiasGuard"}]}