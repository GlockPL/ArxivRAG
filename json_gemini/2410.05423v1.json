{"title": "Incorporating Talker Identity Aids With Improving Speech Recognition in Adversarial Environments", "authors": ["Sagarika Alavilli", "Annesya Banerjee", "Gasser Elbanna", "Annika Magaro"], "abstract": "Current state-of-the-art speech recognition models are trained to map acoustic signals into sub-lexical units. While these models demonstrate superior performance, they remain vulnerable to out-of-distribution conditions such as background noise and speech augmentations. In this work, we hypothesize that incorporating speaker representations during speech recognition can enhance model robustness to noise. We developed a transformer-based model that jointly performs speech recognition and speaker identification. Our model utilizes speech embeddings from Whisper and speaker embeddings from ECAPA-TDNN, which are processed jointly to perform both tasks. We show that the joint model performs comparably to Whisper under clean conditions. Notably, the joint model outperforms Whisper in high-noise environments, such as with 8-speaker babble background noise. Furthermore, our joint model excels in handling highly augmented speech, including sine-wave and noise-vocoded speech. Overall, these results suggest that integrating voice representations with speech recognition can lead to more robust models under adversarial conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "Linguistic and acoustic information are essential elements of human speech. Although it is recognized that voice and talker characteristics influence speech perception [1], research has traditionally examined these aspects separately. This divide is also evident in automatic speech recognition (ASR) systems, which prioritize linguistic content and often overlook speaker-specific acoustic cues. However, emerging evidence shows that familiarity with talker-specific features can significantly improve speech comprehension and recognition [2]-[5].\nDeep learning-based ASR models are typically trained to convert acoustic signals into lexical (words) [6] or sub-lexical (characters or phonemes) [7] outputs. While they perform well in controlled settings, such as clear audio or reading speech [8], their accuracy drops significantly with minimally altered inputs, like sine-wave or noise-vocoded speech, or in noisy conditions [9]. We hypothesize that these vulnerabilities arise partly because the representations learned by these models are incentivized to be invariant to indexical (non-speech) information.\nIn this study, we evaluate the performance of Whisper [6], a state-of-the-art speech recognition model, under adversarial conditions by introducing background speaker babble and applying sine-wave and noise-vocoding transformations. We find that Whisper's performance deteriorates significantly even with slight adversarial manipulations. To address this, we propose a model trained on a dual task of speech and speaker recognition, encouraging the retention of both speech and non-speech information in its representations. By combining speech embeddings from the pre-trained Whisper model with speaker embeddings from the ECAPA-TDNN model [10], our \"joint\" model demonstrates improved robustness. Our experiments reveal the relevance of speaker representations for generalizable speech recognition."}, {"title": "II. METHODS", "content": "A. Candidate Models\nWhisper [6] is a supervised, transformer-based encoder-decoder model that takes mel spectrograms as an input with 30 sec long audio samples. This work used the huggingface pre-trained model openai/whisper-base with 74M parameters and was trained on 680K hours of transcribed audio. The penultimate layer is used to extract embeddings $e_w \\in R^{512}$.\nECAPA-TDNN is a state-of-the-art model for speaker recognition [10], pre-trained on 7205 speakers from the VoxCeleb 1 and 2 corpora [11], [12]. It processes MFCCs ($\\in R^{80}$) from audio windows of 25ms with a hop size of 10ms. The model uses stacked Squeeze-Excitation Res2Blocks, followed by a feature aggregation layer, and applies attentive statistical pooling. The pooled features are then fed into a linear layer and an AAM-softmax output layer, with the penultimate layer being $e_{ec} \\in R^{192}$ speaker embeddings.\nJoint Model. We developed a multi-layer transformer model combining ECAPA-TDNN and Whisper embeddings. The concatenated 704-dimensional embeddings are processed through multi-head transformers, producing embeddings of size H, where H is the number of feed-forward nodes. For speech recognition, these embeddings are passed through a linear layer with ReLU activation, mapped to class logits,"}, {"title": "B. Experiments", "content": "8 Speaker Babble Experiment. It has previously been es-tablished that speech recognition models are vulnerable to background noise [9]. However, it is possible that a model trained to represent voice along with speech might generalize better to testing in noise. By training a model on speech and speaker identification tasks, our model might be better at identifying a target voice among background noise.\nTo test this, we tested both Whisper models and our joint model on a series of speech clips embedded in 8-speaker babble. We varied the signal-to-noise ratio (SNR) from -15 to 20 dB, in steps of 5 dB, for 1000 background-foreground pairings. Every model was presented with all background-foreground pairs at every SNR. We calculated the character error rate (CER) as our model outputs a character (Equation 1).\n$CER = \\frac{substitutions + deletions + insertions}{n characters}$ (1)\nAugmented speech experiment. Humans are robust to differ-ent forms of augmented speech. Two well-known speech aug-mentations are sine wave speech and noise-vocoded speech. We tested the joint model and Whisper's performance on these two augmentations. Sine wave speech was generated by dividing a speech signal into four formants. The center frequencies of these formants were then found and were replaced with sine-wave modulated center frequencies (Fig. 2). To create noise-vocoded speech, a speech signal was filtered into several subbands or channels, and the amplitude envelopes of these channels were applied to noise. Then, the subbands were added together, resulting in a noise-vocoded signal. This type of speech processing simulates the processing that a cochlear implant does [14]. The noise-vocoded speech was generated with 1, 4, 16, and 64 channels (Fig. 2). We calculated the CER for Whisper and the joint model for each condition."}, {"title": "C. Datasets", "content": "Training Data. For training the joint model, we used the Common Voice (English) dataset [15]. The texts were lowercase, and punctuation was removed. The texts were further tokenized before passing on to the model. Following data preprocessing, the vocabulary size was 30 (English letters 'a' to 'z,' space, padding, the start and end of the sentence). We used utterances from 200 speakers, i.e., a 200-class classification task for speaker recognition. The 100 male and 100 female speakers with the highest frequency of occurrence in the dataset were selected. For speakers with more than 400 occurrences in the dataset, we randomly sampled 400 occurrences. This resulted in a set of speakers with between 300 and 400 occurrences in the dataset. This same dataset was used to train the Whisper (Character) model. For evaluation, we used stimuli from the test split of the Common Voice dataset.\n8 Speaker Babble Data. To test the impact of background noise on the models, we created a dataset of 1000 speech-babble pairs. Both the speech and 8-talker speech babble were randomly sampled from the test split of the Common Voice dataset. For each speech-babble pair, we combined them at dB SNRs from -15 to 20, increasing in 5 dB increments. We also tested models on clean speech at infinite SNR. All speech-babble pairs were generated at all SNRs to ensure that the acoustic difficulty of the signal only varied from SNR.\nAugmented speech. Speech augmentations were applied to a subset of the general evaluation dataset. For noise-vocoded speech, 1000 speech excerpts were randomly sampled from the evaluation dataset. These were transformed into noise-vocoded speech with 1, 4, 16, and 64 channels. For sine wave speech, 795 speech excerpts were randomly sampled from the evaluation dataset and were transformed into sine wave speech with four bands."}, {"title": "III. RESULTS", "content": "A. Speech and Speaker Recognition Performance\nWe first trained and evaluated our joint model performance on the speech and speaker recognition tasks. To do so, we trained multiple variants of the joint model, each with a slightly different architecture. Table I shows the joint model performance for our top three example architectures. Here, we varied the number of multi-head transformers used in the stack and the number of feed-forward nodes in each transformer layer. Note that we used 8 heads for each transformer in all cases. The joint model speaker recognition performance was consistently high across all the variants. The word error rate was also low but showed notable differences across architecture variants. Therefore, we chose the architecture"}, {"title": "B. 8 Speaker Babble Experiment", "content": "To analyze the impact of background noise on speech recognition, we plotted CER as a function of SNR for Whisper and our joint model (Fig. 3). We see that Whisper outperforms our model at infinite SNR. Both models perform comparably well at high SNRs. However, we see that Whisper begins dropping in performance at around 5 dB SNR much more rapidly than our model. At -10 dB SNR, our model has an average CER of 1, while Whisper's is at 3. This suggests that learning a joint representation of voice and speech can result in a model that is more robust to background noise."}, {"title": "C. Augmented Speech Experiment", "content": "We analyzed the joint model's speech recognition perfor-mance vs. Whisper on two forms of augmented speech: noise-vocoded speech and sine-wave speech. Both of these types of augmentations make the voice unrecognizable, but humans are generally more robust than models. We hypothesized that the joint model's more human-like representation would yield greater robustness to these conditions.\nWe first plotted the CER of the joint model and Whisper as a function of the number of channels in the noise-vocoded speech. We found that Whisper performed slightly better than the joint model for the 16-channel and 64-channel conditions of noise-vocoded speech. However, for the 1-channel and 4-channel noise-vocoded conditions, augmentations that deviate much further from normal speech, the joint model had much higher performance than Whisper, with Whisper's CER reach-ing almost 2. The joint model's CER stays below 1. Similarly, we see better generalization to sine-wave speech in the joint model (Fig. 4)."}, {"title": "IV. DISCUSSION AND CONCLUSION", "content": "In this work, we hypothesized that integrating speaker representations could improve speech recognition under ad-versarial conditions. We developed a transformer-based model combining speaker and speech embeddings to perform speech and speaker recognition. The joint model was tested against the pre-trained Whisper model, and our model's variant focused only on speech recognition. As shown in Figure 3, the joint model matches Whisper's performance at high signal-to-noise ratios (SNRs) but outperforms it at lower SNRs. This improve-ment is likely due to Whisper's focus on word prediction, which struggles with noisy inputs. To ensure a fair comparison, we also tested a variant of our model that generates characters without speaker recognition.\nResults showed that while the character-generating Whisper variant performed better than the original Whisper at low SNRs, both versions were still outperformed by the joint model trained on both speaker and speech recognition tasks. These findings suggest that incorporating speaker identity representations can be beneficial for speech recognition, par-ticularly in noisy conditions. The joint model might leverage voice representations to maintain speaker tracking, which may facilitate more accurate speech recognition even in adverse acoustic environments.\nFurthermore, we evaluated the models using various types of augmented speech to simulate degraded signal conditions. As shown in Figure 4, a similar performance pattern emerged, consistently demonstrating the superiority of the joint model when handling degraded signals. This further reinforces the potential advantage of incorporating speaker identity represen-tations for robust speech recognition in challenging acoustic environments.\nOur joint model, trained on both speaker and speech recognition tasks, outperformed Whisper in handling noisy and out-of-distribution speech. This work highlights the value of incorporating speaker information into speech recognition models. Rather than aiming for speaker invariance, leveraging speaker-specific representations can enhance robustness under challenging conditions like noise and signal degradation. In-tegrating speaker identity cues allows models to adapt better to acoustic variations, improving recognition accuracy and re-liability. This approach offers new possibilities for developing more resilient speech recognition systems."}]}