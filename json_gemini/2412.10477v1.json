{"title": "Benchmarking large language models for materials synthesis: the case of atomic layer deposition", "authors": ["Angel Yanguas-Gil", "Matthew T. Dearing", "Jeffrey W. Elam", "Jessica C. Jones", "Sungjoon Kim", "Adnan Mohammad", "Chi Thang Nguyen", "Bratin Sengupta"], "abstract": "In this work we introduce an open-ended question benchmark, ALDbench, to evaluate the performance of large language models (LLMs) in materials synthesis, and in particular in the field of atomic layer deposition, a thin film growth technique used in energy applications and microelectronics. Our benchmark comprises questions with a level of difficulty ranging from graduate level to domain expert current with the state of the art in the field. Human experts reviewed the questions along the criteria of difficulty and specificity, and the model responses along four different criteria: overall quality, specificity, relevance, and accuracy. We ran this benchmark on an instance of OpenAI's GPT-40. The responses from the model received a composite quality score of 3.7 on a 1 to 5 scale, consistent with a passing grade. However, 36% of the questions received at least one below average score. An in-depth analysis of the responses identified at least five instances of suspected hallucination. Finally, we observed statistically significant correlations between the difficulty of the question and the quality of the response, the difficulty of the question and the relevance of the response, and the specificity of the question and the accuracy of the response as graded by the human experts. This emphasizes the need to evaluate LLMs across multiple criteria beyond difficulty or accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "The past couple of years have seen a surge of interest in large language models (LLMs) both for both general purpose and scientific applications, including areas such as chemistry and materials science. Views on the usefulness of LLMS in chemistry and materials domains are somewhat mixed. On one hand, recent papers have framed LLMs as superhuman chemists\u00b9 or have posited that the future of chemistry is language.\u00b2 This is motivated by the growing number of works that have explored the use of LLMs for tasks such as materials property prediction, reaction optimization, or the design of novel materials3\u20135, as well as the performance of state of the art LLMs on scientific benchmarks such as ChemBench\u00b9.\nThis optimistic view is balanced by a more nuanced or skeptical approach towards LLMs, supported by their shortcomings on some of the same tasks. For instance, when analyzing the performance of LLMs in chemistry tasks comprising the ChemBench benchmark, the authors found that the models are still limited in their ability to address knowledge-intensive questions. Likewise, in the same work they questioned whether the model's performance in tasks involving molecular structures based on SMILES was consistent with any chemical reasoning ability\u00b9.\nOne of the areas still lacking relevant benchmarks is materials synthesis and, in particular, thin film growth. In this work, we introduce a new benchmark to evaluate LLM's knowledge and expertise in the field of atomic layer deposition (ALD), a gas phase thin film growth technique that is based on the self-limited surface reactions of gaseous precursors.\u2076 Beyond its applied interest in areas such as energy and microelectronics\u2077, ALD as a field brings together some key topics that are representative in chemistry-driven synthesis, such as heterogeneous reactions, metalorganic and inorganic molecules, materials microstructure, transport processes, and application-specific knowledge. Consequently, results on the LLM's proficiency in ALD can provide insights into the model capabilities in these areas. ALD is also a quantitative technique, with well-defined growths per cycle for many different processes. This makes it an interesting model system to explore LLM's ability to learn specific information about materials synthesis.\nIn this work we have the following two goals. First, we seek to develop a benchmark that can help us probe the capabilities of LLMs in the context of atomic layer deposition beyond traditional natural language processing or multiple choice question approaches. Here we have focused on an open-ended question format where both the questions and the model responses are graded by human domain experts.\nSecond, we seek to understand LLMs performance in the context of ALD, focusing primarily on knowledge and research assistance. The open answer question format allows us to select questions that are representative of the potential use of LLMs by any user. We also focused on questions whose answers require either significant domain expertise or would be expected from an expert in an adjacent field. This sets our approach apart from other benchmarks in the literature that are usually focused on general knowledge questions at the undergraduate and graduate levels."}, {"title": "II. METHODOLOGY", "content": "The benchmark was created using human-generated questions: six of the eight co-authors of this work contributed to the curation of the dataset. They are each domain experts in the area of ALD, and were asked to write \"questions that a researcher or a graduate student who is not familiar with a specific process/application would ask an AI assistant.\" Questions were written independently and collated to create an open answer dataset comprising 70 questions. Any modifications to these questions were only to ensure that each prompt asked a single question with a verifiable answer.\nWe ran this benchmark using an instance of OpenAI's GPT-40 large language model. For our implementation, we leveraged the internal generative AI interface available to researchers within Argonne National Laboratory, called Argo. Private copies of multiple LLMs are available through Argo, offering a data-secure and experimentally controllable environment for research-grade LLM interactions. This custom platform provides a standard API through which we pass the necessary parameters for the LLM inference process. Argo acts as a gateway to a selected LLM by making the appropriate inference call with our submitted content and returns the response from the LLM through the API.\nFor this experiment, no system prompt was provided, so the LLM responses were intentionally not guided by additional context or direction. A temperature value of 0.1 and a top_p of 0.9 were assigned for all LLM inference queries, allowing for a consistent configuration to ensure less hallucination and the model to respond with the most likely tokens. Finally, each query from the benchmark was submitted to the LLM independently of all others so that a continuous conversational thread with the LLM was not implemented during the experiment. This approach made certain that the generated answer from any query did not provide inappropriate context or direction for subsequent queries from our benchmark.\nSeven of the eight coauthors, all domain experts in the field, evaluated the quality of the open-ended questions by manually reviewing and scoring the answers provided by the LLM. These domain experts were requested to score the answers according to four criteria: overall quality, relevance, specificity, and accuracy, using a Likert scale of 1 to 5. In addition to grading the answers, the domain experts were asked to grade all questions in the dataset along two criteria: difficulty, and specificity of the required response, also using a 1 to 5 Likert scale. A rubric for each of the criteria is shown in Table I. The domain experts performed their reviews independently.\nComposite benchmark scores in each of the criteria were calculated considering the average score of each domain expert. This approach assigns equal weight to the experience of each domain expert with the LLM regardless of the number of responses reviewed. In addition to the statistical analysis of the responses, we subsequently carried out an in-depth analysis focused on hallucination as well as contextual and logical issues with specific answers."}, {"title": "III. RESULTS", "content": "Through the process described in Section II we compiled 70 open-ended questions. These are shown in Tables II-V. An a posteriori analysis of the questions lead us to group these across four categories: 1) \"how to grow\", where the query pertains to how to grow a specific material using ALD; 2) \"specific questions about ALD processes\", comprising more detailed questions looking for specific information about a process or materials, including growth per cycle, microstructure, or nucleation behavior; 3) \"general ALD knowledge\", where questions tend to be broader in nature, ranging from general properties of self-limited processes to questions related to simulations; and 4) \"applications\", a smaller subset of questions focused on properties or applications of ALD. The resulting set is not intended to be comprehensive in scope, but instead reflects some of the main areas of expertise of the co-authors that contributed questions to the dataset. It is worth mentioning that it also includes two questions focused on atomic layer etching.\nThe domain experts graded each of these questions according to two different criteria: difficulty and specificity of the question. A key differentiating factor of this benchmark with respect to other approaches is that hard questions require state-of-the-art knowledge available to only a few experts in the field and require a dedicated search to find them. This expands the range of questions in traditional LLM benchmark beyond the usual ceiling of graduate-level questions. In Figure 1 We show a color map with the difficulty score of each of the domain experts. Questions are presented in increasing order of average difficulty. The corresponding average is also shown in the Figure. The average difficulty for the dataset across all evaluations was 2.7, with the benchmark receiving 37 independent ratings (7.5%) in the top category. Figure 1 also shows a significant dispersion among domain experts. This highlights the breadth of scope that can be found even in highly focused benchmarks such as the one explored in this work, particularly as the difficulty and specificity of the questions increase.\nSimilarly, in Figure 2 we show the breakdown of the specificity scores by domain expert. The average specificity score is 3.0. Compared to difficulty, the specificity scores tend to be more clustered around intermediate values.\nBased on the dispersion in Figures 1 and 2, we explored how the average score differs across different domain experts. In Figures 3(A) and 3(B) we show the average scores together with errors bars indicating the standard deviation across all the benchmark questions. Despite individual differences observed in Figures 1 and 2, the aggregated values are similar across domain experts, with the average values of difficulty and specificity being close to each other. The differences in aggregate tend to be smaller than the standard deviation of the scores.\nOne motivation for exploring both dimensions is that difficulty and specificity do not have to be correlated: a question may be simple but require requires a very specific answer. In Figure 3(C), we show the correlation between the average difficulty and specificity of each of the questions in the benchmark. The Pearson correlation coefficient is 0.12, indicating a very low correlation between these two values."}, {"title": "B. Quantitative evaluation of GPT40 performance", "content": "The responses from the LLM were gathered and distributed to all domain experts. Experts were free to review a subset of questions. Through this process, we gathered 236 independent reviews.\nResponses were evaluated in terms of four criteria: overall quality, specificity of the answer, relevance of the answer, and accuracy. Our motivation to include specificity and relevance is that these are two criteria that are hard to gauge using autocompletion or multiple choice question benchmarks, and yet they can factor in on the perception of response quality. In particular, specificity refers to the LLM's ability to provide specific responses to a question vs more generic or discursive answers. With the criterion of relevance, we are addressing the model's ability to pick the most relevant example or response among many (i.e. the most commonly used ALD process for a specific material).\nIn Figure 4 we show the average scores of the responses generated by our GPT40 instance in each of these four criteria for all our domain experts. The error bars represent the standard deviation. In terms of overall quality, the responses from GPT40 received a composite score of 3.7, with the average score of each domain expert ranging from 3.2 to 4.1. With the 1-5 scale used in this work, this represents an above average grade. 34 scores, or 14% of the reviews, were below average (either 1 or 2). This resulted in 25 of the questions (36%) receiving at least one below average grade in terms of GPT40's overall quality of the response.\nThe aggregated average scores in specificity, relevance, and accuracy were 3.6, 3.7, and 3.9 respectively. The breakdown by domain expert of these scores are shown in Figures 4(B)-(D). Interestingly, GPT4o responses were graded higher in accuracy than in specificity or relevance. Agreement among the domain experts was greater for the specificity and accuracy criteria, while the relevance scores showed a variance similar to that of the average quality of the responses. 29 (41%), 28 (40%), and 18 (26%) of the questions received at least one below average grade in specificity, relevance, and accuracy, respectively.\nTo better understand how the specificity, relevance, and accuracy criteria correlate with the overall quality, we calculated the average of the domain expert scores for each of the questions. In Figure 5, we show the correlation between the average quality score of each response and their average specificity, relevance, and accuracy [Figures 5(A)-(C)]. We notice that accuracy metric consistently outperforms the quality score, while the relevance scores tend to align better with the overall quality. The corresponding Pearson correlation coefficients are 0.75 (specificity), 0.84 (relevance) and 0.83 (accuracy).\nFinally, we explored whether there was a correlation between the LLM response scores and the difficulty and specificity of each question as scored by each domain expert. We broke down all scores into two groups: one including above average grades (5 and 4) and at or below average grades (3-1). We then built 2x2 contingency tables to explore significant correlations between the scores of the responses and those of the benchmark questions, shown in the appendix. The resulting p-values, obtained from applying the Fisher exact test to each table, are summarized in Table VI.\nWe observed significant differences (p-values < 0.05) in three cases:\n1.  The overall quality of the answers correlated with the difficulty assigned to each question. That is, above average quality scores tend to go to questions rated as easier by the domain experts.\n2.  We observed significant differences in the relevance score of the responses depending on whether their difficulty score was above average or at or below average: questions rated at or below average in difficulty had a significantly large share of above average relevance scores.\n3.  We found a strong anti-correlation between the specificity of the question and the accuracy score of the responses. Responses highly rated in accuracy tend to come from questions rated lower in specificity. Differences here were the most significant, with a p-value lower than 0.01."}, {"title": "C. In-depth analysis of the LLM answers", "content": "In addition to the quantitative results shown in Section III B, we have also carried out an in-depth analysis of some of the LLM responses. Questions in the \"how to grow\" category (Table II) are particularly useful, because they request information about possible ALD synthesis approaches that can be easily validated against the literature and existing databases. Within these questions, we were able to identify a few examples of hallucination, where the LLM response proposed processes that, to our knowledge, have not been reported in the literature. One example pertains the ALD of MgF2. The LLM response mentions the following about candidates for fluorine sources for ALD (see Supporting Information for the full response): \u201cHydrogen fluoride-pyridine (HF-pyridine) complex, TiF4, or other fluorine-containing gases like NF3 or SF6 can be used. HF-pyridine is often preferred due to its reactivity and ability to form stable MgF2 films\". While the choice of HF-pyridine or TiF4 are factually correct9,10, we could not find any precedent of an ALD process for MgF2 utilizing either NF3 or SF6 as co-reactants (See for instance Ref11). Another example of a factually incorrect ALD process can be found on the LLM's response about ALD processes for C03O4. The response includes cobalt(II) nitrate as a precursor. Again, to our knowledge could not find references in the literature to such ALD process. In total, we were able to identify at least five instances of suspected hallucinations in the LLM responses.\nWe have also observed instances of the model struggling with chemical nomenclature: for instance it failed to recognize \"Bis(tert-butylimido) bis(dimethylamido) molybdenum (Mo(NtBu)2(NMe2)2)\" as an alkylamide precursor. In a separate instance, the response reverses the order of the ligands and the metal, such as \"(Cp)2Mg\". These are minor concerns compared to factual mistakes, but together these observations highlight the importance of developing benchmarks that prompt LLMs to generate outputs involving chemicals, and particularly metalorganic compounds.\nA second recurring pattern in the LLM responses involves too vague or too broad quantitative information. Examples include ranges instead of precise values for the growth per cycle of ZnO ALD from diethyl zinc and water or temperature conditions for an ALD process. These are all factually correct responses, but they fall short of the expected level of precision. These observations are consistent with the significant anticorrelation observed in Section III B between the accuracy scores of the responses and the specificity scores of the questions (more specific questions tend to receive lower accuracy scores). Finally, the responses tend to list precursors and co-reactants as if they are interchangeable. While it is true that many precursors may be reactive towards the same co-reactants (i.e. water and ozone in many oxide ALD processes), this is not generally the case. Also, growths per cycle may differ depending on the co-reactant used, for instance in the case of trimethyl aluminum / water vs trimethylaluminum / ozone12. This structuring of the information is observed in almost all responses involving descriptions of ALD processes (see Supporting Info). On the other hand, the LLM responses do a good overall job emphasizing the key features of self-limited processes, except for isolated instances where the model is asked to reason about the consequences of self-limiting reactions."}, {"title": "IV. DISCUSSION", "content": "Open-ended question benchmarks such as the one presented in this work are much harder to scale than conventional benchmarks used in automatic evaluation, as they usually require evaluation by domain experts. On the other hand, by exploring domain-specific open-ended questions, we were able to evaluate aspects of LLM's performance that cannot be evaluated using natural language processing tasks or multiple-choice questions. Through our benchmark we targeted evaluation criteria in the model responses such as relevance or specificity of the response. We were also able to gauge the model's propensity for hallucinating processes, and the ability to reason about the fundamental aspects of ALD.\nAn analysis of the scores of the benchmark questions lead us to two key conclusions: first, specificity and difficulty are largely uncorrelated. This highlights the need of considering other criteria beyond difficulty when designing a benchmark. Subsequent analysis of the GPT40 responses showed significant correlations between questions rated as above average in the difficulty and specificity criteria and scores of the model responses. Second, we observed high variability on the assessment of difficulty by different domain experts. We believe that this is due to the high degree of specialization in many scientific disciplines once one moves beyond general knowledge."}, {"title": "B. GPT4o as a baseline LLM for ALD and materials synthesis", "content": "We used GPT40 as our baseline LLM for our benchmark. This model did not have access to external search or data, so the responses reflect the model's intrinsic ability to generate text based on its training. On the one hand, the fact that the model received an aggregated passing score in all four evaluation criteria is certainly a technical achievement. On the other hand, between 15% and 35% of the questions received at least one below-average score in one of the evaluation criteria. The model also hallucinated some responses, particularly those involving chemical precursors. Some hallucinations are consistent with the model's inability to identify the proper context. For instance, the mention of NF3 and SF6 as possible fluorine sources for the ALD of metal fluorides makes sense given that SF6 plasmas have been used for ALD of AlF3 and LiF13,14. NF3 also appears in the literature in the context of etching of ALD films15. Thus, there is a real association between both compounds and ALD that gets misinterpreted during the response generation process. This highlights one of the challenges of existing LLMs, particularly those not specifically trained for scientific applications, when dealing with complex scientific problems. This is consistent with prior observations on chemistry benchmarks\u00b9.\nFinally, we must point out that there are strategies that can further improve the model's performance that we did not explore in this work. One approach is prompt engineering, wherein queries can be prefaced by text that conditions the answer the model provides (i.e. \"you are an awesome model and one of the leading experts in ALD in the world, please answer the following question to the best of your abilities:\"). A second method is through fine-tuning the hyperparameters used as inputs for the generative process. A third approach is augmenting LLMs with tools that provide external knowledge16. One final factor that we haven't considered is the stochasticity in text generation: the sequence of tokens is generated probabilistically, thus potentially leading to different responses to the same query in each interaction. We are currently in the process of generating a dataset to evaluate the prevalence and reproducibility of fabricated outputs using these strategies. The results will be presented in a future work."}, {"title": "V. CONCLUSIONS", "content": "In this work we introduced a new open-ended question benchmark for materials synthesis, ALDbench, specifically related to atomic layer deposition. Our benchmark complements other approaches such as MatSciML8, which are more geared towards natural language processing tasks. Instead, our benchmark focuses primarily on knowledge-based questions that experts would likely ask an Al expert. While this does not cover other potential uses of LLMs such as the design of new processes or the property prediction currently being pursued in the literature, we believe that a strong knowledge-based foundation is critical for the success of these more advanced applications. Consequently, benchmarks such as ALDbench will likely play an increasingly important role when evaluating the potential of LLMs for different scientific domains."}]}