{"title": "Improving Out-of-Distribution Detection by Combining Existing Post-hoc Methods", "authors": ["Paul Novello", "Yannick Prudent", "Joseba Dalmau", "Corentin Friedrich", "Yann Pequignot"], "abstract": "Since the seminal paper of Hendrycks et al. [1], Post-hoc deep Out-of-Distribution (OOD) detection has expanded rapidly. As a result, practitioners working on safety-critical applications and seeking to improve the robustness of a neural network now have a plethora of methods to choose from. However, no method outperforms every other on every dataset [2], so the current best practice is to test all the methods on the datasets at hand. This paper shifts focus from developing new methods to effectively combining existing ones to enhance OOD detection. We propose and compare four different strategies for integrating multiple detection scores into a unified OOD detector, based on techniques such as majority vote, empirical and copulas-based Cumulative Distribution Function modeling, and multivariate quantiles based on optimal transport. We extend common OOD evaluation metrics like AUROC and FPR at fixed TPR rates to these multi-dimensional OOD detectors, allowing us to evaluate them and compare them with individual methods on extensive benchmarks. Furthermore, we propose a series of guidelines to choose what OOD detectors to combine in more realistic settings, i.e. in the absence of known OOD data, relying on principles drawn from Outlier Exposure [3]. The code is available at https://github.com/paulnovello/multi-ood.", "sections": [{"title": "1 Introduction", "content": "Even though current Machine Learning (ML) and Deep Learning (DL) models are able to perform several complex tasks that previously only human beings could (e.g. image classification, natural language processing), we are still a step away from their widespread adoption, in particular in safety-critical applications. One of the main reasons for the slow adoption of ML in safety-critical applications is the difficulty to certify a ML component, mainly due to a poor control of the circumstances that may provoke such a ML component to fail. In order to prevent failures of a ML component, practitioners tend to avoid using the model on data that differs greatly from the data used to train the model. Which bears the obvious question, how do we distinguish between data that is similar to the training data, and data that is not? Out-of-Distribution (OOD) detection tries to answer this question, and besides being a very active branch in machine learning research, OOD detection has being recognized as an essential step in the certification of ML systems by multiple certification authorities (see e.g. Sections 5.3 and 8.4 of [4] or Section 5.1 of [5]).\nGiven the difficulty of training ML and in particular DL models, post-hoc OOD detection has emerged as the solution of choice for OOD detection for most practitionners. Yet, the plethora of different existing OOD detection methods to choose from can be daunting to practitionners unfamiliar with the literature. Several recent solutions have made a great effort in order to foster accessibility to the field, e.g. surveys categorizing the different problems and associated methods [6, 7] or libraries for benchmarking and comparing the existing methods [2, 8, 9]. Nevertheless, the choice of different methods is still vast, and it is rather unclear from the literature what method one should choose for a"}, {"title": "2 Related Works", "content": "Most works that introduce score combinations for OOD detection take a single score function (e.g., Mahalanobis [10] or DKNN [11]) and apply it across different layers of a neural network. This is the case of [12], where the aggregation is performed via simple operators like mean, max or min; [13] where the different scores are combined using a weighted average, the weights being functions of both the score of the test example and the scores of the training examples; or yet [14], where they either combine the scores using a simple function like the maximum function, or if they have access to a reference dataset they fit a linear regression on the reference dataset in order to aggregate the scores. More unusual ways of combining scores are given in [15], where a measure of the complexity of the test image is used in order to decide which layer to compute the score from; or [16], where aggregation is performed by training a random forest classifier. The work [17] is slightly different, since aggregation of the feature vectors is performed before computing the score. This can be done thanks to the transformer architecture of the underlying network, which ensures that feature vectors have the same dimension for different layers.\nSome works go further and combine scores obtained accross different layers of different neural networks. [18] build a score of the form: $s(x) = s_{cla}(x) + s_{rec}(x)$ by combining two scores for a same image: a score $s_{cla}(x)$ obtained from the latent space of a classifier, and a score $s_{rec}(x)$ obtained from the latent space of an autoencoder. In a similar vein, [19] build a score combining multiple Mahalanobis distances. The different distances are obtained from the feature spaces of the layers of a classifier network and a distance metric learning network. The scores are combined via a weighted sum, where the weights are fitted using a validation dataset that contains OOD samples. [20] approach OOD detection from a Bayesian perspective. Assuming there is an underlying classification task for ID data, they train both a classifier on the ID data and a binary discriminator on ID data and proxy OOD data (via e.g. outlier exposure). The classifier and the discriminator can either share the same latent representation or be trained independently. The paper derives multiple OOD score functions that combine the output of the classifier and the discriminator.\nPerhaps the closest work to our current approach is that of [21], where a series of hypothesis tests are written based on a different score each, then a text example is declared to be ID or OOD using a Benjamini-Hochberg type procedure (see [22]). They use conformal p-values in their hypothesis tests, thus obtaining a probabilistic guarantee. They combine both different score functions (Mahalanobis,"}, {"title": "3 Setting", "content": "Given a probability distribution $P_{id}$ on a space $\\mathcal{X}$ and a realization $x$ of a random variable $x$ in $\\mathcal{X}$, the task of Out-of-Distribution (OOD) detection consists in assessing if $x \\sim P_{id}$ ($x$ is considered In-Distribution (ID)) or not ($x$ is considered OOD). In most scenarios, the only information that one has about the distribution $P_{id}$ is a set $D_{id}$ of i.i.d. realizations drawn from the ID distribution. The most common procedure for OOD detection is to construct a score function $s : \\mathcal{X} \\rightarrow \\mathbb{R}$ and a threshold $\\tau \\in \\mathbb{R}$ such that:\n\\begin{equation}\n\\begin{cases}\nx \\text{ is declared OOD} & \\text{if } s(x) > \\tau, \\\\\nx \\text{ is declared ID} & \\text{if } s(x) \\le \\tau.\n\\end{cases}\n\\label{eq:ood_def}\n\\end{equation}\nWe call $s$ an OOD score function. Usually the set $D_{id}$ is split into a set $D_{fit}$, which is used to construct the score functions, and a set $D_{eval}$ that is used for evaluation purposes. Assume there are $p$ samples $\\{x^{(1)},...,x^{(p)}\\}$ in the test dataset $D_{eval}$. To evaluate the quality of $s$, we consider $p$ additional OOD samples $\\{x^{(1)},...,x^{(p)}\\}$ drawn from another distribution $P_{ood} \\neq P_{id}$ (typically, another dataset). We apply $s$ to obtain $\\{s(x^{(1)}),..., s(x^{(p)}), s(x^{(1)}),...,s(x^{(p)})\\}$. Then, we assess the discriminative power of $s$ by evaluating metrics depending on the threshold $\\tau$. By considering ID samples as negative and OOD as positive, we can will mainly be using the Area Under the Receiver Operating Characteristic (AUROC): we compute the False Positive Rate (FPR) and the True Positive Rate (TPR) for $\\tau_{i} = s(x^{(i)}), i \\in \\{1,...,p\\}$, and compute the area under the curve with FPR as x-axis and TPR as y-axis.\nIn this work, we will be considering several OOD scores at the same time. For $d$ different score functions $s_{1}, ..., s_{d}$, we denote by $s : \\mathcal{X} \\rightarrow \\mathbb{R}^{d}$ the multi-dimensional OOD score function obtained by setting $x \\rightarrow s(x) = (s_{1}(x), ..., s_{d}(x))$. In order to build a multi-dimensional OOD detector from the score $s$, we further split the set $D_{eval}$ in three disjoint subsets: $D^{cal}_{eval}$, for calibrating the multi-dimensional detector, $D^{val}_{eval}$, for comparing the different multi-dimensional detectors to each other and chosing the best ones, and $D^{test}_{eval}$, for final evaluation purposes."}, {"title": "4 OOD Scores Combination", "content": "The main motivation for our combining scores is the empirical observation that different OOD score functions capture different information. This is obvious from Figure 1, where the two-dimensional point clouds are seen to better separate ID from OOD data than the respective marginals."}, {"title": "4.1 Combination Methods", "content": "We propose here four different methods for combining different score functions into a single multi-dimensional OOD detector. Assume we are given $d$ OOD score functions $s_{1},...,s_{d}$, and the multi-dimensional OOD score $s$ is constructed as in section 3. A multi-dimensional OOD detector is defined by choosing a subset $A \\subset \\mathbb{R}^{d}$ and setting\n\\begin{equation}\n\\begin{cases}\nx \\text{ is declared OOD} & \\text{if } s(x) \\in A \\\\\nx \\text{ is declared ID} & \\text{if } s(x) \\notin A\n\\end{cases}\n\\label{eq:detector}\n\\end{equation}\nWe only consider multi-dimensional OOD detectors that are monotonous in the following sense: $1\\{ (s_{1},..., s_{d}) \\in A\\}$ is a monotonous function of $s_{k}$ for every $k$ when all other coordinates are kept fixed. Note that this is a natural constraint, since each individual score $s_{k}$ is itself an OOD score."}, {"title": "4.1.1 Majority Vote", "content": "The majority vote combination approach is the most naive one: the individual binary decisions $s_{i}(x) \\ge T_{i}$ (where $T_{i}$ is the threshold for the single OOD detector $i$) are aggregated through a simple majority voting scheme, where the final classification is determined by the most popular choice among the individual OOD detectors. In case where an equal number of detectors vote for ID and OOD (a possibility when the number of methods is even), we adopt a tie-breaking rule that favors OOD attribution. For example, if four OOD detectors assign two ID and two OOD predictions, the decision is OOD. Mathematically, it corresponds to choosing\n$\\begin{equation}\nA = \\{ a \\in \\mathbb{R}^d | \\sum_{i=1}^d 1\\{ s_{i} \\ge T_{i} \\} \\ge \\lfloor \\frac{d}{2} \\rfloor \\}\n\\label{eq:majority}\n\\end{equation}\nFor each OOD detector, a suitable threshold $T_{i}$ for binary classification must be determined. In practice, this threshold is typically computed by identifying the quantile that corresponds to a specific false positive rate on ID data, e.g. 5% FPR. Like for single OOD detection evaluation, performance metrics - including AUROC, FPR95TPR, and TPR5FPR - can be calculated for the majority vote approach by systematically varying the common quantile used to determine individual thresholds."}, {"title": "4.1.2 Empirical Cumulative Distribution Function", "content": "The next two methods are based on the estimation of the multi-dimensional Cumulative Distribution Function (CDF) of the considered scores. Given an ID random variable $x \\sim P_{id}$, its multi-dimensional score $s(x)$ is a random vector in $\\mathbb{R}^{d}$, we can define the CDF of $s(x)$ as being the function $F_{s(x)} : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ such that $F_{s(x)} (y) = P(s_{1}(x) \\le y_{1}, ..., s_{d}(x) \\le y_{d})$. The CDF is a natural way to combine scores because it can naturally take several scores as input and output a scalar. A first way to do so is to nonparametrically estimate $F_{s(x)}$ using the empirical CDF function:\n\\begin{equation}\nF_{s(x)} (y) = \\frac{1}{|D_{id}^{fit}|} \\sum_{x \\in D_{id}^{fit}} 1(s_{1}(x) \\le y_{1},..., s_{d}(x) \\le y_{d}).\n\\label{eq:ecdf}\n\\end{equation}\nIn order to build a multi-dimensional OOD detector we pick a threshold $\\tau$ in $(0, 1)$ and define\n\\begin{equation}\nA = \\{ y \\in \\mathbb{R}^{d} | F_{s(x)} (y) > \\tau \\}.\n\\label{eq:set_a}\n\\end{equation}"}, {"title": "4.1.3 Parametric Cumulative Distribution Function with Copulas", "content": "Another way of estimating $F_{s(x)}$ is by using a parametric estimator. Let us consider the marginal CDFs of $s(x)$, namely $F_{s_{1}(x)}, ..., F_{s_{d}(x)}$. Each $F_{s_{k}(x)}$ can be estimated with $F^{\\theta_{k}} : \\mathbb{R} \\rightarrow [0, 1]$ chosen from a family of a known probability distribution, e.g., Gaussian, Beta, etc., of parameter $\\theta_{k}$, the parameter $\\theta_{k}$ is inferred from the ID scores $D^{fit}_{eval}$. For each choice of the family of distributions, the parameter $\\theta_{k}$ is fitted on $\\{s_{k}(x): : x \\in D^{fit}_{id}\\}$ using Maximum Likelihood Estimation (MLE).\nHowever, as shown by the correlations between scores in the Figure 1, the scores might be dependent, so it is not enough to model the marginal CDFs $F^{\\theta_{k}}$; the dependence between the random variables $s_{k}(x)$ has to be taken into account. This can be achieved by considering copulas. Let consider a"}, {"title": "4.1.4 Center-Outward Quantiles", "content": "Center-outward quantiles, introduced in [23], generalize univariate quantiles to multivariate settings, extending distribution and quantile functions to higher dimensions. This concept, rooted in optimal transportation theory, addresses the challenge of the lack of a canonical ordering in multidimensional spaces. In the context of combining OOD scores, we use this approach to estimate quantiles for the multi-dimensional OOD score. A new sample is considered as ID for a fixed decision threshold $q$ if it can be matched to an ID quantile inferior to $q$; otherwise, the sample is considered as OOD. Here are the key steps of this approach:\nStep 1: Generating Points on Nested Hyperspheres To represent the source distribution for the optimal transport problem, we generate $N$ points uniformly on the surfaces of $k$ nested hyperspheres in $\\mathbb{R}^{d}$, intersected with the first quadrant $\\mathbb{R}_{+}^{d}$. Each point $z^{(i)}$ is sampled from one of the hyperspheres with radius $r_{t}, t = 1,..., k$, such that:\n\\begin{equation}\nz^{(i)} = r_{t_{i}}u^{(i)}, i = 1,..., N,\n\\label{eq:montecarlo}\n\\end{equation}\nwhere $u^{(i)} \\in \\mathbb{R}^{d}$ are points uniformly distributed on $\\mathbb{S}^{d-1} \\cap \\mathbb{R}_{+}^{d}$ ($\\mathbb{S}^{d-1}$ being the unit hypersphere in $\\mathbb{R}^{d}$), and $t_{i}$ are uniformly distributed over the set $\\{1, ..., k\\}$. The radius $r_{t}$ is scaled linearly such that $r_{t} = t/k$.\nStep 2: Optimal Transport Plan Assume that the set $D^{fit}_{id}$ is of size $M$ and denote by $s^{(1)},..., s^{(M)} \\in \\mathbb{R}^{d}$ their scores. Given the set of reference points $z^{(i)}$ on the hyperspheres we compute the optimal transport plan $P \\in \\mathbb{R}^{N \\times M}_{+}$ that minimizes the transportation cost:\n\\begin{equation}\nP = \\underset{P \\in \\Pi(a, b)}{arg \\hspace{1mm} min} \\sum_{i=1}^{N} \\sum_{j=1}^{M} P_{i,j} \\cdot c(z^{(i)}, s^{(j)}),\n\\label{eq:transport}\n\\end{equation}\nwhere $\\Pi(a, b)$ is the set of all transportation plans that satisfy the marginal constraints a and b (uniform distributions over $\\{1, ..., N\\}$ and $\\{1, ..., M\\}$, respectively), and $c(z^{(i)}, s^{(j)})$ is the cost function, typically the squared Euclidean distance $c(z, y) = ||z - y||^{2}$. Informally speaking, the meaning of $P$ is that a mass $P_{i,j}$ is transported from the point $z^{(i)}$ to the point $s^{(j)}$, given that each point $z^{(i)}$ in the initial distribution has a mass of $1/N$ and each point $s^{(j)}$ has a mass of $1/M$.\nStep 3: Estimating Quantiles The estimated quantile values $q$ for the in-distribution scores are derived from the transport plan $P$ and the radii of the hyperspheres. The quantile value for each score $s^{(j)}$ is given by:\n\\begin{equation}\nq_{j} = \\sum_{i=1}^{N} r_{t_{i}} P_{i,j},\n\\label{eq:quantile}\n\\end{equation}\nwhere $r_{t_{i}}$ corresponds to the radius of the hypersphere from which the point $z^{(i)}$ was sampled.\nStep 4: Constructing Quantile Contours To visualize and interpret the quantiles, we construct a convex hull for the points below a specific quantile threshold $Q$. The convex hull is defined as:\n\\begin{equation}\nConv(\\{s^{(j)} | q_{j} \\leq Q\\}),\n\\label{eq:convex}\n\\end{equation}\nwhere $Q$ is the desired quantile level."}, {"title": "4.2 OOD Metrics for multi-dimensional OOD detectors", "content": "It is not straightforward to generalize the commonly used metrics for OOD detection evaluation to the case of a multi-dimensional OOD score $s$. Indeed, the multi-dimensional detectors constructed above involve drawing a decision boundary (determined by the subset $A$) rather than simply using a threshold $\\tau \\in \\mathbb{R}$. In order to generalize the common metrics to the newly defined multi-dimensional OOD detectors, we choose to define a one-parameter family of sets $(A_{t})_{0<t\\leq 1}$ adapted to each method.\n*   For the majority vote, $A_{t}$ is obtained by picking thresholds $T_{i}$ such that the FPR associated to the individual score $s_{i}$ is equal to $t$.\n*   For the CDF based multi-dimensional detectors, $A_{t}$ can be defined by using iso-probability levels, e.g. for the empirical CDF $A_{t} = \\{ y \\in \\mathbb{R}^{d} | F_{s(x)} (y) > t\\}$.\n*   For the center-outward quantiles we take extended version of $A_{t} = \\mathbb{R}_{+}^{d} \\setminus Conv\\{s^{(j)} | q_{j} \\leq t\\}$"}, {"title": "4.3 Search Strategies", "content": "The purpose of this section is to choose the best score combinations for a given task. We are using 28 individual scores as candidates for being combined with each other. Since evaluating all the possible combinations is computationally infeasible, we propose a few more lightweight alternatives. The comparison between the different combinations is performed using both ID and OOD data. The ID data that we use is that of the set $D_{id}^{val}$ (see Section 3). Given an OOD dataset $D^{ood}_{val}$ we split it into two disjoint subsets, the set $D^{val}_{id}$ is used along with $D_{id}^{val}$ for model selection purposes, while the set $D^{test}_{id}$ is used along the set $D^{test}_{id}$ for final evaluation purposes. The test sets are never used for score fitting, calibration of model combination, or model selection."}, {"title": "4.3.1 Best Pairs", "content": "A first approach to finding promising sets of scores to combine is to consider only pairs of individual score functions and simply test all the combinations on the validation datasets. In our case, we consider 28 different univariate OOD scores, which involves testing 378 combinations. Each of the individual OOD scores is only computed once, and can then be used in all of the combinations it is involved in. As a result, once the individual OOD scores are computed, such a benchmark can be performed in a few hours with a modern CPU."}, {"title": "4.3.2 Sensitivity Analysis", "content": "The previous method only allows to select pairs of univariate OOD scores. This section provides a methodology for selecting sets of more than two OOD scores using basic linear Sensitivity Analysis. Specifically, we sample $N = 1000$ d-dimensional random binary vectors $\\{b_{1}, ..., b_{N}\\}$ \u2013 one dimension for each univariate OOD score \u2013, and create a set of OOD scores to combine out of each of these vectors (the score is included for 1's and not included for 0's). We observed that combining more than 4 OOD scores seemed not to improve the AUROC, or even to decrease it, so we only drew sets of 4 scores at most. We compute the AUROC of each of the $N$ multi-dimensional OOD detector and store the result in $\\{z_{1},..., z_{N}\\}$. The AUROC values are then used to build an output variable $y_{i} := 1(z_{i} > z_{p})$, where $z_{p}$ is the p-percentile of $\\{z_{1}, ..., z_{N}\\}$. Then, we perform a logistic regression of the pairs $\\{(b_{1}, y_{1}), ..., (b_{N}, y_{N})\\}$ and consider the coefficients of this regression as sensitivity indices. Note that using $y_{i} = 1(z_{i} > z_{p})$ rather than $z_{i}$ allows us to assess the importance of each univariate score in reaching the p-percentile."}, {"title": "4.3.3 Beam Search", "content": "Beam search, originally introduced in the context of speech recognition systems [25], is a heuristic algorithm that efficiently explores large search spaces by maintaining a fixed number of promising solutions at each step. This method strikes a balance between breadth and depth, enabling comprehensive exploration without exhaustive computation.\nIn our approach, we use beam search to identify optimal combinations of univariate OOD scores. The algorithm iteratively combines the best-performing scores, as evaluated on the calibration datasets, to form sets. At each iteration, we retain a specified number of top-performing combinations (beam width) and expand them by adding new scores, up to a predetermined maximum ensemble size (beam depth). This process allows us to systematically explore and identify effective OOD score combinations. For more details, refer to the pseudo-code of the algorithm in the Appendix B."}, {"title": "5 Experiments", "content": "In this section, we explore the performances of the combination methods and the search strategies on OpenOOD [2], an extensive benchmark for OOD detection. We study 28 individual OOD detectors listed in Appendix A, using the Post-Hoc OOD detection library Oodeel [9]. We test the methods on three ID datasets, CIFAR-10, CIFAR-100 and Imagenet-200, with a set of OOD benchmarks gathered under two categories: Near-OOD and Far-OOD. The models considered are ResNet18 for CIFAR and ResNet50 for Imagenet200, trained on ID datasets, downloaded from OpenOOD. Note that the OOD datasets included in these two categories are not the same for each ID dataset. More details on the benchmark are found in Appendix D. In addition, if there are any, we defer implementation details of the combination methods and the search strategies to Appendix B.\nWe consider two approaches corresponding to common practical settings. The first case is when, as a practitioner, one has to choose between a large number of available OOD detectors and test them on their own data. In this case, we assume that some OOD data is available to select the most appropriate detector. In the second scenario, we consider that no OOD data is available for selecting a set of OOD detectors to combine. In that case, we draw inspiration from Outlier Exposure principles [3] and select some external data as proxies for OOD data. We use data specifically curated as Outlier Exposure data by OpenOOD [2]. Details on these datasets are found in Appendix D."}, {"title": "5.1 Combination Methods and Search Strategies on OpenOOD", "content": "In this part, we present the results of each of the four combination meth-ods, with combination sets selected us-ing the different search strategies. For each set of univariate OOD detectors to combine, the multi-dimensional detector is fit on $D^{cal}_{id}$. The search strategies consider AUROCS for each multi-dimensional detector on $D^{val}_{id}$ and $D^{ood}_{val}$. Once the best set is se-"}, {"title": "5.2 Selecting Promising Sets Without OOD Data", "content": "In many real-world cases, the access to OOD data is limited. Outlier Exposure [3] has emerged to cope with that lack of OOD data. It is now a common practice to use Outlier Exposure (OE), i.e. selecting external datasets as proxies for OOD data, to tune OOD detectors [26, 27, 28, 29, 30, 31, 32]. OpenOOD benchmark actually cu-rates datasets labeled Outlier Expo-sure for each ID dataset, which is strictly disjoint from the correspond-ing test datasets. In this section, based on the intuition that combinations of detectors that are performant on Out-"}, {"title": "6 Limitations", "content": "While we believe that combining OOD scores is good way to boost the performance of individual OOD detectors, we acknowledge that the methods proposed have limitations: ID data availability. Our methods involve using a training set, a calibration set, a validation set and a test set, which might be prohibitive for applications in which data is scarce. OOD data availability. Picking the best combination of scores requires OOD data (proxy or not), while we believe that this might be a limitation in some context like Anomaly Detection where OOD data is rather scarce, we are also aware that using OOD data is a common practice for model selection and benchmarking purposes. Extra compute resources. We are aware that searching for the best combination of scores carries an extra computation effort compared to selecting the best individual scores. However, combinations of over four scores provide negligible improvement, and some OOD detector can be implemented jointly to avoid repeated forward passes. We thus believe that the extra compute power required is only a minor limitation of our method."}, {"title": "7 Conclusion", "content": "In this work, we investigated the combination of existing OOD detectors to improve OOD detection capabilities. We introduced four different combination methods based on majority vote, empirical CDF, parametric CDF with copulas or center outward. We also introduced several search strategies to choose which individual detectors to combine and showed that this whole methodology could significantly improve the state-of-the-art for CIFAR-10, CIFAR-100, and ImageNet-200 test cases as designed on the OpenOOD benchmark. The methodology exhibits several additional advantages. It is versatile because it can be applied to any reference metrics we use AUROC, but depending on the industrial application, it is possible to apply it to any other metric, e.g., FPR@TPR95 or TPR@FPR5. Finally, it will stay automatically up to date with state-of-the-art post-hoc OOD detection because its performance stems from the underlying individual OOD detectors."}, {"title": "B.1 Majority vote", "content": "Compared to other combination methods, the majority vote approach does not combine scores but rather combines binary decisions (ID or OOD). For each individual OOD detector, thresholds $T_{i}$ are used to make the decision.\nThere are several ways to combine multiple binary values and produce a single binary decision. We defined and experimented with four of these methods:\n*   ALL: An element is declared OOD if all individual OOD detectors classify it as OOD.\n*   ANY: An element is declared OOD if at least one individual OOD detector classifies it as OOD.\n*   LOOSE: An element is declared OOD if at least half of the individual OOD detectors classify it as OOD (with a tie-breaking rule in favor of OOD)."}, {"title": "B.2 Copulas", "content": "As stated in the main paper, the copula method involves choosing a parametric distribution for marginal distributions, and a copula function, which can be chosen among different types of parametric copulas.\nWe selected the marginal distribution among Gaussian, Beta, and Uniform distributions and found that choosing Uniform consistently outperformed other distributions. As for the Copulas, we considered 6 possible different functions parametrized by $\\theta$:\nClayton Copula (bivariate) The Clayton Copula function is defined as:\n$C(u_{1}, u_{2}) = (u_{1}^{-\\theta} + u_{2}^{-\\theta} - 1) ^{-1/\\theta}$\nwhere $u_{1}, u_{2} \\in [0, 1]^{2}$.\nFrank Copula (bivariate) The Clayton Copula function is defined as:\n$C(u_{1}, u_{2}) = -\\frac{1}{\\theta} log (1 + \\frac{(e^{-\\theta u_{1}} - 1) (e^{-\\theta u_{2}} - 1)}{e^{-\\theta} - 1})$\nwhere $u_{1}, u_{2} \\in [0, 1]^{2}$.\nGumbel Copula (bivariate) The Clayton Copula function is defined as:\n$\\theta_{n} = \\frac{1}{1 - \\hat{\\tau_{n}}}$\nwhere $u_{1}, u_{2} \\in [0, 1]^{2}$, and $\\hat{\\tau_{n}}$ is the Kendall-$\\{tau\\}$, which measures the association between pairs of data points. It is defined as:\n$\\tau = \\frac{C - D}{\\frac{1}{2}n(n - 1)}$\nwhere: - C is the number of concordant pairs. - D is the number of discordant pairs. - n is the number of data points.\nFor a given set of n pairs $(u^{(i)}_{1}, u^{(i)}_{2})$ where $i = 1, 2, ..., n$:\n*   Concordant Pair: A pair of observations $(u^{(i)}_{1}, u^{(i)}_{2})$ and $(u^{(j)}_{1}, u^{(j)}_{2})$ is concordant if the order of $u^{(i)}_{1}$ and $u^{(j)}_{1}$ is the same as the order of $u^{(i)}_{2}$ and $u^{(j)}_{2}$. That is, either both $u^{(i)}_{1} < u^{(j)}_{1}$ and $u^{(i)}_{2} < u^{(j)}_{2}$ or both $u^{(i)}_{1} > u^{(j)}_{1}$ and $u^{(i)}_{2} > u^{(j)}_{2}$.\n*   Discordant Pair: A pair of observations $(u^{(i)}_{1}, u^{(i)}_{2})$ and $(u^{(j)}_{1}, u^{(j)}_{2})$ is discordant if the order of $u^{(i)}_{1}$ and $u^{(j)}_{1}$ is different from the order of $u^{(i)}_{2}$ and $u^{(j)}_{2}$. That is, either $u^{(i)}_{1} < u^{(j)}_{1}$ and $u^{(i)}_{2} > u^{(j)}_{2}$ or $u^{(i)}_{1} > u^{(j)}_{1}$ and $u^{(i)}_{2} < u^{(j)}_{2}$."}, {"title": "B.3 Center Outward", "content": "B.3.1 Monotonicity property on convex hulls\nIn order to ensure monotonicity property as defined in 4.1", "follows": "For each dimension", "quantiles": "Convex hulls vs KNN\nUsing convex hulls for center-outward quantiles ensures generalization for new points by finding the smallest convex hull that contains them. However", "straightforward": "when estimating quantiles for in-distribution calibration scores $D^{cal"}, {"parameters": "n*   Scaling of data points: We rescale multidimensional score points in [0", "estimation": "For the computational reasons evoked in B.3.2", "title": "Improving Out-of-Distribution Detection by Combining Existing Post-hoc Methods", "authors": ["Paul Novello", "Yannick Prudent", "Joseba Dalmau", "Corentin Friedrich", "Yann Pequignot"], "abstract": "Since the seminal paper of Hendrycks et al. [1], Post-hoc deep Out-of-Distribution (OOD) detection has expanded rapidly. As a result, practitioners working on safety-critical applications and seeking to improve the robustness of a neural network now have a plethora of methods to choose from. However, no method outperforms every other on every dataset [2], so the current best practice is to test all the methods on the datasets at hand. This paper shifts focus from developing new methods to effectively combining existing ones to enhance OOD detection. We propose and compare four different strategies for integrating multiple detection scores into a unified OOD detector, based on techniques such as majority vote, empirical and copulas-based Cumulative Distribution Function modeling, and multivariate quantiles based on optimal transport. We extend common OOD evaluation metrics like AUROC and FPR at fixed TPR rates to these multi-dimensional OOD detectors, allowing us to evaluate them and compare them with individual methods on extensive benchmarks. Furthermore, we propose a series of guidelines to choose what OOD detectors to combine in more realistic settings, i.e. in the absence of known OOD data, relying on principles drawn from Outlier Exposure [3]. The code is available at https://github.com/paulnovello/multi-ood.", "sections": [{"title": "1 Introduction", "content": "Even though current Machine Learning (ML) and Deep Learning (DL) models are able to perform several complex tasks that previously only human beings could (e.g. image classification, natural language processing), we are still a step away from their widespread adoption, in particular in safety-critical applications. One of the main reasons for the slow adoption of ML in safety-critical applications is the difficulty to certify a ML component, mainly due to a poor control of the circumstances that may provoke such a ML component to fail. In order to prevent failures of a ML component, practitioners tend to avoid using the model on data that differs greatly from the data used to train the model. Which bears the obvious question, how do we distinguish between data that is similar to the training data, and data that is not? Out-of-Distribution (OOD) detection tries to answer this question, and besides being a very active branch in machine learning research, OOD detection has being recognized as an essential step in the certification of ML systems by multiple certification authorities (see e.g. Sections 5.3 and 8.4 of [4] or Section 5.1 of [5]).\nGiven the difficulty of training ML and in particular DL models, post-hoc OOD detection has emerged as the solution of choice for OOD detection for most practitionners. Yet, the plethora of different existing OOD detection methods to choose from can be daunting to practitionners unfamiliar with the literature. Several recent solutions have made a great effort in order to foster accessibility to the field, e.g. surveys categorizing the different problems and associated methods [6, 7] or libraries for benchmarking and comparing the existing methods [2, 8, 9]. Nevertheless, the choice of different methods is still vast, and it is rather unclear from the literature what method one should choose for a"}, {"title": "2 Related Works", "content": "Most works that introduce score combinations for OOD detection take a single score function (e.g., Mahalanobis [10] or DKNN [11]) and apply it across different layers of a neural network. This is the case of [12], where the aggregation is performed via simple operators like mean, max or min; [13] where the different scores are combined using a weighted average, the weights being functions of both the score of the test example and the scores of the training examples; or yet [14], where they either combine the scores using a simple function like the maximum function, or if they have access to a reference dataset they fit a linear regression on the reference dataset in order to aggregate the scores. More unusual ways of combining scores are given in [15], where a measure of the complexity of the test image is used in order to decide which layer to compute the score from; or [16], where aggregation is performed by training a random forest classifier. The work [17] is slightly different, since aggregation of the feature vectors is performed before computing the score. This can be done thanks to the transformer architecture of the underlying network, which ensures that feature vectors have the same dimension for different layers.\nSome works go further and combine scores obtained accross different layers of different neural networks. [18] build a score of the form: $s(x) = s_{cla}(x) + s_{rec}(x)$ by combining two scores for a same image: a score $s_{cla}(x)$ obtained from the latent space of a classifier, and a score $s_{rec}(x)$ obtained from the latent space of an autoencoder. In a similar vein, [19] build a score combining multiple Mahalanobis distances. The different distances are obtained from the feature spaces of the layers of a classifier network and a distance metric learning network. The scores are combined via a weighted sum, where the weights are fitted using a validation dataset that contains OOD samples. [20] approach OOD detection from a Bayesian perspective. Assuming there is an underlying classification task for ID data, they train both a classifier on the ID data and a binary discriminator on ID data and proxy OOD data (via e.g. outlier exposure). The classifier and the discriminator can either share the same latent representation or be trained independently. The paper derives multiple OOD score functions that combine the output of the classifier and the discriminator.\nPerhaps the closest work to our current approach is that of [21], where a series of hypothesis tests are written based on a different score each, then a text example is declared to be ID or OOD using a Benjamini-Hochberg type procedure (see [22]). They use conformal p-values in their hypothesis tests, thus obtaining a probabilistic guarantee. They combine both different score functions (Mahalanobis,"}, {"title": "3 Setting", "content": "Given a probability distribution $P_{id}$ on a space $\\mathcal{X}$ and a realization $x$ of a random variable $x$ in $\\mathcal{X}$, the task of Out-of-Distribution (OOD) detection consists in assessing if $x \\sim P_{id}$ ($x$ is considered In-Distribution (ID)) or not ($x$ is considered OOD). In most scenarios, the only information that one has about the distribution $P_{id}$ is a set $D_{id}$ of i.i.d. realizations drawn from the ID distribution. The most common procedure for OOD detection is to construct a score function $s : \\mathcal{X} \\rightarrow \\mathbb{R}$ and a threshold $\\tau \\in \\mathbb{R}$ such that:\n\\begin{equation}\n\\begin{cases}\nx \\text{ is declared OOD} & \\text{if } s(x) > \\tau, \\\\\nx \\text{ is declared ID} & \\text{if } s(x) \\le \\tau.\\\n\\end{cases}\n\\label{eq:ood_def}\n\\end{equation}\nWe call $s$ an OOD score function. Usually the set $D_{id}$ is split into a set $D_{fit}$, which is used to construct the score functions, and a set $D_{eval}$ that is used for evaluation purposes. Assume there are $p$ samples $\\{x^{(1)},...,x^{(p)}\\}$ in the test dataset $D_{eval}$. To evaluate the quality of $s$, we consider $p$ additional OOD samples $\\{x^{(1)},...,x^{(p)}\\}$ drawn from another distribution $P_{ood} \\neq P_{id}$ (typically, another dataset). We apply $s$ to obtain $\\{s(x^{(1)}),..., s(x^{(p)}), s(x^{(1)}),...,s(x^{(p)})\\}$. Then, we assess the discriminative power of $s$ by evaluating metrics depending on the threshold $\\tau$. By considering ID samples as negative and OOD as positive, we can will mainly be using the Area Under the Receiver Operating Characteristic (AUROC): we compute the False Positive Rate (FPR) and the True Positive Rate (TPR) for $\\tau_{i} = s(x^{(i)}), i \\in \\{1,...,p\\}$, and compute the area under the curve with FPR as x-axis and TPR as y-axis.\nIn this work, we will be considering several OOD scores at the same time. For $d$ different score functions $s_{1}, ..., s_{d}$, we denote by $s : \\mathcal{X} \\rightarrow \\mathbb{R}^{d}$ the multi-dimensional OOD score function obtained by setting $x \\rightarrow s(x) = (s_{1}(x), ..., s_{d}(x))$. In order to build a multi-dimensional OOD detector from the score $s$, we further split the set $D_{eval}$ in three disjoint subsets: $D^{cal}_{eval}$, for calibrating the multi-dimensional detector, $D^{val}_{eval}$, for comparing the different multi-dimensional detectors to each other and chosing the best ones, and $D^{test}_{eval}$, for final evaluation purposes."}, {"title": "4 OOD Scores Combination", "content": "The main motivation for our combining scores is the empirical observation that different OOD score functions capture different information. This is obvious from Figure 1, where the two-dimensional point clouds are seen to better separate ID from OOD data than the respective marginals."}, {"title": "4.1 Combination Methods", "content": "We propose here four different methods for combining different score functions into a single multi-dimensional OOD detector. Assume we are given $d$ OOD score functions $s_{1},...,s_{d}$, and the multi-dimensional OOD score $s$ is constructed as in section 3. A multi-dimensional OOD detector is defined by choosing a subset $A \\subset \\mathbb{R}^{d}$ and setting\n\\begin{equation}\n\\begin{cases}\nx \\text{ is declared OOD} & \\text{if } s(x) \\in A \\\\\nx \\text{ is declared ID} & \\text{if } s(x) \\notin A\n\\end{cases}\n\\label{eq:detector}\n\\end{equation}\nWe only consider multi-dimensional OOD detectors that are monotonous in the following sense: $1\\{ (s_{1},..., s_{d}) \\in A\\}$ is a monotonous function of $s_{k}$ for every $k$ when all other coordinates are kept fixed. Note that this is a natural constraint, since each individual score $s_{k}$ is itself an OOD score."}, {"title": "4.1.1 Majority Vote", "content": "The majority vote combination approach is the most naive one: the individual binary decisions $s_{i}(x) \\ge T_{i}$ (where $T_{i}$ is the threshold for the single OOD detector $i$) are aggregated through a simple majority voting scheme, where the final classification is determined by the most popular choice among the individual OOD detectors. In case where an equal number of detectors vote for ID and OOD (a possibility when the number of methods is even), we adopt a tie-breaking rule that favors OOD attribution. For example, if four OOD detectors assign two ID and two OOD predictions, the decision is OOD. Mathematically, it corresponds to choosing\n$\\begin{equation}\nA = \\{ a \\in \\mathbb{R}^d | \\sum_{i=1}^d 1\\{ s_{i} \\ge T_{i} \\} \\ge \\lfloor \\frac{d}{2} \\rfloor \\}\n\\label{eq:majority}\n\\end{equation}\nFor each OOD detector, a suitable threshold $T_{i}$ for binary classification must be determined. In practice, this threshold is typically computed by identifying the quantile that corresponds to a specific false positive rate on ID data, e.g. 5% FPR. Like for single OOD detection evaluation, performance metrics - including AUROC, FPR95TPR, and TPR5FPR - can be calculated for the majority vote approach by systematically varying the common quantile used to determine individual thresholds."}, {"title": "4.1.2 Empirical Cumulative Distribution Function", "content": "The next two methods are based on the estimation of the multi-dimensional Cumulative Distribution Function (CDF) of the considered scores. Given an ID random variable $x \\sim P_{id}$, its multi-dimensional score $s(x)$ is a random vector in $\\mathbb{R}^{d}$, we can define the CDF of $s(x)$ as being the function $F_{s(x)} : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ such that $F_{s(x)} (y) = P(s_{1}(x) \\le y_{1}, ..., s_{d}(x) \\le y_{d})$. The CDF is a natural way to combine scores because it can naturally take several scores as input and output a scalar. A first way to do so is to nonparametrically estimate $F_{s(x)}$ using the empirical CDF function:\n\\begin{equation}\nF_{s(x)} (y) = \\frac{1}{|D_{id}^{fit}|} \\sum_{x \\in D_{id}^{fit}} 1(s_{1}(x) \\le y_{1},..., s_{d}(x) \\le y_{d}).\n\\label{eq:ecdf}\n\\end{equation}\nIn order to build a multi-dimensional OOD detector we pick a threshold $\\tau$ in $(0, 1)$ and define\n\\begin{equation}\nA = \\{ y \\in \\mathbb{R}^{d} | F_{s(x)} (y) > \\tau \\}.\n\\label{eq:set_a}\n\\end{equation}"}, {"title": "4.1.3 Parametric Cumulative Distribution Function with Copulas", "content": "Another way of estimating $F_{s(x)}$ is by using a parametric estimator. Let us consider the marginal CDFs of $s(x)$, namely $F_{s_{1}(x)}, ..., F_{s_{d}(x)}$. Each $F_{s_{k}(x)}$ can be estimated with $F^{\\theta_{k}} : \\mathbb{R} \\rightarrow [0, 1]$ chosen from a family of a known probability distribution, e.g., Gaussian, Beta, etc., of parameter $\\theta_{k}$, the parameter $\\theta_{k}$ is inferred from the ID scores $D^{fit}_{eval}$. For each choice of the family of distributions, the parameter $\\theta_{k}$ is fitted on $\\{s_{k}(x): : x \\in D^{fit}_{id}\\}$ using Maximum Likelihood Estimation (MLE).\nHowever, as shown by the correlations between scores in the Figure 1, the scores might be dependent, so it is not enough to model the marginal CDFs $F^{\\theta_{k}}$; the dependence between the random variables $s_{k}(x)$ has to be taken into account. This can be achieved by considering copulas. Let consider a"}, {"title": "4.1.4 Center-Outward Quantiles", "content": "Center-outward quantiles, introduced in [23], generalize univariate quantiles to multivariate settings, extending distribution and quantile functions to higher dimensions. This concept, rooted in optimal transportation theory, addresses the challenge of the lack of a canonical ordering in multidimensional spaces. In the context of combining OOD scores, we use this approach to estimate quantiles for the multi-dimensional OOD score. A new sample is considered as ID for a fixed decision threshold $q$ if it can be matched to an ID quantile inferior to $q$; otherwise, the sample is considered as OOD. Here are the key steps of this approach:\nStep 1: Generating Points on Nested Hyperspheres To represent the source distribution for the optimal transport problem, we generate $N$ points uniformly on the surfaces of $k$ nested hyperspheres in $\\mathbb{R}^{d}$, intersected with the first quadrant $\\mathbb{R}_{+}^{d}$. Each point $z^{(i)}$ is sampled from one of the hyperspheres with radius $r_{t}, t = 1,..., k$, such that:\n\\begin{equation}\nz^{(i)} = r_{t_{i}}u^{(i)}, i = 1,..., N,\n\\label{eq:montecarlo}\n\\end{equation}\nwhere $u^{(i)} \\in \\mathbb{R}^{d}$ are points uniformly distributed on $\\mathbb{S}^{d-1} \\cap \\mathbb{R}_{+}^{d}$ ($\\mathbb{S}^{d-1}$ being the unit hypersphere in $\\mathbb{R}^{d}$), and $t_{i}$ are uniformly distributed over the set $\\{1, ..., k\\}$. The radius $r_{t}$ is scaled linearly such that $r_{t} = t/k$.\nStep 2: Optimal Transport Plan Assume that the set $D^{fit}_{id}$ is of size $M$ and denote by $s^{(1)},..., s^{(M)} \\in \\mathbb{R}^{d}$ their scores. Given the set of reference points $z^{(i)}$ on the hyperspheres we compute the optimal transport plan $P \\in \\mathbb{R}^{N \\times M}_{+}$ that minimizes the transportation cost:\n\\begin{equation}\nP = \\underset{P \\in \\Pi(a, b)}{arg \\hspace{1mm} min} \\sum_{i=1}^{N} \\sum_{j=1}^{M} P_{i,j} \\cdot c(z^{(i)}, s^{(j)}),\n\\label{eq:transport}\n\\end{equation}\nwhere $\\Pi(a, b)$ is the set of all transportation plans that satisfy the marginal constraints a and b (uniform distributions over $\\{1, ..., N\\}$ and $\\{1, ..., M\\}$, respectively), and $c(z^{(i)}, s^{(j)})$ is the cost function, typically the squared Euclidean distance $c(z, y) = ||z - y||^{2}$. Informally speaking, the meaning of $P$ is that a mass $P_{i,j}$ is transported from the point $z^{(i)}$ to the point $s^{(j)}$, given that each point $z^{(i)}$ in the initial distribution has a mass of $1/N$ and each point $s^{(j)}$ has a mass of $1/M$.\nStep 3: Estimating Quantiles The estimated quantile values $q$ for the in-distribution scores are derived from the transport plan $P$ and the radii of the hyperspheres. The quantile value for each score $s^{(j)}$ is given by:\n\\begin{equation}\nq_{j} = \\sum_{i=1}^{N} r_{t_{i}} P_{i,j},\n\\label{eq:quantile}\n\\end{equation}\nwhere $r_{t_{i}}$ corresponds to the radius of the hypersphere from which the point $z^{(i)}$ was sampled.\nStep 4: Constructing Quantile Contours To visualize and interpret the quantiles, we construct a convex hull for the points below a specific quantile threshold $Q$. The convex hull is defined as:\n\\begin{equation}\nConv(\\{s^{(j)} | q_{j} \\leq Q\\}),\n\\label{eq:convex}\n\\end{equation}\nwhere $Q$ is the desired quantile level."}, {"title": "4.2 OOD Metrics for multi-dimensional OOD detectors", "content": "It is not straightforward to generalize the commonly used metrics for OOD detection evaluation to the case of a multi-dimensional OOD score $s$. Indeed, the multi-dimensional detectors constructed above involve drawing a decision boundary (determined by the subset $A$) rather than simply using a threshold $\\tau \\in \\mathbb{R}$. In order to generalize the common metrics to the newly defined multi-dimensional OOD detectors, we choose to define a one-parameter family of sets $(A_{t})_{0<t\\leq 1}$ adapted to each method.\n*   For the majority vote, $A_{t}$ is obtained by picking thresholds $T_{i}$ such that the FPR associated to the individual score $s_{i}$ is equal to $t$.\n*   For the CDF based multi-dimensional detectors, $A_{t}$ can be defined by using iso-probability levels, e.g. for the empirical CDF $A_{t} = \\{ y \\in \\mathbb{R}^{d} | F_{s(x)} (y) > t\\}$.\n*   For the center-outward quantiles we take extended version of $A_{t} = \\mathbb{R}_{+}^{d} \\setminus Conv\\{s^{(j)} | q_{j} \\leq t\\}$"}, {"title": "4.3 Search Strategies", "content": "The purpose of this section is to choose the best score combinations for a given task. We are using 28 individual scores as candidates for being combined with each other. Since evaluating all the possible combinations is computationally infeasible, we propose a few more lightweight alternatives. The comparison between the different combinations is performed using both ID and OOD data. The ID data that we use is that of the set $D_{id}^{val}$ (see Section 3). Given an OOD dataset $D^{ood}_{val}$ we split it into two disjoint subsets, the set $D^{val}_{id}$ is used along with $D_{id}^{val}$ for model selection purposes, while the set $D^{test}_{id}$ is used along the set $D^{test}_{id}$ for final evaluation purposes. The test sets are never used for score fitting, calibration of model combination, or model selection."}, {"title": "4.3.1 Best Pairs", "content": "A first approach to finding promising sets of scores to combine is to consider only pairs of individual score functions and simply test all the combinations on the validation datasets. In our case, we consider 28 different univariate OOD scores, which involves testing 378 combinations. Each of the individual OOD scores is only computed once, and can then be used in all of the combinations it is involved in. As a result, once the individual OOD scores are computed, such a benchmark can be performed in a few hours with a modern CPU."}, {"title": "4.3.2 Sensitivity Analysis", "content": "The previous method only allows to select pairs of univariate OOD scores. This section provides a methodology for selecting sets of more than two OOD scores using basic linear Sensitivity Analysis. Specifically, we sample $N = 1000$ d-dimensional random binary vectors $\\{b_{1}, ..., b_{N}\\}$ \u2013 one dimension for each univariate OOD score \u2013, and create a set of OOD scores to combine out of each of these vectors (the score is included for 1's and not included for 0's). We observed that combining more than 4 OOD scores seemed not to improve the AUROC, or even to decrease it, so we only drew sets of 4 scores at most. We compute the AUROC of each of the $N$ multi-dimensional OOD detector and store the result in $\\{z_{1},..., z_{N}\\}$. The AUROC values are then used to build an output variable $y_{i} := 1(z_{i} > z_{p})$, where $z_{p}$ is the p-percentile of $\\{z_{1}, ..., z_{N}\\}$. Then, we perform a logistic regression of the pairs $\\{(b_{1}, y_{1}), ..., (b_{N}, y_{N})\\}$ and consider the coefficients of this regression as sensitivity indices. Note that using $y_{i} = 1(z_{i} > z_{p})$ rather than $z_{i}$ allows us to assess the importance of each univariate score in reaching the p-percentile."}, {"title": "4.3.3 Beam Search", "content": "Beam search, originally introduced in the context of speech recognition systems [25], is a heuristic algorithm that efficiently explores large search spaces by maintaining a fixed number of promising solutions at each step. This method strikes a balance between breadth and depth, enabling comprehensive exploration without exhaustive computation.\nIn our approach, we use beam search to identify optimal combinations of univariate OOD scores. The algorithm iteratively combines the best-performing scores, as evaluated on the calibration datasets, to form sets. At each iteration, we retain a specified number of top-performing combinations (beam width) and expand them by adding new scores, up to a predetermined maximum ensemble size (beam depth). This process allows us to systematically explore and identify effective OOD score combinations. For more details, refer to the pseudo-code of the algorithm in the Appendix B."}, {"title": "5 Experiments", "content": "In this section, we explore the performances of the combination methods and the search strategies on OpenOOD [2], an extensive benchmark for OOD detection. We study 28 individual OOD detectors listed in Appendix A, using the Post-Hoc OOD detection library Oodeel [9]. We test the methods on three ID datasets, CIFAR-10, CIFAR-100 and Imagenet-200, with a set of OOD benchmarks gathered under two categories: Near-OOD and Far-OOD. The models considered are ResNet18 for CIFAR and ResNet50 for Imagenet200, trained on ID datasets, downloaded from OpenOOD. Note that the OOD datasets included in these two categories are not the same for each ID dataset. More details on the benchmark are found in Appendix D. In addition, if there are any, we defer implementation details of the combination methods and the search strategies to Appendix B.\nWe consider two approaches corresponding to common practical settings. The first case is when, as a practitioner, one has to choose between a large number of available OOD detectors and test them on their own data. In this case, we assume that some OOD data is available to select the most appropriate detector. In the second scenario, we consider that no OOD data is available for selecting a set of OOD detectors to combine. In that case, we draw inspiration from Outlier Exposure principles [3] and select some external data as proxies for OOD data. We use data specifically curated as Outlier Exposure data by OpenOOD [2]. Details on these datasets are found in Appendix D."}, {"title": "5.1 Combination Methods and Search Strategies on OpenOOD", "content": "In this part, we present the results of each of the four combination meth-ods, with combination sets selected us-ing the different search strategies. For each set of univariate OOD detectors to combine, the multi-dimensional detector is fit on $D^{cal}_{id}$. The search strategies consider AUROCS for each multi-dimensional detector on $D^{val}_{id}$ and $D^{ood}_{val}$. Once the best set is se-"}, {"title": "5.2 Selecting Promising Sets Without OOD Data", "content": "In many real-world cases, the access to OOD data is limited. Outlier Exposure [3] has emerged to cope with that lack of OOD data. It is now a common practice to use Outlier Exposure (OE), i.e. selecting external datasets as proxies for OOD data, to tune OOD detectors [26, 27, 28, 29, 30, 31, 32]. OpenOOD benchmark actually cu-rates datasets labeled Outlier Expo-sure for each ID dataset, which is strictly disjoint from the correspond-ing test datasets. In this section, based on the intuition that combinations of detectors that are performant on Out-"}, {"title": "6 Limitations", "content": "While we believe that combining OOD scores is good way to boost the performance of individual OOD detectors, we acknowledge that the methods proposed have limitations: ID data availability. Our methods involve using a training set, a calibration set, a validation set and a test set, which might be prohibitive for applications in which data is scarce. OOD data availability. Picking the best combination of scores requires OOD data (proxy or not), while we believe that this might be a limitation in some context like Anomaly Detection where OOD data is rather scarce, we are also aware that using OOD data is a common practice for model selection and benchmarking purposes. Extra compute resources. We are aware that searching for the best combination of scores carries an extra computation effort compared to selecting the best individual scores. However, combinations of over four scores provide negligible improvement, and some OOD detector can be implemented jointly to avoid repeated forward passes. We thus believe that the extra compute power required is only a minor limitation of our method."}, {"title": "7 Conclusion", "content": "In this work, we investigated the combination of existing OOD detectors to improve OOD detection capabilities. We introduced four different combination methods based on majority vote, empirical CDF, parametric CDF with copulas or center outward. We also introduced several search strategies to choose which individual detectors to combine and showed that this whole methodology could significantly improve the state-of-the-art for CIFAR-10, CIFAR-100, and ImageNet-200 test cases as designed on the OpenOOD benchmark. The methodology exhibits several additional advantages. It is versatile because it can be applied to any reference metrics we use AUROC, but depending on the industrial application, it is possible to apply it to any other metric, e.g., FPR@TPR95 or TPR@FPR5. Finally, it will stay automatically up to date with state-of-the-art post-hoc OOD detection because its performance stems from the underlying individual OOD detectors."}, {"title": "B.1 Majority vote", "content": "Compared to other combination methods, the majority vote approach does not combine scores but rather combines binary decisions (ID or OOD). For each individual OOD detector, thresholds $T_{i}$ are used to make the decision.\nThere are several ways to combine multiple binary values and produce a single binary decision. We defined and experimented with four of these methods:\n*   ALL: An element is declared OOD if all individual OOD detectors classify it as OOD.\n*   ANY: An element is declared OOD if at least one individual OOD detector classifies it as OOD.\n*   LOOSE: An element is declared OOD if at least half of the individual OOD detectors classify it as OOD (with a tie-breaking rule in favor of OOD)."}, {"title": "B.2 Copulas", "content": "As stated in the main paper, the copula method involves choosing a parametric distribution for marginal distributions, and a copula function, which can be chosen among different types of parametric copulas.\nWe selected the marginal distribution among Gaussian, Beta, and Uniform distributions and found that choosing Uniform consistently outperformed other distributions. As for the Copulas, we considered 6 possible different functions parametrized by $\\theta$:\nClayton Copula (bivariate) The Clayton Copula function is defined as:\n$C(u_{1}, u_{2}) = (u_{1}^{-\\theta} + u_{2}^{-\\theta} - 1) ^{-1/\\theta}$\nwhere $u_{1}, u_{2} \\in [0, 1]^{2}$.\nFrank Copula (bivariate) The Clayton Copula function is defined as:\n$C(u_{1}, u_{2}) = -\\frac{1}{\\theta} log (1 + \\frac{(e^{-\\theta u_{1}} - 1) (e^{-\\theta u_{2}} - 1)}{e^{-\\theta} - 1})$\nwhere $u_{1}, u_{2} \\in [0, 1]^{2}$.\nGumbel Copula (bivariate) The Clayton Copula function is defined as:\n$\\theta_{n} = \\frac{1}{1 - \\hat{\\tau_{n}}}$\nwhere $u_{1}, u_{2} \\in [0, 1]^{2}$, and $\\hat{\\tau_{n}}$ is the Kendall-$\\{tau\\}$, which measures the association between pairs of data points. It is defined as:\n$\\tau = \\frac{C - D}{\\frac{1}{2}n(n - 1)}$\nwhere: - C is the number of concordant pairs. - D is the number of discordant pairs. - n is the number of data points.\nFor a given set of n pairs $(u^{(i)}_{1}, u^{(i)}_{2})$ where $i = 1, 2, ..., n$:\n*   Concordant Pair: A pair of observations $(u^{(i)}_{1}, u^{(i)}_{2})$ and $(u^{(j)}_{1}, u^{(j)}_{2})$ is concordant if the order of $u^{(i)}_{1}$ and $u^{(j)}_{1}$ is the same as the order of $u^{(i)}_{2}$ and $u^{(j)}_{2}$. That is, either both $u^{(i)}_{1} < u^{(j)}_{1}$ and $u^{(i)}_{2} < u^{(j)}_{2}$ or both $u^{(i)}_{1} > u^{(j)}_{1}$ and $u^{(i)}_{2} > u^{(j)}_{2}$.\n*   Discordant Pair: A pair of observations $(u^{(i)}_{1}, u^{(i)}_{2})$ and $(u^{(j)}_{1}, u^{(j)}_{2})$ is discordant if the order of $u^{(i)}_{1}$ and $u^{(j)}_{1}$ is different from the order of $u^{(i)}_{2}$ and $u^{(j)}_{2}$. That is, either $u^{(i)}_{1} < u^{(j)}_{1}$ and $u^{(i)}_{2} > u^{(j)}_{2}$ or $u^{(i)}_{1} > u^{(j)}_{1}$ and $u^{(i)}_{2} < u^{(j)}_{2}$."}, {"title": "B.3 Center Outward", "content": "B.3.1 Monotonicity property on convex hulls\nIn order to ensure monotonicity property as defined in 4.1", "follows": "For each dimension", "quantiles": "Convex hulls vs KNN\nUsing convex hulls for center-outward quantiles ensures generalization for new points by finding the smallest convex hull that contains them. However", "straightforward": "when estimating quantiles for in-distribution calibration scores $D^{cal"}, {"parameters": "n*   Scaling of data points: We rescale multidimensional score points in [0", "1": "using a Quan-tileTransformer form Sklearn library", "estimation": "For the computational reasons evoked in B.3.2, we use KNN approach with"}]}]}