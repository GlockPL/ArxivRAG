{"title": "MGH Radiology Llama: A Llama 3 70B Model for Radiology", "authors": ["Yucheng Shi", "Peng Shu", "Zhengliang Liu", "Zihao Wu", "Quanzheng Li", "Xiang Li"], "abstract": "In recent years, the field of radiology has increasingly harnessed the power of artificial intelligence (AI) to enhance diagnostic accuracy, streamline workflows, and improve patient care. Large language models (LLMs) have emerged as particularly promising tools, offering significant potential in assisting radiologists with report generation, clinical decision support, and patient communication. This paper presents an advanced radiology-focused large language model: MGH Radiology Llama. It is developed using the Llama 3 70B model, building upon previous domain-specific models like Radiology-GPT and Radiology-Llama2. Leveraging a unique and comprehensive dataset from Massachusetts General Hospital, comprising over 6.5 million de-identified medical reports across various imaging modalities, the model demonstrates significant improvements in generating accurate and clinically relevant radiology impressions given the corresponding findings. Our evaluation, incorporating both traditional metrics and a GPT-4-based assessment, highlights the enhanced performance of this work over general-purpose LLMs.", "sections": [{"title": "Introduction", "content": "The field of radiology has been increasingly leveraging artificial intelligence to enhance diagnostic accuracy, streamline workflows, and improve patient care. Large language models (LLMs) have shown particular promise in this domain, offering the potential to assist radiologists in report generation, clinical decision support, and patient communication [1, 2, 3, 4]. Previous work such as Radiology-GPT [3] and Radiology-Llama2 [5] demonstrated the effectiveness of domain-specific LLMs in radiology tasks, outperforming general-purpose models in generating coherent and clinically relevant impressions from radiological findings.\nBuilding upon these foundations, we present an advanced iteration of radiology-focused LLM, which we have developed using the Llama 3 70B [6] model as its base. This new model represents a significant leap forward in scale and capability, addressing key challenges in applying LLMs to the radiology domain:\n\u2022 Enhanced Model Scale: By utilizing the Llama 3 70B model, we significantly increase the capacity and potential performance of our system compared to previous 7B-based models. This larger model allows for more nuanced understanding and generation of radiology-specific language.\n\u2022 Unique and Comprehensive Dataset: We leverage an extensive and diverse dataset from Massachusetts General Hospital (MGH), comprising over 6.5 million medical reports spanning a decade (2008-2018). This dataset is unique in its scale and breadth, encompassing a wide range of imaging modalities (including CT, MRI, X-ray, and Fluoroscopic imaging) and body"}, {"title": "Related work", "content": "General-purpose LLMs such as GPT-3 [13], GPT-4 [14], and PaLM [15] have demonstrated impressive capabilities across various domains, including healthcare. These models have shown potential in medical question answering, clinical decision support, and even assisting in medical diagnosis. Their ability to process and generate human-like text has opened up new possibilities for automating various aspects of healthcare, from patient communication to preliminary analysis of medical literature.\nThe versatility of these models allows them to adapt to a wide range of medical topics, potentially serving as powerful tools for healthcare professionals and researchers alike.\nHowever, the application of general-purpose LLMs in specialized medical fields faces significant limitations. These models often lack the depth of domain-specific knowledge required in areas like radiology, potentially leading to inaccuracies or misinterpretations. Privacy concerns are paramount, as using these models typically involves sending sensitive medical data through external APIs, conflicting with strict patient confidentiality requirements. Moreover, these LLMs may not capture the nuanced language and reporting style used by medical professionals, particularly in specialized fields. The need for on-premise solutions in many healthcare institutions further complicates the deployment of these large models. These limitations have spurred the development of specialized models for healthcare applications, designed to address these challenges while leveraging the power of large language models."}, {"title": "Domain-Specific Language Models in Radiology", "content": "Several attempts have been made to create domain-specific language models for radiology. RadBERT [16] and ClinicalBERT [17] are examples of BERT-based models fine-tuned on radiology reports. These models showed improvements in tasks such as named entity recognition and report classification but were limited in their ability to generate coherent, full-length reports."}, {"title": "Methodology", "content": "Our method to build this model involves two main steps: Preprocessing of Massachusetts General Hospital (MGH) dataset and instruction fine-tuning. For fine-tuning, we utilize both fully fine-tuning and QLoRA fine-tuning[21] on Llama 3 70B model."}, {"title": "Dataset and preprocessing", "content": "We utilize abundant data from Massachusetts General Hospital where it contents over 6,500,000 medical reports from patients who were admitted to MGH between 2008 and 2018. These comprehensive medical reports encompass a wide range of imaging modalities and cover various body parts. The imaging modalities include CT (Computed Tomography), MRI (Magnetic Resonance Imaging), X-ray, and Fluoroscopic imaging. The reports provide detailed examinations of different body regions such as the abdomen, cardiac (heart), chest, limbs, and more.\nWe focus on the radiology reports since they contain plenty of information such as radiologists' interpretations for corresponding medical images. We leverage the exam codes, original reports and impressions to establish our dataset. The exam codes indicate the modality of corresponding images of the original reports. Original reports contain the findings that specify the detailed observation from radiology images. And impressions part summarizes these detailed observation.\nTo build our dataset, we extract findings from the original reports and impressions to form pairs for instruction fine-tuning. Given an instruction with findings as input, the model should generate desired impression as output. For instruction, we follow [22] to create 20 different instructions asking for impressions. We randomly select one instruction for each pair. We also combine the exam code with finding in the input to provide richer information. Similarly, in the output impression, we put a prepend phrase selected randomly from 10 sentences list with a certain probability. Since these radiology reports come from MGH, which means they contain sensitive privacy information such as patient names and doctor names, we remove all the names in the reports by using regular expression to match the sentences that includes names. Then, we remove the unqualified reports that would cause potential hallucinations. They can be summarized as: (1) reports that don't have either findings or impressions, (2) reports with findings less than 10 words. Besides, we remove the extra white space to avoid format problems. Finally, we obtain 4,354,321 reports as training data."}, {"title": "Fine Tuning Methods", "content": "When fine-tuning large language models like the Llama 3 70B model, two primary methods are commonly employed: fully fine-tune and LoRA fine-tune. Fully fine-tune involves adjusting all the parameters of the pre-trained model on a specific downstream task. This method is powerful as it allows the model to fully adapt to the new task, but it requires substantial computational resources and large amounts of data, which can be costly and time-consuming.\nOn the other hand, LoRA fine-tune is a more efficient approach that focuses on learning low-rank updates to the model's weights, rather than modifying all the parameters. This method introduces additional trainable matrices with much fewer parameters than the original model, significantly reducing the computational burden while maintaining high performance. LORA fine-tune is particularly useful when resources are limited or when fine-tune needs to be done quickly, making it a popular choice for adapting large models like Llama 3 70B to specific tasks with fewer data and compute requirements.\nQLORA allows for further computation resources saving from LoRA. QLORA usually first quantize the model weights to a lower precision, typically 4-bit, and then apply low-rank adaptation. This approach significantly reduces the memory footprint of the model, enabling the fine-tuning of models with billions of parameters on hardware with very limited memory, such as a single GPU. Despite the reduction in precision and the use of low-rank matrices, QLORA maintains competitive performance, making it a powerful method for adapting large-scale pre-trained models to specific tasks efficiently.\nIn our experiments, we implement both fully fine-tune and QLoRA methods since we have enormous"}, {"title": "Experiments", "content": "In this section, we describe our experimental setup for training and evaluating the MGH Radiology Llama, a large language model specialized for radiology. Our experiments encompass model preparation, dataset handling, and the training process, all optimized for high-performance computing infrastructure."}, {"title": "Model Preparation", "content": "We based our model on the Llama 3 architecture, specifically the meta-llama/Meta-Llama-3-70B-Instruct variant [6]. To optimize for both performance and efficiency, we applied 4-bit quantization using BitsAndBytesConfig [23]. This quantization technique significantly reduced the model's memory footprint, allowing us to work with larger models within our GPU constraints. We further enhanced the model's adaptability by implementing Low-Rank Adaptation (LoRA) [24], configured using PeftConfig. LoRA enabled us to fine-tune the model efficiently by adapting a small subset of parameters while maintaining overall performance."}, {"title": "Dataset Handling", "content": "Our dataset processing pipeline was designed to maximize efficiency and GPU utilization. We began by loading our dataset from a JSON file, which was converted from raw CSV files containing radiology reports and associated metadata. To optimize the tokenization process, we pre-tokenized the text using the LLama 3 tokenizer and stored the results in cache, eliminating the need for repeated tokenization during training. A key innovation in our approach was the implementation of sequence packing [25]. This technique allowed us to concatenate multiple sequences up to a maximum length of 2048 tokens, significantly improving GPU utilization. We carefully managed attention masks to ensure the model could differentiate between packed sequences. The final step in our data preparation involved splitting the dataset into training and evaluation sets, with 99.9% allocated for training and 0.1% reserved for evaluation."}, {"title": "Training Process", "content": "For the training phase, we employed the SFTTrainer for supervised fine-tuning, incorporating several advanced techniques to optimize performance and resource utilization. We enabled gradient checkpointing to balance memory efficiency with computational demands, allowing us to train larger models than would otherwise be possible given our memory constraints [26]. We leveraged DeepSpeed ZeRO Stage 3 for distributed training across multiple GPUs [27]. This approach sharded model parameters, gradients, and optimizer states across our GPU cluster, enabling us to train substantially larger models than a single GPU could handle. Our training process also utilized mixed precision training with BF16 (Brain Float 16), chosen for its wider dynamic range and reduced risk of numerical issues compared to FP16 [28]. To further optimize our training, we implemented gradient accumulation, effectively increasing our batch size without proportionally increasing memory usage."}, {"title": "Hardware Utilization", "content": "Our experiments were conducted on a high-performance computing cluster equipped with 8 NVIDIA H100 GPUs. This advanced hardware allowed us to fully leverage the benefits of our optimized training pipeline. The H100's Tensor Cores were particularly beneficial for our BF16 mixed precision training, providing significant speedups in computational operations [30]. The combination of 4-bit quantization and distributed training techniques enabled us to efficiently utilize the massive parallel processing capabilities of our GPU cluster. This setup allowed us to process larger batch sizes and train on more extensive datasets than would be possible with less optimized configurations. By integrating these advanced training techniques with state-of-the-art hardware, we were able to efficiently train and fine-tune the MGH Radiology Llama model, achieving high performance while effectively managing computational resources. This approach positions our model to potentially advance the state of the art in AI-assisted radiology interpretation and reporting."}, {"title": "Evaluation", "content": "In this section, in order to have a comprehensive evaluation of our fine-tuned model and to compare with baseline Llama 3, we adapt two types of methods: Traditional metrics which focus more on low level words matching and GPT evaluation that estimates the accuracy and similarity for high semantic level."}, {"title": "Traditional Metrics", "content": "For traditional metrics, we use Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and BERTScore[12] for the evaluations. ROUGE metric is a very common metric in natural language processing (NLP) region. It is a set of metrics used for evaluating automatic summarization and machine translation in NLP. The metrics compare the generated impressions against the reference ground truth. ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity. Since ROUGE-1 and ROUGE-2 only compare overlap of unigrams (each word) and bigrams between produced impressions and reference ground truth, here we apply ROUGE-L for Longest Common Subsequence (LCS). LCS takes into account sentence-level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.\nBERTScore is another popular automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence[12]. Instead of searching for exact matching, BERTScore compute the similarity in contextual embeddings. The results are shown in table 1."}, {"title": "GPT Evaluation", "content": "Following the approach in [31], we utilize GPT-40 for MGH to evaluate our model's performance. We design a prompt aimed at leveraging GPT-40's strong capability to compare the accuracy and similarity of generated impressions. The prompt design is shown in figure 2. The score varies from 0-10 where higher score means the generated impression matches expected real impression better. An explanation will come after the score, indicating the criteria and reason GPT-40 assign this score. An example of GPT-40 evaluation is shown in figure 3. It is obvious from the explanation that the evaluation of GPT-40 follows requirement in prompt. GPT-40 can distinguish the missing part and determine stylistic concordance and precision of medical terminology. We also list this experiment result in table 1."}, {"title": "Experiment Results", "content": "From table 1, we can find that both of our fine-tuned Llama 3 models outperform baseline model. Our models achieve significant improvement. We get 100% promotion for ROUGE-L score, 4%-5% for BERTScore precision and more than 1 point on GPT-40 score. This means our models extract more precise radiology impressions from the findings. Comparing with baseline, our generated impressions align better with expected real conclusions derived from radiologists. From low level words matching to high level semantic accuracy, our models perform much better.\nWe notice that fully fine-tune model is the best model but QLoRA fine-tuned model outperforms in some tests. However, the difference between QLoRA and fully fine-tune model is not obvious. We think it is because Llama 3 70B model comes with enormous parameters such that even adjusting limited amount of weights, Llama 3 70B model is still able to dig out its background knowledge in radiology and learn to expresses more precisely like a radiology expert. While in the mean time, fully"}, {"title": "Conclusion", "content": "In this project we utilize a huge amount of data from MGH to leverage the potential of Llama 3 70B on radiology reports. We demonstrate the diversity of radiology reports collecting from MGH in both size and modality. We also demonstrate our de-identified data to protect patients privacy. By applying QLoRA and fully fine-tune, Llama 3 70B model can achieve much better performance in this down stream tasks. We find that model with massive parameters can benefit more from QLoRA fine-tune by obtaining similar performance but with much fewer computation resources.\nOur project still remains some limitations. Firstly, due to the time limit we didn't compare our models with previous state-of-art work such as Radiology-llama2 [5]. Secondly, Meta has released the latest Llama 3.1 models. They mention the performance of new series has exceeded previous version, which means our models cannot take advantage of these powerful models. Thirdly, our model still exists hallucination. It is not qualified for automatic radiology impression generation.\nFor the future work, we have following tasks to be continued. Firstly, we will retrain our model with advanced Llama 3.1 70B model to make full use of foundation model. Since QLORA performs well for larger model, we will attempt to retain model with Llama 3.1 405B model. Secondly, we will apply LLM involved methods to further clean our data. It enables safer patient privacy and"}]}