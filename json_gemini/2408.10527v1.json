{"title": "EdgeNAT: Transformer for Efficient Edge Detection", "authors": ["Jinghuai Jie", "Yan Guo", "Guixing Wu", "Junmin Wu", "Baojian Hua"], "abstract": "Transformers, renowned for their powerful feature ex-traction capabilities, have played an increasingly prominent role in various vision tasks. Especially, recent advancements present trans-former with hierarchical structures such as Dilated Neighborhood Attention Transformer (DiNAT), demonstrating outstanding ability to efficiently capture both global and local features. However, trans-formers' application in edge detection has not been fully exploited. In this paper, we propose EdgeNAT, a one-stage transformer-based edge detector with DiNAT as the encoder, capable of extracting ob-ject boundaries and meaningful edges both accurately and efficiently. On one hand, EdgeNAT captures global contextual information and detailed local cues with DiNAT, on the other hand, it enhances feature representation with a novel SCAF-MLA decoder by utiliz-ing both inter-spatial and inter-channel relationships of feature maps. Extensive experiments on multiple datasets show that our method achieves state-of-the-art performance on both RGB and depth im-ages. Notably, on the widely used BSDS500 dataset, our L model achieves impressive performances, with ODS F-measure and OIS F-measure of 86.0%, 87.6% for multi-scale input, and 84.9%, and 86.3% for single-scale input, surpassing the current state-of-the-art EDTER by 1.2%, 1.1%, 1.7%, and 1.6%, respectively. Moreover, as for throughput, our approach runs at 20.87 FPS on RTX 4090 GPU with single-scale input.", "sections": [{"title": "1 Introduction", "content": "Edge detection is fundamental for various computer vision tasks[45, 21, 37]. The primary objective of edge detection is to precisely ex-tract object boundaries and visually salient edges from input images. As illustrated in Figure 1, inherent challenges of this task include the presence of distant objects, blurring boundary in complex back-grounds, intense color variations within an objects, etc. Therefore, it requires appropriate representation of not only local features like color and texture, but also global semantic information to suppress noise as well as to distinguish object boundary from complex back-ground.\nTraditional edge extraction methods [22, 6] mostly rely on lo-cal information, such as variation of color and texture. CNN-based deep learning based edge detectors can learn global and semantic features[4, 5] with expansion of receptive field, but are likely to lose detail information. To preserve both intricate local information as well as global context, former deep learning detectors [42, 26] em-ploy multi-level aggregation to effectively integrate global features and local details. To mitigate the limitations in the absence of a hi-erarchical structure in ViT[11], the first transformer based edge de-tector EDTER[32] implements a two-stage approach to obtain and combine global features and local details, which demonstrates supe-rior edge detection ability than CNN-based detectors. However, the computational burden known for vision transformer is exacerbated by EDTER's two-stage design.\nRecently, DiNAT[17], an improved hierarchical transformer com-bining both neighbor attention and dilated neighbor attention, has exemplified significant progress in various vision tasks. Since Di-NAT is able to preserve locality, maintain translation equivariance, expand the receptive field exponentially, and capture longer-range inter-dependencies, edge detector based on it (Figure 2) could aban-don the two-stage design, significantly improving throughput. Fur-thermore, to take better usage of rich channels information in the feature map generated by transformer-based encoder, we introduce a novel decoder, Spatial and Channel Attention Fusion-Multi-Level Aggregation (SCAF-MLA). The Spatial and Channel Attention Fu-sion Module (SCAFM) of the decoder integrates both spatial and channel attention concurrently. As a whole, our detector is capable of extracting local detail information at lower levels, which is benefi-cial to the detection of edges associated with distant blurred objects, and extracting global semantic feature information at higher levels, which is beneficial to mitigating excessive noise within the object and to distinguishing inconspicuous edges.\nWith elaborate design, our models exhibit excellent capability of generating accurate and crisp edge maps. To verify the scalability of our edge detector, we further propose five versions of models with"}, {"title": "2 Related Work", "content": "Edge Detection. Early edge detectors[22, 6] mainly rely on local features, like significant variation of color, texture and intensity to detect edges. Machine learning-based methods [24, 30] employ hand-crafted low-level features to train classifiers and achieve impressive performance compared to earlier approaches. Such methods are al-ways ignorant of global information and semantic boundaries. CNN-based deep learning techniques are able to expand receptive fields to capture global features, thus yield remarkable progress in edge de-tection. DeepEdge[4] employs a multi-scale CNN to classify edge candidate points extracted by Canny edge detector. Recent methods have further enhanced edge detection by exploiting hierarchical and multi-scale feature maps CNN encoders produce. [26, 42] learn rich hierarchical features by supervising the layers at each level, lead-ing to improved detection performance. BDCN[19], on the other hand, achieves greater accuracy through a bidirectional feature pro-cessing structure. PiDiNet[35] introduces pixel-differential convolu-tional integration into the CNN model. EDTER[32] is the first at-tempt to introduce Vision Transformer (ViT)[11] for edge detection tasks. To capture multi-scale features, EDTER proposes a two-stage architecture to remedy the lack of hierarchical structure in ViT. The first stage focuses on global feature while the second stage focuses on local features. Features learned in both stages are fused, result-ing in significant improvements in performance and achieving SOTA in edge detection task. PEdger[13] enhances edge detection perfor-mance by leveraging information obtained from different training moments and heterogeneous structures. UAED[48] investigates the subjectivity and ambiguity of different annotations through uncer-tainty based on the fact that dataset labels have multiple annotations.\nVision Transformer. Since the introduction of ViT[11], transform-ers have been widely used in vision field[3, 28, 36]. After years of development, Transformer with multi-scale hierarchical structure are playing increasingly important role in downstream vision tasks. Swin Transformer[27] proposes Window Self Attention (WSA) and Shift Window Self Attention (SWSA), with SWSA expanding the receptive field, enabling it to capture both local and global features. NAT[18] proposes Neighborhood Attention (NA), the first efficient and scalable sliding window attention mechanism, which restricts self-attention to localised windows and preserves translation equiv-ariance. DiNAT[17] extends NA to Dilated Neighborhood Atten-tion (DiNA), which expands receptive fields exponentially and thus captures long-range inter dependency and global features. Besides, Neighborhood Attention Extension (NATTEN) [18] is developed to better implement NA and DiNA as an extension to PyTorch with an efficient CUDA kernel.\nFeature Fusion Module. The feature fusion module is commonly used in edge detection and other vision tasks to strengthen fea-ture representations, which is crucial to improving the accuracy. SENet[20] investigates channel relationships and introduces a novel architectural unit, the Squeeze-and-Excitation (SE) block, enhances global feature extraction by computing channel attention using global average pooling. CBAM[40] employs both global average pooling and global maximum pooling to compute attention maps on two separate dimensions, namely, channel attention and spatial attention, the latter being overlooked by SENet. CBAM is able to extract informative features by blending cross-channel and spatial information together. ECA[38] proposes a local cross channel in-teraction strategy implemented via 1D convolution and a method to adaptively select kernel size of 1D convolution. PP-LiteSeg[31] in-troduces UAFM, a feature fusion module that leverages channel at-tention or spatial attention to enrich the representation of fused fea-tures, with spatial and channel attention modules exploiting inter-spatial and inter-channel relationships of the input features."}, {"title": "3 EdgeNAT", "content": "Figure 2 illustrates the overall framework of EdgeNAT, a one-stage end-to-end edge detector. DiNAT is employed as the encoder since it exhibits exceptional performance in preserving locality, maintain-ing translation equivariance, expanding receptive field, and captur-ing long-range dependencies, etc. SCAF-MLA, a novel decoder with SCAFM to exploit both spatial and channel features from feature maps, is introduced to effectively facilitate feature fusion. We further improve the performance of SCAF-MLA by pre-fusion, that is, for the fusing operation, the feature channels of each layer are reduced to the number of channels in first level of the encoder, denoted as C in Figure 2, rather than to 1."}, {"title": "3.1 Review Dilated Neighborhood Attention\nTransformer", "content": "Below is a brief introduction on DiNAT, encoder of our network, following the work presented in [17].\nTo begin with, DiNAT employs two 3 x 3 convolutional layers with a stride of 2 as a tokenizer to obtain a feature map with a resolution of one-fourth of the input image. Additionally, DiNAT utilizes a single 3\u00d7 3 convolutional layer with a stride of 2 for downsampling between hierarchical levels, reducing the spatial resolution by half while dou-bling the number of channels. The resulting feature maps are thus of\nsizes\nH/4\u00d7W/4\u00d7C\n, \nH/8\u00d7W/8\u00d72c\n, \nH/16\u00d7W/16\u00d74c\nand\nH/32\u00d7W/32\u00d78c\n.\nDiNAT adopts a straightforward stacking of DiNA layers, fol-lowing a similar structural pattern as other commonly used Trans-formers. For simplicity, we keep notations limited to single dimen-sional NA and DiNA. Given input\nX \u2208 R^{n\u00d7d}\n, whose rows are d-dimensional token vectors, and query and key linear projections of X, Q and K, and relative positional biases between any two tokens i and j,\nB(i, j)\n, d-dilated neighborhood attention weights for the i-th token with neighborhood size k,\nAk\n(\u03ba,\u03b4)\n, is defined as the matrix multiplication of the i-th token's query projection, and its k nearest neighboring tokens' key projections with dilation value \u03b4:\nAk(\u03ba,\u03b4)=\n\nQ_iK_{\u03c1_1(i)}^T+B(i,\u03c1_1(i))\\ \nQ_iK_{\u03c1_2(i)}^T+B(i,\u03c1_2(i))\\ \n:\\ \nQ_iK_{\u03c1_k(i)}^T+B(i,\u03c1_k(i))\\ \n\n(1)"}, {"title": "3.2 SCAF-MLA Decoder", "content": "Decoders play a critical role in various vision tasks. Taking inspi-ration from multilevel feature fusion techniques employed in vision tasks, we propose a novel decoder, SCAF-MLA, to effectively uti-lize numerous channels in the feature maps output from transformer-based encoder. SCAF-MLA enables the supervision on multiple lev-els, and learns rich hierarchical features, thus enhances the perfor-mance of edge detection. Besides, SCAF-MLA Decoder is more computationally efficient, without the commonly employed PPM[47] and bottom-up path[19, 32], while experimental results demonstrate that our designed decoder achieves more superior performance.\nSCAFM.\nInspired by UAFM[31] in multi-level features fusing, we pro-pose the Spatial and Channel Attention Fusion Module (SCAFM) as the main component of the SCAF-MLA. SCAFM is designed to extract both spatial and channel features, concurrently preserv-ing the distinctive attributes of the current level while capturing higher-level features. The architecture of SCAFM is depicted in Fig-ure 3. SCAFM consists of a spatial attention module (SAM) and a channel attention module (CAM) to compute inter-spatial and inter-channel weights, denoted as\n\u03b1_{sp}\nand\n\u03b1_{ch}\n, respectively. Specifically, the upper-level feature is denoted as\nF_{high}\nand the current-level fea-ture as\nF_{low}\n. To begin with, bilinear interpolation is employed to up-sample\nF_{high}\nto the same size as\nF_{low}\n. Subsequently, convolutional operations is utilized to increase the channels of\nF_{low}\nto match those of\nF_{high}\n, denoted as\nF_{conv}\n. For\n\u03b1_{sp}\n, we start by performing mean and max operations along the channel dimension on\nF_{up}\nand\nF_{conv}\n, resulting in the generation of four features, each with a dimension of\n\u211d^{1\u00d7H\u00d7W}\n. Subsequently, these four features are concatenated and processed through convolutional and sigmoid operations, yielding\n\u03b1_{sp} \u2208 \u211d^{1\u00d7H\u00d7W}\n. This process can be represented by thefollowing equations:\nFup\n= Up(\nFhigh\n),\nFconv\n= Conv(\nFlow\n),\n\u03b1sp\n= Sigmoid(Conv(Cat(Mean(\nFup\n),\nMax(\nFup\n), Mean(\nFconv\n),\nMax(\nFconv\n)))),\n(4)\nRegarding\n\u03b1ch\n, average pooling and max pooling operations are applied on\nFup\nand\nFconv\n, generating four features with dimensions\n\u211d^{C\u00d71\u00d71}\n. Then, these features are concatenated and subjected to con-volutional and sigmoid operations, generating\n\u03b1_{ch} \u2208 \u211d^{C\u00d71\u00d71}\n, described as:\n\u03b1ch\n= Sigmoid(Conv(Cat(AvgPool(\nFup\n),\nMaxPool(\nFup\n), AvgPool(\nFconv\n),\nMaxPool(\nFconv\n)))),\n(5)\nThe input features are then fused with the generated weights\n\u03b1_{sp}\nand\n\u03b1_{ch}\nthrough multiplication and addition operations, resulting in features\nF_{sp}\nand\nF_{ch}\n. Subsequently, these features are concate-"}, {"title": "3.3 Loss Function", "content": "We employ the loss function proposed in [42] for the 4 side edge maps and 1 primary edge map. Given an edge map E and its corre-sponding ground truth Y, the loss function is computed as follows:\nl(E,Y) = \u2212 \u2211\ni,j\n(Yijalog(Eij)\n+ (1 \u2212 Yij)(1 \u2212 \u03b1) log (1 \u2212 Ei,j)),\n|Y\u2212|\n(7)\nwhere\nE_{ij}\nand\nY_{i,j}\nare the (i, j)th element of matrix E and Y, re-spectively.\n\u03b1 =\n|Y+|\nrepresents the percentage of negative pixel samples, with\n|Y+|\nand\n|Y|\ndenoting the number of positive and negative sample pixels, respectively. Since BSDS500 dataset is annotated by multiple annotators, we first normalize the multiple an-notations into edge probability maps within the range of [0, 1]. Then, if the probability of a pixel is greater than a threshold value \u03b7, it is labeled as a positive sample; otherwise, it is labeled as a negative sample."}, {"title": "4 Experiments", "content": "4.1 Datasets\nTwo mainstream datasets are used to evaluate our proposed Ed-geNAT, namely, BSDS500 and NYUDv2.\nBSDS500[2] consists of 500 RGB images, with 200 for training, 100 for validation, and 200 for testing. Similar to [42, 26], the dataset is augmented to 28,800 images by flipping, scaling, and rotating. PAS-CAL VOC Context dataset[12] is used as additional training data and its 10,103 training images are also augmented to 20,206 by flipping, as in most previous works[26, 19]. The model is pre-trained with the augmented PACSAL VOC Context dataset and then fine-tuned with the 300 training and validation images of BSDS500 dataset, and is evaluated on 200 testing images.\nNYUDv2[34] consists of 1449 labeled pairs of aligned RGB and depth images, with 381 training images, 414 validation images, and 654 testing images. As in [42, 26], the training and validation sets are combined and augmented to train the model."}, {"title": "4.2 Implementation Details", "content": "Our EdgeNAT is implemented with PyTorch and is based on mmsegmentation[7] and NATTEN[18]. We use the pre-trained weights of DiNAT[17] to initialize EdgeNAT's transformer blocks. To generate binary edge maps, for BSDS500, we set the threshold \u03b7 to 0.3 to select positive samples. For NYUDv2, there is only one annotation per picture, so there is no need to set the threshold \u03b7.\nWe use the AdamW optimizer and train for 40k iterations using a cosine decay learning rate scheduler, where the first 15k iterations warm up the learning rate in a linear manner, and the remaining ones are decayed according to the scheduler. The initial learning rate is 0 and a preset learning rate is set to 6e-5. For BSDS500, we set its batch size to 8, and for NYUDv2, we set its batch size to 4.\nAll experiments were conducted on RTX 4090 GPU. The training of the L model of EdgeNAT (472.38MB) takes 6 hours, far more efficient than Transformer-based model EDTER (468.84MB)[32], which takes 26.4 hours. The inference runs at 20.87 FPS on RTX 4090, nearly ten times the speed of EDTER on V100 (2.2 FPS). During training, since our model is a one-stage edge detection model, for 320\u00d7320 images, the GPU memory requirement is about 20GB, 2/3 of EDTER(29GB)."}, {"title": "4.3 Ablation Study", "content": "Ablation experiments are performed on the BSDS500 data set to ver-ify the effectiveness of our proposed decoder. Specifically, we first compare the effect of pre-fusion (reduce the channels of feature map to C) and final-fusion (reduce the channels of feature map to 1); then the effect of bottom-up path is also verified. From the quantitative results shown in Table 1, it is clear that regardless of pre-fusion or final-fusion, Bottom-up Path has negative effects on edge detection performance, indicating it is not suitable for DiNAT. For edge detec-tion models with relatively large number of feature map channels, pre-fusion without PPM will be a better choice."}, {"title": "4.4 Network Scalability", "content": "EdgeNAT-L has a relatively large amount of parameters (472.38MB). In order to adapt to different application scenarios, we conduct scal-ability experiments on different model sizes. The configuration set-tings of the encoder of the L, S0, S1, S2, and S3 variants of our EdgeNAT are the same as those of the Large, Mini, Tiny, Small, and Base versions of the DiNAT[17]. Extensive experiments are con-ducted to study the scalability and throughput of EdgeNAT variants. The result is shown in Figure 4. The models are all trained using the BSDS500 training and validation sets with or without PASCAL VOC, and evaluated with the BSDS500 test set. As expected, when the size of our model decreases, the ODS and OIS will decrease ac-cordingly, and the throughput increases.\nIt is worth noting that the processing speed of the SO model is much higher than that of other models. This should be contributed to the fact that its third level has only 6 layers, while the others models"}, {"title": "4.5 Comparison with State-of-the-arts", "content": "On BSDS500 dataset. We compare our L model with traditional detectors such as Canny[6], gPb-UCM[1], SCG[41], SE[10] and OEF[16], and CNN-based detector such as DeepEdge[4], DeepContour[33], HED[42], Deep Boundary[23], CEDN[44], RDS[25], AMH-Net[43], RCF[26], CED[39], LPCB[9], BDCN[19], DSCD[8], PiDiNet[35], UAED[48] and PEdger[13], and transformer-based detector such as EDTER[32]. The results are summarized in Table 3 and Figure 6, respectively. We notice that our L model, trained on the BSDS500 dataset, achieves an ODS of 84.3% with single-scale inputs, outperforming all competing detec-tors. Furthermore, when employing multi-scale inputs, our method achieves an even higher ODS of 85.5%. By utilizing additional training data and adopting multi-scale input (following the config-urations of RCF, EDTER, etc.), our method attains 86.0%(ODS), 87.6%(OIS), which clearly demonstrate the superiority of our method over all existing state-of-the-art edge detectors. Several qualitative results are presented in Figure 5. It can be observed that our proposed EdgeNAT demonstrates a distinct advantage in terms of prediction quality. The generated outputs exhibit clear and exact edge predictions, further validating the efficacy of our method.\nOn NYUDv2 dataset. We conduct experiments on three types of inputs (RGB, HHA, and RGB-HHA). The RGB-HHA results are obtained by averaging the edge detections from RGB and HHA. We compare our L model with deep learning-based detectors, in-cluding HED[42], COB[29], RCF[26], AMH-Net[43], LPCB[9], BDCN[19], PiDiNet[35], PEdger[13], and EDTER[32]. All results are based on single-scale inputs. The results are shown in Table 4. It can be observed that our L model achieveds ODS of 78.9%, 72.6%, and 79.4% for RGB, HHA, and RGB-HHA, respectively, surpass-ing the second-best method by 1.5%, 0.9% and 1.0%, respectively. Furthermore, our approach also attains the highest OIS among all the evaluated methods. The results of our other models, and the precision-recall curves, will be presented in the supplementary ma-terial."}, {"title": "5 Conclusion", "content": "Our contributions are summarized as follows: firstly, we introduce DiNAT as the encoder, which enables our proposed edge detector not only more accurate than current SOTA EDTER, but also ten times faster than it. Secondly, we propose SCAFM, a module that concate-nates spatial attention and channel attention, to generate richer and more accurate feature representation for the decoder. Thirdly, we design five version of models with different parameter sizes to adapt to complex and diverse application scenarios and conduct extensive experiments on the BSDS500 and NYUDv2 datasets, demonstrating that EdgeNAT achieves superiority in both efficiency and accuracy."}, {"title": "Appendices", "content": "In this appendices, we provide additional detailed information, in-cluding EdgeNAT configurations and dilation values, as well as more experimental results and their visualization on BSDS500[2] and NYUDv2[34]."}, {"title": "A EdgeNAT Configurations and Dilation Values", "content": "Our EdgeNAT model, including the S0, S1, S2, S3 and L variants, utilizes a hierarchical backbone network architecture divided into four levels. Each level consists of a Downsampler (or Tokenizer) and multiple DiNAT blocks. The configurations and dilation values of these levels closely resemble those of the Mini, Tiny, Small, Base, and Large variants of DiNAT, as presented in Table 5."}, {"title": "B More Results on BSDS500", "content": "On BSDS500[2] dataset, since the main text presents the experi-mental results with single-scale input, in the supplementary mate-rial, we conduct experiments on the S0, S1, S2, and S3 models with multi-scale inputs. Besides, the single-scale and multi-scale exper-iments with additional training data are also conducted. We com-pare our models with traditional detectors such as Canny [6], gPb-UCM[1], SCG[41], SE[10] and OEF[16], CNN-based detector such as DeepEdge[4], DeepContour[33], HED[42], Deep Boundary[23], CEDN[44], RDS[25], AMH-Net[43], RCF[26], CED[39], LPCB[9], BDCN[19], DSCD[8], PiDiNet[35], UAED[48] and PEdger[13], and transformer-based detector such as EDTER[32]. The results are pre-sented in Table 6. Moreover, Figure 7 shows the Precision-Recall curves of transformer-based detector, Figure 9 provides qualitative results of the L model, Figure 10 shows the visual results for differ-ent variants of EdgeNAT, and Figure 11 presents the visual results compared with other approaches. It is evident that even the variants of EdgeNAT with smaller parameter sizes yield highly competitive results."}, {"title": "C More Results on NYUDv2", "content": "On the NYUDv2[34] dataset, we conduct experiments on all variants of the model, exploring three types of inputs: RGB, HHA, and RGB-HHA. We compare our models with tradi-tional detectors including gPb-UCM[1], gPb+NG[14], SE[10], SE+NG+[15], OEF[16], SemiContour[46], CNN-based detectors in-cluding HED[42], COB[29], RCF[26], AMH-Net[43], LPCB[9], BDCN[19], PiDiNet[35], PEdger[13], and transformer-based de-tector including EDTER[32]. The results are shown in Table 7. The Precision-Recall curves of the experiment of RGB-HHA inputs are presented in Figure 8. The visualizations of RGB, HHA, and RGB+HHA results are presented in Figure 12, Figure 13, and Fig-ure 14, respectively."}]}