{"title": "Evaluating Vision-Language Models for Zero-Shot Detection, Classification, and Association of Motorcycles, Passengers, and Helmets", "authors": ["Lucas Choi", "Ross Greer"], "abstract": "Motorcycle accidents pose significant risks, particularly when riders and passengers do not wear helmets. This study evaluates the efficacy of an advanced vision-language foundation model, OWLv2, in detecting and classifying various helmet-wearing statuses of motorcycle occupants using video data. We extend the dataset provided by the CVPR AI City Challenge and employ a cascaded model approach for detection and classification tasks, integrating OWLv2 and CNN models. The results highlight the potential of zero-shot learning to address challenges arising from incomplete and biased training datasets, demonstrating the usage of such models in detecting motorcycles, helmet usage, and occupant positions under varied conditions. We have achieved an average precision of 0.5324 for helmet detection and provided precision-recall curves detailing the detection and classification performance. Despite limitations such as low-resolution data and poor visibility, our research shows promising advancements in automated vehicle safety and traffic safety enforcement systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Motorcycle accidents are frequent causes of injury and death worldwide, especially for occupants not wearing helmets [1]\u2013[4]. Specifically, in India, in 2022, two-wheeler deaths accounted for 44% of total road fatalities with 74,897 deaths, the highest out of all modes of transport\u00b9. Helmets are 35% effective in reducing the risk of Abbreviated Injury Scale 3+ head injuries [5]. Additionally, 4 people die every hour in India because they do not wear a helmet\u00b2, causing 44,666 deaths in 2019 [6].\nSection 129 of the Motor Vehicles Act in India states that \"Every person on a motorcycle of any class or description shall, while in a public place, wear protective headgear conforming to such standards as may be prescribed by the Central Government.\" [7] Despite regulations mandating helmet use, compliance is inconsistent, leading to preventable injuries.\nIterations of the CVPR AI City Challenge [8] have prompted researchers to address this challenge, stating \u201cMotorcycles are one of the most popular modes of transportation, particularly in developing countries such as India. Due to lesser protection compared to cars and other standard vehicles, motorcycle riders are exposed to a greater risk of crashes. Therefore, wearing helmets for motorcycle riders is mandatory as per traffic rules, and automatic detection of motorcyclists without helmets is one of the critical tasks in enforcing strict regulatory traffic safety measures.\u201d We suggest that, besides the enforcement of traffic safety measures, there is also an even greater benefit in the ability of IoT-style communication between infrastructure or egocentric perception devices. Such systems could detect the presence of motorcyclists and passengers (with or without helmets) and alert the surrounding vehicles whose drivers (autonomous or human) may be otherwise unaware of the vulnerable road users in their proximity [9].\nAccordingly, to perceive holistic information about motor- cycles and their occupants in a scene, the goal task we evaluate in this paper is the detection and classification of the following objects in every frame of a large video dataset:\n1) Motorcycle,\n2) Drivers wearing helmets,\n3) Drivers not wearing helmets,\n4) Passengers wearing helmets,\n5) Passengers not wearing helmets,\n6) 2nd Passengers wearing helmets,\n7) 2nd Passengers not wearing helmets,\n8) Children sitting in front of the driver wearing helmets,\n9) Children sitting in front of the driver not wearing helmets.\nIn this research dataset, these scenes are captured by infrastructure-mounted cameras, though the same models can also be applied to egocentric views. This is especially the case given the zero-shot learning approaches we take, which do not require specific-view training data to be applied. We show sample data of these classes in Figure 1.\nWith many data-driven applications, a common challenge is the ability of a training set to adequately represent the diversity of instances that appear in the real world [10], [11]. For this reason, data-driven methods excel when given the most data possible, as this increases the likelihood of learning similar patterns to a real-world instance. To this end, we create a method that extends beyond the dataset presented by Shuo et al. [8] by employing a pre-trained vision-language foundation"}, {"title": "II. RELATED RESEARCH", "content": "Conventional machine learning object detection algorithms rely on manual annotations and specialized algorithms, which can be time-consuming and resource-intensive to label, especially as the models are limited to learning from provided datasets. Moreover, these methods often lack the flexibility to adapt to new environments or variations in helmet designs [13].\nFoundation models, with billions of parameters trained on enormous collections of information, have recently led to effective zero-shot techniques for a variety of tasks [14], where a learned model can provide strong performance on datasets unseen during training [15]. One such foundation model is OWL-ViT [16]; OWL stands for \u201copen-world localization\", referring to this model's ability to function in an \"open\" world (i.e., non-rigidly specified set of expected classes). The ViT portion of OWL-ViT refers to the Vision Transformer, an architecture that applies the attention mechanism to images instead of the prior standard of convolution. The OWL family of models uses contrastive learning between batches of image patch encodings and text embeddings, with image patch encodings producing proposed classes and proposed bounding boxes, and treating detection as a bipartite matching problem between these decoded image classes and bounding boxes, as introduced in the Detection Transformer (DETR) technique [17]\u2013[19]. Together, these methods were shown to be effective in zero-shot object detection (identifying a bounding box around desired classes of interest within an image). This method was refined and scaled up using self- training as OWLv2 [12], whereby pseudo-box annotations are provided from an existing detector, and it is this further-trained model that we use in the method shared in this research.\nFor the same application of detecting and classifying the given objects detailed in Section I, many different approaches have been tried in the previous AI City Challenges; in the 2023 AI City Challenge [20], Tran et al. [21] used YOLOv8 for a score of 0.7754 for the mean average precision (mAP). Cui et al. [22] used DETA [23] ensemble and Detectron2 for a mAP of 0.8340. In the 2024 AI City Challenge [8], mainly transformer models combined with ensemble techniques were used. Vo et al. [24] used Co-DETR [25] with a Minority Optimizer for class imbalance and a Virtual Expander for a mAP of 0.4860. Chen et al. [26] used a DETA and DETR fusion model for a score of 0.4824 mAP."}, {"title": "III. ALGORITHMS FOR IMAGE PROCESSING WITH VISION-LANGUAGE DETECTION", "content": "To address the challenges of accurately detecting and classifying motorcycles, their passengers, and helmet usage, we developed a cascading detection algorithm for OWLv2. Furthermore, due to OWLv2's shortcomings, we employed an AlexNet for the seat classification task. This section outlines our cascading detection algorithm using OWLv2 and discusses our approach for the seat classification task.\nWe first note that there are abstract classes that relate the target classes to one another; for example, \u201cmotorcycle\" and \"person\" are the abstract classes represented in the data scheme, where \u201cperson\u201d can be further classified based on the attributes of helmet-wearing and seating position. Due to this, our first goal is to detect these high-level classes. Further, we know that there is no driver or passenger without a motorcycle, so we only detect \"person\" in association with a particular motorcycle instance.\nOur detection algorithm, illustrated in Figure 2, begins with a detection stage. We provide a scene image (resized to 960 by 960 pixels and values normalized in [0, 1], relative to the size of each individual image in the batch) as input to OWLv2 along with the text \"motorcycle\". The CLIPTokenizer, from [15], encodes the text to be wrapped by the processor with the normalized image.\nTo detect the person instances on the motorcycles, we expand the re-scaled bounding box by 50 pixels on the left, right, and top sides to encapsulate any person instances surrounding the motorcycle. Using the expanded box, we crop the original image and run the OWLv2 model over this cropped image with the prompt \"person\" to detect person instances.\nThe algorithm's subsequent step is to perform the next level of detection, focusing on helmets, by cropping each person instance and running the OWLv2 over the cropped image with a text input of \"helmet\". Because our task is to classify each person based on whether they are wearing a helmet and not necessarily to detect the helmet itself, we store the boolean result of this detection as an attribute of the person (rather than noting the bounding box).\nWe note that there is a general difficulty of OWLv2 in differentiating a person's semantic position on the motorcycle (such as driver, passenger, second passenger, etc.), as noted in Section IV. For this particular portion of the task, we take a supervised learning approach. We seek to provide each person detected on the motorcycle with an attribute of location between the positions enumerated in the introduction.\nTherefore, we use a neural network (a variant of AlexNet [27], with a final layer output of four) to classify the seating position on a motorcycle of the person instances detected with OWLv2.\nDue to the use of the AlexNet in the seat position clas- sification task, we recognize that the whole process is not completely zero-shot. It is rather a hybrid of zero-shot learning and supervised learning, with zero-shot for the association and detection of motorcycles, their passengers, and their helmet status, and supervised learning for the seat classification of the passengers. In this way, the methods in this paper actually address four tasks (motorcycle detection, person detection, helmet detection, and seat classification); three of these are solved in a zero-shot manner, and we include a learned approach to seat classification as this is a relevant safety task that should also be considered in conjunction.\nIn total, this algorithmic sequence of tasks can provide detections of motorcycles, associated people, their positions, and their helmet status for each image in a video."}, {"title": "IV. EXPERIMENTAL METHOD AND EVALUATION", "content": "Using the cascaded object and attribution detection algo- rithm detailed in the previous section, we performed detection on the dataset of 100 videos provided by [8], with further implementation details described in this section.\nWe first conducted the motorcycle and person detection of our cascaded detection process as described in Section III. The threshold for OWLv2 is a confidence threshold, meaning it is the minimum confidence score that a predicted bounding box must have to be considered a valid detection. The OWLv2 will discard any detection with a confidence score below the given threshold. The confidence score is calculated as logits on a per-detection basis.\nWe performed the cascaded detection process with thresholds of 0.1 to 0.7 on the OWLv2 to examine the sensitivity of precision and recall to thresholds. 0.7 was chosen as the last threshold, as OWLv2 made no detections with a threshold higher than 0.7. Using the output of our detections, we calculated the precision and recall at each confidence threshold.\nTo evaluate the ability of OWLv2 to classify a passenger's helmet status, regardless of error in upstream person detection, we detected helmets within the ground truth bounding boxes of passengers to classify the passenger's helmet status. As in the previous detections, we experimented with a threshold of 0.05 to 0.7.\nWhen performing the seat classification based on the person detection, we attempted to determine a passenger's seat with OWLV2, first using the text prompts provided by the labels in the dataset, such as \"passenger 1\" and \"child passenger.\" However, with these prompts, OWLv2 tended to miss some passengers and mislabel the people. Assuming this was due to the inputs, we attempted more specific prompts such as \u201cchild in front of driver\u201d or \u201cpassenger behind driver\u201d. Nevertheless, this also yielded similar results. We hypothesize that the prompt inputs were not the determining factor of OWLv2's failure to detect and differentiate the different people on a motorcycle, showing possible shortcomings of model training for this particular type of task. Furthermore, the task of classifying people based on their relative location to other people and the motorcycle may be too specific for the model.\nAfter observing OWLv2's shortcomings with our intersec- tion data, we used a modified AlexNet for the seat classifica- tion subtask [27]. We modified the last layer of the AlexNet from 10 outputs to 4 to suit our task.\nWe used an approximate inverse class frequency to over- come the severe class imbalance in the dataset as shown in Table I. At first, we tested the weighting of 1.147, 7.908, 785.229, and 2093.944, calculated by inverse class weighting. However, this was insufficient, as the model did not appear to learn the child class and appeared to over-favor the driver class. Therefore, we incrementally increased the weighting of the classes of passenger1, passenger2, and child passenger relative to the driver, updating the previously mentioned weights to 1, 10, 800, and 3000, respectively.\nWe split the data 70/15/15 for the training, testing, and validation. We used a cross-entropy loss. Finally, we trained the model using a learning rate of 0.0001 for 100 epochs. Then, we used the model with the lowest loss on the validation set to make inferences on the test set."}, {"title": "B. Results", "content": "Our OWLv2 detected motorcycles with accuracies shown in Table II and detected persons with accuracies as shown in Table III. For motorcycle detection, the average precision is 0.4122, calculated by the area under the curve of Figure 4, and for person detection, the average precision is 0.3561, obtained from Figure 5.\nOur helmet-status classification was done through helmet detection with OWLv2 on the provided ground truth bounding boxes of passengers, with a representative classification based on the helmet's presence or absence. This resulted in the precisions and recalls in Table IV, tested over multiple thresholds, resulting in an average precision of 0.5324, as further illustrated in Figure 6. A naive classifier, which always predicts the rider to be wearing a helmet, would have a precision of 69.7% and a trivial recall of 100% based on the ground truth data described in Section IV A; at all thresholds, our precision is higher than the naive classifier, showing a reduction in false positives and negatives.\nThe IoU in Tables II, III, and IV stands for intersection over union, which is the metric for evaluating the accuracy of a predicted bounding box. The IoU is calculated as follows:\nIoU = $\\frac{Area\\ of\\ Overlap}{Area\\ of\\ Union}$ = $\\frac{ANB}{AUB}$ or IOU = $\\frac{ANB}{Area\\ of\\ Union}$, where A stands for the predicted bounding box and B stands for the ground truth bounding box. In our evaluation, for a given detected bounding box, if the IoU with the ground truth is greater than or equal to 0.5, then the detection is considered a \u201ctrue positive\".\nFinally, our neural network's seat classification achieved an accuracy of 95.17% on the validation set, with the classification results on the test set displayed in Figure 3. We note that the severe class imbalance does leave the child passenger class unsuccessfully classified, though this does not have much impact on the accuracy metric. This reveals an insufficiency in the model learning and cautions us of evaluating performance for such an imbalanced dataset without examining class performance in the confusion matrix."}, {"title": "C. Sensitivity of Helmet Detection to OWLVIT Detection Threshold", "content": "Due to the nature of the dataset, it is important to find an optimal threshold in our detections. As many videos within the data are often unclear, too high of a threshold may omit the detections within the unclear regions of the data. On the contrary, too low of a threshold may yield unrelated detections, such as detecting a bike as a motorcycle. Therefore, an optimal threshold between these two extremes is necessary to achieve the highest accuracy. We show the results of exploring multiple thresholds in our research and note that continual tuning will be important when applying these methods to additional datasets or tasks."}, {"title": "V. CONCLUDING REMARKS AND FUTURE RESEARCH", "content": "Zero-shot learning demonstrates the potential of this appli- cation as it can overcome some limitations of incomplete and biased training datasets. As noted, the provided dataset lacks instances of child passengers with helmets and second pas- sengers with helmets, making training traditional supervised- learning models difficult. Zero-shot learning leverages pre- training on diverse data, classifying unseen instances more accurately. With further fine-tuning and training, zero-shot learning has a strong potential for accurately handling real- world data.\nSeveral sources of error are demonstrated in the helmet classification. The ground truth bounding boxes do not always encompass the whole person. Many boxes were taken from the lower half of their body as they entered the frame of the video. Additionally, overlapping bounding boxes with passengers and drivers, where drivers have helmets on, but the passengers do not, often confuses the OWLV2, claiming that it had detected the helmet in both cases. This also impacts person detection, as the OWLV2 cannot detect the passenger due to the driver obstructing most of the passenger's body.\nFurthermore, AlexNet and the OWLv2 foundation model were challenged when faced with 'real-world' noise-filled scenarios. Many of the videos provided in the dataset had very low resolutions, with blurred-out time stamps at the top left and bottom right obstructing the view of motorcycles. Data collected during the night further reduced visibility, as the headlights of motorcycles and cars create a blurry effect throughout the video. The regular poor conditions of fog or heavy air pollution compounded these factors, as shown in Figure 3. All of these various aspects made image detection and classification challenging and sometimes near impossible.\nFuture investigations are necessary to apply zero-shot learn- ing in the real world. In this application, accurately detecting helmets will help to raise awareness as the detections will provide a more accurate measure of the frequency at which people do not wear helmets, as well as assist in enforcing the wearing of helmets. The ability to respond to unanticipated data is crucial for safety systems, as real-world scenarios often surpass the scope of any pre-existing dataset. Ongoing development and refinement of the model will be imperative to fully harness their potential in practical safety systems. Our future research will focus on enhancing the accuracy, robustness, and consistency of zero-shot learning models in our detections.\nTo handle noisy data, pre-training the OWLv2 on further diverse datasets will allow it to better handle uncertain de- tections. Furthermore, preprocessing the data will mitigate some of these issues. Moreover, a possible improvement is the further integration of AlexNet and OWLv2 for seat classification. A hybrid approach using these two models will involve ensemble methods to balance their strengths for a more accurate result [28].\nFinally, we will address task-specific shortcomings. For example, at times, the OWLv2 model fails to get the bounding box over the whole person, specifically the head, which is especially crucial for this task. A primary focus will be improving the model's ability to localize and classify these critical areas accurately.\nDespite current limitations and imbalances in data, this re- search shows the potential of foundation models and language- based prompting toward the zero-shot handling of important safety challenges. We address all components of the AI City Challenge Helmet Detection and Occupancy tasks, showing possibilities for the OWL model to address the sub-tasks of detection and association of vehicles, their occupants, and safety state information. This application has the potential to extend upon I2V communication. The detections from the infrastructure point of view can be sent to the vehicle's egocentric perception in order to alert drivers of the presence of motorcycles for safer intersection driving."}]}