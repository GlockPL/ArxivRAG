{"title": "Phi-3 Safety Post-Training: Aligning Language Models with a \"Break-Fix\" Cycle", "authors": ["Microsoft"], "abstract": "Recent innovations in language model training have demonstrated that it is possible to create highly performant models that are small enough to run on a smartphone. As these models are deployed in an increasing number of domains, it is critical to ensure that they are aligned with human preferences and safety considerations. In this report, we present our methodology for safety aligning the Phi-3 series of language models. We utilized a \"break-fix\" cycle, performing multiple rounds of dataset curation, safety post-training, benchmarking, red teaming, and vulnerability identification to cover a variety of harm areas in both single and multi-turn scenarios. Our results indicate that this approach iteratively improved the performance of the Phi-3 models across a wide range of responsible AI benchmarks.", "sections": [{"title": "Introduction", "content": "Given the computational cost associated with large language models (LLMs), there is increasing interest in developing smaller models with lower compute and memory requirements. Recent research has demonstrated that it is possible to create performant small language models (SLMs) by training on highly curated and synthetic datasets [GZA+23, LBE+23, JBA+23]. Microsoft recently released the Phi-3 series of SLMs, including Phi-3-mini (3.8B), Phi-3-small (7B), and Phi-3-medium (14B). For example, Phi-3-mini is small enough to run on a smartphone but achieves 69% on MMLU and 8.38 on MT-Bench, making it competitive with Mixtral 8x7B and GPT-3.5. [AJA+24].\nOpen-source SLMs enable an exciting array of on-device generative AI applications. At the same time, the proliferation of language models in an increasing number of domains underscores the importance of aligning models to human preferences and safety considerations. In this report, we present our approach to aligning the Phi-3 series of language models. We utilized a \"break-fix\" cycle that relies on multiple rounds of vulnerability identification and safety post-training. In the sections that follow, we detail our methodology, quantitative benchmarks, and red teaming results."}, {"title": "Safety Alignment", "content": "Microsoft adopted an iterative approach to safety post-training that consisted of five main stages:\n1. Safety Dataset Curation: We utilized existing publicly available datasets with various modifications and generated additional datasets based on feedback from the AI Red Team for further safety post-training.\n2. Safety Post-Training: The safety datasets mixed with standard preference datasets were used in both the supervised fine-tuning (SFT) and direct preference optimization (DPO) [RSM+23] stages."}, {"title": "Approach", "content": "Microsoft adopted an iterative approach to safety post-training that consisted of five main stages:\n1. Safety Dataset Curation: We utilized existing publicly available datasets with various modifications and generated additional datasets based on feedback from the AI Red Team for further safety post-training.\n2. Safety Post-Training: The safety datasets mixed with standard preference datasets were used in both the supervised fine-tuning (SFT) and direct preference optimization (DPO) [RSM+23] stages.\n3. Quantitative and Qualitative RAI Evaluations: A wide spectrum of responsible AI (RAI) evaluations, described in detail below, were considered to select release candidates (RCs) to share with the AI Red Team.\n4. AI Red Teaming: The release candidates were shared with a centralized and independent AI Red Team (AIRT), which leveraged a variety of adversarial techniques to probe models for harmful content. The red teaming strategy is described below.\n5. Vulnerability Identification: Based on the RAI evaluations and AIRT findings, potential vulnerabilities are identified to inform further safety post-training.\nAs illustrated by Figure 1, we repeated this cycle multiple times and gradually fine-tuned the Phi-3 models to generate safe responses in a variety of contexts. We found that this iterative \"break-fix\" approach made it possible to mitigate many more risks than what can typically be achieved by a single fine-tuning job. In addition to RAI benchmarks, we monitored multiple performance metrics to ensure that safety post-training did not degrade the quality of generated text. The datasets, red teaming strategies, and evaluation benchmarks used are detailed in the sections below."}, {"title": "Safety Alignment", "content": "For safety post-training of the Phi-3 models, we used a combination of open-source and in-house datasets. To improve the quality and effectiveness of the open-source datasets including [BJN+22] and [JLD+23], we used a variety of approaches including regenerating responses with GPT-4 and applying the instruction conversion method outlined in [BSA+24]. While the open-source datasets covered a wide range of safety aspects, in-house datasets were curated to mitigate specific risks reported by AIRT as fine-tuning or preference optimization datasets depending on their effectiveness."}, {"title": "Red Teaming", "content": "The Phi-3 release candidates were shared with a centralized Microsoft AI Red Team (AIRT), which leveraged a variety of techniques to test the models for responsible AI (RAI) risks across a range of categories. More specifically, the AIRT probed Phi-3 models for harmful content using both single-turn prompts and multi-turn conversations. These strategies were further split into \"low-skilled adversary\" and \"intermediate adversary\" personas, as summarized in Table 1. These personas were scoped based on the most common ways in which users might elicit harmful content from models. A low-skilled adversary was defined as a typical chatbot user who tries to generate harmful content by asking for it directly, while an intermediate adversary actively attempts to subvert a model's safety guardrails using, for example, basic encodings and jailbreaks. To gauge the risk posed by Phi-3 models in comparison to open-source equivalents, the AIRT performed the same testing on Gemma-7B [TMH+24], Mixtral-8x7B [JSR+24], and Llama-3-In [TLI+23].\nThe AIRT operated independently of the safety post-training team and probed for harmful content across a range of categories, including content related to current events, phishing and cybersecurity, fairness and bias, hate speech, sexual content, and violence. Note that these harm categories were selected in accordance with Microsoft's Responsible AI Standard [Mic22] and were not necessarily the same as those covered by the preference datasets used for safety post-training.\nTo red team the Phi-3 models at scale and cover as much of the risk surface as possible, the AIRT utilized PyRIT: Python Risk Identification Toolkit [LML+24].\u00b9 PyRIT is an open-source project that provides automation to support prompt generation, prompt conversion (e.g., to apply encodings and jailbreaks), response scoring, and even multi-turn conversations driven by an attacker LLM. To verify the accuracy of PyRIT automation, the AIRT manually checked the scored results and made corrections where necessary. Note that PyRIT was developed specifically to support red teaming operations and is separate from automation used to calculate responsible AI benchmarks. For the intermediate adversary multi-turn scenario, AIRT manually applied strategies like Crescendo, which uses seemingly benign prompts and gradually escalates the conversation to jailbreak a model [RSE24]."}, {"title": "Safety Evaluation", "content": "In the next section, we present the results of the final Phi-3 models on a range of responsible AI benchmarks, along with the results achieved by comparable open-source models. We utilized these benchmarks to track overall safety performance and compare release candidates throughout the safety post-training process. It is important to note, however, that a model's full risk profile can never be fully captured by a single set of metrics. In contrast with safety benchmarks, red teaming targets emerging harm areas, leverages the latest adversarial techniques, and can address ambiguous scenarios in which a model's behavior might be interpreted in multiple ways. Importantly, red teaming centers the human elements of AI safety and can help answer questions related to how users might feel while interacting with a model. For example, in what scenarios might the model make users feel uncomfortable? Is the model at risk of providing dangerous or harmful advice? Are users likely to trust the model? During the red teaming phase of the break-fix cycle, questions like these played an important role in deciding where to focus further safety post-training efforts."}, {"title": "RAI Safety Benchmarks", "content": "Throughout the safety alignment process, we used a range of responsible AI (RAI) benchmarks \u2013 including both public datasets and Microsoft internal measurements \u2013 to track the safety performance of Phi-3 models and compare release candidates. In this section, we explain these benchmarks in detail and present the results achieved by the final release candidates, as well as those achieved by Mistral-7B, Gemma-7B, and Llama-3-In, for comparison."}, {"title": "Internal Automated Measurement", "content": "We used one of Microsoft's automated measurement systems that leverages highly capable models like GPT-4 to simulate multi-turn conversations between adversarial AI agents and a target model [MHJ+23]. Among diverse conversation templates available, we ran experiments for the five scenarios below.\n\u2022 Grounding: Asking the model to reason based on the information provided in prompts.\n\u2022 3rd Party Content: Asking a model to provide protected third-party content.\n\u2022 Harmful Content Continuation: Asking the model to generate harmful content.\n\u2022 Harmful Content Summarization: Asking the model to summarize harmful content.\n\u2022 Jailbreak: Asking the model to bypass behavior protocols learned during safety post-training.\nThis technique probes for harmful content by making multiple requests in a multi-turn setting or by posing hypothetical scenarios that make the model more likely to respond. (e.g., asking the model to generate harmful content \"for a research purpose\")."}, {"title": "XSTest", "content": "XSTest is a public dataset of 250 safe prompts across ten prompt categories (e.g., definitions, historical events, etc.) that well-calibrated models should comply with, and 200 unsafe prompts that most general-purpose models should refuse [RKV+24].\nThe following two refusal metrics are computed in this benchmark:\n\u2022 Inappropriate Prompt Refusal Rate (IPRR): Measures the rate that the model refuses to answer inappropriate or harmful prompts (higher is better).\n\u2022 Valid Prompt Refusal Rate (VPRR): Measures the rate that the model refuses to answer appropriate or innocuous prompts (lower is better).\nTable 3 shows IPRR and VPRR values for the Phi-3 and baseline models. We observed that higher IPRR values are often associated with higher VPRR values. In other words, models that are more likely to refuse harmful prompts are also more likely to refuse harmless prompts. This behavior is well known in responsible AI research and is often described as a tradeoff between helpfulness (higher VPRR) and harmlessness (higher IPRR). In the table below, we observe this tradeoff across all models tested. More specifically, we see that Phi-3-small and Gemma achieve similar balances between IPRR and VPRR. In addition, Llama-3-In is comparable to, but slightly outperforms, Phi-3-medium."}, {"title": "Decoding Trust", "content": "Decoding Trust is a comprehensive trustworthiness evaluation methodology which considers diverse risks including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness [WCP+24]. In our DecodingTrust benchmarks, we covered all of these risks except for toxicity, which is separately covered by the ToxiGen benchmark."}, {"title": "ToxiGen", "content": "ToxiGen is a large-scale machine generated dataset for adversarial and implicit hate speech detection [HGP+22]. As mentioned above, we used the ToxiGen benchmark in favor of the toxicity category in DecodingTrust because ToxiGen is a richer dataset with more prompts (274K) than DecodingTrust toxicity (under 100K). A high ToxiGen score means that the model can detect harmfulness in prompts well. The scores are shown in Table 5. All three Phi-3 model variants outperformed Mistral and Gemma."}, {"title": "Iterative Safety Alignment", "content": "The RAI benchmark scores reported above reflect the performance of the final Phi-3-mini, Phi-3-small, and Phi-3-medium release candidates. However, multiple iterations of safety post-training, red teaming, and vulnerability identification were required to achieve the best results. Given the vast number of scenarios that users might encounter, we found that this iterative approach, as opposed to a single fine-tuning job, was necessary to identify and mitigate a realistic range of real-world risks.\nTo further quantify the overall improvement in safety alignment, we evaluated the Phi-3 models before and after completing several rounds of the \"break-fix\" cycle on a dataset of prompts used by the AI Red Team. In Figure 2, we plot the percentage of harmful responses generated by models with and without safety alignment across several harm categories. On average, we observe a 75% reduction in the amount of harmful content generated, which indicates the efficacy of the overall \"break-fix\" approach."}, {"title": "Responsible AI Considerations for Developers", "content": "While the Phi-3 models benefit from a robust safety post-training approach, developers should consider how to adapt models with further fine-tuning to their specific use case and safety requirements. In addition to fine-tuning, developers should explore building or adopting additional safety-related tools and approaches to ensure that model outputs are appropriate for their context. These may include safety classifiers run on inputs or outputs, prompt engineering techniques, or other guidance to end-users about how to interpret or use model outputs appropriately. Further guidance and open-source tools are available via Microsoft's Responsible AI Toolbox repository.2"}, {"title": "Responsible Downstream Development", "content": "While the Phi-3 models benefit from a robust safety post-training approach, developers should consider how to adapt models with further fine-tuning to their specific use case and safety requirements. In addition to fine-tuning, developers should explore building or adopting additional safety-related tools and approaches to ensure that model outputs are appropriate for their context. These may include safety classifiers run on inputs or outputs, prompt engineering techniques, or other guidance to end-users about how to interpret or use model outputs appropriately. Further guidance and open-source tools are available via Microsoft's Responsible AI Toolbox repository.2"}, {"title": "Additional Considerations", "content": "At Microsoft, we are committed to advancing the state of the art in AI and ensuring that our AI products and services are safe, secure, and trustworthy. Language models have great potential to enable new capabilities and benefit society by driving an open innovation cycle and enabling an extensive value chain built on open-source projects. These include direct and indirect benefits, such as advancing AI safety and security, fostering global collaboration and academic research, and inviting greater participation in the development of AI systems.\nWe have also considered the potential risks and believe the release of the Phi-3 models does not have a meaningful impact on marginal risk of the AI ecosystem due to the availability of larger, advanced AI models and other open information. Based on the evaluations and safety post-training detailed in this white paper, we have assessed the potential benefits of open innovation and research will outweigh potential risks specific to this model.\nThere are specific areas our team has considered and taken steps to address, but we have not designed or evaluated these models for every potential downstream use case. Developers should take into account common limitations of this technology as they select use cases, as well as conduct evaluations and implement appropriate safeguards in additional fine-tuning and deployment stages. Developers have a responsibility to apply responsible AI best practices and ensure that a specific use case complies with relevant laws. Important areas for consideration include:\n\u2022 Allocation: While we have implemented mitigations in post-training to address potential biases, given the known limitations of language models, these models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities without performing further assessments and applying additional debiasing techniques. For example, in conferring legal rights or an individual's access to credit, education, employment, healthcare, housing, insurance, social welfare benefits, services, or opportunities, or the terms on which they are provided.\n\u2022 High-Risk Scenarios: Developers should also assess the suitability of using models in high-risk scenarios in situations where unfair, unreliable, or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (e.g., legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n\u2022 Misinformation: Language models may produce inaccurate information. Developers should consider and adopt best practices for transparency and disclosure to inform end-users that they are interacting with an AI system. As part of applications, developers can also build mechanisms for feedback as well as pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (i.e., \"RAG\").\n\u2022 Generation of Harmful Content: While safety post-training has reduced the likelihood that the model will generate some forms of harmful content, developers will need to make assessments based on their context and use available safety classifiers or custom solutions based on their use cases.\n\u2022 Privacy: Developers should be aware of and adhere to privacy laws in the jurisdictions where they operate and deploy applications.\n\u2022 Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations."}, {"title": "Conclusion", "content": "In this report, we presented our approach to safety aligning the Phi-3 series of language models. We adopted an iterative \u201cbreak-fix\" approach by performing multiple rounds of dataset curation, post-training with DPO, responsible AI benchmarking, red teaming, and vulnerability identification. Safety alignment is a challenging and open-ended task, but our results indicate that this cycle significantly reduced the amount of harmful content generated by the Phi-3 models in a range of scenarios. Nonetheless, the Phi-3 models are susceptible to the same fundamental limitations as any modern language model, and we hope that the responsible AI considerations outlined in this report will encourage users to think carefully about additional safety mitigations that may be necessary for their specific use cases."}]}