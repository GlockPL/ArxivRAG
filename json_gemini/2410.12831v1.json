{"title": "Segment as You Wish: Free-Form Language-Based Segmentation for Medical Images", "authors": ["Longchao Da", "Rui Wang", "Xiaojian Xu", "Parminder Bhatia", "Taha Kass-Hout", "Hua Wei", "Cao Xiao"], "abstract": "Medical imaging is crucial for diagnosing a patient's health condition, and accurate segmentation of these images is essential for isolating regions of interest to ensure precise diagnosis and treatment planning. Existing methods primarily rely on bounding boxes or point-based prompts, while few have explored text-related prompts, despite clinicians often describing their observations and instructions in natural language. To address this gap, we first propose a RAG-based free-form text prompt generator, that leverages the domain corpus to generate diverse and realistic descriptions. Then, we introduce FLanS, a novel medical image segmentation model that handles various free-form text prompts, including professional anatomy-informed queries, anatomy-agnostic position-driven queries, and anatomy-agnostic size-driven queries. Additionally, our model also incorporates a symmetry-aware canonicalization module to ensure consistent, accurate segmentations across varying scan orientations and reduce confusion between the anatomical position of an organ and its appearance in the scan. FLans is trained on a large-scale dataset of over 100k medical images from 7 public datasets. Comprehensive experiments demonstrate the model's superior language understanding and segmentation precision, along with a deep comprehension of the relationship between them, outperforming SOTA baselines on both in-domain and out-of-domain datasets.", "sections": [{"title": "1 Introduction", "content": "Medical imaging is crucial in healthcare, providing clinicians with the ability to visualize and assess anatomical structures for both diagnosis and treatment. Organ segmentation is vital for numerous clinical applications, including surgical planning and disease progression monitoring [55, 14, 48]. However, accurately segmenting organs and tissues from these medical images, i.e., medical image segmentation (MIS), remains a significant challenge due to the variability in patient positioning, imaging techniques, and anatomical structures [42, 63]. Recent advancements in large foundation models, such as Segment Anything Model (SAM) [28] and MedSAM [62], have shown promise in achieving more accurate and faster MIS. These models often require the users to input a predefined category name, a box, or a point as a prompt. However, in real-world scenarios, clinicians often rely on natural language commands to interact with medical images, such as \u201cHighlight the right kidney\" or \"Segment the largest organ\". An accurate segmentation model with flexible text comprehension capability is therefore essential for a wide range of clinical applications.\nThe first challenge lies in the development of a segmentation model that can handle text prompts, offering greater flexibility and adaptability in real-world clinical environments. Unlike traditional"}, {"title": "2 Related Work", "content": "Medical Image Segmentation Medical image segmentation (MIS) aims at accurately delineating anatomical structures in medical images. Traditionally, MIS methods tend to segment the correct regions from an image that accurately reflects the input query [2]. The researchers improve the performance of MIS methods by either optimizing segmentation network design for improving feature representations [8, 69, 7, 20], or improving optimization strategies, e.g., proposing better loss functions to address class imbalance or refining uncertain pixels from high-frequency regions to improve the segmentation quality [64, 49, 67]. However, they require a pre-known medical region from the user as an input for segmentation on where it is expected to be segmented and a precise match between the segment's name and the labels used in the training set, restricting their flexibility in real-world application. Another category of methods are SAM-based approaches [28, 38, 71] that mainly rely on the Bboxes or points as prompts for segmentation. While such methods do not need strict labels, they neglect the descriptive understanding of the image, revealing a deficiency in performing arbitrary description-based segmentation, in comparison, our method handles well in Labels, free-form Text prompts without losing ability of Point and Bbox, as shown in the Table. 1.\nText Prompt Segmentation Text prompt segmentation, also referred to as expression segmentation [23], utilizes natural language expressions as input prompts for image segmentation tasks, moving beyond the traditional reliance on class label annotations [34]. Early research in this area employed CNNs and RNNs for visual and textual feature extraction, which were later combined through feature fusion for segmentation [31]. The success of attention mechanisms further inspired a new line of work [50, 66]. More recently, transformer-based architectures have improved segmentation performance by using either carefully designed encoder-based feature fusion modules [16, 65, 27] or decoder-based approaches [58, 35, 13]. Among these, [70] introduced a text-promptable mask decoder for efficient surgical instrument segmentation. However, there is no existing work that has focused on free-form language segmentation for diagnosis-related medical imaging tasks as introduced in this work.\nEquivariant Medical Imaging Equivariant neural networks ensure that their features maintain specific transformation characteristics when the input undergoes transformations, and they have achieved significant success in various image processing tasks [11, 60, 10, 4]. Recently, equivariant networks have also been applied to medical imaging tasks, including classification [61], segmentation [29, 15, 21], reconstruction [6], and registration [3]. Equivariance can be incorporated in different ways, such as through parameter sharing [17], canonicalization [25], and frame averaging [43]. In our work, since we leverage a pretrained segmentation network, we achieve equivariance/invariance through canonicalization [41], which, unlike other methods, does not impose architectural constraints on the prediction network. It uses a simple equivariant canonicalization network that transforms the input to a canonical form before feeding it to an unconstrained prediction network. By leveraging this technique, the performance and robustness of our model are greatly enhanced."}, {"title": "3 Methodology", "content": "In this section, we introduce a paradigm to equip the segmentation model with free-form language understanding ability while maintaining high segmentation accuracy. It employs the RAG framework to generate text prompts based on real world clinical diagnosis records. The generated free-form"}, {"title": "3.1 The Retrieval Augmented Query Generator", "content": "To equip a MIS model M with language comprehension abilities, it is essential to prepare a suitable natural language query 4 corpus C in correspondence with the target organ label set $L = \\{l_1, l_2, ...l_n\\}$, where $l_1 = Liver, l_2 = Kidney$, etc., as in Appendix Fig. 9. Since manual annotation is time-consuming and can be biased towards individual linguistic habits, we designed a RAG-based free-form text prompts generator to automate this process. RAG allows pre-trained LLMs to retain their free-form language generation capabilities while incorporating domain-specific knowledge and style from the provided data source S. We collect corpus from three types of data sources. Two of these, $S_1 = Domain Expert$, $S_2 = Non-Expert$, serve as the corpus set to simulate various styles of descriptions for segmentation purposes,. The third source, $S_3 = Synthetic$, is directly generated by GPT-40 to imitate descriptions for segmentation purposes.\nFor $S_1 = Domain Expert$, we collected over 7,000 reports written by doctors and identified 4,990 clinical diagnosis records that are relevant to 24 labeled organs for this study. After de-identification, we embed such Electronic Medical Records (EMRs) into semantic vector space through Med-BERT [45], which outperforms the general language embedding models such as Bert or GPTs in the bioinformatics context understandings. Then, we built a retrieval augmented generation fashion generator agent G, as shown in Fig. 2, provided with medical domain corpus and practitioner's language usage preference. It retains the original LLM's natural language ability such as sentences extension and rephrasing. Finally, we construct a query prompt template: \"System: You are an agent able to query for segmenting label {Liver} in this {CT} scan. Please write the query sentence and output it.\" Given a label $l_i = Liver$, where $l \\in L$ regarding an arbitrary organ label with CT modality, the G produces a free-form query $q_i$, this query is taken as prompt in the later text-aware segment model training. E.g., \"(1) Examine this CT scan to determine the extent of hepatic damage present. (2) As the symptoms suggest cirrhosis, we should analyze the related part in this CT scan for any signs of the disease\". These retrieved augmented results show that the interested organ may not always be explicitly mentioned, but can be inferred based on terms like 'cirrhosis' and 'hepatic', which are all liver-specific illnesses in clinical practice."}, {"title": "3.2 Free-Form Language Segmentation for Medical Images", "content": "After generating a large corpus of free-form text queries via our retrieval augmented query generator, the next step is to align these queries with medical imaging segmentaion tasks.\nFor free-form anatomy-informed text prompts, the text encoder must learn embeddings that group similar organ segmentation intents together while clearly separating unrelated intents in distinct semantic clusters. We adopt the CLIP [44] as the foundation of text encoder for its capability of understanding semantics. Given a text prompt $p \\in P_r$ associated with the image x, the CLIP text encoder converts it into an embedding vector $t_p$ in a shared embedding space: $t_p = Encoder^T(p) \\in R^D$, where D is the dimensionality of the text embedding space. To further strengthen the model's ability to differentiate between organ segmentation, we introduce an intention head on top of the text embeddings by CLIP. This head is a linear layer $W_{cls} \\in R^{C \\times D}$, where C = 24 is the number of organ class. The intention logits $y_p$ are derived for each encoded vector $t_p$: $y_p = W_{cls}t_p + b_{cls}$. Given a corresponding medical image embedding $z^x$, we train the model by following loss function:\n$L = arg min_{\\{W_{cls}, b_{cls}, W_E, W_D,W_P\\}} \\frac{1}{|X|} \\sum_{x \\in X} \\frac{1}{|P_x|} \\sum_{p \\in P} [L_{Dice} (m^x, m^g) + L_{ce}(m^x, m^g) + L_{ce}(y_p, l_p)]$ (1)\nwhere $m^x = Decoder(z^x, t_p)$ and $m^g$ are predicted and ground truth masks. $l_p \\in [0, ..., 23]$ is the ground truth organ class for the prompt. $W_E$, $W_D$ and $W_P$ represent the image encoder, decoder and CLIP text encoder weights, respectively. We use both Dice loss $L_{Dice}$ and cross-entropy loss $L_{ce}$ for predicted masks. The classification loss $L_{ce}(y_p, l_p)$ encourages the model to correctly classify organs based on text prompts, ensuring the text embedding aligns with the intended organ class.\nFor anatomy-agnostic descriptions, which do not explicitly mention specific organs but instead focus on spatial attributes (e.g., \u201cleftmost\u201d, \u201clargest\"), the model must learn from spatial features $k_x \\in K$ to pair with the corresponding mask $m^g_x$ for every $x \\in X$. Anatomy-agnostic queries share the same embedding space as anatomy-informed queries, but $k_x$ is not necessarily associated with a specific organ. In this case, we use the same loss function as shown in Eq. 1 but without the last classification term.\""}, {"title": "3.3 Semantics-aware Canonicalization Learning", "content": "We incorporate roto-reflection symmetry [11] into our architecture for two key reasons: 1) Organs and anatomical structures can appear in various orientations and positions due to differences in patient positioning, imaging techniques, or inherent anatomical variations. Equivariance ensures that the model's segmentation adapts predictably to transformations of the input image. 2) We aim to ensure our model reliably interprets and segments organs that have positional terms in their names, such as \"left\" or \"right kidney\" from text prompts regardless of the scan's orientation, thereby enhancing the model's robustness and accuracy.\nFollowing [25, 41], we train a separate canonicalization network $h : X \\rightarrow G$, where X represents the medical image sample space, G represents the desired group, and h is equivariant to G. This network generates group elements that transform input images into canonical frames, standardizing the image orientation before applying the prediction function. The Eq. 2 shows how this canonicalization process maps the transformed input back to a common space where the segmentation prediction network p operates,\n$f(x) = p_{out}(h(x)) p(p_{in}(h^{-1}(x))x, t)$ (2)\nWhere p is the segmentation prediction network (composed of the Image Encoder and Mask Decoder in Figure 11), t is the text prompt embedding produced by our text encoder, and $p_{in}$ and $p_{out}$ are input and output representations. The segmented images or masks produced by p can be transformed back with $p_{out} (h(x))$ as needed. Without this transformation, f is invariant; otherwise, it is equivariant. Thus, the FLanS architecture visualized in Figure 11 is invariant. We use ESCNN [5] to build the canonicalization network. This approach has the advantage of removing the constraint from the main prediction network and placing it on the network that learns the canonicalization function. Appendix A provides a detailed introduction of symmetry and equivariant networks.\nAs the entire architecture achieves invariance or equivariance through canonicalization, the model produces the same segmentation or consistently transforms the segmentation according to the transformed input. In other words, the model always segment the same areas of interest regardless of the image's orientation with the same text prompt. For example, as long as the ground truth \u201cright kidney\" mask of a CT image has been shown to the model once, no matter how the orientation of the CT image and the location of the right kidney changes, the model will always segment the same area.\nHowever, without proper training, $h(x)$ might map different images to inconsistent canonical frames, causing a distribution shift in the inputs to the prediction network and affecting performance. Thus,"}, {"title": "3.4 Training Strategy", "content": "We employ a three-stage training strategy for FLanS: 1) Learning canonicalization: we train the canonicalization network independently using FLARE22 training samples applied with random transformations from the O(2) group. The network is optimized using MSE loss between the canonicalized samples and their original counterparts. This encourages the canonicalization network to map transformed samples back to their canonical orientations as seen in the FLARE22 dataset, preventing it from selecting arbitrary orientations that could degrade the performance of the prediction network. 2) Learning text-prompted segmentation: we train FLanS with the queries from Generator G as introduced in Section 3.1, without the canonicalization network on the original scans, using both anatomy-informed and anatomy-agnostic prompts. This ensures that the segmentation network learns to respond accurately to different types of prompts without interference from canonicalization and data augmentation. 3) Learning augmentation and alignment: In the final stage, we perform joint training on all scans, applied with random O(2) transformations. Since the canonicalization network may not always generate the exact canonical orientation the segmentation network is accustomed to in the beginning, this serves as a form of free augmentation for the segmentation networks. Over time, the canonicalization and segmentation networks align."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Datasets and Experiments Setup", "content": "Image Datasets To develop an effective organ segmentation model, we collected 1,437 CT scans from 7 public datasets, covering 24 partially labeled organs. Of these, 1,089 scans from MSD [1], BTCV [19], WORD [37], AbdomenCT-1K [40], FLARE22 [39], and CHAOS [26] are used for training. The rest 65 scans, consisting of 10% of the FLARE22 dataset (in-domain), the official validation set of WORD (in-domain), and the official test set of RAOS [36] (out-of-domain), were used to evaluate model performance. To standardize the quality and reduce domain gaps across datasets, we applied pre-processing techniques such as slice filtering and intensity scaling to all CT scans. The finalized dataset comprised 91,344 images for training and validation, and 9,873 for testing. Detailed information on the dataset statistics and pre-processing steps are in Appendix B.\nText Datasets Our text dataset was constructed using two types of queries: anatomy-agnostic and anatomy-informed. First, for each image, we identified organs corresponding to 6 representative positions: leftmost, rightmost, topmost, bottom, smallest, and largest. For each of these 6 position indicators, 100 anatomy-agnostic queries were generated, resulting in a set of 600 queries to serve as anatomy-agnostic segmentation prompts.5 Second, for each organ, we generated 480 anatomy-informed queries in an expertise-driven style using the RAG query generator. By combining both anatomy-agnostic and anatomy-informed queries, we formed a text dataset comprising 12,120 unique queries for model training. During testing, a comprehensive text set was used, containing both in-domain and out-of-domain queries. Specifically, we generated 30 RAG-generated expertise-style queries (25%, in-domain), 30 human-generated non-expertise-style queries (25%, out-of-domain), and 60 RAG-generated non-expertise-style queries (50%, out-of-domain) for each organ, forming a test set of 120 queries per organ and 2,880 queries across all organs. Detailed information on the generation of the text queries is in Appendix B.\nExperiment Setup All experiments were conducted on an AWS ml.p3dn.24xlarge instance equipped with 8 V100 GPUs, each with 32 GB of memory. We used a batch size of 16 and applied the CosineAnnealingLR learning rate scheduler, initializing the learning rate for all modules"}, {"title": "4.2 Anatomy-informed Segmentation", "content": "As for the baselines, the Universal Model [33] is the only published medical imaging foundation model that considers free-form text descriptions. This model integrates text description embeddings during training, while segmentation at the testing and inference stages is performed using organ"}, {"title": "4.3 Anatomy-Agnostic Segmentation", "content": "To evaluate our model's ability to understand anatomy-agnostic text prompts, we tested its segmentation performance using prompts that contain only positional or size-related information. To the best of our knowledge, no existing model is designed to handle anatomy-agnostic text prompts. Therefore, we chose state-of-the-art MedSAM [62] (SAM fine-tuned on medical imaging datasets) and the latest SAM2 [46] as baselines. However, instead of text prompts, these mod-els were provided with ground-truth organ Bboxes or point prompts. Our goal in this experiment is for FLanS to achieve comparable results to the baselines because FLanS is only given text prompts with positional or size information while the baselines are given the bounding box or point prompts of ground truth organ.\nAs shown in Table 3, FLanS the best or second-best performance across both in-domain and out-of-domain test sets. MedSAM performs well on the FLARE and WORD test sets but struggles on the RAOS test set due to the lack of training on that dataset. SAM2, when provided with bounding box prompts, consistently performs well across all test sets and demonstrates strong generalizability. However, its performance significantly degrades with point prompts, likely because medical scans lack the distinct edges present in the datasets SAM2 was originally trained on. The right panel of Fig. 5 visualizes the segmentations produced by the best baseline and FLanS, along with their corresponding anatomy-agnostic text prompts. It demonstrates that FLanS can reliably segment the correct organs based on the provided positional or size information, such as largest and lower right."}, {"title": "4.4 Ablation Study on the Model Architecture", "content": "We conducted an ablation study of FLanS on the FLARE22 dataset [39] to understand the contribution of each component, as presented in Table 4. Using an 80%-10%-10% train-validation-test split on the public FLARE22 training set, we evaluate the models' performance on both the held-out test set and a transformed test set, which contained samples applied with random transformations from O(2). Table 4 shows the prediction performance of FLanS and its variants, with components progressively removed. The results highlight that each component plays a crucial role in the model's"}, {"title": "4.5 Effective Understanding of Free-Form Text Prompts", "content": "Fig. 7 left visualizes the t-SNE embeddings of free-form text prompts corresponding to all 13 FLARE22 data classes, including liver, right kidney, spleen, and others. The text prompt encoder effectively clusters these prompts, revealing anatomically structured semantics. This demonstrates FLanS has a strong capability in understanding and distinguishing free-form text prompts."}, {"title": "4.6 Effectiveness of the Canonicalization", "content": "The right side of Fig. 7 shows the canonicalized CT scans from D4 and D8 canonicalization networks for a batch of original scans from the FLARE22 dataset applied with random transformations from O(2) group. As the group order of the canonicalization network increases, the scans become more consistently aligned to a particular canonical orientation. The canonicalization networks use a shallow architecture with three layers, a hidden dimension of 8, and a kernel size of 9, demonstrating that even a simple network with a larger kernel can effectively achieve canonicalization.\nMore importantly, applying canonicalization before feeding the scans into the main segmentation network and making the entire architecture equivariant or invarianthelps prevent confusion caused by positional terms in the organ name. A text-prompt segmentation model understands positional cues such as \"left\" vs \"right\" but it may get confused between the anatomical position and the organ's appearance in the scan. For example, Fig. 6 shows segmentation predictions from models with and without canonicalization, given the anatomy-informed text prompt, \"Highlight the right renal organ.\" Since the CT scan is not in the standard orientation, the right kidney appears on the left side of the image. Without canonicalization, the non-equivariant model incorrectly segments the left kidney,"}, {"title": "5 Conclusion", "content": "In this work, we presented FLanS, a novel medical image segmentation model capable of handling diverse free-form text prompts, including both anatomy-informed and anatomy-agnostic descriptions. By integrating equivariance, our model ensures accurate and consistent segmentation across varying scan orientations, addressing a critical challenge in medical imaging. We also developed a RAG query generator for both realistic and synthetic prompt generation, and trained FLanS on over 100k medical images from 7 public datasets, covering 24 organ categories. FLanS outperforms baselines in both in-domain and out-of-domain tests, demonstrating superior language understanding and segmentation accuracy. Future works including extend FLanS to multi-organ segmentation tasks and further enhance RAG generator with multimodal data."}, {"title": "A Equivariance and Symmetry", "content": "Equivariant neural networks are designed to explicitly incorporate symmetries that are present in the underlying data. Symmetries, often derived from first principles or domain knowledge, such as rotational or translational invariance, allow the network to process inputs in a way that is consistent with these transformations. This is particularly important when the ground truth functions respect such symmetries, as the incorporation of these properties can significantly enhance model performance and generalization.\nA group of symmetries or simply group is a set G together with a binary operation 0: $G \\times G\\rightarrow G$ called composition satisfying three properties: 1) identity: There is an element $1 \\in G$ such that $1 \\omicron g = g \\omicron 1 = g$ for all $g \\in G$; 2) associativity: $(g_1\\omicron g_2)\\omicron g_3 = g_1\\omicron(g_2\\omicron g_3)$ for all $g_1, g_2, g_3 \\in G$; 3) inverses if $g \\in G$, then there is an element $g^{-1} \\in G$ such that $g\\omicron g^{-1} = g_{-1} \\omicron g = 1$.\nExamples of groups include the dihedral groups $D_4$ (symmetries of a square) and $D_8$ (symmetries of an octagon), as well as the orthogonal group O(2), which represents all rotations and reflections in 2D space. Both $D_4$ and $D_8$ are discrete subgroups of O(2).\nA group representation defines how a group action transforms elements of a vector space by mapping group elements to linear transformations on that space. More specifically, a group representation of a group G on a vector space V is is a homomorphism: $p: G \\rightarrow GL(X)$, where GL(X) is the group of invertible linear transformations on V. This means for any $g_1, g_2 \\in G$, p is a linear transformation (often represented by a matrix) such that the group operation in G is preserved:\n$p(g_1g_2) = p(g_1)p(g_2)$ (3)\nFormally, a neural network is said to be equivariant to a group of transformations G if applying a transformation from the group to the input results in a corresponding transformation to the output. Mathematically, for a function f : X \u2192 Y to be G-equivariant, the following condition must hold:\n$f(p_{in}(g)(x)) = p_{out}(g)f(x)$ (4)\nfor all x \u2208 X and g \u2208 G, where $p_{in}: G \\rightarrow GL(X)$ and $p_{out}: G \\rightarrow GL(Y)$ are input and output representations [4]. Invariance is a special case of equivariance where the output does not change under the group action. This occurs when the output representation $p_{out}(g)$ is trivial. Figure 8 visualize how the equivariant and invariant networks work.\nOne of the primary approaches to incorporating symmetry into neural networks is through weight sharing [47, 9, 56]. This approach enforces equivariance by constraining the network's architecture so that the weights are shared across different group elements. For example, in G-convolutions [11], the same set of weights is shared across the transformed versions of the input, ensuring that the network's predictions remain consistent under those transformations. In a layer of G-steerable CNNs [60], a set of equivariant kernel bases is precomputed based on the input and output representations, and the convolution kernel used is a linear combination of this equivariant"}, {"title": "B Detailed Dataset Description", "content": "For model development and evaluation, we collected 1,437 CT scans from 7 public datasets. A detailed summary of the datasets is provided in Table 5. In total, 24 organs are labled in the assembled datasets, with a strong focus on segmentation targets in the abdominal region. The organ class distribution across the datasets is shown in Fig 9. To standardize quality and reduce domain gaps, we applied a preprocessing pipeline to all datasets. Specifically, we mapped the Hounsfield unit range [-180, 240] to [0, 1], clipping values outside this range. To address dimension mismatches between datasets, masks, and images, all scans and masks were resized to 1024 \u00d7 1024. The 3D scan volumes were sliced along the axial plane to generate 2D images and corresponding masks. To ensure labeling quality, organ segments with fewer than 1,000 pixels in 3D volumes or fewer than 100 pixels in 2D slices were excluded. The finalized dataset consisted of 101,217 images, with 91,344 (90.25%) used for training and validation, and 9,873 (9.75%) reserved for testing."}, {"title": "Test Data Creation", "content": "Different from existing work that solely chases for a higher segmentation accuracy, in this paper, we expect to evaluate the segment model's performance in dual tasks: The free-form text understanding ability and segmentation ability."}]}