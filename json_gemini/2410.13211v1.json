{"title": "ESTIMATING THE PROBABILITIES OF RARE OUTPUTS\nIN LANGUAGE MODELS", "authors": ["Gabriel Wu", "Jacob Hilton"], "abstract": "We consider the problem of low probability estimation: given a machine learning\nmodel and a formally-specified input distribution, how can we estimate the prob-\nability of a binary property of the model's output, even when that probability is\ntoo small to estimate by random sampling? This problem is motivated by the need\nto improve worst-case performance, which distribution shift can make much more\nlikely. We study low probability estimation in the context of argmax sampling\nfrom small transformer language models. We compare two types of methods: im-\nportance sampling, which involves searching for inputs giving rise to the rare out-\nput, and activation extrapolation, which involves extrapolating a probability dis-\ntribution fit to the model's logits. We find that importance sampling outperforms\nactivation extrapolation, but both outperform naive sampling. Finally, we explain\nhow minimizing the probability estimate of an undesirable behavior generalizes\nadversarial training, and argue that new methods for low probability estimation\nare needed to provide stronger guarantees about worst-case performance.", "sections": [{"title": "INTRODUCTION", "content": "Modern ML systems undergo black-box optimization to minimize a loss function on samples drawn\nfrom a training distribution. Although models produced in this way perform desirably on average\nover this distribution, they can still produce highly undesirable outputs on very rare inputs. This is\na problem, because these rare inputs can become much more likely in the presence of distribution\nshift, especially one chosen adversarially, such as with large language model \u201cjailbreaks\" (Carlini\net al., 2024; Wei et al., 2024).\nPreventing such highly undesirable outputs is a notoriously challenging problem. The most common\nremedy is adversarial training, in which inputs that produce these undesirable outputs are searched\nfor and used as additional training data (Goodfellow et al., 2014; Madry, 2017), but the transfer\nbetween different search methods is generally weak (Kang et al., 2019; Wei et al., 2024). In this\nwork, we propose the more modest goal of simply estimating the probability that an input drawn\nfrom some distribution will produce a certain kind of output, which has been considered before in\nthe context of computer vision in Webb et al. (2019). We will show that even this intermediate goal\nis challenging, but successful methods could enable new ways of preventing undesirable outputs by\nminimizing their estimated probability.\nTo advance work on this problem, we study low probability estimation in the context of small trans-\nformer language models. We consider various formally-defined input distributions in which each\ninput token is sampled independently, and develop methods for estimating the probability that a\nparticular target token will have the largest output logit. We constrain the computational budget of\nour methods and obtain ground truth probabilities by random sampling using a much larger compu-\ntational budget. The target tokens are chosen to have ground truth probabilities between 10-9 and\n10-5, which are too small for random sampling to produce a good estimate under the constrained\ncomputational budget.\nIn this context, we study two types of methods:"}, {"title": "PROBLEM STATEMENT", "content": "Given an input space X, an output space Y, an input distribution D\u2208 \u2206(X), a model M : X \u2192 \u0423,\nand a formal boolean property of model outputs C: \u0423 \u2192 {0, 1}, low probability estimation is the"}, {"title": "OUR SETTING", "content": "In this paper, we study low probability estimation in the setting of argmax sampling from language\nmodels with single-token behaviors. Let M : V* \u2192 V be a transformer language model that\npredicts the next token given a string of previous tokens, where V is the token vocabulary. Note that\nwe sample at temperature 0, so M is deterministic. Given a distribution D over V* and a target token\nt\u2208 V, the low probability estimation problem for single-token behaviors is the task of estimating\nPr[M(x) = t].\nx~D\nLetting M\u2081(x) be the logit the model assigns to token i \u2208 V, this can also be written as:\nPr[Mt(x) > Mi(x) \u2200i \u2260t].\nx~D\nIn general, D can be any distribution that can be formally specified. However, in this paper we focus\nonly on distributions D with independent tokens. That is, we specify an input length k and token\ndistributions D1,..., Dk \u2208 \u2206(V), then write D as the product D\u2081 \u00d7 \u00b7\u00b7\u00b7 \u00d7 Dk. shows the\n8 distributions that were tested, with tokens colored for clarity. To prevent overfitting, the methods\nwere only run on the first four distributions during development, and they were finalized before\ntesting on the last four distributions. The results were qualitatively the same on both halves of the\nsplit."}, {"title": "ESTIMATION METHODS", "content": "We introduce four methods in this section: two importance sampling methods (Independent Token\nGradient and Metropolis-Hastings), and two activation extrapolation methods (Quadratic Logit De-\ncomposition and Gaussian Logit Difference). We also compare against the baseline of outputting an"}, {"title": "IMPORTANCE SAMPLING METHODS", "content": "Naive sampling fails to produce good estimates for low probability events because it takes too many\nsamples from D to observe a positive example. To address this, we can instead draw samples from\na different distribution that up-weights regions of input space most likely to produce the behavior\nof interest. If we re-weight our observations properly, this gives an unbiased estimator for the\ntrue probability. This is known as importance sampling, and it enjoys the same advantages that\nadversarial training has over standard training: by using a narrower input distribution, we can more\nefficiently discover positive examples of the target behavior.\nFormally, let p(x) be the probability mass function of D, and let q(x) be the PMF of any other\ndistribution. Then\nPr [M(x) = t] = Ex~p[1[M(x) = t] = Exg [(p(x)/q(x)) [M(x) = t}]\nx~p\nbut the latter may have less variance (and so require fewer samples to get a good estimate).\nThe following two importance sampling methods take q(x) to be a Boltzmann posterior with prior\np(x). The first defines q(x) with independent tokens, while the second defines q(x) to have non-\nindependent tokens and so requires a more sophisticated sampling method."}, {"title": "INDEPENDENT TOKEN GRADIENT IMPORTANCE SAMPLING (ITGIS)", "content": "We want q to up-weight tokens that contribute to t being outputted. One way to do this is to continue\nto treat each input token as independent, but change the probability of tokens according to their\naverage linear contribution to the logit of t. Let x = (x1,...,xk) \u2208 Vk be an input of length k, and\nsay that p(x) factors as p\u2081 (x1) \u00b7\u00b7\u00b7 Pk (xk). Then we define q(x) = 91(x1)\uff65\uff65\uff65qk(xk), where\nqi (Xi) x Pi(xi) exp(Si(Xi) /T)\nand\nSi(Xi) = Ex'~q[\u2207x'Mt(x')]i,x\u2081.\nT is a temperature parameter, and the gradient is taken by treating x' as a one-hot vector in Rkx|V|.\nIntuitively, the gradient \u2207x'Mt(x')i,x\u2081 gives us a linear approximation to how much the logit of t\nwould change if we replaced i-th token of x' with xi (up to an additive constant w.r.t. xi). Thus, Si\nscores each token value according to its average linear contribution to Mt, and qi is defined as the\nBoltzmann distribution with respect to this score function.\nHowever, since si and q are both defined in terms of each other, we can't calculate si directly. To\novercome this, we construct a sequence of score functions s\u2070), s(1),... and associated distributions\nq(0), q(1),... that are adaptively refined with respect to each other (see Appendix B.i for details).\nSampling from each q(i) lets us calculate an importance sampling estimate, and the final output of\nthe method is the average value of these estimates across all j."}, {"title": "METROPOLIS-HASTINGS IMPORTANCE SAMPLING (MHIS)", "content": "A problem with ITGIS is that the new sampling distribution q(x) still treats all tokens as indepen-\ndent, and it only accounts for linear effects of tokens on the target logit. Thus, ITGIS may fail to\nsample into the most important regions of the input space if the model is sensitive to non-linear\ninteractions between tokens (e.g., if the model's target logit is only high when the last two tokens of\nthe input are the same as each other).\nMt(x)\nq(x) xp(x) exp(\n).\nT\nagain using a Boltzmann distribution to up-weight regions of input space that are more likely to have\npositive samples.\nUnlike ITGIS, we cannot explicitly compute q because it does not factor into independent distribu-\ntions over each token. Instead, we use the Metropolis-Hastings algorithm to produce a random walk\nin input space that has a stationary distribution of q. To do so, we must define a proposal distri-\nbution \u03c0(x|x/) that suggests the next element of the walk. To encourage fast mixing, this proposal\ndistribution should be good at exploring into regions of input space that q weights highly.\nHere we take inspiration from Greedy Coordinate Gradient, an algorithm that optimizes a discrete\nprompt to jailbreak a model using gradients (Zou et al., 2023). We adapt this optimization procedure\ninto a proposal distribution: to pick a proposed next step x' of the walk, we choose a random token\nposition i to replace, compute the gradient of s(x) with respect to xi, then sample a replacement\ntoken for position i according to a Boltzmann distribution defined by this gradient (similarly to\nITGIS). The final output of the method is the average importance sampling estimate taken after a\nburn-in period. For a precise description of the algorithm, see Appendix B.ii."}, {"title": "ACTIVATION EXTRAPOLATION METHODS", "content": "The importance sampling methods search for explicit examples of inputs that cause the given behav-\nior. This makes their task at least as hard as the adversarial training search problem-if it is difficult\nto find an x \u2208 supp(D) such that M(x) = t, the importance sampling estimators will likely fail to\nproduce a positive estimate.\nWe hope to find low probability estimation methods that work even when the search problem for\nimportance sampling is hard. To do this, we introduce activation extrapolation: first fit a distribution\nto the activations or logits of M, then estimate the probability of the output property of interest under\nthis idealized distribution. Our first such method is Quadratic Logit Decomposition, which applies a\npresumption of independence between uncorrelated subspaces the model's pre-unembed activations.\nWe also develop Gaussian Logit Difference, which is intended as a simple baseline method."}, {"title": "QUADRATIC LOGIT DECOMPOSITION (QLD)", "content": "Let the random vector v(x) \u2208 Rd be the activation of the model right before applying the unembed\nmatrix Wu \u2208 Rdx|V|. That is, v(x) \u00b7 Wu represents the model's output logit vector M(x)1,...,|V|.\nWe first collect n samples of v (call them v(1),..., v(n)). We then choose some unit direction\nd \u2208 Rd (see below), then decompose each v(i) into a(z) + b(i), where a lies in the subspace\nspanned by d, and b lies in the complementary subspace that is orthogonal in a whitened basis.\nThis decomposition is chosen such that the random vectors a and b are uncorrelated across the n\nsamples.\nNext, by treating the random vectors a and b as independent, we can use our n samples of each to\nobtain n\u00b2 \"synthetic\" samples of u. The final output of QLD is the proportion of these synthetic\nsamples that cause t to be outputted:\n(1/n\u00b2)* |{(i, j) \u2208 [n]2 | a(i) + b(i) \u2208 S}|,\nwhere SC Rd is the \u201cacceptance region\u201d of activation space corresponding to activations that result\nin the target logit being highest after unembedding. Despite the fact that there are n\u00b2 synthetic\nsamples, this proportion can be computed in O(n) time by first sorting the samples a(2). A more\ncomplete description of the QLD algorithm can be found in Appendix B.iii."}, {"title": "Choice of direction.", "content": "We rely on the following two assumptions for QLD to perform well: 1) a\nand b are independent, and 2) the contribution towards the output behavior is split roughly equally\nbetween these two terms (see Appendix C for more discussion). After some initial experimenta-\ntion with a variety of candidate directions, we decided to set d to be the direction of the shortest\nvector in whitened space that results in the model outputting t. It can also be thought of as the\nmaximum likelihood value of v under a Gaussian prior, conditioned on observing the model output\nt. Appendix D describes the algorithm we use to compute d."}, {"title": "GAUSSIAN LOGIT DIFFERENCE", "content": "On any given input, we can record the difference \u2206t := Mt(x) \u2013 maxi M\u00bf(x). We wish to estimate\nthe probability that \u2206t \u2265 0. A natural estimation method, which we view as a simple baseline, is to\ntreat At as Gaussian by estimating its mean \u00b5 and standard deviation o with samples, then calculate\nPr[N(\u03bc, \u03c3\u00b2) \u2265 0]. In practice, we use a slightly different functional form that captures the Gaussian\nPDF, which approximates the CDF well in the tails. The output of the Gaussian Logit Difference\nmethod is:\nexp(-((a\u03bc)/(\u03c3+e))\u00b2+b+c),\nwhere a, b, c, and e are parameters that are fit to minimize loss across all target tokens associated\nwith a given distribution (see Section 4.2)."}, {"title": "EXPERIMENTAL SETUP", "content": "We apply our methods on three models: a 1-layer, a 2-layer, and a 4-layer transformer from Nanda &\nBloom (2022). All models have a hidden dimension of d = 512, a vocabulary size of |V| = 48262,\nGELU non-linearities (Hendrycks & Gimpel, 2023), and were trained on the C4 dataset (Raffel\net al., 2023) and CodeParrot (Tunstall et al., 2022).\nFor each of the 8 distributions (listed in ) and for each model, we generate ground-truth token\nprobabilities by running forward passes on 232 random samples. We then select a random set of 256\ntokens among those with ground-truth probabilities between 10-9 and 10-5, and we test all of our\nmethods on these tokens.\nWe give each method a computational budget of 216 model calls (see details in Appendix F). This\nbudget was chosen so that naive sampling would almost never result in any positive estimates for\nthe range of token probabilities we test (216 < 105), but the theoretical quadratic gains from QLD\nwould still be enough to get signal on the entire range of probabilities ((216)2 > 109)."}, {"title": "ITAKURA-SAITO LOSS", "content": "We measure the quality of the method with a loss function inspired by the Itakura-Saito divergence\n(Itakura & Saito, 1968). If p is the ground-truth probability of a particular target token, then an\nestimate of q incurs a loss of:\nDis(p,q) = p/q - ln(p/q) - 1.\nTwo considerations went into the choice of this loss function. First, Itakura-Saito loss is a proper\nscoring rule (Buja et al., 2019). Second, since it only depends on the ratio p/q, Itakura-Saito loss\nis sensitive to small probabilities: if p = 10-100 and q = 10-10, then Dis(p,q) is very large.\nIn contrast, the squared error loss function (p \u2013 q)2 would be extremely small. Intuitively, this"}, {"title": "AFFINE FITS", "content": "Many methods often report estimates of 0, but Dis is undefined for q = 0. To address this, we\nfit a transformation x \u2192 ax + b to the outputs of each method, where a, b and care chosen\nto minimize Itakura-Saito loss. ax can be thought of an affine transformation in log-space, and\nadding b prevents values from being too small while barely affecting larger outputs. To ensure that\nthis transformation is not overfitting to the particular set of 256 tokens, we report the leave-one-out\ncross-validation (LOOCV) loss of each method. We train a separate fit for each (method, input\ndistribution) pair.\nNote that the Gaussian Logit Difference method has a special functional form of its fit ((\u03bc, \u03c3) \u2192\nexp((-((\u03bc)/(\u03c3+e))\u00b2+b+c) instead of x \u2192 ax + b) but is otherwise evaluated in the same way."}, {"title": "RESULTS", "content": "shows the performances of each method. The relative ordering is clear: both importance\nsampling methods outperform Quadratic Logit Decomposition, which in turn outperforms Gaussian\nLogit Difference. GLD is barely better than outputting an optimal constant (which can be interpreted\nas the performance of naive sampling). shows that there is a fair amount of variation in\nmethod performance across the 8 distributions: some behaviors like hex and icl favor MHIS,\nwhile others like spanish heavily favor ITGIS. A more detailed table of results is in Appendix G.\nAmong the two importance sampling methods, ITGIS does better on smaller models, while MHIS\ndoes better on larger models. We believe this is because larger models are less easily approximated as\nlinear functions and are more likely to have complex behaviors arising from inter-token interactions.\ndisplays example scatter plots of ITGIS, MHIS, and QLD estimates before a fit is applied.\nEach point represents the ground-truth and estimated probability of a different target token. More\nscatter plots can be found in Appendix J; note that the qualitative performances of the methods can\nvary significantly on different input distributions. We perform an ablation study on our choice of\nloss function in Appendix H, in which we score methods based on squared error in log-space instead\nof Itakura-Saito loss."}, {"title": "DISCUSSION", "content": "One might ask: if a particular model behavior is so rare that it never arises during training, why\nwould we care about estimating its probability? There are a few reasons. First, some AI systems\nmay be run on many more inputs during the course of deployment than during training. Thus, if a\ncertain model behavior would be so catastrophic that it is unacceptable for it to occur even once in\ndeployment, we cannot rely on training to drive down its probability low enough. Second, there may\nbe distributional shift between training and deployment such that events that occur extremely rarely\nduring training become more likely in deployment. This could occur because of an input chosen\nadversarially, but it could also occur because of goal misgeneralization (Shah et al., 2022).\nA particularly challenging case is deceptive alignment, the possibility that an ML model would look\nfor clues about whether it is in a training or a deployment environment, and only behave well in\ntraining (Hubinger et al., 2021). To detect whether a model is deceptively aligned, one could craft\nan input distribution that is \u201cwide enough\u201d to assign some probability mass, even if very small, to\nany possible deployment-time input, then apply low probability estimation methods to detect if the"}, {"title": "DISTRIBUTION SHIFT AS MOTIVATION", "content": "One might ask: if a particular model behavior is so rare that it never arises during training, why\nwould we care about estimating its probability? There are a few reasons. First, some AI systems\nmay be run on many more inputs during the course of deployment than during training. Thus, if a\ncertain model behavior would be so catastrophic that it is unacceptable for it to occur even once in\ndeployment, we cannot rely on training to drive down its probability low enough. Second, there may\nbe distributional shift between training and deployment such that events that occur extremely rarely\nduring training become more likely in deployment. This could occur because of an input chosen\nadversarially, but it could also occur because of goal misgeneralization (Shah et al., 2022).\nA particularly challenging case is deceptive alignment, the possibility that an ML model would look\nfor clues about whether it is in a training or a deployment environment, and only behave well in\ntraining (Hubinger et al., 2021). To detect whether a model is deceptively aligned, one could craft\nan input distribution that is \u201cwide enough\u201d to assign some probability mass, even if very small, to\nany possible deployment-time input, then apply low probability estimation methods to detect if the"}, {"title": "RELATION TO RED-TEAMING AND ADVERSARIAL TRAINING", "content": "Our importance sampling methods for low probability estimation involve finding inputs for which\nthe rare event occurs. This amounts to the well-studied task of \"red-teaming\". As long as the\nrequired importance sampling ratios can be computed, any method for red-teaming can be turned\ninto an importance sampling method for low probability estimation, as we demonstrate with our\nadaptation of Greedy Coordinate Gradient into MHIS (Zou et al., 2023). However, our activation\nextrapolation methods such as QLD do not correspond to any red-teaming method.\nA further reason to be interested in low probability estimation is that it could be used to reduce the\nprobability of the rare event, by optimizing the model to produce a lower estimate. For example, this\ncould be done using gradient descent, if the estimate were a differentiable function of the model's\nparameters. For an importance sampling method, this amounts to finding inputs for which the rare\nevent occurs (i.e., red-teaming) and using them as training data, which is essentially the well-known\nmethod of adversarial training (Goodfellow et al., 2014). However, since our activation extrapola-\ntion methods do not correspond to any red-teaming method, new activation extrapolation methods\npotentially provide us with new ways to reduce the probabilities of rare events."}, {"title": "IMPORTANCE SAMPLING VERSUS ACTIVATION EXTRAPOLATION", "content": "In our experiments, we found that importance sampling methods outperformed activation extrapo-\nlation. Nevertheless, there are theoretical cases in which importance sampling performs worse than\nother methods. For example, consider a model that outputs the SHA-256 hash of its input: find-\ning any input that gives rise to a particular output is computationally infeasible, yet it is still easy\nto estimate the probability of a particular output by modeling the output of the hash function as\nrandom.\nMore generally, we are excited about low probability estimation as a concrete problem for which for\nwhich it may be necessary to leverage internal model activations. In place of importance sampling,\nwe may be able to use deductive estimates based on a presumption of independence (Christiano"}, {"title": "LIMITATIONS", "content": "There are two main limitations of our experimental setup. First, we only use input distributions\nthat factor into independent tokens. This choice is necessary for the definition of ITGIS. It is also\nvery convenient for the implementation of MHIS, because it gives efficient sampling access to the\nproposal distribution. To move beyond independent token input distributions, we could define the\ninput distribution to be the output of a separate generative model and adapt some of the current\nestimation methods appropriately (GLD and QLD are input-agnostic and could be applied in their\ncurrent form).\nSecond, we only study model behaviors that consist of a single token sampled at temperature 0.\nThis is unrealistic because in practice, if we were concerned about specific single-token outputs, it\nwould be easy to block them with a filter. In contrast, the types of behaviors we actually worry about\nlikely involve long chains of autoregressive generation or interaction with the external world (e.g.,\nwhen forming and executing a plan). We are excited to see future work extending our setting in this\ndirection.\nNevertheless, it is worth noting that formally-defined distributions and behaviors are more general\nthan they may initially seem. For example, we could formalize the event \"M writes buggy code\",\nas: When M's output is given to GPT-4 along with the prompt \u201cDoes this code contain any bugs?\nLet's think step by step.\", does GPT-4 end its response with YES?\""}, {"title": "RELATED WORK", "content": "The problem of low probability estimation was previously considered in the context of computer\nvision by Webb et al. (2019), where they propose using an Adaptive Multi-Level Splitting algorithm\nwith Metropolis Hastings. However, they only study the problem in the context of computer vision\nwith continuous input spaces, and their approaches still require finding positive samples, unlike our\nactivation extrapolation methods. Phuong et al. (2024) and H\u00f8jmark et al. (2024) attempt to estimate\nthe probability that a language model passes certain capability evaluations, even when its success\nrate is low, though their methods are not directly applicable to our formal setting.\nOur importance sampling methods can be viewed as solving a special case of controlled text genera-\ntion (Zhang et al., 2023) in which we want to sample from an autoregressive distribution conditioned\non a property of the full output (in our case, that the last token is t). Yang & Klein (2021) do this\nby training Future Discriminators to steer model generation towards the desired attribute. Lew et al.\n(2023) approach the problem with a Sequential Monte Carlo steering approach; however, their in-\nfilling algorithm doesn't provide any benefit over naive sampling when all tokens except the last are\nindependent. These works don't consider the problem of low probability estimation.\nZhao et al. (2024) focus on the problem of estimating the partition function of an unnormalized\ntarget distribution over sequences, which is a more general case of our low probability estimation\nproblem. Their Twisted Sequential Monte Carlo methods can be viewed as more advanced versions\nof our importance sampling methods. In contrast, in this work we focus on motivating the low\nprobability estimation problem and introducing methods that do not involve searching for positive\nsamples, such as activation extrapolation.\nFinally, there is a large body of work applying adversarial training to improve worst-case model\nperformance (Bai et al., 2021; Goodfellow et al., 2014; Ilyas et al., 2019), especially in the context\nof language models (Madry, 2017; Liu et al., 2020). Perez et al. (2022) explores using language\nmodels themselves to aid in red-teaming other models. Latent adversarial training (Casper et al.,\n2024) generalizes standard adversarial training by optimizing over perturbations in activation space;\nthis means that, like activation extrapolation methods, it can be effective even when the adversarial\ntraining search problem over input space is hard."}, {"title": "CONCLUSION", "content": "In this paper, we introduce the problem of low probability estimation along with four novel esti-\nmation methods. We define and collect ground-truth probabilities for 8 different input distributions,\nthen use them to evaluate the performance of our proposed methods. We find that the two importance\nsampling-based methods perform the best, with larger models favoring MHIS over ITGIS.\nWe are excited for future work that extends our empirical setup to non-independent input distribu-\ntions and output behaviors that involve more than one token. We are also looking forward to future\npapers that develop more accurate estimation methods, especially methods like QLD that move be-\nyond importance sampling."}]}