{"title": "PLPP: Prompt Learning with Perplexity Is Self-Distillation for Vision-Language Models", "authors": ["Biao Liu", "Wenyi Fang", "Xiaoyu Wu", "Yang Zheng", "Zheng Hu", "Bo Yuan"], "abstract": "Pre-trained Vision-Language (VL) models such as CLIP have demonstrated their excellent performance across numerous downstream tasks. A recent method, Context Optimization (CoOp), further improves the performance of VL models on downstream tasks by introducing prompt learning. CoOp optimizes a set of learnable vectors, aka prompt, and freezes the whole CLIP model. However, relying solely on CLIP loss to fine-tune prompts can lead to models that are prone to overfitting on downstream task. To address this issue, we propose a plug-in prompt-regularization method called PLPP (Prompt Learning with PerPlexity), which use perplexity loss to regularize prompt learning. PLPP designs a two-step operation to compute the perplexity for prompts: (a) calculating cosine similarity between the weight of the embedding layer and prompts to get labels, (b) introducing a language model (LM) head that requires no training behind text encoder to output word probability distribution. Meanwhile, we unveil that the essence of PLPP is inherently a form of self-distillation. To further prevent overfitting as well as to reduce the additional computation introduced by PLPP, we turn the hard label to soft label and choose top-k values for calculating the perplexity loss. For accelerating model convergence, we introduce mutual self-distillation learning, that is perplexity and inverted perplexity loss. The experiments conducted on four classification tasks indicate that PLPP exhibits superior performance compared to existing methods.", "sections": [{"title": "Introduction", "content": "In recent years, the advent of CLIP (Radford et al. 2021) and ALIGN (Jia et al. 2021) have driven increased exploration of VL models, which are capable of training and reasoning by using both visual and textual data. It is important to note that such models have a high demand for data and require extensive training on a large-scale image-text pairs to achieve good performance. For instance, the training scheme of CLIP model involves a staggering 400 million image-text pairs. Following the pre-training phase, VL models can perform image classification by employing a carefully crafted prompt, such as \u201ca photo of a {category},\u201d as input for the text encoder. Simultaneously, the image encoder processes the visual input. Subsequently, the classification results are obtained by computing the cosine similarity between text and image representations across all categories.\nWhile the development of high-quality contextual prompts (Jin et al. 2022) has demonstrated the capacity to enhance the performance of CLIP and other similar VL models, it often relies upon a considerable expenditure of time and the specific domain knowledge of human experts. Furthermore, this resource-intensive process may also prove to be ineffective when confronted with novel or unforeseen scenarios. Moveover, the combination of vast parameter space and constraints on available training data, particularly in a few-shot setting, make it infeasible to fully fine-tune the entire model for downstream tasks.\nFine-tuning the entire model on specific tasks risks erasing its general knowledge and can lead to overfitting. Such fine-tuning may hinder the model's adaptability to unforeseen scenarios and reduce its generalization to broader applications. To address these challenges, inspired by recent advances in Natural Language Processing (NLP) (Vaswani et al. 2017; Gao, Fisch, and Chen 2021; Jiang et al. 2020; Lester, Al-Rfou, and Constant 2021; Li and Liang 2021; Shin et al. 2020; Zhong, Friedman, and Chen 2021), CoOp (Zhou et al. 2022b) introduces a prompt learning method as an alternative to manually crafting prompts for specific tasks. Diverging from the prior fine-tuning paradigms, CoOp keeps both the image and text encoders of CLIP fixed, exclusively fine-tuning the learnable prompt, which consists of a set of vectors. Following in the footsteps of CoOp, several approaches have been proposed to enhance the training paradigm of prompt or to introduce the learnable prompt to different layers, as exemplified by (Hantao Yao 2023; Zhou et al. 2022a; khattak et al. 2023; Lu et al. 2022; Zhu et al. 2023; Chen et al. 2023; Xing et al. 2023; Khattak et al. 2023). However, these methods still tend to lead models to overfit in downstream tasks.\nTo mitigate the issue of overfitting in prevailing approaches, it is essential to develop new methods that can prevent the model from overtraining. Therefore, we propose a plug-in prompt regularization method called PLPP. The rationale behind our method is straightforward. In the domain of NLP, perplexity is a crucial metric used to evaluate the performance of LMs, where lower perplexity values indicate better model performance in predicting the next word in a sequence of text. In the context of VL models, we fix image and text encoders, integrate perplexity to regularize the process of prompt learning. We use a two-step operation to calculate perplexity. a) obtaining labels: we calculate co-"}, {"title": "Summary", "content": "sine similarity between the weight of the embedding layer and each vector in prompt, then the index with the largest cosine similarity is used as the corresponding label. b) outputting word probability distribution: We place a LM head, which requires no training, behind the encoder to output the word probability distribution. After these two steps, we can easily calculate the perplexity.\nIn summary, our main contributions are as follows:\n\u2022 We propose PLPP to mitigate the common issue of prompt overfitting in VL models by introducing an explainable metric perplexity. PLPP regularizes prompts optimization by constraining the distribution of prompts close to the output distribution.\n\u2022 PLPP unites prompt learning and perplexity by incorporating a LM Head that without training. PLPP is a plug-in method, which can be easily integrated into any prompt-based learning methods in VL model without increasing the parameters that need to be optimized.\n\u2022 We unveil the essence of PLPP is a self-distillation and turn hard label distribution to soft label distribution to make the model training more stable. We also choose the top-k values in distribution to significantly reduce the computational cost. We introduce mutual self-distillation learning to accelerate model convergence."}, {"title": "Related Works", "content": "Pre-training for VL models. The pre-training phase of the VL model requires unsupervised learning on a large number of image-text pair datasets, due to its voracious appetite for data.. The goal of this phase is to facilitate the model to align the image features with the corresponding text features. CLIP (Radford et al. 2021) and ALIGN (Jia et al. 2021) utilize more than four million image-text pairs for pre-training. To ensure the proximity of analogous inputs within the same modality, TCL (Yang et al. 2022) employs a combination of cross-modal and intra-modal self-supervision, yielding synergistic advantages in representation learning. In a concerted effort to bolster training efficiency, DeCLIP (Li et al. 2022) not only exploits cross-modal multi-view and intra-modal supervision but also introduces a novel cross-modal Nearest-Neighbor Supervision mechanism, which leverages information emanating from analogous pairs in a more nuanced manner. OneR (Jang et al. 2023) and MS-CLIP (You et al. 2022) adopt a unified transformer encoder architecture for image-text pairs. HiCLIP (Geng et al. 2023) enhances the vision and text encoders of CLIP with hierarchy-aware attentions, enabling the model to learn semantic hierarchies in a layer-by-layer fashion.\nEnhancement of Modules in Pre-trained VL Models. CALIP (Guo et al. 2023) introduces an ingenious attention module devoid of parameters, thereby augmenting the zero-shot performance of CLIP (Radford et al. 2021). CLIP-Adapter (Gao et al. 2024), on the other hand, introduces an additional bottleneck layer into the model. This layer is responsible for acquiring novel features and executing residual-style feature fusion with the originally pretrained features. Meanwhile, Tip-Adapter (Zhang et al. 2022) inherits the advantageous property of being training-free, as seen in CLIP-Adapter. Furthermore, generating weights through a key-value cache model derived from the few-shot training set enhances the adaptability and effectiveness of the downstream task performance. ATC (Yang et al. 2023) introduces a novel two-branch architecture. One branch employs ConditionNet to synthesize a textual cache from image features, while the other constructs an learnable visual cache to enhance versatility of the model.\nLearnable Prompt for Pre-trained VL Models. Prompt learning represents significant advancement in the field of NLP. CoOp (Zhou et al. 2022b) represents a pioneering effort in the field of computer vision to custom extend VL models using this approach. This innovation yields substantial improvements in performance when compared to manually crafted prompts in downstream tasks, particularly in the realm of few-shot classification. Nonetheless, CoOp has limitations in terms of its ability to generalize to broader, unseen categories within the same dataset. In response, CoCoOp (Zhou et al. 2022a) expands this paradigm by training a lightweight neural network to generate an input-conditional token for each image, which leads to enhancing generalization performance. Prompt-Adapter (Sun et al. 2023) integrates pre-trained prompt fine-tuning with an efficient adaptation network, achieving enhanced few-shot classification performance. Using prompts in a single branch of CLIP is suboptimal because it limits the model's adaptability to the two representation spaces for adjusting downstream tasks. MaPLe (khattak et al. 2023), which pioneers prompt learning in both the visual and language branches, has been shown to significantly enhance the alignment of representations. Moreover, MaPLe introduces a profound prompting strategy that extends the scope of prompt learning not only to the input but also across multiple transformer blocks. DPT (Xing et al. 2023) proposes a novel paradigm that simultaneously incorporates the insights derived from textual and visual prompts. Furthermore, it develops the Class-Aware Visual Prompt Tuning scheme, which generates visual prompts in a dynamic manner based on both task-related and instance-specific prompts. When CoOp-based methodologies are employed in training downstream tasks, learnable prompts tend to accrue task-specific textual knowledge but overlook the essential reservoir of general textual knowledge that underpins robust generalization. Kg-CoOp (Hantao Yao 2023) intervenes to minimize the divergence between the textual embeddings generated by learned prompts and their hand-crafted prompts, averting the loss of essential knowledge. Besides, ProGrad (Zhu et al. 2023) introduces a selective update mechanism for prompts, exclusively attending to those prompts whose gradients align with the gradients of the Kullback-Leibler (KL) loss, calculated by reconciling learnable prompts and hand-crafted prompts. This alignment criterion necessitates that the angle between the two kinds of gradients falls below 90\u00b0. Plot (Chen et al. 2023) uses optimal transport to align text and image output by minimizing transport cost from prompt features to local features of image. Moreover, Plot uses the optimal transport distance to evaluate the match between images and categories, and convert match score to a prediction probability. PromptSRC (Khattak et al. 2023) proposes"}, {"title": "Methodology", "content": "In this section, we provide an overview of CoOp, and introduce the evaluation metric perplexity in NLP and what its essence is in the prompt learning of VL models. Additionally, we introduce our method, Prompt Learning with PerPlexity (PLPP), which aims to leverage perplexity to regularize the learning process of prompts, leading to addressing the overfitting issue of prompts."}, {"title": "An Overview of CoOp", "content": "CoOp, which is designed to enhance the performance of CLIP in few-shot and domain generalization tasks, introduces prompt learning to VL models. Instead of using hand-crafted prompt templates, CoOp initializes a set of learnable vectors, each vector dimension is 512, which is consistent with the dimension of word embeddings. The number of vectors is usually set to 2, 4, 8, or 16. Concretely, the learnable vector set is denoted as V = {V1, V2, . . ., VM }, with M being the count of vectors. Each prompt, denoted as pi = {V1, V2, . . ., VM, Ci}, amalgamates these learnable vectors with the class token embedding ci, where ci represents the tokenized class name corresponding to the i-th class. Subsequently, all prompts are feed into CLIP's text encoder, denoted as g(.). Assuming f represents the visual embedding of x, the ultimate prediction probability for predicting the image x as i-th class is calculated as follows:\n$p(y = i|x) = \\frac{exp(sim(g(p_i), f)/T)}{\\sum_{j=1}^{K} exp(sim(g(p_j), f)/T)},$ (1)\nwhere sim(...) signifies a metric function such as cosine similarity, and \u03c4 corresponds to the temperature. Finally, given an image and its label, the prediction probability and"}, {"title": "Perplexity", "content": "Perplexity (Pillutla et al. 2021) serves as a prominent metric used to assess the quality of a LM, quantifying its capacity to predict a given sequence. At its core, the LM strives to output a probability distribution over a predefined vocabulary of words. Consequently, when subjected to test sets comprising hand-crafted sentences, a higher probability assigned to the corresponding output signifies a superior LM, while conversely, a lower probability suggests otherwise. For a given sentence in the test set, denoted as W = {w1, W2, ..., wn}, where N signifies the total sentence length, the perplexity of the sentence is calculated as follows:\n$Perplexity(W) = P(W)^{- \\frac{1}{N}}$\n$= e^{-\\frac{1}{N} \\log P(W_i|W<i)}$ (2)\n$= e^{H(Q,P)}$\nThe calculation of perplexity is as Equation 2. Here, P(wi W<i) represents the probability of a word appearing at the i-th position in a sentence, with the specific word index being denoted earlier, i.e., model prediction distribution. Qcorresponds to the indices of these words in vocab_size which maps words to integers, i.e. ground truth and H signifies the cross-entropy function. It is worth noting that in the realm of NLP, perplexity is employed to evaluate the efficacy of a LM when exposed to human-written sentences. In the context of our work, we freeze the text encoder, and use perplexity to regularize the learning process of the prompt. Minimizing perplexity is to find prompts that closely match their encoded outputs, thus capturing more global information within each prompt vector. KL divergence can be used to measure the difference in information between the two distributions. In the context of prompt learning for VL mod-"}, {"title": "Summary", "content": "els, the KL divergence can be related to perplexity as Equation 3 shows:\n$KL(Q||P) = \\sum_{x \\in X}Q(x) \\log \\frac{Q(x)}{P(x)}$\n$= -H(Q) + H(Q, P) = H(Q, P) = log \\ PPL,$ (3)\nwhere Q and P represent input and output distribution, and PPL is a simplified representation of Perplexity(W) in Equation 2. Since Q is one-hot distribution, it is obviously H(Q) = 0 and can be omitted. Thus, perplexity can be regarded as a hard label in self-distillation (Zhang et al. 2019), which is a kind of model distillation. Perplexity aligns the input prompt distribution with the output distribution of the text encoder, that is to let shadow layer features learn deep layer features."}, {"title": "Prompt Learning with Perplexity", "content": "In this subsection, we detail the implementation of PLPP, a novel plug-in method that breaks the mold by bridging the gap between perplexity (Pillutla et al. 2021) evaluation and prompt learning in VL models. Previously, these two concepts were seen as separate and independent. PLPP unites them in a novel and powerful way.\nFrom the previous subsection we can know that the relationship between self-distillation and perplexity, which is shown in Equation 3. The Q distribution is calculated by using cosine similarity between prompts and embedding layer. As for P, we introduce an LM head positioned after the text encoder to output the distribution P. \"The LM head consists of a simple linear layer without bias, with its weights initialized from the transpose of the embedding.weight. Besides, we demonstrate that perplexity can serve as hard label for self-distillation. In order to further prevent overfitting, we replace hard label Q with soft label, and soft label is commonly used in knowledge distillation. Since we introduce the soft label technique and the index is over 40,000, these increase the computation significantly. To mitigate the computational cost, we employ top-k strategy in Q to retrain only the largest k values, resulting in topk(Q) as the updated Q. At the same time, we save the indexes of the largest k values and use these indexes to get the final P. To ensure training stability and accelerate model convergence, we introduce mutual self-distillation learning. The perplexity loss is then defined as follows:\n$L_{PPL} = e^{-\\frac{1}{\\tau} \\cdot KL(Q_1||P_1)} + e^{-\\frac{1}{\\tau} \\cdot KL(P_1||Q_1)},$ (4)\n$L_{IPPL} = e^{-\\frac{1}{\\tau} \\cdot KL(Q_2||P_2)} + e^{-\\frac{1}{\\tau} \\cdot KL(P_2||Q_2)},$ (5)\nwhere Q1 is obtained by topk(Q) and the indexes of the largest k values are saved to obtain P\u2081, and P2 is obtained by topk(P) and the indexes of the largest k values are saved to obtain Q2.\nThen we calculate the predicted probabilities for all categories on the corresponding dataset according to Equation 1. Subsequently, the cross-entropy loss LCE is based on the"}, {"title": "Summary", "content": "prediction probabilities and ground-truth label y for image x. The flow chart is shown in Figure 1.\nIn summary, we denote LCE and LPPL, LIPPL as the loss for aligning image-text features and for regularizing learnable prompts through perplexity. \u03b1 controls the importance between LPPL and LIPPL. \u03bb controls the weight of the regularization term. We have the overall loss function of PLPP as in Equation 6.\n$L_{PLPP} = L_{CE} + \\lambda \\cdot (\\alpha \\cdot L_{PPL} + (1 - \\alpha) \\cdot L_{IPPL}).$ (6)"}, {"title": "Experiments", "content": "We conduct a comprehensive evaluation of PLPP across four benchmarks for image recognition task: (1) few-shot classification, (2) base-to-novel generalization, (3) cross-dataset evaluation, domain evaluation, and (4) ablation study for the hyperparameter of perplexity loss. PLPP denotes Propmt-SRC + PLPP if not otherwise specified in the experiments."}, {"title": "Evaluation Settings", "content": "Datasets. In few-shot classification and base-to-novel generalization, we follow to the methodology established by CoOp and CoCoOp, using a total of 11 datasets to evaluate the performance of our method. The 11 datasets include general object recognition datasets: ImageNet (Deng et al. 2009) and Caltech101 (Fei-Fei, Fergus, and Perona 2007), fine-grained image recognition datasets: OxfordPets (Parkhi et al. 2012), StanfordCars (Krause et al. 2013), Flowers102 (Nilsback and Zisserman 2008), Food101 (Bossard, Guillaumin, and Van Gool 2014), and FGVCAircraft (Maji et al. 2013), a satellite image classification dataset: EuroSAT (Helber et al. 2018), an action classification dataset: UCF101 (Soomro, Zamir, and Shah 2012), a texture classification dataset: DTD (Cimpoi et al. 2014), a scene recognition: SUN397 (Xiao et al. 2010). For domain generalization, ImageNet serves as the source dataset, and the target datasets include ImageNetV2 (Recht et al. 2019), ImageNet-Sketch (Wang et al. 2019), ImageNet-A (Hendrycks et al. 2021b) and ImageNet-R (Hendrycks et al. 2021a).\nBaselines. In few-shot classification, we compare PLPP with four baseline methods. The first baseline is Linear probe CLIP, involves training a linear classifier after the CLIP image encoder. CoOp is the second baseline, which learns the unified context prompt through data-driven means instead of time-consuming manual design. The third baseline, MaPLe, is a pioneering work that introduces prompt learning to both the visual and language branches. Lastly, PromptSRC, proposes a self-regularizing method that consists of three components: mutual agreement maximization, prompt self-ensembling regularization, and textual diversity regularization. In all tasks, we use the unified context prompt of CoOp, CoCoOp, MaPLe and PromptSRC as the baseline methods. To further verify the effectiveness of PLPP, we use the best baseline method PromptSRC to integrate our PLPP. For simplicity, we use PLPP to denote PromptSRC + PLPP in all experiments.\nImplementation Details. In few-shot classification, all the methods are trained with 1, 2, 4, 8, and 16 shots from"}, {"title": "Summary", "content": "train set. Subsequently, conducting evaluations on the test dataset. In base-to-novel generalization, cross-dataset evaluation, and domain generalization, all methods are training on 16 shots. To ensure equitable comparisons, we compute the results for all methods and datasets by averaging over three random seeds. For all experiments, we adhere to the guidelines provided in PromptSRC, utilizing vit-b/16 (Dosovitskiy et al. 2021) as the backbone for the image encoder. The number of learnable vectors, denoted as M, is consistently set to 4. We adhere to the training epochs, schedule, and data augmentation settings used in PromptSRC. For domain generalization and cross-data evaluation tasks, we set \u03bb to 10 and \u03b1 to 0.2. Since different datasets have different sensitivities to LPPL and LIPPL, the best \u03bb and \u03b1 varies across different datasets and in few-shot classification and base-to-novel generalization, we list in the appendix the best hyperparameter corresponding to the different datasets. For top-k strategy, we set k to 5 in all tasks."}, {"title": "Few-Shot Classification", "content": "Figure 2 demonstrates the comparative analysis across 11 diverse datasets. Overall, our PLPP manifests distinct advantages over the baseline models across various few-shot scenarios, showcasing significant improvement in average performance compared with Linear Probe CLIP, CoOp, and MaPLe. Furthermore, in comparison with PromptSRC, PLPP demonstrates a mean performance lead under 1, 2, 4, 8, and 16 shots across all the datasets. The average improvement in accuracy for each dataset is as follows: OxfordPets (0.4%), Flowers102 (0.5%), FGVCAircraft (0.5%), DTD (0.6%), EuroSAT (1.3%), StanfordCars (0.2%), Food101 (0.3%), SUN397 (0.2%), Caltech101 (0.3%), UCF101 (0.1%), and ImageNet (0.2%)."}, {"title": "Base-to-Novel Generalization", "content": "Base-to-novel generalization is that we first partition the dataset into two groups based on the classes, one called base classes and the other called novel classes. Then, we train our model on base classes and evaluate the performance of the model on novel classes. We use a harmonic mean (HM) as a composite measure to evaluate the base versus novel classes performance trade-off, which as calculated by:\n$H = \\frac{2 \\cdot Base \\cdot Novel}{Base+Novel}.$\nAs presented in Table 1, our proposed PLPP exhibits the consistent performance advantages in base-to-novel generalization setting on 11 datasets.\nPLPP significantly outperforms CoOp, CoCoOp, and MaPLe in both base and novel classes. Against Prompt-SRC, PLPP achieves better results on 7 out of 11 datasets for base classes and improves on 10 out of 11 datasets for novel classes, with a notable 2.77% increase on EuroSAT."}, {"title": "Cross-Dataset Evaluation", "content": "To assess how well PLPP generalizes across different datasets, we train all the methods on ImageNet and test them on 10 other datasets. As shown in Table 2, PLPP's performance on ImageNet is competitive with other methods. In the 10 target datasets, PLPP exceeds the performance of CoOp and CoCoOp in 10 out of 10 and 9 out of 10 datasets, respectively. PLPP also outperforms MaPLe and Prompt-SRC in 6 out of 10 and 10 out of 10 datasets. Overall, PLPP performs well compared to PromptSRC. However, its average performance on the target datasets is still lower than MaPLe's, which is mainly due to MaPLe's strong results on the Flowers102 and EuroSAT. The sensitivity of EuroSAT to prompts (due to its only 10 classes) affects performance. Additionally, PromptSRC uses independent vision-language prompt tokens, while MaPLe uses dependent tokens that better capture the relationship between text and images, giving MaPLe an edge in cross-dataset generalization."}, {"title": "Domain Generalization", "content": "Domain generalization evaluates a model's ability to generalize to a target domain that is different but related to the source domain. We train our model on ImageNet dataset, and evaluate it on four specially designed bench-"}, {"title": "Conclusion", "content": "Prompt-based learning methods greatly reduce the number of parameters that need to be optimized when the VL model is fine-tuned on downstream tasks, and have achieved astonishing performance on various downstream tasks. However, existing prompt learning methods still ignore the problem that prompts are prone to overfitting, thereby damaging the inherent generalization ability of VL models. Our work proposes a plug-in prompt-regularization learning method called PLPP, which addresses the prompt overfitting problem for better generalization. We reveal the essence of perplexity in prompt learning for VL models is a form of self-distillation. To calculate the perplexity loss, we use embedding layer to obtain label and introduce a LM head that requires no training to output word distribution. Moreover, we introduce soft label and top-k strategy to further prevent overfitting and reduce the computational cost. We also employ mutual self-distillation learning to accelerate model convergence. Extensive experiments on four classification tasks show the effectiveness of our PLPP."}]}