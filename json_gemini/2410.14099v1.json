{"title": "ST-MoE-BERT: A Spatial-Temporal Mixture-of-Experts Framework for Long-Term Cross-City Mobility Prediction", "authors": ["Haoyu He", "Haozheng Luo", "Qi R. Wang"], "abstract": "Predicting human mobility across multiple cities presents significant challenges due to the complex and diverse spatial-temporal dynamics inherent in different urban environments. In this study, we propose a robust approach to predict human mobility patterns called ST-MoE-BERT. Compared to existing methods, our approach frames the prediction task as a spatial-temporal classification problem. Our methodology integrates the Mixture-of-Experts architecture with BERT model to capture complex mobility dynamics and perform the downstream human mobility prediction task. Additionally, transfer learning is integrated to solve the challenge of data scarcity in cross-city prediction. We demonstrate the effectiveness of the proposed model on GEO-BLEU and DTW, comparing it to several state-of-the-art methods. Notably, ST-MoE-BERTachieves an average improvement of 8.29%.", "sections": [{"title": "1 Introduction", "content": "Human mobility is a cornerstone of societal functioning, underpinning economic activities, urban development, and social interactions [34]. Understanding human travel patterns is essential for optimizing transportation systems, enhancing urban planning, and managing public health initiatives [1, 14]. In recent years, human mobility prediction has emerged as a critical component in the development of intelligent urban systems [23]. Accurate forecasting of individual and population movement patterns enables proactive decision-making, improves resource allocation, and enhances the responsiveness of services to dynamic urban environments.\nHowever, predicting human mobility poses several significant challenges. Firstly, mobility data are often sparse and unevenly distributed spatially and temporarily, making it difficult to capture comprehensive movement patterns [39]. Secondly, human behaviors are inherently complex and influenced by a myriad of factors such as social interactions [41] and environmental changes [3], which complicates the modeling of their dependencies [32]. Lastly, transferring predictive models between different cities is challenging due to the heterogeneity in mobility patterns, urban infrastructure, and demographic characteristics [5].\nTo address these challenges, we propose ST-MoE-BERT(Spatial-Temporal Mixture-of-Experts with BERT), an innovative framework that integrates transformer-based architectures with a Mixture-of-Experts (MoE) layer [17]. The structure of our proposed framework is shown in Figure 1. ST-MoE-BERTleverages the sequence modeling capabilities of BERT [7] and the specialized expertise of MoE networks to capture both general and city-specific mobility patterns. Furthermore, our transfer learning strategy enables the model to adapt knowledge from data-rich cities to those with limited datasets, thereby enhancing prediction accuracy across multiple urban environments. Our key contributions are as follows.\n\u2022 We introduce ST-MOE-BERT, a unique transformer-based method that combines BERT with an MoE layer to effectively address the long-term cross-city mobility prediction problem.\n\u2022 We develop a transfer learning strategy that employs differential learning rates, thereby enhancing prediction accuracy in data-scarce environments and facilitating adaptation to diverse spatial distributions.\n\u2022 Experimentally, we demonstrate that ST-MoE-BERToutperforms the state-of-the-art methods on the long-term cross-city prediction task with an average improvement of 8.29%."}, {"title": "2 ST-MoE-BERT", "content": "In this section, we present the proposed ST-MoE-BERT framework for long-term human mobility prediction.\nProblem Setup. Given a sequence of historical mobility records X = {x1, x2,...,x}, where each x \u2208 L represents the user's location at time step t, the goal is to predict the future mobility trajectory \u0423 = {YT+1, YT+2, ..., YT+H}, where each yt \u2208 L denotes the user's location at time step t. Here, T is the length of the historical sequence, and H is the prediction horizon.\nTo achieve this, we aim to train a predictor $f : L^T \\rightarrow L^H$ that minimizes the cross-entropy loss between the predicted and ground-truth trajectories on the training set of N trajectories. The loss function I is defined as:\n$I = - \\sum_{t=T+1}^{T+H} \\sum_{i=1}^{N} \\mathbb{I}(y_{t,i}) log \\hat{y}_{t,i}$,\nwhere $\\mathbb{I}(y_{t,i})$ equals 1 if the prediction for sample i at time t is correct, otherwise 0; and $\\hat{y}_{t,i}$ represents the predicted probability distribution over all possible locations for sample i at time t.\nIn this classification set-up, each location is treated as a distinct class, and the predictor f outputs a probability distribution over the classes for each future time step.\nMotivating Example. Recently, Liu et al. [22] propose modeling multivariate correlations by treating independent time series as tokens using self-attention mechanism. We use this observation as a starting point by considering our spatial-temporal foundation model with a combination embedding of weekday, day, time of day, and weekend. Since our problem is a long-term classification problem, we use the autoregressive model BERT [7] to model the temporal dependencies in the mobility data. To capture diverse and long-term mobility patterns, we incorporate a Mixture of Experts (MoE) layer [17] into our model architecture. The MoE layer consists of multiple specialized expert networks, each designed to model distinct aspects of human mobility. A gating network dynamically assigns the input data to these experts, allowing the model to adaptively focus on the relevant patterns for each prediction task."}, {"title": "2.1 Model Structure", "content": "In this section, we introduce our proposed ST-MoE-BERTframework. The primary architecture combines BERT with a Mixture of Experts (MoE) layer.\nBERT.. To effectively capture the intricate temporal dependencies in human mobility data, we integrate the transformer architecture [31] into our model. Specifically, we employ BERT [7] as the backbone, leveraging its robust self-attention mechanisms to understand the relationship between a user's historical and future trajectories. Additionally, BERT excels at encoding contextual information from input sequences, and thus the model is able to discern complex movement patterns over time.\nThe self-attention mechanism, defined as:\n$Attention(X) = Softmax (\\frac{QK^T}{\\sqrt{d_k}})$,\ncomputes attention weights by projecting the input sequence into query (Q), key (K), and value (V) matrices. This process allows the model to assign varying levels of importance to different time steps in historical sequences to focus on the most relevant locations when predicting future movements. The Softmax function ensures that the attention weights are normalized to effectively aggregate information across the sequence.\nFurthermore, BERT utilizes a [CLS] token to capture global information from the entire input sequence. We leverage this [CLS] token as the input for the subsequent layer, ensuring that both local and global temporal contexts are effectively utilized.\nMixture of Experts. To effectively capture diverse and long-term mobility patterns across multiple cities, we incorporate a Mixture of Experts (MoE) layer [17] into our model architecture. The MoE layer comprises multiple specialized expert networks, each"}, {"title": "2.2 Transfer Learning", "content": "Transfer learning [36] involves taking a pre-trained model on a large dataset and fine-tuning it for a different but related task. F\u00fcrst et al. [9] offers a theoretical analysis of how transfer learning operates within transformer architectures, highlighting the benefits of leveraging pre-trained models on related tasks. In human mobility prediction, transfer learning enables the adaptation of the model's learned representations to new urban environments, thereby leveraging knowledge from previously studied cities to enhance prediction accuracy in others.\nDrawing inspiration from recent advancements in dense associative memory models [9, 16], where the attention mechanism in transformers is conceptualized as a form of associative memory, we propose a transfer learning strategy tailored for multi-city mobility prediction. Specifically, we pretrain our model on a large-scale mobility dataset from one city and subsequently fine-tune it on smaller datasets from other cities. This approach allows the model to capture both general and city-specific mobility patterns.\nTo effectively learn the distinct spatial information of each city, we employ different learning rates during fine-tuning. Specifically, we set the learning rate for the location embeddings to be ten times higher than the base learning rate used for the other model parameters. This higher learning rate enables the location embeddings to rapidly adapt and capture the unique spatial characteristics of the target city. Conversely, we apply a smaller learning rate to the rest of the model parameters to preserve the general knowledge acquired during pretraining. This strategy facilitates the transfer of knowledge from one city to another, improves prediction accuracy, and addresses uneven data distributions across cities."}, {"title": "3 Experiments", "content": "In this section, we conduct a series of experiments to demonstrate the performance and transferability of ST-MoE-BERT.\nModels. In our experiments, we validate our approach using ST-MOE-BERT. We adopt the BERT model 2 with 110 million parameters. The input sequence length is set to 240, and the prediction horizon is 48. We pretrain the model using the masked language modeling (MLM) technique on mobility data from city A and then fine-tune it on mobility data from cities B, C, and D. The hyperparameters of ST-MOE-BERTare detailed in Table 2.\nDatasets. We evaluate our method on human mobility data from four metropolitan areas, labeled as cities A, B, C, and D, as provided by Yabe et al. [40]. Each city is divided into a 200 \u00d7 200 grid, where each cell corresponds to a 500-meter by 500-meter area. The mobility datasets cover a 75-day period, with individual movements recorded at 30-minute intervals within these grid cells.\nEvaluation Metrics. To evaluate the performance of predicting future mobility trajectories, we employ three metrics: Accuracy, GEO-BLEU [29], and Dynamic Time Warping (DTW) [25]. Accuracy quantifies the percentage of correctly predicted grid cells in future trajectories. GEO-BLEU is a metric that accounts for both precision and temporal alignment between the predicted and actual trajectories. DTW measures the alignment and distance between"}, {"title": "3.1 Cross-City Prediction with ST-MoE-BERT", "content": "To evaluate the efficiency of our method on cross-city prediction, we compare ST-MoE-BERTwith baseline models on predictions for cities B, C, and D. Each evaluation is conducted three times using different random seeds, and we report the average performance for each metric.\nBaselines. To evaluate the performance of our method, we use Historical Frequency (HF), naive BERT, and ST-MoE-BERTwithout pretraining to assess the efficiency of ST-MoE-BERTon cross-city prediction. HF predicts future locations using historical visit patterns based on time and weekday.\nResults. As shown in Table 1, we observe an average improvement of 10.30% in GEO-BLEU, 15.50% in DTW, and 21.40% in accuracy across three cities. It is evident that ST-MoE-BERToutperforms all baseline models in predicting mobility for cities B, C, and D. HF scores high on GEO-BLEU by matching frequent past locations but may predict positions far from the true grid. In most configurations, ST-MOE-BERTwithout pretraining (w/o PT) outperforms the vanilla BERT model, highlighting the effectiveness of the MoE mechanism in capturing complex relationships across different cities. However, there are exceptions where the naive BERT model outperforms ST-MOE-BERTwithout pretraining, such as the GEO-BLEU score in city D. One possible reason is that the naive BERT model is more attuned to the distinct data distribution of city D, which differs from the other cities."}, {"title": "3.2 Ablation Study: Impact of Transfer Learning on Prediction Performance", "content": "We conduct experiments to assess the impact of transfer learning on the final prediction results in cross-city prediction tasks. As shown in Table 1, we observe an average improvement of 8.29% in GEO-BLEU, 9.90% in DTW, and 10.76% in accuracy across the three cities. It is evident that ST-MoE-BERT, when utilizing knowledge from other cities, enhances the robustness of cross-city predictions. The only exception, where ST-MoE-BERTwithout transfer learning outperforms the model with transfer learning, is the DTW score in city B."}, {"title": "4 Discussion", "content": "We introduce ST-MOE-BERTfor predicting long-term human mobility patterns across cities. ST-MoE-BERTutilizes a transformer-based architecture with an MoE layer to capture complex spatio-temporal dependencies in mobility data. To enhance model robustness in target cities with limited data, we apply transfer learning by pretraining the model on city A. Our experiments show that ST-MOE-BERTimproves prediction accuracy by an average of 8.29% compared to state-of-the-art models. These results highlight the significance of combining transformer architectures with MoE layers and a strong transfer learning strategy for better long-term cross-city human mobility prediction.\nLimitation and Future Work. Despite its strengths, ST-MoE-BERTrelies on a large-scale dataset for pre-training which may limit the model's applicability in scenarios where such comprehensive data is unavailable. Future work can address these limitations by exploring the integration of LLMs, which have shown promise in capturing intricate and multifaceted aspects of human behavior."}, {"title": "A Related Work", "content": "In this section, we present a concise overview of human mobility prediction and the application of foundation models for time series classification.\nHuman Mobility Prediction. Researchers have applied physics-based models to understand human mobility, depicting movements as scale-free L\u00e9vy flights with distances following a truncated power-law distribution [4, 28]. Patterns such as frequently revisited locations align with Zipf's Law [12], while the number of unique places visited grows sublinearly [30]. Although Markov models have been used to predict future locations based solely on the current state [27], they struggle to capture the complex spatio-temporal dependencies and varied travel behaviors in human mobility.\nWith the advancement of deep learning, more sophisticated models have been developed to address these limitations. Recurrent neural networks, such as STRNN [21], were applied to mobility"}, {"title": "B Experiment Setting", "content": ""}, {"title": "B.1 Hyperparameters of ST-MoE-BERT", "content": "The hyperparameters of ST-MoE-BERTare presented in Table 2."}, {"title": "B.2 Data Completeness Rate", "content": ""}, {"title": "B.3 Distribution of Individuals Across Cities", "content": ""}, {"title": "B.4 Evaluation metrics", "content": "Given two sequences, X = (x1, x2,...,xn) and Y = (Y1, Y2, ..., \u0423\u0442):\nGEO-BLEU: GEO-BLEU extends the BLEU metric to geographic data. It evaluates n-gram similarities between sequences using the formula:\n$GEO-BLEU = BP * exp \\bigg( \\sum_{n=1}^{N} w_n log q_n \\bigg)$\nwhere BP is the brevity penalty, wn are the weights for each n-gram level, and qn is the geometric mean of the n-gram similarities between the sequences.\nDTW: DTW measures the similarity between X and Y by finding the optimal alignment that minimizes the sum of Euclidean distances between corresponding elements. It is defined as:\n$DTW(X, Y) = min \\sum_{(i,j) \\in c} dist(x_i, y_j)$\nwhere c denotes the alignment path and dist(xi, yj) is the Euclidean distance between xi and yj."}]}