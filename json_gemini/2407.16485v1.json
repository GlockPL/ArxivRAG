{"title": "Learning General Continuous Constraint from\nDemonstrations via Positive-Unlabeled Learning", "authors": ["Baiyu Peng", "Aude Billard"], "abstract": "Planning for a wide range of real-world tasks necessitates to know\nand write all constraints. However, instances exist where these constraints are\neither unknown or challenging to specify accurately. A possible solution is to\ninfer the unknown constraints from expert demonstration. The majority of prior\nworks limit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint from\ndemonstration. From a PU learning view, We treat all data in demonstrations\nas positive (feasible) data, and learn a (sub)-optimal policy to generate high-\nreward-winning but potentially infeasible trajectories, which serve as unlabeled\ndata containing both feasible and infeasible states. Under an assumption on\ndata distribution, a feasible-infeasible classifier (i.e., constraint model) is learned\nfrom the two datasets through a postprocessing PU learning technique. The\nentire method employs an iterative framework alternating between updating the\npolicy, which generates and selects higher-reward policies, and updating the\nconstraint model. Additionally, a memory buffer is introduced to record and reuse\nsamples from previous iterations to prevent forgetting. The effectiveness of the\nproposed method is validated in two Mujoco environments, successfully inferring\ncontinuous nonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety.\nKeywords: Constraints inference, Inverse reinforcement learning, Learning from\nexpert demonstrations", "sections": [{"title": "1 Introduction", "content": "Planning for many robotics and automation tasks also requires knowing constraints explicitly,\nwhich define what states or trajectories are allowed or must be avoided [1, 2]. Sometimes these\nconstraints are initially unknown or hard to specify mathematically, especially when they are\ngeneral, continuous, or inherent to an expert's preference and experience. For example, human\ndrivers may determine an implicit minimum distance from other cars based on traffic conditions,\ntraffic rules, and even weather. To learn a driving policy matching human behaviors, an explicit\nconstraint should be inferred somehow, e.g., from existing human demonstration sets.\nA typical approach to recover the underlying constraint is through Inverse Constrained\nReinforcement Learning (ICRL) [3, 4, 5]. It originates from Inverse Reinforcement Learning (IRL),\na method to infer a reward function by observing the demonstrations of an expert [6]. In contrast\nto IRL, ICRL is developed exclusively to infer constraints instead of reward functions. A current"}, {"title": "Related Works:", "content": "Constraint inference through ICRL has drawn more and more attention since\n2018. Chou et al. [7] first explores how to infer constraints given an environment model and a\ndemonstration set. They assume that all possible trajectories that could earn higher rewards than the\ndemonstration must be constrained in some way, or the expert could have passed those trajectories.\nIn practice, they use hit-and-run sampling to obtain such higher reward trajectories and solve an\ninteger program to recover the constrained states in a tabular environment. Scobee and Sastry\n[8] formalizes this idea by casting the problem in the maximum likelihood inference framework,\nwhich is also prevalent in the IRL domain. They introduce a Boltzmann policy model, where the\nlikelihood of any feasible trajectory is assumed to be proportional to the exponential return of the\ntrajectory, while the likelihood of any infeasible trajectory is 0. Then a greedy algorithm is proposed\nto add the smallest number of constraints that maximize the likelihood of the demonstrations. This\nframework has the advantage of being able to work with sub-optimal demonstrations. Alternatively,\nVazquez-Chanlatte et al. [9] formulate a maximum a posterior probability inference problem to\nlearn non-Markovian constraint, or task specifications. Further, Glazier et al. [10] and McPherson\net al. [11] extend the maximum entropy framework from a deterministic setting into a stochastic\nsetting, with soft or probabilistic constraints. In [10], they learn a residual reward (i.e., penalty)\nfrom demonstration using maximum entropy IRL. Assuming the penalty value follows a logistic\ndistribution, the probability of constraint over features can be recovered. In [11], a causal entropy\nlikelihood function is formulated and optimized via a modified version of soft Bellman backup,\nwhich is challenging to scale to continuous state-action spaces.\nUnfortunately, all the methods mentioned above only apply to systems with discrete finite state\nspaces. To overcome this drawback, recent papers have made a few attempts to extend this to\ncontinuous cases. Stocking et al. [12] proposes an algorithm that learns a neural network policy\nvia deep RL and generates high return trajectories with the policy network. Then the constraint\nis again added based on the maximum likelihood principle. However, like the previous methods,\nthis method recovers constraints from a given constraint set. Besides, the method only works\nwith a categorical action policy, not applied to a continuous policy. Recently, Malik et al. [3]\nproposes Maximum Entropy Constraint Learning (MECL) algorithm. It not only approximates\nthe policy with a network but also learns a constraint network, a continuous function that can\nrepresent more general constraints. This constraint network is optimized by making a gradient ascent"}, {"title": "2 Method", "content": "on the maximum likelihood objective function. Although theoretically the method can arbitrary\nconstraint on continuous state spaces and constraints, in practice it was only applied to recover\nplane constraints, e.g. $x > -3$ [3], and, as we show later in the paper, performs relatively poorly\nat learning more complex constraints. This is partly due to an unrecognized problem of constraint\nforgetting that we discuss later in section 2.4. In parallel, in Inverse Optimal Control community,\nanother line of research based on KKT optimality condition has been developed to learn general\ncontinuous constraints [13, 14], but these methods require a closed-form model and its derivative,\nwhich are not always available in reality.\nIn summary, despite impressive advances to solve ICRL, it still remains to be shown that ICRL can\nbe used to learn continuous nonlinear constraint functions from a small sample of demonstrations in\na model-free manner.\nTo overcome the aforementioned challenge, we propose a novel positive-unlabeled approach to learn\nan arbitrary and continuous constraint function in continuous state-action spaces. Our method is\ninspired by a machine learning subarea Positive-Unlabeled (PU) learning [15], and to best of our\nknowledge, this work is the first to handle the constraint inference problem from a PU learning\nperspective. We treat all data in demonstrations as positive (feasible) dataset, and learn a policy\nto generate many potentially unsafe trajectories, which contains unlabeled states that could be\nfeasible or infeasible. Then a postprocessing PU learning technique is applied to identify the truly\ninfeasible states from the two dataset. It makes an assumption on data distribution, and synthesises\nthe feasible-infeasible classifier (i.e., constraint model) from a feasible-unlabeled classifier with a\nmoved classification threshold.\nTo generate potentially unsafe trajectories, we also maintain a constrained RL policy that always\nmaximizes the reward function subject to the current constraint function. If the policy achieves a\nhigher reward than the demonstration assumed to be (sub-)optimal, we believe it implies that the\npolicy must have explored certain unknown unsafe states to win such a high reward [7].\nOur constraint learning paradigm is phrased as an iterative framework, see Fig. 1. At each iteration,\nwe first train a policy maximizing the reward subject to the constraints. Then, a set of higher-\nreward trajectories are generated by sampling from the policy, which are further used along with the\ndemonstrations to infer constraints via PU learning technique. In the next iteration, the new policy is\nupdated with respect to the new constraints. To prevent forgetting the previously learned constraints,\na memory mechanism is introduced, where some most representative infeasible states are recorded\nand used for training in the following interactions."}, {"title": "2.1 Preliminaries and Problem Statements", "content": "For a Markov decision process (MDP) [16, 17], states and actions are denoted by $s \\in S$ and $a \\in A$.\n$\\gamma$ denotes the discount factor, and the real-valued reward is denoted by $r(s, a)$. Note that in ICRL,\nreward is usually assumed to be given [3, 7]. A trajectory $\\tau = \\{s_1, a_1, ..., s_t, a_t\\}$ contains a\nsequence of state-action pairs in one episode. For the trajectory $\\tau$, the total discounted reward\n(return) is defined as $r(\\tau) = \\sum_{t=1}^T \\gamma^t r(s_t, a_t)$. A policy, the mapping between states and actions, is\ndenoted by $\\pi(a|s)$. Note that different from [13, 14], we do not make the assumption of possessing\na closed-form model or its derivatives of the transition dynamics. We only assume a simulator is\navailable that can simulate the dynamics and return the next state and reward, but no information\nabout the true constraint is included.\nThe true constraint set $C^*$ consists of all the actually infeasible states $C^* = \\{s \\in S | s\\text{ is actually infeasible}\\}$. Note that for simplicity of explanation, we only consider a\nstate constraint, but it can be easily extended to state-action constraints by augmenting the state\nwith the action to form an augmented state. We aim to recover the true constraint set from the\ndemonstration. Suppose we have collected a set of demonstrated trajectories $D = \\{\\tau_i\\}_{i=1}^{M_d}$ generated\nfrom an expert $\\pi^*$ navigating in the true environment. We assume that the expert $\\pi^*$ maximizes the"}, {"title": "2.2 Constraint Inference as Positive-Unlabeled Learning", "content": "return $J(\\pi) = E_{\\tau \\sim \\pi}\\{r(t)\\}$ while never visiting the truly infeasible states. To represent and learn\nan arbitrary constraint set in continuous state space, we define a continuous constraint function\n$\\zeta_\\theta(s) \\in (0, 1)$ and its induced constraint set $C_\\theta = \\{s \\in S | \\zeta_\\theta(s) \\leq \\delta\\}$, where $\\theta$ is the function\nparameter and $\\delta$ is a manually specified threshold. The constraint function $\\zeta_\\theta$ can be regarded as\nthe probability that a state is feasible, while $\\delta$ serves as the decision threshold. We further define a\nbinary constraint indicator function $c_\\theta(s)$ to represent the feasibility of a state.\n$c_\\theta(s) = \\begin{cases} 1 & \\text{if } \\zeta_\\theta(s) > \\delta \\text{ (feasible)} \\\\ 0 & \\text{if } \\zeta_\\theta(s) \\leq \\delta \\text{ (infeasible)} \\end{cases}$   (1)\nAs discussed before, we infer the underlying constraint by contrasting the demonstration with a\nhigh-reward-winning policy. In each iteration, we first sample a set of high-reward trajectories\n$P = \\{\\tau_i\\}_{i=1}^{M_p}$ by performing current policy $\\pi_\\phi$ (discussed later in 2.3). Each sampled trajectory\nconsists of sequential state-action pairs $\\tau_p = \\{s_i, a_i\\}_{i=1}^{N_i}$. We speculate that the higher-reward\ntrajectory wins high reward by violating some unknown constraints. However, it remains unclear\nwhich specific state(s) within the trajectory has violated the constraint. In another word, trajectory\n$\\tau_p$ consist of both feasible states and infeasible states but both remains unlabeled. In contrast, it is\ncertain that all states on the demonstrated trajectories are labeled as feasible. Our goal is to classify\neach single state as feasible or infeasible by learning from a batch of fully labeled feasible samples\nand another batch of unlabeled samples.\nThis insight inspires us to formulate constraint inference as a positive-unlabeled learning problem,\nwhich learns from only a positive dataset and an unlabeled dataset containing both positive and\nnegative samples [15]. Within our framework, feasible states are designated as positive samples.\nConsequently, the demonstrations serve as positive samples, while the policy offers unlabeled\nsamples with unknown feasibility. Any datapoint can be represented as a set of triplets $(s, c, l)$\nwith $s$ the state, $c$ the class (1 for feasible), and $l$ a binary variable representing whether the tuple is\nlabeled. If a sample is labeled (i.e., $l = 1$), it must originate from the demonstration and is sure to\nbe feasible, i.e., $Pr(c = 1|l = 1) = 1$. A key quantity from PU learning is the label frequency $f$,\nwhich is defined as the proportion of positive samples that are labeled within the entire dataset:\n$f = Pr(l = 1|c = 1) = \\frac{Pr(l = 1)}{Pr(c = 1)}$     (2)\nIn the constraint learning context, the label frequency $f$ implies the sparsity of the truly feasible\nstates in the state space. A high value $f$ suggests that the majority of truly feasible states are"}, {"title": "2.3 Policy Learning via Constrained RL", "content": "distributed on the demonstrated trajectories and thus labeled, while only a few amount of feasible\nstates are on the higher-reward trajectory and thus unlabeled. Subsequently, we introduce an\nassumption regarding the labeling mechanism [15]:\nAssumption 1 (Selected Completely At Random (SCAR)) Labeled samples are selected completely\nat random, independent of states. For any truly feasible state, its probability of being demonstrated\nand labeled is constant and equal to the label frequency:\n$Pr(l = 1|s, c = 1) = Pr(l = 1|c = 1) = f$    (3)\nIntuitively, the SCAR assumption requires that among all the truly feasible states, those visited more\nfrequently by the expert should also be more frequently visited by the policy. In other words, apart\nfrom truly infeasible regions, the expert and the policy should exhibit similar behaviors and have\nsimilar state distribution elsewhere. An example for SCAR assumption and PU learning is given in\nFig. 2. Consider a planar task called point-circle, a point robot is rewarded for running in a wide\ncircle, but is constrained to stay within a narrow region smaller than the radius of the target circle.\nThe high-reward policy and demonstration have similar distribution inside the truly feasible area,\ni.e., the area between the two boundary.\nSubsequently we introduce a postprocessing approach [18] for uncovering the unknown infeasible\nregions due to its 1) inherent simplicity and 2) compatibility with various models, including neural\nnetworks. Given the SCAR assumption, the probability of an sample being labeled is directly\nproportional to the probability of that sample being positive:\n$Pr(l = 1|s) = Pr(c = 1, l = 1|s) = Pr(c = 1|s)Pr(l = 1|c = 1, s) = Pr(c = 1|s)Pr(l = 1|c = 1) = f Pr(c = 1|s)$    (4)\nwhich leads to\n$Pr(c = 1 | s) = \\frac{1}{f}Pr(l = 1 | s)$  (5)\nConsequently, it is evident that a labeled-unlabeled classifier, trained to predict $Pr(l|s)$ by treating\nunlabeled data as negative and labeled data as positive, can be directly employed to forecast the class\nprobabilities $Pr(c = 1|s)$ and predict the class [18]. Alternatively, noticing that $Pr(c = 1|s) > 0.5$\nis equivalent to $Pr(l = 1|s) > 0.5f$, the labeled-unlabeled classifier can be repurposed as a feasible-\ninfeasible classifier by directly adjusting the decision threshold from $\\delta = 0.5$ to $\\delta = 0.5f$.\nBased on the above derivation, we initiate the process by training a labeled-unlabeled classifier\n$\\zeta_\\theta(s)$, which outputs the labeled probability $Pr(l = 1|s)$. It is trained through gradient descent\nutilizing the binary cross entropy (BCE) loss as in (6). Within the training set, states from $D$ serve\nas positive samples, while those from $P$ are treated as negative samples.\n$\\mathcal{L}(\\theta) = - \\frac{1}{N} \\sum_{s_i \\sim D} \\log \\zeta_\\theta(s_i) - \\frac{1}{M} \\sum_{s_j \\sim P} \\log (1 - \\zeta_\\theta(s_j))$  (6)\nOnce trained, this classifier can be directly employed as a feasible-infeasible classifier by setting the\ndecision threshold to $\\delta = 0.5f$ as in (7).\n$C_\\theta(s) = \\begin{cases} 1 & \\text{if } \\zeta_\\theta(s) > 0.5f \\text{ (feasible)} \\\\ 0 & \\text{if } \\zeta_\\theta(s) \\leq 0.5f \\text{ (infeasible)} \\end{cases}$  (7)\nThe subsequent work involves determining the label frequency $f$. Estimating $f$ is a central concern\nin PU learning, with diverse methods such as partial matching and kernel embedding being proposed\n[15]. In this work, we suggest two approaches. A conventional and straightforward approach [19]\nuses the insight that $Pr(l = 1|s) = f Pr(c = 1|s)$, which is equal to the label frequency $f$ when the"}, {"title": "2.4 Constraint Memory Replay", "content": "true class probability is $Pr(c = 1|s) = 1$. Suppose there will be state for which $Pr(c = 1|s) \\approx 1$,\nthe label frequency can be hence estimated as $f = \\max_{s \\in D} \\zeta_\\theta(s)$. Alternatively, recall that $f$\nindicates the density of infeasible states in the state space, it can be reviewed as a hyper-parameter\nrepresenting the user's belief over the constraint density. A higher value of $f$ suggests a denser\ndistribution of infeasible states. In our following experiments, we adopt the latter perspective and\nset $f = 0.4$.\nLastly, to mitigate over-fitting, we incorporate the following regularization term into the loss\nfunction (6):\n$\\mathcal{L}_r(\\theta) = - \\frac{w_r}{N_s} \\sum_{s_i \\sim \\mathcal{S}} \\zeta_\\theta(s_i)$    (8)\nwhere $w_r$ is the a fixed regularization weight. The state $s_i$ is uniformly sampled from the whole\nstate space.\nIn order to generate high-reward trajectories while satisfying the already learned constraints, an RL\npolicy network is established and maintained during the learning process. Akin to some recent works\n[5, 20], this paper adopts a popular and relatively simple algorithm PPO-penalty [21] for constrained\npolicy optimization. As shown in (9), it reshapes the original reward by turning the constraint as\na penalty term into the original reward function to avoid the infeasible states, where $w_p$ is a fixed\npenalty weight and $c(s)$ is a constraint indicator defined in (1). Hence, a constrained optimal policy\ncan be straightforwardly learned by optimizing the reshaped reward $r'(s, a)$ with the standard deep\nRL algorithm PPO [21].\n$r'(s, a) = r(s, a) - w_p c_\\theta(s)$    (9)\nIt is worth noting that some ICRL researches employ PPO-Lagrangian [3] instead of PPO-\npenalty. Although PPO-Lagrangian offers the advantage of automatically adjusting the penalty\nweight, it is also known to introduce high instability, resulting in significant oscillations in the\npolicy's performance during training and making convergence difficult [22]. When applying PPO-\nLagrangian to ICRL where the constraint function itself changes over iterations, the issue of\ninstability becomes even more detrimental: the oscillation in learning the policy will propagate\nto learning the constraint function, leading to further instability in learning the policy. Recent works\nhave also observed this problem and therefore chosen the penalty method instead of the Lagrangian\nmethod [5, 20].\nThe learning is in an iterative framework alternating between learning constraint and learning policy.\nNormally, it can be very time-consuming to train the policy until convergence in every iteration.\nSimilar to related works [3, 4], we only perform limited timesteps in policy updating to save time.\nHowever, in practice, we notice that this mechanism occasionally leads to catastrophic results.\nSuppose that in some iteration it accidentally learns a very poor policy. Recall that we are inferring\nthe constraint through the difference between demonstration and policy, and a poor policy obviously\ndoes not contain any meaningful information about true constraint. Therefore, the poor policy will\nin turn lead to a poor (usually redundant) constraint, and to an even poorer policy. Finally, the\nalgorithm collapses. To mitigate this drawback, we propose to introduce a policy filter (see Fig. 1)\nthat only lets pass the sampled trajectories with relatively high reward than demonstration, which\nare believed to violate the unknown constraint to be inferred. Concretely, the trajectory $\\tau_i$ can pass\nthe filter if its return satisfies\n$r(\\tau_i) \\geq r_d - \\alpha \\sigma_d$     (10)\nwhere $r_d$ and $\\sigma_d$ are respectively the mean and standard deviation of return among all demonstrated\ntrajectories, $\\alpha$ is a hyperparameter and is always selected as 1 in this work.\nWhen learning complex constraints with approximation functions in an iterative manner, a problem\nof \"constraint forgetting\", which has not been well recognized and studied in similar papers [3, 12],"}, {"title": "3 Experiment", "content": "will emerge. The forgetting problem is demonstrated in Fig. 3: suppose there are two unknown\nrectangular constrained areas shown in red. In the first iteration, by contrasting policy with\ndemonstration, the left infeasible area is uncovered. But in the second iteration, when the policy\nshifts and leaves the left infeasible area, there is no more data in that area. If we update the\nconstraint model with data only from iteration 2, the left area may become feasible again due to\nsome approximation error. This problem will become especially apparent in iteration 3: when the\npolicy and demonstration almost overlap, updating the constraint network with data from iteration\n3 will have a very random influence over the model. The noise in policy learning and function\napproximation may well make the model forget the already learned constrained areas.\nWe call the above phenomenon a \"forgetting\" problem. Note that this is not a problem if we are\nrecovering finite infeasible states from a discrete grid world, where the infeasible states are added\nto the constraint set in a one-by-one manner, and the constraint once learned will be kept forever\n[7, 8]. But when learning constraints with a continuous approximation function, the constrained area\nlearned in previous iterations may be forgotten due to noise in function approximation and policy.\nOne possible remedy is to enhance the exploration and try multiple strategies at the same time to\ncover more state spaces. But as discussed in [12], a usual Gaussian policy is not enough and a\ncategorical discrete action policy is necessary, which does not apply to continuous action space.\nTo mitigate this problem, we propose the constraint memory replay mechanism (CMR). From\nFig. 3, we speculate the sampled trajectories at each iteration contain information about a certain\nconstrained area. Ideally, if we record some of the trajectories $P_m \\subset P$ in each iteration and keep\nusing them for training in the following iterations, the constraint will not be forgotten. In practice,\ninstead of recording the whole trajectories, we find that it is better to record only a bath of states\nthat are most likely to be infeasible, because 1) it is beneficial to avoid overfitting (i.e., learning\nan overly large constrained area); 2) it saves the memory and computation resources. Specifically,\nin each iteration, we rank from low to high all the infeasible states from sampled trajectories (i.e.,\n$\\left\\{s \\in P | C_\\theta(s) = 0\\right\\}$) by their constraint value $\\zeta_\\theta(s)$, and save only the top $1/N_m$ portion of states\ninto a memory buffer $M$ ($N_m$ is a parameter). These recorded states have the lowest $\\zeta_\\theta(s)$ values,\nand are regarded as representatives of the constraint learned in that iteration."}, {"title": "3.1 Experiment Setup", "content": "Environments: Two environments, namely point-circle, point-obstacle and ant-wal, have been\nconsidered to examine the performance of the proposed method (see Fig. 4). In both environments,\nthe agent is a point robot moving in 2-D plane. In the point-circle, the aim is to encourage the agent\nto follow a circle; however, the agent is constrained to stay within a narrow region smaller than the\ndefined circle. In the point-obstacle, the agent is initialized from somewhere at the bottom of the\nenvironment and is rewarded for reaching a target above while avoiding a rectangular obstacle in the"}, {"title": "3.2 Results", "content": "middle. Note that the constraints in the two environments are more complex and nonlinear compared\nwith those considered in the related papers [3, 4], which only learns a plain constraint like $x \\geq -3$.\nThe expert demonstration set consists of 20 safe trajectories generated by an entropy-regularized RL\nagent, trained assuming full knowledge of the true constraint.\nMetrics: Several different metrics have been employed in the literature to quantify the correctness of\nthe learned constraint. Recent methods working with neural network constraints often consider the\npolicy's performance, such as constraint violation or reward, when trained with the NN constraint\nand tested in the true constrained environment [3, 4, 20]. However, we argue that solely comparing\nthe performance of the learned policy may lead to misleading results, as the performance is\nsignificantly influenced by the performance of specific constrained RL algorithm employed. A\ndetailed discussion of metric selection, accompanied by an example of misleading results of a prior\npaper [3], is provided in Appendix 5.2. In this study, we present the evaluation of policy learning\nperformance by utilizing two metrics: 1) the IoU (the intersection over union) index to measure the\ncorrectness of the learned constraints; we uniformly sample points in the state space, and compute\nIoU as (Number of points which are both predicted to be infeasible and truly infeasible) divided by\n(Number of points either predicted to be infeasible or truly infeasible); 2) the per-step true constraint\nviolation rate of the learned policy. Additionally, we provide visualizations of the constraint network\noutput to offer an intuitive understanding of the learning outcomes.\nWe compare the constraint learning performance of the three methods: 1) the propose method, 2) the\nproposed method without CMR (constraint memory replay), and 3) a popular ICRL method MECL\n[3]. The IoU index and constraint violation rate in the first two environments are presented in Fig. 6.\nAll the results are the average of 5 independent runs, and the shaded area represents the variance. The\n5 runs use the same demonstrations and hyper-parameters, but differ in the initialization parameters\nfor the policy and constraint networks.\nOverall Performance: The proposed methods exhibit superior performance compared to the\nbaseline across both environments and metrics. This distinction is particularly large in Point-\nObstacle, characterized by a nonlinear and non-convex constraint. Besides, the baseline suffers from\nstrong performance oscillation, while our method achieves a more stable learning process. This is\npartly caused by the forgetting problem we discussed in 2.4, i.e., it keeps learning and forgetting the\nconstraint during training.\nAblation Study of Constraint Memory Replay: Comparing the performance of our method and\nour method without CMR, we conclude the CMR technique enhances the precision of learned\nconstraints. This improvement is particularly evident in the Point-Obstacle environment in terms of\nIoU, where the method without CMR undergoes a decline in performance (i.e., constraint forgetting)"}, {"title": "4 Conclusions", "content": "around 2e6 and 6e6 timesteps. However, the performance gap in terms of constraint violation is\nrelatively smaller, primarily because constrained states in proximity to the boundary are frequently\nvisited and less prone to forgetting, thus having less influence on the constraint violation. This\nobservation also aligns with our illustrative insights provided in Figure 3.\nVisualization: To provide readers with a better understanding, we include visualizations of the\nlearned constraint when our method reaches convergence. Comparing the learned constraints in\nFig. 5 with the true constraints displayed in Fig. 4 confirms the effectiveness of our method in\nacquiring a model of both linear and nonlinear constraints. While the learned constraint area is\npartially incorrect, it effectively captures the essence of the true constraint and proves sufficient for\ntraining a safe policy. Additionally, we observe that the memorized data points are distributed within\nthe infeasible areas, thereby aiding in preventing the constraint network from forgetting previously\nlearned constraints. These visualizations provide a clear demonstration of the advantages offered by\nthe memory introduced in 2.4.\nThis paper proposed an positive-unlabeled constraint Learning method to infer from demonstration\nan arbitrary continuous constraint on continuous state spaces. The proposed method treats the\ndemonstration as the positive data and the higher-reward-winning policy as the unlabeled data, and\nthus trains a feasibility classifier from the two datasets via a postprocessing PU learning technique.\nIn addition, a memory mechanism was introduced to prevent forgetting. The benefits of the proposed\nmethod were demonstrated in two robotics tasks. It managed to recover the continuous nonlinear\nconstraints and outperforms a baseline method in terms of accuracy and constraint violation. In\nthe future, we will apply the proposed method to more high-dimensional environments with more\ncomplex constraints."}, {"title": "5 Appendix", "content": "To further verify this, we repeat the same experiment for BC but with a feasibility decision threshold\nof $d = 0.2$, i.e., only $\\zeta(s) < 0.2$ will be regarded as infeasible and accumulate cost. This modified\nBC is called BC2. The value of the Lagrange multiplier becomes constant after a while (see Fig.\n7(d)), and the reward will eventually reach the same level as that of the MECL case, as shown in\nFig. 7(c).\nGiven the above reasoning, we conclude that the difference in the reward metric between MECL\nand BC is mainly attributed to the policy learning procedure, not the learned constraint since both\nMECL and BC achieve the same constraint learning performance as demonstrated in Fig. 7(a). This\nconclusion is contradicted to the previous paper [3]. And it is important to also directly evaluate the\nlearned constraint, which is less affected by the performance of the constrained RL adopted.\nMost papers evaluate the constraint with only one of the two described classes. As mentioned in\n3.1, both categories of metrics are utilized in this work. In the first category, IoU (the intersection\nover union, also known as the Jaccard index) is employed to quantify the correctness of the learned\nconstraints. IoU is widely used in the object detection domain. Recovering the area of the state space\nthat corresponds to the constraint bears similarity to finding the region of the image that contains\nan object. Therefore, this paper suggests utilizing the IoU as a metric to evaluate the precision of\nthe learned constraint since it encapsulates both the shape of the learned constraint and its location.\nIn the second category, the violation rate of the true constraint by the trained policy is chosen as a\nmetric."}, {"title": "5.1 Detailed Experiment Setup", "content": "As shown in Fig. 4, two environments, named point-circle and point-obstacle, have been developed\nin MuJoCo for assessing the performance of the proposed framework. In both environments, the\nstate space is three-dimensional, i.e., the state vector is $s := [x, y, \\psi]^T$, where $x, y$ are the positional\ncoordinates of the point robot in the plane, and $\\psi$ is the heading angle. Moreover, the action vector in\nboth environments is the two-dimensional vector $a := [||v||_2, \\omega]$, where $||v||_2$ is the magnitude of\nthe linear velocity, and $\\omega$ is the angular velocity. Both of the actions are limited to the [-0.25, 0.25]\nrange.\nThe agent in the point-circle environment is a point robot that is rewarded to follow a circle with a\nradius of $d = 10$ in a clockwise trend. However, there exist two walls at $x = \\pm 6$, which prevents\nthe full circular motion encouraged by the reward and forces the agent to remain within $-6 <\nx \\leq 6$. The corresponding reward function is formulated in (11), where $d_x = ||v||_2 \\cos(\\psi)$ and\n$d_y = ||v||_2 \\sin(\\psi)$.\n$r(s) = \\frac{yd_x - xd_y}{1+ \\left|| |[x, y] ||_2-d|\\right|||[x, y] ||_2^T}$   (11)\nIn the point-obstacle environment, the same point robot is tasked to reach a target G at $[G_x, G_y] =$\n[0, 10] starting from a random position around $[x, y] = [0, -8]^T$ which is below the rectangular\nobstacle that is situated in the middle $(-2 < x < 5$ and $-2 \\leq y \\leq 2)$. In addition, it is assumed that\nthere is a known wall to the left of this obstacle such that the $x < -2$ region is inaccessible for the\nagent, meaning that the robot can only go around the obstacle from the right. The associated reward\nfunction is defined in (12), where $f = 20$ is a normalization factor. A value of 0.1 is added to this\nreward in case the agent reaches the vicinity of the target (distance smaller than 0.3).\n$r(s) = \\frac{1}{\\sqrt{(x - G_x)^2 + (y - G_y)^2} } - \\frac{1}{f}$  (12)\nA list of important hyperparameters for both environments employed in both MECL and our\nframeworks is given in 1. The hidden activation functions in both constraint and policy neural\nnetworks are Leaky ReLU. Moreover, the output activation function for the constraint network is\nthe sigmoid function, whereas that of the policy network is tanh."}, {"title": "5.2 Disscussion on Metrics for Performance Assessment", "content": "Several metrics have been proposed in the literature to measure the constraint learning performance,\ni"}]}