{"title": "Learning General Continuous Constraint from Demonstrations via Positive-Unlabeled Learning", "authors": ["Baiyu Peng", "Aude Billard"], "abstract": "Planning for a wide range of real-world tasks necessitates to know and write all constraints. However, instances exist where these constraints are either unknown or challenging to specify accurately. A possible solution is to infer the unknown constraints from expert demonstration. The majority of prior works limit themselves to learning simple linear constraints, or require strong knowledge of the true constraint parameterization or environmental model. To mitigate these problems, this paper presents a positive-unlabeled (PU) learning approach to infer a continuous, arbitrary and possibly nonlinear, constraint from demonstration. From a PU learning view, We treat all data in demonstrations as positive (feasible) data, and learn a (sub)-optimal policy to generate high-reward-winning but potentially infeasible trajectories, which serve as unlabeled data containing both feasible and infeasible states. Under an assumption on data distribution, a feasible-infeasible classifier (i.e., constraint model) is learned from the two datasets through a postprocessing PU learning technique. The entire method employs an iterative framework alternating between updating the policy, which generates and selects higher-reward policies, and updating the constraint model. Additionally, a memory buffer is introduced to record and reuse samples from previous iterations to prevent forgetting. The effectiveness of the proposed method is validated in two Mujoco environments, successfully inferring continuous nonlinear constraints and outperforming a baseline method in terms of constraint accuracy and policy safety.", "sections": [{"title": "1 Introduction", "content": "Planning for many robotics and automation tasks also requires knowing constraints explicitly, which define what states or trajectories are allowed or must be avoided [1, 2]. Sometimes these constraints are initially unknown or hard to specify mathematically, especially when they are general, continuous, or inherent to an expert's preference and experience. For example, human drivers may determine an implicit minimum distance from other cars based on traffic conditions, traffic rules, and even weather. To learn a driving policy matching human behaviors, an explicit constraint should be inferred somehow, e.g., from existing human demonstration sets.\nA typical approach to recover the underlying constraint is through Inverse Constrained Reinforcement Learning (ICRL) [3, 4, 5]. It originates from Inverse Reinforcement Learning (IRL), a method to infer a reward function by observing the demonstrations of an expert [6]. In contrast to IRL, ICRL is developed exclusively to infer constraints instead of reward functions. A current"}, {"title": "2 Method", "content": "2.1 Preliminaries and Problem Statements\nFor a Markov decision process (MDP) [16, 17], states and actions are denoted by $s \\in S$ and $a \\in A$.\n$\\gamma$ denotes the discount factor, and the real-valued reward is denoted by $r(s, a)$. Note that in ICRL, reward is usually assumed to be given [3, 7]. A trajectory $\\tau = \\{s_1, a_1,...,s_t,a_t\\}$ contains a sequence of state-action pairs in one episode. For the trajectory $\\tau$, the total discounted reward (return) is defined as $r(\\tau) = \\sum_{t=1}^T\\gamma^t r(s_t, a_t)$. A policy, the mapping between states and actions, is denoted by $\\pi(a|s)$. Note that different from [13, 14], we do not make the assumption of possessing a closed-form model or its derivatives of the transition dynamics. We only assume a simulator is available that can simulate the dynamics and return the next state and reward, but no information about the true constraint is included.\nThe true constraint set $C^*$ consists of all the actually infeasible states $C^* = \\{s\\in S | s$ is actually infeasible$\\}$. Note that for simplicity of explanation, we only consider a state constraint, but it can be easily extended to state-action constraints by augmenting the state with the action to form an augmented state. We aim to recover the true constraint set from the demonstration. Suppose we have collected a set of demonstrated trajectories $D = \\{\\tau_i\\}_{i=1}^{M_d}$ generated from an expert $\\pi^*$ navigating in the true environment. We assume that the expert $\\pi^*$ maximizes the"}, {"title": "2.2 Constraint Inference as Positive-Unlabeled Learning", "content": "As discussed before, we infer the underlying constraint by contrasting the demonstration with a high-reward-winning policy. In each iteration, we first sample a set of high-reward trajectories $\\mathcal{P} = \\{\\tau_i\\}_{i=1}^{M_p}$ by performing current policy $\\pi_{\\phi}$ (discussed later in 2.3). Each sampled trajectory consists of sequential state-action pairs $\\tau_p = \\{s_i, a_i\\}_{i=1}^{N_i}$. We speculate that the higher-reward trajectory wins high reward by violating some unknown constraints. However, it remains unclear which specific state(s) within the trajectory has violated the constraint. In another word, trajectory $\\mathcal{P}_p$ consist of both feasible states and infeasible states but both remains unlabeled. In contrast, it is certain that all states on the demonstrated trajectories are labeled as feasible. Our goal is to classify each single state as feasible or infeasible by learning from a batch of fully labeled feasible samples and another batch of unlabeled samples.\nThis insight inspires us to formulate constraint inference as a positive-unlabeled learning problem, which learns from only a positive dataset and an unlabeled dataset containing both positive and negative samples [15]. Within our framework, feasible states are designated as positive samples. Consequently, the demonstrations serve as positive samples, while the policy offers unlabeled samples with unknown feasibility. Any datapoint can be represented as a set of triplets $(s, c, l)$ with $s$ the state, $c$ the class (1 for feasible), and $l$ a binary variable representing whether the tuple is labeled. If a sample is labeled (i.e., $l = 1$), it must originate from the demonstration and is sure to be feasible, i.e., $Pr(c = 1|l = 1) = 1$. A key quantity from PU learning is the label frequency $f$, which is defined as the proportion of positive samples that are labeled within the entire dataset:\n$f = Pr(l = 1|c = 1) = \\frac{Pr(l = 1)}{Pr(c = 1)}$\nIn the constraint learning context, the label frequency $f$ implies the sparsity of the truly feasible states in the state space. A high value $f$ suggests that the majority of truly feasible states are"}, {"title": "2.3 Policy Learning via Constrained RL", "content": "In order to generate high-reward trajectories while satisfying the already learned constraints, an RL policy network is established and maintained during the learning process. Akin to some recent works [5, 20], this paper adopts a popular and relatively simple algorithm PPO-penalty [21] for constrained policy optimization. As shown in (9), it reshapes the original reward by turning the constraint as a penalty term into the original reward function to avoid the infeasible states, where $w_p$ is a fixed penalty weight and $c_{\\theta}(s)$ is a constraint indicator defined in (1). Hence, a constrained optimal policy can be straightforwardly learned by optimizing the reshaped reward $r'(s, a)$ with the standard deep RL algorithm PPO [21].\n$r'(s, a) = r(s, a) - w_pc_{\\theta}(s)$\nIt is worth noting that some ICRL researches employ PPO-Lagrangian [3] instead of PPO- penalty. Although PPO-Lagrangian offers the advantage of automatically adjusting the penalty weight, it is also known to introduce high instability, resulting in significant oscillations in the policy's performance during training and making convergence difficult [22]. When applying PPO-Lagrangian to ICRL where the constraint function itself changes over iterations, the issue of instability becomes even more detrimental: the oscillation in learning the policy will propagate to learning the constraint function, leading to further instability in learning the policy. Recent works have also observed this problem and therefore chosen the penalty method instead of the Lagrangian method [5, 20].\nThe learning is in an iterative framework alternating between learning constraint and learning policy. Normally, it can be very time-consuming to train the policy until convergence in every iteration. Similar to related works [3, 4], we only perform limited timesteps in policy updating to save time. However, in practice, we notice that this mechanism occasionally leads to catastrophic results. Suppose that in some iteration it accidentally learns a very poor policy. Recall that we are inferring the constraint through the difference between demonstration and policy, and a poor policy obviously does not contain any meaningful information about true constraint. Therefore, the poor policy will in turn lead to a poor (usually redundant) constraint, and to an even poorer policy. Finally, the algorithm collapses. To mitigate this drawback, we propose to introduce a policy filter (see Fig. 1) that only lets pass the sampled trajectories with relatively high reward than demonstration, which are believed to violate the unknown constraint to be inferred. Concretely, the trajectory $\\tau_i$ can pass the filter if its return satisfies\n$r(\\tau_i) \\geq r_d - \\alpha \\sigma_d$\nwhere $r_d$ and $\\sigma_d$ are respectively the mean and standard deviation of return among all demonstrated trajectories, $\\alpha$ is a hyperparameter and is always selected as 1 in this work."}, {"title": "2.4 Constraint Memory Replay", "content": "When learning complex constraints with approximation functions in an iterative manner, a problem of \"constraint forgetting\", which has not been well recognized and studied in similar papers [3, 12], will emerge. The forgetting problem is demonstrated in Fig. 3: suppose there are two unknown rectangular constrained areas shown in red. In the first iteration, by contrasting policy with demonstration, the left infeasible area is uncovered. But in the second iteration, when the policy shifts and leaves the left infeasible area, there is no more data in that area. If we update the constraint model with data only from iteration 2, the left area may become feasible again due to some approximation error. This problem will become especially apparent in iteration 3: when the policy and demonstration almost overlap, updating the constraint network with data from iteration 3 will have a very random influence over the model. The noise in policy learning and function approximation may well make the model forget the already learned constrained areas.\nWe call the above phenomenon a \"forgetting\" problem. Note that this is not a problem if we are recovering finite infeasible states from a discrete grid world, where the infeasible states are added to the constraint set in a one-by-one manner, and the constraint once learned will be kept forever [7, 8]. But when learning constraints with a continuous approximation function, the constrained area learned in previous iterations may be forgotten due to noise in function approximation and policy. One possible remedy is to enhance the exploration and try multiple strategies at the same time to cover more state spaces. But as discussed in [12], a usual Gaussian policy is not enough and a categorical discrete action policy is necessary, which does not apply to continuous action space.\nTo mitigate this problem, we propose the constraint memory replay mechanism (CMR). From Fig. 3, we speculate the sampled trajectories at each iteration contain information about a certain constrained area. Ideally, if we record some of the trajectories $\\mathcal{P}_m \\subset \\mathcal{P}$ in each iteration and keep using them for training in the following iterations, the constraint will not be forgotten. In practice, instead of recording the whole trajectories, we find that it is better to record only a bath of states that are most likely to be infeasible, because 1) it is beneficial to avoid overfitting (i.e., learning an overly large constrained area); 2) it saves the memory and computation resources. Specifically, in each iteration, we rank from low to high all the infeasible states from sampled trajectories (i.e., $\\{s \\in \\mathcal{P}|c_{\\theta}(s) = 0\\}$) by their constraint value $\\zeta_{\\theta}(s)$, and save only the top $1/N_m$ portion of states into a memory buffer $\\mathcal{M}$ ($N_m$ is a parameter). These recorded states have the lowest $\\zeta_{\\theta}(s)$ values, and are regarded as representatives of the constraint learned in that iteration."}, {"title": "3 Experiment", "content": "3.1 Experiment Setup\nEnvironments: Two environments, namely point-circle, point-obstacle and ant-wal, have been considered to examine the performance of the proposed method (see Fig. 4). In both environments, the agent is a point robot moving in 2-D plane. In the point-circle, the aim is to encourage the agent to follow a circle; however, the agent is constrained to stay within a narrow region smaller than the defined circle. In the point-obstacle, the agent is initialized from somewhere at the bottom of the environment and is rewarded for reaching a target above while avoiding a rectangular obstacle in the"}, {"title": "5 Appendix", "content": "5.1 Detailed Experiment Setup\nAs shown in Fig. 4, two environments, named point-circle and point-obstacle, have been developed in MuJoCo for assessing the performance of the proposed framework. In both environments, the state space is three-dimensional, i.e., the state vector is $s := [x, y, \\psi]^T$, where $x, y$ are the positional coordinates of the point robot in the plane, and $\\psi$ is the heading angle. Moreover, the action vector in both environments is the two-dimensional vector $a := [||v||_2, \\omega]$, where $||v||_2$ is the magnitude of the linear velocity, and $\\omega$ is the angular velocity. Both of the actions are limited to the $[-0.25, 0.25]$ range.\nThe agent in the point-circle environment is a point robot that is rewarded to follow a circle with a radius of $d = 10$ in a clockwise trend. However, there exist two walls at $x = \\pm 6$, which prevents the full circular motion encouraged by the reward and forces the agent to remain within $-6 < x \\leq 6$. The corresponding reward function is formulated in (11), where $dx = ||v||_2 \\cos(\\psi)$ and $dy = ||v||_2 \\sin(\\psi)$.\n$r(s) = \\frac{y dx - x dy}{1+ || || [x, y] ||^2 - d| } - || [x, y] ||^2$\nIn the point-obstacle environment, the same point robot is tasked to reach a target G at $[G_x, G_y] = [0, 10]^T$ starting from a random position around $[x, y] = [0, -8]^T$ which is below the rectangular obstacle that is situated in the middle $(-2 < x < 5$ and $-2 \\leq y \\leq 2)$. In addition, it is assumed that there is a known wall to the left of this obstacle such that the $x < -2$ region is inaccessible for the agent, meaning that the robot can only go around the obstacle from the right. The associated reward function is defined in (12), where $f = 20$ is a normalization factor. A value of 0.1 is added to this reward in case the agent reaches the vicinity of the target (distance smaller than 0.3).\n$r(s) = \\frac{1}{\\sqrt{(x - G_x)^2 + (y - G_y)^2}} - f$"}]}