{"title": "Paint Outside the Box: Synthesizing and Selecting Training Data for Visual Grounding", "authors": ["Zilin Du", "Haoxin Li", "Jianfei Yu", "Boyang Li"], "abstract": "Visual grounding aims to localize the image regions based\non a textual query. Given the difficulty of large-scale\ndata curation, we investigate how to effectively learn visual\ngrounding under data-scarce settings in this paper. To ad-\ndress data scarcity, we propose a novel framework, POBF\n(Paint Outside the Box, then Filter). POBF synthesizes im-\nages by inpainting outside the box, tackling a label mis-\nalignment issue encountered in previous works. Further-\nmore, POBF leverages an innovative filtering scheme to\nidentify the most effective training data. This scheme com-\nbines a hardness score and an overfitting score, balanced\nby a penalty term. Experimental results show that POBF\nachieves superior performance across four datasets, deliv-\nering an average improvement of 5.83% and outperforming\nleading baselines by 2.29% to 3.85% in accuracy. Addi-\ntionally, we validate the robustness and generalizability of\nPOBF across various generative models, data ratios, and\nmodel architectures.", "sections": [{"title": "1. Introduction", "content": "Visual grounding [11, 12, 29, 39, 41, 63, 90, 93] is a fun-\ndamental task in vision-language understanding. The ob-\njective is to locate a specific image region based on a free-\nform textual query. However, constructing visual ground-\ning datasets [30, 47, 50, 92] with dense region-text anno-\ntations is both time-intensive and costly, which hinders the\nprogress in this field. Motivated by the difficulty of large-\nscale data curation, we investigate the feasibility of effec-\ntively learning visual grounding under data-scarce settings,\nsuch as with access to only 1% training data.\nWith recent advances in image generation and caption-\ning models [36, 40, 54, 55], a natural approach to address\ndata scarcity is to generate training data. The advanced gen-\nerative models, extensively pretrained on large-scale real-\nworld data, have captured the correspondence between nat-"}, {"title": "2. Problem Definition", "content": "Let $I$ and $T$ denote an input image and a natural language\ndescription, respectively. Visual grounding is the task of\nidentifying the target region $R \\subset I$ within the image that the\ndescription refer to. Typically, $R$ is represented as a bound-\ning box $B = (x,y,w,h)$, where $(x, y)$ specifies the box\ncenter coordinates, and $w$ and $h$ define its width and height.\nIn this paper, we formulate the problem of learning visual\ngrounding from data-scarce scenarios. Given a dataset $D$,\nwe aim to train a model effectively using only a small subset\ncomprising N% of the entire data, where N% \u00ab 1."}, {"title": "3. Methodology", "content": "In this paper, we introduce a novel, model-agnostic frame-\nwork called POBF (Paint Outside the Box, then Filter). An\noverview of our pipeline is provided in Figure 2. Our ap-\nproach begins with a data generation process, where we\nsynthesize images and captions using image inpainting and\ncaptioning techniques. This process utilizes the image\ngenerator and captioner pretrained on large-scale image-\ncaption pairs. Next, we train a teacher model using the lim-\nited real data. We then apply a filtering scheme to select\neffective synthetic images. Specifically, the well-trained\nteacher network assigns a hardness score, an overfitting\nscore, and a penalty term to each synthetic image. Finally,\nwe retain the synthetic images with high scores and com-\nbine them with real data to train the student model. The\nproposed POBF framework is simple, effective, and does"}, {"title": "3.1. Cross-modality Generation", "content": "Image Generation with Inpainting. We begin by syn-\nthesizing images through image inpainting, a process that\nreconstructs masked regions within an image based on a\ngiven prompt, while preserving the rest of the image. To ad-\ndress the label misalignment problem, we propose inpaint-\ning outside of the bounding box instead. For each data point\n$(I,T, B)$, we first use a captioning model $G_T$ to generate a\ncaption $C$ for $I$. The bounding box $B$, image $I$, and cap-\ntion $C$ are then input into the inpainting model $G_I$, yield-\ning an generated image $I'$. As shown in Figure 2, the red\nbus within the bounding box remains unchanged, allowing\nit to continue serving as the ground truth. The synthetic\nimage $I'$ can be paired with the original text $T$ and bound-\ning box $B$ to form a new training sample $(I',T, B)$. By\ngenerating K synthetic images per real sample, denoted as\n${I_1, I_2, ..., I_k}$, the training dataset effectively increases\nin size by a factor of K.\nWe utilize Stable Diffusion XL [54] as the inpainting\nmodel, which is pretrained on image-caption pairs with the\nobjective of predicting masked regions. To increase diver-\nsity, we set a large strength parameter, which controls the\namount of noise added to the base image thereby affects the\nsimilarity of the output to the base image.\nText Generation with Captioning. To augment the origi-"}, {"title": "3.2. Filtering Scheme", "content": "Recognizing that not all synthetic samples contribute pos-\nitively to model performance, we propose a novel filter-\ning scheme to select the most effective images for train-\ning. Initially, we use the accessible real training dataset to\ntrain a teacher model $T$. Then, $T$ evaluates synthetic im-\nages based on two scores and one penalty term to determine\ntheir effectiveness as training samples. We provide an il-\nlustration in Figure 3 to demonstrate the input used when\ncalculating each score for a generated image. For clarity\nin the following discussions, we assume a real data sample\n$(I,T, B)$ and denote the set of images generated from it as\n${I_1, I_2, ..., I_K}$.\nHardness Score. We propose to filter harmful synthetic\nimages via computing hardness. Intuitively, in our data-\nscarce regime, it is difficult to model outliers effectively\nas fundamental patterns may not be fully captured [62].\nSince the model require easiest examples to provide coarse-\ngrained information about the target function, we priori-\ntizes synthetic images with lower losses. We define the"}, {"title": "4. Experiments", "content": "Dataset. RefCOCO [92], RefCOCO+ [92], and Ref-\nCOCOg [47, 50] datasets are derived from MSCOCO 2014.\nFollowing the common splits used in previous studies [11,\n12], we report performance on the testA (containing multi-\nple people) and the testB (containing multiple instances of\nother objects) for RefCOCO and RefCOCO+, and the umd-\ntest splits for RefCOCOg [50]. The ReferItGame dataset\n[30] is collected through an online two-player game. We re-\nport performance on the testset of ReferItGame. The statis-\ntics for these four datasets are presented in Table 1. For each\ndataset, we randomly sample a subset comprising 1% of the\nentire dataset to simulate data-scarce scenarios.\nBaselines. We compare our method with three recent data\ngeneration techniques [7, 16, 96] designed for fine-grained\nunderstanding tasks: (i) X-Paste [96] uses an ensemble of\npretrained segmentation models to identify instances from\ngenerated images, which are then copied into real images."}, {"title": "4.1. Main Results", "content": "Comparison with SOTA Baselines. Table 2 presents a\ncomparative analysis of our proposed method against sev-\neral state-of-the-art baselines across multiple benchmark\ndatasets. We have following observations. (1) Our method\noutperforms the baseline trained solely on real data by an\naverage of 5.83%, highlighting the effectiveness of our in-\npainting strategy and filtering scheme. (2) Models uti-"}, {"title": "4.2. Ablation Study", "content": "Ablation on Synthetic Data. We begin by evaluating the\neffectiveness of synthetic data by separately using gener-\nated images and texts, without any filtering scheme. We ob-\nserve that using only real data results in relatively low per-"}, {"title": "4.3. Filtering Scheme Analysis", "content": "In this section, we isolate the each component in the filter-\ning scheme without the influence of generated text, offering\ninsights into their impact on model performance. We cre-\nate two variants of our method: (i) The RANDOM method\nrandomly select images as training data. (ii) The ALL base-\nline uses all synthetic data indiscriminately for training. We\nhave following observations from Table 4."}, {"title": "4.4. Cross-modality Generative Models", "content": "We evaluate whether POBF can seamlessly integrate with\nother state-of-the-art pretrained generative models. By de-\nfault, POBF utilizes Stable Diffusion XL [54] as the im-\nage generator and BLIP ViT-L [36] as the image captioner.\nIn this experiment, we test two alternative image genera-\ntors: Stable Diffusion 2 [55] and Dreamshaper\u00b9. For image\ncaptioning, we explore replacements such as LLAVA [40]\nand ViT-GPT2\u00b2. Table 5 shows the results. We observe\nthat while the default configuration of POBF delivers ro-\nbust performance, it does not always yield the highest accu-\nracy. Specifically, model combinations using Dreamshaper\nas the image generator achieve superior outcomes, reaching\nan average accuracy of 27.63%. Furthermore, these config-\nurations consistently outperform the baseline without our\nframework. These observations highlight the generalizabil-\nity and effectiveness of our proposed framework."}, {"title": "4.5. Ratios of Training Data", "content": "Table 6 presents the performance of our proposed frame-\nwork under varying amounts of training data. We experi-\nment with two scenarios: using 0.5% and 2.0% of the orig-\ninal training data. The results demonstrate that our method"}, {"title": "4.6. Model Architectures", "content": "We evaluate the proposed POBF framework across different\nmodel architectures to assess its generalizability and effec-\ntiveness. Specifically, we experiment with TransVG [11],\nDynamic-MDTRE [60], and QRNet [90], each of which\ndiffers in terms of the number of parameters and architec-\ntural complexity. Table 7 presents the results. Although\nmodel performance generally increases with model size,\nour POBF framework consistently delivers extensive per-\nformance gains across all tested architectures. Specifically,\nDynamic-MDTRE exhibits an more pronounced improve-\nment, with average accuracy rising from 23.81% to 30.84%,\nresulting in a gain of 7.03% points. QRNet, the most com-\nplex model with 273M parameters, also benefits from a\n4.41% point increase in average accuracy. The results high-\nlight that our method is not tailored to a specific architecture\nbut instead offers consistent performance benefits across a\ndiverse set of models."}, {"title": "5. Related Work", "content": "Visual Grounding. Early approaches addressed visual\ngrounding in a two-stage framework [39, 41, 93], which\nheavily depended on the quality of proposals and large com-\nputational costs due to the large number of proposals. To\novercome the limitations, more efficient one-stage meth-\nods have been developed [11, 12, 29, 60, 63, 76, 77, 82,\n90]. TransVG [11] incorporates a DETR encoder for vi-"}, {"title": "6. Conclusions", "content": "In this paper, we introduce the POBF framework to effec-\ntively train a visual grounding model in challenging data-\nscarce scenarios. By leveraging advanced generative mod-\nels pretrained solely on image-caption pairs, we synthe-\nsize training data efficiently. Specifically, we address the\nlabel misalignment issue by inpainting outside the bound-\ning box. Additionally, to identify the most effective syn-\nthetic training samples, we propose a filtering scheme com-\nprising three components: a hardness score, an overfitting\nscore, and a penalty term. Experimental results show that\nour framework achieves an average improvement of 5.83%\nacross four datasets, outperforming leading baselines with\nan accuracy increase ranging from 2.74% to 4.35%. Ab-\nlation studies highlight that our inpainting strategy and fil-\ntering scheme provide substantial accuracy gains of 3.53%\nand 1.96%, respectively. We further validate the robust-"}, {"title": "S1(I_j) = IOU(T(I_j,T), B)", "content": null}, {"title": "S2(I_j) = 1 - IOU(T(Mask(I'_j, B), T), B)", "content": null}, {"title": "P(I_j) = IOU(T(I'_j, \\O), B)", "content": null}, {"title": "Score(I_j) = \\lambda_1 S_1(I_j) + \\lambda_2 S_2(I_j) + \\lambda_p P(I_j)", "content": null}]}