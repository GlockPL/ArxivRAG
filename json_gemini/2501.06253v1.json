{"title": "The State of Post-Hoc Local XAI Techniques for Image Processing: Challenges and Motivations", "authors": ["RECH LEONG TIAN POH", "SYE LOONG KEOH", "LIYING LI"], "abstract": "As complex Al systems further prove to be an integral part of our lives, a persistent and critical problem is the underlying black-box\nnature of such products and systems. In pursuit of productivity enhancements, one must not forget the need for various technology\nto boost the overall trustworthiness of such Al systems. One example, which is studied extensively in this work, is the domain of\nExplainable Artificial Intelligence (XAI). Research works in this scope are centred around the objective of making AI systems more\ntransparent and interpretable, to further boost reliability and trust in using them. In this work, we discuss the various motivation for\nXAI and its approaches, the underlying challenges that XAI faces, and some open problems that we believe deserve further efforts\nto look into. We also provide a brief discussion of various XAI approaches for image processing, and finally discuss some future\ndirections, to hopefully express and motivate the positive development of the XAI research space.", "sections": [{"title": "1 Introduction", "content": "As AI-driven systems increasingly back and power high-stakes decision-making in various public domains such as\nhealthcare, finance, criminal justice, and autonomous vehicles, the need for explainability becomes ever more prevalent\nand critical for end-users to take informed and accountable actions [75]. Explainable AI (XAI) is the research area that\ntackles the need for clear understanding on how a decision is made by an Al system, as typically there is a lack of\ntrust that comes from the use of black-box AI systems and products. In many cases, human users are having difficulty\ndeciphering the outcome of an Al system, this is especially important when the outcome conflicts with the human\ninterpretation. With this, XAI aims to provide human-understandable information to provide insights into an AI\nsystem's behaviour and processes.\nThere have been many industry applications such as autonomous vehicles (AV), medical, and aviation that have\nstarted to embed AI components into its operation, e.g., in the AV domain, accurate object detection is required to"}, {"title": "2 Background", "content": "To provide some context, we categorise relevant papers and tabulate them in table 1. While each paper presents their\nown take on their respective key topics within XAI research, they can be broadly categorised based according to three\ntype(s); Domain-Specific, Human-Centric, or Socio-Techinical. The papers outlined here can present overlaps in terms of\nthe categories they belong to, depending on their respective scope(s). We define the categories and describe them as the\nfollowing:\n\u2022 Domain-Specific - Survey papers that focus specifically on the XAI approaches, demands, and studies done\nwithin a certain domain (i.e. Medical, Automotive etc.), aligning key objectives and advancements of XAI to the\nrequirements of the respective key stakeholders (i.e. doctors, field experts).\n\u2022 Human-Centric - Survey papers that have a concentrated effort on the human aspects of XAI, such as inter-\npretability, transparency, and bias. Such papers have the underlying objective(s) of highlighting the need for \u03a7\u0391\u0399\nto reinforce fundamental human rights such as privacy and accountability.\n\u2022 Socio-Technical - Survey papers of this nature demonstrate the mapping between the technical elements of\nXAI such as accuracy, consistency, and robustness to societal objectives of XAI such as transparency, ease of use,\nand user experience."}, {"title": "3 Terminology in Explicable AI (XAI)", "content": "This section outlines the terms commonly seen in the field of XAI, with regards to the output explanations generated\nby XAI algorithms."}, {"title": "3.1 Explainability", "content": "This refers to the inner workings and parameters of the model that lead to its decision-making ability(s). Such information\nis usually hidden in deep and complex neural networks, giving rise to their black-box nature. Explainability can also\nrepresent the knowledge of what each node and/or attribute in a neural network, and their contribution or importance\nto the model's result, thus providing a form of justification for the decision(s) made.\nExplanations are therefore systematically generated outputs that provide more information which gives users an\ninsight into an AI system's decision making process. Such information can take many forms, such as top-k features,\nfeatures ranked in terms of influence, degree of correlation etc.\nXAI approaches typically generate explanations in the form of graphs and/or heatmaps in order to aid the visualisation\nand human comprehension. In order to produce such information, XAI approaches leverage on probing techniques or\nsurrogates [45, 67] to derive relational distributions between input and output, thus deducing some degree of causality."}, {"title": "3.2 Interpretability", "content": "While often used interchangeably with explainability, interpretability refers more to the aspect of causality, measuring\nhow accurate a machine learning model can associate or relate cause to effect, or in other words, the correlation from\ninput to output.\nTo describe an example to distinguish between explainability and interpretability, a flowchart of an AI system's\ndecision making process may allow a practitioner to understand the inter-communication between the AI model and\nthe rest of the system (interpretability), but does not provide sufficient information for an in-depth understanding of\nhow and why a certain decision was made (explainability) [36]."}, {"title": "3.3 Trustworthiness", "content": "Trustworthiness is commonly defined as the \"confidence of whether a model will act as intended when facing a given\nproblem [27].\"In other words, trust is built upon aligning user expectations to the outcomes produced by AI systems.\nTrust can also be a culmination of various aspects [27]:\n(1) Human Agency and Oversight\n\u2022 Fundamental Rights - Loss of personal data can often become a large risk that should be reduced and/or justified.\nMechanisms should be put into place to receive external feedback regarding AI systems that potentially infringe\non fundamental rights.\n\u2022 Human Agency - Users should be able to make informed autonomous decisions regarding Al systems. They\nshould be given the knowledge and tools to comprehend and interact with Al systems to a satisfactory degree\nand, where possible, be enabled to reasonably self-assess or challenge the system.\n\u2022 Human Oversight - Human supervision and discretion should be established during use of an Al system, so as\nto ensure the ability to override a decision made by the AI system in the case of failures and unwanted results.\nThe less oversight a human is allowed over an Al system, the more extensive testing and stricter governance\nis required.\n(2) Technical Robustness and Safety\n\u2022 Resilience to Attack and Security - AI systems should be well protected against vulnerabilities that could allow\nthem to be attacked by adversaries.\n\u2022 Fallback Plan and General Safety - AI systems should have contingency plans in case of failures, both to\nsafeguard data and functions that could dampen the safety of its users.\n\u2022 Accuracy - Al systems should be able to make correct judgments and/or predictions. In the case of an inaccurate\nprediction, the system should be able to indicate how likely these errors are.\n\u2022 Reliability and Reproducibility - AI systems should be able to exhibit the same behaviours when given the\nsame set of inputs repeatedly. They should also be able to work properly with a range of inputs and in a variety\nof situations, as long as they are within the scope of its intended design.\n(3) Privacy and Data Governance\n\u2022 Privacy and Data Protection - This must be ensured throughout the system's entire lifecycle. In order to\nallow users to trust the data gathering process, the data collected must not be used to unlawfully or unfairly\ndiscriminate against them.\n\u2022 Quality and Integrity of Data - Prior to training, gathered data needs to be rid of any socially constructed\nbiases, inaccuracies, errors and mistakes. The training processes and datasets used must also be tested and\ndocumented at each step of the development lifecycle.\n\u2022 Access to Data - Data protocols that govern data access should be implemented. Such protocols should dictate\ndata access rights and only allow qualified and competent personnel to access private data.\n(4) Transparency\n\u2022 Traceability - The relevant decisions, data sets and processes that are made with regards to the Al system\nshould be well labelled and documented to allow for traceability and increase in transparency.\n\u2022 Explainability - When AI systems have significant impact on people's lives, it should be possible to demand a\nreasonable explanation of the decision making process."}, {"title": "3.4 Repeatability", "content": "In terms of model performance, the model's inference result should remain the same when given the exact same inputs\nunder the exact same environment to ensure consistency and repeatablity. For example, if a job applicant reapplies to a\nfirm, assuming the exact same position, applicant resume, job acceptance criteria and employment terms, the resultant\noutcome should remain the same. Likewise, in the context of healthcare, an AI-produced diagnosis should remain the\nsame when repeatedly being provided with the same patient data as input."}, {"title": "3.5 Reproducibility", "content": "Similarly, the model's inference result should be achievable when used by different practitioners under different\nenvironments. The same behaviour should be observed in its respective explanation, with a relationship that is\nrepresentative of the AI model's decision-making process."}, {"title": "3.6 Explanation Stability", "content": "To achieve higher levels of explanation stability, an output explanation should stay relatively consistent under multiple\nexecutions or explanation generations. This is an observed shortcoming of perturbation-based explanations approaches\nsuch as [67], where explanation outputs may differ under repeated explanation generations for the same inputs."}, {"title": "3.7 Causality & Cotenability", "content": "Causality refers to the degree to which the AI system's decision-making process can be described using relationships\nbetween inputs and outputs, either graphically or visually.\nIn order for an explanation to be logically sound, we must also consider its cotenability, that is, the co-dependence\namong the various features presented in the black-box model. For example, in order for an Al system to predict an\napplicant's eligibility for a loan, consideration for the applicant's income, marital status, and credit score must be done\nin conjunction and not in isolation from one another. In practice, feature co-dependence is usually observed in most\nblack-box models.\nAnother simple example to describe this correlation would be the computation of one's Body Mass Index (BMI),\nwhere there is a co-dependence between weight and height, such that it is impossible to change the BMI result while\nholding both the height and weight constant, but it is possible to attain the same BMI result for a different set of height\nand weight measurements. This relationship is captured and described very well in [52].\nAn ideal explanation would have considerations for both cotenability and causality, such that the explanation\nrepresents both of these aspects as a causal effect of intervention."}, {"title": "3.8 Faithfulness", "content": "Faithfulness applies to both model predictions and the generated explanations. Both model outputs and their explanations\nare expected to be faithful to the ground truth represented in training data. In other words, faithfulness can also be\ninterpreted as the degree to which outputs are aligned to the facts being represented by features in input data.\nFor example, an NLP (Natural Language Processing) model is deemed as faithful if its output prediction is exactly as\nintended based on its input and the corresponding assigned label. Likewise, the explanation produced is equally as\nfaithful, if it accurately represents the reasoning or representation of words or tokens in the sentence, with relevance to\nthe correct output label."}, {"title": "4 Why Explainable AI?", "content": "The recent hype of AI systems and tools such as ChatGPT, Dall-E, BlackBox.ai, etc has resulted in using Al systems to\nmake decisions or generate supporting information and assets. This has helped boasting productivity boosts with high\nturnaround times. Al has also seen its use in various industry sectors, such as in autonomous vehicles, social networks,\nand medical systems. While being in awe of the sheer capability that AI products bring to the table, one must not forget"}, {"title": "5 Post-Hoc Local Explainability Techniques", "content": "Given the wide variety of XAI approaches that are designed for various scenarios [71], a more common and realistic\nsituation is the given access to an already trained complex model like a deep neural network, but we may not have access\nto its internal structure nor the data with which it was trained on. In this context, the model is considered black-box in\nnature, and we seek insight into how it makes decisions, using explainability techniques. This exact situation where\nexplainability techniques are performed on existing models is referred to as post-hoc explanations.\nLocal post-hoc explanations evade the problem of trying to interpret an entire Al model by focusing on just explaining\na particular subset of decisions. Many local post-hoc methods attempt to simply describe a specific narrow distribution\nof data points under consideration, rather than the entire decision-making process of a model."}, {"title": "5.1 Individual Conditional Expectation (ICE)", "content": "An individual conditional expectation or Fig. 1 shows an ICE plot [30], which takes an individual prediction and shows\nhow it would change upon varying a single feature. Essentially, it answers the question of \"What if a feature had taken\non another value?\" In terms of the input-output function, it examines the change in output for a single dimension of a\ngiven data point 1.\nICE plots have the disadvantage of only being able to examine a single feature at a time, thus being unable to take\ninto account the relationships or correlations between two or more features. It is also possible that some combinations\nof input features may not happen in an operational environment."}, {"title": "5.2 Counterfactual Explanations", "content": "While ICE plots create insight into model behaviour by visualising the effect of changing one of the model inputs by a\nset amount, counterfactual explanations [80] manipulate multiple features but only consider the behaviour within the\nvicinity of a particular input for which to explain.\nCounterfactual explanations are typically used within the context of classification. From the perspective of the\nend-user, it answers the question of \"What changes would I have to make for the model classification to be different?\" For\nexample, in the case of a declined loan application, a counterfactual explanation might indicate that the loan decision\nwould have been different if the application had two less credit cards and an extra $5000 annual income."}, {"title": "5.3 Local Interpretable Model-Agnostic Explanations (LIME)", "content": "The authors of LIME [67] proposed an implementation of local surrogate models which are trained to approximate the\npredictions of a black-box model. Instead of training a global surrogate model, LIME focuses on training local surrogate\nmodels instead, to explain individual predictions. In order to produce the data required to train the surrogate model,\nLIME only uses the black box model probing it several times and then tests what happens to the resulting predictions\nwhen variations of the input are given to the black-box model. The goal is to understand why the model made a certain\nprediction. Over time, a new dataset is generated, consisting of perturbed samples and their corresponding predictions\ngiven by the black-box model. With this new dataset, LIME then trains an interpretable model (the local surrogate)\nwhich is weighted by the distance of the sampled instances to the instance of interest. The resulting learned model then\nbecomes a good approximation of the predictions locally, but it does not have to be a good global approximation. Using\nthe local surrogate model, interpreting it will then generate explanations for individual predictions. Fig 3 illustrates an\nexample of an output explanation generated by LIME."}, {"title": "5.4 SHapley Additive explanations (SHAP)", "content": "The goal of SHAP [45] is to explain the prediction of any given instance to a black-box model by computing the\ncontribution of each feature to the resulting prediction. It uses Shapely Values [49] to produce such explanations and\nhas many variations of application for different types of Machine Learning models.\nShapley value was a method derived from coalitional game theory [49], which tells us how to fairly distribute the\npayout or loot among players of the game. This concept can be applied to machine learning, where each feature is\nconsidered to be a player and each player's payout is the influence or contribution that the individual features have on\nthe final prediction as illustrated in Fig. 4. Essentially, this produces Shapley values as an interpretable additive feature\nattribution method, thus producing a simpler, surrogate explanation model that is more linear in nature."}, {"title": "6 Challenges in XAI", "content": "This section describes some of the challenges faced in the XAI domain in various sectors. A summary and illustration is\nshown in Fig. 6, which illustrates and categorises the various high-level challenges in XAI, according to their respective\nmotivations."}, {"title": "6.1 Lack of Formalism", "content": "A major problem in the XAI space is the lack of a systematic definition and quantification, or cohesive agreement on\nwhat explainability truly is. Various different definitions exist and even disagree with one another in terms of the level\nof depth or understanding required [39, 71]. Without a satisfactory definition of explainability and/or interpretability, it\nmight not be possible to determine if new XAI approaches are better at explaining ML models and AI systems."}, {"title": "6.2 Interpretability of Explanations", "content": "Depending on the nature and the application of an Al system, the types of users who get exposed to them can vary (e.g.\ndomain experts, data scientists, key decision makers, non-experts etc.) User experience and expertise would therefore\nbe a confounding factor that directly affects the level of depth and complexity expected from an output generated\nby XAI algorithms [22]. One of the key challenges towards enhanced interpretability of explanations is the lack of\nconsideration towards bridging the gap between user expertise and depth of explanations [64, 65].\nFor example, a study done in [53] highlights that previous works in XAI for expert systems generally did not\ntake into account the knowledge and abilities of users. Additionally, the key objectives and contexts of the various\nstakeholders and users were not defined. In an effort to address this, works such as [56] have discussed that identifying\nthe users' objectives and keeping up with the agile requirements involves the collection of data from such users. It is\nalso fundamental to develop approaches to manage and detect changes to these user objectives, and the need to adapt\ntowards end-users.\nIn [12], it was mentioned that Deep Learning models often use complex concepts that are unintelligible and often\nchallenging to predict outcomes. Therefore, using such black-box systems would require contextually-aware explanations\nthat can accurately explain a decision, while also being interpretable to the users (e.g. clinician or medical expert). [43]\nsuggests that we can do the following:\n\u2022 Examine the role human-understandable information represented in Deep Learning models.\n\u2022 Analyse the features used by the Deep Learning models in predicting correct decisions, based on incorrect\nreasoning.\n\u2022 Have an understanding of the model's concepts, to reduce reliability concerns and develop trust when deploying\nthe Al system, via active stakeholder engagement and user manuals to boost familiarity with the Al system(s).\n\u2022 Consider the various stakeholders and their expectations of the Al system.\nIn summary, it is crucial to tailor explanations towards user experience and expertise. Explanations should be catered\ndifferently to different users under different contexts [28]. It is also essential to clearly define the objectives of users,\nsystems, and explanations alike. To achieve this, frequent stakeholder engagement is fundamental."}, {"title": "6.3 Implementation: Complexity vs Accuracy Trade-Offs", "content": "There seem to be an underlying notion that complex models provided more accurate outputs, but that is not necessarily\ncorrect [70]. [7] explores model interpretability in a situation where performance is coupled with model complexity.\nIt suggests that XAI techniques could help in minimising the trade-off between model complexity and its accuracy.\nHowever, [4] questions the factors that determines such a trade-off. The authors of [4] have highlighted the importance\nof discussing such trade-offs with end-users, so that they are made aware of the potential risks of mis-classification or\nmodel opacity."}, {"title": "6.4 Diversification of XAI Approaches", "content": "While there are some overlaps between XAI algorithms, each one seems to be addressing a different question [43].\nAccording to [2], a combination of various methods to obtain more detailed explanations is rarely considered. Rather\nthan using each XAI algorithm separately, we should investigate how we can use them as basic components that can\nbe interlinked and synergised to develop more innovative approaches [2]. Furthermore, this could help to provide\nexplanations and related information in simple human-interpretable language [64]. In that regard, initial efforts were\ncited in [43], where authors proposed a model that could provide both visual relevance and textual explanations [59].\nMulti-modal explanations have also been explored in [14], with the proposal of a new framework which encompasses\nXAI algorithms based on a wide range of images and vocabulary. Generated textual explanations are paired with their\ncorresponding visual regions in the image. In doing so, much better logical reasoning and interpretability were achieved\ncompared to some state-of-the-art explanation models. These studies suggest opportunities for future research, with\nthe aim of enhancing both intepretability and accuracy [43]."}, {"title": "6.5 Lack of Research in Causal Explanations", "content": "Causal justifications Creating causal explanations for Al systems that is, explaining why the algorithms generated\nthe predictions rather than how they did so can aid in improving human comprehension [60]. Furthermore, causal\nexplanations make models more resilient to adversarial attacks and increase in significance when incorporated into\ndecision-making processes [50]. But there may be contradictions between causality and performance prediction [50].\nUsing a loan application as an example, causality would deduce that a higher individual's income had a positive\nimpact on loan approval. On the other hand, this might differ from performance prediction, which considers the accuracy\nand correctness of the model under various situations. Causality may not consider for all the types of situations that\naffect model performance, hence producing contradictions or inconsistencies in some cases.\nCausal explanation, given its enhanced representation of logical reasoning, is anticipated to be at the forefront of ML\nresearch and will become an integral part of XAI literature [50]. According to a recent survey on causal interpretation\nfor ML [52], it seems that the absence of ground truth for causal explanations and verification of causal relationships\nmakes the evaluation of causal interpretability even more challenging. Therefore, more research is needed towards\nboth the development and the evaluation of causal interpretability models [52]."}, {"title": "6.6 Limitations of Current XAI Approaches", "content": "The majority of XAI works is mainly on image and textual data. While XAI research do consider other types of data,\nthey were noted to receive less attention [69]."}, {"title": "6.7 XAI for Model Evaluation & Debugging", "content": "Another challenge is if post-hoc explanations methods identify learned relationships by the model that practitioners\nknow to be incorrect, is it possible that practitioners fix these relationships learned and increase the predictive accuracy?\nFurther research in post-hoc explanations can help exploit prior knowledge to improve the predictive accuracy of the\nmodels."}, {"title": "7 Open Problems in XAI", "content": "In this section, we outline the various open problems in XAI that are very current and heavily under exploration, but\nconstantly face setbacks and thus hamper its overall development. We discuss the literature and other related works,\nwith a focus on the same overall objective of achieving various aspects of explanation quality."}, {"title": "7.1 Disagreement Problem in XAI Algorithms", "content": "Situations where explanations produced by multiple XAI approaches disagree with one another [37] were highlighted.\nFor example, they may disagree in the top-K most significant features. In such situations, practitioners might find it\nchallenging to make informed decisions about which explanation and which XAI approach to rely on. It is uncertain\nhow common the disagreement problem is in practice, given little to no study on the frequency of such disagreements,\nand for which types of XAI algorithms they may occur in. This problem is further enhanced by the inherent problem\nwith the stability of output explanations, especially in the case of post-hoc attribution-based XAI methods [45, 67, 71].\nPractitioners must address such disagreements head-on if and when they arise, because failing to do so could force\nthem to rely on misleading explanations, leading to disastrous results. Examples of such consequences include the\nadoption and utility of AI models and systems that are racially biased, believing in inaccurate model predictions, and\nsuggesting less-than-ideal recommendations to users.\nWorks such as [3, 26, 37] have taken initial approaches to quantify and justify disagreements among explanation\noutputs, accounting for data types that include image, textual, and tabular data. To quantify such disagreements,\nAlignment Metrics are used. A list of alignment metrics can be found in the list below.\n\u2022 Feature Agreement (FA) metric computes the fraction of top-K features that are common between a given post-hoc\nexplanation and the corresponding ground truth explanation.\n\u2022 Rank Agreement (RA) metric measures the fraction of top-K features that are not only common between a given\npost hoc explanation and the corresponding ground truth explanation, but also have the same position in the\nrespective rank orders.\n\u2022 Sign Agreement (SA) metric computes the fraction of top-K features that are not only common between a given\npost hoc explanation and the corresponding ground truth explanation, but also share the same polarity (direction\nof contribution) in both the explanations.\n\u2022 Signed Rank Agreement (SRA) metric computes the fraction of top-K features that are not only common between\na given post hoc explanation and the corresponding ground truth explanation, but also share the same feature\nattribution polarity (direction of contribution) and position (rank) in both the explanations.\n\u2022 Rank Correlation (RC) metric computes the Spearman's rank correlation coefficient to measure the agreement\nbetween feature rankings provided by a given post hoc explanation and the corresponding ground truth explana-\ntion.\n\u2022 Pairwise Rank Agreement (PRA) metric captures if the relative ordering of every pair of features is the same for\na given post hoc explanation as well as the corresponding ground truth explanation i.e., if feature A is more\nimportant than B according to one explanation, then the same should be true for the other explanation. More\nspecifically, this metric computes the fraction of feature pairs for which the relative ordering is the same between\nthe two explanations."}, {"title": "7.2 Evaluation Metrics for XAI Algorithms", "content": "To better understand and assess the various XAI algorithms, some studies have adopted evaluation metrics and even\ndeveloped novel approaches to benchmark and categorise the performance of XAI algorithms [3, 10, 26, 37]. In addition\nto Alignment Metrics, the following were found to be adopted for the purpose of evaluating XAI algorithms [3]:\n\u2022 Prediction Gap on Important Feature Perturbation (PGI) which measures the difference in prediction probability\nthat results from perturbing the features deemed as influential by a given post-hoc explanation.\n\u2022 Prediction Gap on Unimportant Feature Perturbation (PGU) which measures the difference in prediction probability\nthat results from perturbing the features deemed as unimportant by a given post-hoc explanation.\n\u2022 Relative Input Stability (RIS), Relative Representation Stability (RRS), Relative Output Stability (ROS) which measures\nthe maximum change in explanation, relative to the changes in the inputs, internal representations learned by\nthe model, and output prediction probabilities respectively.\nIn order to compare XAI algorithms with one another and formulate a benchmark for a specific use-case, the authors\nof [3, 26] have come up with a list of tasks for each XAI algorithm to be assessed to accomplish. Each task is suited\ntowards a quality characteristic such as Fidelity, Fragility, Stability, Fairness, and Consistency. These tasks were expressed\nas Tests, which presented the notion of functional testing practices adopted towards the evaluation of XAI algorithms.\nA list of some of the tasks are as follows:\n\u2022 Fidelity - Does the algorithms's output reflect the underlying model? : Test whether features of different\nimportance are represented correctly, and test the effect of feature product on local explanations.\n\u2022 Fragility - Is the explanation result susceptible to malicious corruption? : Attempt adversarial attacks to\nlower the importance of specific features.\n\u2022 Stability: Is the algorithm's output too sensitive towards slight changes in the data or the model? : Test\neffect of data distribution and noisy input data.\n\u2022 Stress - Can the algorithm explain models trained on larger datasets or big data? : Testing if the XA\u0399\nalgorithm is sensitive to a high number of word tokens (NLP Task), and test detection of dummy pixels in MNIST\ndataset."}, {"title": "7.3 Disagreement Problem in XAI Evaluation Metrics", "content": "While there are ongoing works to develop metrics to evaluate XAI algorithms and approaches [3, 10, 26], as well as\nworks to address such disagreements in explanation outputs [37], the problem does not end there. Such evaluation\nmetrics have also been found to have disagreements among themselves, with authors mentioning that this should serve\nas a wake-up call to the entire XAI community [8].\nAuthors of [8] performed experiments using XAI methods on a range of public empirial datasets, using the tabular\ndata to create classification models. The goal was to find a set of explanations that was deemed to be the most faithful,\nusing the set of XAI evaluation metrics selected. Across the experiments, the ranked correlations showed little consensus\non the notion of faithfulness in the explanations, and the authors claimed that this would leave end users without the\nrequired tools to make informed decisions of their XAI method selection and benchmark."}, {"title": "8 Our Thoughts and Future Directions", "content": "With all the relevant works to further enhance the capabilities and the reliability of XAI algorithms, we propose some\npotential directions that the XAI community can look into, to better meet the needs of the various stakeholders and\ntarget users of XAI algorithms."}, {"title": "8.1 Multi-Modal or Intra-Model XAI Approaches", "content": "Firstly, we see potential in the use of Mutli-Modal and Intra-Model XAI approaches, to further enhance the human\ninterpretability and the predictive capabilities of XAI algorithms. This can be achieved in the following ways:\n\u2022 Intra-Modal - A combination of existing XAI algorithm components or outputs of the same type, for the\nenhancement of XAI algorithms' explanation capabilities and human interpretability. This could refer to the\ncombinations of similar output explanations, or similar approaches to generate explanations.\n\u2022 Multi-Modal - A combination of two or more types of XAI output types (e.g. Image + Text), to better express the\ninsights that can be derived from various types of XAI outputs, to enhance human interpretability. An example of\nthis is the use of both Visual output from SHAP or LIME, and Natural Language output from a Vision-Language\nModels such as Contrastive Language-Image Pre-training (CLIP) [63]. Such modelling techniques are studied in\n[11]."}, {"title": "8.2 Enhancement of XAI Evaluation Metrics", "content": "While there have been multiple works that have expanded on evaluation metrics for XAI algorithms [3, 10, 26, 40],\nfew of these works actually focused on a human-in-the-loop approach. We advocate that such an approach is a key\nfactor that contributes to the overall quality of XAI algorithms and their outputs, since ultimately, such outputs must\nbe interpretable by various stakeholders, while also allowing researchers to better draw correlations between metric\nselection and factors that contribute towards better stakeholder satisfaction. This is particularly important for domains\nof higher levels of expertise, where even human judgment can vary. In such cases, a higher degree of explainability\nmay be required from XAI algorithms, thus warranting stricter evaluation, verification and validation measures and\nbenchmarks."}, {"title": "8.3 Towards Better Understanding of Ground Truth and Stakeholder Requirements", "content": "To date, regulations such as [77] have faced challenges pertaining to requirements for explainability and the relevant\ntechniques. While there is a mandate for the \"right to explainability\"[34], there is little to no specificity as to how such a\ndegree of explainability can be achieved. The solution to such a problem is not as straightforward as we might think,\ntherefore encouraging practitioners and developers alike to better understand the requirements of their stakeholders\n[39], as well as what exactly is deemed as an accurate representation of the ground truth. For example, in very generic\nuse-cases, something as simple as a flow chart could be sufficient to represent the decision-making process of an AI\nmodel. However in more complex and safety-critical use-cases, that is clearly insufficient, and further insights need\nto be derived for predictive maintenance, preventive measures, and remedial actions [4, 35, 60]. One can interpret\nsuch an approach in a similar manner as the Risk Analysis [58, 74] process for typical software development, where\na certain appropriate measure is taken for each and every risk identified, scaling accordingly with their respective\nseverity and impact(s). Such an approach is studied in [38], although used for a different objective. In some specific cases\nwhere domain expertise is involved, such as medical, aviation, and construction domains, stakeholder requirements are\naplenty and the ground truth is often difficult to ascertain [35]. As such, stricter explainability approaches should be\nimplemented, in accordance with the higher stakes [39]."}, {"title": "9 Conclusion", "content": "To summarize this study, we covered the baseline for the XAI domain in terms of the various motivations and challenges.\nWe provided a brief overview of post-hoc local XAI methods, then discussed the open problems and some relevant\nresearch works in the XAI domain, highlighting the advancements that have led the XAI community one step closer\ntowards attaining overall explanation reliability. To conclude the study, we have also included our personal insights on\ncertain topics, proposing further directions in which XAI studies can look into for the better advancement of the XAI\ndomain."}]}