{"title": "Comprehensive Review of Neural Differential Equations for Time Series Analysis", "authors": ["YongKyung Oh", "Seungsu Kam", "Jonghun Lee", "Dong-Young Lim", "Sungil Kim", "Alex A. T. Bui"], "abstract": "Time series modeling and analysis has become critical in various domains. Conventional methods such as RNNs and Transformers, while effective for discrete-time and regularly sampled data, face significant challenges in capturing the continuous dynamics and irregular sampling patterns inherent in real-world scenarios. Neural Differential Equations (NDEs) represent a paradigm shift by combining the flexibility of neural networks with the mathematical rigor of differential equations. This paper presents a comprehensive review of NDE-based methods for time series analysis, including neural ordinary differential equations, neural controlled differential equations, and neural stochastic differential equations. We provide a detailed discussion of their mathematical formulations, numerical methods, and applications, highlighting their ability to model continuous-time dynamics. Furthermore, we address key challenges and future research directions. This survey serves as a foundation for researchers and practitioners seeking to leverage NDEs for advanced time series analysis.", "sections": [{"title": "1 Introduction", "content": "The exponential growth of time series data across diverse domains has necessitated more sophisticated analytical approaches, because of the complex nonlinear nature, irregular sampling, missing values, and continuous latent dynamics. Traditional approaches [Box et al., 2015; Durbin and Koopman, 2012] assume regular sampling and often linear relationships, limiting their applicability to real-world scenarios. While deep learning methods, including Recurrent Neural Network (RNN) [Medsker and Jain, 1999; Rumelhart et al., 1986], Long Short-term Memory (LSTM) [Hochreiter and Schmidhuber, 1997], Gated Recurrent Unit (GRU) [Chung et al., 2014], and Transformer [Vaswani et al., 2017] have shown promise in capturing nonlinearity, they remain constrained by discrete-time formulations [Che et al., 2018; Sun et al., 2020; Weerakody et al., 2021]\nNeural Differential Equations (NDEs) emerged as a paradigm shift, offering a principled framework for continuous-time modeling with neural networks. The seminal work on Neural Ordinary Differential Equations (NODES) [Chen et al., 2018] introduced continuous-time hidden state evolution, spawning numerous advances including Neural Controlled Differential Equations (NCDEs) [Kidger et al., 2020] incorporated external control paths, and Neural Stochastic Differential Equations (NSDEs) [Han et al., 2017; Li et al., 2020; Oh et al., 2024; Tzen and Raginsky, 2019] model uncertainty through stochasticity.\nIn this survey, we provide a comprehensive review of NDE methods in time series analysis, synthesizing advancements across several interconnected areas. We begin by exploring the base model families, including NODEs, NCDEs, and NS-DEs, along with their extensions and variations. Building on this, we analyze the theoretical insights that underpin these models. We also discuss the implementation of NDE methods and analyze applications. This survey aims to provide a structured synthesis for researchers and practitioners, fostering a deeper understanding of NDEs and their potential in tackling complex time series problems."}, {"title": "2 Preliminaries", "content": "Time series modeling seeks to represent sequential data x = (x_0,x_1,...,x_n), where each x_i \u2208 R^d, as a continuous latent process z(t) \u2208 R^{d_z} over a time domain [0, T]. As shown Figure 1, while interpolation methods simply fit approximation through observations (black dots), and RNNs with zero-order hold operate in discrete time steps, NDEs learn the actual dynamics by neural network f(t) = dz(t)/dt that generated the data. By parameterizing the temporal evolution of z(t) with neural networks, NDEs provide a flexible framework for handling irregularly sampled data, missing observations, and long-horizon dynamics, often with more memory efficiency than standard discrete-time architectures [Chen et al., 2018; Kidger et al., 2020; Oh et al., 2024; Rubanova et al., 2019].\nRNNs, including LSTMs [Hochreiter and Schmidhuber, 1997] and GRUs [Chung et al., 2014], operate in discrete time, updating hidden states sequentially based on observed inputs. Transformer-based models [Vaswani et al., 2017] offer an alternative by leveraging self-attention mechanisms, though they typically rely on fixed positional encodings and do not inherently capture continuous-time dynamics."}, {"title": "2.1 Neural Ordinary Differential Equations", "content": "NODEs [Chen et al., 2018] model the latent state z(t) as:\nz(t) = z(0) + \\int_0^t f(s, z(s); \\theta_f) ds,\nwhere z(0) = h(x;\\theta_h), h : R^{d_x} \u2192 R^{d_z} is a neural network parameterized by \u03b8h, and f (s, z(s); \u03b8f) approximates  \\frac{dz(t)}{dt}. The vector field f is typically implemented using multi-layer perceptrons or more sophisticated architectures. One can view these approaches as an infinite-layer generalization of residual networks, allowing integration over time rather than discrete stacking of layers [Chen et al., 2018; Dupont et al., 2019; Kidger et al., 2020; Rubanova et al., 2019]."}, {"title": "2.2 Neural Controlled Differential Equations", "content": "NCDES [Kidger et al., 2020] extend NODEs by incorporating a control path X (t) for updating state over time:\nz(t) = z(0) + \\int_0^t f(s,z(s);\\theta_f) dX(s),\nwhere the integral is interpreted as a Riemann-Stieltjes integral, allowing for discontinuous paths. Piecewise-smooth control paths X(t) are typically constructed using natural cubic splines [Kidger et al., 2020] or Hermite cubic splines [Morrill et al., 2021a]. Furthermore, Morrill et al. [2021b] extend Neural CDEs by incorporating rough path theory for the generalized formulation."}, {"title": "2.3 Neural Stochastic Differential Equations", "content": "NSDEs [Han et al., 2017; Li et al., 2020; Tzen and Raginsky, 2019] incorporate stochasticity through:\nz(t) = z(0) + \\int_0^t f(s, z(s); \\theta_f) ds+ \\int_0^t g(s, z(s);\\theta_g) dW(s),\nwhere W(t) is a Wiener process (or Brownian motion), f(\u00b7;\u03b8f) is the drift function, and g(\u00b7;\u03b8g) is the diffusion function. The stochastic integral follows It\u00f4 or Stratonovich interpretation. While NODEs describe deterministic evolution through ordinary differential equations, NSDEs model uncertainty and noise through Brownian motion terms [Jia and Benson, 2019; Liu et al., 2019]. This extension enables more robust modeling of real-world phenomena where randomness plays a crucial role [Kidger et al., 2021b,c; Oh et al., 2024], each offering different approaches to balancing stability, expressivity, and computational tractability."}, {"title": "3 Theoretical Considerations", "content": "The theoretical foundations of NDEs are essential for understanding their ability to model complex temporal dynamics. Their connection to dynamical systems explains how they capture continuous-time evolution, while universal approximation properties justify their flexibility in learning intricate patterns. Additionally, stability and convergence analysis ensures robustness, making NDEs a reliable framework."}, {"title": "3.1 Dynamical Systems Perspective", "content": "NDEs generalize the notion of discrete layers (as in RNNs or Transformers) to a continuous-time vector field. According to equation (1), the latent representation z(t) evolves:\n\\frac{dz(t)}{dt} = f (t, z(t); \\theta_f),\nwhile in NCDEs, the evolution is driven by a control path:\ndz(t) = f (t, z(t); \\theta_f) dX(t).\nHere, X(t) is typically a spline or piecewise interpolation of observed data \u00e6, as explained in equation (2).\nStochastic extensions, such as NSDEs, incorporate Brownian motion W(t) or jump processes to model noise or uncertainty in the underlying dynamics, as shown equation (3):\ndz(t) = f(x(t); \\theta_f) dt + g(z(t);\\theta_g) dW(t),\nwhere W(t) is a Wiener process. Formally, these methods place the latent trajectory z(t) in a phase space:\nz: [0,T] \u2192 R^{d_z}, z(0) = h(x; \\theta_h),"}, {"title": "3.2 Universal Approximation Properties", "content": "NODES. Chen et al. [2018] established that continuous-time models can be viewed as an infinite-depth limit of residual networks, enabling them to approximate diffeomorphic transformations under mild regularity conditions, as represented in equation (4). However, autonomous ODE flows may limit expressivity by constraining trajectories to remain diffeomorphic transformations of initial states, thus prohibiting intersecting paths in latent space. Dupont et al. [2019] addressed these limitations through Augmented ODEs, which append an auxiliary variable a(t) to the state:\n\\begin{aligned}\n&\\frac{d}{dt} z(t) = f(z(t), a(t); \\theta_z) \\\\\n&\\frac{d}{dt} a(t) = f(z(t), a(t); \\theta_a).\n\\end{aligned}\nAugmenting the latent dimension effectively bypasses strict diffeomorphic constraints, expanding the range of representable data manifolds. Empirically, this technique improves reconstruction and expressivity in tasks where standard Neural ODEs struggle due to their trajectory overlap constraint.\nNCDEs. While NODEs rely solely on a learned vector field and initial value, Kidger et al. [2020] showed that Neural CDEs embed a control path X (t) into the model as explained in equation (5): By allowing data updates to arrive as increments dX(t) at arbitrary times, NCDEs implement a form of continuous recursion, demonstrating universal approximation for continuous paths under sufficiently rich control signals. Extensions to rough paths [Morrill et al., 2021b] further expand coverage to long, irregular time series, handling signals too irregular for traditional ODE integrators.\nNSDES. Stochastic extensions achieve universal approximation for broad classes of continuous-time random processes by learning drift and diffusion terms. Han et al. [2017] and Tzen and Raginsky [2019] formalized how parameterizing f and g via neural networks lets these models approximate various stochastic phenomena, such as Geometric Brownian motion or Ornstein\u2013Uhlenbeck processes. Subsequent works generalize to jump processes and specialized SDE families [Jia and Benson, 2019; Li et al., 2020; Oh et al., 2024; Zhang et al., 2024], underscoring the versatility of NSDEs in capturing noise-driven dynamics."}, {"title": "3.3 Existence and Uniqueness of Solutions", "content": "Deterministic NDEs. For NODEs as explained in equation (4), classic results such as the Picard-Lindel\u00f6f theorem guarantee that a unique solution z(t) exists if the vector field f is Lipschitz continuous in z. Formally, if\n|| f (t, z(t_1); \\theta_f) \u2212 f(t, z(t_2);\\theta_f)|| \u2264 L ||z(t_1) \u2013 z(t_2)||\nfor some constant L > 0, then the integral equation describing z(t) has a unique solution starting from z(0) = h(x) [Chen et al., 2018]. In practice, spectral normalization or weight clipping can enforce such Lipschitz constraints, improving both existence properties and training stability [Massaroli et al., 2020b; Rackauckas et al., 2020].\nIn NCDEs as shown equation (5), existence and uniqueness in this setting hinge on Lipschitz-like conditions for f and on the path X(t) possessing sufficient regularity (e.g., bounded variation or spline-based interpolation) [Kidger et al., 2020]. If X (t) is discontinuous or highly irregular, rough path theory provides a generalized framework under which well-posedness can still be established [Morrill et al., 2021a].\nStochastic NDES. NSDEs embed stochasticity via equation (6), existence and uniqueness then require both f and g to satisfy Lipschitz and linear-growth conditions in z [Oh et al., 2024; Tzen and Raginsky, 2019]. One common conditions for the existence and uniquess of z are Lipschitz continuity and linear growth conditions for f and g such that if there exist constants L and C such that\n\\begin{aligned}\n&||f(z(t_1); \\theta_f) - f (z(t_2); \\theta_f) || + ||g(z(t_1);\\theta_g) - g(z(t_2);\\theta_g) || \\\\\n&< L||z(t_1) - z(t_2)||,\n\\end{aligned}\nand\n||f(z; \\theta_f) || + ||g(z;\\theta_g) || \u2264 C (1 + ||~||),\nthen a unique strong solution z(t) exists for all finite t. Under these conditions, finite-time blow-up is prevented, ensuring the SDE is well-posed [Jia and Benson, 2019; Li et al.,"}, {"title": "3.4 Stability and Convergence", "content": "Deterministic Stability. NODEs and NCDEs achieve stability through Lipschitz constraints on vector fields [Chen et al., 2018; Haber and Ruthotto, 2017; Kidger et al., 2020]. For NODES, spectral norm conditions on network weights keep trajectories bounded [Kidger et al., 2021a]. NCDEs maintain stability by bounding dX/dt [Morrill et al., 2021a].\nStochastic Stability. NSDEs follow dynamics \\begin{equation}\ndz(t) = f(z,t;\\theta_f) dt + g(z,t;\\theta_g) dW(t), \n\\end{equation}where stability depends on drift f and diffusion g terms [Kidger et al., 2021b; Tzen and Raginsky, 2019]. Oh et al. [2024] shows the stochastic stability of the proposed Neural SDEs (Langevin-type SDEs, linear-noise SDEs, and geometric SDEs) under suitable regularity conditions and relates the findings to their robustness against distribution shifts.\nOptimal Transport and Convergence. The convergence of continuous-time methods can be analyzed through optimal transport theory, with the Wasserstein metric offering a principled measure of stability and generalization [Peyr\u00e9 et al., 2019; Villani and others, 2009]. Formally, (\u03a9, F, P) specifies a probability space for random variables in LP(\u03a9) whenever stochastic components arise. For an R^d-valued random variable X, its law L(X) belongs to P(R^d). The Wasserstein distance of order p measures how far two distributions \u03bc,\u03bd \u2208 P(Rd) are in terms of the minimum cost coupling, such as:\nW_p(\u03bc, \u03bd) = \\inf_{\\Pi \\in C(\u03bc,\u03bd)} \\bigg( \\int_{R^d x R^d} |x - x'|^P \\Pi(dx, dx') \\bigg)^{1/p}\nHere, C(\u03bc, \u03bd) denotes the set of couplings whose marginals match \u03bc and v. In the context of NDE models, particularly in stochastic or high-dimensional regimes, this metric provides a valuable tool for analyzing model robustness, generalization, and convergence properties [Kidger, 2021; Oh et al., 2024; Ruiz-Balet and Zuazua, 2023].\nSolver Convergence. NDEs rely on numerical integrators such as Euler, Runge-Kutta, and Dormand-Prince for deterministic models [Chen et al., 2018; Rubanova et al., 2019] and Euler-Maruyama, Milstein, or Reversible Heun for stochastic models [Kidger et al., 2021c; Tzen and Raginsky, 2019]. On the other hand, systems that are stiff, exhibit jumps, or involve high-dimensional dynamics often necessitate specialized implicit solvers to ensure stability and computational efficiency [Jia and Benson, 2019; Kim et al., 2021; Michoski et al., 2020; Rackauckas et al., 2020]."}, {"title": "4 Practical Implementation", "content": "This section expands on the theoretical foundations from Section 3, focusing on the practical aspects of training, regularization, and deployment of NDEs. While discrete-time models are widely studied, continuous-time models remain underexplored despite their advantages."}, {"title": "4.1 Optimization of NDE-based Models", "content": "Adjoint Sensitivity Method. NDEs differ significantly from discrete methods in their reliance on numerical integration. During training, gradients with respect to model parameters must be backpropagated through an ODE, CDE, or SDE solver, which raises memory and stability challenges. Traditional backpropagation is conceptually straightforward but stores all intermediate states, leading to large memory usage in long sequences or high-dimensional latents.\nThe adjoint sensitivity method, introduced by Chen et al. [2018], is a pivotal technique for efficiently computing gradients in NDEs by solving an adjoint equation backward in time. This method addresses the challenge of high memory consumption in gradient-based optimization by reconstructing forward states on demand, reducing the memory complexity from O(N), where N is the sequence length, to approximately O(1). The adjoint state \u03bb(t) evolves according to a differential equation,\n\\frac{d\u03bb(t)}{dt} = -\u03bb(t)\\partial f/\\partial z\nwhere \u03bb(T) = dL/dz(T) initializes the backward integration. This approach enables the computation of gradients via\ndc/d\u03b8 = \\int_0^T \u03bb(t) \\frac{\\partial f}{\\partial \u03b8}dt,\nmaking it suitable for training models with terminal loss functions. However, the adjoint sensitivity method encounters numerical challenges, particularly in stiff or chaotic systems where reverse-time integration can amplify floating-point errors. Alternatively, checkpointing techniques [Gholami et al., 2019; Zhuang et al., 2020], store selected intermediate states during the forward pass, allowing localized recomputation during backward propagation.\nIntegral Loss Functions. Beyond terminal loss functions, NDEs have been extended to incorporate integral loss functions distributed across the entire depth domain S. In the context of optimal control [Pontryagin, 2018], the integral loss is defined as formulated by Massaroli et al. [2020a]:\nL(z(S)) + \\int_0^T l(t, z(t) dt,\nwhere the loss combines terminal contributions L(z(S)) with intermediate terms l(t, z(t)). This formulation allows the latent state z(t) to evolve through a continuum of layers, guiding the model output toward the desired trajectory over the entire depth domain S. The adjoint dynamics for such integral loss functions are modified to include an additional term, -dl/dz(t), accounting for the distributed loss in the backward gradients [Finlay et al., 2020b; Grathwohl et al., 2018; Massaroli et al., 2020a]. This approach enhances the control and flexibility of trajectory-level learning, enabling improved performance in tasks that require supervision over the entire temporal or spatial domain.\nBackpropagation through time. For discrete methods like RNNs, it is a common optimization strategy, where the"}, {"title": "4.2 Regularization Methods", "content": "Advanced Techniques for NDES. While conventional regularization techniques are widely applicable to NDE-based methods, recent advances have introduced specialized approaches tailored to continuous dynamics. Regularization based on principles of optimal transport, proposed by Finlay et al. [2020a,b], simplifies the dynamics of continuous normalizing flow models, thereby accelerating training. Kelly et al. [2020] analyzed equations with differential surrogates for the computational cost of standard solvers, leveraging higher-order derivatives of solution trajectories to reduce complexity.\nContinuous-Time Modifications. Standard regularization techniques have further refined NDE-based methods; stochastic sampling of end times [Ghosh et al., 2020], continuous-time dropout [Liu et al., 2020], and temporal adaptive batch normalization [Zheng et al., 2024]. These methods form a comprehensive toolkit for advancing the performance and scalability of NDE-based models."}, {"title": "4.3 Numerical Solution of NDES", "content": "Comparison with Discrete Architectures. Discrete-time methods, including variants of RNNs and Transformers, rely on layer-wise or self-attention updates without numeric integration. Although this design simplifies backpropagation and typically scales well, these models often struggle to accommodate irregular sampling unless augmented by masking or gating mechanisms [Chung et al., 2014]. RNN-based architectures can also experience vanishing or exploding gradients for lengthy sequences unless they incorporate techniques such as gradient clipping or gated units [Hochreiter and Schmidhuber, 1997; Pascanu et al., 2013]. Transformers replace recurrence with attention mechanisms but depend heavily on positional encodings and expansive parameter sets, which can limit their practicality for extensive time spans with variable sampling [Rae et al., 2019; Vaswani et al., 2017; Wen et al., 2022]. By contrast, continuous-time NDEs use solvers that naturally adapt to asynchronous events, though solver overhead and potential stiffness require careful solver selection and parameter tuning.\nFixed or Adaptive Solvers. NDE-based models rely on numerical integration to evolve their latent states z(t) from initial conditions to final outputs. A fixed-step solver, such as explicit Euler or a basic Runge-Kutta method, updates z(t) at uniform intervals and is computationally simpler but may face instability if the dynamics change abruptly or if the time step is too large [Chen et al., 2018]. Adaptive-step solvers, such as Dormand-Prince method, refine the step size \u0394t in response to local error estimates, offering higher accuracy for stiff or highly non-uniform dynamics [Kidger et al., 2021c]; however, this adaptive nature can cause unpredictable run times in large-scale tasks [Rackauckas et al., 2020]. In the stochastic setting of NSDEs, solvers like Euler-Maruyama or Milstein handle the Brownian increments for the diffusion term, balancing efficiency and stability under well-defined noise [Oh et al., 2024; Tzen and Raginsky, 2019]."}, {"title": "5 Comparison of NDE-based Methods", "content": "Table 1 summarizes the core formulations, primary tasks, and benchmark datasets associated with various NDE-based methods. These methods can be grouped into three main categories\u2014NODE, NCDE, and NSDE\u2014highlighting their unique characteristics and application domains."}, {"title": "5.1 NODE Methods", "content": "Standard form of Neural ODE [Chen et al., 2018] introduced a novel approach to continuous-time modeling by parameterizing the hidden state dynamics with a neural network, allowing for the seamless handling of irregularly sampled time series data. Augmented ODE [Dupont et al., 2019] extend this framework by augmenting the state space with auxiliary variables, enabling the modeling of complex trajectories and avoiding trajectory overlap. ODEVAE [Yildiz et al., 2019] combines NODEs with variational autoencoders to model latent dynamics in irregularly time series by learning both the continuous-time evolution and the underlying latent space.\nOn the other hand, GRU-ODE [De Brouwer et al., 2019] adopts continuous gating mechanisms inspired by GRU cells, where the hidden state evolves dynamically based on time gaps. u(.), and g(\u00b7) are continuous counterpart of GRU. ODE-RNN [Rubanova et al., 2019] combines ODE-based continuous evolution with discrete updates at observation points using an RNN cell. Similar to that, ODE-LSTM [Lechner and Hasani, 2020] incorporates the memory mechanisms of LSTM into the ODE framework, evolving the hidden state between observations using ODE solvers and performing discrete LSTM updates at observation points."}, {"title": "5.2 NCDE Methods", "content": "Neural CDE [Kidger et al., 2020] models the latent state z(t) with a piecewise-smooth control path X(t). The Riemann-Stieltjes integral allows Neural CDEs to handle irregular or asynchronous time series data effectively, making them suitable for tasks like interpolation and forecasting. Neural Rough Differential Equations (Neural RDEs) [Morrill et al., 2021a] extend Neural CDEs by using the log-signature of the control path LogSig[ri,ri+1](X) with X : [to: tn] and certain interval to \u2264 ri < ri+1 < tn, capturing higher-order variations. This enables Neural RDEs to handle rough paths, improving robustness for long and complex time series. Recently, Walker et al. [2024] further extended rough path theory to Log-NCDEs. These approaches leverage continuous-time dynamics, enabling flexible modeling of time series.\nRecent works [Jhin et al., 2022, 2023] emphasize training-based approaches for constructing control paths in NCDEs."}, {"title": "5.3 NSDE Methods", "content": "Neural SDEs, parameterized by neural networks for both drift and diffusion terms, provide a flexible framework for model-"}, {"title": "6 Discussion and Future Directions", "content": "Computational Challenges. NDEs face scalability challenges with continuous-time solvers. Fixed integrators or adaptive integrators, while accurate, can lead to unpredictable computation times in stiff regions [Chen et al., 2018; Kidger, 2021; Kim et al., 2021]. Specialized techniques like parallel-in-time integration and GPU acceleration offer promising solutions [Gholami et al., 2019; Rackauckas et al., 2020]. Future research should focus on developing comprehensive integration schemes that can automatically balance computational efficiency with numerical accuracy, particularly for large-scale applications in real-time settings.\nTheoretical Development. Current theoretical understanding requires expansion, particularly for non-stationary and noisy data. While stability guarantees exist for NODES and NCDEs through Lipschitz constraints [Massaroli et al., 2020b], similar guarantees for NSDEs remain incomplete [Oh et al., 2024]. Drift-diffusion analysis under strong noise conditions needs further investigation [Li et al., 2020; Tzen and Raginsky, 2019]. Development of rigorous frameworks for analyzing convergence properties and error bounds in these stochastic settings would significantly advance both theoretical foundations and practical applications of NDEs.\nPhysics-Informed NDEs. Physics-Informed NDEs (or Physics Informed Neural Networks) incorporate domain knowledge, such as Partial Differential Equations (PDEs) constraints and conservation laws, to align modeled dynamics with physical systems. Recent advances in physics-informed architectures have demonstrated remarkable success in capturing multi-scale phenomena and handling noisy measurements in complex physical systems [Cuomo et al., 2022; Raissi et al., 2019; Rudy et al., 2017]. By embedding structural priors, these approaches ensure interpretability and reliability in real-world tasks.\nHybrid Architectures. Hybrid architectures combine NDEs with models like graph neural networks or Transformers to capture complex time series patterns. Graph-based NDEs handle spatial-temporal data [Choi et al., 2022; Poli et al., 2019], while attention mechanism address long-range dependencies and irregular sampling [Jhin et al., 2024; Li et al., 2024]. Furthermore, Oh et al. [2025] integrates explicit and implicit mechanisms for irregular time series analysis. Future research directions should focus on developing more efficient training algorithms for these hybrid architectures, investigating theoretical guarantees, and exploring applications in real-world domains where both temporal dynamics and structural relationships play crucial roles."}, {"title": "7 Conclusion", "content": "NDEs represent a significant advancement in time series modeling, offering a principled approach to handling continuous dynamics. Through various formulations\u2014NODEs, NCDEs, and NSDEs-they provide flexible frameworks for complex temporal data, including cases with irregular sampling or missing data. While challenges remain in computational efficiency and theoretical understanding, ongoing developments in solver techniques, stability analysis, and hybrid architectures continue to enhance their capabilities. Beyond current applications in classification, interpolation and forecasting, NDEs show potential in anomaly detection, reinforcement learning, and multi-agent systems. As the field evolves, NDEs are increasingly positioned to address sophisticated time series challenges across diverse applications."}]}