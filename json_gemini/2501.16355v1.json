{"title": "How Strategic Agents Respond: Comparing Analytical Models with LLM-Generated Responses in Strategic Classification", "authors": ["Tian Xie", "Pavan Rauch", "Xueru Zhang"], "abstract": "When machine learning (ML) algorithms are used to automate human-related decisions, human agents may gain knowledge of the decision policy and behave strategically to obtain desirable outcomes. Strategic Classification (SC) has been proposed to address the interplay between agents and decision-makers in designing trustworthy ML algorithms. Importantly, prior work on SC has relied on pre-specified assumptions that agents are perfectly or approximately rational, responding to decision policies by maximizing their utilities. However, verifying these assumptions is challenging due to the difficulty of collecting real-world agent responses, making it hard to evaluate the theoretical results and algorithms built on these assumptions. Meanwhile, the growing adoption of large language models (LLMs) makes it increasingly likely that human agents in SC settings will seek advice from these tools. We thus propose using strategic advice generated by LLMs to simulate human agent responses in SC. Specifically, we examine five critical SC scenarios-hiring, loan applications, school admissions, personal income, and public assistance programs and simulate how human agents with diverse profiles seek advice from LLMs and follow their suggestions to achieve positive decision outcomes. We then compare the resulting agent responses with the best responses generated by existing theoretical models. Our findings reveal that: (i) LLMs and theoretical models generally lead to agent score or qualification changes in the same direction across most settings, with both achieving similar levels of fairness; (ii) state-of-the-art commercial LLMs (e.g., GPT-3.5, GPT-4) consistently provide helpful suggestions, though these suggestions typically do not result in maximal score or qualification improvements; and (iii) LLMs tend to produce more diverse agent responses, often favoring more balanced effort allocation strategies. These results suggest that theoretical models align with LLMs to some extent and that leveraging LLMs to simulate more realistic agent responses offers a promising approach to designing trustworthy ML systems.", "sections": [{"title": "1 Introduction", "content": "Individuals subject to algorithmic decisions often adapt their behaviors strategically to the decision rule to receive a desirable outcome. As machine learning is increasingly used to make decisions about humans, there has been a growing interest in developing machine learning methods that explicitly consider the strategic behavior of human agents. A line of research known as strategic classification"}, {"title": "Definition 1.1", "content": "(Hardt et al. (2016)). Consider a population of agents with features space X and label space Y = {0,1}. Let h : X \u2192 {0,1} be the ground truth labeling function\u00b9, D the probability distribution over X, and c : X \u00d7 X \u2192 R the cost function that measures the cost for an agent to modify their features.\n\n1.  The decision-maker (who knows the cost function c, the distribution D, and the true classifier h) publishes a classifier f : X \u2192 {0,1}.\n\n2.  The agents (who knows c, h, D, and f) with feature x \u2208 X move their features to \u2206(x) \u2208 X at cost according to a best response function \u2206 : X \u2192 X.\nHere, \u2206(x) = arg max_{z \\in X} f(z) \u2013 c(x, z) is the best response of an agent with feature x, where c(x, z) stands for the cost it takes for agents to move their features from x to z. Then with a loss function l(f(x), y), the decision-maker hopes to find the best classifier f* = argmin_{f} E_{x~D}[l(f(\u2206(x)), h(x))].\nDef. 1.1 specifies that agents with feature x best respond to the decision policy according to A(x) and has been widely adopted by previous literature (e.g., Dong et al. (2018); Braverman and Garg (2020); Levanon and Rosenfeld (2021); Horowitz and Rosenfeld (2023); Bechavod et al. (2022)]). Typically, f belongs to linear family (e.g., logistic regression / linear regression) and c(x, z) is some distance metric (e.g., 12 norm) measuring the distance between x and z. Note that A(x) in this form assumes the agents have full knowledge of the policy f and are perfectly rational. Some recent works relaxed these constraints to formulate an \"approximate\" agent best response, e.g., by adding noise to f (Jagadeesan et al., 2021), using a probability distribution to model agent actions (Zhang et al., 2022), or assuming agents first estimate complex decision policy and then best respond to the estimated policy (Ghalme et al., 2021; Xie and Zhang, 2024b). However, it is still hard to assess whether they capture the actual agent behavioral patterns in real applications. Because collecting dynamic data of human agents in real-world SC settings can be extremely challenging\u00b2, existing studies in SC are validated using static datasets (e.g., credit data (Kaggle, 2012; Hofmann, 1994), adult data (Ding et al., 2021), law school data (Kaggle, 1988)) with simulated agent response. A few case studies on SC touched upon simulating dynamic data resulting from agent responses in the long term (D'Amour et al., 2020; Miller et al., 2020a). However, they still modeled agent responses"}, {"title": "1.1 Contributions", "content": "This work aims to bridge the gap between the theoretical best response model in SC and the strategic responses of human agents in real-world scenarios. Our primary contribution is to demonstrate that LLMs have great potential to simulate agent responses across various SC settings and provide valuable guidance for refining the existing theoretical model (e.g., personalized, diverse, and balanced effort allocation strategies). We construct five practical SC settings where human agents know only general information about the decision system, without any numerical details of the decision policy, and seek advice from LLMs to achieve desirable decision outcomes. Through a comprehensive set of simulation results, we show that LLMs exhibit several distinct characteristics compared to the"}, {"title": "1.2 Related Works", "content": "Our paper is closely related to two lines of previous works, as we discuss below."}, {"title": "1.2.1 Strategic classification.", "content": "Strategic classification (SC) was first modeled by Hardt et al. (2016) to demonstrate the interaction between individuals and a decision maker as a Stackelberg game. By incorporating individuals' best responses, the decision maker can optimize decisions by anticipating strategic behavior. In recent years, more sophisticated models of strategic classification have emerged (Ben-Porat and Tennenholtz, 2017; Dong et al., 2018; Braverman and Garg, 2020; Jagadeesan et al., 2021; Izzo et al., 2021; Ahmadi et al., 2021; Tang et al., 2021; Zhang et al., 2020, 2022; Eilat et al., 2022; Liu et al., 2022; Lechner and Urner, 2022; Chen et al., 2020; Xie and Zhang, 2024b; Xie et al., 2024c). For example, Ben-Porat and Tennenholtz (2017) developed a linear regression predictor in a competitive setting, where two players aim to outperform each other in prediction accuracy. Dong et al. (2018) extended this to the online version of the strategic classification algorithm. Chen et al. (2020) introduced a strategic-aware linear classifier aimed at minimizing Stackelberg regret. Tang et al. (2021) considered scenarios where the decision maker is only aware of a subset of individuals' actions. Levanon and Rosenfeld (2022) expanded the framework to cases where individuals and the decision maker share aligned interests. Lechner and Urner (2022) proposed a new loss function balancing prediction accuracy with resistance to strategic manipulation, while Eilat et al. (2022) relaxed the assumption of independent individual responses, presenting a robust learning framework utilizing a Graph Neural Network. Xie and Zhang (2024b) developed a framework for welfare-aware strategic learning, and Gemalmaz and Yin (2024) explored how fairness influences the strategies of human agents. Meanwhile, SC is also closely related to algorithmic recourse, with Karimi et al. (2022) providing a comprehensive survey on the topic."}, {"title": "1.2.2 Use LLMs to simulate human agents.", "content": "A growing body of recent literature has explored the use of LLMs to complete complex tasks in social science that require reasoning similar to human behavior (Engel et al., 2024). In addition to the examples mentioned above, Brand et al. (2023) examined whether LLMs can conduct market"}, {"title": "2 Simulation Settings", "content": ""}, {"title": "2.1 Problem Formulation", "content": "In each of the five settings, we construct a binary classification task where the feature space is X \u2282 R^d and the label space Y = {0,1}. Assuming all features are drawn from the distribution D, we first train the ground-truth classifier 1(h(x) \u2265 0.5) using a large training dataset where h: X \u2192 R is assumed to be the ground-truth labeling function.\nDecision-maker's policy. We assume the decision-maker will publish policy 1(f(x) \u2265 0.5), with f: X \u2192 R the scoring function. Note that h and f are not necessarily in the same function family. For example, h can be a highly complex neural network but f can be a logistic scoring function. We consider two scenarios: (i) the labeling function h is relatively simple and the decision-maker publishes f = h; (ii) when the labeling function h is complex, the decision-maker may have to publish a simplistic scoring function f due to various reasons such as the limited number of training samples, the legal enforcement on transparency, and explainability of the scoring functions (Rosenfeld et al., 2020; Xie and Zhang, 2024b).\nTheoretical model for agent best response. Given the decision policy f, we assume a new set of 1000 human agents participate in the decision system, each with features x ~ D, and strategically responds according to the published policy f. Instead of using the analytical model defined in Def. 1.1, we adopt an equivalent formulation from Kleinberg and Raghavan (2020); Guldogan et al. (2022), where human agents have a fixed effort budget \u03b4 and can strategically modify a subset of their features by investing efforts e \u2208 R^d, subject to the constraint |e| < \u03b4. Additionally, we assume that the effort associated with each of the d features has a different conversion ratio, denoted by the effort conversion matrix W = diag(w) \u2208 R^{d\u00d7d}, where w\u2208 R^d. Formally, denote x(e) as the modified feature vector resulting from effort e, then we have x(e) = x + We. Additionally, we assume human agents with features x will use a first-order approximation to estimate the decision policy f, given by Q(x') = x + \u2207f(x)^T(x' \u2013 x). This approximation is motivated by the fact that human agents may not effectively learn f when it is highly complex and non-linear, as highlighted in Xie and Zhang (2024b); Rosenfeld et al. (2020). Note that we adopt this theoretical formulation because it is more"}, {"title": "Definition 2.1", "content": "(Theoretical agent response). Agents with initial features x will strategically change their features to x* as follows.\nX* = max_{||e|| < \u03b4} Q(x(e))"}, {"title": "2.2 Datasets", "content": "We consider five different yet important SC settings and introduce the details of each setting as follows. We set the effort budget \u03b4 = 0.3 for all settings to upper bound the total score increase/qualification improvement 4."}, {"title": "2.3 Seek advice from LLMs.", "content": "Unlike the theoretical best response, we consider a more realistic scenario where human agents do not know f quantitatively. Instead, they seek advice from knowledgeable \"experts\" (i.e., LLMs) and follow their suggestions to modify their features. Specifically, for each of the five settings, we first construct prompts to describe the task and then ask the LLMs to act as advisors, helping users achieve their desired decision outcomes. The right plot in Fig. 1 illustrates the prompt for the loan approval setting, while other prompts are similarly constructed and shown in App. A. For ease of comparison, we restrict the LLMs to output a normalized effort allocation for each user, which includes only the magnitude and direction of efforts allocated to each feature dimension.\nWe illustrate the complete process in the left plot of Fig. 1, where we also specify the JSON format to ensure the outputs are in the same format as the one shown in this plot. Specifically, we generate results using GPT-3.5(Ye et al., 2023) and GPT-4o(OpenAI, 1988) via OpenAI API, with all other"}, {"title": "3 Main Results: Does LLM Align with the Theoretical Model?", "content": "For each of the above five settings, we assume 1000 human agents participate in the decision system and strategically respond to the deployed policy f. We first obtain the theoretical best responses and the agent responses produced by LLM with the procedures specified in Sec. 2.1, and then provide a detailed comparison analysis. Specifically, we will compare the effort allocation strategies produced by LLMs and the theoretical models (Sec. 3.1), as well as the score increase (Sec. 3.2), agent qualification improvement (Sec. 3.3), and unfairness (Sec. 3.4) resulting from the efforts. We first provide the definitions as follows."}, {"title": "Definition 3.1", "content": "(Score increase). Let x and x' be features of an agent before and after responding to the deployed policy f, respectively. The resulting score increase is measured as f(x') \u2212 f(x)."}, {"title": "Definition 3.2", "content": "(Qualification improvement). Let x_c be the subset of causal features that causally affect agents' underlying labels. For an agent that changes their features from x to x', its qualification improvement is measured as h(x') \u2013 h(x_c)."}, {"title": "Definition 3.3", "content": "(Unfairness of effort allocation). For each of the 5 settings, there exists a sensitive feature to divide the human agents into different social groups (details in Sec. 2.2). The unfair- ness resulting from an effort allocation is measured by the difference between the average score increase/qualification improvement of distinct social groups.\nIn our experiments, we assume the deployed policy f is always a logistic classifier, while the ground- truth labeling function h can either be simple (same as f, decision scenario 1) or complex (an MLP classifier, decision scenario 2). For each setting, we provide experimental results for both GPT-3.5 and GPT-4o."}, {"title": "3.1 Effort Allocation Comparisons", "content": "In this section, we illustrate the effort allocation strategies for LLMs and the theoretical model, as shown in Fig. 2 to Fig. 6. Each figure consists of two subfigures presenting results for GPT-3.5 and GPT-4o. The red bars represent the effort allocation distributions generated by the LLMs, while the blue lines correspond to those produced by the theoretical model. This is because the decision-maker employs a logistic classifier, which belongs to the linear model family; therefore, by Def. 2.1, the theoretical best responses for each human agent remain the same.\nOverall, LLMs (especially GPT-4o) tend to produce more diverse and balanced effort allocations compared to the theoretical model. For instance, in the hiring setting, the theoretical model adopts an \"extreme\" effort allocation strategy, concentrating most efforts on ComputerSkills. In contrast, GPT-4o distributes efforts more evenly between ComputerSkills and Education. Similarly, in the law school setting, while the theoretical model allocates most efforts to LSAT, GPT-4o prioritizes LSAT but also allocates significant effort to UGPA. In the public assistance program setting, the theoretical model focuses primarily on SCHL (i.e., manipulating"}, {"title": "3.2 Score Increase", "content": "In this section, we present the score increases achieved when human agents adjust their efforts according to either the theoretical model or the strategies suggested by LLMs, with the decision- maker employing a logistic classifier f as the policy. The average score increases for all human agents are reported in the first rows of each setting in Tab. 2. As expected, the theoretical model yields the highest score increase in each setting. Notably, in three of the five settings-hiring, income, and law school-LLMs, particularly GPT-4o, closely match the score improvements of the theoretical model, showcasing their proficiency in optimizing effort allocation strategies.\nIn the loan approval setting, while LLMs do enhance scores, the improvements are significantly less than those achieved by the theoretical model. This discrepancy may be attributed to the"}, {"title": "3.3 Agent Qualification Improvement", "content": "Next, we compare the agent improvements resulting from effort allocation strategies under the theoretical model and LLM-generated strategies. Specifically, we evaluate these strategies under two distinct decision scenarios, defined as follows.\nDecision scenario 1. This scenario occurs when the decision-maker directly deploys f = h, corresponding to the classic case in strategic classification (e.g., Hardt et al. (2016); Dong et al. (2018); Xie and Zhang (2024a)). The agent qualification improvements are presented in the second lines of Table 2. Notably, since all features are causal in the income and law school settings and"}, {"title": "3.4 Unfairness of the Effort Allocation Strategies", "content": "In this section, we examine whether LLMs generate effort allocation strategies that result in disparate effects on score increases or agent improvements across different social groups. Specifically, we use score increases and qualification improvements as metrics to evaluate the fairness of these effort allocation strategies. We define the unfairness of an effort allocation strategy as the disparity in score increases or qualification improvements between groups of human agents, assuming all agents adhere to the suggested strategy. This aligns conceptually with fairness metrics such as equal improvability (Guldogan et al., 2022) and bounded effort (Heidari et al., 2019), which measure fairness by assessing whether different groups achieve similar improvements after best responding to a decision policy. However, rather than focusing on the fairness of the decision policy itself, we investigate the fairness of the effort allocation strategies and compare LLMs with those generated by the theoretical model.\nIn Tab. 3, we present unfairness with respect to score increases and qualification improvements across both decision scenarios within each of the five settings. The results indicate that, in four out of the five settings, LLMs generate effort allocation strategies with levels of unfairness comparable to those produced by the theoretical model. Furthermore, the findings suggest that the observed"}, {"title": "4 Conclusions & Societal Impacts", "content": "This work examines the gap between theoretical best-response models in strategic classification and real-world human behaviors. By introducing practical scenarios where individuals with limited knowledge of the decision system, behave based on LLMs-generated suggestions, we highlight the value of using LLMs to simulate more realistic responses. Our findings demonstrate that LLMs can effectively provide effort allocation strategies that improve both decision outcomes and the true qualifications of human agents. However, the strategies suggested by LLMs differ from those of the"}, {"title": "B The Connection Between Def. 1.1 and Def. 2.1", "content": "In this section, we prove that when the decision policy is linear, i.e., f = \u03b2^Tx + \u03b2_o and the cost function is quadratic, i.e., c(x, z) = (z \u2212 x)^TB(z \u2212 x) where B is symmetric and full rank, then for each c, there exists w, \u03b4 to let the agent best response defined in Def. 2.1 be equivalent to Def. 1.1.\nProof. We firstly work out the best response using Def. 1.1, where we need to work out:\narg max_{z \\in X} f(z) - c(x, z) = \u03b2^Tz \u2212 (z \u2212 x)^TB(z \u2212 x)\nTake the derivative with respect to x of the above equation to get \u03b2^T \u2013 2 \u00b7 B(z - x) = 0. Thus,\nz = x + \\frac{\u03b2^T}{2}B^{-1}. Next, consider the best response using Def. 2.1, where we need to work out:\narg max_{||e||<\u03b4} B(x(e)) = \u03b2^Tx + \u03b2^T We\nSince \u03b2^TW is fixed and ||e|| is bounded, according to Cauchy-Schwartz inequality, e should be \\frac{\u03b2^T W}{||\u03b2^T W||} and z' = x + \\frac{\u03b2^T W}{||\u03b2^T W||}W\u03b4. Therefore, for each B, we can find appropriate W, \u03b4 to let z' = z.\nFor example, we just need to adjust W^2 to be a multiple of B^{-1} and then adjust \u03b4."}, {"title": "C Additional Experimental Results", "content": "Plots of score distribution changes are shown in Fig. ?? to Fig. ??, while plots of qualification distribution changes are shown in Fig. 17 to Fig. 22."}]}