{"title": "CNN Autoencoders for Hierarchical Feature Extraction and Fusion in Multi-sensor Human Activity Recognition", "authors": ["Saeed Arabzadeh", "Farshad Almasganj", "Mohammad Mahdi Ahmadi"], "abstract": "Deep learning methods have been widely used for Human Activity Recognition (HAR) using recorded signals from Inertial Measurement Units (IMUs) sensors that are installed on various parts of the human body. For this type of HAR, several challenges exist, the most significant of which is the analysis of multivarious IMU sensors data. Here, we introduce a Hierarchically Unsupervised Fusion (HUF) model designed to extract, and fuse features from IMU sensors data via a hybrid structure of Convolutional Neural Networks (CNN)s and Autoencoders (AE)s. First, we design a stack CNN-AE to embed short-time signals into sets of high dimensional features. Second, we develop another CNN-AE network to locally fuse the extracted features from each sensor unit. Finally, we unify all the sensor features through a third CNN-AE architecture as globally feature fusion to create a unique feature set. Additionally, we analyze the effects of varying the model hyperparameters. The best results are achieved with eight convolutional layers in each AE. Furthermore, it is determined that an overcomplete AE with 256 kernels in the code layer is suitable for feature extraction in the first block of the proposed HUF model; this number reduces to 64 in the last block of the model to customize the size of the applied features to the classifier. The tuned model is applied to the UCI-HAR, DaLiAc, and Parkinson's disease gait datasets, achieving the classification accuracies of 97%, 97%, and 88%, respectively, which are nearly 3% better compared to the state-of-the-art supervised methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Inertial Measurement Unit (IMU) sensors installed on different parts of the human body are widely used for Human Activity\n-Recognition (HAR). This type of HAR, which for brevity we refer to it as HAR in this paper, stands out as a pivotal research\narea in healthcare and surveillance domains [1]. HAR can improve the diagnosis of a number of diseases or identify their severity,\nincluding Parkinson's and Alzheimer's disease [2]. The performance of HAR with IMU sensors is significantly enhanced when\nthe sensors are placed on multiple body parts [3]. However, increasing the number of sensors complicates signal processing during\nfeature extraction and feature fusion [4, 5]. To effectively enhance HAR performance, robust methods for analyzing and fusing\ndata from multiple IMU sensors are essential [6].\nTo extract temporal and spatial features and improve the result of HAR, various Deep Neural Network (DNN) approaches, based\non Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have been proposed. Despite significant re-\nsearch efforts in DNNs for HAR, several fundamental challenges still persist. The major ones are: (1) the difficulty of data anno-\ntation, which hinders data collection, (2) the dependency of human actions on age, weight, and sex, and (3) the complexity of\nprocessing data obtained from multiple sensors.\nTo address the first challenge, unsupervised models such as Autoencoders (AE) and Boltzmann machines have been proposed\n[7, 8]. However, these approaches struggle with multi-sensor data and multi-class human activities.\nAmong the supervised methods, CNN-based architectures are a prominent approach for extracting human movement features\nwithin a framework of IMU sensors data [9]. These methods typically concatenate Multivarious IMU sensors Time Series (MITS)\nas input, and simultaneously perform feature extraction, feature fusion, and final classification [6, 10-14]. The performance of the\nCNN-based models often depends on the number of their layers and parameters. However, the limited amount of labeled data in\nHAR hinders the training of deep CNNs through supervised learning. Due to these limitations, such models are not always suitable\nfor HAR purpose [8]. Consequently, alternative architectures, such as combining shallow CNNs with Long Short-Term Memory\n(LSTM) networks and attention mechanisms [8, 15, 16], have been proposed. While these models have improved HAR, their\npractical application remains limited, and typically relies on long input data durations [15, 17, 18].\nProperly fusing MITS information could potentially address the aforementioned challenges in HAR and lead to improved activity\nrecognition. Common fusion techniques include feature fusion, sensor fusion, and decision fusion, with feature fusion showing the\nmost promising results [19]. In this work, we tackle HAR challenges by utilizing CNN and AE architectures to combine the feature\nextraction, feature fusion and sensor fusion tasks in a three-stage structure. We call this method Hierarchical Unsupervised Fusion\n(HUF) model. The output of the HUF is then applied to a final MLP classifier to accomplish the HAR task."}, {"title": "", "content": "The HUF method is a hybrid model consisting of three cascaded blocks, resulting in the input signals being processed in three\ndistinct steps. The first step involves applying independent AEs to each of the accelerometer and gyroscope signals, which produces\nan ensemble of individual features for every installed sensor unit.. In this step, each input signal is embedded to a high dimension\nfeature set to capture its various informative aspects. This means the information from each signal is broken down into more\ndetailed components to create a more meaningful representation of human movement for use in the subsequent steps. In the second\nstep, we fuse the extracted features of the first step's network distinctively for each sensor unit. The aim is to create a salient pattern\nof activity from each sensor unit independently. This approach helps to prevent the fusion of features from incompatible MITS\ndata combinations, particularly those features from non-activity periods [6, 8]. Finally, an additional AE network fuses the local\nfeatures extracted from each sensor units. This step produces the final feature set, representing activity across various body move-\nments.\nThe key contributions of this work are as follows.\nWe introduce an end-to-end hybrid Neural Network (NN) architecture for efficient feature extraction and fusion from sensor\naxes to entire body sensors.\nWe design a Stacked CNN-AE as a signal representative AE network to embed the short-time signal of each axis into high-\ndimensional features. These features efficiently represent and highlight various important aspects of the input signal, and\ndecompose it into its distinct components.\nWe propose a hierarchical feature fusion CNN-AE structure with two stages: the first stage fuses features within each sensor\nunit, while the second stage generates a unique feature vector from all sensors. This hierarchical approach enhances the\nmodel's ability to capture subtle patterns and variations, leading to improved recognition accuracy.\nOur approach is designed for recognizing activities using short time frames of input signals, enabling real-time HAR and\nmaking the system suitable for real-time applications.\nOur model utilizes a cascade of carefully designed autoencoder-based blocks, each of which is inherently robust against\nspecific types of input noise or missing data.\nThe rest of this paper is organized as follows. Section II provides a brief overview of current studies in HAR. Section III describes\nthe proposed NN-based method, detailing the functionalities of each component. Section IV covers the datasets used for evaluation,\nthe training process for the model blocks, and the experimental results. Finally, Section V discusses the evaluation of the proposed\nmethod, and highlights the key findings and limitations of our work."}, {"title": "II. RELATED WORKS", "content": "A typical DNN approach for HAR involves a CNN network followed by a Multi-layer Perceptron (MLP) classifier. In this\nstructure, CNN primarily extracts local input patterns in its initial layers, and the global information fusion in its final layers.\nHowever, this network is sensitive to input noises and requires a large amount of labeled training data. Additionally, this model\nstruggles with feature extraction and fusion on signals obtained from multiple sensors [6, 10-14, 20-22].\nSeveral studies have investigated cascading CNNs with LSTM networks [8, 15, 16]. Although these models outperform simple\nCNNs, they require relatively long input signals. In other studies, attention mechanisms are integrated with CNN-LSTM structure\nto determine and focus on crucial features [15, 23]. In one work, a CNN model is empowered by three LSTMs and a spatial\nattention mechanism [23]. The reported results show that the model is sensitive to the input length variation and label annotation.\nAnother approach utilizes dual-channel CNN-LSTM architectures to process data from different sides of the body [24, 25]. This\napproach aims to capture spatiotemporal features from each body sides and addresses the challenge of non-activity periods during\nanalysis and fusion. However, its effectiveness relies on the availability of mirrored body sensor datasets.\nTo tackle challenges related to feature extraction and fusion, [26] proposes a CNN model that utilizes channel attention layers\nto enhance HAR performance. Unfortunately, the model's significant depth makes it practical only for extremely large datasets.\nHAR methods using IMU sensors heavily rely on large datasets with meticulously labeled activity sequences. However, manual\nlabeling process is resource-intensive, requiring significant human effort and computational power [7]. So, expanding the use of\nthe unsupervised methods in this filed is highly appreciated [27, 28]."}, {"title": "III. METHODOLOGY", "content": "Our proposed HUF fusion model (Figure 1) processes multivariate time series data from multiple IMU sensors located on differ-\nent parts of the body. Each sensor unit contains three accelerometers and three gyroscopes, in different x, y, and z axes, resulting\nin data structure of s = [ax(t), ay(t), az(t), gx(t), gy(t), gz(t)] for each unit. Therefore, for n sensor units, the input data is rep-\nresented as:\nX = [ax\u2081 (t), ay\u2081 (t), gz1 (t), azn(t)]1x6n (1)\nwhere each element represents a time series signal. The proposed model involves four distinct processing steps, visualized in Figure\n1. Each of these steps will be introduced individually in the subsequent parts of this section."}, {"title": "A. Data Representation", "content": "To achieve a deep representation of the recorded signals of the accelerometers and gyroscopes, i.e., x\u2081(t) (i = 1, ..., 6n), we\npropose a stacked convolutional AE network, which is specifically designed to serve as an embedding step. The Data Representa-\ntion Stacked (DR-SAE) architecture incorporates a number of convolutional layers within its encoder and decoder structures to\nhierarchically process the input time series segments (Figure 2). It generates a representation subspace with 256 views spanning\ndifferent frequency bands of the input signal. The employed DR-SAE networks are trained using a stacked AE method, as illus-\ntrated in Figure 2. The training is done layer by layer, by freezing the previous layers and optimizer parameters. When all shallow\narchitectures are cascaded, the resulted deep architecture captures the salience of signals in different scales; in this manner, it could\ndecompose input signals with complex patterns, and extracts their hidden characteristics for further usage.\nObviously, an overcomplete AE architecture, like the designed DR-SAE, tends to learn the identity function in its hidden layers\nto copy the input to the code layer. This problem can be effectively mitigates by employing the stacked method, and freezing the\ntrained layers by resetting optimizer parameters [29, 30]. This method forces the network to learn meaningful representations of\nthe input signals, as illustrated in Figure 3. This figure depicts some samples of the features extracted in the DR-SAE code layer.\nIt also displays a segment (the red box) of a raw input signal in Figure 3-d, alongside six samples of its extracted features, generated\nby some of the channels of the DR-SAE's code layer (Figure 3-a,b,c,e,f,g). These features (the pink lines) represent different aspects\nof the input signal that in many cases might be difficult to perceive directly from the raw signal. For example, Figure 3-a locates\nthe input signal peaks and valleys.\nThe encoder section of the DR-SAE network can be treated as a nonlinear complex function, as given by: F: x \u2192 X,x \u2208\n$R^{H_{0},W_{0},C_{0}}$, X \u2208 $R^{H_{5},W_{5},C_{5}}$, where H, W, C are the height, width and number of channels, respectively. It is obvious that Ho = Co =\n1, since the input signal is a time series. For simplicity, in layer l of F, the V\u2081 = [Vl1, V12, ..., Vic] is a set of filter kernels, where vic\nrefers to parameters that create the c-th channel output. The output of F is given by\n$$X = S(\\sum_{k_{5}=1}^{C_{5}} S(\\sum_{k_{4}=1}^{C_{4}} S(\\sum_{k_{3}=1}^{C_{3}} S(\\sum_{k_{2}=1}^{C_{2}} S(\\sum_{k_{1}=1}^{C_{1}} X * V_{1k_{1}})* V_{2k_{2}})* V_{3k_{3}})* V_{4k_{4}})* V_{5k_{5}})$$ (1)\nwhere * denotes the convolution operator, S is the SELU activation function, and each parenthesis indicates the output of the layer\nthat is equipped with ci filters. The SELU function is a proper activation function for HAR tasks because the decoder"}, {"title": "", "content": "aims to reconstruct the input signal in its output. Since some parts of the input signal are negative, it is essential to these negative\nparts. This function is approximately linear for input values within a specific range (-1, 1). By accepting this assumption that the\nactivation function acts approximately linear in most of the activities of the employed convolutional layers, the output of each\nchannel of a convolutional layer can be formulated by a summation through all previous channels that implicitly entangles with\nthe spatial correlation [31]. For example, if we consider the output of the first channel of the second layer, it can be formulated by:\nX21 = (x * V11) * V211 + (x * V12) * + (x * V13)\nV212\n*\nV213 + ... + (x * V1c\u2081) *\nV211 (2)\nwhere V21; shows the i-th filter in the first channel of the second layer, convolved with the i-th output from the first layer. This can\nbe further expressed as:\nX21 = X * (V11 * V211 + V12 * V212 + V13 * V213 + ... + V1c1 * V21c\u2081) (3)\nThis formulation explains that each output of the second layer is made by the summation of a bundle of FIR filters, such as V11 *\nV211,\u2026, V1c1 * V21c1 \u2022 If we extend the formulation up to the code layer, the summations will involve higher-degree FIR filters,\nfor instance V11 * V211 * V311 * V411 * V511\u00b7 Figure 4 shows the frequency response of a few of these filters by only changing the\ninvolved code layer kernels after the training phase of the network. It is evident that the frequency responses of the resulting filters\nare noticeably different which in turn, discriminate and represent various aspects of the input signal in the code layer outputs."}, {"title": "B. Feature fusion", "content": "As shown in Figure 1, the extracted features by the DR-SAEs are firstly stacked in a tensor for each sensor unit. Next, a Local\nFeature Fusion Autoencoder network (LFF-AE) is proposed to implement the fusion process. As shown in Figure 5, six sets of\nfeatures extracted from the accelerometers and gyroscopes signals of a sensor unit are concatenated and feed the input of an LFF-\nAE. By passing through four convolutional layers of the encoder of LFF-AE, the fused version of the input features is available as\nC4 output channels. In this manner, multiple salient patterns of different sensor units, which are mounted in different parts of the\nbody, are now available and ready to be fused in an additional network in the third step of the proposed model.\nTo further reduce the size of the extracted fused features and enhance their robustness to input variations, two max pooling layers\nare put after the first two convolutional layers, respectively. As reported in [32], this architecture results in lower sensitivity of the\nfeatures to slight distortions and variations within the input data. Additionally, the batch normalization layer (BN) that adjust the\namplitude of the code layer output is embedded just after the third convolutional layer, which experimentally is verified that leads\nto better final classification results. The BN layer works over the output of the previous layer channels, by calculating and using\nthe mean and variance over the input mini-batches, during the network training phase [33]. As depicted in Figure 5, the decoder\nand encoder together form a completely symmetric structure, with the exception that the encoder contains an extra BN layer. All\nconvolutional and trans-convolutional layers utilize a kernel size of 1 \u00d7 3. The max-pooling filters are correspondingly 1 \u00d7 4 and\n1 \u00d7 3, with stride two. Moreover, different values are explored for the channels number (C\u2081 to C4) to identify the optimal settings\nthat lead to the best classification performance. The process of tuning the hyperparameters is detailed in Section IV.C."}, {"title": "1) Local Feature Fusion", "content": "As shown in Figure 1, the extracted features by the DR-SAEs are firstly stacked in a tensor for each sensor unit. Next, a Local\nFeature Fusion Autoencoder network (LFF-AE) is proposed to implement the fusion process. As shown in Figure 5, six sets of\nfeatures extracted from the accelerometers and gyroscopes signals of a sensor unit are concatenated and feed the input of an LFF-\nAE. By passing through four convolutional layers of the encoder of LFF-AE, the fused version of the input features is available as\nC4 output channels. In this manner, multiple salient patterns of different sensor units, which are mounted in different parts of the\nbody, are now available and ready to be fused in an additional network in the third step of the proposed model.\nTo further reduce the size of the extracted fused features and enhance their robustness to input variations, two max pooling layers\nare put after the first two convolutional layers, respectively. As reported in [32], this architecture results in lower sensitivity of the\nfeatures to slight distortions and variations within the input data. Additionally, the batch normalization layer (BN) that adjust the\namplitude of the code layer output is embedded just after the third convolutional layer, which experimentally is verified that leads\nto better final classification results. The BN layer works over the output of the previous layer channels, by calculating and using\nthe mean and variance over the input mini-batches, during the network training phase [33]. As depicted in Figure 5, the decoder\nand encoder together form a completely symmetric structure, with the exception that the encoder contains an extra BN layer. All\nconvolutional and trans-convolutional layers utilize a kernel size of 1 \u00d7 3. The max-pooling filters are correspondingly 1 \u00d7 4 and\n1 \u00d7 3, with stride two. Moreover, different values are explored for the channels number (C\u2081 to C4) to identify the optimal settings\nthat lead to the best classification performance. The process of tuning the hyperparameters is detailed in Section IV.C."}, {"title": "2) Global Feature Fusion", "content": "The third step of the proposed approach involves fusing all the feature sets extracted up to this step to obtain a unique set of\nrichly combined features from different movements of the body parts. This step is performed by designing and employing the\nGlobal Feature Fusion AutoEncoder (GFF-AE). The finally resulted features extensively represent the outputs of all the n sensor\nunits mounted on different parts of the body. By using the proposed hierarchical approach, we prevent the premature fusion of\ndifferent input features during movement recognition. For example, in hand-related activities, the contribution of foot-mounted\nsensors in recognizing the movement is negligible; therefore, combining its outputs with other sensors outputs, in one step, may\nmislead the final classifier network; the proposed multi-step fusion structure successfully overcomes this weakness.\nThe input of GFF-AE is a tensor that contains all the outputs from the code layers of the former LFF-AEs. This combined\nrepresentation reflects the effects of the current activities of all sensors installed on different positions of the body. The architecture\nof GFF-AE net is similar to LFF-AE (Figure 5) except in the details of reducing the dimensions of C\u2081 to C4 layers, which is\ndiscussed in Section IV.C."}, {"title": "C. Classification", "content": "We employ a fully connected network with four subsequent layers as a classifier to evaluate the proposed hierarchical feature\nextraction and feature fusion method. Obviously, the numbers of neurons in its different layers are varied according to the number\nof the output features extracted via the proposed HUF model, and the number of activities that must be investigated and recognized."}, {"title": "IV. EXPERIMENTS", "content": "We employ three datasets to evaluate our proposed HUF model, and address various challenges in HAR: (1) the Daily Life\nActivities (DaLiAc) dataset [17] with 13 HA tasks, recorded by using a complex setup with many sensors mounted on different\nparts of the body; (2) the UCI HAR dataset [34], recorded by employing a smartphone placed on the waist, representing a dataset\nwith a single sensor; and (3) the Multimodal Dataset of Freezing of Gait in Parkinson's Disease (MDFG-PD) dataset, focusing\non Parkinson's disease (PD) gait with various age and PD ranges. To record MDFG-PD dataset, different devices were employed\nwhich we only focus on IMU sensors to detect the walking with or without Freezing of Gait (FOG). MDFG-PD poses additional\nchallenges due to sensor disconnections during walking [35], which requires a robust method to fuse the features of the involved\nsensors. Table 1 and Figure 7 provide additional insights into the introduced datasets and the distribution of each of the tasks."}, {"title": "A. Datasets", "content": "We employ three datasets to evaluate our proposed HUF model, and address various challenges in HAR: (1) the Daily Life\nActivities (DaLiAc) dataset [17] with 13 HA tasks, recorded by using a complex setup with many sensors mounted on different\nparts of the body; (2) the UCI HAR dataset [34], recorded by employing a smartphone placed on the waist, representing a dataset\nwith a single sensor; and (3) the Multimodal Dataset of Freezing of Gait in Parkinson's Disease (MDFG-PD) dataset, focusing\non Parkinson's disease (PD) gait with various age and PD ranges. To record MDFG-PD dataset, different devices were employed\nwhich we only focus on IMU sensors to detect the walking with or without Freezing of Gait (FOG). MDFG-PD poses additional\nchallenges due to sensor disconnections during walking [35], which requires a robust method to fuse the features of the involved\nsensors. Table 1 and Figure 7 provide additional insights into the introduced datasets and the distribution of each of the tasks."}, {"title": "B. Segmentation", "content": "Sliding window segmentation offers advantages for time series analysis, particularly in identifying localized features like short-\nterm trends, and also in improving computational efficiency for large datasets [36]. However, it presents a challenge for HAR\nresearches that focus on short-time activity recognition. For example, the small segments might not perfectly capture the start and\nend points of activities, and this imposes a difficult task to the network to learn many considerable variations occurred in the\nsegments boundaries. In order to deal with this problem by increasing the training data, and enhancing the robustness of the trained\nnetworks, we employ a 50% overlap between adjacent sliding windows. Moreover, we split data into training and test sets, using"}, {"title": "C. Experimental Settings", "content": "The structure of the proposed model applied to each of the mentioned databases should be adapted to the specifications of that\ndatabase. For the DaLiAc dataset, the structure is exactly as shown in Figure 1. For the UCI HAR dataset, since it is recorded by\nemploying a single sensor unit in a mobile phone, we can omit the GFF-AE part whose task is the fusion of the output data of\nmultiple sensors; therefore, it utilizes only one LFF-AE net for feature fusion. For the case of the MDFG-PD dataset, the architec-\nture remains the same as shown in Figure 1. In this dataset, for some of the subjects, some sensor units are inactive. When this\nproblem occurs, all blocks of the proposed model, except for the GFF-AE block, continue to function normally because the DR-\nSAE and LFF-AE blocks rely only on the active sensors. However, in the GFF-AE block, the inputs from the missing sensors are"}, {"title": "1) Model structure", "content": "The structure of the proposed model applied to each of the mentioned databases should be adapted to the specifications of that\ndatabase. For the DaLiAc dataset, the structure is exactly as shown in Figure 1. For the UCI HAR dataset, since it is recorded by\nemploying a single sensor unit in a mobile phone, we can omit the GFF-AE part whose task is the fusion of the output data of\nmultiple sensors; therefore, it utilizes only one LFF-AE net for feature fusion. For the case of the MDFG-PD dataset, the architec-\nture remains the same as shown in Figure 1. In this dataset, for some of the subjects, some sensor units are inactive. When this\nproblem occurs, all blocks of the proposed model, except for the GFF-AE block, continue to function normally because the DR-\nSAE and LFF-AE blocks rely only on the active sensors. However, in the GFF-AE block, the inputs from the missing sensors are"}, {"title": "2) Hyperparameters Configuration", "content": "The hyperparameters of this network are empirically chosen as depicted in Figure 2, and are the same for all the employed\ndatasets. This network is an overcomplete AE, without incorporating max-pooling layers. Furthermore, the activation function of\nCNN layers is the SELU function that has self-normalizing properties, providing a high-level robustness during the learning pro-\ncess. Moreover it allows better training of networks with many layers [37]. In the following, we examine different kernel sizes\nsuch as (1 x 3), (1,\u00d7 5) and (1 \u00d7 7). We found that kernel size of (1 \u00d7 3) leads to better results in the evaluation phases. The\ntraining phase of a network continues until the loss function becomes less than 0.005, and at least five epochs pass."}, {"title": "a) DR-SAE Network", "content": "The hyperparameters of this network are empirically chosen as depicted in Figure 2, and are the same for all the employed\ndatasets. This network is an overcomplete AE, without incorporating max-pooling layers. Furthermore, the activation function of\nCNN layers is the SELU function that has self-normalizing properties, providing a high-level robustness during the learning pro-\ncess. Moreover it allows better training of networks with many layers [37]. In the following, we examine different kernel sizes\nsuch as (1 x 3), (1,\u00d7 5) and (1 \u00d7 7). We found that kernel size of (1 \u00d7 3) leads to better results in the evaluation phases. The\ntraining phase of a network continues until the loss function becomes less than 0.005, and at least five epochs pass."}, {"title": "b) LFF-AE and GFF-AE networks:", "content": "To evaluate and verify the effectiveness of various feature reduction steps taking place inside the proposed HUF model, we\nconducted some designed experiments on the DaLiAc dataset. This dataset includes data from four IMU sensors mounted on four\ndifferent parts of the body. For each sensor unit, an assigned LFF-AE recieves six distict feature sets, extracted from three\naccelerometers and three gyroscops outputs, as detailed in Figure 5. Every feature set consists of 256 feature vectors with the length\nof l, which concatenate to form a matrix with size (6 \u00d7 256, l) as a LFF-AE network block input. This input is transformed into a\nmatrix with the size of (C4LFF-AE, 16) in the code layer where C4LFF-AE represents the number of code layer kernels, and 16 is the"}, {"title": "3) input size", "content": "The size of the input data is another important parameter that greatly impacts the HAR performance. While, the HAR models\nwith short length inputs are more practical, this hyperparameter has often been ignored in previous studies. Figure 7-C illustrates\nthe effects of the input length on the obtained HAR accuracies, while applying HUF model to the DaLiAc dataset. A proper input\npattern length is investigated in the range of 0.64 to 5.12 seconds, and the obtained accuracy results, using the proposed HUF\nmodel, are shown in Figure 7-c. The figure shows that increasing the input length typically leads to higher test accuracies. However,\na slight decrease in the recognition accuracy occurs on the test data for the input duration of 5.12 seconds. This decrease happens\ndue to a trade-off between input pattern length and the available training input data segments. As the input segment length increases,\nthe number of the available training segments significantly decreases. This potentially leads to the model overfitting, and a wider\ngap between the accuracy results obtained over the training and test data."}, {"title": "D. Results", "content": "This section explores the benefits of our proposed feature extraction and fusion method against some of the previous state-of-\nthe-art methods that focused on supervised learning methods. A key advantage of our proposed model is its inherent robustness\nduring feature extraction and fusion, which is especially valuable when dealing with conditions that the data of a sensor is lost\nduring the recording phase. The inherent adaptability of the proposed unsupervised method allows it to handle variations and\nchallenges in a non-annotated data, and makes it well-suited for real-world applications.\nTable 2 presents the experimental results obtained over DaLiAc and UCI-HAR datasets by the feature extraction, feature fusion\nand classification via the proposed method, alongside some other state-of-the-art methods. It is noteworthy that our results are\nobtained from relatively short-time inputs, and are superior to most of the recent studies. Of course, there are two exceptions in\nTable 2: Debache et. al. [38] method in which some handcrafted features are utilized, or Huynh et. al. [39] method in which the\ndeveloped model needs to a long-time input data extracted from image-transformed data."}, {"title": "V. CONCLUSIONS", "content": "In this article, we presented a professionally designed deep model that greatly benefits from the hybrid architecture of CNN-\nbased autoencoders for application to the HAR task. The proposed method specifically tackles the challenges associated with\nfeature fusion from various sensors in scenarios involving weakly labeled datasets and multiple IMU sensors. The first key aspect\nof our approach lies in embedding signals from individual axes of each sensor, extracting extended beneficial features from them.\nThis allows us to effectively fuse the individual extracted feature sets in the subsequent two steps, ultimately enabling the HUF\nmodel to represent activities across the entire body. This approach offers a valuable solution for utilizing unsupervised trainable\nmodels in HAR tasks, particularly when dealing with datasets that lack extensive labeled data. Moreover, due to its hierarchical\narchitecture, the model demonstrates noticeable robustness against noisy input data and missing sensor conditions.\nThe proposed model demonstrates some limitations in differentiating highly similar static activities such as sitting, lying, and\nstanding, as depicted in Figure 8, for the DaLiAc dataset. This can be attributed to the inherent closeness in movement patterns\nbetween the short time activities. However, when evaluated on the UCI-HAR dataset, the model achieves superior accuracy com-\npared to the previous models. This is particularly evident in its ability to recognize walking activities and static postures with\ngreater precision. Furthermore, the model exhibits robust performance when applied to the MDFG-PD dataset. This dataset pre-\nsents an additional challenge due to the presence of missing sensor data in some subjects. The effectiveness of the model in handling\nsuch data underscores its ability to perform well in real-world scenarios where data imperfections are often inevitable.\nTraining the proposed HUF hybrid network is computationally expensive, especially for some employed stacked AEs, but this\ncomputational cost is not a major issue as it can be greatly minimized during the deployment of the fully trained model."}]}