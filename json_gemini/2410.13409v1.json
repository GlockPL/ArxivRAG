{"title": "ATTR-INT: A SIMPLE AND EFFECTIVE ENTITY ALIGNMENT FRAMEWORK FOR\nHETEROGENEOUS KNOWLEDGE GRAPHS", "authors": ["Linyan Yang", "Jingwei Cheng", "Chuanhao Xu", "Xihao Wang", "Jiayi Li", "Fu Zhang"], "abstract": "Entity alignment (EA) refers to the task of linking entities in\ndifferent knowledge graphs (KGs). Existing EA methods rely\nheavily on structural isomorphism. However, in real-world\nKGs, aligned entities usually have non-isomorphic neigh-\nborhood structures, which paralyses the application of these\nstructure-dependent methods. In this paper, we investigate\nand tackle the problem of entity alignment between heteroge-\nneous KGs. First, we propose two new benchmarks to closely\nsimulate real-world EA scenarios of heterogeneity. Then we\nconduct extensive experiments to evaluate the performance\nof representative EA methods on the new benchmarks. Fi-\nnally, we propose a simple and effective entity alignment\nframework called Attr-Int, in which innovative attribute in-\nformation interaction methods can be seamlessly integrated\nwith any embedding encoder for entity alignment, improving\nthe performance of existing entity alignment techniques. Ex-\nperiments demonstrate that our framework outperforms the\nstate-of-the-art approaches on two new benchmarks.", "sections": [{"title": "1. INTRODUCTION", "content": "Recently, knowledge graphs (KGs) have been built and ap-\nplied in various domains. However, most KGs are constructed\nby different organizations and individuals, which inevitably\nbrings about heterogeneity problems. Knowledge fusion [1,\n2] aims to align and merge heterogeneous and redundant in-\nformation in KGs to form globally unified knowledge identi-\nfication and association. Entity Alignment (EA) has been the\nkey technique in knowledge graph fusion. The main purpose\nof EA is to find equivalent entities among different KGs.\nThe methods based on knowledge representation learning\nhave gained their popularity in solving the entity alignment\nproblem. These methods assume that equivalent entities from\nvarious KGs posses similar neighborhood structures (i.e., iso-\nmorphism). However, it is not always the case as different"}, {"title": "2. PROBLEM DEFINITION", "content": "Definition 1 (Knowledge Graph). A knowledge graph\n(KG) is denoted as G = (E, R, A, V, Tr, Ta), where E =\n{\u20ac1, \u20ac2,...,em}, R = {r1, r2, ..., rn}, A = {a1, a2, ..., ap},\nV = {V1, V2, . . .,,vq}, T, \u2286 E\u00d7R\u00d7 E and Ta \u2286 E\u00d7A\u00d7V\nrepresent entity set, relation set, attribute set, attribute value\nset, relation triple set and attribute triple set respectively, and\nm, n, p, q are the numbers of entities, relations, attributes,\nand attribute values respectively.\nDefinition 2 (Entity Alignment in KGs). Given a source\nKG G1 = {E1, R1, A1, V1, T1, T1}, and a target KG G2 =\n{E2, R2, A2, V2, T2, T2}, the aligned entity pairs (training\nset) is denoted as P = {(e, e})\\e \u2208 E1, e \u2208 E2, e = e'},\nwhere = stands for equivalence, i.e., the entity e and the\nentity ef refer to the same thing in the real world. The goal of\nthe entity alignment task is to find remaining equivalent entity\npairs of these two KGs.\nDefinition 3 (Coverage Rate of Entity Pairs). Let (e, e3)\nbe an entity pair, N(e) and N(e) be the sets of neighbor-\ning entities of e and e3 respectively, then the coverage rate\nC(ee) of the entity pair (e, e) is calculated by C(ee) =\n|N(e) \u2229 N(e3)|/min (|N(e)|, |N(e)|), where |\u00b7 | repre-\nsents the size of a set."}, {"title": "3. METHODOLOGY", "content": "Our proposed Attr-Int framework is shown in Figure 1, which\nconsists of three major components: encoder, attribute sim-\nilarity module and interaction module. First, entity embed-\ndings are obtained from the encoder, from which the simi-\nlarities between entities are calculated, and the preliminary\nentity alignment result is obtained. Then, the attribute simi-\nlarity matrix is calculated by the attribute similarity module,\nand the entity alignment result based on attribute information\nis obtained. Finally, the interaction module carries out suf-\nficient cross-learning [19] on the above two results to obtain\nthe final result. It is worth noting that, in the attribute sim-\nilarity module, unlike previous methods leveraging attribute\ninformation, there is no need to learn attribute embedding or\npre-align any attributes."}, {"title": "3.1. Encoder", "content": "Our framework can incorporate any existing entity alignment\ntechnique. For the sake of generality, we consider the repre-\nsentative technique (RDGCN [5]) as the encoder in our frame-\nwork. In order to better incorporate relation information into"}, {"title": "3.2. Attribute Similarity Module", "content": "We assume that the same entities have similar attribute value\ninformation, and the similarity of entity pairs is obtained by\ncalculating the similarity between the attribute values they\npossess.\nWe make the following assumption: if an attribute value is\nobserved to occur only once in a KG, then it can be regarded\nas a crucial attribute value for distinguishing entities. Under\nthis assumption, if two entities from different KGs share such\na same attribute value, it is determined that these two entities\nshould be aligned.\nFormally, given a KG G = (E, R, A, V, Tr, Ta) and\nvi\u2208 V, if attribute value vi only occurs once in G, such\nthat (e, a, vi) \u2208 Ta, then we can consider vi is a crucial\ndistinguishing attribute value. That is, if (e\u2081, a, vi) \u2208\nTa, (e,a,vi) \u2208 T2, we can get e\u2081 = e. Similarly, if\nthe number of occurrences of attribute value v is k, and the\nfrequency Vfre = 1, then the probability of alignment of two\nentities with attribute value vis. For a given entity pair\n(e1, ef), we need to obtain the overlapping attribute value\nsets (1, 3) of the two entities. The similarity of entity pair\nSet, e\u00b2) is calculated by S(e, e') = \\sum{(v_{fre_i}, v_{fre_j})}.\n                                                              k"}, {"title": "3.3. Interaction module", "content": "We propose two interaction approaches in the interaction\nmodule: result correction based on attribute information dis-\ncrimination (RC) and matrix combination based on parameter\nsearch (PS) in order to fully combine the information obtained\nfrom encoder and attribute similarity module."}, {"title": "3.3.1. Result Correction Based on Attribute Information Dis-crimination", "content": "The encoder gets the entity embedding by training the labeled\ndata, and then gets the preliminary entity alignment result by\nembedding distance. Attribute similarity module does not re-\nquire any labeled data training, and attribute similarity of enti-"}, {"title": "3.3.2. Matrix Combination Based on Parameter Search", "content": "Inspired by a inference rule in the formal logic NAL [20]\nwe propose another interaction method, which is to combine\nthe entity embedding similarity matrix SEA obtained by en-\ncoder and attribute similarity matrix SAT, and form the com-\nbined similarity matrix that used to evaluate alignment per-\nformance. For each pair of test entity e \u2208 E\u00b9 and e \u2208 E2,\nSEA and SAT can be seen as similarity belief statements, as\nshown in equation (3). In the truth-value, fEA and fAT rep-\nresent frequency in SEA and SAT respectively, CEA and CAT\nrepresent confidence in SEA and SAT respectively. \u2194 repre-\nsents similarity.\n    e\u2081 \u2194 e'\u2082(fEA, CEA)\n\u2194\n    e\u2081 \u2194 e'\u2082(fAT, CAT)\n\nThen a revision rule inference can be performed to combine\nthe two beliefs due to approximately disjoint evidence bases.\nThe truth-value function is shown in equation (4), where\nf1/f2 and C1/C2 represents the frequency and confidence of\nthe first/second premise belief, f and crepresents those in the\nconclusion belief.\n\nf = \\frac{f_1C_1(1-C_2) + f_2C_2(1-C_1)}{C_1(1-C_2) + C_2(1 \u2013 C_1)}\nC = \\frac{C_1(1-C_2) + C_2(1 \u2013 C_1)}{C_1(1-C_2) + C_2(1 \u2013 C_1) + (1 - \u0441\u2081)(1 \u2013 C_2)}\n\nThe expectation of the conclusion belief e \u2194 e (f, c) is\ncomputed by c\u00d7 (f \u2013 1/2) + 1/2. Finally, the conclusion\nexpectations constitute the elements of the combined simi-\nlarity matrix. The element of SEA and SAT corresponds to\nSEA and AT, respectively. CEA as a parameter represents the\noverall reliability (or we can call it as the expected amount of\nevidence to support the belief) of the entity embedding sim-\nilarity matrix, and analogously cAT as another parameter. A\nparameter grid search for cEA and cAT is performed on each\nexperiment scenario."}, {"title": "4. EXPERIMENT", "content": "4.1. Experimental Settings\n4.1.1. Datasets\nTable 1 outlines the statistics of new benchmark datasets\nobtained by lowering the coverage rate of the original bench-\nmarks, DBP15K [10] and two monolingual datasets with\nsparse version in OpenEA [18]. As shown in Figure 2 (a), we\ncount the percentage of the coverage rate of the entity pairs in\neach stage of the original benchmark datasets DBP15K, D-W-\n15K(V1) and D-Y-15K(V1). As we can see, the vast majority\nof entity pairs have high rates of coverage, which makes it\nchallenging to confirm KG heterogeneity. So we process"}, {"title": "4.1.2. Implementation Details", "content": "For each dataset, we split entity pairs into training, validation,\nand test set with ratio of 2:1:7. Following [10], we use Google\nTranslate to translate all values to English for cross-lingual\ndatasets."}, {"title": "4.2. Compared Methods", "content": "We conduct extensive experiments to evaluate previous rep-\nresentative EA methods on the new benchmark datasets. Ac-\ncording to the differences in the embedding modules, meth-\nods are divided into three categories: Translation-based meth-\nods, GNN-based methods and BERT-based methods. We se-\nlected 11 state-of-the-art EA methods, which cover differ-\nent embedding modules and contain representative methods\nfor using attribute information. Translation-based methods:\nJAPE [10], BootEA [21], AliNet [22], AttrE [14], MultiKE\n[13]. GNN-based methods: GCN-Align [11], MRAEA[4],\nRDGCN [5], RAGA [23]. BERT-based methods: BERT-\nINT [3], SDEA [16].\nLike SDEA, we replace entity descriptions in BERT-INT"}, {"title": "4.3. Main Experiments", "content": "Table 2 compares the overall performance of our method and\nthe baseline methods. BERT-based methods gain the best\nresults which owe to treat entity alignment as the downstream\nobjective to fine-tune a pre-trained BERT model. For BERT-\nINT, it has a strong dependency on entity name. Since FR-EN\n(in DBP15Khs) and D-Y-15K(V1)hs include well-aligned en-\ntity names (which are extracted literally similar), it works well\non these datasets as expected. BERT-INT performs poorly\non D-W-15K(V1)hs, this is because the two KGs on the\ndatasets do not contain literally matched entity names. SDEA\nachieves the best results on ZH-EN (in DBP15Khs) and D-\nW-15K(V1)hs, which is mainly attributed to the design of\nidentifying various contributions of neighbors and handling\nalignment of long-tail entities. As a comparison, although\nour method dose not use BERT to obtain information, we still\nachieve comparable performance with BERT-INT and SDEA.\nMoreover, Attr-Int(PC) outperforms BERT-INT (80.1) by 1.7\nof hits@1 on JA-EN (in DBP15Khs)."}, {"title": "5. CONCLUSIONS", "content": "In this paper, we construct new benchmark datasets to closely\nmimic real-world EA scenarios. Then, we propose a simple\nand effective entity alignment framework (Attr-Int), in which\ninnovative attribute information interaction methods can be\nseamlessly integrated with any embedding encoder for en-\ntity alignment, improving the performance of original entity\nalignment techniques. Experimental results show that our\nframework outperforms the state-of-the-art approaches."}]}