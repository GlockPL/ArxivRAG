{"title": "DRCap: Decoding CLAP Latents with Retrieval-augmented Generation for Zero-shot Audio Captioning", "authors": ["Xiquan Li", "Wenxi Chen", "Ziyang Ma", "Xuenan Xu", "Yuzhe Liang", "Zhisheng Zheng", "Qiuqiang Kong", "Xie Chen"], "abstract": "While automated audio captioning (AAC) has made notable progress, traditional fully supervised AAC models still face two critical challenges: the need for expensive audio-text pair data for training and performance degradation when transferring across domains. To overcome these limitations, we present DRCap, a data-efficient and flexible zero-shot audio captioning system that requires text-only data for training and can quickly adapt to new domains without additional fine-tuning. DRCap integrates a contrastive language-audio pre-training (CLAP) model and a large-language model (LLM) as its backbone. During training, the model predicts the ground-truth caption with a fixed text encoder from CLAP, whereas, during inference, the text encoder is replaced with the audio encoder to generate captions for audio clips in a zero-shot manner. To mitigate the modality gap of the CLAP model, we use both the projection strategy from the encoder side and the retrieval-augmented generation strategy from the decoder side. Specifically, audio embeddings are first projected onto a text embedding support to absorb extensive semantic information within the joint multi-modal space of CLAP. At the same time, similar captions retrieved from a datastore are fed as prompts to instruct the LLM, incorporating external knowledge to take full advantage of its strong generative capability. Conditioned on both the projected CLAP embedding and the retrieved similar captions, the model is able to produce a more accurate and semantically rich textual description. By tailoring the text embedding support and the caption datastore to the target domain, DRCap acquires a robust ability to adapt to new domains in a training-free manner. Experimental results demonstrate that DRCap outperforms all other zero-shot models in in-domain scenarios and achieves state-of-the-art performance in cross-domain scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Automated audio captioning (AAC) is a cross-modal translation task that seeks to generate natural language descriptions for given audio clips [1]. This process involves detailing the audio in terms of events, acoustic scenes, temporal relationships, actions, object interactions, and environmental context [2]. Conventional AAC models often employ an encoder-decoder architecture [3], where an audio encoder extracts fine-grained audio features and a text decoder generates captions auto-regressively conditioned on these audio representations. The audio encoders used in previous studies [4]\u2013[8] are often pre-trained on tasks such as audio tagging or sound event detection [9]\u2013[11], while the text decoders are pre-trained large language models (LLMs) with extensive encyclopedic knowledge, such as BART [12] or GPT-2 [13].\nDespite the significant strides made in AAC, most fully supervised models still rely on extensively human-annotated datasets for training. However, data scarcity remains a critical issue for AAC, as annotating audio data demands careful attention and complex analysis for accuracy. Moreover, given the diversity in audio concepts [6] and annotation styles across different datasets [14], existing fully supervised models often lack the flexibility to generalize to new domains, leading to diminished performance in cross-domain evaluations, where training and test data come from two distinct datasets.\nTo overcome these challenges, researchers proposed zero-shot audio captioning framework [15]\u2013[19], which seeks to generate captions without training on costly audio-text pair data. These works typically leverage the multi-modal capabilities of the CLAP model [20]\u2013[22]. To bridge the modality gap [23] of CLAP, Deshmukh et al. [15] and Kouzelis et al. [17] injected Gaussian noise into CLAP latents, while Zhang et al. [16] crafted soft and hard prompts. However, adding noise can diminish the rich semantic information within the CLAP multi-modal space, while the fixed-category hard prompt in [16] risks misleading the decoder. Moreover, text decoders in previous works struggle to decode CLAP latents into accurate descriptions containing multiple sound events. A stronger LLM is required to fully leverage this joint multi-modal space. As a result, although existing zero-shot audio captioning models demonstrate strong performance in cross-domain scenarios, they still lag significantly behind fully supervised models in in-domain scenarios.\nIn this paper, we propose DRCap, a data-efficient and transferable audio captioning system that leverages the synergy between CLAP and LLM. Based on the aligned multi-modal space of CLAP, DRCap requires only textual data for training, where a Vicuna-7B [25] is fine-tuned with LoRA [24] to reconstruct the original caption from the CLAP text embedding. During inference, the text encoder is replaced by the audio encoder. To mitigate the modality gap and enhance the semantic richness of the generated caption, we use both the projection strategy [26] from the encoder side and the retrieval-augmented generation [27] strategy from the decoder side. Specifically, audio embeddings are first projected onto a text embedding support, incorporating textual information while retaining their original acoustic features. At the same time, semantically similar captions retrieved from an external datastore are fed as prompts to direct the LLM, harnessing its strong generative abilities to create more accurate descriptions. Moreover, both the text embedding support and the caption datastore can be customized to match the target domain, providing our model with robust adaptability to new domains. Experimental results demonstrate that DRCap performs comparably to fully supervised methods in in-domain scenarios and achieves state-of-the-art results in cross-domain scenarios."}, {"title": "II. METHODS", "content": "We leverage the joint multi-modal space of CLAP to perform text-only training and then infer on audio clips in a zero-shot manner. As illustrated in Figure 1 (left), CLAP jointly trains an audio encoder fa(.) and a text encoder ft (.) to align semantically similar audio-text pairs in a shared embedding space. After training, fa (a) \u2248 ft(t) holds for any audio-text pair (a, t).\nGiven a raw caption t \u2208 T, where T represents a caption corpus, the objective in text-only training is to decode its CLAP text embedding Et = ft(t) back into the original caption t. To achieve this, we train a lightweight linear mapping network m to align the CLAP latent space with the LLM, producing et = m(Et). The LLM then reconstructs the original caption using et along with an additional encoded prompt discussed in Section II-B. During inference, given an audio clip a, we replace the text encoder with the audio encoder of the CLAP model, extracting the audio embedding Ea = fa(a). Due to the modality gap between audio and text embeddings, directly feeding Ea to the LLM through m will yield sub-optimal results. To address this issue and enhance the quality of generated captions, we employ both the retrieval-augmented generation strategy from the decoder side and the projection strategy from the encoder side, detailed in Section II-B and Section II-C respectively."}, {"title": "B. Retrieval-augmented Generation", "content": "Retrieval-Augmented Generation (RAG) combines information retrieval from a datastore with a generative model, allowing it to generate more accurate, context-aware outputs by incorporating external knowledge. DRCap leverages the RAG method to take full advantage of the generative capabilities of the LLM, bridging the modality gap and improving its ability in describing unseen sound events.\nDuring training, given a raw caption t, we use its CLAP text embedding Et = ft(t) to retrieve semantically similar captions from the datastore DS. The retrieval process for a candidate caption ti \u2208 DS is based on the cosine similarity between their respective CLAP text embeddings, calculated as follows:\nS(t, ti) = \\frac{f_t(t) \\cdot f_t(t_i)}{\\| f_t (t) \\| \\| f_t (t_i) \\|} (1)\nWe noticed, however, that naively selecting the top k most similar captions can lead the LLM to become lazy, merely reproducing one of the k captions as the output, neglecting et. To mitigate this issue and improve the model's robustness, we proposed a similarity selection strategy, defining a similarity range [Smin, Smax] from which k captions are randomly selected. If fewer than k captions fall within this range, only the qualifying captions are used as input. The effectiveness of our strategy is verified in Section IV-B.\nAdditionally, the model is given a fixed prompt (e.g., \"Describe the audio you hear\") to help the LLM better understand the task. The similar captions and the fixed prompt are encoded using the tokenizer of the LLM. Let es and ep denote the encoded embeddings of the similar captions and the fixed prompt. The model is trained to minimize the cross-entropy loss conditioned on z = Concat(et, es, ep):\nL_{CE} = - \\frac{1}{L_t} \\sum_{i=1}^{L_t} \\log p(t_i | z, t_1, ..., t_{i-1}) (2)\nWhere et = m(ft(t)) is the mapped CLAP text embedding, Lt is the length of the input caption t, ti is the i-th token of t. During training, we froze the CLAP encoder and trained only the linear mapping network, while applying LoRA [24] to fine-tune the large language model, which significantly enhanced training efficiency.\nDuring inference, given an audio clip a, the text-to-text retrieval is replaced with the audio-to-text retrieval, where we use the CLAP audio embedding Ea = fa(a) to retrieve k most similar captions. The cross-modal similarity for a candidate caption ti \u2208 DS is defined as:\nS(a,t_i) = \\frac{f_a(a) \\cdot f_t(t_i)}{\\| f_a(a) \\| \\| f_t(t_i) \\|} (3)\nThe similarity selection is turned off, and instead, we choose the most similar captions to provide the LLM with maximum information, since the audio-to-text retrieval during inference is significantly harder than the text-to-text retrieval during training."}, {"title": "C. Projection-based Decoding", "content": "Moreover, during inference, instead of directly feeding the audio embedding Ea to the linear mapper m, we first project it into the text embedding space of the CLAP model. Assuming that the system is trained on a caption corpus T = {t1, t2, ..., tN}, where N denotes the size of T. We can accumulate the text embeddings used during training, creating an embedding support S = {E1, E2, ..., EN}, where Ei = ft(ti). For a given audio embedding Ea, its corresponding projected text-like embedding Et could be obtained by performing a weighted combination of the text embeddings within the support:\nE_t = \\frac{\\sum_{i=1}^N \\exp(\\frac{(E_a \\cdot E_i)}{\\tau}) E_i}{\\sum_{j=1}^N \\exp((\\frac{E_a \\cdot E_j}{\\tau}))} (4)\nWhere \u03c4 is a temperature parameter, the projected vector Et can capture the extensive semantic information from the support, while retaining its original acoustic features. Et is then aligned with the LLM through the linear mapper m. Conditioned on both the CLAP embedding and the encoded similar captions, the LLM is able to generate accurate and semantically rich textual descriptions in a zero-shot manner."}, {"title": "D. Domain Adaptation", "content": "With the assistance of the text embedding support S and the caption datastore DS, DRCap is capable of generating precise and meaningfully detailed captions. Moreover, the modifiability of both S and DS provides DRCap with the flexibility to quickly adapt to new domains. When encountering new sound event domains, relevant captions can be integrated into the text embedding support and the datastore. The multi-modal latent space of CLAP could then provide semantically rich projected embeddings to decode, with similar captions guiding the LLM to describe the audio. Notably, no further training is needed for this entire process. The construction of DS and S will be discussed in Section III-B."}, {"title": "III. EXPERIMENTAL SETTINGS", "content": ""}, {"title": "A. Datasets", "content": "We train and evaluate DRCap on two most widely used AAC datasets, AudioCaps [29] and Clotho [30]. AudioCaps is a subset of AudioSet [31] that has been reannotated with caption labels. Each audio clip is annotated with a single caption for the training set and five captions for the validation and test sets. Our downloaded version contains 49274 examples for the training set, 494 for the validation set, and 957 for the test set. Clotho consists of audio clips sourced from Freesound, each labeled with 5 captions. In our experiment, we use version 2.1 of Clotho, which contains 3839 examples in the training set, 1045 in the validation set, and 1045 in the test set.\nThe frozen CLAP model employed to extract audio and text embeddings is trained on WavCaps [32] and Sound-VECaps [33]. WavCaps comprises approximately 400k audio clips sourced from AudioSet-SL [34], BBC Sound Effects, FreeSound, and Sound-Bible, while Sound-VECaps contains approximately 1.6M audio clips sourced from AudioSet. Both datasets are weakly annotated with the assistance of ChatGPT [35]. We filtered out the audio clips in the dataset that overlap with AudioCaps or Clotho, assuming that the target domain audio data are unavailable during training."}, {"title": "B. Experimental Setup", "content": "To evaluate the comprehensive performance of DRCap, we conduct experiments in both in-domain and cross-domain setups: (1) We train and evaluate the model on the same dataset Dsource, using the standard split. (2) We train the model on the training set of Dsource, and evaluate on the test set of another dataset Dtarget. During inference, for scenario (1), we use all text embeddings accumulated in the training stage as the support S mentioned in Section II-C, which corresponds to the text embeddings of all captions in the training set of Dsource. For (2), we curate the text embedding support by encoding all captions from the training set of Dtarget. In both settings, we utilize a caption datastore DS consisting of 450k captions sourced from WavCaps and the training sets of AudioCaps and Clotho.\nIn line with other audio captioning research, we use common captioning metrics, including METEOR [36], SPICE [37], CIDEr [38], SPIDEr [39] and FENSE [40]. For all metrics, higher scores indicate better performance."}, {"title": "C. Implementation Details", "content": "DRCap was trained for 40,000 steps on AudioCaps and 20,000 steps on Clotho, with a peak learning rate of le-5, 1000 warm-up steps followed by a linear decay. We use the Adam optimizer [41] and a batch size of 4. Validation was performed every 1000 steps, where the checkpoint with the lowest validation loss was saved for evaluation. Number of captions retrieved is set to k = 3, and the range of similarity selection is fixed as Smin = 0.75, Smax = 0.85.\nThe CLAP model, which employed the text encoder RoBERTa [42] and the audio encoder HTS-AT [10], was trained on WavCaps and Sound-VECaps, with a batch size of 256, a peak learning rate of 5e-5 for 15 epochs. Training followed a cosine annealing schedule with a 2-epoch warm-up phase, and the model from the final epoch was used.\nThe implementation was based on the open-source project SLAM-LLM [43]. The entire training process was conducted on a single NVIDIA A800 GPU, taking approximately 40 hours for CLAP training and 4 hours for DRCap training."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": ""}, {"title": "A. Main Results", "content": "Tables I and II present the performance of DRCap in both in-domain and cross-domain settings. We compare DRCap's performance with fully-supervised AAC models: EnCLAP [28], Prefix-AAC [4], and RECAP [6], all of which are open-source and not trained with additional data. We further compare DRCap with zero-shot audio captioning models ZerAuCap [18], WSAC [17] and Zhang et al. [16]. ZerAuCap [18] uses CLAP to guide the LLM to generate descriptions, WSAC [17] trains a text decoder using the prefix language modeling paradigm conditioned on CLAP embeddings, while Zhang et al. [16] crafts soft and hard prompts to bridge the modality gap between audio and text embeddings of CLAP.\nDRCap surpasses all competitive zero-shot audio captioning systems for in-domain scenarios by a large margin and is comparable with other fully-supervised methods. For cross-domain scenarios, it achieves state-of-the-art results across all metrics, highlighting its robust domain-transfer capability. Furthermore, we found that DRCap outperforms other methods in terms of the FENSE [40] score in both two scenarios. We hypothesize that this advantage is due to DRCap's ability to utilize the semantically rich joint multi-modal space of CLAP, which allows it to generate more refined captions."}, {"title": "B. Ablation Study", "content": "We conduct a comprehensive ablation study to validate each component of DRCap.\nSimilarity Selection We turned off the similarity selection discussed in Section II-B, instead selecting the top k most similar captions during training. Since the ground-truth captions are available in the training stage, the retrieved most similar captions closely match the target in both semantics and vocabulary. This could lead the LLM to simply copy one of the retrieved captions with minor changes, trivializing the captioning task. However, during inference, without access to textual information, audio-to-text retrieval struggles to match the quality of text-to-text retrieval, and simply copying the retrieved captions hinders the model's performance, as shown in Table III and Table IV. Our proposed similarity selection strategy significantly alleviates the learning collapse and compels the LLM to take into account both the CLAP embedding and the retrieved captions, which improves generation quality in both in-domain and cross-domain settings.\nRetrieval-augmented Generation. We dropped all the similar captions in both the training and inference stages to evaluate the impact of RAG. As illustrated in table III and table IV, conditioning solely on CLAP embeddings results in inferior performance across all metrics in both scenarios, showing the advantage of similar captions in guiding the LLM to generate more accurate descriptions.\nLLM Fine-tuning. We froze the LLM during training to conduct the ablation study on LoRA. Table III and IV highlight the significance of efficient LLM fine-tuning. Integrating LoRA adapters proved effective in aligning the CLAP latent space with the LLM.\nProjection-based Decoding We directly fed the audio embedding ea to the linear mapping network m without using projection during inference to assess the benefit of projection-based decoding (PD). As illustrated in table III and IV, the modality gap caused a significant drop in performance when ea was used directly, while PD effectively bridge the discrepancy between audio and text embeddings.\nTarget Domain Information. In this study, we assume that no prior knowledge of the target domain is provided for cross-domain scenarios. Specifically, during inference, we use captions from the training set of Dsource to construct S, rather than using captions from the training set of Dtarget. Additionally, all captions from Dtarget are excluded from the datastore DS. As shown in table IV, incorporating domain knowledge greatly improves DRCap's cross-domain performance, demonstrating its adaptability during inference. Moreover, despite the absence of target domain knowledge, DRCap still performs competitively with state-of-the-art methods, as indicated in Table II."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "We present DRCap, a data-efficient and flexible audio captioning model that requires only textual data for training and can quickly adapt to other domains. Based on the CLAP model and the LLM, DRCap leverages projection-based decoding and retrieval-augmented generation to mitigate the modality gap. Conditioned on both the projected CLAP embedding and the retrieved similar captions, DRCap could produce more accurate and semantically rich descriptions. The replaceability of the text embedding support and the caption datastore guarantees the adaptability of the model. Experimental results show that DRCap outperforms other zero-shot audio captioning models in in-domain scenarios and achieves state-of-the-art performance in cross-domain scenarios."}]}