{"title": "Flexible and Efficient Grammar-Constrained Decoding", "authors": ["Kanghee Park", "Timothy Zhou", "Loris D'Antoni"], "abstract": "Large Language Models (LLMs) are often asked to generate structured outputs that obey precise syntactic rules, such as code snippets or formatted data. Grammar-constrained decoding (GCD) can guarantee that LLM outputs matches such rules by masking out tokens that will provably lead to outputs that do not belong to a specified context-free grammar (CFG). To guarantee soundness, GCD algorithms have to compute how a given LLM subword tokenizer can \"align\" with the tokens used by a given context-free grammar and compute token masks based on this information. Doing so efficiently is challenging and existing GCD algorithms require tens of minutes to preprocess common grammars. We present a new GCD algorithm together with an implementation that offers 17.71x faster offline preprocessing than existing approaches while preserving state-of-the-art efficiency in online mask computation.", "sections": [{"title": "1. Introduction", "content": "Constrained decoding guides the output of Large Language Models (LLMs) by greedily enforcing user-given constraints in highly structured settings. Grammar-constrained decoding (GCD) (Geng et al., 2023) refers to the specific case where the constraint is given as a formal grammar that the LLM's output must conform to. This is done by applying parsing algorithms to build an automaton that interfaces with the LLM's decoding algorithm to mask away all tokens that will provably lead to outputs not in the grammar. For example, GCD can be used to ensure that an LLM generates programs that only use a specific set of function names.\nParsing algorithms for Context-Free Grammars (CFG) achieve efficiency by processing input strings in two phases. Terminals-e.g., variable names or string literals are recognized by a lexer in a preprocessing phase, while the grammatical structure of how terminals can be arranged-e.g., that the body of a loop should be a sequence of statements\u2014is enforced by an automaton operating over lexed terminals. A key challenge with implementing GCD algorithms is that the tokens used by subword tokenizers in LLMs do not align with the terminals used by parsers.\nBecause of this misalignment, GCD approaches either incur high online token-masking overhead (>1 second per token), or high offline preprocessing costs (>30 minutes) to precompute a lookup table that relates LLM tokens to terminals (Beurer-Kellner et al., 2024; Ugare et al., 2024). Thus, existing GCD algorithms are impractical for domains where the constraining grammar frequently changes. Examples of such domains include program synthesis (Alur et al., 2019), where a grammar is provided for every program one may try to synthesize, and grammar prompting (Wang et al., 2024), where grammars are predicted from a given prompt to then guide the LLM towards a particular output structure.\nIn this paper, we introduce a new approach for grammar-constrained decoding that is both flexible\u2014i.e., handles diverse grammars without incurring prohibitive offline preprocessing costs and efficient-i.e., maintains state-of-the-art efficiency in online token masking. The key innovation is a combined analysis of the LLM token vocabulary and set of CFG terminals. The analysis efficiently precomputes a lexer-state-dependent mapping between sequences of CFG tokens and individual LLM tokens. This precomputation allows the decoder to efficiently identify valid LLM tokens at decoding time while avoiding wasteful processing of token combinations that are not realizable within the given LLM vocabulary.\nWe implement our approach in a tool, GREATGRAMMA, which compares favorably to existing GCD approaches. GREATGRAMMA achieves an average 17.71x speedup in offline preprocessing compared to related approaches (Ugare et al., 2024) while maintaining state-of-the-art online masking efficiency (5-32ms per token). Furthermore, our evaluation reveals that existing GCD implementations contain soundness bugs (e.g., they mask tokens that should not be masked or do not terminate when preprocessing simple grammars), whereas our approach lends itself to an elegant implementation consisting of simple modules.\nThis paper makes two contributions:"}, {"title": "2. Approach Overview", "content": "In this section, we give some brief background about parsing (Sec. 2.1) and grammar-constrained decoding (Sec. 2.2), overview the high-level structure of our decoding approach (Sec. 2.3), and discuss our improvements over prior work."}, {"title": "2.1. Lexing and Parsing", "content": "While it is possible to build parsers that operate directly on input text, practical parsing tools first perform a preprocessing pass using a lexer for efficiency. A lexer is built from a set of regular expressions which define terminals (e.g. identifiers, keywords, or integer literals). Fig. 1a illustrates two regular expressions defining two terminals B (strings consisting of an a followed by a sequence of b's) and C (strings consisting of an a followed by a sequence of c's).\nThe lexer then takes as input a string and tokenizes it so that substrings matching regular expressions for terminals are grouped together. Here, our terminals consist of an a followed by either a sequence of b or c. For instance, the string \"abaccab\" lexes as ab acc ab.\nThe pass by the lexer ensures that the parser can be defined directly over terminals instead of individual characters. This separation of concerns avoids mixing character-level processing with higher-level grammar rules, making both components easier to implement and maintain. In particular, the grammar for the language can be defined at the terminal level. In our example, the grammar shown in Fig. 1e accepts sequences of terminals of the form BCBCBC... (where B and C can be any strings matching those terminals).\nOne way of defining a parser is as an automaton: it takes as input a sequence of terminals and either accepts or rejects the sequence. To handle all context-free grammars the automaton needs to be a pushdown automaton (PDA) that is equipped with a stack. In our example, the simple stack-free automaton shown in Fig. 1f is enough to describe all valid sequences of terminals."}, {"title": "2.2. Using Parsing for Constrained Decoding", "content": "We recall the general structure of a constrained decoder. When given a sequence of tokens $t_{1:k}$, CONSTRAINEDDECODING (Alg. 1 in Appendix A) uses a checker C during the decoding process to mask out any next token $t_{k+1}$ that would result in a prefix $t_{1:k+1}$ for which no possible completion satisfies the constraint. Specifically, given a prefix $t_{1:k}$ and vocabulary $V \\subseteq \\Sigma^{+}$, a checker C computes a Boolean mask $C(t_{1:k}; V) = m \\in \\{0,1\\}^{|V|}$, where a value of 1 denotes a viable token (i.e., one for which there exists a sentence completion that can lead to constraint satisfaction), and 0 denotes an invalid one (i.e., one for which no sentence completion can lead to constraint satisfaction). The mask is then applied to the logits (Deutsch et al., 2019) produced by the language model to exclude invalid tokens from consideration.\nGoing back to grammars, a parser either accepts or rejects a string. A simple extension is to make this process online- the parser rejects as soon as it consumes a token that will ensure the input string fails to parse. Intuitively, the key idea behind grammar-constrained decoding is that the parser primitive for checking if a token will be allowed or not by the parser can be used to build the checker C.\nHowever, there is a key problem with using a parser as a checker for constrained decoding: a parser reads language level tokens (which this paper calls terminals to avoid confusion), while an LLM outputs tokens according to its subword vocabulary. The LLM tokens may span multiple or only fragments of language terminals, complicating the decoding process. This problem is known as token misalignment (Poesia et al., 2022), and is the chief source of complexity for GCD implementations. For example some of the LLM tokens in Fig. 1c correspond to terminals whereas others can span multiple terminals."}, {"title": "2.3. Our Approach", "content": "Fig. 1 illustrates the overall structure of our approach, which is based on a data structure we introduce called a token spanner table. Intuitively, the token spanner table stores what sequences of terminals could be emitted by the lexer if it reads an LLM token from a given state.\nOur GCD approach is split into two parts: an offline preprocessing phase in which the table is constructed and an online phase where the table is then used to generate token masks during decoding.\nThe offline portion of our algorithm takes two inputs: the LLM vocabulary V (Fig. 1c) and the definitions (as regexes) for terminals (Fig. 1a). It then constructs a finite state automaton (FSA) with a set of states Q representing the lexer (Fig. 1b). For example, state $q_2$ represents a state where the lexer has already read the substring ab and can (optionally) read more b's to emit a B terminal. We use this automaton together with the LLM's vocabulary to build the token spanner table (Fig. 1d). The keys of the token spanner table are pairs (q, v) where $q \\in Q$ is a state of the lexer automaton and $v \\in V$ is an LLM token. The (q, v) entry of the token spanner table contains the set of sequences of terminals that can eventually be produced if the lexer is in state q and is fed in one additional LLM token v. For example, given state $q_2$ and LLM token aba we can either produce a sequence of terminals BBB or BBC-this is because we are in a lexer state corresponding to having started a B token, lex a complete B token from ab, and then start an arbitrary new token.\nIn the online portion, our algorithm uses the token spanner table together with the parser to construct masks stating which LLM tokens are valid at each step. During decoding, the algorithm tracks the state of the lexer (Fig. 1b) and the state of the parser PDA (Fig. 1f) on the prefix the LLM has generated so far. The algorithm then analyzes the state the parser is in to determine the possible sequences of terminal the parser could consume next. Then, the algorithms consult the token spanner table using these sequences and the current state of the lexer to determine what LLM tokens should be masked and kept."}, {"title": "3. Offline Token Preprocessing", "content": "Our approach starts by preprocessing the lexer to efficiently construct a lookup table that relates LLM tokens to terminal sequences (Sec. 3.1) and vice versa (Sec. 3.2). The preprocessed lexer is then used to analyze the parser to determine what terminal sequences are actually possible sequences in the grammar (Sec. 3.3)."}, {"title": "3.1. Lexer Preprocessing", "content": "Let $\\Sigma$ denote the set of string characters, $\\Sigma^{*}$ denote the set of strings over this alphabet, and $\\Gamma$ denote the set of terminals (i.e., grammar-level tokens). A lexer is a function Lex that given an input string $w \\in \\Sigma^{*}$, returns a pair $(T_{1} ... T_{k}, w_{r})$, where $T_{1} ... T_{k} \\in \\Gamma^{*}$ is a sequence of terminals and $w_{r} E \\Sigma^{*}$ is the suffix of w that has not been lexed (i.e., mapped to language terminals) yet.\nTypically lexers resolve ambiguity by making some simplifying assumptions that also help improve efficiency and avoid backtracking. We use the same assumptions and describe them next.\nConsider a language that contains two different tokens for the increment operator ++ and the addition operator +. Although the input string ++ could be tokenized as two separate + addition tokens, in practice lexers prioritize the longer valid token to resolve ambiguity (and which usually captures the intended syntax of the programming language). This behavior is called the maximal munch principle: the lexer matches the longest possible substring starting at the current position that aligns with a defined token pattern.\nUnder a strict interpretation of the maximal munch principle, if the lexer reaches the end of the input stream while processing a partial triple-quoted Python string \"\"\"a\", the lexer should tokenize the input as two strings \"\" and \"a\". However, supporting such cases would require either waiting until the end of the input string to produce any tokens or allowing backtracking. As such, in practice many lexers (including Python's) will raise an error and stop lexing if the scanned prefix cannot be tokenized as a single terminal. This greedy behavior disallows some strings, but guarantees that the lexer can resolve all tokenization ambiguities by inspecting only the next character at each step.\nDefinition 3.1 (1-lookahead). A lexer Lex is 1-lookahead if for every string $w \\in \\Sigma^{*}$ and valid continuation $c \\in \\Sigma$ of w, whenever $Lex(w) = (T_{1} ... T_{k},r)$ then $Lex(wc)$ is either $(T_{1}...T_{k}T_{k+1},c)$ for some $T_{k+1} \\in \\Gamma$ or $(T_{1} ...T_{k},rc)$.\nTerminals are specified as a set of regular expressions. It is oftentimes convenient to work with a lexing automaton, which is the finite state automaton (FSA) that accepts strings matching any terminal definition (McNaughton & Yamada, 1960). We refer the reader to Sec. B.1 for formal definitions of the semantics of FSA, but recall that an FSA is a tuple $A = (\\Sigma, Q, q_{0}, \\delta, F)$ where $\\Sigma$ is the alphabet, Q is the set of states with initial state $q_0 \\in Q$, $\\delta$ contains transitions of the form $q \\xrightarrow{c} q'$, and $F \\subset Q$ is the set of final states.\nA 1-lookahead maximal munch lexer can be defined from a lexing automaton as follows: The input is processed character-by-character by transitioning through the FSA's states. When no valid transition exists for the next character c, the lexer checks whether the current state corresponds to a valid language token. If it does and the tokenizer has at this point produced a pair $(T_{1}... T_{k}, w_{r})$, the not-yet tokenized string $w_r$ is tokenized with token $T_{k+1}$ corresponding to the reached state, the FSA is reset to its initial state $q_0$ (and the tokenizer state $(T_{1} ...T_{k}T_{k+1},\\epsilon)$ with the empty string $\\epsilon$), and the character c is consumed as the starting character of a new token $q_0$. If the current state does not correspond to a valid token or if c cannot be consumed at $q_0$, then c is invalid. Invalid characters inform what tokens should be masked during constrained decoding.\nThis process can be formalized as a finite-state transducer (FST), an extension of a finite-state automaton that can produce output terminals when reading characters. Given the original lexing automaton A representing valid tokens, we write $T_A$ to denote the lexing transducer, the FST corresponding to A. The construction of $T_A$ from A is formalized in Alg. 2 in Appendix A, but at a high level the process simply adds transitions to handle cases where no valid transition exists for the current character c, outputting terminals and exiting final states. Fig. 2 shows the lexing transducer derived from the FSA in Fig. 1."}, {"title": "3.2. Realizable Terminal Sequences", "content": "Now that we have defined the formal machinery behind lexing, we are ready to explain how LLM tokens can be mapped to possible sequences of terminals.\nWhen the transducer $T_{A \\circ V}$ in Fig. 4 consumes the LLM token aba from the initial state $q_0$, it produces the terminal B and moves to state $q_1$. If we inspect the grammar $G_{BC}$ in Fig. 1e, we can deduce that the parser, which receives as input sequences of tokens, expects/requires the next language token to be c. Since $T_{A \\circ V}$ in state $q_1$ does not immediately produce any output when consuming b, the generated terminal sequence so far (i.e., B) is still a valid prefix according to the grammar $G_{BC}$. However, after transitioning to $q_2$, no possible path can generate C next.\nAs illustrated by the above example, for each transition $q \\xrightarrow{t:T_{1}...T_{k}} q'$ in the lexing transducer $T_{A \\circ V}$, we should check whether there is a terminal T such that (i) T can be generated along some path from $q'$, and (ii) $T_{1} ...T_{k}T$ is accepted by the grammar. This observation leads to the following definition, which describes which terminal sequences need to be considered by the parser.\nDefinition 3.2 (Realizable Terminal Sequences). Given a token vocabulary V and FSA A, let $\\delta$ be the set of transitions in the token-level lexing transducer $T_{A} \\circ T_{y}$. The set of realizable terminal sequences $Reg_{AV}$ is defined as\n$Reg_{AV} = \\{T_{1}...T_{k}T \\mid q \\xrightarrow{t:T_{1}...T_{k}} q' \\in \\delta and T \\text{ can be generated along some path from } q'\\}$.\nNote that the LLM vocabulary V contains all characters in $\\Sigma$, ensuring that $\\Sigma^{*} = V^{*}$. Therefore, any next terminal producible in $T_{A}$ is also producible in the combined transducer $T_{A \\circ V}$ and vice versa. This equivalence allows us to simplify producibility checks: instead of analyzing the large combined transducer $T_{A \\circ V}$, we need only compute reachability to accepting states within $T_A$ to determine producible next terminals.\nInverse Token Spanner Table Alg. 4 computes the set of realizable terminal sequences and constructs the key data structure we use to perform constrained decoding.\nDefinition 3.3 (Inverse Token Spanner Table). Given a lexer state $q \\in Q_{A \\circ V}$ and $T_{1}...T_{k}T \\in Red_{AV}$, the entry $T_{inv} (q, a)$ in the inverse token spanner table $T_{inv}$ is the set of tokens that generates $T_{1} ... T_{k}T$ from state q. Formally,\n$T_{inv}(q, T_{1}...T_{k}T) = \\{t \\mid q \\xrightarrow{t:T_{1}...T_{k}} q' \\in \\delta and T \\text{ can be generated along some path from } q'\\}$"}, {"title": "3.3. Parser Preprocessing", "content": "We do not formally define context-free grammars (CFG) for brevity and refer the reader to Sec. B.2 for details. For the sake of this paper, the reader only needs to know that a CFG parser is typically formalized as a pushdown automaton (PDA), an extension of FSA with an execution stack (Sch\u00fctzenberger, 1963). The definition of a PDA is similar to that of an FSA, but transitions are also allowed to manipulate a stack over symbols $\\Pi$ via push and pop operations. Therefore, in a PDA, a configuration after reading a sequence of input characters is a pair of automaton state $q \\in Q$ and execution stack $\\gamma \\varepsilon \\Pi^{*}$.\nWe refer the reader to Sec. B.4 for the formal definition of a PDA, but informally, each PDA transition $q \\xrightarrow{c[\\beta/\\beta']} q'$ can only be activated if the character being read is a c and the top of the current stack configuration $\\gamma$ matches to sequence of stack symbols $\\beta\\in \\Pi^{*}$. If the transitions is activated, the current state becomes $q'$, and the top $\\beta$ elements of the stack are replaced with new stack elements $\\beta' \\in \\Pi^{*}$.\nAs with lexer preprocessing, our goal is to also preprocess the parser to avoid iterating over every terminal sequence generated by the lexer at runtime. To achieve this objective, we directly compose the detokenizing transducer $Re_{AOV}$ (Definition 3.2), with the PDA P produced by a parser generator (and where transitions operate over single terminals) (Allauzen & Riley, 2012) to obtain a new pushdown automaton $P \\circ T_{h eAOV}$ where transitions operate over terminal sequences.\nThis last transducer can efficiently determine valid sequences of terminal symbols from each parser state. We note that preprocessing both the lexer and parser in tandem is a key feature that distinguishes our work from prior work (Beurer-Kellner et al., 2024; Ugare et al., 2024).\nOne key source of efficiency resulting from our approach is that many transitions in the combined transducer $T_{AOV}$ produce the same terminal sequence $T_{1}...T_{k}T$, making $|Reg_{ov}|$ smaller than $|V|$ or $|\\delta|$. Thus, the set of realizable terminal sequences $Reg_{ov}$ enables efficient precomputation of what sequences of terminals the parser should consider.\nBecause pushdown automata need a stack to be able to parse arbitrary context-free grammars, we cannot decide entirely at preprocessing time whether a given terminal sequence can be accepted by a PDA at any given state-one has to inspect the content of the stack at execution time. However, many terminal sequences can be identified as always accepted or always rejected, independent of the current execution stack.\nStack invariance provides a sufficient condition for knowing when a sequence of terminals is accepted.\nProposition 3.4 (Stack Invariance). If a PDA P accepts an input sequence w in state q with stack configuration $\\gamma$, then w is also accepted in the same state q when the stack configuration is $\\gamma' \\cdot \\gamma$ for some $\\gamma'$ (i.e., when $\\gamma$ appears at the top of the stack with additional symbols beneath it).\nIt follows that a terminal sequence accepted by the PDA starting with an empty stack configuration is accepted under any stack configuration.\nThe following proposition shows how to construct a stack-free finite-state automaton that overapproximates the set of sequences accepted by a PDA.\nProposition 3.5 (Overapproximation via FSA). Consider an FSA $A_p$ obtained by removing all stack operations from a PDA P. If an input sequence w is not accepted by $A_p$ in state q, then w cannot be accepted by P in state q with any stack configuration.\nFollowing Proposition 3.5, a terminal sequence is always rejected if it is rejected by the FSA obtained by removing all stack operations.\nThe above reasoning can be formalized by computing the set of always-accepted tokens A and of context-dependent terminal sequences D. Given a lexer state $q_A$ and a parser state $q_P$, we denote by A($q_A$, $q_P$) the set of tokens that are accepted regardless of the stack configuration $\\gamma$, and by D($q_A$, $q_P$) the set of terminal sequences that may be accepted by some configuration $\\gamma$."}, {"title": "4. Online Token Masking", "content": "With a preprocessed inverse token spanner table $T_{inv}$, along with the table of always-accepted tokens A and context-dependent sequences D, we are now ready to describe the online part of our grammar-constrained decoder (Alg. 6).\nAt each decoding step, Alg. 6 analyzes the lexer state $q_A$, parser state $q_P$, and the current stack configuration $\\gamma$ to produce the set of exactly all LLM tokens $V_{allowed}$ that can lead to a sequence accepted by the the grammar.\nAlthough we construct the combined PDA $P \\circ T_{h eAov}$ for preprocessing what terminal sequences a token can result in, using the same automaton for checking at decoding time whether LLM tokens can lead to valid parsing sequences would be inefficient. PDAs cannot always be determinized and one would have to compute reachability for nodes in $P_{OT_{RE AOV}}$ under different stack configurations to determine valid language token sequences. This computation would effectively involve testing all sequences in $Re_{Aov}$ at decoding time, making the preprocessing of the parser meaningless. Therefore, Alg. 6 uses the set of always-accepted tokens A whenever possible to avoid traversing the PDA and only analyzes what terminal the PDA can accept at decoding time for context-dependent sequences described by D. The set of next legal tokens is then obtained by looking up, for each sequence of terminals accepted by the grammar, the set of tokens that can lead to that sequence. This is done by consulting the inverse token spanner table $T_{inv}$."}, {"title": "5. Experiments", "content": "We implemented our algorithms for computing token masks in a tool called GREATGRAMMA. A testament to the simplicity of our approach is that GREATGRAMMA is implemented in just 900 lines of Python code built on top of the LALR parser provided by the LARK library (Shinan et al.). Specifically, we used LARK to parse regular expressions and context-free grammars, and used LARK'S LALR parser to generate a parse table representing a deterministic PDA for parsing sequences of tokens in a given context-free grammar.\nIn this section, we evaluate GREATGRAMMA by answering the following two questions:\n\u2022 RQ1: How does the offline preprocessing overhead of GREATGRAMMA compares to existing GCD approaches?\n\u2022 RQ2: How does the online per-token decoding overhead of GREATGRAMMA compares to existing GCD approaches?\nBecause all GCD approaches in theory produce the same token masks and only differ in execution time, we do not need to evaluate GREATGRAMMA's performance on specific downstream tasks; the effectiveness of GCD has already been evaluated in prior work (Geng et al., 2023; Beurer-Kellner et al., 2024; Park et al., 2024).\nModels and Grammars. We conduct our experiments using three different tokenizers: Llama (32K tokens), Llama-3 (128K), and Qwen-2 (151K). We consider simplified Go, Java, and Python grammars from prior work on GCD (Ugare et al., 2024). We choose these grammars for three reasons: they are large grammars (87-99 terminals, 109-145 nonterminals, 353-520 production rules, 7,441-9,319 bytes), they capture real programming languages that are used in existing applications of constrained decoding, and they illustrate well the trade-off between offline preprocessing and online masking times. While other smaller grammars appear in existing work on GCD, the tools we compare against all take slightly different grammar formats with various restrictions (e.g., no left recursion was allowed in XGRAMMAR), and we had to manually translate all grammars we considered between these formats to perform our evaluation (a very time-consuming task).\nBaselines. We compare GREATGRAMMA against OUTLINES (Willard & Louf, 2023), SYNCODE (Ugare et al., 2024), and XGRAMMAR (Dong et al., 2024), the three state-of-the-art GCD tools that can nominally handle grammars of practical sizes.\nMeasures. For each combination of tokenizer and grammar, we measured the time taken to preprocess the grammar for that tokenizer and the average time taken by each GCD implementation to produce a token at inference time. To fairly evaluate per-token computation time, we wanted to ensure that all three tools followed the same token selection process and recorded the time required to compute the mask at each step-i.e., the online overhead. We manually built 5 programs in each grammar and used them to decide what sequence of tokens the decoder was going to produce-i.e., for each example program, the decoder produced each token in the program in order and computed the token masks at each step. Following this setup, we could exactly measure the same online average per-token overhead across all GCD approaches.\nDiscussion In summary, GREATGRAMMA emerges as the new state-of-the-art GCD approach. GREATGRAMMA is 17.71x faster at offline preprocessing than SYNCODE, the only other GCD tool with acceptable online overhead (RQ1), and exhibits the lowest online masking overhead than any other GCD tool (RQ2).\nWhile OUTLINES has the lowest preprocessing offline overhead, its seconds-per-token online overhead does not meet the needs of most practical applications of LLMs. OUTLINES' online overhead is due to the fact that its CFG module verifies the acceptability of each token at runtime.\nThe improvement in offline pre-processing over SYNCODE is expected as our new approach targets the inefficiency of SYNCODE's algorithm for token alignment. The slight improvement in online token-mask computation over SYNCODE is likely due to how GREATGRAMMA only consults the PDA for context-dependent sequences, while SYNCODE does so for all terminal sequences up to a bound.\nWhile of course we cannot guarantee that GREATGRAMMA is free of bugs, the simplicity of our approach makes its implementation easier (just 900 lines of code).\nFurthermore, GREATGRAMMA'S offline preprocessing times have allowed us to heavily test our implementation without incurring in multi-hour testing times.\nLimitations The current implementation of GREATGRAMMA works under the lexing assumptions described in Sec. 3.1: maximal munch principle and 1-lookahead lexing. It therefore does not support languages for which lexing requires more than 1-lookahead or instances where the same input sequence must be lexed differently depending on the parsing context.\nFor example, in Java, the end of the nested generic List<List<T>> is erroneously tokenized by a lexer operating under maximal munch as >>\u2014i.e., the bitwise right-shift operator. However, in this context the parser instead expects two consecutive > terminals denoting the closure of a generic type. (The Java grammar we used in our evaluation, sourced from the SYNCODE benchmark, does not support generic types.)"}, {"title": "6. Related Work", "content": "There are many different implementations of grammar constrained decoding (Geng et al., 2023; Willard & Louf, 2023; Lundberg et al.; Beurer-Kellner et al., 2024; Ugare et al., 2024; Dong et al., 2024; Gerganov, 2024), but we focus our comparison on the ones that can handle realistic grammars (e.g., those of modern programming languages).\nSYNCODE (Ugare et al., 2024) is the only that can provide low online overhead for the large grammars in our evaluation. SYNCODE speculatively unrolls future terminal sequences up to a fixed bound (i.e., every terminal sequence in Pk) and precomputes a table similar to our inverse token spanner table but for all sequences. Our evaluation shows that our table can be computed significantly faster since it only contains the set of realizable terminal sequences.\nDOMINO (Beurer-Kellner et al., 2024) precomputes a tree of terminal sequences similar to our lexing transducer. However, at decoding time, DOMINO must traverse the entire tree because the set of realizable terminal sequences varies for each lexer state. In contrast, our approach computes a single global set of realizable terminal sequences $Re_{Aov}$, which we then use to preprocess the parser. DOMINO's implementation is not publicly available, but their paper reports 20s for offline preprocessing and 22% online overhead for an extremely simplified C grammar with approximately 70 rules when using the smaller Llama tokenizer. For the same tokenizer, GREATGRAMMA can preprocess much larger grammars (353-520 rules) with similar times (25-35s).\nXGRAMMAR (Dong et al., 2024) reduces runtime checks by preprocessing context-independent tokens for character-based nondeterministic PDAs, but mentions the misalignment problem in their related work. By preprocessing the set of realizable terminal sequences and the inverse token spanner table, we efficiently integrate their context-independent token caching approach with a parser that uses a separate lexer. Furthermore, our overapproximation via FSA can identify more always-rejected sequences compared to their expanded suffix analysis, as our method can also follow rule-reduction edges by treating them as e-transitions."}, {"title": "7. Conclusion", "content": "We present GreatGrammA, a tool for grammar-constrained decoding that is both flexible (low offline overhead) and efficient (low online overhead). GREATGRAMMA precomputes a succinct and easy-to-generate data structure that only captures terminal sequences that are realizable by LLM tokens in a given lexer state. This data structure also speeds up online masking compared to similar approaches by further reducing the number of inputs the parser has to check during decoding.\nGREATGRAMMA is built with simple primitives that are already supported by existing parsing libraries and lends itself to an easy implementation consisting of just 800 lines of Python. This simple implementation is easier to test (we have identified that other tools often crash or produce incorrect outputs) and opens the door to many future possible research directions-e.g., finding ways to leverage the PDA stack to perform more aggressive precomputation or implementing decoding directly on GPUs."}, {"title": "A. Algorithms", "content": "In this section we formalize several algorithms presented in the paper. Alg. 1 describes the abstract algorithm for general constrained decoding. 2 describes how to build the lexing transducer from a lexing automaton given as a FSA. Alg. 3 describes the construction of the detokenizing transducer, which converts sequences of tokens to sequences of the characters they contain. Alg. 5 describes the parser preprocessing and how the always-accepted token table and context-dependent sequence table are built."}, {"title": "B. Formal Definitions", "content": ""}, {"title": "B.1. Finite-State Automata", "content": "A finite-state automaton (FSA) is defined as a tuple A = (\u03a3, Q, q\u03bf, \u03b4, F) where \u03a3 is the input alphabet, Q is the set of states,\nqo \u2208Q is the initial state, d \u2286 Q \u00d7 (\u2211 \u222a {e}) \u00d7 Q is the set of transitions, and F C Q is the set of accepting states. Each\ntransition (q, c, q'), also denoted by q\n transitions to state q'."}, {"title": "B.2. Context-Free Grammar", "content": "A context-free grammar (CFG) G is a tuple (\u0393, N, S, R) where \u0393 is a finite set of terminal symbols (e.g. constants, variable\nnames, and keywords), N is a finite set of non-terminal symbols, S \u2208 N is a start nonterminal, R is a set of production\nrules A \u2192 \u03b1\u2081, ..., an where A \u2208 N and a\u00a1 \u2208 NU\u0393.\nFormally, a grammar G defines a single-step derivation relation on sequences of symbols \u03b1, \u03b2, \u03b3 \u2208 (NU\u0393)*: \u03b1\u0391\u03b3 - \u03b1\u03b2\u03b3\nif A \u2192 \u03b2\u2208 R. The reflexive transitive closure of this relation is called derivation and written \u21d2*. A sequence of\ntokens w is a sentence if it is derivable from S; the set of all sentences is called the language of the grammar G, that is,\nL(G) = {w \u2208 \u0393* | S \u21d2* w}."}, {"title": "B.3. Finite State Transducer", "content": "An FST is a tuple T = (\u03a3, \u0393, Q, qo, \u03b4, F) where all components are analogous to those of a FSA but each FST transition\nq c:T1....Tr> q' in 8 denotes that when reading a character c from state q, the FST moves to a new state q' and also outputs\nthe sequence T\u2081 ... Tk of elements over the output alphabet \u0393."}, {"title": "B.4. Pushdown Automata", "content": "A pushdown automaton is a tuple P = (\u03a3, \u03a0, Q, q\u03bf, \u0396\u03bf, \u03b4, F) where \u03a3, Q, qo and F are as in their FSA definitions"}]}