{"title": "SAC-GLAM: Improving Online RL for LLM agents\nwith Soft Actor-Critic and Hindsight Relabeling", "authors": ["Loris Gaven", "Cl\u00e9ment Romac", "Thomas Carta", "Sylvain Lamprier", "Olivier Sigaud", "Pierre-Yves Oudeyer"], "abstract": "The past years have seen Large Language Models (LLMs) strive not only as\ngenerative models but also as agents solving textual sequential decision-making\ntasks. When facing complex environments where their zero-shot abilities are\ninsufficient, recent work showed online Reinforcement Learning (RL) could be\nused for the LLM agent to discover and learn efficient strategies interactively.\nHowever, most prior work sticks to on-policy algorithms, which greatly reduces the\nscope of methods such agents could use for both exploration and exploitation, such\nas experience replay and hindsight relabeling. Yet, such methods may be key for\nLLM learning agents, and in particular when designing autonomous intrinsically\nmotivated agents sampling and pursuing their own goals (i.e. autotelic agents).\nThis paper presents and studies an adaptation of Soft Actor-Critic and hindsight\nrelabeling to LLM agents. Our method not only paves the path towards autotelic\nLLM agents that learn online but can also outperform on-policy methods in more\nclassic multi-goal RL environments.", "sections": [{"title": "1 Introduction", "content": "In recent years, LLMs have demonstrated impressive capabilities in solving various tasks, from\nfollowing instructions to complex reasoning and real-world interactions [Ahn et al., 2022, Li et al.,\n2022, Wang et al., 2023c,b]. Despite these successes, challenges remain in applying LLMs as agents\nin sequential decision-making tasks, mainly due to misalignment when understanding environmental\nstates and action spaces. In a recent line of work [Carta et al., 2023, Wen et al., 2024a, Putta et al.,\n2024, Wen et al., 2024b] the use of online RL has been explored to interactively align LLM agents\nwith complex environments. Following these promising results, one could wonder whether such\nagents could learn in a more autotelic way where goals are not directly provided by the environment.\nA natural next step would be giving these LLM agents more autonomy, allowing them to freely\nexplore environments and set their own goals [Colas et al., 2021]. When designing such agents, prior\nwork notably showed how language can be leveraged to imagine new goals and shape the agent's\ncurriculum [Colas et al., 2022, 2020]. Recent studies successfully showed LLMs could generate\ngoals [Wang et al., 2023a, Zhang et al., 2024] and reward functions [Fan et al., 2022, Du et al., 2023]\nfor an autonomous agent, leveraging their world-modeling capabilities and commonsense knowledge\nto drive the agent towards interesting goals. While mixing prior work using LLMs as goal generators\nand LLMs as RL agents may appear straightforward, several challenges remain in obtaining autotelic\nLLM agents.\nAmong them, autotelic architectures usually involve experience replay and hindsight relabbelling to\nbest utilize all trajectories, even unsuccessful ones, which can be predominant when agents set their\nown goals. However, most current approaches to finetune LLM agents with RL rely on on-policy\nalgorithms that cannot handle experience replay and trajectory relabeling mechanisms. While a\nhandful of attempts Wen et al. [2024a], Putta et al. [2024] to use off-policy RL on LLMs exist,\nthey consider token-level actions (better suited for text generation tasks) and complex architecture\nbalancing token-level actions and environment-level actions (usually sequences of tokens). As a step\ntwoards autotelic LLM agents, this work introduces a version of Soft Actor-Critic (SAC) [Haarnoja\net al., 2018] explicitly designed for LLMs as RL agents combining the simplicity of prior on-policy\nmethods focusing on environment-level actions (e.g. GLAM [Carta et al., 2023]) and the advantages\nof off-policy learning. We then extend it with Hindsight Experience Replay (HER) [Andrychowicz\net al., 2017] and show our new method (SAC-GLAM) outperforms GLAM (called PPO-GLAM\nbelow) in a classic multi-goal environments by showcasing a better sample efficiency while staying\non par in time efficiency in spite of the stability and efficiency challenges of using an actor-critic\napproach with a pre-trained LLM actor and a randomly initialized critic."}, {"title": "2 SAC-GLAM", "content": "We consider a textual RL setting where, given a language vocabulary V, the environment returns an\nobservation \u03bf \u2208 VN and a reward r\u2208 R after an action a \u2208 A C VM (i.e., actions are sequences\nof tokens). The task or goal description g \u2208 G C VK conditions the reward. This environment\ncan be modeled as a goal-augmented Partially Observable Markov Decision Process (POMDP)\nM = (S, V, A, T, R, G, O, y), where S is the state space, A is the action space, G is the goal space,\nT:S\u00d7A\u2192 S is the transition function, R : S \u00d7 A \u00d7 G \u2192 R is the goal-conditioned reward\nfunction, O : S \u2192 VN is the observation function that maps a state to a textual description, and y is\nthe discount factor.\nIn such settings, PPO shined for its efficiency and simplicity when finetuning stochastic pre-trained\npolicies such as LLM agents [Carta et al., 2023, Wen et al., 2024b]. However, it cannot leverage\nHER as analyzed in Appendix G. We propose in sections below an off-policy alternative that keeps\nGLAM's simplicity on how the LLM is used as a stochastic policy over discrete environment-level\nactions (i.e. sequences of tokens)."}, {"title": "2.1 Soft Actor Critic with an LLM actor", "content": "We adapt the discrete version of SAC [Christodoulou, 2019] to our stochastic LLM-based policy.\nWe first follow GLAM's approach to obtaining a stochastic policy with an LLM by computing the\nprobability of each action ai \u2208 A from our environment as the probability of its token sequence\nai = {W1, W2, ..., W|az|} (with Wi \u2208 V) to follow a prompt containing both o and g:\nLPLLM(ai|o,g) = \\sum_{j=0}^{|a_i|}log PLLM(wj|o, g, w<j). (1)\nSubsequently, a softmax function is applied to generate a probability distribution over all possible\nactions and obtain our stochastic policy:\n\u03c0(a_i|o, g) = \\frac{e^{LPLLM(a_i | o, g)}}{\\sum_{a_j \\in A} e^{ELPLLM (a_j | o, g)}} (2)\nThen, we follow SAC and simultaneously learn both this policy and a Q-function, which we implement\nas an MLP added on top of the final decoder block of our LLM."}, {"title": "2.2 Pre-trained policy and randomly initialized critic", "content": "When running SAC with a pre-trained LLM as the policy, several unique challenges arise. In SAC,\nthe actor's updates rely on feedback from the critic. While poor estimates from a randomly initialized\ncritic may not significantly impact a randomly initialized policy, our situation is different. We aim to\nprotect the pre-trained LLM actor from harmful updates that could degrade its performance. One\npotential solution is to introduce a warm-up period where only the critic is trained.\nTo make this warmup period as short as possible, one must seek for faster critic convergence. First,\none could speed up its learning not only backpropagating gradients through the MLP but also through\nthe LLM's weights, while potentially impairing the actor's pre-trained capabilities. Second, while\na natural choice would be to have an MLP with |A| outputs that take as input the representation\nproduced by the LLM for the observation and goal in the prompt, we also investigated leveraging\nthe LLM's capabilities by rather using an MLP with a single output with the LLM encoding the\nconcatenation of the prompt and an action. This requires | A forward passes through the LLM and\nMLP to obtain all the actions' Q-value but may also ease the MLP's task leading to faster convergence.\nFinally, the critic's sample efficiency can also be improved after the warmup period. One way to\nachieve this is through the use of n-step returns with a target expressed as:\nY_t = E_{\\tau} [\\sum_{i=0}^{n-1} \\gamma^{i} r_{t+i} + \\gamma^{n} E_{a_{t+n} \\sim \\pi(\\cdot | O_{t+n}),g} [Q(S_{t+n}, a_{t+n}) - \\alpha log \\pi(a_{t+n} | O_{t+n}, g)]]. (3)\nIn standard SAC n = 1. This approach reduces the reliance on bootstrapping by accumulating rewards\nover multiple steps before bootstrapping, often leading to faster critic convergence. However, n-step\nreturns introduce the risk of older transitions becoming highly off-policy, as intermediate rewards\nand actions depend on the policy in place when the transitions were collected. To mitigate this issue,\nwe focus on minimizing the age of the oldest transitions (i.e., the number of policy updates since the"}, {"title": "2.3 Hindsight Experience Replay", "content": "We augment our SAC-GLAM agent with HER, enabling the algorithm to learn from failed attempts\nby relabeling portions of the failed trajectories with accidentally reached goals. At the end of an\nepisode, the trajectory with its initial goal and the relabeled sections of the trajectory are added to the\nreplay buffer. When training, we randomly sample a batch transitions from this buffer such that the\nbatch contains 50% of relabeled transitions."}, {"title": "3 Experiments", "content": "We evaluated our method in the Playground-text environment, a text-based adaptation (goals, observa-\ntions, and actions are represented as text) of the original Playground environment [Colas et al., 2020].\nWe selected this environment because it features a social partner describing goals reached during\na trajectory. In this environment, the agent receives a textual goal and must interact with different\nobjects (plants, animals, food and water) to complete the task. It features a sparse reward context\nwhere the agent receives 1 if the task is completed and 0 otherwise. In the original Playground\nenvironment, the agent moves along the x and y axes to reach objects. In the text-based version,\nwe simplified this by introducing actions like \"Go to {object}\", which directly move the agent to\nthe selected object. The environment features two types of tasks: \"Grasp {object}\" and \"Grow\n{object}\". Animals can be grown using either water or food, while plants can only be grown using\nwater. We also introduced sequential tasks, where the agent must complete two tasks in the correct\norder, significantly expanding the task space to 8,470 tasks. A detailed description of the environment\nis given in Appendix A.\nWe compared SAC-GLAM with and without HER to PPO-GLAM using Flan-T5-base [Chung et al.,\n2024] as the pre-trained actor for both approaches. Our results in Figure 2 indicate that for a fixed\nbudget of 400K steps, SAC-GLAM alone outperforms PPO-GLAM in sample efficiency while being\nslower. When equipped with HER, SAC-GLAM becomes even more sample efficient and achieves\ncomparable performance in terms of training time."}, {"title": "3.1 Ablations", "content": "We study in Appendix B different possible architectures for the critic as proposed in Section 2.2.\nWe show that sharing the policy's weights and using an MLP with a single output leads to better\nsample efficiency and greater stability. We hypothesize that the increased stability is due to the latent\nrepresentation provided to the MLP, which directly incorporates information about both the prompt\nand the action."}, {"title": "4 Discussion", "content": "In this paper, we introduce SAC-GLAM, an off-policy RL approach adapting SAC and HER to LLMS\nas RL agents. We notably highlight how Actor-Critic methods applied to pre-trained policies such\nas LLMs introduce challenges in balancing the policy and critic convergence and showed using a\nwarmup period, weights sharing, single Q-value head and n-step returns make SAC-GLAM obtain\ncomparable time efficiency to PPO-GLAM while surpassing it in sample efficiency. Finally, in\naddition to outperforming PPO-GLAM in a classic multi-goal RL setting, SAC-GLAM also paves\nthe path toward autotelic LLM agents where using HER and off-policy RL is key."}]}