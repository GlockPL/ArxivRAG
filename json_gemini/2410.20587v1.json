{"title": "GENERATOR MATCHING: GENERATIVE MODELING WITH ARBITRARY MARKOV PROCESSES", "authors": ["Peter Holderrieth", "Marton Havasi", "Jason Yim", "Neta Shaul", "Itai Gat", "Tommi Jaakkola", "Brian Karrer", "Ricky T. Q. Chen", "Yaron Lipman"], "abstract": "We introduce generator matching, a modality-agnostic framework for generative modeling\nusing arbitrary Markov processes. Generators characterize the infinitesimal evolution of\na Markov process, which we leverage for generative modeling in a similar vein to flow\nmatching: we construct conditional generators which generate single data points, then\nlearn to approximate the marginal generator which generates the full data distribution.\nWe show that generator matching unifies various generative modeling methods, including\ndiffusion models, flow matching and discrete diffusion models. Furthermore, it provides\nthe foundation to expand the design space to new and unexplored Markov processes such\nas jump processes. Finally, generator matching enables the construction of superpositions\nof Markov generative processes and enables the construction of multimodal models in\na rigorous manner. We empirically validate our method on protein and image structure\ngeneration, showing that superposition with a jump process improves image generation.", "sections": [{"title": "INTRODUCTION", "content": "Early deep generative models-like VAEs (Kingma, 2013) and GANs (Goodfellow et al., 2014) generated\nsamples in a single forward pass. With denoising diffusion models (DDMs) (Song et al., 2020; Ho et al.,\n2020), a paradigm shift happened were step-wise updates are used to transform noise into data. Similarly,\nscalable training of continuous normalizing flows (CNFs; Chen et al. 2018) via flow matching (Lipman\net al., 2022; Liu et al., 2022; Albergo et al., 2023) allowed for high-quality and fast generative modeling by\nsimulating an ODE. Since then, similar constructions based on diffusion and flows have also been applied to\nother modalities such as discrete data (Campbell et al., 2022; Gat et al., 2024) or data on manifolds (De Bortoli\net al., 2022; Huang et al., 2022; Chen & Lipman, 2024) leading to a variety of models for different data types.\nThe single common property of the aforementioned generative models is their iterative step-wise nature:\nstarting with a sample X0 ~ Psimple from an easy-to-sample distribution Psimple, they iteratively construct\nsamples Xt+h of the next time step depending only on the current state Xt. Mathematically speaking, this\nmeans that they are all Markov processes. In this work, we develop a generative modeling framework that\nsolely relies on that Markov property. At the core of our framework is the concept of a generator that\ndescribes the infinitesimal change of the distribution of a Markov process. We show that one can easily learn\na generator through a family of scalable training objectives a framework we coin generator matching (GM).\nGenerator matching unifies many existing generative modeling techniques across modalities such as denoising\ndiffusion models (Song et al., 2020), flow matching (Lipman et al., 2022), stochastic interpolants (Albergo\net al., 2023), discrete diffusion models (Campbell et al., 2022; Gat et al., 2024; Lou et al., 2024a), among\nmany others (see sec. 8). Most importantly, GM gives rise to new, unexplored models, and allows us to\ncombine models across different classes of Markov processes. We make the following contributions:"}, {"title": "GENERATIVE MODELING VIA PROBABILITY PATHS", "content": "Let S be a state space. Important examples are S = Rd (e.g., images, vectors), S discrete (e.g., language), S\na Riemannian manifold (e.g., geometric data) or their products for multimodal data generation. In generative\nmodeling, we are given samples x1,..., XN ~ Pdata from a distribution Pdata on S and our goal is to generate\nnovel samples z ~ Pdata. GM works for arbitrary distributions, in particular those that do not have densities\n(e.g., with discrete support). If for a distribution p a density exists, we write p(x) for its density and consider\np as a function p : S \u2192 R>o. For general probability measures p, we use the notation p(dx) where \"dx\" is a\nsymbolic expression denoting integration with respect to p in a variable x. For a reader unfamiliar with the\nnotation, one can simply assume the density exists and simply read p(dx) = p(x)dx.\nA fundamental paradigm of recent state-of-the-art generative models is that they prespecify a transformation\nof a simple distribution Psimple (e.g. a Gaussian) into Pdata via probability paths. Specifically, a conditional\nprobability path is a set of time-varying probability distributions (pt(dx|z))0<t\u22641 depending on a data point\nz \u2208 S. The data distribution Pdata induces a corresponding marginal probability path\nPt(dx) = Ez~pdata [Pt(dx|z)]\nThe main feature of the conditional probability path pt(dx|z) is that it is easy to sample from. Given a dataset\nof samples from Pdata, one can then also efficiently draw samples from the marginal pt(dx): first sample a\ndata point z ~ Pdata and then sample x ~ pt(dx|z). As we will see, this makes training scalable.\nThe key design requirement for the conditional probability path is that its associated marginal probability\npath interpolates between Psimple and Pdata, leading to the first design principle of GM:\nPrinciple 1: Given a data distribution Pdata, choose a prior Psimple and a conditional probability path such\nthat its marginal probability path (pt)0<t\u22641 fulfills psimple = po and Pdata = P1."}, {"title": "MARKOV PROCESSES", "content": "We briefly time-continuous Markov processes, a fundamental concept in this work (Ethier & Kurtz, 2009). For\nt\u2208 [0, 1], let Xt \u2208 S be a random variable. We call (Xt)0<t\u22641 a Markov process if it fulfills the following\ncondition for all 0 < t1 < t2<\u2026\u2026<tn<tn+1 < 1 and ACS (measurable):\nP[Xtn+1 \u2208 A|Xt1, Xt2,..., Xtn] = P[Xtn+1 \u2208 A|Xtn] \nInformally, the above condition says that the process has no memory. If we know the present, knowing the\npast will not influence our prediction of the future. In table 1, we give an overview over important classes\nof Markov processes. Each Markov process has a transition kernel (kt+h|t)0<t<t+h\u22641 that assigns every\nx \u2208 S a probability distribution kt+h|t(\u00b7|x) such that P[Xt+h \u2208 A|Xt = x] = kt+h\\t(A|x). Due to the\nMarkov assumption, a Markov process is fully specified by a transition kernel and its initial distribution po.\nConversely, any initial distribution and transition kernel define a Markov process.\nIn the context of GM, we use a Markov process as follows: Given a marginal path (pt(dx))0<t\u22641 (see\nsec. 2), we want to train a model that allows to simulate a Markov process such that X0 ~ Psimple \u21d2 Xt ~\nPt for all 0 \u2264 t \u2264 1. That is, if starting with the right initial distribution Xo ~ po, the marginals of Xt\nwill be pt. Once we have found such a Markov process, we can simply generate samples from p\u2081 = Pdata by\nsampling Xo ~ po and simulating Xt+h~kt+h\t(\u00b7|Xt) step-wise up to time t = 1. The challenge with such\nan approach is that an arbitrary general kernel kt+h|t is hard to parameterize in a neural network. One of the\nkey insights in the development diffusion models was that for small h > 0, the kernel kt+h|t can be closely\napproximated by a simple parametric distribution like Gaussian (Sohl-Dickstein et al., 2015; Ho et al., 2020).\nOne can extend this idea to Markov processes leading to the concept of the generator."}, {"title": "GENERATORS", "content": "Let us consider the transition kernel kt+h|t for small h > 0. Specifically, we consider an informal 1st-order\nTaylor approximation in t with an error term o(h):\n\"kt+h\\t = kt\\t + hLt+o(h)\", Lt := $\\frac{d}{dh}|_{h=0}$kt+ht, ktt(x) = \u03b4x\nWe call the 1st-order derivative Lt the generator of kt+h|t (Ethier & Kurtz, 2009; R\u00fcschendorf et al.,\n2016). Similar to derivatives, generators are first-order linear approximations and, as we will see, easier to\nparameterize than kt+h|t. As we will see, diffusion, flow, and other generative models can all be seen as\nalgorithms to learn the generator of a Markov process (see table 1). However, as a probability measure is not\na standard function, equation 1 is not well-defined yet. We will make it rigorous using test functions.\nTest functions. Test functions are a way to \"probe\u201d a probability distribution. They serve as a theoretical\ntool to handle distributions as if they were real-valued functions. Specifically, we use a family T of bounded,"}, {"title": "KOLMOGOROV FORWARD EQUATION AND MARGINAL GENERATOR", "content": "Beyond parameterizing a Markov process, the generator has a further use-case in the generator matching\nframework: checking if a Markov process generates a desired probability path pt. We discuss the latter now\nusing the Kolmogorov Forward Equation (KFE). Specifically, the evolution of the marginal probabilities pt\nof a Markov process Xt are governed by the generator Lt, as can be seen by computing:\nOt [Pt f] =\n Pt [kt+h|t f] = Pt \u00a9\nO\nf=[Pt+hf]=Pt\nLtf,\nwhere we used that the pt operation is linear to swap the derivative, and the fact that pt [kt+h\t f] =\nPt+h f. This shows that given a generator Lt of a Markov process Xt we can recover its marginal\nprobabilities via their infinitesimal change,\ndt [Pt f]= Pt [Ltf]\nConversely, if a generator Lt of a Markov process Xt satisfies the above equation, then Xt generates the\nprobability path (Pt)0<t<1, i.e. initializing Xo ~ po will imply that Xt ~ pt for all 0 \u2264 t \u2264 1 (see item 5)\n(Rogers & Williams, 2000). Therefore, a key component of the Generator Matching framework will be:\nPrinciple 3*: Given a marginal probability path (pt)0<t\u22641, find a generator satisfying the KFE."}, {"title": "GENERATOR MATCHING", "content": "We now discuss how to train a parameterized generator Lf to approximate the \u201ctrue\u201d marginal generator Lt.\nIn practice, Lt is linearly parameterized by a neural network Fe : S \u00d7 [0, 1] \u2192 \u03a9 where CV is convex\nsubset of some vector space V with inner product \u3008\u00b7, \u00b7\u3009 (see app. A.5 for details). Our goal is to approximate\nthe ground truth parameterization Ft: S \u00d7 [0, 1] \u2192 \u03a9 of Lt. For example, Ft = ut for flows, F\u2081 = \u03c3\nfor diffusion, or Ft = Qt for jumps (see table 1). We train the neural network Fe to approximate Ft. As a\ndistance function on \u03a9, we consider Bregman divergences defined via a convex function 4 : \u03a9 \u2192 Ras\nD(a,b) = (a) \u2013 [(b) + (a \u2013 b, \u2207\u00a2(b))], a, b \u2208 \u03a9\nwhich are a general class of loss functions including many examples such as MSE or the KL-divergence (see\napp. C.4.1). We use D to measure how well Ff approximates Ft via the generator matching loss defined as\nLgm(0) def Et~Unif,x~pt [D(Ft(x), F(x))]\nUnfortunately, the above training objective is intractable as we do not know the marginal generator Lt and\nalso no parameterization Ft of the marginal generator. To make training tractable, let us set F to be a linear\nparameterization of the conditional generator L\u1ec5 with data point z (see app. A.5). For clarity, we reiterate\nthat by construction, we know Fe, F\u00b2, pt(z), D as well as can draw data samples z ~ Pdata but the shape of\nFt is unknown. By proposition 1, we can assume that F\u2081 has the shape F\u2081(x) = \u222b F(x)P1\\t(dz|x). This\nenables us to define the conditional generator matching loss as\nLegm(0) def Et~Unif,z~Pdata,x~Pt(:\\z) [D(F(x), F(x))]\nThis objective is tractable and scalable. It turns out that we can use it to minimize the desired objective.\nProposition 2. For any Bregman divergence, the GM loss Lgm has the same gradients as the CGM loss Lcgm,\ni.e. VoLgm(0) = \u22070Lcgm(0). Therefore, minimizing the CGM loss with Stochastic Gradient Descent will\nalso minimize the GM loss. Further, for this property to hold, D must necessarily be a Bregman divergence.\nNote the significance of proposition 2: we can learn Lt without having ever access to it with a scalable\nobjective. Further, we can universally characterize the space of loss functions. The proof can be found in\napp. C.4. In table 1, we list examples of several CGM loss functions. Often it is also possible to derive losses\nthat give upper bounds on the model log-likelihood (ELBO bounds). We illustrate this in app. D.\nPrinciple 4: Train L\u0165 by minimizing the CGM loss with a Bregman divergence.\nWith this, we arrived at the last principle of GM. In alg. 1, we summarize the generator matching recipe for\nconstructing generative models."}, {"title": "APPLICATIONS OF GENERATIVE MATCHING THEORY", "content": "GM provides a unifying framework for many existing generative models (see sec. 8), as well as gives rise to\nnew models. Beyond that, the generality of GM in itself has several use cases that we discuss in this section."}, {"title": "COMBINING MODELS", "content": "The generator is a linear operator and the KFE dt[pt \u00a9 f] = Pt [Ltf] is a linear equation. These two\nproperties enable us to combine generative models for the same state space S in different ways.\nProposition 3 (Combining models). Let pt be a marginal probability path, then the following generators\nsolve the KFE for pt and consequently define a generative model with pt as marginal:\n1. Markov superposition: a\u0142 Lt + a L't, where Lt, L' are two generators of Markov processes solving\nthe KFE for pt, and a\u0142, a\u00b2 \u2265 0 satisfy a + a = 1. We call this a Markov superposition.\n2. Divergence-free components: Lt + BtLiv, where Liv is a generator such that pt \u2299 [L\u00a2iv f] = 0\nfor all f \u2208 T, and \u1e9et \u2265 0. We call such Liv divergence-free.\n3. Predictor-corrector: a\u2021Lt + a7Lt, where Lt is a generator solving the KFE for pt in forward-time\nand Lt is a generator solving the KFE in backward time, and a\u0142, a\u00b2 \u2265 0 with a\u0142 \u2013 \u03b1\u00b2 = 1.\nA proof can be found in app. C.5. Markov superpositions can be used to combine generative models of\ndifferent classes, e.g., one could combine a flow and a jump model. These can be 2 networks trained separately\nor we can train two models in one network simultaneously. We illustrate Markov superpositions in fig. 2. To\nfind divergence-free components, one can use existing Markov-Chain Monte-Carlo (MCMC) algorithms\nsuch as Hamiltonian Monte Carlo, Langevin dynamics, or approaches based on detailed balance - all of these\nalgorithms are general recipes to find divergence-free components."}, {"title": "MULTIMODAL AND HIGH-DIMENSIONAL GENERATIVE MODELING", "content": "GM allows us to easily combine generative models from two state spaces S1, S2 into the product space\nS1 S2 in a rigorous, principled, and simple manner. This has two advantages: (1) we can design a joint\nmulti-modal generative model easily and (2) we can often reduce solving the KFE in high dimensions to the\none-dimensional case. We state here the construction informally and provide a rigorous treatment in app. C.6.\nProposition 4 (Multimodal generative models - Informal version). Let q\u2021 (\u00b7|z1), q\u2021 (\u00b7|z2) be two conditional\nprobability paths on state spaces S1, S2. Define the conditional factorized path on S1 \u00d7 S2 as pt(\u00b7|21, 22) =\nq+ (z1)q(22). Let pt(dx) be its marginal path.\n1. Conditional generator: To find a solution to the KFE for the conditional factorized path, we only\nhave to find solutions to the KFE for each S1, S2. We can combine them component-wise.\n2. Marginal generator: The marginal generator of pt(dx) can be parameterized as follows: (1)\nparameterize a generator on each Si but make it values depend on all dimensions; (2) During\nsampling, update each component independently as one would do for each Si in the unimodal case.\n3. Loss function: We can simply take the sum of loss functions for each Si.\nAs a concrete example, let us consider joint image-text generation with a joint flow and discrete Markov\nmodel with S\u2081 = Rd, S2 = {1, . . ., N}. To build a multimodal model, we can simply make the vector field\nut(x1, x2) \u2208 Rd depend on both modalities x1, x2 but update the flow part via X++h = X + hut(X1, X2).\nSimilarly, the text updates depend on both (X1, X2). In app. F, we give another example for jump models."}, {"title": "RELATED WORK", "content": "GM unifies a diversity of previous generative modeling approaches. We discuss here a selection for S = Rd\nand S discrete. App. H includes an detailed discussion and models for other S (e.g. manifolds, multimodal)."}, {"title": "EXPERIMENTS", "content": "The design space of the GM framework is extraordinarily large. At the same time, single classes of models\n(e.g., diffusion and flows) have already been optimized over many previous works. Therefore, we choose to\nfocus on 3 aspects of GM: (1) The ability to design models for arbitrary state spaces (2) Jump models as a\nnovel class of models (3) The ability of combining different model classes into a single generative model."}, {"title": "DISCUSSION", "content": "We introduced generator matching, a general framework for scalable generative modeling on arbitrary state\nspaces via Markov generators. The generator abstraction offers key insights into the fundamental equations\ngoverning Markov generative models: Generators are linear operators, the KFE is a linear equation, and\nmany losses (Bregman divergences) are linear in the training target. Therefore, any minimization we do\nconditionally on a data point, implicitly minimizes the training target marginalized across a distribution of\ndata points. These principles allowed us to unify a diversity prior generative modeling methods such as\ndiffusion models, flow matching, or discrete diffusion models. Further, we could universally characterize\nthe space of Markov models and loss functions. GM allows us to combine generative models of different\nclasses on the same state space (Markov superpositions) and how to build multimodal generative models from\nunimodal components. Future work could further explore the design space of GM. For example, we showed\nhow one can learn a diffusion coefficient \u03c3\u03b5 of a diffusion model. In addition, jump models on Euclidean\nspace offer a large class of models that we could only study here in its simple instances. In addition, future\nwork can explore better samplers or distillation to minimize computational cost during sampling. To conclude,\ngenerator matching provides both a rigorous theoretical foundation and opens up a large practical design\nspace to advance generative modeling across a diverse range of applications."}, {"title": "OVERVIEW OF MARKOV PROCESSES AND THEIR GENERATORS", "content": "SETUP AND DEFINITIONS\nState space. Throughout this work, we assume (S, d) is a Polish metric space, i.e., S is a set and there is a\nmetric d : S \u00d7 S \u2192 R>0 defined on S such that (S, d) is complete (i.e., any Cauchy sequence converges)\nand separable (i.e., it has a countable dense subset). We endow S with its Borel \u03c3-algebra B(S) and consider\na set ACS as measurable if A \u2208 B(S). Any function f : S \u2192 R considered in this work is assumed to\nbe measurable. Throughout this work, we assume that T is a set of functions f : S \u2192 R on S such two\nprobability measures \u03bc1, \u03bc2 are equal if and only if Ex~\u00b5\u2081 [f(Xt)] = Ex~\u00b52 [f(x)] for all f \u2208 T. We call\nelements in T test functions.\nMarkov process. Let (\u03a9, F, P) be a probability space. A Markov process (Xt)0<t1 is a collection of\nintegrable random variables X\u2081 : \u03a9 \u2192 S such that\nMarkov assumption: P[Xtn+1 \u2208 A|Xt1, Xt2,..., Xtn] = P[Xtn+1 \u2208 A|Xtn]\nfor all 0 < t1 < t2 <\u2026\u2026 < tn < tn+1 \u2264 1, A C S measurable\nWe denote by kt+h\\t(A|x) = E[Xt+h \u2208 A|X\u2081 = x] the transition kernel of Xt.\nSemigroup. We define the action of marginals pt and of transition kernels kt+h|t on test functions f as in\nthe main paper via:\nPt f =\n\u222b\nf(y)pt(dy) = E [f(X+)]\n[kt+ht f] (x) =\n\u222b\nf(y)kt+h\\t(dy|x) = E [f(Xt+h)|Xt = x]\nwhere the marginal action maps each test function f to a scalar pt f \u2208 R, while transition action maps a\nreal-valued function x \u2192 f(x) to a another real-valued function x \u2192 [kt+h\\t \u00a9 f](x). The tower property\nimplies that pt [kt+h\\t \u00a9 f] = Pt+h f. Considering kt+h|t as a linear operator as above, we know by the\nMarkov assumption and the tower property that there are two fundamental properties that hold:\nkt\\t =Id, Psu = Pt/uPs\\t,\n||Ps|tf||\u221e\u2264||f||\u221e, 0 \u2264 t < s < 1\nwhere || || describes the supremum norm."}, {"title": "DEFINITIONS FOR TIME-HOMOGENEOUS MARKOV PROCESSES", "content": "A complication in the theory we develop here is that we (need to) consider time-inhomogeneous Markov\nprocesses, while most of the mathematical theory and literature resolves around time-homogeneous Markov\nprocesses. Therefore, we first give the definitions for time-homogeneous Markov processes and then explain\nhow this can be translated to the time-homogenous case.\nA time-homogeneous Markov process is a Markov process Xt such that kt+h\\t = kh|0 for all t, h > 0 - i.e.\nthe evolution is constant in time. This implies the semigroup property\nkt|o = Id, ks|0 \u00a9 kt|0 = ks+t|0\nLet Co(S) be the space of continuous functions f: S \u2192 R that vanish at infinity, i.e. for all \u20ac > 0 there\nexists a compact set KCS such that |f(x)| < \u20ac for all x \u2208 S \\ K."}, {"title": "Feller process.", "content": "We call Xt a Feller process if it holds that\n1. Operators on continuous functions: For any f \u2208 Co(S) and t \u2265 0", "continuity": "For any f \u2208 Co(S):\nlim ||kt|off||\u221e = 0\nt10\nGenerator. We define the generator L of a Feller process Xt as follows: for any f \u2208 Co(S) such that the\nlimit\nkt of - f\nlim"}]}