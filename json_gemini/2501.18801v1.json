{"title": "Every Image Listens, Every Image Dances: Music-Driven Image Animation", "authors": ["Zhikang Dong", "Weituo Hao", "Ju-Chiang Wang", "Peng Zhang", "Pawel Polak"], "abstract": "Image animation has become a promising area in multi-modal research, with a focus on generating videos from reference images. While prior work has largely emphasized generic video generation guided by text, music-driven dance video generation remains underexplored. In this paper, we introduce MuseDance, an innovative end-to-end model that animates reference images using both music and text inputs. This dual input enables MuseDance to generate personalized videos that follow text descriptions and synchronize character movements with the music. Unlike existing approaches, MuseDance eliminates the need for complex motion guidance inputs, such as pose or depth sequences, making flexible and creative video generation accessible to users of all expertise levels. To advance research in this field, we present a new multimodal dataset comprising 2,904 dance videos with corresponding background music and text descriptions. Our approach leverages diffusion-based methods to achieve robust generalization, precise control, and temporal consistency, setting a new baseline for the music-driven image animation task.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence has made significant strides across various domains recently [8, 15\u201318, 34, 37, 40, 49, 50, 68]. In particular, image animation have advanced through the use of various of guidances, such as motion [43, 44, 67], text [12]. These methods animate static images by applying motion to main characters within them. However, the area of music-guided image animation remains relatively underexplored. This task involves animating characters to dance in sync with music, ensuring the video aligns with the musical theme and follows the beats. For example, fast-paced pop music tracks suit street dance, while calm piano music complements classical waltz performances. Additionally, we aim to enhance customization by incorporating user-provided text. Even with the same appearance and background, users can express different dance motions guided by the music, creating personalized content that significantly enhances user engagement and amplifies the reach of the music. This approach has huge potential for widespread applications in industries like film, social media, and education, where users frequently create videos choreographed to popular music.\nSeveral challenges in image animation, particularly in dance video generation, remain unresolved. The primary issues are: (1) the scarcity of large-scale datasets containing dance videos with a variety of subjects-not limited to human dancers-synchronized with background music and accompanied by corresponding text captions; (2) the lack of flexible control over the generation process; and (3) limited generalization beyond the training dataset's scope. Current motion transfer models rely on motion guidance inputs such as pose, video, depth, and skeleton-based sequences to animate reference images. However, these methods often fail to meet user preferences. For instance, finding suitable dance poses for classic pieces by Beethoven or Mozart can be challenging. The availability of such guidance is limited and often requires specialized domain knowledge to create, which most users lack. Additionally, existing dance motion transfer models focus predominantly on human motion, as the guidance is generally designed for human figures, thereby restricting creativity. In contrast, any object-such as animated characters in Disney movies\u2014could potentially \"dance\". To address these limitations, we propose an end-to-end music-guided dance generation model that animates the reference image using only music and text. This approach enables users, regardless of expertise, to create diverse dance videos featuring not only humans but also a wide range of objects.\nIn this paper, we introduce MuseDance, a flexible, end-to-end multimodal image animation framework that generates dance videos from a static reference image, a music piece, and a descriptive text prompt of the desired motions. As shown in Figure 1, MuseDance allows the figure in the reference image to dance rhythmically to the music with specified movements, and the video can extend seamlessly based on prior frames. Built on a diffusion-based model,"}, {"title": "2. Related Work", "content": "Video Generation Diffusion Model. Video generation is a very important task in AIGC. Methods like variational RNNs [2, 9, 14, 31] and GANS [33, 36, 42, 47, 55, 56, 64] have been explored to tackle this problem. However, most of those works are limited to low-resolution, the lack of large scale high-quality datasets or loose control ability. Diffusion models are proposed to solve this problem. [5] introduces temporal dimension to the latent diffusion image generation model. Make-A-Video [45] enhances DALL-E2 [39], a text-to-image model, by using joint text-image priors and super-resolution strategies to produce high-quality videos. Stable Video Diffusion [4] presents a large-scale text-to-video foundation model, which also supports various downstream tasks like image-to-video generation, camera motion adaptability and multi-view objects synthesis. In addition to open source models, closed-source video generative models, in particular GEN-2 [20], PikaLabs [30], Sora [7] and Kling [29] provide state-of-the-art video generation capabilities for general use.\nMusic-guided Dance Generation. Music-guided dance generation in 3D sequences has been explored in recent works. Bailando [46] proposes a pose Q-VAE with a motion GPT to predict future pose tokens given music. EDGE [51] presents a physics-constrained transformer-based diffusion to generate more realistic 3D dance sequences. M2C [35] introduces a music code extractor to replace existing music feature processor to enhance music's role in 3D dance motion generation. LM2D [63] integrates lyrics information to enable the generation of more diverse 3D dance sequences. 3D dance sequence generation models produce only skeleton keypoints, rather than full dance videos, limiting their practical application. Music-guided dance generation in 2D videos is still largely unexplored. [58] utilizes a diffusion model to generate optical flow, which is then combined with a reference image for animation. However, their approach operates at low resolution and has not been tested on specific, large-scale datasets, leading to lower diversity in 2D dance video generation. Our work aims to address these limitations.\nHuman Motion Transfer. Earlier works [3, 6, 13, 19, 60] on human body motion transfer demonstrate lower accuracy and require significant human intervention. Recently, deep learning techniques have enabled more realistic motion transfer with highly automated training pipelines. MoCo-GAN [53] introduces an unsupervised adversarial training method for transferring motion and facial expressions onto target subjects. [1] extends the StyleGAN [28] generator to learn the warped local features. [10] utilizes a video-to-video synthesis method to generate new motions by giving a 2D video and 2D skeleton sequences. Dreampose [27] proposes a diffusion model to animate a reference human image using a target pose sequence and fabric textures. [25] utilizes a lightweight pose guider to enable controllable continuous character movement across various downstream tasks.\nIn human dance transfer domain, DISCO [57] generates human dance videos from dance skeleton sequences and a reference human image. Their method generalizes to unseen human references, backgrounds, and poses. MagicAnimate [62] combines a video encoder with an appearance encoder to generate temporally consistent dance videos from a reference image. MagicPose [11] incorporates facial keypoints with body skeletons as guidance to generate realistic human dance videos. However, these approaches still require pose guidance to generate dance videos, and such pose sequences are not always available, limiting the generalization capability of these models. Our approach aims to address this limitation."}, {"title": "3. Method", "content": "We propose a two-step training framework to animate images in dancing based on music and text input. In the first step, the model is trained on individual frames from the target video to learn visual features and acquire prior knowledge. In the second step, we introduce music and text as triggers to generate animated frames that align with these inputs. The process is illustrated below.\n3.1. Preliminaries\nLatent Diffusion Models denote a class of diffusion models that operate within the encoded latent space produced by an autoencoder, represented as $D(E(\\cdot))$. One of the most widely used models in this category is Stable Diffusion [40], which incorporates a VQ-VAE and a time-conditioned U-Net architecture. Additionally, Stable Diffusion utilizes a text encoder from the CLIP [38] model to encode text prompts into embeddings. Given an image $I \\in \\mathbb{R}^{H_1 \\times W_1 \\times 3}$ and its corresponding text embedding $C_{text} \\in \\mathbb{R}^{D_e}$, we obtain the latent representation $z_0 = E(I) \\in \\mathbb{R}^{H_z \\times W_z \\times D_z}$ and apply it to a predefined diffusion process across $T$ timesteps, modeled as a Gaussian process. This process approximates a standard Gaussian distribution, $z_t \\sim N(0, I)$. The training objective in Stable Diffusion is to iteratively denoise $z_t$ back to the original latent representation $z_0$.\nDuring inference, the original latent $z_t$ is reconstructed using sampling methods, such as denoising diffusion implicit models [48]. Then, the latent $z_t$ is decoded by the decoder $D$ to generate the final, clear image. Latent Diffusion Models can produce high-fidelity images and align the generated images with the text-conditioned prompt.\nCross Attention Mechanism is a key component in the U-Net of latent diffusion models. This mechanism integrates information from the latent representation and the conditioning embedding, enabling latent diffusion models to generate images that semantically align with the given condition. More generally, the conditioning modality can be text, motion flows, audio, etc., providing semantic guidance for the generation process.\nControlNet [65] is a structure designed to control the generation process without modifying the pretrained diffusion model. ControlNet duplicates the original parameters into a trainable copy while freezing the original weights, allowing the model to retain its generalization ability from large-scale image datasets while fine-tuning the trainable copy for the specific task. The two copies of the model are connected"}, {"title": "3.2. Appearance Pretraining", "content": "As shown in Figure 2, the goal of the first training stage is to generate motions similar to those in the reference image while preserving the same appearance. Specifically, we randomly select one frame as the input image and another frame within a fixed time window as the target image. Given a video with N frames, denoted as $V = {I_1, ..., I_N}$, we randomly sample a frame $I_i$ as the input image, where $i \\in (w, N-w)$, with w representing the fixed time window.\nWe then randomly select a frame $I_j$ as the target image, where $j \\in (i \u2013 w, i + w)$ and $j \\neq i$. Using the DensePose [22] mask, we obtain a dense and robust pose signal of the dancer in the reference image, denoted as $D_i$. We employ a mask encoder $F_m$, which consists of a series of convolutional layers to progressively downsample and extract mask features, yielding $m_i = F_m(D_i)$. Similar to ControlNet [65], we incorporate mask condition features in a residual manner.\n$Z_0 = E(I_i) + Conv(D_i), \\quad (1)$\nSimilar to approaches in [25, 61], we employ a ReferenceNet\u2014a U-Net-based Stable Diffusion model with the same layers as our backbone diffusion model\u2014to extract visual information from the reference image, denoted as R. Suppose we have features from the denoising U-"}, {"title": "3.3. Dynamic Trigger Video Generation", "content": "Figure 2 illustrates our second-stage training process. Here, the model learns to generate dance videos based on the reference image, music input, and text guidance. To preserve the visual generation ability from the appearance pretraining stage, we freeze the spatial attention blocks.\nTo achieve temporal alignment in driving the reference image, we add three new modules to the denoising U-Net: a music understanding module, a beat alignment module, and a motion alignment module.\nMusic Understanding Module. This module aims to extract musicality information from the music and use it to control frame generation. We use the AST [21] model to obtain the music embedding. Given the hidden states from the previous module, $z_t \\in \\mathbb{R}^{K\\times (H_zW_z) \\times d}$, where K is the number of generated frames, and the music embedding $C_{music} \\in \\mathbb{R}^{L\\times d}$, where L is the sequence length of the music embedding, we apply a cross-attention mechanism between the music embeddings and frames to facilitate information flow across these two modalities, allowing the music dynamics to control frame generation. To further improve temporal alignment, we reshape the hidden states into $z_t \\in \\mathbb{R}^{(H_zW_z) \\times K \\times d}$ and compute self-attention along the temporal dimension.\nBeat Alignment Module. We observe that, in most music dance videos, the music beat serves as a strong signal, often marking the start, stop, or change in dance style. To capture this pattern, we incorporate beat information into the denoising U-Net. We use Librosa to identify beat locations in the music, converting them into a one-hot encoded format. This produces a binary vector $b_{binary} \\in \\mathbb{R}^{K}$, where frames with a beat are assigned a value of 1, and others a value of 0. We align the beat information with video sequences, inspired by token processing in NLP tasks, and apply a lookup embedding layer to transform the discrete embedding into a continuous dense embedding $b_{dense} \\in \\mathbb{R}^{K\\times d}$. We then apply the cross-attention mechanism to help the hidden states learn the beat information. Similar to the music understanding module, we reshape the hidden states and apply temporal attention layers to ensure temporal continuity.\nMotion Alignment Module. In video generation, maintaining content continuity across frames is crucial, especially for generating coherent dance motions. In addition to the temporal attention layers in the music understanding and beat alignment modules, we employ a motion alignment module to capture temporal dependencies across frames. Inspired by [23, 61], we use several previously generated frames as guidance, concatenating them with the current hidden states and performing self-attention across the temporal sequence dimension. Specifically, we form a concatenated hidden state $Z_{motion} = concat(z_t, z_t)$, where $z_t \\in \\mathbb{R}^{(H_zW_z) \\times M \\times d}$ represents the hidden states from the previous M generated frames. By applying self-attention on $Z_{motion}$ across the temporal axis, we select the last K hidden states as the current generated frames."}, {"title": "4. Experimental Results", "content": "4.1. Music Dance Dataset\nIn this paper, we introduce the first music-dance video dataset. The raw videos are collected from YouTube, totaling 304 videos. These videos feature a diverse range of dance genres, from popular styles found on short video platforms to traditional Chinese dance. The videos also provide diversity in dancer appearance and background settings. Given that dancers often perform to the same music, some music tracks overlap across videos. Human-object videos are primarily from TikTok dance collections or dance challenge series, while non-human object videos consist of synthetic animations of animals dancing. We split each video into multiple clips. All these sub-videos are in a vertical, object-centered format, paired with background music, and vary in length from 10 to 30 seconds. We manually edit each video to remove irrelevant segments, such as intros, outros, and conversational parts, retaining only the music-driven dance sections. To ensure consistency, we include only videos with a single dancer, with plans to expand the dataset to include multiple-dancer videos in the future. Following these preprocessing steps, our dataset comprises 2,904 videos, each paired with background music and lasting approximately 4 seconds. The dataset includes a total of 454 unique music tracks. All dance videos undergo manual review to ensure that any harmful or inappropriate content is excluded.\nWe also include a text description of motion for each pair"}, {"title": "4.2. Implement Details", "content": "In our experiments, both training and inference processes are conducted on a computational platform with 32 NVIDIA A100 GPUs, each has 80 GB memory. The training framework consists of two stages, each comprising 30,000 steps. The batch size is set to 12 in the first stage and 2 in the second stage for each GPU, with video dimensions maintained at 640 \u00d7 640 pixels. During the second stage of training, each instance generates a 4-second video at a frame rate of 12 fps. The music has a sample rate of 16,000 Hz and is in mono. To ensure consistency in the generated content, the hidden states of the last two generated frames are utilized within the motion module. A learning rate of le-5 is applied across both training stages, and the Adam optimizer is employed for parameter updates. The ReferenceNet and Denoising U-Net are initialized based on stable-diffusion-v1-5, while the motion module is initialized with weights derived from Animatediff [23]. To enhance video generation quality, a dropout rate of 0.05 is applied. Additionally, we use DDIM to sample the generated frames."}, {"title": "4.3. Quantitative Results", "content": "Similar to the approach in [57], we randomly select 10 videos as the test split, including various figures, such as human and non-human ones. We evaluate the quality of our generated dancing videos using several metrics. For single-frame quality, we employ SSIM [59], LPIPS [66], and PSNR [24]. To assess overall video quality, we use Fr\u00e9chet Video Distance (FVD) [54].\nDue to the lack of open-source code and data for the"}, {"title": "4.4. Ablation Studies", "content": "To illustrate the effectiveness of each module in the second training stage, we conduct ablation studies by removing the music module, beat module, or motion module to examine their influence on the generated content and consistency across frames. For a fair comparison, we freeze the modules trained at the first stage. As shown in Table 3, we observe that adding the music, motion, and beat modules effectively enhances generation quality. Notably, the motion module significantly improves temporal consistency."}, {"title": "4.5. Qualitative Results", "content": "Non-human Object Generation. Unlike existing works, our model has the capability to generate dancing videos of non-human objects. As shown in Figure 4, our model produces realistic dancing motions for non-human objects based on the music input and tempo. We observe that regardless of whether the text description provides detailed motion instructions or just generalized guidance, the model still performs well. This demonstrates the strong language understanding ability of our model.\nText Semantic Preservation. We evaluate our model's ability to preserve semantic consistency by controlling the input text prompt. In Figure 5, we control the text guidance and use different music dynamics to drive various reference images. The results demonstrate that our model can accurately interpret the text guidance while maintaining the flexibility to incorporate the music input and generate coherent dance videos."}, {"title": "5. Potential Applications", "content": "Dynamic Content Creation in Social Media and Interactive Marketing. MuseDance's ability to animate a wide range of objects, not limited to human figures, in sync with musical and textual guidance opens up new possibilities in social media content creation and interactive marketing. Users can create engaging videos featuring personalized or branded objects that \"dance\" to trending music tracks, significantly enhancing engagement and shareability. This unique flexibility extends beyond traditional motion transfer applications and can cater to users' need for quick, dynamic content on platforms like TikTok, YouTube and Instagram.\nInteractive Virtual Environments and Gaming. MuseDance can introduce lifelike animation to various in-game assets or virtual environments, where objects or characters move rhythmically with background music, creating an immersive atmosphere. For instance, in rhythm-based games or virtual concerts, MuseDance can animate avatars, objects, or even abstract art elements in response to the music track, amplifying user immersion and engagement. Unlike existing methods that require explicit pose guidance, MuseDance's reliance on music and text input makes it highly adaptable, enabling the seamless generation of synchronized movements for any character or object within the virtual scene.\nEnhanced Music and Dance Education Tools. MuseDance's capacity to synchronize animations with diverse types of music and rhythm patterns could be harnessed in educational platforms focused on music and dance. By animating reference images based on music, the model can help students visualize how various music styles\u2014ranging from classical to jazz or pop\u2014affect dance movements. This visual and interactive aid can serve as a practical tool for students to study the influence of rhythm and melody on motion, supporting not only traditional dance education but also aiding musicians in understanding dance as a form of physical expression of music."}, {"title": "6. Limitations", "content": "Although our framework allows for the generation of videos of unlimited length, extending videos for too long can reduce frame consistency and compromise the realism of the content. As the sequence progresses, minor inconsistencies may accumulate, leading to flickering or unrealistic transitions. Additionally, while our dataset includes detailed text descriptions that capture body motions sequentially, these descriptions lack explicit temporal information. This limitation affects the model's ability to disentangle appearance and motion effectively, which may impact the temporal consistency of the generated video. In future work, we plan to expand our dataset with richer text annotations that include explicit timeline information, providing clearer temporal guidance. This enhancement could improve the model's ability to separate appearance and motion, resulting in more coherent and consistent video sequences."}, {"title": "7. Conclusions", "content": "In this work, we explore the potential of using music dynamics and text guidance to animate static images, creating an end-to-end framework for flexible dance video generation. We developed the first music-driven dance video generation dataset from publicly available YouTube videos and proposed a diffusion-based model capable of integrating visual, auditory, and textual cues. Our model not only \"sees\" the reference image but also \"listens\" to music, senses tempo and beat, and interprets text instructions to produce realistic, synchronized dance videos. Extensive experiments demonstrate the strong performance of our model on this novel multimodal task. We have also outlined potential applications and acknowledged current limitations, which we aim to address in future work. We hope this study sparks further exploration and innovation in this exciting area."}]}