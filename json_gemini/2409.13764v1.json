{"title": "Local Explanations and Self-Explanations for Assessing Faithfulness in black-box LLMs", "authors": ["Christos Fragkathoulas", "Odysseas S. Chlapanis"], "abstract": "This paper introduces a novel task to assess the faithfulness of large language models (LLMs) using local perturbations and self-explanations. Many LLMs often require additional context to answer certain questions correctly. For this purpose, we propose a new efficient alternative explainability technique, inspired by the commonly used leave-one-out approach. Using this approach, we identify the sufficient and necessary parts for the LLM to generate correct answers, serving as explanations. We propose a metric for assessing faithfulness that compares these crucial parts with the self-explanations of the model. Using the Natural Questions dataset, we validate our approach, demonstrating its effectiveness in explaining model decisions and assessing faithfulness.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) and machine learning models have become ubiquitous in various domains, ranging from healthcare to finance and beyond. Large language models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like text. However, many top-performing models are proprietary and accessible only via APIs, acting as black boxes and offering little insight into their thought processes. This opacity poses significant challenges [30], especially in applications where understanding the decision-making process of a model is critical. LLMs are also known to suffer from issues such as hallucinations [34], where they generate plausible-sounding but incorrect or nonsensical answers and the generation of overly verbose outputs that may obscure the relevant information [35]. Even when models provide a chain-of-thought explanation [29], it may not reflect their actual reasoning [25]. This underscores the urgent need for faithful explainability in AI.\nExplainability refers to the capability of explaining or describing the behavior of models in terms that are comprehensive to humans [12, 13]. Explainable AI is crucial in helping users trust and effectively utilize AI systems [11, 20], enabling developers to debug and improve models [3, 28, 33], ensuring compliance with regulatory standards by providing transparency into automated decision-making processes [1], to help in bias identification, provide causal understanding, and mitigate biases within models [14].\nBoth surveys [10, 35] review extensive research on language model explainability, including attention mechanisms, gradient-based explanations, and post-hoc techniques. Many works leverage gradients to directly link input features to model outputs, enhancing the interpretability of NLP models, while others utilize attention-based mechanisms to focus on the most relevant parts of the input data, providing insights into the decision-making process [10]. However, these methods are not applicable to commercially available LLMs where internal architectures are inaccessible. Other approaches employ perturbations on the input to observe changes in model behavior, such as Shapley-based approaches [27], which"}, {"title": "2 Problem Definition", "content": "We introduce a novel task to assess the faithfulness of the large language models (LLMs) self-explanations on question-answer (QA) benchmarks that provide a helpful context snippet. Given question, context and answer triplets $D = \\{(q_i, c_i, e_i)\\}_{i=1}^N$, where $q_i$ is a question, $c_i$ is the corresponding context, and $e_i$ is the ground truth answer, the goal is to assess how faithfully LLMs generate self-explanations (in the form of keywords taken from the context $c_i$) that align with their usefulness in generating correct answers. This involves identifying the crucial parts of the context that the model relies on to generate the answer.\nLet M be the language model. For each question-context pair $(q_i, c_i)$, the model generates a response in the form of a triplet: thought $t_i$, keywords $k_i$, and answer $a_i$. For simplicity, we ignore the thought and the keywords which are only used for visualization, and denote the response of the model as $a_i$:\n$a_i = M(q_i, c_i)$\nOur objective is twofold:\n\u2022 Identify the sufficient regions $SR_i$ set within the context $c_i$ that contain sufficient information for the model to answer correctly. Formally, let $s \\in SR_i, s \\subseteq c_i$, iff $M(q_i, s) = e_i$.\n\u2022 Within a sufficient region $s \\in SR_i$, pinpoint the necessary keywords $NK_s$ whose masking results in the model providing an incorrect answer. Formally, let $t \\in NK_s$, where $t \\subseteq s$, iff $M(q_i, s.mask(t)) \\neq e_i$. We define mask as the function that replaces the string t with the underscore '' in a string s."}, {"title": "3 Method", "content": null}, {"title": "3.1 Dataset", "content": "For this analysis, we use the Natural Questions dataset [17] which is designed to spur the development of open-domain question-answering systems and it has been used for benchmarking at QA studies like [7, 15, 18, 19, 21, 31]. Specifically, we are using the same context snippets as in [21]. It contains questions from real users and requires systems to read and comprehend segments of Wikipedia articles to find answers. A QA example of this dataset is:\n\u2022 Question: When did the watts riot start and end?\n\u2022 Long Answer: The Watts riots, sometimes referred to as the Watts Rebellion, took place in the Watts neighborhood of Los Angeles from August 11 to 16, 1965.\n\u2022 Short Answer: August 11 to 16, 1965\nIn figure 1 the long answer is the highlighted context (without the highlights) and the short answer is the same as the LLM Answer."}, {"title": "3.2 Retrieval-Hard subset", "content": "Evaluating black-box models is challenging due to their unknown pretraining corpus. Models might rely on internal knowledge rather than context, leading to unfair comparisons. To address this, we use the Retrieval-Hard subset, which includes only those samples the model fails to answer correctly without context. This framework is applicable to any dataset that has retrieved helpful context snippets and is useful for evaluating LLMs fairly in this setup."}, {"title": "3.3 QA Evaluation", "content": "Given a short answer $a_i$ from the model M for a question $q_i$ and a specified context $c_i$, our goal is to evaluate the correctness of the short answer of the model. Unlike previous work [7, 15], who are using only the exact-match accuracy metric for evaluation, we have designed a hybrid metric that combines the results of exact-match, normalized exact-match, fuzzy exact-match, model-based embedding cosine similarity and date transformations. Exact match often fails due to natural language variability, such as different formatting of names or dates. Our hybrid metric addresses these issues by capturing semantic equivalence and format variations, offering a more robust evaluation of the answers of the model. The mathematical formulation of these metrics is as follows:\n$ExactMatch(e_i, a_i) = [e_i = a_i]$\n$NormExactMatch(e_i, a_i) = ExactMatch(norm(e_i), norm(a_i))$\n$FuzzyExactMatch(e_i, a_i) = [fuzzyMatch(e_i, a_i) \\geq 90]$\n$EmbedSimilarity(e_i, a_i) = [cosSim(embed(e_i), embed(a_i)) \\geq 0.9]$\n$DateMatch(e_i, a_i) = ExactMatch(normDate(e_i), normDate(a_i))$\nThus, the hybrid metric encompasses all previous challenges:\n$evaluate(e_i, a_i) = ExactMatch(e_i, a_i) \\lor ((NormExactMatch(e_i, a_i) \\\n\\lor FuzzyExactMatch(e_i, a_i) \\lor EmbedSimilarity(e_i, a_i)) \\\nDateMatch(e_i, a_i))$\nwhere [] stands for the Inverson bracket, norm transforms text into a standard format (e.g., removing punctuation, lowercasing), fuzzyMatch computes a similarity score between answers based on edit distance, cosSim stands for cosine similarity, embed retrieves an embedding representation from a pre-trained sentence transformer\u00b9, and normDate converts dates into a standard format for comparison."}, {"title": "3.4 Prompting", "content": "Large Language Models (LLMs) use the concept of prompting to tailor responses according to specific formats and requirements. This involves providing the model with structured input that guides it to produce desired outputs. By illustrating the task, expected behavior, and desired answer format through a few input-output examples, LLMs can excel in various straightforward question-answering tasks [6]. In our case, the desired response is a chain-of-thought explanation and a few exact words from the text, which we call keywords, and are considered crucial words by the model. To achieve this, we define a structured dialogue framework process for interacting with the model, as:\n\u2022 System Message: \"To answer the given question, first generate a thought that explains the answer according to the text,\nthen identify the most important words (keywords) from the\ntext that helped you with your thought, and finally provide\na short answer.\"\n\u2022 User Message: \"The following text might be useful in answering the question: [context] Question: [query]\"\n\u2022 Assistant Message: \"Thought: [thought process], Keywords:\n[keywords], Short answer: [answer]\""}, {"title": "3.5 Explainability Algorithm", "content": "Our algorithm extends the Leave One Out (LOO) method [24], a powerful baseline in previous work ([24]). It follows a two-step process: first, identifying the sufficient regions within the context, and second, detecting necessary keywords in these regions. A key advantage of our approach is its constant complexity concerning the number of samples and model queries, which significantly reduces costs when using proprietary models."}, {"title": "3.5.1 Sufficient Regions algorithm", "content": "In order to identify the sufficient regions, $SR_i$, of the context we split it into p equal parts which are candidate regions, $CR_i$. We treat p as a hyperparameter (we selected p=3 for our experiments). Then we generate an answer for each candidate region $s \\in CR_i$; and if it is correct then the corresponding region s is considered sufficient and is then added to the Sufficient Regions set: $SR_i$. The details of the algorithm can be seen in Algorithm 1."}, {"title": "3.5.2 Necessary Keywords algorithm", "content": "The sufficient regions are usually sentence-level explanations (depending on the length of the input). To detect phrase-level explanations we adopt a slightly modified version of LOO and we call the results necessary keywords. We apply this algorithm on every sufficient region s of $SR_i$. Instead of masking a single word or a predetermined number of words as in traditional LOO approaches, we mask q groups with equal number of words each (q=5 in our experiments), as in Sufficient Regions. This way, the total number of API calls to the LLM is going to be 1 + pxq (p=3 for SR, q=5 for NK, and one call to get the self-explanation keywords, 16 in total for our experiments), independently of the length of the input. For each group of words, we replace it with an underscore'', a process we call masking. We use the masked region s to generate an answer and if it is wrong we add this group to the Necessary Keywords set: $NK(s)_i$. The details of the algorithm can be seen in Algorithm 2."}, {"title": "3.6 Faithfulness Evaluation", "content": "To quantify the faithfulness of the response of the model, we compare the keywords K provided by the model with the sufficient regions $SR_i$ and the necessary keywords $NK_j$ identified with our proposed explainability algorithm 3.5. We define the faithfulness score $f_i$ for each question-context-answer triplet as follows:\n$f_i = \\underset{s\\in SR_{C_i}}{max} \\frac{(f_{SR}(s) + f_{NK}(s))}{2}$\nwhere $f_{SR}(s), f_{NK}(s)$ is the faithfulness score based on the sufficient regions and the necessary keywords respectively, defined as:\n$f_{SR}(s) = {1 | \\exists k \\in K such that k\\subseteq s, otherwise 0}$\n$f_{NK}(s) = \\frac{1}{NK_{s}} \\sum_{t \\in NK_{S}} g(t)$\nwhere $g(t) = {1 | \\exists k \\in K such that k\\subseteq t, otherwise 0}$.\nThe overall faithfulness score F for the dataset D is then given by the average faithfulness score over all question-context pairs:\n$F = \\frac{1}{N} \\sum_{i=1}^N f_i$"}, {"title": "3.7 Visualization", "content": "We color sufficient regions in green, necessary keywords in blue, and highlight model-generated keywords in bold. Figure 1 illustrates this method, showing how the self-explanations of the model align or not with the important context regions, along with the faithfulness score."}, {"title": "4 Experiments", "content": "We have performed preliminary experiments which are still in progress. The OpenAI API 2 was used to implement this framework, configuring interactions based on 3.4. Currently, we have used 790 samples, only 311 of them are in the Retrieval-Hard subset 3.2 (GPT-40 failed to answer without external context). We evaluate GPT-3.5 on the same subset. We produced explanations with our multi-step approach and the success rate for each step can be seen in Table 1. An explanation might fail for three reasons: the model answered incorrectly even when the original context was given to it (row 2), no sufficient regions were identified (row 3), or no necessary keywords were found (row 4). Our proposed explainability method is successful 100 out of 224 times and 149 out of 227 times respectively for GPT-3.5 and GPT-40. There is a trade-off between API-calls/cost and explainability success. Our hyperparameter (p = 3,q = 5) results in 16 API calls per sample, achieving a 45% explainability success rate on average. This rate can be improved by adjusting p, q. For a fair comparison, we evaluate the faithfulness of GPT-3.5 and GPT-40 on the common subset of successfully explained samples only. The total common successful samples are 62. Preliminary results indicate that GPT-40 shows higher faithfulness than GPT-3.5, aligning more accurately with key context regions and keywords. This suggests advancements in model training and algorithmic refinement in newer LLM versions."}, {"title": "5 Related Work", "content": "Our focus is on local post-hoc explanations for LLMs, given that textual output is the sole result. LLMs can provide themselves explanations in line with subsequent outputs, referred to as chain-of-though [29], and can perform in-context few-shot learning by using prompts, where users illustrate the task using a few input-output examples with great success in various straightforward QA tasks [6].\nMosca et al. [23] highlight the scarcity of studies on perturbation-based explanations for text inputs. Ribeiro et al. [26] generate perturbed variations of the context to train an interpretable model that mimics the local predictions of the black-box model. In NLP tasks, traditional Shapley values oversimplify feature impact by ignoring contextual interactions among words. Instead, relevance assessments should extend to multi-level tokens or entire sentences [23]. Chen et al. [9] introduced L-Shapley and C-Shapley for better interpretation: L-Shapley examines local interactions through neighboring feature perturbation, while C-Shapley assesses multi-level tokens and full sentences.\nHierarchical Explanation via Divisive Generation (HEDGE) [8] exemplifies a SHAP-based method addressing the challenge of lengthy texts. HEDGE sequentially breaks down text into shorter phrases and words based on their weakest interactions, assigning relevance scores at each level to achieve a hierarchical explanation. Similarly, PartitionSHAP 3, adopts a comparable approach by forming hierarchical coalitions of features and evaluating their interactions. Also, CaptumLIME, a modified version of LIME [26] adapted for text generation tasks using features from the Captum library [22], allows users to define units for attribution within the input manually. It addresses sequence outputs by computing log probabilities for tokens in the output and summing them.\nPaes et al. [24] extend perturbation-based methods to handle text outputs and long inputs, using scalarizers to map text outputs to real numbers for input importance assessments. They use user studies and BARTScore [32] to measure the likelihood of a reference text conditioned on the generated text [32]. Ribeiro et al. [26] examine faithfulness by comparing the features the model claims to rely on with those identified by an explanation technique, using a restricted set of gold features. Schnake et al. [2] observe that a higher area under the activation curve indicates more faithful explanations."}, {"title": "6 Summary and Future Work", "content": "We introduce a novel approach for assessing LLM faithfulness using local and self-explanations. Inspired by the leave-one-out technique, our approach identifies essential and sufficient parts of the context affecting model answers. We propose a metric for evaluating faithfulness by comparing these parts with the model's self-explanations. Experiments with the Natural Questions dataset demonstrate the approach's effectiveness. Future work will involve testing on broader QA datasets [4, 5, 16], analyzing the trade-off between explanation success rate and API calls, and comparing our method with existing baselines."}]}