{"title": "WDMOE: Wireless Distributed Mixture of Experts for Large Language Models", "authors": ["Nan Xue", "Yaping Sun", "Zhiyong Chen", "Meixia Tao", "Xiaodong Xu", "Liang Qian", "Shuguang Cui", "Wenjun Zhang", "Ping Zhang"], "abstract": "Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, but the role of wireless networks in supporting LLMs has not been thoroughly explored. In this paper, we propose a wireless distributed Mixture of Experts (WDMoE) architecture to enable collaborative deployment of LLMs across edge servers at the base station (BS) and mobile devices in wireless networks. Specifically, we decompose the MoE layer in LLMs by placing the gating network and the preceding neural network layer at BS, while distributing the expert networks among the devices. This deployment leverages the parallel inference capabilities of expert networks on mobile devices, effectively utilizing the limited computing and caching resources of these devices. Accordingly, we develop a performance metric for WDMoE-based LLMs, which accounts for both model capability and latency. To minimize the latency while maintaining accuracy, we jointly optimize expert selection and bandwidth allocation based on the performance metric. Moreover, we build a hardware testbed using NVIDIA Jetson kits to validate the effectiveness of WDMoE. Both theoretical simulations and practical hardware experiments demonstrate that the proposed method can significantly reduce the latency without compromising LLM performance.", "sections": [{"title": "I. INTRODUCTION", "content": "The exciting advancements in large language models (LLMs) have sparked a new wave of AI innovation. LLMs, exemplified by ChatGPT [2], have demonstrated emergent abilities [3], including better generalization, nuanced meaning comprehension, and remarkable reasoning and generation capabilities. These advancements have led to widespread applications across various fields, illuminating the vision of artificial general intelligence (AGI) [4]. In the field of 6G wireless networks, LLMs have been used for wireless network resource allocation [5]\u2013[7], and applied in internet of vehicles [8] and immersive communications [9].\nThe emergent abilities of LLMs stem from extensive computation, a large number of model parameters, and massive training datasets [3], [10], [11]. The vast number of model parameters poses significant challenges for training, inference, and deployment. The training phase of LLMs involves significant costs in time and computational power for most individuals and organizations. Regarding LLMs inference and deployment, they also require fast responses and ample memory. In this paper, we mainly focus on LLMs inference and deployment.\nCurrently, LLMs can be classified into cloud-based LLMS and on-device LLMs based on their deployment characteristics. Cloud servers with numerous graphics processing units (GPUs) and sufficient power supply are responsible for the majority of model inference and deployment. Due to concerns over latency and data privacy, the potential of on-device LLMs is gaining increasing attention [12]. Researchers compress LLMs through pruning [13], quantization [14], and distillation [15] to meet the memory, computation, and energy requirements of mobile devices. Limited by generation speed and model capabilities, even a company as strong as Apple has not been able to deploy a fully satisfactory LLM on mobile devices. On the latest iPhone 16 Pro series, only simple tasks are completed locally by a model with around 3 billion parameters, whereas complex tasks are still handled by cloud-based models like ChatGPT [16], [17]. Although in practical LLMs application transformer's KV cache [18] can speed up the inference, it will cause considerable memory overhead, which presents an obstacle for on-device LLMs.\nIn light of the rapid advancements of LLMs and the widespread adoption of 5G/6G wireless networks, a natural question arises: can wireless networks support LLMs? If so, how? The answer lies in fully leveraging the multidimensional resources of wireless networks, incorporating computing, communications, and caching (3C) to support LLMs and enhance user experience [19]. As a key technology of 5G, mobile edge computing (MEC) has been thoroughly studied to improve the quality of service for various network applications. Recently, edge-cloud collaborative training and fine-tuning are also researched [20], [21]. However, there is a lack of specialized optimization research tailored to the characteristics of LLMs in the scenario of distributed deployment in wireless networks. To address this issue, this paper aims to bridge the gap by proposing a distributed deployment of LLMs powered"}, {"title": "II. THE PROPOSED WDMOE BASED LLM", "content": "The network structure of MoE-based LLMs is depicted in Fig. 1(a), where each MoE block is based on a Transformer. In these blocks, the FFN module is replaced by an MoE layer [31]. An MoE layer consists of a gating network, also known as a router, and multiple expert networks, each of which can be any type of neural network. The gating network, which is a simple neural network, processes the input token to produce a weight vector for each expert."}, {"title": "III. PERFORMANCE METRICS AND PROBLEM FORMULATION", "content": "The output tensor retains the same shape as the input tensor, indicating that the data transmission size for the uplink is equivalent to that for the downlink. The communication latency for the j-th token in the i-th block, processed by expert k, is defined as:\n\\(t_{i,j,k}^{comm} = \\frac{L^{comm}}{R_k^d} + \\frac{L^{comm}}{R_k^u}\\)\n(6)\nThe computation latency for the j-th token in the i-th block processed by the expert k is given by:\n\\(t_{i,j,k}^{comp} = \\frac{L^{comp}}{C_k}\\),\n(7)\nwhere Ck denotes the computational capacity of expert k.\nThe total delay of the j-th token in the i-th block processed by the expert k is given by:\n\\(t_{i,j,k} = t_{i,j,k}^{comm} + t_{i,j,k}^{comp}\\) Vi, j, k                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  (8)"}, {"title": "A. Attention Waiting Latency", "content": "Once each expert completes processing a token, the result is transmitted back to the BS. The BS aggregates the results, after which the tokens are forwarded to the next block. The first operation in the subsequent block is the self-attention mechanism. To more accurately model the latency experienced by users, we account for the impact of token latency on"}, {"title": "B. Distributed Deployment of WDMOE", "content": "Experts operate in parallel and do not impact each other, making this approach suitable for deployment in distributed mobile edge networks. We consider a BS equipped with an edge server that possess powerful computing capability, to process multiple data streams simultaneously for n mobile devices, as shown in Fig. 1(b). In the WDMoE architecture, the attention mechanism and gating network are deployed at the BS, while only expert networks are assigned to mobile devices. Typically, the expert network consists of a simple multilayer perceptron (MLP). When a user sends a prompt, its embedding operation can be completed either locally or at BS, depending on the choice of the user. In this paper, we focus on the communications and computing costs incurred during interactions between BS and mobile devices following the first embedding module. The proposed WDMoE can be integrated with existing splitting methods.\nThe mobile devices are connected to the BS via wireless links. We denote the set of mobile devices as U \u2261 {1,\u2026\u2026,U}. Let Bk \u2208 [0, B] denote the bandwidth allocated to the k-th device. For the downlink transmission, the data transmission rate from the BS to the k-th device is formulated as:\n\\(R_k^d = B_k \\log_2 \\left(1 + \\frac{P_{BS,k} g_{BS,k}}{N_0 B_k}\\right)\\) Vk \u2208 U,\n(2)\nwhere Pd denotes the transmission power for device k, gBS,k represents the channel gain from the BS to the k-th device, and No is the noise power spectral density. The uplink transmission rate from the k-th device to the BS is:\n\\(R_k^u = B_k \\log_2 \\left(1 + \\frac{P_k g_{k,BS}}{N_0 B_k}\\right)\\), Vk \u2208 U,\n(3)\nwhere Pu denotes the transmission power of the k-th device, and gk,BS represents the channel gain from the k-th device to the BS. For WDMOE, the token embeddings are transmitted between the BS and the devices. The size of each token embedding is denoted by m. The data size of a token embedding, represented as Lcomm, can be calculated as follows:\n\\(L^{comm} = \\epsilon \\times m\\),\n(4)\nwhere \u20ac is a coefficient determined by the quantization precision. For instance, a half-precision floating-point occupies 16 bits [38] and thus \u20ac = 16."}, {"title": "C. Weight-to-Latency Ratio", "content": "The attention waiting latency provides a more comprehensive and precise reflection of the latency experienced by users. For any device involved in the network, it is crucial to prioritize the overall benefits derived from processing the assigned tokens. Thus, we introduce a weight-to-latency ratio from the perspective of the processing device to quantify its processing efficiency. The WLR for device k in the i-th block is defined as\n\\(WLR_{i,k} = \\frac{\\sum_{j=1}^J q_{j,k} w_{j,k}}{\\tilde{t}_k}\\),\n(12)"}, {"title": "D. Problem Formulation", "content": "We formulate a bilevel optimization problem [40] to minimize the total system latency while ensuring comprehensive system performance. For the bilevel optimization, the upper level problem seeks to minimize total latency by optimizing the bandwidth allocation {Bk} and the expert selection matrix Q, whereas the lower level problem aims to maximize device profit by optimizing the expert selection matrix Q. The upper-level problem is formulated as:\n\\( \\begin{aligned} &\\min_{B, Q} \\sum_{i=1}^I \\tilde{t}^i \\\\ &\\text { s.t. } \\sum_{k=1}^U B_k=B, \\\\ &B_k \\geq 0, \\forall k \\in U, \\\\ &Q \\in \\Phi(Q), \\end{aligned} \\) (13)\n(14)\n(15)\nwhere \u03a6(Q) = arg maxQ P-1 2-1 WLR is the expert selection solution space at the lower level problem. The constraints (13) and (14) represent that the bandwidth allocated to each device is within the total bandwidth B.\nThe lower level problem is formulated as:\n\\( \\begin{aligned} &\\max _Q \\sum_{i=1}^I \\sum_{k=1}^U \\sum_{j=1}^J \\frac{q_{j,k} \\omega_{j,k}}{Ikt_{i,k}} \\\\ &\\text { s.t. } \\sum_k q_{j, k} \\geq 1, j \\in {1, \\ldots, J}, \\\\ &q_{j, k} \\in {0,1}. \\end{aligned} \\)\n(16)\n(17)"}, {"title": "IV. EXPERT SELECTION POLICY AND BANDWIDTH ALLOCATION", "content": "In this section, we first propose an expert selection policy to solve the lower level problem P2, based on which we optimize the bandwidth allocation to address problem P1. In particular, the expert selection policy is first designed to dynamically adjust the expert selection to improve the system's overall performance. Based on the proposed expert selection policy, the bandwidth allocation algorithm is devised to optimize the bandwidth allocation to minimize the total latency of the system."}, {"title": "A. Expert Selection Policy", "content": "Retraining a LLM to account for wireless communication conditions presents significant challenges, particularly in enabling the model to recognize and adapt to varying channel conditions in distributed environments. Retraining the gating network of an MoE layer is generally not advisable, as it is both time-consuming and computationally demanding."}, {"title": "B. Bandwidth Allocation", "content": "Given the expert selection Q, the upper level optimization objective function is reorganized as\n\\(t^{i}(B) = max_{k \\in U} \\{\\frac{\\sum_{j=1}^J q_{j,k}}{\\log_2 (1 + \\frac{P g_{BS,k}}{N_0 B_k})} + \\frac{L^{comm}}{C_k} +\\frac{\\sum_{j=1}^J q_{j,k}}{\\log_2 (1 + \\frac{P g_{k, BS}}{N_0 B_k})} + \\frac{L^{comp}}{C_k} \\}\\)\n(19)\nand Problem P1 is equivalent to\nmin 2 + Lcomp +Lcomm +\n\\( \\begin{aligned} &\\text { min } \\sum_{i=1}^I \\tilde{t}^i \\\\ &\\text { s.t. } \\sum_{k=1}^U B_k=B, \\\\ &B_k \\geq 0, \\forall k \\in U. \\end{aligned} \\)\nEq.(19) can be can be viewed as a composite form of several functions:\n\\( \\begin{aligned} &L = max_{KU} {fk(Bk)} \\\\ &fk(Bk) = \\sum {jqj, kvjk (Bk)} \\\\ &qj k (Bk) = h vj,k(Bk) +h(vj,k(Bk)) + ck+Lcomp +Lcomm+ Ck \\end{aligned} \\)\n(20)\n(21)"}, {"title": "V. SIMULATION RESULTS", "content": "In this section, we evaluate the performance of the proposed WDMOE based on numerical results obtained from experiments conducted on NVIDIA A40 GPUs."}, {"title": "A. Experiment Settings", "content": "In the simulation, we consider an MEC server deployed at BS and 8 mobile devices. The expert network q in each MoE layer is deployed on the q-th device. The distance between the q-th device and BS is denoted as dq. We consider Rayleigh fading channels with a mean PL(dq)\n10 , where the path loss is\nPL(dq)(dB) = 32.4 + 20log10(fcarrier) + 20log10(dq). The carrier frequency fcarrier is set as 3.5 GHz. The transmission power of BS and the device is 10 Watts and 0.2 Watts, respectively. The total bandwidth is 100 MHz, allocated evenly among all devices.\nWe leverage OpenCompass Platform [43] to conduct an in-depth and holistic evaluation of large language models. The benchmarks include MMLU [44], PIQA [45], ARC-Easy, ARC-Challenge [46], Humaneval [47], GSM-8K [48], BoolQ [49], MBPP [50]."}, {"title": "B. Performance Evaluation", "content": "We compare WDMoE with state-of-the-art models released during the same period, including Llama 2 with 7B, 13B and 70B parameters, as well as Mistral (referred to as Mixtral 7B-v0.1) and Mixtral (referred to as Mixtral 8x7B-Instruct-v0.1), across various benchmarks. Besides, we evaluate the latency of Mixtral and WDMoE in the wireless network. The Mixtral-based method represents distributedly deploy Mixtral and allocates bandwidth evenly.\nModel Capability. The detailed results are shown in Table I. Due to the dynamic nature of WDMoE, the total number of active parameters in the MoE remains under 13B, which is fewer than both Llama 2 13B and Mixtral in certain instances. From Table I, we observe that the WDMoE model outperforms Llama 2 70B across most benchmarks while activating at most 20% of the parameters and generally surpassing Mixtral. Specifically, WDMoE achieves the best results on the PIQA,"}, {"title": "C. Ablation Study", "content": "We perform ablations on both the upper level and lower level optimizations in the bilevel optimization problem to evaluate their contributions to reducing latency. In Fig. 7, using the change in latency with the number of tokens from the ARC-C dataset as an example, the expert selection policy achieves a 6.89% performance gain (WDMoE vs. WDMoE w./o expert selection). Meanwhile, the bandwidth allocation shows a 36.59% improvement (WDMoE vs. WDMoE w./o bandwidth allocation). The primary reason for this fact is that the upper level optimization focuses on reducing overall attention waiting latency, and its solution is specifically designed to optimize this aspect. The ablation study is conducted on all datasets used in this paper, and the results are presented in Table II. A key finding from Table II is that the expert selection yields a greater improvement when combined with bandwidth optimization compared to the Mixtral with uniform bandwidth allocation method. This suggests that the proposed expert selection is particularly effective in communication scenarios involving bandwidth allocation."}, {"title": "D. Insight for Practical Deployment of WDMoE-based LLMs", "content": "In terms of expert network with hundreds of million parameters, as the number of mobile devices in the wireless network increases, mobile devices can load more expert networks from a specific MoE layer while loading fewer expert networks from other MoE layers. In the simulation, we observe a significant phenomenon that various tokens in a text sequence are transmitted to the same experts. Fig. 8 shows the proportion of the same expert selection for distinct tokens in the first MoE layer, the sixteenth MoE layer and the thirty-second MoE layer. In the ARC-C dataset, the maximum proportion of expert selection pairs exceeds 25% in most MoE layers. Similar situations also appeared in other datasets and MoE layers. We can conclude that if we deploy expert networks of an MoE layer using an adjustable method to harness the"}, {"title": "VI. HARDWARE TESTBED FOR WDMOE-BASED LLMS", "content": "In this section, we develop a hardware testbed for WDMOE using NVIDIA Jetson Kits to demonstrate its efficiency in terms of latency and model accuracy in a practical environment."}, {"title": "A. System Components", "content": "As shown in Fig. 9(a), the hardware platform consists of two NVIDIA Jetson AGX Orin devices, an NVIDIA Jetson Xavier NX, a computer with an NVIDIA RTX 4070 Ti, a server equipped with four NVIDIA A40 GPUs, and a TP-LINK AX3000 router.\nThe router serves as a wireless access point, simulating the BS in our experiment and is responsible for receiving processed results from mobile devices and distributing new tokens to them. Devices equipped with different GPUs exhibit varying computational resources, and communication latency can be controlled by adjusting the physical placement of the devices to vary their distance from the BS.\nEach device is equipped with a subset of experts from all MoE blocks. Due to the limited number of devices, only four experts are allocated to each device for each MoE layer, while the remaining expert networks are executed on the server."}, {"title": "B. Simulated Scenarios", "content": "The simulated scenario involves the collaboration of multiple mobile devices for the deployment and inference of LLMS in communication networks, particularly in wireless networks, according to our proposed WDMoE architecture. Fig. 9(b) illustrates the geographical distribution. The length of the square is 1.45 m, and the width is 0.8 m. The experiment is conducted indoors. Fig. 9(b) shows the layout of the transmitter and mobile devices. The transmitter is centrally located, as represented by the blue square, while the UEs are distributed around it\u2014UE 1 (orange circle), UE 2 and UE 3 (green and purple stars), and UE 4 (yellow triangle). This ensures a realistic wireless network environment for testing."}, {"title": "C. Wireless Distributed Acceleration Algorithm", "content": "The primary purpose of this hardware testbed system is to verify the effectiveness of the WDMoE deployment architecture and the basic concepts for expert selection, without estimating channel conditions, predicting transmission rates, or allocating communication bandwidth to reduce latency. Therefore, the optimization algorithm based on estimated latency, as discussed in Section IV, is not suitable for this system. As analyzed in Section V, balancing the workload across experts can effectively accelerate processing under certain conditions without degrading model performance. Consequently, we propose an expert selection policy specifically designed for this hardware system to achieve faster model inference.\nDuring program execution, the historical transmission and processing latency for device k at each layer is recorded as tk. The corresponding number of tokens processed by device k is denoted as Jk. The average latency per token is given by:\n\\overline {t_{k}}=\\frac{t_{k}}{J_{k}}\n(30)\nBased on experimental data, the variance in latency per token for each expert is sufficiently low, allowing the mean latency per token to serve as a reliable estimate. Since the gating network is deployed on the server, the server can use this average latency per token, along with the number of tokens processed by each device, to predict the total latency for each device:\n\\overline {t}=\\overline {t_{k}}\\cdot J_{k}\n(31)"}, {"title": "D. Hardware Experiment Results", "content": "We evaluate the performance of our hardware testbed system from both model capability and attention waiting latency, based on theoretically simulated experiments, and compare the proposed method with the baseline Mixtral-based method. The"}, {"title": "VII. CONCLUSION", "content": "In this paper, we introduce WDMOE, a wireless distributed MoE architecture for LLMs. This architecture enables the collaborative deployment of LLMs across the MEC server at the BS and mobile devices in the wireless networks. By leveraging the parallel characteristics of expert networks, the deployment architecture effectively models attention waiting latency and incorporates the WLR metric, which jointly assesses model performance and service latency. Through the development of expert selection and bandwidth allocation strategies, we solve the formulated bilevel optimization problem. Besides, we build a hardware testbed to validate the effectiveness of the proposed method. Extensive experiments demonstrate WDMoE ensures high performance and significantly reduces latency, which verifies the feasibility of distributed LLMS in wireless scenarios and highlights the promising future of cooperative edge-device large models."}]}