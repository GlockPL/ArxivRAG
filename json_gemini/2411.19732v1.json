{"title": "Improving Generalization of Robot Locomotion Policies via Sharpness-Aware Reinforcement Learning", "authors": ["S. Bochem", "E. Gonzalez-Sanchez", "Y. Bicker", "G. Fadini"], "abstract": "Reinforcement learning often requires extensive training data. Simulation-to-real transfer offers a promising approach to address this challenge in robotics. While differentiable simulators offer improved sample efficiency through exact gradients, they can be unstable in contact-rich environments and may lead to poor generalization. This paper introduces a novel approach integrating sharpness-aware optimization into gradient-based reinforcement learning algorithms. Our simulation results demonstrate that our method, tested on contact-rich environments, significantly enhances policy robustness to environmental variations and action perturbations while maintaining the sample efficiency of first-order methods. Specifically, our approach improves action noise tolerance compared to standard first-order methods and achieves generalization comparable to zeroth-order methods. This improvement stems from finding flatter minima in the loss landscape, associated with better generalization. Our work offers a promising solution to balance efficient learning and robust sim-to-real transfer in robotics, potentially bridging the gap between simulation and real-world performance.", "sections": [{"title": "Introduction", "content": "Reinforcement Learning (RL) has been successfully employed to learn robust control policies for robotic environments from data. A major downside of RL is the large amount of training experience it needs to approximate the policy gradient, which may become unfeasible online. In response to this challenge, transfer learning in robotics allows for policy development in simulation before deployment on real robots [1, 2], bypassing the challenges of direct real-world learning [3]. However, the gap between simulations and the real world remains a significant challenge [4]. Differentiable simulators have emerged as powerful tools for sample-efficient policy optimization, enabling the use of first-order policy gradient (FoPG) in policy training [5]. These methods leverage analytic gradients of a policy's value function, leading to faster convergence and improved sample efficiency compared to zeroth-order methods. However, the effectiveness of FoPG methods relies heavily on the quality and accuracy of simulator gradients. In real-world robotics applications, particularly those involving complex contact interactions, the dynamics are often non-differentiable [6, 7] producing rugged landscapes with sharp local minima [8]. Approximations used to make simulators differentiable can introduce bias and high variance in the computed gradients [9]. The trade-off between using gradient-based information and zeroth-order methods in reinforcement learning remains an open question in the field of policy optimization for robotics. Gradient-based methods typically offer more efficient parameter updates but may struggle with non-smooth or discrete action spaces, whereas zero-order methods seem to handle these scenarios more easily but may require more samples.\nAdditionally, the generalization capabilities of first-order policy optimization methods, which have become increasingly popular, still need to be thoroughly tested across a wide range of environments and tasks. As the field progresses, understanding the relative strengths and limitations of these approaches in different contexts remains an important area of research that we aim to address.\nThe rest of the paper is structured as follows. The following section 2 discusses recent work on differentiable simulation and the generalizability of trained policies. Section 4 briefly introduces the necessary background of this paper. In the methods section 5 we formally introduce our proposed method to improve the robustness of policies trained with differentiable simulation. In section 7, we showcase that policies trained with our proposed algorithm are able to improve generalizability. Finally, we give an outlook on the limitations of our work and future research directions."}, {"title": "Related Work", "content": "Recent works such as Brax [10], DiffPD [11], Dojo [12], and ADD [13] provide differentiable simulators to enhance sample-efficiency of policy optimization in robotics. Building upon this foundation, algorithms like Policy Optimization with Differentiable Simulation (PODS) [5] and Short Horizon Actor Critic (SHAC) [14] have leveraged the analytical gradients provided by differentiable simulators to significantly improve sample efficiency compared to model-free methods like PPO.\nSHAC, in particular, has made strides in addressing the challenges of contact-rich dynamics through techniques like truncated learning windows and critic function smoothing. More recently, Adaptive Horizon Actor-Critic (AHAC) [15] has further refined this approach by dynamically adjusting the optimization horizon based on contact information.\nFor successful transfer to unknown environments, learned policies must be robust to the discrepancies between simulated and real-world environments. Prior work has approached this challenge from various angles, such as domain randomization [16] and domain adaptation [17].\nPrevious work in deep learning introduces the concept of flatter minima in the optimization landscape and shows that they lead to more generalizable models [18]. Furthermore, empirical studies in deep learning have shown that models converging to these minima tend to exhibit better out-of-distribution performance [19], a property highly desirable for sim-to-real transfer in robotics. A technique to actively search for these flatter minima is Sharpness-Aware Minimization (SAM) [20] and its adaptive counterpart, Adaptive Sharpness Aware Minimization (ASAM) [21]. As these optimizers require two backward passes in each optimization step, a later work, Sharpness-Aware Training for Free (SAF) proposes a sharpness measure based on the KL-divergence between the outputs of DNNS with the current weights and past weights, overcoming the high computational cost of ASAM [22]. Efficient Sharpeness-Aware Minimization (ESAM) improves the computational overhead of ASAM from 100% to 40% [23]. To the best of our knowledge, none of the existing FoPG algorithms have incorporated this insight."}, {"title": "Contributions", "content": "In this work, we demonstrate that while the first-order policy method SHAC [14] is more sample-efficient and achieves better rewards than the zeroth-order method Proximal Policy Optimization (PPO) [24], it struggles with generalization, particularly in noisy and out-of-distribution environments. To address this robustness issue, we introduce a novel approach SHAC-ASAM that incorporates sharpness-aware optimizers [21] into the training process of first-order methods. Our experimental results demonstrate that SHAC-ASAM significantly enhances the robustness of policies compared to vanilla SHAC in both the Ant and Humanoid environments, effectively bridging the gap between first-order efficiency and zeroth-order robustness. Our approach balances sample efficiency with generalization, crucial for developing policies that can effectively navigate the sim-to-real gap in robotics. By combining the rapid learning of first-order methods with an enhanced ability to generalize, we aim to create policies that are both efficient to train and robust in real-world applications."}, {"title": "Preliminaries", "content": null}, {"title": "Differentiable Simulation gradients", "content": "A differentiable simulator defines a differentiable function $s_{t+1} = F(s_t, a_t)$, mapping the current state $s_t$ and action $a_t$ to the next state $s_{t+1}$. During the forward pass, the simulator generates a trajectory by applying the policy $\\pi_\\theta$ and simulating the environment dynamics using $F$. During the backward pass, it computes the gradients of the policy loss $\\mathcal{L}_\\theta$ with respect to $s_t$ and $a_t$:\n$\\frac{\\partial \\mathcal{L}_\\theta}{\\partial s_t} = \\frac{\\partial \\mathcal{L}_\\theta}{\\partial s_{t+1}} \\frac{\\partial F}{\\partial s_t}$ , $\\frac{\\partial \\mathcal{L}_\\theta}{\\partial a_t} = \\frac{\\partial \\mathcal{L}_\\theta}{\\partial s_{t+1}} \\frac{\\partial F}{\\partial a_t}$                                   (1)\nA differentiable simulator enables efficient policy gradient computation by backpropagating the policy loss through the simulator's computation graph, resulting in exact, low-variance gradients for faster convergence and more stable optimization. In this work, we aim to improve the robustness of policies learned by algorithms that leverage differentiable simulations."}, {"title": "Short Horizon Actor-Critic", "content": "SHAC leverages analytical gradients from a differentiable simulator to address the challenges of contact-rich dynamics, long horizons, and sample efficiency in reinforcement learning [14]. The algorithm splits the task horizon into sub-windows of smaller horizons and samples $N$ short-horizon trajectories of length $h \\ll H$ in parallel from the simulator, where $H$ is the full task horizon. The policy is updated with:\n$\\mathcal{L}_\\theta = \\frac{1}{N} \\sum_{i=1}^{N}(\\sum_{t=t_0}^{t_0+h-1}(\\sum_{t'=t_0}^{t}\\gamma^{t-t_0} r(s_t^i, a_t^i)) + V(s_{t_0+h}^i))$(2)\nwhere $s_t^i$ and $a_t^i$ are the state and action at step $t$ of the $i$-th trajectory, $\\gamma$ is the discount factor, $V$ is the critic function with parameters $\\phi$, $R(s_t^i, a_t^i)$ is the reward function, $t_0$ is the initial time step of the short horizon, and $h$ is the length of the short horizon. The short horizon reduces the effect of exploding/vanishing gradients and helps deal with severe discontinuities, leading to a smoother loss landscape. The value function is trained using the following loss:\n$\\mathcal{L}_\\phi = E_{s \\in {\\tau_i}} \\|V_\\phi(s) - \\hat{V}(s)\\|^2$(3)\nwhere $\\hat{V}(s)$ is the estimated target value of the true value function for state $s$, computed from the sampled short-horizon trajectories using a suitable algorithm like TD($\\lambda$) learning. Here, $\\tau_i$ represents an individual trajectory, which is a sequence of states, actions, and rewards experienced by the agent during a single episode or rollout of the environment."}, {"title": "Adaptive Sharpness-Aware Minimization", "content": "ASAM [21] improves model generalization by minimizing loss value and loss sharpness of the parameter space simultaneously. The objective is a minimax optimization problem:\n$\\underset{\\theta}{min} \\ \\mathcal{L}_{ASAM}(\\theta) + \\frac{\\lambda}{2} \\|\\theta\\|^2$\nwhere: $\\mathcal{L}_{ASAM}(\\theta) \\stackrel{\\triangle}{=} \\underset{\\|\\epsilon\\|_p \\leq \\rho}{max} \\  \\mathcal{L}_s(\\theta + \\epsilon)$,\n(4)\nThe inner maximization of ASAM uses a scale-invariant perturbation $\\epsilon$ within an $l_p$ norm ball of radius $\\rho$ to maximize the training loss $\\mathcal{L}_s(\\theta)$, identifying the worst-case scenario. The maximization can be interpreted as finding the highest loss value corresponding to sharp peaks within the local neighborhood. The outer minimization updates $\\theta$ to minimize this worst-case loss, promoting flatter minima in the loss landscape, which leads to better generalization performance. The normalization"}, {"title": "SHAC-ASAM Algorithm", "content": "Our algorithm SHAC-ASAM is detailed in Alg. 1. It combines SHAC's sample efficiency with ASAM's robustness, resulting in stable policies for contact-rich, long-horizon tasks with limited samples, potentially improving sim-to-real transfer. In every episode of the policy optimization, the minimax optimization problem (4) is solved. Our SHAC-ASAM approach utilizes the ASAM optimizer proposed by [21], which offers improved scale-invariance. This extension leverages the unofficial PyTorch repository [25], which provides a straightforward way of integrating SAM and ASAM into existing PyTorch-based pipelines. In practice, the SAM optimizer acts as a wrapper around a base optimizer (in our case, Adam [26]), computing a \"sharpness-aware\" gradient that simultaneously minimizes both the loss value and its sharpness. This approach seeks parameters that lie in neighborhoods with uniformly low loss, potentially leading to better generalization. The ASAM variant adapts the neighborhood size based on the parameter scale, further enhancing robustness.\nThe optimization problem in (4) is more complex than the one solved in vanilla SHAC and hence the integration of ASAM leads to additional computational overhead. In particular, two forward-backward passes per optimization step are required, as outlined in algorithm 1. While the computational cost is increased, the potential improvements in generalization and robustness make it a valuable tool for reinforcement learning tasks, particularly those aimed at sim-to-real transfer."}, {"title": "Environment Perturbation", "content": null}, {"title": "Noise Injection Mechanism on Actions", "content": "To evaluate the robustness of the learned policies against controlled perturbations, we introduce an action noise injection mechanism. It is important to note that although this action noise is\nThe parameter $\\tau_k \\in \\mathbb{R}$ adjusts the neighborhood's size and shape based on model parameters, ensuring invariance to parameter scaling. This maximization is approximated by a first-order Taylor expansion around $\\theta$, yielding a closed-form solution.\n$\\epsilon_t = arg \\underset{\\|\\epsilon\\|_p}{max} \\mathcal{L}_s (\\theta_t + T_{\\theta_t}\\epsilon)$(5)\n$\\approx \\rho  sign(\\nabla \\mathcal{L}_s(\\theta_t)) \\frac{T_{\\theta_t}  \\nabla \\mathcal{L}_s(\\theta_t)}{\\|T_{\\theta_t}  \\nabla \\mathcal{L}_s(\\theta_t)\\|_q^{q-1}}$\n(6)\nwhere $\\epsilon = T_{\\theta_t}e$. When applying ASAM to SHAC, the perturbation term is computed from the gradient of the loss function in equation 2, with respect to $\\nabla_\\theta \\mathcal{L}_s (\\theta_t)$, and the two-step procedure iteratively solves the minimax optimization problem:\n$\\epsilon_t = \\rho T_{\\theta_t} sign(\\nabla \\mathcal{L}_s(\\theta_t))\\frac{T_{\\theta_t}  \\nabla \\mathcal{L}_s(\\theta_t)}{\\|T_{\\theta_t}  \\nabla \\mathcal{L}_s(\\theta_t)\\|_q^{q-1}}$(7)\n$\\theta_{t+1} = \\theta_{t} - \\alpha_t (\\nabla \\mathcal{L}_s(\\theta_t + \\epsilon_t) + \\lambda \\theta_t)$\nfor t = 0, 1, 2, ..., where $\\alpha_t$ is the learning rate and $\\rho$ controls the norm-ball radius and hence the perturbation strength. Using the Euclidean norm (p = 2) and normalization operator $T_{\\theta_t}$ = diag($|\\theta_t|$), the perturbation term simplifies to:\n$\\epsilon_t = \\rho \\frac{T_{\\theta_t}  \\nabla \\mathcal{L}_s(\\theta_t)}{\\|T_{\\theta_t}  \\nabla \\mathcal{L}_s(\\theta_t)\\|_2}$(8)\nWhile our experiments focus on SHAC, it is important to note that the principles underlying our approach are not limited to this specific algorithm. The integration of sharpness-aware optimization techniques should, in principle, apply to other algorithms that leverage the differentiability of the underlying simulator, such as AHAC[15] and PODS [5]."}, {"title": "Environment Parameter Variation for Sim-to-Real Transfer Assessment", "content": "As an additional mechanism to evaluate policy robustness, we systematically vary key simulation parameters, aiming to approximate sim-to-real transfer:\n\u2022 Contact stiffness ($k_c$): Affects the rigidity of interactions between the robot and its environment\n\u2022 Coefficient of Friction ($\\mu$): Influences the force required for surfaces to slide against each other\n\u2022 Contact damping ($k_d$): Impacts energy dissipation during contact\nThis approach simulates out-of-distribution environments, assesses policy generalization to shifted physical properties, provides insights into potential real-world performance, and helps identify limitations in the learned policies.\nBy varying these parameters, we create a spectrum of environments that challenge the policies beyond their training distribution, offering a first approximation of the challenges in sim-to-real transfer. This"}, {"title": "Results & Discussion", "content": null}, {"title": "Comparing Robustness of First-Order and Zeroth-Order Methods", "content": "Next, we test our intuition that the gradients provided by differentiable simulators in FoPG methods may lead to convergence towards sharp local minima. These sharp minima are characterized by high sensitivity to small perturbations in both policy and environmental parameters. We then study the effect of these two critical sensitivities:\n1. Policy Sensitivity: small perturbations in policy outputs can displace the solution from the optimal point within the sharp local minimum, resulting in significant deterioration of performance.\n2. Environmental Sensitivity: Sharp features are highly sensitive to environmental changes, causing previously optimal solutions to lose effectiveness when conditions deviate from the training scenario.\nTheoretical and empirical studies suggest that flatter minima often correspond to more generalizable solutions [18]. The inherent noise in zeroth-order policy gradient (ZoPG) methods may naturally bias them towards these more robust solutions, potentially leading to better generalization in diverse scenarios.\nTo empirically test this hypothesis, we compare the robustness of policies trained using SHAC and PPO in the Ant environment. The Ant environment used in our experiments is a reimplementation of the classical Ant MuJoCo environment [27] in NVIDIA's DFlex, providing a differentiable simulation platform for our study. We examine environmental sensitivity by varying two critical parameters that significantly impact the dynamics of contact-rich scenarios: contact stiffness and damping which are described in 6.2.\nThe results of our environmental sensitivity experiment, visualized in Figure 1, reveal a stark contrast between SHAC and PPO. The heatmaps show the performance of each algorithm across varying levels of contact stiffness and damping in the Ant environment. While SHAC achieves higher peak rewards under specific parameter combinations as indicated by the brighter regions in the upper heatmap, PPO demonstrates greater robustness to parameter variations. This is evidenced by the more uniform distribution of rewards across the parameter space in the bottom heatmap. However, SHAC policy's performance degrades more rapidly as we move away from its optimal parameter region, indicating a higher sensitivity to changes in these environmental parameters compared to PPO. In contrast, ZoPG, here exemplified by PPO, relies on stochastic gradient estimates. Their loss landscape seems to be more consistent across parameters change, indicating to come up with policies"}, {"title": "SHAC-ASAM Generalization Capabilities", "content": "In this section, we investigate the generalization capabilities of policies trained using SHAC-ASAM by evaluating their performance in terms of rewards and comparing them against the baseline SHAC and PPO algorithms. We assess the policies' robustness under noise perturbations applied to the actuator actions and under varying contact parameters. Additionally, we explore the trade-offs between generalization and performance when choosing the p parameter for ASAM, and the trade-off regarding training times of SHAC vs. SHAC-ASAM. The experiments are conducted in the Ant and Humanoid environments."}, {"title": "Sensitivity to Action Perturbations", "content": "Figure 2a shows the average episode reward versus the level of noise injected into the policy actions as described in Section 6.1. To evaluate the robustness of the policies, we varied the parameter $\\lambda$ of the convex combination between the original action and uniform noise. We tested $\\lambda$ values ranging from 0 (no noise) to 0.5 (equal weight to original action and noise).\nFrom Figure 2a, it is evident that applying our SHAC-ASAM) algorithm results in a notable improvement in the robustness of SHAC. Compared to the baseline SHAC, SHAC-ASAM maintains a higher"}, {"title": "Sensitivity to Contact Parameters Modification", "content": "Next, to evaluate the generalization capabilities of the trained policies under varying environmental conditions, we perturbed the friction coefficient of the environments. As shown in Fig. 2a, training SHAC with the ASAM optimizer with p = 0.75 results in better generalization capabilities than training plain SHAC. A similar observation can be made for the humanoid environment in Fig. 3b which is generally less robust to noise than the Ant environment. Still, our sharpness-aware approach can match the generalizability of PPO in this case."}, {"title": "Balancing Specialization and Generalization in ASAM", "content": "Fig. 4 illustrates the key trade-off in selecting the p parameter for SHAC-ASAM, demonstrating how we can tune the balance between generalization and performance within the same sample budget. The results align with our theoretical expectations: larger p values lead to flatter minima in the loss landscape, resulting in better generalization but potentially lower peak performance. For lower p values, we observe higher rewards when the testing environment closely matches the training conditions. However, these policies show steeper performance degradation as action noise increases. Conversely, policies trained with larger p values exhibit more stable performance across varying noise levels, indicating superior generalization. We observed that policies with higher p values can be trained for more iterations to achieve better overall performance, albeit at the cost of requiring more samples. This suggests that increasing p slows down the learning process. Nonetheless, this finding supports our main hypothesis: SHAC-ASAM allows for fine-tuning the generalization-specialization trade-off while maintaining sample efficiency. These results underscore the flexibility of SHAC-ASAM in adapting to different requirements, whether prioritizing high performance in known conditions or robust generalization in uncertain environments."}, {"title": "Trade-off Between Training Time and Generalization", "content": "The table given in 1 depicts the training times for Ant and Humanoid of Vanilla SHAC and our novel method SHAC-ASAM incorporating sharpness-aware optimization. Notably, our approach takes around twice the time of SHAC which aligns with the reported computational complexity in the ASAM paper [21]. However, the higher training cost can potentially be reduced by incorporating"}, {"title": "Conclusion & Future Work", "content": "In this work, we presented a novel method incorporating sharpness-awareness into differentiable policy optimization. Our work contributes to developing RL algorithms that are both sample-efficient and robust to environmental changes, a crucial step towards successful real-world RL applications.\nSimulation experiments on commonly used Mujoco environments demonstrate that our method is effectively improving the robustness of first-order policy optimization methods like SHAC while maintaining most of its sample-efficiency. In this work we mainly focused on SHAC as a FoPG algorithm, but in principle, our approach of applying ASAM is agnostic of the underlying algorithm and results should transfer to other first-order methods such as PODS or AHAC and other sharpness-aware optimizers. Hence, we plan to extend and test this approach also for other first-order algorithms in our future work. While SHAC-ASAM shows an increased generalization capability, two backward passes are required for each training step. Another aspect we aim to address in future work is the exploration of less computationally costly sharpness-aware optimizers tailored for applications in robotics. Finally, we plan to demonstrate the effectiveness of our method on more environments and sources of perturbation before validating our findings with sim-to-real testing of our method on real hardware."}]}