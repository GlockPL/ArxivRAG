{"title": "MANISKILL3: GPU PARALLELIZED ROBOTICS\nSIMULATION AND RENDERING FOR\nGENERALIZABLE EMBODIED AI", "authors": ["Stone Tao", "Fanbo Xiang", "Arth Shukla", "Yuzhe Qin", "Xander Hinrichsen", "Xiaodi Yuan", "Chen Bao", "Xinsong Lin", "Yulin Liu", "Tse-kai Chan", "Yuan Gao", "Xuanlin Li", "Tongzhou Mu", "Nan Xiao", "Arnav Gurha", "Zhiao Huang", "Roberto Calandra", "Rui Chen", "Shan Luo", "Hao Su"], "abstract": "Simulation has enabled unprecedented compute-scalable approaches to robot\nlearning. However, many existing simulation frameworks typically support a\nnarrow range of scenes/tasks and lack features critical for scaling generalizable\nrobotics and sim2real. We introduce and open source ManiSkill3, the fastest\nstate-visual GPU parallelized robotics simulator with contact-rich physics tar-\ngeting generalizable manipulation. ManiSkill3 supports GPU parallelization of\nmany aspects including simulation+rendering, heterogeneous simulation, point-\nclouds/voxels visual input, and more. Simulation with rendering on ManiSkill3\ncan run 10-1000x faster with 2-3x less GPU memory usage than other plat-\nforms, achieving up to 30,000+ FPS in benchmarked environments due to min-\nimal python/pytorch overhead in the system, simulation on the GPU, and the\nuse of the SAPIEN parallel rendering system. Tasks that used to take hours to\ntrain can now take minutes. We further provide the most comprehensive range\nof GPU parallelized environments/tasks spanning 12 distinct domains including\nbut not limited to mobile manipulation for tasks such as drawing, humanoids, and\ndextrous manipulation in realistic scenes designed by artists or real-world digital\ntwins. In addition, millions of demonstration frames are provided from motion\nplanning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set\nof baselines that span popular RL and learning-from-demonstrations algorithms.", "sections": [{"title": "1 INTRODUCTION", "content": "One of the grand challenges of robotics is robust and generalized manipulation. However, unlike\nvision and language research, there are still no good datasets for robotic manipulation that can be\ntrained on. One approach has been to create human-scalable real-world teleoperation tools (Fu\net al., 2024; Cheng et al., 2024) to then perform imitation learning. Another is to set up real-\nworld reinforcement learning to fine-tune trained offline policies (Feng et al., 2023). However, real-\nworld imitation learning approaches require enormous amounts of data that are infeasible to collect\nefficiently at low costs only to achieve relatively low success rates that are otherwise impractical\nfor real-world deployment (Zhao et al., 2024). Real-world reinforcement learning approaches are\npromising, but require extensive setup in the real world to generate real-world rewards/success and\nenvironment resets.\nGPU parallelized simulations such as Isaac (Makoviychuk et al., 2021) and Mujoco's MJX (Todorov\net al., 2012) have made massive advancements in solving some robotics problems such as robot\nlocomotion by training in large-scale GPU parallelized simulations with reinforcement learning (RL)\n(Rudin et al., 2021). GPU parallelized simulation makes data incredibly cheap to generate. However,\nwhen it comes to manipulation, success is often limited to narrower ranges of manipulation tasks and\ntypically requires strong state estimation (Handa et al., 2023) to replace visual inputs like RGB or\npointcloud. Existing GPU simulators have limitations that hinder the generalization and scalability\nof previous work. These simulators lack support for heterogeneous simulation, where each parallel\nenvironment contains different scenes. Additionally, they often don't support fast parallel rendering\ncapabilities. As a result, algorithms like reinforcement learning (RL) that operate on visual input\ntrain too slowly to be practical.\nThe core contributions of ManiSkill3 that set it apart from existing simulators are as follows:\n1) State-of-the-art GPU Parallelized Simulation and Rendering: RL algorithms like PPO (Schul-\nman et al., 2017) can now solve visual tasks in 10-1000x less time it would have taken on other sim-\nulators due to fast parallel rendering and low overhead in the system design of ManiSkill3, leading\nto highly efficient use of the GPU. Depending on task the simulation + rendering FPS can reach up\nto 30,000+, massively accelerating visual data collection by 10-1000x compared to other simulators.\nImportantly ManiSkill3 maintains extremely low GPU memory usage, typically 2-3x lower than that\nof other simulators which enables on device visual RL and larger neural networks during training.\n2) Most comprehensive range of environments with 12 different categories of environments\nand 20+ different robots provided out of the box, all GPU parallelized: ManiSkill3 out of the\nbox provides a diverse set of different types of environments including but not limited to mobile ma-\nnipulation, room-scale scenes, drawing, and humanoid/bi-manual manipulation. We further support\n20+ different robot embodiments out of the box such as quadrupeds, floating grippers, humanoids,\nand dextrous hands. Furthermore we support several sim2real and real2sim setups for manipula-\ntion. Importantly, extensive documentation/tutorials are provided to teach users on how to add new\nenvironments/robots, as well as how to make open-source contributions to expand the repository of\nsimulated tasks/robots.\n3) Heterogeneous Simulation for Generalizable Learning: ManiSkill3 makes it possible to sim-\nulate and render completely different objects, articulations, even entire room-scale scenes in each\nparallel environment. This is done thanks to a data-oriented system design and easy-to-use API to\nmanage GPU memory of objects/articulations even if they may have different degrees of freedom.\n4) Simple Unified API to Easily Manage and Build GPU Simulated Tasks: ManiSkill3 distin-\nguishes itself from other GPU-parallelized robotics simulators and benchmarks by offering a user-\nfriendly API for creating diverse robotics environments. Key improvements include object-oriented\nAPIs and the elimination of complex tensor indexing. The platform provides feature-rich tooling to\nstreamline various operations, such as domain randomization (e.g., camera poses, robot controllers),\ntrajectory replay, action space conversion, and more.\n5) Scalable Dataset Generation Pipeline from Few Demonstrations: For tasks in ManiSkill3\nwhere reward design is difficult, we provide a pipeline that leverages demonstration efficient, wall-\ntime fast, online imitation learning algorithms, to learn a generalized neural network policy from a\nfew teleoperated/hardcoded demonstrations. The generalized task-specific neural network policy is\nthen used to rollout many more demonstrations to form larger datasets."}, {"title": "2 RELATED WORK", "content": "Robotics Simulation Frameworks: Isaac Lab (previously called Isaac Orbit) (Mittal et al., 2023)\nand Mujoco (Todorov et al., 2012) are some open-source general-purpose rigid-body GPU paral-\nllelized robotics simulators. Isaac Lab and Brax (Freeman et al., 2021) (which supports the Mujoco\nMJX backend) are the most similar to ManiSkill in that they provides out of the box environments for\nreinforcement learning/imitation learning, as well as APIs to build environments. There are robotics\nframeworks like Robocasa (Nasiriany et al., 2024), Habitat, (Szot et al., 2021), AI2THOR (Kolve\net al., 2017), OmniGibson (Li et al., 2022), RLBench (James et al., 2020) that only have CPU simu-\nlation backends and thus run magnitudes slower than Isaac Lab and ManiSkill, limiting researchers\nto often only explore imitation learning/motion planning approaches instead of reinforcement learn-\ning/online learning from demonstrations methods. Isaac Lab relies on the closed source Isaac Sim\nframework for GPU parallelized simulation and rendering whereas ManiSkill relies on the open-\nsource SAPIEN (Xiang et al., 2020) for the same features. Brax/Mujoco uses the MJX backend\nand currently does not have parallel rendering. Both Isaac Lab and ManiSkill use PhysX for GPU\nsimulation.\nRobotics Datasets: Amongst existing datasets there are typically two kinds, real-world and simu-\nlated datasets. Open-X (Collaboration et al., 2023) is one of the largest real-world robotics datasets\nbut suffers from issues with inconsistent data labels and overall poor data quality. DROID (Khaz-\natsky et al., 2024) addresses some of Open-X's problems by using a consistant data collection plat-\nform. However, both Open-X and DROID require immense amounts of human labor to collect data\nand are inherently difficult to scale up to the sizes of typical vision/language datasets. Among sim-\nlated datasets, frameworks like AI2-THOR (Kolve et al., 2017), and OmniGibson (Li et al., 2022)\nhave complex room-scale scenes but do not readily provide demonstrations or ways to generate\nlarge-scale demonstrations for use in robot learning. Robocasa has a myriad of tasks and realistic\nroom-scale scenes, but further leverages MimicGen (Mandlekar et al., 2023) to scale human teleop-\nerated demonstrations by generating new demonstrations.\nManiSkill sources large-scale demonstrations through a combination of different methods. For eas-\nier tasks, motion planning and rewards for RL are used to generate demonstrations. For more com-"}, {"title": "3 CORE FEATURES OF MANISKILL3", "content": "ManiSkill3 is the most feature-rich GPU simulation framework compared to popular alternatives as\nshown in Table 1. For the largest features, we detail them in subsequent subsections below."}, {"title": "3.1 UNIFIED GPU PARALLELIZED TASKS SUPPORTED OUT OF THE BOX", "content": "The engineering of ManiSkill3 enables it to easily support many different kinds of task categories\nvia a flexible task-building API, contributing to a unified interface for GPU parallelized robotics\nsimulation. Of the existing popular robotics simulators ManiSkill3 supports the most categories of\ndifferent tasks. Concretely we categorize the 12 distinct categories as follows: Table top manipu-\nlation, mobile manipulation, room-scale scenes for manipulation, quadruped/humanoid locomotion,\nhumanoid/bi-manual manipulation, multi-agent robotics, drawing/cleaning, dextrous manipulation,\nvision-tactile manipulation, classic control, digital twins, and soft body manipulation environments.\nAll of these tasks are GPU parallelized and can be rendered fast in parallel as well, with examples\nof the tasks shown in Fig. 1. Each of these task categories has various optimizations done to run\nmore accurately and faster to achieve state-of-the-art simulation speeds. Other simulators typically\nsupport a smaller subset of the type of tasks ManiSkill3 supports easily. Additional details on the\nexact optimizations/implementations and available robots are detailed in Appendix A."}, {"title": "3.2 GPU PARALLELIZED SIMULATION AND RENDERING", "content": "ManiSkill3 distinguishes itself from its predecessors and other robotics simulators by offering robust\nsupport for GPU-parallelized simulation and rendering. ManiSkill3 is the first benchmark to enable\nfast RL from visual inputs on complex robot manipulation tasks, with Isaac Lab recently adding a\nsimilar feature. Tasks such as picking up a cube or controlling a quadruped to reach a goal from\npixel inputs are now solved on the order of minutes instead of hours. RL training results/speed are\ndetailed in Section 4.3.\nThe performance results shown in Figure 2 are the results after simulating + rendering RGB, depth,\nand segmentation data simultaneously for various tasks. In terms of speed and GPU memory use,"}, {"title": "3.3 HETEROGENEOUS GPU SIMULATION", "content": "ManiSkill3 is so far the only simulation framework that supports heterogeneous GPU simulation.\nThis is the feature of being able to simulate different object geometries, different numbers of objects,\nand different articulations with different DOFs across different parallel environments. For example,\nin the Open Cabinet Drawer task, for each parallel environment, we build a different cabinet (all with\ndifferent DOFs) and sample a random drawer link that needs to be opened to succeed. In the Pick"}, {"title": "3.4 SIM2REAL AND REAL2SIM FOR ROBOT MANIPULATION", "content": "Towards the goal of robust real-world robotics beyond simulation, we verify sim2real and real2sim\nare both possible using ManiSkill3 via digital twins on some tasks. Figure 7 showcases several\ndigital twins supported with real world counterparts. For the cube picking task, we segment out\nrelevant objects to feed into a PointNet model and train with PPO on only simulated data, ultimately\nachieving a real world success rate of 95% over a 50cm x 50cm workspace. For the vision-tactile\npeg insertion task, we simulate the tactile sensor made of silicone as a softbody and refer readers\nto the results showcased in the original work by Chen et al. (2024) for those environments, which\nachieved a 95.08% success rate in the real world. Finally, for the real2sim digital twins we evaluate\nOcto and RT-1X on the ManiSkill3 GPU parallelized version of 4 tasks in SIMPLER (Li et al.,"}, {"title": "3.5 SIMPLE UNIFIED API FOR BUILDING GPU SIMULATED ROBOTICS TASKS", "content": "A core reason behind the flexibility of the ManiSkill3 system to support so many different distinct\ntask categories is the clean and simple API for task-building. The simple API also enables users to\neasily build and customize their own robotics tasks."}, {"title": "3.5.1 OBJECT-ORIENTED API FOR ARTICULATIONS, LINKS, JOINTS, AND ACTORS", "content": "ManiSkill3 is the only framework with a complete object-oriented API around the high-level articu-\nlations/actors down to individual links/joints and meshes. In contrast, IsaacGymEnvs requires users\nto instantiates relevant GPU buffers for holding articulation state such as root pose and joint angles.\nIsaac Lab improves on this with a partially object-oriented articulation API that allows one to create\nan articulation object (e.g., for a cabinet). However, one still has to often play around with index\nvalues to get the relevant articulation data they need. We use a cabinet opening task as a case study.\nIn a cabinet drawer opening task, to write good reward functions you need to access the drawer\nlink's handle mesh's pose, as well as the joint angle between the drawer and the cabinet. A visual\ncomparison of the 3 APIs (simplified from the actual code) is shown in Figure 8.\nFurthermore, pose information in ManiSkill3 is object-oriented and stored as batched Pose objects,\nenabling an easy to read, method chaining pattern of programming for working with poses. For"}, {"title": "3.5.2 ROBOTS AND CONTROLLERS", "content": "ManiSkill3 supports both URDF and Mujoco MJCF definition formats natively and builds articu-\nlated robots based on the URDF/MJCF directly. For each robot, ManiSkill3 further provides a num-\nber of configurable controller options for both GPU parallelized joint position control and inverse-\nkinematic (IK) control, modified from ManiSkill2 for GPU simulation. ManiSkill3 builds upon\nthe PyTorch Kinematics package (Zhong et al., 2024) to support inverse-kinematic based controllers\nparallelized on the GPU. These options are easily configured at runtime with either preset configura-\ntions or user-supplied controller configurations. Currently, there are 20+ different robots supported\nout of the box in ManiSkill3, a subset of which are visualized in Figure 11 in the Appendix. Finally,\nManiSkill3 comes with extensive tutorials and examples of how to tune and optimize robots for fast\nsimulation, which has often proven a stumbling block for those new to robot simulation importing\ncomplex robots for the first time. The tutorials highlight details from how to simulate mobile-bases\nto simulation hyperparameter tuning for improving simulation speed and/or accuracy."}, {"title": "3.6 DEMONSTRATION DATASETS", "content": "We leverage a variety of approaches to collect/generate our demonstration datasets infinitely at scale.\nFor the simplest tasks, we write and open source some motion planning-based solutions to generate\ndemonstration data. Some tasks with easy-to-define reward functions have dense reward functions\ndefined and converged RL policies are used to generate demonstration data. For more difficult tasks,\nwe collect demonstration data (typically about 10 demonstrations) via teleoperation tools. Then, we\nuse RFCL (Tao et al., 2024) or RLPD (Ball et al., 2023) to run fast online imitation learning and\ngenerate data from converged policies.\nWe further adapt the trajectory replay tool from ManiSkill2 to work with both CPU and GPU simu-\nlated demonstration data. The replay tool enables users to change the observations stored (e.g., state\nor rgbd), as well as modify the actions stored (e.g. joint position or delta end effector pose). The\ntool makes working with demonstration data more flexible in ManiSkill3, such as converting joint\nposition actions generated by motion planning to other control modes like end-effector control. We\nare currently still in the process of generating demonstrations for more tasks."}, {"title": "4 BASELINES AND RESULTS", "content": "ManiSkill3 provides several popular robot learning baselines that span 4 different categories:\nWall-time Efficient Reinforcement Learning: We include a torch based vectorized implementa-\ntion of model-free RL algorithms PPO and SAC (Haarnoja et al., 2018), as well as the state-of-the-art\nmodel-based RL algorithm TD-MPC2 (Hansen et al., 2024). Configurations for baselines are tuned\nto minimize training wall time with no regard to sample efficiency.\nSample Efficient Reinforcement Learning: All of the RL baselines in the wall-time efficient set-\nting are included here with configurations tuned towards more gradient updates and fewer environ-\nment steps to maximize sample efficiency.\nOffline Imitation Learning: These are supervised learning and offline RL style baselines. We cur-\nrently provide Behavior Cloning, offline variants of SAC, and the current state-of-the-art Diffusion\nPolicy (Chi et al., 2023) as baselines.\nOnline Imitation Learning: Online imitation learning generally refers to algorithms that learn from\ndemonstrations in addition to collecting online environment transitions. We currently provide the\ntwo state-of-the-art baselines in this area, Reinforcement Learning from Prior Data (RLPD) (Ball\net al., 2023), and Reverse Forward Curriculum Learning (RFCL) (Tao et al., 2024). Note that RFCL\nleverages simulation state resets.\nFurthermore, ManiSkill3 provides tools that establish clear specifications for how to evaluate poli-\ncies that address common issues in other simulators that can often cause confusion."}, {"title": "4.1 REINFORCEMENT LEARNING TRAINING/EVALUATION SETUP", "content": "As part of an effort to unify baselines and robot simulation to allow easy comparison and re-\nsearch/testing, we ensure that all baselines report the same metrics and run the same evaluation envi-\nronment setups. With shared metrics, this enables us to form a massive Weights and Biases (Wandb)\nonline dashboard of every RL (and IL) algorithm's results and allows for easy cross-comparison.\nWe observe that in past simulators/research, there is often inconsistency in how episode trunca-\ntions/terminations are handled and how to report environment success/fail. For example, Isaac Lab\nby default terminates episodes early during training before reaching the timelimit if the agent suc-\nceeds or fails, but many RL algorithms (especially off-policy ones) like TD-MPC2 and SAC model\nenvironments as having infinite horizon and do not terminate environments early due to success/fail.\nMoreover, Isaac Lab is inconsistent with whether termination means success or failure in different\nenvironments, as shown in their Franka Open Cabinet code and Anymal C Locomotion code.\nTo ensure better consistency in metrics reported not only by baselines provided in ManiSkill3, but\nalso those reported by users, we provide an environment wrapper for evaluation environments that\nauto-records these defined metrics. We provide clear instructions on how to set it up, record results,\nand exactly what each metric means. Concretely, the wrapper ensures evaluation of policies never\nterminate environments early (it is permitted during training as early termination can accelerate some\nRL algorithms), and the wrapper records episode return, length, success/fail once, and success/fail\nat end metrics.\nSuccess once is equivalent to early termination due to success and is set true if the environment\nachieves success once at any point during an episode before truncation. Success at the end is equiv-\nalent to recording whether the environment is in a success state at the final episode step before trun-\ncation. Similarly, fail once/at the end operate the same way but for failure conditions. Both \"once\"\nand \"at the end\" metrics are recorded to indicate a clear distinction in the types of success/failure a\npolicy might achieve to avoid confusion.\nFinally, we make a key distinction in sample-efficient and wall-time-efficient training results by\nexplicitly tagging which training runs are using sample/wall-time-efficient configurations. Research\ninto both sample efficiency and wall-time efficiency remains active in the robot learning community\nso we ensure this distinction is clearly labelled. The final results on the Wandb dashboard are still\nbeing uploaded."}, {"title": "4.2 IMITATION LEARNING TRAINING/EVALUATION SETUP", "content": "Mandlekar et al. (2021) show that imitation learning algorithm performance heavily depends on how\nthe demonstrations were collected, particularly on how \u201cmultimodal\u201d the data is. For example, many\nbehavior cloning algorithms regress actions and perform poorly when trained on motion planning or\nhuman teleoperated data, but can perform very well if trained on data generated by a deterministic\nneural network. With this caveat in mind, we explicitly track in all our imitation learning (online\nand offline) baselines how many demonstrations are used (an integer), what type of demonstrations\nare used (neural net, motion planning, or human), and where the demonstrations are sourced from\nexactly (a longer description e.g. neural net trained via TD-MPC2, teleoperation via Meta-Quest\nVR). On the ManiSkill3 Weights and Biases online dashboard, these key configurations over the\ndataset can be searched/filtered, enabling users to easily compare algorithm performance on any\ndemonstration dataset ablation they wish.\nFurthermore, we provide shared code/evaluation harnesses that enable users to easily use a vector-\nized CPU backend or the GPU simulation backend for fast evaluation of imitation learning policies\nduring training. Success/fail once/at the end metrics are similarly also required and reported auto-\nmatically following the same evaluation setup as RL algorithms. The final results on the Wandb\ndashboard are still being uploaded."}, {"title": "4.3 TRAINING SPEED", "content": "We run experiments using the Proximal Policy Optimization (PPO) (Schulman et al., 2017) RL\nalgorithm on the ManiSkill3 GPU simulation and the ManiSkill2 CPU simulation. ManiSkill2 was\npreviously the fastest robotics simulation+rendering framework until ManiSkill3. The experiments"}, {"title": "5 CONCLUSION", "content": "ManiSkill3 introduces a state-of-the-art framework/benchmark for generalizable robotics simulation\nand rendering. ManiSkill3 runs faster, uses less GPU memory, and supports the most diverse range\nof robotics tasks compared to alternative simulators. In particular, ManiSkill3 offers unparalleled\nsupport for both sim2real and real2sim environments in manipulation tasks. We believe that this\ncomprehensive approach will encourage the research community to tackle manipulation challenges\nmore extensively through compute-scalable simulation. Furthermore, ManiSkill3 provides an easy-\nto-use object-oriented API for building GPU simulated heterogeneous tasks, democratizing access\nto scalable robot learning. Finally, demonstrations and RL/IL baselines with clearly defined metrics\nare open sourced for users to use."}, {"title": "A ENVIRONMENTS AND ROBOTS LIST", "content": "This section covers key implementation and optimization details of the general supported task cat-\negories. Moreover, comparisons are made when possible with other robot simulation frameworks\nthat have similar tasks/categories. For a complete list with videos of every task, see ManiSkill3\nTasks Documentation. A sample of 16 of the supported robots, which include mobile manipulators,\nfloating grippers, quadrupeds, etc. is displayed in Figure 11. For a complete list with details/pictures\nof every robot, see ManiSkill3 Robots Documentation."}, {"title": "A.1 TABLE TOP MANIPULATION", "content": "Table-top manipulation is primarily related to controlling one or more robot arms to manipulate an\nobject on a table. Robots like Franka Emika Panda and Universal Robots 5 fall under this cate-\ngory. Typical tasks may include picking up objects, inserting a peg, assembling a structure, pushing\nobjects, etc.\nRobosuite (Zhu et al., 2020) is an existing robotics framework with CPU simulation+rendering\nimplementations of a few table top tasks. Isaac Lab (Mittal et al., 2023) has only one table-top ma-\nnipulation task supported out of the box. ManiSkill3 has 10+ different table-top manipulation tasks,"}, {"title": "A.2 MOBILE MANIPULATION", "content": "Mobile manipulation here refers to tasks in which a robot arm has a mobile base. Robots like Fetch\nand Stretch fall into this category. Typical tasks may include placing objects on surfaces, opening\ncabinet doors/drawers, picking up objects off the ground, etc.\nImplementation Details: The default robot supported is Fetch. The mobile base in particular is\nnot simulated by driving the wheels, and is modeled similarly to AI2-THOR (Kolve et al., 2017)\nand Habitat (Szot et al., 2021) with one joint controlling forward/backward movement and another\ncontrolling rotation of the base. The Fetch robot definition and collision meshes have further been\ntuned to be simpler for faster simulation. Several impossible self-collisions between some links have\nbeen explicitly ignored to speed up simulation."}, {"title": "A.3 ROOM SCALE ENVIRONMENTS", "content": "ManiSkill3 provides out-of-the-box code to build the ReplicaCAD environment from Habitat (Szot\net al., 2021) and all AI2-THOR environments (Kolve et al., 2017) using assets compiled by the\nauthors of the Habitat Synthetic Scenes Dataset (Khanna et al., 2023). Photo-realism is also possible\nby turning on the ray-tracing shader options when creating the environment.\nAI2-THOR (Kolve et al., 2017) and Habitat (Szot et al., 2021) have long-horizon mobile manipula-\ntion tasks but rely on slow CPU simulation / slow rendering systems, and do not support contact-rich\nphysics for manipulation (only magical grasp) and do not look photorealistic. Robocasa (Nasiriany\net al., 2024) has contact-rich long-horizon mobile manipulation tasks in photorealistic room-scale\nenvironments. However, Robocasa simulates and renders these scenes at around 25FPS as it does\nnot use GPU parallelized simulation and rendering. In contrast ManiSkill3 can simulate the complex\nReplicaCAD environment up towards 2000+ FPS with rendering.\nImplementation Details: We further make several modifications to ReplicaCAD to make it com-\npletely interactive as some of the collision meshes for articulations were modelled incorrectly and\nthus did not support low-level grasping. Via CoACD (Wei et al., 2022) we run convex decomposi-\ntion on objects in AI2-THOR scenes to generate simulatable non-convex collision meshes so those\nobjects can e.g. be grasped and moved around correctly. Via manual annotation by ManiSkill3 au-\nthors, certain categories of objects in AI2-THOR are made to be kinematic so they cannot be moved\naround (e.g. tables, TVs, clocks, paintings) with the rest allowed to be dynamic to be fully simulated\n(e.g. apples, baseball bat, cups)."}, {"title": "A.4 LOCOMOTION", "content": "Locomotion here refers to controlling robot joints to move a robot from one location to another.\nQuadrupeds such as AnyMAL-C and humanoids such as Unitree-H1 fall into this category. Typical\ntasks may include standing still, moving to a target location, running as fast as possible, etc.\nThis type of task can be found in Isaac Lab which has several predefined locomotion tasks with\ntuned rewards and sim2real results.\nImplementation Details: Similar to Isaac Lab, quadruped robots in locomotion tasks are modified\nsuch that the large majority of collision meshes are removed, leaving behind just the feet, ankles, and\nthe body. Moreover following Isaac Lab, joint limits are significantly constrained such that random\nactions do not easily cause the robot to fall over."}, {"title": "A.5 HUMANOID / BI-MANUAL MANIPULATION", "content": "Tasks here refer to the use of a humanoid embodiment such as the Unitree H1 robot or bi-manual\nrobot embodiments for manipulation tasks."}, {"title": "A.6 MULTI-AGENT ROBOTS", "content": "Multi-Agent robots refer to support for controlling multiple different robots in the same simulation\nto perform a task. Setups such as multiple quadrupeds or robot arms fall into this category. A\ncommon task is the handover of objects.\nThis type of task can be found in Robosuite. Isaac Lab supports this type of task, but not out of\nthe box. Past environments based on older versions of Isaac have example tasks with multiple robot\narms/hands running on GPU simulation (Chen et al., 2022).\nImplementation Details: By default the action space is a dictionary action space in multi-agent\nenvironments with a dictionary key for each controllable agent. This action space can easily be\nflattened into a single vector if necessary via a environment wrapper."}, {"title": "A.7 DRAWING/CLEANING", "content": "Drawing/Cleaning refers to tasks for dynamically \"adding/removing\" objects to simulate the effect\nof drawing or cleaning. A task could be to draw the outline of a shape on a canvas or clean dirty\nspots on a table surface.\nManiSkill3 is the only framework that supports this kind of task with GPU parallelization.\nImplementation Details: The drawing/cleaning effect is achieved by building ahead of time 1000s\nof small thin cylinders that represent \u201cink\u201d or dirty spots. For a drawing task, all of these cylinders\nare hidden away from the camera view. When a robot moves a drawing tool close to a surface/canvas,\nthe cylinders have their pose set to be on top of the surface right under where the drawing tool\nis. For a cleaning task, all the cylinders/dirty spots are visible and removed once the cleaning tool\nmoves over the dirty spot. Currently the drawing/cleaning environments in ManiSkill3 do not require\nintricate grasping (nor is it the focus) of the drawing/cleaning tool, so the solver position/velocity\niteration values are tuned down."}, {"title": "A.8 DEXTROUS MANIPULATION", "content": "Dextrous Manipulation refers to tasks often involving multi-fingered hands and dense/rich contacts\noccurring during manipulation. An example task is in-hand rotation.\nThis type of task has been heavily explored and exists in Isaac Lab and ManiSkill3 with GPU\nparallelization.\nImplementation Details: Not many special optimizations are made here. Tactile sensing is further\nprovided in environments via touch sensors placed on the dextrous hand at various points."}, {"title": "A.9 VISION TACTILE MANIPULATION", "content": "Vision Tactile Manipulation refers to tasks that rely on processing tactile information like images to\nsolve manipulation tasks. Tasks could include key insertion which require tactile inputs to solve due\nto visual occlusions.\nManiSkill3 is the only simulator with vision tactile manipulation tasks that also have sim2real capa-\nbilities.\nImplementation Details: The vision-tactile, sim2real, manipulation environments are ported over\nfrom the 2024 ManiSkill Vision-based-Tactile Manipulation Skill Learning Challenge (Chen et al.,\n2024)."}, {"title": "A.10 CLASSIC CONTROL", "content": "Classic control is quite broad but in this context refers to fake robots tasked with achieving some\nstable pose or moving in a direction at desired speeds. Tasks include cartpole balancing, humanoid\nwalking, hopper etc.\nDM-Control (Tunyasuvunakool et al., 2020) has the most implemented control tasks and Isaac Lab\nhas a few GPU parallelized variants. ManiSkill3 has GPU parallelized simulation+rendering vari-\nants of the most control tasks.\nImplementation Details: ManiSkill3 uses its MJCF loader to load the MJCF robot definitions from\nthe original DM-Control repository and tunes robot pd joint delta position controllers to align as\nclosely as possible to the behavior seen in DM-Control."}, {"title": "A.11 DIGITAL TWINS", "content": "Digital twins have two variants included, environments for real2sim and environments for sim2real.\nThe distinction here is real2sim environments simply need to be designed so that a model trained on\na real world equivalent of the simulation environment achieves similar success rates when evaluated\nin simulation. Sim2real digital twins are environments designed so that models trained on simulation\ndata can be more easily used for real world deployment.\nFor real2sim environments, ManiSkill3 ports over and GPU parallelizes some environments from\nSIMPLER (Li et al., 2024), which enables efficient evaluation of policies trained on real world data\nlike the generalist RT-1 and Octo models. The primary tricks include green-screening a real world\nimage and texture matching which have been copied over and parallelized. In depth documenta-"}]}