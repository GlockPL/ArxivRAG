{"title": "Decoding FL Defenses: Systemization, Pitfalls, and Remedies", "authors": ["MOMIN AHMAD KHAN", "VIRAT SHEJWALKAR", "YASRA CHANDIO", "AMIR HOUMANSADR", "FATIMA MUHAMMAD ANWAR"], "abstract": "While the community has designed various defenses to counter the threat of poisoning attacks in Federated Learning (FL), there are no guidelines for evaluating these defenses. These defenses are prone to subtle pitfalls in their experimental setups that lead to a false sense of security, rendering them unsuitable for practical deployment. In this paper, we systematically understand, identify, and provide a better approach to address these challenges. First, we design a comprehensive systemization of FL defenses along three dimensions: i) how client updates are processed, ii) what the server knows, and iii) at what stage the defense is applied. Next, we thoroughly survey 50 top-tier defense papers and identify the commonly used components in their evaluation setups. Based on this survey, we uncover six distinct pitfalls and study their prevalence. For example, we discover that around 30% of these works solely use the intrinsically robust MNIST dataset, and 40% employ simplistic attacks, which may inadvertently portray their defense as robust. Using three representative defenses as case studies, we perform a critical reevaluation to study the impact of the identified pitfalls and show how they lead to incorrect conclusions about robustness. We provide actionable recommendations to help researchers overcome each pitfall.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) [59] is an emerging approach in machine learning (ML) where multiple data owners, called clients, collaboratively train a shared model, known as the global model, while keeping their individual training data private. The central server (service provider) iteratively aggregates model updates from each client, which are generated based on their local data. The server merges these updates using an aggregation rule (AGR) and uses them to update the global model. Following each training iteration (also known as round), the refined global model is distributed to the clients participating in the next round. Prominent distributed platforms such as Google's Gboard [2] for next-word prediction, Apple's Siri [1] for automatic speech recognition [72], and WeBank [93] for credit risk predictions, have adopted this FL mechanism. Its intrinsic characteristic of promoting collaboration while preserving privacy has rendered it indispensable in critical applications, notably in medical diagnosis [29, 46, 75], activity recognition [26, 68, 86, 107], and next-character prediction [87].\nFL is gaining popularity due to its privacy-preserving and collaborative nature, yet it faces vulnerabilities to poisoning attacks [28, 83, 84, 91], where malicious or compromised clients intentionally corrupt FL training and poison the global model. This can result in a poisoned model that performs poorly on all inputs in untargeted poisoning attacks or on specific inputs in targeted poisoning attacks. To address these threats, the community has developed various defense mechanisms. Robust AGRs such as Multi-krum [14] and Trimmed-mean [103], detect and discard malicious updates. Certified defenses like CRFL [97] and Ensemble FL [19] provide robustness certifications. Tools like FLDetector [106] proactively identify and remove malicious clients during training. Meanwhile, FedRecover [20] focuses on post-poisoning recovery after an attack, aiming to restore the global model's performance. Ditto [52] integrates fairness and robustness by regularizing the local training objective, and Cronus [23] enhances security and privacy through knowledge distillation.\nSystemizing FL Defenses: Besides these few defenses, the variety of available options (Table 4) poses a challenge for practitioners, i.e., determining the right defense for a specific use case or integrating multiple defenses for enhanced robustness becomes complex without a clear understanding of where a defense and its dependencies fit in the FL pipeline.\nTo address these research gaps, we conduct a comprehensive systemization (\u00a73) of FL defenses, organizing them along three crucial dimensions: processing of client updates, server's knowledge, and defense phase. While existing works [10, 13, 38, 40, 80, 84] have designed taxonomies primarily focused on adversarial ML, including those that guide the selection of the appropriate attacks and settings for FL, a dedicated systemization for FL defenses has been lacking.\nTo the best of our knowledge, we are the first to propose such a systemization. Our framework simplifies the selection and integration of defenses, clarifying when and where each defense is applied (\u00a73.1). For example, Figure 2 shows that FLDetector operates in the pre-aggregation phase(\u201cwhen\u201d) at the server(\u201cwhere\"). Moreover, the systemization highlights underrepresented defense types, encouraging further exploration and innovation. For example,\nour analysis identified FedRecover as the only post-training, update-modification technique using estimation, prompting exploration of not only other estimation-based but also post-training defenses. From our systemization in Table 1, we can see that most defenses are on the server side, operate during pre-aggregation, and employ metric-based processing of client updates. Importantly, our systemization is designed to be expandable to incorporate more attributes in the future.\""}, {"title": "2 Background", "content": "2.1 Federated Learning (FL)\nIn FL [41, 44, 59], a service provider, called server, trains a global model, $ \\theta_g $, on the private data from multiple collaborating clients, all without directly collecting their individual data. During the tth FL round, the server selects n out of total N clients and shares the most recent global model ($ \\theta_g^t $) with them. Then, a client k uses their local data $D_k$ to compute an update $ \\nabla_k^t $ and shares it with the server. These updates serve as a client's contribution towards refining a global model. Depending on how a client computes their update, FL algorithms can be broadly divided [59] into FedSGD and FedAvg. In FedSGD, a client computes the update by sampling a subset b from their local data and calculating a gradient of loss $l(b; \\theta_g^t)$ of the global model on the subset, i.e., $ \\nabla_k^t = \\nabla l(b; \\theta_g^t) / \\theta_g^t $. In FedAvg, a client k fine-tunes $ \\theta_g^t $ on their local data using stochastic gradient descent (SGD) for a fixed number of local epochs E, resulting in an updated local model $ \\theta_k^{t+1} $. The client then computes their update as the difference $ \\nabla_k^t = \\theta_k^{t+1} - \\theta_g^t $ and shares $ \\nabla_k^t $ with the server. Next, the server computes an aggregate of client updates using an AGR, fagg(such as mean), i.e., using $ \\nabla_{agg}^t = f_{agr}(\\{\\nabla_k^t\\}_{k \\in [n]}\\)$. Finally, the server updates the global model of the (t + 1)th round using SGD as $ \\theta_g^{t+1} = \\theta_g^t + \\eta \\nabla_{agg}^t $ with server's learning rate $ \\eta $. Due to these differences, FedAvg achieves faster convergence and attains higher accuracy than FedSGD [59].\nAfter discussing the update process of FL models and the collaboration between servers and clients, we present the FL applications (where it is deployed) and setups (how it is used). There are two main types of deployments: cross-device and cross-silo, as explained in [41]. In cross-device FL, N is very large (ranging from a few thousand to billions) [78], and only a tiny fraction of them are chosen in each FL training round (n < N). These clients are typically resource-constrained devices such as mobile phones, smartwatches, and other IoT devices [2, 107]. Contrastingly, in cross-silo FL, N is moderate (up to 100) [49], and all clients are selected in each round (n = N). These clients are typically large corporations, including banks [93] and hospitals [66].", "2.2 Poisoning Attacks in FL": "There are various poisoning attacks in literature [9, 11, 12, 14, 28, 57, 60, 63, 83, 99]. An untargeted poisoning attack aims to lower the test accuracy for all test inputs indiscriminately [11, 28, 57, 60, 99]. A targeted poisoning attack [9, 12] lowers the accuracy on specific test inputs. For instance, in backdoor attacks [9] (a sub-category of targeted attacks), the goal is to misclassify only those test inputs that have an embedded backdoor trigger. Since these attacks only affect a subset of inputs, they are much weaker than untargeted attacks.", "2.2.1 Attacks used in our Study": "In our evaluation, we focus on untargeted model poisoning attacks as they are stronger [84].\nStat-Opt [28]: provides a generic model poisoning method and tailors it to specific AGRs such as TrMean [103], Median [103], and Krum [14]. The adversary first calculates the mean of the benign clients' updates, $ \\nabla_b $, and finds the static malicious direction w = \u2212sign($ \\nabla_b $). It directs the benign average along the calculated direction and scales it with $ \\gamma $ to obtain the final poisoned update, -yw.\nDyn-Opt [83]: proposes a general poisoning method and tailors it to specific AGRs, similar to Stat-Opt. The key distinction lies in the dynamic and data-dependent nature of the perturbation. The attack starts by computing the mean of benign updates, $ \\nabla_b $, and a data-dependent direction, w. The final poisoned update is calculated as $ \\nabla' = \\nabla_b + \\gamma w $, where the attack finds the largest $ \\gamma $ that can bypass the AGR. They compare their attack with Stat-Opt and show that the dataset-tailored w and optimization-based scaling factor $ \\gamma $ make their attack a much stronger one."}, {"title": "3 Systemization of Defenses Against FL Poisoning", "content": "Here, we introduce a systematization for FL defenses and use it to rationalize the selection of three representative defenses from the literature. Later, in \u00a76, we use these chosen defenses to conduct a comprehensive impact analysis of the pitfalls outlined in \u00a74."}, {"title": "3.1 Classification of Defenses", "content": "Here, we present the three key dimensions along which we classify FL defenses in literature, as shown in Table 1. In Figure 2, we group defenses according to the third dimension, i.e., defense phase, and highlight their dependencies."}, {"title": "3.1.1 Processing of client updates", "content": "Client model updates undergo several processing steps before they are aggregated at the server. The commonly used processing operations are:\nFiltering updates to entirely or partially eliminate local updates. Filtering defenses fall into two main categories: those based on the values of local model updates, termed update-based filtering [14, 73, 103], and those relying on some metrics associated with local models, known as metric-based filtering [28, 39, 77, 106].\na) Update-based filtering [14, 73, 103] is based on the values of local model updates. It further divides into dimension-wise and vector-wise filtering. Dimension-wise filtering defenses, such as TrMean [103] and Median [103], filter out malicious values along each update dimension, while vector-wise filtering defenses, such as, Krum [14], remove entire malicious updates.\nb) Metric-based filtering [28, 39, 77, 106] relies on some metrics associated with local models, e.g., FLDetector [106] uses a suspicious score as the metric to identify and remove malicious clients from the training process. We describe FLDetector in detail in \u00a73.2.1. In loss-based rejection [28], the loss associated with and without incorporating an update for aggregation is calculated, and updates with higher loss are removed. Similarly, error-based rejection [28] removes updates by assessing the error instead of the loss.\nUpdate re-weighing involves assigning a weight to each local update, reflecting its perceived level of maliciousness. Various re-weighting approaches exist, including:\na) Similarity-based re-weighting [8, 18] is shown by FLTrust [18], where the server assigns a trust score to each client based on the similarity of its updates to the server's update, computed on a small dataset."}, {"title": "3.1.2 Server's Knowledge", "content": "The defense is generally applied at the server, where it collects all local updates and strives to obtain the best possible global model that is least affected by malicious updates. The server's knowledge varies across different defenses and can be described in terms of data and local model updates:\nKnowledge of Data: In the no-knowledge setting [20, 23, 103, 106], as the name suggests, the server lacks information about the data used for training and testing. It only possesses knowledge of the collected local model updates, examples of which include TrMean [103], Krum [14], and Median [103]. Conversely, in the partial-knowledge setting [18, 71, 100], the server possesses a small dataset, which it deploys in various ways, such as calculating entropy associated with updates [71] or assigning a trust score to each client [18] to enhance aggregation robustness against attacks.\nKnowledge of Local Model Updates: In the full-knowledge setting [14, 21, 51], the server has complete access to the model parameters of all clients, representing the widely used scenario. In the partial-knowledge, or distilled-knowledge setting [23, 85], the server only has access to some distilled form of the model parameters, such as the output layers [23]."}, {"title": "3.1.3 Defense Phase", "content": "We categorize defenses based on the training phase, specifying when and where in the training pipeline the defense is applied.\nAggregation-based defenses are applied during the aggregation phase. These defenses can be further categorized into pre-aggregation [14, 60, 71, 77, 103] defenses that perform processing of client updates such as dimension-wise filtering before aggregating updates, or post-aggregation [19, 21, 28] defenses, e.g., Ensemble FL [19] that creates all possible aggregations of k models from N clients, then selects the most frequent predicted label as the correct one.\nNon-aggregation-based defenses that are not performed at aggregation can be further categorized based on their phase:"}, {"title": "3.2 Our Selected Case Study Defenses", "content": "We choose three defenses for brevity to analyze the impact of pitfalls in \u00a76; TrMean, FLDetector, and FedRecover. We first provide each of their descriptions and then give a detailed justification for choosing them for our impact analysis."}, {"title": "3.2.1 Descriptions of our chosen defenses", "content": "Trimmed Mean (TrMean) [103] is a foundational defense used in advanced defenses [20, 83, 106]. It sorts each input dimension j of the client updates, discards the m largest and smallest values (where m indicates compromised clients), and averages the rest.\nFLDetector [106] is designed to detect and eliminate malicious clients, ensuring a byzantine-robust FL system obtains a precise global model. FLDetector operates on the principle that malicious updates, tainted by adversaries, differ statistically from benign ones.\nFor discerning these updates, the server estimates a global model update for client k at round t using the L-BFGS algorithm: $ \\nabla_k^{t-1} = \\nabla_k^{t-1} + \\hat{H}_k^{\\dagger} \\cdot (\\theta_k^t - \\theta_k^{t-1}) $. Here, $ \\nabla_k^{t-1}$. The server retains past N global model differences ($ \\Delta \\theta_t$) and updates differences ($ \\Delta \\nabla_{t-1}$) to compute the HVP (Hessian Vector Product) with L-BFGS. It then gauges a client's suspicious"}, {"title": "4 Pitfalls in FL Defense Evaluations", "content": "After presenting the detailed systemization of defenses, it is imperative to unveil critical pitfalls in FL robustness evaluations. By scrutinizing 50 defenses (Table 4 in Appendix), we link each pitfall to specific components in the FL workflow (Figure 1). We examine each pitfall's prevalence (Figure 3) across the 50 works, discuss their implications, and conclude with practical recommendations."}, {"title": "Pitfall-1: Intrinsically robust datasets", "content": "The chosen datasets are intrinsically robust and lead to incorrect conclusions about a defense's performance.\nDescription: A designed defense may seem to perform well against specific attacks upon evaluation [103]. However, the evaluation dataset might be inherently robust because it is simple and lacks complexity. Therefore, in such situations, we cannot tell if an attack is mitigated due to the inherent robustness of the dataset or the effectiveness of the defense.\nPrevalence and implications: While it is intuitive that a defense mechanism's performance inevitably varies across datasets, using overly simple datasets like MNIST fails to yield meaningful insights into the true robustness of a defense mechanism, (\u00a76.1). MNIST is a class-balanced dataset used in FL using synthetic techniques such as Dirichlet Distribution [61]. Conversely, real-world FL tasks are complex and characterized by highly class-imbalanced datasets (\u00a76.2.1).\nDespite efforts to create open-source datasets mirroring real-world scenarios [17, 25], our survey reveals that MNIST remains predominant, constituting 30% of the works [19, 42, 48, 55, 94, 103] (Figure 3a). CIFAR10 and FashionMNIST, though commonly used, lack true FL representation due to their class-balanced nature (\u00a76.2.1). FEMNIST [17], a real-world dataset specifically curated for FL, is used by only 20%. Another interesting observation from Figure 3a is the exclusive reliance on image-classification datasets, despite the popularity of language and vision-language models in contemporary research.\nRecommendations: Future Evaluations should consider using FL tasks of varying complexities, such as FEMNIST and CIFAR10, for classification and exploring other modalities, such as language, for NLP tasks. In our evaluations in \u00a76 we use image classification datasets, FEMNIST and CIFAR10, and a language dataset, StackOverflow, in \u00a76.4.1 for large-scale FL evaluation."}, {"title": "Pitfall-2: Homogeneous client data distributions", "content": "Using i.i.d. (homogeneous) distributions with low heterogeneity may create a deceptive sense of system robustness that does not reflect real-world complexities.\nDescription: Evaluating a defense on a particular dataset may yield perceived robustness due to inherent homogeneity (\u00a76.2.1) in the dataset distribution rather than the efficacy of the defense technique itself.\nPrevalence and implications: We find that around 50% of the works, use i.i.d. distributed data, despite evidence that it is easier to defend against such distributions [11, 28, 83, 104] (as seen in Figure 3b). The second most common approach involves a natural data distribution where each sample is associated with a client such as StackOverflow [3]. Other artificial distributions include FCJ [28], Dirichlet (Dir) [9, 78] and Mcmahan [59]. In \u00a76.2.1, we prove that Dirichlet more closely aligns with real-world distributions, informing our subsequent analysis in \u00a76.2 on how defense performance varies with different distributions and levels of heterogeneity.\nRecommendations: Future works should prioritize the use of real-world datasets to provide a more realistic evaluation. When working with class-balanced datasets like MNIST, FashionMNIST, and CIFAR10, it is crucial to distribute them among clients heterogeneously. This approach aims to mimic, to some extent, the diversity found in real-world distributions."}, {"title": "Pitfall-3: Slow-converging algorithms", "content": "Evaluations often overlook the use of state-of-the-art, fast-converging algorithms, thereby compromising robustness.\nDescription: The robustness evaluation of an FL defense may heavily rely on the choice of FL algorithms, such as FedAvg or FedSGD, used in its implementation. Using a superior algorithm can enhance system robustness by addressing potential weaknesses in the FL algorithm rather than focusing solely on defense improvement.\nPrevalence and implications: Despite FedAvg's recognized advantages in performance, faster convergence, and lower communication overhead compared to FedSGD [59], approximately 40% of prior works employ the slow-converging FedSGD algorithm for evaluations (Figure 3c). This suboptimal choice contributes to a larger window for attacks and results in a more significant accuracy drop (\u00a76.3).\nRecommendations: Future evaluations should prioritize state-of-the-art, fast-converging FL algorithms to remove any weaknesses (such as slow convergence) associated with the FL algorithm."}, {"title": "Pitfall-4: Limited FL settings", "content": "Considering only limited scenarios while ignoring practical limitations and factors related to scalability, such as computation, communication, cost, and storage.\nDescription: The performance of a defense can vary when constrained by real-world limitations, e.g., cost constraints may lead to selecting a very low percentage of malicious clients. Also, the computation, communication, cost, and storage overhead associated with scaling up the system might not be feasible in practical scenarios.\nPrevalence and implications: Only 24% of the works in our survey use the cross-device setting (Figure 3d). As demonstrated by [84] on FEMNIST, CIFAR10, and Purchase datasets with the cross-device setting, using a low percentage of malicious clients due to cost constraints reduces attack performance. We show that on an even larger scale using the naturally distributed Stackoverflow dataset in the cross-device setting, the attack shows shows no effect on the non-robust mean AGR (\u00a76.4.1). Additionally, defenses that rely on consistent historical information [20, 106] are incompatible with the cross-device setting (\u00a76.4.2) because a client is not selected in every round. We thoroughly discuss the scalability issues of FedRecover and FLDetector in \u00a76.4.2.\nRecommendations: Future evaluations should include deployment conditions of a much larger scale (cross-device) and ensure that the defense provides significant utility compared to the computation and communication cost it incurs. While we do not label the cross-silo setting as impractical, we emphasize designing defenses that are compatible with the cross-device setting as well."}, {"title": "Pitfall-5: Naive attacks", "content": "Evaluating defenses solely against simple and naive attacks rather than incorporating strong state-of-the-art attacks makes a defense seem robust.\nDescription: The true robustness of a defense emerges when tested against strong and adaptive attacks, i.e., attacks tailored for a defense algorithm. Relying on attacks known to be weak and naive for evaluating a new defense will not give us a true picture of the defense's robustness.\nPrevalence and implications: Figure 3e shows the frequency of various attacks used in our survey. Despite the existence of several strong poisoning attacks in the literature [11, 28, 83, 91, 99], our analysis reveals that about 40% of the works opt for simplistic approaches such as random Gaussian [14], label flipping [52], sign flipping [48], bit"}, {"title": "Pitfall-6: Unfair metrics", "content": "Not capturing clients' performances separately and only reporting global accuracy metrics does not give a good measure of per-client robustness.\nDescription: Data heterogeneity across clients in practical FL systems results in varying performance across clients. This phenomenon is also known as representation disparity [37]. Global accuracy does not give us any idea of the individual performances of clients. In addition to this, real-world datasets are class imbalanced [17] as opposed to synthetic datasets such as MNIST and CIFAR10. The commonly reported overall accuracy metric lacks information on per-class performance.\nPrevalence and implications: Despite heterogeneity being a well-known issue, only 4% of the surveyed works incorporate personalized evaluations (Figure 3f). Our per-client analysis of TrMean and FedRecover demonstrates that per-client performances vary a lot, highlighting the need to account for these variations in real-world FL systems (\u00a76.6). We also highlight the importance of reporting per-class accuracies, as we show that the overall accuracy is a misleading metric (\u00a76.6.3).\nRecommendations: Future evaluations should include personalized evaluations along with global evaluations to capture the variation in clients' performances. In addition to this per-client evaluation, future works should also report per-class performance, especially when using class-imbalanced datasets."}, {"title": "5 Experimental setup", "content": "In this section, we provide the details of our experimental setup."}, {"title": "5.1 FEMNIST [17, 24]", "content": "FEMNIST (Figure 5) is a character recognition classification task with 3,400 clients, 62 classes (52 for upper and lower case letters and 10 for digits), and 671,585 grayscale images. Each client has data of their own handwritten digits or letters. We use 300 randomly selected clients with their original data in a cross-silo fashion, as FedRecover uses the cross-silo setting in its implementation. We use the CNN used by [20] and use the Xavier weight initialization.\nHyperparameters for re-eval: For FEMNIST, we run over 200 epochs with 300 clients. In the attack setting, 60 clients are malicious. The results in Figure 6 use $T_w = 10$, and $T_c = 10$. The FL algorithm used here is FedAVG with a local learning rate of 0.05 and a global learning rate of 1. We keep the batch size to 32. The number of local epochs is kept at 1. For Figure 12a, we consider the possibility of benign clients being misclassified as malicious or malicious clients being misclassified as benign, so we vary the false negative and false positive rates between 0.1 and 0.5."}, {"title": "5.2 CIFAR10 [45]", "content": "CIFAR10 is a 10-class classification task with 60,000 total RGB images, each of size 32 \u00d7 32. We divide all the data among 100 clients using either Dirichlet [78] or FCJ [28] distributions, which are the two most popular synthetic strategies to generate the FL dataset. We use a Resnet20 model with the CIFAR dataset.\nHyperparameters for re-eval: We run over 100 epochs with 100 clients. In the attack setting, 20 clients are malicious. The FL algorithm used here is FedAVG, with a local learning rate of 0.01 and a global learning rate of 1. We keep the batch size to 16. The number of local epochs is kept at 2. The results in Figure 6 use $T_w = 10$, and $T_c = 5$ and the fang distribution. Contrary to the rest of the datasets used, we use $T_c = 5$ because CIFAR10 was a much more challenging learning task."}, {"title": "5.3 MNIST [47]", "content": "MNIST is a 10-class digit image classification dataset, which contains 70,000 grayscale images of size 28 \u00d7 28. We consider 100 FL clients and divide all data using Dirichlet or FCJ distributions. We use the same CNN as the FEMNIST dataset.\nHyperparameters for re-eval: For MNIST, we run over 2000 epochs with 100 clients, a learning rate of 0.03, and a batch size of 32. In the attack setting, 20 clients are malicious. We set $T_w = 20$, and $T_c = 10$. The FL algorithm used here is FedSGD. The results reported in Figure 6 use the FCJ distribution."}, {"title": "5.4 Fashion-MNIST [96]", "content": "Fashion-MNIST is a 10-class image classification dataset with grayscale images of clothing of size 28 \u00d7 28. It contains 70,000 total images. We consider 100 FL clients and divide all 70,000 images using Dirichlet or FCJ distributions. For CIFAR10, MNIST, and FashionMNIST, we divide each client's data in train/test/validation splits in the ratio of 10 : 1 : 1. We combine clients' validation data and use it for validation and hyperparameter tuning and report accuracy on test data. We use the same CNN as the FEMNIST dataset.\nHyperparameters for re-eval: We run over 2000 epochs with 100 clients, a learning rate of $3 \\times 10^{-3}$, and a batch size of 32. In the attack setting, 20 clients are malicious. We set $T_w = 20$, and $T_c = 10$. The FL algorithm used here is FedSGD. The results reported in Figure 6 use the FCJ distribution."}, {"title": "5.5 StackOverflow [7]", "content": "StackOverflow is a language-modeling dataset that is used for tag prediction and next-word prediction. It consists of 342,477 users who are used as clients, and the training data consists of 135,818,730 examples. We use an RNN with a 96-dimensional embedding and a 10000-word vocabulary. The complete network consists of an input layer followed by an embedding layer, an LSTM layer, and two dense layers. We use the cross-device setting to obtain the baseline in [78] by using the fedjax [79] framework. The FL algorithm is FedAdam, and the training consists of 1500 rounds with 50 clients chosen every round with one local epoch. We keep the batch size to 16, the client optimizer as SGD with a learning rate of $10^{-3}$, and Adam as the server optimizer with a learning rate of $10^{-2}$."}, {"title": "6 Impact Analysis of Pitfalls", "content": "In this section, we analyze the impact of each identified pitfall. We test representative defenses across diverse setups by following the recommendations outlined in \u00a74 and scrutinizing their implications. This systematic exploration is intended to help researchers make informed decisions about the robustness of FL defenses."}, {"title": "6.1 Intrinsically Robust Datasets", "content": "First, we evaluate how the selection of datasets impacts the robustness of our three representative FL defenses: TrMean, FedRecover, and FLDetector."}, {"title": "6.1.1 TrMean is only robust with MNIST", "content": "To assess TrMean's sensitivity to different datasets, we employ FedAvg and FedSGD on MNIST, FashionMNIST, CIFAR10, and FEMNIST, both without attacks and under Stat-Opt [28]. Results in Figure 4a highlight MNIST's intrinsic robustness. Despite a high (20%) amount of malicious clients, MNIST-trained FL model accuracy drops less than 1% in FedAvg, while other datasets experience more substantial declines, peaking at 50% for CIFAR10.\nThe variation in performance can be attributed to task complexity. Baseline accuracies in Figure 4 reflect this complexity, with MNIST having the highest no-attack accuracy and CIFAR10 the lowest. From this set of experiments, we can conclude that TrMean is highly robust using MNIST-based evaluations, but not with other datasets as the evaluations of TrMean using other three datasets, FashionMNIST, FEMNIST, and CIFAR10 show."}, {"title": "6.1.2 FedRecover works better with simple datasets", "content": "We evaluate FedRecover on FashionMNIST, FEMNIST, and CIFAR10, in addition to MNIST, since MNIST is heavily evaluated in [20]. We test FedRecover under recovery-from-benign and"}, {"title": "6.2 Homogeneous Data Distribution", "content": "In this section, we first show that the Dir (Dirichlet) distribution is more real-world than the FCJ distribution through statistical analyses and visualization. Subsequently, we show the effect of these distributions and their varying levels of heterogeneity on the robustness of TrMean and FedRecover. The original work on FLDetector [106] already analyzed different levels of heterogeneity, so we skip this here."}, {"title": "6.2.1 Statistical analyses of FCJ and Dir distributions", "content": "We consider a classification task with a total of C classes; we generate client datasets using Dir and FCJ for 100 clients with varying degrees of non-i.i.d. We provide analyses for CIFAR10", "datasets": "n(Stat-1): We plot the number of samples per user", "clients).\n(Stat-2)": "In Figure 7b", "59": ".", "distribution.\n(Stat-3)": "For each client, we compute a C-dimensional vector where the ith dimension represents the number of samples from class i; here, we use CIFAR10 with 100 clients; hence we get 100 10-dimensional vectors. Then, we plotted T-SNE projections of these 100 vectors; we plotted them for both Dir and FCJ distributed client datasets. Figures 8b and 8c show the projections for Dir and FCJ, respectively. For reference, we also show in Figure 8a how the T-SNE projections look like for (1) 100 clients with perfectly i.i.d. datasets and (2) the real-world FEMNIST dataset.\nFor FCJ distribution, we note that, for bias values greater than 0.2 (Figure 8b center and right), the client datasets form local clusters, i.e., within these clusters, the clients have highly i.i.d. datasets. This is expected because FCJ forms C groups of clients where ith"}]}