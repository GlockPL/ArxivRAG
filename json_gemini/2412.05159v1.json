{"title": "Enhancing Cross-Language Code Translation via Task-Specific Embedding Alignment in Retrieval-Augmented Generation", "authors": ["Manish Bhattarai", "Minh Vu", "Javier E. Santos", "Ismael Boureima", "Daniel O' Malley"], "abstract": "We introduce a novel method to enhance cross- language code translation from Fortran to C++ by integrating task-specific embedding align- ment into a Retrieval-Augmented Generation (RAG) framework. Unlike conventional re- trieval approaches that utilize generic embed- dings agnostic to the downstream task, our strategy aligns the retrieval model directly with the objective of maximizing translation qual- ity, as quantified by the CodeBLEU metric. This alignment ensures that the embeddings are semantically and syntactically meaning- ful for the specific code translation task. Our methodology involves constructing a dataset of 25,000 Fortran code snippets sourced from Stack-V2 dataset and generating their corre- sponding C++ translations using the LLaMA 3.1-8B language model. We compute pairwise CodeBLEU scores between the generated trans- lations and ground truth examples to capture fine-grained similarities. These scores serve as supervision signals in a contrastive learn- ing framework, where we optimize the em- bedding model to retrieve Fortran-C++ pairs that are most beneficial for improving the lan- guage model's translation performance. By in- tegrating these CodeBLEU-optimized embed- dings into the RAG framework, our approach significantly enhances both retrieval accuracy and code generation quality over methods em- ploying generic embeddings. On the HPC For- tran2C++ dataset, our method elevates the aver- age CodeBLEU score from 0.64 to 0.73, achiev- ing a 14% relative improvement. On the Nu- merical Recipes dataset, we observe an increase from 0.52 to 0.60, marking a 15% relative im- provement. Importantly, these gains are real- ized without any fine-tuning of the language model, underscoring the efficiency and practi- cality of our approach.", "sections": [{"title": "Introduction", "content": "Cross-language code translation is a critical task in modern software development, especially as legacy programming languages, such as Fortran, continue to be prevalent in scientific computing, while more contemporary languages like C++ are favored for their performance and versatility in production en- vironments. The goal of automatic translation from Fortran to C++ is to preserve the functionality and structure of legacy code while benefiting from the optimizations and ecosystem of C++. However, achieving high-quality translations that adhere to the syntax and semantic norms of the target lan- guage remains a challenging problem, particularly when there is a lack of large, aligned datasets or evaluation metrics that cover both source and target languages effectively.\nTraditional approaches to cross-language trans- lation, such as Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) typically involve two phases: first, retrieving relevant examples from a database, followed by a language model generat- ing code conditioned on both the query and the retrieved examples. In prior efforts, the retrieval models in RAG systems have relied on general- purpose embedding models (Bhattarai et al., 2024; Li et al.), which are not tailored to the specific nu- ances of code translation. These embeddings aim to retrieve relevant pairs from the source and tar- get languages but do not directly optimize for the quality of the generated code. As a result, while the retrieved examples may be relevant in a broad sense, they often fail to guide the language model towards producing translations that maximize fi- delity to the ground truth in the target language. This gap is particularly problematic in scenarios where explicit metrics, such as CodeBLEU (Ren et al., 2020)-designed to assess both syntactic and semantic correctness of translated code are only available for the target language (e.g., C++ in this case). Without aligning the retrieval mechanism to such a task-specific metric, the system may re- trieve suboptimal examples, leading to poor code generation performance. The inability to leverage task-relevant quality metrics during retrieval weak- ens the overall system, limiting its effectiveness in high-accuracy code translation tasks. To address these limitations, we propose a novel contrastive learning framework that aligns the retrieval phase of the RAG system with the goal of maximizing the CodeBLEU (Feng et al., 2020) score for the gen- erated C++ code. We collect a dataset of 25,000 Fortran code examples from Stack V2 (Lozhkov et al., 2024) and use the LLaMA 3.1-8B (Tou- vron et al., 2023) model to generate corresponding C++ translations. In the absence of ground truth C++ translations, we evaluate the quality of these translations using pairwise CodeBLEU similarity scores. This metric captures both syntactic correct- ness and semantic fidelity, providing a robust signal for aligning the retrieval model through contrastive learning.\nThe proposed approach aims to addresses the shortcomings of general-purpose embedding mod- els by integrating task-specific metrics into the re- trieval optimization process. By aligning the re- trieval model with the downstream task of produc- ing high-quality C++ code, our method ensures that the examples retrieved during inference are not just broadly similar but are semantically and syntacti- cally aligned in a way that enhances the LLM's generative performance. The result is a significant improvement in translation quality, as measured by CodeBLEU, over previous methods that lack such alignment.\nOur contribution is twofold: first, we demon- strate the effectiveness of contrastive learning for fine-tuning retrieval models in the context of cross- language code translation, using a task-specific metric to guide alignment. Second, we show that optimizing retrieval for downstream generation tasks can lead to state-of-the-art results, particu- larly in cases where aligned datasets are not readily available for both source and target languages. This work not only advances the field of code translation but also opens up new possibilities for applying similar techniques to other language pairs and do- mains where task-specific evaluation metrics are available for only one side of the translation."}, {"title": "Related Work", "content": "Historically, code translation strategies before the advent of LLMs relied heavily on rule-based and statistical machine translation (SMT) sys- tems (Koehn, 2009). These systems used prede- fined rules or statistical mappings between the source and target programming languages, such as tree-based translation approaches that mapped syn- tax trees between languages. While these methods provided structured and interpretable outputs, they were limited in their ability to handle the semantic complexities of different programming languages and struggled with code diversity, edge cases, and idiomatic translations.\nWith the rise of deep learning and LLMs, fine- tuning models on large datasets became the go- to method for improving code translation. Mod- els like CodeBERT (Feng et al., 2020) and Codex (Chen et al., 2021), when fine-tuned on spe- cific language pairs, improved translation quality by leveraging vast amounts of parallel code data. However, the main limitation of LLM fine-tuning lies in the resource-intensive process. Fine-tuning requires substantial amounts of labeled data and computational resources, making it impractical for niche or legacy languages like Fortran, where par- allel data may be scarce.\nAs a next step, task-specific alignment of LLMS emerged to improve translation by better guiding the model's output. While alignment techniques help improve output fidelity, they still necessitate fine-tuning or explicit modification of the LLM itself, which can be resource-intensive and may still fall short of generalization when translating between languages with significant structural dif- ferences (Mishra et al., 2024).\nRAG introduced a more flexible approach by allowing LLMs to retrieve and condition their out- puts on example pairs from a relevant dataset. While RAG improves translation by augmenting the model's input, the effectiveness of this strat- egy depends on the quality and relevance of the retrieved examples. In an example case (Bhattarai et al., 2024), the retrieval step relies on general- purpose embeddings like Nomic-Embed or Code- BERT, which, although effective at retrieving se- mantically similar code, are not optimized for spe- cific downstream metrics like CodeBLEU. As a result, the LLM might not always retrieve the exam- ples that would best assist in producing translations aligned with target-specific quality metrics.\nThe approach we propose offers a significant advantage by focusing on semantic alignment of the retrieval mechanism without the need to fine-tune the LLM itself. Through contrastive learn- ing, we optimize the embedding model to retrieve Fortran-C++ pairs that are more likely to maximize"}, {"title": "Methods", "content": "This section provides the technical description of our proposed method."}, {"title": "Problem setting", "content": "We consider the standard code translation scenario leveraging a language model G, in which a tar- get translated code ct of a query source code cs is generated using G:\n$c^{t} = G(c^{s})$\nIn practice, conditioning G on k example pairs of source and target code $D := \\{(c_i^{s},c_i^{t})\\}_{i=1}^{k}$, can significantly enhance translation. This few-shot learning approach can be expressed as:\n$c^{t} = G(c^{s}, D)$\nIn a RAG framework, this process is further re- fined by integrating a retrieval mechanism R that identifies the most pertinent k example pairs from a large corpus C based on the query $c^s$. By ex- pressing this retrieval step as $D = R(c^s,C)$, we can describe the conventional translation scenario leveraging G as\n$c^{t} = G(c^{s}, R(c^{s}, C))$\nIn practice, the input source code are embedded using a neural network $\\Psi$, which are generally ag- nostic to the downstream task. We denote $c_{\\Psi}^{s}$ as the embedding of the source code $c^{s}$ under the embed- ding $\\Psi$. Hence, Eq. 2 can be expressed as\n$c^{t} = G(c^{s}, R(c_{\\Psi}^{s}, C_{\\Psi}))$\nunder the usage of the embedding model $\\Psi$. Here, the notation $C_{\\Psi}$ refers to the fact that the embedding is applied onto the corpus of c."}, {"title": "Task-Specific Embedding Alignment", "content": "Our method involves aligning the Fortran embed- ding model $\\Psi$ using contrastive learning based on CodeBLEU similarity scores, followed by apply- ing this aligned model within a RAG framework for improved cross-language code translation from Fortran to C++, as shown in Figure 11.\nEmbedding Similarity: Given a pre-trained embedding module $\\Phi$, we directly leverage the CodeBLEU similarity computed from the language model G to train an aligned embedding module $\\Psi$ for the downstream code translation task. The following discusses how to extract the CodeBLEU similarity from G.\nFrom a source dataset of Fortran code snippets $D^{F} = \\{c_i^{F}\\}_{i=1}^{N}$, we generate the corresponding C++ translations $D^{C} = \\{c_i^{C}\\}_{i=1}^{N}$ using G without RAG retrieval:\n$c_i^{C} = G(c_i^{F}), \\forall i = 1, ..., N$\nThen, we compute the pairwise CodeBLEU sim- ilarity scores (Ren et al., 2020) between all gener- ated translation pairs $(c_i^{C}, c_j^{C})$:\n$S_{ij}^{C} = CodeBLEU(c_i^{C}, c_j^{C})$\nwhere the CodeBLEU score matrix $S^{C} \\in [0,1]^{N \\times N}$ is a weighted linear combination of four components: the n-gram match $S_{n-gram}$, the weighted n-gram match $S_{w-n-gram}$, the syntactic AST match $S_{syntax}$, and the semantic data flow match $S_{semantic}$. These components capture the syn- tactic and semantic similarities between the gener- ated C++ translations:\n*   $S_{n-gram}$ is the traditional BLEU score up to n-grams.\n*   $S_{w-n-gram}$ assigns weights to n-grams based on their importance."}, {"title": "Experiments and Results", "content": "In our study, we utilized three datasets to enhance code translation through RAG and embedding alignment. The HPC Fortran2CPP dataset (Lei et al., 2023), comprising 315 Fortran-C++ code pairs, and the Numerical Recipes dataset (Press et al., 1988), containing 298 Fortran-C++ pairs, were employed for RAG retrieval and evaluation with LLMs. Additionally, we used the Stack-V2 dataset (Lozhkov et al., 2024), which includes over 500,000 Fortran code snippets, for RAG alignment. From Stack-V2, we sampled 25,000 high-quality and diverse Fortran code snippets by selecting files larger than 500 bytes and prioritizing those with the highest combined star and fork counts, indi- cating relevance and popularity. Since Stack-V2 lacks Fortran-C++ pairs, we extracted files contain- ing metadata, code, and comments, and utilized the Llama 3.1-70B Instruct model to extract ex- ecutable Fortran code, discarding other metadata. We selected the StarCoder model (Li et al., 2023) as the embedding backbone for our RAG pipeline and aligned it using contrastive learning on the Stack- V2 dataset. Initially, we use the LLaMA 3.1-8B model to translate the cleaned Fortran code snippets into corresponding C++ code. After code trans- laton, we computed pairwise CodeBLEU scores between the generated C++ code snippets to quan- tify the syntactic and semantic similarities of their translations. Leveraging these CodeBLEU metrics and the embeddings from the Fortran codes, we em- ployed the proposed Soft-InfoNCE loss function with a temperature of 0.1 to align the embeddings, effectively training the embedding model to map semantically similar code snippets closer in the embedding space.\nThe embedding model was trained using the Adam optimizer with a learning rate of 10-3 and a batch size of 128 per GPU, sampling approximately 1,280,000 code pairs for alignment. This training process was distributed across 256 GH200 GPUs to accelerate the process, though it can also be per- formed on fewer GPUs at a significantly slower pace. After alignment, we integrated the embed- ding model into the RAG pipeline, storing Fortran- C++ pairs along with their Fortran embeddings in a vector database. We then evaluated the per- formance using the LLaMA 3.1-8B, LLaMA 3.1- 70B, Mistral123B, and Mixtral 8x22B models-all instruct-tuned under zero-shot, 1-shot, 2-shot, and 3-shot settings. The evaluation was conducted on the benchmark datasets HPC Fortran2C++ and Numerical Recipes, following the setup described by (Bhattarai et al., 2024). The CodeBLEU scores for both the aligned and unaligned models were obtained by comparing the RAG-augmented gener- ated C++ translations against the ground truth C++ code."}, {"title": "Conclusion", "content": "We introduced a novel method for enhancing cross- language code translation from Fortran to C++ by aligning embeddings within a RAG framework. By leveraging contrastive learning based on Code- BLEU similarity scores, we aligned the Fortran em- bedding model so that code snippets yielding high- quality translations are positioned closer in the em- bedding space. This alignment enables the RAG system to retrieve semantically meaningful exam- ples that effectively guide th LLM during code gen- eration. Our experimental results demonstrate sub- stantial improvements in translation quality with- out the need for fine-tuning the LLM. Specifically, using aligned embeddings increased the average CodeBLEU score from 0.64 to 0.73 on the HPC Fortran2C++ dataset and from 0.52 to 0.60 on the Numerical Recipes dataset, representing rel- ative improvements of approximately 14% and 15%, respectively. The larger model (11ama3.1 70b) consistently outperformed the smaller model (1lama3.1 8b), indicating that increased model ca- pacity enhances the effectiveness of our approach. Additionally, we observed diminishing returns be- yond two-shot prompting, suggesting that most per- formance gains are achieved with just one or two examples. Thus, our approach significantly im- proves code translation performance by optimizing the retrieval mechanism through task-specific em- bedding alignment, rather than relying on compu- tationally expensive fine-tuning of the LLM. This method is computationally efficient, scalable, and adaptable to other code translation tasks, particu- larly when aligned datasets are scarce or evaluation metrics like CodeBLEU are critical. Future work could extend this alignment strategy to additional programming languages and explore integrating other evaluation metrics to further enhance transla- tion quality."}, {"title": "Limitations", "content": "Our approach leverages CodeBLEU as a task- specific metric for performing contrastive learning via a custom Soft-InfoNCE loss in the alignment of embedding models for code translation. While this approach introduces several improvements, it also brings specific limitations. First, using CodeBLEU as the basis for contrastive learning focuses pri- marily on syntactic and semantic alignment, which may not always translate into functional equiva- lence. CodeBLEU, while effective at evaluating linguistic features of generated code, does not fully capture the functional behavior of code, meaning that two semantically similar snippets could still behave differently at runtime (Ren et al., 2020). This limitation can lead to cases where the retrieval mechanism selects semantically similar but func- tionally incorrect examples, impacting the over- all quality of the translation task. Second, con- trastive learning, particularly with InfoNCE loss, relies heavily on the assumption that maximizing the similarity between pairs (based on CodeBLEU) leads to better downstream performance. However, InfoNCE loss is limited by its focus on pulling positive samples closer while pushing away neg- ative ones, which in the case of code translation, does not always capture the subtle nuances of code equivalence across languages (Khosla et al., 2020). Code snippets with different syntactic structures but similar functionality may be treated as nega- tive examples, leading to a misaligned embedding space and suboptimal retrieval. Third, the granu- larity of the CodeBLEU score presents an inherent challenge. Since CodeBLEU provides a continu- ous similarity metric (between 0 and 1), aligning embeddings through InfoNCE loss may not fully capture the wide range of functional similarities or dissimilarities between code snippets. This results in an embedding space that reflects linguistic rather than purely functional similarity, which can lead to errors in retrieval when applied to real-world translation tasks where functional correctness is paramount (Feng et al., 2020). Additionally, the use of CodeBLEU as a basis for contrastive learn- ing is highly dependent on the quality of the gener- ated code samples and their reference translations. Any noise or imperfections in the training data (e.g., low-quality code or inconsistent style) may degrade the alignment process. Since InfoNCE relies on subtle positive and negative distinctions, noisy CodeBLEU scores can introduce ambiguity, further distorting the learning process and leading to poorer retrievals during generation (Wang and Liu, 2021)."}]}