{"title": "Textless NLP - Zero Resource Challenge with Low Resource Compute", "authors": ["Krithiga Ramadass", "Abrit Pal Singh", "Srihari J", "Sheetal Kalyani"], "abstract": "This work addresses the persistent challenges of substantial training time and GPU resource requirements even when training lightweight encoder-vocoder models for Textless NLP. We reduce training steps significantly while improving performance by a) leveraging learning rate schedulers for efficient and faster convergence b) optimizing hop length and c) tuning the interpolation scale factors for better audio quality. Additionally, we explore the latent space representation for Indian languages such as Tamil and Bengali for the acoustic unit discovery and voice conversion task. Our approach leverages a quantized encoder architecture, in conjunction with a vocoder which utilizes the proposed mixture of optimized hop length, tuned interpolation scale factors and a cyclic learning rate scheduler. We obtain consistently good results across English, Tamil and Bengali datasets. The proposed method excels in capturing complex linguistic patterns, resulting in clear reconstructed audio during voice conversion with significantly reduced training time.", "sections": [{"title": "I. INTRODUCTION", "content": "The availability of text data for low-resource languages has always been a challenge and transfer learning from multilingual models has its own limitations. End-to-End spoken systems without involving text have received significant attention in the recent years. The Zero-Resource challenge (ZRC) [1] has enabled addressing the low-resource language representation problem and has been a significant driver in this area. In the acoustic unit discovery task for ZRC, high-dimensional input speech data is mapped to its latent representation to capture high-level information while discarding the low-level information. [2]. These latent representations are then used as input for downstream tasks, such as the vocoder in a voice conversion task. Extensive research has been carried out on the self-supervised methods for its improved latent representation. Numerous architectures and its variations are proposed, ranging from small architectures like Contrastive Predictive Coding (CPC) [3] to large transformer based architectures like Wav2Vec [4] and HuBERT [5]. Similarly, in downstream voice cloning tasks, various architectures like Wavenet [6], GANs [7] and simple RNN/ LSTM architecture [8] have been proposed and investigated. However even the most lightweight architectures require substantial GPU resources. Our focus is on enabling Zero-Resource Challenge namely Textless NLP with low resource compute. We show that we can reduce the training time as well as the required compute resources significantly with our proposed approach while maintaining similar or an improved performance. Our major contributions are first, we leverage an appropriately tuned One-Cycle Learning Rate (OCLR) scheduler [9] to reduce the training time-we observe a significant reduction up to 80% in training steps, thereby accelerating training while maintaining the quality of the reconstructed signal. Then, we improve the performance by further optimizing the hop length and tuning the interpolation scale factors to enhance the audio quality. We also apply this proposed approach on Indian language datasets, specifically Tamil and Bengali. Our experiments demonstrate that having a Vector-Quantized CPC as an encoder followed by an LSTM-based vocoder, incorporating the proposed approach, effectively enables textless NLP tasks with low resource compute across multiple languages."}, {"title": "II. SYSTEM MODEL", "content": "We leverage the Vector-Quantized Contrastive Predictive Coding (VQ-CPC) [8] as the encoder in our speech processing pipeline. The input audio files are preprocessed and extracted as log-Mel spectrograms. The initial processing involves convolution and normalization layers to extract high-level features. These features are then passed through an auto-regressive network, which predicts future representations of the input based on past information. One of the key characteristics of VQ-CPC is its use of vector quantization as a bottleneck to discretize the continuous embeddings extracted by the autoregressive network into a finite set of discrete codes. This discretization is achieved by mapping each continuous embedding to the nearest entry in a codebook of fixed-size vectors. Compared to more complex architectures like Wav2Vec and HuBert, which utilize transformers in place of the autoregressive component, VQ-CPC offers a simpler architecture. This simplicity contributes to its efficiency and effectiveness, particularly in low-resource language scenarios. For the vocoder, we use a lightweight LSTM-based model. Despite being lightweight in comparison to a Wavenet Autoencoder [6] or HifiGAN [7], it still requires significant training time."}, {"title": "III. PROPOSED APPROACH", "content": "Typically for NLP applications, it requires training for longer duration and hence the Zero-Resource challenge is even more computationally intensive when one is restricted by low compute GPU resources. The vocoder [8] which is trained for 160k steps on a single T4 GPU instance takes 28 hours of training time. In this work, we try to address this issue of long training time using our proposed combination of learning rate schedulers, smart interpolators, and optimized hop lengths."}, {"title": "A. Learning Rate Scheduler", "content": "The baseline architecture is a lightweight RNN-vocoder trained with a multi-step learning rate scheduler. [8]. In this approach, the learning rate was kept constant for the first 50k steps at 4e-4 and later halved every 25k steps until 160k steps. We observed that loss convergence occurred around 40k steps, but the step decay at constant intervals needed more training steps and that led to inefficient use of resources. Since we wanted to reduce the training time, we resorted to utilize learning rate schedulers which have a cyclic behavior i.e. these schedulers vary the learning rate in a dynamic and non-monotonic fashion. Cyclic learning rate schedulers [10], and cosine with warm restarts [11] are used extensively in image processing to speed up convergence.\nWe used the OCLR approach [9], where the learning rate starts at a minimum, attains maximum and returns to the minimum during the first few training steps, followed by a decay phase, where the learning rate continues to decay for the rest of the training steps. This phasic training with higher learning rates in the early stages of training allows the model to escape from local minima and explore the parameter space more effectively. Meanwhile, using lower learning rates towards the end of training helped the model converge to a more optimal solution. Determining the base and the maximum learning rate for the cyclic phase is crucial. Using a cyclic learning rate scheduler eliminated the need for an extensive parameter tuning, instead we used a simple Learning Rate Range Test [10] as a pre-run to the actual training. We performed this test for a very short period starting from a small learning rate value and gradually increasing the learning rate over a few epochs while monitoring the training loss. To understand, how the loss changes as the learning rate is increased at different step rates, we performed three experiments having three step rates 1, 5 and 50. We observed the loss formed a plateau initially, followed by descent and explosion. We identified the peak learning rate for the one-cycle as 4e-3 from our LRRT experiments. This also helped us identify the optimal learning rate range for our dataset.\nThe baseline architecture is RNN-based and suffers from exploding gradient issues, hence we replaced it with a LSTM block. This is required since the RNN-based vocoder is quite sensitive and integrating it with OCLR leads to exploding gradient issues. This lightweight architecture when trained with OCLR scheduler, performed well on English and Indian languages Tamil and Bengali. We obtained the desired reconstruction quality in just 30k training steps."}, {"title": "B. Tuning Interpolation", "content": "To further improve the quality of the reconstructed output, we experimented with interpolation methods and scale factors in the vocoder. From Fig. 1, one can observe that there are two up-sampling blocks: one before and one after the LSTM block. The first interpolation has an upsampling scale factor of 2 while the second one has an upsampling scale factor of 160. From a signal processing perspective, this is highly imbalanced. To address this, we try to introduce balanced scale factors of 16 and 20, respectively. This simple idea led to a significant improvement in error rates and audio quality metrics. (See Table II for interpolation results for English datasets. See Table III and Table IV for Tamil and Bengali results.) This balanced scale factor contributes to smoother transitions and a more stable speaker's voice. We also experimented with different interpolation methods. The nearest-neighbor method uses the closest known data point to interpolate, while the linear method estimates values between adjacent data points, assuming a straight line between them. Although we hypothesized that the linear method might yield better results, we did not observe any significant performance improvements.\nFurther, we recognized that the existing upsampling methods operate solely in the spatial domain. The recent work in [12] explores using Discrete Fourier Transform (DFT) to transfer the spatial features to the Fourier domain enabling the spatial-Fourier interaction in the context of general architectures. This method does not replace spatial upsampling but complements it. We implemented tiling and periodic padding in the Fourier domain for our architecture (See Fig 1.) The original implementation [12] was designed for images (2D), but since we work with signals, we adapted it using 1D convolutions. This adaptation led to a noticeable improvement in PSNR (Peak Signal-to-Noise Ratio). The results are shown in the Table II."}, {"title": "C. Hop length and number of frames", "content": "While working on the Fourier upsampling approach, we noticed that using a shorter hop length combined with more sample frames during training yielded better results. A shorter hop length and more sample frames provided more context per batch, improving the training outcomes. We used OCLR to bring down the training time from 28 hours (baseline: 160k steps) to (6,8,12) hours for (30k, 40k, 60k) steps. However, after tuning the hop length, the training time increased to (10,12,19) hours for the same step counts. This increase in training time is relatively less compared to our baseline. Moreover, these changes in the input signal contributed to significantly better evaluation scores (as seen in Table II) and clearer audio output.\nIn summary, our proposed method leverages signal processing techniques, exploiting them for improved performance. Our method is neither context nor language dependent and we show this by extending our experiments to three very different languages - English, Tamil and Bengali. Note: Tamil and Bengali are low-resource Indian languages in the context of NLP."}, {"title": "IV. RESULTS", "content": "For English, our experiments were conducted on ZRC-2019 and ZRC-2017 datasets. The results shown in Table I are based on ZRC-2019. This dataset includes 102 speakers and consists of 9k train files and 193 test files, amounting to approximately 22 hours of train and test data. The audio files are divided into smaller chunks to enable faster processing. The log-Mel spectrograms are generated and used as input to both the encoder and the vocoder. For Tamil speech corpus IISc-MILE Tamil ASR [13], [14], we used a collection of randomly chosen sentences spoken by 112 speakers. We used 37 hours of speech data, with an average duration of speakers varying around 6 seconds per file. In our experiments for Bengali corpus [15], we use approximately 40 hours of data collected from 500 speakers. The dataset consists of randomly chosen sentences spoken by various speakers to generalize the linguistic content and speaking styles.\nFor the voice conversion task, we added two voice samples, duration of an hour which was used as the target speaker's voice for conversion. Similar to [8], the speech signals in the three datasets are resampled to 24 kHz. We extract 80-dimensional Mel-spectrogram features using a 40 ms Hanning window, a 12.5 ms frame shift, a 1024-point FFT, and lower and upper frequency cutoffs of 0 Hz and 12 kHz, respectively. The resulting Mel-spectrogram features are then subjected to log dynamic range compression, followed by min-max normalization."}, {"title": "A. Evaluation Metrics", "content": "For encoder, ABX discrimination tests [16] and bitrate [17] were used to evaluate the discovered acoustic latent units. To evaluate the vocoder, we measured the character error rate (CER) and phoneme error rate (PER). In addition, to measure the audio quality [18], we used the structural similarity index measure (SSIM), log-Mel spectrogram mean squared error (LS-MSE), and Peak Signal-to-Noise Ratio (PSNR)."}, {"title": "B. Results", "content": "Table I shows the experimental results for LR scheduler. Our model's switch from the multi-step learning rate scheduler to OCLR (Proposed) gave us significantly better performance. With this change, the number of training steps was drastically reduced from 160k to 30k, or a significant 80% drop. This resulted in a considerable reduction in the total training duration and a reduction in error rates too. We further extended the training to 60k steps and saw improved performance.\nTable II shows the Vocoder experiments for Interpolation and Hop length performed on ZRC 2019 dataset. We assessed the various interpolation techniques within our learning rate scheduler model aimed at mitigating shrillness, enhancing the speaker's voice stability and improving the overall audio quality. We identified notable improvements in performance associated with interpolation methods. Changing the scale factors from 2:160 to 16:20 improved the performance. Adding datasets.\nFourier interpolation, further improved the performance. Finally, bringing down the hop length to 80 and setting frames at 102 gave even better outcomes. For the hop length experiment, we adjusted the interpolation scale factor to 10:16 to balance the dimensions of the neural network.\nTable III and Table IV shows the vocoder experiments on Interpolation and Hop length for Tamil and Bengali. We used a fine-tuned version of wav2vec2 [19] to calculate WER and PER scores for Tamil and Bengali to obtain transcriptions.1 The availability of speech data for these low-resource Indian languages has allowed us to address the lack of sufficient text. In addition, using the proposed tuning combination, the model shows significant results for these languages. Table V shows the ABX results for Encoder run on Tamil and Bengali"}, {"title": "V. CONCLUSION", "content": "In summary, we show that one can achieve very promising results for textless NLP, even with limited computational time and resource by using a combination of a) smart interpolation, b) tuned hop length and c) appropriate learning rate scheduler. Our approach not only reduces training time but also enhances audio quality, with successful outcomes observed not just for English but also for low-resource Indian languages like Tamil and Bengali. This study also opens the door to re-looking at the various blocks of neural architecture for speech from a signal processing perspective in terms of upsampling, downsampling interpolation etc., and the possibility of obtaining improved performance with simple changes in these blocks. Additionally, the method proposed here, to reduce the training time can also be incorporated into more compute-heavy architectures which could be worth investigating in future work."}]}