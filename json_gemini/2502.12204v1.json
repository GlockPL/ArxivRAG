{"title": "Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration", "authors": ["Xianbing Zhao", "Yiqing Lyu", "Di Wang", "Buzhou Tang"], "abstract": "Automatic depression detection provides cues for early clinical intervention by clinicians. Clinical interviews for depression detection involve dialogues centered around multiple themes. Existing studies primarily design end-to-end neural network models to capture the hierarchical structure of clinical interview dialogues. However, these methods exhibit defects in modeling the thematic content of clinical interviews: 1) they fail to explicitly capture intra-theme and inter-theme correlation, and 2) they do not allow clinicians to intervene and focus on themes of interest. To address these issues, this paper introduces an interactive depression detection framework. This framework leverages in-context learning techniques to identify themes in clinical interviews and then models both intra-theme and inter-theme correlation. Additionally, it employs AI-driven feedback to simulate the interests of clinicians, enabling interactive adjustment of theme importance. PDIMC achieves absolute improvements of 35% and 12% compared to the state-of- the-art on the depression detection dataset DAIC-WOZ, which demonstrate the effectiveness of modeling theme correlation and incorporating interactive external feedback.", "sections": [{"title": "Introduction", "content": "Depression stands as one of the primary factors af-fecting individuals' mental health (Wei et al., 2022), exerting negative impacts on their work and daily life through its influence on cognitive processes and behavioral patterns. Statistics indicate a year-on-year increase in the prevalence of depressive symptoms within populations in both China and the United States (Rinaldi et al., 2020), a trend that con-tinues to escalate with advancements in detection methods. However, it remains a challenging task as it requires accurately capturing the relationships across hundreds of turns in clinical interview dia-logues and predicting the depressive state (Mallol-Ragolta et al., 2019; Yang et al., 2024; Yao et al., 2024). Specifically, clinical interview dialogues are structured around multiple themes, such as family, work, and medical history. These themes exhibit inter-dependencies, and their correlations with depressive states vary in strength. As illustrated in Figure 1, clinical interview dialogues begin with natural dialogues and then evolve around multiple themes (Gratch et al., 2014; Rinaldi et al., 2020), which are closely related to the depressive state of participant.\nIn recent years, significant efforts have been made to address the aforementioned challenges by modeling the hierarchical structure of clinical interview dialogues. Based on the type of hierarchi-cal neural network employed, existing approaches can be broadly classified into two categories: 1) modeling the hierarchical dependencies in multi-turn question-answering (Wu et al., 2023; Anshul et al., 2024; Zhang et al., 2024). To facilitate the understanding of complex clinical interview dia-logues, this approach focuses on capturing the cor-relation between individual question-answer pairs as well as the sequential dependencies across mul-tiple turns. 2) capturing implicit themes (Gong and Poellabauer, 2017; Rinaldi et al., 2020). This ap-proach relies solely on learnable parameters, mod-eling each question-answer pair's category as an implicit theme. To smoothly introduce themes, clin-ical interview dialogues often include dialogues that are unrelated to the depressive state. The for-mer approach tends to trap the model in trivial details, while the latter fails to explicitly identify theme content. Moreover, neither of these methods allows clinicians to intervene in the model to focus on themes of interest.\nTo tackle these downsides, we introduce a novel interactive depression detection framework, namely Predicting Depression in Screening Inter-views from Interactive Multi-Theme Collaboration (PDIMC), which is the first depression detection framework incorporating explicit theme correlation learning and external feedback intervention. As illustrated in Figure 1, clinical interview dialogues revolve around themes such as family, work, men-tal, and medical. To provide a more comprehen-sive evaluation, we additionally introduce a virtual theme overall. Based on the above observations, we first design a theme-oriented in-context learning (TICL) module to extract theme content from com-plex clinical interview dialogues, preventing the model from being overwhelmed by trivial details. To capture both intra-theme and inter-theme cor-relation, we develop a theme correlation learning (TCL) module. Subsequently, we introduce a in-teractive theme adjustment strategy (ITAS), which leverages the large language model (LLM) to sim-ulate clinician feedback, emphasizing key informa-tion from the feedback to further adjust the impor-tance of different themes. Extensive experimental results on the well-known clinical interview depres-sion dataset DAIC-WOZ demonstrate the effective-ness and superiority of our approach. Our main contributions are threefold:\n\u2022 We introduce an interactive depression detec-tion framework for automated depression de-tection. To the best of our knowledge, this is the first framework that explores interactive depression detection using clinical interview data.\n\u2022 We introduce a theme-oriented in-context learning technique to extract themes from clin-ical interview dialogues and design a theme correlation learning module to model both intra-theme and inter-theme correlation.\n\u2022 We propose an interactive theme adjustment strategy, which leverages the LLM to simu-late clinician feedback, dynamically adjusting the importance of theme. This enables the model to focus on clinician-preferred themes for more effective depression detection."}, {"title": "Related Work", "content": "2.1 In-context Learning\nIn-context Learning (ICL) (Brown et al., 2020; Dong et al., 2024) is a technique that uses few-shot in-context learning samples to guide the pre-traind autoregressive LLM to produce satisfying results, without additional training or fine-tuning. The in-context learning samples are usually specifically designed for designated downstream task and can serve as auxiliary parameters attached to the model to guide the generation process. Liu et al. (2022) ex-plored the way to better design the context sample through distance metrics, and impact of different kinds of the distance metrics like Euclidean dis-tance and so on. Levy et al. (2023) proposed a method to reinforce generalization ability through sample diversification. Chung et al. (2022) intro-duced a perplexity based method for designing in-context samples. Sorensen et al. (2022) conducted cross-lingual contextal learning experiments using clustering methods. Tanwar et al. (2023) used ICL techniques to perform the alignment task of differ-ent languages.\n2.2 Automatic Depression Detection\nPlenty of works have been dedicated to automating the process of depression detection by implement-ing methods like natural language processing, ma-chine learning, multimodal model LLM. To start with, researchers used traditional methods to tackle with the issue. For instance, Abdurrahim and Fud-holi (2024) introduced a Convolutional Neural Net-work (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) based deep learning model to perform depression detecting tasks upon social me-dia posts. Cai et al. (2023) and Misgar and Bhatia (2022) have posed time series based LSTM to de-tect suicide risk. Meanwhile, there are also some works (e.g. Wang et al., 2024; Yao et al., 2024; Ying et al., 2024; Dai et al., 2021), combining fea-ture engineering with machine learning algorithms and deep neural network methods for diagnosing"}, {"title": "METHODOLOGY", "content": "Our goal is to learn multiple themes $T_i$, $i \\in {family,work,mental,medical,overall}$ from multi-turn clinical interview dialogues S, then model both intra-theme and inter-theme correlation ($X^{inter}$, $X^{intra}$, $i \\in D$). Additionally, we intro-duce feedback simulated by the LLM to imitate feedback of clinician, enabling preference learn-ing to adjust themes. Finally, we fuse the adjusted themes $X^{fd}_i$, $i \\in D$ to obtain final representation $X^{final}$ for depression prediction $\\hat{y}$. In this sec-tion, we introduce each component of the proposed model, as illustrated in Figure 2. Specifically, we first present the theme-oriented in-context learn-ing module in Section 3.1, which extracts themes from clinical interview dialogues. Afterwards, we introduce the theme correlation learning module, which captures both intra-theme and inter-theme correlation in Section 3.2. Finally, we describe the interactive theme adjustment strategy in Section 3.3, which enables the model to focus on clinician-preferred themes for depression detection."}, {"title": "Theme-oriented In-context Learning", "content": "Clinical interview dialogues are structured around multiple themes, encompassing both thematic con-tent and trivial details. The thematic content is closely related to the depressive state. To effec-tively capture these theme-related contents, we design a theme-oriented in-context learning mod-ule. This module leverages in-context learning techniques and LLM to extract depression-related themes while discarding irrelevant details. The in-context learning technique guides the LLM to generate output text $y_i$ based on the in-context tem-plate I and user input sequence X. Formally, this process can be represented as:\n$P(y_j|S, I) \\equiv p_\\theta(X, I)$,\n$\\hat{y}_j = argmaxP(y_j|S, I)$,\nwhere P represents the token probabilities and $\\hat{y}_i$ denotes the token with the highest probability. The operation of the theme-oriented in-context learning technique can be formally summarized as:\n$T_i = p_\\theta(S, I), \\ \u0456 \\in D$,\n$D = {family, work, mental, medical, overall}$,\nwhere the LLM p utilizes in-context prompt I and model parameters $\\theta$ to extract theme content $T_i$ from the clinical interview dialogues, filtering out trivial details and preserving information relevant to the depressive state. The in-context template, as illustrated in Figure 2, consists of theme content (family, work, mental, medical, and overall) along with system prompts."}, {"title": "Theme Correlation Learning", "content": "To fully leverage the advantages of themes, we model both intra-theme and inter-theme correlation. The intra-theme correlation aims to capture how key tokens within a theme influence the depressive state, while the inter-theme correlation focuses on learning how semantic relationships across themes contribute to depression assessment. This module enhances the model's ability to highlight key se-mantics within a theme while also capturing seman-tic dependencies across themes. Figure 2 illustrates the proposed learning framework, demonstrating how these correlations contribute to a more compre-hensive depression detection process. We employ the attention mechanism to learn both intra-theme and inter-theme correlation. Formally,\n$X^* = f_{corr}(X), * \\in {intra, inter}$,\nwhere $f_{corr}(\\cdot)$ represents the correlation function with parameter $\\phi$, while X and $X^*$ denote the input before correlation and the output after correlation, respectively. We first calculate the correlation ma-trix A. The intra-theme correlation matrix is com-puted by calculating the token similarity scores, while the inter-theme correlation matrix is deter-mined by calculating the semantic similarity across themes. Formally,\n$A(X) = softmax(\\frac{XW_QW_K^TX^T}{\\sqrt{d}})$, \nThe computational operation of the correlation function $f_{corr}(\\cdot)$ is as follows,\n$f_{corr}(X) = A(X) \\cdot X \\cdot W_V$,\nwhere A(\u00b7) represents the correlation matrix func-tion, while $W_{{Q,K,V}}$ denotes the learnable param-eter matrix of $\\phi$. We first use the pre-trained large language model $p_\\theta$ to extract the features $X_i$ of the i-th theme $T_i$. Formally,\n$X_i = p_\\theta(T_i) \\in R^{L_i\\times d}, i \\in D$,\nThe form of intra-theme correlation learning $X^{inter}$ is as follows,\n$X^{intra} = f_{correr}(X_i) \\in R^{L_i\\times d}$,\nwhere $L_i$ represents the sequence length of the i-th theme, and d denotes the feature dimension. The $\\phi_{inter}$ denotes the learnable parameter matrix. Af-ter learning the token correlation semantics within the theme, we concatenate multiple themes along the sequence dimension. Formally,\n$X^{inter} = Concat\\{x^{inter}_i\\}_{D} \\in R^{(\\sum L_i)\\times d}$\nimmediately, we utilize the correlation learning function $f_{corr}()$ to learn the semantic of inter-theme correlation. Formally,\n$X^{intra} = f_{corr}(X^{inter}) \\in R^{(\\sum L_i)\\times d}$,\nwhere $\\phi_{intra}$ denotes the learnable parameter ma-trix. The inter-theme and intra-theme respectively highlight the importance of tokens within a theme and the importance of the theme itself."}, {"title": "Interactive Theme Adjustment Strategy", "content": "To address the issue that depression detection mod-els cannot incorporate feedback from clinician, we have designed a customized interactive theme ad-justment strategy. This strategy introduces simu-lated clinician feedback into the depression detec-tion model, enabling the model to focus on the parts of interest to clinician. When incorporating exter-nal feedback information containing categories c to constrain the model output in a generative model (Dhariwal and Nichol, 2021; Sanchez et al., 2024), the model probability likelihood $P_\\theta$ consists of the model prediction $P_\\theta$ and the prediction of the feed-back information $P_\\phi$, resulting in the approximated modified distribution,\n$P_\\theta(x|c) \\propto P_\\theta(x) \\cdot P_\\phi(c|x)$,\nwhere $\\gamma$ represents the constraint strength, which controls the model's degree of focus on the con-straint. By removing the auxiliary classification task without classifier guidance using Bayes rule, the same model $P_\\theta$ simultaneously supports both conditional and unconditional predictions to re-formulate Equation (12) as: $P_\\theta(x|c) \\propto \\frac{P_\\theta(x)}{P_\\theta}$. The sampling process of classifier-free guidance can be reformulated as:\n$P_\\theta(x|c) \\propto \\frac{P_\\theta(x|c)}{P_\\theta(x)^{(\\gamma-1)}}$',\nTaking the logarithm of both sides of Equation (13) results in the following form,\n$logP(x|c) = logP_\\theta(x|c) - (\\gamma - 1)logP_\\theta(x)$,\nlet $P(x) = (1 - \\gamma)logP_\\theta(x)$, $\\beta = \\frac{\\gamma}{(1-\\gamma)}$, the probability distribution of the model's prediction can be rewritten as follows,\n$P(x) = \\beta logP_\\theta(x|c) + logP_\\theta(x)$,\nwhen introducing conditional constraints into the model prediction, the model's prediction is com-posed of the superposition of the prediction from the vanilla information and the prediction from the conditional constraint information.\nBased on the above theory, we integrate the classifier-free feedback of clinician on the theme with the vanilla theme information to diagnose de-pression. Formally,\n$X^{fd}_Id = X^{intra} + w^{d}_i X^{intra}, X^{fd} \\in R^{L_i\\times d}$,\nThe integration results for each theme are as fol-lows:\n$X^{final} = \\sum X^{fd}, i \\in D, w^d \\in W^{fd}$,\nwhere $w^{fd}$ represents the weight of the simulated clinical feedback from the LLM. Finally, we use the Softmax function to normalize the weight of (1 + $W^{fd}$) and integrate the weights of multiple themes for depression detection."}, {"title": "Depression Detection Layer", "content": "The final fused representation $X^{final}$ is passed through several fully connected layers and activa-tion functions to obtain the prediction result $\\hat{y}$. We compute the cross-entropy loss between the pre-dicted value $\\hat{y}$ and the ground truth y to optimize the model:\n$L_{CE} = - \\sum y_i log(\\hat{y}_i) + (1 - y_i) log(1 - \\hat{y}_i)$\nwhere N denotes the number of training samples."}, {"title": "Experiments", "content": "4.1 Experimental Settings\nDatasets and Evaluation Metrics. DAIC-WOZ (Gratch et al., 2014) is a clinical interview dialogue dataset which is collected and released by the Uni-versity of Southern California to help veterans back to civilian life. For metrics, we have conducted a series of experiments along with previous state-of-the-art hierarchical dialogue models (Jung et al., 2024) upon the indices of Accuracy, Precision, Re-call, F1-score and G-means, and we also report the Precision, Recall, F1-score of Weighted Average version (denoted as WA*Prec., WA*Rec., WA*F1 in Table 1). Meanwhile, to compare against meth-ods of latent theme, we reported F1 score of de-pression and non-depression on test set, following the previous work (Rinaldi et al., 2020) (shown in Table 2).\nImplementation Details. We implemented PDIMC on Nvidia A100 GPU. Applying PyTorch library, our Adam Optimizer is configured with batch size of 32, learning rate of 10-5, and training epochs of 80. For pretrained large language model, we employed Qwen2.5, an open source LLM, to extract text feature."}, {"title": "Performance Comparison", "content": "To demonstrate the effectiveness of our poposed PDIMC, we compared our proposed methods with several baselines. These methods are hierarchical dialogue models and latent theme models respec-tively. The hierarchical dialogue models includes: TFN (Zadeh et al., 2017), BiLSTM-1DCNN (Lin et al., 2020), MulT (Tsai et al., 2019), MISA (Hazarika et al., 2020), Depression Vlog (D-vlog) (Yoon et al., 2022), BC-LSTM (Poria et al., 2017), ERSDL (Satt et al., 2017), ATSM (Hanai et al., 2018), TopicModel (Gong and Poellabauer, 2017), CADL (Lam et al., 2019), Speechformer (Chen et al., 2022), GRU/BiLSTM (Shen et al., 2022), Hi-erarchical Question Embedding Network (HiQuE) (Jung et al., 2024). The latent theme models include: PR, BERT, JLPC, JLPCPost (all reported in Rinaldi et al., 2020).\nThe comparison results are summarized in Table 1 and 2. By comparing the results, we could draw the following conclusions. 1) Compared to hier-archical dialogue models, our proposed method PDIMC shows remarkable improvement on all metrics. Among these baseline models, HiQuE (Jung et al., 2024) achieved the best performance in nearly every metric, except weighted average preci-sion (WA*Prec.). For macro metrics, our proposed method achieved 11% better in Precision and F1-Score, 12% better in Recall, compared to HiQuE. 2) Compared to model of latent theme JLPC(Rinaldi et al., 2020), our model PDIMC of explicit theme achieved a 35% improvement in the F1 score of depression. The experimental results indicate that explicit themes provide more clues related to de-pressive states than implicit themes. This result suggests that the hierarchical dialogue model strug-gles to learn information related to depressive states when modeling hundreds of clinical interview di-alogues. It further underscores the importance of learning themes and incorporating clinical feed-back."}, {"title": "Ablation Study", "content": "To assess the effectiveness of each module we pro-posed, we conducted a series of decremental ab-lation study, removing one component once. The results are displayed in Table 3, and we are giving a specific explanation. 1) Without Single Theme. In this section we alternately removed each theme and use the rest four themes to perform predic-tion. According to the results, removing any theme would cause a performance drop, from 5% drop on WA*F1 of work theme to 11% of mental theme and overall theme. This demonstrated the irreplace-ability of each theme, as they can maximize the uti-lization of the input text and collaboratively predict depression predisposition. 2) Without TCL Mod-ule. We removed the Theme Correlation Learning module, which means self-attention mechanism is disabled. From the result, we can see that the performance suffers the most performance decline, about 15% on WA*F1. This manifested the va-lidity of self-attention mechanism which learns correlation both intra-theme and inter-theme and reinforces the accuracy of model predictions. 3) Without ITAS Module. This variant removes the Interactive Theme Adjustment Strategy module, and this means no discrepancy of weights would be applied during the multi-theme fusion process. Evidently, the 11% drop on WA*F1 indicated the indispensability of considering the prioritization of each theme when fusing them together. In gen-eral, the complete version of our model PDIMC outperformed the others with module removed in depression detection tasks. This result verify the effectiveness and complementary of the three com-ponents."}, {"title": "Theme-oriented In-context Learning Analysis", "content": "Apart from achieving the superior performance, the key advantage of our model over other approaches is its ability to learn thematic content from com-plex clinical dialogues duo to theme-oriented in-context learning. To this end, we carried out ex-periments to explore the ability of the module to extract thematic descriptions from the transcript text of the interview based on the in-context sam-ples. Figure 3 illustrates the themes extracted from hundreds of dialogue rounds, as well as the re-sults of simulated clinical feedback generated by the large language model. By examining Figure 3, we can observe that the theme-oriented in-context learning technique accurately captures content re-lated to family, work, mental health, and medical themes within the dialogue. Furthermore, the feed-back simulated by LLM aligns well with common knowledge (for more details, refer to Section 4.6). This result proved the capability of TICL module to extract valid thematic information from complex dialogues."}, {"title": "Theme Correlation Learning Analysis", "content": "To qualitatively validate the effectiveness of the TCL module to capture correlation both intra-theme and inter-theme, we conducted experiments and visualized the map of attention distributions. Figure 4 displays the token weights of intra-theme correlation distribution. From Figure 4, we could observe the token distribution within the \"Mental\" them and the corresponding atten-tion weights. Some tokens, such as \"diagnosed,\" \"depression\",\"extremely down\" and \"low mood,\" show a clear depressive tendency, and their corre-sponding attention weights are significantly higher than those of other tokens. This result indicates that intra-theme correlation learning can highlight token semantic information related to depressive states, thereby enhancing the performance of the depression detection model.\nTo gain the deep insights into our proposed inter-theme of theme correlation learning, we analyzed the weights of several inter-theme correlation. We obtained the following observations: 1) Themes with consistent depressive semantics have higher inter-theme correlation weights, while themes with inconsistent depressive semantics have lower cor-relation weights. For example, the highest simi-larity score among themes with consistent depres-sive semantics is 0.951, whereas the lowest sim-ilarity score among themes with inconsistent de-pressive semantics is 0.204. 2)The average weight of themes with consistent depressive semantics is higher than that of themes with inconsistent depres-sive semantics (0.742 vs. 0.525). 3)The inter-theme correlation weights are highly correlated with and accurately reflect depressive semantic information. These results indicate that inter-theme correlation learning effectively captures cross-theme semantic correlations, aiding the model's decision-making process and ultimately improving prediction accu-racy."}, {"title": "Interactive Theme Adjustment Strategy Analysis", "content": "To validate the effectiveness of the interactive theme adjustment strategy, we compared the weights obtained through inter-theme correlation learning with those learned through the interac-tive theme adjustment strategy. Specifically, we first used the LLM to simulate clinical doctor feed-back by scoring each theme, and then converted the scores into weights to intervene in the model's prediction. From Figure 6, we can observe that the LLM's scoring of theme importance is highly intu-itive, such as the \"Mental\" theme receiving a high weight due to its direct correlation with depressive states. This result indicates that using the LLM to simulate clinician feedback is accurate. From Figure 6, we also could see that inter-theme correlation learning could, to some extent, capture semantic information related to depression, such as the higher weight of the \"Mental\" theme compared to other themes. However, the distinc-tion is still not sufficiently clear, which may lead to incorrect predictions. After applying the inter-active theme adjustment strategy, the weight of the \"Mental\" theme becomes significantly higher than that of other themes, leading to the correct pre-diction. This is because, during the multi-theme fusion phase, the distinction between themes is not clear enough, reducing the influence of key themes on the prediction results. These results demon-strate the importance of introducing simulated clin-ician feedback through the LLM, as it highlights key theme information to guide the model towards making accurate predictions."}, {"title": "Conclusion", "content": "In this paper, we proposed a novel interactive de-pression detection framework. The framework in-cludes three components: 1) The theme-oriented in-context learning module learns interview theme-related information from multi-turn dialogues. 2) The theme correlation learning module mines cor-relation of inter-theme and intra-theme. 3) The interactive theme adjustment strategy introduces clinical feedback simulated by the LLM to guide the model in aligning with the preferences indi-cated by external feedback. Extensive experiments demonstrate the effectiveness and superiority of our proposed model. It not only achieves accurate depression diagnosis but also allows for clinical in-tervention, enabling integration of expert feedback into the diagnostic process."}, {"title": "Limitations", "content": "There are two limitations in this study. First, al-though our approach achieves outstanding perfor-mance on depression detection datasets, it only uti-lizes textual data from clinical interview dialogues without incorporating multimodal information. In-tegrating multimodal data could potentially further enhance the model's performance. Second, in de-signing the interactive depression detection frame-work, we use the large language model to simulate clinical feedback as external guidance. However, the professional reliability of this simulated feed-back may require further evaluation."}]}