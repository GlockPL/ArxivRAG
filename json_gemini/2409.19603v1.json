{"title": "One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos", "authors": ["Zechen Bai", "Tong He", "Haiyang Mei", "Pichao Wang", "Ziteng Gao", "Joya Chen", "Lei Liu", "Zheng Zhang", "Mike Zheng Shou"], "abstract": "We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.", "sections": [{"title": "1 Introduction", "content": "We live in a dynamic world. Localizing objects of interest in videos according to human intent is a crucial task for intelligent models and systems. Language, as a natural interface, serves as the primary reference for identifying target objects. However, language expressions vary widely across different scenarios, presenting varying levels of difficulty. While category names are straightforward references, detailed text descriptions from tasks like referring segmentation [27, 46, 56] introduce greater complexity. In real-world applications, these expressions can be more complex, involving intent understanding, reasoning, and world knowledge, making them more user-friendly yet significantly more challenging for models to understand and act upon.\nRecent advancements in the image domain have shown progress in language-instructed reasoning for detection and segmentation tasks. Models leveraging multimodal large language models (MLLMs), such as those in DetGPT [51] and LISA [30], have demonstrated the ability to localize target objects by harnessing the implicit reasoning capabilities and world knowledge embedded in large language models (LLMs). However, these advancements have not seamlessly translated to video tasks, particularly video object segmentation (VOS). The primary challenge in VOS stems from the additional temporal dimension, which introduces complexities absent in static images. VOS requires"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Video Object Segmentation", "content": "In computer vision, video object segmentation is a well-studied task [65]. Specifically, referring video object segmentation (RVOS) aims to segment the target object mentioned in a natural language expression in a video [56, 8, 60, 66, 34, 61, 47]. Compared with image segmentation, RVOS is more challenging since both the action and appearance of the referred object must be segmented in a video. Gavrilyuk et al. (2018) were the first to propose the RVOS task and the A2D-Sentences benchmark [23]. This field continues to evolve with new benchmarks emerge such as Ref-DAVIS-17 [28], Ref-YouTube-VOS [56], and MeViS [14]. Many previous studies have primarily adapted referring image segmentation approaches for frame-by-frame object segmentation. For example, URVOS [56] and RefVOS [6] utilize cross-modal attention for per-frame segmentation. Some recent works, such as ReferFormer [61] and MTTR [8], employ a DETR-like structure, which simplifies the referring pipeline and achieves impressive performance. R2VOS [34] enhances multi-modal alignment through text reconstruction. OnlineRefer [60] proposes an online model with explicit query propagation. SgMg [47] proposes a segment-and-optimize paradigm to solve the feature drift issue. Despite the impressive results achieved by these methods, several challenges remain. First, most existing methods are deficient in comprehending the motion information in videos and languages, as revealed by the recent MeViS [14] benchmark. Second, there are few studies on complex reasoning-based segmentation in the video domain, both methodologically and benchmark-wise."}, {"title": "2.2 Multimodal Large Language Model", "content": "The remarkable advancements of large language models (LLMs) motivate the research community to extend the foundational capabilities of LLMs to the visual domain, leading to multimodal large language models (MLLMs) [67, 4]. The pioneering works of MLLMs, such as LLaVA [39], MiniGPT-4 [71], and InstructBLIP [13], exhibit impressive visual understanding capabilities, including image captioning [57, 3] and visual question answering. When extending into the video domain, a prominent issue is handling the temporal dimension. One straightforward approach is to concatenate the tokens from multiple frames [37], though the temporal length might be limited by computational resources. To address this, one line of work [45, 26, 25] explores pooling (merging) strategies to reduce the number of tokens, such as pooling along the spatial and temporal dimensions separately [45], token merging based on similarity [26], and pooling with different strengths at a slow-fast pace [25]. Another line of work [33, 68] utilizes the Q-former [32] architecture to extract abstracted features, which greatly reduces the number of tokens.\nMore recently, some studies have further integrated region-level image understanding and grounding abilities into MLLMs. Kosmos-2 [50] and Shikra [10] directly quantize bounding boxes into discrete location tokens or numeric representations of positions. GPT4RoI [69] uses a simple pooling operation to extract features within boxes or masks as the region representations. Another line of work leverages the reasoning ability of MLLMs and resorts to off-the-shelf models for localization. For example, DetGPT [51] utilizes a pre-trained LLM and an open-vocabulary object detector to detect the target object based on human intent described in natural language. LISA [30] connects an MLLM and the Segment Anything (SAM) [29] model using a special token to produce fine-grained segmentation masks. Although these works have achieved impressive performance on image tasks, they are still incapable of processing videos. For object segmentation in videos, very few studies have leveraged the reasoning ability of LLMs to overcome current limitations. PG-Video-LLaVA [49] utilizes off-the-shelf object detector and tracker to obtain the target objects first and then match it with the entities mentioned in the generated text. TrackGPT [72] makes a straightforward extension of LISA by iteratively updating the special token with video progresses. However, the absence of video learning module significantly limits its perception and reasoning of temporal dynamics."}, {"title": "3 Method", "content": "The task of language-instructed reasoning segmentation in videos can be formally defined as follows. Given a video Xvid and a language expression Xtxt, the model takes both as input and outputs the pixel-level segmentation masks M for all frames. Xtxt is a free-form text that particularly emphasizes implicit intent reasoning, world knowledge, and video temporal dynamics."}, {"title": "3.1 Architecture", "content": "Fig. 1 illustrates the model architecture. It consists of a visual tokenizer, an LLM, a vision encoder, and a promptable mask decoder. We omit the text tokenizer in the LLM for simplicity. The visual tokenizer and LLM are initialized from LLaVA [39, 53]. The vision encoder and mask decoder are initialized from SAM [29]. Given a video, we first uniformly sample Tsparse frames and encode them into visual tokens via the visual tokenizer, resulting in Tsparse \u00d7 L tokens in total. Ideally, larger Tsparse would be better for capturing temporal dynamics. However, it is prohibitive to let the LLM process such a large number of tokens. Thus, we develop the Sparse Dense Sampling strategy to reduce the number of tokens, which will be elaborated in Sec. 3.2. After that, the visual tokens are concatenated with text tokens and fed into the LLM.\nTo equip the LLM with segmentation capabilities, following previous work [30], we extend the vocabulary of the LLM with a special token <TRK>. During generation, this special token carries rich semantic information from the text prompt and video content, providing signals for decoding pixel-level segmentation masks. Specifically, we extract the last layer embedding corresponding to the <TRK> token and transform it into a prompt embedding with a multi-layer perceptron (MLP). At the same time, the vision encoder extracts per-frame features from the video. Finally, the prompt embedding and the visual features are processed by the mask decoder to produce the segmentation masks. Note that for one video, there is only one prompt embedding that is in charge of all the frames. The One-Token-Seg-All approach will be introduced in Sec. 3.3."}, {"title": "3.2 Sparse Dense Sampling", "content": "Given the Tsparse \u00d7 L tokens, we aim to reduce the number of tokens while preserving enough spatial details and temporal dynamics. Therefore, we further sample Tdense frames out of Tsparse frames. The visual tokens of the Tdense frames are all preserved in full resolution, i.e., dense tokens. Then, we apply global average pooling on the Tsparse frames to reduce them to low resolution, i.e., sparse tokens. In our implementation, each frame is represented by only one token. Finally, the total number of tokens is reduced to Tsparse + Tdense \u00d7 L, which is significantly smaller than Tsparse \u00d7 L. The rationale behind this strategy is the inherent temporal redundancy in video data. By exploiting this, we reduce the computational burden without losing critical information. The dense tokens provide visual details for their adjacent sparse frames, while the sparse tokens capture the temporal dynamics for the dense frames. In Sec. 2.2, we have discussed several popular temporal learning strategies in video-LLM. Although they exhibit remarkable performance in general video understanding tasks, our empirical studies (see Tab. 5) demonstrate that these popular strategies are not seamlessly transferable to video object segmentation. This is likely because they either lose spatial details or temporal information, both of which are essential in dense prediction tasks in videos."}, {"title": "3.3 One Token Seg All", "content": "As shown in Fig. 1, throughout the video, we use a single special <TRK> token for segmenting all the frames. We provide an in-depth analysis of the rationale behind this approach. In our model, the promptable segmentation model is initialized from SAM, in which the decoder takes the prompt"}, {"title": "3.4 Training and Inference", "content": "Training Data. The training data for our model mainly consists of two parts: 1) image segmentation and 2) video segmentation. For the image part, we follow the setting of LISA [30]. For the video data, we employ video object segmentation (VOS) and referring video segmentation data (RVOS). During pre-processing, we fill the original category name or referring expression in the dataset into a template. For example: \u201cUSER: <VIDEO> Can you segment {description} in this scene? ASSISTANT: Sure, it is <TRK>.\", where {description} is the placeholder to fill. For VOS data that contain videos with multi-class labels, we randomly choose one class and merge all the masks belonging to this class into one binary mask. Training Objective. The model is trained end-to-end using the text generation loss Ltxt and segmentation loss Lseg. The segmentation loss"}, {"title": "4 Benchmark", "content": "The versatile abilities of our model can be evaluated using public benchmarks that assess various aspects. RVOS benchmarks [28, 56] evaluate temporal-related abilities, involving referring expression comprehension, video temporal understanding, and temporal consistent segmentation. Complex reasoning abilities can be assessed by the image-based reasoning segmentation benchmark [30]. However, there is still a lack of a benchmark that comprehensively evaluates the reasoning segmentation abilities of videos. Towards this goal, we have organized the ReasonVOS benchmark. Specifically, we annotate language expressions based on the videos and mask annotations from existing datasets, including MOSE [15], MeViS [14], VIPSeg [48], and BURST [2]. The criteria for data collection and annotation processes are as follows. Each language expression should encompass at least one of the following aspects: 1) complex reasoning, 2) world knowledge, 3) temporal dynamics. For the video and mask selection, objects with explicit movement are highly prioritized to evaluate the temporal consistency of masks. As a result, we manually annotated 105 samples as initial seed data. Following previous practices [14, 72], we use a LLM to rephrase the language expressions for augmentation and perform another round of human checking. The resulting ReasonVOS benchmark comprises 458 video-instruction-mask data samples. This benchmark is specifically designed for zero-shot evaluation purposes, as the reasoning ability is embedded in the LLM and can be triggered by existing image-based reasoning segmentation data."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setting", "content": "Datasets Our model is trained on a variety of segmentation datasets. The image-based datasets include 1) semantic segmentation: ADE20K [70], COCO-Stuff [9], PACO-LVIS [52], and PASCAL-Part [11]; 2) referring segmentation: refCLEF, refCOCO, refCOCO+ [27], and refCOCOg [46]; 3) reason segmentation: 239 ReasonSeg samples from LISA [30]. The video-based datasets we use include: 1) semantic VOS: YouTube-VOS [63]; 2) referring VOS: Refer-YouTube-VOS [56] and MeViS [14]. The evaluation benchmarks will be elaborated in the corresponding experiment sections.\nImplementation Details We implement our model with LLaVA-Phi-3-V [53], a multimodal LLM based on Phi-3 [1] with 3.8B parameters. We adopt the vision encoder and mask decoder from SAM [29]. We conduct joint training using both image and video datasets. For video data, we set Tsparse = 32 and Tdense = 4 according to our GPU memory. For image data, we duplicate the images as pseudo video data. We train our model using 64 NVIDIA 24G A10 GPUs with a distributed training script based on DeepSpeed [54]. We use the AdamW [42] optimizer with the learning rate and weight decay set to 0.0003 and 0, respectively. We also adopt WarmupDecayLR as the learning rate scheduler, with the warmup iterations set to 100. The weights of the text generation loss (Atxt) and the mask loss (Aseg) are both set to 1.0. The weights of the BCE loss (Abce) and the DICE loss (Adice) are set to 2.0 and 0.5, respectively. The per-device batch size is set to 2. For ablation studies, the total number of iterations is 3,000 and each experiment takes around 10 hours. For the final model used for comparison, we scale up the training to 6, 000 iterations, which takes 20 hours."}, {"title": "5.2 Evaluation on Video Tasks", "content": ""}, {"title": "5.2.1 Referring Video Object Segmentation", "content": "We adopt two benchmarks of standard referring video object segmentation. Ref-Youtube-VOS is evaluated on the official challenge server 2. Ref-DAVIS-17 is evaluated by the official evaluation code 3. The evaluation results are shown in Tab. 1. Our method demonstrates competitive performance on both benchmarks, achieving comparable or superior results to existing methods. For Refer-DAVIS-17, our method achieves state-of-the-art performance, outperforming all the other methods by a considerable margin. In Refer-YouTube-VOS, our method performs well compared to traditional RVOS methods, achieving a high rank. State-of-the-art methods, such as SgMg [47], achieve remarkable performance, thanks to its dedicated video backbones, such as Video-Swin [41]. However, among LLM-based methods with reasoning ability, our model, despite having only 3.8B parameters, outperforms other methods with much larger LLMs, such as LISA-13B and TrackGPT-13B."}, {"title": "5.2.2 Motion-guided Video Object Segmentation", "content": "We further evaluate our model on motion-guided VOS using the MeViS [14] benchmark. Consistent with previous studies [14, 24], we evaluate our model's performance on the validation set of the MeViS benchmark. The results in Tab. 2 demonstrate that our method achieves state-of-the-art performance in this benchmark, outperforming previous methods by a large margin. We attribute this performance gap to our model's adeptness in capturing temporal dynamics and cross-modal interaction, facilitated by the Sparse Dense Sampling-based temporal module and the One-Token-Seg-All training paradigm."}, {"title": "5.2.3 Reasoning Video Object Segmentation", "content": "In Tab. 3, we compare various methods on the newly organized ReasonVOS benchmark. For traditional VOS methods, the metrics are evaluated using their released checkpoints pre-trained on the Ref-YouTube-VOS dataset. This benchmark focuses on complex reasoning, temporal understanding, and segmentation temporal consistency, which present significant challenges for existing VOS methods and image-based reasoning segmentation methods. It can be observed that most previous methods exhibit unsatisfactory performance on this benchmark. Traditional RVOS methods, such as ReferFormer [61], excel at tracking moving objects but struggle with comprehending complex language expressions, particularly those requiring multi-step reasoning with world knowledge. On the other hand, LLM-based models, like LISA [30], have better language understanding and reasoning capabilities. The main reasons for the poor performance are: 1) incapability to capture temporal dynamics in the video, and 2) difficulty in segmenting temporally consistent masks. In contrast, our VideoLISA model demonstrates remarkable performance, thanks to the advanced model design that considers all these crucial aspects."}, {"title": "5.3 Evaluation on Image Tasks", "content": "We use the image reasoning segmentation benchmark [30] to assess the reasoning capability of our model. During testing, we duplicate an image into multiple frames as a pseudo video. The results are shown in Tab. 4. We observe that our VideoLISA achieves state-of-the-art performance on both validation set and test set. Remarkably, despite our model employing an LLM with significantly fewer parameters, it outperforms larger models, such as LISA-7B and LISA-13B, demonstrating its exceptional reasoning capability. We attribute the impressive performance to the following aspects. From a data perspective, VideoLISA benefits from joint training on both image and video datasets, allowing it to learn from more abundant and diverse supervision signals. On the model aspect, the temporal learning module and the One-Token-Seg-All training encourage the model to leverage multiple frames of video simultaneously to conduct reasoning, rather than focusing on one image. Even when generalizing to image tasks, where the video is simulated by an image, the model's reasoning capability remains effective. We provide more experiment results on image referring segmentation in the appendix. These experiments demonstrate that our model is capable of image-based tasks, suggesting the potential for unifying image/video referring/reasoning segmentation tasks into a language-instructed object segmentation task solvable by a single VideoLISA model."}, {"title": "5.4 Ablation Studies", "content": "We conduct ablation studies on various design choices of our model. The detailed experiment results are provided in the appendix. Here, we summarize the main takeaways for each study."}, {"title": "6 Limitation and Future Work", "content": "Despite the remarkable performance shown on various benchmarks, our model still has limitations. We discuss them in this section to inspire future work. First, our model exhibits deficiencies in computational efficiency. Although we have already reduced the size of LLM to 3.8B, which is much smaller than previous models (7B, 13B), it still incurs a relatively high computational cost compared to previous work on video object segmentation. In other words, introducing a MLLM brings remarkable understanding and reasoning ability to the model, while also inducing computational costs. Exploring methods to achieve a trade-off between these aspects presents an interesting avenue for future research. Second, we observe that state-of-the-art approaches to video object segmentation often employ dedicated video backbones to enhance performance. Intuitively, using vision encoder pre-trained on videos would be beneficial for temporal-related tasks, such as object tracking. However, integrating a video backbone while ensuring compatibility with LLM and SAM decoder is non-trivial. In this work, we focus on empowering video segmentation tasks with reasoning capabilities based on LLM. Exploring the integration of a video backbone represents a potential avenue for future research."}, {"title": "7 Conclusion", "content": "In this work, we propose VideoLISA, a video-based LLM designed for language instructed reasoning segmentation in videos. It leverages the reasoning capabilities of LLM and employs SAM to produce segmentation masks. To address the unique challenges in marrying LLM with video object segmentation, we propose two key innovations. Firstly, a Sparse Dense Sampling strategy is designed to enable LLM to capture and understand temporal dynamics in videos. By leveraging the inherent temporal redundancy property of videos, this strategy achieves a delicate balance between preserving visual details and temporal context, making it favorable for video object segmentation tasks. Secondly, we propose a One-Token-Seg-All approach to achieve temporally consistent segmentation masks in the promptable mask decoding paradigm. Based on a dedicated investigation of the potential and challenges associated with using a single unified prompt to segment video frames, we enhance this capability from both input information foundation and training objective perspectives. Extensive ablation studies have investigated the function and rationale of the design choices of two modules. Equipped with the two designs above, our VideoLISA model shows impressive capabilities in video object segmentation, particularly emphasizing complex reasoning, temporal understanding, and object tracking, as validated by our newly organized ReasonVOS benchmark. Furthermore, it demonstrates notable performance on image segmentation tasks, positioning it as a potential unified model for language-instructed object segmentation."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Evaluation on Image Segmentation", "content": "In this section, we evaluate our VideoLISA model on the referring image segmentation task with three widely adopted benchmarks. The results are presented in Tab. 7. On the refCOCO and refCOCO+ benchmarks, our VideoLISA achieves comparable performance with the image-based LISA model. On the refCOCOg benchmark, VideoLISA outperforms previous methods, achieving state-of-the-art performance. In general, the results of this experiment, along with the image reasoning segmentation results shown in the main paper, effectively demonstrate that our VideoLISA model is a strong competitor in image segmentation tasks."}, {"title": "A.2 Ablation Studies", "content": "In this section, we present ablation studies on the temporal learning module (the Sparse Dense Sampling strategy), the temporal mask association module (the One-Token-Seg-All approach), and the training data recipe. For fair comparisons, unless specified, all VideoLISA variants are uniformly trained with the same training setting: 1) 3k iterations in total, 2) the same training data recipe, 3) the same learning rate scheduler, and 4) the same training objective. Three benchmarks are used for analysis: 1) ReasonSeg [30] evaluates the reasoning ability of the model; 2) MeViS [14] reflects the model's performance on temporal learning; and 3) Ref-DAVIS-17 [28] measures the general RVOS capability of the model. For evaluation on video benchmarks, the performance metrics of VideoLISA are computed using the simple One-Token-Seg-All approach without post-optimization, revealing the model's essential capabilities."}, {"title": "A.2.1 Temporal Learning Module", "content": "In Tab 8, we compare various strategies for temporal learning. The first row shows the vanilla LISA-7B model, which only focuses on image-based reasoning segmentation. To infer LISA-7B on video data, we employ a similar One-Token-Seg-All strategy, where the <TRK> token (called [SEG] in the original LISA) comes from the first frame. This performance serves as a baseline for comparison. In the second row, we construct a naive solution to adapt LISA to the video domain. Specifically, we finetune LISA-7B on the aforementioned video segmentation datasets. The results show that simply finetuning on video data does not significantly improve video performance and even hurts the performance on image reasoning segmentation. Although training on video datasets may enhance the model's ability to understand temporally related text queries, it still lacks temporal modeling ability from video data, resulting in undesirable performance.\nNext, we compare various temporal learning strategies within the VideoLISA framework using the One-Token-Seg-All training objective. We first experiment with a straightforward video training strategy, called n-frame, which directly concatenates the visual features from n sampled frames as input to the large language model. In our implementation, the value of n is set to the same as Tdense for comparison. As shown in the third row, we observe that with this simple strategy, the model achieves surprisingly good performance across the benchmarks, significantly outperforming LISA-based methods. Exposure to multiple frames enables the model to perceive temporal dynamics,"}, {"title": "A.2.2 Temporal Association Module", "content": "In Tab. 9, we compare the design choices for the temporal association module, i.e., tracking. As in previous comparisons, the One-Token-Seg-All strategy in LISA-7B serves as the baseline in the first row. One straightforward solution based on LISA is to plug an off-the-shelf tracker into the model. During inference, LISA outputs the segmentation mask of the first frame based on language instruction. The tracker then tracks the segmented object through the video, yielding segmentation masks for the subsequent frames. Specifically, we adopt the popular XMem [12] model as the tracker, as shown in the second row of the table. Compared to VideoLISA (both One-Token-Seg-All and post-optimization), LISA+XMem achieves worse performance on these benchmarks. This validates that simply plugging an existing tracker into an image-based reasoning segmentation model does not address the problem of video reasoning segmentation. The vital issue is that the LLM in charge of perception and reasoning does not capture the entire video content, making its predictions nonsensical. In contrast, VideoLISA's temporal learning module and dedicated training objective enrich the <TRK> token with semantic information, enabling it to find the target object across all frames."}, {"title": "A.2.3 Ablation on Training Data", "content": "Our model undergoes joint training on both image and video datasets. An investigation of the training data is presented in Tab. 10. We first observe that with image-only segmentation datasets, the model achieves decent performance in reasoning segmentation. However, the performance on video benchmarks is unsatisfactory, possibly due to insufficient temporal information in the training data. When using video-only segmentation settings, compared to image-only, the performance on video benchmarks increases significantly. Simultaneously, the model experiences a dramatic drop in performance in reasoning segmentation. This comparison demonstrates that video training is helpful for the VOS task, while image data is also necessary to exploit the reasoning ability of the model. When combining the image and video segmentation datasets, the model yields remarkable performance across various benchmarks.\nNext, we additionally explore the effect of using visual question answering (VQA) data. We first observe that after adding Image-QA data into training, the model experiences a slight performance drop in all benchmarks. Then, with the involvement of Video-QA data, the model achieves much better performance on the reasoning segmentation benchmark. Among the two video benchmarks, compared to the model trained with segmentation-only data, this model shows slightly better performance on the MeViS offline validation set yet worse performance on Ref-DAVIS-17. Intuitively, VQA data has the potential to enhance the model's reasoning ability. However, it may also make the multi-task training more challenging, as revealed by the performance fluctuation among different benchmarks. Maintaining the compatibility of different types of training data and tasks is left for future work."}, {"title": "A.3 Qualitative Results", "content": "In Fig. 3, we use a representative video to showcase the versatile language-instructed reasoning capabilities of our model. VideoLISA can do segmentation in videos via language referring, world knowledge reasoning, and video temporal reasoning. Additionally, the model can discern subtle differences in language instructions and is not biased to salient or moving objects.\nIn Fig. 6 and Fig. 7, we provide more abundant qualitative examples of VideoLISA. The red text is only for illustration purposes. No special prompting techniques were employed. It's important to note that these examples were generated using the One-Token-Seg-All inference approach without post-optimization."}]}