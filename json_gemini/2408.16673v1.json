{"title": "Entropic Distribution Matching in Supervised Fine-tuning of LLMs: Less Overfitting and Better Diversity", "authors": ["Ziniu Li", "Congliang Chen", "Tian Xu", "Zeyu Qin", "Jiancong Xiao", "Ruoyu Sun", "Zhi-Quan Luo"], "abstract": "Large language models rely on Supervised Fine-Tuning (SFT) to specialize in downstream tasks. Cross Entropy (CE) loss is the de facto choice in SFT, but it often leads to overfitting and limited output diversity due to its aggressive updates to the data distribution. This paper aim to address these issues by introducing the maximum entropy principle, which favors models with flatter distributions that still effectively capture the data. Specifically, we develop a new distribution matching method called GEM, which solves reverse Kullback-Leibler divergence minimization with an entropy regularizer. For the SFT of Llama-3-8B models, GEM outperforms CE in several aspects. First, when applied to the UltraFeedback dataset to develop general instruction-following abilities, GEM exhibits reduced overfitting, evidenced by lower perplexity and better performance on the IFEval benchmark. Furthermore, GEM enhances output diversity, leading to performance gains of up to 7 points on math reasoning and code generation tasks using best-of-n sampling, even without domain-specific data. Second, when fine-tuning with domain-specific datasets for math reasoning and code generation, GEM also shows less overfitting and improvements of up to 10 points compared with CE.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [38, 53, 52] are powerful generative models excelling in specialized tasks across various fields. Typically, LLMs first go through a pre-training stage [42, 7], where they learn to predict the next token from a large corpus of texts, such as books, scientific papers, and code. Despite this extensive pre-training, LLMs often struggle to follow instructions and answer users'\nqueries effectively, because such scenarios are not commonly encountered during pre-training. To improve their performance in these tasks, instruction tuning [44, 60, 10], also known as Supervised Fine-Tuning (SFT) [39, 3], is employed. This process involves using high-quality labeled data (i.e., prompt-response pairs) and typically utilizes supervised learning with Cross Entropy (CE) loss to maximize the likelihood of the labeled data. Later on, these models may be further aligned with human preferences to ensure their outputs align with human values [39, 3].\nSFT elicits the knowledge acquired from pre-training to answer various downstream questions and further paves the way for future developments, making it crucial part of the post-training pipeline [68, 54, 34, 65]. We expect models to generalize well by providing accurate answers and hope these answers are diverse as well. While the importance of generalization is clear, we also stress the significance of generation diversity. In creative writing, diversity sparks new ideas [12], and in chit-chat dialogues, users often appreciate having multiple output options to suit their preferences [30]. Many modern AI interfaces, such as ChatGPT and Claude AI, recognize this need by incorporating features like regeneration buttons. Additionally, generation diversity is vital when advanced generation algorithms are applied in LLMs to tackle complex tasks [50]. For example, the best-of-n sampling, commonly used in math reasoning [50] and code generation [8], benefits from selecting the optimal response from a diverse set of generated options.\nHowever, models trained using CE loss in SFT often struggle with overfitting (see, e.g., [16, 17]) and poor generation diversity (see, e.g., [40, 37]). In theory, optimizing CE loss corresponds to minimizing the forward Kullback-Leibler (KL) divergence between the data distribution and the generative distribution of the LLM. This distribution matching process aggressively increases the likelihood of observed data, which often exhibit narrow coverage of real distributions that have diverse outcomes that we want the LLM to learn. However, the CE loss is unaware of this, and the optimized model biases toward low-entropy distributions, resulting in reduced output diversity. One particular example is shown in Figure 1 (details are provided in Appendix E). Prior research [41, 14] links low-entropy predictive distributions with poor generalization, indicating that these issues are interconnected. While practitioners often use weight decay regularization with CE loss [39, 3], it does not fully address these problems, necessitating more principled approaches.\nOur contributions. We formulate fine-tuning of LLMs as a distribution matching problem"}, {"title": "2 Related Work", "content": "Supervised Fine-tuning. SFT is the first stage of the post-training pipeline and plays an important role in subsequent developments. As mentioned in the introduction, using CE loss during the SFT stage often leads to overfitting and reduced output diversity. To address this, there is a line of research in scaling up SFT data (see, e.g., [66, 62, 67]), which, while effective, increases computational burden. Our work aims to develop training methods that more effectively leverage supervised data to mitigate overfitting and to enhance output diversity. While recent studies such as [9, 29] attempt to improve CE-trained models through techniques like self-play, we aim to address the limitations of CE loss and design methods that can improve the pre-trained models directly.\nWe also emphasize the importance of output diversity based on previous research. SFT-trained models are often further refined through Reinforcement Learning from Human Feedback (RLHF) to better align with human values [39, 3]. Xiao et al. [64] studied the impact of SFT models on preference learning in RLHF, showing that if an SFT model collapses (i.e., becomes biased toward certain outputs with near-certain probability), it may further lead to preference collapse in the alignment. Their findings highlight the need to address collapse during the SFT stage. Additionally, SFT-trained models are often used as synthetic data generators for self-improvement (see, e.g.,"}, {"title": "3 Preliminary", "content": "Large Language Models (LLMs). LLMs have a large vocabulary, denoted as $[K] = {1, 2, ..., K}$ and process text by splitting it into a series of tokens $(x_1,...,x_T)$, where each token $x_i \\in [K]$ and $T$ represents the sequence length. Let $f$ be the generative distribution modeled by the language model. The notation $f(\\cdot|x_1,...,x_{t-1})$ specifies the categorical distribution over $[K]$, conditioned on the context $(x_1,...,x_{t-1})$. Typically, $f$ is parameterized by a neural network, often a transformer [57], with the parameter $\\theta$. For the $i$-th token at time step $t$, its prediction probability is given by\n$f_\\theta(i|x_1,...x_{t-1}) = \\text{softmax}(z_t) = \\frac{\\exp(z_t[i])}{\\Sigma_{i'} \\exp(z_t[i']))}$ where $z_t \\in \\mathbb{R}^K$ is the logit output from the neural network given the input $(x_1,..., x_{t-1})$, and $z_t[i]$ is i-th element of $z_t$. This auto-regressive process specifies the joint probability of a sequence of tokens as $f_\\theta(x_1,...,x_T) = \\Pi_{t=1}^T f_\\theta(x_t|x_1,..., x_{t-1})$.\nLLMs are pre-trained to predict the next token in a sequence, thereby learning complex conditional probability distributions from vast amounts of data. In practical applications, LLMs are tasked with generating a response $y$ to a given prompt $x$. However, these question-answer scenarios often differ from the textbook-like pre-training data, causing pre-trained LLMs to struggle in generating responses that follow human instructions effectively.\nSupervised Fine-Tuning. To address the above issue, Supervised Fine-Tuning (SFT) is introduced. This process involves using a supervised dataset with high-quality prompt-response pairs $\\{(x^i, y^i)\\}_{i=1}^n$ for demonstration. The CE loss is the de facto training objective for this purpose: $\\min_\\theta \\Sigma_i - \\log f_\\theta(y^i|x^i)$. In theory, this corresponds to minimizing the forward KL divergence between the data distribution $p$ and the generative distribution $f_\\theta$:\n$\\min_\\theta D_{KL}(p, f_\\theta) \\rightarrow \\max_\\theta \\mathbb{E}_{x \\sim p(x)}\\mathbb{E}_{y \\sim p(y|x)}[\\log f_\\theta(y|x)]$, where $p$ is the prompt distribution, which is usually not modeled during the SFT stage. Thus, the"}, {"title": "4 Entropic Distribution Matching", "content": "In this paper, we explore principled approaches to address limitations of existing SFT that uses the CE loss, especially when dealing with limited data. We present two core principles: the first focuses on the methodology of distribution matching, while the second offers guidance on learning from limited data.\nOur first principle advocates for a generative approach to distribution matching. This approach encourages the model to learn from its own generated data and mistakes, rather than merely imitating supervised demonstrations. Unlike the traditional CE loss, which leads the model to imitate training data labels passively, a generative approach involves active learning through self-generated feedback. This principle is grounded in cognitive science [47, 20], which demonstrates that children learn more effectively through exploration and experimentation, adjusting their understanding based on discrepancies between expectations and reality. Similarly, research on Generative Adversarial Networks (GANs) [18, 24] supports this notion by showing how models can learn to produce realistic data through iterative refinement. To summarize, we propose:\nPrinciple 1: The distribution matching approach should be \"generative\", meaning the model learns from both ground truth supervision and its own generated mistakes.\nOur second principle addresses the challenge of overfitting. We draw inspiration from neuro- science, specifically the concept of avoiding over-memorization and achieving balanced learning. In neuroscience, synaptic plasticity, particularly homeostatic plasticity, underscores the importance of maintaining balance in learning processes [56, 55]. Overly strengthening certain neural connections can lead to rigid, maladaptive behaviors, analogous to how assigning excessively high probabilities to observed tokens can result in over-memorization in models, thereby limiting their ability to adapt and generalize. Based on these insights, especially for limited data, we propose:\nPrinciple 2: The model should assign higher probabilities to the observed data while preventing over-memorization."}, {"title": "4.1 Proposed Formulation: Reserve KL with Entropy Regularization", "content": "To implement the two principles outlined above, we propose studying the formulation of reverse KL divergence minimization with maximum entropy regularization. The objective is defined as follows:\n$\\max_f \\mathbb{E}_x {\\mathbb{E}_{y \\sim p(\\cdot|x)}[\\log p(y|x)] - \\mathbb{E}_{y \\sim f(\\cdot|x)}[\\log f (y|x)] +\\gamma \\cdot \\mathbb{E}_{y \\sim f(\\cdot|x)}[\\log f(y|x)] }.\\tag{1}$"}, {"title": "4.2 Proposed Algorithm: GEM", "content": "In this section, we present a practical algorithm for solving the optimization problem of reverse KL with entropy regularization. As discussed earlier, the key is to obtain an estimate for the log probability density function $\\log p$. We first address the case where $y$ is not sequential. We begin by outlining a conceptually simple but technically complicated solution. Building on this proposal, we then introduce a more tractable solution by new techniques.\nAn Initial Proposal. Drawing inspiration from GANs [18, 25], one may propose the following formulation for estimating the distribution $p$:\n$\\min_r \\max_q \\mathbb{E}_x \\mathbb{E}_{y_{\\text{real}} \\sim p(\\cdot|x)} \\mathbb{E}_{y_{\\text{gene}} \\sim q(\\cdot|x)} [h (r(x,y_{\\text{real}}) \u2013 r(x,y_{\\text{gene}}))].\\tag{2}$ Here we use the subscript real to denote the supervised data and gene to denote the model- generated data for clarity. In addition, $h$ is a monotonically increasing function (e.g., a linear function) that will be discussed later, and $r : \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ acts as a discriminator, designed to maximize the gap between samples drawn from the data distribution $p$ and those from the opponent distribution $q$. The goal of the opponent $q$ is to minimize the gap. When $h$ is a linear function, it simplifies to the reward maximization problem $\\max_q \\mathbb{E}_{y_{\\text{gene}} \\sim q(\\cdot|x)} [r(x, y_{\\text{gene}})]$, a fact that shall be used later.\nOn its theoretical foundation, Jolicoeur-Martineau [25] proved that the inner maximization problem is to find a divergence function between the distributions $p$ and $q$, and thus the outer minimization problem is to reduce this divergence. Therefore, the solved $q$ serves as a good estimate of $p$. If we can solve problem (2), we can substitute $\\log p = \\log q$ in Equation (1). While this offers a conceptually viable solution, it also presents several challenges. In particular, the optimization problems in Equations (1) and (2) involves three variables: $r$, $q$, and $f$, which are hard to solve:\n\u2022 The variables $q$ and $r$ are introduced to be adversarially trained in Equation (2);\n\u2022 The distribution $f$ needs to be further solved in Equation (1) after $q$ has been solved.\nIt is important to note that optimizing the reward function $r$ and the distribution $q$ can be particularly challenging when dealing with sequential data, often requiring Reinforcement Learning (RL) algorithms (see, e.g., [22]). Our initial attempts to implement the above proposal were not successful. We introduce a new tractable solution below."}, {"title": "Proposed Solution", "content": "At a high level, our approach simplifies the process by solving a single-stage optimization problem, eliminating the need to first estimate the distribution $p$ before solving for $f$. Our approach involves two key techniques:\n\u2022 Reparameterization: We reparameterize the discriminator using the generative distribution $f$. Specifically, we define $r$ as a real-valued function and parameterize it as $\\log f(y|x)$, ensuring it remains real-valued.\n\u2022 Direct entropy regularization: We introduce entropy regularization for the distribution $q$ directly, deriving a closed-form solution for $q$. This technique offers two advantages. First, it establishes a connection between $q$ and $f$, eliminating the need of solving problem (1) separately. Second, since $q$ has a closed-form solution, it does not require explicit training.\nSpecifically, our formulation is that:\n$\\max_f \\mathcal{L}_q(f) \\triangleq \\mathbb{E}_x \\mathbb{E}_{y_{\\text{real}} \\sim p(\\cdot|x)} \\mathbb{E}_{y_{\\text{gene}} \\sim q(\\cdot|x)} [h (\\log f (y_{\\text{real}}|x) \u2013 \\log f (y_{\\text{gene}}|x))]\\tag{3}$ s.t. $q = \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_x \\mathbb{E}_{y \\sim \\pi(\\cdot|x)} [\\log f(y|x)] + 1/\\beta\\cdot \\mathcal{H}(\\pi(\\cdot|x)) = \\text{softmax}(1/\\beta * \\log f).\\tag{4}$ In this formulation, we optimize $f$ as if we were optimizing the reward function $r$. Simultaneously, we optimize $q$ with the reward given by $\\log f(y|x)$, incorporating entropy regularization. Fortunately, this yields a closed-form solution, so we do not need to maintain or explicitly train $q$. We only need to optimize the distribution $f$ in Equation (3). Note that although $q$ is dependent on $f$ generally, we do not calculate the gradient through $q$ when calculating the gradient of $f$. This is similar to the target network used in RL [35, 32]. We have the following theoretical justification for this formulation.\nProposition 1. Assume that h is a linear function. For any distribution $p$, $\\mathcal{L}_q(f)$ has a unique stationary point, and this stationary point (with $\\beta = 1/(\\gamma + 1) > 0$) corresponds to the optimal solution of Problem (1).\nProposition 1 implies that solving the proposed problem in Equations (3) and (4) provides the optimal solution of reverse KL with entropy regularization in Equation (1). In practice, we can parameterize $f$ using a transformer and optimize the parameters with gradient ascent. We outline such a training procedure in Algorithm 1, referring to this approach as GEM, which stands for Generative and Entropy-regularized Matching of distributions. Note that the exact expectation $\\mathbb{E}_{y_{\\text{gene}} \\sim q(\\cdot|x)} [\u22c5]$ is calculated because we assume $y_{\\text{gene}}$ is non-sequential here (we will deal with the sequential case later). Concretely, in this case the generative distribution is categorical and has finite support, making the expectation easy to compute. This differs from the continuous optimization in"}, {"title": "Extensions to Sequential Data", "content": "In the above part, we have derived the algorithm for the case $y$ is non-sequential. We note that optimization in the sequential case could be highly difficult. With a little abuse of notations, let $y = (y_1, \u2026\u2026\u2026, y_T) \u2261 y_{1:T}$. We can extend the formulation in Equations (3) and (4) to the following:\n$\\max_f \\mathbb{E}_x \\mathbb{E}_{y_{\\text{real}} \\sim p(x)} \\mathbb{E}_{y_{\\text{gene}} \\sim q(x)} [h (\\log f(y_{\\text{real}}|x) \u2013 \\log f(y_{\\text{gene}}|x))] $\ns s.t. $q = \\underset{\\pi}{\\operatorname{argmax}} \\mathbb{E}_x \\mathbb{E}_{y_{1:T} \\sim \\pi(\\cdot|x)} [\\log f (y_{1:T}|x)] + 1/\\beta\\cdot \\mathcal{H}(\\pi(\\cdot|x)) $\nHere, we encounter a challenge: the joint distribution of $y_{1:T}$, as a cascaded categorical distribution, is quite complicated. This results in the expectation $\\mathbb{E}_{\\text{gene}} \\mathbb{E}_{\\text{gene}} []$ cannot be easily calculated as before. While Monte Carlo estimation-drawing samples to approximate the gradient-might seem like a viable solution, we found it does not work in experiments. We believe the main reason is that the sample space is huge, and the pre-trained distribution $f$ is quite different from the data distribution $p$ that we aim to learn. As a result, when we use stochastic sampling to estimate the gradient, it does not provide effective feedback.\nTo deal with the above challenge, we propose decomposing the multi-stage sequential optimization problem into multiple single-stage optimization problems and solve each efficiently. Concretely, we restrict the distribution matching to the case that the prefix samples up to time step $t$ are drawn from the data distribution $p$ and solves the optimization problem at the $t$-th time step as before. Its mathematical formulation is given below:\n$\\max_f \\mathcal{L}_{seq}^T(f) = \\mathbb{E}_x \\{ \\sum_{t=1}^T \\mathbb{E}_{y_{1:t-1}^{\\text{real}} \\sim p(\\cdot|x)} \\mathbb{E}_{y_t^{\\text{real}} \\sim p(\\cdot|x, y_{1:t-1}^{\\text{real}})} \\mathbb{E}_{y_t^{\\text{gene}} \\sim q(x, y_{1:t-1})} [\\Delta] \\} $\nwhere $\\Delta = [h (\\log f (y_t^{\\text{real}}|x, y_{1:t-1}^{\\text{real}}) \u2013 \\log f (y_t^{\\text{gene}}|x, y_{1:t-1}^{\\text{real}}))],\\tag{5}$ The main advantage of this formulation is that for each sub-problem, we still have access to the exact conditional distribution, so the gradient estimation is accurate. The same idea applies to the training of distribution $q$:\nq = \\underset{\\pi}{\\operatorname{argmax}} \\{ \\sum_{t=1}^T \\mathbb{E}_x \\mathbb{E}_{y_{1:t-1}^{\\text{real}} \\sim p(\\cdot/x)} \\mathbb{E}_{y_t \\sim \\pi(\\cdot|x,y_{1:t-1}^{\\text{real}})} [\\log f(y_t|x, y_{t-1})] + 1/\\beta\\cdot \\mathcal{H}(\\pi(\\cdot|x, y_{1:t-1}^{\\text{real}})) \\} $\nThat is, we still have the closed-form solution that $q(x, y_t) = \\text{softmax}(1/\\beta \u00b7 \\log f (\u00b7|x, y_{t-1}^{f1}))$ when used in Equation (5). We outline the proposed procedure in Algorithm 2 and its PyTorch code is provided in Appendix A for reference. We acknowledge that our technique draws inspiration from the data distribution \u201creset\u201d trick introduced by [45] in imitation learning, in which the teacher first shows few demonstration actions and then the student is asked to finish the other actions in a full trajectory."}, {"title": "5 Experiments", "content": "In this section, we present our experiment results for fine-tuning the Llama-3-8B model (specifically, its pre-trained version). A brief overview of our experiment setting is provided below, with further details available in Appendix D."}, {"title": "5.1 General-Purpose Fine-tuning", "content": "Set-up. In this section, we develop an LLM that is capable of following instructions for various prompts. To this end, we utilize the UltraFeedback dataset [13], specifically the version filtered by the HuggingfaceH4 team7. This dataset contains prompts from instruction datasets like Evol-Instruct and UltraChat, and responses generated by models such as GPT-4 and Llama-2-7B/13B/70B-Chat. For more information, see [13].\nEach data point comprises two responses: one selected as the preferred option and the other as the rejected option, with the selection made by GPT-4. In our study, we use the preferred response for SFT, a practice commonly adopted in previous research [39, 3]. Following [66, 34, 13] we set the learning rate to $2 \u00d7 10^{\u22125}$, employing a cosine learning rate decay schedule, and use a macro batch size of 128. The maximum sequence length, encompassing both the prompt and response, is set to 2,048 tokens. Models are trained for three epochs.\nWe implement the proposed GEM method with $\u03b2 = 0.7$. As discussed, GEM has two variations: GEM-LS (GEM with log-sigmoid), and GE-Linear, each depending on the choice of the function $h$. Our primary baseline is the standard CE loss. Additionally, we explore a variant incorporating a weight decay of 0.1, which has been commonly used in previous studies [39, 3]. We refer to this approach as CE + WD. We also implement a method called CE + Entropy, which adds an entropy regularization term of 0.1 to the CE loss. This method aligns with the proposed Principle 2 but not Principle 1 (see Appendix C for more discussion).\nInstruction-Following. We first examine the model's learned ability in terms of instruction-following. We follow the IFEval benchmark in [69], which includes 500 prompts from 25 types of verifiable instructions, such as writing more than 400 words. Depending on whether we use strict or loose judgment and whether we measure accuracy in the prompt space or instruction space, there are four evaluation criteria: prompt-level strict accuracy, instruction-level strict accuracy, prompt-level loose accuracy, and instruction-level loose accuracy. For all metrics, a higher value indicates better performance.\nWe evaluate the trained models using greedy decoding and present the results in Table 1. We observe that CE underperforms compared with regularization-based methods, such as weight decay and entropy regularization, suggesting that CE suffers from overfitting. On average across the four criteria, GEM-LS improves by 1.1 points (2.5% relative) and 1.4 points (3.2% relative) compared to CE. We also observe this overfitting in the evaluation perplexity: GEM-LS and GEM-Linear achieve lower perplexity (around 3.16) than CE (3.48); see Appendix E. It is important to note that this overfitting is not due to over-optimization, as performance continues to improve over three training epochs for CE (36.15 in epoch 1, 41.45 in epoch 2, and 43.70 in epoch 3).\nCreative Writing. We continue to assess models' output diversity in two creative writing tasks: poem writing and story writing. For poems, we use prompts from the poetry dataset on the"}, {"title": "5.2 Domain-specific Fine-tuning", "content": "In this section, we conduct experiments with domain-specific datasets. For math reasoning, we use the dataset MetaMathQA [66]. For code generation, we use the dataset Magicoder-OSS-Instruct"}, {"title": "6 Conclusion", "content": "In this paper, we propose an alternative method for the SFT of LLMs to tackle the challenges of overfitting and limited generation diversity, which are often caused by the aggressive updates of the CE loss and limited data. We demonstrate the effectiveness of combining generative distribution matching with entropy regularization. We note that the improved diversity also boosts performance in downstream tasks when advanced generation methods, such as the best-of-n sampling, are used. Overall, our results indicate that the proposed method is well-suited for generative models.\nWe focus on the initial stage of post-training pipeline in this paper and recognize that the models trained with our proposed methods can be further refined in subsequent stages. Notably, the enhanced diversity achieved by our approach can be advantageous in several contexts: it supports scaling up test-time computation [6, 50], improves exploration in RL methods [46, 33], addresses the preference collapse issue [64], facilitates self-improvement through distillation with best-of-n techniques [48], and helps mitigate mode collapse in synthetic data generation [49, 5, 63]. We see significant potential for our method in these areas and plan to explore these topics in future work."}, {"title": "A Implementation of GEM", "content": null}, {"title": "B Proof", "content": "Proof of Proposition 1. When $h$ is a linear function, we have that\n$\\mathcal{L}_q(f) = \\mathbb{E}_x \\mathbb{E}_{y_{\\text{real}} \\sim p(\\cdot|x)} \\mathbb{E}_{y_{\\text{gene}} \\sim q(\\cdot|x)} [\\log f (y_{\\text{real}}|x) \u2013 \\log f (y_{\\text{gene}}|x)] $\n$= \\mathbb{E}_x \\mathbb{E}_{y_{\\text{real}} \\sim p(\\cdot|x)} \\mathbb{E}_{y_{\\text{gene}} \\sim q(\\cdot|x)} [\\log f (y_{\\text{real}}|x)] \u2013 \\mathbb{E}_x \\mathbb{E}_{y_{\\text{real}} \\sim p(\\cdot|x)} \\mathbb{E}_{y_{\\text{gene}} \\sim q(\\cdot|x)} [\\log f (y_{\\text{gene}}|x)] $\n$= \\mathbb{E}_x \\mathbb{E}_{y_{\\text{real}} \\sim p(\\cdot|x)} [\\log f (y_{\\text{real}}|x)] - \\mathbb{E}_x \\mathbb{E}_{y_{\\text{gene}} \\sim q(\\cdot|x)} [\\log f (y_{\\text{gene}}|x)]$\nFor any $x \u2208 \\mathcal{X}$, we have that\n$\\frac{d\\mathcal{L}}{df} = \\frac{p-q}{f} \\tag{6}$\nTo calculate the stationary point of $\\mathcal{L}$, we require that $p = q$. Since $q = \\text{softmax}(1/\u03b2 \u00b7 \\log f)$, the above equality requires that $f = \\text{softmax}(\u03b2\u00b7\\log p)$. As analyzed in Proposition 2, for $\u03b2 = 1/(\u03b3+1)$, this corresponds to the the optimal solution of minimizing reverse KL with entropy regularization.\n$\\square$\nProposition 2. For the entropy-regularized KL minimization problem in Equation (1), in the function space, we have the optimal solution:\n$f^*(y/x) = \\frac{p(y/x)^{1/(\\gamma+1)}}{Z_x}$ where $Z_x$ is a normalization constant $\\sum_{y'} p(y'|x)^{1/(\\gamma+1)}.$"}, {"title": "C Discussion", "content": "We discuss the formulation of forward KL with entropy regularization in this section:\n$\\max_f \\mathbb{E}_x {\\mathbb{E}_{y \\sim p(x)}[\\log f(y|x)] + \\gamma \u00b7 \\mathbb{E}_{y \\sim f(\\cdot|x)}[\u2212 \\log f(y|x)] }\\tag{7}$\n=$-D_{KL}(p, f) +constant$\n=$\\mathcal{H}(f)$\nThis formulation supports the proposed Principle 2 but not Principle 1. We find that this formulation leads to an improper increase in tail probabilities when maximizing the entropy, as illustrated in Figure 5. In the context of LLMs, this increase often translates into nonsensical tokens in the vocabulary, leading to undesirable generation outputs (if additional strategies like top-k and top-p sampling are not used). A concrete example is provided in Table 3. The core issue arises because the gradient of the entropy regularizer can dominate for tokens with low probabilities. Specifically, the gradient of the forward KL is computed as -p/f, where the division is element-wise, and the gradient of the entropy is \u2013(1 + log f). Consequently, for tokens with low probabilities in both f and p, the gradient given by the forward KL is much smaller than that given by the entropy regularizer, thus disproportionately increasing the tail probabilities. In contrast, the proposed reverse KL formulation with entropy regularization does not have this issue. This is because the optimization is defined over the generative distribution $f$ in our formulation, ensuring balanced gradients even for tokens with low probabilities (refer to Equation (6))."}, {"title": "D Experiment Details", "content": "All experiments are conducted using A800-80GB GPUs with the DeepSpeed distributed training framework, utilizing ZeRO-2 and gradient checkpointing without offloading. We use flash-attention-2 with deterministic backward for reproducibility. The experiments are based on the pretrained Llama- 3-8B model, using Adam as the optimizer with a global batch size of 128. Following [66, 34, 13], the learning rate is set to 2e-5, with a warm-up ratio of 0.03 and cosine learning rate decay. Training is performed over 3 epochs. All supervised datasets are formatted into the chat format using the Llama-3-8B-Instruct's tokenizer. When generation of responses is required for evaluation, we use the vLLM to accelerate inference."}, {"title": "D.1 UltraFeedback", "content": "We use the dataset filtered by HuggingfaceH4 team, which is available at https://huggingface. co/datasets/HuggingFaceH4/ultrafeedback_binarized. The dataset contains 61,135 training samples and 1,000 test samples. For training, we set the maximum sequence length to 2,048, dropping longer sequences and padding shorter ones. To achieve a global batch size of 128, we use a per-device batch size of 4, a gradient accumulation step of 4, and 4 GPUs. The training times takes about 24 GPU hours. For the CE method, we have tuned hyperparameters for weight decay and entropy regularization, selecting values from {0.1, 0.01, 0.001}. In both cases, a value of 0.1 provided the best overall results.\nEvaluation metrics, including perplexity, and entropy, are based on these 1,000 test samples. For entropy calculation, we compute the conditional entropy, whose expectation can be calculated exactly, and average over the sequence. For the instruction-following evaluation, we use the IFEval benchmark from [69]. We apply greedy decoding with a maximum generation length of 1,024 tokens.\nFor the diversity evaluation in poem writing, we use prompts derived from the poetry dataset on the Huggingface website, which includes 573 poems on themes like love, nature, and mythology by poets such as William Shakespeare. We prompt the trained models with questions like, \u201cWrite a poem titled '[X]' with no more than 200 words,\" where [X] is a title from the dataset. For story writing, we create 500 prompts based on the ROC Story dataset (2017 winter) [36], asking models to \u201cWrite a story titled \u2018[X]' with no more than 200 words,\" where [X] is a title from the dataset. The maximum number of generation tokens is set to 512. The evaluation script follows the methodology from previous work by [26], using the script available at https://github.com/facebookresearch/ rlfh-gen-div. For each question, 16 samples with the generation configuration temperature=1.0, top_k=50, top_p=0.9 is used.\nFor the chat evaluation, we use the 805 test questions from the AlpacaEval dataset and employ the reward model FsfairX-LLaMA3-RM-v0.1. The maximum generation sequence length is set to 2048. For each question, 32 samples are generated with the configuration temperature=0.6, top_k=50, top_p=0.9. To calculate the win rate, we use the Bradley-Terry model:\n$P(y > y' | x) = \\frac{\\exp(r(x, y))}{\\exp(r(x, y)) + \\exp(r(x, y'))}$ We use GPT-4 generated responses as a baseline for calculating the win rate, specifically the gpt4_1106_preview11 version.\nFor the math reasoning task on GSM8K, we use the following prompt:"}, {"title": "D.2 MagiCoder", "content": "We use the MagiCoder-OSS-Instruct dataset [62"}]}