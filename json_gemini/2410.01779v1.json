{"title": "COMPOSING GLOBAL OPTIMIZERS TO REASONING TASKS VIA ALGEBRAIC OBJECTS IN NEURAL NETS", "authors": ["Yuandong Tian"], "abstract": "We prove rich algebraic structures of the solution space for 2-layer neural networks with quadratic activation and L2 loss, trained on reasoning tasks in Abelian group (e.g., modular addition). Such a rich structure enables analytical construction of global optimal solutions from partial solutions that only satisfy part of the loss, despite its high nonlinearity. We coin the framework as CoGO (Composing Global Optimizers). Specifically, we show that the weight space over different numbers of hidden nodes of the 2-layer network is equipped with a semi-ring algebraic structure, and the loss function to be optimized consists of monomial potentials, which are ring homomorphism, allowing partial solutions to be composed into global ones by ring addition and multiplication. Our experiments show that around 95% of the solutions obtained by gradient descent match exactly our theoretical constructions. Although the global optimizers constructed only required a small number of hidden nodes, our analysis on gradient dynamics shows that overparameterization asymptotically decouples training dynamics and is beneficial. We further show that training dynamics favors simpler solutions under weight decay, and thus high-order global optimizers such as perfect memorization are unfavorable.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have shown impressive results in various disciplines (OpenAI,\n2024; Anthropic; Team, 2024b;a; Dubey et al., 2024; Jiang et al., 2023), while they also make sur-\nprising mistakes in basic reasoning tasks (Nezhurina et al., 2024; Berglund et al., 2023). Therefore,\nit remains an open problem whether it can truly do reasoning tasks. On one hand, existing works\ndemonstrate that the models can learn efficient algorithms (e.g., dynamic programming (Ye et al.,\n2024) for language structure modeling, etc) and good representations (Jin & Rinard, 2024; Wijmans\net al., 2023). Some reports emergent behaviors (Wei et al., 2022) when scaling up with data and\nmodel size. On the other hand, many works also show that LLMs cannot self-correct (Huang et al.,\n2023), and cannot generalize very well beyond the training set for simple tasks (Dziri et al., 2023;\nYehudai et al., 2024; Ouellette et al., 2023), let alone complicated planning tasks (Kambhampati\net al., 2024; Xie et al., 2024).\nTo understand how the model performs reasoning and further improve its reasoning power, people\nhave been studying simple arithmetic reasoning problems in depth. Modular addition (Nanda et al.,\n2023; Zhong et al., 2024), i.e., predicting a + b mod d given a and b, is a popular one due to its\nsimple and intuitive structure yet surprising behaviors in learning dynamics (e.g., grokking (Power\net al., 2022)) and learned representations (e.g., Fourier bases (Zhou et al., 2024)). Most works\nfocus on various metrics to measure the behaviors and extracting interpretable circuits from trained\nmodels (Nanda et al., 2023; Varma et al., 2023; Huang et al., 2024). Analytic solutions can be\nconstructed and/or reverse-engineered (Gromov, 2023; Zhong et al., 2024; Nanda et al., 2023) but it\nis not clear how to construct a systematic framework to explain and generalize the results.\nIn this work, we systematically analyze 2-layer neural networks with quadratic activation and L2\nloss on predicting the outcome of group multiplication in Abelian group G, which is an extension\nof modular addition. We find that global optimizers can be constructed algebraically from small\npartial solutions that are optimal only for parts of the loss. We achieve this by showing that (1) for"}, {"title": "RELATED WORKS", "content": "Algebraic structures for maching learning. Many works leverage symmetry and group structure\nin deep learning. For example, in geometric deep learning, different forms of symmetry are incor-\nporated into network architectures (Bronstein et al., 2021). However, they do not open the black\nbox and explore the algebraic structures of the network itself during training.\nExpressibility. Existing works on expressibility (Li et al., 2024; Liu et al., 2022) gives explicit\nweight construction of neural networks weights (e.g., Transformers) for reasoning tasks like au-\ntomata, which includes modular addition. However, their works do not discover algebraic structures\nin the weight space and loss, nor learning dynamics analysis, and it is not clear whether the con-\nstructed weights coincide with the actual solutions found by gradient descent, even in synthetic data.\nFourier Bases in Arithmetic Tasks. Existing works discovered that pre-trained models use Fourier\nbases for arithmetic operations (Zhou et al., 2024). This is true even for a simple Transformer, or\neven a network with one hidden layer (Morwani et al., 2023). Previous works also construct ana-\nlytic Fourier solutions (Gromov, 2023) for modular addition, but with the additional assumption of\ninfinite width, unaware of the algebraic structures we discover. Existing theoretical work (Morwani\net al., 2023) also shows group-theoretical results on algebraic tasks related to finite groups, also for\nnetworks with one-hidden layers and quadratic activations. Compared to ours, they use the max-\nmargin framework with a special regularization (L2,3 norm) rather than L2 loss, do not characterize\nand leverage algebraic structures in the weight space, and do not analyze the training dynamics."}, {"title": "DECOUPLING L2 LOSS FOR REASONING TASKS OF ABELIAN GROUP", "content": "Basic group theory. A set G forms a group, which means that (1) there exists an operation. (i.e.,\n\u201cmultiplication\u201d): $G \\times G \\rightarrow G$ and it satisfies association: $(g_1 \\cdot g_2) \\cdot g_3 = g_1 \\cdot (g_2 \\cdot g_3)$. Often we write\n$g_1g_2$ instead of $g_1 \\cdot g_2$ for brevity. (2) there exists an identity element $e \\in G$ so that $eg = ge = g$,\n(3) for every group element $g \\in G$, there is a unique inverse $g^{-1}$ so that $gg^{-1} = g^{-1}g = e$. In some\ngroups, the multiplication operation is commutative, i.e., $gh = hg$ for any $g,h \\in G$. Such groups\nare called Abelian group. Modular addition forms a Abelian (more specifically, cyclic) group by\nnoticing that there exists a mapping $a \\leftrightarrow e^{2 \\pi a i / d}$ and $a+b\\_mod d$ is $e^{2 \\pi a i / d} \\cdot e^{2 \\pi b i / d} = e^{2 \\pi (a+b) i / d}$.\nBasic Ring theory. A set $Z$ forms a ring, if there exists two operations, addition + and multipli-\ncation *, so that (1) $\\langle Z, +\\rangle$ forms an Abelian group, (2) $\\langle Z, *\\rangle$ is a monoid (i.e., a group without\ninverse), and (3) multiplication distributes with addition (i.e., $a * (b + c) = a * b + a * c$ and\n$(b + c) * a = b * a + c * a$). $Z$ is called semi-ring if $\\langle Z, +\\rangle$ is a monoid.\nNotation. Let $R$ be the real field and $C$ be the complex field. For a complex vector $z$, $z^T$ is its\ntranspose, $\\bar{z}$ is its complex conjugate and $z^*$ its conjugate transpose. For a tensor $Z_{ijk}$, $Z_{\\cdot jk}$ is a\nvector along its first dimension, $Z_{i \\cdot k}$ along its second dimension, and $z_{ij \\cdot}$ along its last dimension."}, {"title": "DECOUPLING L2 LOSS FOR REASONING TASKS OF ABELIAN GROUP", "content": "We consider the following 2-layer networks with q hidden nodes, trained with\n(projected) $l2$ loss on prediction of group multiplication in Abelian group G with |G| = d:\n$l=\\sum_i P_o|| \\sigma \\lbrack V_o(W^T f\\lbrack i \\rbrack) \\rbrack - l[i]||^2 = \\sum_i P_o||V_o(W^T f[i]) -l [i]||^2 \\\\ =\\sum_i P_o||V_o(\\sum_j w_j^T f[i])- l[i]||^2 = \\sum_i P_o|| \\sum_j v_j o(w_j^T f[i])- l[i]||^2$\\n(1)\nwhere $\\sigma(x) = x^2$ is the quadratic activation function (Du & Lee, 2018; Allen-Zhu & Li, 2023),\n$P_o = I - \\frac{1}{d}11^T$ is the zero-mean projection matrix, $W = [W_1,...,W_q] \\in R^{d\\times q}$, $V =\n[V_1,...,V_q] \\in R^{d\\times q}$ are learnable parameters. $f[i] \\in R^d$ are input embeddings. $i$ is the sam-\nple index. Note that variants of quadratic activation have been used empirically, e.g. squared ReLU\nand gated activations (So et al., 2021; Shazeer, 2020; Zhang et al., 2024).\nInput and Output. The input contains the two group elements $g_1[i]$ and $g_2[i]$, encoded as $f[i] =$\n$U_{G_1}e_{g_1[i]} + U_{G_2}e_{g_2[i]}$, where $U_{G_1}$ and $U_{G_2}$ are column orthogonal embedding matrices. The output\nis the result $g_1[i]g_2[i] \\in G$, encoded as the label $l[i] = g_1[i]g_2[i]$ to be predicted. We can extend our\nframework to group action prediction, in which $g_2$ may not be a group element but any object (e.g.,\na discrete state in reinforcement learning). See Appendix E for more details.\nLet $\\phi_k = [\\phi_k(g)]_{g\\in G} \\in C^d$ be the scaled Fourier bases (or more formally, character function of the\nfinite Abelian group G, see Appendix A). Then weight vector $w_j$ and $v_j$ can be written as:\n$w_j = U_{G_1} \\sum_{k\\neq 0} z_{akj} \\phi_k + U_{G_2} \\sum_{k\\neq 0} z_{bkj} \\phi_k, \\ \\ v_j = \\sum_{k\\neq 0} z_{ckj} \\phi_k$\\n(2)\nwhere $z := \\{z_{pkj}\\}$ are the complex coefficients, $p \\in \\{a,b,c\\}$, $0 \\leq k < d$ and $j$ runs through $q$\nhidden nodes. We exclude $0=1$ because the constant bias term has been filtered out by the top-\ndown gradient from the loss function. Since $w_j$ and $v_j$ are all real, the Hermitian constraints holds,\ni.e., $z_{ckj} = \\bar{\\phi_{v_j}} = \\phi_{-k}^* v_j = z_{c,-k,j}$ (and similar for $z_{akj}$ and $z_{bkj}$). Leveraging the property of\nquadratic activation functions, we can write down the loss function analytically (see Appendix A):\nTheorem 1 (Analytic form of L2 loss with quadratic activation). The objective of 2-layer MLP\nnetwork with quadratic activation can be written as $l = d^{-1} \\sum_{k\\neq 0} l_k + (d - 1)/d$, where\n$l_k = -2r_{kkk} + \\sum_{k_1k_2} |r_{k_1k_2k}|^2 + \\sum_{p \\in \\{a,b\\}} \\sum_{k'} |\\Gamma_{p,k',-k',k}|^2 + \\sum_{m \\neq 0} \\sum_{p \\in \\{a,b\\}} |\\Gamma'_{p,k',m-k',k}|^2$\\n(3)"}, {"title": "DECOUPLING L2 LOSS FOR REASONING TASKS OF ABELIAN GROUP", "content": "Here $r_{k_1k_2k} := \\sum_j z_{ak_1j}z_{bk_2j}z_{ckj}$ and $\\Gamma_{p,k_1k_2k} := \\sum_j z_{pk_1j}z_{pk_2j}z_{ckj}$.\nNote that for cyclic group G, the frequency k is a mod-d integer. For general Abelian group which\ncan be decomposed into direct sum of cyclic groups according to Fundamental Theorem of Finite\nAbelian Groups (Diaconis, 1988), k is a multidimensional frequency index. For convenience, we\ndefine -k := k as the conjugate representation of $\\phi_k$. Since weights $w_j$ and $v_j$ are all real, the\nHermitian constraints holds, i.e., $z_{ckj} = \\bar{\\phi_{v_j}} = \\phi_{-k}^* v_j = z_{c,-k,j}$ (and similar for $z_{akj}$ and $z_{bkj}$).\nTherefore, $z_{p,-k,j} = \\bar{z_{pkj}}$, $\\Gamma_{-k,-k,-k} = \\bar{\\Gamma_{kkk}}$ and l is real and can be minimized.\nEqn. 3 contains different r terms, which play an important role in determining global optimizers.\nDefinition 1 (0/1-set). Let $R := \\{r\\}$ be a collection of r terms. The weight z is said to have 0-set\n$R_0$ and 1-set $R_1$ (or 0/1-sets $(R_0, R_1)$), if $r(z) = 0$ for all $r \\in R_0$ and $r(z) = 1$ for all $r \\in R_1$.\nWith 0/1-sets, we can characterize rough structures of the global optimizers to the loss:\nLemma 1 (A Sufficient Conditions of Global optimizers of Eqn. 3). If the weight z to Eqn. 3 has\n0-sets $R_c\\cup R_n\\cup R_*$ and 1-set $R_g$, i.e.\n$\\Gamma_{kkk}(z) = [ (k \\neq 0), \\Gamma_{k_1k_2k}(z) = 0, \\Gamma_{p k_1k_2k}(z) = 0$\n(4)\nthen it is a global optimizer with zero loss $l(z) = 0$. Here $R_g := \\{\\Gamma_{kkk}, k \\neq 0\\}$, $R_c :=$\n$\\Gamma_{k_1k_2k, k_1,k_2, k not all equal\\}$, $R_n := \\{\\Gamma_{p,k',-k',k}\\}$ and $R_* := \\{\\Gamma_{p,k',m-k',k, m \\neq 0\\}$.\nLemma 1 provides sufficient conditions since there may exist solutions that achieve global optimum\n(e.g., $\\sum_{k'} \\Gamma'_{p,k',m-k',k}(z) = 0$ but $\\Gamma'_{p,k',m-k',k}(z) \\neq 0$). However, as we will see, it already leads\nto rich algebraic structure, and serves as a good starting point. Directly finding the global optimizers\nusing Eqn. 4 can be a bit complicated and highly non-intuitive, due to highly nonlinear structure of\nEqn. 3. However, there are nice structures we can leverage, as we will demonstrate below."}, {"title": "BEYOND FIXED PARAMETER SPACE: THE SEMI-RING STRUCTURE", "content": "THE SEMI-RING STRUCTURE OF THE SOLUTION SPACE\nWe define the weight space $Z_q = \\{z\\}$ to include all the weight matrices with $q$ hidden nodes\n($Z_0$ means an empty network), and $Z = \\bigcup_{q\\geq 0} Z_q$ be the solution space of all different number\nof hidden nodes. Interestingly, Z naturally is equipped with a semi-ring structure, and each term\nof the loss function can effective interact with such a semi-ring structure, yielding provable global\noptimizers, including both the Fourier solutions empirically reported in previous works (Zhou et al.,\n2024; Gromov, 2023), and the perfect memorization solution (Morwani et al., 2023).\nTo make our argument formal, we start with a few definitions.\nDefinition 2 (Order of z). The order $ord(z)$ of $z \\in Z$ is its number of hidden nodes.\nDefinition 3 (Scalar multiplication). $az \\in Z$ is element-wise multiplication $[az_{pkj}]$ of $z \\in Z$.\nDefinition 4 (Identification of Z). In Z, two solutions of the same order that differ only by a per-\nmutation along hidden dimension j are considered identical.\nFor any two solutions $z_1 := \\{z_{pkj}^{(1)}\\}$ and $z_2 := \\{z_{pkj}^{(2)}\\}$, we can define their operations:\nDefinition 5 (Addition and Multiplication in Z). Define $z = z_1 + z_2$ in which $z_{pk\\cdot} :=$\n$concat(z_{pk\\cdot}^{(1)}, z_{pk\\cdot}^{(2)})$ and $z = z_1 * z_2$, in which $z_{pk\\cdot} := \\{z_{pkj}^{(1)}z_{pkj}^{(2)}\\}$. The addition and multiplication\nrespect Hermitian constraints and the identity element 1 is the 1-order solutions with $\\{z_{pk0} = 1\\}$.\nNote that the multiplication definition is one special case of Khatri-Rao product (Khatri & Rao,\n1968). Although the Kronecker product and concatenation are not commutative, thanks to the iden-\ntification (Def. 4), it is clear that $z_1 + z_2 = z_2 + z_1$ and $z_1 * z_2 = z_2 * z_1$ and thus both operations\nare commutative. Then we can show:\nTheorem 2 (Algebraic Structure of Z). $(Z, +, *)$ is a commutative semi-ring.\nAs we will see, the semi-ring structure of Z paves the way to construct explicitly global optimizers."}, {"title": "THE MONOMIAL POTENTIALS AND ITS CONNECTION TO SEMI-RING Z", "content": "Now let us study the structure of the loss function Eqn. 3 and how they are related to the semi-ring\nstructure of Z. For this, we first define the concept of monomial potentials:\nDefinition 6 (Monomial potential (MP)). $r(z) := \\sum_j \\prod_{(p,k)\\in idx(r)} z_{pkj}$ is called monomial poten-\ntial (MP), where idx(r) specifies the indices involved in the monomial terms.\nFollowing this definition, terms in the loss function (Theorem 1) are examples of MPs.\nObservation 1 (Specific MPs). $\\Gamma_{k_1k_2k}(z)$ and $\\Gamma_{pk_1k_2k}(z)$ defined in Theorem 1 are MPs.\nSo what is the relationship between MPs, which are functions that map a weight z to a complex\nscalar, and the semi-ring structure of Z? The following theorem tells that MPs are ring homomor-\nphism, that is, these mappings respect addition and multiplication:\nTheorem 3. For any monomial potential $r : Z \\rightarrow C$, $r(1) = 1$, $r(z_1 + z_2) = r(z_1) + r(z_2)$ and\n$r(z_1 * z_2) = r(z_1)r(z_2)$ and thus r is a ring homomorphism.\nObservation 2. The order function $ord : Z \\rightarrow N$ is also a ring homomorphism.\nSince the loss function $l(z)$ depends on the weight z entirely through $\\Gamma_{k_1k_2k}(z)$ and $\\Gamma_{pk_1k_2k}(z)$,\nwhich are MPs, due to the property of ring homomorphism, it is possible to construct a global\noptimizer from partial solutions that satisfy only some of the constraints\u00b9:\nLemma 2 (Composing Partial Solutions). If $z_1$ has 0/1-sets $(R_1^-, R_1^+)$ and $z_2$ has 0/1-sets\n$(R_2^-, R_2^+)$, then (1) $z_1 * z_2$ has 0/1-sets $(R_1^-\\cup R_2^-, R_1^+ R_2^+)$. (2) $z_1 + z_2$ have 0/1-sets\n$(R_1^-\\cap R_2^-, R_1^+\\cup R_2^+)$.\nOnce we reach 0/1-sets $(R_c\\cup R_n\\cup R_*, R_g)$, we find a global optimizer. In addition, we also imme-\ndiately know that there exists infinitely many global optimizers, via ring multiplication (Def. 5):\nDefinition 7 (Unit). z is called a unit if $\\Gamma_{kkk}(z) = 1$ for all $k \\neq 0$.\nCorollary 1. If z is a global optimizer and y is a unit, then $z * y$ is also a global optimizer."}, {"title": "COMPOSING GLOBAL OPTIMIZERS", "content": "CONSTRUCTING PARTIAL SOLUTIONS WITH POLYNOMIALS\nWhile intuitively one can get global optimizers by manually crafting some partial solutions and\ncombining, in this section, we provide a more systematic approach to compose global optimizers as\nfollows. Since Z enjoys a semi-ring structure, we consider a polynomial in Z in the following form:\n$z = u^L + c_1 * u^{L-1} + c_2 * u^{L-2} + ... + c_L$\\n(5)\nwhere the generator u and coefficients $c_q$ are order-1 and the power operation $u^L$ is defined by ring\nmultiplication. The following construction of a polynomial leads to a partial solution.\nTheorem 4 (Construction of partial solutions). Suppose u has 1-set $R_1$, $\\Omega_R(u) := \\{r(u)|r \\in\nR\\} \\subset C$ is a set of evaluations on R (multiple values counted once), then if $1 \\notin \\Omega_R$, then the\npolynomial solution $p_R(u) := \\prod_{s \\in \\Omega_R(u)}(u + \\hat{s})$ has 0/1-set (R, R\u2081) up to a scale. Here $\\hat{s}$ is any\norder-1 weight that satisfies $r(\\hat{s}) = -s$ for any $r \\in R \\cup R^+$. For example, $\\hat{s} = -s^{1/3^{1}}$.\nFor convenience, we use $p(u)$ to represent the maximal polynomial, i.e., when $R =$\n$arg \\max_{R \\subset \\Omega_R(u)} |\\Omega_R(u)|$ is the largest subset of MPs with $1 \\notin \\Omega_R(u)$. Our goal is to find low-order\n(partial) solutions, since gradient descent prefers low order solutions (see Theorem 6). Although\nthere exist high-degree but low-order polynomials, e.g., $u^q + 1$, in general, degree $L$ and order $q$ are\ncorrelated, and we can find low-degree ones instead. To achieve that, u should be properly selected\n(e.g., symmetric weights) to create as many duplicate values (but not 1) in R as possible.\nMathematically, the kernel $Ker(r) := \\{z : r(z) = 0\\}$ of a ring homomorphism r is an ideal of the ring,\nand the intersection of ideals are still ideals. For brevity, we omit the formal definitions."}, {"title": "COMPOSING GLOBAL OPTIMIZERS", "content": "We first consider the case that the generator u is only nonzero at frequency k (and thus -k by\nHermitian constraints), but zero in other frequencies, i.e., $u_{pk'0} = 0$ for $k' \\neq \\pm k$. Such solutions\ncorrespond to Fourier bases in the original domain. Also, u has 1-set $R_1 = \\{\\Gamma_{kkk}\\}$. This means that\nu can be characterized by three numbers $u_{ak0} = a$, $u_{bk0} = b$, and $u_{cko} = c$ with abc = 1. In this\ncase, only a subset of monomial potentials (MPs) whose indices only involve a single frequency k\nare non-zero (e.g., $\\Gamma_{k,-k,k} \\in R_c$ and $\\Gamma_{b,-k,k,k} \\in R_n$), which makes our construction much easier.\nFollowing Theorem 4, we can construct different partial solutions. Some examples are shown in\nTable 1, which do not reach the complete set $R_c\\cup R_n\\cup R_*$ and therefore are not global. Note that\nit is possible to create a generator so that all MPs are not 1 (e.g., $u_{3c} * u_{4a}$), but then $|\\Omega_R(u)|$ will\nbe too large, producing high-degree polynomials (e.g., $u_{3c} * u_{4a}$ gives a 10-th-degree polynomial).\nHowever, utilizing these partial solutions, with Lemma 2 we can construct global optimizers:\nCorollary 2 (Order-6 global optimizers). The following \u201c3 \u00d7 2", "odd)": "n$z_{F6}^{(k)} := p(u_{syn}^{(k)}) \\text{ and } z_{F6} := \\frac{1}{\\sqrt{6}}\\sum_{k=1}^{(d-1)/2} z_{syn}^{(k)} * z_v^{(k)} * y_k$\n(6)\nHere $z_{syn}^{(k)} := u_{syn}^{(k)} + u_v^{(k)}$ (i.e., not maximal polynomial), where $u_{syn}$ and $u_v$\nare defined in Table 1. y is an order-1 unit. As a result, $ord(z_{F6}) = 3 \\cdot 2 \\cdot 1 \\cdot (d - 1)/2 = 3(d - 1)$\nand each frequency are affiliated with 6 hidden nodes (order-6).\nOther solutions. We may replace $u_{syn}$ and $u_v$ with any other pairs that collectively cover all MPs.\nFor example, $u_{syn}$ can be combined with any of $\\{u_{3c}, u_{3a}, u_{4a}\\}$, and $u_{v=\\pm i}$ can be coupled with\n$u_{3a}$ or $u_{4a}$, etc. Here we pick one with a small order. Compared to construction from Gromov\n(2023), ours is much more concise and does not use infinite-width approximation.\nEven d. For even d, simply replace (d \u2013 1)/2 with $\\lfloor(d - 1)/2\\rfloor$ and add an additional order-2 term\n$p(u_{one}) = u_{one} + 1$ (Tbl. 1) for the frequency d/2. Note that the frequency k = d/2 only has $\\Gamma_{kkk}$,\n$\\Gamma_{akkk}$ and $\\Gamma_{bkkk}$, and all other conjugate combinations are absent. Thus $u_{u_{one}} + 1^k$ covers them all.\nFig. 2 shows a case with d = 7. In this case, each frequency, out of (d - 1)/2 = 3 total number of\nfrequencies, is associated with 6 hidden nodes. If we remove the last term in the loss that corresponds\nto $R_*$, then an order-3 solution suffices (i.e. $z_{syn} = p(U_{syn})$)."}, {"title": "COMPOSING GLOBAL SOLUTIONS", "content": "Using polynomials, we can also construct perfect-memorization solutions. For this, we first define\ntwo generators $u_a$ with $u_{a,ko} = [\\omega, 1, \\bar{\\omega}]\\[(k \\neq 0), and u_b with u_{b,ko} = [1, \\omega, \\omega]\\[(k \\neq 0$. Here\n$\\omega_d := e^{2\\pi i / d}$ is the d-th root of unity."}, {"title": "GRADIENT DYNAMICS", "content": "Now we have characterized the structures of global optimizers. One natural question arises: why\ndoes the optimization procedure not converge to the perfect memorization solution $z_M$, but to the\nFourier solutions $z_{F6}$ and $z_{F4/6}$? The answer is given by gradient dynamics.\nLet $r = [\\Gamma_{k_1k_2k}, \\Gamma_{pk_1k_2k}] \\in C^{4d^3}$ be a vector of all MPs, and $J := \\frac{\\partial r}{\\partial W}$ be the Jacobian matrix\nof the mapping $r = r(z(W))$ in which W is the collection of original weights. Note that when we\ntake derivatives with respect to r and apply chain rules, we treat r and its complex conjugate (e.g.,\n$\\Gamma_{kkk}$ and $\\bar{\\Gamma_{-k,-k,-k}} = \\Gamma_{kkk}$) as independent variables. Since we run the gradient descent on W, will\nsuch (indirect) optimization leads to a descent of r towards the desired targets (Lemma 1)? This is\nconfirmed by the following theorem:\nTheorem 5 (Dynamics of MPs). The dynamics of MPs satisfies $\\dot{r} = -JJ^*\\nabla_r l$, which has positive\ninner product with the negative gradient direction $-\\nabla_r l$.\nCorollary 1 shows that by ring multiplication, we could create infinitely many global optima from a\nbase one. The following theorem answers which solution gradient dynamics picks.\nTheorem 6 (The Occam's Razer: Preference of low-order solutions). If $z = y * z'$ and both z (of\norder q) and z' are global optimal solutions, then there exists a path of zero loss connecting z and z'\nin the space of $Z_q$. As a result, lower-order solutions are preferred if trained with L2 regularization.\nThis shows that gradient dynamics with weight decay will pick a lower-order (i.e., simpler) solution,\nsuggests that perfect memorization may not be not favorable in dynamics. The following theorem\nshows that the dynamics also enjoys asymptotic freedom:\nTheorem 7 (Infinite Width Limits at Initialization). Considering the modified loss of Eqn. 3 with\nonly the first two terms: $l_k := -2\\Gamma_{kkk} + \\sum_{k_1k_2} |\\Gamma_{k_1k_2k}|^2$, if the weights are i.i.d Gaussian and\nnetwork width $q \\rightarrow +\\infty$, then JJ* converge to diagonal and the dynamics of MPs is decoupled."}, {"title": "EXPERIMENTS", "content": "Setup. We train the 2-layer MLP on the modular addition task, which is a special case of outcome\nprediction of Abelian group multiplication. We use Adam optimizer with learning rate 0.01, MSE\nloss, and train for 10000 epochs with weight decays. We tested on |G| = d \u2208 {23, 71, 127}. All\ndata are generated synthetically and training/test split is 90%/10%.\nSolution Distributions. As shown in Fig. 3, we see order-4 and order-6 solutions in each frequency\nemerging from well-trained networks on d = 23. The mixed solution $z_{F4/6}$ can be clearly observed\nin a small-scale example (Fig. 6). This is also true for larger d (Fig. 4). Although the model is\ntrained with heavily over-parameterized networks, the final solution order remains constant, which\nis consistent with Corollary 1. Large weight decay shifts the distribution to the left (i.e., low-order\nsolutions) until model collapses (i.e., all weights become zero), consistent with our Theorem 6 that\ndemonstrates that gradient descent with weight decay favors low-order solutions. Similar conclu-\nsions follow for fewer and more overparameterization (Appendix H).\nExact match between theoretical construction and empirical solutions. A follow-up question\narises: do the empirical solutions match exactly with our constructions? After all, distribution\nof solution order is a rough metric. For this, we identify all solutions obtained by gradient de-\nscent at each frequency, factorize them and compare with theoretical construction up to conjuga-\ntion/normalization. To find such a factorization, we use exhaustive search (Appendix H).\nThe answer is yes. Tbl. 2 shows that around 95% of order-4 and order-6 solutions from gradient\ndescent can be factorized into 2 \u00d7 2 and 2 \u00d7 3 and each component matches our theoretical construc-\ntion in Corollary 2 and 4, with minor variations. Furthermore, when d is large, most of the solutions"}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this work, we propose CoGO (Composing Global Optimizers), a theoretical framework that mod-\nels the algebraic structure of global optimizers when"}]}