{"title": "MAKING LLMS REASON?\nTHE INTERMEDIATE LANGUAGE PROBLEM IN\nNEUROSYMBOLIC APPROACHES", "authors": ["Alexander G. Beiser", "David penz"], "abstract": "Logical reasoning tasks manifest themselves as a challenge to Large Language\nModels (LLMs). Neurosymbolic approaches use LLMs to translate logical rea-\nsoning problems formulated in natural language into a formal intermediate lan-\nguage. Subsequently, the usage of symbolic reasoners yields reliable solving\nthereof. However, LLMs often fail in translation due to poorly chosen intermedi-\nate languages.\nWe introduce the intermediate language problem, which is the problem of choos-\ning a suitable formal language representation for neurosymbolic approaches. The-\noretically, we argue that its origins lie in the inability of LLMs to distinguish\nsyntax from semantics and the relative independence of the problem from its rep-\nresentation. We showcase its existence experimentally by contrasting two inter-\nmediate languages, Answer Set Programming and the Python Knowledge Engine.\nIn addition, we demonstrate the effects of varying degrees of supplementary con-\ntext information. Our results show a maximum difference in overall-accuracy of\n53.20% and 49.26% in execution-accuracy. When using the GPT40-mini LLM we\nbeat the state-of-the-art in overall-accuracy on the ProntoQA dataset by 21.20%\nand by 50.50% on the ProofWriter dataset.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) perform surprisingly well on logical reasoning tasks (Saparov &\nHe, 2023). Actually, they perform better than humans on some datasets while still falling prey to the\nsame fallacies as humans do (Lampinen et al., 2024). Take, for example, the following two syllogism\nchains and determine whether they are correct or false: (1) All cats are mammals, all mammals are\nanimals, all cats are animals, and (2) all tumpus are wumpus, all wumpus are vumpus, all tumpus\nare vumpus. Research indicates that both humans and LLMs perform better on reasoning chains of\ntype (1) (real ontologies) than on type (2) (fictional ones) (Lampinen et al., 2024). This is interesting,\nas in logic, there is no distinction between (1) and (2) (both are correct), as there is a separation of\nsemantics from its representation (syntax).\nImproving performance of LLMs on logical reasoning tasks (datasets) must involve getting LLMs\nto reason more abstractly, thereby better separating semantics from syntax. One such attempt is\nChain of Thought (CoT) (Wei et al., 2022) prompting. However, LLM's reasoning chains are non-\nfaithful in general (Lyu et al., 2023). Faithful reasoning chains are obtained by Neurosymbolic AI\nby using LLMs to translate a logical reasoning problem (posed in a natural language) into a formal\n(symbolic) language. This translation is subsequently (faithfully) solved by a symbolic reasoner.\nFinally, its output is translated back into natural language. One such approach is Logic-LM (Pan\net al., 2023).\nHowever, these neurosymbolic approaches fall short of acknowledging the impact of the intermediate language\u00b9 on translation. This paper investigates the current state-of-the-art (SOTA) approach\nLogic-LM and demonstrates that the choice of representation language matters. For this task, we"}, {"title": "2 RELATED WORK", "content": "The seminal Attention is All You Need (Vaswani et al., 2017) paper laid the foundations for LLMs\nsuch as GPT-4 (Achiam et al., 2024), Gemini (Georgiev et al., 2024), or Llama (Grattafiori et al.,\n2024). Surprisingly, LLMs perform decent on reasoning tasks, especially if prompted via a Chain-\nof-Thought (CoT) approach (Wei et al., 2022). This behavior is part of an emergent property of\nLLMs named in-context-learning or few-shot-learning (Shanahan, 2024). Although CoT achieves\nastonishing results on reasoning benchmarks, it is not faithful\u00b2 (Lyu et al., 2023). Further, it is argued\nthat not only is the reasoning not faithful but also, that LLMs \u201cremain limited in their capabilities\nto performing probabilistic retrieval\u201d and, therefore, that \u201cpure statistical learning can not cope with\nthe combinatorial explosion inherent in many common-sense reasoning tasks\u201d (Panas et al., 2024).\nRelated results show that LLMs do not acquire systematic problem solving skills (Dziri et al., 2023).\nThe logical reasoning capability of LLMs is measured with datasets such as the ProntoQA (Saparov\n& He, 2023), the ProofWriter (Tafjord et al., 2021), or the FOLIO (Han et al., 2024) dataset. Im-\nproving LLM's reasoning capability was approached by different angles. Geva et al. (2020) try\nto improve numerical capabilities by injecting additional numerical data in the pre-training phase\nand further fine-tune the model. Other approaches focus on fine-tuning (Yang et al., 2022). How-\never, it was argued that these approaches fail to address the inherent inability of LLMs to reason\nmathematically (Panas et al., 2024).\nNeurosymbolic AI (Garcez & Lamb, 2023) approaches offer an alternative to the pure sub-symbolic\napproaches. Examples include differentiable logic (Badreddine et al., 2022), designing neural\nnetworks that act as Turing machines (Siegelmann & Sontag, 1995), or visual question answer-\ning with logic-programming and deep learning (Eiter et al., 2023). For LLM logical reasoning\ntasks, Logic-LM (Pan et al., 2023) is a neurosymbolic method that combines LLMs with symbolic\nsolvers. The studied solvers include a Prolog (K\u00f6rner et al., 2022), First-Order-Logic (FOL) (En-\nderton, 1972), Constraint-Satisfaction-Problems (Kumar, 1992), and a Satisfiability-Problem (Cook,\n1971) solver. Implementation-wise, Logic-LM uses Python libraries for these solvers. For Pro-\nlog they use Pyke (Frederiksen, 2008), for SMT solving (SAT) they use Z3 (de Moura & Bjorner,\n2008), for FOL they use Prover9 (McCune, 2010), and for constraint solving they use the Python-\nconstraint (Niemeyer et al., 2024) library. Logic-LM++ (Kirtania et al., 2024) claims to improve on\nLogic-LM by adding an improved self-refinement module that takes more solver information into\naccount. Lam et al. (2024) acknowledge performance differences between solvers but fail to iden-\ntify that these stem from the chosen intermediate language. For knowledge based systems previous\nresearch shows that different query languages have an impact on LLM understanding (Liu et al.,\n2024).\nDiffering from these approaches, we study the impact of the used syntax inherent to the intermediate\nlanguage of neurosymbolic logical reasoners. In particular, this paper contrasts the syntax used by\nLogic-LM, to Pyke's and Answer Set Programming's (ASP) syntax. Answer Set Programming\n(ASP) (Gelfond & Leone, 2002) is a declarative problem-solving paradigm. As our ASP solver we\nuse Clingo (Kaminski et al., 2023) due to its readily available Python support."}, {"title": "3 PRELIMINARIES", "content": "We consider LLMs as black-box next-token predictor machines. This means that given the token\nvector (array) t, they select the token t in token-space T with the maximum predicted value:\n$\\displaystyle f(t) = \\arg \\max_{t \\in T} p(t|t)$\nTake for example the token-space T = {dead, alive} (reduced for this example), and the to-\nkens t = (Schr\u00f6dinger's, cat, is). Then, provided the LLM has the following p values :\n(p(alive) = 0.51,p(dead) = 0.49), we obtain f(t) = alive."}, {"title": "3.1 CHAIN-OF-THOUGHT (COT) PROMPTING", "content": "Chain-of-Thought (CoT) prompting is an in-context-learning technique that has applications ranging\nfrom helping LLMs to express their uncertainty (Xiong et al., 2024), to improving the reasoning\ncapabilities of LLMs on reasoning datasets (Wei et al., 2022). CoT nudges the LLM to mimic a\nhuman reasoning chain, where we show an example adapted from the ProntoQA dataset (Saparov\n& He, 2023) as used for Logic-LM (Pan et al., 2023):\n1 The following example showcases the line of reasoning you have to follow:\n2\nQuestion\n3 Each cat is a carnivore. Fae is a cat.\n4 True or false: Fae is a carnivore\n5\nReasoning\n6 Fae is a cat. Each cat is a carnivore. So Fae is a carnivore.\nWe show a full CoT example in the Appendix. Reasoning chains are faithful whenever the result\nfollows from the individual steps in the reasoning chain. However, LLM's reasoning chains are\nnon-faithful in general (Lyu et al., 2023)."}, {"title": "3.2 LOGIC PROGRAMMING", "content": "As intermediate languages between LLMs and symbolic reasoning, we consider Logic Programming\nlanguages. In more detail, we consider Answer Set Programming (ASP) and the intermediate lan-\nguage used for the Python knowledge engine (Pyke). We start with introducing ASP, and continue\nto introduce Pyke.\nASP is a declarative rule-based paradigm commonly used for modeling and solving complex plan-\nning or scheduling problems (Abels et al., 2021). We provide a brief summary of the main concepts\nof ASP. For details, we refer to (Eiter et al., 2009). An ASP program II consists of rules r\u2208 \u03a0of\nthe form :\n$\\displaystyle p_1(X_1) \\lor... \\lor p_\\iota(X_\\iota) :- p_{\\iota+1}(X_{\\iota+1}), ..., p_m(X_m), \\lnot p_{m+1}(X_{m+1}),...,\\lnot p_n(X_n)$\nWe call pi a literal, and X\u1d62 its term vector. Both stand for abstract concepts with an arbitrary naming.\nFor the exact syntax definition, see (Calimeri et al., 2020). For our purposes, we consider term\nvectors consisting of variables, integers, and lower- and upper-case letters. Let pi(X\u00bf) for 1 \u2264 i \u2264 l\nbe the rule's head Hr, pi(Xi) for l + 1 < i < m be the positive body B+, and pi(Xi) for m + 1 <\ni < n be the negative body B. Semantically, a rule fires (meaning (one h\u2208 Hr) Hr holds),\nwhenever its body is true. A body is true whenever all literals of B+ hold, but no literal of B\nholds. Grounding replaces variables by their concrete domain values. Grounding is computationally\nvery expensive and a topic of current research (Beiser et al., 2024). The solutions to a grounded\nprogram are called answer sets.\nPyke defines rules and facts. Rules in Pyke express if-then statements. Their syntax is Python like.\nSemantically Pyke works with forward, or backward chaining algorithms."}, {"title": "3.3 NEUROSYMBOLIC LLM REASONING", "content": "Logic-LM (Pan et al., 2023) is a neurosymbolic approach that integrates symbolic reasoners into\nthe reasoning steps of LLMs. It translates a natural language posed problem into its formal sym-\nbolic reasoner acceptable formulation. Subsequently, the symbolic reasoner solves the problem by"}, {"title": "4 THE INTERMEDIATE LANGUAGE PROBLEM", "content": "We introduce the intermediate language problem, which is the problem of choosing a suitable in-\ntermediate language for Neurosymbolic reasoning tasks, using LLMs as a translation tool between\nnatural and formal (intermediate) languages. Intuitively, the intermediate language problem stems\nfrom two observations: (i) LLMs are unable to separate syntax from semantics, and (ii) the actual\nformal intermediate language is (largely) irrelevant for formal problems. We start by justifying our\nclaim for the intertwined syntax and semantics of LLMs, which we follow by an argument for the\nirrelevancy of the choice of the formal intermediate language."}, {"title": "4.1 LLMS CANNOT SEPARATE SYNTAX FROM SEMANTICS", "content": "We base our claim LLMs cannot separate syntax from semantics on both, experimental observations\nand theoretical considerations. First observe that in logics, the representation (syntax) does not have\nan effect on semantics. Take for example the syllogism chain in Equation (1).\n$\\displaystyle (\\forall x : A(x) \\rightarrow B(X) \\land \\forall x : B(X) \\rightarrow C'(X)) \\rightarrow \\forall x : A(X) \\rightarrow C(X)$\nThis statement is a valid syllogism chain, where A, B, and C stand for abstract concepts. Therefore,\nlogically, it does not matter whether A is a cat or a wumpus. This leads us to the following obser-"}, {"title": "4.2 INTERMEDIATE LANGUAGE IS INDEPENDENT OF THE SOLVER", "content": "We take a computational complexity theory point of view, for arguing that we can effectively choose\nan arbitrary intermediate language. Therefore, we briefly introduce some necessary concepts of\ncomplexity theory, where a detailed account of this field is given in (Papadimitriou, 1994). A first\ncrucial observation to our discussion is the distinction between a formal problem, and a solver. A\nformal problem P is an instance I, with an attached question Q. On the other hand, a solver takes\na problem P, encoded in a solver specific intermediate (representation) language, and produces a\nsolution S. Take for example the famous boolean satisfiability problem (SAT). Its instance consists\nof a propositional (\u2248 boolean) formula I, such as (a \u2227 (b V c) \u2227 \u00aba), and its attached question Q\nis: Does there exist a satisfying assignment for T4? Solvers on the other hand take the instance\nI encoded in a specific way, be it the DIMACS format, in a logic programming representation,\nor hypothetically in the unicode representation from above. This is our first observation for our\nargument. Note that for SAT highly efficient solvers are available, such as Z3 (de Moura & Bjorner,\n2008).\nTaking a complexity theoretic perspective we note that SAT is in NP (Actually NP-complete (Cook,\n1971)). Remind yourself that NP means (intuitively) that the problem is decidable by a non-\ndeterministic Turing machine in polynomial time. This definition gives rise to the concept of a\ncomplexity class C, which intuitively encodes a set of problems that all can be solved by a certain\ntype of turing machine, under certain resource constraints. The last crucial concept we need to in-\ntroduce for our argument is the concept of a reduction. Intuitively, a reduction takes a problem P,\nand efficiently translates it into another problem P', s.t. the solutions exactly match.\nSuch a reduction can be of high practical use. Assume that a problem P has no efficient specialized\nsolver, but there exists an efficient reduction to SAT. Then by reducing P to SAT, P can be solved\nefficiently. Therefore, every solver suitable for solving SAT problems, can solve problems that can\nbe reduced to SAT. So, coming back to our hypothesis that for many formal problems the choice of\nformal intermediate language is irrelevant, the concept of the reduction is our second argument."}, {"title": "4.3 CHOOSING A SUITABLE INTERMEDIATE LANGUAGE", "content": "From this discussion we follow two crucial observations: First, one should pick a representation\nlanguage that is inherently beneficial for LLM translation tasks. So, as an example, we think it is\nmore suitable to encode the problem: \"If it rains, then the street is wet.\" into a propositional logic\nformulation (Rains \u2192 WetStreet) than into a bit-representation. Second, the actual encoding in the\nrepresentation language should provide supplementary information to the LLM. Going back to the\nexample, an encoding such as Rains \u2192 WetStreet, provides more supplementary information to the\nLLM than pl \u2192 p2. In our experimental evaluation we demonstrate that both observations hold."}, {"title": "5 EXPERIMENTS", "content": "We performed a set of experiments to empirically verify our intermediate language problem hypoth-\nesis, i.e., that the intermediate language has an effect on solving performance. In the following, we\nwill briefly go over our benchmark scenarios of different intermediate languages and our benchmark\nsetup before coming to our results."}, {"title": "5.1 INTERMEDIATE LANGUAGES AND ADDITIONAL CONTEXT", "content": "We conducted experiments on 11 scenarios. The scenarios differ by the used in-context-learning\ntechniques, mainly in which intermediate language is used and what additional context is provided.\nOur baselines are the scenarios used in Logic-LM. Namely the standard scenario, which is direct\nprompting of the LLM, the CoT scenario, which is the chain-of-thought prompting of the LLM,\nand the Logic-LM scenario without self-refiner, which uses a self-defined intermediate language to\ncommunicate with the Pyke solver. Our other 8 scenarios can be split into two groups: Those with\nPyke (4 scenarios), and those with ASP (4 scenarios). The differences in the scenarios of each\ngroup manifest themselves by different levels of additional supplementary information provided to\nthe LLM.\nTo showcase how the neurosymbolic scenarios differ, we show snippets of example prompts. Note\nthat we used the in-context-learning approach for these scenarios. All prompts include high-level in-\nformation about what the LLM should do, which is followed by an example problem translation into\nthe desired solver language. Consider for this the following example (snippet) from the ProntoQA,\nwhich we use as our running example to demonstrate the different example translations:\n1 Problem: Wren is a tumpus. Each tumpus is a wumpus.\n2 Wumpuses are brown. Each wumpus is not metallic.\n3 Question:\n4 Is the following statement true or false? Wren is not metallic.\nWhen translating the above-stated problem by hand to ASP, one has to define suitable predicates,\nterms, rules, and a query. Suitable predicates are wumpus(X), brown(X), and metallic(X). Terms\nare the variable X and wren, and rules encode the relationships, such as Each tumpus is a wumpus\nby wumpus(X) :- tumpus(X) in ASP. Therefore, for the ASP scenario Text we use the following\nin-context-learning example translation:\n1 Facts:\n2 tumpus (wren).\n3 Rules:\n4 wumpus(X):-tumpus(X). brown(X):-wumpus(X). -metallic(X):-wumpus(X).\n5 Query:\n6 -metallic(wren)."}, {"title": "5.2 BENCHMARK SETUP", "content": "We used the ProntoQA and ProofWriter datasets, as their problems contain fragments of first-order\nlogic that are solvable by ASP. The datasets were used in the Logic-LM configuration, where the\nProntoQA dataset was used in the fictional characters variant in the 5 hops version with 500 sam-\nples and the ProofWriter dataset with 600 samples and 5 hops reasoning depth. Our experiments\nwere conducted on an adapted Logic-LM implementation that features an ASP symbolic solver and\nresult interpreter based on Clingo, a new Pyke solver and interpreter, and different levels of ad-\nditional context according to the previous section. As we do not compare the understandability of\nthe syntax errors messages of different solvers, we disabled the self-refinement module. For the\nLLM we used GPT-40-mini without any modifications and temperature 0. We measured the num-\nber of correct syntactical- (#EXEC) and the number of correctly solved instances (#TRUE). An\ninstance is syntactically correct, whenever the produced output of the LLM adheres to the solver\ninput language. Similarly, we consider a problem as correctly solved whenever the solver input\nis syntactically correct, and the solver produces the correct answer. From these we compute the\nexecution-rate, which is the fraction of correct syntactical outputs (Exec-Rate, $ \\frac{\\text{#EXEC}}{\\text{#D}}$ ), execution-\naccuracy, which is the fraction of correctly solved instances of all syntactically correct ones (Exec-Acc, $ \\frac{\\text{#TRUE}}{\\text{#EXEC}}$), and overall-accuracy, which is the fraction of correctly solved instances over the\nentire dataset (Overall-Acc, $ \\frac{\\text{#TRUE}}{\\text{#D}}$)."}, {"title": "5.3 RESULTS", "content": "We show in Table 1 the results of our experiments 10. As discussed above, the main aim of\nour experiments is to empirically verify our intermediate language problem hypothesis from\nSection 4. Secondary, we analyze which techniques help in obtaining better results. To\nverify our hypothesis, observe the pairs of Pyke and ASP for {No-C., Text, Prog., Comm.}.\nObserve the difference in execution-rate for ProntoQA: which is {82.00, 99.00, 99.60, 99.80}\nfor Pyke, and {86.0,90.40, 91.80, 97.00} for ASP. However, not only the execution-rate dif-\nfers, but also the execution-accuracy, which is {50.73, 76.16, 94.12, 91.18} for Pyke, and\n{49.30, 77.88, 88.45, 98.56} for ASP. Further, observe the differences for ProofWriter in execution-\nrate, which is {81.50, 65.17, 78.67,78.33} for Pyke and {95.33, 82.17, 79.33, 90.17} for ASP\nand the difference in execution-accuracy for Pyke, which is {52.35, 72.63, 70.13, 74.04} and\n{51.57, 66.73, 67.86, 77.45} for ASP. Also the overall-accuracy measures differ.\nNext we show the impact of the increase in additional context (Figure 2). We observe a monotone\nincrease in overall-accuracy for the ProntoQA dataset when using ASP from 42.40% (No-C.) to\n95.60% (Comm.), which is a difference of 53.20%. The execution-rate difference between No-C.\nand Comm. is 49.26%. For Pyke the difference is still striking, as it goes from 41.60% (No-C.) to\n91.00% (Comm.), however it is not monotone. The ProofWriter dataset shows a smaller increase\nfrom 49.17% (No-C.) to 69.83% for ASP and 42.67% to 58.00% for Pyke.\nComparing the baseline measures to the other experiments we observe an increase in overall-\naccuracy for both the ProntoQA and ProofWriter datasets. For ProntoQA the increase from Baseline\n(Logic-LM) to ASP (Comm.) is 21.20% in overall-accuracy and 24.16% in execution-accuracy,\nwhereas for ProofWriter the increase from Baseline (Standard) to ASP (Comm.) is 50.50% in\noverall-accuracy and 58.12% execution-accuracy."}, {"title": "5.4 INTERPRETATION", "content": "We can confirm our intermediate language hypothesis, as we were able to show differences in\noverall- and execution-accuracy between different intermediate languages. In addition to that, our\nresults suggest that an increase in context information leads to an increase in accuracy. This can be\nseen by the increase in accuracy when going from the no-context scenario to the comments scenario.\nThe execution-rate remains relatively unaffected by different intermediate languages."}, {"title": "6 DISCUSSION", "content": "In this paper we discussed the impact of the representation language for solving logical reasoning\nproblems in the neurosymbolic approach for LLMs. Using the neurosymbolic approach one uses\nLLMs for translating a natural-language-posed problem into an intermediate formal language, which\nis subsequently solved by a symbolic solver. This is enabled by the in-context-learning capability\nof LLMs. Previous results showed that with this approach the logical reasoning capability of LLMs\ncan be greatly enhanced (Pan et al., 2023).\nAlthough these approaches discuss the impact of different solvers, they fail to recognize that the\nintermediate formal language is the main factor that impacts performance in terms of overall- and\nexecution-accuracy. This stems from two facts: (i) LLMs suffer from concept entanglement, as\ndo humans (Lampinen et al., 2024). Intuitively, this means that when logical reasoning problems\nare posed in a counter-intuitive way, humans (and LLMs) perform worse (on average). (ii) Formal\nproblems (like a logical reasoning task) are independent of their solver, and consequently of their\nrepresentation language.\nFrom this analysis we introduced the intermediate language problem, which is the problem of choos-\ning a suitable formal language for the symbolic solver. We demonstrated its existence through ex-\nperiments and additionally showed that additional context in the in-context learning tends to yield\nbetter results. By using our highest additional context configuration, we were able to beat the current\nstate-of-the-art."}, {"title": "7 LIMITATIONS", "content": "Although we are confident in our observations about the intermediate language problem and the gen-\neral tendency that adding more context information to the in-context learning technique will improve\nlogical reasoning capabilities, we are also aware of our experimental and inherent limitations.\nExperiment-wise, we conducted experiments on the GPT-40-mini model for economic and environ-\nmental reasons. We expect a significant increase in all scores when other, more advanced or bigger,\nmodels are used, such as GPT-40, GPT-01, GPT-03, or Gemini 2. The increase in performance can\nrender the accuracy differences on the ProntoQA and ProofWriter datasets insignificant. We are still\nconfident that when moving to harder datasets, such as FOLIO, or when using different intermediate\nlanguages, the effects can be reproduced.\nInherent to the neurosymbolic approach is the inclusion of a separate symbolic reasoning system.\nHowever, in an ideal integration of a symbolic solver into an LLM, the symbolic solver's details are\nhidden from the end-user. We mean by that the hiding of the symbolic solver from the end-user in the\nchat and in the API and the automatic decision of the LLM, when the usage of the symbolic solver\nis possible and beneficial. Nonetheless, the symbiosis of the LLM with the symbolic solver into\none coherent system that automatically detects when the usage of the symbolic solver is beneficial,\nmight pose a major challenge."}, {"title": "A APPENDIX", "content": "A.1 ADDITIONAL EXPERIMENTAL DETAILS\nWe show in Figures 3\u20136 our analysis of the distribution of errors. No distribution is apparent. There\nis no example, where all methods fail. However, there are some where all methods succeed."}, {"title": "A.1.1 WHY DO PARSING ERRORS OCCUR?", "content": "A qualitative analysis of the errors results in an observation that the (strong) negation in our ASP\nencoding seems to be hard. Take for example ProntoQA example number 94 (ASP with Comments).\nThere ChatGPT produces the erroneous results:\n1 \"Facts:\\n```\\n% Max is (..) -not (spicy(X)) :- vumpus(X). [..]\nIn ASP a statement like -not is not allowed, as it mixes default with strong negation.\nFor Pyke, we take example ProntoQA example 197 (Prog.). There ChatGPT translated the fact to a\nrule:"}, {"title": "A.2 FURTHER DETAILS ON LOGIC-LM", "content": "In addition to the already discussed concepts in the main part, we now present the self-refiner: If the\ninput contains syntax errors, the symbolic reasoner produces an error with a suitable error message.\nWhenever Self-Refinement is activated, the system tries to correct the syntax error, by re-prompting\nthe problem formulator with the error message from the symbolic reasoner."}, {"title": "A.2.1 RUNNING EXAMPLE", "content": "Logic-LM uses five different datasets to compare their results, of four different symbolic reasoners.\nWe explain the workings of Logic-LM along the ProntoQA dataset, with their logic programming11\nformulation using the Pyke solver. The first part of the example shows the problem, the second one\nthe question. The goal is to decide the question, which is a true or false question, based on the\ninferences and facts in the problem:\n1 Problem: Wren is a tumpus. Each tumpus is a wumpus.\n2 Wumpuses are brown. Each wumpus is not metallic.\n3 Question:\n4 Is the following statement true or false? Wren is not metallic.\nThis example needs two inference steps, (i) deduce that Wren is a wumpus, (ii) that wren is not\nmetallic. The number of necessary inference steps for ProntoQA can be set as a parameter in the\ndataset generation phase. Further, the ProntoQA dataset includes misleading statements, such as\nWumpuses are brown, which do not help in the inference."}, {"title": "A.2.2 PROBLEM FORMULATOR", "content": "For translating the natural language problem into the formal language, Logic-LM leverages on the\nfew-shot learning technique. Thereby, in addition to the problem specification, detailed instruction\non how to convert the natural language to the formal language are given. Furthermore, they provide\none example to the LLM, including problem specification, formal language translation, and result.\nWe show an example for the in-context learning for the ProntoQA Saparov & He (2023) dataset\nprompt. The LLM translates a single ProntoQA problem into a formal language that can be parsed\nwith a rule based parser to the Pyke Frederiksen (2008) Prolog style. The ProntoQA data consists of\na general task description, followed by an example problem, including an example translation. We\nstart with the general task description:\n1 Task Description: You are given a problem description and a question.\n2 The task is to: 1) define all the predicates in the problem\n3 2) parse the problem into logic rules based on the defined predicates\n4 3) write all the facts mentioned in the problem\n5 4) parse the question into the logic form\nThis is followed by an example problem. In the following we show a snippet of the actual example\nproblem.\n1 Problem [Snippet]: Alex is a tumpus. Tumpuses are vumpuses. Each vumpus\nis a\n2 yumpus. Yumpuses are numpuses. Each numpus is a dumpus. Every dumpus is\nnot shy.\n3 Question: True or false: Alex is not shy.\nFinally, Logic-LM provides an example translation into their intermediate language format. First\nthe predicate specification is shown, followed by the facts, and finally by the rules."}, {"title": "A.2.3 SYMBOLIC REASONER, INTERPRETER AND SELF-REFINER", "content": "Logic-LM takes the output of the LLM, parses it into Pyke's format, and then calls and interprets\nPyke. The parsing is done entirely in Python, which splits the output into a fact, and a rule knowledge\nbase. From this knowledge base, Pyke uses its backward and forward chaining mechanisms to obtain\nan answer. For ProntoQA, Logic-LM uses a rule-based mechanism to interpret the answer.\nBelow, we show how the first rule \"Each tumpus is a wumpus\" of our running example (which we\nassume is correctly translated) gets parsed into Pyke's format. First we show the translated rule:\n1 Tumpuses($x, True) >>> Wumpus ($x, True)\nThe symbolic reasoner parses the translated rule to the following Pyke rule:\n1 foreach\n2\nfacts. Tumpus ($x, True)\n3 assert\n4\nfacts. Wumpus ($x, True)\nIf the symbolic reasoner is unable to execute the program (for example, due to a syntax error), then\nthe (optional) self-refiner takes the error message of the symbolic solver into account. Ideally, the\nthen-generated is correct."}, {"title": "A.2.4 DISCUSSION OF LOGIC-LM", "content": "Logic-LM claims that their method shows an improvement of 39.20% over standard prompting,\nand 18.40% over CoT prompting for GPT-3.5. However, as they are using two different GPT-\n3.5 versions (gpt-3.5-turbo and text-davinci-003), and further not show how they compute these\nimprovement values, we were not able to verify their claim. Although we acknowledge that both,\nthe relative improvement for gpt-3.5-turbo, and for text-davinci-003, is approximately in this range\n(see Appendix Section A.2.5).\nAlthough the idea of self-refinement is promising, the current self-refinement mode is less significant\nthan choosing a better representation language. Their maximum increase in execution-accuracy is\n2.50% points (ours 49.82% points), while their maximum execution-rate increase is 17.60% points\n(ours 17.80% points). Lastly, as they are randomly guessing solutions when they encounter a syntax"}]}