{"title": "DRL-STNet: Unsupervised Domain Adaptation for Cross-modality Medical Image Segmentation via Disentangled Representation Learning", "authors": ["Hui Lin", "Florian Schiffers", "Santiago L\u00f3pez-Tapia", "Neda Tavakoli", "Daniel Kim", "Aggelos K. Katsaggelos"], "abstract": "Unsupervised domain adaptation (UDA) is essential for medical image segmentation, especially in cross-modality data scenarios. UDA aims to transfer knowledge from a labeled source domain to an unlabeled target domain, thereby reducing the dependency on extensive manual annotations. This paper presents DRL-STNet, a novel framework for cross-modality medical image segmentation that leverages generative adversarial networks (GANs), disentangled representation learning (DRL), and self-training (ST). Our method leverages DRL within a GAN to translate images from the source to the target modality. Then, the segmentation model is initially trained with these translated images and corresponding source labels and then fine-tuned iteratively using a combination of synthetic and real images with pseudo-labels and real labels. The proposed framework exhibits superior performance in abdominal organ segmentation on the FLARE challenge dataset, surpassing state-of-the-art methods by 11.4% in the Dice similarity coefficient and by 13.1% in the Normalized Surface Dice metric, achieving scores of 74.21% and 80.69%, respectively. The average running time is 41 seconds, and the area under the GPU memory-time curve is 11,292 MB. These results indicate the potential of DRL-STNet for enhancing cross-modality medical image segmentation tasks.", "sections": [{"title": "1 Introduction", "content": "In the realm of medical imaging, accurate segmentation of anatomical structures is crucial for diagnostics, treatment planning, and patient monitoring. However, acquiring annotated data for every imaging modality is both costly and time-consuming. This challenge is exacerbated when multiple modalities are involved, as it is impractical to obtain paired data for every patient due to logistical constraints."}, {"title": "2 Method", "content": "In this study, annotations $Y^a$ for volume $X^a$ from the source modality $a$ (e.g., CT scans) are available, while annotations for the volume $X^b$ from the target modality $b$ (e.g., MRI scans) are not available. The goal is to achieve precise segmentation on the target volume, a common challenge in clinical applications where obtaining annotations can be time-consuming, costly, or logistically difficult. Additionally, acquiring multiple imaging modalities from the same patient can be challenging due to logistical constraints. Even when possible, the process can take place several days apart, and the data may not be aligned. These factors often result in unpaired datasets $X^a$ and $X^b$. This study addresses these issues by focusing on unsupervised domain adaptation (UDA) to improve cross-modality segmentation accuracy. While this paper specifically addresses CT-to-MRI translation, the methodology can be applied to other modality pairs, such as PET-to-CT or ultrasound-to-MRI, depending on the specific clinical requirements and data availability.\nAs shown in Fig. 1, the proposed DRL-STNet framework addresses unsupervised domain adaptation (UDA) for cross-modality segmentation through image translation and segmentation. The framework includes a 2D image translation network that converts slices from the source modality $X^a$ to the target modality $X^b$. This allows us to synthesize an artificial target dataset (e.g., MRI) from the source data (CT) with ground-truth segmentation labels, enabling the training of a segmentation model that works in the MRI domain. While training a segmentation model"}, {"title": "2.1 Image Translation", "content": "Jiang et al. and Yao et al. have shown that disentangled learning is highly effective in style transfer, particularly for cross-modality medical imaging, while maintaining anatomical content. Inspired by them, our image translation model is composed of one shared content encoder $E_c$, two style encoders $E_a$ and $E_b$, one shared decoder $G$, and two image discriminators $D_a$ and $D_b$, and one content discriminator $D_c$, as depicted in Fig. 2.\nAll encoders and the decoder are based on ResNet, and all discriminators are based on LSGAN . The image in each domain is disentangled into the content and style representations, $c_a$, $s_a$, $c_b$, and $s_b$. Each representation is a feature map obtained from the content or style encoder with a size of $C \\times H \\times W$ (128 \u00d7 128 \u00d7 128 in the following experiments), where $C, H,W$, respectively, represents the channel number, height, and width. The decoder $G$ reconstructs images by combining the content and style representations, obtaining four reconstructed images of the form $x^{ij} = G(c^i, s^j)$, where $i,j \\in \\{a,b\\}$. Note that the same decoder $G$ is used for all image modalities, enabling it to learn a joint image representation. The discriminators $D_a, D_b$ are designed to distinguish at the image level, and $D_c$ are designed for the content level. A total of seven models, $\\{E_c, E_a, E_b, G, D_a, D_b, D_c\\}$, are jointly trained using the reconstruction and adversarial losses. The details about the losses are described in the following:\nReconstruction loss: Reconstruction losses at the image level are introduced to ensure the content and style encoders capture the entire image representation. The content representation $c_a$ ($c_b$) should contain all content information, and the style representation $s_a$ ($s_b$) should contain all style information in the modality a (b). Based on this, the network should restore the original $x^a$ ($x^b$) from $c^a$ ($c^b$) and $s^a$ ($s^b$), constrained by:\n$L_{rec} = E_{x_a \\in X_a} ||x_a - G(c^a, s^a)|| + E_{x_b \\in X_b} ||x_b - G(c^b, s^b)||$.\nAdversarial loss: Adversarial losses at the image and content levels are used to maintain the image and feature alignment. In a generative adversarial network (GAN), the generator is trained to synthesize images to fool the discriminator, while the discriminator is trained to distinguish fake images from real ones. To ensure the quality of transferred images $x^{ba}$ ($x^{ab}$), $D_a$ ($D_b$) is trained to maximize $L_{adv}$ ($L_{adv}$), while $E_a, E_c, G$ ($E_s, E_c, G$) are trained to minimize $L_{adv}$ ($L_{adv}$). Additionally, $D_c$ is introduced to align content representation $L_{adv}$\nThe adversarial losses are defined as:\n$L_{adv} = E_{x_i} E_{x_i} [log(D_i(x^i))] + E_{c_i \\in C_i, s_i \\in S_i} [log(1 - D_i(G(c^i, s^j)))]$\nfor $i, j \\in \\{a,b\\}$ and $i \\neq j$,"}, {"title": "2.2 Self-Training via Pseudo-Labeling", "content": "In Stage 2, given a volume and its corresponding annotation $(X^a, Y^a)$ from the source domain, a slice $x^b$ from a volume in the target domain is randomly selected for the style representation. The $X^{ab}$ is generated through the 2D image translation model mentioned in Section 2.1 slice by slice. In Stage 3, the synthetic pairs $\\{X^{ab},Y^a\\}$ are used to train a segmentation network $f$ that minimizes the segmentation loss:\n$L = \\sum L_{seg} (Y^a, f(X^{ab}))$\nThen in Stage 4, the pseudo label $Y^b$ of an unlabeled target scan $X^b$ is obtained by the trained segmentation model:\n$Y^b = f(X^b)$\nSynthetic target scans may have distribution gaps compared to real target scans but come with precise annotations. In contrast, real target scans are paired with incomplete pseudo labels. Literature  shows that integrating labeled synthetic source scans $(X^{ab}, Y^a)$ and pseudo-labeled real target scans $(X^b, Y^b)$ enhances the generalization ability. Therefore, these are combined in Stage 5 to fine-tune the previously trained segmentation model $f$ to minimize:\n$L = \\sum L_{seg} (Y^a, f(X^{ab})) + \\sum L_{seg} (Y^b, f(X^b))$\nA 3D self-configured nnU-Net was utilized in this work for medical image segmentation to better capture the correlations among slices within a single scan. we do not optimize the segmentation efficiency."}, {"title": "3 Experiments", "content": "The training dataset is curated from more than 30 medical centers under the license permission, including TCIA , LiTS , MSD , KiTS, autoPET, AMOS , LLD-MMRI, TotalSegmentator , and AbdomenCT-1K, and past FLARE Challenges. The training set includes 2050 abdomen CT scans and over 4000 MRI scans. The validation and testing sets include 110 and 300 MRI scans, respectively, which cover various MRI sequences, such as T1, T2, DWI, and so on. The organ annotation process used ITK-SNAP, nnU-Net, MedSAM, and Slicer Plugins.\nThe pseudo labels generated by the FLARE22 algorithms were utilized in this study. Due to the variability in imaging orientations in MRI scans, we specifically selected unlabeled MRI scans in the axial view. Additionally, we excluded scans that captured other body parts, such as the heart, shoulder, or leg, retaining only those with abdominal organs to maintain consistency in the dataset.\nTo account for differences in size and resolution across the dataset, each slice was cropped and resized to a uniform size of 512 \u00d7 512 pixels. During preprocessing, z-score normalization was applied. Additionally, data augmentation techniques were employed, including rotations, scaling, Gaussian noise, Gaussian blur, adjustments to brightness and contrast, and mirroring."}, {"title": "3.2 Evaluation measures", "content": "The evaluation metrics encompass two accuracy measures-Dice Similarity Coefficient (DSC) and Normalized Surface Dice (NSD)\u2014alongside two efficiency measures-running time and area under the GPU memory-time curve. These metrics collectively contribute to the ranking computation. Furthermore, the running time and GPU memory consumption are considered within tolerances of 60 seconds and 4 GB, respectively."}, {"title": "3.3 Implementation details", "content": "Environment settings The development environments and requirements are presented in Table 1. For all experiments in our work, we utilized a workstation equipped with a single NVIDIA Quadro RTX 8000 GPU with 48 GB of memory, an Intel(R) Xeon(R) Gold 6226R CPU, and running CentOS 7.9.\nTraining protocols The training protocols for our experiments are summarized in Table 2. We initialized the network using the Kaiming normal distribution and trained it with a batch size of 2, using 3D patches of size 48 \u00d7 192 \u00d7 192. The model was trained for a total of 800 epochs, employing Stochastic Gradient Descent (SGD) as the optimizer with an initial learning rate of 0.01. The learning rate followed a polynomial decay schedule. The entire training process spanned 17 hours. For the loss function, we combined Dice loss with cross-entropy to optimize segmentation performance. The model contained 30.71 million parameters, and the computational cost was measured at 1297.09 giga floating-point operations per second (GFLOPs)."}, {"title": "4 Results and discussion", "content": ""}, {"title": "4.1 Image Translation", "content": "Examples of source, target, and generative slices are shown in Fig. 3. Since the difference between $x^{CT}$ and $x^{CT->CT}$ and the difference between $x^{MRI}$ and $x^{CT->MRI->CT}$ are hard to tell, the content and style representations extracted from the encoders can fully represent the slice image in the CT domain and the decoder G effectively reconstruct the image from these disentangled representations. The same conclusion applies to the MRI domain. Based on the conclusions above, $x^{CT->MRI}$ and $x^{MRI->CT}$ are highly likely to be reliable. For a UDA problem, it is hard to evaluate the quality of $x^{CT->MRI}$ and $x^{MRI->CT$ quantitatively, since they are unpaired."}, {"title": "4.2 Segmentation Results", "content": "Comparison with state-of-the-art methods and ablation study To compare the efficiency of our method with state-of-the-art approaches, we trained the segmentation model using the same architecture on synthetic MRIs generated by different Unsupervised Domain Adaptation (UDA) methods. The segmentation results on MRIs from the FLARE dataset's validation set are presented in Table 3. UDA methods, including CycleGAN , SIFA , and our proposed DRL-STNet, significantly enhance segmentation accuracy."}, {"title": "4.3 Results on final testing set", "content": ""}, {"title": "4.4 Limitation and future work", "content": "Despite the promising results, our approach has several limitations. First, the method relies heavily on the quality of the disentangled representations and the accuracy of the image translation process. Any errors or inconsistencies in these steps can propagate through the network and affect the final segmentation results. Additionally, while our method works well with the FLARE dataset, its generalizability to other datasets and modalities remains to be thoroughly evaluated.\nAnother limitation is the potential for synthetic data to introduce artifacts that do not exist in real target modality images. This can lead to segmentation inaccuracies, especially in regions with complex anatomical structures. Furthermore, our approach currently requires significant computational resources and training time, which may limit its practical applicability in real-world clinical settings.\nFuture research directions can focus on addressing these limitations and improving the robustness and efficiency of the DRL-STNet framework. Potential areas for improvement include:\nMulti-Modality and Multi-Task Learning: Extending the framework to handle multiple modalities and tasks simultaneously could improve the generalizability and applicability of the method.\nEnhanced Representation Learning: Developing more robust methods for disentangled representation learning to minimize the introduction of artifacts and ensure more accurate image translations. Exploring alternative disentanglement techniques such as variational autoencoders (VAEs) could be beneficial ."}, {"title": "5 Conclusion", "content": "In this paper, we presented DRL-STNet, an innovative framework for unsupervised domain adaptation (UDA) in cross-modality medical image segmentation. By leveraging generative adversarial networks (GANs), disentangled representation learning, and self-training, our method effectively translates images from the source to the target modality, allowing for accurate segmentation of unannotated target images. Experimental results on the FLARE challenge dataset demonstrated that DRL-STNet outperforms state-of-the-art methods in both the Dice similarity coefficient and Normalized Surface Dice metrics, particularly in segmenting abdominal organs.\nIn summary, while DRL-STNet shows great potential for unsupervised domain adaptation in medical image segmentation, there are several areas where further research and development are needed to enhance its performance and applicability. Addressing these challenges will be crucial for the successful integration of UDA techniques in clinical practice."}]}