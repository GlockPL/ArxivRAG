{"title": "ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries", "authors": ["Kishan Maharaj", "Vitobha Munigala", "Srikanth Tamilselvam", "Prince Kumar", "Sayandeep Sen", "Palanivel Kodeswaran", "Abhijit Mishra", "Pushpak Bhattacharyya"], "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their ability to understand both natural language and code, driving their use in tasks like natural language-to-code (NL2Code) and code summarization. However, LLMs are prone to hallucination-outputs that stray from intended meanings. Detecting hallucinations in code summarization is especially difficult due to the complex interplay between programming and natural languages. We introduce a first-of-its-kind dataset with ~10K samples, curated specifically for hallucination detection in code summarization. We further propose a novel Entity Tracing Framework (ETF) that a) utilizes static program analysis to identify code entities from the program and b) uses LLMs to map and verify these entities and their intents within generated code summaries. Our experimental analysis demonstrates the framework's effectiveness, leading to a 73% F1 score. This approach provides an interpretable method for detecting hallucinations by grounding entities, allowing us to evaluate summary accuracy.", "sections": [{"title": "1 Introduction", "content": "Hallucination in natural language processing is defined as a condition in which a language model produces a text that is either incoherent or does not faithfully represent the provided source input (Ji et al., 2023). Similarly, in the context of code summarization, hallucination can be defined as a condition in which the generated summary does not accurately capture the intent and implementation details of the given input code.\nHallucination can originate from a combination of factors, where one common reason could be the misinterpretation of code entities. This misunderstanding can impact the model's ability to interpret the intended functionality of the code, resulting in an inaccurate portrayal of its purpose. For instance, consider the Example 1, where the intention of the int16 java method is to create a new 16-bit integer column (ColumnInt16) with a specified name, update the position for the next column, add it to the list of columns, and then return the RowBuilder object. However, the generated explanation introduces a non-existent int16 datatype and proceeds to discuss the rest of the logic as if it were valid. This could mislead a novice Java developer into believing that an int16 datatype exists in Java. Furthermore, several large language models (LLMs) like LLaMA and Granite failed to detect this hallucination. One reason could be that int16 is a valid datatype in other programming languages such as C, C++, C#, and Go, causing both humans and LLMs to confuse it with learning from those languages. Similarly, in the example shown in Figure 1, the java method getJobID() takes \u201cjobName\" as an argument and simply returns -1. The summary generated by the model provides a detailed explanation, including how the method getJobID() connects to the database and attempts to retrieve the jobID using the given jobName. Additionally, the summary mentions it stores the \u201cjobStatus\" in a variable. Clearly, the generated summary has no supporting entities in the code for db access and the model is relying on the method name to hallucinate a plausible summary of the method.\nIn this work, we study different factors that can lead to hallucination and list down a taxonomy to map the common causes easily. We also notice a lack of datasets to reliably research this topic. Therefore, we create a first-of-its-kind dataset for studying hallucination in code summarization with 411 summaries generated by seven different large language models, broken into 9933 entity-level samples. This dataset consists of code and a corresponding summary describing the code. The annotation consists of: a) Named Entity Recognition b) Entity Level Description Verification (c) Overall Summary Quality based on inaccuracies (not focusing on code completeness or conciseness). We then introduce a framework that focuses on validating the correctness of the generated summary with respect to code entities. For this, we verify if the entities discussed in the summary are present in the code and correctly described in the summary. The framework leverages code parsers like javalang\u00b9 to list the different entities in the code snippet and prompt-based approaches to detect entities in summary. We note that detecting entities in the generated summary is more difficult due to the high degree of polysemy (Tabassum et al., 2020). For example, entities like \"list\", \"while\", \"if\", etc, can be a code entity or natural language entities. This necessitates our reliance on large language models with high reasoning capabilities for detecting entities on the summary side. We then map the detected entities from the summary to code by using string-matching heuristics. The sentences with unmapped entities can be considered as ungrounded (source of extrinsic hallucination). For each mapped entity, we then have a tuple , where the intent-related sentence can be considered as the sentence in summary mentioning the entity. The final step is to verify each tuple from the summary for intrinsic hallucination and aggregate them to calculate the overall correctness of the code summary. Our experiments demonstrate the importance of localizing entities in the summary for effective hallucination detection. Our contributions are :\n\u2022 A taxonomy covering diverse reasons that might lead to hallucination in the code summarization (Figure 2).\n\u2022 A novel dataset\u00b2 for studying hallucination detection in code summarization, featuring 411 summaries from 7 LLMs and 10K entity-level samples (Table 1).\n\u2022 A first-of-its-kind approach for entity-level hallucination detection in code summarization inspired by the insights from human behaviour during code reviews leading to a performance of 73% F1 score (Table 2)."}, {"title": "2 Related Work", "content": "Recent advances in the NLP community have witnessed significant improvements in hallucination detection pipelines. In this section, we discuss some of the works that are relevant to ours.\nHallucination in Natural Langauge: Rawte et al. (2024); Sahoo et al. (2024) review recent advances in hallucination detection in natural language, emphasizing its practical significance. Recently, prompt-based methods (Arora et al. (2022), Manakul et al. (2023), Agrawal et al. (2023), Dhuliawala et al. (2023)) are being used to detect hallucinations in the text produced by LLMs. Xiao and Carenini (2022) and Zhang et al. (2022) attempt to address entity-level verification in natural language inputs. Both of these works involve improving the correctness of natural language summaries and do not discuss anything in the context of code. Maynez et al., 2020; Ji et al., 2023 discuss further fine-graining of hallucination in natural language as intrinsic and extrinsic hallucination. More specifically, Intrinsic hallucination occurs when the given text contradicts the reference, while Extrinsic hallucination happens when the text cannot be verified against the reference. We use a similar convention in our paper.\nHallucination in Code Generation: The code generation space has captured significant attention due to its practical significance in software development. (Jiang et al., 2024) discusses recent developments in code generation and suggests the importance of addressing hallucination for improving the reliability of LLMs. Liu et al. (2024) studies hallucination in code generation and proposes a categorization that encompasses five categories of hallucinations based on the conflicting objectives and varying degrees of deviation observed in code generation. Tian et al., 2024; Agarwal et al., 2024; Spracklen et al., 2024 further advanced the field by proposing valuable datasets and frameworks to tackle hallucination in code generation. These studies indicate the tendencies of LLMs to produce code that is syntactically correct and even semantically reasonable but fails to execute as intended or meet the specified requirements.\nAlthough there have been significant advances in hallucination detection in code generation, the field of code summarization is still nascent. Recently Kang et al., 2024; Zhang, 2024 have investigated inconsistencies in comment generation, but their focus has been limited to specific aspects such as verifying design constraints like parameter types and ranges. Additionally, some of their approaches, such as generating test cases for comments, face practical challenges due to heavy reliance on execution environments. In contrast, our approach aims to validate the entire functionality described in the output, independent of external environments or dependencies. By grounding entities based on the input code and verifying their intent, our framework offers a more reliable and less dependent solution."}, {"title": "3 Datasets", "content": "To create the hallucination dataset for code summarization, we consider code snippets from Java programming language and CodeXGLUE (Lu et al., 2021) - Code-To-Text dataset. We focused on Java programming language due to its widespread relevance in the industry. It offers a rich set of entities (such as classes, methods, and variables) due to its structured design and strict typing system. We report the statistics for the data in Table 1 and describe the curation below:\nSummary Generation: We generate summaries from CodeXGLUE by prompting seven different LLMs (Appendix A) on 600 different code snippets and consider ~10% of this data for the final hallucination annotation task (Table 1). By generating multiple summary variants, we can assess the extent of hallucination generation by different LLMs and study the hallucination detection techniques under diverse conditions."}, {"title": "4 Categorization of Factors for Hallucination in Code Summarization", "content": "In this section, we describe the various factors that could lead to hallucination in code summaries (Figure 2) based on what we learned from the annotation process. This classification, based on the underlying factors of hallucination, offers insights into the generative behaviors of language and code models. We categorize hallucination factors in code summarization into the following groups:\nHC1: Based on Identifier Name Bias: Name Bias refers to the tendency of language models to rely on identifier names when interpreting code. We classify this bias into three subcategories based on its source: 1)variables, 2)functions, 3)libraries. The model can misinterpret code due to the linguistic characteristics of these entity names. As the semantics of the code is defined by the underlying logic rather than their lexical meaning of entities, this may lead to hallucination. In the example shown in Figure 1, the model (Granite-20B) incorrectly assumes that getJobID is about retrieving a job ID, based purely on their names, even though the actual code logic suggests otherwise.\nHC2: Insufficient knowledge: This involves scenarios where the model generates incorrect summaries due to lack of knowledge. This may include an incorrect explanation of the imported libraries that the model did not see in its training data, incorrect information about the keyword, etc. We further divide this category into two parts:\n1) Contextual code This occurs when the model is unable to correctly explain the code present in the input. This may happen if the model hasn't encountered the code's functionality during training or if it's dealing with a low-resource language like COBOL, where basic rules might be misrepresented in the summary.\n2) Non-contextual code involves the scenario when the input does not contain the complete code and mentions an unseen library or an unknown construct whose functionalities are not understood by the model. For example, in the code sample shown in the HC2 Example, the model incorrectly describes the purpose of SQLException.\nHC3: Code Complexity: This category highlights the model's tendency to produce incorrect code summaries due to high code complexity. From another perspective, the model may not have enough reasoning capabilities to understand the code correctly and may deviate from the instruction given by the user. These complexities can arise due to various reasons, such as 1) Length: Longer code tends to be more complex because it often requires more understanding, has more potential points of failure, and may involve more interdependencies. 2) Lexical Complexity: This refers to scenarios where complex vocabulary in code, including diverse operands (variables, constants) and operations (functions, operators), increases complexity due to the higher number of elements to track and understand. 3) Logical complexity: This refers to scenarios where the code's logic is complex, often due to high cyclomatic complexity, indicating many independent paths. Complexity increases with numerous paths or method invocations, especially when these are not directly visible or are called from distant parts of the codebase.\nHC4: Natural language Context: This category refers to cases where natural language in code snippets, such as outdated comments or log statements, causes hallucinations in code summaries. In the code snippet shown in HC4 Example, the LLama3-70B model incorrectly infers that the property variable contains a list of key-value pairs inferred from a commented line. However, the 'property\" variable contains an alphanumeric string followed by one or more semi-colons."}, {"title": "5 Methodology", "content": "A code summary typically has a global and local view similar to texts (Maharaj et al., 2023). While the global view includes purpose, functionality, control flow, data flow, etc., the local view includes the details of key entities (variables, functions, etc.) from the source code and their purpose (hereby referred to as the intent of the entity). Our approach is based on the intuition that software developers, while verifying the documentation for a given code repository, first understand the local aspects of the code and then build a bottom-up concept for understanding the global aspects of the code. This involves reading the code line by line and tracing the specific code entities from the documentation to the original code.\nThis behaviour aligns with working memory theory in cognitive science (Baddeley and Hitch, 1994); working memory is a brain system that temporarily stores and manipulates the information necessary for complex cognitive tasks like learning and reasoning. The capacity of working memory is bounded by 7\u00b12 object at any point in time, which further reduces to 2-3 objects if the objects have relational dependencies with each other. Since code summaries often involve interdependent objects, developers must focus on local aspects to build a global understanding, suggesting a bottom-up heuristic for code summary comprehension.\nWe leverage these behavioural insights to design an LLM-powered framework for detecting hallucinations in code summaries, which involves tracing the entities from the summary to the code. This aspect of mapping the entities from the summary to the code aims to simulate the bottom-up behavioural model of verifying the description of coding entities at a time. With these insights, we aim to measure the correctness of a code summary as a two-step process: (1) Entity Verification and (2) Entity-Intent Verification. The detailed flow of this framework can be found in Figure 1.\n5.1 Entity Verification\nIn entity verification, we check if the entities in the summary are present in the source code to detect extrinsic hallucination. This involves extracting entities from both the code and summary, then mapping entities from summary to the code. We elaborate on this process below:\nEntity Extraction from code: We leverage program analysis to extract entities from code (Javalang Python package 3). The code is tokenized (lexer) and parsed into an abstract syntax tree (AST). This tree structure represents the hierarchical organization of code elements, making it easier to analyze. This yields a fine-grained classification of all the tokens present in the code such as variable names, class names, function names, etc.\nEntity Extraction from summary: Code summaries often focus on properties like Variables, Classes, Functions, and Tables, which cannot be categorized using traditional NLP tagsets. To address this, (Tabassum et al., 2020) propose the task of entity detection in code summaries and introduce a relevant NER tagset that includes categories such as CLASS, VARIABLE, FUNCTION, LIBRARY, etc. We adopt this tagset for extracting entities from code summaries (Prompt: Appendix A Figure 3). Since we are leveraging LLMs to recognize the entities, there is a possibility of encountering hallucinations during this step. This involves the scenario when the model fabricates some entities from summaries and outputs the entities that might not be present in the code summary itself. To address this, we add a filtration step which removes any entities not present in the summary. We evaluate Gemini and GPT-4-Omni for the task of Code NER on the collected human data and report their results in Appendix C. We also evaluate the open-source model for this task as an additional contribution. We observe a strong correlation between GPT-4-Omni predictions and human data, making it suitable for entity detection in our framework.\nEntity Matching: Once the entities from the code and summary are extracted, we compare them to identify the subset of entities present in the summary but not in the code. These entities are termed ungrounded, and all the sentences in the summary containing these entities can be labelled as extrinsic hallucination. The subset of entities in both the summary and the code goes through an additional verification round for intrinsic hallucination. This is to validate if the intent of the entity in the summary is correctly described as per the code.\n5.2 Entity-Intent Verification\nThe presence of an entity in both the summary and code indicates that the entity is valid but does not warrant the correctness of the context in which it is discussed. For example, in Figure 1 jobId is a correct entity, but the context of retrieving jobID from the database is incorrect. To address this problem, we propose verifying whether the intent of each mapped entity is accurately described in the summary. We extract all sentences containing the entity of interest from the summary to form its intent context. To identify these relevant sentences that describe an entity's intent, we explored two approaches: (1) prompting a language model to find relevant sentences and (2) using string-matching heuristics. Qualitative assessment showed that rule-based heuristics were more effective and efficient than prompt-based methods, which suffered from hallucinations. Therefore, we relied on string-matching-heuristics for our framework. After identifying the entity and intent context, we use LLMs with zero-shot prompting to verify their correctness against the code (Prompt: Appendix 5).\n5.3 Instance Level Hallucination Detection\nTo identify the quality of the whole summary with respect to the code, we aggregate the individual"}, {"title": "6 Experiments and Results", "content": "For our experiments, we consider summaries from instruction-tuned versions of the SOTA code and language models, from IBM-Granite family (20B-instruct; 34B-instruct) (Mishra et al., 2024), Llama3 family (8B-instruct and 70B-instruct) (Touvron et al., 2023), CodeLlama family (7B and 34B) (Roziere et al., 2023) and Mistral family (7B-instruct) (Jiang et al., 2023). We use GPT4-Omni (Achiam et al., 2023) and Gemini-1.5-Flash (Team et al., 2023) for our entity-intent verification task. Hyperparameters and other details can be found in Appendix (B). We discuss the results from two different aspects of evaluation below:\nEntity-Intent Verification: In this aspect of evaluation, we aim to verify the intent of an individual entity. We report the results of entity-intent verification in the Table 2. It can be observed that the GPT4-Omni F1-Score is 0.61 while the Gemini F1 Score is 0.55. Upon analysis, we found that these models often classify INCORRECT tuples as CORRECT. This is mainly due to the convincing nature of the summary, which may also be subjective due to a lack of proper code context. For instance, we observe significant errors when the code references a function or library that is not defined in the input. In such cases, the model infers the functionality based on the library name (Identifier Name Bias 4), which can be difficult to verify.\nInstance Level Hallucination Verification: In this aspect of evaluation, we aim to verify the overall summary instance. To compare our approach, we consider a direct setup which involves providing a  tuple to identify if the summary is hallucinated or not. We provide these results in Table 2, and it can be observed that our approach provides significant improvement in F1-Score when compared to the Direct approach. In general, the direct evaluation method suffers from hallucinations, such as when identified entities for hallucination are absent from the summary or when natural language entities are mistakenly considered code entities, overall resulting in poor performance. This conveys that our finer-grained evaluation provides a reliable method to identify hallucinated summaries. It also helps with interpretability as it identifies the hallucinated sections of the summary."}, {"title": "7 Analysis", "content": "In this section, we discuss various quantitative and qualitative insights of our framework. We first discuss summaries generated by individual models and then elaborate on the general predictive behaviour of our framework and cases of errors.\n7.1 Quantitative Analysis\nAs shown in Table 3, Granite-20B produced shorter summaries, while Llama3-70B generated longer ones. Other models had similar average lengths, reflecting varying elaboration due to differences in training methodologies. For entity mapping, we observe that Llama3-70B has the most mapped entities, indicating the tendency of the model to stay grounded. While Granite-20B has the most unmapped entities, which indicates its tendency to produce content which may not be directly related to the code leading to extrinsic hallucination.\n7.2 Predictive Analysis\nOur framework can be utilized to capture the ungrounded entities within the summary. In the example given below, the model tries to guess the summary based on the mentioned keyword \"HubException\" and discusses a non-existent Hub API. For this example, the unmapped entities are {'squid : s1166', ' Hub', ' squid : s1172', ' Hubdatabase'}. We can see that the ungrounded entities like \"Hub\" are captured here, leading to more fine-grained and interpretable detection of hallucination.\n7.3 Error Analysis\nThis section discusses the two major error cases in our framework:\nError Case 1-Creative Summary:\nDuring the generation, the model may mention certain aspects of the code in a creative way which may not be incorrect. In the given example, the code summary discusses a more elaborate version of the input code by restating an elongated version. Here, the entities present in the summary are predicted to be ungrounded by our framework.\nError Case 2- Changed Entity Form: The language models may not refer to the exact names of code entities during generation of the summary. In the example given below, the summary mentions the entity 'PreparedStatement\" as 'prepared statement\" which may not be captured during the named entity recognition phase due to changed entity form. The verification of these kinds of summary may not reflect the inaccuracies due such sentences."}, {"title": "8 Conclusion and Future Work", "content": "Our work addresses the critical challenge of detecting hallucinations in code summarization, a task that demands a deep understanding of both programming and natural languages. By introducing a novel dataset and the Entity Tracing Framework (ETF) with 73% F1 score, we establish a systematic approach to grounding code entities within summaries, enabling a more interpretable and accurate evaluation of explanations. In future, this framework can be enhanced by incorporating a multi-agent system, leveraging multiple LLMs in tandem to improve prediction accuracy. Additionally, the current framework can be further developed to better mitigate the occurrence of hallucinations."}, {"title": "9 Limitations", "content": "While the framework is intended to be generic, certain components\u2014such as the code parsers for entity detection\u2014are tailored to specific programming languages. Additionally, the performance of large language models (LLMs) can be highly sensitive to the prompts used. Despite significant time and effort spent optimizing prompts for each model, there may still be unexplored prompts that could lead to improved performance. Furthermore, the experimental results presented are based on Java code from the CodeXGLUE benchmark. As a result, additional research is required to verify whether the approach generalizes effectively to other programming languages."}, {"title": "A Prompts", "content": "Summary Generation Prompt\nAssume you are an expert in understanding JAVA code.\nQuestion: As a Java Expert, please provide a detailed summary of the following Java code with the following sections:\n1. Inputs and outputs of the method\n2. Business purpose\n3. Detailed functional summary of the method.\nNamed Entity Recognition Prompt\nAssume you are an expert in understanding Java and performing named entity recognition related to Java code. You have to label the entities by considering the following labels:\nCode Entities: CLASS, VARIABLE, FUNCTION, LIBRARY, VALUE, DATA TYPE, and HTML or XML TAG\nNatural Language Entities: APPLICATION, UI ELEMENT, LANGUAGE, DATA STRUCTURE, ALGORITHM, FILE TYPE, FILE NAME, VERSION, DEVICE, OS, WEBSITE, and USER NAME.\nFor every entity in the input mention the entity_type in the given format only. Strictly follow this template and only print the output without any other word. You can follow the example below:\nIntent Verification Prompt\nAssume you are an expert in understanding JAVA code. Your task is to verify whether the description of 'mapped_entity' in the given text is correct, incorrect, or irrelevant with respect to the code. Only output one of the following labels: [\u201cCORRECT\", \"INCORRECT\", \"IRRELEVANT\"].\nDirect Evaluation Prompt\nAssume you are an expert in understanding JAVA code. Your task is to verify if the description of the code entities present in the given summary is correctly described or NOT as per the code logic. Output all the 'entity_name' and a relevant_sentence' corresponding to the 'entity_name', which are incorrectly described. Do not provide any other details. Strictly follow this format:\nB Experimental Setup\nIn our setup, we conducted all the experiments using NVIDIA A100-SXM4-80GB GPU in a single or multi-GPU environment. For our experiments, we consider instruction-tuned versions of the SOTA code and language models, from\nC NER Evaluation\nThis section discusses the NER performance of various models considered in this work. To perform NER using LLMs, we provide the code summary and NER tagset in the prompt (Appendix 3) using a one-shot in-context example to extract all the entities discussed in the summary accompanied by their types. To evaluate the entity extraction, we assess two key aspects: entity coverage and entity type correctness. 1) Entity Coverage: This measures whether all valid entities in the summary are detected. We quantify this using the Jaccard Similarity between the entities in the generated output and those in the ground truth. 2) Entity Type Correctness: This evaluates whether the detected entities have been assigned the correct types. For this, we use the F1 score as the metric."}]}