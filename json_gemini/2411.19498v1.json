{"title": "Protecting Multiple Types of Privacy Simultaneously in EEG-based Brain-Computer Interfaces", "authors": ["Lubin Meng", "Xue Jiang", "Tianwang Jia", "Dongrui Wu"], "abstract": "A brain-computer interface (BCI) enables direct communication between the brain and an external device. Electroencephalogram (EEG) is the preferred input signal in non-invasive BCIs, due to its convenience and low cost. EEG-based BCIs have been successfully used in many applications, such as neurological rehabilitation, text input, games, and so on. However, EEG signals inherently carry rich personal information, necessitating privacy protection. This paper demonstrates that multiple types of private information (user identity, gender, and BCI-experience) can be easily inferred from EEG data, imposing a serious privacy threat to BCIs. To address this issue, we design perturbations to convert the original EEG data into privacy-protected EEG data, which conceal the private information while maintaining the primary BCI task performance. Experimental results demonstrated that the privacy-protected EEG data can significantly reduce the classification accuracy of user identity, gender and BCI-experience, but almost do not affect at all the classification accuracy of the primary BCI task, enabling user privacy protection in EEG-based BCIs.", "sections": [{"title": "I. INTRODUCTION", "content": "A brain-computer interface (BCI) enables the user to interact with or control an external device (computer, wheelchair, robot, etc.) using brain signals. The electroencephalogram (EEG) [1], which captures the brain's electrical activities from the scalp, is the most popular input of BCIs, due to its convenience and low cost. EEG-based BCIs have found successful applications in neurological rehabilitation [2], emotion recognition [3], robotic device control [4], and so on.\nMachine learning has achieved great successes in BCIs, which recognizes complex patterns [5] in EEG signals and builds high-performance classification/regression models [6]. Typically, a substantial amount of EEG data is required to train an accurate machine learning model. However, EEG data not only capture task-specific information but also include significant personal and private details [7]. For instance, Martinovic et al. [8] demonstrated that EEG signals have the potential to disclose various private details, such as credit cards, PIN numbers, known people, and residential addresses. Choi et al. [9] found that user identity can be inferred from resting-state EEG signals with an accuracy of 88.4%. Meng et al. [10] further showed that user identity can be easily inferred from EEG signals across different BCI tasks. Kaushik et al. [11] revealed that user gender and age could be predicted by analyzing resting-state EEG recordings.\nIn response to growing privacy concerns, numerous laws have been enacted worldwide, such as the General Data Protection Regulation of the European Union and the Personal Information Protection Law of China, aimed at enforcing stringent user privacy protections. Consequently, multiple privacy-protection techniques have been developed for EEG-based BCIs. They can be categorized into two groups. The first is cryptographic, including secure multiparty computation, homomorphic encryption, and secure processors. Agarwal et al. [12] introduced cryptographic protocols based on secure multiparty computation to safeguard privacy in EEG-based driver drowsiness estimation. The second category is privacy-preserving machine learning, enabling machine learning without directly accessing raw EEG data or model parameters. Typical approaches include federated learning and source-free transfer learning. Xia et al. [13] proposed augmentation-based source-free adaptation, which enables privacy-preserving transfer learning without accessing the source EEG data and/or model parameters. Zhang and Wu [14] used lightweight source-free transfer to address a similar issue. Zhang et al. [15] further proposed unsupervised multi-source decentralized privacy-preserving transfer.\nCryptographic and privacy-preserving machine learning safeguard privacy by restricting EEG data sharing. However, this significantly constrains the data availability in many scenarios. For example, the model generated by privacy-preserving machine learning may not be optimal, and without access to the EEG data, users cannot further improve the models or design better algorithms. On the other hand, if EEG datasets could be publicly shared, it may accelerate research and discovery, particularly in the development of large models which typically require extensive data for training. To balance privacy protection and data accessibility, perturbation was widely used for privacy protection, which adds noise to or transforms the original data to conceal private information while maintaining the downstream task information. Meng et al. [10] deliberately designed two perturbations to convert the original EEG data into identity-unlearnable EEG data, which can be used to protect the user identity privacy while maintaining the primary BCI task performance. However, the generated perturbations can only protect a single type of private information (identity) in the EEG data.\nBroader access to EEG data necessitates more comprehensive privacy safeguards. This paper further demonstrates that in addition to the user identity information, other private information (e.g., gender, BCI-experience) can also be inferred from EEG data. To protect multiple types of privacy simultaneously, we design the perturbation to convert the"}, {"title": "II. METHOD", "content": "This section introduces the details of protecting multiple types of private information in EEG data simultaneously.\nGiven an EEG dataset $D = \\{(x_i, y_i, s_i)\\}_{i=1}^N$, where $x_i \\in X \\subset \\mathbb{R}^{e \\times t}$ is the i-th EEG trial with e channels and t time domain samples, $y_i \\in Y = \\{1, ..., K\\}$ the task label (e.g., target or non-target in event related potential classification), $s_i = \\{p_{i,m}\\}_{m=1}^M$ the set of privacy label that can be inferred from the EEG trial $x_i$, and $p_{i,m} \\in P_m = \\{1, ..., P_m\\}$ a specific privacy label (e.g., male and female in gender).\nTypically, a Task-Classifier F can be trained on D to learn the mapping from the EEG input space to the task label space, i.e., $F: X \\rightarrow Y$. However, EEG data not only record task-related information but may also contain rich personal privacy. So, a Privacy-Classifier $G_m$ can be constructed from EEG data to mine the private information, i.e., $G_m : X \\rightarrow P_m$ maps the EEG input space into a specific privacy space.\nThis paper aims to generate perturbations for the original EEG data to protect multiple types of private information simultaneously, while preserving the task-related information to ensure the normal usage. Specifically, we generate a perturbed EEG dataset $D' = \\{(x'_i, y_i, s_i)\\}_{i=1}^N$, where $x' = x + \\delta$ and $\\delta$ is a deliberately designed perturbation. For any privacy $P_m$, it is difficult to train a Privacy-Classifier $G_m'$ from the perturbed dataset, making the private information in EEG data unlearnable. However, this perturbed dataset can still be used to train a good Task-Classifier F as the original unperturbed dataset D."}, {"title": "A. Problem Statement", "content": "Given an EEG dataset $D = \\{(x_i, y_i, s_i)\\}_{i=1}^N$, where $x_i \\in X \\subset \\mathbb{R}^{e \\times t}$ is the i-th EEG trial with e channels and t time domain samples, $y_i \\in Y = \\{1, ..., K\\}$ the task label (e.g., target or non-target in event related potential classification), $s_i = \\{p_{i,m}\\}_{m=1}^M$ the set of privacy label that can be inferred from the EEG trial $x_i$, and $p_{i,m} \\in P_m = \\{1, ..., P_m\\}$ a specific privacy label (e.g., male and female in gender).\nTypically, a Task-Classifier F can be trained on D to learn the mapping from the EEG input space to the task label space, i.e., $F: X \\rightarrow Y$. However, EEG data not only record task-related information but may also contain rich personal privacy. So, a Privacy-Classifier $G_m$ can be constructed from EEG data to mine the private information, i.e., $G_m : X \\rightarrow P_m$ maps the EEG input space into a specific privacy space.\nThis paper aims to generate perturbations for the original EEG data to protect multiple types of private information simultaneously, while preserving the task-related information to ensure the normal usage. Specifically, we generate a perturbed EEG dataset $D' = \\{(x'_i, y_i, s_i)\\}_{i=1}^N$, where $x' = x + \\delta$ and $\\delta$ is a deliberately designed perturbation. For any privacy $P_m$, it is difficult to train a Privacy-Classifier $G_m'$ from the perturbed dataset, making the private information in EEG data unlearnable. However, this perturbed dataset can still be used to train a good Task-Classifier F as the original unperturbed dataset D."}, {"title": "B. Perturbation Generation", "content": "To prevent Privacy-Classifiers from learning private information in the EEG data, we design perturbations highly correlated with the private information, which can mislead the Privacy-Classifier to learn the perturbation pattern rather than the true privacy pattern. Additionally, due to the distribution differences between the privacy feature space and the task feature space, the perturbations generated for privacy protection may have little impact on the primary BCI task, which will be verified in Sections III-G and III-H.\nSpecifically, we first train a Privacy-Classifier $G'_m$ for each privacy type $P_m$ to evaluate the influence of perturbations on the privacy-related information, which may be different from $G_m$. The loss function for $G'_m$ is\n$\\min\\limits_{G'_m} E_{(x,p_m)\\sim D} \\mathcal{L}_{CE}(G'_m(x), p_m), $\nwhere $\\Theta_{G'_m}$ is the parameter set of $G'_m$, and $\\mathcal{L}_{CE}$ the cross-entropy loss.\nThen, for each privacy $P_m$, we generate a class-wise perturbation $\\delta_m = [\\delta_{m,1}, \\dots, \\delta_{m,P_m}]$ by minimizing following loss function:\n$\\min\\limits_{\\delta_m} E_{(x,p_m)\\sim D} [\\mathcal{L}_{CE}(G'_m(x+\\delta_{m, p_m}), p_m) + \\alpha ||\\delta_{m, p_m}||^2],$\nwhere $\\delta_{m,p_m}$ is the perturbation for class $p_m$ of privacy type $P_m$, and $\\alpha$ is a trade-off parameter. The first term enhances the correlation between the perturbation $\\delta_{m,p_m}$ and the privacy class $p_m$, and the second term constrains the perturbation amplitude.\nAfter generating perturbation for each privacy type, we can add them to each EEG trial x in D to protect multiple different types of privacy simultaneously:\n$x' = x + \\sum\\limits_{m=1}^M \\delta_{m,p_m}.$\nFinally, the EEG dataset with privacy protection is $D' = \\{(x'_i, y_i, s_i)\\}_{i=1}^N$. Since the perturbation for each privacy type is very small, the aggregated perturbation is still small enough to have little impact on the primary task performance.Algorithm 1 gives the pseudo-code of privacy-protected EEG dataset generation."}, {"title": "III. EXPERIMENTS", "content": "This section introduces the experimental settings and results for validating the performance of the privacy-protected EEG data."}, {"title": "A. Datasets", "content": "The publicly available EEG dataset introduced by Lee et al. [16] was used in our experiments. It was collected from 54 healthy subjects, among which 16 were experienced BCI users. Each subject performed a sequence of three tasks: a 36-symbol event-related potential (ERP) [17] task, a binary-class motor imagery (MI) [6] task, and a four-target steady-state visually evoked potential (SSVEP) [18] task. During each task, 62-channel EEG data were recorded with a sampling rate of 1,000Hz. The entire experimental procedure was repeated twice, so the dataset includes two sessions. The details of each task are as follows:\n1) ERP: In the ERP task, the subjects were instructed to focus on the target symbol which was flashed randomly to elicit a P300 response. The goal was to classify"}, {"title": "B. Models", "content": "We used the following three convolutional neural networks (CNN) as Privacy-Classifiers:\n1) EEGNet [19]: EEGNet is a compact CNN architecture tailored for EEG-based BCIs. It comprises two convolutional blocks, utilizing depthwise and separable convolutions instead of traditional convolutions, to reduce the number of model parameters.\n2) DeepCNN [20]: DeepCNN is composed of four convolutional blocks. The initial block is tailored for processing EEG data, while the subsequent three blocks follow a standard convolutional design.\n3) ShallowCNN [20]: ShallowCNN is a more straightforward variant of DeepCNN, drawing from filter bank common spatial patterns. It features a single convolutional block with a larger kernel and employs a different pooling strategy compared to DeepCNN"}, {"title": "C. Performance Measure", "content": "Given class-imbalance in gender and BCI-experience classification, balanced classification accuracy (BCA) was used to evaluate the Privacy-Classifier's performance:\n$\\textrm{BCA}_m = \\frac{1}{P_m} \\sum\\limits_{p_m=1}^{P_m} \\frac{1}{N_{p_m}} \\sum\\limits_{i=1}^{N} I(G'_m(x_i) = p_m),$\nwhere $P_m$ is the number of classes for privacy type $P_m$, $N_{p_m}$ the number of test samples in class $p_m$, and $I(\\cdot)$ an indicator function."}, {"title": "D. Experimental Settings", "content": "We performed leave-one-session-out cross-validation, i.e., trials from three tasks in one session were mixed as the training set, and the other session was used as the test set. The average BCA from two sessions were computed. The entire cross-validation process was repeated 5 times, and the average results are reported."}, {"title": "E. Identity, Gender, and BCI-experience Privacy Mining", "content": "The original EEG signals contain rich personal private information. To demonstrate that, Privacy-Classifiers were trained on the unperturbed EEG dataset to classifier user identity, gender, and BCI-experience. The test performance of Privacy-Classifiers are shown in the 'Unperturbed EEG' panel of Table I. Regardless of which CNN model (EEG-Net, DeepCNN, or ShallowCNN) was used as the Privacy-Classifier, the BCAs for the three types of private information were much higher than random guess. These results confirmed that rich personal private information can be inferred from EEG data across different BCI tasks, highlighting the necessity of privacy protection in EEG-based BCIs."}, {"title": "F. Identity, Gender, and BCI-experience Privacy Protection", "content": "To safeguard the private information contained in EEG data, we converted the original EEG data into privacy-protected EEG data, making it difficult for machine learning models to learn the private information.\nSpecifically, we generated privacy-protected EEG data as described in Algorithm 1. The test performance of Privacy-Classifiers trained on privacy-protected EEG data are shown in the \u2018Perturbed EEG' panel of Table I. For each privacy type (identity, gender, and BCI-experience) classification, the BCAs of the Privacy-Classifiers trained on the perturbed EEG data were significantly lower than their counterparts on the original EEG data. Especially, for gender and BCI-experience classification, the BCAs after perturbations were close to random guess, demonstrating that little private information can be learned from the privacy-protected EEG data.\nNotice that although the privacy-protected EEG data were generated by EEGNet, they remained effective for DeepCNN and ShallowCNN, indicating good generalization of privacy-protected EEG data."}, {"title": "G. BCI Task Performance", "content": "In addition to privacy protection, the perturbations should also minimize the impact on the primary BCI tasks, i.e., the performance of Task-Classifiers trained on privacy-protected EEG data should be similar to their counterparts on the original unperturbed EEG data.\nTable II shows the test performance of the three CNN models and a traditional model [i.e., xDAWN [21] filtering and Logistics Regression (LR) classification for ERP, common spatial pattern (CSP) [22] filtering and LR classification for MI, and canonical correlation analysis (CCA) [23] for SSVEP] as Task-Classifier trained on the privacy-protected EEG data and the original unperturbed EEG data. The BCAs after applying perturbations were very close to their counterparts on the original unperturbed EEG, suggesting that privacy-protected EEG had almost no negative impact on the performance of primary BCI tasks."}, {"title": "H. Characteristics of the Privacy-Protected EEG Data", "content": "We further show that the characteristics of the EEG data before and after privacy protection are very similar from various perspectives, ensuring the effectiveness of the primary BCI tasks.\nFig. 1 shows the original unperturbed EEG trials and their perturbed counterparts from ERP, MI and SSVEP tasks. For clarity, only three EEG channels (F4, Cz, and F3) are shown. We can observe that the privacy-protected EEG trials and the original unperturbed EEG trials are almost identical for all three BCI tasks.\nFig. 2 shows the average Cz channel spectrograms of the original unperturbed EEG trials and the privacy-protected EEG trials for the target class on the ERP task, the imagination of the right hand on the MI task, and the target with 12Hz flickering frequency on the SSVEP task. The perturbed spectrograms were almost identical to the unperturbed counterparts, suggesting that the perturbations hardly affect the spectrogram of the EEG trials.\nFig. 3 shows the average topoplots of the original unperturbed EEG trials and the privacy-protected EEG trials for the target class on the ERP task, the imagination of the right hand on the MI task, and the target with 12Hz flickering frequency on the SSVEP task. One can hardly observe any differences between the unperturbed and perturbed topoplots regardless of the BCI task, indicating that perturbations hardly change the spatial information of the EEG trials."}, {"title": "I. Visualization of the Training Process", "content": "Fig. 4 shows how the training and test BCAs of the Privacy-Classifiers and the Task-Classifiers change with the number of training epochs on the original unperturbed EEG data and their perturbed counterparts, respectively. Observe that:\n1) The training BCAs of the Privacy-Classifiers on the original unperturbed EEG data and the privacy-protected EEG data were close after convergence; however, the test BCAs of the Privacy-Classifiers trained"}, {"title": "IV. CONCLUSIONS", "content": "There is rich private information in EEG signals, such as user identity, gender and BCI-experience, necessitating privacy protection in EEG-based BCIs. This paper has demonstrated that user's identity, gender, and BCI-experience can be easily inferred by machine learning models from EEG signals, exposing a serious privacy issue in EEG-based BCIs, which may significantly impact users' willingness to share their EEG data. To address this issue, we designed perturbations to convert the original EEG data into privacy-protected EEG data, which can conceal user private information while preserving the performance of the primary BCI tasks. Experimental results showed that the generated privacy-protected EEG data can significantly reduce simultaneously the classification accuracies on user identity, gender, and BCI-experience, while almost do not impact the primary BCI task classification performance at all."}]}