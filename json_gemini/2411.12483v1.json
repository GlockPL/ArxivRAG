{"title": "Analysing Explanation-Related Interactions in Collaborative Perception-Cognition-Communication-Action", "authors": ["Marc Roig Vilamala", "Jack Furby", "Julian de Gortari Briseno", "Mani Srivastava", "Alun Preece", "Carolina Fuentes Toro"], "abstract": "Effective communication is essential in collaborative tasks, so AI-equipped robots working alongside humans need to be able to explain their behaviour in order to cooperate effectively and earn trust. We analyse and classify communications among human participants collaborating to complete a simulated emergency response task. The analysis identifies messages that relate to various kinds of interactive explanations identified in the explainable AI literature. This allows us to understand what type of explanations humans expect from their teammates in such settings, and thus where AI-equipped robots most need explanation capabilities. We find that most explanation-related messages seek clarification in the decisions or actions taken. We also confirm that messages have an impact on the performance of our simulated task.", "sections": [{"title": "I. INTRODUCTION", "content": "Effective human-robot teaming is seen as a key enabler of future 'front line' situations including emergency response and disaster relief. In such highly dynamic settings, coordination among team members is a critical success factor. The increasing sophistication of modern artificial intelligence (AI) has led to significantly improved perception, cognition, communication and action (PCCA) capabilities embodied in robots. However, many of the key technologies are 'black box' in nature, making it hard to engineer robots that operate in a sufficiently transparent manner to their human collaborators. This work is a step towards analysing human expectations in terms of explainability in relation to task coordination. The setting is a simulation environment, TeamCollab [1], in which humans and AI-equipped robots collaborate to clear an area of dangerous objects. The environment is designed to highlight PCCA capabilities; humans and robots are intended to work as peer agents, and inter-agent communication is a key factor in task success.\nWe analyse results from TeamCollab experiments with human participants to better understand what humans expect from their teammates in relation to explanation. We adopt a recent explainable AI (XAI) framework [2] that takes a dialogue-centric view of explanation, labelling the exchanged messages in terms of their relationship to elements of the framework. We show that there is a positive relationship between message exchange and team performance: volume of communication correlates with task success. This analysis seeks to answer the research question: What types of explanations do AI-equipped robots need based on human"}, {"title": "II. RELATED WORK", "content": "As AI systems will be required to perform in highly cognitively demanding environments, it has become critical to understand the communication mechanisms that would better support human-AI teams. Transparency and explainability are key components of situational awareness to allow agents to understand better the dynamically changing world they constantly perceive [4]. Thus, XAI aims to improve the understanding and interpretation of the decision processes and results of machine learning algorithms [5], [6] to support, e.g., human-robot teaming. Current approaches to XAI have mostly focused on static explainability, with a single message to cover, without input or user preferences involved in the process [7]. Due to the social nature of XAI, interactive explanations are gaining attention [8], as this involves an iterative process that considers user's information needs and is more similar to people's patterns on how explanations are expected to be provided [2], [6]. Some taxonomies have been proposed to explore the interactive aspect of XAI. Liao et. al [5] present an XAI question bank framework with a set of prototypical questions that users may ask when requesting an explanation from an AI system. Authors in [2] synthesize 48 empirical studies to create a two-level taxonomy of interactive techniques in XAI based on their cognitive processes and tasks.\nDuring communication in critical timing scenarios, asking for an explanation may not always be explicit. The theory behind team communication in human groups identifies that a significant amount of communication goes through non-explicit channels, meaning that messages are interpreted in context, and there are additional communicative channels used as eye-gaze, gestures or non-verbal statements critical for performance that complement the message [9]. Therefore, to better understand implicit explanation dialogues in context, we analyse the communication interaction of human-human data in a team environment and map awareness factors into a taxonomy of interactive techniques in XAI."}, {"title": "III. METHODS", "content": "The experiment was designed to run in TeamCollab [1], a simulated environment based on the ThreeDWorld physics"}, {"title": "B. Data Collection", "content": "All participants were asked to connect to the simulation through their web browser and to avoid the use of any outside communication with other participants for the duration of the session. Participants were provided with a proximity-based text chat, where communications were broadcast to all agents within a range of 5 metres. The system recorded all communications, capturing the following parameters:\n\u2022 File & session ID: identify each individual session;\n\u2022 Timestamp: time in seconds when the message was sent;\n\u2022 Sender: the agent that sent the message.\n\u2022 Receivers: the agents that received the message;\n\u2022 Message: the text that the agent sent.\nWe ran 20 sessions, resulting in 2,607 messages."}, {"title": "C. Data Analysis", "content": "Following a qualitative approach, we conducted a linguistic analysis of communication logs to identify explanation-related messages. Previous approaches have been followed to understand in-depth interactions that emerge from collaborative dialogues and spoken instructions [11], [12]. We analyse the messages from the perspective of the recent XAI taxonomy presented by [2]. This framework was chosen because it offers a synthesis of findings from 48 empirical studies evaluating interactive explanations with human users. The resulting taxonomy classifies XAI techniques according to the type of support interaction: (i) select, which allows users to choose the information they want to see, (ii) mutate, which considers hypotheses or different situations, and (iii) dialogue with, which provides interactivity. Each cognitive support type is divided into three task-oriented categories. For our experiment, we focus on four categories: (i) select/clarify, which gives additional information on demand, (ii) mutate/simulate, which considers predictions for a given set of inputs, (iii) dialogue/progress, which guides the user through an explanation sequence and (iv) dialogue/answer, which gives feedback or edits explanation components. In our analysis, we are not only looking for communications that explicitly request explanation (e.g., questions asking, \"Why...\") but also identifying parts of the human-human dialogues where our participants are, in the judgement of the annotators, implicitly calling for explanation within the four sub-categories. The purpose here is to focus on the collaborative activities where robots will most need to be equipped to offer explanations in response to implicit as well as explicit requests. Following an iterative process guided by a CodeBook [13], four researchers labelled 1,000 of the collected messages, which came from 15 different sessions.\n1) Procedure: Researchers used a 3-stage qualitative analysis to label the communication logs. First, 100 messages were individually labelled and checked for inter-code agreement, adjusting the CodeBook where necessary. Then, 150 more messages were labelled and discussed to ensure inter-code agreement. Finally, a total of 1,000 messages were labelled, including the re-labelled first 100 messages.\n2) CodeBook: The team worked through the codes to label the messages, classifying and refining them until all the researchers agreed. The labels were then used as themes to classify the messages."}, {"title": "IV. RESULTS", "content": "Before analyzing the messages sent, we first wanted to evaluate whether team communication had any impact on the performance of the team. For that purpose, in this section we compare the number of messages sent by the team as a whole with different performance indicators.\nFig. 2a shows that the total number of collected objects tends to have an inverse correlation to the number of messages sent by the team as a whole, with a Pearson Correlation Coefficient (PCC) of -0.28. Intuitively, this makes sense, as participants need to spend time typing the messages, which prevents them from taking other actions in the environment. As such, it might seem that sending messages is detrimental to the performance of the team as a whole. However, Fig. 2b shows that the number of actually dangerous objects collected by the teams does not seem to depend on the number of messages sent (PCC of 0.05). We believe this is thanks to the knowledge that each agent gains from communicating with the others, which allows everyone to compare the predictions from different sensors. This means agents can more accurately predict which objects are actually dangerous, leading to less wasted time on trips carrying benign objects. This is reflected in Fig. 2c, which shows that a higher percentage of the objects collected by more communicative teams are actually dangerous (PCC of 0.30).\nAs such, it seems that, while there may be some downsides to too much communication, the advantages of getting input from other teammates can make up for the time spent typing messages. It is also worth considering that in a real scenario, the disposal of dangerous objects might have a monetary or time cost beyond the time spent moving them to the goal area represented in the simulation. In such cases, being more accurate in identifying which objects are considered dangerous might have even further benefits."}, {"title": "B. Understanding XAI-Related Language", "content": "We present the results of the qualitative linguist analysis conducted to understand what XAI language emerges from the communications. Four annotators categorised and labelled 1,000 messages with a good level of inter-annotator agreement (61% of samples were labelled with at least three annotators agreeing on the label, and 5% of samples were unclassified or with no similarities). Fig. 3 shows the majority label distribution for the annotated messages. That is, the labels where at least 3 annotators agreed. The most frequent label is autocompleted messages, which participants can easily send by pressing a UI button to share all sensed attributes for an object. An example of this type of message is \"Object 18 (weight: 1) Last seen in (-7.5,-5.5) at 04:06 -Status Danger: dangerous,Prob. Correct: 72.5%\". The ease with which these messages can be generated likely had an influence on their frequency. The second most common label is confirm, for messages confirming communications, such as \"K sounds good\", and \u201cOk 9\u201d. This is followed by doing, for messages that either ask what actions are being taken or can be interpreted as answering such questions. Some examples are \"I'll explore in the next room\u201d, \u201cdo you want to pick up another one, or should we call it a day\". Messages discussing the features of an object are also common, including cases such as: \"Nothing here for me. Only 2 but that one is too heavy\u201d; \u201cI see, that's high\". All other labels are significantly less common, with why and make-safe having no cases of majority agreement. There also were no cases where a majority of annotators agreed on assigning multiple labels to a single message.\nIt is worth noting that most of the labelled messages fall into the select/clarify category (auto, doing, features), followed by dialogue/answer (confirm). Annotators also remarked that, while more traditional XAI questions such as \"Why do you think object X is dangerous?\" do not tend to appear explicitly, participants did discuss why they thought an object was dangerous or not. This is likely why messages labelled with auto and features are quite frequent, as participants shared their opinions on how to act based on shared sensor outputs. Some messages from confirm were also part of such discussion, as conclusions were reached. Most of the other messages labelled confirm were in response to doing messages, which tended to be used to inform teammates of actions and coordinate."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "In this work, we sought to gain a better understanding of what explanation capabilities are required of PCCA robots in team collaboration tasks in dynamic emergency response type settings. Our expectation that effective communication matters in such settings is borne out by data showing that the volume of dialogue correlates with the performance of the team. More specifically, we have shown that, while teams that communicate more tend to collect fewer objects, they are more accurate, with a higher percentage of collected objects being actually dangerous. As a result, we see that the implementation of TeamCollab is successful in rewarding teams that use effective communication while having the potential to penalise teams that communicate ineffectively. Our analysis of the content of the communications re-"}]}