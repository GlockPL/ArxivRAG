{"title": "Efficient Data-Sketches and Fine-Tuning for Early Detection of Distributional Drift in Medical Imaging", "authors": ["Yusen Wu", "Hao Chen", "Alex Pissinou Makki", "Phuong Nguyen", "Yelena Yesha"], "abstract": "Distributional drift detection is important in medical applications as it helps ensure the accuracy and reliability of models by identifying changes in the underlying data distribution that could affect diagnostic or treatment decisions. However, current methods have limitations in detecting drift; for example, the inclusion of abnormal datasets can lead to unfair comparisons. This paper presents an accurate and sensitive approach to detect distributional drift in CT-scan medical images by leveraging data-sketching and fine-tuning techniques. We developed a robust baseline library model for real-time anomaly detection, allowing for efficient comparison of incoming images and identification of anomalies. Additionally, we fine-tuned a vision transformer pre-trained model to extract relevant features using breast cancer images as an example, significantly enhancing model accuracy to 99.11%. Combining with data-sketches and fine-tuning, our feature extraction evaluation demonstrated that cosine similarity scores between similar datasets provide greater improvements, from around 50% increased to 100%. Finally, the sensitivity evaluation shows that our solutions are highly sensitive to even 1% salt-and-pepper and speckle noise, and it is not sensitive to lighting noise (e.g., lighting conditions have no impact on data drift). The proposed methods offer a scalable and reliable solution for maintaining the accuracy of diagnostic models in dynamic clinical environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Distributional drift [1], also known as dataset drift, in medical imaging refers to changes in data distribution over time, which can significantly affect the performance of machine learning models used for diagnostic purposes. This drift may result from various factors, including alterations in imaging equipment, differences in imaging protocols, variations in patient demographics, or updates in image preprocessing techniques. Detecting and managing drift is critical in the medical field to ensure that models remain accurate and reliable. Ignoring drift can lead to incorrect diagnoses or suboptimal treatment recommendations, thereby potentially compromising patient care. Therefore, continuous monitoring and adaptation of models are essential to maintain their effectiveness in dynamic clinical environments. Despite significant advancements in data drift detection within medical imaging, current methodologies exhibit several limitations that must be addressed to improve their effectiveness. We discuss the limitations of L1, L2 and L3 methods as follows:\n(LI): Analyzing the distribution of image data is particularly challenging due to its high-dimensional characteristics. Unlike numerical data, where statistical properties like mean and variance can be straightforwardly computed and compared, image data necessitates specialized feature extraction and dimensional reduction techniques to perform meaningful distribution analysis.\n(L2): Abnormal datasets can introduce bias in drift detection, which can often be mitigated without compromising the overall data quality. However, efficiently identifying and isolating these abnormal datasets is essential to preserve data integrity. Such abnormalities may arise from data entry errors, sensor malfunctions, or even malicious activities. Implementing effective and efficient anomaly detection techniques is critical for pre-processing data before applying drift detection methods.\n(L3): Another limitation is the absence of real-time processing solutions for handling abnormal data, especially during the preprocessing stage. This process often relies on hospital experts to manually label and remove invalid data before training, which significantly increases the cost and complexity of model development. When datasets contain substantial noise or inconsistencies, the drift detection process may incorrectly identify or overlook drifts, thereby compromising the reliability of the analysis.\nTo overcome the identified limitations, we fine-tuned a pre-"}, {"title": "II. BACKGROUND", "content": "The reminder of this paper is organized as follows: Section\nII provides an overview of the relevant background. Section\nIII details our proposed methods. Section IV presents the\nexperimental results, while Section V discusses related work.\nFinally, we conclude our research in Section VI."}, {"title": "A. Distributional Drift (Dataset Shift) in Medical Images", "content": "Distributional drift, also known as dataset shift, refers\nto changes in the statistical properties of data between the\ntraining and deployment phases of machine learning models\n[1]. In medical imaging, this drift can manifest as differences\nin pixel intensities, textures, or other visual characteristics\nbetween the images used for training and those encountered\nduring real-world application. This phenomenon is particularly\nprevalent in medical imaging due to the continuous evolution\nof imaging technologies, variations in imaging equipment, and\nchanges in patient demographics. Such variations can result in\nsignificant differences between the training data and new data,\nchallenging the model's ability to make accurate predictions.\nAddressing distributional drift is critical because it directly\naffects the performance and reliability of machine learning\nmodels in clinical settings. A model trained on a specific\ndataset may perform well during initial testing but might fail\nto maintain its accuracy when applied to new datasets that\ndiffer in subtle yet significant ways. This discrepancy can\nlead to misdiagnosis or incorrect treatment recommendations,\npotentially compromising patient safety. As medical imaging\npractices and technologies continue to evolve, it is essential\nto continuously monitor and adapt models to ensure their\neffectiveness. Ensuring that models can robustly handle or\nadapt to distributional drift is vital not only for maintaining\nhigh standards of patient care but also for complying with\nregulatory requirements governing the use of AI in healthcare."}, {"title": "B. Causes of Distributional Drift in Image Data", "content": "Distributional drift in image data can occur due to various\nfactors related to changes in data, environmental conditions,\nor the image acquisition process. One common cause is vari-\nations in lighting conditions, which can occur due to different\ntimes of day, weather conditions, or light sources, leading\nto alterations in brightness and contrast that affect the data\ndistribution. Changes in imaging devices, such as switching\ncameras or scanning devices, inconsistent calibration, lens\naging, or fluctuations in sensor performance, can also result\nin changes to image quality and subsequently impact the data\ndistribution. Another contributing factor is subject variations,\nwhere changes in the shape, posture, or size of the subject\nbeing imaged can lead to shifts in the feature distribution of\nthe images. Additionally, modifying the resolution or size of\nimages can alter the distribution of image features, potentially\ncausing drift. Different devices or sensors capturing the same\nscene may record varying levels of detail, leading to distri-\nbutional drift. Finally, images taken from different angles of\nthe same subject can introduce changes in viewpoint, resulting\nin shifts in the data distribution. These factors highlight the"}, {"title": "III. PROPOSED METHODS", "content": "To effectively address the challenges of distributional drift\nin medical imaging and enhance the accuracy of feature\nextraction, we propose a comprehensive solution that combines\nadvanced fine-tuning techniques with real-time anomaly detec-\ntion using data sketching, as shown in Fig. 2. This approach\nnot only optimizes model performance but also ensures the\nreliable detection of subtle changes in data distribution within\ndynamic clinical environments."}, {"title": "A. Fine-tuned Pre-trained Model for Feature Extractions", "content": "Our method leverages a pre-trained deep learning model\nfor feature extraction, capitalizing on its ability to encode rich\nfeature representations from input data. Pre-trained models,\ntrained on extensive datasets, offer the advantage of transfer\nlearning, allowing the knowledge gained from large-scale data\nto be applied to specific tasks with limited data availability.\nThis approach utilizes the pre-trained model within the context\nof our problem domain to enhance feature extraction."}, {"title": "1) Data Preparation:", "content": "The first step in fine-tuning a model\nfor medical image drift detection involves thorough data prepa-\nration. This process includes selecting an qualified dataset, pre-\nprocessing the images, and applying data-sketches to improve\nthe data quality."}, {"title": "2) Layer Freezing and Fine-Tuning:", "content": "To preserve the gen-\neral features learned during pretraining, the some of the layers\nfrom the pre-trained model are frozen, not updated during\nthe fine tuning process. Only the final few layers, which are\nresponsible for task-specific features, are fine-tuned. Let W =\n{W1,W2,..., Wn} be the pre-trained model parameters, and\nn \u2208 Z+ is the index of the layers with trainable parameters.\nSupposing the data flow is from the layer-1 propagating till\nlayer-n, we can have the first l-frozen layers and regular layers\nas\n$W_{frozen} = {W_1, W_2, ..., W_l}$ (1)\n$W_{tunable} = {W_{l+1}, W_{l+2}, ..., W_n}$ (2)\nwhere $W_{frozen}$ are the weights of the frozen layers, and\n$W_{tunable}$ are the weights of the layers to be fine-tuned. The\npre-trained models [10]\u2013[12] are targeting on other tasks\nrather than detecting the data drift. Therefore, the minor\ncustomization are added to the final layer. It is replaced with a\nfully connected layer with sigmoid function as the activation\nfunction, tailored for the binary classification task:\n$P = \\frac{1}{1 + e^{-w^T x_n + b}}$\nwhere $x_n$ corresponds to output embedding from the previous\nlayer, $T$ denotes the vector transpose and wn and b \u2208 R\nconsist of wn, the parameters of the last layer."}, {"title": "3) Loss Function:", "content": "The loss function used for training is the\nBinary Cross-Entropy (BCE) loss, which is suitable for binary\nclassification tasks. Given the output probability p from the\nfinal layer, the BCE loss is defined as:\n$L_{BCE} = -\\frac{1}{B} \\sum_{i=1}^{B} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]$\nwhere yi is the true label (1 for the correct class, 0 otherwise),\npi is the predicted probability for training sample i and B\u2208\nZ+ is the size of the input, typically as the mini-batch size."}, {"title": "4) Optimization Algorithm:", "content": "We use mini-batch stochastic\ngradient descent employing the Adam optimizer for training,\nwhich adjusts the learning rate for each parameter individually"}, {"title": "B. Data Sketches for Real-time Anomaly Detection", "content": "Data sketches are efficient probabilistic data structures that\nprovide approximate representations of large datasets. These\nstructures allow for scalable computations of various statistical\nproperties without requiring the entire dataset to be stored\nor processed at once. In this subsection, we will delve into\nthe mathematical foundation of data sketches and explain\nour anomaly detection steps. We summarized our method in\nAlgorithm 1."}, {"title": "1) Feature Extraction and Hashing:", "content": "Given a set of images\n$I = {I_1, I_2, ..., I_m}$, we first extract feature vectors $f_i \\in$\n$R^d$ from each image $I_i$ using a pre-trained model. Let $F =$\n{$f_1, f_2,..., f_m$} represent the set of extracted features and the\nfeature extractor, namely the corresponding layers of the neural\nnetwork, are noted as\n$f_i = \\phi(I_i), \\forall I_i \\in I$\nTo create a data sketch, we apply a hash function$h: R^d \\rightarrow R^k$\nthat maps each feature vector to a lower-dimensional space.\n$h_i = h(f_i), h_i \\epsilon R^k, \\forall f_i \\epsilon F$\nHash functions are well-performed in clustering and anomaly\ndetection, because they can be used to group similar data\npoints together and detect anomalies in data by identifying\ndata points with unusual hash values."}, {"title": "2) Constructing the Baseline Library:", "content": "The baseline library\nB is constructed by aggregating the hashed feature vectors of\na representative set of CT-scan images. Let $F_{baseline}$ denote the\nset of features from the baseline images.\n$B = {h_1, h_2, ..., h_m}, h_i = h(f_i), f_i \\epsilon F_{baseline}$"}, {"title": "3) Integration with Real-time Feature Extraction:", "content": "Our ap-\nproach seamlessly integrates data sketches with feature extrac-\ntion to enhance the accuracy and efficiency of drift detection.\nThe pre-trained model extracts intricate features from each\nimage, converting them into data sketches that allow for quick"}, {"title": "4) Real-time Anomaly Detection:", "content": "For an incoming image\n$I_{new}$, we extract its feature vector $f_{new}$ and hash it to obtain\nthe corresponding data sketch $h_{new}$.\n$f_{new} = \\phi(I_{new})$\n$h_{new} = h(f_{new})$\nTo detect data drift, we compare the distribution of the new\ndata sketch hnew with the baseline library B using the Jaccard\nsimilarity. The Jaccard score measures the maximum differ-\nence between the empirical cumulative distribution functions\n(CDFs) of the two datasets."}, {"title": "C. Similarity Comparisons for Drift Detection", "content": "1) Kolmogorov-Smirnov (KS) Statistic: Let $F_B(x)$ and\n$F_{new}(x)$ be the empirical CDFs of the baseline library and the\nnew data sketch, respectively. The KS statistic D is defined\nas:\n$D = sup_x |F_B(x) - F_{new}(x)|$\nwhere sup denotes the supremum function. The KS test then\nevaluates whether D exceeds a critical value Da based on the\ndesired significance level a."}, {"title": "2) Cosine Similarity:", "content": "Cosine similarity is a useful metric\nfor comparing two medical images by evaluating the similarity\nbetween their feature vectors. Each image is represented as\na vector of features, and the cosine similarity between these\nvectors is calculated as:\n$cosine\\_similarity(a, b) = \\frac{a \\cdot b}{||a|| ||b||}$\nHere, $a \\in R^M$ and $b \\in R^M$ are the feature vectors of the two\nimages and M \u2208 Z+ is arbitrary. The value ranges from -1 to\n1, with 1 indicating identical images, 0 indicating no similarity,\nand -1 indicating completely negative identical images."}, {"title": "D. Sensitivity Evaluation via Incremental Noise Introduction", "content": "To assess the sensitivity of cosine similarity in detecting\ndata drift, we systematically introduced incremental noise\ninto our experimental dataset and monitored the similarity\nmetric. Specifically, Gaussian noise was added in 10-20%\nincrements to simulate real-world disturbances in medical\nimaging data, such as fluctuations in image acquisition or\nprocessing conditions. Beginning with a dataset free of noise,\nwe gradually added noise at each phase and recalculated\nthe cosine similarity for each pair of images following each\nincrease. This approach allowed us to methodically evaluate\nhow the similarity scores adjusted to escalating levels of\ndata corruption. Our objective was to pinpoint the threshold\nat which the cosine similarity metric signaled a significant\ndeviation from the baseline, thus indicating the onset of drift."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, we will delve into\nthe mathematical foundation of data sketches and explain\nour anomaly detection steps. We summarized our method in\nAlgorithm 1."}, {"title": "A. Experimental Datasets", "content": "In our research, we utilize the MedMNIST [13] dataset as\nbenchmark and 9,987 breast cancer images for fine-tuning. The\nMedMNIST is an extensive repository of standardized biomed-\nical images specifically created to support the development\nand assessment of machine learning algorithms in medical\nimaging. MedMNIST mirrors the structure of the MNIST\ndataset but is significantly larger and specifically adapted for\nbiomedical purposes. It comprises ten distinct 2D datasets and\neight 3D datasets. The ten datasets in MedMNIST encompass\na range of medical conditions and imaging techniques. For\nour analysis, we have selected eight common 2D datasets\nas benchmarks including PathMNIST, BloodMNIST, Organ-\nMNIST, ChestMNIST, DermaMNIST, BreastMNIST, O\u0421\u0422\u041c-\nNIST and PneumoniaMNIST."}, {"title": "1) Data Splits:", "content": "To ensure robust evaluation, each dataset in\nMedMNIST is split into 7 to 10 groups for simulating different\nscenarios of data distributional shifts."}, {"title": "B. Evaluation of Model Accuracy", "content": "We evaluated the performance of 5 distinct mod-\nels-Customized Model, VGG16, and ResNet50, ResNet152"}, {"title": "C. Evaluation of Drift Detection", "content": "Baseline 1 (without data-sketches and fine-tuning). From\nthe experimental results shown in Fig. 3, it is evident that sig-\nnificant data drift starts to emerge around the sixth month. This\ndrift is reflected by the noticeable increase in the Kolmogorov-\nSmirnov (KS) statistic across most of the MNIST dataset\nvariants, such as PathMNIST, BloodMNIST, and others. This\nsuggests that changes in the underlying data distribution begin\nat this point, which could negatively impact model perfor-\nmance.\nAlthough there is some level of similarity between datasets\nover time, this similarity is unstable. The fluctuations in the\nKS statistic across different time points indicate inconsistency\nin the relationship between the datasets. For certain months,\nthe data distributions may remain relatively stable, while in\nothers, there are substantial shifts. This instability makes it\nchallenging to draw reliable conclusions from the observed\nsimilarities, especially in a sensitive domain like medical\nimaging. Moreover, the absolute values of the KS statistic\nshow that the overall similarity scores are relatively low.\nWithout fine-tuning, the feature extraction process struggles\nto capture the deeper similarities between datasets, resulting\nin poor comparison scores. This indicates that the raw features\nlack sufficient discriminatory power when comparing different\ndatasets, especially when significant data drift occurs. The\ninability of the non-fine-tuned model to adequately perceive\nfeature differences highlights the necessity of fine-tuning to\nbetter adapt to the specific characteristics of the datasets and\nimprove the accuracy and stability of the feature comparisons.\nBaseline 2 (without data-sketches and fine-tuning). In Fig.\n4, it is clear that the cosine similarity scores across the 8 Med-\nMNIST datasets remain consistently low. This suggests that\nthe model, as it stands without fine-tuning, is not able to\ngenerate feature vectors that sufficiently differentiate between\nsubtle variations in the image data. This is problematic,\nespecially when dealing with medical imaging tasks where\naccurate feature representation is crucial for anomaly detection\nor diagnosis. The fact that even visually similar images yield\nlow cosine similarity scores points to a fundamental issue in\nthe feature extraction mechanism, underscoring the need for\nfine-tuning to improve the quality of these extracted features.\nBaseline 3 (fine-tuned without data-sketches and noises). In\nFig. 5 illustrates that without data-sketches, there is noticeable\nfluctuation in the cosine similarity scores across different\ndatasets over time. This fluctuation indicates inconsistencies\nin data quality, leading to uneven similarity comparisons. The\nvariations suggest that certain datasets have more significant\ndifferences, resulting in a drop in similarity scores.\nIncorporating data-sketches can help mitigate these fluc-\ntuations by making similarity comparisons more consistent\nand fair. Data-sketches allow for more compact and robust\nfeature representations, reducing noise and outliers in the data.\nThe poor similarity scores, around 45% to 55%, imply that\notherwise."}, {"title": "D. Sensitivity Evaluation", "content": "In the background, Section II-B, we introduced several fac-\ntors that can cause dataset drift. Here, we introduce four types\nof noise-Gaussian Noise, Salt-and-Pepper Noise, Speckle\nNoise, and Poisson Noise [15] each corresponding to one\nof these drift-inducing factors. These noises are used to test\nthe sensitivity of our method under different conditions that\nmay simulate real-world drift scenarios\n1) Gaussian Noise: Gaussian noise is often associated with\nsensor performance fluctuations or low-light conditions. It can\noccur due to electronic noise in the imaging sensor, which\nmight be influenced by temperature or poor lighting.\n2) Salt-and-Pepper Noise: This type of noise can be linked\nto transmission errors or intentional tampering. It's a form of\nimpulse noise where random pixels in the image are corrupted,\nleading to black-and-white spots, resembling salt and pepper.\n3) Speckle Noise: Speckle noise is commonly observed\nin coherent imaging systems such as ultrasound, radar, or\nSAR. It results from the interference of wavefronts, leading to\ngranular noise, and can be considered when evaluating subject\nvariations or device-specific characteristics.\n4) Poisson Noise: Poisson noise is related to the statistical\nnature of photon counting and is often linked to lighting\nconditions. In low-light imaging scenarios, the number of\nphotons hitting the sensor follows a Poisson distribution,\nleading to this type of noise. Therefore, it can be associated\nwith varying lighting conditions or low-light environments.\nFig. 7 shows the cosine similarity scores over time with\njust 1% random noise applied to the Med-MNIST datasets.\nThe results highlight the sensitivity of the model, as even\nwith this minimal noise level, the system is able to detect\ndrift effectively. Notably, when drift occurs around the fourth\ndataset, the similarity score drops significantly from nearly\n100% to around 81%.\nThis sharp decline demonstrates the model's capability to\nrespond to even small distributional changes, making it highly\nsensitive to drift (with qualified datasets). The ability to detect\nsuch subtle shifts with only 1% noise showcases the robustness\nof the approach, ensuring that even minor anomalies or devia-\ntions in the data can be identified promptly. This sensitivity is\nparticularly valuable in medical imaging, where early detection\nof even the slightest anomalies can be critical for diagnosis and\ntreatment."}, {"title": "V. RELATED WORK", "content": "Current research [16]\u2013[19] sought to mitigate the limitations\nof data drift by utilizing transfer learning, which adapts pre-"}, {"title": "VI. CONCLUSION", "content": "This paper introduces an advanced method for detecting\ndistributional drift in CT-scan medical imaging by utilizing\ndata-sketching techniques and fine-tuning a ViT model. The\napproach enhances anomaly detection accuracy and stability,\noffering a scalable solution for maintaining model performance\nin dynamic clinical settings. By leveraging data sketches and\ncosine similarity, the method enables efficient real-time image\ncomparison, improving model robustness against subtle data\nshifts."}]}