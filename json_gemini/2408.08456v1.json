{"title": "Efficient Data-Sketches and Fine-Tuning for Early Detection of Distributional Drift in Medical Imaging", "authors": ["Yusen Wu", "Hao Chen", "Alex Pissinou Makki", "Phuong Nguyen", "Yelena Yesha"], "abstract": "Distributional drift detection is important in medical applications as it helps ensure the accuracy and reliability of models by identifying changes in the underlying data distribution that could affect diagnostic or treatment decisions. However, current methods have limitations in detecting drift; for example, the inclusion of abnormal datasets can lead to unfair comparisons. This paper presents an accurate and sensitive approach to detect distributional drift in CT-scan medical images by leveraging data-sketching and fine-tuning techniques. We developed a robust baseline library model for real-time anomaly detection, allowing for efficient comparison of incoming images and identification of anomalies. Additionally, we fine-tuned a vision transformer pre-trained model to extract relevant features using breast cancer images as an example, significantly enhancing model accuracy to 99.11%. Combining with data-sketches and fine-tuning, our feature extraction evaluation demonstrated that cosine similarity scores between similar datasets provide greater improvements, from around 50% increased to 100%. Finally, the sensitivity evaluation shows that our solutions are highly sensitive to even 1% salt-and-pepper and speckle noise, and it is not sensitive to lighting noise (e.g., lighting conditions have no impact on data drift). The proposed methods offer a scalable and reliable solution for maintaining the accuracy of diagnostic models in dynamic clinical environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Distributional drift [1], also known as dataset drift, in medical imaging refers to changes in data distribution over time, which can significantly affect the performance of machine learning models used for diagnostic purposes. This drift may result from various factors, including alterations in imaging equipment, differences in imaging protocols, variations in patient demographics, or updates in image preprocessing techniques. Detecting and managing drift is critical in the medical field to ensure that models remain accurate and reliable. Ignoring drift can lead to incorrect diagnoses or suboptimal treatment recommendations, thereby potentially compromising patient care. Therefore, continuous monitoring and adaptation of models are essential to maintain their effectiveness in dynamic clinical environments. Despite significant advancements in data drift detection within medical imaging, current methodologies exhibit several limitations that must be addressed to improve their effectiveness. We discuss the limitations of L1, L2 and L3 methods as follows:\n(LI): Analyzing the distribution of image data is particularly challenging due to its high-dimensional characteristics. Unlike numerical data, where statistical properties like mean and variance can be straightforwardly computed and compared, image data necessitates specialized feature extraction and dimensional reduction techniques to perform meaningful distribution analysis.\n(L2): Abnormal datasets can introduce bias in drift detection, which can often be mitigated without compromising the overall data quality. However, efficiently identifying and isolating these abnormal datasets is essential to preserve data integrity. Such abnormalities may arise from data entry errors, sensor malfunctions, or even malicious activities. Implementing effective and efficient anomaly detection techniques is critical for pre-processing data before applying drift detection methods. For instance, as shown in Fig. 1, three medical images were identified as unsuitable for model training, thereby impacting the overall data quality.\n(L3): Another limitation is the absence of real-time processing solutions for handling abnormal data, especially during the preprocessing stage. This process often relies on hospital experts to manually label and remove invalid data before training, which significantly increases the cost and complexity of model development. When datasets contain substantial noise or inconsistencies, the drift detection process may incorrectly identify or overlook drifts, thereby compromising the reliability of the analysis.\nTo overcome the identified limitations, we fine-tuned a pre-"}, {"title": "II. BACKGROUND", "content": "A. Distributional Drift (Dataset Shift) in Medical Images\nDistributional drift, also known as dataset shift, refers to changes in the statistical properties of data between the training and deployment phases of machine learning models [1]. In medical imaging, this drift can manifest as differences in pixel intensities, textures, or other visual characteristics between the images used for training and those encountered during real-world application. This phenomenon is particularly prevalent in medical imaging due to the continuous evolution of imaging technologies, variations in imaging equipment, and changes in patient demographics. Such variations can result in significant differences between the training data and new data, challenging the model's ability to make accurate predictions. Addressing distributional drift is critical because it directly affects the performance and reliability of machine learning models in clinical settings. A model trained on a specific dataset may perform well during initial testing but might fail to maintain its accuracy when applied to new datasets that differ in subtle yet significant ways. This discrepancy can lead to misdiagnosis or incorrect treatment recommendations, potentially compromising patient safety. As medical imaging practices and technologies continue to evolve, it is essential to continuously monitor and adapt models to ensure their effectiveness. Ensuring that models can robustly handle or adapt to distributional drift is vital not only for maintaining high standards of patient care but also for complying with regulatory requirements governing the use of AI in healthcare.\nB. Causes of Distributional Drift in Image Data\nDistributional drift in image data can occur due to various factors related to changes in data, environmental conditions, or the image acquisition process. One common cause is variations in lighting conditions, which can occur due to different times of day, weather conditions, or light sources, leading to alterations in brightness and contrast that affect the data distribution. Changes in imaging devices, such as switching cameras or scanning devices, inconsistent calibration, lens aging, or fluctuations in sensor performance, can also result in changes to image quality and subsequently impact the data distribution. Another contributing factor is subject variations, where changes in the shape, posture, or size of the subject being imaged can lead to shifts in the feature distribution of the images. Additionally, modifying the resolution or size of images can alter the distribution of image features, potentially causing drift. Different devices or sensors capturing the same scene may record varying levels of detail, leading to distributional drift. Finally, images taken from different angles of the same subject can introduce changes in viewpoint, resulting in shifts in the data distribution. These factors highlight the"}, {"title": "III. PROPOSED METHODS", "content": "To effectively address the challenges of distributional drift in medical imaging and enhance the accuracy of feature extraction, we propose a comprehensive solution that combines advanced fine-tuning techniques with real-time anomaly detection using data sketching, as shown in Fig. 2. This approach not only optimizes model performance but also ensures the reliable detection of subtle changes in data distribution within dynamic clinical environments.\nA. Fine-tuned Pre-trained Model for Feature Extractions\nOur method leverages a pre-trained deep learning model for feature extraction, capitalizing on its ability to encode rich feature representations from input data. Pre-trained models, trained on extensive datasets, offer the advantage of transfer learning, allowing the knowledge gained from large-scale data to be applied to specific tasks with limited data availability. This approach utilizes the pre-trained model within the context of our problem domain to enhance feature extraction.\n1) Data Preparation: The first step in fine-tuning a model for medical image drift detection involves thorough data preparation. This process includes selecting an qualified dataset, preprocessing the images, and applying data-sketches to improve the data quality.\n2) Layer Freezing and Fine-Tuning: To preserve the general features learned during pretraining, the some of the layers from the pre-trained model are frozen, not updated during the fine tuning process. Only the final few layers, which are responsible for task-specific features, are fine-tuned. Let W = {W1,W2,..., Wn} be the pre-trained model parameters, and n \u2208 Z+ is the index of the layers with trainable parameters. Supposing the data flow is from the layer-1 propagating till layer-n, we can have the first l-frozen layers and regular layers as\n\\begin{equation}\nW_{\\text{frozen}} = \\{W_1, W_2, ..., W_l\\}\n\\end{equation}\n\\begin{equation}\nW_{\\text{tunable}} = \\{W_{l+1}, W_{l+2}, ..., W_n\\}\n\\end{equation}\nwhere Wfrozen are the weights of the frozen layers, and Wtunable are the weights of the layers to be fine-tuned. The pre-trained models [10]\u2013[12] are targeting on other tasks rather than detecting the data drift. Therefore, the minor customization are added to the final layer. It is replaced with a fully connected layer with sigmoid function as the activation function, tailored for the binary classification task:\n\\begin{equation}\nP = \\frac{1}{1 + e^{-w^T x_n + b}}\n\\end{equation}\nwhere xn corresponds to output embedding from the previous layer, T denotes the vector transpose and wn and b \u2208 R consist of wn, the parameters of the last layer.\n3) Loss Function: The loss function used for training is the Binary Cross-Entropy (BCE) loss, which is suitable for binary classification tasks. Given the output probability p from the final layer, the BCE loss is defined as:\n\\begin{equation}\nL_{BCE} = - \\frac{1}{B} \\sum_{i=1}^{B} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n\\end{equation}\nwhere yi is the true label (1 for the correct class, 0 otherwise), pi is the predicted probability for training sample i and B\u2208 Z+ is the size of the input, typically as the mini-batch size.\n4) Optimization Algorithm: We use mini-batch stochastic gradient descent employing the Adam optimizer for training, which adjusts the learning rate for each parameter individually"}, {"title": "B. Data Sketches for Real-time Anomaly Detection", "content": "Data sketches are efficient probabilistic data structures that provide approximate representations of large datasets. These structures allow for scalable computations of various statistical properties without requiring the entire dataset to be stored or processed at once. In this subsection, we will delve into the mathematical foundation of data sketches and explain our anomaly detection steps. We summarized our method in Algorithm 1.\n1) Feature Extraction and Hashing: Given a set of images I = {I1, I2, ..., Im}, we first extract feature vectors fi \u2208 Rd from each image I\u2081 using a pre-trained model. Let F = {f1, f2,..., fm} represent the set of extracted features and the feature extractor, namely the corresponding layers of the neural network, are noted as\n\\begin{equation}\n f_i = \\phi(I_i), \\forall I_i \\in I\n\\end{equation}\nTo create a data sketch, we apply a hash functionh: Rd \u2192 Rk that maps each feature vector to a lower-dimensional space.\n\\begin{equation}\nh_i = h(f_i), h_i \\in R^k, \\forall f_i \\in F\n\\end{equation}\nHash functions are well-performed in clustering and anomaly detection, because they can be used to group similar data points together and detect anomalies in data by identifying data points with unusual hash values.\n2) Constructing the Baseline Library: The baseline library B is constructed by aggregating the hashed feature vectors of a representative set of CT-scan images. Let Fbaseline denote the set of features from the baseline images.\n\\begin{equation}\nB = \\{h_1, h_2, ..., h_m\\}, h_i = h(f_i), f_i \\in F_{\\text{baseline}}\n\\end{equation}\n3) Integration with Real-time Feature Extraction: Our approach seamlessly integrates data sketches with feature extraction to enhance the accuracy and efficiency of drift detection. The pre-trained model extracts intricate features from each image, converting them into data sketches that allow for quick and scalable comparisons. This integration ensures that our method can handle high-dimensional medical images while maintaining high performance in real-time applications. By leveraging the power of data sketches and the KS statistic, we provide a comprehensive solution for detecting and managing data drift in CT-scan medical images, ensuring the reliability and accuracy of diagnostic models.\n4) Real-time Anomaly Detection: For an incoming image Inew, we extract its feature vector fnew and hash it to obtain the corresponding data sketch hnew.\n\\begin{equation}\n f_{\\text{new}} = \\phi(I_{\\text{new}})\n\\end{equation}\n\\begin{equation}\nh_{\\text{new}} = h(f_{\\text{new}})\n\\end{equation}\nTo detect data drift, we compare the distribution of the new data sketch hnew with the baseline library B using the Jaccard similarity. The Jaccard score measures the maximum difference between the empirical cumulative distribution functions (CDFs) of the two datasets."}, {"title": "C. Similarity Comparisons for Drift Detection", "content": "1) Kolmogorov-Smirnov (KS) Statistic: Let FB(x) and Fnew (2) be the empirical CDFs of the baseline library and the new data sketch, respectively. The KS statistic D is defined as:\n\\begin{equation}\nD = \\sup_x |F_B(x) - F_{\\text{new}}(x)|\n\\end{equation}\nwhere sup denotes the supremum function. The KS test then evaluates whether D exceeds a critical value Da based on the desired significance level a.\n2) Cosine Similarity: Cosine similarity is a useful metric for comparing two medical images by evaluating the similarity between their feature vectors. Each image is represented as a vector of features, and the cosine similarity between these vectors is calculated as:\n\\begin{equation}\n\\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{||a|| ||b||}\n\\end{equation}\nHere, a \u2208 RM and b \u2208 RM are the feature vectors of the two images and M \u2208 Z+ is arbitrary. The value ranges from -1 to 1, with 1 indicating identical images, 0 indicating no similarity, and -1 indicating completely negative identical images. In the context of medical imaging, including CT scans and MRIs, cosine similarity is valuable for tasks like identifying abnormalities or monitoring disease progression. It quantifies the similarity between a new image and either a reference image or a collection of normal images. This metric is particularly effective in detecting subtle changes in image features, which are crucial for accurate diagnosis and treatment planning."}, {"title": "D. Sensitivity Evaluation via Incremental Noise Introduction", "content": "To assess the sensitivity of cosine similarity in detecting data drift, we systematically introduced incremental noise into our experimental dataset and monitored the similarity metric. Specifically, Gaussian noise was added in 10-20% increments to simulate real-world disturbances in medical imaging data, such as fluctuations in image acquisition or processing conditions. Beginning with a dataset free of noise, we gradually added noise at each phase and recalculated the cosine similarity for each pair of images following each increase. This approach allowed us to methodically evaluate how the similarity scores adjusted to escalating levels of data corruption. Our objective was to pinpoint the threshold at which the cosine similarity metric signaled a significant deviation from the baseline, thus indicating the onset of drift."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "A. Experimental Datasets\nIn our research, we utilize the MedMNIST [13] dataset as benchmark and 9,987 breast cancer images for fine-tuning. The MedMNIST is an extensive repository of standardized biomedical images specifically created to support the development and assessment of machine learning algorithms in medical imaging. MedMNIST mirrors the structure of the MNIST dataset but is significantly larger and specifically adapted for biomedical purposes. It comprises ten distinct 2D datasets and eight 3D datasets. The ten datasets in MedMNIST encompass a range of medical conditions and imaging techniques. For our analysis, we have selected eight common 2D datasets as benchmarks including PathMNIST, BloodMNIST, Organ-MNIST, ChestMNIST, DermaMNIST, BreastMNIST, O\u0421\u0422\u041c-NIST and PneumoniaMNIST.\n1) Data Splits: To ensure robust evaluation, each dataset in MedMNIST is split into 7 to 10 groups for simulating different scenarios of data distributional shifts.\nB. Evaluation of Model Accuracy\nWe evaluated the performance of 5 distinct models-Customized Model, VGG16, and ResNet50, ResNet152"}, {"title": "C. Evaluation of Drift Detection", "content": "Baseline 1 (without data-sketches and fine-tuning). From the experimental results shown in Fig. 3, it is evident that significant data drift starts to emerge around the sixth month. This drift is reflected by the noticeable increase in the Kolmogorov-Smirnov (KS) statistic across most of the MNIST dataset variants, such as PathMNIST, BloodMNIST, and others. This suggests that changes in the underlying data distribution begin at this point, which could negatively impact model performance.\nAlthough there is some level of similarity between datasets over time, this similarity is unstable. The fluctuations in the KS statistic across different time points indicate inconsistency in the relationship between the datasets. For certain months, the data distributions may remain relatively stable, while in others, there are substantial shifts. This instability makes it challenging to draw reliable conclusions from the observed similarities, especially in a sensitive domain like medical imaging. Moreover, the absolute values of the KS statistic show that the overall similarity scores are relatively low. Without fine-tuning, the feature extraction process struggles to capture the deeper similarities between datasets, resulting in poor comparison scores. This indicates that the raw features lack sufficient discriminatory power when comparing different datasets, especially when significant data drift occurs. The inability of the non-fine-tuned model to adequately perceive feature differences highlights the necessity of fine-tuning to better adapt to the specific characteristics of the datasets and improve the accuracy and stability of the feature comparisons.\nBaseline 2 (without data-sketches and fine-tuning). In Fig. 4, it is clear that the cosine similarity scores across the 8 Med-MNIST datasets remain consistently low. This suggests that the feature extraction process, without fine-tuning, performs poorly in capturing meaningful similarities between highly similar images. Despite the datasets containing images that should be visually close, the cosine similarity scores indicate otherwise. The poor similarity scores, around 45% to 55%, imply that the model, as it stands without fine-tuning, is not able to generate feature vectors that sufficiently differentiate between subtle variations in the image data. This is problematic, especially when dealing with medical imaging tasks where accurate feature representation is crucial for anomaly detection or diagnosis. The fact that even visually similar images yield low cosine similarity scores points to a fundamental issue in the feature extraction mechanism, underscoring the need for fine-tuning to improve the quality of these extracted features.\nBaseline 3 (fine-tuned without data-sketches and noises). In Fig. 5 illustrates that without data-sketches, there is noticeable fluctuation in the cosine similarity scores across different datasets over time. This fluctuation indicates inconsistencies in data quality, leading to uneven similarity comparisons. The variations suggest that certain datasets have more significant differences, resulting in a drop in similarity scores. Incorporating data-sketches can help mitigate these fluctuations by making similarity comparisons more consistent and fair. Data-sketches allow for more compact and robust feature representations, reducing noise and outliers in the data."}, {"title": "D. Sensitivity Evaluation", "content": "In the background, Section II-B, we introduced several factors that can cause dataset drift. Here, we introduce four types of noise\u2014Gaussian Noise, Salt-and-Pepper Noise, Speckle Noise, and Poisson Noise [15]\u2014each corresponding to one of these drift-inducing factors. These noises are used to test the sensitivity of our method under different conditions that may simulate real-world drift scenarios\n1) Gaussian Noise: Gaussian noise is often associated with sensor performance fluctuations or low-light conditions. It can occur due to electronic noise in the imaging sensor, which might be influenced by temperature or poor lighting.\n2) Salt-and-Pepper Noise: This type of noise can be linked to transmission errors or intentional tampering. It's a form of impulse noise where random pixels in the image are corrupted, leading to black-and-white spots, resembling salt and pepper.\n3) Speckle Noise: Speckle noise is commonly observed in coherent imaging systems such as ultrasound, radar, or SAR. It results from the interference of wavefronts, leading to granular noise, and can be considered when evaluating subject variations or device-specific characteristics.\n4) Poisson Noise: Poisson noise is related to the statistical nature of photon counting and is often linked to lighting conditions. In low-light imaging scenarios, the number of photons hitting the sensor follows a Poisson distribution, leading to this type of noise. Therefore, it can be associated with varying lighting conditions or low-light environments.\nFig. 7 shows the cosine similarity scores over time with just 1% random noise applied to the Med-MNIST datasets. The results highlight the sensitivity of the model, as even with this minimal noise level, the system is able to detect drift effectively. Notably, when drift occurs around the fourth dataset, the similarity score drops significantly from nearly 100% to around 81%. This sharp decline demonstrates the model's capability to respond to even small distributional changes, making it highly sensitive to drift (with qualified datasets). The ability to detect such subtle shifts with only 1% noise showcases the robustness of the approach, ensuring that even minor anomalies or deviations in the data can be identified promptly. This sensitivity is particularly valuable in medical imaging, where early detection of even the slightest anomalies can be critical for diagnosis and treatment."}, {"title": "V. RELATED WORK", "content": "Current research [16]\u2013[19] sought to mitigate the limitations of data drift by utilizing transfer learning, which adapts pre-"}, {"title": "VI. CONCLUSION", "content": "This paper introduces an advanced method for detecting distributional drift in CT-scan medical imaging by utilizing data-sketching techniques and fine-tuning a ViT model. The approach enhances anomaly detection accuracy and stability, offering a scalable solution for maintaining model performance in dynamic clinical settings. By leveraging data sketches and cosine similarity, the method enables efficient real-time image comparison, improving model robustness against subtle data shifts."}]}