{"title": "PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation", "authors": ["Muntasir Wahed", "Kiet A. Nguyen", "Adheesh Juvekar", "Xinzhuo Li", "Xiaona Zhou", "Vedant Shah", "Tianjiao Yu", "Pinar Yanardag", "Ismini Lourentzou"], "abstract": "Despite significant advancements in Large Vision-Language Models (LVLMs), existing pixel-grounding models operate on single-image settings, limiting their ability to perform detailed, fine-grained comparisons across multiple images. Conversely, current multi-image understanding models lack pixel-level grounding. Our work addresses this gap by introducing the task of multi-image pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates pixel-level grounding with robust multi-image reasoning capabilities to produce contextually rich, pixel-grounded explanations. Central to PRIMA is an efficient vision module that queries fine-grained visual representations across multiple images, reducing TFLOPs by 25.3%. To support training and evaluation, we curate M4SEG, a new reasoning segmentation benchmark consisting of ~224K question-answer pairs that require fine-grained visual understanding across multiple images. Experimental results demonstrate PRIMA outperforms state-of-the-art baselines.", "sections": [{"title": "1. Introduction", "content": "Large Vision-Language Models (LVLMs) have demonstrated remarkable success in reasoning and perception tasks, due to their ability to jointly understand and process visual and textual information. LVLMs leverage large-scale multimodal data that leads to improved performance on various downstream tasks, i.e., question answering [29], image captioning [26, 44], object detection [8], etc. The synergy between vision and language has also paved the way for more sophisticated image understanding capabilities, including spatial reasoning [7], semantic segmentation [23], reasoning segmentation [22], etc. These advancements have enabled precise pixel-level grounding and reasoning referring to objects within an image, which improves both the quality and the explainability of the generated response.\nDespite these advances in single-image visual perception, fine-grained comparative analysis across images remains an open challenge, especially in scenarios where understanding subtle differences or similarities between images is crucial. Such multi-image understanding is fundamentally different from single-image understanding because different tasks would require a joint understanding of different visual elements across images. For example, as shown in Figure 1, depending on the nature of the task, the model must identify varied fine-grained differences in visual scenes, such as the presence and functions of objects, compare objects in terms of spatial arrangements, and ground part-level functionalities across diverse contexts.\nThe ability to analyze multi-image contexts allows models to discern specific visual features that may differ across instances of the same object, or that serve as distinguishing features between different objects, enabling a wide range of applications, from detailed visual analysis (e.g., medical imaging) to tasks like e-commerce, where comparing product details is crucial. Furthermore, fine-grained cross-image comparisons with pixel-level grounding can also enhance the interpretability of LVLMs, offering localized visual explanations for why certain visual elements are prioritized.\nExisting works on LVLM-based pixel grounding and reasoning, e.g., LISA [22] or GLaMM [37], have focused on aligning textual descriptions with specific regions or pixels within an image. However, they primarily focus on single-image scenarios, lacking the ability to perform coherent multi-image reasoning. On the other hand, recent works such as SparklesChat [16] and VPG-C [25] have explored multi-image understanding by facilitating dialogue systems and visual perception across multiple images. These approaches highlight the importance of cross-image comparison and dialogue-based understanding but lack the detailed pixel-level grounding essential for fine-grained multi-image visual analysis.\nTo bridge the research gap between multi-image understanding and pixel-level grounding, in this work, we introduce the new task of multi-image pixel-grounded reasoning segmentation, where given a comparative free-form question involving two or more images, a model must produce a natural language response accompanied by pixel-level grounding of the relevant objects and parts. To our knowledge, this is the first work to formalize this combined fine-grained reasoning and grounding task. To facilitate evaluation in this novel task, we introduce M4SEG, a new benchmark consisting of ~224K comparative questions that require simultaneous reasoning across multiple images along with corresponding answers grounded on the objects and part information. M4SEG contains 115 unique objects and 251 unique parts, with each question-answer pair featuring multiple cross-image target objects and part segmentation masks, making it a challenging benchmark for multi-image pixel-grounded reasoning segmentation. Additionally, we propose PRIMA, the first Pixel-gRounded Multi-Image Segmentation Reasoning Vision-Language Model for this task. PRIMA employs an instruction-guided multi-image adaptation module to reason across multiple images with fine-grained grounding. Experimental results demonstrate strong performance and computational efficiency compared to state-of-the-art baselines (Figure 2). In summary, the contributions of this work are as follows:\n\u2022 We propose the novel task of multi-image pixel-grounded reasoning segmentation, which necessitates fine-grained comparison and contextual understanding across multiple images at the pixel level, requiring models to produce responses grounded in specific objects and parts.\n\u2022 We introduce M4SEG, a new challenging benchmark with over 224K multi-image QA pairs, annotated with multiple object and part segmentation masks to enable and evaluate pixel-grounded multi-image visual understanding.\n\u2022 We propose PRIMA, a vision-language model specifically designed for this new task. Unlike existing models, PRIMA excels in generating natural language responses accompanied by contextually grounded segmentations across multiple images. PRIMA is optimized for computational efficiency by incorporating a cross-modal attention mechanism, which enables instruction-guided alignment of relevant visual features across images, reducing overhead while maintaining high accuracy in pixel-level reasoning. Extensive experiments demonstrate PRIMA'S performance and efficiency against strong baselines."}, {"title": "2. Related Work", "content": "LVLMs have made significant strides in reasoning, segmentation, and multimodal understanding. However, as summarized in Table 1, existing models either generate ungrounded responses or perform single-image pixel-grounding, restricting their utility for fine-grained multi-image visual reasoning tasks. Our work aims to address this gap with a new multi-image pixel-grounding reasoning segmentation task that supports detailed, cross-image comparisons and contextual reasoning for objects and parts.\n2.1. Large Vision Language Models (LVLMs)\nRecent advancements have broadened the research focus from traditional text-only language models to models integrating both visual and textual information, thereby enhancing multimodal reasoning capabilities. Models like LLaVA [29], BLIP-2 [26], and Flamingo [1] have explored combining visual and language modalities through various integration techniques, from embedding visual features into the LLM input space to deeper fusion within intermediate layers. Other developments, such as Kosmos-2 [33], Shikra [5], and Ferret [55], have introduced object localization tasks to enhance visual comprehension, while Emu [42], CogVLM [45], and DreamLLM [14] have focused on incorporating visual generation capabilities.\nBuilding on this foundation, various methods have emerged to further enhance LVLM reasoning. VICL [67] aligns relevant visual examples with text prompts to improve prediction accuracy without parameter updates, while ViGOR [52] enhances visual grounding through fine-grained reward modeling, leveraging both human feedback and automated visual perception tools for better alignment between generated text and image content. STIC [13] employs a two-stage self-training process by generating preference datasets from unlabeled images and fine-tuning the model on this data, enhancing comprehension without the need for pre-labeled inputs. Multimodal-CoT [64] extends chain-of-thought prompting to multimodal inputs to improve logical coherence across modalities, while MVP [34] addresses hallucinations with a multi-view multi-path reasoning mechanism that selects accurate answers. However, these models primarily focus on high-level reasoning, unlike specialized segmentation approaches that improve the capabilities LVLMs to perform pixel-level understanding.\n2.2. LVLM-Based Multi-Image Understanding\nRecent developments in LVLM-based multi-image understanding have introduced various methods aimed at enhancing fine-grained reasoning, cross-modal interactions, and multimodal dialogues. SparklesChat [16] and VPG-C [25] improve coherence across images in multimodal instruction-following tasks by integrating multiple images into dialogue at the word level and generating intermediate-layer guidance to enrich visual reasoning, respectively. CaD-VI [28] and Leopard [17] both enhance comprehension of relationships between images, with CaD-VI focusing on structured visual comparisons between image pairs and Leopard leveraging an adaptive high-resolution encoding module for text-rich, multi-image tasks. To address challenges in handling diverse modalities and long image sequences, methods such as MMMModal [68], LLaVA-NeXT-Interleave [24], and MANTIS [18] employ interleaving techniques to facilitate richer multi-modal reasoning. MMMModal aligns visual and audio data for complex, context-aware interactions, while LLaVA-NeXT-Interleave integrates images, videos, and 3D data to capture intricate visual relationships. In contrast, MANTIS uses interleaved text and images to develop co-reference, reasoning, and temporal understanding. mPLUG-Owl3 [54] and LongLLaVA [48] address long image-sequence understanding using cross-attention mechanisms and hybrid architectures to scale model performance for up to 1000 images. These approaches underscore the need for cross-image comparisons and dialogue-based understanding, but they lack the detailed pixel-level grounding required for fine-grained visual analysis.\n2.3. LVLM-Based Pixel Grounding\nIn the pursuit of enabling LVLMs to generate segmentation masks, several methods have been proposed to tackle challenges in pixel-level grounding. GPT4ROI [60], Ferret [56], Osprey [57] are multi-modal models that process images with bounded regions and text prompts, using techniques like spatial instruction tuning, hybrid region representations, mask-text tuning, and localized visual tokenization."}, {"title": "3. Method", "content": "3.1. Problem Definition\nIn multi-image pixel-grounded reasoning segmentation, we are given a natural language prompt Tp and N\u2081 images I={I\u2081,\u2026, IN\u2081 }, where each image Ij \u2208 R3\u00d7Hj\u00d7Wj is of height Hj and width Wj. The objective is to generate a response that addresses the query by grounding relevant noun phrases in segmentation masks across the series of images. The resulting response includes sets of grounded segments G={G\u2081,\u2026,GN\u2081 }, with each G\u2081 = {gj1,\u06f0\u06f0\u06f0,gjG; } \u2208 [RG; \u00d7 H j \u00d7 W j containing G; text groundings associated with image j. These groundings correspond to a set of segmentation masks M: specifically, for image j, the groundings Gj map to a set of binary masks Mj = {mj1,\u2026,mjG; } \u2208 {0,1}GjHjWj, where a pixel in mji is assigned the value 1 if it covers the visual element associated with grounding gji in image j, and 0 otherwise. Each image may correspond to any number of masks, including zero, provided that there is at least one grounding in the response. In other words, Gj \u2265 0 for any image j and \u2211j=1G; \u2265 1.\nNI\nTo accomplish this task, we propose PRIMA, a model that integrates an LVLM L and a segmentation decoder S. Here, the model response TPRIMA = L (Tp, I) inherently enables grounding via special tags that enclose the grounded noun phrases in G and segmentation tokens SCTPRIMA immediately following G. The segmentation masks M are obtained from embedded information in S using decoder S, formalized as M=S(I, S).\n3.2. PRIMA Architecture\nOur PRIMA architecture, as illustrated in Figure 3, consists of an LVLM backbone and a vision module that incorporates self-supervised semantic features, alongside a segmentation module for pixel grounding.\nLarge Vision-Language Model L. To enable multimodal understanding for text generation, we integrate a pretrained Vicuna-7B architecture [9] L = {E, fi-t} with a vision module & and an image-to-text projection layer fit that aligns visual and text embeddings. The images are first encoded by E : R3\u00d7H\u00d7W \u2192 RL1\u00d7D1 to get image embeddings I\u025b = E(I) \u2208 RNI\u00d7L1\u00d7D1 with sequence length L\u2081 and hidden size D\u2081. These embeddings are then passed into fit : RDI \u2192 RD to obtain Iproj = fit (Is) in the language model space of hidden size D, which are then interleaved within the embedded text prompt Tembed f\u00a3 embed (TP) \u2208 RLT\u00d7D to form multimodal embeddings T\u2208R(LT+NIL1)\u00d7D to be fed into the Vicuna backbone.\nIn practice, given a text prompt or instruction, we prepend it with a sentence that activates the model's visual processing, e.g., \"The <image> (IMAGE1), <image> (IMAGE2), and <image> (IMAGE3) provide an overview of the pictures,\"\nwhere each <image> token is replaced with the corresponding projected image embeddings Iproj \u2208 RN1\u00d7L1\u00d7D, and the remaining prompt content is embedded via a standard text embedding layer fc embed. The LVLM is trained to generate text that combines reasoning about the question with information necessary for pixel grounding.\nVision Module E. To enable efficient multi-image understanding, we propose utilizing BLIP-2's Q-Former cross-attention mechanism [26] to query image representations from visual foundation models. Standard LLaVA-style LVLMs [22, 24, 37] incorporate a CLIP image encoder [35] alongside a simple linear projection layer to bring visual information to the language space. However, this preserves the image embeddings' sequence lengths, which are often large (e.g., 256 [22] or 576 [37]) and may quickly become expensive as the number of images grows. On the other hand, Q-Former has demonstrated strong results when used in tandem with LVLMs [11], especially in multi-image understanding [25], while having a much shorter sequence length. Therefore, we leverage Q-Former, which uses a set of query tokens alongside an optional text prompt, to extract visual representations from image features before projecting and feeding them into our PRIMA architecture.\nFormally, we construct the pre-projection embeddings from I using our vision module E, i.e., I\u025b = E(I). Given image features Iv = V(I) \u2208 RNI\u00d7LEXDE from an image encoder module V, we employ Q-Former to query efficient embeddings from Iv, utilizing a set of learnable query tokens q \u2208 RLq\u00d7D1 and the embedded text TQ = fQ-Former embed (TP) \u2208 RLQ\u00d7D1. Specifically, we use the concatenation of these embeddings as the query and Iv as the and value for Q-Former to fuse representations:\nI\u025b = Q-Former (qTQ, Iv) \u2208 RN1\u00d7L1\u00d7D1,\nwhere L\u2081 = Lq + LQ and denotes concatenation.\nSemantic Feature Extractor. To further improve fine-grained object understanding and segmentation, we propose leveraging semantic features from self-supervised Vision Transformers, specifically the DINO series [4, 12, 32]. These features have been shown to contain strong semantic signals useful for object-part understanding, particularly in tasks like semantic correspondence, co-segmentation, and co-part segmentation [2, 59]. To accomplish this, our vision module V = {CLIP, DINO, Cross-Attention} is composed of a Cross-Attention layer to fuse inputs from CLIP and DINO. We first obtain global features from an EVA-CLIP-g/14 model [41] to get ICLIP = CLIP(I) \u2208 RN1\u00d7LcxDc. Simultaneously, we also process input images through a pretrained DINOv2-L model with registers [12] to extract semantic features: IDINO = DINO(I) \u2208 RNI\u00d7LD\u00d7DD. We then employ cross-attention to fuse these features with ICLIP as the query and IDINO as the key and value to get:\nIv=Cross-Attention (ICLIP, IDINO) \u2208RNI \u00d7 Lc\u00d7Dc.\nFinally, Iv is passed to the Q-Former to obtain image embeddings I\u025b, as discussed in the previous section. We finetune the pretrained query tokens q to imbue them with semantic understanding of the DINOv2 features.\nPixel Grounding. We equip PRIMA with pixel grounding capabilities by incorporating SAM [21], a robust pretrained open-world segmentation model capable of generating segmentation masks from diverse types of input prompts, including text. To leverage this, we follow past works [22, 37] that utilize the LVLM's text space by training a new token, [SEG], whose final hidden state serves as the prompt for the segmentation model, alongside the encoded images. Consequently, each grounded noun phrase gji in the LVLM's output is paired with its unique [SEG] token. To distinguish tokens associated with different images, we append an image identifier to each segmentation token, e.g., [SEG) (IMAGE2). Finally, to clearly delineate noun phrases within the text output, we train a pair of grounding tokens, <p> and </p>, to enclose each noun phrase. After text generation, we pass the [SEG] tokens, alongside the corresponding SAM-encoded image embeddings, into the finetuned SAM decoder for segmentation. Formally, from the generated output, we obtain the embeddings S = {S1,\u2026,SN\u2081} of the segmentation tokens [SEG], each Sj = {Sj1,\u2026\u2026,SjG;} \u2208 RGj\u00d7D containing the Gj segmentation tokens corresponding to the j-th image. We then feed these through a text-to-segmentation projection layer ft-s: RD \u2192 RDs to bring the tokens to the SAM model S's encoding space of hidden size Ds and get Sencode = ft-s (S). We also encode the images I with S's frozen encoder Sencode : R3\u00d7H\u00d7W \u2192 RDs to get Isegmentation = Sencode(I). Finally, we obtain segmentation masks M via the finetuned SAM decoder Sdecode by M=Sdecode (Isegmentation, Sencode).\nTraining Objective PRIMA is trained for text generation using a standard next-token prediction cross-entropy loss, denoted as Ltext. The segmentation decoder is optimized with a combination of Dice loss (LDice) [31] and focal loss (Lfocal) [40] as employed in prior works [21, 22, 37]. The overall training objective is defined as L = a\u00a3text + \u03b2LDice + \u03b3Lfocal, where \u03b1, \u03b2, and \u03b3 are hyperparameters."}, {"title": "4. M4SEGDataset", "content": "As summarized in Table 1, existing benchmarks either do not contain part-level information [18, 24, 39], segmentation masks [18, 24] or multiple images [3, 37, 39]. Given the lack of multi-image understanding benchmarks for pixel-grounded reasoning segmentation, we introduce M4SEG, a novel dataset that enables the evaluation of large vision-language models on this task. Contrary to previous works, our proposed benchmark has open-set question-answer pairs that incorporate object- and part-level information with corresponding segmentation masks and bounding boxes across multiple images.\nFigure 1 presents several examples of our dataset, illustrating the diversity and complexity of comparisons required at varied reasoning levels. For instance, in the first example, chairs and tables appear in different contexts across three images, with the answer capturing their distinct functions-dining, storage, and workspace\u2014along with the corresponding segmented masks. In another example, our M4SEG dataset includes comparisons of objects based on subtle part-level differences, such as variations in the animal and people poses across different images. The open-world nature of examples makes M\u2074SEG a challenging reasoning segmentation benchmark that requires complex cross-image visual co-reference, relational reasoning, and grounding at both object and part-level specificities.\nDataset Construction: We utilize three publicly available datasets that contain object- and part-level segmentation masks, ADE20K-Part-234 [49, 65], Pascal-Part [6], and PACO-LVIS [36], to synthesize multi-image comparison question-answer pairs using the GPT-40 API. For each image in these datasets, we sample a set of pairs and triplets of images, ensuring that (1) images represent similar scenes in terms of background or activities, and (2) they contain comparable objects in the scenes that may share some similarity but are not completely identical. We then pass each sampled image set along with their associated bounding box information for the annotated objects and parts to the GPT-40 API and generate question-answer pairs. We ensure that each question requires multi-image reasoning and engages implicit relational understanding, while each answer grounds specific objects and parts across the images. Apart from meticulously designed prompts, we apply a series of filtering steps to further ensure data quality. Specifically, we exclude responses that lack grounding information across multiple images, questions and answers referring to objects or parts not present in the images, and responses involving more than 16 segmentation masks.\nThe final M4SEG data consists of 224k question-answer (QA) pairs. Figure 4 and Table 2 present detailed statistics and distributions of objects, parts, and targets in the dataset. Each QA pair corresponds to an average of 6 targets (objects or parts), with the maximum number in a QA pair reaching"}, {"title": "5. Experiments", "content": "To the best of our knowledge, PRIMA is the first model capable of addressing complex coarse to fine-grained pixel reasoning tasks that involve multiple targets across multiple images. To establish baselines, we adapt two state-of-the-art LVLMS, LISA [22] and GLAMM [37] to take multiple images as inputs. Specifically, we append image identifiers (e.g., (IMAGE2)) to the segmentation tokens in the input to differentiate tokens from distinct images, then retrieve image-specific segmentation tokens to generate the corresponding masks using SAM. For a fair comparison, we finetune all three models on M4SEG using the same hyperparameters. Each model is fine-tuned for 30 epochs with a batch size of 4, using four A100-40GB GPUs. Following prior works [37, 57], we employ multiple metrics to evaluate the generated segmentation masks and natural language responses, including mIoU, Recall, Semantic Similarity (SS), and S-IoU. Details are provided in Appendix B.\n5.1. Experimental Results\nTable 3 compares PRIMA against reasoning segmentation baselines. We observe that PRIMA improves both segmentation and reasoning performance. Specifically, in the segmentation-based metrics, PRIMA achieves 2.1 percentage"}, {"title": "5.2. Ablation Experiments", "content": "Number of objects in the images: In Figure 5 (left plot), we observe that models perform worse as the number of objects in the images grows, which indicates that crowded scenes pose a significant challenge. PRIMA consistently outperforms GLaMM in most cases, demonstrating its robustness in handling higher object density.\nNumber of target masks: In Figure 5 (right plot), we also observe that model performance degrades with the number of target masks in the images, which is expected since it implies a more complex task. Nonetheless, PRIMA consistently outperforms baselines even in more challenging scenarios with a larger number of target masks.\nObject visibility: In Figure 6, we observe that model performance improves as the visibility of the objects in input"}, {"title": "5.3. Qualitative Analsysis", "content": "Figure 7 shows a qualitative comparison between PRIMA, GLaMM, LISA, and Ground Truth data. The top row in the table shows the questions posed to the models, while the corresponding images display the answers and segmentation masks generated by each model, alongside the Ground Truth reference. For clarity, we use the same colors for segmentation masks as the highlighted text corresponding to the object/part. Results demonstrate that PRIMA outperforms both baselines in segmentation tasks, while also providing more accurate answers. In contrast, GLaMM and LISA confuse the reasoning for the stability of the stool and lamp, respectively, while LISA demonstrates slightly better reasoning than GLaMM."}, {"title": "6. Conclusion", "content": "In this work, we introduce the new task of multi-image pixel-grounded reasoning segmentation. To support training and evaluation, we release the M4SEG benchmark, laying the foundation for future research in multi-image pixel-grounding LVLMs. Finally, we propose PRIMA, a novel vision-language reasoning segmentation model that combines pixel-level grounding with multi-image reasoning capabilities. Experimental results demonstrate PRIMA's superior performance and efficiency over SoTA baselines. We hope this work encourages future vision-language applications in real-world, complex reasoning tasks."}]}