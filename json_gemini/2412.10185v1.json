{"title": "Solving Robust Markov Decision Processes: Generic, Reliable, Efficient", "authors": ["Tobias Meggendorfer", "Maximilian Weininger", "Patrick Wienh\u00f6ft"], "abstract": "Markov decision processes (MDP) are a well-established model for sequential decision-making in the presence of probabilities. In robust MDP (RMDP), every action is associated with an uncertainty set of probability distributions, modelling that transition probabilities are not known precisely. Based on the known theoretical connection to stochastic games, we provide a framework for solving RMDPs that is generic, reliable, and efficient. It is generic both with respect to the model, allowing for a wide range of uncertainty sets, including but not limited to intervals, L\u00b9- or L\u00b2-balls, and polytopes; and with respect to the objective, including long-run average reward, undiscounted total reward, and stochastic shortest path. It is reliable, as our approach not only converges in the limit, but provides precision guarantees at any time during the computation. It is efficient because - in contrast to state-of-the-art approaches - it avoids explicitly constructing the underlying stochastic game. Consequently, our prototype implementation outperforms existing tools by several orders of magnitude and can solve RMDPs with a million states in under a minute.", "sections": [{"title": "1 Introduction", "content": "Robust Markov decision processes. Markov decision processes (MDPs) (Puterman 1994) are the standard model for sequential decision making and planning in the context of non-determinism and uncertainty. In brief, an MDP proceeds as follows: Starting in some state of the modelled system, an agent chooses an action (resolving non-determinism) and the MDP continues to a successor state, sampled from a probability distribution associated with the state-action pair (resolving uncertainty). In practice, this uncertainty is often not known precisely, but rather estimated from data. Robust Markov decision processes (RMDPs) (Nilim and Ghaoui 2005; Iyengar 2005) are an extension of MDPs that lift the assumption of knowing every transition probability exactly. Instead of one precise probability distribution per state-action pair, RMDPs have an uncertainty set consisting of (potentially uncount-ably) many probability distributions. RMDPs have been used in, e.g., healthcare applications (Zhang, Steimle, and Denton 2017). However, existing approaches to solving RMDP suffer from significant drawbacks, e.g. they are limited to very specific objectives or classes of uncertainty sets, or do not provide any guarantees on the correctness of their result. Alleviating all these problems, we present a framework for solving RMDPs that is generic, reliable, and efficient.\nGeneric Uncertainty Sets. In the literature, many variants of uncertainty sets exist: Firstly, polytopic uncertainty sets (Chatterjee et al. 2024) generalize simple interval uncertainty, e.g. (Givan, Leach, and Dean 2000; Tewari and Bartlett 2007), L\u00b9-balls around a given probability distribution, e.g. (Strehl and Littman 2004; Ho, Petrik, and Wiesemann 2018), and contamination models (Wang et al. 2024). Definable uncertainty sets (Grand-Cl\u00e9ment, Petrik, and Vieille 2023) are a recent generalization of polytopic uncertainty. Secondly, non-polytopic (and non-definable) uncertainty sets include the Chi-square (Iyengar 2005), Kullback-Leibler divergence (Nilim and Ghaoui 2005), and Wasserstein distance (Yang 2017) uncertainty sets, and LP-balls around a distribution for 1 < p < \u221e, all of which state-of-the-art methods cannot handle in general. In this paper, we introduce the Constant-Support Assumption, which intuitively requires that the successors of an action are certain, and only the transition probabilities are unknown. It allows us to capture all the listed non-polytopic variants and more. Constant-Support uncertainty sets are incomparable to both polytopic and definable uncertainty sets, i.e. there exists polytopic (and definable) uncertainty sets that do not satisfy the Constant-Support Assumption and vice versa.\nSolution algorithms are always restricted to some particular representation of uncertainty. Considering more general uncertainty sets comes with several complications, e.g. that in some cases optimal policies may cease to exist (see Ex. 1). In this work, we consider a large class of uncertainty sets by investigating ones that are polytopic or that satisfy the Constant-Support Assumption.\nGeneric Objectives. RMDPs mainly have been investigated with discounted or finite-horizon objectives, which put an emphasis on the immediate performance of the system, see e.g. the seminal works (Nilim and Ghaoui 2005; Iyengar 2005) or the recent overview (Wang et al. 2024, Sec. 1.2). While these objectives can be included in our framework, they are not the focus of this paper and are only discussed in App. B. Recently, several works studied RMDPs with"}, {"title": "2 Preliminaries", "content": "A probability distribution over a finite or countable set X is a mapping $d : X \\rightarrow [0,1]$, such that $\\sum_{x \\in X}d(x) = 1$. The set of all probability distributions on X is D(X). We denote the support of a probability distribution p \u2208 D(X) by supp(p) = {x \u2208 X | p(x) > 0}.\nMarkov Decision Process. A (finite-state, discrete-time) Markov decision process (MDP), e.g. (Puterman 1994), is a tuple M = (S, A, P, r), where S is a finite set of states; A is a finite set of actions; P: S \u00d7 A \u2192 D(S) is a (partial) transition function mapping state-action pairs to a distribution over successor states; and r: S \u00d7 A \u2192 N is a reward function mapping state-action pairs to non-negative rewards (see App. A for a reduction from commonly occurring reward functions to natural numbers). We denote by A(s) \u2260 0 the available actions of a state s where P(s, a) is defined.\nThe semantics of MDPs are defined by means of policies which are mappings from finite paths (also called histories) to distributions over actions, formally (S \u00d7 A)* \u00d7 S \u2192 D(A). Intuitively, a path in an MDP initially consists only of some state s, and evolves under a policy \u03c0 by sampling an action a according to the distribution \u03c0(s), receiving the reward r(s, a), and transitioning to the next state s' sampled according to P(s, a). The process continues in this way ad infinitum, always using the whole path as input for the policy (e.g. \u03c0(sas') in the second step). A policy is memoryless if it only depends on the current state and deterministic if it assigns probability 1 to a single action.\nRobust MDP. A robust MDP (RMDP) $M = (S, A, P, r)$ (Nilim and Ghaoui 2005) is a generalization of MDPs, where instead of a fixed distribution the transition function yields an uncertainty set P(s, a). More formally, P: S \u00d7 A \u2192 2D(S), where 2X denotes the set of all subsets of X. We say that an RMDP is closed if P(s, a) is closed for every state-action pair. We employ the classical assumption that uncertainty sets are (s,a)-rectangular as in, e.g. (Nilim and Ghaoui 2005; Chatterjee et al. 2024; Wang et al. 2024), i.e. the uncertainty sets are independent for each state-action pair. Intuitively, there is one additional step in the evolution of an RMDP: Before the successor state is sampled, the environment chooses a distribution from the uncertainty set P(s, a) according to an environment policy \u03c4, i.e. $\u03c4(s_0 a_0 ...s_n a_n) \\in P(s_n, a_n)$. Formally, a pair of policies \u03c0 for the agent and 7 for the environment induces a probability measure $P_M^{\\pi, \\tau}$ over infinite paths in an RMDP M, see App. A for details. We denote by $E_M^{\\pi, \\tau}$, the expectation under this probability measure when using s as initial state.\nObjectives. Objectives define a mapping from infinite paths p = $s_0a_0s_1a_1$... to their payoff. We consider undiscounted total reward (TR) as well as long-run average reward (LRA) (Puterman 1994, Chps. 7 & 8), where\n$TR(p) = \\sum_{t=0}^{\\infty} r(s_t, a_t)$ and\n$LRA(p) = \\liminf_{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{t=0}^{n-1} r(s_t, a_t)$.\n(Further details can be found in App. A, in particular how stochastic shortest path (SSP) is a variant of TR.) Using Payoff \u2208 {TR, LRA}, the value of a state s in an RMDP M under policies (\u03c0, \u03c4) is\n$V_M^{\\pi, \\tau}(s) = E_{M, s}^{\\pi, \\tau}$[Payoff]."}, {"title": "3 Connection between RMDP and SG", "content": "Since the environment in an RMDP acts as an antagonist to the agent, there is a natural correspondence between RMDP and SG, as noted in e.g. (Nilim and Ghaoui 2005). Intuitively, we can add a second player who chooses the instance of the RMDP. This player then chooses one action from the uncertainty set at every state. Thus, we alternate between original MDP state s where the optimizing player chooses action a, and a new antagonistic state sa where the environment selects some probability distribution from the uncertainty set, see, e.g., (Iyengar 2005; Nilim and Ghaoui 2005; Chatterjee et al. 2024). For the formal definition, recall that opt \u2208 {max, min} denotes the optimization direction of the agent's objective and $\\overline{opt}$ is the environment's optimization direction, i.e. the \"inverse\" of opt.\nDefinition 2 (Induced SG for arbitrary RMDP). For an arbitrary RMDP M = (S, A, P,r) with an opt-objective, its induced SG $G_M = (S^G, A^G, P^G, r^G)$ is defined as follows:\n\u2022 $S^G = S_{opt} \\cup S_{\\overline{opt}}$ where\n\u2022 $S_{opt} = S$, and $S_{\\overline{opt}} = \\{s_a | s \\in S, a \\in A(s)\\}$;\n\u2022 for agent states $s \\in S_{opt}$, we have\n$A^G(s) = A(s)$,\n$P^G(s,a) = \\{s_a \\rightarrow 1\\}$ for $a \\in A^G(s)$, i.e. it surely transitions to the newly added environment state, and\n$r^G(s, a) = r(s, a)$ for $a \\in A^G(s)$;\n\u2022 for environment states $s^a \\in S^G$ we have\n$A^G(s_a) = P(s, a)$, i.e. the uncertainty set,\n$P^G (s_a, P) = P$ for $P\\in A^G(s_a)$, and\n$r^G (s_a, P) = r_n(s, a)$ for $P\\in A^G(s_a)$, where $r_n$ is an (objective-dependent) neutral reward.\nIntuitively, $r_n$ is chosen in such a way that removing all opt-states does not affect the Payoff of a path, i.e. for a an infinite path in the SG $p = s_0a_0s_1a_1...$ with $s_0 \\in S_{opt}$ we have Payoff(p) = Payoff($s_0a_0s_2a_2... s_{2k}a_{2k}$...).\nFor TR objectives, the neutral reward is 0 and for LRA objectives, we define $r_n (s_a) = r(s, a)$ for all $s_a \\in S_{\\overline{opt}}$.\nIn general, this reduction results in an infinite-action SG, since $A^G(s_a) = P(s, a)$, and the uncertainty set commonly contains uncountably many distributions. However, for RMDPs with polytopic uncertainty sets, we can utilize the fact that the polytope can be captured by randomizing over its finitely many corner points. That is, each action inside the polytope can be simulated by a probabilistic policy randomly choosing between actions corresponding to corners of the polytope. This allows for a finite representation:\nDefinition 3 (Induced SG for polytopic RMDP). For a polytopic RMDP M = (S, A, P, r) with $C(s,a) = \\{P_1,...,P_{|C(s,a)|}\\} \\subseteq P(s,a)$ denoting the corner points of the polytopic confidence region for P(s, a), its induced SG $G_{poly} = (S^G, A^G, P^G, r^G)$ can be obtained as in in Def. 2, only changing the available actions for environment states $s_a\\in S_{\\overline{opt}}$ as $A^G(s_a) = \\{a_1,..., a_{|C(s,a)|}\\}$, i.e. the corner points of the polytope.\nWe prove the correctness of both reductions for all considered objectives, uncertainty sets, and semantics. Our proof is similar to (Chatterjee et al. 2024, Sec. 3.2); the novelty is the addition of TR objectives and the formalization of the infinite-action reduction, where the latter requires several changes.\nTheorem 1 (Connection to SG \u2013 Proof in App. C). Let M be an arbitrary RMDP and $G_M$ its induced infinite-action SG (Def. 2). Then for all s \u2208 S and any TR or LRA objective $V_M^{opt} (s) = V_{G_M}^{opt} (s)$. Moreover, if M is polytopic and $G'_M$ its induced finite-action SG (Def. 3), $V_M^{opt} (s) = V_{G'_M}^{opt} (s)$.\nImplications of the Polytopic Reduction. Using this observation, we can immediately generalize (Chatterjee et al. 2024, Cor. 1) to undiscounted reward on polytopic RMDP: In finite-action SGs, memoryless deterministic policies are sufficient for optimizing TR objectives in SGs (Bertrand et al. 2023). With Thm. 1, we thus get that there is an optimal memoryless environment policy in RMDPs that is attained at the vertices of the polytope.\nCorollary 1 (Environment Policy Semantics \u2013 Polytopic). In polytopic RMDPs with TR objectives, both agent and environment have deterministic memoryless optimal policies. Thus, stationary and time-varying semantics coincide.\nComplications for Arbitrary Uncertainty Sets. Finite-action SG have many useful properties, e.g. existence of memoryless deterministic optimal policies. For RMDPs with arbitrary uncertainty sets, this is not the case in general (see also (Grand-Cl\u00e9ment, Petrik, and Vieille 2023, Prop. 3.2)):\nExample 1 (Optimal Policy Need Not Exist). Consider the RMDP (in fact, a Robust Markov chain) in Fig. 1. The only action in state $s_{init}$ has reward 0 and uncertainty set given by 0 \u2264 q = p\u00b2. The other states have values V($s_{goal}$) = 1 and V($s_{sink}$) = 0. Then the value of $s_{init}$ is V($s_{init}$) = 0 if p = 0 and 1+ otherwise. This function is discontinuous at p = 0. When the environment is maximizing (i.e. the agent is minimizing costs), there is no optimal environment policy; the supremum over all environment policies is 1, but it cannot be attained. Even restricting to closed convex uncertainty sets is not sufficient: Intuitively, convex combinations can only increase q in relation to p, and thus decrease the value.\nSufficient Assumptions. So far, we have not put any restrictions on the uncertainty sets; Thm. 1 does not even require them to be closed or convex. However, for our approach we require that optimal policies exist for both agent and environment. To this end, we introduce a sufficient assumption,"}, {"title": "4 Implicit Bellman Updates", "content": "We now discuss how to solve RMDPs with value iteration (VI) in an implicit way. This means that we avoid constructing the induced SG explicitly, and the algorithm works directly on the RMDP. The motivation for this is twofold: Firstly, in polytopic RMDPs (given in H-representation), the induced finite SG (and thus any approach solving it explicitly) requires exponential space. Secondly and more importantly, in general the induced SG has an uncountably infinite number of actions, and thus cannot be constructed explicitly at all.\nBellman Updates in SGs. VI centrally relies on the Bell-man update. For example, for SGs with TR objective, we start from a lower bound $L_0$ on the value (e.g. $L_0(s) = 0$ for all states s) and iteratively apply the update\n$L_{i+1}(s) = opt_{a \\in A(s)}(r(s, a) + \\sum_{s' \\in S}P(s, a)(s') \\cdot L_i(s'))$,\nwhere opt = max if s \u2208 Smax and opt = min otherwise (Chen et al. 2013). Intuitively, this performs one step in the SG, back-propagating all rewards. In the limit, this sequence of estimates converges for TR objectives (Chen et al. 2013). Moreover, $L_i$ converges to the LRA value (Kret\u00ednsk\u00fd, Meggendorfer, and Weininger 2023b, Lem. 8).\nBellman Updates in RMDPs \u2013 Robust VI. Observe that in the induced SG, for any action that the agent chooses, the game surely transitions to the environment state corresponding to the chosen state-action pair, and it is the environment's turn to pick the uncertainty set. We can aggregate these two steps to one update in the RMDP by\n$L_{i+1}(s) = opt_{a \\in A(s)}(r(s, a) + opt_{P(s, a) \\in P(s, a)} (\\sum_{s' \\in S} P(s, a)(s') \\cdot L_i(s)))$.\nTheorem 3 (Robust VI convergence \u2013 Proof in App. D). Let M be a polytopic or closed constant-support RMDP. For a TR objective, the sequence Li obtained from Eq. (2) converges to the value in the limit, i.e. for all s \u2208 S it holds that $lim_{i\\rightarrow\\infty} L_i(s) = V_M^{opt} (s)$. Similarly, for an LRA objective, $lim_{i\\rightarrow\\infty} L_i(s) = V_M^{opt} (s)$.\nEq. (2) generalizes robust VI for discounted reward as in, e.g. (Nilim and Ghaoui 2005). Unlike (Wang et al. 2024), we impose no restrictions on the structure of the RMDP. A result similar to this theorem is (Grand-Cl\u00e9ment, Petrik, and Vieille 2023, Thm. 5.2), which shows convergence for RMDPs with \"definable\" uncertainty and LRA objective.\nImplicit Updates. By itself, Eq. (2) is only of theoretical value for now, as P(s, a) might be uncountably infinite. The key to an effective algorithm is the ability to evaluate the inner expression in Eq. (2). This only requires optimizing a linear"}, {"title": "5 Implicit Anytime Value Iteration", "content": "The approach of Sec. 4 converges in the limit (similar to (Grand-Cl\u00e9ment, Petrik, and Vieille 2023; Wang et al. 2024)), however we cannot bound the distance between Li and $V_M^{opt}$ for any concrete i. In other words, we do not know how close we are to the true value at any time and thus cannot give any guarantees upon stopping the VI. This absence of a stopping criterion is explicitly noted as an open question in (Grand-Cl\u00e9ment, Petrik, and Vieille 2023, Sec. 5.2). Even for non-robust systems, efficient stopping criteria were a major challenge. One prominent solution is bounded value iteration (BVI), see e.g. (Kret\u00ednsk\u00fd, Meggendorfer, and Weininger 2023a). The main idea is to compute an additional sequence of upper bounds U\u00bf that over-approximates the value and converges to it in the limit, yielding an anytime algorithm. Our goal is to obtain such an algorithm for RMDP.\nDefinition 4 (Anytime Algorithm with Stopping Criterion). An anytime algorithm (with stopping criterion) for RMDPs maintains two sequences Li, Ui such that for all states s (i) for every iteration i \u2208 N, $L_i(s) < V_M^{opt} (s) \\leq U_i(s)$, and (ii) $lim_{i\\rightarrow\\infty} U_i(s) \u2013 L_i(s) = 0$.\nIntuitively, an anytime algorithm is correct at every step and guarantees a precision of $U_i(s) \u2013 L_i(s)$; moreover, eventually the algorithm terminates for every precision \u025b > 0.\nUsing Thm. 1, we can obtain an explicit anytime algorithm for polytopic RMDPs, namely by constructing the induced finite-action SG and applying the algorithms of (Kret\u00ednsk\u00fd, Meggendorfer, and Weininger 2023a).\nKey Contribution. To obtain an algorithm that is efficient and applicable for arbitrary uncertainty sets, we now propose an implicit anytime algorithm. For ease of presentation, the descriptions in this section focus on closed constant-support RMDPS with TR objective. We later provide an intuition how to extend the results to LRA objectives and non-constant support RMDPs (details are provided in Apps. E and F).\nChallenges. Obtaining converging upper bounds is not as simple as for lower bounds. In particular, just applying Bell-man updates to upper bounds does not necessarily converge."}, {"title": "7 Conclusion", "content": "We have generalized the connection between RMDPs and SGs to include arbitrary uncertainty sets and total reward objectives, we have shown that and how Bellman updates can be performed implicitly and efficiently, and we have provided anytime algorithms with stopping criteria. Together, we have presented a framework for solving RMDPs that is generic, reliable and efficient. In the future, we aim to to investigate what form solutions can take when lifting the Constant-Support Assumption."}]}