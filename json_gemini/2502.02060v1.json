{"title": "CH-MARL: CONSTRAINED HIERARCHICAL MULTIAGENT\nREINFORCEMENT LEARNING FOR SUSTAINABLE MARITIME\nLOGISTICS", "authors": ["Saad Alqithami"], "abstract": "Constrained multiagent reinforcement learning (MARL) offers a powerful paradigm for coordinating\ndecisions among autonomous agents, yet few approaches address the simultaneous need for global\nemissions control, partial observability, and fairness in dynamic industrial settings. In this paper, we\npropose a novel Constrained Hierarchical MARL (CH-MARL) framework for optimizing energy effi-\nciency and reducing greenhouse gas emissions within maritime logistics. Our method formulates the\nenvironment as a partially observable, non-stationary system in which vessel agents, port agents, and\nregulatory agents cooperate under global environmental caps. Specifically, we extend conventional\npolicy-gradient techniques by introducing a real-time constraint-enforcement layer that dynamically\nadjusts the agents' feasible action space and shared reward signals to ensure overall compliance\nwith environmental targets. To handle the inherent complexity of maritime operations, we adopt a\nhierarchical approach: high-level agents learn strategic decisions such as route planning and emission\nbudgeting, while lower-level agents fine-tune local actions (e.g., speed control, berth scheduling). We\nfurther embed a fairness-aware objective to balance resource allocation among vessels of different\nsizes and capacities, preventing disproportionate costs for smaller stakeholders. Experimental evalua-\ntions on a digital-twin testbed of multiple shipping lanes and variable port conditions demonstrate\nthat CH-MARL outperforms baseline methods by significantly reducing total emissions and fuel\nconsumption without compromising operational throughput. Moreover, the real-time constraint layer\nconsistently maintains global emissions below specified limits, and fairness metrics indicate minimal\ndisparities among agents in fuel cost and delay. Our findings highlight the scalability and adaptability\nof CH-MARL for sustainability-driven maritime logistics and pave the way for broader applications\nin other constrained, multi-objective industrial domains.", "sections": [{"title": "1 Introduction", "content": "The advent of globalized trade has led to unprecedented growth in the volume and complexity of maritime logistics. As\none of the most cost-effective modes of transportation, maritime shipping has become indispensable for connecting\neconomies and supporting international trade. However, this growth comes with substantial environmental and\noperational challenges. The sector's heavy reliance on fossil fuels contributes significantly to global greenhouse\ngas (GHG) emissions, accounting for nearly 2.89% of global emissions Smith et al. [2014], [IMO]. Moreover, the\nInternational Maritime Organization (IMO) has outlined a strategy to reduce GHG emissions from international shipping\nby at least 50% by 2050 compared to 2008 levels, aiming for eventual decarbonization [IMO]. These ambitious targets\nunderscore the pressing need for transformative solutions to meet regulatory requirements and societal expectations.\nEnvironmental pressures are further compounded by the intricate logistics of coordinating diverse stakeholders, including\nshipping companies, port authorities, and policymakers, each with unique objectives and constraints. Maritime shipping"}, {"title": "1.1 Motivation", "content": "Maritime logistics underpins around 80% of global trade, highlighting its role as the cornerstone of international\ncommerce [IMO]. Rising trade volumes coincide with growing urgency to reduce the sector's environmental impact,\nmost notably by curbing GHG emissions [IMO]. Regulatory bodies, alongside greater societal awareness of climate\nchange, have amplified demands for cleaner shipping and sustainable maritime operations.\nHence, stakeholders across the maritime sector, from international shipping conglomerates to smaller shipping lines,\nare compelled to adopt more sustainable practices. Optimizing energy efficiency, reducing emissions, and maintaining\noperational throughput simultaneously is challenging in a competitive, dynamic environment. These complexities\nmotivate the development of novel algorithms and frameworks that harness MARL to address environmental imperatives\nwithout compromising economic viability."}, {"title": "1.2 Problem Statement", "content": "Achieving sustainability in maritime logistics involves optimizing multiple objectives concurrently. Stakeholders must\nensure adherence to global emission caps while preserving efficiency and upholding fairness in resource allocation.\nSuch objectives become more complicated in real-world maritime environments characterized by partial observability,\nnon-stationarity, and the interactions of numerous autonomous agents Hernandez-Leal et al. [2019], Foerster et al.\n[2016].\nExisting approaches tend to address particular aspects of these intricacies. They may focus on localized optimization or\npartial constraint enforcement, lacking the comprehensive, integrated mechanisms needed to balance global constraints\nand fairness considerations effectively. As a result, there is a pressing need for frameworks that holistically incorporate\nconstraint satisfaction, equitable resource distribution, and adaptive multi-agent coordination."}, {"title": "1.3 Contributions", "content": "This paper presents a Constrained Hierarchical Multiagent Reinforcement Learning (CH-MARL) framework that\naddresses these multifaceted requirements within sustainable maritime logistics. Our main contributions are:\n1. Dynamic constraint-enforcement layer: We propose real-time mechanisms to ensure compliance with global\nemission caps, thereby reinforcing sustainability goals throughout the operational horizon.\n2. Fairness-aware reward shaping: We integrate fairness metrics into the reward function, promoting equitable\ncost and resource sharing among multiple stakeholders.\n3. Validation in a maritime digital twin: We demonstrate significant enhancements in efficiency and emissions\nreduction within a digital twin environment for maritime logistics, highlighting the practicality and effectiveness\nof our proposed framework.\nBy addressing operational efficiency, environmental sustainability, and stakeholder equity, CH-MARL extends the\ncapabilities of existing MARL models and paves the way for robust, scalable solutions in real-world maritime settings."}, {"title": "1.4 Paper Organization", "content": "The remainder of this paper is organized as follows: Section 2 provides a detailed review of the background and related\nwork, highlighting advancements in Multiagent Reinforcement Learning (MARL), Constrained Reinforcement Learning\n(CRL), and their applications in maritime logistics. Section 4 presents the proposed CH-MARL framework, detailing\nits architecture, methodology, and key components. Section 6 outlines the experimental setup, including the digital twin\nenvironment and evaluation metrics. Results and discussions are presented in Section 6, followed by conclusions and\nfuture work in Section 7."}, {"title": "2 Background and Related Work", "content": "The rapid advancements in artificial intelligence have paved the way for the development of multiagent systems, where\nmultiple autonomous entities interact within shared environments to achieve individual or collective goals. Multiagent\nReinforcement Learning (MARL) has emerged as a critical area of research within AI, addressing the unique challenges\nposed by these systems. These challenges include coordinating behaviors in dynamic and uncertain environments,\nmanaging conflicts between agents with competing objectives, and ensuring the scalability of solutions as the number\nof agents increases. MARL is particularly relevant in domains where complex decision-making and adaptability are\nrequired, such as robotics, logistics, and industrial automation.\nIn parallel, there has been growing interest in Constrained Reinforcement Learning (CRL), which extends traditional\nreinforcement learning to incorporate safety and performance constraints. This paradigm is crucial for ensuring that\nAI systems operate within acceptable boundaries, especially in high-stakes applications such as autonomous driving,\nhealthcare, and energy management. Integrating constraints into MARL adds another layer of complexity, requiring\ninnovative approaches to balance local and global requirements across multiple agents.\nThe maritime and logistics industries stand out as prominent domains where MARL and CRL have significant potential.\nAs global trade and transportation continue to grow, optimizing operations in these sectors has become increasingly\ncritical for reducing costs, improving efficiency, and minimizing environmental impact. However, the dynamic and\ninterconnected nature of maritime logistics presents unique challenges, including the need for hierarchical decision-\nmaking and adherence to international regulations on emissions and sustainability.\nFairness in multiagent systems is another critical aspect that has garnered attention, particularly in scenarios where\nresource allocation or policy decisions affect diverse stakeholders. Ensuring equitable outcomes is essential for\nfostering cooperation and maintaining trust among participants. This is especially relevant in industrial contexts, where\nimbalanced policies can disadvantage smaller players, leading to inefficiencies and conflicts.\nBy exploring the intersections of MARL, CRL, maritime logistics, and fairness, this work aims to address key research\ngaps and advance the state of the art in these fields. The following sections provide a comprehensive review of existing\nliterature and identify opportunities for future research."}, {"title": "2.1 Multiagent Reinforcement Learning", "content": "Multiagent Reinforcement Learning (MARL) has gained increasing prominence as a method to coordinate autonomous\nagents in complex, dynamic environments. MARL settings can be broadly categorized into cooperative and competitive\ndomains. In cooperative scenarios, multiple agents aim to maximize a shared objective, exemplified by tasks such as\ncooperative robotics or emergency response Panait and Luke [2005], Foerster et al. [2018]. In contrast, competitive\nMARL addresses settings where agents have conflicting or opposing goals, including zero-sum games and competitive\nmarket models Lowe et al. [2017], Silver et al. [2017].\nOne core challenge in MARL involves scalability: as the number of agents increases, the growth of state and action\nspaces becomes exponential, making learning computationally intractable Hernandez-Leal et al. [2019]. Techniques\nsuch as decentralized training with centralized execution (DTCE) Oliehoek et al. [2008] and parameter sharing Gupta\net al. [2017] have partially mitigated this issue, although these methods often assume relatively homogeneous agent types.\nAdditionally, agents typically operate under partial observability, having access only to localized information Foerster\net al. [2016], which can impede learning. Recent approaches that incorporate recurrent architectures Hausknecht and\nStone [2015] and communication protocols Sukhbaatar et al. [2016] show promise, but also increase computational\noverhead. Finally, non-stationarity arises when multiple agents learn in parallel, making convergence to stable policies\nor equilibria more challenging Zhang et al. [2021]. Mechanisms such as opponent modeling Albrecht and Stone [2018]\nand equilibrium learning Shou et al. [2022] are active research areas aimed at alleviating these challenges."}, {"title": "2.2 Constrained Reinforcement Learning", "content": "Constrained Reinforcement Learning (CRL) introduces explicit constraints often reflecting safety or performance\nthresholds-into the learning process. Early work employed Lagrangian relaxation Altman [1999], translating con-\nstrained optimization into unconstrained forms through penalty functions. More recent research on Constrained Policy\nOptimization (CPO) Achiam et al. [2017] uses trust-region methods to ensure constraints remain satisfied, offering\ntheoretical performance guarantees at the cost of higher computational demands.\nAlternative methods, such as primal-dual optimization and constraint sampling, propose additional ways of managing\nconstraints. However, these techniques tend to struggle with scalability in multiagent contexts. When applied to MARL,\nthe necessity of balancing both individual-agent constraints and system-wide limitations complicates policy design."}, {"title": "2.3 MARL for Maritime & Logistics", "content": "The maritime sector has begun exploring MARL to optimize factors such as vessel routing, port operations, and\nscheduling. For instance, vessel routing studies show MARL can simultaneously reduce fuel consumption and improve\nscheduling efficiency. Meanwhile, research on port operations applies MARL to berth allocation, crane dispatch, and\ncontainer handling.\nDespite notable achievements, existing approaches rarely incorporate global emission caps enforced by international\nregulations like those of the IMO [IMO]. Fairness considerations are also overlooked, potentially disadvantaging smaller\nshipping lines. While hierarchical coordination can help manage decisions across different levels (e.g., vessel-level,\nfleet-level), such methods remain relatively underexplored in the maritime domain. These gaps highlight the potential\nfor a more robust and integrated MARL framework that can reconcile emission constraints and fairness goals with\noverarching operational objectives."}, {"title": "2.4 Fairness in Multiagent Systems", "content": "Fairness is crucial in multiagent resource-allocation problems, as policy decisions may disproportionately affect certain\nstakeholders. Foundational concepts like max-min fairness, envy-freeness, and Gini coefficients offer theoretical bases\nfor diagnosing and mitigating inequality Nash [1950], Lipton et al. [2004]. In practice, such metrics have been employed\nin traffic management Aloor et al. [2024] and cloud computing, among other areas.\nIn industrial logistics, smaller firms risk systematic disadvantage when competition arises for shared resources,e.g.,\ndocking berths or emission budgets. Ensuring equitable outcomes is not only a moral imperative but also fosters\ncooperation, enhancing the real-world viability of multiagent solutions. Nonetheless, fair allocation is challenging to\nimplement in MARL, where efficiency and equity often compete Rawls [1971]. More research is needed to integrate\nfairness constraints effectively in systems that also target high performance.\nThe literature indicates a strong need for hierarchical methods that incorporate both global constraints and fairness into\nMARL, especially in sustainable maritime logistics. Existing work offers only partial solutions, typically addressing\nisolated aspects like constrained optimization or static fairness scenarios. By bridging these gaps, future MARL\nframeworks can more effectively guide sustainable decision-making and policy enforcement in real-world maritime\ncontexts."}, {"title": "3 Problem Formulation", "content": "In this section, we define the maritime logistics problem targeted by our hierarchical multi-agent framework. We first\ndescribe the overall system, including ports, vessels, and dynamic factors (Sec. 3.1), followed by an explanation of agent\nroles in a hierarchical setting (Sec. 3.2). We then detail the formal state, action, and reward structures (Sec. 3.3), illustrate\nthe global constraints that must be respected (Sec. 3.4), and motivate the fairness objectives that ensure equitable\nresource distribution among agents (Sec. 3.5). Finally, we summarize the domain assumptions and simplifications\nadopted for tractability (Sec. 3.6)."}, {"title": "3.1 System Description", "content": "Maritime logistics networks are composed of multiple ports and vessels transporting goods along specified routes.\nEach port provides critical services\u2014such as berths, cranes, and cargo-handling facilities\u2014to the vessels arriving at\nscheduled or unscheduled intervals. In our setting, we assume a network G = {P,R}, where P is the set of ports and\nR is the set of possible sea routes between these ports.\nA typical planning or operational horizon (e.g., daily or weekly) involves multiple vessels traveling between ports under\ndynamically changing factors:\n\u2022 Weather conditions: Storms, wind, and waves can affect fuel consumption and vessel speed.\n\u2022 Port congestion: High traffic at a port may lead to queuing delays and increased emissions due to idling.\n\u2022 Route variations: Different sea lanes or alternative channels can exist, each with distinct distance, fuel cost,\nand traffic patterns.\n\u2022 Mechanical uncertainties: Vessels can experience engine wear, maintenance needs, or unexpected failures\nthat alter operational performance."}, {"title": "3.2 Agents and Hierarchical Roles", "content": "We model the network using a set of autonomous, decision-making agents. Following a hierarchical paradigm, these\nagents are divided into two primary layers:\nHigh-Level Agents (Strategic Layer). These agents operate at a coarser timescale (e.g., hours or days) and have a\nbroader view of the logistics network. Their responsibilities include:\n\u2022 Route Planning: Selecting high-level paths through the port network for each vessel, considering distance,\ncongestion, and overall fleet coordination.\n\u2022 Emission Budgeting: Allocating emission allowances across multiple vessels or routes based on predicted\nfuel consumption and regulatory thresholds.\n\u2022 Scheduling Coordination: Coordinating vessel departure/arrival times at ports to minimize queueing and\nturnaround delays.\nLow-Level Agents (Operational Layer). At finer granularity (e.g., every few minutes or seconds), low-level agents\nmake local decisions consistent with the strategic directives. Examples include:\n\u2022 Speed Control: Dynamically adjusting vessel speed to meet emission targets, fuel constraints, or arrival\ndeadlines.\n\u2022 Berthing and Cargo Handling: Managing docking assignments and crane usage based on immediate port\nconditions, workforce availability, and mechanical constraints.\n\u2022 On-Board Resource Management: Monitoring engine performance, fuel flow rates, and battery/hybrid\nsystems (if available).\nThis division of roles allows each layer to focus on decision-making at its own operational scope, thereby increasing\nscalability and adaptability in large-scale maritime scenarios."}, {"title": "3.3 State, Action, and Reward Structures", "content": "State Space. Let st \u2208 S denote the global state of the maritime system at time t, which may include:\n\u2022 Vessel states: Locations, velocities, fuel levels, mechanical health indicators.\n\u2022 Port states: Queue lengths, berth occupancy, crane availability, operational schedules.\n\u2022 Environmental data: Partial or noisy weather forecasts, wave heights, visibility.\n\u2022 Historical usage: Aggregated emissions or fuel usage up to time t, to track proximity to global caps.\nIn practice, each agent observes only a subset of the global state (i.e., of \u2286 st), reflecting partial observability.\nAction Space. Each agent i selects an action a from its individual action space A\u2081. Examples include:\n\u2022 High-Level Actions: Assign route rk to vessel vj, issue emission allowance Bj for a time window At.\n\u2022 Low-Level Actions: Increase or decrease vessel speed, request a specific berth at port pm, dispatch cargo-\nhandling equipment.\nThe hierarchical structure means that high-level actions can constrain the choices available to low-level agents (e.g.,\nlimiting feasible speed ranges or specifying which port to approach next).\nReward Structure. We define a weighted or composite reward to balance multiple objectives:\n$r_i = R_{cost}(a_i) + R_{emission}(a_i) + R_{fair}(a_i)$.\nConcretely, Rcost might be negatively proportional to fuel consumption or berth fees; Remission rewards strategies that\nlower CO2 outputs; and Rfair enforces equity constraints (see Sec. 3.5). Depending on the global or local nature of each\nobjective, these terms may be shared or agent-specific."}, {"title": "3.4 Global Constraints", "content": "Aside from local reward optimization, maritime operators must comply with strict global constraints, such as:\n\u2022 Emission Caps: A regulatory or self-imposed limit on total CO2 or other pollutants (e.g., NOx) for each route,\nport area, or entire shipping alliance over a fixed horizon.\n\u2022 Port Resource Capacity: Physical constraints on the number of vessels that can be serviced at a port\nsimultaneously (limited berths, crane scheduling, labor availability).\nIn a multi-agent context, these constraints are shared by all agents; thus, a single vessel's high fuel usage or a congested\nport schedule can push the system over its limit. Our framework addresses these concerns via primal-dual penalty\nmechanisms or other constraint-enforcement algorithms, ensuring real-time tracking of resource usage and immediate\npenalization for exceeding thresholds."}, {"title": "3.5 Fairness Objectives", "content": "Due to the heterogeneous nature of vessels and shipping lines, fairness emerges as a critical objective. Without explicit\nfairness considerations, certain agents (e.g., smaller operators) may routinely end up with unfavorable slots or higher\noperational costs, undermining cooperation and real-world adoption. We thus define a fairness metric, F({ci}), where\nci represents the cumulative cost or burden accrued by agent i. Possible metrics include:\n\u2022 Gini coefficient: Measures inequality in the distribution of ci values.\n\u2022 Max-min fairness: Minimizes the maximum deviation between agents' costs.\n\u2022 Envy-freeness: Ensures no agent would prefer another agent's allocation of resources to its own.\nWe incorporate F into agent rewards or treat it as an auxiliary constraint to ensure that operational burdens and benefits\nare equitably distributed across the fleet."}, {"title": "3.6 Assumptions and Simplifications", "content": "To manage complexity, we adopt the following assumptions and simplifications:\n\u2022 Discrete Time Steps: Both high-level and low-level decisions occur at discrete intervals (e.g., 15-minute or\nhourly increments), approximating continuous processes.\n\u2022 Simplified Weather Model: While weather is stochastic, we assume a finite set of forecast scenarios (e.g.,\ncalm, moderate, storm). Agents receive partial updates regarding the current scenario's likelihood at each step.\n\u2022 Limited Mechanical Failures: We consider mechanical uncertainties (engine wear, malfunctions) in a\nsimplified manner, modeling them as random events with fixed probability distributions.\n\u2022 Agent Collaboration vs. Competition: We focus primarily on cooperative or semi-cooperative settings,\nacknowledging that in real-world shipping alliances, competition can also shape decision incentives. If needed,\ncompetition can be introduced by adjusting reward functions or agent objectives.\nFor tractability, we adopt a discrete-time model with a finite set of weather scenarios, each occurring with known\nprobability. Mechanical failures are similarly modeled as random events with specified likelihoods. We primarily\nconsider cooperative or semi-cooperative scenarios where agents aim to achieve shared sustainability and operational\ngoals, although competitive dynamics can be introduced by altering reward structures. Under these assumptions, the\nproblem becomes a constrained hierarchical MARL setting: agents in each layer learn policies \u03c0\u00b2 that optimize local\nrewards while adhering to global constraints and fairness criteria, thereby aiming to achieve robust, equitable, and\nenvironmentally sound maritime logistics.\nBy formally describing the maritime environment, agents, actions, rewards, constraints, and fairness goals, we establish\na clear foundation for the Methodology (Sec. 4). In particular, the state-action-reward definitions guide how each\nhierarchical agent learns policies, while the global constraints and fairness objectives inform our choice of constrained\nand fairness-aware reinforcement learning algorithms."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Theoretical Foundations", "content": "Reinforcement learning (RL) is a paradigm in which an agent interacts with an environment to maximize long-term\nrewards. In MultiAgent RL (MARL), multiple agents concurrently learn and act within a shared environment. A\nstandard model for an N-agent system is (S, {Ai}i=1N, P, {Ri}i=1N, \u03b3), where S is the state space, Ai is agent i's action\nset, P(. s, a) specifies the transition dynamics, Ri is the reward function for agent i, and 0 < \u03b3 \u2264 1 is the discount\nfactor."}, {"title": "4.2 CH-MARL Framework", "content": "We now synthesize the above elements into Constrained Hierarchical Multi-Agent Reinforcement Learning, illustrated\nin Figure 1. High-level (strategic) agents operate on extended timescales to make macro-decisions, while low-level\n(operational) agents refine these directives at fine-grained intervals. A primal-dual constraint layer enforces global limits\n(e.g., emission caps), and a fairness module adjusts the reward signals to prevent excessive burdens on any single agent."}, {"title": "4.2.1 Hierarchical Agent Architecture:", "content": "High-level agents address route planning, emission allocations, and schedule coordination, informed by partial yet\nbroader observations (e.g., aggregated port loads or regulatory updates). Low-level agents then execute localized\ndecisions, such as speed adjustments or berth assignments, in alignment with high-level directives. By offloading\noperational details to low-level agents, the overall system reduces complexity and accelerates learning in large-scale\ndomains."}, {"title": "4.2.2 Primal-Dual Constraint Enforcement:", "content": "A global emission budget Cmax enforces sustainable operations. At each step, cumulative emissions are measured,\nand any overshoot updates the Lagrange multiplier \u03bb. All agents experience a penalty proportional to \u03bb and their\nincremental emissions, driving them toward compliance. This mechanism efficiently handles collective constraints\nunder partial observability, since agents learn to avoid globally penalized actions."}, {"title": "4.2.3 Fairness-Aware Reward Shaping:", "content": "In addition to meeting sustainability goals, maritime systems must ensure that smaller or less-resourced stakeholders are\nnot systematically disadvantaged. We incorporate a fairness function F({ci}), where ci denotes agent i's cumulative\nburden (e.g., fuel spent or waiting time). A fairness term Afair is added to or subtracted from each agent's reward to\nencourage parity, thereby balancing efficiency with equitable outcomes."}, {"title": "4.2.4 Integrated Workflow of CH-MARL:", "content": "The full CH-MARL process proceeds in the following loop: 1. High-Level Action Selection: Each strategic agent\nsamples a macro-action (e.g., route choice, emission allocation) at a coarse timescale. 2. Low-Level Refinement:\nOperational agents refine these high-level directives into granular actions (e.g., throttle settings, berth scheduling)\nusing local observations. 3. Environment Step: The environment transitions to a new state and returns partial rewards,\nemission data, and fairness-relevant statistics. 4. Constraint & Fairness Updates: A primal-dual module updates the\nLagrange multiplier \u03bb if emissions exceed Cmax, and a fairness module adjusts rewards based on the observed resource\ndistribution. 5. Policy Updates: Each agent-both strategic and operational uses an RL algorithm (e.g., actor-critic\nor Q-learning) to update its policy according to the adjusted reward signals. Algorithm 1 provides an overview of the\nCH-MARL training loop. It illustrates how high-level (strategic) and low-level (operational) agents make decisions,\nhow primal-dual updates enforce global emission constraints, and how fairness terms adjust individual rewards. After\nsufficient training episodes, the hierarchical policies converge to strategies that simultaneously respect global emission\nconstraints, maintain operational efficiency, and promote fair outcomes among a diverse set of agents."}, {"title": "4.3 Complexity Analysis and Practical Scalability", "content": "Time Complexity The hierarchical design partitions policy learning into a strategic layer and an operational layer.\nFor each episode of length T, high-level policy updates scale with O(T|AH||S|), while the n low-level policies\nadd O(nT|AL|). Thus, total cost is O(T|AH||S| + nT|AL|). Practical frameworks reduce these costs via state\nabstraction, concurrent policy updates, or approximate solvers.\nSpace Complexity: In addition to storing policies (potentially neural networks) for both hierarchical layers, replay\nbuffers or model-based structures require memory proportional to the size of (S, A). For CMDPs, transition tables can\nadd complexity up to O(|S|2 |AH|).\nScalability: CH-MARL handles moderate-scale domains effectively, yet large state-action spaces require tech-\nniques, e.g., aggregating similar states to reduce dimensionality, simultaneously update multiple low-level policies\non multi-core/distributed architectures, and pruning action spaces or using deep neural function approximators for\nhigh-dimensional inputs. These strategies ensure it remains tractable and responsive in complex and dynamic settings.\nCH-MARL harnesses hierarchical RL to divide global tasks and partial observability among multiple agents, employs\nprimal-dual updates to enforce dynamic constraints such as emission caps, and embeds fairness objectives to protect\nsmaller or more vulnerable stakeholders. The synergy of these components offers a scalable pathway toward optimizing\nlarge, industrial-grade systems that must balance efficiency, sustainability, and equity."}, {"title": "5 Experimental Setup", "content": ""}, {"title": "5.1 Digital Twin Environment", "content": "We develop a digital twin that closely emulates real-world maritime logistics operations on a smaller, controlled scale.\nThe simulation environment implements four primary components: a) Shipping Routes: The network is modeled\nas directed edges connecting ports (nodes), each with an associated distance, typical journey duration, and potential\nweather disruptions. b) Vessel Movements: Each vessel agent operates at discrete time intervals (e.g., hourly), updating\nits position and fuel usage. Agents can alter speeds or perform minor route deviations within prescribed bounds,\nmirroring practical operational constraints. c) Port Operations: Ports have finite berth capacities, modeled by limiting\nthe number of vessels that can be simultaneously served. If more vessels arrive at a port than available berths, queues\nform, thereby increasing waiting times and idle fuel consumption. Each vessel accumulates queue time (in hours)\nwhenever it is forced to wait. d) Weather Stochasticity: Wind speed, storms, and other meteorological factors are\nsampled from historical or synthetic distributions. These conditions impact vessel navigation (e.g., speed reductions)\nand fuel burn rates, introducing real-world uncertainty into the simulation.\nFor fidelity, the digital twin incorporates empirical data on port statistics (e.g., berth occupancy, crane throughput),\nvessel characteristics (e.g., hull design, engine power, fuel-consumption curves), and weather records (e.g., wind speed,\nwave height). These elements collectively provide a realistic yet tractable environment for evaluating multi-agent\npolicies."}, {"title": "5.2 Key Performance Indicators (KPIs)", "content": "We track the following KPIs to evaluate economic, environmental, and fairness dimensions: a) Energy Consumption:\nThe total fuel usage aggregated over all vessels. Lower consumption indicates improved efficiency and reduced\noperational costs. b) Total Emissions: Represents greenhouse gas outputs (e.g., CO2, NOx, SOx) in tons, capturing\ndirect environmental impacts of decisions like route selection and speed control. c) Fairness Indices: Metrics such as\nthe Gini coefficient gauge how equitably resources or costs (e.g., fuel usage) are distributed among agents. A lower Gini\ncoefficient indicates a more balanced distribution. d) Operational Throughput: The total number of voyages completed\nor total cargo tonnage moved, reflecting overall system productivity within a simulation horizon. e) Constraint Violation\nRates: Monitors how frequently and by how much global constraints (e.g., emission caps, port capacity limits) are"}, {"title": "5.3 Comparative Baselines", "content": "To evaluate the effectiveness of CH-MARL, we compare its performance against three baselines: (a) A decentralized,\nmulti-agent RL setup without explicit emission caps or fairness mechanisms. This baseline typically optimizes\nthroughput or other local objectives but risks exceeding emission targets and privileging larger stakeholders. (b) A\nsimplified approach that aggregates the entire system into one \"super agent\" responsible for adhering to emission\nlimits. While this can handle small problems, it scales poorly and disregards the autonomy of individual vessels. (c)\nA hierarchical method that omits global emission caps and fairness shaping. By comparing against this baseline, we\nisolate whether improved performance stems primarily from hierarchical decomposition or from explicitly incorporating\nenvironmental and equity considerations."}, {"title": "5.4 Training and Hyperparameters", "content": "Each training run consists of N episodes, each lasting T discrete time steps. For instance, one episode might simulate a\nweek of maritime operations with hourly decision points, yielding T = 168. We primarily use actor-critic or policy\ngradient algorithms with a learning rate \u03b1 \u2208 [10\u22124, 10-3]. In our experiments, the Adam optimizer is employed for\nboth actor and critic networks, with potential hyperparameters such as \u03b1actor = 5 \u00d7 10\u22124 and \u03b1critic = 1 \u00d7 10\u22123. We set\nthe discount factor \u03b3 = 0.99 to weight long-term returns. Exploration is maintained through either stochastic policies\n(with an entropy bonus) or an e-greedy approach. Typical schedules might start at  \u03f5 = 0.2 and decay linearly to 0.01\nover the first 500 episodes.\nWhen applying emission caps through primal-dual updates, a dual variable \u03bb is incrementally adjusted based on\nconstraint violations. The corresponding learning rate \u03b1\u03bb usually lies between 0.001 and 0.01. To encourage equitable\nallocations, each agent's reward is penalized by a fairness term (e.g., a scaled Gini coefficient), whose weight Yi can\nbe tuned (default 0.1). Ablation studies are conducted to examine how different fairness weights affect the trade-off\nbetween equality and efficiency."}, {"title": "6 Experiments and Results", "content": "We evaluate the proposed CH-MARL framework in a synthetic maritime environment featuring 8 ports (each with\nlimited berth capacity) and 5 vessels. Each vessel is assigned a speed profile and cubic fuel-consumption curve typical\nof maritime operations. We train a single proximal policy optimization agent Schulman et al. [2017] for up to 1200\nepisodes, with each episode spanning T = 50 steps, totaling 60,000 time steps. Across multiple runs, we activate\nor deactivate the following key features: a) Emission Caps: A global emission threshold Cmax is enforced. Once the\nsystem's cumulative emissions exceed Cmax, additional penalties prompt agents to conserve fuel. b) Fairness: An\noffline penalty is computed based on the disparity in fuel usage among vessels, nudging agents to adopt more equitable\ncost distributions. c) Partial Observability: Half of the state variables are randomly masked at each step, capturing\nrealistic uncertainty in sensor data or communications. d) Storms: Each time step, a 20% chance of adverse weather\nreduces vessel speeds and inflates fuel consumption to simulate real-world disruptions.\nWe illustrate the training dynamics for four main configurations: a) Run A (Base): No emission cap, no fairness, no\nstorms, and full observability. b) Run B (Cap): Employs an emission cap Cmax = 800, but omits fairness and storms.\nc) Run C (Fair+Storms): Excludes an emission cap but enables fairness penalties, partial observability, and storms. d)\nRun D (Cap+Fair+Storms): Activates all features: Cmax = 800, fairness penalties, storms, and full observability.\nFigures 2 and 3 show the evolution of average reward and cumulative emissions under each configuration to highlight\nhow each feature influences policy convergence. The addition of emission caps (Run B, Run D) consistently pulls\nemissions downward, whereas fairness and storms (Run C, Run D) can amplify stochasticity and reduce final rewards.\nTable 1 lists each run's mean results at the last training iteration. Run A achieves a relatively high (less negative) reward\nbut also incurs higher emissions. Adding an emission cap in Run B effectively keeps emissions near 4.73, albeit at the\ncost of a slightly lower reward. Meanwhile, Run C (Fair+Storms) imposes a strong fairness penalty and faces storm\ndisruptions, causing the reward to drop to -11.40 while emissions remain moderate at 4.09. Combining all features in\nRun D leads to the lowest emissions (4.07) with a mid-range reward of \u201311.37, reflecting the interplay among global\nconstraints, partial-equity objectives, and environmental disturbances."}, {"title": "7 Conclusion and Future Work", "content": "In this work, we introduced the Constrained Hierarchical Multiagent Reinforcement Learning (CH-MARL) framework,\na novel approach to addressing the multifaceted challenges of sustainable maritime logistics. By integrating dynamic\nconstraint enforcement, fairness-aware reward mechanisms, and hierarchical decision-making, CH-MARL effectively\nbalances emissions control, operational efficiency, and fairness among stakeholders. The framework was validated using\na digital twin for maritime logistics, demonstrating significant improvements in energy efficiency and greenhouse gas\nemission reductions while promoting equitable resource allocation. These findings underscore CH-MARL's potential\nto align economic objectives with environmental sustainability and fairness, making it a promising tool for fostering\ninnovation in maritime and other industrial operations.\nThe hierarchical structure of CH-MARL enables"}]}