{"title": "The Artificial Intelligence Act: critical overview", "authors": ["Nuno Sousa e Silva"], "abstract": "This article provides a critical overview of the recently approved Artificial Intelligence Act. It starts by presenting the main structure, objectives, and approach of Regulation (EU) 2024/1689. A definition of key concepts follows, and then the material and territorial scope, as well as the timing of application, are analyzed. Although the Regulation does not explicitly set out principles, the main ideas of fairness, accountability, transparency, and equity in Al underly a set of rules of the regulation. This is discussed before looking at the ill-defined set of forbidden Al practices (manipulation and e exploitation of vulnerabilities, social scoring, biometric identification and classification, and predictive policing). It is highlighted that those rules deal with behaviors rather than Al systems. The qualification and regulation of high-risk Al systems are tackled, alongside the obligation of transparency for certain systems, the regulation of general-purpose models, and the rules on certification, supervision, and sanctions. The text concludes that even if the overall framework can be deemed adequate and balanced, the approach is so complex that it risks defeating its own purpose of promoting responsible innovation within the European Union and beyond its borders.", "sections": [{"title": "Introduction", "content": "The rapid technological evolution of recent decades \u2013 generating a vast collection of digitized and accessible information (made possible by the Internet) and advances in terms of hardware and software \u2013 has allowed certain mathematical techniques (so-called machine learning) to become revolutionary. This is at the root of the dizzying developments in Artificial Intelligence that have taken place in the last few years.\nHowever, despite the numerous advantages that this development brings, the catastrophist tone has gained prominence.\nIn the second decade of the 21st century, safety in Artificial Intelligence (hereinafter \"AI\") has established itself as an interdisciplinary branch of study, going beyond ethical considerations. There are discussions regarding the transparency and explainability of decisions made by Al systems, the potential for discrimination or injustice in the use of these systems, and the challenges to control and align Al systems with human values. There is a pressing need to guarantee the robustness and technical quality of Al. The extractive practices of both data (some of it protected by intellectual property rights) and minerals and the energy consumption of Al are also a matter of concern.\nIn recent years, lawyers and politicians have started to consider laws to deal with the multiple challenges of Al. The issues are manyfold and have a subatantial impact on fundamental rights (freedom, work and employment, privacy, equality and non-discrimination, democratic participation, access to justice, freedom of expression and information, political organization, environmental protection), civil and criminal liability, personal data protection, privacy and personality rights, intellectual property, competition law, environmental law, criminal law, tax law and administrative law.\nAlthough regulatory initiatives are taking place all over the world, the European Union has taken the lead. On February 16, 2017, the European Parliament adopted a resolution with recommendations to the European Commission on civil law rules on robotics. This resolution recognizes the dangers and opportunities of robotics and artificial intelligence and makes various suggestions for their regulation, urging the Commission to present a legislative proposal on legal issues related to the development and use of robotics and Artificial Intelligence. Annexed to this document were"}, {"title": "Structure, objectives and approach", "content": "The Regulation is an example of the so-called \u201cregulatory brutality\u201d trend. As will become clear, this piece of legislation is particularly complex, involving 68 definitions, 113 articles, 13 annexes and 180 recitals. The penalties are severe (up to 7% of the offender's global revenue or 35 million euros), the territorial scope of application is particularly broad, and supervision is carried out at national and EU level, establishing a new regulatory architecture, which includes the EU AI Office, the EU AI Board, an advisory forum and a scientific panel of independent experts (arts. 64 ff.) and, at national level, at least one national notifying authority and one national market surveillance authority (Art. 70).\nThis legislative instrument is made up of 13 chapters: 1) general provisions; 2) prohibited practices; 3) high-risk systems; 4) transparency obligations for certain types of systems; 5) general purpose models; 6) measures in support of innovation; 7) governance; 8) high-risk system database; 9) post-market monitoring, information sharing, and market surveillance; 10) codes of conduct and guidelines; 11) delegation of powers and Committee procedure; 12) sanctions and 13) final provisions.\nThe major division of the Regulation is based on a risk classification of Al systems. This classification considers the uses or applications of Al systems. It is, therefore, a question of knowing what the system is designed for, the so-called \"intended"}, {"title": "Concepts", "content": "The Regulation has taken a maximalist approach to definitions, defining terms that are already part of the European acquis such as \"personal data\", \"non-personal data\", \"profiling\", \"biometric data\", enshrining unhelpful definitions such as \"Al literacy\" and terms that are self-explanatory such as \"publicly accessible space\", \"training data\" or \"instructions for use\".\nOn the other hand, the concept \"law enforcement\" is important but not obvious This term, which appears 98 times in the Regulation, is defined in Article 3/46 as \"activities carried out by law enforcement authorities or on their behalf for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including safeguarding against and preventing threats to public security\", with \"law enforcement authority\" being \"any public authority competent for"}, {"title": "Artificial Intelligence System", "content": "The first challenge for regulation was to find a suitable definition of Artificial Intelligence. Many definitions associate intelligence with human intelligence, the ability to use reasoning to achieve goals. Other perspectives approach the concept through the programming techniques used. After much discussion, the Regulation ended up adopting the definition of \"Artificial Intelligence systems\", which replicates the updated OECD definition: \" a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments\" (art. 3/1)."}, {"title": "Subjects", "content": "The Regulation mentions several roles that form part of the Al value chain: the provider, the importer, the distributor, the authorised representative, and the deployer,\nall of whom are covered by the generic notion of \"operator\". As we shall see, the Regulation applies to any provision of the system in the EU, even if it is free of charge.\nThe central subject, the main target of the Al Act rules, is the provider, defined in Article 3/3 as \"a natural or legal person, public authority, agency or other body that develops an Al system or a general-purpose Al model or that has an Al system or a general-purpose Al model developed and places it on the market or puts the Al system into service under its own name or trademark, whether for payment or free of charge;\".\nThe central feature that defines someone as a provider is the fact that they offer an Al-system under their own name. Providers, when not established in the EU, must fulfill their obligations through authorised representatives established in the EU (defined in art. 3/5), as provided for in arts. 22 (in the case of high-risk Al systems) and 54 (general purpose Al models).\nThe user, except for those who use the system as part of a personal, non-professional activity, is the \"deployer\" (art. 3/4) and also has obligations of their own, namely, to supervise the operation of the system (cf. arts. 26 and 50/3 and /4).\nImporters, i.e. those persons located in the EU who place an Al system on the internal market (art. 3/6), will have certain obligations to verify and guarantee conformity, as well as to collaborate with the authorities (art. 23). The Regulation reserves the term \"placing on the market\" for the initial act making available of an Al system on EU territory (art. 3/9), with \u201cmaking available\u201d being defined as any supply in the context of a commercial activity (art. 3/10). Thus, importers carry out \"placing on the market\", while distributors (Art. 3/7) are engaged in \"making available on the market\" following importation. Distributors are subject to obligations of verification and cooperation with the authorities that are very similar to those of importers (Art. 24).\nAnother concept, which is not defined but is included in the concept of operator, is that of \"product manufacturer\" (referred to in Article 2/1/e)). Given that what is at stake is the joint provision of a product and an Al system under one's own name or brand, product manufacturers should be considered providers.\nA person can become a provider if they \"put their name or trademark on a high-risk Al-system already placed on the market\" (art. 25/1/a)), \"make a substantial modification to a high-risk Al-system that has already been placed on the market or put into service, in such a way that it remains a high-risk Al-system\" (art. 25/1/b)) or \"modify the intended purpose (...) so that the Al-system concerned becomes a high-risk Al-system\" (Art. 25/1/c)). Although the reverse is not expressly spelled out, changing the intended use of the Al system to one that is not considered high-risk will allow the modified system to escape the application of certain rules or even the Regulation as a whole."}, {"title": "Scope of application", "content": "Despite being a general regulation, the Al Act explicitly safeguards the application of the rest of the regulatory framework (art. 2, paragraphs 5, 7 and 9) and allows complementary national rules to be adopted in certain areas, such as more favorable standards for the protection of workers (art. 2/11) or rules on the use of remote biometric identification systems (art. 5/5 and /10). In addition, the application of some legislation (art. 2/2, referring to the list in Section B of Annex I) and sectoral"}, {"title": "Principles", "content": "Although not in the initial proposal, which was essentially aimed at determining prohibited practices and regulating high-risk applications, there was consideration of enshrining a set of general principles applicable to all operators and all Al systems subject to the Regulation. In the final version, the only duty with such breadth is the obligation imposed on providers and implementers to ensure that people operating or using Al systems \"have a sufficient level of Al literacy\" (art. 4).\nNevertheless, those principles still underlie the requirements placed on high-risk systems (arts. 8 to 15) and their operators (arts. 16 to 27).\nAt issue is a set of concerns developed in the interdisciplinary field known as Al safety or FATE (Fairness, Accountability, Transparency, Ethics) Al, including concerns of control, transparency, alignment, non-discrimination, robustness, and security.\nSome principles are hard to parse. Of course, we are all in favor of fairness. The great difficulty, which is the field of philosophy and then politics, translating into the committed choice of each society at a certain time and place through positive law, lies in defining what is just, equitable, and fair. This problem is both conceptual and technical-mathematical. In practical terms, not much can be drawn from this principle.\nThere are similar difficulties with algorithmic bias. Some of the known problems result from the poor quality of the data used (namely lack of representativeness or quantitative or qualitative insufficiency) or programming errors. On the other hand, many problematic situations simply result from the system having been optimized to achieve a given beneficial or innocuous objective. For example, if an algorithm is designed to favor what an internet user pays more attention to, it could end up recommending alcoholic drinks (having indirectly detected that (s)he is an alcoholic), or promoting offensive or aggressive speech (since this is what most people will pay more attention to). These challenges, especially those posed by recommender systems, are already partially addressed in the Digital Services Act (\"DSA\"). In any case, the Al Act places significant emphasis on diversity and the prevention of discrimination and bias.\nPutting an end to these occurrences is impossible, but there is an obligation to make adequate efforts to follow best practices to prevent easily avoidable mistakes.\nTransparency can be understood as referring to several different concepts. One of them, employed in Article 50, refers only to the identification of the origin of a given content or agent as being or coming from Al systems. Transparency is also covered by the obligation to provide and maintain technical documentation (arts. 11, 18, 20 and Annex IV), record-keeping (arts. 12 and 19), the provision of information (art. 13), and cooperation with authorities (art. 21).\nWhen transparency refers to the characteristic of the Al system, this concept can allude to the description of the human tasks of designing, configuring and making the system available, even if the system is itself (i.e. in its operation) opaque. Transparency is sometimes used to refer to interpretability, i.e. the ability to understand how an Al system works, and/or explainability, i.e. the clarification of why a certain result was obtained by operating the system. A system can be interpretable, but produce concrete results that are not explainable. For example, we know the parameters used and the steps followed by the system to assign a premium value in an insurance contract, but we can't explain why individual A has a higher premium than individual B. There are, however, artificial intelligence techniques that generate totally opaque systems (e.g. large language models, such as GPT); we know almost nothing about their inner workings. For these, interpretability and explainability are not technically possible.\nThe Al Act does not impose a general obligation to generate explainable models or decisions. However, in the case of high-risk systems, it establishes a right to an explanation of the role of the system (arts. 13 and 86), and to understand the main principles of its operation and the decision taken (arts. 14 and 86). The text of Article 86 (and recital 171) is not entirely clear as to whether it is necessary to explain the specific decision or whether a general explanation is sufficient. On the other hand, the references to the relevant technical capacities to explain the results (Art. 13/3/b)/iv)) and \"where appropriate, information enabling those responsible for the deployment to interpret the results of the high-risk Al system and to use them appropriately\" (Art. 13/3/b)/vii)) are made in the context of technical documentation, which seems to indicate that a generic and abstract explanation (interpretability) is at stake and not a real explainability. Furthermore, even if a right to an explanation of the specific decision were established, the protection of personal data, business secrets and other types of secrecy would act as a limit to the exercise of this right. In this sense, in my opinion, Al techniques that do not allow explanations to be generated (e.g. deep learning neural networks or support vector machines) remain legally admissible, even in the case of high-risk systems.\nSupervision and human control are reflected in the obligation for the provider to adopt a risk management system (art. 9), quality control (art. 17), to monitor its post-marketing operation (art. 72), to report serious incidents (art. 73) and to design high-risk systems in a way that allows for understanding and intervention in their operation (art. 14), namely the existence of a kill switch (art. 14/4/e)). These aspects intersect with cybersecurity and robustness concerns (art. 15) to which an important legislative"}, {"title": "Prohibited practices", "content": "At an early stage, the Commission proposed the establishment of four prohibited practices, said to pose an unacceptable risk, which could be summarily described as subliminal manipulation systems, systems that exploit vulnerabilities causing behavioral distortion and damage, social scoring systems and real-time biometric identification systems (e.g. facial recognition). These predictions had some exceptions and used particularly vague language. After intense discussions and negotiations, the language has been refined, the list of prohibited practices has been extended, but the end result is not much better. They are now prohibited:\n\u2022 Manipulation and exploitation of vulnerabilities \u2013 art. 5/1/a) and b)\n\u2022 General social scoring \u2013 art. 5/1/c)\n\u2022 Predictive policing \u2013 art. 5/1/d)\n\u2022 Creation of facial recognition databases \u2013 art. 5/1/e)\n\u2022 Emotion recognition systems in the workplace or education - art. 5/1/f)\n\u2022 Biometric classification of protected categories \u2013 art. 5/1/g)\n\u2022 Special cases of real-time biometric identification \u2013 art. 5/1/h)\nThis list is not exhaustive. Other practices may be prohibited or unlawful on other grounds (Art. 5/8). For example, systems that generate deep fakes are not normally seen as high-risk but are only subject to transparency obligations (Art. 50/4). However, when such a system is configured or prepared to generate child pornography that will be a crime"}, {"title": "Manipulation and exploitation of vulnerabilities", "content": "A prerequisite for freedom in general, especially freedom of thought, choice and expression, is an adequate perception/representation of reality. Private autonomy and the free development of the personality require this. For this reason, the national legal system makes legal transactions concluded on the basis of defects of will voidable, and prohibits and punishes unfair commercial practices and misleading advertising. The autonomy of the will, as a reflection of the dignity of the human person, is also reflected in the prohibition of experimentation on people and the requirement of free and informed consent, especially in the case of voluntary limitation of personality rights.\nSome Al systems have the potential to manipulate and mislead, interfering with the free formation of thoughts, opinions and choices. In this sense, the text of art. 5/1/a) of the Regulation prohibits \u201cthe placing on the market, the putting into service or the use of an Al system that deploys subliminal techniques beyond a person's consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harms.\" This wording uses indeterminate concepts and qualified language (\"materially\", \"appreciably\", \"significant\", \"reasonably likely\"). These qualifiers seem to indicate that not every advertising technique or hidden or misleading practice will be covered. In fact, I believe that the criteria of advertising law and consumer protection will be less demanding, i.e., certain conduct qualified as aggressive or misleading advertising and/or commercial practices will not fall under Article 5/1/a) of the Regulation. In such cases,"}, {"title": "Social scoring", "content": "The practice of scoring, i.e. assigning numerical values to individuals, although not defined, is already covered by the GDPR, as it almost always involves profiling and often also an automated decision. This operation is often necessary so that computer systems can perform their functions. However, it raises concerns, especially considering what certain countries, such as India and China, have implemented: social classification systems, which take into account the generality of citizens' behavior in order to assign a classification that determines or influences their treatment in various contexts.\nThe Regulation only prohibits Al systems \"for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following (...) detrimental or unfavourable treatment (...) in social contexts that are unrelated to the contexts in which the data was originally generated or collected (...) [or] that is unjustified or disproportionate to their social behaviour or its gravity\u201d (art. 5/1/c)). What is at stake is what is known as general social scoring, i.e. the overall assessment of a natural person's behavior. On the other hand, Al systems that do more restricted scoring, such as those dedicated to credit scoring, solvency assessment or risk assessments and the pricing of life or health insurance, will be classified as high-risk (Annex III, 5/b) and c)). Finally, systems that score for the purposes of detecting financial fraud or for setting prices in car insurance will not even be covered by the Regulation. Again, what determines the risk classification of the system is the purpose of the quantitative assessment and not the practice of scoring.\nAs has been pointed out, scoring is usually associated with an automated decision, which, when involves the processing of personal data and produces effects on the legal sphere or significantly affects the personal data subject, may from the outset be prohibited under Article 22 GDPR. However, it is important to note that Article 22 of the GDPR only applies to fully automated decisions. Therefore, at least in the case of"}, {"title": "Biometric identification and classification, including sentiment detection", "content": "Biometric identification systems, especially those for facial and emotion recognition, generated significant controversy during the legislative process. From the outset, these systems constitute an attack on individual privacy and freedom, with a high discriminatory potential. In this sense, companies such as Clearview.Al, which systematically scraped the Internet (especially social networks) to generate a facial recognition database, had already been sanctioned for violating the GDPR. In any case, the Regulation now expressly prohibits this practice (art. 5/1/e)).\nThe use of emotion recognition systems has been challenged on technical grounds. It is argued that expressions are variable at an individual level and depend on the social and cultural context, so these systems are not reliable. In addition, they have a high discriminatory potential. Paradoxically, the Regulation only prohibits the use of these emotion recognition systems in the context of work and education. In all other cases, emotion recognition systems are considered high-risk systems (Annex III/1/c). Both teaching and work can be done remotely, but I believe these situations are covered by the ban. On the other hand, the ban does not cover \u201cAl systems placed on the market strictly for medical or safety reasons, such as systems intended for therapeutical use\". This will raise questions in cases where systems are used for safety or medical reasons in the areas of workplace and education institutions. In that scenario, the intention seems to be allowing the use of such systems. Automatic interview systems should be classified as high-risk (Annex III,4), unless they also include an emotion recognition component.\nOn the other hand, the very notion of emotion recognition must be read restrictively. Recital 18 explains: \"The notion refers to emotions or intentions such as happiness, sadness, anger, surprise, disgust, embarrassment, excitement, shame, contempt, satisfaction and amusement. It does not include physical states, such as pain"}, {"title": "Predictive policing", "content": "The definition of profiles is based on the repeatability and standardization of behaviour. It is based on the idea that the past repeats itself in the future and that there are certain features of individuals that have predictive capacity. The application of these techniques in the criminal context raises special concerns, especially given the potential consequences of an error or injustice and the presumption of innocence.\nThus, the Regulation prohibits predictive policing practices that use Al systems to assess the risk of a natural person committing a criminal offense \"based solely on the profiling of a natural person or on assessing their personality traits and characteristics \" (art. 5/1/d)).\nHowever, \"this prohibition shall not apply to Al systems used to support the human assessment of the involvement of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity.\" In other words, the system must consider the concrete behavior and particular traits of a specific person and not their membership in certain categories or groups. This exception recognizes the potential usefulness of Al in the context of criminal investigation and prevention while ensuring that the assessment is based on actual data and not exclusively on (necessarily speculative) profiling.\nIn fact, predictive policing can be geared towards predicting crimes, predicting or identifying criminals, and/or predicting or identifying potential victims of crime. Most of these systems, when not based exclusively on profiling, will fall under the high-risk classification (Annex III, 6). In this vein, recital 42 clarifies that the prohibition of Article 5/1/d) does not cover \"Al systems using risk analytics to assess the likelihood of financial fraud by undertakings on the basis of suspicious transactions or risk analytic tools to predict the likelihood of the localisation of narcotics or illicit goods by customs authorities, for example on the basis of known trafficking\"."}, {"title": "High-risk systems", "content": "The definition of high-risk systems is made in article 6 by reference to two Annexes.\nAnnex I includes legislation on certain categories of products (such as toys, vehicles, explosives, elevators, or medical devices) and, according to Article 6/1, when Al systems are used as safety components in these products (or the Al systems are themselves products) subject to a conformity assessment obligation, this is a high-risk system.\nIn turn, Article 6/2 refers to Annex III, which specifies certain uses such as biometric identification, management of critical infrastructures, admission and classification in educational establishments, job interviews, monitoring of workers, access to and use of essential services (public and private), use in border control, in a judicial context or by law enforcement agencies. As PHILIP HACKER points out, more important than the context of use is the purpose a system used for medical operations or triage does not carry the same risk as a system that manages medical appointments.\nThe system works with auto-classification, i.e. each operator will determine the risk classification of their system. It is important to read the various hypotheses carefully and consider the Regulation's recitals. The Commission will adopt guidelines specifying"}, {"title": "Rules", "content": "In simple terms, the Regulation requires high-risk systems to be well-made, properly maintained, and controlled. The operators must have adequate documentation to prove compliance with the Regulation's rules.\nMachine learning systems are subject to data quality requirements, particularly in terms of representativeness and the application of measures to detect and mitigate biases (Art. 10). Article 10(5) even creates a new basis for the lawful processing of sensitive data (in addition to those in Article 9 of the GDPR) by establishing that, under specific conditions, it will be possible to process special categories of personal data \"to ensure bias detection and correction \". On the other hand, most of the Regulation's provisions will legitimize the processing of non-sensitive data since this will occur in order to comply with legal obligations (art. 6/1/c) GDPR).\nProviders of high-risk systems are responsible for meeting the requirements of articles 8 to 15 (art. 16), as well as ensuring the existence of a quality management system (art. 17), keeping documentation for a period of 10 years after the system has been placed on the market or put into service (art. 18), and maintaining logs (art. 19). There is also a duty to cooperate with competent authorities (articles 20/2, 21 and 73), to adopt corrective measures (article 20/1), and perform post-market monitoring (article 72). This monitoring includes a duty to inform the authorities in the event of a serious incident (Art. 73), defined in Art. 3/49 as \"any incident or malfunctioning in an Al-system which, directly or indirectly, has any of the following consequences: (a) death of a person or serious harm to a person's health (b) a serious and irreversible disruption of the management or operation of a critical infrastructure, (c) infringement of obligations under Union law designed to protect fundamental rights, (d) serious harm to property or the environment\".\nFrom a more bureaucratic point of view, in addition to a duty of documentation and record-keeping, providers of high-risk Al systems are obliged to identify themselves as"}, {"title": "Obligation of transparency for certain systems", "content": "Article 50 of the Regulation, the only one in Chapter IV, deals with certain systems defined in the light of their purpose, imposing minimum transparency/information requirements. The first two paragraphs of this article impose duties on providers, while paragraphs 3 and 4 concern the duties of those responsible for implementing these Al systems. These duties apply to the systems mentioned in Article 50 regardless of their risk classification.\nArticle 50/1 regulates Al systems \"intended to interact directly with natural persons\", i.e. so-called chatbots or conversational systems. These systems must be designed in such a way that it is clear to natural persons \"that they are interacting with an Al system, unless this would be obvious from the point of view of a natural person"}, {"title": "General purpose models", "content": "When the European Commission presented the proposal for a Regulation in April 2021, there were already some Al models with diversified capabilities, but the term \"foundational models\", used to indicate those models trained with large amounts of data and with the potential for various applications, had not yet been coined. It wasn't until August 2021 that a paper by Stanford researchers used this notion for the first time.\nThe real explosion of foundational models, which include GPTs from the OpenAl company and competitors PALM, BERT and Gemini (Google), Claude (Anthropic), Luminous (Aleph Alpha), Mistral 7B and LlaMA (Meta) took place in 2023.\nThis technology has particularities that are especially challenging. On the one hand, they have high development costs, which create considerable barriers to entry. Unlike the specialized systems for which the Regulation was initially intended, these models have a capacity for generalization and will often be made available through programming interfaces (APIs) so that third parties can optimize and adapt them to specific applications. In this sense, these models, as ANDREJ KARPATHY explains, are close to operating systems, generating considerable dependencies. These considerations are typically addressed by Competition Law, but the Regulation has dedicated a chapter to them. Articles 89/2 and 93 provide for the protection of downstream providers, i.e., those who integrate a general-purpose model or system into their system and who become dependent on a general-purpose system that they do not control.\nOn the other hand, these large general-purpose models are often opaque: they are a vast array of numbers (the so-called parameters and weights of a neural network) that interact in ways that are beyond human comprehension. This lack of understanding raises concerns of security, control, and alignment.\nIn addition, developing these models requires massive amounts of data, much of which is taken from the Internet and includes personal data and data protected by intellectual property rights. Furthermore, contrary to what was initially thought, these models retain some of the data in \"memory\". This makes assessing the lawfulness of these uses even more complex.\nFinally, most foundational models have \"creative\" capacities and also fall into the category of generative Al covered by Article 50.\nThe Regulation deals with all general purpose Al models (in Articles 53 and 54) and imposes additional duties (in Article 55) for so-called general purpose Al models with systemic risk. According to Article 51, systemic risk exists if the model has \"high"}, {"title": "Certification, supervision, and sanctions", "content": "The Regulation establishes preventive and repressive measures, although it essentially focuses on the placing on the market or putting into service of high-risk Al systems. Although civil liability is not directly addressed, some of the Regulation's rules if breached, could give rise to an indemnifying obligation under national rules. In addition, there is a reference to the possibility of collective claims under the terms of Directive 2020/1828 (art. 110).\nSince this is product safety legislation, articles 28 ff. provide for a certification and control scheme. There will be at least one national notifying authority and a national market surveillance authority (art. 70), which will be the competent national authorities under the terms of the Regulation.\nThe notifying authority is the one that assesses, designates, and supervises the conformity assessment bodies: typically, independent private entities that carry out testing, certification, and inspection activities on the systems to ensure that they meet"}, {"title": "Conclusion", "content": "In my opinion, the Regulation contains generally balanced and reasonable solutions. However, given its length, complexity, and poor legislative quality, it will become somewhat difficult to implement. There is, therefore, a real risk that the European Union will negatively affect innovation and investment in the field of Artificial Intelligence. It is also possible that there will be a reduction in the supply and/or divergence of products or services, with the European public receiving different and less advanced versions. As MIGEL PEGUERA POCH writes, the Regulation is a remarkably complex instrument with unpredictable effects.\nThe main hope lies in the use of standards, whose mass adoption could significantly reduce compliance costs and reduce the considerable uncertainty that this legislative instrument will inevitably generate. Another contribution to overcoming the limitations of this piece of legislation will have to come from lawyers."}]}