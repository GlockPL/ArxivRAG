{"title": "ZebraPose: Zebra Detection and Pose Estimation using only Synthetic Data", "authors": ["Elia Bonetto", "Aamir Ahmad"], "abstract": "Abstract-Synthetic data is increasingly being used to address the lack of labeled images in uncommon domains for deep learning tasks. A prominent example is 2D pose estimation of animals, particularly wild species like zebras, for which collecting real-world data is complex and impractical. However, many approaches still require real images, consistency and style constraints, sophisticated animal models, and/or powerful pre-trained networks to bridge the syn-to-real gap. Moreover, they often assume that the animal can be reliably detected in images or videos, a hypothesis that often does not hold, e.g. in wildlife scenarios or aerial images. To solve this, we use synthetic data generated with a 3D photorealistic simulator to obtain the first synthetic dataset that can be used for both detection and 2D pose estimation of zebras without applying any of the aforementioned bridging strategies. Unlike previous works, we extensively train and benchmark our detection and 2D pose estimation models on multiple real-world and synthetic datasets using both pre-trained and non-pre-trained backbones. These experiments show how the models trained from scratch and only with synthetic data can consistently generalize to real-world images of zebras in both tasks. Moreover, we show it is possible to easily generalize those same models to 2D pose estimation of horses with a minimal amount of real-world images to account for the domain transfer. Code, results, trained models; and the synthetic, training, and validation data, including 104K manually labeled frames, are provided as open-source at https://zebrapose.is.tue.mpg.de/.", "sections": [{"title": "I. INTRODUCTION", "content": "Research in unconventional domains like wildlife scenar-ios, pose estimation of animals, and detection from images captured from aerial point-of-views, is often slowed by the lack of labeled data [1]\u2013[4]. However, manually annotating data is often time-consuming, costly, difficult, and prone to errors and imprecisely labeled instances (Fig. 2). Leveraging semi-automatic labeling systems, such as VICON halls, can also be impractical, e.g. for wild animals. Indeed, restraining and setting animals in still stances and different poses, as often done for humans [5], is hard, making the collection of such datasets impractical. Moreover, some data, like images taken from aerial views, or rare objects/creatures, are under-represented in currently available datasets (see Tab. I or [4], [6]) making training of deep learning (DL) models difficult. This is the case for zebras, some species of which, e.g. Gr\u00e9vy's zebra, are endangered and at risk of extinction.\nFor them, solving detection and pose estimation in the wild could boost animal conservation and monitoring research by allowing automatic and non-intrusive observation, e.g. from UAVs. While detection is fundamental to localizing the animal in the frame, pose estimation can be used for activity identification and as a prior in both shape estimation and health assessment.\nNotably, to generate synthetic images one must both i) create an automatic procedure to cover the target distribution, and ii) address the syn-to-real gap. Many methods use patched models over random backgrounds, sacrificing the realism of the generated images. Then, without adopting style transfer techniques or real data (either labeled or unlabeled) during training, they often fail to generalize to real images. To overcome this, we need large synthetic datasets with high visual quality and variability [3], [4], [6], [7]. Moreover, most works using synthetic data assume detection can be achieved and focus only on 2D pose estimation. However, as we show in this work (Sec. IV-C), such an assumption might not hold as detection is a task tied to background appearance, camera-subject distance, and point-of-view used during training [3], [4], [8]. On the other hand, 2D pose estimation is mostly related to the variability and understanding of relative depth between the joints [3], [9]. Therefore, in this work, we focus on achieving both detection and 2D pose estimation of zebras using only synthetic data.\nOur strategy is to employ a 3D photorealistic simulator, generate data from it, and use that to solve both tasks. We leverage the work of [4] by augmenting their synthetic image dataset to overcome one of the main limitations of that work, i.e. the generalization to common images (see Fig. 2), without generating new data. This process includes cropping and scaling those synthetic images, resulting in more variability than the original data, which we then use to"}, {"title": "II. RELATED WORK", "content": "The common synthetic data generation technique involves the stitching of the CAD or SMAL-based models, often animated through manual manipulation, pre-fitting to real images, or through posing VAE, over randomized back-grounds [1], [2], [7], [12], [13]. This often leads to differ-ences in scale, incorrect blending with the background, and lighting conditions. These datasets are then used to perform 2D pose estimation of the animal, a task mostly related to the relative orientation and position of the joints rather than the general visual appearance [3]. Indeed, there is no work except [4] that learns how to detect animals from synthetic images. However, as shown recently also for humans [14] the realism of the generated images is crucial to close the syn-to-real gap. To achieve that, [4] proposed to generate visually appealing data using easily obtainable models and a photorealistic simulator [15]. However, their randomized viewpoint generation strategy does not produce data for training good detectors on common, non-aerial, images.\nDue to the variations that we observe in the animal king-dom, even within families like Canidae, and the limitations of collecting ground-truth data, it is challenging to obtain a general model like SMPL [5] for animals. Therefore, many studies face problems due to the absence of robust prior and unrealistic generation strategies, as explained above. As a result, these works to address the syn-to-real gap often rely on domain adaptation or semi-supervised learning tech-niques. These often include leveraging considerable quanti-ties of unlabeled real images, or generation with consistency, style, and other constraints [2], [7], [12], [16]. However, while some methods obtain good performance thanks to the inclusion of unlabeled data, they suffer from a noise problem resulting from an overfit on the refinement over synthetic data predictions [1], [3], [16]. Moreover, they often evaluate their performance only on a few datasets, thus not fully benchmarking their method. More recently, methods such as [17], [18] leverage LLM alongside synthetic data to enhance pose estimation techniques, especially to solve common problems of 2D pose estimation such as left/right flipping. However, although they present a good generalization capability, their performance is still far from the one obtainable from models such as ViTPose+ [11].\nFinally, all methods working with keypoints estimation and synthetic data assume that detection is available [1], [3]. Moreover, the synthetically generated data often only consists of a tight crop around the animal [7], [12]. While this makes the rendering fast and the image crisp, it makes the data unusable to learn detection models, as the animal would always only occupy the full image. Then, due to this and the lack of realism, most methods do not evaluate their synthetic data in the joint task of detection and pose estimation while assuming that good data and methods for that are already available. In this work, by using the data from [4] and evaluating exactly this scenario, we ensure the applicability of the data for both the aforementioned tasks."}, {"title": "III. APPROACH", "content": "The goal is to obtain a full top-down system for the detection and 2D pose estimation of zebras in real images using only synthetic data for training. We use the popular YOLOv5 [19] for detecting the animals and ViTPose+ [11] for estimating the 2D keypoints. To generate synthetic data, we build upon [4], which uses the GRADE framework [15] and introduces in it an animated zebra model\u00b9 with ten different environments from the Unreal Engine marketplace.\nFirst, an environment is loaded into the simulation. Then 250 3D zebra models are created. Each one is randomly scaled to vary its size and posed using a randomly selected animation sequence and timestep. Subsequently, 2D occu-pancy boxes are computed for each animal and a random area of the environment is selected. The boxes of the randomly"}, {"title": "A. Detection", "content": "Following the intuitions above, instead of generating new synthetic data by creating a new pipeline, we introduce a new pre-processing step to augment the 'SC' dataset to obtain wider coverage of the desired distribution. Since the original dataset has been rendered from random viewpoints, the individuals in the images tend to be small. Therefore, to create images with bigger, and thus virtually 'closer' to the camera, animals we employ a new targeted crop and scale procedure. We first select all the animals whose bounding box area is greater than a given threshold for each image in the training and validation sets. Each box is randomized using a random offset between 0 and 150 pixels on each side, creating a rectangular area around the animal. We then crop this box, and re-scaled it to the original image size (i.e. 1920 \u00d7 1080). The annotations are finally generated from the upscaled ground-truth segmentation masks of the crop itself. Note that the upscaling operation degrades the quality of the final image, as it acts as a digital zoom and we do not re-render the scene. We set the threshold on the original"}, {"title": "B. 2D Pose Estimation", "content": "The 'SC' data does not contain keypoints information out of the box. To obtain that, we exploited the information at our disposal, i.e. the known 3D location of the camera and the 3D vertices location for each individual. We create 27 different groups of vertices, corresponding to 27 different keypoints. The resulting keypoints, depicted in Fig. 1, are: four hoofs, four knees, four thighs, the tail start and end locations, eyes, ear tips and bases, start and end of the neck, nose, skull, body middle, and back end and front. From each set of vertices we compute the 3D average location and, using the ground truth camera pose, project them to the image. Using the COCO convention, we mark as visible the keypoints within the instance mask of the animal, and as occluded the remaining ones. Note how this procedure is easily customizable. For example, by leveraging the known depth information or additional heuristics, one could skip some keypoints or decide to label all occluded keypoints. We label only the individuals whose maximum bounding box dimension, either width or height, exceeds 30 pixels. As we use a top-down approach, each annotation is separately cropped and scaled to the input size of the selected model and only then used for training and validation, with the loss computed only on the labeled keypoints. This automatically diminishes the impact of the relative distance between the camera and the animal, while influencing the sharpness of the final crop. Finally, we map the 17 canonical keypoints of datasets such as APT-36K to our 27 to be able to train models using mixed data. Note that the whole set of animations of the synthetic zebra model consists of only 888 different poses. Out of those, 440 are part of idling ones, thus practically static; a rather limited number of them if compared to other datasets [2], [12]."}, {"title": "IV. EXPERIMENT AND EVALUATIONS", "content": "In the next sections, we use the short names of the datasets introduced here (in bold). When mixed datasets are used during training procedures, we will concatenate the naming using the '+' symbol. A recap of the datasets used, the"}, {"title": "C. Detection performance", "content": "We train a YOLOv5s detector with randomly initialized weights following the same training procedure as in [4] for fairness, i.e. 300 epochs, default learning rate, and input image size 1920 \u00d7 1920, except for CZ640 for which we use 640 \u00d7 640. While it is common practice to test the model using images scaled to the training size, we evaluate all the models using both 1920 \u00d7 1920 and 640 \u00d7 640 image sizes. Here, due to the limited availability of images, we use the totality of the A36oz (1200 images) and A10oz datasets (200 images). During these tests we noticed an overlap between A10oz and CZ of at least nine images.\nIn Tab. II, we report the results obtained using images scaled to 1920 \u00d7 1920. All the trainings done with the syn-thetic data SC5K achieve better results than the corresponding training done with the old SC dataset. For example, on the A36oz the model trained with SC5K shows +36% on mAP50 when compared to the one trained with SC. On average, when YOLOv5s is trained only on the synthetic SC5K we see performances that are as good as or better than when only CZ640 is used, with exception for RP. Moreover, the same model significantly outperforms CZ1920 in both RP and R123, i.e. on aerial images. The results obtained using images scaled to 640 \u00d7 640, reported in Tab. III, show how most of the considerations we made still hold true, i.e. models trained with synthetic data perform well on average while using only COCO perform poorly on R123 and RP."}, {"title": "D. 2D pose estimation performance", "content": "We use the popular ViTPose architecture, in its ViTPose+ [11] variation. For the most part, we employ the common training settings, i.e. 210 epochs, with decay steps at 170 and 200. The optimizer is Adam, with a 5e-4 learning rate. The target type is Gaussian heatmap and groundtruth bounding boxes are utilized during training and validation. We use the small network model and train with the standard 17 keypoints for the real datasets and 27 keypoints when using synthetic or mixed datasets. Due to a small misalignment between our synthetic data annotation of the thighs and the tail base of a few pixels we also report a filtered average result that does not consider these. We train the network first without any pre-training and then with the default MAE pre-trained backbone.\nFirst, in Tab. IV, we consider the results obtained without the pre-trained backbone. The network trained only on SC performs similarly to the ones trained only on zebras, i.e. with A36oz or A10oz, for both PCK metrics. With respect to them, it also shows better generalization performance for both horses (TDH) and the full A10 and A36 datasets, with improvements of more than 13%. This trend holds also when we compare SC to SpacNet, with improvements between 3 and 25%. Moreover, the filtered results, i.e. the model trained on SC without the filtered keypoints, significantly outperforms the models trained on A36oz and A10oz. Thus, despite the low variance of poses expressed in the SC datasets due to the limited number of animated frames (Sec. III), the models trained only on synthetic data can successfully train a 2D pose estimator for real images. Naturally, training the model on the full A10 and A36 datasets achieves the best performance overall when using a single dataset, thanks to the domain transfer effect between similar species and the different animals contained in those. However, considering Zebra-300 and Zebra-Zoo, the filtered performance of the model trained with SC are similar or better than the one obtained when using A10 and A36. Furthermore, SC data generalizes better to TigDog Horses (TDH) than how TDH data does on zebras. As expected, introducing real-world data boosts the performance of all models, even with a minimal integration of A1099. SC+A1099 significantly outperforms SpacNet+A1099 in both zebras-specific keypoints detection and generalization capabilities, with peaks of 20% for both PCKs. Similarly, training SC+TDH99 achieves performance on the TDH dataset similar to the ones that can be obtained by using the full A10 or A36 datasets. Finally, training SC combined with the totality of the A10 or A36 datasets does not bring any considerable improvement except on the detections performed on non-zebras datasets.\nIn Tab. V, we show how using a pre-trained backbone gen-erally improves the performance of all models, datasets, and"}, {"title": "V. CONCLUSIONS", "content": "In this paper, we show how only synthetic data can be used to successfully train both a detector and a 2D pose estimation network without the use of style transfer techniques and potentially without real-world data. We improved detection performance considerably across all datasets and data types with respect to [4]. At the same time, we showed remarkable results in estimating the pose of zebras despite the low number of animations. Finally, the inclusion of just 99 real images improves performance and generalization capabilities up to comparable baseline methods. The results thus suggest that our generation strategy focused on visual realism and variability successfully addresses the problem even when not using any pre-trained network or complex animal model. Through our extensive validation, we also highlight how i) using the same image sizes for training and testing with YOLO is not always effective, despite being common practice, ii) extensive synthetic datasets can overcome the need for pre-trained backbones, and iii) how extensive testing with diverse data is fundamental. Future work will include the generalization of this method to different animals and to 3D pose and shape estimation."}]}