{"title": "Leveraging ChatGPT's Multimodal Vision Capabilities to Rank Satellite Images by Poverty Level: Advancing Tools for Social Science Research", "authors": ["Hamid Sarmadi", "Ola Hall", "Thorsteinn R\u00f6gnvaldsson", "Mattias Ohlsson"], "abstract": "This paper investigates the novel application of Large Language Models (LLMs) with vision capabilities to analyze satellite imagery for village-level poverty prediction. Although LLMs were originally designed for natural language understanding, their adaptability to multimodal tasks, including geospatial analysis, has opened new frontiers in data-driven research. By leveraging advancements in vision-enabled LLMs, we assess their ability to provide interpretable, scalable, and reliable insights into human poverty from satellite images. Using a pairwise comparison approach, we demonstrate that ChatGPT can rank satellite images based on poverty levels with accuracy comparable to domain experts. These findings highlight both the promise and the limitations of LLMs in socioeconomic research, providing a foundation for their integration into poverty assessment workflows. This study contributes to the ongoing exploration of unconventional data sources for welfare analysis and opens pathways for cost-effective, large-scale poverty monitoring.", "sections": [{"title": "I. INTRODUCTION", "content": "The use of unconventional data sources to assess and predict poverty has gained significant traction in recent years, driven by the need for timely, granular insights into economic well-being. This is highly relevant for tracking progress towards the Sustainable Development Goals (SDGs). Traditional poverty assessments, such as surveys and censuses, are often costly, time-consuming, and infrequent, particularly in developing countries where resource limitations and/or political constraints hinder comprehensive data collection [4]. As a response, researchers have explored innovative data streams such as satellite imagery, mobile phone usage patterns prediction [2], nightlight intensity [11], and social media activity [1], which offer the potential for faster and more cost-effective poverty assessment at fine spatial resolutions.\nOut of the above, satellite imagery has received increased attention. The breakthroughs in the use of nighttime lights to observe global economic activity two decades ago have paved the way for more detailed and advanced observation [5]. However, satellite imagery is inherently unstructured, requiring significant interpretation and processing to extract meaningful insights. This complexity can introduce challenges in analysis, but the availability of large volumes of high-quality satellite imagery combined with advancements in analytics, including machine learning and deep neural networks, has enabled even more powerful insights. Satellite imagery provides consistent coverage across diverse geographic areas and allows for monitoring changes over time, sometimes on an hourly scale. By integrating remote sensing and machine learning, researchers can address the data gaps faced by traditional methods, providing more timely and scalable insights into socioeconomic conditions. However, training custom neural networks for such analysis can be challenging due to the enormous data requirements and computational resources needed. The availability of sufficient labeled data is crucial for model training, and the complexity of interpreting unstructured data such as satellite imagery further complicates this process.\nIn this context, we explore a novel approach: leveraging Large Language Models (LLMs) to interpret and analyze satellite imagery to derive insights about village-level human poverty and. LLMs, initially developed for natural language understanding, have shown remarkable adaptability across diverse domains, including image captioning, pattern recognition, and even geospatial analysis. However, their potential for extracting socioeconomic indicators directly from satellite images remains unexplored. Recent advancements in language models with vision capabilities provide an opportunity to extend traditional satellite-based analyses. Vision-enabled LLMs can leverage both pre-trained knowledge and visual data to analyze images in a broader context, reducing the need for extensive, domain-specific training.\nThis paper aims to assess the feasibility and accuracy of utilizing LLMs for poverty estimation using satellite data. Specifically, we address the question: Can LLMs effectively analyze satellite images to provide information related to human poverty? Our objective is to determine whether LLMs can provide reliable, efficient, and interpretable insights at the village level, and to identify the opportunities and challenges of using such models for socioeconomic research. If LLMs can extract relevant information from satellite images equally well as humans, then this would enable large-scale and potentially more interpretable poverty analysis from satellite images. We find that by using a pairwise comparison approach, it is possible to get ChatGPT to rank satellite images based on poverty level, with results that are as good as ranking based"}, {"title": "II. DATA", "content": "The ground truth data for this study comes from the 2015/2016 Tanzania Demographic and Health Survey (DHS) dataset, which is based on 12,563 households, grouped into 608 clusters across the 30 regions of the country. Each cluster ranges in sample size from 12 to 22 households per cluster. While it is not without problems, such as potential inaccuracies and limitations in capturing all aspects of poverty, it remains the best available source of comprehensive socio-economic information. The DHS data is collected through extensive household surveys and provides critical socio-economic indicators, including the asset wealth index. This index is a composite measure of a household's ownership of various assets, ranging from consumer goods to housing characteristics, and serves as a proxy for socio-economic status.\nWe base our ground truth on the DHS HV271 variable; the wealth index factor score [9], which is calculated using principal component analysis on data on ownership of basic assets (e.g. bicycles, television sets, sanitation assets, and water access). It represents wealth on the household level. For each cluster, we use the average of the HV271 variable for all households in that cluster.\nWe also use the DHS V191 variable, which is a wealth index factor score computed on a more individual basis and not for the household. The value should have a large overlap with the DHS HV271 value, but V191 and HV271 are not identical, and we use the match between them to estimate the maximum possible accuracy in ranking by the wealth index. The wealth index is used as a proxy for poverty as in similar studies.\nA recurring problem in poverty analysis is how to find reliable ground truth data, which is essential for training and validation of models. The work from Burke et. al. [4] highlights the critical issue of noise in ground-truth data and its implications for machine learning models. Ground-truth data, such as survey information, often suffers from measurement errors, sampling biases, and temporal misalignment with satellite imagery. Additionally, some noise is introduced intentionally through techniques like jittering, which adds small random perturbations to preserve the privacy and integrity of respondents. These inaccuracies and intentional perturbations introduce significant challenges in the training and validation of machine learning models, particularly in contexts where reliable data is scarce. Temporal mismatches exacerbate these issues by misaligning features in satellite data with outdated or inconsistent ground observations, undermining the development of accurate predictive models. As a result, noisy ground-truth data can obscure important relationships, mislead model performance evaluations, and diminish the reliability of predictions.\nSupervised machine learning models are particularly vulnerable to these issues because they rely heavily on the quality of their training data. Noisy labels can distort feature-label relationships, leading to mis-classifications and reduced generalization across different spatial or temporal contexts. Models trained in one region and applied to another may fail due to regional variability and non-uniform data quality. Similarly, hybrid models combining satellite imagery with auxiliary data like socioeconomic surveys often struggle with integration when ground-truth datasets vary in quality or coverage. Even unsupervised or semi-supervised approaches, which are less dependent on labeled data, can be undermined by noise in preprocessing or validation stages, complicating the interpretation of results and the robustness of clustering or feature extraction efforts."}, {"title": "B. Satellite Imagery", "content": "Sentinel-2 imagery was used in the training of machine learning models based on convolutional neural networks, described below, while high-resolution Google imagery was used as input for LLM. These images were selected to closely match the spatial area of the surveyed clusters, allowing for a direct visual representation of the physical environment where the households reside. The satellite imagery, including high-resolution Google imagery and multi-spectral Sentinel-2 data, provides a rich source of information for understanding environmental factors such as building density, road infrastructure, vegetation cover, and land use patterns, which are often correlated with socio-economic conditions."}, {"title": "III. METHODS", "content": "The wealth ranking of the images was conducted using a pairwise comparison strategy, leveraging OpenAI's GPT-40 model to evaluate wealth indicators such as infrastructure quality and visible amenities. The total number of images is N, one for each DHS survey site. Every pair of images was compared, asking GPT-40 which image that represents the wealthier location. The total number of comparisons is $\\binom{N}{2}$, some of which can end up as ties. All comparisons that were not tied were used to produce the final ranking.\nWe used Google Cloud Storage and generated signed URLS for each image, with a caching mechanism implemented to optimize the number of API requests, thus improving performance and minimizing costs. The pairs were compared using OpenAI's GPT-40 model by assessing wealth indicators such as building materials, number of floors, visible greenery, and amenities. The model was asked to answer which image of the two represented the wealthiest location. One retry was done if the model provided an ambiguous response; \"ambiguous\" means that it answered both images or none of the images. If the response was ambiguous also the second time, then the comparison was considered a tie."}, {"title": "B. Producing the final ranking", "content": "Inferring a total ranking from pairwise comparisons is denoted the Bradley-Terry model. For solving this, we used the Iterative Luce Spectral Ranking (I-LSR) [14] algorithm as implemented in the choix\u00b9 Python library. The I-LSR algorithm searches for the stationary distribution of a Markov chain with a transition matrix that encodes the pairwise comparisons. We also tried two other algorithms for this in the same library: Luce Spectral Ranking (LSR) [14] and Rank Centrality (RC) [15]."}, {"title": "C. Convolutional Neural Network (CNN)", "content": "For comparing the ranking results to a state-of-the-art image based method, we report how well a CNN-model does on ranking the wealth index for the DHS clusters. The CNN was trained using transfer learning on 10 m/pixel-resolution images from Sentinel-2, with an area of about 2 \u00d7 2 square kilometers. The training method and data sets used are described in detail in Sarmadi et al. [16]. The training followed very much the methodology described by Xie et al [18], [10], but used lower resolution images. Our experience is that using the lower resolution Sentinel-2 images yields very similar results to using the higher resolution Google images."}, {"title": "D. Random Forest regressor", "content": "To compare the ranking with domain expert performance, we use a Random Forest regressor [3] using expert defined features detected in the images. The features and the profile for the domain experts are described in [17], where it is also shown that this Random Forest model is better at ranking the images by wealth than the domain experts are themselves when asked to judge the wealth level from the satellite image."}, {"title": "E. Evaluation measures", "content": "Two evaluation measures are reported in the results section. The first is Spearman's rank correlation ($\\rho$); the correlation between two lists of ranked values. Spearman's rank correlation varies between -1 and +1, with the latter indicating perfect alignment. A value of 0 indicates no alignment, i.e., random guessing. The other is Matthew's correlation coefficient (MCC) [13], or $\\phi$ coefficient [19], which indicates how well values in a confusion matrix match. In the binary classification case, i.e, when the confusion matrix is 2 \u00d7 2, MCC varies between -1 and +1, with +1 indicating perfect classification. An MCC value of 0 indicates random guessing. We use Gorodkin's generalization of MCC [7] for the multiclass case with a 5 x 5 confusion matrix when wealth indices (true and predicted) are grouped into quintiles. For the multiclass MCC, a value of +1 also indicates a perfect classification and 0 indicates random guessing. However, the minimum is not always -1; it can fall between -1 and 0.\nFor significance testing, the standard deviations are estimated using the bootstrap method [6]. In all tests, the significance level is 0.05."}, {"title": "IV. RESULTS", "content": "The final prompt presented in Section III-A was developed through a sequence of experimental steps. Initially, we experimented with more loosely formulated prompts, such as \"Which image is wealthier?\" However, the model struggled with interpreting such high-level concepts effectively without detailed instructions. The lack of specificity in these prompts led to inconsistent or ambiguous results, highlighting the need for clear and measurable criteria to guide the analysis.\nThe goal was to reach a reliable method to assess and compare the relative visual wealth of two images using observable indicators tied to economic or developmental status. The analysis required clear and actionable results, expressed as a binary conclusion (\"Image 1 is wealthier\" or \"Image 2 is wealthier\") to ensure practical utility and consistency.\nTo achieve this, we formulated key indicators of wealth that are visually measurable and relevant across different contexts. These included: the quality of infrastructure, reflecting economic investment and maintenance; the number of floors in buildings, indicative of development and resource allocation; the presence of visible greenery or well-maintained areas, a marker of environmental care often associated with higher economic status; and visible amenities such as paved roads and power lines, which signify advanced public infrastructure and services.\nWhile this approach draws inspiration from methods like the DHS asset wealth index, which evaluates household wealth based on observable assets and infrastructure, it is not identical. Our method focuses on visual indicators observable in images, emphasizing comparative assessment rather than deriving an absolute wealth score.\nNotably, OpenAI's models initially faced restrictions on engaging with topics like wealth and welfare due to the complexity and sensitivity of these subjects. Over time, advancements in model capabilities and refinements in guidance allowed for more nuanced discussions grounded in structured frameworks. By late 2023, the OpenAI model GPT-40 demonstrated improved performance in analyzing socio-economic indicators when given clear, detailed instructions. This development enabled the formulation of prompts like ours, which rely on specific, observable criteria to achieve reliable results. The final prompt was designed to explicitly list these indicators, ensuring clarity and focus in the analysis while maintaining a scalable approach."}, {"title": "B. From pair-wise comparisons to the overall ranking", "content": "The number of images is 608 and the number of pairwise comparisons is $\\binom{608}{2} = 184, 528$. Out of these, 26,757 were judged as ties. The remaining 157,771 were used to construct the final ranking with the I-LSR algorithm. We denote these as valid comparisons.\nThe consistency of the 157,771 comparisons was checked by creating a directed graph from the comparisons and counting the number of 3-cycles in this graph. A 3-cycle corresponds to three sites that do not follow the transitivity rule, i.e., they behave like Penrose stairs (A is wealthier than B, which is wealthier than C, which in turn is wealthier than A). We found 155,546 such 3-cycles. To get a scale for the number of 3-cycles we can assume that all edges belong to a clique, which gives us $n \\approx \\sqrt{2E}$, where n is the number of nodes in the clique and E is the number of edges (comparisons). With E = 157,771 we have n\u2248 562. The possible number of 3-cycles of a tournament graph with n nodes is at least $n(n^2 - 4)/24$ [8], which gives 7,395,920 possible 3-cycles. Hence, the number of observed cycles (155,546) is only about 2% of the total number of possible 3-cycles. This indicates a low rate of contradictions between the pair-wise comparisons.\nThe three methods for inferring the total rank inference (I-LSR, LSR, and RC) were tested, and the number of conflicts and the ranking conflicts for their solutions are shown in Table I. The number of conflicts is simply the number of pairwise comparisons that disagree with the final ranking. The ranking conflicts are the sums of differences in rank between pairs that disagree with the final overall ranking. The pairwise comparison matrix is quite dense (many pairwise comparisons were done) so we used the lowest regularization parameter (a) possible; a value of a = 0 caused errors. Table I shows that the I-LSR algorithm gave somewhat better results, and they were used in the further analysis."}, {"title": "C. The quality of the rankings", "content": "The upper left panel illustrates the agreement between the pairwise comparisons made by ChatGPT and a ranking by the DHS HV271 value."}, {"title": "V. DISCUSSION", "content": "This study explored the potential of utilizing LLMs for village-level human poverty predictions using satellite images. Specifically, the GPT-40 model by OpenAI was instructed to rank pairs of images, concluding which image shows a more wealthy location than the other. All possible pairwise comparisons were done and combined into a final wealth ranking of all images. The results show that the ability of the LLM is comparable with human-level ranking, i.e., ranking based on features identified by human domain experts, which in turn is better than if domain experts are asked to estimate wealth directly from the image."}, {"title": "A. Significance", "content": "We find this result remarkable in several aspects. The domain experts have several years of regional experience and high educational levels. The LLM used in this study possesses strong language skills and outperforms humans in many aspects but was not fine-tuned for the particular task of estimating wealth in Tanzania. Even though the prompt was designed to include key indicators of wealth that should be visible in the images, it lacked other possibly helpful factors, such as scale and regional information. Furthermore, the performance could most likely be increased by an elaborate chain-of-thought prompting, where examples of reasoning steps would help the LLM reach a better ranking. The LLM approach is also scalable. The pairwise comparison method employed in this study allowed for rapid analysis of a large dataset (608 sites and nearly 200,000 comparisons). This efficiency contrasts sharply with extensive manual annotations performed by domain experts, which are resource-intensive and time-consuming. The LLM ranking of the 608 images was performed by comparing all possible pairs of images. A more selective approach can likely be employed to reduce the number of pairwise comparisons, still achieving a final accurate ranking of the images. While the overall performance of the GPT-40 model was similar to the Random Forest model the more granular comparison indicates that the LLM has a better performance for the images representing very poor areas. One can speculate why. Perhaps the text corpus and the image collections seen by the LLM have a bias towards characteristics of poor areas rather than non-poor ones.\nThe CNN approach is clearly very capable of estimating wealth from the satellite image and there have been further developments on such models. The highest accuracies with a CNN have been achieved by Lee & Braithwaite [12] who, e.g., reported $R^2$ = 0.90 for Tanzania. We did not compare to their specific approach since our purpose was to illustrate the remarkable ability by ChatGPT, not to present the best wealth prediction. Also, the code for the results in [12] is not available and they make some hard to replicate adjustments to the data, which make it difficult for us to reproduce their model. However, the CNN approach used here for comparison is well tested, which is why we chose this as benchmark."}, {"title": "B. Visual analysis of ChatGPT's ranking", "content": "Figure 4 presents three interesting observations regarding the relationship between ChatGPT-derived rankings and the HV271 wealth index rankings (considered the ground truth).\nStrong agreement between rankings: In several instances, the rankings provided by ChatGPT align closely with the HV271 wealth index rankings. These cases highlight the model's ability to effectively analyze image features and identify socioeconomic indicators. Poor images are characterized by sparsely populated areas, dryness, and a lack of greenery, which align with our understanding of poverty. Conversely, wealthy images typically feature green vegetation and town- or city-like spatial layouts, which are consistent with our expectations. Notably, ChatGPT rankings align well with subjective appraisals, supporting their plausibility.\nFalse positive wealth assessments: Some images are ranked as wealthy by ChatGPT, despite being classified as poor according to the HV271 wealth index. These images share similarities with those ranked as wealthy and have little in common with those ranked as poor. Observing the images reveals that ChatGPT's rankings appear reasonable, even if they diverge from the ground truth.\nFalse negative wealth assessments: Conversely, there are cases where ChatGPT ranks images as poor, while the HV271 wealth index identifies them as wealthy. These observations suggest that ChatGPT's image-based socioeconomic analysis follows a logical pattern that aligns well with subjective appraisals based on visual cues. For the images analyzed, ChatGPT appears to identify plausible indicators of wealth or poverty, supporting its reliability in this context. However, discrepancies with the HV271 wealth index raise the possibility that issues in the ground truth data, such as displaced DHS coordinates or limitations in the index itself, could contribute to the observed misalignments. Further investigation is needed to determine the extent to which these factors influence the discrepancies."}, {"title": "VI. CONCLUSION", "content": "We found the performance of ChatGPT-40 quite remarkable in ranking the areas based on their wealth level using remote sensing images without any pretraining. It should be mentioned that in our prompt we did not give any information regarding the area the images correspond to."}]}