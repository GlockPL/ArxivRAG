{"title": "Intrinsic Bias is Predicted by Pretraining Data and Correlates with Downstream Performance in Vision-Language Encoders", "authors": ["Kshitish Ghate", "Isaac Slaughter", "Kyra Wilson", "Mona Diab", "Aylin Caliskan"], "abstract": "While recent work has found that vision-language models trained under the Contrastive Language Image Pre-training (CLIP) framework contain intrinsic social biases, the extent to which different upstream pre-training features of the framework relate to these biases, and hence how intrinsic bias and downstream performance are connected has been unclear. In this work, we present the largest comprehensive analysis to-date of how the upstream pre-training factors and downstream performance of CLIP models relate to their intrinsic biases. Studying 131 unique CLIP models, trained on 26 datasets, using 55 architectures, and in a variety of sizes, we evaluate bias in each model using 26 well-established unimodal and cross-modal principled Embedding Association Tests. We find that the choice of pre-training dataset is the most significant upstream predictor of bias, whereas architectural variations have minimal impact. Additionally, datasets curated using sophisticated filtering techniques aimed at enhancing downstream model performance tend to be associated with higher levels of intrinsic bias. Finally, we observe that intrinsic bias is often significantly correlated with downstream performance (0.3 \u2264 r \u2264 0.8), suggesting that models optimized for performance inadvertently learn to amplify representational biases. Comparisons between unimodal and cross-modal association tests reveal that social group bias depends heavily on the modality. Our findings imply that more sophisticated strategies are needed to address intrinsic model bias for vision-language models across the entire model development pipeline. Warning: This study contains figures and information that may be triggering and/or offensive to readers.", "sections": [{"title": "1 Introduction", "content": "Neural network models are prone to learning patterns based on statistical associations between concepts within their training data that might lead to harmful bias when it relates to social groups or model performance (Fabbrizzi et al., 2022). This phenomenon has been observed in a number of vision and language models, each of which are unimodal and learn information within a single modality (Caliskan et al., 2017; Guo and Caliskan, 2021; Steed and Caliskan, 2021; Wolfe and Caliskan, 2022c; Omrani Sabbaghi et al., 2023). Cross-modal models, which learn information from both vision and language modalities, also learn biased information relating to social group associations (Goh et al., 2021; Wolfe and Caliskan, 2022a; Wolfe et al., 2022a; Wolfe and Caliskan, 2022b; Wolfe et al., 2023; Janghorbani and De Melo, 2023; Berg et al., 2022; Mandal et al., 2023; Hall et al., 2023). These results have largely been found using intrinsic bias tests adapted from Natural Language Processing (NLP): evaluations that compare relative distances between a model's representations of stimuli representing different concepts and social groups. Despite their prevalence in model evaluation, however, there is limited work connecting them to other factors of model design and optimization. For example, upstream factors such as training datasets, model architectures, and model sizes directly determine the representations learned and consequently may be reflected in intrinsic bias tests. These representations are then directly used for downstream tasks such as zero-shot image classification, which suggests a potential connection between the intrinsic bias of a model and its performance on downstream tasks. By investigating intrinsic bias as it explicitly relates to these two upstream pre-training and downstream zero-shot performance factors, we are able to draw novel insights about the ways in which models can be optimized to reduce harmful or undesirable biases. We measure the associations between social groups and valence, the pleasantness or unpleasantness of a concept (Toney-Wails and Caliskan, 2020). Valence is a robust dimension of human cognition as it relates to shaping attitudes and biases (Harmon-Jones et al., 2013). Social groups and valence associations also exist in unimodal models (Wolfe and Caliskan, 2022c). In this work, we examine whether these associations are observed cross-modally and how they relate to upstream factors and downstream model performance.\nTo our knowledge, our work is the first which connects 26 tests of intrinsic bias, including those related to race, gender, age, and baseline associations with respect to non-social group concepts such as flowers and insects, to upstream factors of model training (including 26 training datasets, 55 model architectures, and model sizes ranging between 100 million and 5 billion parameters). While variations on all of these features have been proposed to either mitigate biases or improve performance on downstream tasks, we are the first to address the variance in the magnitude of the effect of these features in CLIP models. Additionally, we connect intrinsic bias tests to a suite of 35 zero-shot image classification and retrieval tasks (Schuhmann et al., 2022). A novel contribution of our work is that we show that optimizing for performance is not sufficient to mitigate intrinsic biases. The scale of our experiments allows us to obtain high statistical power in analyzing the relationship between intrinsic bias and both upstream factors and downstream performance and making the following generalizable knowledge contributions about cross-modal models\u00b9:"}, {"title": "2 Background and Related Work", "content": "We now introduce background information on CLIP models and methods for measuring intrinsic bias in these models.\nUnimodal Embedding Association Tests Caliskan et al. (2017) introduced EATs to measure the associations between static word embeddings which encode concepts related to targets (typically social groups) and attributes, similar to Implicit Association Tests (IATs) for human associations (Greenwald et al., 1998b). As contextual word embeddings replaced static word embeddings, alternate methods to measure intrinsic bias and associations were also developed (Guo and Caliskan, 2021; May et al., 2019). The Sentence Encoder Association Test (SEAT) introduced by May et al. (2019) measures intrinsic bias by operating over a set of target sentences and attribute sentences which are semantically bleached excluding the words which represent the concepts of interest.\nAdditionally, EATs have also been developed for modalities other than text, including vision. Steed and Caliskan (2021) introduce the Image Embedding Association Test (iEAT), which is similar to static word embedding EATs except that it operates over embeddings which represent single images rather than words. EATs in both the textual and visual domains have been shown to replicate biases which are observed in humans (Caliskan et al., 2017; Steed and Caliskan, 2021), making them valuable tools for investigating learned associations in both unimodal and cross-modal models.\nContrastive Language-Image Pre-training CLIP models are some of the most widely used vision-language models due to their success in zero-shot classification tasks (Radford et al., 2021) as well as their usage as components in popular text-to-image generation systems such as DALL-E and Stable Diffusion. CLIP models have separate image and text encoders, which are connected in a joint cross-modal embedding space (Radford et al., 2018). During pre-training, datasets of image-caption pairs comprising hundreds of millions of observations are fed into the models. The model's objective maximizes the cosine similarity between an image and its paired caption, while minimizing the cosine similarity between the image and all other captions in the pre-training batch. Several variations on the original model architecture and training dataset have been proposed to improve CLIP; see Section 3."}, {"title": "Biases in Vision Language Models", "content": "Cabello et al. (2023) investigate the mechanisms of gender bias amplification in pre-training and fine-tuning stages with vision-language models based on the LXMERT architecture (Tan and Bansal, 2019). This work builds upon theirs by investigating intrinsic biases within models based on the CLIP architecture, which is more commonly used, and through non-human, race, and age bias tests as well as gender bias. Additionally, our method does not rely on having access to training corpora or curating lexicons, making it more flexible to apply to a wider variety of models and biases.\nMost work investigating biases in CLIP models does so only for individual models. For example Janghorbani and De Melo (2023) study a CLIP model and find that it tends to associate images representing homosexuality with text such as \"offending\" and \"vulgar,\" while for heterosexual images with words such \"blissful\u201d and \u201cawesome,\u201d among other associations. Further biases in CLIP's embedding space related to race in 3 CLIP models (Wolfe and Caliskan, 2022b; Wolfe et al., 2022a; Wolfe and Caliskan, 2022a), and gender in 9 CLIP models (Wolfe et al., 2023).\nThe only comparative approach that studies bias in CLIP models to our knowledge Berg et al. (2022) compares gender bias across 9 CLIP models. They find that larger pre-training datasets tend to lead to decreased bias and hence associated with better zero-shot classification performance, they examine only a small set of biases, model architectures, and training datasets. It is not clear from this work the extent to which these trends would extend to other biases or to other CLIP models."}, {"title": "3 Experimental Setup", "content": "We now describe the experimental setup and data for upstream pre-training factors, intrinsic bias, and downstream performance.\nCLIP Models and Upstream Factors The models we study include the original nine CLIP models released by OpenAI (Radford et al., 2021); 29 models introduced by Cherti et al. (2023), which were pre-trained on variable-sized English-only subsets of the LAION 5B dataset (Schuhmann et al., 2022); and 93 additional models from the OpenCLIP project (Ilharco et al., 2021).\nThe 131 models studied range in size from 102 million to 5 billion parameters, and all use transformer-based text encoders. While most use transformer-based image encoders as well, 17 of the 131 models we study use ResNets or ConvNeXts, convolutional architectures. The pre-training factors that we consider are the size of the model (measured in the number of parameters), model architecture, pre-training dataset, and size of the pretraining dataset (measured in the number of samples). Model architecture has been tied to bias in other modalities (Ladhak et al., 2023), and model size and pre-training dataset have been tied to bias in nine CLIP models by Berg et al. (2022).\nThe datasets used for pre-training these CLIP models consist of image-caption pairs sourced from the Internet and created under varying levels of supervised curation. For example, the OpenAI WebImageText dataset (Radford et al., 2021) includes pairs whose text contains an element from a set of pre-defined phrases, while the LAION 5B dataset (Schuhmann et al., 2022) was filtered to remove images suspected of containing illegal content. Some datasets, such as CC 12M, are revised even further to remove or mentions of social groups or names in order to minimize biases which can be learned from the data. The datasets range in size from 12 million to 5 billion pairs; further statistics for pre-training datasets and architectures are available in the Appendix section A.\nEmbedding Association Test Stimuli To measure bias in CLIP models, we use a controlled approach concerning a broad set of concepts across both language and vision modalities. We consider five sets of association tests: non-human (flowers-insects; instruments-weapons), race (European American-African American), gender (women-men), and age (young-old). For all of these target categories, the attribute categories are positively or negatively valenced concepts, due to their strong associations with social groups both in human cognition and unimodal models (Toney-Wails and Caliskan, 2020; Harmon-Jones et al., 2013). For each domain, the expected outcome is that the first group is more positively valenced than the second.\nWe use the EATs introduced in Steed and Caliskan (2021) and May et al. (2019) for vision (iEAT) and language (SEAT) modalities, respectively, in order to test associations between groups and valence. In SEAT, the human groups are represented both with names and highly associated words in individual tests, meaning there are seven textual EATs and four image EATs. We want to note that the 'Gender/Valence' category was not tested in the original SEAT and iEAT studies.\nWe follow previous work (Caliskan et al., 2022; Charlesworth et al., 2024) that establishes women as being more associated with positive valence compared to men, and thus consider women to be the first group in the gender comparison, to represent the stereotype-congruent direction in our analysis. Additionally, the \u2018instruments' group was not included in the original iEAT study. Following the text stimuli from May et al. (2019) we carefully curated new image stimuli that satisfy the iEAT requirements.\nFurthermore, we introduce a variation in the iEAT and SEAT attribute stimuli in order to use text and images which are more principled and grounded. Specifically, we use new image stimuli from the OASIS dataset (Kurdi et al., 2017) and new text stimuli from the NRC-VAD lexicon (Mohammad, 2018) which contain images and words/phrases respectively that are rated and validated by humans and offer more control and human-grounded valence inputs. isualisation of the non-human category EAT using our new stimuli. Further details of these stimuli and how they are selected are provided in the Appendix A.2.\nEATs are computed across all modality combinations. iEAT consists only of image targets and image attributes, while SEAT consists only of text targets and text attributes. To perform the cross-modal analysis, we combine image and text stimuli, resulting in additional combinations: images as a target with textual attributes, and text as a target with image attributes. Biases are thus computed for four modality combinations: five All Image, eight All Text, five Image as Target, and eight Text as Target, totaling 26 tests.\nDownstream Performance (VTAB+) Because work from Berg et al. (2022) found an association between zero-shot performance and bias in nine CLIP models, we also test the relationship between performance and bias for models that have performance data available. We employ performance measured on VTAB+ (Schuhmann et al., 2022), a suite of 35 image classification and retrieval tasks, which includes broad sets of images such as ImageNet (Deng et al., 2009), sets of natural images captured with standard or specialized equipment such as Caltech-101 (Li et al., 2022) or Diabetic Retinopathy (Gulshan et al., 2016), as well as structural images, such as SmallNORB (LeCun et al., 2004)."}, {"title": "4 Approach", "content": "We describe the method we use for quantifying biases in CLIP models, as well as the regression models we use for exploring the relationship between biases, upstream pre-training factors, and downstream performance.\nMeasuring Intrinsic Bias We measure intrinsic bias using EATs (Caliskan et al., 2017; Guo and Caliskan, 2021; Steed and Caliskan, 2021; Wolfe and Caliskan, 2022a; Wolfe et al., 2023), which provide a generalizable and principled method for quantifying biases related to a variety of concepts, such as race and gender, grounded in literature in cognitive and experimental psychology (Blodgett et al., 2020). An EAT compares similarities between four sets of embeddings created by a model: Two sets of target embeddings which represent social groups, denoted X and Y, and two sets of attribute embeddings which represent valence denoted A and B as described in Section 3. Each EAT gives an effect size d, whose magnitude indicates the strength of the bias, calculated as follows:\n$d = \\frac{mean_{x \\in X} s(x, A, B) \u2013 mean_{y \\in Y} s(y, A, B)}{std\\_dev_{w \\in X \\cup Y} s(w, A, B)}$\nwhere s is given by:\n$s(w, A, B) = mean_{a \\in A} cos(w, a) - mean_{b \\in B} cos(w, b)$,\nand $cos$ refers to cosine similarity, a distance metric used in quantifying associations by capturing information overlap between embeddings. We order the sets of stimuli such that a positive d value indicates a bias that is congruent with a stereotype that has been documented in society (i.e. flowers, instruments, women, European American, and young are more associated with pleasantness).\nIntrinsic Bias and Upstream Factors To investigate the relationship between intrinsic bias, measured by the EAT effect size (d), and various upstream factors, we employ a mixed effects regression model. The upstream factors considered include the log of parameter size (log(param)), model architecture (arch), pre-training dataset (dataset), and the log of dataset size (log(dataset size)). The model is specified as follows:\n$d_{ij} = B_0 + B_1 log(param)_{ij} + B_2 arch_{ij} + B_3 dataset_{ij} + B_4 log(datasetsize)_{ij} + u_{0j} + u_{1j} log(param)_{ij} + u_{2j} log(datasetsize)_{ij} + E_{ij}$\nwhere i indexes individual observations and j indexes groups defined by modality and test order combinations. Here, $B_0$ is the fixed intercept, while $B_1$ to $B_4$ are fixed coefficients for the predictors. The terms $u_{0j}$, $u_{1j}$ and $u_{2j}$ represent random intercepts and slopes for $log(param)$ and $log(dataset\\_size)$, capturing group-specific baseline d and variability in the effect of model size. The residual error is denoted by $E_{ij}$.\nSignificant fixed effects for upstream factors indicate their contribution to intrinsic bias. For example, a significant positive $B_3$ suggests that models trained on certain pre-training datasets exhibit higher intrinsic bias. The inclusion of random effects allows the model to account for unobserved heterogeneity across different groups, thereby enhancing the accuracy and generalizability of the estimates. Reproducibility details are provided in Appendix B.\nIntrinsic Bias and Downstream Performance We compute Pearson's correlation between intrinsic bias (EAT effect size d) and performance on the VTAB+ benchmark, considering zero-shot classification and captioning tasks relevant to each modality. Correlations are computed separately for each test category and modality combination in order to reveal modality-specific trends in the relationship between intrinsic bias and performance."}, {"title": "5 Experiments and Results", "content": "Following May et al. (2019) and Steed and Caliskan (2021), we compute the EAT effect sizes following the SEAT and iEAT methods across the 131 models and 26 tests for a total of 3,406 data points. Our analysis spans four modality combinations: All Text, All Image, Image as Target, and Text as Target, across various bias tests including \u2018Flower-Insect/Valence', 'Instrument-Weapon/Valence', 'Gender/Valence', 'Race/Valence', and 'Age/Valence' (as introduced in Section 3). The EAT effect sizes computed using stimuli from the original SEAT and iEAT studies demonstrate a high overall variance of 0.62. In an effort to offer more control in our experiments thereby making the effect sizes more comparable across models and reducing the impact of outliers, we recompute them across using the newly controlled and grounded attribute stimuli for our tests, drawn from the NRC-VAD lexicon (Mohammad, 2018) and OASIS (Kurdi et al., 2017) datasets.\nWe observe reduced effect size variance across models and tests to 0.59 overall (a 4.8% decrease) when replacing SEAT and iEAT attributes with our new attribute stimuli. The decrease in variance was particularly pronounced for the All Text modality (a 33.96% reduction in variance), which suggests that our set is less susceptible to noise and idiosyncrasies that may have plagued previous test sets. Furthermore, changing the stimuli shows better alignment with human stereotypes, showing a significant effect size (d >= 0.2) in 70.23% of the 3406 instances using the new stimuli, while this number is lesser at 67.88% using the old stimuli. We note that even after using more controlled stimuli, a high aggregate variance is not surprising, due to the scale and nature of the study. Our experiments in subsequent sections investigate this variation from known dataset choice and model architectural sources. Instead, by employing these carefully curated and grounded stimuli, we gain a clearer lens through which to examine the underlying biases present in various models. Consequently, we present all further results using these new stimuli, providing more robust and generalizable insights into bias in VLMs.\nAs with humans, 'Flower-Insect/Valence,' and 'Instrument-Weapon/Valence' show the largest effect sizes across modalities (d > 1), and associations between valence and social groups are weaker but still present. In all cases, the magnitude of effect sizes varies depending on the modality. Our findings indicate that biases in CLIP models generally align with those found in human assessments in 78.86% of the 3,406 cases (where d > 0). , we show that the direction of effect size across groups is consistent with the original SEAT and iEAT stimuli.\n also contains error bars that represent the standard deviation of effect sizes for the different test categories and modality combinations. We note that there is consistently lower variance for the non-human baselines, such as 'Flower-Insect/Valence' and \u2018Instrument-Weapon/Valence', compared to the variance observed in social bias categories like 'Age/Valence' and 'Race/Valence' indicating that these categories are inherently more susceptible to variability, likely due to the complexity and diversity of social concepts across different training datasets.\nRelationship between Intrinsic Bias and Upstream Factors We conducted a comprehensive mixed effects regression across the 3,406 observations within 16 different combinations of modality and EAT test category to understand how various upstream factors influence intrinsic bias, measured through the EAT effect size d. The model included random slopes and intercepts, effectively capturing high (\u03b2 = 0.46) group-level variability insights into both fixed and random effects across different combinations of modalities and test categories. For detailed and reproducible information regarding model specifications, variable definitions, and the experimental setup, see Appendix B.\nAs shown in , our findings reveal that dataset family plays a crucial role in determining the magnitude of intrinsic bias. Specifically, several dataset families, including 'dfn' (\u03b23 = 0.608), 'commonpool' (\u03b23 = 0.399), \u2018merged2b' (\u03b23 = 0.396), 'webli' (\u03b23 = 0.387), 'datacomp' (\u03b23 = 0.360), 'openai_wit' (\u03b23 = 0.351), \u2018laion' (\u03b23 = 0.333), and 'metaclip' (\u03b23 = 0.314) showed significant positive associations (p < 0.01) with intrinsic bias effect size with respect to the reference dataset of 'CC12m', chosen because we hypothesized its curation strategy would lead to the lowest levels of intrinsic bias. Marginal associations observed in 'yfcc15m' and \u2018CC12m' suggests that training on certain datasets contributes more to bias compared to others. This highlights the substantial influence of pretraining data on the biases present in the models.\nIn contrast, variations in model architecture (although having a positive direction of influence) had no statistically significant impact on intrinsic bias. None of the architectural families demonstrated a significant impact on effect size of bias compared to the reference category, suggesting that, at least within the scope of our study, architectural differences do not play a primary role in influencing bias. Additionally, \u2018log_params' and 'log_dataset_size'-did not exhibit significant effects on effect size of bias either.\nRelationship between Intrinsic Bias and Downstream Performance\nWe investigated the relationship between intrinsic biases measured in CLIP models and their performance on downstream tasks using the VTAB+ benchmark (Schuhmann et al., 2022) to understand how intrinsic biases in the models relate to their downstream performance across different modality combinations. Previous research has suggested that biases can influence model performance, particularly as models optimized for accuracy tend to learn and amplify societal biases (Hall et al., 2022).\nAmong the different test categories and modality combinations, we observed positive associations between intrinsic bias and downstream performance (meaning increased bias correlates to improved performance) for the non-human categories of 'Flower-Insect/Valence' and 'Instrument-Weapon/Valence' for All Image (r = 0.55, 0.81), All Text (r = 0.59, 0.71), Image as Target (r = 0.69, 0.75), and Text as Target (r = 0.44, 0.82) in addition to the human category 'Race/Valence' in the All Image combination (r = 0.35). These are shown in . We found negative correlations (meaning increased intrinsic bias correlates to worse performance) for the 'Gender/Valence' for Image as Target (r = -0.51) and All Text (r = -0.27). Insignificant correlations were observed primarily for 'Race/Valence' and 'Age/Valence' in various modalities and 'Gender/Valence' in the Text as Target modality."}, {"title": "6 Discussion", "content": "In this work, we explore how intrinsic biases and associations in VLMs are influenced by the interaction between modalities and upstream pretraining factors, and their impact on downstream task performance. Our findings highlight that the magnitude of bias effect sizes depends on the modality combination and test category being observed. However, across the board, the effect sizes are significantly influenced by dataset selection and correlate with model performance on downstream tasks. These insights highlight existing pitfalls in the data and training pipelines of VLMs with respect to fairness considerations and provide important implications for mitigating biases in the future development of these models.\nBias in Cross-Modal Interactions Our analysis reveals that intrinsic biases manifest differently across combinations of text and image modalities. The 'Flower-Insect/Valence' and 'Instrument-Weapon/Valence' tests show consistently high effect sizes across all modality combinations, indicating a strong association that is unaffected by modality. 'Gender/Valence' biases show positive effect sizes in the All Image setting, aligning with previous findings that women are associated with more positive terms than men (Caliskan et al., 2022; Charlesworth et al., 2024).\nThe representation of bias in models also varies substantially depending on the category of bias and the modality combination. Notably, for 'Age/Valence', the direction of the effect sizes differs based on the modality. When analyzing images, older individuals are associated with more positive valence, whereas in text, younger individuals tend to be more positively associated. This discrepancy highlights a critical aspect of modality-specific bias propagation, where the amount and type of information conveyed through visual modalities can differ from that in textual modalities, leading to distinct biases. Such modality-dependent differences in the representation of 'Age/Valence' suggest that crossmodal VLMs are influenced by biases specific to each modality. Text stimuli often use older names that are less frequent and therefore may not be represented accurately (Wilson and Caliskan, 2024), while images can convey richer, more nuanced information about age, potentially leading to different bias patterns.\nImpacts of Upstream Factors We focus on identifying which upstream factors\u2014such as dataset characteristics and model design decisions\u2014most significantly influence the intrinsic biases in CLIP models. By examining these biases from both unimodal and crossmodal perspectives, we aim to understand how different training inputs and model architectures contribute to intrinsic bias. This includes an in-depth analysis of how various datasets, including filtering strategies used to curate the selection of datasets, impact the emergence of biases.\nIn our analysis presented , we demonstrate that the choice of training dataset significantly impacts intrinsic bias, independent of other upstream factors such as model architecture or parameter count. We observe models curated with both automated (e.g. 'dfn' (Fang et al., 2023b)) and heuristic filtering strategies (e.g., dataset versions of 'commonpool' and 'datacomp' (Gadre et al., 2024)) to ensure high data quality and subsequent high downstream performance on tasks such as ImageNet accuracy exhibited significantly higher levels of bias, which is likely due to lack of consideration for equitable group identity representation in the dataset curation process.\nFiltering methods that rely on automated neural network-driven decisions (Fang et al., 2023b), while outperforming heuristic-based approaches in downstream tasks, tend to exacerbate societal biases even further. These results provide strong evidence that bias amplification often originates from the decisions made during the data curation phase, underscoring the need for more ethically-conscious dataset curation practices.\nOur findings align with suggestions from Gadre et al. (2024) to exercise caution when using models trained on these datasets to actively make decisions that impact people. One potential avenue for dataset-related bias mitigation in CLIP models could be replacing names with a generic \"person\" token, like in CC 12M (Changpinyo et al., 2021) which removed some social group signals contained in the dataset. We hypothesize this may have contributed to lesser bias observed in , but the full impact of hypernymization is still unclear and left for future work.\nArchitecture choice was found to be less impactful compared to dataset selection, which aligns with expectations since most of the text and image encoders in our study were transformer-based, with a few cases being CNN-based image encoders. The synthetic processing in these models was not extensive enough to introduce significant additional bias amplification, and the parameter count remained within a reasonable range without incorporating components, unlike more complex architectures involved in applications like text-to-image generation.\nDataset size was also not a significant contributor to bias, which contradicts the findings of Berg et al. (2022). Our findings indicate that simply increasing the model size or the size of the training dataset does not inherently mitigate or exacerbate intrinsic bias. Instead, other factors such as the composition and characteristics of the dataset are more critical in determining the level of bias.\nEffects on Downstream Performance Our investigation into the correlation between intrinsic biases and downstream task performance, as assessed by VTAB+, reveals significant modal dependencies. We demonstrate that higher intrinsic bias levels correlate with increased performance in downstream tasks across unimodal and crossmodal settings.\nThe 'Flower-Insect/Valence' and 'Instrument-Weapon/Valence' bias shows a high positive correlation across modality combinations suggesting that biases linked to non-human concepts may benefit from consistent training signals, improving model performance and that some associations are universally amplified in conjunction with downstream task performance improvement. For \u2018Gender/Valence' in the Image as Target (-0.506 r) and All Text (-0.273 r) settings, we observed negative correlations, implying that the associations with positive valence increased for the stereotype incongruent 'Men' group while model performance improves. This suggests that biases shift as models are further optimized, potentially reinforcing gender-specific stereotypes.\nThese findings indicate significant modal dependencies in how biases affect downstream task performance. The stark contrast between image-only and text-only settings, particularly test categories that involve social groups such as race and age, suggests that biases are not uniformly propagated across modalities but are instead highly dependent on the type of data and the specific tasks."}, {"title": "7 Conclusion", "content": "In this work, we conducted the largest analysis to date on the biases in vision-language models, examining 131 unique CLIP models across 26 datasets and 55 architectures. Our study highlights that the choice of dataset during pre-training, particularly those curated using automatic and heuristic-based filtering approaches that optimise downstream VLM performance, significantly influences intrinsic bias, reinforcing existing disparities. Additionally, we found that biases in models often correlate with improved downstream task performance, across modality settings, suggesting that the possibility that performance optimization can inadvertently amplify certain intrinsic biases as VLMs learn stronger associations between concepts. These findings emphasize the need for more ethically informed dataset curation and bias mitigation strategies to ensure fairer AI models. We release our code and data at https://github.com/kshitishghate/CLIP_bias."}, {"title": "8 Limitations", "content": "Further empirical studies are needed to compare a broader range of datasets and model configurations to provide a more robust statistical basis for our observations. Our analysis focuses on specific dataset families, model architectures, and parameter sizes, but an in-depth examination of dataset composition could offer more insights into mitigating biases effectively. Additionally, there is room for improvement in the measurements of bias using EATs. The stimuli we used are grounded in existing theories, but further controlling the stimuli, such as examining the frequency of stimuli composition (Wilson and Caliskan, 2024; Wolfe and Caliskan, 2021), could provide a more nuanced understanding of factors that impact bias effect size measures.\nAdditionally, we only considered monolingual English-based analyses of vision-language models, while with training datasets are curated using multilingual and multicultural sources such as 'webli' (Chen et al., 2022) and culture-specific biases present in those sources could also be inherited (Ruggeri et al., 2023). While our findings are expected to generalize broadly, extending the study to multilingual settings could yield valuable insights. Additionally, focusing primarily on well-established EATs like race, gender, and age leaves out a broader set of possible biases that could be explored in future work, such as those related to socio-economic status or intersectional identities. Limiting the scope of the analysis to these particular biases may risk oversimplifying the complex interrelationships of factors contributing to biased outcomes in VLMs."}, {"title": "Ethical Considerations", "content": "As vision-language models become increasingly employed in widespread scenarios, the potential for social impact, both positive and negative, grows with it. This study investigates biases within VLMs by explicitly focusing on how these biases are influenced by pre-training factors such as the choice of the training dataset, model architecture and parameter count. We also see how the instrinsic biases directly relate to a number of downstream zero-shot tasks that VLMs are employed for. By doing so, we aim to increase transparency and understanding of how biases are embedded and manifest in the application of VLMs, with the broader goal of promoting the development of fairer AI systems.\nThe potential applications of our findings include both the improvement and misuse of AI systems. Understanding how intrinsic biases relate to model performance could lead to targeted interventions to reduce bias. However, the same insights could also be used to amplify biases if misapplied. We caution against the use of biased models in high-stakes scenarios such as hiring, healthcare, or law enforcement, where even minor biases can lead to significant ethical consequences. Our intent is to inform researchers, developers, and policymakers of the importance of addressing biases during model development, especially when deploying models in sensitive areas.\nTo mitigate ethical risks, we advocate for more comprehensive evaluation and auditing frameworks that explicitly quantify and address biases across a diverse set of social categories. This should include incorporating multiple languages and cultural contexts, as well as addressing more diverse and intersectional group identities to ensure the broadest level of inclusivity. Moreover, we believe that transparency in dataset curation and pre-training processes is critical, and encourage the broader research community to prioritize the use of datasets that are both representative and ethically curated.\nLastly, we acknowledge that our own biases as researchers may influence the design and interpretation of our experiments. We strived for impartiality and accuracy, but we recognize that all research inherently carries subjective perspectives. We urge future researchers to build upon our work while expanding its ethical considerations, ensuring a more inclusive and equitable approach to AI development."}, {"title": "A Data", "content": "All code and data used in this study will be made available publicly."}, {"title": "A.1 Datasets Used by Contrastive Language Image Pre-training Models", "content": "Details concerning the datasets that were used for pre-training the CLIP models we study are provided below."}, {"title": "A.1.1 OpenAI WebImageText", "content": "The OpenAI WIT dataset (Radford et al., 20"}]}