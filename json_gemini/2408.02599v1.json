{"title": "Progressively Selective Label Enhancement for Language Model Alignment", "authors": ["Biao Liu", "Ning Xu", "Xin Geng"], "abstract": "Large Language Models have demonstrated impressive capabilities in various language tasks but may produce content that misaligns with human expectations, raising ethical and legal concerns. Therefore, it is important to explore the limitations and implement restrictions on the models to ensure safety and compliance, with Reinforcement Learning from Human Feedback (RLHF) being the primary method. Due to challenges in stability and scalability with the RLHF stages, researchers are exploring alternative methods to achieve effects comparable to those of RLHF. However, these methods often depend on large high-quality datasets and inefficiently utilize generated data. To deal with this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement for Language Model Alignment, a framework that fully utilizes all generated data by guiding the model with principles to align outputs with human expectations. Using a dynamically updated threshold, our approach ensures efficient data utilization by incorporating all generated responses and weighting them based on their corresponding reward scores. Experimental results on multiple datasets demonstrate the effectiveness of PSLE compared to existing language model alignment methods.", "sections": [{"title": "Introduction", "content": "Large Language Models, such as the LLama series (Touvron et al., 2023) and OpenAI's GPT series (Floridi & Chiriatti, 2020; OpenAI, 2023), have demonstrated their powerful capabilities across various language tasks, including translation (Zhang et al., 2023), summarization (Pilault et al., 2020), and conversational interaction (Wang et al., 2023a). In certain scenarios, they have even exhibited performance that matches that of human experts (Ouyang et al., 2022).\nHowever, these language models may not always generate text as expected by humans and can even produce content that violates human ethics or legal boundarie (Bai et al., 2022a; Askell et al., 2021). Therefore, it is important for researchers to explore the limitations of these models and implement restrictions on output generation to ensure safety and compliance, a process known as AI alignment.\nThe most prominent method for achieving AI alignment is Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022). RLHF employs"}, {"title": "Related Work", "content": "The alignment of language models refers to the process of ensuring that the models behave in ways that are consistent with human values, ethical principles, and intended purposes (Leike et al., 2018). The most prominent and effective methods currently used to achieve this alignment is Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022). The framework of RLHF first employs Supervised Fine-Tuning (SFT) to guide the model in following human instructions with an imitative manner (Wang et al., 2023b; Taori et al., 2023). The next steps involve training a Reward Model on a dataset reflecting human preferences, created from human evaluators' ratings of the SFT model's outputs (Ouyang et al., 2022). Using reinforcement learning algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017), the SFT model is further optimized by continuously generating outputs, receiving evaluations from the Reward Model, and updating its parameters to maximize alignment with the Reward Model (Askell et al., 2021; Bai et al., 2022b).\nHowever, due to the challenges of stability and scalability involved in the interactions between multi-ple models in RLHF, researchers have started exploring other more direct and efficient methods for"}, {"title": "Preliminaries", "content": "We first introduce the formal notation for the language model alignment problem. Let V be a vocabulary of a language model. The goal of alignment is to ensure that the language model \u03c0 : X \u2192 Y generates response y \u2208 Y that are consistent with human values and preferences given a query x \u2208 X, where the query x = [x1, x2, . . ., xm] and response y = [y1, y2, . . ., yn] are sequences of tokens, the input space X = Vm and the output space Y = Vn.\nThe alignment process typically begins with Supervised Fine-Tuning (SFT), which adjusts the language model using Maximum Likelihood Estimation on a human-labeled high-quality dataset Dsft = {(xi, Yi)}=1:\n\nLsft = \\frac{1}{N} \\sum_{i=1}^N \\sum_{j=1}^{n_i} log P(y_j | y_{<j}; \\theta),\n(1)\n\nwhere N is the number of training examples, ni is the length of the i-th target sequence, and \u03b8 represents the parameters of the language model \u03c0\u03b8. For the notational simplicity, y = \u00d8 denotes an empty placeholder.\nThe goal of language model alignment is to ensure that the model's responses to queries align with human preferences. These preferences are typically captured by a reward model R : (X,Y) \u2192 R, where higher scores indicate responses that better align with human values and preferences. Conversely, lower scores indicate less alignment. An ideal model maximizes the expected reward:\n\n\u03c0* = arg max\u03c0 Ex~p(x),y~\u03c0(\u00b7|x) [R(x, y)],\n(2)\n\nwhere \u03c0* represents the optimal policy that maximizes the expected reward according to the reward model R."}, {"title": "The Proposed Method", "content": "In this section, we present our novel framework named PSLE, i.e., Progressively Selective Label Enhancement for Language Model Alignment. As illustrated in Figure 1, during the sample generation phase, we use principles to direct the model's outputs. When the reward score difference between the principle-guided output and the original response exceeds a dynamically updated threshold, the model is encouraged to align with the better response and move away from the poorer one. If the difference is below the threshold, both responses are used in training since both are considered of similar quality."}, {"title": "PSLE", "content": "Language model alignment requires a large amount of high-quality data, but this is impractical in many scenarios. Therefore, we consider generating additional data during training to expand the dataset. Motivated by the self-align approach (Wang et al., 2023b; Sun et al., 2023), we design a set of principles to guide the model in generating responses that align closely with human preferences:\nwhich is denoted as p = [p\u00b9,\u2026\u2026\u2026, pr], where np is the token length of the principle prompt. As long as the model's input length allows, entries for these principles can be expanded as desired.\nLet it be the SFT-aligned model optimized by Eq. (1) and we use it as the initial model. During training, for each query x \u2208 Dquery = {xi}i=1, where N\u2081 is the number of queries, the model samples a response y ~ \u03c0\u03bf(x). In addition, the principle-guided model then samples a response: yprompt ~ \u03c0\u04e9([p, x]) based on the set of principles designed to ensure ethical, informative, and helpful output. The reward model R assigns the scores s = R(x, y) and sprompt = R(x, yprompt).\nWhen the difference between the reward scores, sprompt s, exceeds a threshold T, we consider that the current model has generated a better response based on the principles compared to the original response. Therefore, to encourage the model to generate responses closer to the better response and away from the poorer response for the given input x, we adopt a ranking loss. This ranking loss aims to adjust the model's parameters so that the likelihood of generating the better response is increased while the likelihood of generating the poorer response is decreased. The formula is as follows:\n\nCrank = \\sum_{s_{prompt} - s > T} \\pi_{\\theta}(y|x) - \\pi_{\\theta}(y^{prompt}|x).\n(3)\n\nWhen the difference between the reward scores, sprompt s, is less than or equal to the threshold T, we consider that the response generated by the principle-guided model and the original response are of similar quality. Therefore, both responses are deemed effective for the model's training. We include both responses in the dataset for subsequent training, with their weights determined by the magnitude of their scores. The formula for this process is as follow:\n\nLweighted-sft = \\sum_{s_{prompt} - s < T} (\\omega \\cdot \\pi_{\\theta}(y/x) + \\omega^{prompt} \\cdot \\pi_{\\theta}(y^{prompt}|x)),\n(4)\n\nwhere the weights w and wprompt are calculated as:\n\n\\omega = \\frac{e^s}{e^s + e^{s_{prompt}}},\\qquad \\omega^{prompt} = \\frac{e^{s_{prompt}}}{e^s + e^{s_{prompt}}}\n(5)\n\nThis approach ensures that both the original and the principal-guided responses contribute to the training process, with their influence proportional to their respective reward scores. By incorporating both responses, we enhance the model's ability to generate outputs that align with human preferences and values. Then the final objective function is:\n\nL = Lrank + Lweighted-sft\n(6)\n\nIn the training process, as the model's output scores for the original responses become increasingly close to the principle-guided responses, indicating the model's improved capability, we progressively reduce the threshold. This allows the loss function to adapt to these smaller variations. Here's how the threshold adjustment can be expressed:\n\nT_t = T_0 \\cdot \\alpha,\n(7)\n\nwhere Tt is the threshold at training step t, T0 is the initial threshold, and \u03b1 \u2208 (0,1) is a decay factor that progressively reduces the threshold over time.\nThe whole process of PSLE is shown in Algorithm 1."}, {"title": "Experiments", "content": "Experimental Configurations\nDatasets. We use Anthropic's Helpful and Harmless (HH) dataset as our experimental dataset (Bai et al., 2022a). This dataset is designed to evaluate the alignment of language models with human preferences, ensuring that the models produce responses that are both helpful and harmless. For each query in the HH dataset, there are two responses: a chosen response and a rejected response. The chosen response is preferred based on human evaluators' ratings, while the rejected response is deemed less appropriate or effective.\nBaselines. We compare our method with several existing language model alignment approaches, including:\nSFT (Ouyang et al., 2022): Supervised Fine-Tuning (SFT) trains the model by predicting the next token in a sequence based on a dataset of human-labeled examples to guide it towards desired outputs.\nPPO (Ziegler et al., 2019): Proximal Policy Optimization (PPO) is a reinforcement learning algorithm commonly used in the RLHF process. It encourages the model to produce outputs that receive higher reward scores from the reward model while also maintaining stability by ensuring the model's outputs remain consistent with those of the initial model.\nDPO (Rafailov et al., 2023): Direct Policy Optimization (DPO) simplifies the RLHF process by deriving an equivalent optimization objective of PPO. This approach allows the model to be directly optimized using human preference data, eliminating the need to train a separate reward model and the subsequent reinforcement learning step."}, {"title": "Main Results", "content": "The main results of our method and the baselines on the HH dataset are summarized in Table 1. For the PLL metric, since the training objective of SFT is aligned with the PPL metric, SFT achieves the best results on this metric. However, our method obtains comparable results to SFT. For the"}, {"title": "Conclusion", "content": "In this work, we addressed the challenges of aligning Large Language Models with human ex-pectations by proposing PSLE (Progressively Selective Label Enhancement for Language Model Alignment). Unlike existing methods that depend on large high-quality datasets and inefficiently utilize generated data, PSLE fully leverages all generated responses. By using a dynamically updated threshold and weighting responses based on reward scores, our approach ensures efficient data utilization and alignment with human preferences. Experimental results on HH dataset validate the effectiveness of PSLE, demonstrating its superiority over existing language model alignment methods."}]}