{"title": "PLASMA-CYCLEGAN: PLASMA BIOMARKER-GUIDED MRI TO PET CROSS-MODALITY\nTRANSLATION USING CONDITIONAL CYCLEGAN", "authors": ["Yanxi Chen", "Yi Su", "Celine Dumitrascu", "Kewei Chen", "David Weidman", "Richard J Caselli", "Nicholas Ashton", "Eric M Reiman", "Yalin Wang"], "abstract": "Cross-modality translation between MRI and PET imaging is\nchallenging due to the distinct mechanisms underlying these\nmodalities. Blood-based biomarkers (BBBMs) are revolu tionizing Alzheimer\u2019s disease (AD) detection by identifying\npatients and quantifying brain amyloid levels. However, the\npotential of BBBMs to enhance PET image synthesis remains\nunexplored. In this paper, we performed a thorough study on\nthe effect of incorporating BBBM into deep generative mod els. By evaluating three widely used cross-modality transla tion models, we found that BBBMs integration consistently\nenhances the generative quality across all models. By visual\ninspection of the generated results, we observed that PET im ages generated by CycleGAN exhibit the best visual fidelity.\nBased on these findings, we propose Plasma-CycleGAN, a\nnovel generative model based on CycleGAN, to synthesize\nPET images from MRI using BBBMs as conditions. This is\nthe first approach to integrate BBBMs in conditional cross modality translation between MRI and PET.", "sections": [{"title": "1. INTRODUCTION", "content": "Alzheimer\u2019s disease (AD) is a major neurodegenerative con dition affecting millions worldwide, with the number of pa tients and associated societal costs continually escalating. In\nthe A/T/N classification system for AD diagnosis, brain amy loid, tau pathology, and neurodegeneration are identified via\nPET scans using various tracers. Among them, amyloid PET\nscans detect brain amyloid deposition, which signifies an ele vated risk of AD clinical symptoms [1].\nDespite its accuracy, PET imaging\u2019s high cost, radioactiv ity exposure, and limited availability restrict its widespread\nuse. Alternatively, brain Magnetic Resonance Imaging (MRI)\nis a non-invasive, widely available tool that detects brain at rophy and has been employed in AD diagnosis. Therefore,\nsynthesizing PET images from MRI scans presents a promis ing strategy to reduce costs and minimize radiation exposure.\nNumerous researchers have utilized advanced image gen eration algorithms for cross-modality translation, such as\nGANs and their variants, for this task [2, 3, 4, 5, 6]. Cycle GAN, in particular, has been widely adopted due to its ability\nto handle unpaired datasets, albeit with some limitations in\nachieving pixel-wise accuracy [7]. Some noteworthy mod els for cross-modality translation have been developed based\non GAN and CycleGAN. For example, Jin et al. developed\nBPGAN, which employed gradient profile (GP) loss and\nstructural similarity index measure (SSIM) loss and achieved\nimprovement in SSIM on ADNI dataset [3]. Hu et al. pro posed a 3D end-to-end synthesis network named bidirectional\nmapping GAN (BMGAN) that learns a high-dimension em bedding of semantic information of PET images [5]. Re cently, the denoising diffusion probabilistic model (DDPM)\nhas emerged as state-of-the-art generative models. Li et al.\ndeveloped a diffusion model for pathology-aware MRI to\nPET cross-modality translation (PASTA), which can pre cisely generate pathology information [8]. Unfortunately,\nmost available methods are based on 2D generative back bones, such as 2D U-Net [9], 2D cGAN [10] and 2D ViT [4].\n2D generative models focus on convolution in 2D space,\nwhich lacks integrity in one of the dimensions. Therefore,\nthis study focuses on baseline algorithms for standard 3D\nimage inputs using publicly available code.\nRecently, blood-based biomarkers (BBBMs) have emerged\nas a promising, minimally invasive alternative for early detec tion of brain amyloid pathology. Available BBBMs mainly\ninclude plasma A\u03b242/40 and phosphorylated Tau family [11].\nNotably, the plasma A\u03b242/40 ratio has demonstrated strong\npotential in detecting brain amyloid burden and distinguish ing AD patients from healthy individuals [12, 13, 14]. How ever, the integration of BBBMs into cross-modality image\ntranslation models has not been thoroughly studied.\nThis study explored the effect of integrating BBBMs into\nthe MRI-to-PET translation models. By conditioning the gen"}, {"title": "2. METHODS", "content": "All data used in this study were obtained from the Alzheimer\u2019s\nDisease Neuroimaging Initiative (ADNI) and were freely\navailable online at the LONI Image and Data Archive (IDA)\ndata repository (https://ida.loni.usc.edu/). We downloaded\nand processed 1338 image instances of 456 individuals, in cluding 31 AD, 231 mild cognitive impairment (MCI), and\n194 cognitively normal (NL) individuals. For each individ ual, paired MRI and amyloid PET scan results and plasma\nA\u03b242/40 ratio from a blood test were available."}, {"title": "2.2. Data Preprocessing and Augmentation", "content": "Our baseline framework initially processed the input as 256 \u2217\n256 \u2217 256 3-D voxel cubes, encompassing co-registered MRI\nand PET image pairs. Practically, for more efficient training,\nwe downsampled the images to a size of 128\u2217128\u2217128 voxels.\nWe implemented a series of data augmentation techniques,\nincluding 1) additive Gaussian noise, 2) recursive Gaussian\nnoise, 3) random rotation around each axis, 4) random flip, 5)\nbrightness and contrast variation, and 6) translation. All aug mentation methods were under 3D space. During the training\ncycles, each input image was loaded and went through one or\nmore randomly selected augmentation steps."}, {"title": "2.3. Baseline Models", "content": "Pix2pix Baseline Pix2pix is a deep generative model for im age style translation proposed by Isola et al. in 2017 [15].\nWhile a GAN model learns a mapping from a random noise\nvector z to output image y: $G : z \\rightarrow y$, Pix2pix is based on\nconditional generative adversarial networks (cGANs), which\nlearns a mapping from the observed image x and a random\nnoise vector z to the output image y: $G : \\{x, z\\} \\rightarrow y$\nand has been proved to achieve sufficiently good results in\nmultiple tasks. The objective of cGAN can be expressed\nas: $L_{cGAN} (G, D) = E_{x,y}[logD(x, y)] + E_{x,z}[log(1 \u2212\nD(x, G(x, z)))]$, where G is the generator and D is the\ndiscriminator. Pix2pix model also integrates an L1 loss:\n$L_{L1}\n(G) = E_{x,y,z}[\\|y \u2212 G(x, y)\\|_{1}]$. The final objective is:\n$G^{*} = arg min_{G} max_{D} L_{cGAN} (G, D) + \\lambda L_{L1}(G)$\nCycleGAN Baseline CycleGAN was proposed in 2017\nand has been widely applied to image translation tasks [7]. a\nCycleGAN architecture, comprising two generators G1 and\nG2 and two discriminators D1 and D2. Any network with\nan encoder-decoder architecture can be used as a generator.\nCycleGAN facilitates a bidirectional mapping between sMRI\nand PET domains. Precisely, G1 mapped the MRI domain\n(XM) to the PET domain (XP ) and vice versa for G2, de noted as $G_{1} : X_{M} \\rightarrow X_{P }$, and $G_{2} : X_{P } \\rightarrow X_{M}$, respec tively. Generators G1 and G2 comprised 3-layer 3D CNNs on\nboth encoder and decoder sides, interspersed with 6 Resnet\nblocks. Each discriminator comprised 5 convolutional lay ers, followed by 3 fully connected layers. The loss function\nwas: $L(G_{1}, G_{2}, D_{1}, D_{2}) = L_{G}(G_{1}, D_{2}) + L_{G}(G_{2}, D_{1}) +\n\\lambda_{1}L_{C} (G_{1}, G_{2}) + \\lambda_{2}L_{C} (G_{2}, G_{1}) + \\lambda_{idt}L_{idt}$. Here, $L_{G}$ de noted the generator loss that enforced the similarity between\nreal and fake images through adversarial training against the\ndiscriminators. $L_{C}$ represented the cycle-consistency loss,\nand $L_{idt}$ was the identity loss that imposed an identity con straint. In our implementation, the learning rate scheduler\nwas replaced by a customized one, and the discriminator was\nmodified following the architecture of ShareGAN [16].\nShareGAN Baseline ShareGAN is an unsupervised\ncross-modal synthesis network proposed as a part of a joint\nlearning framework for AD diagnosis [16]. ShareGAN im plements an inter-conversion between 3D MRI and PET in\na single model. Essentially, ShareGAN has the same archi tecture as CycleGAN, but the parameters in the two gener ators are shared. The objective of the ShareGAN model is:\n$L = L_{GAN} + \\lambda_{Cycle}L_{Cycle} + \\lambda_{Ide}L_{Ide} + \\lambda_{Cls}L_{Cls}$, where\nboth GAN loss $L_{GAN}$ and cycle-consistency loss $L_{Cycle}$\nhave the same definition as in CycleGAN. Identity loss $L_{Ide}$\nforces the synthesis model to achieve an identity mapping if\nthe input is from the output domain, and is defined as: $L_{Ide} =$\n$E_{xp \\in X_{p}} \\|G_{p}(x_{p})\u2212x_{p}\\|_{1}+E_{xm \\in X_{m}}\\|G_{m}(x_{m})\u2212x_{m}\\|_{1}$, where\n$X_{m}$ and $X_{p}$ are MRI domain and PET domain, respectively.\nIn this study, we excluded the objective for AD classification\nby setting $\u03bb_{cls} = 0$."}, {"title": "2.4. Incorporating BBBMs", "content": "We included the BBBM information in our architecture by\nthree methods: 1) expanding normalized plasma A\u03b242/40\nlevel to the input image size and adding to the image. 2)\nexpanding normalized plasma A\u03b242/40 level to the size of\nthe latent feature map and adding to the feature map. 3) ex panding normalized plasma A\u03b242/40 level to the size of one\nchannel in latent feature map and concatenating with the fea ture map in latent space (Fig. 1). Specifically, the size of\nfeature maps after convolution layers was 128 \u2217 16 \u2217 16 \u2217 16.\nTo integrate BBBM, we took the normalized A\u03b242/40 levels\ncorresponding to the input images from the clinical data table\nand expanded the tensor to the same size as the feature map in\nour bottleneck. The resulting single-channel tensor was con catenated with the feature map along the channel dimension,\nresulting in a new feature map of size 129 \u2217 16 \u2217 16 \u2217 16.\nA 1x1 convolution layer was applied to reduce the feature\nmap\u2019s channel size back to 128 \u2217 16 \u2217 16 \u2217 16. We applied\nall three methods for each baseline model and generated the"}, {"title": "3. EXPERIMENTS AND RESULTS", "content": "With 1338 paired MRI and FBP PET images, We randomly\nsplit the dataset into 910 training, 242 validation and 186 test ing. The difference of validation and testing sets was to avoid\ndata leakage. Data leakage was avoided by keeping all images\nof one individual in the same subset. All models were trained\nusing a learning rate of 0.0002. The coefficients of the loss\nfunction were set to $\u03bb_{1} = \u03bb_{2} = 10.0$, and $\u03bb_{idt} = 0.3$ for Cy cleGAN and $\u03bb_{1} = \u03bb_{2} = 10.0$, and $\u03bb_{idt} = 0.5$ for ShareGAN,\nwhich achieved best generative quality, respectively. All mod els were implemented with Pytorch and trained on one Tesla\nA100 GPU for 100 epochs."}, {"title": "3.2. Quality Assessment of Generated Images", "content": "With 186 amyloid PET images generated from the test set,\nwe evaluated the quality using the most widely used simi larity metrics for generative models in computer vision field,\nincluding structural similarity index measure (SSIM), peak\nsignal-to-noise ratio (PSNR) and mean squared error (MSE).\nWe evaluated all three baseline models with and without in"}, {"title": "3.3. Standardized Uptake Value Ratio Analysis", "content": "Amyloid positivity is a key marker for Alzheimer\u2019s disease,\nwhich refers to a state where the amyloid plaque load in the\nbrain is above a certain threshold. Brain amyloid positivity is\nusually identified by the masked cerebellum standardized up take value ratio (MCSUVR). Therefore, in addition to SSIM,\nPSNR and MSE, measuring similarity of SUVR values be tween generated and true PET was also crucial. In this study,\nwe conducted two experiments on different SUVR similar ity measures, namely SUVR correlation and SUVR classifi cation.\nSUVR Correlation To evaluate the model performance\nwith and without BBBM, we calculated the MCSUVR val ues for each generated image. We assessed the correlation\nbetween the generated PET images and the ground truth by\ncalculating the Pearson correlation coefficients (PCCs). As\na result, PCCs were greater than 0.5 (0.770 \u2212 0.813), and p values were significant (< 3e\u221240) for all models, which indi cated a strong correlation between generated PET and ground\ntruth PET. Although the highest PCC was achieved\nby Pix2pix+concat model (P CC = 0.813), incorporating\nBBBM information enhanced the correlation in all models.\nSUVR Classification The diagnosis of preclinical AD is"}, {"title": "4. DISCUSSION AND CONCLUSION", "content": "In this study, we investigated the impact of incorporating\nBBBM data in generative models for MRI to PET cross modality translation. This is the first approach to combine\nBBBMs with generative models for cross-modality transla tion from MRI to PET. We trained three models with and\nwithout including BBBMs on a subset of the ADNI cohort"}, {"title": "5. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This research study was conducted retrospectively using hu man subject data available in ADNI repository and can be ac cessed through the LONI Image and Data Archive (IDA) data\nrepository (https://ida.loni.usc.edu/). As confirmed by the li cense attached to the open-access data, ethical approval was\nnot required."}]}