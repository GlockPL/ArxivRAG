{"title": "Next Steps in LLM-Supported Java Verification", "authors": ["Samuel Teuber", "Bernhard Beckert"], "abstract": "Recent work has shown that Large Language Mod-els (LLMs) are not only a suitable tool for code generation butalso capable of generating annotation-based code specifications.Scaling these methodologies may allow us to deduce provablecorrectness guarantees for large-scale software systems. In com-parison to other LLM tasks, the application field of deductiveverification has the notable advantage of providing a rigoroustoolset to check LLM-generated solutions. This short paperprovides early results on how this rigorous toolset can be used toreliably elicit correct specification annotations from an unreliableLLM oracle.", "sections": [{"title": "I. INTRODUCTION", "content": "Along with the pervasive success of Large LanguageModels [4] (LLMs) across many application domains, asteadily growing body of research is exploring the usage ofLLMs for the synthesis of specification annotations basedon a program's source code [3, 5, 7, 9, 11, 12, 14, 16, 17,18]. The appeal of using LLMs in this context comes fromthe \"NP-like\" imbalance in the problem formulation: Whileit is, in general, arbitrarily difficult to generate a correct,useful specification for a given piece of code, auto-activeverifiers [13], in principle, allow us to verify the correctnessof a given specification. The combination of LLM-basedspecification synthesis and deductive specification verificationhas now been validated in multiple case studies and has evenbeen applied for real-world programming languages such asJava and JML [10] using the verification tool KeY [1] (see [3]).In this light, LLM-based specification generation, as acanonical example of Intersymbolic AI [15], has the potentialto usher in a \"golden age\" of deductive verification in whichthe average software engineer can finally apply rigorous pro-gram analysis techniques to their large-scale software systemsand prove system correctness on their own. Generating amethod's top-level specification still requires human interven-tion to ensure the specification matches the requirements, butit has the potential to ease the workload to achieve provablecorrectness. This work focuses on the other promising application field, namely auxiliary specification generation. Here, thetop-level specification is still provided by a human and theLLM works under the supervision of an automated verifierwhich checks whether the LLM's annotations are sufficient toprove the top-level specification."}, {"title": "II. LLMS FOR AUXILIARY SPECIFICATION GENERATION", "content": "This work considers the specification of Java programsusing the Java Modelling Language (JML) [2]. JML allowsthe contract-based specification of Java methods and classesvia JavaDoc-like comments and is a supported specificationlanguage of multiple Java verifiers [1, 2, 6]. In this work,we made use of the KeY tool [1], which deductively verifiesthat a Java program adheres to a given contract-based JMLspecification. KeY comes with a built-in proof search strategythat allows automated verification and supports modular verification: Using the abstractions provided by method contracts,specifications of larger software systems can be decomposedinto verification tasks for individual methods.\nLLM-based synthesis of JML annotations: In priorwork [3], we proposed an architecture for combining(deductive) verification tools such as KeY [1] with LLM-based specification generation and evaluated our approach ona set of Java programs. In our approach, outlined in Figure 1,a partially annotated Java Program (1) is handed to an LLMwith instructions to complete the annotation. Subsequently,the annotations generated by the LLM are added to theprogram code, and the Annotation Draft (2) is handed toa verification tool, e.g. the Java verification tool KeY. IfKeY successfully verifies the correctness of the AnnotationDraft, we return the annotated file (3). If verification fails,we obtain a description of the error (4) from the verifier andnow require a strategy to recover from this error we willfurther discuss this part of the approach below.\nDepending on the application context, the guaranteesprovided by our approach can vary: If the Java Program (1)contains no prior annotations and the LLM extends the"}, {"title": "III. ERROR RECOVERY", "content": "In prior work [3], we used rudimentary error messages fromKeY to generate feedback for the LLM. Here, we can distinguish two kinds of failures: syntactical errors in the generatedspecification (e.g., missing semicolons, wrong use of variables,etc.) and semantical errors (e.g., a postcondition that is notsatisfied by the method's implementation). For syntacticalerrors, we provide the LLM with the parser's error message;for semantical errors, the LLM receives the (KeY-generated)labels of open proof branches. These labels, describe whichpart of the proof failed, e.g., the part where KeY tried toprove that a loop invariant is initially valid (\"Invariant InitiallyValid\") or that the post-condition holds when the value of anarray is not null(\"Normal Execution (_a != null)\").\nHowever, this raises the question of whether the providedfeedback is helpful for the LLM in correcting the specification.As a simple baseline comparison, we can use a sampling-basedapproach: Instead of providing feedback to the LLM and con-tinuing the same conversation string, we can restart the (prob-abilistic) process from scratch whenever an LLM-generatedspecifications cannot be verified. To this end, we reran the ex-periments from prior work for the synthesis of loop invariantsand subcontracts with the current version of GPT 40 and com-pared the previous feedback-based approach to a sampling-based approach, which does not provide KeY's feedback to theLLM. As a benchmark set, we used 27 tasks for JML invariantsynthesis and 14 tasks for the specification of called methods.The benchmarks cover a wide range of JML features includingarray usage, quantifiers, assignable clauses, reference to pre-execution values, etc. (see [3, Table 1]). For each instance, wesampled (i.e., restarted the process) up to 10 times. Figure 2provides a first overview of these results: Here, we see how therate of success increases with the number of feedback stepsresp. sampled solutions for invariant and subcontract synthesis.\nOn the one hand, the results show that an iterative processwith feedback increases the likelihood of success. This is apositive result because it means we are not in a situation"}, {"title": "A Mixed Approach", "content": "Based on the observations fromFigure 2, we hypothesize that the feedback-based approacheventually stagnates. Moreover, the feedback-based approachcannot be scaled to arbitrary step sizes due to the size of theLLM's context window which only admits limited conversation length. A logical step to scale our approach to even moreattempts is thus to mix the sampling-based approach with thefeedback-based approach. To this end, we can use the datagathered in our previous experiments to compare the successrate of 50 samples with the success rate of 5 samples each ofwhich has a 10-step feedback iteration. In both cases, up to50 solutions are evaluated. The results of the mixed approachcan be observed in Table I. Importantly, there is an overlapbetween the unsolved cases: the mixed approach solves allbenchmarks solved by sampling and two additional ones. Forinvariants, the additional solved benchmark is a loop invariantfor array reversal. This invariant is found by GPT 4O at the8th feedback step (in two out of five runs). For subcontracts,the instance only found by the mixed approach is a contractfor copying a range of an array. However, that contract wasfound in the first iteration, i.e., without any feedback.\nResults: As a positive result, our experiments provide evi-dence that iterative approaches (whether feedback or sampling-based) improve the LLM's success rate for annotation genera-tion. This insight suggests that more refined, iterative strategiescould lead to even stronger results which warrants furtherexploration. Our observation also strengthens the case for anintersymbolic approach that couples LLM generation with ver-ification as humans cannot be expected to inspect the LLM'smany solutions candidates. We also note that an approach solv-ing approx. 85% of JML invariant synthesis problems and 57%of submethod contract problems is unquestionably promising.\nLimitations: The shaded areas in Figures 2 and 3 indicatethat our results are not yet statistically significant. Although"}, {"title": "IV. DISCUSSION", "content": "This work presents our current methodology for evaluatingdifferent LLM prompting strategies for error recovery andpresents some first evaluation results on error recovery inLLM-based JML synthesis. We found that an iterative ap-proach indeed improves the success rate which strengthens theargument for our verification-based intersymbolic approach.For future work, we hope to repeat and extend the experimentspresented using larger benchmark datasets. More generally, wehope to extend our work in the following directions:\nSampling vs. Feedback: Unlike many other domains,specification generation has the advantage of providing a vastset of tools to check the correctness of solutions. In principle,we could thus \"simply\" generate a large number of solutionsand find the needle in the haystack, i.e., the one specificationthat is consistent with our partially annotated input file. Thereare of course limitations to this approach: When our codehas multiple annotation gaps, it may be necessary to fill allof them before verification, which quickly becomes work-intensive if an individual solution by the LLM has a lowchance of being correct. Hence, moving up the left end ofthe curve in Figures 2 and 3 is a highly desirable objective:We want to obtain successful specifications within the firstattempts as often as possible. We hope that an evaluation w.r.t.a recently released dataset [8] for JML annotation may help usbetter understand the advantages and drawbacks of differentprompting techniques in this context.\nFuture work will also focus on deriving textual represen-tations of failed proof attempts that are more effective asfeedback to the LLM. How this information can be extractedfrom KeY's proof tree is a nontrivial question.\nBeyond Verification: For the quick evaluation of specifi-cation candidates, we plan to explore additional checks using,e.g., testing/fuzzing before attempting verification. Counterex-amples could then also be used to devise feedback to the LLM.\nBeyond Filling Gaps: Our experiments evaluated theability of LLMs to fill the one gap in a partially JML-annotatedJava file. As a next step, we want to devise a strategy thatgenerates all specifications to prove a given top-level specifi-cation (as a first step, by generating all necessary submethodand loop-invariant specifications). This could greatly increasethe practicability of the approach."}, {"title": "A. LLM Prompts", "content": "Below we provide examples for the prompts used to elicitspecifications from the LLM.\n1) Invariant Generation: We invoke GPT with the follow-ing system message:\nYou are an assistant for JML annotation.\nIn the first message of this conversation, you areprovided with a Java class with partial JMLannotation.\nAdditionally, you are provided with natural language instructions that describe the taskto be completed.\nJML annotation is a complicated task, but you arevery capable and a perfect fit for this job.First, think step by step: What does the code do?What is the context in which the code isexecuted? What variables are relevant?Draft a behavioral description in natural language\nSubsequently, translate this description into JMLannotation.\nYou should only provide the JML annotation and notthe Java code.\nWe will then use the KeY verification system tocheck if the JML annotation is correct.If the program verification fails, you will beprovided with information about the failureand asked to correct the JML annotation.Always add the JML keyword 'normal_behavior' tothe contract this guarantees that noexceptions are being thrown.Your answers should always have the followingformat where the the is substituted by theJML annotation suggested by you:\n\n/*@  */\n\nSubsequently, we prompt GPT with the following prompt togenerate a loop invariant:\nGiven the following Java class:\n\"\"\"\n \n\"\"\"\nPlease provide a loop invariant for the loopconstruct with the comment '//Add invarianthere' of the method ' \nBeware this annotation has to be a loop invariant.A loop invariant typically has the followingstructure:\n/*@ loop_invariant ...;\n@decreases ...;\n@assignable ...;\n@*/\n\nFor feedback-based approaches, we distinguish between syntactical and semantical errors. For syntactical errors, we usethe following prompt to continue the conversation:\nThe provided code is not valid JML. Please tryagain and make sure to provide a valid JMLloop invariant.This might describe the reason why change isrequired:Beware that the error message may contain variablenames from inside the proof system that arenot available in the original code.Such variable names must not in your JML answer;only use variable names from the original code \nAn example for a parser error message could be:\nError during JML parsing: Failed to parse JMLfragment: Encountered unexpected token: \"(\"\"(\"at line 3, column 7.\nWas expecting one of: \nFor semantical errors we use the following prompt:\nThe provided JML does not solve the task. The KeYverification told me that the JML is wrongbecause some proof goals were not closed:Beware that the error message may contain variablenames from inside the proof system that arenot available in the original code.Such variable names must not in your JML answer;only use variable names from the original codeDuring verification, the following proof branchescould not be closed:\nPlease fix the JML loop invariant.Examples for a proof branch label could be:\nNormal Execution (a != null)\nBody Preserves Invariant\nUse Case"}, {"title": "2) Submethod Contract Generation", "content": "We invoke GPT withthe following system message:\nYou are an assistant for JML annotation.\nIn the first message of this conversation, you areprovided with a Java class with partial JMLannotation.\nAdditionally, you are provided with naturallanguage instructions that describe the taskto be completed.\nJML annotation is a complicated task, but you arevery capable and a perfect fit for this job.First, think step by step: What does the code do?What is the context in which the code isexecuted? What variables are relevant?Draft a behavioral description in natural language\nSubsequently, translate this description into JMLannotation.\nYou should only provide the JML annotation and notthe Java code.\nWe will then use the KeY verification system tocheck if the JML annotation is correct.If the program verification fails, you will beprovided with information about the failureand asked to correct the JML annotation.Always add the JML keyword 'normal_behavior' tothe contract this guarantees that noexceptions are being thrown.Your answers should always have the followingformat where the the is substituted by theJML annotation suggested by you:\n\n/*@  */\n\nSubsequently, we prompt GPT with the following prompt togenerate a contract:\nGiven the following Java class:\n\"\"\"\n \n\"\"\"\nPlease provide a JML annotation to the method 'such that the contractspecified by '' is satisfied.A contract for a submethod typically has thefollowing structure:\n/*@ normal_behavior // Ensures that no exceptionsare thrown by submethod@requires ...; // Requirements for aninvocation of the submethod, based on thecalling method.@ensures ...; // Guarantees that the submethodwill satisfy after the invocation.@assignable ...; // Fields that are assignablein the called method.@*/\n\nFor feedback-based approaches, we distinguish betweensyn- tactical and semantical errors. For syntactical errors, weuse the following prompt to continue the conversation:\nThe provided code is not valid JML. Please tryagain and make sure to provide a valid JMLcontract.This might describe the reason why change isrequired:Beware that the error message may contain variablenames from inside the proof system that arenot available in the original code.Such variable names must not in your JML answer;only use variable names from the original code \nWas expecting one of: Always add the JML keyword 'normal_behavior' tothe contract this guarantees that noexceptions are being thrown.An example for a parser error message could be:\nError during JML parsing: Failed to parse JMLfragment: Encountered unexpected token: \"loop_invariant\" \"loop_invariant\"at line 9, column 5.\nFor semantical errors we use the following prompt:\nThe provided JML does not solve the task for themethod ''.\nThe KeY verification told me that the JML is wrongbecause some proof goals were not closed:Beware that the error message may contain variablenames from inside the proof system that arenot available in the original code.Such variable names must not in your JML answer;only use variable names from the original codeDuring verification, the following proof branchescould not be closed:\nPlease fix the JML contract.Always add the JML keyword 'normal_behavior' tothe contract this guarantees that noexceptions are being thrown.Examples for a proof branch label could be:\nPost ()"}]}