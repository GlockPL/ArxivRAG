{"title": "Guess What I Think: Streamlined EEG-to-Image Generation with Latent Diffusion Models", "authors": ["Eleonora Lopez", "Luigi Sigillo", "Federica Colonnese", "Massimo Panella", "Danilo Comminiello"], "abstract": "Generating images from brain waves is gaining increasing attention due to its potential to advance brain-computer interface (BCI) systems by understanding how brain signals encode visual cues. Most of the literature has focused on fMRI-to-Image tasks as fMRI is characterized by high spatial resolution. However, fMRI is an expensive neuroimaging modality and does not allow for real-time BCI. On the other hand, electroencephalography (EEG) is a low-cost, non-invasive, and portable neuroimaging technique, making it an attractive option for future real-time applications. Nevertheless, EEG presents inherent challenges due to its low spatial resolution and susceptibility to noise and artifacts, which makes generating images from EEG more difficult. In this paper, we address these problems with a streamlined framework based on the ControlNet adapter for conditioning a latent diffusion model (LDM) through EEG signals. We conduct experiments and ablation studies on popular benchmarks to demonstrate that the proposed method beats other state-of-the-art models. Unlike these methods, which often require extensive preprocessing, pretraining, different losses, and captioning models, our approach is efficient and straightforward, requiring only minimal preprocessing and a few components. Code will be available after publication.", "sections": [{"title": "I. INTRODUCTION", "content": "Advancing the Brain-Computer Interface (BCI) by understanding how the human brain represents the world is central to neurocognitive research. Indeed, BCIs have the potential to revolutionize areas such as healthcare, ranging from prevention to rehabilitation of neuronal injuries, as well as education and entertainment [1]. Among these advancements, some areas have been widely studied, such as emotion recognition [2]-[4] and mental workload prediction [5]. More recently, with the advancement of generative models, the reconstruction of visual stimuli from brain signals, a task that had previously stagnated due to the limitations of earlier methods such as Generative Adversarial Networks (GANs), has resurfaced. Early neurocognitive works have shown that brain waves retain information about object structures from visual cues [6]. Building on this, many studies have since developed methods to reconstruct images from functional MRI (fMRI) signals [7]\u2013[9]. Thanks to its high spatial resolution and the generation abilities of diffusion models, these approaches are achieving increasingly accurate images. However, fMRI is cost-prohibitive because of the expensive equipment needed, and its lack of portability makes it unsuitable for real-time BCI systems [10]. On the other hand, EEGs capture electrical activity in the brain through electrodes placed on the scalp, offering high temporal resolution. EEG is a portable, non-invasive, and low-cost neuroimaging technology, making it an appealing candidate for brain-to-image reconstruction and real-time applications [11]. Nonetheless, this task is inherently challenging, even with the high spatial resolution of fMRI data, and using EEG presents even more difficulties. In fact, EEGs are highly susceptible to noise, resulting in a very low signal-to-noise ratio (SNR), with artifacts frequently caused by factors such as electrode misplacement or body movement [11]. Studies addressing this task through EEG face three main drawbacks. First, they require extensive preprocessing that demands domain knowledge [12]. Second, they rely on outdated generative models, such as GANs, which have been surpassed by more advanced diffusion models [13], [14]."}, {"title": "II. RELATED WORKS", "content": "Advancements in generative methods have made it possible to reconstruct external stimuli, such as audio [19], images [20], and video [21], from brain signal recordings. Given the high spatial resolution of fMRI data, numerous studies have investigated the use of fMRI to reconstruct visual stimuli [7], [8], [20]. EEG, by contrast, is more accessible and economically viable than fMRI but it also presents more challenges, e.g., low SNR and spatial resolution. Many works exploring EEG-to-Image tasks are based on GANs [13], [14], [22], [23]. In contrast, studies focused on reconstructing visual information from fMRI have shown highly promising results by leveraging diffusion models [7], [8], [20], [21]. Indeed, diffusion models have achieved significant success across various tasks, including image generation [9], super-resolution [24], and audio generation [25]. Following this advancement, recent research has begun to develop diffusion-based methods for reconstructing images from EEG. Among these, many studies propose using alignment losses to mitigate the semantic gap between EEG and text/image data, developing EEG encoders that require pretraining on a large dataset [12], [15], [16], [26], as well as integrating captioning models and silhouette extraction networks [10], [15]. Moreover, many methods require extensive preprocessing that demands significant domain knowledge [12], [15], [16], whereas a recent approach has shown that excessive preprocessing can actually hinder image decoding performance [14]."}, {"title": "III. PROPOSED METHOD", "content": "We propose a simple framework, GWIT, comprising ControlNet as a mechanism for controlling a latent diffusion model [9] using EEG signals.\nProblem formulation. Let {y, x} be a pair from the dataset, where y \u2208 \u211d^{C\u00d7L} is an EEG signal with C channels and L time steps, and x \u2208 \u211d^{H\u00d7W\u00d73} is the corresponding image with height H, width W, and 3 channels. The overall framework involves processing the image x with the LDM in the standard manner, while the ControlNet adapter handles the input conditioning EEG y, as it is able to adapt diverse modalities for conditioning [8], [17], [18]. Additionally, we add a coarse-grained control c_l, i.e., a caption of the form \"Image of [label]\", following the approach of [12]. The label is predicted by a pretrained (frozen) EEG image decoder. The framework is then trained following the original ControlNet formulation, where the weights of the UNet encoder are copied and trained in an efficient way.\nConditioning. The image is processed by the (pretrained) LDM, where it is mapped into a latent code z_{img} \u223c \u03b5_{VAE}(x) via the VAE stochastic encoder, with z_{img} \u2208 \u211d^{H_z\u00d7W_z\u00d7D}, where H_z and W_z are the spatial dimensions and D the number of latent channels. Then, noise is progressively added through a forward Gaussian process indexed by t\u2208 [0,T] to obtain z_{img}^t. The EEG is mapped into the same latent space of z_{img} via a simple 1D convolutional neural network and reshape operations, i.e., z_{eeg} = f_{proj}(y), where f_{proj}: \u211d^{C\u00d7L} \u2192 \u211d^{H_z\u00d7W_z\u00d7D}. Then, z_{eeg} is passed thorugh a 1 \u00d7 1 zero convolution layer and summed with z_{img}^t, i.e., the control input to ControlNet is c_{eeg} = z_{img}^t + Z(z_{eeg}), with Z the zero-initilized convolution. We apply zero convolutions following the approach of ControlNet, as they prevent harmful noise from affecting the LDM backbone during the initial training steps [17]. Then, the control input is processed by the ControlNet block. The adapter is defined as a trainable copy of the encoder of the underlying UNet architecture of the LDM which implements the diffusion model. That is, assuming the LDM with a UNet backbone formed by an encoder \u03b5_\u03b8 and a decoder D_\u03b8, the ControlNet adapter is defined as an encoder with a trainable copy of the weights \u03b5_{\u03b8'}. Thus, given input image x, the corresponding noisy latent code z_{img}^t, and a set of conditions including the time step t, the coarse control c_l and the EEG control c_{eeg}, they are processed by the ControlNet \u03b5_{\u03b8'} adapter as:\nControlNet(c_{eeg}, c_l, t) = \u03b5_{\u03b8'}(c_{eeg}, c_l, t) = \u03b5_{\u03b8'}(z_{img}^t + Z(z_{eeg}), c_l, t) = \u03b5_{\u03b8'}(z_{img}^t + Z(f_{proj}(y)), c_l, t). (1)\nWhen training on multiple subjects we include a linear layer S(y, s) that encodes information on the subject by feeding it the subject id s \u2208 \u2115, as done in [18], [27]. In this case, the EEG is processed as f_{proj}(S(y, s)).\nTraining. The training loss used by ControlNet is identical to the original LDM loss, with two key differences: it includes an additional task-specific input condition, in this case, the EEG signal, and the UNet backbone remains frozen. Only the weights of the ControlNet adapter \u03b5_{\u03b8'} and f_{proj} are updated as follows:\n\u2112 = \u03b5_{z_{img},z_{eeg},c_l,t,\u03f5\u223c\ud835\udca9(0,1)} ||\u03f5 - \u03b5_\u03b8(z_{img}^t, z_{eeg}, c_l, t)||^2, (2)\nwhere \u03f5 is implemented by the UNet backbone and learns to predict the progressively added noise to the image. In this way, it can decode the image using the VAE-based decoder, i.e., the final generated image is x = D_{VAE}(z_{img}), where z_{img} is the denoised sample of z_{img}^t \u223c \ud835\udca9(0, 1)."}, {"title": "IV. EXPERIMENTS", "content": "Datasets. We evaluate our method with the EEGCVPR40 [34], [35] and ThoughtViz [36] benchmark datasets. The first contains EEG recordings from 6 subjects who were shown 50 images for each of 40 classes from the ImageNet dataset [37]. EEGs were recorded for 0.5s at 1 kHz following the 128-channel system. We used the official training, validation, and test sets. Instead, ThoughtViz contains recordings of EEG of 10s sampled at 128Hz with 14 electrodes from 23 participants. The samples were split into chunks of 32 time steps with overlap [23]. We employed the subset of EEGs relative to images ranging in 10 classes. For preprocessing, we apply only standardization following [14] and directly employ raw EEG data.\nImplementation. For the projection f_{proj} we use 1D convolutional layers with (320, 640, 1280, 2560) channels and strides (5, 2, 2, 2). Finally, we apply padding and reshaping to map the EEG conditioning to the same dimension as the latent image. We employ Stable Diffusion [9] as LDM, using the weights of the 2.1 version, and the LSTM EEG imade decoder proposed in [14]. The model, i.e., the ControlNet adapter and f_{proj} are trained for 100 epochs with Adam and a learning rate of 1e - 5. During training, we drop the coarse control for half of the samples to further emphasize the EEG conditioning [17]. Lastly, for sampling the generated image we employ the guess mode of ControlNet which enforces the model to prioritize the conditioning over the coarse control [17].\nMetrics. We evaluate the generated images in terms of generation quality with Fr\u00e9chet inception distance (FID) and inception score (IS). Moreover, we evaluate the semantic accuracy of generated images with N-way Top-k classification accuracy (ACC) following [7], [10], [15]. This metric evaluates whether the original visual cue and the generated image are assigned to the same class by a pretrained ImageNet classifier, i.e., a ViT [15]. Finally, we employ the Learned Perceptual Image Patch Similarity (LPIPS) to measure the similarity between original and generated images.\nResults. For the EEGCVPR40 dataset, we train two variants of our method, i.e., a single and multi-subject model. For the first, the model is trained on EEGs corresponding to subject 4 [15], [16]. Instead, the multi-subject model is trained on the whole dataset, as is done in the ThoughtViz experiments. The quantitative evaluation of these results is reported in Tab. I. Our streamlined approach GWIT achieves state-of-the-art results in every scenario, in both generation quality and semantic correctness. In particular, we improve the semantic accuracy by a great margin, i.e., by 85.71% and 7% for single and multi-subject variants, respectively. This result is highly significant as this metric directly measures if the images generated from EEG signals are semantically correct with respect to the original visual cues, which is the most crucial aspect. Finally, in Fig. 2 and Fig. 4 we present the qualitative results on EEGCPVR40, while in Fig. 3 we show results on ThoughtViz. The images generated with our method demonstrate superior quality compared to other models. Moreover, we attain semantic accuracy with respect to original visual cues as observed by comparing the generated images with samples of the original dataset in Fig. 3 and Fig. 4.\nAblations. In Tab. II we conduct an ablation study to investigate the influence of the EEG control. Specifically, we demonstrate that images generated with EEG conditioning reach a lower LPIPS, i.e., they are closer to the \"ground truth\" image, compared to images generated with only coarse control. This shows that the EEG actually guides the diffusion model to generate images semantically closer to the original visual stimuli. Lastly, in Tab. III we investigate how the \"drop\" and \"guess\" modes utilized during training and inference respectively allow to prioritize the EEG conditioning. Lower LPIPS and higher accuracy indicate that the model is actually following the EEG conditioning, as they are related to distance and semantic correctness with respect to the original visual cue. We find the guess mode to be much more influential than the drop mode, however also the latter leads to an improvement."}, {"title": "V. CONCLUSION", "content": "In this paper, we have explored the use of the ControlNet adapter to handle EEG data for conditioning a latent diffusion model, allowing the reconstruction of images from EEG signals. We have developed a streamlined method that requires minimal preprocessing, no pretraining, and efficient fine-tuning, with the goal of moving towards real-time BCIs, while surpassing state-of-the-art methods that rely on very complex frameworks."}]}