{"title": "Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation", "authors": ["Xiaoxiong Zhang", "Zhiwei Zeng", "Xin Zhou", "Zhiqi Shen"], "abstract": "Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative learning of entity and relation embeddings from distributed Knowledge Graphs (KGs) across multiple clients, while preserving data privacy. Training FKGE models with higher dimensions is typically favored due to their potential for achieving superior performance. However, high-dimensional embeddings present significant challenges in terms of storage resource and inference speed. Unlike traditional KG embedding methods, FKGE involves multiple client-server communication rounds, where communication efficiency is critical. Existing embedding compression methods for traditional KGs may not be directly applicable to FKGE as they often require multiple model trainings which potentially incur substantial communication costs. In this paper, we propose a lightweight component based on Knowledge Distillation (KD) which is titled FedKD and tailored specifically for FKGE methods. During client-side local training, FedKD facilitates the low-dimensional student model to mimic the score distribution of triples from the high-dimensional teacher model using KL divergence loss. Unlike traditional KD way, FedKD adaptively learns a temperature to scale the score of positive triples and separately adjusts the scores of corresponding negative triples using a predefined temperature, thereby mitigating teacher over-confidence issue. Furthermore, we dynamically adjust the weight of KD loss to optimize the training process. Extensive experiments on three datasets support the effectiveness of FedKD.", "sections": [{"title": "1 Introduction", "content": "A knowledge graph (KG) describes real-world facts in the form of triples (head entity, relation, tail entity). Knowledge graph embedding (KGE) aims to encode entities and relations in the KG into continuous vector representations which capture the semantic meanings and relationships inherent in the graph structure, enabling various downstream tasks such as disease diagnosis [1, 21], recommendation system [28, 29, 32], question answering system [10] and so on.\nWith the promulgation of General Data Protection Regulation (GDPR) [20], KGs from multiple sources are no longer stored centrally on one device as a whole KG, but instead, in a more decentralized manner on multiple clients. Formally, these distributed multi-source KGs are referred to as federated knowledge graphs (FKG). A major line of research in FKG is federated learning-based Federated Knowledge Graph Embedding (FKGE) [5, 6, 19, 35]. Through federated learning [3, 22, 25, 31, 33], each client obtains useful information from other clients. The aggregated information, together with local triples, is used to update the entities and relations embeddings. The final learned entities and relations embeddings are used to predict non-existent triples for each client.\nTo enhance the performance of learned embeddings for FKG, it is common practice to increase embedding dimension during the training phase. However, pre-training high-dimensional embeddings is often impractical in many real-world downstream scenarios, and the low-dimensional embeddings are necessary [40]. In some cases, the storage capacity and computational resources are limited (such as deploying learned embeddings from FKG on edge computing devices or mobile platforms) while fast inference speed is required (such as online financial systems demanding fast market decisions). Besides, some cases need to further fine-tune the pre-trained embedding of FKG rather than using them directly to address downstream tasks (such as recommendation system) effectively, also expecting to keep the cost of fine-tuning as low as"}, {"title": "2 Related Work", "content": "Federated Learning (FL) represents a novel distributed paradigm in machine learning that enables multiple clients to collaboratively train high-performing models, simultaneously preserving data privacy and security [4, 14, 15, 24, 34]. Based on the FL paradigm, federated knowledge graph embedding aims to facilitate collaborative learning of entity and relation embeddings from multi-source KGs distributed across multiple clients, while ensuring raw triplets are kept confidential and not shared among participants.\nFedE [5], inspired by FedAvg [18], serves as a pioneering model where a server coordinates clients' local embedding learning by iteratively aggregating their entity embeddings and distributing the aggregated result for the subsequent round of local training. Inspired by Moon [13], FedEC introduces embedding contrastive learning into FedE to further improve embedding performance. However, both FedE and FedEC generate a single set of global consensus embeddings for all clients, which may not fully accommodate data heterogeneity. As a result, the embeddings trained to fit an \"average KG\" might not generalize well to KGs that exhibit high heterogeneity with the global training dataset. To address it, several personalized FKGE methods are proposed. PFedEG proposed the personalized embedding aggregation mechanism on the server based on a client-wise relation graph [37]. FedLu introduces mutual knowledge distillation to facilitate the transfer of information from local entity embeddings to global entity embeddings, followed by the incorporation of knowledge gleaned from the global entity embeddings [38]. In contrast to the aforementioned methods that predominantly handle scenarios where clients possess partially shared entities but mutually exclusive relations, FedR [35] specifically addresses cases where clients not only share entities but also share relations. In this framework, all clients receive identical embeddings of shared relations from the server and subsequently engage in local embedding learning using their respective local triples together with the shared relation embeddings.\nUnlike the aforementioned methods that rely on a client-server architecture, FKGE [19] facilitates collaborative embedding learning among clients in a peer-to-peer manner. Drawing inspiration from MUSE [7], FKGE adopts a Generative Adversarial Network"}, {"title": "2.1 Federated Knowledge Graph Embedding", "content": "Recently, there are several knowledge-distillation-based methods [17, 27, 40] to compress high-dimensional embeddings to low-dimensional ones for KG while there is little research about embedding compression specifically for FKG. The embedding learning process for FKGE is distinct from that of traditional KGE. For FKGE, the process typically entails multiple rounds of communication between clients and a central server. Due to constraints such as limited network bandwidth and data usage on clients, minimizing the transmission of parameters during FKGE training is crucial. Since there is no concern about communication efficiency in traditional KGE, existing KGE embedding compression techniques are often complex and involve multiple model trainings. For example, the method in [27] trains multiple high-dimensional teacher models and use them to teach the low-dimensional student model collaboratively. The approach described in [17] employs iterative knowledge distillation to progressively decrease the model sizes to a desired target, which aims to mitigate the adverse effects of substantial dimension gap between the student and the teacher model on the performance of knowledge distillation. During the process, multiple models of intermediate sizes are trained. As a result, these methods adopted in KGE usually are unsuitable for direct application in FKGE due to the significant communication costs they potentially entail. Therefore, it is necessary to develop more efficient and lightweight embedding compression methods tailored specifically for FKGE.\n\u2022 We introduce a lightweight component, FedKD, designed for existing FKGE methods, which aims to reduce embedding size without substantial compromise to embedding performance. To the best of our knowledge, this is the first method tailored specifically for embedding compression in FKGE.\n\u2022 We propose an Adaptive Asymmetric Temperature Scaling mechanism aimed at mitigating the adverse impact of teacher model over-confidence in positive samples on the performance of on knowledge distillation. Besides, we employ dynamic weight adjustment to alleviate the optimization disparity between the hard and soft label loss.\n\u2022 We assess the efficacy of FedKD across three datasets through comprehensive experiments. The findings indicate a notable reduction in the embedding dimension of FKGE with with only marginal (even no) decrease in performance."}, {"title": "2.2 Knowledge Graph Embedding Compression with Knowledge Distillation", "content": "Knowledge distillation (KD) is a prominent technique employed in model compression [9, 30, 39]. In this approach, a larger, pre-trained model, referred to as the teacher model, provides guidance to a smaller model, known as the student model. The objective is for the student model to approximate the outputs of the teacher model, thereby achieving similar performance [11].\nMany embedding methods demonstrate enhanced performance with higher embedding dimensions. However, the trade-off includes longer inference times and larger model sizes for high-dimensional Knowledge Graph Embedding. Recently, several knowledge distillation methods [17, 27, 40] have been proposed to compress high-dimensional embeddings into lower-dimensional representations for knowledge graphs. MulDE[27] employs multi-teacher distillation techniques aimed at improving the efficacy of low-dimensional models. However, it does not adequately address the dimension gap issue, which can significantly affect the effectiveness of the distillation process. DualDE [40] presents an innovative two-stage distillation approach where only a high-dimensional teacher model is pre-trained. Initially, the teacher model remains fixed while only the student model is updated under its guidance during the first stage. Subsequently, in the second stage, both the teacher and student models reciprocally guide each other's embedding updates. Despite involving only a pre-trained teacher model, DualDE still encounters significant training costs due to the necessity of updating the teacher model during the second stage [17]. IteDE [17] introduces an iterative knowledge distillation approach aiming at progressively reducing model sizes to a predefined target. Throughout this process, multiple models of intermediate sizes are trained under the guidance of adjacent higher-dimensional models. Essentially, it remains within the realm of multiple-teacher strategies. These methods typically prioritize enhancing the performance of low-dimensional embeddings with limited regard for the associated training costs.\nHowever, in the context of FKGE, the training procedure typically involves iterative communication between clients and a central server. Given constraints such as limited network bandwidth and client-side data usage, minimizing parameter transmission during FKGE training is crucial. From this perspective, the methods commonly employed in traditional KGE may not be directly applicable to FKGE due to their potential for significant communication costs. Hence, there is a need to devise more lightweight embedding compression techniques tailored specifically for FKGE."}, {"title": "3 Preliminaries", "content": "In this section, we explain the preliminary knowledge about KG, FKG, FKGE, FedE (the pioneering FKGE method) and the task of FKGE Compression based on Knowledge Distillation.\nA Knowledge Graph (KG) G with entity set & and relation set R is defined as G = {(h,r,t) | h,t \u2208 &, r\u2208 R}, where each triplet (h, r, t) represents the fact that the head entity h has relation r with the tail entity t.\nFederated Knowledge Graph (FKG) is a set of KGs which are stored in multiple clients for the aim of data privacy. Formally, the definition of FKG is as follows:\nDefinition 3.1. Federated Knowledge Graph: Given C clients, each client holds a knowledge graph Gc = {&c, Rc, Tc | c\u2208 [0, C]}, and the entity set in these C knowledge graphs can overlap, but the relation sets are mutually exclusive. The set of these C knowledge graphs is called Federated Knowledge Graph: GF = {Gc}C=1\nFederated Knowledge Graph Embedding (FKGE) aims to let all KGs in FKGE collaboratively engage in embedding learning to achieve enhanced embeddings with improved performance, without exposing raw triples to each other. This collaborative process leverages the presence of shared entities among clients' KGs, allowing each client to contribute additional semantic information to others.\nFedE is the first FKGE method which mainly includes two parts: client update and server update. We take the communication round t as an example to explain them, considering the different rounds share the same process. For server updates, the server first collects the updated entity embeddings at round t 1 from each client. Subsequently, the server performs an average embedding aggregation for each entity. For client updates, each client initially receives the aggregated embeddings from the server and replaces their local entity embeddings with these updated versions. Following this, each client proceeds with local embedding learning using a KGE method. The objective of local embedding learning is scoring positive triples higher than negative ones based on the score function of the chosen KGE method.\nFKGE Compression based on Knowledge Distillation (FKGE-CD) aims at reducing the embedding dimension for a FKGE method using knowledge distillation techniques, and simultaneously avoiding decrease embedding performance significantly. In this paper, we further assume that the FKGE-CD training process should follow the existing client and server communication mechanisms, and it should only intervene in the client local learning process. Other scenarios are reserved for future research."}, {"title": "4 Methodology", "content": "In this section, we explain our proposed method FedKD in detail."}, {"title": "4.1 Overview of FedKD", "content": "FedKD, as a supplementary component functioning only in client local training phrase, is orthogonal to existing FKGE models, which aims to decrease their embedding dimensions without significant performance decrease. Because all clients at each communication round share the same process, we take client c as an example to explain it."}, {"title": "4.2 Client Local Training with Knowledge Distillation", "content": "During client local training at round t, for each triple (h, r, t) of client c, we first generate a set of negative samples of size n: Nc (h, r, t) = {(h,r,t)|i \u2208 [1,n]; t \u2260 t; (h,r,t) \u2209 Tc}. Subsequently, we compute scores for the triple (h,r, t) and its negative samples using the teacher and student models, respectively. For the triple (h, r, t), its scores at student and teacher models are computed as follows: (Similarly, this process applies to its negative samples.)\n$$S_{stu}^{(h,r,t)} = S((h,r,t); E_{stu}, R_{stu})$$\n$$S_{tea}^{(h,r,t)} = S((h,r,t); E_{tea}, R_{tea})$$\nwhere S(.) is the scoring function adopted in the original FKGE model; Estu and Rstu (Etea and Rtea) are the entity and relation embeddings of student (teacher) model.\nThe score of a triple indicates its likelihood of being predicted as positive. Therefore, we transfer knowledge from the teacher model to the student model by aligning their score distributions for each positive triple and its corresponding negative samples. Formally, for the triple (h, r, t), we compute its soft label loss for the student model as follows:\n$$L_{h,r,t} = KL(P_{stu}^{(h,r,{t,t'})}, P_{tea}^{(h,r,{t,t'})})$$\nwhere {t, t} (i \u2208 [1, n]) denotes the tail entity set of triple (h, r, t) and its negative samples; KL() denotes the Kullback-Leiber Divergence; $$P_{stu}^{(h,r,{t,t'})}$$ and $$P_{tea}^{(h,r,{t,t'})}$$ are the score distributions of the student and the teacher model about triple (h, r, t) and its negative samples, respectively. For student model, $$P_{stu}^{(h,r,{t,t'})}$$ is composed by the score probability $$P_{stu}^{(t \\in {t,t'})}$$ of triple (h, r, t) and its negative samples. $$P_{stu}^{(h,r,t)}$$ is computed as follows:\n$$P_{stu}^{(h,r,t)} = \\frac{exp(S_{stu}^{(h,r,t)}/ \\tau)}{\\Sigma_{i \\in {t,t'}} exp(S_{stu}^{(h,r,i)}/ \\tau)}$$\nwhere \u03c4 is the temperature for scaling scores of triples; $$P_{tea}^{(h,r,{t,t'})}$$ can be computed in the similar way.\nHowever, we note that ATS (Asymmetric Temperature Scaling) [16] highlights a potential issue in knowledge distillation when the teacher model exhibits over-confidence. Specifically, if the teacher assigns excessively high scores to the correct class and lower or less varied scores to incorrect classes, applying a uniform temperature to scale the logits of the neural network can lead to reduced class discriminability. This phenomenon can adversely impact the effectiveness of the knowledge distillation process. To address this issue, ATS proposes the use of two distinct temperature values: one for the correct class and another for the incorrect classes. Inspired by the insight from ATS, we introduce a novel mechanism called Adaptive Asymmetric Temperature Scaling. Unlike ATS, which employs two pre-defined temperature parameters shared across all input samples, our method adaptively learns the temperature for each positive sample while only keep the temperature for the negative samples pre-defined. This adaptive approach aims to tailor the temperature scaling to the specific characteristics of each positive sample, thereby enhancing the effectiveness of the knowledge distillation process.\nHence, we further improve the score distributions of the student and teacher model: $$P_{stu}^{(h,r,{t,t'})}$$ and $$P_{tea}^{(h,r,{t,t'})}$$. Specifically, we assign an adaptive temperature to each positive triple, determined by the teacher model's confidence in that triple. To quantify the teacher model's confidence in the positive triple (h, r, t), we utilize its score probability (i.e., $$P_{tea}^{(h,r,t)}$$ as shown in Eq. 5. Here, we set the \u03c4 as 1) in this paper. A higher score probability for a positive triple indicates greater confidence by the teacher model. We also think that other methods, such as using the entropy of the teacher's score distribution over the set of positive and negative samples (i.e., $$P_{tea}^{(h,r,{t,t'})}$$), could also be explored. We consider these alternative measures as potential avenues for future research.\nSpecifically, we calculate the temperature $$t_{(h,r,t)}$$ for each positive triple (h, r, t) using a two-layer MLP with ReLU as the activation function and the hidden dimension as 32. Formally,\n$$T_{(h,r,t)} = (T_{max} - T_{min}) \\times \\sigma(MLP(P_{tea}^{(h,r,t)})) + T_{min}$$\nwhere Tmax and tmin define the upper and lower bounds of the temperature range, respectively; \u03c3 denotes the Sigmoid function."}, {"title": "5 Experiment", "content": "In this section, we apply the proposed FedKD to the seminal FKGE model Fede and evaluate the effectiveness of embedding compression."}, {"title": "5.1 Experiment Setup", "content": "5.1.1 Dataset. We perform experiments using three widely used datasets for FKGE: FB-R3, FB-R5, and FB-R10. These datasets are derived by evenly partitioning relations from the FB15k-237 dataset and distributing associated triples across three, five, and ten clients respectively. Each dataset maintains the same division ratio for training, validation, and testing sets: 0.8/0.1/0.1.\n5.1.2 Baselines. We apply the proposed FedKD component on the initial FKGE model FedE. To leverage the benefits of personalized modeling, we enhance FedE by adopting local embeddings to evaluate the performance on validation and testing sets instead of using global embeddings throughout the training process. The low-dimensional FedE integrated with the FedKD component is referred to as the student model, denoted as FedEKD for simplicity. The high-dimensional FedE serves as the teacher model and acts as a baseline for comparison. The model is denoted as FedEH for convenience. Additionally, we use FedE with the same dimension as FedEKD as another baseline, labeled as FedEL. Furthermore, we include a model (i.e., Local) trained only on local triples,"}, {"title": "5.3 Ablation Study", "content": "In this section, we investigate whether the Adaptive Asymmetric Temperature Scaling mechanism can improve the performance. We first remove the mechanism from the component FedKD and then apply the remains of FedKD to FedE. The derived model is denoted FedEKD* for clarity. We follow the same setting for the other parameters as described in 5.1.4 and conduct experiments to compare the performance of FedEKD and FedEKD*. The result is shown in the Table 2.\nThe data presented in the table indicates that FedEKD generally outperforms FedEKD* across all evaluated metrics, except for Hits@1 with the RotatE KGE method and Hits@10 with the TransE KGE method. Moreover, compared to TransE, both RotatE and Complex exhibit more pronounced improvements overall. Specifically, the increase in MRR is 0.11% for TransE, 0.15% for RotatE, and 0.16% for ComplEx. For Rotate and Complex, the enhancements in metric Hits@5 and Hits@10 are more notable than the metric MRR. Rotate shows an increase of 0.46% in Hits@5 and"}, {"title": "6 Conclusion", "content": "FKGE enables collaborative learning of entity and relation embeddings across distributed knowledge graphs while ensuring data privacy. Higher-dimensional embeddings are typically favored in FKGE models for enhanced performance, yet they introduce challenges like resource-intensive storage and slower inference. Unlike traditional methods, FKGE involves iterative client-server communication, requiring efficient data transfer. Compression techniques for traditional KG embeddings may not suit FKGE due to their reliance on multiple model training and potential communication costs. This study introduces FedKD, a novel component tailored for FKGE, utilizing Knowledge Distillation to enhance training on client devices. FedKD adjusts temperature parameters dynamically for positive triples to refine score distributions, mitigating issues of teacher model over-confidence. Additionally, adaptive adjustment of KD loss weighting optimizes the training process. Experimental results indicate that FedKD achieves a reduction in embedding dimensions by half with minimal to negligible degradation in performance."}]}