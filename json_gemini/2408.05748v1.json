{"title": "Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation", "authors": ["Xiaoxiong Zhang", "Zhiwei Zeng", "Xin Zhou", "Zhiqi Shen"], "abstract": "Federated Knowledge Graph Embedding (FKGE) aims to facilitate collaborative learning of entity and relation embeddings from distributed Knowledge Graphs (KGs) across multiple clients, while preserving data privacy. Training FKGE models with higher dimensions is typically favored due to their potential for achieving superior performance. However, high-dimensional embeddings present significant challenges in terms of storage resource and inference speed. Unlike traditional KG embedding methods, FKGE involves multiple client-server communication rounds, where communication efficiency is critical. Existing embedding compression methods for traditional KGs may not be directly applicable to FKGE as they often require multiple model trainings which potentially incur substantial communication costs. In this paper, we propose a lightweight component based on Knowledge Distillation (KD) which is titled FedKD and tailored specifically for FKGE methods. During client-side local training, FedKD facilitates the low-dimensional student model to mimic the score distribution of triples from the high-dimensional teacher model using KL divergence loss. Unlike traditional KD way, FedKD adaptively learns a temperature to scale the score of positive triples and separately adjusts the scores of corresponding negative triples using a predefined temperature, thereby mitigating teacher over-confidence issue. Furthermore, we dynamically adjust the weight of KD loss to optimize the training process. Extensive experiments on three datasets support the effectiveness of FedKD.", "sections": [{"title": "1 Introduction", "content": "A knowledge graph (KG) describes real-world facts in the form of triples (head entity, relation, tail entity). Knowledge graph embedding (KGE) aims to encode entities and relations in the KG into continuous vector representations which capture the semantic meanings and relationships inherent in the graph structure, enabling various downstream tasks such as disease diagnosis [1, 21], recommendation system [28, 29, 32], question answering system [10] and so on.\nWith the promulgation of General Data Protection Regulation (GDPR) [20], KGs from multiple sources are no longer stored centrally on one device as a whole KG, but instead, in a more decentralized manner on multiple clients. Formally, these distributed multi-source KGs are referred to as federated knowledge graphs (FKG). A major line of research in FKG is federated learning-based Federated Knowledge Graph Embedding (FKGE) [5, 6, 19, 35]. Through federated learning [3, 22, 25, 31, 33], each client obtains useful information from other clients. The aggregated information, together with local triples, is used to update the entities and relations embeddings. The final learned entities and relations embeddings are used to predict non-existent triples for each client.\nTo enhance the performance of learned embeddings for FKG, it is common practice to increase embedding dimension during the training phase. However, pre-training high-dimensional embeddings is often impractical in many real-world downstream scenarios, and the low-dimensional embeddings are necessary [40]. In some cases, the storage capacity and computational resources are limited (such as deploying learned embeddings from FKG on edge computing devices or mobile platforms) while fast inference speed is required (such as online financial systems demanding fast market decisions). Besides, some cases need to further fine-tune the pre-trained embedding of FKG rather than using them directly to address downstream tasks (such as recommendation system) effectively, also expecting to keep the cost of fine-tuning as low as possible. Hence, it is necessary for FKGE to balance the need for low dimension and high performance.\nRecently, there are several knowledge-distillation-based methods [17, 27, 40] to compress high-dimensional embeddings to low-dimensional ones for KG while there is little research about embedding compression specifically for FKG. The embedding learning process for FKGE is distinct from that of traditional KGE. For FKGE, the process typically entails multiple rounds of communication between clients and a central server. Due to constraints such as limited network bandwidth and data usage on clients, minimizing the transmission of parameters during FKGE training is crucial. Since there is no concern about communication efficiency in traditional KGE, existing KGE embedding compression techniques are often complex and involve multiple model trainings. For example, the method in [27] trains multiple high-dimensional teacher models and use them to teach the low-dimensional student model collaboratively. The approach described in [17] employs iterative knowledge distillation to progressively decrease the model sizes to a desired target, which aims to mitigate the adverse effects of substantial dimension gap between the student and the teacher model on the performance of knowledge distillation. During the process, multiple models of intermediate sizes are trained. As a result, these methods adopted in KGE usually are unsuitable for direct application in FKGE due to the significant communication costs they potentially entail. Therefore, it is necessary to develop more efficient and lightweight embedding compression methods tailored specifically for FKGE.\nIn this paper, we propose a lightweight method titled Low-Dimensional Federated Knowledge Graph Embedding via Knowledge Distillation (FedKD). It, acting as an adjunct to current FKGE methods, enables the transfer of insights from a pre-trained high-dimensional teacher model to a student model during the clients' local training phrase at each communication round. Specifically, during the client's local training phase in each communication round, in addition to applying the original local training loss (i.e., hard label loss), FedKD enables the low-dimensional student model to emulate the score distribution of triples derived from the pre-trained high-dimensional teacher model by employing KL divergence loss. The KL divergence loss is denoted as soft label loss, formally. However, the over-confidence in positive triples by the teacher model can usually lead to a less discriminative distribution of corresponding negative samples and thereby impacting the efficacy of knowledge distillation. To mitigate the issue, in contrast to traditional knowledge distillation methods using KL divergence, we introduce the Adaptive Asymmetric Temperature Scaling mechanism which adaptively learns a temperature to scale the score of the positive triple and separately scales the scores of corresponding negative triples using a predefined temperature. Moreover, the discrepancy in optimization objectives between hard label loss and soft label loss can inadvertently affect training efficiency. To address this, we dynamically adjust the weight of the soft label loss by prioritizing the hard label loss at the early stage of training and progressively transit towards placing greater emphasis on the soft label loss as the training progresses. Extensive experimental results substantiate the effectiveness of FedKD.\nOur contributions are summarized as follows:\n\u2022 We introduce a lightweight component, FedKD, designed for existing FKGE methods, which aims to reduce embedding size without substantial compromise to embedding performance. To the best of our knowledge, this is the first method tailored specifically for embedding compression in FKGE.\n\u2022 We propose an Adaptive Asymmetric Temperature Scaling mechanism aimed at mitigating the adverse impact of teacher model over-confidence in positive samples on the performance of on knowledge distillation. Besides, we employ dynamic weight adjustment to alleviate the optimization disparity between the hard and soft label loss.\n\u2022 We assess the efficacy of FedKD across three datasets through comprehensive experiments. The findings indicate a notable reduction in the embedding dimension of FKGE with with only marginal (even no) decrease in performance."}, {"title": "2 Related Work", "content": "2.1 Federated Knowledge Graph Embedding\nFederated Learning (FL) represents a novel distributed paradigm in machine learning that enables multiple clients to collaboratively train high-performing models, simultaneously preserving data privacy and security [4, 14, 15, 24, 34]. Based on the FL paradigm, federated knowledge graph embedding aims to facilitate collaborative learning of entity and relation embeddings from multi-source KGs distributed across multiple clients, while ensuring raw triplets are kept confidential and not shared among participants.\nFedE [5], inspired by FedAvg [18], serves as a pioneering model where a server coordinates clients' local embedding learning by iteratively aggregating their entity embeddings and distributing the aggregated result for the subsequent round of local training. Inspired by Moon [13], FedEC introduces embedding contrastive learning into FedE to further improve embedding performance. However, both FedE and FedEC generate a single set of global consensus embeddings for all clients, which may not fully accommodate data heterogeneity. As a result, the embeddings trained to fit an \"average KG\" might not generalize well to KGs that exhibit high heterogeneity with the global training dataset. To address it, several personalized FKGE methods are proposed. PFedEG proposed the personalized embedding aggregation mechanism on the server based on a client-wise relation graph [37]. FedLu introduces mutual knowledge distillation to facilitate the transfer of information from local entity embeddings to global entity embeddings, followed by the incorporation of knowledge gleaned from the global entity embeddings [38]. In contrast to the aforementioned methods that predominantly handle scenarios where clients possess partially shared entities but mutually exclusive relations, FedR [35] specifically addresses cases where clients not only share entities but also share relations. In this framework, all clients receive identical embeddings of shared relations from the server and subsequently engage in local embedding learning using their respective local triples together with the shared relation embeddings.\nUnlike the aforementioned methods that rely on a client-server architecture, FKGE [19] facilitates collaborative embedding learning among clients in a peer-to-peer manner. Drawing inspiration from MUSE [7], FKGE adopts a Generative Adversarial Network\n2.2 Knowledge Graph Embedding\nCompression with Knowledge Distillation\nKnowledge distillation (KD) is a prominent technique employed in model compression [9, 30, 39]. In this approach, a larger, pre-trained model, referred to as the teacher model, provides guidance to a smaller model, known as the student model. The objective is for the student model to approximate the outputs of the teacher model, thereby achieving similar performance [11].\nMany embedding methods demonstrate enhanced performance with higher embedding dimensions. However, the trade-off includes longer inference times and larger model sizes for high-dimensional Knowledge Graph Embedding. Recently, several knowledge distillation methods [17, 27, 40] have been proposed to compress high-dimensional embeddings into lower-dimensional representations for knowledge graphs. MulDE[27] employs multi-teacher distillation techniques aimed at improving the efficacy of low-dimensional models. However, it does not adequately address the dimension gap issue, which can significantly affect the effectiveness of the distillation process. DualDE [40] presents an innovative two-stage distillation approach where only a high-dimensional teacher model is pre-trained. Initially, the teacher model remains fixed while only the student model is updated under its guidance during the first stage. Subsequently, in the second stage, both the teacher and student models reciprocally guide each other's embedding updates. Despite involving only a pre-trained teacher model, DualDE still encounters significant training costs due to the necessity of updating the teacher model during the second stage [17]. IteDE [17] introduces an iterative knowledge distillation approach aiming at progressively reducing model sizes to a predefined target. Throughout this process, multiple models of intermediate sizes are trained under the guidance of adjacent higher-dimensional models. Essentially, it remains within the realm of multiple-teacher strategies. These methods typically prioritize enhancing the performance of low-dimensional embeddings with limited regard for the associated training costs.\nHowever, in the context of FKGE, the training procedure typically involves iterative communication between clients and a central server. Given constraints such as limited network bandwidth and client-side data usage, minimizing parameter transmission during FKGE training is crucial. From this perspective, the methods commonly employed in traditional KGE may not be directly applicable to FKGE due to their potential for significant communication costs. Hence, there is a need to devise more lightweight embedding compression techniques tailored specifically for FKGE."}, {"title": "3 Preliminaries", "content": "In this section, we explain the preliminary knowledge about KG, FKG, FKGE, FedE (the pioneering FKGE method) and the task of FKGE Compression based on Knowledge Distillation.\nA Knowledge Graph (KG) G with entity set & and relation set R is defined as G = {(h,r,t) | h,t \u2208 &, r\u2208 R}, where each triplet (h, r, t) represents the fact that the head entity h has relation r with the tail entity t.\nFederated Knowledge Graph (FKG) is a set of KGs which are stored in multiple clients for the aim of data privacy. Formally, the definition of FKG is as follows:\nDefinition 3.1. Federated Knowledge Graph: Given C clients, each client holds a knowledge graph Gc = {&c, Rc, Tc | c\u2208 [0, C]}, and the entity set in these C knowledge graphs can overlap, but the relation sets are mutually exclusive. The set of these C knowledge graphs is called Federated Knowledge Graph: GF = {Gc}C=1\nFederated Knowledge Graph Embedding (FKGE) aims to let all KGs in FKGE collaboratively engage in embedding learning to achieve enhanced embeddings with improved performance, without exposing raw triples to each other. This collaborative process leverages the presence of shared entities among clients' KGs, allowing each client to contribute additional semantic information to others.\nFedE is the first FKGE method which mainly includes two parts: client update and server update. We take the communication round t as an example to explain them, considering the different rounds share the same process. For server updates, the server first collects the updated entity embeddings at round t \u2212 1 from each client. Subsequently, the server performs an average embedding aggregation for each entity. For client updates, each client initially receives the aggregated embeddings from the server and replaces their local entity embeddings with these updated versions. Following this, each client proceeds with local embedding learning using a KGE method. The objective of local embedding learning is scoring positive triples higher than negative ones based on the score function of the chosen KGE method.\nFKGE Compression based on Knowledge Distillation (FKGE-CD) aims at reducing the embedding dimension for a FKGE method using knowledge distillation techniques, and simultaneously avoiding decrease embedding performance significantly. In this paper, we further assume that the FKGE-CD training process should follow the existing client and server communication mechanisms, and it should only intervene in the client local learning process. Other scenarios are reserved for future research."}, {"title": "4 Methodology", "content": "In this section, we explain our proposed method FedKD in detail.\n4.1 Overview of FedKD\nFedKD, as a supplementary component functioning only in client local training phrase, is orthogonal to existing FKGE models, which aims to decrease their embedding dimensions without significant performance decrease. Because all clients at each communication round share the same process, we take client c as an example to explain it.\nTo reduce the embedding dimension of a FKGE model, it begins with pre-training the FKGE mdoel with high embedding dimension. These obtained high-dimensional embeddings are collectively referred as the teacher model. Correspondingly, the FKGE model configured with the specified low embedding dimension is denoted as the student model. For the student model, during client c's local training at round t, besides applying the original local training loss function of the FKGE method as the hard label loss LH, the FedKD component also encourages the student model to mimic the score distributions of triples from the teacher, known as the soft label loss Ls. Formally, the client c's total local training loss at round t is as follows:\nLc = LH + \u03bb * ( LH / (LH + LS))\n(1)\nwhere \u03bb\u2208 R\u00b9 is a constant; the term LH / (LH + LS) is used to dynamically balance the hard and soft label loss, which is explained in the following parts in detail.\nIn this work, we use the Kullback-Leibler (KL) divergence as the the soft label loss. Different from traditional knoweldge distillation with KL, we introduce the Adaptive Asymmetric Temperature Scaling mechanism into the KL-based soft label loss. It aims to mitigate the problem that the teacher can be too confident about the positive triple. When the teacher is too confident about the positive triple, the distribution of corresponding negative samples are less discriminative, which affects the performance of knowledge distillation [16]. Besides, the disparity in optimization objectives between hard label loss and soft label loss can inadvertently impact training efficiency [17]. To deal with it, we begin local training by emphasizing the importance of the hard label loss, gradually shifting towards greater emphasis on the soft label loss as training advances.\nWe will explain FedKD in the following section in detail.\n4.2 Client Local Training with Knowledge\nDistillation\nDuring client local training at round t, for each triple (h, r, t) of client c, we first generate a set of negative samples of size n: Nc (h, r, t) = {(h,r,t')|i \u2208 [1,n]; t' \u2260 t; (h,r,t') \u2209 Tc}. Subsequently, we compute scores for the triple (h,r, t) and its negative samples using the teacher and student models, respectively. For the triple (h, r, t), its scores at student and teacher models are computed as follows: (Similarly, this process applies to its negative samples.)\nSstu(h,r,t) = S((h,r,t); Estu, Rstu)\n(2)\nStea(h,r,t) = S((h,r,t); Etea, Rtea)\n(3)\nwhere S(.) is the scoring function adopted in the original FKGE model; Estu and Rstu (Etea and Rtea) are the entity and relation embeddings of student (teacher) model.\nThe score of a triple indicates its likelihood of being predicted as positive. Therefore, we transfer knowledge from the teacher model to the student model by aligning their score distributions for each positive triple and its corresponding negative samples. Formally, for the triple (h, r, t), we compute its soft label loss for the student model as follows:\nLstu(h,r,t) = KL(Pstu(h,r,{t,t'}), Ptea(h,r,{t,t'}))\n(4)\nwhere {t, t'} (i \u2208 [1, n]) denotes the tail entity set of triple (h, r, t) and its negative samples; KL() denotes the Kullback-Leiber Divergence; Pstu(h,r,{t,t'}) and Ptea(h,r,{t,t'}) are the score distributions of the student and the teacher model about triple (h, r, t) and its negative samples, respectively. For student model, Pstu(h,r,{t,t'}) is composed by the score probability Pstu(t\u2208 {t,t'}) of triple (h, r, t) and its negative samples. Pstu(h,r,{t,t'}) is computed as follows:\nPstu(h,r,t) = exp(Sstu(h,r,t)/\u03c4) / \u03a3i\u2208{t,t'} exp(Sstu(h,r,ti)/\u03c4)\n(5)\nwhere \u03c4 is the temperature for scaling scores of triples; Ptea(h,r,{t,t'}) can be computed in the similar way.\nHowever, we note that ATS (Asymmetric Temperature Scaling) [16] highlights a potential issue in knowledge distillation when the teacher model exhibits over-confidence. Specifically, if the teacher assigns excessively high scores to the correct class and lower or less varied scores to incorrect classes, applying a uniform temperature to scale the logits of the neural network can lead to reduced class discriminability. This phenomenon can adversely impact the effectiveness of the knowledge distillation process. To address this issue, ATS proposes the use of two distinct temperature values: one for the correct class and another for the incorrect classes. Inspired by the insight from ATS, we introduce a novel mechanism called Adaptive Asymmetric Temperature Scaling. Unlike ATS, which employs two pre-defined temperature parameters shared across all input samples, our method adaptively learns the temperature for each positive sample while only keep the temperature for the negative samples pre-defined. This adaptive approach aims to tailor the temperature scaling to the specific characteristics of each positive sample, thereby enhancing the effectiveness of the knowledge distillation process.\nHence, we further improve the score distributions of the student and teacher model: Pstu(h,r,{t,t'}) and Ptea(h,r,{t,t'}). Specifically, we assign an adaptive temperature to each positive triple, determined by the teacher model's confidence in that triple. To quantify the teacher model's confidence in the positive triple (h, r, t), we utilize its score probability (i.e., Ptea(h,r,t) as shown in Eq. 5. Here, we set \u03c4 as 1) in this paper. A higher score probability for a positive triple indicates greater confidence by the teacher model. We also think that other methods, such as using the entropy of the teacher's score distribution over the set of positive and negative samples (i.e., Ptea(h,r,{t,t'})), could also be explored. We consider these alternative measures as potential avenues for future research.\nSpecifically, we calculate the temperature \u03c4(h,r,t) for each positive triple (h, r, t) using a two-layer MLP with ReLU as the activation function and the hidden dimension as 32. Formally,\n\u03c4(h,r,t) = (\u03c4max \u2212 \u03c4min) \u00d7 \u03c3(MLP(Ptea(h,r,t))) + \u03c4min\n(6)\nwhere \u03c4max and \u03c4min define the upper and lower bounds of the temperature range, respectively; \u03c3 denotes the Sigmoid function. Hence, the score distributions of both student and teacher model (i.e., Pstu(h,r,{t,t'}) and Ptea(h,r,{t,t'})) is further revised. Take student model as an example, for the positive triple (h, r, t), its score probability is adjusted as follows:\nPstu(h,r,t) = exp(Sstu(h,r,t)/\u03c4(h,r,t)) / ( \u03a3i=1n exp(Sstu(h,r,ti)/\u03c4) + exp(Sstu(h,r,t)/\u03c4(h,r,t)))\n(7)\nFor the negative triple (h, r, ti), its score probability is adjusted as follows:\nPstu(h,r,ti) = exp(Sstu(h,r,ti)/\u03c4) / ( \u03a3i=1n exp(Sstu(h,r,ti)/\u03c4) + exp(Sstu(h,r,t)/\u03c4(h,r,t)))\n(8)\nwhere \u03c4(h,r,t) is the adaptive temperature for positive triple (h, r, t) and \u03c4 is the pre-defined temperature for all negative triples.\nThe score distribution of teacher model Ptea(h,r,{t,t'}) is revised in the similar way.\nThe divergence in optimization goals between hard label loss and soft label loss may inadvertently affect the efficiency of training [17]. We opt to prioritize the hard label loss during the initial stages of local training, gradually augmenting the weight of the soft label loss to emphasize its influence during the later stages of local training. In our approach, we scale the soft label loss Ls with the coefficient \u03bb* (LH / (LH + LS)), as the Eq. 1 shows. As training progresses and both hard and soft label losses diminish towards model convergence, this coefficient increases. Consequently, the influence of the soft label loss intensifies over the course of training."}, {"title": "5 Experiment", "content": "In this section, we apply the proposed FedKD to the seminal FKGE model Fede and evaluate the effectiveness of embedding compression.\n5.1 Experiment Setup\n5.1.1 Dataset. We perform experiments using three widely used datasets for FKGE: FB-R3, FB-R5, and FB-R10. These datasets are derived by evenly partitioning relations from the FB15k-237 dataset and distributing associated triples across three, five, and ten clients respectively. Each dataset maintains the same division ratio for training, validation, and testing sets: 0.8/0.1/0.1.\n5.1.2 Baselines. We apply the proposed FedKD component on the initial FKGE model FedE. To leverage the benefits of personalized modeling, we enhance FedE by adopting local embeddings to evaluate the performance on validation and testing sets instead of using global embeddings throughout the training process. The low-dimensional FedE integrated with the FedKD component is referred to as the student model, denoted as FedEKD for simplicity. The high-dimensional FedE serves as the teacher model and acts as a baseline for comparison. The model is denoted as FedEH for convenience. Additionally, we use FedE with the same dimension as FedEKD as another baseline, labeled as FedEL. Furthermore, we include a model (i.e., Local) trained only on local triples, without federated learning, but at the same high dimension as the teacher model, as an additional baseline for our experiments. During the clients' local training phase, we follow the standard practice of utilizing three representative KGE methods from previous FKGE studies: TransE [2], RotatE [23], and Complex [26].\n5.1.3 Evaluation Metrics. We evaluate the model performance by link prediction task which predicts the the tail (head) entity when given the head (tail) entity and relation. The link prediction evaluation metric are Mean Reciprocal Rank (MRR) and Hits@N (N is 1, 5 and 10). The overall metric value across clients is computed by weighted average, where the weights correspond to the proportions of the triple sizes across the clients.\n5.1.4 Implementation Details. We assume full participation of all clients in each communication round. The parameters in the experiments are as follows. For client training, the batch size is set to 512 and the number of local epochs is 3. The high and low embedding dimension are set as 256 and 128, respectively. The initialization parameter \u03f5 is set as 2 for both high and low embedding dimension, while another initialization parameter \u03b3 is set as 8 and 6 for high and low embedding dimension, respectively. During training, we employ an early stopping strategy with a patience threshold of 3. For the Local model, performance evaluation occurs every 10 epochs, while for all other models, evaluation takes place every 5 epochs. The optimizer Adam [12] is adopted and the learning rate is set as 0.0001. The papameter \u03bb is set as 3. The parameter \u03c4min and \u03c4 is set as 1 for all cases. For dataset FB-R3, the choice of \u03c4max is 2 for TransE and RotatE, and 1.5 for ComplEx. For dataset FB-R5, \u03c4max is set to 1.5 for Rotate and ComplEx. In dataset FB-R10, \u03c4max is 1.5 for TransE. In all other cases, we set the default value of \u03c4max to 10.\n5.2 Main Experiment\nIn this section, we present a quantitative assessment of the effectiveness of the FedKD component. This evaluation is conducted by comparing the performance metrics of FedEKD-a variant derived by integrating FedKD into FedE-against established baseline models. The experimental results across three distinct datasets are detailed in Table 1.\nAnalysis of the data presented in Table 1 reveals a marked decline in performance for the FedEL model compared to FedEH. This decline is particularly pronounced when utilizing Complex as the KGE method. In particular, for the FB-R3, FB-R5, and FB-R10 datasets, the FedEL model shows a decrease in MRR by 10.13%, 9.55%, and 17.85%, respectively. Similar downward trends are also observed in the Hits@1, Hits@5, and Hits@10 metrics. These findings indicate that training low-dimensional FKGE directly leads to suboptimal performance outcomes.\nWhen comparing FedEL and FedEKD, both operating in a 128-dimensional space, FedEKD shows substantial performance improvement. Furthermore, FedEKD achieves performance levels that approximate those of the teacher model FedEH with higer dimension(256). Specifically, when TransE is employed as the KGE method, FedEKD achieves a relative increase in MRR of 1.85%, 3.12%, and 2.04% on the FB-R3, FB-R5, and FB-R10 datasets, respectively. FedEKD attains performance levels of 99.17%, 99.26%, and 99.55% relative to FedEH.\nWhen Rotate is used as the KGE method, compared with FedEL, FedEKD achieves the relative increases in MRR on these datasets are 0.74%, 1.89%, and 1.93%. It finally attains MRR performance levels of 99.54%, 99.68%, and 99.81% relative to FedEH. Notably, with Complex as the KGE method, FedEKD, compared with FedEL shows a more pronounced improvement in MRR, with increases of 11.10%, 10.24%, and 20.51% on the FB-R3, FB-R5, and FB-R10 datasets, respectively. It achieves MRR levels of 99.84%, 99.74%, and 99.00% relative to FedEH. A significant upward trend of FedEKD compared with FedEL is also observed across the metrics Hits@1, Hits@5, and Hits@10. Unexpectedly, FedEKD even outperforms FedEH in certain cases, in terms of Hits@1, Hits@5, and Hits@10 metrics. These results collectively demonstrate that the FedKD component facilitates a reduction in FKGE embedding dimensions from 256 to 128 with minimal to negligible performance degradation.\n5.3 Ablation Study\nIn this section, we investigate whether the Adaptive Asymmetric Temperature Scaling mechanism can improve the performance. We first remove the mechanism from the component FedKD and then apply the remains of FedKD to FedE. The derived model is denoted FedEKD* for clarity. We follow the same setting for the other parameters as described in 5.1.4 and conduct experiments to compare the performance of FedEKD and FedEKD*. The result is shown in the Table 2.\nThe data presented in the table indicates that FedEKD generally outperforms FedEKD* across all evaluated metrics, except for Hits@1 with the RotatE KGE method and Hits@10 with the TransE KGE method. Moreover, compared to TransE, both RotatE and Complex exhibit more pronounced improvements overall. Specifically, the increase in MRR is 0.11% for TransE, 0.15% for RotatE, and 0.16% for ComplEx. For Rotate and Complex, the enhancements in metric Hits@5 and Hits@10 are more notable than the metric MRR. Rotate shows an increase of 0.46% in Hits@5 and"}, {"title": "6 Conclusion", "content": "FKGE enables collaborative learning of entity and relation embeddings across distributed knowledge graphs while ensuring data privacy. Higher-dimensional embeddings are typically favored in FKGE models for enhanced performance, yet they introduce challenges like resource-intensive storage and slower inference. Unlike traditional methods, FKGE involves iterative client-server communication, requiring efficient data transfer. Compression techniques for traditional KG embeddings may not suit FKGE due to their reliance on multiple model training and potential communication costs. This study introduces FedKD, a novel component tailored for FKGE, utilizing Knowledge Distillation to enhance training on client devices. FedKD adjusts temperature parameters dynamically for positive triples to refine score distributions, mitigating issues of teacher model over-confidence. Additionally, adaptive adjustment of KD loss weighting optimizes the training process. Experimental results indicate that FedKD achieves a reduction in embedding dimensions by half with minimal to negligible degradation in performance."}]}