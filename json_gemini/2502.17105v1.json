{"title": "SFLD: Reducing the content bias for AI-generated Image Detection", "authors": ["Seoyeon Gye", "Junwon Ko", "Hyounguk Shon", "Minchan Kwon", "Junmo Kim"], "abstract": "Identifying Al-generated content is critical for the safe and ethical use of generative Al. Recent research has fo- cused on developing detectors that generalize to unknown generators, with popular methods relying either on high- level features or low-level fingerprints. However, these methods have clear limitations: biased towards unseen con- tent, or vulnerable to common image degradations, such as JPEG compression. To address these issues, we propose a novel approach, SFLD, which incorporates PatchShuf- fle to integrate high-level semantic and low-level textural information. SFLD applies PatchShuffle at multiple lev- els, improving robustness and generalization across various generative models. Additionally, current benchmarks face challenges such as low image quality, insufficient content preservation, and limited class diversity. In response, we in- troduce TwinSynths, a new benchmark generation method- ology that constructs visually near-identical pairs of real and synthetic images to ensure high quality and content preservation. Our extensive experiments and analysis show that SFLD outperforms existing methods on detecting a wide variety of fake images sourced from GANs, diffusion models, and TwinSynths, demonstrating the state-of-the-art performance and generalization capabilities to novel gener- ative models. The TwinSynths dataset is publicly available at https://huggingface.co/datasets/koooooooook/ TwinSynths.", "sections": [{"title": "1. Introduction", "content": "The rapid advancement of AI image generation tech- nologies has brought significant achievements but also growing social concern, as these technologies are increas- ingly misused for the creation of fake news, malicious defamation, and other forms of digital deception. In re- sponse, AI-generated image detection is receiving more at- tention. There is a wide variety of generative models, along with commercial models with unknown internal architec- tures. This highlights the need for a generalized detector capable of distinguishing between real and fake images, re- gardless of the generative model structure.\nIn this context, early research focused on identifying the characteristic fingerprints of generated images. Recent work, NPR [46] shows that pixel-level features, induced by the upsampling layers commonly found in current genera- tive models, can serve as cues for detection. However, there are clear practical limitations to relying on low-level fin- gerprints. First, the approach is vulnerable to simple image degradations, such as JPEG compression or blurring, which are common in real-world online environments [49]. Addi- tionally, the model may become biased toward the specific fakeness seen at training in cases where generalization to novel generators is not sufficiently considered [32, 56]. For instance, a detector trained on GAN-generated images may learn the characteristics of GANs as the fake features, while mistakenly perceiving images generated by diffusion mod- els as real. This bias limits the detector's generalizability across different types of generative models.\nTo tackle these limitations, UnivFD [32] utilizes a robust, pre-trained image encoder. This image embedding is task- agnostic, enabling it to capture high-level semantic informa- tion from images. However, we found that UnivFD exhibits a bias towards the observed content in the training images, learning another specific fakeness. Fig. 1 shows that Uni- vFD misclassifies most GAN-generated images of a novel class (StyleGAN-bedroom) as real. The bedroom class is absent from the training set, which may lead the detector to mistakenly classify most images as real, demonstrating the detector's reliance on seen content during training.\nWe propose a novel technique called PatchShuffle, which is the core of our fake image detection model, SFLD (pronounced \"shuffled\"). PatchShuffle divides the image into non-overlapping patches and randomly shuffles them. This procedure disrupts the high-level semantic structure of the image while preserving low-level textural information. This allows the detection model to better focus on both con- text and texture. SFLD utilizes an ensemble of classifiers at multiple levels of PatchShuffle, leveraging hierarchical in- formation across various patch sizes. This approach ensures that the model leverages both the semantic and textural as- pects of the image to improve fake image detection. The re- sults demonstrate that SFLD achieves superior performance with enhanced robustness and better generalization.\nFurthermore, we observe that previous benchmarks have three limitations: (1) low image quality. The previous benchmarks contain a significant portion of low-quality im- ages that lag behind the capabilities of current generative models. As a result, the practical usefulness of these bench- marks is significantly reduced. (2) lack of content preser- vation. Some subsets-particularly foundation generative models-lack access to the training data used for the check- points. Consequently, the content of the generated and real images often differs significantly, making it difficult to de- termine whether a detector focuses on real/fake discrimina- tive features or other irrelevant features. (3) limited class diversity. Existing benchmarks primarily focus on expand- ing the variety of generative models without considering the generated class diversity and scalability among generative models. As shown in Fig. 1, this makes it difficult to iden- tify detection bias towards certain classes, as well as hard to represent the in-the-wild performance of the detector due to limited class diversity.\nTo address these challenges, we propose a new bench- mark generation methodology and corresponding bench- mark, TwinSynths. It consists of synthetic images that are visually near-identical to paired real images for practical and fair evaluations. TwinSynths constructs image pairs that preserve both quality and content while retaining the ar- chitectural characteristics of each generative model. Also, TwinSynths enables flexible class expansion by generat- ing synthetic images tailored to the real image. Using this benchmark, we evaluate the performance of our proposed SFLD method as well as existing detection models.\nOur main contributions are summarized as follows:\n\u2022 We propose SFLD, a novel AI-generated image detection method that integrates semantic and texture artifacts on generated images, achieving state-of-the-art performance.\n\u2022 We propose a new approach on benchmarks and the sub- set of generated images that can ensure the quality and content of generated images.\n\u2022 We validate our method through extensive experiments and analysis that support our hypothesis."}, {"title": "2. Method", "content": ""}, {"title": "2.1. Patch Shuffling Fake Detection", "content": "Backbone. We utilize the visual encoder of CLIP VIT- L/14 [13, 36] to leverage the pre-trained feature space."}, {"title": "SFLD", "content": "SFLD combines multiple classifiers trained on shuffled images with different patch sizes. By varying the patch size, SFLD incorporates models that focus on various levels of structural features, ranging from fine-grained local details to more global patterns.\nDuring testing, \\(N_{\\text{views}} = 10\\) shuffled views are gener- ated for each patch size. The logits from these views are av- eraged and processed by the corresponding classifier. The final probability \\(P_{\\text{SFLD}}(y|x)\\) is computed by averaging the logits across patch sizes and applying the sigmoid function:\n\\begin{equation}\nP_{\\text{SFLD}}(y|x) = \\sigma \\left( \\sum_{j=1}^{k} \\frac{1}{k} \\left( \\frac{1}{N_{\\text{views}}} \\sum_{i=1}^{N_{\\text{views}}} f_{s_j}(x_i) \\right) \\right).\n\\end{equation}\nwhere k is the number of patch sizes used in the ensemble (e.g., k = 3 in our configuration).\nBinary classification is done using a threshold of 0.5 on \\(P_{\\text{SFLD}}\\). Although the fusion method is simple and not tuned for each test class, its simplicity enables strong gener- alization across diverse fake image sources. By combining classifiers trained on different patch sizes, SFLD achieves a robust and general detection performance. Algorithm 1 shows the full workflow of SFLD, especially the fusion of multiple classifiers during inference."}, {"title": "PatchShuffle", "content": "To effectively integrate both semantic and textural features, PatchShuffle disrupts the global struc- ture of an image while preserving local features. In the PatchShuffle process, the input images are divided into non- overlapping patches of size \\(s \\times s\\) and then randomly shuf- fled. This operation produces a new shuffled image \\(x_s\\).\nFor a given s, the logit score of the shuffled image is,\n\\begin{equation}\nz_s = \\psi(f(x_s)),\n\\end{equation}\nwhere \\(f(\\cdot)\\) represents a pre-trained CLIP encoder and \\(\\psi(\\cdot)\\) is a single fully connected layer appended to f.\nThere are classifiers for each patch size of shuffled im- ages to leverage local structure information hierarchically within the image. We selected patch sizes of 28, 56, and 224 for the proposed SFLD. As shown in Fig. 3, \\(s_0\\) is 224, \\(s_1\\) is 56 and \\(s_2\\) is 28. These configurations are studied in detail in Sec. 4.5. For each patch size \\(s_j\\), the classifier \\(\\psi_{s_j}\\) is trained independently. Notably, UnivFD takes a center- cropped 224x224 image as input to the CLIP encoder. Therefore, when using a patch size of 224 in PatchShuffle, it effectively corresponds to the same setting as UnivFD [32].\nWe employ binary cross-entropy loss for each classifier:\n\\begin{equation}\n\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j} [y_i \\log \\sigma(z_{s_j}) + (1 - y_i) \\log (1 - \\sigma(z_{s_j}))],\n\\end{equation}\nwhere N is the number of data and \\(y_i \\in \\{0,1\\}\\) is the label whether an input \\(x_i\\) is real (\\(y_i = 0\\)) or fake (\\(y_i = 1\\))."}, {"title": "2.2. TwinSynths", "content": "In Sec. 1, we pointed out three shortcomings in the previ- ous benchmarks: low image quality, lack of content preser- vation, and limited class diversity. This issue must be ad- dressed to allow a comprehensive comparison of detectors. Therefore, we propose a novel dataset creation method- ology and TwinSynths benchmark, consisting of GAN- and diffusion-based generated images that are paired with visually-identical real counterparts. To create a practical benchmark for evaluating generated image detectors, it is essential to ensure the generation of high-quality images that preserve the original content. To achieve this, the image generation process should ideally sample a distribution that closely resembles a real distribution. From this perspective, the image generation or sampling process can be interpreted as effectively fitting the generator to a single real image. Through this approach, we construct image pairs that pre- serve the content of the images while reflecting the archi- tectural traits of the generative models. Additionally, this methodology allows for the expansion of target classes in the benchmark by generating paired images for any real im- age. Fig. 2b are some examples of TwinSynths. We can see that the content of the paired real image is faithfully repro- duced and the quality of the generated image is guaranteed.\nTwinSynths-GAN benchmark. The GAN-based sub- sets in the previous benchmark have disparate training con- figurations, especially the class of training images, resulting in a discrepancy between the generated and the real images. In order to generate a high quality image that preserves the content of the paired real image while leveraging the train- ing methodology of GANs, we trained the generator from scratch using a single real image. The MSE loss was pro- vided to the generator to generate an image that is identi- cal to the original image. For reproduction, the latent vec- tor for the generator input is maintained at a fixed value. We created 8,000 generated images from 80 selected Im- ageNet [41] classes, which is much larger than previous benchmarks. We selected 40 classes following the ProGAN subset in ForenSynths [49], while the other 40 classes were chosen arbitrarily. We utilized DCGAN [35] architecture.\nTwinSynths-DM benchmark. In comparison to GAN- based subsets, diffusion-based subsets in conventional benchmarks were generated with off-the-shelf pretrained models, having much severer content discrepancy between real and generated images. In order to generate a high qual- ity image that preserves the content of paired real image while leveraging the inference process of diffusion mod- els, we used DDIM inversion [44] to generate image that is similar to the real image. We apply a DDIM forward process to the real image to make it noisy and perform text-conditioned DDIM denoising process using the prompt template 'a photo of {class name}'. For the prompts, we used the class names from ImageNet. This process makes TwinSynths-DM preserve the similarity with the paired real images. We used the same image classes used to create TwinSynths-GAN. We utilized the pretrained decoder and scheduler of [44]."}, {"title": "3. Experiments", "content": ""}, {"title": "3.1. Settings", "content": "Datasets. Following the conventions of AI-generated image detection, all detectors were trained using the Foren- Synths train set [49]. This train set consists of real images used to train ProGAN [18] and ProGAN-generated images. We evaluate the performance of SFLD on several bench- marks, including conventional benchmarks, TwinSynths, and low-level vision/perceptual loss benchmarks. For more detailed descriptions of the datasets and configurations used, please refer to Appendix B.\nBaseline methods. We compare the performance of the proposed SFLD with existing AI-generated image detec- tion methods. It includes CNNSpot [49], FreDect [14], GramNet [25], Fusing [17], LNP [23], LGrad [45], Uni- vFD [32], and NPR [46]. We conducted evaluations on the detection methods with our test dataset. The evaluation is done by the official models [32, 49], re-implemented mod- els [14, 17,23,25,45] by Zhong et al. [54], or trained model with the official codes using 20-classes train set [46].\nEvaluation metrics. We assess the performances of the detection models by average precision score (AP) and classification accuracy (Acc.), following previous works [32,46,49]. The AP metric is not dependent on the threshold value, whereas the Acc. is calculated with a fixed threshold of 0.5 across all generation models."}, {"title": "3.2. Results on Conventional Benchmark", "content": "Tabs. 1 and 2 shows the detection performance on conventional benchmarks in AP and Acc. All baselines are trained on only the ProGAN train dataset consist- ing of 20 classes. Higher performance is colored darker. SFLD demonstrates robust and generalized performance across various generators in the benchmark. Note that SFLD achieves above 90.0 AP on every unseen genera- tor. SFLD has an average of 98.43 AP, outperforming the best-performing baseline, UnivFD, by up to 2.14 in aver- age. While for some tasks NPR has shown outperforming AP values in some generators, it has shown relatively low performance on some generators. In this regard, we found that NPR is sensitive to some image degradation or differ- ent post-processing methods in different generative models, which limits its practicality. Refer to Sec. 4.3 for further comparison of robustness on image degradation.\nSFLD also exhibits state-of-the-art performance in clas- sification accuracy. It performs particularly well on chal- lenging datasets like DeepFake and ADM. On DeepFake, it improves accuracy from 74.6% to 84.2% (+9.6), and on ADM, from 79.5% to 86.0% (+6.5). These gains highlight its superior generalization in difficult scenarios."}, {"title": "3.3. Analysis on TwinSynths", "content": "Tab. 3 illustrates the detection performance on Twin- Synths in AP. The results demonstrate that SFLD is effec- tive in TwinSynths while some detectors have shown a sig- nificant drop in performance. Note that the TwinSynths fo- cused on three key aspects: image quality, content preserva- tion, and class diversity. This suggests that the high perfor- mance on conventional benchmarks may not guarantee the detector's performance in real-world scenarios.\nThe results of TwinSynths allow an indirect analysis of the factors that the detectors focus on. For convenience, we now define high-level features and low-level features. high- level features are semantic information and their artifacts originate from distribution disparity between real images and generated images. low-level features are texture infor- mation and their artifacts stem from the generator traces and image quality of generated images. The TwinSynths-GAN preserves the content of the real image with minimal alter- ation, as the images are generated from a single real im- age. This results in UnivFD, which captures high-level fea- ture artifacts on the entire image, resulting in poor perfor- mance on the TwinSynths-GAN subset. In contrast, NPR, which captures high-frequency artifacts in neighboring pix- els, demonstrates better performance than UnivFD on the TwinSynths-GAN subset. On the other hand, the generated images in TwinSynths-DM contain low-level discriminative features introduced by the DDIM decoder, which incorpo- rates additional fully connected layers and post-processing steps following the upsampling blocks. We can see that NPR exhibits lower performance, whereas UnivFD demon- strates higher performance. Nevertheless, SFLD demon- strates superior and robust performance on both bench- marks, indicating its ability to capture both low-level feature artifacts and high-level feature artifacts. Notably, no exist- ing detector has ever exhibited such a high level of perfor- mance on both benchmarks."}, {"title": "3.4. Low-level Vision and Perceptual Benchmark", "content": "Tab. 4 shows the detection performance on different benchmarks from ForenSynths [49]. Low-level vision mod- els, including SITD and SAN, preserve high-level features of real images. Perceptual models (CRN and IMLE) color semantically segmented images to match real images, pre- serving semantic information. Notably, while NPR was able to detect some super-resolution images from SAN, it failed to perform well in other image-to-image translation tasks. This indicates that detectors specialized in identifying low- level feature artifacts from ProGAN struggle to general- ize to images generated from different vision tasks. Con- versely, a detector that focuses on high-level feature ar- tifacts demonstrates strong performance on these bench- marks. SFLD integrates semantic and structural information from different patch sizes to show superior performance on low-level vision and perceptual benchmarks."}, {"title": "4. Discussion", "content": ""}, {"title": "4.1. Detailed Comparison with UnivFD", "content": "This section presents a comprehensive comparison of SFLD against UnivFD. Tab. 5 shows the classification ac- curacy of the prediction results of real and fake images on each generator in the conventional benchmark. It is evident that SFLD exhibits superior performance in predicting gen- erated images. Notably, UnivFD is unable to predict fake images in some generated subsets, whereas SFLD demon- strates its strength in both real and generated images. This result supports that SFLD can capture both low-level feature artifacts and high-level feature artifacts, making the detec- tor better generalize on novel generators."}, {"title": "4.2. Score Ensembling", "content": "Scatter plots. Ensembling of the detection scores of the original image and patch-shuffled images is supported by Fig. 4. In all cases, ensembling the two detectors with patch sizes 224 and 28 as an average of the two logit scores con- sistently improved binary separation and thus resulted in su- perior performance with the default threshold (as evidenced by Tabs. 1 and 2). This proves that the two detection meth- ods work as complementary functions."}, {"title": "A closer look into failure cases", "content": "Fig. 5 visualizes some exact failure cases with StyleGAN-generated images (Fig. 4a). Fig. 5a shows a case where UnivFD fails and PatchShuffle succeeds. These images seem to cause Uni- VFD to fail because the high-level feature is well gener- ated (high global structure fidelity). In contrast, PatchShuf- fle, which focuses on local structure, succeeds in detec- tion. Our method with score ensembling was able to capture these examples illustrated as the green line in Fig. 4. On the other hand, Fig. 5b shows a case where PatchShuffle fails and UnivFD succeeds. These generated images have well- generated local structures like textures but have defects in global structures such as ears, eyes, and faces. However, there are very few examples corresponding to this. This analysis indicates that using both local and global informa- tion is necessary for detecting generated images."}, {"title": "4.3. Robustness Against Image Degradation", "content": "Applying a Gaussian blur and JPEG compression to an image is a common degradation that can naturally occur. Fig. 6 illustrates the impact of each attack on two subsets of generated images. The diffusion-subset and GAN-subset are subsets of diffusion and GAN generators, respectively, drawn from the conventional benchmark. Gaussian indi- cates the addition of a Gaussian blur with a standard devia- tion of \u03c3. JPEG indicates the application of JPEG compres- sion with a specified compression quality. Note that JPEG compression with quality 100 does not result in the same image, as JPEG compression reduces color information and rounds coefficients, thereby losing some information.\nIf the model is vulnerable to image degradation, we can infer that it is influenced by the features targeted by the degradation. Specifically, Gaussian blur affects both high- and low-level features in the image, while JPEG com- pression primarily targets low-level features (see Fig. 13). Figs. 6a and 6b demonstrates that SFLD always shows the best performance against Gaussian blur, since it integrates both high- and low-level features through ensemble/fusion, enabling each to compensate for the information lost in the other. Figs. 6c and 6d illustrates that SFLD restores robustness against JPEG compression, supporting the fun- damental principle behind our model. Additionally, Uni- vFD, which focuses on capturing high-level feature artifacts is also robust against JPEG compression. However, NPR, which focuses on capturing low-level feature artifacts, is vulnerable to both Gaussian blur and JPEG compression even at JPEG compression quality 100."}, {"title": "4.4. Qualitative Analysis", "content": "GradCAM visualization. See Fig. 7 for image attribu- tion heat maps generated using GradCAM [15, 43]. The ex- amples are from the ProGAN test set. In addition, the heat maps are averaged across ten predictions to reduce the ran- domness from the patch permutation. The CAM of UnivFD focuses on the class-dependent salient region, whereas the patch-shuffled detector focuses on the entire image region.\nFeature visualization. Because taking an average of the logits generated via a linear layer is equivalent to taking an average of the feature embeddings, we can understand the SFLD embeddings by taking the average of the embeddings over multiple shuffles. Fig. 8 visualizes the feature embed- dings by projecting onto a 2D plane using UMAP [42]. We used the ProGAN test set to extract the embeddings.\nBecause UnivFD learns the features directly from the CLIP visual encoder, the embeddings form class-dependent clusters. This creates class-dependent decision boundary, which may introduce unintended content bias to the real- fake detector. In contrast, because PatchShuffle destroys class-related information from the image, the correspond- ing embeddings show more dispersion within each class."}, {"title": "4.5. Effect of PatchShuffle Hyperparameters", "content": "Improving feature extraction with PatchShuffle. We suggest additional details to get better CLIP features from the shuffled images. To improve stability against the ran- domness introduced by PatchShuffle, we use the averaged logits of \\(N_{\\text{views}} = 10\\) randomly shuffled patch combina- tions for each input image during testing.\nMoreover, in our problem setup, training images are fixed at 256x256 size, while test images can vary in size. Resizing test images is avoided, as image degradation due to resizing (e.g., JPEG compression or blur) has been shown to impact the detection of AI-generated images negatively [49]. Instead, recent detectors [32, 46] prefer cropping over resizing. Our backbone model without PatchShuffle also ex- tracts CLIP features from 224x224 center-cropped images without resizing. However, we can extract information not only from the center of the image but from the entire image by taking advantage of the proposed PatchShuffle, which allows non-consecutive patchwise combinations. We divide the entire test image into non-overlapping patches of the given patch size and combine these patches into 224x224 images. This approach enables the detector to analyze infor- mation from the entire image, rather than being constrained to a single central region. See Appendix D for more details.\nPatch size. The optimal patch size should be sufficiently small to disrupt the underlying image structure while pre- serving some high-level feature artifacts. The results for the performance difference according to patch sizes on a con- ventional benchmark are presented in Fig. 9a. Each patch size model in x-axis refers to the ensemble between the corresponding PatchShuffle model and UnivFD(patch size 224). It can be observed that an too small patch size and an excessively large patch size do not assist the model in capturing useful high-level and low-level feature artifacts. Therefore, the majority of experiments in this paper utilized patch sizes 28x28 and 56x56 according to this result.\nNumber of shuffled views. To ensure the stability of the random patch shuffle, SFLD generates multiple versions of shuffled image from a single test image and employs the average of them as the score. As illustrated in Fig. 9b, mAP enhances with higher \\(N_{\\text{views}}\\). However, due to the tradeoff with inference time, we chose \\(N_{\\text{views}} = 10\\), and all results presented in this paper were obtained with this setting. The results in Fig. 9b are from the PatchShuffle model with a patch size of 28, without an ensemble with UnivFD. The inference time was measured using RTX 4090 GPU."}, {"title": "5. Conclusions", "content": "In this paper, we introduced SFLD, a novel method for detecting AI-generated images that effectively combines global semantic structures and textural structures to im- prove detection performance. By leveraging random patch shuffling and an ensemble of classifiers trained on patches of varying sizes, our approach effectively addresses the shortcomings of existing methods, such as their content bias and susceptibility to image perturbations. Also, We pro- posed a new quality-ensuring benchmark, TwinSynths. It is the first to consider a scenario of infinitely real-like fake images, providing a valuable resource for future research in this area. We demonstrated that SFLD outperforms SOTA methods in generalization to various generators, even in challenging scenarios simulated with TwinSynths."}, {"title": "I. In-the-wild applications of SFLD", "content": "We applied our SFLD to in-the-wild AI-generated im- age detection, especially to a deepfake detection bench- mark. We have already demonstrated performance on a FaceForensics++ [39] subset, which is a deepfake detection benchmark created using face manipulation software [11]. Here, we have added Tab. 8 with experiments using Gener- ated Faces in the Wild [1] datasets. SFLD shows state-of- the-art performance in detecting real-world deepfakes."}, {"title": "J. Pseudocode of SFLD", "content": "See Algorithm 1."}, {"title": "K. Related works", "content": "AI-generated image detection on specific image gen- eration models Research on distinguishing between syn- thetic and real images using deep learning models has in- creased with the development of image generation models. Early works were focused on finding the fingerprints in images generated with GANs, which were targeted at high-performing image generation models. Two major ap- proaches were the use of statistics from the image domain [28, 30] and the training of CNN-based classifiers. In par- ticular, in the case of using CNNs, there are two main ap- proaches: focusing on the image domain [29, 47, 52] or the frequency domain [14,27,48]. Specifically, GAN-generated images have been found to exhibit sharp periodic artifacts in this frequency domain, leading to a variety of applica- tions [8, 14,37].\nRecently, generative models took a big leap forward with the advent of diffusion models, which called for fake im- age detection methods that are able to respond to diffusion models. However, some studies show that existing models trained to detect conventional GANs often fail in images from diffusion models. For example, periodic artifacts that were clearly visible in GAN were rarely found in diffusion models [8, 37]. In response, new detection methods opti- mized for diffusion models have emerged, for example, ap- proaches that use diffusion models to reconstruct test im- ages and evaluate them based on how well they are recon- structed [26, 50, 53].\nGeneralization of AI-generated image detection Re- cently, the community has shifted its focus towards gen- eral AI-generated image detectors that are not specific to GAN or diffusion. In particular, the development of com- mercially deployed generated models that do not reveal the model structure has increased the demand for such a univer- sal detector.\nApart from existing attempts to learn a specialized fea- ture extractor that simply classifies real/fake in a binary manner, Ojha et al. [32] used the features extracted from a strong vision-language pre-trained encoder that is not trained on a particular AI-generated image. Zhu et al. [56] combined anomaly detection methods to increase the dis- crepancy between real and fake image features.\nFurthermore, several studies have concentrated on an- alyzing pixel-level traces on images inevitably left by the image generators. Tan et al. [46] exploited the artifacts that arise from up-sampling operations, based on the fact that most popular generator architectures include up-sampling operations. Chai et al. [4] tried to restrict the receptive field to emphasize local texture artifacts.\nWe design a simple yet powerful general AI-generated image detector that utilizes the feature space of the large pre-trained Vision Language Model. We apply image ref- ormation to capture not only global semantic artifacts but local texture artifacts from the input images, ensuring de- tection performance and generalizability on unseen genera- tors."}]}