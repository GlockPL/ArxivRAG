{"title": "RELBENCH: A Benchmark for Deep Learning on Relational Databases", "authors": ["Joshua Robinson", "Rishabh Ranjan", "Weihua Hu", "Kexin Huang", "Jiaqi Han", "Alejandro Dobles", "Matthias Fey", "Jan E. Lenssen", "Yiwen Yuan", "Zecheng Zhang", "Xinwei He", "Jure Leskovec"], "abstract": "We present RELBENCH, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RELBENCH provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. We use RELBENCH to conduct the first comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, we conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RELBENCH.", "sections": [{"title": "1 Introduction", "content": "Relational databases are the most widely used database management system, underpinning much of the digital economy. Their popularity stems from their table storage structure, making maintenance relatively easy, and data simple to access using powerful query languages such as SQL. Because of their popularity, AI systems across a wide variety of domains are built using data stored in relational databases, including e-commerce, social media, banking systems, healthcare, manufacturing, and open-source scientific repositories (Johnson et al., 2016; PubMed, 1996).\nDespite the importance of relational databases, the rich relational information is typically foregone, as no model architecture is capable of handling varied database structures. Instead, data is \"flattened\u201d into a simpler format such as a single table, often by manual feature engineering, on which standard tabular models can be used (Kaggle, 2022). This results in a significant loss in predictive signal, and creates a need for data extraction pipelines that frequently cause bugs and add to software complexity.\nTo fully exploit the predictive signal encoded in the relations between entities, a new proposal is to re-cast relational data as an exact graph representation, with a node for each entity in the database, edges indicating primary-foreign key links, and node features extracted using deep tabular models, an approach termed Relational Deep Learning (RDL) (Fey et al., 2024). The graph representation allows Graph Neural Networks (GNNs) (Gilmer et al., 2017; Hamilton et al., 2017) to be used as predictive models. RDL is the first approach for an end-to-end learnable neural network model with access to all possible predictive signal in a relational databases, and has the potential to unlock new levels of predictive power. However, the development of relational deep learning is limited by a complete lack of infrastructure to support research, including: (i) standardized benchmark databases and tasks to compare methods, (ii) initial implementation of RDL, including converting data to graph form and GNN training, and (iii) a pilot study of the effectiveness of deep learning.\nHere we present RELBENCH, the first benchmark for relational deep learning. RELBENCH is intended to be the foundational infrastructure for future research into relational deep learning, providing a comprehensive set of databases across a variety of domains, including e-commerce, Q&A platforms, medical, and sports databases. RELBENCH databases span orders of magnitude in size, from 74K entities to 41M entities, and have very different time spans, between 2 weeks and 55 years of training data. They also vary significantly in their relational structure, with the total number of tables varying between 3 and 15, and total number of columns varying from 15 to 140. Each database comes with multiple predictive tasks, 30 in total, including entity classification/regression and recommendation tasks, each chosen for their real-world significance.\nIn addition to databases and tasks, we release open-source software designed to make relational deep learning widely available. This includes (i) the RELBENCH Python package for easy database and task loading, (ii) the first open-source implementation of relational deep learning, designed to be easily modified by researchers, and (iii) a public leaderboard for tracking progress. We comprehensively benchmark our initial RDL implementation on all RELBENCH tasks, comparing to various baselines.\nThe most important baseline we compare to is a strong \"data scientist\u201d approach, for which we recruited an experienced individual to solve each task by manually engineering features and feeding them into tabular models. This approach is the current gold-standard for building predictive models on relational databases. The study, which we open source for reproducibility, finds that RDL models match or outperform the data scientist's models in accuracy, whilst reducing human hours worked by 96%, and lines of code by 94% on average. This cons14titutes the first empirical demonstration of the central promise of RDL, and points to a long-awaited end-to-end deep learning solution for relational data.\nOur website is a comprehensive entry point to RDL, describing RELBENCH databases and tasks, access to code on GitHub, the full relational deep learning blueprint, and tutorials for adding new databases and tasks to RELBENCH to allow researchers to experiment with their problems of interest."}, {"title": "2 Overview and Design", "content": "RELBENCH provides a collection of diverse real-world relational databases along with a set of realistic predictive tasks associated with each database. Concretely, we provide:\n\u2022 Relational databases, consisting of a set of tables connected via primary-foreign key relation-ships. Each table has columns storing diverse information about each entity. Some tables also come with time columns, indicating the time at which the entity is created (e.g., transaction date).\n\u2022 Predictive tasks over a relational database, which are defined by a training table (Fey et al., 2024) with columns for Entity ID, seed time, and target labels. The seed time indicates at which time the target is to be predicted, filtering future data.\nNext we outline key design principles of RELBENCH with an emphasis on data curation, data splits, research flexibility, and open-source implementation."}, {"title": "3 Relational Deep Learning Implementation", "content": "As part of RELBENCH, we provide an initial implementation of relational deep learning, based on the blueprint of Fey et al. (2024).3 Our implementation consists four major components: (1) heterogeneous temporal graph, (2) deep learning model, (3) temporal-aware training of the model, and (4) task-specific loss, which we briefly discuss now.\nHeterogeneous temporal graph. Given a set of tables with primary-foreigh key relations between them we follow Fey et al. (2024) to automatically construct a heterogeneous temporal graph, where each table represents a node type, each row in a table represents a node, and a primary-foreign-key relation between two table rows (nodes) represent an edge between the respective nodes. Some node types are associated with time attributes, representing the timestamp at which a node appears. The heterogeneous temporal graph is represented as a PyTorch Geometric graph object. Each node in the heterogeneous graph comes with a rich feature derived from diverse columns of the corresponding table. We use Tensor Frame provided by PyTorch Frame (Hu et al., 2024) to represent rich node features with diverse column types, e.g., numerical, categorical, timestamp, and text.\nDeep learning model. First, we use deep tabular models that encode raw row-level data into initial node embeddings using PyTorch Frame (Hu et al., 2024) (specifically, we use the ResNet tabular model (Gorishniy et al., 2021)). These initial node embeddings are then fed into a GNN to iteratively update the node embeddings based on their neighbors. For the GNN we use the heterogeneous version of the GraphSAGE model (Hamilton et al., 2017; Fey and Lenssen, 2019) with sum-based neighbor aggregation. Output node embeddings are fed into task-specific prediction heads and are learned end-to-end.\nTemporal-aware subgraph sampling. We perform temporal neighbor sampling, which samples a subgraph around each entity node at a given seed time. Seed time is the time in history at which the prediction is made. When collecting the information to make a prediction at a given seed time, it is important for the model to only use information from before the seed time and thus not learn from the future (post the seed time). Crucially, when sampling mini-batch subgraphs we make sure that all nodes within the sampled subgraph appear before the seed time (Hamilton et al., 2017; Fey et al., 2024), which systematically avoids time leakage during training. The sampled subgraph is fed as input to the GNN, and trained to predict the target label.\nTask-specific prediction head and loss. For entity-level classification, we simply apply an MLP on an entity embedding computed by our GNN to make prediction. For the loss function, we use the binary cross entropy loss for entity classification and L\u2081 loss for entity regression.\nRecommendation requires computing scores between pairs of source nodes and target nodes. For this task type, we consider two representative predictive architectures: two-tower GNN (Wang et al., 2019) and identity-aware GNN (ID-GNN) (You et al., 2021). First, the two-tower GNN computes the pairwise scores via inner product between source and target node embeddings, and the standard Bayesian Personalized Ranking loss (Rendle et al., 2012) is used to train the two-tower model (Wang et al., 2019). Second, the ID-GNN computes the pairwise scores by applying an MLP prediction head on target entity embeddings computed by GNN for each source entity. The ID-GNN is trained by the standard binary cross entropy loss."}, {"title": "4 RELBENCH Datasets", "content": "RELBENCH contains 7 datasets each with rich relational structure, providing a chal-lenging environment for developing and comparing relational deep learning methods. The datasets are carefully processed from real-world relational databases and span diverse domains and sizes. Each database is asso-ciated with multiple individual predictive tasks defined in Section 5. Detailed statis-tics of each dataset can be found in Table 1. We briefly describe each dataset."}, {"title": "5 Predictive Tasks on RELBENCH Datasets", "content": "RELBENCH introduces 30 new predictive tasks defined over the databases introduced in Section 4. A full list of tasks is given in Table 2, with high-level descriptions given in Appendix A (and our website) due to space limitations. Tasks are grouped into three task types: entity classification (Section 5.1), entity regression (Section 5.2), and entity link prediction (Section 5.3). Tasks differ significantly in the number of train/val/test entities, number of unique entities (the same entity may appear multiple times at different timestamps), and the proportion of test entities seen during training. Note this is not data leakage, since entity predictions are timestamp dependent, and can change over time. Tasks with no overlap are pure inductive tasks, whilst other tasks are (partially) transductive."}, {"title": "5.1 Entity Classification", "content": "The first task type is entity-level classification. The task is to predict binary labels of a given entity at a given seed time. We use the ROC-AUC (Hanley and McNeil, 1983) metric for evaluation (higher is better). We compare to a LightGBM classifier baseline over the raw entity table features. Note that here only information from the single entity table is used.\nExperimental results. Results are given in Table 3, with RDL outperforming or matching baselines in all cases. Notably, LightGBM achieves similar performance to RDL on the study-outcome task from rel-trial. This task has extremely rich features in the target table (28 columns total), giving the LightGBM many potentially useful features even without feature engineering. It is an"}, {"title": "5.2 Entity Regression", "content": "Entity-level regression tasks involve predicting numerical labels of an entity at a given seed time. We use Mean Absolute Error (MAE) as our metric (lower is better). We consider the following baselines:\n\u2022 Entity mean/median calculates the mean/median label value for each entity in training data and predicts the mean/median value for the entity.\n\u2022 Global mean/median calculates the global mean/median label value over the training data and predicts the same mean/median value across all entities.\n\u2022 Global zero predicts zero for all entities.\n\u2022 LightGBM learns a LightGBM (Ke et al., 2017) regressor over the raw entity features to predict the numerical targets. Note that only information from the single entity table is used.\nExperimental results. Results in Table 4 show our RDL implementation outperforms or matches baselines in all cases. A number of tasks, such as driver-position and study-adverse, have matching performance up to statistical significance, suggesting some room for improvement. We analyze this further in Appendix C, identifying one potential cause, suggesting an opportunity for improved performance for regression tasks."}, {"title": "5.3 Recommendation", "content": "Finally, we also introduce recommendation tasks on pairs of entities. The task is to predict a list of top K target entities given a source entity at a given seed time. The metric we use is Mean Average Precision (MAP) @K, where K is set per task (higher is better). We consider the following baselines:\n\u2022 Global popularity computes the top K most popular target entities (by count) across the entire training table and predict the K globally popular target entities across all source entities.\n\u2022 Past visit computes the top K most visited target entities for each source entity within the training table and predict those past-visited target entities for each entity.\n\u2022 LightGBM learns a LightGBM (Ke et al., 2017) classifier over the raw features of the source and target entities (concatenated) to predict the link. Additionally, global popularity and past visit ranks are also provided as inputs."}, {"title": "6 Expert Data Scientist User Study", "content": "To test RDL in the most challenging circumstances possible, we undertake a human trial wherein a data scientist solves each task by manually designing features and feeds them into tabular methods such at LightGBM or XGBoost (Chen and Guestrin, 2016; Ke et al., 2017). This represents the prior gold-standard for building predictive models on relational databases (Heaton, 2016), and the key point of comparison for RDL.\nWe structure our user study along the five main data science workflow steps:\n1. Exploratory data analysis (EDA): Explore the dataset and task to understand its characteristics, including what column features there are, and if there is any missing data.\n2. Feature ideation: Based on EDA and intuition from prior experiences, propose a set of entity-level features that the data scientist believes may contain predictive signal for the task.\n3. Feature enginnering: Using query languages such as SQL to compute the proposed features, and add them as extra columns to the target table of interest.\n4. Tabular ML: Run tabular methods such as LightGBM or XGBoost on the table with extra features to produce a predictive model, and record the test performance.\n5. Post-hoc analysis of feature importance (Optional): Common tools include SHAP and LIME, which aim to explain the contribution of each input feature to the final performance.\nConsider for example the rel-hm dataset (schema in Appendix D) and the task of predicting customer churn. Here the CUSTOMER table only contains simple biographical information such as username and joining date. To capture more predictive information, additional features, such as time since last purchase, can be computed using the other tables, and added to the CUSTOMER table. We give a detailed walk-through of the data scientist's work process for solving this specific task in Appendix C. We strongly encourage the interested reader to review this, as it highlights the significant amount of task-specific effort that this workflow necessitates.\nLimitations of Manual Feature Engineering. This workflow suffers from several fundamental limitations. Most obviously, since features are hand designed they only capture part of the predictive signal in the database, useful signal is easily missed. Additionally, feature complexity is limited by human reasoning abilities, meaning that higher-order interactions between entities are often overlooked. Beyond predictive signal, the other crucial limitation of feature engineering is its extremely manual nature-every time a new model is built a data scientist has to repeat this process, requiring many hours of human labor, and significant quantities of new SQL code to design features (Zheng and Casari, 2018). Our RDL models avoid these limitations (see Section 6.1).\nData Scientist. To conduct a thorough comparison to this process, we recruit a high-end data scientist with Stanford CS MSc degree, 4.0 GPA, and 5 years of experience of building machine learning models in the financial industry. This experience includes a significant amount of time building machine learning models in exactly above five steps, as well as broader data science expertise.\nUser Study Protocol.\nBecause of the open-ended nature of feature engineering and model development, we follow a specific protocol for the user study in order to standardize the amount of effort dedicated to each dataset and task. Tracking the 5 steps outlined above, we impose the following rules:\n1. EDA: The time allotted for data exploration is capped at 4 hours. This threshold was chosen to give the data scientist enough time to familiarize themselves with the schema, visualize key relationships and distributions, and take stock of any outliers in the dataset, while providing a reasonable limit to the effort applied.\n2. Feature ideation: Feature ideation is performed manually with pen and paper, and is limited to 1 hour. In practice, the data scientist found that 1 hour was plenty of time to enumerate all promising features at that time, especially since many ideas naturally arise during the EDA process already.\n3. Feature engineering: The features described during the ideation phase are then computed using SQL queries. The time taken to write SQL code to generate the features is uncon-strained in order to eliminate code writing speed as a factor in the study. We do, however, record code writing time for our timing benchmarking. This stage presented the most variability in terms of time commitment, partly because it is unconstrained, but mostly because the implementation complexity of the features itself is highly variable.\n4. Tabular ML: For tabular ML training, we provide a standardized LightGBM training script including comprehensive hyperparameter tuning. The data scientist needs only to feed the table full of engineered features into this training script, which returns test performance results. However, there is some non-trivial amount of work required to transform the output of the SQL queries from the previous section into the Python objects (arrays) required for training LightGBM. Again, the time taken for this additional pre-preocessing is recorded.\n5. Post-hoc analysis of feature importance: Finally, after successfully training a model, an evaluation of model predictions and feature importance is carried out. This mostly serves as a general sanity check and an interesting corollary of the data scientist's work that provides task-specific insights (see Appendix C). In practice, this took no more than a few minutes per task and this time was not counted toward the total time commitment.\nReproducibility. All of the data scientist's workings are released to ensure reproducibility and demonstrate the significant lengths gone through to build as accurate models as possible. In Appendix C we walk through a complete example for a single dataset and task, showing the data-centric insights it yields. An important by-product is a close analysis of which features contribute to model performance, which we believe will help inspire future well-motivated RDL research directions."}, {"title": "6.1 Results", "content": "As well as (i) raw predictive power, we compare the data scientist to our RDL models in terms of (ii) hours of human work, and (iii) number of new lines of code required to solve each task. We measure the marginal effort, meaning that we do not include code infrastructure that is reused across tasks, including for example data loading logic and training scripts for RDL or LightGBM models.\nSummary. Figures 3, 4, and 5 show that RDL learns highly predictive models, outperforming the data scientist in 11 of 15 tasks, whilst reducing hours worked by 96% on average, and lines of code by 94% on average. On average, it took the data scientist 12.3 hours to solve each task using traditional feature engineering. By contrast it takes roughly 30 minutes to solve a task with RDL.\nThis observation is the central value proposition of relational deep learning, pointing the way to unlocking new levels of predictive power, and potentially a new economic model for solving predictive tasks on relational databases. Replacing hand-crafted solutions with end-to-end learnable models has been a key takeaway from the last 15 years of AI research. It is therefore remarkable how little impact deep learning has had on ML on relational databases, one of the most widespread applied ML use cases. To the best of our knowledge, RDL represents the first proposal for a deep learning approach for relational databases that has demonstrated efficacy compared with established data science workflows.\nWe highlight that all RELBENCH tasks were solved with a single set of default hyperparameters (with 2 exceptions requiring small modifications to learning rate, number of epochs, and GNN aggregation function). This demonstrates the robustness of RDL, and that the performance of RDL in Figure 3 is not due to extensive hyperparamter search. Indeed, the single set of RDL hyperparameters is compared to a carefully tuned LightGBM, which was allowed to search over 10 sets of hyperparameters.\nPredictive Power. Results shown in Figures 3. Whilst outperforming the data scientist in 11 of 15 tasks, we note that RDL best outperforms the data scientist on classification tasks, struggling more on regression. Indeed it was necessary for us to apply a \u201cboosting\u201d to the RDL model to improve performance (see Appendix C for details). Even with boosting, the data scientist model outperforms RDL in several cases. One cause we identify is that the MLP output head of the GNN is poorly suited"}, {"title": "7 Related Work", "content": "Graph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet (Deng et al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et al., 2020), TUDataset (Morris et al., 2020), and more recently, the Temporal Graph Benchmark (Huang et al., 2024) have sustained the growth and maturation of graph machine learning as a field. RELBENCH differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RELBENCH presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RELBENCH significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RELBENCH will involve progress in all of these directions.\nRelational Deep Learning. Several works have proposed to use graph neural networks for learning on relational data (Schlichtkrull et al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et al. (2024) proposed a general end-to-end learnable framework for solving"}, {"title": "8 Conclusion", "content": "This work introduces RELBENCH, a benchmark to facilitate research on relational deep learning (Fey et al., 2024). RELBENCH provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction. In addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist. We hope RELBENCH will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering."}, {"title": "A Additional Task Information", "content": "For reference, the following list documents all the predictive tasks in RELBENCH.\n1. rel-amazon\nNode-level tasks:\n(a) user-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n(b) user-ltv: For each user, predict the $ value of the total number of products they buy and review in the next 3 months.\n(c) item-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n(d) item-ltv: For each product, predict the $ value of the total number purchases and reviews it recieves in the next 3 months.\nLink-level tasks:\n(a) user-item-purchase: Predict the list of distinct items each customer will pur-chase in the next 3 months.\n(b) user-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the next 3 months.\n(c) user-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the next 3 months.\n2. rel-avito\nNode-level tasks:\n(a) user-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n(b) user-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n(c) ad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\nLink-level tasks:\n(a) user-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n3. rel-f1\nNode-level tasks:\n(a) driver-position: Predict the average finishing position of each driver all races in the next 2 months.\n(b) driver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n(c) driver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n4. rel-hm\nNode-level tasks:\n(a) user-churn: Predict the churn for a customer (no transactions) in the next week.\n(b) item-sales: Predict the total sales for an article (the sum of prices of the associated transactions) in the next week.\nLink-level tasks:\n(a) user-item-purchase: Predict the list of articles each customer will purchase in the next seven days.\n5. rel-stack\nNode-level tasks:\n(a) user-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n(b) post-votes: For each user post predict how many votes it will receive in the next 3 months\n(c) user-badge: For each user predict if each user will receive in a new badge the next 3 months.\nLink-level tasks:\n(a) user-post-comment: Predict a list of existing posts that a user will comment in the next two years.\n(b) post-post-related: Predict a list of existing posts that users will link a given post to in the next two years.\n6. rel-trial\nNode-level tasks:\n(a) study-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n(b) study-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n(c) site-success: Predict the success rate of a trial site in the next 1 year.\nLink-level tasks:\n(a) condition-sponsor-run: Predict whether this condition will have which spon-sors.\n(b) site-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n7. rel-event\nNode-level tasks:\n(a) user-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n(b) user-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n(c) user-ignore: Predict whether a user will ignore more than 2 event invitations in the next 7 days."}, {"title": "B Experiment Details and Additional Results", "content": "B.1 Detailed Results\nTables 6, 7 and 8 show mean and standard deviations over 5 runs for the entity classification, entity regression and link prediction results respectively.\nB.2 Hyperparameter Choices\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf. Table 9. This verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128 and \"sum\" aggregation) and sample subgraphs identically (disjoint subgraphs of 512 seed entities with a maximum of 128 neighbors for each foreign key). Across task types, we only vary the learning rate and the maximum number of epochs to train for.\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.0001, a \"mean\" neighborhood aggregation scheme, 64 sampled neighbors, and trained for a maximum of 20 epochs. For the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\nB.3 Ablations\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our RELBENCH dataset and tasks present.\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on REL-BENCH. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig. 6 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in Fey et al. (2024).\nNode features and text embeddings. Here we study the effect of node features used in RELBENCH. In the experiments depicted in Fig. 7, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our RELBENCH dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig. 8, we compare GloVe text embedding (Pennington et al., 2014) and BERT text embedding (Devlin et al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in RELBENCH with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig. 9. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model."}, {"title": "CUser Study Additional Details", "content": "C.1 Data Scientist Example Workflow\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RELBENCH tasks. For data scientist solutions to all tasks", "https": ""}]}