{"title": "LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification", "authors": ["Zhen Qin", "Junru Wu", "Jiaming Shen", "Tianqi Liu", "Xuanhui Wang"], "abstract": "We introduce LAMPO, a novel paradigm that leverages Large Language Models (LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike conventional methods, which concatenate all demonstration examples with the test instance and prompt LLMs to produce the pointwise prediction, our framework uses the LLM as a preference machine that makes a relative comparative decision between the test instance and each demonstration. A self-supervised method is then introduced to aggregate these binary comparisons into the final ordinal decision. LAMPO addresses several limitations inherent in previous methods, including context length constraints, ordering biases, and challenges associated with absolute point-wise estimation. Extensive experiments on seven public datasets demonstrate LAMPO's remarkably competitive performance across a diverse spectrum of applications (e.g., movie review analysis and hate speech detection). Notably, in certain applications, the improvement can be substantial, exceeding 20% in an absolute term. Moreover, we believe LAMPO represents an interesting addition to the non-parametric application layered on top of LLMs, as it supports black-box LLMs without necessitating the outputting of LLM's internal states (e.g., embeddings), as seen in previous approaches.", "sections": [{"title": "1 Introduction", "content": "In-Context Learning (ICL) with few-shot demonstrations, also known as few-shot prompting, is a prominent approach for multi-class classification using Large Language Models (LLMs) (Brown et al., 2020). ICL's remarkable performance and training-free nature, combined with off-the-shelf LLMs (e.g., GPT-4 (OpenAI, 2023) and PaLM2 (Google et al., 2023)), has significantly reduced the model adaption costs for new tasks and facilitated the concept of language-model-as-a-service (Sun et al., 2022). Despite these benefits, recent studies indicate that few-shot prompting can sometimes produce unreliable predictions and maybe not always outperform zero-shot prompting (Zhang et al., 2023). To understand these phenomena, extensive research has been conducted, leading to the following insightful observations:\nRestrictive Context Length Limit. Despite extensive research efforts (Ratner et al., 2023;\nPress et al., 2021), the input context length limitation remains a major challenge in applying LLMs in many practical scenarios, especially given limited access to advanced commercial systems such as Reid et al. (2024). This constraint limits the number of demonstrations that can be included in a prompt, adversely affecting LLM performance, especially in challenging tasks with a large label space or long example texts. Also note that supporting long contexts does not mean the models necessarily perform well (Li et al., 2024) due to issues such as ordering bias, discussed next.\nDestructive Ordering Bias. Recent studies (Liu et al., 2023; Lu et al., 2022) have identified a pronounced ordering bias in input prompts for LLMs. For standard few-shot multi-class classification tasks, the arrangement of demonstrations within the prompt can drastically affect the LLM performance, ranging from near chance to state-of-the-art levels. Addressing this issue is inherently complex due to the exponentially increasing combinations of demonstration orders as the number of examples grows."}, {"title": "Challenges of Pointwise Estimation", "content": "In many applications, pointwise estimation is notably more complex compared to comparative estimation. For instance, resolving pointwise relevance estimation addresses relevance ranking, but not conversely, a fundamental observation underpinning the learning-to-rank domain (Liu, 2009). Similarly, in fine-grained sentiment classification, LLMs may struggle to differentiate subtle sentiment nuances, such as between \"very positive\u201d and \u201cpositive\u201d, especially when given limited demonstrations in unfamiliar domains, or facing instructions not covered in their pre-training or instruction fine-tuning phases.\nThis study focuses on ordinal classification (Guti\u00e9rrez et al., 2016), an important subset of multi-class classification problem where the labels possess a natural order. Ordinal classification has extensive applications such as customer reviews analysis (Zhang et al., 2022), relevance rating in web search (Thomas et al., 2023), and opinion monitoring in social media (Barbieri et al., 2020). We propose to use instruction-following LLMs as preference machines to make pairwise comparisons between the test instance and each demonstration. In this paradigm, LLMs make comparative, rather than absolute, judgments in the ordinal label space, and we propose an unsupervised aggregation method to convert these comparison outcomes into the final ordinal prediction.\nWe present LAMPO, a novel framework that leverages LArge language Models as Preference machines for Ordinal classification. LAMPO implicitly addresses the above discussed ICL methods' limitations from three perspectives. First, it avoids packing all demonstrations in a single prompt and thus enables LLMs to leverage an arbitrary number of demonstrations. Second, it includes only two examples (one demonstration and one test instance) in each prompt and treats all prompts independently during the aggregation stage, which substantially mitigates the demonstration ordering bias in LLMs. Third, LAMPO employs the LLM as a preference machine and thus transforms the difficult pointwise estimation problem into a more manageable pairwise comparison problem.\nThe advantages of LAMPO are demonstrated by its competitive performance across seven diverse datasets, compatibility with both open-sourced (i.e., Flan-T5) and black-box LLMs (i.e., PaLM2), and reliance solely on binary generative decisions. This broad versatility makes it adaptable to various black-box LLMs, including those API-based LLMs with restricted access to internal states (e.g., embeddings) (Zhao et al., 2021). However, it is essential to acknowledge that our paradigm entails an increased number of LLM API calls (though with shorter sequences) and is only applicable to instruction-following LLMs, which we elaborate upon in Section 6.\nOur contributions are summarized as follows:\n\u2022 We introduce the LAMPO framework for the important ordinal classification problem. This framework effectively addresses several issues in the existing ICL paradigms.\n\u2022 We present a pipeline that operates without the need for internal logits or embeddings from LLMs, and does not rely on an additional development dataset.\n\u2022 We conduct extensive experiments on seven challenging datasets, covering the analysis of positivity, aspect-based sentiment, hatefulness, irony, and offensiveness of texts. LAMPO consistently demonstrates competitive performance when employed with various LLMs, where the improvements over state-of-the-art ICL approaches can be substantial."}, {"title": "2 Preliminary", "content": "In this section, we first introduce the original ICL method for few-shot ordinal classification with basic notations. Then, we discuss two representative ICL methods, which will shed lights on the difference between our work and the existing literature."}, {"title": "2.1 In-Context Learning (ICL) for Few-shot Ordinal Classification", "content": "ICL is a paradigm that allows LLMs to learn tasks given only a few demonstration examples. The standard m-way multi-class classification aims to assign the test input text x to one candidate answer y in the label space Y = {Yo, ..., Ym\u22121}. This paper focuses on ordinal"}, {"title": "2.2 Contextual Calibration (CC)", "content": "Existing work (Zhao et al., 2021; Fei et al., 2023) found that ICL is sensitive to the quality and ordering of demonstrations, leading to the calibration issue where the model biases towards a certain answer regardless of the input x. To mitigate this issue, contextual calibration (CC) (Zhao et al., 2021) first estimates the bias towards each answer by asking for the prediction probabilities $f_{pcf}$ of a content-free (cf) input such as \u201cN/A\u201d, and then uses them to correct the prediction probabilities $f_{px}$ ($[P(Y_{1}|x), ..., P(Y_{m}|x)]^{T}$) as\n$p^{new} = diag(P_{cf})^{-1}p^{x}$.\nThere are two major limitations of CC. First, CC requires LLM's internal states (i.e., probability logits) thus is not applicable to many black-box LLMs. Second, CC implicitly assumes the context-free input \u201cN/A\u201d can catch the deviation from a uniform prediction of the labels. This assumption might be less problematic in simpler classification scenarios, like binary sentiment analysis (positive vs negative). However, it does not hold for many real-world applications. For instance, in fine-grained classification tasks that include a \u201cneutral\" label, the \u201cN/A\" input likely has a non-trivial probability mass, leading to questionable calibration. Similarly, \u201cN/A\u201d assigns a probability mass to labels such as \u201cnon-hate\u201d in hatefulness detection, \u201cnon-offensive\u201d in offensiveness detection, etc. Thus, the practical utility of CC in such complex scenarios is limited."}, {"title": "2.3 Global Entropy (GlobalE)", "content": "Lu et al. (2022) studied the impact of demonstration ordering given a fixed demonstration set and presented a GlobalE method to identify prompts of specific demonstration orderings that prevent the extremely unbalanced prediction issue. Specifically, GlobalE first generates multiple sets of candidate contexts by sampling different orderings of the demonstrations. Then, for each candidate contexts set Cm, it constructs a probing set by sampling from the LLM: (x,y) ~ PM(\u00b7|Cm). See more details of probing set construction in Appendix B. After that, GlobalE computes the predicted label \u0177\u00a1 for sampled data point under each Cm as follows:\n$\\hat{y}_{i} = arg \\max f_{m}(Y_{j}, I, C_{m}, x_{i}).$\nFinally, it ranks all candidate contexts based on the category label entropy of the predictions of the probing set, and uses the top-ranked context for actual inference. This method is"}, {"title": "3 Scope", "content": "To address the ambiguity surrounding the term \"ICL\u201d (Dong et al., 2022), it is necessary to delineate the scope of this research. We focus on the practical \u201ctrue\u201d k-shot learning (Perez et al., 2021) for multi-class classification with off-the-shelf LLMs, where at most k demonstrations are available for each class. This is distinct from previous studies that can retrieve a fixed number of demonstrations from a potentially large labeled dataset (Liu et al., 2021).\nFurthermore, this work does not assume the presence of a labeled development set for tuning hyperparameters, as such an assumption would violate the aforementioned setting. This better aligns with the standard setting from the original ICL work (Brown et al., 2020) and several key subsequent ICL research (Zhao et al., 2021; Lu et al., 2022) discussed above. The comparison with methods involving test example dependent demonstration retrieval and LLM fine-tuning is beyond the scope of this paper."}, {"title": "4 LAMPO", "content": "We now describe our new paradigm LAMPO for few-shot ordinal classification, with the overall diagram shown in Figure 1. LAMPO consists of two steps: (1) a scoring step that uses the LLM to compute the comparative score between each test instance with each demonstration, and (2) a decision making step that converts those scores into ordinal labels by using offline learned thresholds without an additional labeled development set."}, {"title": "4.1 Scoring", "content": "We score each test instance x as follows:\n$S(x) \\triangleq \\sum_{(x_{i}y_{i}) \\in C} s(F(x, x_{i}),l(y_{i})).$\nThe fundamental computation unit of LAMPO is denoted as F(x, x\u2081), which involves LLM calls to compare input x with each demonstration x\u2081 within the ordinal label space. For example, in hatefulness detection, the prompt can be \u201cGiven Passage A: {x} and Passage B:"}, {"title": "4.2 Decision Making", "content": "Given the score S(x), we need to convert it back into the ordinal label space, which can be reduced to the problem of identifying m 1 thresholds T = {T1, ..., Tm\u22121}, so that:\n$\\begin{cases}Y_{m-1} & \\text{if } S(x) \\geq T_{m-1} \\\\Y_{m-2} & \\text{if } T_{m-2} < S(x) < T_{m-1} \\\\y & \\\\\\Y_{0} & \\text{if } S(x) < T_{1}\\end{cases}$\nNote that if there exists a labeled development set, the search of thresholds would be easy. However, it will violate the \u201ctrue\u201d few-shot setting. We thus propose the following strategies to find T:\nExpected Thresholds. Intuitively, imagine we have a test example with label value Yj, we know the scores of comparing it with each demonstration from Eq. 6 if everything is noise-free. We can easily derive the expected score of an example of any label value Yj:\n$S_{j} = \\sum_{(x_{i}y_{i})|y_{i}output, the query's output is provided by LLM, and y\u1d62 is the type of y\u1d62. This transforms each example into a standard format sentence, which linearises each element in the set into the natural language space defined as S' = {t\u1d62},i =\n1,..., n.\nWe then define a full permutation function group of n training examples, F = {fm},m =\n1, ..., n!, where each function fm takes S' as input and outputs Cm: the concatenation of\na unique permutation. For each prompt candidate Cm, we then sample from the large\nlanguage model M to obtain the probing sequence gm ~ PM(\u00b7|Cm). We stop decoding\nfrom the language model upon generating the special end-of-sentence token defined by\na template, or reach the generation length limit. The probing set construction method is\nillustrated in Figure 2, where the objective is to generate a probing set that shares a similar\ndistribution to the training examples.\nWe run this sampling process for sampled prompt ordering permutations and extract\nprobing examples from them, then gather extracted examples together to form the probing\nset D. Although the probing set contains predicted label for each sentence, there is no\nguarantee on the validity of these labels. Therefore, we discard them from the probing set\nas we are only interested in sampling probes from the language model corresponding to the\ninput distribution."}, {"title": "C Prompt Templates", "content": "We show the prompt template used by LAMPO on each dataset. In general, we tried to\navoid the usage of specific words such as \u201cTweet\u201d or \u201cReview\u201d, but use a more generic\nword \"Passage\u201d. Also, we did not heavily tune the prompts on each dataset, some datasets\neven share the same prompt template. These demonstrate the generality and robustness\nof the LAMPO paradigm, and better performance should be achieved with better prompt\nengineering."}, {"title": "C.1 Twitter Dataset", "content": "Given two Passages, compare their sentiments with labels from ['negative', 'neutral',\n'positive'].\nPassage A: {item\u2081}\nPassage B: {item2}\nWhich Passage is more positive in terms of its sentiment?\nOutput Passage A or Passage B:"}, {"title": "C.2 SST-5 and Yelp-5 Datasets", "content": "Given two Passages, compare their sentiments with labels from ['very negative',\n'negative', 'neutral', 'positive', 'very positive'].\nPassage A: {item\u2081}\nPassage B: {item2}\nWhich Passage is more positive in terms of its sentiment?\nOutput Passage A or Passage B:"}, {"title": "C.3 Lap14 Dataset", "content": "Given two Passages, compare their sentiments towards their respective aspects with\nlabels from ['negative', 'neutral', 'positive'].\nPassage A: {item\u2081} (sentiment towards {aspect1}),\nPassage B: {item2} (sentiment towards {aspect2})\nWhich Passage is more positive in terms of its sentiment towards its aspect?\nOutput Passage A or Passage B:"}, {"title": "C.4 Hate Dataset", "content": "Given two Passages, compare their sentiments with labels from ['non-hate', 'hate'].\nPassage A: {item\u2081}\nPassage B: {item2}\nWhich Passage is more hateful in terms of its sentiment?\nOutput Passage A or Passage B:"}, {"title": "C.5 Offensive Dataset", "content": "Given two Passages, compare their sentiments with labels from ['non-offensive',\n'offensive'].\nPassage A: {item\u2081}\nPassage B: {item2}\nWhich Passage is more offensive in terms of its sentiment?\nOutput Passage A or Passage B:"}, {"title": "C.6 Irony Dataset", "content": "Given two Passages, compare their irony with labels from ['non_irony', 'irony'].\nPassage A: {item\u2081}\nPassage B: {item2}\nWhich Passage is more ironic in terms of its sentiment?\nOutput Passage A or Passage B:"}]}