{"title": "LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification", "authors": ["Zhen Qin", "Junru Wu", "Jiaming Shen", "Tianqi Liu", "Xuanhui Wang"], "abstract": "We introduce LAMPO, a novel paradigm that leverages Large Language\nModels (LLMs) for solving few-shot multi-class ordinal classification tasks.\nUnlike conventional methods, which concatenate all demonstration ex-\\amples with the test instance and prompt LLMs to produce the pointwise\nprediction, our framework uses the LLM as a preference machine that makes a\nrelative comparative decision between the test instance and each demonstra-\ntion. A self-supervised method is then introduced to aggregate these binary\ncomparisons into the final ordinal decision. LAMPO addresses several lim-\nitations inherent in previous methods, including context length constraints,\nordering biases, and challenges associated with absolute point-wise es-\ntimation. Extensive experiments on seven public datasets demonstrate\nLAMPO's remarkably competitive performance across a diverse spectrum\nof applications (e.g., movie review analysis and hate speech detection).\nNotably, in certain applications, the improvement can be substantial, ex-\nceeding 20% in an absolute term. Moreover, we believe LAMPO represents\nan interesting addition to the non-parametric application layered on top of\nLLMs, as it supports black-box LLMs without necessitating the outputting\nof LLM's internal states (e.g., embeddings), as seen in previous approaches.", "sections": [{"title": "1 Introduction", "content": "In-Context Learning (ICL) with few-shot demonstrations, also known as few-shot prompt-\ning, is a prominent approach for multi-class classification using Large Language Models\n(LLMs) (Brown et al., 2020). ICL's remarkable performance and training-free nature, com-\nbined with off-the-shelf LLMs (e.g., GPT-4 (OpenAI, 2023) and PaLM2 (Google et al., 2023)),\nhas significantly reduced the model adaption costs for new tasks and facilitated the concept\nof language-model-as-a-service (Sun et al., 2022). Despite these benefits, recent studies\nindicate that few-shot prompting can sometimes produce unreliable predictions and maybe\nnot always outperform zero-shot prompting (Zhang et al., 2023). To understand these\nphenomena, extensive research has been conducted, leading to the following insightful\nobservations:\nRestrictive Context Length Limit. Despite extensive research efforts (Ratner et al., 2023;\nPress et al., 2021), the input context length limitation remains a major challenge in applying\nLLMs in many practical scenarios, especially given limited access to advanced commercial\nsystems such as Reid et al. (2024). This constraint limits the number of demonstrations that\ncan be included in a prompt, adversely affecting LLM performance, especially in challenging\ntasks with a large label space or long example texts. Also note that supporting long contexts\ndoes not mean the models necessarily perform well (Li et al., 2024) due to issues such as\nordering bias, discussed next.\nDestructive Ordering Bias. Recent studies (Liu et al., 2023; Lu et al., 2022) have identified a\npronounced ordering bias in input prompts for LLMs. For standard few-shot multi-class\nclassification tasks, the arrangement of demonstrations within the prompt can drastically\naffect the LLM performance, ranging from near chance to state-of-the-art levels. Address-\ning this issue is inherently complex due to the exponentially increasing combinations of\ndemonstration orders as the number of examples grows."}, {"title": "Challenges of Pointwise Estimation.", "content": "In many applications, pointwise estimation is notably\nmore complex compared to comparative estimation. For instance, resolving pointwise\nrelevance estimation addresses relevance ranking, but not conversely, a fundamental ob-\nservation underpinning the learning-to-rank domain (Liu, 2009). Similarly, in fine-grained\nsentiment classification, LLMs may struggle to differentiate subtle sentiment nuances, such\nas between \"very positive\u201d and \u201cpositive\u201d, especially when given limited demonstrations\nin unfamiliar domains, or facing instructions not covered in their pre-training or instruction\nfine-tuning phases.\nThis study focuses on ordinal classification (Guti\u00e9rrez et al., 2016), an important subset\nof multi-class classification problem where the labels possess a natural order. Ordinal\nclassification has extensive applications such as customer reviews analysis (Zhang et al.,\n2022), relevance rating in web search (Thomas et al., 2023), and opinion monitoring in social\nmedia (Barbieri et al., 2020). We propose to use instruction-following LLMs as preference\nmachines to make pairwise comparisons between the test instance and each demonstration.\nIn this paradigm, LLMs make comparative, rather than absolute, judgments in the ordinal label\nspace, and we propose an unsupervised aggregation method to convert these comparison\noutcomes into the final ordinal prediction.\nWe present LAMPO, a novel framework that leverages LArge language Models as Preference\nmachines for Ordinal classification. LAMPO implicitly addresses the above discussed ICL\nmethods' limitations from three perspectives. First, it avoids packing all demonstrations in\na single prompt and thus enables LLMs to leverage an arbitrary number of demonstrations.\nSecond, it includes only two examples (one demonstration and one test instance) in each\nprompt and treats all prompts independently during the aggregation stage, which substan-\ntially mitigates the demonstration ordering bias in LLMs. Third, LAMPO employs the LLM\nas a preference machine and thus transforms the difficult pointwise estimation problem into\na more manageable pairwise comparison problem.\nThe advantages of LAMPO are demonstrated by its competitive performance across seven\ndiverse datasets, compatibility with both open-sourced (i.e., Flan-T5) and black-box LLMs\n(i.e., PaLM2), and reliance solely on binary generative decisions. This broad versatility\nmakes it adaptable to various black-box LLMs, including those API-based LLMs with\nrestricted access to internal states (e.g., embeddings) (Zhao et al., 2021). However, it is\nessential to acknowledge that our paradigm entails an increased number of LLM API calls\n(though with shorter sequences) and is only applicable to instruction-following LLMs,\nwhich we elaborate upon in Section 6.\nOur contributions are summarized as follows:\n\u2022 We introduce the LAMPO framework for the important ordinal classification problem.\nThis framework effectively addresses several issues in the existing ICL paradigms.\n\u2022 We present a pipeline that operates without the need for internal logits or embeddings\nfrom LLMs, and does not rely on an additional development dataset.\n\u2022 We conduct extensive experiments on seven challenging datasets, covering the analysis of\npositivity, aspect-based sentiment, hatefulness, irony, and offensiveness of texts. LAMPO\nconsistently demonstrates competitive performance when employed with various LLMs,\nwhere the improvements over state-of-the-art ICL approaches can be substantial."}, {"title": "2 Preliminary", "content": "In this section, we first introduce the original ICL method for few-shot ordinal classification\nwith basic notations. Then, we discuss two representative ICL methods, which will shed\nlights on the difference between our work and the existing literature."}, {"title": "2.1 In-Context Learning (ICL) for Few-shot Ordinal Classification", "content": "ICL is a paradigm that allows LLMs to learn tasks given only a few demonstration examples.\nThe standard m-way multi-class classification aims to assign the test input text x to one\ncandidate answer y in the label space \\(Y = {Yo, ..., Ym\u22121}\\). This paper focuses on ordinal"}, {"title": "2.2 Contextual Calibration (CC)", "content": "Existing work (Zhao et al., 2021; Fei et al., 2023) found that ICL is sensitive to the quality and\nordering of demonstrations, leading to the calibration issue where the model biases towards\na certain answer regardless of the input x. To mitigate this issue, contextual calibration\n(CC) (Zhao et al., 2021) first estimates the bias towards each answer by asking for the\nprediction probabilities \\(fpcf\\) of a content-free (cf) input such as \u201cN/A\u201d, and then uses them\nto correct the prediction probabilities \\(fpx ([P(Y\u2081|x), ..., P(Ym|x)]T)\\) as\n\\[p^{new} = diag(P^{cf})^{-1}p^{x}.\\]\nThere are two major limitations of CC. First, CC requires LLM's internal states (i.e., proba-\nbility logits) thus is not applicable to many black-box LLMs. Second, CC implicitly assumes\nthe context-free input \u201cN/A\u201d can catch the deviation from a uniform prediction of the labels.\nThis assumption might be less problematic in simpler classification scenarios, like binary\nsentiment analysis (positive vs negative). However, it does not hold for many real-world\napplications. For instance, in fine-grained classification tasks that include a \u201cneutral\" label,\nthe \u201cN/A", "N/A": "ssigns a probability mass to labels such as \u201cnon-hate\u201d in hatefulness\ndetection,"}, {"title": "2.3 Global Entropy (GlobalE)", "content": "Lu et al. (2022) studied the impact of demonstration ordering given a fixed demonstration\nset and presented a GlobalE method to identify prompts of specific demonstration orderings\nthat prevent the extremely unbalanced prediction issue\u00b2. Specifically, GlobalE first generates\nmultiple sets of candidate contexts by sampling different orderings of the demonstrations.\nThen, for each candidate contexts set \\(C_m\\), it constructs a probing set by sampling from the\nLLM: \\((x_i, y_i) \\sim P_M(\\cdot|C_m)\\). See more details of probing set construction in Appendix B. After\nthat, GlobalE computes the predicted label \\(\\hat{y}_i\\) for sampled data point under each \\(C_m\\) as\nfollows:\n\\[\\hat{y}_i = arg \\underset{Y_j}{max} f_M(Y_j, I, C_m, x_i).\\]\nFinally, it ranks all candidate contexts based on the category label entropy of the predictions\nof the probing set, and uses the top-ranked context for actual inference. This method is"}, {"title": "3 Scope", "content": "To address the ambiguity surrounding the term \"ICL\u201d (Dong et al., 2022), it is necessary to\ndelineate the scope of this research. We focus on the practical \u201ctrue\u201d k-shot learning (Perez\net al., 2021) for multi-class classification with off-the-shelf LLMs, where at most k demon-\nstrations are available for each class. This is distinct from previous studies that can retrieve\na fixed number of demonstrations from a potentially large labeled dataset (Liu et al., 2021).\nFurthermore, this work does not assume the presence of a labeled development set for\ntuning hyperparameters, as such an assumption would violate the aforementioned setting.\nThis better aligns with the standard setting from the original ICL work (Brown et al., 2020)\nand several key subsequent ICL research (Zhao et al., 2021; Lu et al., 2022) discussed above.\nThe comparison with methods involving test example dependent demonstration retrieval\nand LLM fine-tuning is beyond the scope of this paper."}, {"title": "4 LAMPO", "content": "We now describe our new paradigm LAMPO for few-shot ordinal classification, with the\noverall diagram shown in Figure 1. LAMPO consists of two steps: (1) a scoring step that\nuses the LLM to compute the comparative score between each test instance with each\ndemonstration, and (2) a decision making step that converts those scores into ordinal labels\nby using offline learned thresholds without an additional labeled development set."}, {"title": "4.1 Scoring", "content": "We score each test instance x as follows:\n\\[S(x) \\triangleq \\sum_{(x_i,Y_i) \\in C} s(F(x, x_i), l(y_i)).\\]\nThe fundamental computation unit of LAMPO is denoted as \\(F(x, x_i)\\), which involves LLM\ncalls to compare input x with each demonstration \\(x_i\\) within the ordinal label space. For\nexample, in hatefulness detection, the prompt can be \u201cGiven Passage A: {x} and Passage B:"}, {"title": "4.2 Decision Making", "content": "Given the score \\(S(x)\\), we need to convert it back into the ordinal label space, which can be\nreduced to the problem of identifying m 1 thresholds \\(T = {T_1, ..., T_{m-1}}\\}, so that:\n\\[\\hat{y} = \\begin{cases} Y_{m-1} & \\text{if } S(x) \\geq T_{m-1} \\\\ Y_{m-2} & \\text{if } T_{m-2} < S(x) < T_{m-1} \\\\ & ... \\\\ Y_0 & \\text{if } S(x) < T_1 \\end{cases}\\]\nNote that if there exists a labeled development set, the search of thresholds would be easy.\nHowever, it will violate the \u201ctrue\u201d few-shot setting. We thus propose the following strategies\nto find T:\nExpected Thresholds. Intuitively, imagine we have a test example with label value \\(Y_j\\),\nwe know the scores of comparing it with each demonstration from Eq. 6 if everything is\nnoise-free. We can easily derive the expected score of an example of any label value \\(Y_j\\):\n\\[S_j = \\sum_{(x_i,y_i)|y_i<Y_j} (l(y_i) + 1) + \\sum_{(x_i,y_i)|y_i=Y_j} l(y_i) +\\sum_{(x_i,y_i)|y_i>Y_j}(l(y_i) - 1)\\]\nThis computation does not require any LLM calls since we just derive the expected score\ngiven the demonstration labels. Then the expected threshold \\(T_j\\) is simply \\((S_j + S_{j-1})/2\\), and\nthe other thresholds can be derived similarly. However, this is under the assumptions that\nthe demonstration labels are noise free and pairwise comparisons are unbiased globally,\nwhich is hardly true in practice. Still, we can view this as a global prior of the thresholds.\nSelf-supervised Thresholds. Systematic bias can arise from both the demonstrations and\nLLM itself and the expected thresholds may not be optimal. We are thus interested in\ndecision thresholds that are adaptive to the current demonstrations and LLM. Inspired\nby Lu et al. (2022), we leverage the probing set to rank candidate thresholds based on the\nglobal entropy of predicted labels. While we believe applying the probing set to our problem\nis a contribution, the probing set construction method itself is not our contribution, which\nis deferred to Appendix B. Different from Lu et al. (2022) where their candidate space is\ncombinatorial in terms of the number of demonstrations, our search space is combinatorial\nin terms of the number of integer thresholds, which is usually small. Note that the LLM\ncalls that compare each example in the probing set and demonstrations are only required\nonce before the threshold search. The major caveat of this self-supervised approach is\npotentially large variance due to the limited number of demonstrations. We can treat these\n\"self-supervised thresholds\u201d as the empirical observation."}, {"title": "The Mixture.", "content": "Given the pros and cons of the above two methods, we propose to use a\nmixture of above two strategies, which can be treated as a simple combination of prior and\nempirical thresholds. Since we do not assume a development set, we simply use the average\nof their corresponding thresholds without tuning the weights for all experiments."}, {"title": "4.3 A Running Example", "content": "Let's consider a 5-shot 3-way ordinal classification with labels {negative (0), neutral (1),\npositive (2)}. The expected thresholds would be {10, 20}, which can be derived as follows:\na negative example is expected to have a score 5 in a noise-free setting (it will tie with the 5\nnegative demonstrations, and lose to the other 5 neutral and 5 positive demonstrations, thus\n0+0+5), a neutral example with score 15(5 + 5 + 5), and a positive example with score\n25(5+10+10), thus the expected thresholds are 10 = (5 + 15)/2 and 20 = (15 + 25)/2.\nFurthermore, let's assume the self-supervised thresholds are {12, 21}, then our final thresh-\nolds would be {11, 20.5}. Now given a test instance x, we do comparisons using LLM with\neach of the 15 demonstrations and get a score S(x) = 16 according to Eq. 5, then the final\npredicted label of x will be \u201cneutral\u201d."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Datasets", "content": "We use 7 challenging ordinal classification datasets, i.e., Twitter, SST-5, Hate, Yelp-5, Lap14,\nOffensive, and Irony. We follow the publicly hosted setting\u00b3 where each dataset provides 3\nseeds of demonstrations in each configuration, i.e., 5-shot and 10-shot, which are used to\ncompute mean and standard deviation of the concerned metrics on the test set. The same\npublicly hosted setting is used for all compared methods for a fair and reproducible comparison. The\ndatasets are summarized in Table 3 in Appendix A, including their tasks, metrics, and label\nspaces."}, {"title": "5.2 Baselines and Model Configurations", "content": "For few-shot LLM-based methods, we compare LAMPO with ICL, CC, and GlobalE, which\nwere discussed in Section 2. For few-shot LLM-based methods, we use the black-box PaLM2-S model (text-Bison\u2074) and the white-box Flan-T5-XXL (Chung et al., 2022). We use the more\ngeneral generation mode, that does not request any internal states of LLMs, for ICL, GlobalE,\nand LAMPO. We use the scoring mode to get the required internal logit values of label texts\nwith Flan-T5-XXL for CC.\nNote that due to potential data leakage issues of modern LLMs (Zhou et al., 2023) (see more\ndiscussions in Section 6.4), we will focus on comparing different methods under the same\nLLM, and only briefly compare trends of performance across different LLMs."}, {"title": "5.3 Experimental Results", "content": "This section describes our experimental results, which are shown in Table 1 and Table 2. We\ndiscuss datasets in groups due to their diverse settings and behaviors.\nResults on Twitter. LAMPO exhibits the best performance on PaLM2-S and Flan-T5-XXL.\nContrastingly, CC is ineffective due to the presence of a \"neutral\" class that violates its\ncalibration assumptions. Furthermore, it's worth noting that LAMPO is the sole method\ncapable of handling 10-shot scenarios on this dataset using the concerned popular LLMs. The\nslightly lower performance in this 10-shot setting compared to the 5-shot setting across both\nLLMs highlights the importance of the demonstrations' quality, as other factors, including\nordering bias, have been largely mitigated.\nResults on SST-5. LAMPO works well on this highly challenging dataset with 5-level la-\nbels, outperforming ICL and GlobalE with both LLMs. 10-shot also consistently outperforms\n5-shot with LAMPO, while it is infeasible to perform 10-shot with other methods. CC with\n5-shot performs well with Flan-T5-XXL, as it avoids generation errors on this challenging\n(for Flan-T5-XXL) dataset. See more discussions in the summary of results below.\nResults on Yelp-5. Results on Yelp-5 mainly shows the capability of LAMPO - no other\nmethods are feasible given the long example texts. Also, 10-shot works better than 5-shot\non both LLMs for LAMPO. LAMPO allows us to find that there still exists a gap between\nLAMPO and supervised methods, so the community is suggested to focus on challenging\ndatasets like Yelp-5 for future work.\nResults on Lap14. LAMPO works well on this dataset, with the best performance on\nboth PaLM2-S and Flan-T5-XXL, showing LAMPO is compatible with the challenging\naspect-based sentiment analysis task. CC works poorly since there is a \u201cneutral", "general": "ateful", "non-hate": "or the \u201cN/A\u201d input.\nResults on Offensive and Irony. LAMPO is competitive and robust for these two datasets\non both LLMs. One noticeable benefit of LAMPO is its robustness across different shots -\nthe performance of other methods on Offensive actually drops with 10 shots, but LAMPO\nis robust and generally improves performance with more shots. CC does not work as the\nprobability mass will be concentrated on \u201cnon-offensive"}, {"title": "6 Discussion and Limitation", "content": ""}, {"title": "6.1 When do we expect LAMPO to be most valuable?", "content": "LAMPO shows competitive performance across different domains. One most prominent\nimprovement LAMPO got is on the hatefulness detection dataset, where we hypothesize\nthat PaLM2-S does not have a strong prior understanding of the task, especially concerning\nthe \u201cnon-hate\u201d label. However, LAMPO mainly leverages more general knowledge such\nas \"which passage is more hateful?\u201d and does not use the actual label text until the very last\ndecision making step. Thus, we believe LAMPO is particularly valuable when the LLM\nlacks a strong prior understanding of the task's label space."}, {"title": "6.2 Restriction to Instruction-following LLMs", "content": "Earlier research before the ubiquity of instruction-following LLMs on ICL mainly depends\non text continuation. On the other hand, as hinted by the prompts in Appendix C, the com-\nparison prompts used in LAMPO are instruction-following and are likely only applicable on\ninstruction-following LLMs. This is one limitation of our framework. However, we argue\nthe limitation may not be significant as instruction-following LLMs are becoming the norm."}, {"title": "6.3 Cost", "content": "We acknowledge that LAMPO does involve an increased number of LLM API calls compared\nto standard ICL methods. However, we note that: (1) The number of calls only linearly scales\nwith the number of demonstrations, which is usually not big in the concerned \u201cfew-shot\u201d\nsetting. On the other hand, LAMPO's new capability allows for the potential utilization of\nan unlimited number of demonstrations, making the pursuit of more efficient methods a\npromising future research direction. (2) Each prompt is shorter, so each API call is faster\nand less expensive than traditional ICL. (3) All comparison API calls can be paralleled, thus\nthe latency can be lower than traditional ICL, given sufficient parallelism."}, {"title": "6.4 Data Leakage", "content": "It is possible that certain LLMs saw some datasets during their pre-training or instruction\nfine-tuning, which is a major concern of fair evaluation in the era of LLMs (Zhou et al.,\n2023). We mainly focused on comparing different methods using the same LLMs so the\ncomparisons are fair. In fact, as most existing LLM pre-training and instruction tuning\nmethods adopt the pointwise strategy, a potential data leakage will benefit them more,\ncompared to our pairwise framework. Thus, the improvements achieved by LAMPO over\nthese baselines further demonstrate its effectiveness."}, {"title": "6.5 Reproducibility", "content": "As noted, all datasets used in this paper are publicly available, and we did not change any\nof their demonstrations, labels, or any other content. The exact prompt templates we used\nare shown in Appendix C."}, {"title": "7 Related Work", "content": ""}, {"title": "In-Context Learning.", "content": "We have discussed several popular ICL-based methods in Section 2,\nwhich is a prominent family of approaches that leverages LLM for few-shot multi-class\nclassification. Another emerging paradigm, which has gained popularity recently, involves\nretrieving distinct demonstrations for each test instance (Rubin et al., 2021). These demon-\nstrations are sourced from a potentially vast annotated demonstration pool, which is very\ndifferent from the \u201ctrue\u201d few-shot setting studied in this work."}, {"title": "Comparisons using LLMs.", "content": "We note that the \"pairwise\" or \"comparative\" paradigm is a\nclassic concept in various domains. LLMs can serve as evaluators to compare preferences\nof generative model's outputs (Kocmi & Federmann, 2023; Vu et al., 2024; Yan et al., 2023),\nsuch as to determine which summary is better in text summarization, with many important\napplications such as language model alignment (Liu et al., 2024). In contrast, our approach\ncompares inputs to derive an output. For web search ranking, given a query, pairs of\ncandidate documents can be compared using LLMs in terms of their relevance to the\nquery (Qin et al., 2024). This task significantly differs from ours, and it involves a quadratic\nincrease in API calls with the number of documents under each query. In summary, existing\nwork only share the very general idea of doing pairwise comparisons with very different\nproblem setups where natural pairs are readily available. In contrast, our work pioneers the\nexploration of LLMs as preference machines for the important few-shot ordinal classification\ntask and demonstrates LLMs' comparative capability in broader dimensions (i.e., traditional\nsentiment, hatefulness, offensiveness, irony, and aspect-based sentiment)."}, {"title": "\"Non-parametric\" application of LLMs.", "content": "Very recently, Xu et al. (2023) have shown a \u201cnearest\nneigbor\" style application of LLMs. We posit that our paradigm offers a compelling addition\nto this non-parametric perspective of LLM utilization, where existing research (Xu et al.,\n2023) hinges upon extracting embeddings that are not generally accessible, and neglects to\nexplore the preference machine perspective of LLMs.\""}, {"title": "8 Conclusion", "content": "This paper introduces a simple yet novel paradigm, LAMPO, for the important k-shot\nmulti-class ordinal classification problem with LLMs. LAMPO effectively addresses several\ninherent limitations of traditional ICL methods, and demonstrates strong performance\non 7 publicly available datasets covering diverse topics. Moreover, LAMPO is versatile,\ncompatible with both black-box and white-box LLMs, and capable of accommodating an\narbitrary number of demonstrations."}]}