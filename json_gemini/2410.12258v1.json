{"title": "Understanding Expert Structures on Minimax Parameter Estimation in Contaminated Mixture of Experts", "authors": ["Fanqi Yan", "Huy Nguyen", "Dung Le", "Pedram Akbarian", "Nhat Ho"], "abstract": "We conduct the convergence analysis of parameter estimation in the contaminated mixture\nof experts. This model is motivated from the prompt learning problem where ones utilize\nprompts, which can be formulated as experts, to fine-tune a large-scaled pre-trained model for\nlearning downstream tasks. There are two fundamental challenges emerging from the analysis:\n(i) the proportion in the mixture of the pre-trained model and the prompt may converge to zero\nwhere the prompt vanishes during the training; (ii) the algebraic interaction among parameters\nof the pre-trained model and the prompt can occur via some partial differential equation and\ndecelerate the prompt learning. In response, we introduce a distinguishability condition to\ncontrol the previous parameter interaction. Additionally, we also consider various types of\nexpert structures to understand their effects on the parameter estimation. In each scenario, we\nprovide comprehensive convergence rates of parameter estimation along with the corresponding\nminimax lower bounds.", "sections": [{"title": "Introduction", "content": "Originally introduced by Jacobs et al. [13], mixture of experts (MoE) has been widely used as\na statistical machine learning framework to integrate the power of several sub-models based on\nthe principle of divide and conquer. In particular, it consists of two main components, namely\nmultiple experts formulated as either regression functions [12] or classifiers [20], each of which is\nresponsible for handling a few specific tasks; and a gating function softly partitioning the input space\ninto regions where corresponding specialized experts are assigned larger weights than the others.\nThanks to its flexibility, there is a surge of interests in utilizing MoE model for various applications,\nincluding natural language processing [14, 35, 5, 28, 4, 26], computer vision [29, 18, 30], autonomous\ndriving [27], multimodal fusion [8], reinforcement learning [1, 25] and others [17, 9, 7, 19]. Recently,\npractitioners have also incorporated MoE models into parameter-efficient fine-tuning methods for\nlarge-scale pre-trained models [34, 32, 16]. More specifically, they attach small trainable experts\ncalled prompts to a frozen pre-trained model, which we refer to as a contaminated MoE in the sequel.\nBy doing so, they are able to learn downstream tasks efficiently without having to train the whole\nmodel expensively. Nevertheless, the potentials for further enhancing this fine-tuning technique\nhave been restricted owing to the absence of the theoretical foundation for the contaminated MoE\nmodel. Thus, our main objective in this work is to provide a comprehensive convergence analysis"}, {"title": "Preliminaries", "content": "We start this section with the following distinguishability condition to control the merging of the\nprompt into the pre-trained model mentioned in Section 1.\nDefinition 1 (Distinguishability). We say that $f$ is distinguishable from $f_0$ if the following holds:\nGiven any two distinct components $(a_i, b_i, v_i) \\in \\Theta$, for $i = 1,2$, if we have real coefficients $\\eta_i$ for\n$0 \\leq i \\leq 2$ such that $\\eta_1\\eta_2 \\leq 0$ and\n$\\eta_0f_0(Y\\vert\\varphi(a_0^T X + b_0), v_0) + \\sum_{i=1}^2\\eta_i f(Y\\vert\\sigma((a_i)^T X + b_i), V_i))TX = 0,$\nfor almost surely $(X,Y) \\in \\mathcal{X} \\times \\mathcal{Y}$, then $\\eta_i = 0, \\forall i = 1,2$.\nExample. If $f_0$ belongs to the family of Student's t-distribution, we can verify that $f$ is distin-\nguishable from $f_0$. By contrast, if $f_0$ is a Gaussian density and $\\varphi = \\sigma$, then $f$ is not distinguishable\nfrom $f_0$.\nNote that if $f$ is distinguishable from $f_0$, the merging $f_0(Y\\vert\\varphi(a_0^T X +b_0),v_0) = f(Y\\vert\\sigma((a_i)^T X +\nb_i), v_i)$, for almost surely $(X, Y) \\in \\mathcal{X} \\times \\mathcal{Y}$, cannot occur as it violates the distinguishability condition.\nFurthermore, according to Proposition 1, whose proof is in Appendix D.2, this condition also ensures\nthat the contaminated MoE model (1) is identifiable.\nProposition 1 (Identifiability). Let $(\\Lambda,G), (\\Lambda', G')$ be two components in $\\Xi$. Suppose that $f$ is\ndistinguishable from $f_0$, then if the identifiability equation $p_{\\Lambda,G}(Y\\vert X) = p_{\\Lambda',G'}(Y\\vert X)$ holds for almost\nsurely $(X, Y) \\in \\mathcal{X} \\times \\mathcal{Y}$, we obtain that $(\\Lambda, G) = (\\lambda', G')$.\nSubsequently, we proceed to characterize the convergence behavior of density estimation in\nTheorem 1."}, {"title": "Linear Expert \u03c3", "content": "In this section, we determine the convergence rates of parameter estimation when the expert $\\sigma$ is\nlinear, that is, it takes the form $\\sigma(z) = a'z + b'$ for any $z \\in \\mathbb{R}$, where $a', b'$ are known constants.\nSince the rates vary with the family of the density $f_0$, we further consider two scenarios when $f_0$ is\nand is not a Gaussian density."}, {"title": "When $f_0$ is not a Gaussian density", "content": "We begin with the simpler scenario when $f_0$ is not a Gaussian density as we can verify that $f$ is\ndistinguishable from $f_0$ under this scenario in Proposition 2.\nProposition 2. If $f_0$ does not belong to the family of Gaussian densities, then $f$ is distinguishable\nfrom $f_0$.\nGiven this result, we are now ready to capture the\nconvergence of parameter estimation in the following theorem:\nTheorem 2 (MLE rates). When the expert function $\\sigma$ is of linear form and $f_0$ is not a Gaussian\ndensity, we obtain the MLE convergence rates as follows:\n$\\sup_{\\lambda^*,G^* \\in \\Xi} E_{p_{\\lambda^*,G^*}} [\\vert\\hat{\\lambda}_n - \\lambda^*\\vert^2] \\leq \\frac{\\log n}{n},$\n$\\sup_{\\lambda^*,G^* \\in \\Xi} E_{p_{\\lambda^*,G^*}} [(\\lambda^*)^2\\vert\\vert\\hat{G}_n - G^*\\vert\\vert^2] \\leq \\frac{\\log n}{n}.$\nThe first bound indicates that the mixing pro-\nportion estimator $\\hat{\\lambda}_n$ converges to the ground-truth value $\\lambda^*$ at the rate of order $\\~{O}(n^{-1/2})$, which\nis parametric on the sample size $n$. On the other hand, it follows from the second bound that\nthe convergence rate of the prompt parameter estimation $\\hat{G}_n = (\\hat{a}_n,\\hat{b}_n, \\hat{v}_n)$ to its true counter-\npart $G^* = (a^*, b^*, v^*)$ is slower than $\\~{O}(n^{-1/2})$ as it hinges upon the vanishing rate of $\\lambda^*$. This\nrate dependence reflects the prompt vanishing issue mentioned in the \"Challenges\" paragraph in\nSection 1."}, {"title": "When $f_0$ is a Gaussian density", "content": "Next, we consider the scenario when $f_0$ belongs to the family of Gaussian densities. Under this\nscenario, it can be checked that $f$ might not be distinguishable from $f_0$, depending on the structure\nof the expert $\\varphi$ in the pre-trained model. To this end, we continue to divide the analysis into two\nsmaller scenarios where the function $\\varphi$ is linear and non-linear, respectively."}, {"title": "Linear expert \u03c6", "content": "Recall that in this case, both the experts $\\varphi$ and $\\sigma$ are of linear forms, and the density $f_0 = f$ is a\nGaussian density. Notably, this setting induces two potential obstacles in our analysis.\nFirstly, the prompt might merge into the pre-trained model. In particular, let us assume that\n$\\varphi(a_0^T X + b_0) = \\alpha_0(a_0X + b_0) + \\beta_0$ and $\\sigma((a^*)^T X + b^*) = a^*((a^*)^T X + b^*) + \\beta^*$, where $\\alpha_0, \\alpha^* \\neq 0$\nand $\\beta_0, \\beta^*$ are some known constants. Then, if $a^*\\rightarrow a_0$ and $b^* \\rightarrow \\alpha_0 \\frac{b_0 + \\beta_0 - \\beta^*}{\\alpha^*}$ and $v^* \\rightarrow v_0$ as\n$n \\rightarrow \\infty$, it follows that the expert $\\sigma((a^*)^T X + b^*)$ will converge to its counterpart $\\varphi(a_0X + b_0)$.\nConsequently, we can justify that $f$ is not distinguishable from $f_0$. Furthermore, since the prompt\n$f(Y\\vert\\sigma((a^*)^TX+b^*), v^*)$ will also converge to the pre-trained model $f_0(Y\\vert\\varphi(a_0X+b_0), v_0)$, we have\nto cope with the prompt merging issue mentioned in the \"Challenges\" paragraph in Section 1 as\nwell. This issue will be demonstrated to make the convergence behavior of the mixing proportion\nestimator become complicated in Theorem 4. For simplicity, we will focus only on the case when\n$\\varphi$ and $\\sigma$ are identity functions, that is, when $a_0 = a^* = 1$ and $\\beta_0 = \\beta^* = 0$. Other cases can be\nnaturally generalized based on the aforementioned limits of $a^*$ and $b^*$.\nSecondly, there is an interaction between parameters $b$ and $v$ via the following heat equation:\n$\\frac{\\partial^2 f}{\\partial b^2}(Y\\vert a^T X + b,v) = \\frac{\\partial}{\\partial v} f(Y\\vert a^T X + b,v).$\nThe derivation of this equation can be seen in Appendix D. Note that such interaction has been\ncaptured in prior works on Gaussian MoE [12, 24] where it is shown to decelerate the convergence\nrates of involved parameters significantly. However, to the best of our knowledge, the effects of\nthat interaction has never been explored when the ground-truth parameter values $\\lambda^*, G$ depend\non the sample size $n$. Thus, we will demistify this problem in Theorem 4.\nFor simplicity, let us denote $\\Delta G := (\\Delta a, \\Delta b, \\Delta v) := (a - a_0, b - b_0, v - v_0)$ for any component\n$(a, b, v) \\in \\Theta$."}, {"title": "Non-linear expert \u03c6", "content": "In this section, we draw our attention to the scenario where $f_0$ is a Gaussian density, the expert $\\sigma$\nis of linear form, while $\\varphi$ is a non-linear expert. For instance, $\\varphi$ can be one among the activation\nfunctions sigmoid, ReLU and tanh, which are commonly used in practice."}, {"title": "Non-linear Expert \u03c3", "content": "Moving to this section, we will look into the convergence behavior of the MLE when the expert $\\sigma$\nin the prompt takes a non-linear form such as sigmoid, ReLU and tanh functions, etc. Similar to\nSection 3, we also separate the analysis into two scenarios when $f_0$ is excluded from and included\nin the family of Gaussian densities, respectively."}, {"title": "When $f_0$ is not a Gaussian density", "content": "Firstly, when $f_0$ does not belong to the family of Gaussian densities, we can verify that $f$ is\ndistinguishable from $f_0$ by employing similar arguments for Proposition 2. Thus, the MLE admits\nthe same convergence behavior as that in Section 3.1, which is exhibited in Theorem 7 whose proof\nis in Appendix A.3.\nTheorem 7 (MLE rates). When the expert function $\\sigma$ is of non-linear form and $f_0$ is not a\nGaussian density, we obtain the MLE convergence rates as follows:\n$\\sup_{\\lambda^*,G^* \\in \\Xi} E_{p_{\\lambda^*,G^*}} [\\vert\\hat{\\lambda}_n - \\lambda^*\\vert^2] \\leq \\frac{\\log n}{n},$\n$\\sup_{\\lambda^*,G^* \\in \\Xi} E_{p_{\\lambda^*,G^*}} [(\\lambda^*)^2\\vert\\vert\\hat{G}_n - G^*\\vert\\vert^2] \\leq \\frac{\\log n}{n}.$"}, {"title": "When $f_0$ is a Gaussian density", "content": "Subsequently, we examine the convergence behavior of parameter estimation when $f_0$ belongs to\nthe family of Gaussian densities. Since the distinguishability of the prompt $f$ from the pre-trained\nmodel $f_0$ hinges upon the structure of the expert function $\\varphi$, we proceed to divide the analysis into\ntwo smaller cases: when $\\varphi$ is a linear and non-linear expert function, respectively."}, {"title": "Linear expert \u03c6", "content": "In this section, we concentrate on the scenario where the expert $\\sigma$ takes a non-linear form, while $\\varphi$\nis a linear expert function.\nAlthough this scenario is opposite to that in Section 3.2.2 where the expert $\\sigma$ is linear and its\ncounterpart $\\varphi$ is non-linear, these two scenarios have one thing in common, which is the expert\nstructure distinction. Therefore, it can be validated that they share several convergence properties.\nIn particular, since the structures of the two experts $\\varphi$ and $\\sigma$ are different from each other, it\nis obvious that $\\sigma((a^*)^TX + b^*)$ cannot converge to $\\varphi(a) X + b_0)$ as $n \\rightarrow \\infty$ for almost surely\n$X \\in \\mathcal{X}$. As a result, the prompt $f(Y\\vert\\sigma((a^*)^TX + b^*), v^*)$ does not converge to the pre-trained\nmodel $f_0(Y\\vert\\varphi(a) X + b_0), v_0)$, either. This means that the expert structure difference helps tackle\nthe prompt merging issue. Furthermore, it also guarantees that $f$ is distinguishable from $f_0$.\nConsequently, we demonstrate in the following theorem that the convergence behavior of the MLE\nin this scenario resembles that in Section 3.2.2.\nTheorem 8 (MLE rates). Suppose that $f_0$ is a Gaussian density, the expert function $\\sigma$ is non-\nlinear, while its counterparty $\\varphi$ is a linear expert. Then, we obtain the following MLE convergence\nrates:\n$\\sup_{\\lambda^*,G^* \\in \\Xi} E_{p_{\\lambda^*,G^*}} [\\vert\\hat{\\lambda}_n - \\lambda^*\\vert^2] \\leq \\frac{\\log n}{n},$\n$\\sup_{\\lambda^*,G^* \\in \\Xi} E_{p_{\\lambda^*,G^*}} [(\\lambda^*)^2\\vert\\vert\\hat{G}_n - G^*\\vert\\vert^2] \\leq \\frac{\\log n}{n}.$"}, {"title": "Non-linear expert \u03c6", "content": "In this section, we take into account the last scenario when both the expert functions $\\varphi$ and $\\sigma$ are\nnon-linear.\nSince both $\\varphi$ and $\\sigma$ are non-linear expert functions, we observe that if $\\varphi$ is not equal to $\\sigma$\nalmost everywhere, then the fact $\\sigma((a^*)^TX + b^*)$ converges to $\\varphi(a) X + b_0)$ as $n \\rightarrow \\infty$ for almost\nsurely $X \\in \\mathcal{X}$ does not hold. Consequently, the prompt $f$ become distinguishable from the the\npre-trained model $f_0$, which allows us to demonstrate that the MLE convergence behavior in this"}, {"title": "Discussion", "content": "In this paper, we carry out the convergence analysis of maximum likelihood parameter estimation\nunder the contaminated MoE model which is the mixture of a frozen pre-trained model and a\ntrainable prompt for learning downstream tasks. Due to the dependence of ground-truth parameters\non the sample size, there are several challenges for the theoretical analysis, namely, the prompt\nvanishing when the mixing proportion tends to zero; the prompt merging when it converges to the\npre-trained model; and the parameter interaction via the heat equation. For better understanding,\nwe divide our analysis into multiple scenarios based upon the linear and non-linear structures of\nthe expert functions as well as the distribution family of the pre-trained model. In each scenario,\nwe provide corresponding MLE convergence rates along with their minimax lower bounds to show\nthat those rates are optimal. Additionally, we also conduct experiments to verify our theoretical\nresults.\nPractical implications. From our convergence analysis whose result is summarized in Table 1,\nwe observe that the MLE convergence behaves better when the expert functions associated with the\npre-trained model and the prompt do not share the same structure. Since several large-scale pre-\ntrained models often adopt non-linear experts in practice, our theory implies that we can achieve a\ngood performance on downstream tasks even when attaching the prompts with simple linear experts\nto those models. However, if ones still would like to employ non-linear experts in the prompt, then\nour analysis suggests that it is better to use different expert functions from those in the pre-trained\nmodel.\nLimitations and Future directions. A few natural directions arise from our work. Firstly, the\ncurrent contaminated MoE model considers only one prompt. It is of practical importance to extend"}]}