{"title": "Decoding-based Regression", "authors": ["Xingyou Song", "Dara Bahri"], "abstract": "Language models have recently been shown capable of performing regression tasks wherein numeric predictions are represented as decoded strings. In this work, we provide theoretical grounds for this capability and furthermore investigate the utility of causal auto-regressive sequence models when they are applied to any feature representation. We find that, despite being trained in the usual way - for next-token prediction via cross-entropy loss - decoding-based regression is as performant as traditional approaches for tabular regression tasks, while being flexible enough to capture arbitrary distributions, such as in the task of density estimation.", "sections": [{"title": "1. Introduction", "content": "Despite being originally developed for the purpose of text generation and chat applications, large language models (LLMs) have recently been applied for new applications, one of which is regression tasks, and more broadly the prediction of numeric outcomes. Vacareanu et al. (2024) have shown service-based LLMs such as ChatGPT and Gemini capable of in-context regression with performance comparable to that of traditional regression methods such as random forests, while Song et al. (2024) have shown smaller models can be trained specifically on large amounts of regression data to even outperform baselines.\nFor input-output pair (x, y), where x is a feature vector and y is a real number, a regression model's performance is determined by two factors: how it processes x and how it models the output y - e.g. the representation of y as well as distributional assumptions on $p(y|x)$. While previous works in text-to-text regression can be seen as cases where both representations are based on tokens, this combination is not necessarily required. Tang et al. (2024) investigate the isolated case where LLM embeddings of x are attached to traditional numeric tensor heads, while Nguyen et al. (2024) investigate the case when embeddings for x are attached to an in-context neural process. Both can be seen as particular instances of \u201ctext-to-anything\" regression using different regression heads. However, there has not been work investigating the inverse \u201canything-to-text\" regression case, i.e. using decoding-based regression heads alone, where for example, 1.23 is represented as <1><.><2><3>, despite this style of tokenization being used in several works training language models for specific numeric tasks, such as arithmetic (Nogueira et al., 2021), linear algebra (Charton, 2022), and symbolic regression (d'Ascoli et al., 2022).\nIn contrast to traditional numeric or even parameterized distribution (e.g. Gaussian) heads, decoding-"}, {"title": "2. Related Work and Motivation", "content": "The idea of text-to-text regression is especially relevant as LLMs are currently being fine-tuned as \u201cGenerative Reward Models\" (Mahan et al., 2024), i.e. end-to-end scoring methods for reinforcement learning feedback (Bai et al., 2022; Ziegler et al., 2019) or LLM-as-a-Judge (Zheng et al., 2023). Such reward modeling methods can be simpler than other forms such as Bradley-Terry (Bradley & Terry, 1952) which requires appending additional prediction heads. However, little analysis has been done in isolation on the theoretical and modeling capabilities of using text, or more generally tokens, to represent floating point numbers. Understandably, one could argue that regular supervised fine-tuning over numbers represented as strings is unprincipled, considering that there is no notion of numeric distance when using cross-entropy loss.\nHowever, we argue that this approach is actually a natural outcome of observed phenomena and techniques proposed in recent works - below, we provide a brief overview of relevant work on regression heads, with additional techniques outlined in Appendix C.\nTensor-based Representations: By far, the most commonly used regression head is pointwise value tensor, i.e. a learnable projection of $\\phi(x)$, although deterministic transformations may be performed afterwards; e.g. Bradley & Terry (1952) can be seen as appending an additional sigmoid head to enforce the raw outputs to be within [0, 1]. All such cases require y-values to be in a normalized space, to allow stability during training.\nIn the case of probabilistic outputs, one may apply parametric distribution heads such as Gaussians, e.g. $p(y|x) = N(\\mu, \\sigma^2)$ where $\\mu, \\sigma$ are learned projections over $\\phi(x)$. However, these apply heavy assumptions on the underlying output distribution, which may not hold true in real world cases. One can extend the head to be a finite (Bishop, 1994) or even infinite (Rasmussen, 1999) mixture of Gaussians to increase flexibility. Such mixture techniques can be more broadly seen within the realm of density estimation (Parzen, 1962; Rosenblatt, 1956) in which a complex distribution may be estimated using multiple simpler basis distributions.\nHistogram (Riemann) Distribution: One such basis that has recently received wide use in deep learning applications is the piece-wise constant (Riemann) basis, for learning histograms over a finite support set ${y_1,..., y_n} \\subset R$ via softmax parametrization, i.e. $p(y_i|x) = Softmax(i) (\\phi(x)^Tw)$, which has led to strong results in value-based reinforcement learning (Bellemare et al., 2017; Imani & White, 2018) and tabular data (Chen et al., 2022; Hollmann et al., 2023). However, when applied as a"}, {"title": "3. Decoding-Based Regression", "content": "In a standard regression problem (generalized for probabilistic outputs), we assume there exists some underlying conditional distribution p(y|x) over features $x \\in R^D$ and output $y \\in R$. We wish to learn parametric model $p_\\theta(y|x)$ of the conditional distribution given training data ${(X_i, Y_i)}_1^N$.\nWe now formalize the representation of floating point numbers as a sequence of tokens. Given a vocabulary of all possible tokens V, we define a token representation as a mapping from y \u2208 R to fixed-length sequence of tokens $(t_1, . . ., t_k) \\in V$. Since the token space is finite while R is uncountable, this mapping is lossy (i.e. not invertible) and introduces a notion of rounding error.\nGiven a feature representation, or encoder $\\phi(x) \\in R^d$, we can apply a decoding-based regression head, represented as auto-regressive prediction model $p_\\theta(t_k | \\phi(x), t_1, . . ., t_{k-1})$ from which we may thus obtain an end-to-end model $p_\\theta(y|x) = p_\\theta(t_1, ..., t_k | \\Phi(x)) = \\prod_{k=1}^K P_\\theta(t_k | \\phi(x), t_1 ... t_{k-1})$.\nIf y are restricted to [0, 1] (via scale normalization for example), then we show we can represent any smooth density p(y|x) with an increasing level of granularity as more tokens are used in the numeric representation, under some \u201cuniversality\u201d assumptions on $p_\\theta$. This can be seen intuitively with a tree-based construction, i.e. for a base B, the vocabulary contains <0>, <1>, ..., <B \u2013 1>, and y is simply represented by its base-B expansion up to a length K. This setting aligns with standard data-science practices of also normalizing y-values according to training data or known bounds.\nHowever, there are cases in which we would like to use an unnormalized tokenization scheme. Such cases include multi-task regression, in which different tasks may have varying y-scales, or expressing very wide y-ranges for which appropriately normalizing y-values for the correct balance between numeric stability and expressiveness would be very tedious.\nIn this case, we provide a generalization of the common IEEE-754 floating point representation (IEEE, 2019) for base-2, to any base B. This can be constructed by observing that each number can be represented as $s \\cdot B^e \\cdot m$ where $s \\in \\{-1,+1\\}$ is the sign, $e \\in Z$ is the exponent, and $m\\in [0, B)$ is the mantissa. Given length parameters E and M, our tokenization is therefore $<s><s_e><e_1>...<e_{\\mathcal{E}}><m_1>...<m_{\\mathcal{M}}>$ where $s_e, e_1, e_2, . . ., e_{\\mathcal{E}}$ are the sign and base-B representation of the exponent e and $m_1, m_2, . . ., m_{\\mathcal{M}}$ are the most significant base-B digits of the mantissa m. E.g. if (B=10, E=3, M=4), then 10$^{-222}$ \u00d7 1.23456789 will be represented as <+><-><2><2><2><1><2><3><4>.\nSigns <s>, <$s_e$> can have their own dedicated <->, <+> tokens or optionally reuse the <0>,<1> tokens from V; this made little difference in results."}, {"title": "3.3. Pointwise Estimation", "content": "In many cases, one may only be interested in a scalar quantity of interest M($p_\\theta$) of the model's distribution (e.g. its mean). If $p_\\theta$ matches the true distribution p perfectly, then for a given pointwise loss $l : R^2 \\rightarrow R$ the goal is then to select M(p) which minimizes $E_{y\\sim p(\\cdot|x)} [l(M(p), y)]$. For common error functions such as mean squared error and mean absolute error, it is well known that the optimal values are the mean and median of p(x) respectively, an observation which Lukasik et al. (2024) also use to improve LLM decoding.\nIt goes without saying that the mode can be approximated using e.g. beam search (Graves, 2012), but efficiently estimating other common general pointwise representatives M(p) from pure temperature samples is a broad topic - for example, one can efficiently approximate the true median from the Harrell-Davis estimator (Harrell & Davis, 1982), and more generally we refer the reader to Lehmann (1983) on statistical point estimators.\nEspecially for unnormalized tokenization, additional care needs to be taken, since in practice, the model can have a miniscule but non-zero probability of decoding an arbitrarily large outlier, even if the underlying true distribution is bounded. Such outliers can easily sway non-robust estimators such as the sample mean, as observed in Song et al. (2024). This issue fundamentally comes from the fact that some tokens are more significant than others, prompting the use of alternative tokenizations based on coding theory which are robust to corruptions, which we show can be effective in our experiment section.\nAlternatively, decoding techniques from the LLM literature can also be used, e.g. top-k (Fan et al., 2018) or top-p (Holtzman et al., 2020), or even simply decreasing the temperature to increase model confidence and thereby filter out possible outliers. One can also avoid decoding altogether and use recently proposed RAFT (Lukasik et al., 2025) which estimates M(p) using a query-based approach using a fixed evaluation set Y, e.g. for mean, $E_{y\\sim p_\\theta} [y] \\approx \\frac{N}{|Y|} \\sum_{y'\\in Y} P_\\theta(y) \\cdot y$, although the choice of Y may be non-trivial to obtain an unbiased estimate, especially over unnormalized tokenizations. This may also defeat the purpose of using a decoder head, which offers several density estimation benefits, as we discuss below. Overall, the choice of method for computing pointwise representations we leave as a hyperparameter to be tuned depending on the application."}, {"title": "3.4. Density Estimation and Theory", "content": "During training, to allow the full probabilistic modeling benefits of using a decoder head, we apply the standard cross-entropy loss over all sequence tokens. For a model $p_\\theta$ and target $y = (t_1, ..., t_K)$, a single unit of cross-entropy loss (omitting x to simplify notation) will be:\n$H(y, p_\\theta) = \\sum_{k=1}^K\\sum_{t_k \\in V} -1(t_k = t_k) log p_\\theta(t_k|t_1, ..., t_{k-1})$\nThe expected loss over all y sampled from the true distribution is then $E_{y\\sim p} [H(y, p_\\theta)]$.\nGiven our tree-based tokenization and training loss, we provide formal guarantees for estimating one-dimensional densities on [0, 1]. Note that densities with finite support can be shifted and rescaled to have support in [0, 1]. Define $\\lambda_k : [0, 1) \\rightarrow \\{0, 1\\}^k$ be the operation that returns the first k bits after the radix point in the (possibly infinite) binary representation of y. Concretely, if y = 0.$b_1b_2b_3b_4$... then $\\Lambda_k(y) = (b_1, ..., b_k)$. We abuse notation and interpret $\\Lambda_k$'s output either as a sequence or as the real number it represents ($\\Sigma_1^k b_i2^{-k}$) depending on the context. The analysis is presented using binary (base-2) representations (e.g. V = {0, 1}) for simplicity, but it holds for arbitrary bases. First, we define an assumption on the learnability of our model.\nLet H(p, q) = $E_{y\\sim p} -log q(y)$ denote the cross-entropy between discrete distributions p and q. Note that H(p, p) is just the Shannon entropy of p. Call parametric model $p_\\theta$ K-bit universal if for all discrete distributions p on K-bit strings (equivalently, $2^K$ categories),  \nDefinition 1 (K-bit universality).\n$\\min_{\\theta} H(p, p_\\theta) = H(p, p)$.\nIn other words, $p_\\theta$ is K-bit universal if it is flexible enough to fit any discrete distribution on $2^K$ categories.\nWe now give our main result below. The proof is deferred to Appendix B.\nAssume our decoding-based regression model $p_\\theta : \\{0,1\\}^K \\rightarrow \\triangle^{2^K}$ is K-bit universal. Furthermore, define $p^k_\\theta$ as probability of the first k bits under $p_\\theta$, marginalizing out the remaining bits. Concretely,\nTheorem 1.\n$p^k_\\theta((b_1,...,b_k)) = \\sum_{\\{b_{k+1},...,b_{K}\\}} P_\\theta((b_1,..., b_k))$.\nSeen another way, $p^k_\\theta$ is the distribution over k-bit strings that results from auto-regressive decoding $p_\\theta$ for exactly k steps.\nNow let f: [0, 1] \u2192 R be any smooth one-dimensional density function. With ${Y_1, . . ., Y_N }$ as i.i.d. draws from f, define $\\theta_e$ as the maximum likelihood estimator on the truncated sequence of K bits. Concretely,  \n$\\hat{\\theta}_e (Y_1, ..., Y_N) = argmin_{\\theta} \\frac{1}{N} \\sum_{n=1}^N - log p_\\theta (\\lambda_K (Y_n))$.\nDefine risk R as the mean integrated squared error between true density f and its estimator $f_N(Y_1, ..., Y_N)$:\n$R(f, \\hat{f}) = E_{Y_1,..., Y_N\\sim f} \\int_0^1 (f(x) - \\hat{f}(x))^2 dy$.\nDefine density $\\hat{f}_N(y) = 2^k p^k_{\\hat{\\theta}_e(Y_1,...,Y_N)}(\\lambda_K(y))$ for $y \\in [0, 1]$. Then we have,\n$R(f, \\hat{f}_N) \\approx \\int_0^1 \\frac{2^{-2k}}{12} f'(y)^2 dy + \\frac{2^k}{N}$, $\\forall k \\leq K$."}, {"title": "4. Experiments", "content": "Our main goals for experiments are to:\n\u2022 Demonstrate decoders can be effective swap-in replacements to common pointwise regression heads.\n\u2022 Establish the density estimation capabilities of the decoding-based head over any distribution over R.\n\u2022 Ablate the effect of decoder size and sequence-specific methods such as error correction on performance.\nTo maintain fairness and minimize confounding factors, all neural network methods have access to the same encoder $\\phi(x)$, which is a large multi-layer perceptron (MLP) with ReLU activations, with hyperparameter sweeps over number of layers (up to 5) and hidden unit sizes (up to 2048). This minimizes the decoding head's contribution to representation learning (as opposed to numeric modeling), since it will be relatively negligible in size to $\\phi(x)$, i.e. less than 10% of the total network parameter count."}, {"title": "5. Conclusion and Future Work", "content": "Throughout this paper, we thoroughly investigated the many benefits but also drawbacks of using decoding-based regression. We described a natural tokenization scheme for both normalized and unnormalized y-values, and theoretically established its risk minimization properties. Empirically, we showed that it can be competitive as, or even outperform traditional pointwise heads for regression tasks. Furthermore, it is also capable of density estimation over a variety of conditional distributions p(y|x), and can further outperform common baselines such as Gaussian mixtures and Riemann distributions.\nNumerous ways to extend decoding-based regression include improved tokenization schemes or other basis distributions besides piecewise constants. Further applications exist in other domains such as computer vision, where the encoder may be a convolutional network, or for multi-target regression, where the regressor needs to predict multiple different y-value targets. More broadly however, we hope this will also be a valuable reference for the language modeling community, and provides a principled explanation for the use of supervised fine-tuning over numeric targets."}, {"title": "Appendix", "content": "For full transparency, we display the plots over all tasks in AMLB (Gijsbers et al., 2024). We confirm the data-efficiency of the decoder head against the Riemann distribution head on nearly every regression task. Furthermore, we observe numerous cases where both distributional methods outperform the pointwise head, especially in low data regimes."}, {"title": "A.3. Alternative Tokenization Schemes: Hamming-Distance", "content": "One possible criticism of the default tree-based tokenization in the normalized decoding case, is the vulnerability to small changes in the left-most significant tokens, which can cause large numeric changes in the actual number. Qin (2018) notes this and proposes an alternative \u201cHamming Distance-based\" binary representation which is robust to bitwise edits, and upper bounds the possible distortion |y' \u2013 y| as a function of the edit distance between the Hamming representations of y' and y. For example, if the binary length is 3, the representation for all integers {0,1,..., 23} is {(000), (001), (010), (100), (011), (101), (110), (111)} which can also be used in the normalized case for {0/23, 1/23, ..., 7/23} c [0, 1]. In Figure 13, we show however, such a representation may not lead to better regression results, which we hypothesize is due to this representation being more difficult to learn."}]}