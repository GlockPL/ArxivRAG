{"title": "Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models", "authors": ["Jifeng Wang", "Kaouther Messaoud", "Yuejiang Liu", "Juergen Gall", "Alexandre Alahi"], "abstract": "Recent progress in motion forecasting has been substantially driven by self-supervised pre-training. However, adapting pre-trained models for specific downstream tasks, especially motion prediction, through extensive fine-tuning is often inefficient. This inefficiency arises because motion pre-diction closely aligns with the masked pre-training tasks, and traditional full fine-tuning methods fail to fully leverage this alignment. To address this, we introduce Forecast-PEFT, a fine-tuning strategy that freezes the majority of the model's parameters, focusing adjustments on newly introduced prompts and adapters. This approach not only preserves the pre-learned representations but also significantly reduces the number of parameters that need retraining, thereby enhancing efficiency. This tailored strategy, supplemented by our method's capability to efficiently adapt to different datasets, enhances model efficiency and ensures robust performance across datasets without the need for extensive retraining. Our experiments show that Forecast-PEFT outperforms traditional full fine-tuning methods in motion prediction tasks, achieving higher accuracy with only 17% of the trainable parameters typically required. Moreover, our comprehensive adaptation, Forecast-FT, further improves prediction performance, evidencing up to a 9.6% enhancement over conventional baseline methods.", "sections": [{"title": "I. INTRODUCTION", "content": "TRAJECTORY prediction is a challenging task in au-tonomous driving, crucial for ensuring safety and op-erational efficiency [14], [22], [24], [18]. The complexitystems from accounting for dynamic interactions among roadusers and varying conditions. To address these challenges,significant advancements have been made in self-supervisedlearning (SSL) methods. Pre-trained models such as Traj-MAE [6], SEPT [26], and Forecast-MAE [7] have shownconsiderable potential in enhancing trajectory prediction.\nDespite these advancements, current SSL approaches typ-ically utilize only pre-trained encoders and rely on full fine-tuning for downstream tasks. This practice fails to leverage thefull capabilities of pre-trained models. In other domains, suchas language [10], vision [19], and 3D point cloud analysis[45], masked autoencoders are pre-trained with \u201cmaskingand reconstruction\" tasks but fine-tuned on classification ordiscrimination tasks. For motion prediction, this approachoften overlooks the connection between pre-training tasks andmotion prediction. Specifically, trajectory prediction is a formof reconstruction, indicating that pre-trained decoders couldbe effectively used for this task.\nMoreover, integrating a new, randomly initialized decoderduring fine-tuning can significantly alter the encoder's parame-ters, leading to catastrophic forgetting and the loss of valuablepre-trained knowledge. This highlights the need for fine-tuningstrategies that preserve the encoder's learned capabilities whileadapting to motion prediction tasks.\nParameter Efficient Fine-Tuning (PEFT) has gained tractionin fields such as vision and NLP [23], [37]. These techniquesmodify only a small subset of a pre-trained model's param-eters while freezing the majority, thereby reducing computa-tional and storage demands. Building on the PEFT paradigm,we propose Forecast-PEFT, a parameter-efficient fine-tuningframework tailored specifically for trajectory prediction.\nForecast-PEFT retains the encoder and decoder from thepre-trained model, with their parameters remaining frozenduring the fine-tuning stage. To bridge the input gap, task dif-ference, and domain gap between pretraining and finetuning,we propose the following components:\nContextual Embedding Prompt (CEP): The input ofencoder shifts from masked historical trajectories, fu-ture trajectories, and contextual data during pre-trainingto complete historical trajectories and contextual dataduring finetuning and inference. CEPs bridge this gap,preserving pre-trained knowledge by directing attentionto critical features, ensuring the model maintains itscapabilities and focuses on relevant information.\nModality-Control Prompt (MCP): The task of decodertransitions from reconstructing masked elements duringpretraining to forecasting multi-modal future trajectories.MCPs generate multiple trajectory modes, guiding thedecoder to produce diverse predictions, and seamlesslyadapting pre-trained knowledge to forecasting tasks.\nParallel Adapter (PA): PAs are integrated into eachtransformer layer to capture local information, com-plementing the pre-trained layers that focus on globaldependencies. This design allows the model to efficientlyadapt to new data without extensive retraining, enhancingits ability to generalize across different datasets andimproving overall model robustness and efficiency.\nWhile effective fine-tuning within a single dataset is im-portant, autonomous driving systems must operate in a widerange of environments, requiring models to generalize across"}, {"title": "II. RELATED WORK", "content": "1) Self-Supervised Pretraining in Motion Prediction: Re-cently, studies on self-supervised pretraining methods formotion prediction have achieved remarkable success. Theseapproaches adopt pretext tasks to pretrain foundation modelsto learn latent semantic information of both map and mo-tion of road users, and then fine-tune the model on motionprediction task. Existing pre-training methods can be dividedinto contrastive representation learning [30], [43], [2], [35],[8], [1] and generative masked representation learning [12],[6], [27], [7], [26]. The contrastive learning approaches alignand distinguish representations of views with the same high-level semantics or not. Social-NCE[30] employs contrastiveloss and negative sample augmentation to extract informationabout social interactions from motion trajectories. PreTraM[43] and MENTOR[35] explore the use of contrast learningof intra-domain (map-map) and cross-domain (trajectory-map)features. Generative masked pretraining methods mainly relyon the masked autoencoders [19] to learn latent representa-tion by reconstructing the masked input. Traj-MAE[6] andSEPT[26] explore to pretrain the map and motion encoderseparately with specially designed mask-reconstruction strat-egy. Forecast-MAE[7] takes agents' trajectory and poly-linesof lane segments as single tokens, and then randomly masksout the entire agents' history or future trajectory, as wellas lane segments, at the token level, before feeding theminto the Transformer [39] backbone. Despite the success ofpretraining in motion prediction, two issues have not beenfully explored. Firstly, all methods in the pretraining stageutilize a decoder that is subsequently discarded. During thefine-tuning stage for motion prediction, a new decoder mustbe designed specifically for prediction purposes. Secondly, theadaptation for motion prediction still demands the resource-intensive full fine-tuning method, and effective Parameter-Efficient Fine-Tuning (PEFT) methods have yet to be explored.Thus, we explore both the re-use of pre-trained decoder andPEFT techniques in motion prediction domain for parameter-efficient fine-tuning."}, {"title": "III. METHOD", "content": "In this section, we first introduce the standard fine-tuningpipeline for a pre-trained motion prediction model in SectionIII-A. Then, in Section III-B and III-C, we illustrate ourForecast-PEFT and Forecast-FT frameworks, respectively.\n\nA. Standard Fine-Tuning Pipeline in Forecast-MAE\n1) Overview of Forecast-MAE: Forecast-MAE [7] uses aself-supervised MAE-based [19] method for motion forecast-"}, {"title": "2) Parameter-efficient Fine-tuning:", "content": "Parameter-efficientfine-tuning (PEFT) aims to fine-tune only a small set ofparameters, which might be a set of newly added parameters[23], [28] or a small subset of the pre-trained model'sparameters. Existing PEFT methods can be primarily dividedinto additive methods and selective methods, along with ahybrid of these methods. The additive methods, which includeapproaches like Prompt Tuning [23], Adapter Tuning [20],[34], [33], LORA [21], and Side Tuning [48], involve addingnew trainable parameters or modules to pre-trained modelsfor efficient fine-tuning while preserving original parameters.The selective methods such as LayerNorm-Tuning [37], BiasTuning [46], [4], and strategies like Partial-k [19], [44], focuson tuning a subset of existing model parameters, targetingthose critical for specific tasks to reduce computational load.Additionally, there is an increasing trend towards integratingthese methods, aiming to combine the flexibility of additivemethods with the targeted efficiency of selective methodsfor optimal model performance and resource efficiency.One recent work MoSA [25] introduces a modular low-rankadapter for motion style transfer, but only for adapting motionforecasting models pre-trained on one domain with sufficientdata to new domains such as unseen agent types and scenecontexts. In this paper, we propose a PEFT approach for thegeneral motion prediction domain, which includes contextualembedding prompts, modality control prompts, and paralleladapters."}, {"title": "A. Standard Fine-Tuning Pipeline in Forecast-MAE", "content": "First, agent trajectories and lane segments are transformedinto vectorized tokens. These tokens undergo processing us-ing Feature Pyramid Network (FPN) [29] for N agents(TH = FPN(AH), TF = FPN(AF), where TH,F \u2208RN\u00d7C) and Mini-PointNet [36] for M lane segments (TL =MiniPointNet(L), where TL \u2208 RM\u00d7C). TH,TF,TL arehistory, future, lane tokens respectively, and C is the em-bedding dimension. Additionally, semantic and positional em-beddings (PE) are incorporated into these tokens. Notably,Forecast-MAE masks either the future or history trajectoriesof agents in a complementary manner. This means that if thehistory trajectory of an agent is masked, its future trajectoryremains unmasked, and vice versa.\nThen, the architecture employs a standard Transformer-based [39] autoencoder. The encoder and decoder processesare formulated as:\nTE = Encoder(concat (TH,TF,TL) + PE), (1)\nM' = Decoder(concat (TE, M) + PE), (2)\nwhere M = (MH, MF, ML) is the mask tokens concate-nated to the encoded latent tokens of Encoder, and M' =(MH, MF, M\u2081\u2082) is the decoded mask tokens.\nThe decoder's role is to reconstruct masked elements,predicting normalized coordinates for masked trajectories andlane segments, with reconstruction targets processed through:\nPi = PredictionHead;(M), i \u2208 {H, F, L}. (3)"}, {"title": "2) Pre-training Loss:", "content": "The model utilizes L1 loss LH, LFfor trajectory reconstruction and mean squared error (MSE)loss LL for lane segments reconstruction, and XH, XF,ALcorresponds to the loss weight respectively. The final loss,denoted as LRE and defined in the following equation, is also"}, {"title": "B. Forecast-PEFT", "content": "The pre-trained Forecast-MAE model, depicted in Figure2, is enhanced by Forecast-PEFT through the integration ofspecialized prompts and adapters as additive modules, fine-tuned to enhance trajectory prediction tasks:\n1) Contextual Embedding Prompt (CEP): The Forecast-PEFT framework incorporates CEP vectors denoted as PCE toaugment the pre-trained Encoder's contextual comprehensionof input data. Similar to VPT-Deep [23], CEPs are introducedat each Transformer layer's input level. Applying deep tuningin the encoder allows for more nuanced and complex featureextraction from the input data. It enhances the encoder's abilityto understand and encode subtle patterns and relationshipswithin the data, setting a strong foundation for the decodingprocess. The encoder process is formulated as:\nTE = Encoder (concat (TH, PCE,T\u2081) + PE), (7)\nwhere PCE \u2208 RNP\u00d7C\u00d7LE, and NP, C, LE denote the numberof CEPs, embedding dimension, and number of the trans-former Encoder layers.\n2) Modality-Control Prompt (MCP): The pre-trained de-coder's capability is expanded by MCPs to handle multi-modaloutput generation. Similar to VPT Shallow [23], MCP vectors,denoted as PMC, are inserted only at the level of the firstTransformer layer of the decoder. We adopt VPT Shallowhere since each MCP set is designed to guide the decoder ingenerating a distinct mode of future trajectories. The decoderfinetuning process is formulated as:\nM' = Decoder(concat (TE, PMC, MF) + PE), (8)\nPF = PredictionHeadF (MF), PF \u2208 RN\u00d7K\u00d7T\u00d72, (9)\nwhere PMC \u2208 RNP\u00d7C\u00d7K, and Np,C, K denote the numberof MCPs, embedding dimension, and number of modes. In thisway, the pre-trained decoder is reused for generating multi-modal future trajectories.\n3) Parallel Adapter (PA): Complementing these CEPs andMCPS, PAs are added to the Multiheaded Self-Attention(MSA) and Feed-Forward Networks (FFN) modules across theTransformer layers as shown in Figure 2. To be specific, ouradapters consist of a downscale fully connected layer (Down),"}, {"title": "4) Selective Unfreezing:", "content": "Above is our design and utilizationof additive PEFT modules. For selective PEFT modules, wealso unfreeze the Bias, LayerNorm and PredictionHead of thepre-trained model, which have a limited number of tunableparameters \u2013 18.9K, 3.7K, and 31.7K respectively.\n5) Variants: We denote the Forecast-PEFT which onlytunes the additive modules (prompts and adapters) as Forecast-PEFT (A), and Forecast-PEFT represents the framework thattunes both additive modules and selective modules, if notspecified.\n6) Loss for Fine-tuning: The target agent's loss is calcu-lated using Huber loss for trajectory regression and cross-entropy loss for confidence classification. A winner-takes-all(WTA) approach determines the regression loss by comparingthe best prediction, chosen based on the lowest averagedisplacement error across all timesteps, with the ground truth.\n7) Cross-Dataset Finetuning: Our Forecast-PEFT exploresanother way to make the best use of the general knowledgeof large datasets, which is using pre-training and parameter-efficient fine-tuning, instead of transfer learning. Our additivemodules and selective modules can be seen together as a smallplug-in module, and we only let this module learn domain-specific knowledge."}, {"title": "C. Forecast-FT", "content": "The Forecast-FT framework is closely aligned with theForecast-PEFT approach, differing primarily in the ex-tent of fine-tuning. While Forecast-PEFT focuses on aparameter-efficient fine-tuning strategy by integrating special-ized prompts and adapters and selectively fine-tuning partsof the model, Forecast-FT extends this concept by engagingin a more comprehensive fine-tuning process. Specifically,Forecast-FT removes the Parallel Adapters integrated into theForecast-PEFT framework and proceeds to fully fine-tune allparameters of the pre-trained model. This approach aims toharness the full potential of the pre-trained model, adjustingall its parameters to enhance its predictive accuracy."}, {"title": "IV. EXPERIMENTS", "content": "To evaluate the effectiveness of our proposed Forecast-PEFT framework, we conducted a series of comprehensiveexperiments across multiple motion forecasting datasets. Section IV-A provides a detailed overview of our experimentalsetup. A thorough analysis of the performance of Forecast-PEFT on individual datasets is presented in Sections IV-B andIV-C. We examine its ability to preserve pre-trained knowledgein Section IV-D and its generalization capabilities acrossdifferent datasets in Section IV-E. Additionally, the efficiencyof Forecast-PEFT in terms of training time, data usage, andstorage requirements is investigated in Section IV-F. Finally,we present an ablation study to assess the contributions of"}, {"title": "A. Experiment Setup", "content": "1) Datasets: For our experiments, we utilize Argoverse 2[42] (AV2) and Argoverse 1 (AV1) [5] motion forecastingdatasets, while nuScenes datasets [3] is used only for cross-dataset evaluations.\nAV2 dataset [42] provides 250,000 real-world scenarios,divided into 199,908 training samples, 24,988 validation sam-ples, and 24,984 test samples. Each scenario features high-definition maps and 11-second trajectories sampled at 10 Hz,with 5 seconds dedicated to observed history and 6 secondsfor future forecasting.\nAV1 dataset [5] contains 324,000 scenarios, which are splitinto 205,942 training samples, 39,472 validation samples, and78,143 test samples. This dataset includes high-definition mapsand 5-second trajectories also sampled at 10 Hz, providing 2seconds of history and 3 seconds of future data.\nnuScenes dataset [3] offers high-definition semantic mapsand 8-second trajectories, with 2 seconds for observed historyand 6 seconds for future forecasting. Unlike the Argoverse datasets, nuScenes has a lower sampling rate of 2 Hz. Itconsists of 1000 driving scenes, split into 700 training scenes,150 validation scenes, and 150 test scenes.\n2) Metrics: All the metrics we adopt are from AV2 [42]and AV1 [5] official benchmark. We use minimum AverageDisplacement Error (minADE), minimum Final DisplacementError (minFDE), missing rate (MR), and brier minimumFinal Displacement Error (b-minFDE), which default to sixprediction modes, unless specified otherwise.\n3) Implementation details: Our base model is Forecast-MAE [7], featuring a pre-trained encoder and decoder with4 transformer layers each. We adopt a balanced setting forreconstruction losses in pre-training with \u03bbH = \u03bbF = 1.0 and\u03bbL = 0.35. The model has a feature dimension C = 128and supports six modes (K = 6), with CEPs and MCPs"}, {"title": "B. Quantitative Analysis", "content": "1) Performance of Forecast-PEFT: Tables I and II showthat our Forecast-PEFT framework surpasses the full fine-tuning baseline Forecast-MAE (FT) over all metrics on bothAV2 and AV1 datasets, with only 20% trainable parameter. OurForecast-PEFT(A), as shown in Table V, achieves comparableresults as the baseline with only 17% trainable parameters.These results consistently indicate the parameter efficiencyof our framework and the effectiveness of our additive andselective modules.\n2) Performance of Forecast-FT: Table I and II on bothleaderboards of AV2 and AV1 show that our Forecast-FTsurpasses the original full fine-tuning baseline method [7]for at least 4% over all metrics. Specifically, Forecast-FTachieves an improvement of MR by 9.6% and 8.6% on AV2and AV1, respectively. These results indicate the advantageand effectiveness of our design by using both the pre-trainedencoder and decoder.\n3) Comparison with other PEFT methods: As no specificPEFT method exists for motion forecasting, we benchmarkagainst VPT [23] from vision, LoRA [21] from language,IDPT [47] from 3D point clouds, and Head tuning that adjuststhe original multi-modal decoder (MD), as our comparisoncounterparts. VPT, LoRA, and IDPT all necessitate retain-ing the original multi-modal decoder to accommodate theirprompts. As shown in Table III, we compared our Forecast-PEFT framework with the above PEFT techniques, where ourstrategy excels in performance using a comparable amount oftrainable parameters."}, {"title": "4) Choosing Forecast-PEFT for Efficiency and Flexibility:", "content": "Although Forecast-FT, our fully fine-tuned variant, demon-strates the effectiveness of our approach without any designchanges, we choose Forecast-PEFT for its efficiency andflexibility. Forecast-PEFT tunes only 15% (0.38M) of thetrainable parameters required by Forecast-FT (2.5M) whilemaintaining good performance."}, {"title": "C. Qualitative Analysis", "content": "In Figure 7, we display qualitative results from AV2 us-ing pre-trained Forecast-MAE, and our fine-tuned versions,Forecast-PEFT and Forecast-FT. The pre-trained Forecast-MAE's encoder and decoder demonstrate reasonable single-mode predictions, supporting our assertion that the pre-traineddecoder is suitable for direct future trajectory prediction.Forecast-PEFT, leveraging the pre-trained components whileminimally adjusting parameters, achieves diverse, multi-modalpredictions. Forecast-FT, with all pre-trained weights un-frozen, shows enhanced trajectory prediction accuracy, asconfirmed by the visualization results."}, {"title": "D. Preservation of Pre-trained Knowledge", "content": "Figure 1(a) illustrates a performance comparison ofForecast-MAE (FT), Forecast-PEFT (A), and Forecast-PEF Ton lane and trajectory reconstruction (pre-training task) andtrajectory prediction (downstream task). Forecast-PEFT (A)maintains Forecast-MAE's reconstruction performance (noforgetting) with only 17% of parameters adjusted, achievinga trajectory prediction minFDE of 1.373, demonstrating itscapability to preserve pre-training knowledge while ensur-ing prediction accuracy. Forecast-PEFT improves predictionwith a minFDE of 1.351 using 20% adapted parameters andonly shows a 1.32 increase in reconstruction forgetting. Theretained pre-trained decoder in PEFT modules helps avoidcatastrophic forgetting, preserving encoder performance andeffectively adapting to prediction tasks.\nContrastingly, Forecast-MAE (FT) fully tunes all parametersbut does not surpass Forecast-PEFT (A) in performance, whilethe reconstruction error increases by factor 3, i.e., forgetsto reconstruct the trajectories and lanes of pre-training. Thisindicates that full fine-tuning compromises pre-trained knowl-edge without enhancing downstream effectiveness, suggestinga potential loss of valuable pre-trained representations duringdecoder re-learning."}, {"title": "E. Cross-Dataset Finetuning", "content": "Trajectory prediction models often face significant chal-lenges due to distribution shifts when applied across differentdatasets. Factors such as differing prediction horizons, urbanenvironments, data collection protocols, and object types con-tribute to these shifts, as observed between the Argoverse 1and Argoverse 2 datasets [42]. These domain gaps can leadto performance degradation, known as catastrophic forgetting,when a model fine-tuned on one dataset is applied to another."}, {"title": "1) Data Standardization:", "content": "To enable effective cross-datasetfine-tuning, it is crucial to standardize the data from differentdatasets. This ensures that the model can handle variationsin data characteristics, such as sampling rates and predictionhorizons, without requiring extensive architectural modifica-tions. In this work, we focus on standardizing data from theArgoverse 1 (AV1), Argoverse 2 (AV2), and nuScenes datasets.\nFor the Argoverse datasets, AV1 and AV2, we align theobservation and prediction horizons to AV2's longer durationsof 5 seconds for observation and 6 seconds for prediction.This contrasts with AV1's shorter horizons of 2 seconds forobservation and 3 seconds for prediction. During fine-tuningon AV1, we pad the absent history and future data withmasking to match the AV2 format, as illustrated in Figure3.\nThe nuScenes dataset presents additional challenges dueto its different sampling rate of 2Hz compared to AV2's10Hz. To ensure consistency, we standardize both datasets toa 10Hz sampling rate by padding the missing time frames andalign their historical data durations. Specifically, we extendnuScenes' original 2-second history to match AV2's 5-secondhistory. Both datasets have the same 6-second predictionhorizon. Figure 3 demonstrates how we address discrepanciesin historical data and sampling frequency through strategicpadding during the fine-tuning process. This approach ensuresthat the Forecast-PEFT framework can be efficiently fine-tunedacross different datasets without necessitating any changes toits architecture or modules, illustrating its robust flexibility."}, {"title": "2) Modular Fine-Tuning Strategy:", "content": "Our Forecast-PEFTframework offers a modular and efficient fine-tuning strategyto address the challenges posed by cross-dataset variations. Asillustrated in Figure 1(c), the tunable parameters in Forecast-PEFT can be viewed as a plug-in module. This design allowsus to fine-tune only the plug-in module, containing 383Kparameters, for each dataset while keeping the rest of themodel frozen. This design significantly reduces the numberof parameters that need adjustment compared to the compre-hensive fine-tuning required by Forecast-MAE, which involves1.9M parameters per dataset.\nThe flexibility and robustness of the Forecast-PEFT frame-work are validated through experiments involving the AV2,AV1, and nuScenes datasets. After pre-training on the AV2dataset, the model is fine-tuned on AV1 and nuScenes byadjusting only the 383K parameters in the plug-in module.Despite the significant differences among these datasets, ourapproach achieves results comparable to Forecast-MAE, whichrequires full fine-tuning of 1.9M parameters for each dataset,as demonstrated in Table IV. This efficiency underscoresForecast-PEFT's capability for effective trajectory predictionacross diverse datasets, making it a powerful and flexible toolfor this task.\nIn summary, our cross-dataset experiments validate the ef-fectiveness of Forecast-PEFT in maintaining high performancewith significantly fewer trainable parameters. This showcasesits practical applicability in real-world autonomous drivingwhere models must generalize across varied envi-ronments. The ability to efficiently fine-tune across datasetswithout requiring extensive architectural changes highlightsForecast-PEFT's potential for broad deployment in diverseautonomous driving contexts."}, {"title": "F. Efficiency Analysis", "content": "Our framework, Forecast-PEFT, is designed with the in-tention of saving on three critical resources: training time,training data, and storage space. This section outlines thespecific advantages of Forecast-PEFT in these areas."}, {"title": "1) Training Time:", "content": "Forecast-PEFT significantly reducesoverall training time. As shown in Figure 4, by eliminatingthe need for pre-training on each dataset, Forecast-PEFT saves54% of pre-training time compared to Forecast-MAE. Addi-tionally, the fine-tuning time for Forecast-PEFT is comparableto that of Forecast-MAE. For instance, fine-tuning on theArgoverse 2 dataset takes around 5 hours for both Forecast-PEFT and Forecast-MAE."}, {"title": "2) Training Data:", "content": "Forecast-PEFT is also efficient in termsof the amount of training data required. In our evaluation, weinvestigate the impact of varying the fine-tuning dataset size onthe performance of Forecast-PEFT and Forecast-MAE models.This experiment aims to understand how different proportionsof the dataset, ranging from 10% to 100%, influence themodels' predictive accuracy in motion forecasting tasks.\nBy incrementally increasing the dataset fraction used forfine-tuning, we seek to determine how dataset size limitationsaffect the performance of each model and whether smallerdatasets hinder the models' ability to make accurate predic-tions.\nThe results, depicted in Figure 5, show experiments con-ducted on the AV2 validation set and reveal a consistent trendacross two metrics: minADE and minFDE. As the dataset sizeincreases from 10% to 100%, both models exhibit a decreasein prediction error, indicating improved forecasting accuracy.However, the extent of improvement differs between the twomodels. As shown in Figure 5, Forecast-PEFT can match theperformance of Forecast-MAE while using 30% to 40% lesstraining data, specifically 30% less for minADE and 40% lessfor minFDE. This underscores the efficiency of Forecast-PEFTin maintaining relatively good performance even with limiteddata.\nImportantly, our Forecast-PEFT model consistently outper-forms the Forecast-MAE model across all dataset sizes. Thisconsistent superiority demonstrates the effectiveness of theForecast-PEFT framework in leveraging limited data, show-casing its potential for reliable performance even when theavailable data for fine-tuning is constrained."}, {"title": "3) Storage Space:", "content": "In terms of storage space, Forecast-PEFTrequires 2.42M+0.38M\u00d7N parameters, while Forecast-MAErequires 1.9M \u00d7 N parameters (where \"N\" is the number ofdatasets/domains and \u201cM\u201d denotes 1 million parameters). Foreach new dataset, Forecast-PEFT needs an additional 0.38Mparameters compared to 1.9M for the baseline, resulting insignificant storage savings as the number of datasets increases.\nBy leveraging these efficiencies, Forecast-PEFT stands outas a highly effective and practical framework for trajectoryprediction across diverse datasets, making it suitable for real-world autonomous driving applications."}, {"title": "G. Ablation Study", "content": "1) Effectiveness of Main Components: Table V presentsan ablation study on the components of Forecast-PEFT us-ing the AV2 validation set. The bottom row of the tableshows the baseline results of Forecast-MAE, while the grayrows represent the performance of Forecast-PEFT(A) and thefull Forecast-PEFT. Forecast-PEFT(A) exhibits some perfor-mance drops in minADE and minFDE compared to completefine-tuning. However, by selectively unfreezing modules inForecast-PEFT, the framework outperforms full fine-tuningacross all metrics, including MR. This validates the effec-tiveness of our designed components in the Forecast-PEFTframework.\n2) Effects of Prompt Length and Depth: Our ablationstudies examine the impact of prompt length and depth forContextual Embedding Prompts (CEP) and Modality-ControlPrompts (MCP), crucial components of the Forecast-PEFTframework. The results offer valuable insights into optimizingthese parameters for improved performance.\nContextual Embedding Prompt (CEP): During pretrain-ing, the encoder processes masked historical trajectories, fu-ture trajectories, and contextual data. In contrast, during fine-tuning and inference, it only handles historical trajectories andcontextual data. CEPs are introduced to bridge this input gap,preserving pre-trained knowledge. Our experiments indicatethat increasing the depth of CEPs by inserting them intomore encoder layers consistently enhances model perfor-mance. Specifically, Table VI reveals that integrating CEPsinto up to four encoder layers optimizes results, ensuring thatno pre-trained knowledge is lost and the encoder remainsfocused on relevant historical and contextual features.\nModality-Control Prompt (MCP): The role of the decodershifts from reconstructing masked data during pretrainingto forecasting future trajectories during fine-tuning. MCPS"}, {"title": "H. Other Pre-trained Motion Forecasting Models", "content": "Our method is adaptable to other transformer-based pre-trained motion forecasting models, such as Traj-MAE [6] andSEPT [26], which include unused pre-trained trajectory recon-struction decoders. This compatibility makes them suitable forour PEFT strategy. However, we look forward to using themas baselines to validate our method once their codes becomeavailable."}, {"title": "I. Limitations", "content": "The efficacy of Forecast-PEFT largely depends on thequality and relevance of the pre-trained models. The successof fine-tuning can greatly fluctuate depending on the alignmentbetween pre-training and the specific needs of motion forecast-ing tasks. Additionally, the scalability of Forecast-PEFT forlarger datasets or more complex forecasting scenarios requiresfurther investigation."}, {"title": "J. Potential Future Research Avenues", "content": "Our work can serve as a baseline for exploring PEFTmethods in motion forecasting. With more datasets and largermodels becoming available, the advantages of our methodwill become more evident. Through our approach, pre-trainingon one large dataset and then applying PEFT across variousdatasets can significantly benefit real-world applications."}, {"title": "V. CONCLUSION", "content": "Our research presents Forecast-PEFT, a parameter-efficientfine-tuning framework specifically designed for motion fore-casting in autonomous driving. We discovered that traditionalfull fine-tuning methods are suboptimal, as they do not fullyexploit the synergy between self-supervised learning tasksand trajectory prediction. Forecast-PEFT addresses this issueby effectively utilizing the pre-trained model's encoder anddecoder, considerably reducing the number of trainable pa-rameters without sacrificing performance. This efficiency iscrucial for deploying advanced autonomous systems wherecomputational resources are limited. The framework's robust-ness is further demonstrated through successful cross-datasetfine-tuning, highlighting its versatility and broad applicability."}]}