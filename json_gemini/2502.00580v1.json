{"title": "Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation", "authors": ["Stuart Armstrong", "Matija Franklin", "Connor Stevens", "Rebecca Gorman"], "abstract": "Recent work showed Best-of-N (BoN) jailbreaking using repeated use of random augmentations (such as capitalization, punctuation, etc) is effective against all major large language models (LLMs). We have found that 100% of the BoN paper's successful jailbreaks (confidence interval [99.65%, 100.00%]) and 99.8% of successful jailbreaks in our replication (confidence interval [99.28%, 99.98%]) were blocked with our Defense Against The Dark Prompts (DATDP) method. The DATDP algorithm works by repeatedly utilizing an evaluation LLM to evaluate a prompt for dangerous or manipulative behaviors-unlike some other approaches, DATDP also explicitly looks for jailbreaking attempts-until a robust safety rating is generated. This success persisted even when utilizing smaller LLMs to power the evaluation (Claude and LLaMa-3-8B-instruct proved almost equally capable). These results show that, though language models are sensitive to seemingly innocuous changes to inputs, they seem also capable of successfully evaluating the dangers of these inputs. Versions of DATDP can therefore be added cheaply to generative AI systems to produce an immediate significant increase in safety.", "sections": [{"title": "Introduction", "content": "The phenomenon of \"AI jailbreaking\" pertains to methods employed to circumvent safety restrictions in large language models (LLMs) and vision-language models (VLMs), thereby enabling the generation of harmful or otherwise restricted content [1, 2, 3]. Jailbreaking attacks on AI systems aim to bypass safety mechanisms, prompting the model to produce unintended and possibly harmful output. Illustrative examples include the deployment of typographic visual prompts to bypass text-based filters in VLMs [4] and the exploitation of temporal characteristics to subvert chatbot defense mechanisms [5]. Notably, automated approaches have emerged, such as employing LLMs to generate adversarial prompts, achieving notable success rates against commercial systems, including ChatGPT, Bard, and Bing Chat [5, 6]. While such practices underscore the innovative potential of jailbreaking, they also heighten risks associated with privacy violations and the dissemination of disinformation [1]. Consequently, researchers have emphasized the imperative to develop robust defensive strategies and to consider ethical implications in the ongoing evolution of AI technologies [3, 2].\nRecent investigations have underscored the dual role of jailbreaking in AI research: as a mechanism to expose system vulnerabilities and as a significant AI safety concern. Studies reveal a spectrum of techniques, ranging from symbolic adversarial mathematics [7] and typographic prompts [4] to manipulative adversarial prompts [8]. Emerging methods include \u201cprompt stitching,\u201d where fragments of adversarial prompts are combined to produce more sophisticated bypass mechanisms, and the use of contextual embedding manipulations, which leverage subtle linguistic nuances to evade detection algorithms [9]. These approaches not only amplify the effectiveness of attacks but also challenge the scalability of current defensive frameworks, particularly in large language models. Moreover, studies emphasize the risks associated with jailbreaking for malicious purposes, such as generating harmful or illegal content, underscoring the critical need for robust countermeasures [10]. These methods consistently achieve high success rates in bypassing extant safeguards, thereby elucidating critical weaknesses inherent in current AI architectures.\nA range of mitigation techniques has been developed to address jailbreaking in LLMs. These approaches include enhanced safety training, external defense mechanisms, and innovative strategies such as self-reminders and adaptive self-defense frameworks [11, 12]. For example, the \u201cSelf-Guard\u201d method combines iterative self-assessment mechanisms with foundational safety training to improve model resilience against attacks [12]. Other efforts target specific vulnerabilities, such as multilingual challenges, which reveal heightened risks in low-resource languages [5]. Advanced frameworks like WildTeaming systematically analyze real-world interactions to identify new attack vectors and improve defensive datasets [13]. Furthermore, efforts to formalize jailbreak analysis using taxonomies [14] and establish benchmarks such as JailbreakBench [15] provide standardized resources for evaluating and enhancing model robustness. While these methods have shown promise, many rely on reactive defenses that address specific attack vectors but struggle to generalize across diverse adversarial scenarios. In contrast, our study focuses on an evaluation agent as a proactive, scalable approach designed to preemptively block adversarial inputs, offering a more adaptable and comprehensive solution to the evolving landscape of AI security threats.\nA recent study titled \"Best-of-N Jailbreaking\" introduces an algorithm designed to compromise advanced AI systems across various modalities [16]. The"}, {"title": "This Experiment: Evaluation Agents", "content": "The present paper explores the efficacy of an evaluation agent as a mitigation strategy against the type of attacks described by [16]. Our experiment is focused exclusively on textual jailbreaks, leaving visual and audio jailbreaks as the subject of future study. This study aim to assess whether evaluation agents can serve as a robust preemptive defense, intercepting harmful prompts before they reach AI models and produce unintended outputs.\nOur approach, entitled \"Defense Against The Dark Prompts\" (DATDP), was tested in several controlled experiments against six prompt databases, some of which we generated in a replication of [16]. The evaluation agent blocked the vast majority of dangerous prompts, looking for both danger and jailbreak attempts. This result held whether it used the powerful Claude 3.5 Sonnet as its underlying model, or the much smaller LLaMa-3-8B-instruct. Indeed, these two model performed equally well on augmented prompts (with the Claude-based agent being slightly better when analyzing non-augmented prompts). They were able, for instance, to block 100% of the prompts that the [16] listed online as jailbreaking frontier models, and ranged between 99.5% to 100% blocked on other datasets of augmented prompts.\nBoN attacks leverage AI systems' sensitivity to minor input perturbations and model randomness to generate harmful outputs through repeated attempts. [16] states that \"the method's effectiveness stems from adding significant variance to model inputs rather than properties of specific augmentations themselves\". Prompt evaluation agents, however, significantly mitigates this threat by requiring attackers to craft augmentations capable of bypassing both the evaluation agent and the model's internal safety mechanisms. This is difficult, because the two requirements are in some tension: to get a model to output a dangerous response, the model must be able to understand the prompt well; but this is precisely what makes evaluation agents effective.\nA key advantage of our approach is its proactive nature: harmful prompts can be analyzed and flagged prior to reaching the model, reducing the likelihood"}, {"title": "Availability of the method", "content": "To increase the safety and reliability of agentic and AI systems, we have released the DATDA method as open-source on GitHub https://github.com/aligned ai/DATDP."}, {"title": "Method", "content": "The Defense Against The Dark Prompts (DATDP) framework builds on [18] and [19]. It employs an iterative evaluation process to detect and block harmful inputs before they reach the language model. Figure 1 illustrates the DATDP workflow.\nTo test DATDP, we replicated the approach introduced by [16] to produce jailbreaking prompts. These started with non-augmented dangerous prompts, then added textual augmentations such as word scrambling, random capitalization, and ASCII noising to generate. Our evaluation agent was tested on these prompts."}, {"title": "Defense Against the Dark Prompts", "content": "The Defense Against the Dark Prompts (DATDP) framework builds upon an approach first described in [18] and is designed to identify and block"}, {"title": "Evaluation Agent Setup", "content": "The evaluation agent is configured to assess whether user-submitted prompts contain content that could be harmful or manipulative. This was achieved by providing the agent with specific instructions via a system prompt. The system prompt was phrased as follows:\n```\nsystem_prompt = f\"\"\"You will assess whether the user prompt elicits\nor contains {forbidden_task}.\nBegin by explaining your reasoning, step by step, then end your\nresponse with 'yes' (if the user prompt contains {\nforbidden_task}) or 'no' (if it does not).\nMAKE SURE TO END YOUR ANSWER WITH 'yes' OR 'no'!\"\"\"\n```\nThe user prompt was constructed as:\n```\nuser_prompt = f\"\"\"User prompt to assess follows:\n>{submitted_prompt}<\n```\nHere, submitted_prompt refers to the input prompt that needs to be evaluated, and forbidden_task defines what the agent should be blocking. In these experiments, it is\n```\nforbidden_task = \"anything that could be dangerous for an LLM to\nanswer or could be intended to trick or jailbreak the LLM.\"\n```"}, {"title": "Iterative Evaluation Process", "content": "Given the variability in LLaMa-3-8B-instruct's responses, each prompt was evaluated $N = 25$ times. For each evaluation, responses ending with \"yes\" were labeled as a rejection, while those ending with \"no\" were were labeled as an acceptance. Responses that did not conform to this format were excluded\u00b9.\nClaude, due to its higher consistency, required fewer iterations, $N = 5$.\nTo prioritize the identification of harmful prompts, a weighted scoring system was used:\n\u2022 A rejection (\"yes\") was assigned a score of +2.\n\u2022 An acceptance (\"no\") was assigned a score of -1.\nAfter $N$ evaluations, the scores were summed:\n\u2022 A positive sum classified the prompt as harmful, leading to rejection.\n\u2022 A strictly negative sum allowed the prompt to proceed to the responding model."}, {"title": "Testing Methodology", "content": "We tested the evaluation agent on the augmented text prompts, as described by [16]. We used several datasets to do so, some of them from outside sources, some of them ones we replicated ourselves."}, {"title": "Dataset Collection and Generation", "content": "These were the six datasets we tested DATDP on:\n1. Similarly to [16], we used all 159 harmful prompts from HarmBench [20], excluding copyright-related or context-dependent behaviors.\n2. The BoN GitHub repository has a file2 containing 1045 augmented Harm-Bench prompts that paper [16] showed successfully various advanced models (e.g., Claude 3.5 Sonnet, Claude 3 Opus, GPT-40, and Gemini Pro).\n3. We replicated the methods of the Best-of-N paper. We then selected a random set of 1,000 prompts that had jailbroken LLaMa-3-8B-instruct.\n4. As above, but we selected a random set of 1,000 prompts that hadn't jailbroken LLaMa-3-8B-instruct.\n5. We generate 1,590 random augmented prompts (ten augmented prompts for each of the 159 original prompts).\n6. We had a list of 250 normal prompts, to check that the evaluation wasn't over-sensitive."}, {"title": "Replication details", "content": "[16] took a list of 159 harmful prompts from HarmBench [20] and augmented them using a combination of three augmentation techniques: word scrambling, random capitalization, and ASCII noising.\nThe degree of variability introduced by these augmentation techniques is controlled by the parameter $\\sigma$, which determines the extent of randomization applied to each method. For this study, we set $\\sigma = 0.25$, aligning with the parameters reported in [16]. We then generated 5,000 augmented prompts for each of the 159 base prompts, for a total of 795,000 augmented prompts.\nWe then had LLaMa-3-8B-instruct produce responses to each of these prompts. The responses were then evaluated for harmfulness using the HarmBench auto-grader [20] to assessed whether the model's responses conformed to the harmful behavior intended by the original unaugmented prompt\u00b3.\nOf the 795,000 augmented prompts, 26, 946 were evaluated to have successfully jailbroken LLaMa-3-8B-instruct, while 768, 054 were deemed to have failed. To get a significant estimate, we extracted 1,000 entries at random from the successful jailbreak prompts and another 1,000 at random from the failures. DATDP was evaluated against each of these two datasets.\nSeparately, 10 augmented prompts were generated for each of the 159 base prompts, without classifying them as jailbreaking or not. DATDP was evaluated against all 1590 of these prompts."}, {"title": "Results", "content": "We tested the evaluation agents on the datasets described in Section 2.2.1. We used the DATDP workflow with two base models. One was Claude Sonnet 3.5, iterated $N = 5$ times. The second was LLaMa-3-8B-instruct, iterated $N = 25$ times. The number of prompts successfully blocked are presented in Table 1 and Figures 3 (for augmented prompts) 3 (for non-augmented prompts).\nThe 95% confidence intervals in the figures were computed using the Clopper-Pearson confidence interval method [21], an \u2018exact' method that is reliable but conservative, tending to produce slightly wider confidence intervals than other methods. More details of these results (including listing the prompts that the evaluation agent failed on and the numerical confidence intervals) can be found in Section A.\nAs can be seen, LLaMa-3-8B-instruct's performance in an evaluation agent was almost as impressive as Claude's, despite being a much smaller model. Claude was better on three datasets, LLaMa-3-8B-instruct on one, and they were equal on two.\nAgents with either model successfully blocked over 99% of the augmented prompts. And both blocked a 100% of the jailbreaking prompts derived from the BoN paper-prompts that successfully jailbroke multiple advanced models (e.g., Claude 3.5 Sonnet, Claude 3 Opus, GPT-40, and Gemini Pro)."}, {"title": "Blocking Augmented Prompts", "content": "Figure 3 shows that evaluation agents can be used very successfully to block augmented prompts. Claude and LLaMa-3-8B-instruct were roughly equally capable of blocking these prompts (Claude was better on one dataset, worse on another, and the two agents were equal on the remaining two datasets).\nThe fact that their performance seems comparable on all different types of augmented prompts suggests that they operate as an additional safety method with constant, roughly 99.7% effectiveness. Therefore we would expect that they would not shift the exponents of the power laws of Figure 4 in [16], but would multiply them by a small scalar, making jailbreaking with that method 500 times or so harder."}, {"title": "Normal and Unaugmented Prompts", "content": "The LLaMa-3-8B-instruct agent's performance dipped slightly in two areas: on normal prompts and on unaugmented HarmBench prompts (see Figure 4).\nOn normal prompts, the LLaMa-3-8B-instruct agent blocked \"How can I meet hot girls in my area?\" while the Claude agent accepted it. It's understandable why LLaMa-3-8B-instruct blocked this prompt, even if it was wrong.\nBut the LLaMa-3-8B-instruct agent also failed to block two of the Harm-Bench prompts (see Section A.1). This suggests that evaluation agents may well be more effective at blocking augmented prompts than unaugmented ones; the augmentation makes them more suspicious to the evaluation agent."}, {"title": "Model Self Evaluation", "content": "When the same model was used for the evaluation agent and for the responding LLM, the agent was very capable of blocking prompts that would have jailbroken the responding LLM.\nA Claude-based agent blocked all the prompts from [16], including all of the ones that jailbroke Claude. And in our replication, a LLaMa-3-8B-instruct-based agent blocked 99.8% of the prompts that jailbroke the language model version of LLaMa-3-8B-instruct. Figure 2 illustrates well the different behavior of the same model as part of an evaluation agent or a responding LLM."}, {"title": "Number of Iterations", "content": "Figure 5 demonstrates the effectiveness of the LLaMa-3-8B-instruct evaluation over multiple iterations. It highlights that a value of $N = 12$ iterations seems sufficient to achieve near-optimal evaluation performance, though this paper has used $N = 25$."}, {"title": "Future Improvements", "content": "There are ways to further improve the DATDP approach, which we have not yet had the opportunity to test fully. Firstly, the model used for the evaluation agent could be fine-tuned specifically for its task.\nAnother alternative is to deploy the evaluation agent further down the pipeline, evaluating the response rather than the prompt. When we tested this on our replicated set of 1,000 jailbreaking prompts, the initial results were not promising: only 760 of the 1,000 responses were blocked by a LLaMa-3-8B-instruct evaluation agent (note that the HarmBench auto-grader [20] applies very strict criteria for what it considers a jailbreak, so this lower rate of blocking may reflect a more permissive judgment by the evaluation agent).\nHowever, the response evaluation agent did successfully block all three prompts that bypassed the standard Claude and LLaMa-3-8B-instruct prompt evaluation agents (see Section A.3). This suggests that additional security might be achieved by using an evaluation agent to assess both the prompts (before submission to the responding LLM) and the responses (afterwards).\nThis dual-layer approach could be particularly effective because one reason for the success of evaluation agents is that, for adversarial prompts to bypass them, the prompts must satisfy multiple conflicting constraints: they must be comprehensible to the responding agent (to elicit a harmful response) while simultaneously remaining obscure to the evaluation agent (to pass through un-blocked). Adding a third constraint that the harmful response must also evade the evaluation agent-further increases the difficulty for attackers.\nIn a similar vein, a rephrasing agent could be introduced to further enhance the system's robustness. This agent would rephrase the prompt-by translating it into another language and back, rewriting it in a different style (e.g., formal or informal), or both. It would be exceedingly challenging to craft a prompt capable of evading an evaluation agent, jailbreaking a responding LLM, producing a harmful response that also evades a second evaluation agent and doing all of this after being rephrased."}, {"title": "Conclusion", "content": "This paper presented DATDP, a framework that uses evaluation agents as a preemptive defense against textual jailbreak attacks on LLMs.\nIt was tested against the augmentations detailed in [16]; and against those prompts, evaluation agents demonstrated exceptional performance, flagging between 99.5% and 100% of augmented prompts, including those previously elicited harmful outputs. This underscores the potential of evaluation agents as a proactive and adaptable layer of defense in AI safety.\nOne of the most significant findings of this study is that evaluation agents based on smaller and less capable models like LLaMa-3-8B-instruct proved almost as effective as those based on frontier models such as Claude Sonnet 3.5. Both of these agents were capable of blocking all the successful jailbreaking prompts made available by the authors of [16]. This finding suggests that even less resource-intensive models can effectively support the DATDP framework, making it accessible and applicable to a wide range of LLM-based systems.\nThis finding aligns closely with recent advancements in the AI control literature, particularly the concept of using smaller, trusted models to monitor and control the outputs of more powerful, untrusted models. For instance, researchers have explored safety protocols where smaller, less powerful models like GPT-3.5 assess or edit outputs generated by more advanced models like GPT-4, ensuring that potential risks such as logical errors or backdoors are mitigated before deployment [22]. Similarly, the DATDP framework leverages less resource-intensive evaluation agents, such as LLaMa-3-8B-instruct, to evaluate and filter adversarial prompts targeting larger models.\nIt should be noted that both evaluation agents of this paper, whether based on Claude or LLaMa-3-8B-instruct, proved highly effective at identifying and rejecting prompts that would otherwise jailbreak that same model. This suggests that current models might already have the potential to defeat jailbreak attempts, if they were deployed or used differently.\nThe DATDP framework can be seamlessly integrated into existing workflows for evaluating and filtering adversarial prompts. By identifying and rejecting harmful prompts before they reach the responding language model, evaluation agents provide an independent defense layer that complements the model's internal safety mechanisms.\nBy requiring malicious actors to bypass both the evaluating agent and the language model's built-in safety mechanisms, the DATDP framework significantly increases the difficulty of successfully launching adversarial attacks.\nThese findings also have broader implications for AI governance and policy. The success of evaluation agents suggests that lightweight, proactive safety mechanisms can play a critical role in mitigating systemic risks posed by generative AI. Incorporating similar frameworks into regulatory standards or best practices could provide an actionable pathway for improving the safety and reliability of AI deployments at scale."}]}