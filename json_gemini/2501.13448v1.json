[{"title": "BMG-Q: Localized Bipartite Match Graph Attention Q-Learning for Ride-Pooling Order Dispatch", "authors": ["Yulong Hu", "Siyuan Feng", "Sen Li"], "abstract": "This paper introduces Localized Bipartite Match Graph Attention Q-Learning (BMG-Q), a novel Multi-Agent Reinforcement Learning (MARL) algorithm framework tailored for ride-pooling order dispatch. BMG-Q advances ride-pooling decision-making process with the localized bipartite match graph underlying the Markov Decision Process, enabling the development of novel Graph Attention Double Deep Q Network (GATDDQN) as the MARL backbone to capture the dynamic interactions among ride-pooling vehicles in fleet. Our approach enriches the state information for each agent with GATDDQN by leveraging a localized bipartite interdependence graph and enables a centralized global coordinator to optimize order matching and agent behavior using Integer Linear Programming (ILP). Enhanced by gradient clipping and localized graph sampling, our GATDDQN improves scalability and robustness. Furthermore, the inclusion of a posterior score function in the ILP captures the online exploration-exploitation trade-off and reduces the potential overestimation bias of agents, thereby elevating the quality of the derived solutions. Through extensive experiments and validation, BMG-Q has demonstrated superior performance in both training and operations for thousands of vehicle agents, outperforming benchmark reinforcement learning frameworks by around 10% in accumulative rewards and showing a significant reduction in overestimation bias by over 50%. Additionally, it maintains robustness amidst task variations and fleet size changes, establishing BMG-Q as an effective, scalable, and robust framework for advancing ride-pooling order dispatch operations.", "sections": [{"title": "I. INTRODUCTION", "content": "THE widespread adoption of mobile communication and Global Positioning System technology has allowed Transportation Network Companies (TNCs) such as Uber, Lyft, and Didi to provide on-demand mobility services on a global scale [1], [2]. Ever since [3], the advantages of flexible and collaborative ride-sharing operations have become increasingly recognized within the transportation research community. In line with this trend, there has been an expanding body of research on operational policies for ride-sharing, including multi-hop ride-sharing [4], the coordination of ride-hailing with public transportation [5], ride-sharing with passenger transfers [6], integration of ride-sharing with parcel delivery [7], and the coordination of autonomous vehicles with conventional vehicles [8]. These advancements are propelled by breakthroughs in deep learning and Multi-Agent Reinforcement Learning (MARL) frameworks [9]\u2013[11].\nNevertheless, several hurdles must be overcome to unlock the full potential of MARL for developing effective, scalable, and robust real-time operational strategies for ride-pooling order dispatch. A principal challenge in this context is the complex interdependence in decision-making among vehicles, which leads to an exponential increase in both state and action spaces within large fleets [12]. One approach to address this is by traditional independent learning approaches, such as Independent Q-Learning (IQL) and Independent Proximal Policy Optimization (IPPO) [13], [14], which ignore the interdependence. In the context of ride-pooling, it is common in the existing literature to combine single-agent independent Reinforcement Learning (RL) with bipartite matching. For instance, [15] and [4] proposed to adopt a Deep Q-Network (DQN) for relocating ride-pooling agents and bipartite matching for order dispatch, and later on, extended it to multi-hop ride-sharing and parcel delivery [7]. To improve the transferability and scalability of the framework, [16] introduced additional techniques such as limited-memory upper confidence bound and reward smoothing. Moreover, [5] coordinated ride-hailing with public transit by encoding the decisions of subway stations into the states of tabular temporal difference learning. Similarly, [6] facilitated ride-pooling with passenger transfer. Yet, the practice of merging independent reinforcement learning with bipartite matching, while improving scalability, often overlooks the agents' complex interdependence during the RL exploitation and training phases. This can lead to significant overestimation of rewards, a critical concern in highly competitive environments such as ride-pooling, where the pronounced interdependence among agents intensifies the issue.\nTo accommodate the intricate interdependence among agents, several MARL frameworks have been introduced, including state-of-the-art Centralized Training with Decentralized Execution (CTDE) algorithms such as Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [17], Q-mix [18], Q-tran [19], and Multi-Agent Proximal Policy Optimization (MAPPO) [20]. However, these methods are typically applied to much smaller-scale problems. In contrast, large-scale ride-pooling order dispatch involves thousands of agents, rendering these approaches infeasible. To enhance algorithmic scalability while capturing agent interactions, researchers have explored novel concepts such as Mean-Field MARL [21], where agents interact with an average representation of all other agents. However, in the ride-pooling context [22], this average state may not accurately represent agent interdependence as each agent has unique passengers with different itineraries, and summing this up may lead to misleading information for the decision maker. One approach to overcome this limitation is by considering the Attention-based MARL [23], [24], which instead of relying on average state information, allows distinct neighboring agent to has distinct weights that can be flexibly adjusted over time. Nevertheless, the application of Attention-based MARL is limited in small-scale scenarios and single-passenger ride-hailing [23]\u2013[25], where the possibility of multiple riders sharing the same ride is not explicitly considered.\nTo fill the above-mentioned research gaps, we introduce a novel MARL framework specifically crafted for ride-pooling order dispatch-the Localized Bipartite Match Graph Attention Q-Learning (BMG-Q). This framework is adept at capturing the intricate interdependencies that typically arise within a localized graph, defined by the bipartite matching radius. By implementing the Graph Attention Double Deep Q-Network (GATDDQN), we provide ride-pooling agents with enriched state representations that factor in the influence of other agents' actions during decision-making. Techniques such as gradient clipping and graph sampling have been employed to bolster the robustness and scalability of the GATDDQN, ensuring that agents retain learned information over time rather than overfitting to recent transitions. In addition, we have seamlessly integrated the GATDDQN with a bipartite matching mechanism through a posterior score function and Integer Linear Programming (ILP). This integration enhances the central matcher's efficiency and refines the balance between exploration and exploitation. Our comprehensive numerical studies show that BMG-Q can effectively model the complex interactions among agents, reduce overestimation bias, and improve overall performance. The study also verifies that BMG-Q retains its robustness in the face of task variability and training hyper-parameter changes, thus establishing it as an effective, scalable, and robust approach for advancing ride-pooling operations. The major contributions of this paper are summarized below:\n\u2022 We propose a novel BMG-Q framework to address multi-agent interactions in MARL within the context of large-scale ride-pooling order dispatch. The propsoed framework leverages the novel localized bipartite match interdependent Markov Decision Proces (MDP) formulation with the Graph Attention Double Deep Q Network (GAT-DDQN) as backbones. It captures the interdependence among agents and thus leads to more optimal assignment decisions compared to existing works.\n\u2022 Our work stands at the forefront of developing graph-based MARL techniques for large-scale ride-pooling order dispatch systems. While contemporary studies in the realm of MARL have started to explore the incorporation of GNN with RL [26]\u2013[30], they often encounter limitations due to scalability, stability, and robustness. By employing strategic measures such as gradient clipping and random graph sampling, our BMG-Q framework showcases a consistently robust training and validation performance in systems comprising thousands of agents and the face of task variations and parameter changes.\n\u2022 We validate the BMG-Q framework through a case study in New York City, utilizing a real-world taxi trip dataset [31], [32]. We demonstrate that our proposed framework not only significantly reduces overestimation issues but also outperforms benchmark frameworks. This is evidenced by an approximate 10% increase in total accumulated rewards and a more than 50% reduction in overestimation, underscoring the enhanced performance of our BMG-Q ride-pooling dispatch operations."}, {"title": "II. RELATED WORKS", "content": "Ride-Pooling Order Dispatch. The operational dynamics of ride-pooling have garnered considerable attention due to their promising yet unpredictable real-time demand, as evidenced by various studies [3], [33], [34]. The nature of this uncertainty, coupled with the full potential of ride-pooling systems, introduces complexity into the process of coordinating vehicles with multiple passengers. Effective coordination requires not only addressing the needs of current passengers but also anticipating the needs of future riders, which includes managing new ride requests and those already being served. Initial investigations in this field have considered short-sighted, or myopic, policies that make vehicular assignments based on presently available information [3], [33]. Specifically, [3] notably advances this by introducing the shareability graph, which identifies possible sharing opportunities between new requests and vehicles on standby. They put forward a batch-matching strategy and crafted a sequential method that divides the decision-making process into vehicle routing and passenger assignment tasks. For more efficient real-time operations, [34] reduces the complexity of the matching problem by limiting the process to pairing a single passenger with a vehicle at each time step. More recent advancements in the field have shifted towards a better incorporation of the uncertainties related to future demand into the decision-making processes via methods such as model predictive control [35]\u2013[37], approximate dynamic programming [38]\u2013[40], and stochastic integer programming [41]. Note that these works are model-based, requiring explicit characterization of system dynamics and/or future uncertainties.\nMARL Framework for Ride-Pooling Dispatch. Given the super-human capabilities of RL and MARL showcased in a range of notable achievements [9]\u2013[11], the prospect of crafting practical MARL systems for the real-time optimization of ride-sharing dispatch grows increasingly compelling. While some researchers have endeavored to deploy multi-agent reinforcement learning approaches such as Mean-Field MARL [22], Q-mix [42], and Attention-based MARL [25], [43] in ride-sourcing scenarios, these methods continue to grapple with challenges like stability and scalability when it comes to training in large-scale and complex settings. To address the scalability issue in large-scale ride-sharing systems, it is common in the ride-sharing research community to combine single agent RL (or equivalently, Independent RL) with bipartite match. Specifically, [4], [15] propose to adopt DQN for ride-pooling agents' relocation and bipartite match for order dispatch, and later on extend it into multi-hop ride sharing and parcel delivery [7]. To improve the transferability and scalability of full deployment of the framework in ride hailing, [16] proposes additional techniques such as limited-memory upper confidence bound and reward smoothing. Moreover, [5] coordinates ride-sourcing with public transit through encoding the decision of subway stations into states of tabular temporal difference learning. [8] coordinates autonomous vehicles with conventional vehicles through two-sided deep reinforcement learning. [6] enable ride-pooling with passenger transfer. However, the aforementioned works have yet to adequately address the intricate interdependencies among vehicles in the ride-pooling context while also ensuring scalability for MARL.\nGraph-based MARL. As the computational efficiency and representational power of GNN models such as Graph Convolutional Network (GCN) [44], GraphSAGE [45], Graph Attention Network (GAT) [46], and Relational Graph Convolutional Network (RGCN) [47] gain increasing recognition in complex and adaptive representation learning, researchers have begun to investigate the integration of these potent GNN models with MARL. This nascent area of research seeks to tackle a variety of challenges within MARL, such as the complex task of encoding environmental dynamics from the perspective of individual agents, as well as the decomposition of value functions and the nuanced distribution of credit across the collective team [30]. Specifically for coordination games, on the one hand, [26], [27] propose to adopt graph convolution RL and two-stage attention mechanism to learn abstract interplay representation between agents within graph topology. Following this trend, [48] comprehensively utilizes GATs and RGCN to capture both explicit and implicit relations simultaneously among agents. On the other hand, [28], [29] and [49] propose the idea of coordination graph and utilize GATs to factorize the join team value function or team policy to enable coordination behavior among agents. Despite these advancements, these advances have not yet been applied to ride-pooling, a highly complex and large-scale system, where achieving scalability, stability, and robustness concurrently remains a significant challenge."}, {"title": "III. PROBLEM FORMULATION & BENCHMARK METHODS", "content": "In this section, we formulate the ride-pooling order dispatch problem and review the strategies commonly used in previous literature. The ride-pooling vehicles are conventionally considered as independent and homogeneous agents under the bipartite matching process. A benchmark method will be established, against which we can compare our proposed algorithm in subsequent discussions.\nIn particular, we will begin by presenting the MDP formulation for ride-pooling order dispatch, detailing each agent's state, action, reward, discount factor, and transition function under various scenarios in Subsection A. We then explore how the assumptions of independence and homogeneity, prevalent in the ride-pooling community's approach, serve to decentralize the original MDP. Building upon the analysis, we illustrate the integration of Independent RL with ILP and then outline how RL techniques, such as Double Deep Q-Network (DDQN), could be applied to learn and represent the system's dynamics to finally form a benchmark framework, termed ILPDDQN, for ride-pooling order dispatch."}, {"title": "A. MDP Formulation for Ride-Pooling Order Dispatch", "content": "The ride-pooling order dispatch problem is normally formulated as a multi-agent MDP, with each ride-pooling vehicle representing an agent (we refer to it as agent or vehicle agent hereafter). Each vehicle agent's definition of state, action, reward, and transition function could be detailed as follows:\n1) State: For each vehicle n at time t, its state is $S_{n,t} = (l_{n,t}, U_{n,t}, p_{n,t}, O_{n,t}, D_{n,t}, t)$, where $l_{n,t}$ encodes the current location; $U_{n,t}$ is the number of vacant seats; $p_{n,t}$ encodes the information of passengers on board, including their estimated remaining time on board, drop-off locations, and current additional travel time; $O_{n,t}$ and $D_{n,t}$ represent a set of origin and destination pairs of the observed incoming orders within the matching distance of agent n, respectively; and t is the current time.\n2) Action: For available vehicle n (i.e., the vehicle is not full or in the process of picking up a new passenger) at time t, after seeing the incoming new orders, platform assigns action $a_{n,t}$ to decide whether to pick up one of the observed passengers: if not, we have $a_{n,t} = 0$ and then vehicle n will remain idle or continue with the remainder of its trip as determined by the onboard passenger's itinerary; otherwise if the vehicle is assigned by the platform to pick up the $z^{th}$ request (among all observed incoming orders of vehicle n), then we have $a_{n,t} = z$.\n3) Reward Function: If vehicle n is not available or does not accept any of the observed new order at time t, then the reward function at time t is as:\n$r_{n,t}(S_{n,t}, a_{n,t}) = -C_o$, (1)\nwhere $c_o$ is the cost of the vehicle, including both operational cost and amortized capital cost. If vehicle n accepts any of the observed new order, then the reward function can be written as:\n$r_{n,t}(S_{n,t}, a_{n,t}) = \\beta_0 + \\beta_1 \\cdot Dis \\\\ - \\beta_2 \\cdot Pickup \\\\ - \\beta_3 \\cdot min(Add, thre) \\\\ - \\beta_4 \\cdot max(Add - thre, 0) - C_o$ (2)\nwhere the first term is the starting revenue of a vehicle picking up a new passenger; the second term is the revenue based on the distance between between new order's origin and destinations (denoted as Dis); the third term is the cost of the new passenger waiting to be picked up (with waiting time denoted by Pickup); the intuition of the fourth and fifth terms is to give a small penalty if the total additional travel time due to ride pooling compared with a direct non-sharing ride-hailing trip (denoted as Add) is below the threshold time (denoted as thre) but give a heavy penalty if the additional time is above the threshold. For the platform as a whole, the total reward $R_t$ at time t is the summation of rewards of all N agents at time t:\n$R_t(S_t, A_t) = \\sum_{n=1}^{N} r_{n,t}(S_{n,t}, a_{n,t})$ (3)\nwhere state $S_t$ and action $A_t$ at time t are respectively the collections of state and action of all N agents at time t:\n$S_t = [s_{1,t}, s_{2,t}, ..., s_{N,t}]$ (4)\n$A_t = [a_{1,t}, a_{2,t}, ..., a_{N,t}]$ (5)\n4) State Transition Function: The transition function can be represented in the form of $P(S_{t+1} | S_t, A_t)$. The explicit form of P(\u00b7, \u00b7) and reward function R(\u00b7, \u00b7) is unknown and will be learned later via RL/MARL methods.\nTo further delineate the decision-making process of agents in ride-pooling scenarios more clearly, we refer to the illustrative example presented in Figure 1, which features two collaborative agents. At time t, the figure displays two pooling vehicles, Vehicle 1 and Vehicle 2, awaiting dispatch decisions. Vehicle 1 is already committed to picking up a passenger from zone 2 and dropping him/her off at zone 4, while Vehicle 2 is idle at the moment. Two new requests are observed, including: (a) Rider 1 from zone 3 to zone 5; and (b) Rider 2 from zone 9 to zone b. The platform then collaborative dispatches two vehicles: Vehicle 1 is assigned action $a_{1,t}$ = 1, to pick up Rider 1. This action is integrated seamlessly with its current route, optimizing the journey for the existing passenger and enhancing operational efficiency. Concurrently, Vehicle 2 is designated action $a_{2,t}$ = 2, to pick up Rider 2, effectively utilizing its idle status. With assistance of RL methods, the collaborative decisions should enable Vehicle 1 to address the immediate needs of its onboard passenger while also strategically planning for an anticipated future pickup in zone 6."}, {"title": "B. Independent and Homogeneous Assumptions in MDP", "content": "Consider a ride-pooling platform with N vehicles. At time t, agent n can observe its own state $S_{n,t}$ and chooses action $a_{n,t}$. The Q-value of the overall platform (encompassing all the vehicles), represented as $Q_{tot}(S_t, A_t)$, could be expressed as:\n$Q_{tot}(S_t, A_t) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} | S_t, A_t ]$, (6)\nwhere I(\u00b7) is the centralized policy that maps the state space to the action space, \u03b3 is the discounted factor, $R_t$ is the joint reward of all agents at time t.\nThe objective for the platform is to find the optimal policy that maximizes the joint expected discounted cumulative reward over time, which could be expressed as:\n$Q_{tot}^{*}(S_t, A_t) = E_{\\pi^{*}}[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} | S_t, A_t ]$ (7)\nwhere II() is the centralized optimal policy, a mapping from the state space to the action space.\nHowever, in ride-pooling, thousands of agents might need to be simultaneously dispatched, which will lead to a prohibitively high dimensional state and action space for the above centralized MDP. Encountering this challenge, it is common in the previous literature to assume agents are independent [4]\u2013[6], [15], [16], [50], i.e., each agent's transition function and reward function has no interdependence with other agents' actions (note that our proposed method does not require this assumption), which largely reduces the MDP dimensionality and decentralizes the original transition function from $P(S_{t+1}|S_t, A_t)$ to $p(s_{t+1}|s_t, a_t)$. The independent assumption modifies the centralized MDP in Equation (6) into:\n$Q_{tot}(S_t, A_t) = \\sum_{n=1}^{N} E_{\\pi_n} [\\sum_{k=0}^{\\infty} \\gamma^{k} r_{n,t+k+1} | s_{n,t}, a_{n,t}] \\\\ = \\sum_{n=1}^{N} Q_n(s_{n,t}, a_{n,t})$ (8)\nwhere $\u03c0_n$ is the individual policy held by agent n and $r_{n,t}$ is the reward of agent n at time t, and $Q_n$ is defined as the expected accumulative reward of agent n under $s_{n,t}, a_{n,t}$.\nFurthermore, by assuming the vehicle agents are homogeneous (which is often the case in TNCs) and share the same policy, Equation (8) could be further simplified into:\n$Q_{tot}(S_t, A_t) = \\sum_{n=1}^{N} E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^{k} r_{n,t+k+1} | s_{n,t}, a_{n,t}] \\\\ = \\sum_{n=1}^{N} Q(s_{n,t}, a_{n,t})$ (9)\nThen, the goal of the simplified MDP model is to find the optimal policy $\u03c0*$ that maximizes the joint expected discounted cumulative reward over time:\n$Q_{tot}^{*}(S_t, A_t) = \\sum_{n=1}^{N} E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^{k} r_{n,t+k+1} | s_{n,t}, a_{n,t}]$ (10)"}, {"title": "C. Merging Independent Learning with Bipartite Matching", "content": "Since simply adopting independent assumptions will make the system ignore the complexity compounded by the likelihood of agents making concurrent decisions and fall into conflicts of agents accepting the same order, previous research has often employed a hybrid approach that melds independent reinforcement learning's policy function or value function with a bipartite matching process [4]\u2013[6], [15], [16], [50]. Here we adopt the representative integration of ILP with Independent value function like Q(s, a) to illustrate this idea.\nAt each matching time window, the central platform calculates the estimated cumulative total rewards for every feasible agent-order pair and determines the optimal order assignment to vehicles to maximize the platform's overall profit. In this scenario, the platform's objective when making assignment decisions can be approximated by the sum of all Q-values. The optimal assignment problem can thus be formulated as an ILP problem, as presented in Equation (11) below:\n$\\underset{X_{i,j}}{\\text{maximize }} Z(X) = \\sum_{n=1}^{N} \\sum_{z=0}^{Z_t} Q(S_{n,t}, z)X_{n,z}$ \nsubject to $\\sum_{n=1}^{N} x_{n,z} \\leq 1, \\forall z,$ \n$\\sum_{z=1}^{Z_t} x_{n,z} \\leq 1, \\forall n,$ \n$x_{n,z} \\in \\{0, 1\\}, \\forall n, z$ \n$\\sum_{n=1}^{N} x_{n,z} d_{n,z} < R_{match}, \\forall z,$ (11)\nwhere N is the total number of vehicles, $Z_t$ is the total number of observed orders by the platform at time t, $X_{n,z}$ denotes the matching decision for a specific vehicle-order pair, and $d_{n,z}$ is the distance between vehicle n and order z. This formulation is subject to constraints ensuring that at each decision time window, a vehicle (e.g., vehicle n) can only be matched with one order within the matching distance $R_{match}$ (e.g., such as order z), and similarly, one order can only be matched with one vehicle within this matching radius.\nRemark 1. In our study, we adopt the common assumption consistent with many existing literature: each vehicle is assigned only one request per time period. This aligns with many established methodologies, as seen in [15], [43], [51], [52]. Note that this assumption does not impose a significant loss of optimality compared to assigning bundled orders to the same vehicle simultaneously [3]. Specifically, in our context, dispatch decisions are made very frequently (e.g., every minute) in a dynamic manner. If the system intends to assign multiple requests to the same vehicle, it can first assign one order at the current time step, and even before the first order is picked up, it can assign another order to the same vehicle in a subsequent time period. This approach can actually be more optimal than assigning two orders to the same vehicle simultaneously. This is because deferring bundling decisions to future time points, when new information may become available, allows the platform greater flexibility to dynamically adjust decisions under uncertainties."}, {"title": "D. ILPDDQN Benchmark Framework", "content": "To learn the dynamics of the environment under the above-formulated framework, we will first review DDQN [53] as the backbone structure to learn reward and transition function from vehicle trajectories with format as (si, ai, ri, si), where si is the current state of vehicle i, ai is the action taken by vehicle i, ri is the reward received by vehicle i, and s'i is the next state of vehicle i. Compared with DQN [9], DDQN manages to mitigate the overestimation of Q-value by using the training network to select the best action for the generation of TD target in the loss calculation during training update, which could be formulated as follows:\n$L = E_{\\tau \\sim D} [r_i + \\gamma Q(s'_i, \\arg \\max_{a'_i} Q(s'_i, a'_i,; \\theta); \\theta^{-}) - Q(s_i, a_i; \\theta)]^2$ (12)\nwhere Q(s, \u03b1; \u03b8) is the Q value estimated by the training network whose neural network parameter is @ and Q(s, \u03b1; \u03b8\u00af) is the Q value estimated by the target network \u03b8\u00af, \u03c4 is the trajectory from the sampled mini-batch D. The parameters of DDQN training network will be updated through gradient descent with the equation as follows, where \u03b1 is the learning rate.\n$\\theta = \\theta - \\alpha\\Delta L$ (13)\nFor updating DDQN target network, Polyak Average is popular to be adopted for soft update [54] to map training network parameters to target network parameters after every training step as follows to help to stabilize the training process:\n$\\theta^- = \\rho\\cdot\\theta + (1-\\rho)\\cdot\\theta^-$ (14)\nwhere p is the soft update hyper-parameter.\nMoreover, to encourage exploration and exploitation trade-off at the early stage of the game, exploration decay and epsilon-greedy policy are adopted in DDQN. The formulation of exploration decay and epsilon-greedy policy is given in Equations (15) and (16) respectively, where e is the current exploration rate, \u1e9e is the decay rate, and er is a small predefined threshold exploration rate. DDQN still also employs experience replay to break the correlation of sequential experiences. This is a critical feature that prevents the update process from becoming cyclical and counterproductive, ensuring a more stable and effective learning progression.\n$\\epsilon = \\max(\\epsilon \\cdot \\beta, \\epsilon_T)$ (15)\n$\\pi^{*}(s_t) = \\begin{cases} \\arg \\max_{a_{n,t}} Q^{*}(s_{n,t}, a_{n,t}), & \\text{with prob } 1 - \\epsilon \\\\ a \\text{ random action}, & \\text{with prob } \\epsilon \\end{cases}$ (16)\nOverall, we could assemble the MARL with bipartite matching to establish a benchmark algorithmic framework, terms as ILPDDQN, which is detailed in Algorithm 1. In particular, after initialization simulator and DDQN networks in step 1 to 4, at every time window of the episode, central platform (matcher) will firstly update order information in step 6 and then perform bipartite match according to ILP calculations in step 7. Under bipartite match, vehicle agents will observe the matched orders, perform order choice actions, and then finally update DDQN network parameters from step 8 to 11. However, it is worth noting that now the experiences are collected and shared by every agent due to homogeneity and independence assumptions."}, {"title": "IV. LOCALIZED BIPARTITE MATCH INTERDEPENDENT MDP AND GATDDQN", "content": "While the previously introduced framework that combines independent RL with bipartite matching (i.e., ILPDDQN) significantly enhances scalability, it overlooks the intricate interdependence among agents in the MARL exploitation and training process. This oversight can lead to substantial overestimation of rewards, potentially leading to suboptimal solutions, particularly in the highly competitive environment of ride-pooling. Therefore, in this section, we present our novel Graph-based MARL algorithm, termed as GATDDQN, which effectively captures the agent interdependence with localized bipartite matching graph within a large-scale ride-pooling system. This serves as the novel MARL backbone for our BMG-Q framework.\nThe rest of this section will proceed as follows. We will initiate our discussion by showing how to build upon the previous MDP framework to incorporate localized bipartite matching. We will then review the fundamentals of classical Graph Attention Neural Network techniques. Following this, we will delve into the structure and formulation of our GAT-DDQN, which is designed to capture agent interdependence through a localized bipartite matching graph."}, {"title": "A. Localized Bipartite Match Interdependent MDP", "content": "At time t, when coordinating vehicle agent fleets, the interdependence among vehicles primarily emerges from the order matching process. Agents within the pickup range of the same orders may encounter the same orders, leading to potential competition. With this in mind, for agent n, we can define a localized bipartite match graph $g_{n,t} = \\{U_{n,t}, e_{n,t}\\}$, where $U_{n,t}$ denotes the nodes representing agents within the localized graph, and $e_{n,t}$ denotes the edges. In such a graph, edges are drawn between the ego agent (refers to agent n itself) and other agents only if those agents fall within a predefined proximity threshold. For the platform, as shown in Step 1 of Figure 3, we define an adjacency matrix where both the number of columns and rows correspond to the number of agents on the ride-pooling platform. A proximity threshold, referred to as the bipartite match radius, has been predefined. In this matrix, the entry (i,j) is set to 0 if the distance between agent i and agent j exceeds this radius (like agent A and I in Figure 3), and to 1 if their distance is within the radius (like agent A and B in Figure 3). Consequently, the adjacency matrix for the localized bipartite match graph $g_{n,t}$ can be derived by referencing either the n-th row or column of the platform's matrix. Here, we define N(n) as the set of neighbors for agent n within a certain radius, including the vehicle n itself. By utilizing the localized bipartite match graph, we can refine the previously independent MDP model from Section III into a localized bipartite match interdependent MDP model, where theQ value is redefined accordingly:\n$Q_{tot}(S_t, A_t) = \\sum_{n=1}^{N} E_{\\pi_{nm}} [\\sum_{k=0}^{\\infty} r_{n,t+k+1} | S_{n,t}, g_{n,t}, a_{n,t}] \\\\= \\sum_{n=1}^{N} Q(S_{n,t}, g_{n,t}, a_{n,t})$ (17)\nwhere $\u03c0_{om}$ represents the control policy for the localized bipartite match interdependent MDP. In this case, the goal of novel localized interdependent MDP model is to find the optimal policy $\u03c0_{om}$ that maximizes the joint expected discounted cumulative reward over time:\n$Q_{tot}^{*}(S_t, A_t) = \\sum_{n=1}^{N} E_{\\pi_{om}} [\\sum_{k=0}^{\\infty} r_{n,t+k+1} | S_{n,t}, g_{n,t}, a_{n,t}]$ (18)"}, {"title": "B. Classical Graph Attention Neural Network", "content": "However", "21": "the challenge intensifies in ride-pooling dispatch", "45": "to learn interdependencies within a bipartite match graph can yield misleading or inaccurate information for decision-makers. For illustrative purposes", "2": "an empty ride-pooling agent is surrounded by eight others (in dark blue)", "directions": "two heading east", "46": "are designed to handle data structured as graphs G = {V"}, {"45": "parameterized by a weight matrix $W \\in R^{F/ \\times F"}, "."], "N(i)": "n$c_{ij"}, "frac{exp(\\sigma(a^T[\\overline{s_i", "overline{s_j", {"a": "R^{F'} \\times R^{F'} \\rightarrow R$ is a learnable shared attention mechanism"}]