{"title": "BMG-Q: Localized Bipartite Match Graph Attention Q-Learning for Ride-Pooling Order Dispatch", "authors": ["Yulong Hu", "Siyuan Feng", "Sen Li"], "abstract": "This paper introduces Localized Bipartite Match Graph Attention Q-Learning (BMG-Q), a novel Multi-Agent Reinforcement Learning (MARL) algorithm framework tailored for ride-pooling order dispatch. BMG-Q advances ride-pooling decision-making process with the localized bipartite match graph underlying the Markov Decision Process, enabling the development of novel Graph Attention Double Deep Q Network (GATDDQN) as the MARL backbone to capture the dynamic interactions among ride-pooling vehicles in fleet. Our approach enriches the state information for each agent with GATDDQN by leveraging a localized bipartite interdependence graph and enables a centralized global coordinator to optimize order matching and agent behavior using Integer Linear Programming (ILP). Enhanced by gradient clipping and localized graph sampling, our GATDDQN improves scalability and robustness. Furthermore, the inclusion of a posterior score function in the ILP captures the online exploration-exploitation trade-off and reduces the potential overestimation bias of agents, thereby elevating the quality of the derived solutions. Through extensive experiments and validation, BMG-Q has demonstrated superior performance in both training and operations for thousands of vehicle agents, outperforming benchmark reinforcement learning frameworks by around 10% in accumulative rewards and showing a significant reduction in overestimation bias by over 50%. Additionally, it maintains robustness amidst task variations and fleet size changes, establishing BMG-Q as an effective, scalable, and robust framework for advancing ride-pooling order dispatch operations.", "sections": [{"title": "I. INTRODUCTION", "content": "THE widespread adoption of mobile communication and Global Positioning System technology has allowed Transportation Network Companies (TNCs) such as Uber, Lyft, and Didi to provide on-demand mobility services on a global scale [1], [2]. Ever since [3], the advantages of flexible and collaborative ride-sharing operations have become increasingly recognized within the transportation research community. In line with this trend, there has been an expanding body of research on operational policies for ride-sharing, including multi-hop ride-sharing [4], the coordination of ride-hailing with public transportation [5], ride-sharing with passenger transfers [6], integration of ride-sharing with parcel delivery [7], and the coordination of autonomous vehicles with conventional vehicles [8]. These advancements are propelled by breakthroughs in deep learning and Multi-Agent Reinforcement Learning (MARL) frameworks [9]\u2013[11].\nNevertheless, several hurdles must be overcome to unlock the full potential of MARL for developing effective, scalable, and robust real-time operational strategies for ride-pooling order dispatch. A principal challenge in this context is the complex interdependence in decision-making among vehicles, which leads to an exponential increase in both state and action spaces within large fleets [12]. One approach to address this is by traditional independent learning approaches, such as Independent Q-Learning (IQL) and Independent Proximal Policy Optimization (IPPO) [13], [14], which ignore the interdependence. In the context of ride-pooling, it is common in the existing literature to combine single-agent independent Reinforcement Learning (RL) with bipartite matching. For instance, [15] and [4] proposed to adopt a Deep Q-Network (DQN) for relocating ride-pooling agents and bipartite matching for order dispatch, and later on, extended it to multi-hop ride-sharing and parcel delivery [7]. To improve the transferability and scalability of the framework, [16] introduced additional techniques such as limited-memory upper confidence bound and reward smoothing. Moreover, [5] coordinated ride-hailing with public transit by encoding the decisions of subway stations into the states of tabular temporal difference learning. Similarly, [6] facilitated ride-pooling with passenger transfer. Yet, the practice of merging independent reinforcement learning with bipartite matching, while improving scalability, often overlooks the agents' complex interdependence during the RL exploitation and training phases. This can lead to significant overestimation of rewards, a critical concern in highly competitive environments such as ride-pooling, where the pronounced interdependence among agents intensifies the issue.\nTo accommodate the intricate interdependence among agents, several MARL frameworks have been introduced, including state-of-the-art Centralized Training with Decentralized Execution (CTDE) algorithms such as Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [17], Q-mix [18], Q-tran [19], and Multi-Agent Proximal Policy Optimization (MAPPO) [20]. However, these methods are typically applied to much smaller-scale problems. In contrast, large-scale ride-pooling order dispatch involves thousands of agents, rendering these approaches infeasible. To enhance algorithmic scalability while capturing agent interactions, researchers have explored novel concepts such as Mean-Field MARL [21], where agents"}, {"title": "II. RELATED WORKS", "content": "Ride-Pooling Order Dispatch. The operational dynamics of ride-pooling have garnered considerable attention due to their promising yet unpredictable real-time demand, as evidenced by various studies [3], [33], [34]. The nature of this uncertainty, coupled with the full potential of ride-pooling systems, introduces complexity into the process of coordinating vehicles with multiple passengers. Effective coordination requires not only addressing the needs of current passengers but also anticipating the needs of future riders, which includes managing new ride requests and those already being served.\nInitial investigations in this field have considered short-sighted, or myopic, policies that make vehicular assignments based on presently available information [3], [33]. Specifically, [3] notably advances this by introducing the shareability graph, which identifies possible sharing opportunities between new requests and vehicles on standby. They put forward a batch-matching strategy and crafted a sequential method that divides the decision-making process into vehicle routing and passenger assignment tasks. For more efficient real-time operations, [34] reduces the complexity of the matching problem by limiting the process to pairing a single passenger with a vehicle at each time step. More recent advancements in the field have shifted towards a better incorporation of the uncertainties related to future demand into the decision-making processes via methods such as model predictive control [35]\u2013[37], approximate dynamic programming [38]\u2013[40], and stochastic integer programming [41]. Note that these works are model-based, requiring explicit characterization of system dynamics and/or future uncertainties.\nMARL Framework for Ride-Pooling Dispatch. Given the super-human capabilities of RL and MARL showcased in a range of notable achievements [9]\u2013[11], the prospect of crafting practical MARL systems for the real-time optimization of ride-sharing dispatch grows increasingly compelling. While some researchers have endeavored to deploy multi-agent reinforcement learning approaches such as Mean-Field MARL [22], Q-mix [42], and Attention-based MARL [25], [43] in ride-sourcing scenarios, these methods continue to grapple with challenges like stability and scalability when it comes to training in large-scale and complex settings. To address the scalability issue in large-scale ride-sharing systems, it is common in the ride-sharing research community to combine single agent RL (or equivalently, Independent RL) with bipartite match. Specifically, [4], [15] propose to adopt DQN for"}, {"title": "III. PROBLEM FORMULATION & BENCHMARK METHODS", "content": "In this section, we formulate the ride-pooling order dispatch problem and review the strategies commonly used in previous literature. The ride-pooling vehicles are conventionally considered as independent and homogeneous agents under the bipartite matching process. A benchmark method will be established, against which we can compare our proposed algorithm in subsequent discussions.\nIn particular, we will begin by presenting the MDP formulation for ride-pooling order dispatch, detailing each agent's state, action, reward, discount factor, and transition function under various scenarios in Subsection A. We then explore how the assumptions of independence and homogeneity, prevalent in the ride-pooling community's approach, serve to decentralize the original MDP. Building upon the analysis, we illustrate the integration of Independent RL with ILP and then outline how RL techniques, such as Double Deep Q-Network (DDQN), could be applied to learn and represent the system's dynamics to finally form a benchmark framework, termed ILPDDQN, for ride-pooling order dispatch."}, {"title": "A. MDP Formulation for Ride-Pooling Order Dispatch", "content": "The ride-pooling order dispatch problem is normally formulated as a multi-agent MDP, with each ride-pooling vehicle representing an agent (we refer to it as agent or vehicle agent hereafter). Each vehicle agent's definition of state, action, reward, and transition function could be detailed as follows:\n1) State: For each vehicle n at time t, its state is $S_{n,t}=(I_{n,t}, U_{n,t}, P_{n,t}, O_{n,t}, D_{n,t}, t)$, where $I_{n,t}$ encodes the current location; $U_{n,t}$ is the number of vacant seats; $p_{n,t}$ encodes the information of passengers on board, including their estimated remaining time on board, drop-off locations, and current additional travel time; $o_{n,t}$ and $d_{n,t}$ represent a set of origin and destination pairs of the observed incoming orders within the matching distance of agent n, respectively; and t is the current time.\n2) Action: For available vehicle n (i.e., the vehicle is not full or in the process of picking up a new passenger) at time t, after seeing the incoming new orders, platform assigns action $a_{n,t}$ to decide whether to pick up one of the observed passengers: if not, we have $a_{n,t} = 0$ and then vehicle n will remain idle or continue with the remainder of its trip as determined by the onboard passenger's itinerary; otherwise if the vehicle is assigned by the platform to pick up the $z^{th}$ request (among all observed incoming orders of vehicle n), then we have $a_{n,t} = z$.\n3) Reward Function: If vehicle n is not available or does not accept any of the observed new order at time t, then the reward function at time t is as:\n$r_{n,t}(S_{n,t}, a_{n,t}) = -C_o$,                                                                       (1)\nwhere $c_o$ is the cost of the vehicle, including both operational cost and amortized capital cost. If vehicle n accepts any of the observed new order, then the reward function can be written as:\n$r_{n,t}(S_{n,t}, a_{n,t}) = \\beta_0 + \\beta_1 \\cdot Dis - \\beta_2 \\cdot Pickup - \\beta_3 \\cdot min(Add, thre) - \\beta_4 \\cdot max(Add - thre, 0) - c_o$                                                                                       (2)\nwhere the first term is the starting revenue of a vehicle picking up a new passenger; the second term is the revenue based on the distance between between new order's origin and destinations (denoted as Dis); the third term is the cost of the new passenger waiting to be picked up (with waiting time denoted by Pickup); the intuition of the fourth and fifth terms is to give a small penalty if the total additional travel time due to ride pooling compared with a direct non-sharing ride-hailing trip (denoted as add) is below the threshold time (denoted as thre) but give a heavy penalty if the additional time is above the threshold. For the platform as a whole, the total reward"}, {"title": "B. Independent and Homogeneous Assumptions in MDP", "content": "Consider a ride-pooling platform with N vehicles. At time t, agent n can observe its own state $S_{n,t}$ and chooses action $a_{n,t}$. The Q-value of the overall platform (encompassing all the vehicles), represented as $Q_{tot}(S_t, A_t)$, could be expressed as:\n$Q_{tot}(S_t, A_t) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t, A_t]$,                                                                                 (6)\nwhere I(\u00b7) is the centralized policy that maps the state space to the action space, y is the discounted factor, Rt is the joint reward of all agents at time t.\nThe objective for the platform is to find the optimal policy that maximizes the joint expected discounted cumulative reward over time, which could be expressed as:\n$Q_{tot}^{*}(S_t, A_t) = E_{\\pi^{*}} [\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t, A_t]$,                                                            (7)\nwhere II()* is the centralized optimal policy, a mapping from the state space to the action space.\nHowever, in ride-pooling, thousands of agents might need to be simultaneously dispatched, which will lead to a prohibitively high dimensional state and action space for the above centralized MDP. Encountering this challenge, it is common in the previous literature to assume agents are independent [4]\u2013[6], [15], [16], [50], i.e., each agent's transition function and reward function has no interdependence with other agents' actions (note that our proposed method does not require this assumption), which largely reduces the MDP dimensionality and decentralizes the original transition function from P($S_{t+1}$|$S_t$, $A_t$) to p($S_{t+1}$|$S_t$, $a_t$). The independent"}, {"title": "C. Merging Independent Learning with Bipartite Matching", "content": "Since simply adopting independent assumptions will make the system ignore the complexity compounded by the likelihood of agents making concurrent decisions and fall into conflicts of agents accepting the same order, previous research has often employed a hybrid approach that melds independent reinforcement learning's policy function or value function with a bipartite matching process [4]\u2013[6], [15], [16], [50]. Here we adopt the representative integration of ILP with Independent value function like Q(s, a) to illustrate this idea.\nAt each matching time window, the central platform calculates the estimated cumulative total rewards for every feasible agent-order pair and determines the optimal order assignment to vehicles to maximize the platform's overall profit. In this scenario, the platform's objective when making assignment decisions can be approximated by the sum of all Q-values. The optimal assignment problem can thus be formulated as an ILP problem, as presented in Equation (11) below:\n$\\text{maximize}  \\quad Z(X) = \\sum_{n=1}^N \\sum_{z=0}^{Z_t} Q(S_{n, t}, z) X_{n, z}$\n$\\text{subject to} \\quad \\sum_{n=1}^N x_{n, z} \\leq 1, \\quad \\forall z,$\n$\\sum_{z=1}^{Z_t} X_{n, z} \\leq 1, \\quad \\forall n,$\n$x_{n, z} \\in \\{0, 1\\}, \\quad \\forall n, z$\n$\\sum_{n=1}^N x_{n, z} d_{n, z} < R_{match}, z,$                                                                                                                                                                      (11)\nwhere N is the total number of vehicles, $Z_t$ is the total number of observed orders by the platform at time t, $X_{n,z}$ denotes the matching decision for a specific vehicle-order pair, and $d_{n,z}$ is the distance between vehicle n and order z. This formulation is subject to constraints ensuring that at each decision time window, a vehicle (e.g., vehicle n) can only be matched with one order within the matching distance $R_{match}$ (e.g., such as order z), and similarly, one order can only be matched with one vehicle within this matching radius.\nRemark 1. In our study, we adopt the common assumption consistent with many existing literature: each vehicle is assigned only one request per time period. This aligns with many established methodologies, as seen in [15], [43], [51], [52]. Note that this assumption does not impose a significant loss of optimality compared to assigning bundled orders to the same vehicle simultaneously [3]. Specifically, in our context, dispatch decisions are made very frequently (e.g., every minute) in a dynamic manner. If the system intends to assign multiple requests to the same vehicle, it can first assign one order at the current time step, and even before the first order is picked up, it can assign another order to the same vehicle in a subsequent time period. This approach can actually be more optimal than assigning two orders to the same vehicle simultaneously. This is because deferring bundling decisions to future time points, when new information may become available, allows the platform greater flexibility to dynamically adjust decisions under uncertainties."}, {"title": "D. ILPDDQN Benchmark Framework", "content": "To learn the dynamics of the environment under the above-formulated framework, we will first review DDQN [53] as the backbone structure to learn reward and transition function from vehicle trajectories with format as (si, ai, ri, si), where si is the current state of vehicle i, ai is the action taken by vehicle i, ri is the reward received by vehicle i, and s'i is the next state of vehicle i. Compared with DQN [9], DDQN manages to mitigate the overestimation of Q-value by using the training network to select the best action for the generation of TD target in the loss calculation during training update, which could be formulated as follows:\n$L = E_{\\tau \\sim D}[r_i+ \\gamma Q(s'i, arg \\max_{a}Q(s'_i, a; \\theta); \\theta^{-}) - Q(s_i, a_i; \\theta)]$,                                                                                                                                                                                                                                                                                                                                                                                                                                                                   (12)\nwhere $Q(s, \\alpha; \\theta)$ is the Q value estimated by the training network whose neural network parameter is @ and $Q(s, \\alpha; \\theta^{-})$ is the Q value estimated by the target network \\theta\u00af, \u03c4 is the trajectory from the sampled mini-batch D. The parameters of DDQN training network will be updated through gradient descent with the equation as follows, where a is the learning rate.\n$\\theta = \\theta - \\alpha \\Delta L$                                                                                                                                                                                                                                                                                                                                                                                                                                            (13)\nFor updating DDQN target network, Polyak Average is popular to be adopted for soft update [54] to map training network"}, {"title": "IV. LOCALIZED BIPARTITE MATCH INTERDEPENDENT MDP AND GATDDQN", "content": "While the previously introduced framework that combines independent RL with bipartite matching (i.e., ILPDDQN) significantly enhances scalability, it overlooks the intricate interdependence among agents in the MARL exploitation and training process. This oversight can lead to substantial overestimation of rewards, potentially leading to suboptimal solutions, particularly in the highly competitive environment of ride-pooling. Therefore, in this section, we present our novel Graph-based MARL algorithm, termed as GATDDQN, which effectively captures the agent interdependence with localized bipartite matching graph within a large-scale ride-pooling system. This serves as the novel MARL backbone for our BMG-Q framework.\nThe rest of this section will proceed as follows. We will initiate our discussion by showing how to build upon the previous MDP framework to incorporate localized bipartite matching. We will then review the fundamentals of classical Graph Attention Neural Network techniques. Following this,"}, {"title": "A. Localized Bipartite Match Interdependent MDP", "content": "At time t, when coordinating vehicle agent fleets, the interdependence among vehicles primarily emerges from the order matching process. Agents within the pickup range of the same orders may encounter the same orders, leading to potential competition. With this in mind, for agent n, we can define a localized bipartite match graph $g_{n,t} = \\{U_{n,t}, e_{n,t}\\}$, where $U_{n,t}$ denotes the nodes representing agents within the localized graph, and $e_{n,t}$ denotes the edges. In such a graph, edges are drawn between the ego agent (refers to agent n itself) and other agents only if those agents fall within a predefined proximity threshold. For the platform, as shown in Step 1 of Figure 3, we define an adjacency matrix where both the number of columns and rows correspond to the number of agents on the ride-pooling platform. A proximity threshold, referred to as the bipartite match radius, has been predefined. In this matrix, the entry (i,j) is set to 0 if the distance between agent i and agent j exceeds this radius (like agent A and I in Figure 3),"}, {"title": "B. Classical Graph Attention Neural Network", "content": "However, unlike the previous case of ride-sourcing [21], the challenge intensifies in ride-pooling dispatch, where fully understanding and aggregating the interdependence within the bipartite match graph becomes more complex. In a ride-pooling environment, each agent may have unique passengers with different itineraries. Traditional GCNs that employ average or max aggregation methods [45] to learn interdependencies within a bipartite match graph can yield misleading or inaccurate information for decision-makers. For illustrative purposes, consider the scenario depicted in Figure 2: an empty ride-pooling agent is surrounded by eight others (in dark blue), each evaluating its decision concerning a new order request heading southeast. Each neighboring agent carries passengers destined for various directions: two heading east, two west, two south, and two north. When this empty agent attempts to assess the interdependencies of its neighbors to make informed decisions, simplistic aggregation methods like averaging or maximum can introduce significant inaccuracies. Averaging the directions might falsely suggest that these agents lack specific destinations, effectively dismissing all directional data (Figure 2a). Conversely, using maximum aggregation could distort the representation, focusing only on a single direction and ignoring the diversity of passenger destinations (Figure 2b).\nTherefore, after constructing the localized bipartite match graph for each agent, we can employ GATs to enable an unassigned agent to dynamically weigh its neighbors based on the current scenario. For example, it might assign higher relevance to agents heading south and east, aligning more closely with a southeast-bound order request. To this end,"}, {"title": "C. GATDDQN for Localized Bipartite Match Graph", "content": "In this subsection, we will combine GATs with the localized bipartite matching graph to capture the localized bipartite match interdependence of agents, leading to the GATDDQN in Step 2 of Figure 3. However, different from the simple attention mechanism in the preceding subsection, here for aggregation layer, we adopt transformer-style attention mechanism [55] in Equation (23) to compute the attention score in order to better capture and aggregate the complex information lying within the localized graph of ride-pooling system:\n$e_{ij} = softmax_j \\Big( \\frac{Q_i K_j}{\\sqrt{d_k}} \\Big)$                                                                               (23)\nwhere $Q_i = W^Q s_i, K_j = W^K s'_j$, and $V_i = W^V s'_j$ with $W^Q, W^K, W^V \\in R^{F'\\times F'}$ being the weight matrices for queries, keys, and values, respectively, and dk is the dimensionality of the key vectors for scaling. Similarly, we adopt multi-head attention to stabilize the learning process and enrich model capacity as follows:\n$s_i^{*} = W^{''} (\\sum_{j \\in N(i)}^{K} e_j W^{k} s_j)$                                                                          (24)\nwhere $W^{''} \\in R^{KF'\\times F'}$ is the final linear transformation layer that maps the concatenation of the K heads' aggregated features into the same dimensions of $s_i$, and $||_{k=1}^{K}$ represents the operation of concatenating multiple heads' aggregated features (i.e., $\\sum_{j \\in N(i)} e_j W^k s_j$).\nWith information aggregated, we could concatenate the state information of ego agent (i.e., si) and aggregated states"}, {"title": "V. BMG-Q: EFFECTIVE, SCALABLE, AND ROBUST RIDE-POOLING ORDER DISPATCH FRAMEWORK", "content": "With the ideas of localized bipartite match interdependent MDP and GATDDQN backbone established, in this section we wil discuss how GATDDQN's value estimations could be combined with ILP via our proposed posterior score function for the bipartite match process to finally form the proposed BMG-Q framework."}, {"title": "A. Dynamic ILP via Posterior Score Function", "content": "In existing literature, the ILP for the bipartite matching process typically relies on Q(s, a, 0), such as (11) in Section II. However, this approach encounters two issues: (1) it lacks an exploration mechanism in the bipartite matching process, which restricts the exploration of the vehicle agents' state space; (2) matching based solely on Q(s, a, \u03b8) is prone to bias due to variability in the estimates, which can affect the decision-making process. These will potentially lead to suboptimal solutions.\nTo deal with the above two issues, we propose posterior score function to better integrate the value function of GATDDQN with ILP. The formulation of our posterior score function $S(s_{n,t}, g_{n,t}, a_{n,t})$ is given in Equation (26) below, with its visualization shown in Figure 4. To effectively balance exploration and exploitation, we implement an e-greedy strategy and introduce a term $S_{explore}$ during the exploration phase. This term represents the upper bound of the Q-value, S(Sn,t, gn,t, an,t), which is set to a significantly high value (e.g., 100,000) to encourage exploration in exploration stage:\n$S(S_{n,t}, g_{n,t}, a_{n,t}) =\\begin{cases}\nQ(S_{n,t}, g_{n,t}, a_{n,t}) - b(S_{n,t}, g_{n,t}), & \\text{with prob 1 - } \\epsilon \\\\\nS_{explore}, & \\text{with prob } \\epsilon\n\\end{cases}$                                                                         (26)\nIn the exploitation stage, to mitigate variance, we adjust the Q-value by subtracting a bias term $b(s_{n,t}, g_{n,t})$, which remains unaffected by the actions of the agents. This term can either be a constant or the state's value function, $V(S_{n,t}, g_{n,t})$. Employing $V(S_{n,t}, g_{n,t})$ gives rise to the advantage function $A(S_{n,t}, g_{n,t}, a_{n,t}) = Q(S_{n,t}, g_{n,t}, a_{n,t}) - V(S_{n,t}, g_{n,t})$. In this context, the advantage function $A(s_{n,t}, g_{n,t},z)$ signifies the relative benefit of assigning agent n to pick up a particular order z, hence quantifying the importance of the assignment. Replaced with our score function, the novel dynamic ILP formulation can be written in Equation (27) below:\n$max_{X_{n,z}} \\quad \\sum_{n=1}^N \\sum_{z=0}^{Z_t} S(S_{n,t}, g_{n,t}, z) X_{n, z}$\nsubject to  $\\sum_{n=1}^N x_{n, z} \\leq 1,\\quad \\forall z,$\n$\\sum_{z=1}^{Z_t} X_{n, z} \\leq 1, \\quad \\forall n,$\n$X_{n,z} \\in \\{0,1\\}, \\quad \\forall n, z$\n$\\sum_{n=1}^N x_{n, z} d_{n, z} < R_{match}, \\forall z,$                                                                                                              (27)\nCompared with the ILP commonly adopted in the existing literature (11), the proposed score function not only captures the dependence of value estimations on the localized graph, but also captures the importance of order z for vehicle n. We will show through numerical simulation that the proposed approach can significantly reduce overestimation and improve the overall performance of the proposed BMG-Q framework."}, {"title": "B. Training GATDDQN for Large-Scale System", "content": "Since GATs could be trained with downstream neural network loss functions, the GATDDQN backbone could be trained end to end via slightly modifying the TD error introduced in Equation (12) into the Equation (28) below.\n$L = E_{\\tau \\sim D}[r_i+ \\gamma Q(S'_i, g'_i, arg \\max_{a}Q(S'_i, g'_i, a; \\theta); \\theta^{-}) - Q(S_i, g_i, a_i; \\theta)]$                                                                                                                                                                                                                                                                           (28)"}, {"title": "VI. CASE STUDIES", "content": "This sections presents a case study that utilizes real-world data from Manhattan, New York City. We will first detail the implementation of our simulation framework, after which we will showcase the effectiveness, scalability and robustness of our BMG-Q framework through the training and validation results."}, {"title": "A. Simulation Setup", "content": "The simulation environment for this study is based on the public dataset of taxi trips in Manhattan, New York City [15], [31]. This dataset includes detailed information for each trip, such as pickup and dropoff times, origin and destination geo-coordinates, trip distance, and duration. Focusing on peak hours-specifically from 8:00 AM to 10:00 AM\u2014we tailored the training dataset to include data from trips that occurred between 8:00 AM and 8:30 AM on May 4, 2016. During this half-hour period, the average order density reached approximately 275 trips per minute in central Manhattan, totaling about 8,250 orders. For the validation dataset, we similarly extracted data from trips within the same half-hour window but on various days throughout May 2016. We divided Manhattan into 57 zones. This zoning was informed by the distribution of orders and a resolution of 800m x 800m was used, for which a visualization is shown in Figure 6. To serve these demand with a minimum service rate of 85% across all"}, {"title": "C. Summary of the proposed BMG-Q Framework", "content": "Finally, we give a summary of our whole BMG-Q Frame-work. The framework could be visualized using Figure 5 below. Specifically, during each time window, when new orders arrive, unmatched orders and vehicle information are first resorted and updated. With the evaluations from the GATDDQN network, the central platform assigns these orders to vehicle agents via solving the ILP. After bipartite match assignments, the vehicle agents perform their respective actions and collect their experiences for further GATDDQN learning. Subsequently, the routing system updates the routes and estimated times of arrival (ETAs), which are then communicated back to the central platform.\nThe training details of our BMG-Q framework could be found at Algorithm 2. Specifically, after initializing the simulator and GATDDQN in steps 1 through 4, we enter the training phase. To achieve a balance between exploitation and exploration, we perform exploration decay to exploration rate of the bipartite matching process in step 5. This gradual reduction in exploration rate is designed to transition the focus from exploration to exploitation as the learning advances. In steps 9 to 12, similar to the DDQN backbone of ILPDDQN, our GATDDDQN backbone adopts double networks, experience replay, and soft update. Thanks to the localized bipartite match graph topology, graph sampling, and gradient clipping introduced in steps 9 to 11, GATDDQN backbone manages to learn to capture the localized interdependence in very large-scale system with thousands of agents, thus leading to more optimal assignment decisions of the overall BMG-Q framework."}, {"title": "VII. CONCLUSION", "content": "This paper proposes the Localized Bipartite Match Graph Attention Q-Learning (BMG-Q), a novel effective, scalable, and robust MARL algorithm framework tailored for large-scale ride-pooling order dispatch. By integrating localized bipartite match within the MDP of the ride-pooling system, we developed GATDDQN as a novel MARL backbone to accurately capture the dynamic interactions among agents in the large-scale ride-pooling order dispatch systems. Enhanced by gradient clipping and localized graph sampling, our GATDDQN improves scalability and robustness for very large-scale system, while the inclusion of a posterior score function in ILP captures the online exploration-exploitation trade-off and assists to reduce potential overestimation bias of agents. Through extensive experiments and validation, we show that BMG-Q demonstrates a superior performance in both training and operations of thousands of vehicle agents, outperforming benchmark RL frameworks by around 10% in accumulative rewards and showing a significant reduction in overestimation bias by over 50% while maintaining robustness and effectiveness amidst task variations and fleet size changes. Potential enhancements to our framework could be achieved by extending its application to multimodal/intermodal transportation systems [4], [5]. Additionally, refining the framework by integrating BMG-Q learning with KL-control methods [67] or conducting a more thorough theoretical analysis and proof of the underlying MDP may bring significant further advancements [58]."}]}