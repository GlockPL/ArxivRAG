{"title": "Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base for Geospatial Code Generation Tasks Using Large Language Models", "authors": ["Shuyang Hou", "Anqi Zhao", "Jianyuan Liang", "Zhangxiao Shen", "Huayi Wu"], "abstract": "The rapid emergence of spatiotemporal data and the growing need for geospatial modeling have led to the tendency of automating these tasks using large language models (LLMs), for research efficiency and productivity. The general LLMs encounter coding hallucinations in geospatial code generation due to the lack of domain-specific knowledge about geopsatial functions and relative operators. The retrieval-augmented generation (RAG) technique, coupled with an external operator-function knowledge base to provide description for these geospatial functions and operators, presents an effective solution to this challenge. However, a broadly recognized framework to develop such knowledge base is still absent. This study proposes a comprehensive framework to build the operator-function knowledge base, leveraging the rich semantic and structural knowledge embedded within geospatial scripts. The framework contains three core components: Function Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination Statistics (Geo-FuST), and Combination and Semantic Framework Mapping (Geo-FuM). Geo-FuSE integrates techniques including Chain-of-Thought (CoT), TF-IDF, t-SNE, and Gaussian Mixture Model to uncover semantic features in the scripts. Geo-FuST employs abstract syntax trees (ASTs) and the APRIORI algorithm to identify frequent operator combinations from the code structures. Geo-FuM combines LLMs with the fuzzy matching algorithm to align these operator combinations with the function semantic framework, and forms the Geo-FuB knowledge base. An instance of Geo-FuB has been developed based on 154,075 Google Earth Engine scripts and available on https://github.com/whuhsy/Geo-FuB. A comprehensive evaluation metric is introduced to assess both the structural integrity and semantic accuracy of the framework. The overall accuracy reaches 88.89%, with structural accuracy reaching 92.03% and semantic accuracy at 86.79%. In addition, this study highlights the potential of Geo-FuB for optimizing geospatial code generation tasks based on RAG paradigm and fine-tuning paradigm, respectively, offering both research inspiration and empirical resources to address the limitations in such tasks.", "sections": [{"title": "1. Introduction", "content": "With the growing need for comprehensive and detailed understanding of geospatial processes, spatiotemporal analysis has become increasely complex. Geospatial analysis platforms\u2014such as Google Earth Engine (GEE), ArcGIS, QGIS, and ENVI have become essential tools to build geospatial models and perform spatiotemporal analysis[1]. Confronted with vast amounts of spatiotemporal data and the increasing complexity of modeling requirements, researchers are seeking to escape from the tedious and repetitive task of crafting basic codes. Automatic geospatial code generation has emerged as a necessary strategy to simplify and streamline the geospatial modeling process[2].\nGeospatial code generation is a knowledge-intensive task, facing challenges such as ambiguous and non-standard user requirements, the complexity of interdisciplinary knowledge, and the data heterogeneity[3]. Large language models (LLMs), with their powerful contextual learning, logical reasoning, and natural language generation capabilities, offer the potential for automatic geospatial code generation[4]. However, LLMs are typically trained on extensive general-purpose corpora, which often fall in short of incorporating domain-specific expertise[5]. Moreover, due to the probabilistic nature of the generation in LLMs, their prediction mechanisms are characterized by dynamic variability, instability, inherent randomness, and weak controllability. While these biases have limited impact on natural language tasks, they can lead to significant errors in code generation, often known as \u201ccoding hallucinations[6]\", such as confusion in coding language, the invention of non-existent operator functions, and the fabrication of operator names. As a result, there is considerable demand for enhancing the reliability of LLMs to generate geospatial code automatically.\nAn effective method for integrating domain-specific and task-specific insights into LLMs is to integrate the Retrieval-Augmented Generation (RAG) technology with customized external knowledge base to guide the LLM in generating more accurate results[7]. However, creating an external knowledge base for geospatial modeling tasks that covers comprehensive operator combinations, function descriptions, and domain-specific logic requires not only precise and complete content but also well-organized and easily retrievable structure[8]. Currently, a widely-recognized framework or protocol that can guide the development and assessment of these specialized knowledge bases is still lack. Generally, user-developed geospatial scripts in different geospatial analysis platforms include two principal categories of knowledge that are crucial for geospatial modeling[9]: (1) Semantic Information, including function descriptions and process annotations, which reveal the purpose and operational logic of the scripts. (2) Structural Knowledge, encompassing computational logic, dependencies between operators, and combination patterns[10]. However, to the best of our knowledge, these geospatial scripts have not yet been systematically mined, organized, and utilized for constructing domain-specific knowledge bases in the geospatial modeling field.\nTo address the issues faced by LLMs in geospatial code generation, including the coding hallucination, the lack of specialized knowledge bases, and insufficient mining of geospatial script knowledge, this paper proposes a framework named Geo-FuB to construct an operator-function knowledge base for geospatial code generation tasks using LLMs. This framework includes three core components: Function Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination Statistics (Geo-FuST), and Combination and Semantic Framework Mapping (Geo-FuM). Geo-FuSE integrates the Chain of Thought (CoT), TF-IDF, t-SNE, Gaussian Mixture Model (GMM) and expert experience to uncover semantic features in different geospatial scripts. Geo-FuST employs abstract syntax trees (ASTs) and the APRIORI algorithm to identify frequent operator combinations from the code. Geo-FuM leverages LLMs and the fuzzy matching algorithm to align various operator combinations with the proposed semantic framework, and utimately form the operator-function knowledge base. A instance of the Geo-FuB knowledge base has been constructed based on 154,075 geospatial scripts in GEE environment and is available on the https://github.com/whuhsy/Geo-FuB. Furthermore, a comprehensive evaluation\""}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Domain Specialization of Large Language Models", "content": "Pre-trained language models (PLMs) based on self-attention mechanism and Transformers can extract general language representations from large-scale unlabeled data in an unsupervised manner[11]. These models have demonstrated significant advantages across a spectrum of natural language processing (NLP) tasks[12], such as text generation[13,14], machine translation[15], question answering system[16-18], sentiment analysis[19], and knowledge extraction[20]. As the scale of model parameters has grown from millions to billions, the contextual understanding and logical reasoning capabilities of PLMs have significantly improved[21]. These large-scale PLMs are referred to as large language models (LLMs).\nLLMs overwhelm other small-scale PLMs owing to the extensive corpora used in their training process. However, the large-scale parameters and extensive training data act as a \u201cdouble-edged sword", "knowledge hallucination[23]": "Therefore, integrating domain knowledge to LLMs and promoting their specialization in domain[24](i.e., the \u201cLLM+X\u201d model) has become a research focus.\nBased on domain-specific adjustments, LLMs have been applied to specialize tasks of various fields, including social sciences (such as education[25], finance[26] and law[27]), natural sciences (such as biomedicine[28] and earth sciences[29]), and applied sciences (such as human-computer interaction[30], software engineering[31], and cybersecurity[32]). Examples of these applications include: (1) Advanced Information Extraction, aiming at identifying entities, relationships, and"}, {"title": "2.2. Geospatial Code Generation with Large Language Models", "content": "The geospatial domain faces unique challenges in leveraging LLMs for automated geospatial modeling becasue of the highly specialized code requirements in different geospatial analysis platforms. Automatic geospatial modeling requires LLMs not only to handle complex geospatial data but also to generate code that conforms to platform specifications (such as GEE[39]). These platforms typically provide programming environments based on languages like Python or Java and have developed specifications and operational logics for geospatial analysis, which necessitates LLMs to take extra efforts to learn and adapt.\nLLMs have been used in automatic code generation[41,42], such as generating or analyzing code based on natural language descriptions[43], identifying errors[44], or proposing improvements, nevertheless, existing research predominantly focused on general code generation and has not been specifically adapted to the unique code of geospatial platforms. The foundational training corpora of LLMs mainly involve general programming knowledge and lack the specific function libraries and operational workflows unique in different geospatial analysis platforms. The knowledge gap makes it challenging for the LLMs to generate code that meets the requirements of these geospatial analysis platforms. Even though techniques like prompt engineering and CoT have improved the accuracy and flexibility of LLMs, their effectiveness remains inadequate when faced with the specialized needs of the geospatial analysis."}, {"title": "2.3. Construction of Specialized Knowledge Bases for Geospatial Tasks", "content": "The generation and reasoning performance of LLMs depend on the quality of their training corpora[45]. It is crucial to construct target-oriented and high-quality knowledge bases to enhance the models' generative capabilities for the geospatial field. Researchers have integrated specialized knowledge bases to optimize the performance of LLMs in downstream tasks. For instance, Cheng Deng et al. compiled 6 million geoscience academic papers to create the GeoSignal dataset, which includes article content, classifications, references, and entity information, providing valuable resources for tasks such as geospatial question answering, named entity recognition and conceptual relationship reasoning[46]. Similarly, Yifan Zhang et al. developed the BB-GeoPT knowledge base, which combines high-quality GIS journal papers, Wikipedia GIS pages, general instruction sets, and specialized instruction data to enhance the performance of geospatial knowledge question-answering systems[47].\nAlthough these efforts have advanced the development and application of geospatial knowledge bases, a comprehensive and systematic knowledge base for automated geospatial modeling has not yet been established. Such knowledge base should establish a complete and retrievable knowledge system that covers independent operator knowledge of geospatial analysis platforms, geoprocessing function semantics, operator combination information and function-operator mappings. Incorporating these corpora into the training or inference process of LLMs can improve the accuracy and efficiency of geospatial code generation, thereby enhancing the models' automatic modeling capabilities and providing more intelligent and efficient solutions for geospatial research.\nCrowdsourced geospatial scripts are invaluable resource[48] that include both the subjective semantic insights provided by detailed function descriptions and process annotations, along with the objective statistical data that maps out the intricate dependencies and combination patterns of operators. These information can be transformed into retrievable knowledge and harnessed as vital input for LLMs in terms of retrieval augmentation, generation optimization, and fine-tuning processes."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Overall Framework", "content": "This study proposes a comprehensive framework for constructing an operator-function knowledge base. The complete workflow of the knowledge base construction is illustrated in Figure 1, which encompasses the three main components: Geo-FuSE for Function Semantic Framework Construction (the green part), Geo-FuST for Frequent Operator Combination Mining (the blue part), and Geo-FuM for Combination and Semantic Framework Mapping (the teal part).\nIn the Function Semantic Framework Construction stage, geospatial scripts are treated as domain-specific natural language texts. Through prompt engineering with LLMs, the workflows within the scripts are distilled, while the raw code with annotations are parsed into a collection of functional statements (i.e., natural language descriptions of a series of geospatial operations within each script). Next, the TF-IDF algorithm is used to construct vector matrices for these functional statements. The vector matrices are reduced to two dimensions based on t-SNE, and clustered through the Gaussian Mixture Model (GMM) and Bayesian Index Criterion (BIC) index. The clustered results are then refined and semantically labeled with expert experience to construct a three-level standard Function Semantic Framework, Geo-FuSE.\nIn the Frequent Operator Combination Mining stage, geospatial scripts, after annotation"}, {"title": "3.2. Geo-FuSE: Function Semantic Framework Construction", "content": "In geospatial code generation tasks, a function is defined as a specific task or operation accomplished through the collaboration of a set of operators. A function in the geospatial script contains the following characteristics: Clarity: Each function is composed of a set of operators and has a clear boundary and objective. Operability: Functions can be refined into specific operational steps, and accomplished through the implementation of operators. Hierarchical Structure: Functions are defined in a layered manner, with primary, secondary, and tertiary categories, which facilitates systematic classification, indexing, and hierarchical evaluation. The overall workflow of Geo-FuSE is illustrated in Figure 2."}, {"title": "3.2.1. CoT-based Prompt Engineering for Function Semantic Extraction", "content": "Geospatial scripts usually contain rich functional semantic[49] annotations for the design logic, code functionality explanation, dataset characteristics, and research objectives. Meanwhile, the operator names in the code can also implicitly convey functional semantics that are difficult to parse with traditional language models. Long scripts usually contain multiple functional units for complex, multi-step tasks such as large-scale terrain data analysis, while short scripts mainly focus on single data processing objective such as meteorological parameter extraction and usually contain less or even one functional unit. This difference makes it challenging to delineate functional granularity. Fortunately, the powerful natural language parsing and semantic understanding capabilities of LLMs allow them to address this challenge and focus on deeper functional semantic analysis.\nThe Chain of Thought (CoT) method[50] simulates the thought process of human teaching by setting a series of highly relevant prompt words, and provides a heuristic thinking mode for LLMs to progressively generate the correct answer. By combining the CoT with a designated template that contains clear and structured prompting instructions, LLMs are able to accurately parse the set of functional statements. The design of the CoT-based prompt template is shown in Figure 3, with the words adaptable to different geospatial processing platforms and languages."}, {"title": "3.2.2. TF-IDF Feature Matrix", "content": "The extraction of functional workflows primarily relies on the internal knowledge generation of LLMs, which poses challenges while producing semantically-correct descriptions because of the diversity in expressions. The first challenge is the issue of semantic redundancy, as the same function may be described significantly differently in various scripts, thus causing redundancy. For example, the \"data retrieval\" function might be described as \"perform data retrieval operation,\" \"conduct data search,\" or \"retrieve and extract data.\". The second challenge is the ambiguity of functional boundaries. For example, \"data analysis and visualization\" and \"data visualization display\" are similar in semantics, they can be considered either different aspects of the same function or divided into two separate functions. To address these issues, the natural language descriptions of functional statements are converted into vector representations in a high-dimensional space, thereby quantifying the subtle semantic differences between functional statements at a granular level.\nTF-IDF (Term Frequency-Inverse Document Frequency) is a method to evaluate the importance of natural language segments within the entire collection of segments by calculating term frequency and inverse document frequency[51]. Specifically, TF (term frequency) measures the frequency of"}, {"title": "3.2.3. t-SNE Dimensionality Reduction", "content": "Since the functional statements are constructed into high-dimensional vectors after the TF-IDF, directly clustering high-dimensional vectors will consume substantial computational resources and be difficult to be visualized. Therefore, the dimensionality reduction method is applied as a prevalent approach to understand the inherent connections between functional statements represented in high-dimensional vectors.\nAmong the dimensionality reduction methods, t-SNE (t-Distributed Stochastic Neighbor"}, {"title": "3.2.4. Gaussian Mixture Model Clustering", "content": "The results from t-SNE dimensionality reduction are organized into groups with clear semantic boundaries through clustering analysis to classify similar term vectors. To achieve this, the Gaussian Mixture Model (GMM), a clustering method based on probabilistic statistical theory, is used for clustering analysis in this study[53]. GMM has significant advantages over other classic clustering methods, such as K-means and DBSCAN[54]. K-means requires a preset number of clusters and lacks an effective evaluation mechanism, making it difficult to determine the optimal value. Additionally, it is only suitable for clusters with relatively fixed sizes and densities, limiting its flexibility. DBSCAN can identify clusters of arbitrary shapes, but its performance is highly dependent on parameter settings, making it difficult to handle clusters with varying densities and high-dimensional data. {Fan, 2023 #6} GMM does not require a preset number of clusters and can automatically determine the best number of clusters within the desired range. Furthermore, GMM captures clusters of different shapes and sizes in the data set flexibly through the linear combination of Gaussian distributions.\nThe core idea of GMM is to consider the dataset as a complex distribution composed of multiple Gaussian distributions. Each Gaussian distribution in this context represents a potential cluster, each of which is a set of terms with similar functions. During the clustering process, GMM fits the data by estimating the parameters of each Gaussian distribution. Mean vector $\\mu_k$ represents the center of the k-th cluster. Covariance matrix $\\Sigma_k$, defines the shape and size of the k-th cluster. Mixing weight $\\pi_k$ indicates the importance of the k-th cluster in the entire dataset, with\n$\\sum_{k=1}^{K} \\pi_k = 1$. Through the estimation of these parameters, GMM can flexibly adjust the linear combination of different Gaussian functions to accommodate the complexity and diversity of the data. The probability density function for a term vector $X_i$ is defined by:\n$p(x_{i} | \\Theta) = \\sum_{k=1}^{K} \\pi_k \\cdot N(X_{i} | \\mu_k, \\Sigma_k)$#(9)\nWhere $\\Theta = {\\pi_k, \\mu_k, \\Sigma_k}$ represents the set of model parameters, $p(x_i)$ represents the probability density of term vector $X_i$ given the parameter set 0, $K$ represents the total number of clusters, $\\pi_k$ is the weight of the k - th Gaussian distribution, and $N(x|\\mu_k, \\Sigma_k)$ is the probability density function of a Gaussian distribution with mean $\\mu_k$ and covariance matrix $\\Sigma_k$. This formula"}, {"title": "3.2.5. Expert Experience-Based Classification", "content": "Expert experience is integrated to interpret semantic groups and establish a standardized function semantic framework based on clustering results. The framework adopts a three-level classification strategy based on comprehensive considerations of the following three dimensions:\nFirst, it follows the coding common sense and operational logic. Geospatial data processing usually follows a workflow of \u201cpreprocessing-core operation-postprocessing\". Therefore, the framework sets this workflow as the first level to grasp the function categories at a macro level. The bottom level directly corresponds to the fine-grained functional units obtained from the clustering analysis, representing specific implementations. To connect these two levels, an intermediate second-level category is established to summarize and organize function units with similar properties or logical connections, making the framework more logical and complete.\nSecond, the multi-level classification system supports precision and flexibility for future function mapping. It constructs a multi-tiered matching mechanism that allows the system to automatically backtrack to the previous level when the finest-grained functional unit cannot be precisely matched, until a generalized match is achieved. This mechanism enhances the reliability and applicability of the mapping results.\nFinally, integrating the function semantic framework into the application scenario of the CoT methodology. The framework not only provides a clear hierarchical structure but also supports a progressively deeper, step-by-step reasoning path. Each level of classification within the framework becomes a node in the chain, allowing for consecutive questioning that narrows down from broad categories to specific functional units. For instance, a user might inquire whether a certain function belongs to the first-level major category (e.g., preprocessing) at the begining, then further refine to the second level (e.g., data cleaning), and finally determine the specific third-level function (e.g., outlier removal). The CoT method matches human's natural process of handling complex problems that progressively narrows down the scope to approach the answer,"}, {"title": "3.3. Frequent Operator Combination Mining", "content": "A function is defined by a specific combination of operators, however, the operator combinations to achieve the same function can vary. For example, a function might be realized by the collaboration of operator A and operator B, or through the combination of operator C and operator D. Given the infinite possibilities of operator combinations, relying solely on manual enumeration has significant limitations. On one hand, theoretically feasible operator combinations might not meet practical application needs and thus be overlooked. On the other hand, effective combinations in practice might be missed due to the limitations of manual enumeration. Therefore, identifying and assessing operator combinations based on human experience is impractical and inefficient, and lacks the flexibility to find significant combinatorial patterns.\nTherefore, a data-driven approach is adopted by analyzing a large number of scripts and using statistical methods to identify frequent operator combinations. The frequent occurrence of these combinations statistically indicates their importance and synergy in achieving specific functions and represents how users tend to use the operators in practical. This data-driven approach compensates for the shortcomings of the enumeration method and provides operator combination information for function framework construction, thus enhancing its value in LLM modeling tasks."}, {"title": "3.3.1. Cleaning and Structuring", "content": "When constructing the function semantic framework, it is difficult to accurately get operator call frequencies with a rule-based traversal of raw scripts due to the diversity in user habits. Therefore, the code needs to be converted into a structured representation to design effective traversal algorithms for precise operator statistics. The Abstract Syntax Tree (AST) technology is introduced to represent the code in a structured form. The AST is a tree-like data structure that represents the syntactic structure of a program, clearly showing the hierarchical relationships between code components. Since geospatial scripts are based on languages like Java or Python, they can be effectively converted into structured JSON format with AST parsing, as illustrated in"}, {"title": "3.3.2. Operator Relationship Frequency Statistics and Frequent Pattern Mining", "content": "Direct traversal methods show limitations in analyzing and identifying frequent operator combinations due to the uncertainty in combination lengths. In this study, a statistical method based on pairwise operator relationships is introduced. First, the pairing relationships and their occurrences between operators in each script are counted. Then, the same operator call relationships from different scripts are merged to form a comprehensive operator call frequency table. However, it is insufficient to merely count pairwise relationships and frequencies as the lengths of operator combinations are uncertain. Therefore, frequent pattern mining is necessary to identify high-frequency operator combinations, which provide essential data for mapping operator combinations to the function semantic framework and constructing the knowledge base.\nThe Apriori algorithm, a classic algorithm for frequent itemset and association rule mining[55], is"}, {"title": "3.4. Mapping Frequent Operator Combinations to the Function Semantic Framework", "content": "The statistical data of frequent operator combinations and the function semantic framework should be aligned so that the operator combinations can be accurately categorized into semantic functions. This issue involves mapping from statistical pattern recognition to semantic understanding. To achieve this, the LLM is employed for processing and understanding complex natural language texts. Furthermore, a Frequent Pattern Semantic Mapping (FPSM) technique that combines LLMs with voting mechanism is introduced for knowledge base construction, as illustrated in Figure 5.\nThe proposed Frequent Pattern Semantic Mapping (FPSM) technique is an innovative framework divided into four key steps: Operator Sequence Orchestration and Distribution, Semantic Framework Indexing and Embedding Guidance, LLM-Guided Mapping Initialization, Existence Verification and Fuzzy Matching.\nIn the Operator Sequence Orchestration and Distribution, high-frequency operator combinations are distilled from the set of frequent ones, which serve as the fundamental elements"}, {"title": "4. Experimental Results", "content": "Based on the proposed method, an instance of Geo-SORB knowledge base is developed using geospatial scripts in GEE. To evaluate the construction result, IGeo-Fu\u00df metric is also designed to measure its semantic correctness and structural consistency."}, {"title": "4.1. Dataset and Experimental Setup", "content": "The dataset used in this study consists of 154,075 syntactically-checked GEE scripts. These scripts are written in the JavaScript programming language, spanning from September 2015 to September"}, {"title": "4.2. Semantic Framework Construction", "content": ""}, {"title": "4.2.1. Preliminary Quantitative and Qualitative Analysis", "content": "Based on 154,075 GEE geospatial scripts, the function semantic framework was constructed using GPT-4, where 520,335 functional statements are parsed. The quantitative analysis results of the script functional statements are illustrated in Figure 6."}, {"title": "4.2.2. Word Vector Construction and Clustering", "content": "The parameters used for word vector construction and clustering are shown in Table 5:\nIn the t-SNE algorithm, parameter perplexity is used to balance the local and global data structure, with a commonly used value of 30. Parameter n_iter sets the number of iterations for optimizing the low-dimensional embedding, with higher iteration counts (such as 3000) helping to improve convergence. Parameter random_state provides a seed for the random number generator in both t-SNE and GMM algorithms, ensuring the reproducibility of results, which allows for consistent output and easier comparison of results across different runs. Parameter scaler, specifically StandardScaler, is used to standardize the data, ensuring that each feature has equal influence in the analysis. The standardized data has a mean of 0 and a variance of 1, which is crucial for t-SNE and GMM as these algorithms are sensitive to the scale of input features.\nThe TF-IDF values for 520,335 functional statements were calculated and t-SNE is performed for dimensionality reduction to generate low-dimensional vector points. By computing the BIC values for GMM clustering with the number of word vector clusters ranging from 1 to 500, the optimal number of clusters is identified as 42, as shown in Figure 8. Since the \u201celbow point\" occurs at 42, where the BIC value is minimized, the inter-cluster distance is maximized, the intra-cluster distance is small, and the computational redundancy is reduced. Therefore, we chose to cluster the word vectors into 42 clusters.\""}, {"title": "4.3. Operator Combination Results", "content": ""}, {"title": "4.3.1. Operator Frequency Statistics Results", "content": "The GEE platform's operator library includes 1,395 operators. Based on the frequency statistics on the dataset, 3,317 different operator pairs are identified, representing 3,317 unique call patterns, covering 322 operators. We defined 190 calling operators and 306 called operators according to the call sequence. The visualization of operator call relationships is shown in Figure 10, in which nodes represent operators, and edges represent call relationships. Larger nodes indicate operators with more call relationships, and thicker edges indicate higher call frequency."}, {"title": "4.3.2. Frequent Operator Combination Mining", "content": "By applying the Apriori algorithm to the 3,317 unique call patterns and setting the minimum support to 0.05, only itemsets with a support of at least 5% were considered frequent itemsets. This process identified 4,350 frequent itemsets, referred to as frequent operator combinations. The distribution of the number of operators within these combinations is shown in Table 8."}, {"title": "4.4. Mapping Results", "content": "Four LLMs, including GPT-4, Llama 3-8B, ERNIE-4.0-8K, and ERNIE-Speed-128K, are used for mapping frequent operator combinations into the function semantic framework. The experimental setup is as follows: (1) The operator combinations were sorted based on combination length and call frequency before being input into each model. The three-level semantic framework was embedded into the prompt. (2) Five consecutive mappings performed independently with each of the four LLMs, and the results of these five mappings were voted on to determine each LLM's final functional label output. (3) The outputs of each model were further integrated, including pairwise combinations (10 mapping results), three-model combinations (15 mapping results), and all four models combined (20 mapping results), with a comprehensive vote taken to obtain the final integrated result. We generate a total of 15 model combination results through the experiments, which will be further evaluated.\nTo assess the structural consistency and semantic accuracy of the model mapping results, we used expert annotations as the ground truth for verification, labeling 150 common operator combinations. A comprehensive sampling strategy was employed to ensure the representativeness of samples. Initially, random sampling was conducted based on the proportion of operator combination sequence lengths. To examine the models' ability to recognize small differences, some similar combinations, differing by only one operator, were specifically selected. Additionally, given the differences in operator occurrence frequency, some combinations of low-frequency operators are particularly chosen. Each sample was labeled by three experts for its functional category, samples with ambiguous functions were excluded based on the annotation results to ensure the accuracy of the annotations.\nA scientific metrics system for evaluating verification results was designed in this study, which is divided into structural consistency metrics (weighted 40%) and semantic correctness metrics (weighted 60%). The evaluation is based on three-level labels, where a Boolean value Ti is used to represent the correctness of the i-th level label. The semantic correctness metric Isemantic is calculated independently of the structural consistency metric Istructure The semantic correctness metrics Isemantic is calculated as follows:\n*   First, the correctness of the third-level label is verified: if the third-level label is completely correct, the semantic correctness metric scores full points.\n*   If the third-level label is incorrect, regardless of whether the first and second-level labels"}, {"title": "5. Discussions", "content": ""}, {"title": "5.1. Application Scenario of the Geo-FuB Knowledge Base", "content": "The construction method proposed in this paper can be applied to build knowledge bases for"}, {"title": "5.1.1. Chain-of-Thought Retrieval-Augmented Generation", "content": "In the geospatial modeling domain, applying LLMs directly to generate geospatial code often poses challenges because of the absence of highly specialized domain knowledge, the ambiguity of user requirements, and the potential misalignment between the model's comprehension and the task's unique demands. Therefore, it is essential to craft prompt engineering that employs a Chain-of-Thought (CoT), followed by the Retrieval-Augmented Generation (RAG) to access the external Geo-FuB knowledge base, thereby optimizing this knowledge-intensive geospatial code generation. The CoT method simulates the thought process of human teaching by setting a series of highly relevant prompt words, provides a heuristic thinking environment for LLMs, and guides them to progressively approach the correct answer. RAG enhances the model's retrieval"}, {"title": "5.1.2. Fine-Tuning Large Language Models", "content": "By incorporating domain-specific knowledge from the knowledge base through fine-tuning approaches, general LLMs can retain their general language processing capabilities while acquiring the specialized knowledge required for specific tasks. The fine-tuning methods mainly include Full-Parameter Fine-Tuning, Adapter Tuning, Prefix Tuning, and LoRA (Low-Rank Adaptation) Fine-Tuning. Full-Parameter Fine-Tuning involves retraining all the model parameters to ensure optimal performance in specific tasks, and requires substantial computational resources. Adapter Tuning, Prefix Tuning and LoRA Fine-Tuning reduce parameters to be retrained and computational resources required by inserting adapter modules into the model, adjusting the prefix part of the model input or adjusting the model's weight matrices through low-rank approximation, thus increasing the fine-tuning efficiency.\nCompared to RAG technology, the process of fine-tuning a model demands a more substantial investment of computational resources. However, this method yields a higher degree of domain adaptability, which leads to improved accuracy and efficiency when tackling domain-specific tasks. Fine-tuning is particularly suitable for scenarios that call for profound domain expertise. Therefore, by fine-tuning LLMs with the Geo-FuB knowledge base, these LLMs can be specialized to precisely grasp the intricacies of terminology, logic, and operations of the geospatial modeling domain, thereby enhancing their proficiency in geospatial code genenration tasks."}, {"title": "5.2. Limitations", "content": "This study proposes an effective method for constructing the knowledge base that facilitates LLMs in automated geospatial code generation for the first time. By mining the semantic annotations and operator combination statistics from geospatial scripts, this method effectively guides LLMs to achieve better performance in the geospatial domain. However, the current study has several limitations. For example, the study primarily focuses on operator combinations without including parameter passing and dataset information as statistical indicators in the knowledge base. It only considers the existence of operators without addressing the impact of their connection order. To further improve the model's performance and applicability, future research should expand the scope of the knowledge base, thoroughly mining and fully utilizing the information from geospatial scripts. Additionally, the proposed function semantic framework could be further integrated with existing geospatial standard frameworks, such as the OGC (Open Geospatial Consortium) geoprocessing ontology, to enhance its interpretability of the functional division."}, {"title": "6. Conclusion and Future Work", "content": "This paper proposes the Geo-FuB framework for constructing an operator-function knowledge base for LLMs in the geospatial code generation task. The framework includes three core components: Function Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination Statistics (Geo-FuST), and Combination and Semantic Framework Mapping (Geo-FuM), which are suitable for knowledge base construction of various geospatial analysis platforms. Based on 154,075 geospatial scripts in Google Earth Engine (GEE) , a Geo-FuB instance is constructed that includes a semantic framework with 3 primary categories, 8 secondary categories, and 21 tertiary categories, as well as 4,350 sets of frequent operator combinations and their complete knowledge base mapping results. This knowledge base instance is now open source on https://github.com/whuhsy/Geo-FuB and ready-for-use in code generation and other downstream tasks. We proposed a comprehensive evaluation metric that considers both structural integrity and semantic accuracy. The evaluation results show an overall accuracy of 88.89%, with structural accuracy reaching 92.03% and semantic accuracy at 86.79%. Through various application scenarios, this paper highlights how Geo-FuB optimizes the automatic generation of geospatial modeling scripts in two cutting-edge research paradigms: Retrieval-Augmented Generation (RAG) and fine-tuning, which encourages further exploration and expansion of the\nknowledge base in downstream applications.\nFuture research may consider the sequence of operators in their combinations and establish an external knowledge base using graph structures. The potential and feasibility of using graph structures in LLMs for geospatial code generation tasks will be explored. Additionally, we plan to establish related geospatial code datasets and use the knowledge base proposed in this study to fine-tune open-source general LLMs for geospatial code generation."}]}