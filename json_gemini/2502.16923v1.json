{"title": "A Systematic Survey of Automatic Prompt Optimization Techniques", "authors": ["Kiran Ramnath", "Kang Zhou", "Sheng Guan", "Soumya Smruti Mishra", "Xuan Qi", "Zhengyuan Shen", "Shuai Wang", "Sangmin Woo", "Sullam Jeoung", "Yawei Wang", "Haozhu Wang", "Han Ding", "Yuzhe Lu", "Zhichao Xu", "Yun Zhou", "Balasubramaniam Srinivasan", "Qiaojing Yan", "Yueyan Chen", "Haibo Ding", "Panpan Xu", "Lin Lee Cheong"], "abstract": "Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework.", "sections": [{"title": "1 Introduction", "content": "Since McCann et al. (2018) cast multi-task NLP as Question Answering, using prompts as inputs has become the standard way to elicit desired responses from Large Language Models (LLMs). Furthermore, LLMs' few-shot learning (Brown et al., 2020), instruction-following (Ouyang et al., 2022), and zero-shot reasoning capabilities (Kojima et al., 2023) have led to a widespread proliferation of prompting tricks for various tasks and model variants. However, LLMs still exhibit unpredictable sensitivity to various factors (explanation of the task (Li et al., 2023b), ordering (Liu et al., 2024a), stylistic formatting (Sclar et al.), etc.) causing a performance gap between two prompts that are semantically similar, thereby adding impediments for adoption by end users. Against this backdrop, Black-Box Automatic Prompt Optimization (APO) techniques have emerged that improve task performance via automated prompt improvements. The possess various attractive features - (1) they do not require parameter access on LLMs performing the task, (2) they systematically search through the prompt solution space, and (3) they retain human interpretability of prompt improvements. In this survey paper, we aim to highlight the advances in the field. Our core contribution is a 5-part APO taxonomy combined with a comprehensive fine-grained categorization of various design choices therein (see Fig. 1, Tables 2, 3, 4 in Appendix). We hope our framework will be informational for new and seasoned researchers alike, enabling further research on open questions."}, {"title": "2 Automatic Prompt Optimization Formulation", "content": "We formalize the process of automatic prompt optimization (APO) as follows. Given a task model Mtask, initial prompt p \u2208 V, the goal of an APO-system MAPO is to obtain the best performing prompt-template popt under a metric f \u2208 F and eval-set Dval\npopt := arg max Ex~Dval[f(Mtask(p\u2295x))] (1)\n\u03c1\u03b5\u03bd\nThis objective function is not tractable for discrete prompt optimization as token-sequence search spaces are combinatorial. Instead, APO techniques follow the general anatomy as described in Algorithm 1 to obtain approximate solutions."}, {"title": "3 Initialize Seed Prompts", "content": ""}, {"title": "3.1 Manual Instructions", "content": "Several approaches use a seed of manually created instructions that offer interpretable and strong baselines as the basis for further improvement,inter alia., ProteGi (Pryzant et al., 2023), GPS (Xu et al., 2022), SPRIG (Zhang et al., 2024b). While obtaining quality examples can be costly, APE (Zhou et al., 2022) \u00b9 showed that a few hundred samples are sufficient for further optimization."}, {"title": "3.2 Instruction Induction via LLMS", "content": "Honovich et al. (2023) were the first to propose inducing LLMs to infer human-readable prompts based on a few demonstrations E (see Appendix 14.1 for prompt). APE (Zhou et al., 2022) and DAPO (Yang et al., 2024c) use the induced seed instructions for further optimization, while MOP (Wang et al., 2025) and GPO (Li et al., 2023c) use APE to induce cluster-specific prompts. Apart from demonstrations, SCULPT (Kumar et al., 2024) induced instructions from task-READMEs, while UniPrompt (Juneja et al., 2024) used LLMs to fill-"}, {"title": "4 Inference Evaluation and Feedback", "content": "The evaluation step helps identify promising prompt candidates in each iteration. Some methods also use LLM feedback on prompt-response pairs to help generate more prompt candidates."}, {"title": "4.1 Numeric Score Feedback", "content": ""}, {"title": "4.1.1 Accuracy", "content": "Using task-specific accuracy metrics is the most straightforward and widespread way of eliciting feedback, i.a., (Zhou et al., 2022, 2023; Zhang et al., 2024b; Khattab et al., 2022). Classification and MCQ-based QA tasks use exact accuracy, while code-related tasks measure execution accuracy. Text generation tasks (summarization, translation, creative writing) employ flexible metrics like BLEU-N, Rouge-N, Rouge-N-F1, or embedding-based measures such as BERTScore (Zhang* et al., 2020) (Honovich et al., 2023; Dong et al., 2024b)."}, {"title": "4.1.2 Reward-model Scores", "content": "Given the limitations of rigid accuracy metrics, some approaches proposed using learned reward models to provide more nuanced evaluations of prompts-response pairs (Deng et al., 2022; Sun et al., 2024a; Kong et al., 2024). OIRL (Sun et al., 2024a) trained an XGBoost-based reward model that takes query-prompt embedding pairs as input and predicts whether the prompt will elicit correct answers from the language model and use it to select appropriate prompts for specific queries using a best-of-N strategy. DRPO (Amini et al., 2024) follows an LLM-based reward modeling approach using both predefined and dynamic reward criteria. It first optimizes in-context learning examples E, and using that it optimizes the specific task prompt."}, {"title": "4.1.3 Entropy-based Scores", "content": "Entropy-based scores evaluate the entire output distribution induced by candidates, as opposed to a single inference instance. They are gradient-free but require access to the entire output probability distribution, something not usually possible with black-box LLMs. CLAPS (Zhou et al., 2023) leverages the negative incremental cross-entropy of \u03c0(xi\u2295v\u2208V) v/s \u03c0(xi) to identify promising words v \u2208 V to add to the prompt. The topK words are then used as candidate tokens from which to construct candidate prompts. GRIPS (Prasad et al., 2023) simply added an entropy term to"}, {"title": "4.1.4 Negative Log-likelihood of Output", "content": "Some approaches like APE, GPS (Xu et al., 2022), PACE (Dong et al., 2024b) consider the negative log-likelihood (NLL) of token sequences under the target LLM, i.e., \u2013 log(\u03c0\u03c1(y)). This however requires the log-probabilities to be accessible during the decoding of each token, limiting its applicability. The NLL for ground truth one-hot token-sequence is equivalent to the cross-entropy."}, {"title": "4.2 LLM Feedback", "content": "A popular paradigm to augment or fully replace numeric scores is to use textual feedback generated by LLMEvaluator (Wang et al., 2024a; Long et al., 2024; Sinha et al., 2024). It is versatile because it can evaluate both the response as well as the prompt input. It can directly aid the prompt rewriting process while being flexible to individual tasks as it only needs natural language instructions for general-purpose LLMs as opposed to task-specific handcrafting of metrics. A potential downside is the inference cost incurred due to an additional LLM call. All the LLM feedback approaches provide multiple feedback data and broadly fall into two categories - improving a single prompt candidate versus improving multiple prompt candidates (discussed below, examples in Appendix 14.3)."}, {"title": "4.2.1 Improving Single Candidate", "content": "SCULPT (Kumar et al., 2024) introduces a systematic method for tuning long, unstructured prompts by employing a hierarchical tree structure and two-step feedback loops - preliminary assessment and error assessment to evaluate and correct prompts before and after execution. The feedback updates the hierarchical prompt tree which is then back-synthesized into a new prompt candidate. PACE (Dong et al., 2024b) applies an actor-critic editing framework to the prompt refinement process itself, allowing for more dynamic and adaptive adjustments. Overcoming the limitations of optimizing a single metric, CRISPO (He et al., 2025) adopts a multi-aspect critique-suggestion meta-prompt to highlight flaws in the generated response across multiple dimensions such as style, precision, and content alignment. Thereafter it leverages detailed, aspect-specific feedback and iteratively updates the prompts. Autohint (Sun et al., 2023)"}, {"title": "4.2.2 Improving Multiple Candidates", "content": "ProTeGi (Pryzant et al., 2023) and TextGrad (Yuksekgonul et al., 2024) leverage textual \u201cgradients\u201d to guide the discrete prompt optimization procedure, very similar to the gradient-descent style of continuous prompt optimization approaches. Different from continuous gradient-descent, ProTeGi sampled multiple \"gradients\" i.e. directions of improvement, and each such \u201cgradient\u201d is used to generate several prompt candidates for evaluation in the next iteration. PromptAgent (Wang et al., 2024a) similarly used an error collection approach to emulate expert-written prompts that consisted of clear sections like \u201cTask description\u201d, \u201cDomain Knowledge\u201d, \u201cSolution Guidance\u201d, \u201cException Handling\u201d, \u201cOutput Formatting\u201d. PREFER (Zhang et al., 2024a) utilizes a feedback-reflect-refine cycle to aggregate feedback into multiple prompts in an ensemble to improve the model's ability to generalize across various tasks. Survival of the Safest (SOS) (Sinha et al., 2024) added safety-score into a multi-objective prompt optimization framework that used an interleaved strategy to balance performance and security in LLMs simultaneously. To avoid accidentally damaging well-functioning prompts, StraGo (Wu et al., 2024) summarized strategic guidance based on both correct and incorrect predictions as feedback."}, {"title": "4.3 Human-feedback", "content": "A few works also incorporate human feedback, either during compile-time or inference-time in the prompt construction / optimization process. Joko et al. (2024) proposed \"Generative Active Task Elicitation\" to better capture human preferences. It prompts a language model to interactively ask questions and infer human preferences conditioned on the history of free-form interaction. Cheng et al. (2024) trained a smaller LLM to optimize input prompts based on user preference feedback, achieving up to 22% increase in win rates for ChatGPT and 10% for GPT-4. PROMST (Chen et al., 2024) tackles the challenges of multi-step tasks by incorporating human-designed feedback rules and a learned heuristic model. APOHF (Lin et al., 2024) focuses on optimizing prompts using only human preference feedback rather than numeric scores, employing a dueling bandits-inspired strategy to efficiently select prompt pairs for preference feedback, proving effective for tasks like text-to-image generation and response optimization."}, {"title": "5 Candidate Prompt Generation", "content": "In this step, one or more candidate prompts are generated that are most likely to result in an improvement in a metric of interest f \u2208 F. The approaches reviewed below range from simple rule-based edits (sec. 5.1) to sophisticated agentic systems that combine with LLM-based evaluations (sec. 4.2) and various filtering strategies (sec. 6)."}, {"title": "5.1 Heuristic-based Edits", "content": "Several works proposed heuristic-based mechanisms to make edits to intermediate prompt candidates to generate newer candidates. They range from edits at the word / phrase / sentence-level (either simple rule-based or LLM-generated), or metric-driven incremental search. While these strategies may not result in the most optimal solution, they help in making the discrete prompt optimization problem computationally tractable."}, {"title": "5.1.1 Monte Carlo Sampling", "content": "ProTeGi (Pryzant et al., 2023) uses Monte carlo sampling to explore combinatorial discrete solution spaces in an incremental fashion - it samples multiple textual gradients to use to generate prospective candidates, and spawns paraphrases as monte-carlo successors for evaluation. PromptAgent (Wang et al., 2024a) uses a tree-variant called Monte Carlo Tree Search (MCTS) which consists of 4 steps - Selection, Expansion, Simulation, and Backpropagation (also explained in Sec. 6)."}, {"title": "5.1.2 Genetic Algorithm", "content": "A significant line of work applies the well-studied genetic algorithms to make discrete edits to texts. The common recipe for several genetic algorithms is 1/ Mutate and 2/ Cross-over components from promising candidates. Token mutations: SPRIG (Zhang et al., 2024b) and CLAPS perform token-level mutations. SPRIG uses a starting corpus of 300 components grouped into categories like COT, roles, styles, emotions, scenarios, and good properties. It performs add/rephrase/swap/delete, highlighting complementary strengths of optimizing system prompts alongside task-prompts (via methods like ProTeGi) to enhance accuracy across multiple diverse domains, languages, and tasks without needing repeated task-specific optimizations. LLM-based mutation: LMEA (Liu et al., 2023), SOS (Sinha et al., 2024), and StraGo (Wu et al., 2024) uses mutation prompts with LLMs to overcome the traditional complexity of designing tailored operators for cross-over / mutation. PromptBreeder (Fernando et al., 2023) advocates self-referential improvement of all prompts in the prompt optimization system - Direct Mutation of task prompts, Hypermutation of mutation prompts themselves, Lamarckian Mutation where prompts are reverse-engineered from successful examples (similar to Instruction Induction Honovich et al. (2023), and finally Crossover and Shuffling to improve diversity of the prompt pool. EvoPrompt (Guo et al., 2024) use Differential Evolution where differences between existing prompts is incorporated to form new prompt candidates to overcome the problem of local optima. AELP (Hsieh et al., 2024) also uses mutation operators to perform sentence-level edits in an iterative fashion. They include sentence-level histories of reward {(St-1, St,rt)} in the mutation prompt in order to avoid local optima and accidentally returning"}, {"title": "5.1.3 Word / Phrase Level Edits", "content": "Several word-edit approaches first identify \"influential\" tokens in the prompts. COPLE (Zhan et al., 2024) argued that LLMs exhibit lexical sensitivity, showing that merely replacing a few words with their synonyms can yield significant improvements. First, \"influential\u201d tokens are identified where expected loss on dev-set EDval [L(y, \u0177)] drops the most after removing that token versus the original prompt, and then influential tokens are replaced using predictions from a Masked-Language Models. This token-replacement approach is also attractive as a standalone post-processing step for long prompts that are already optimized using other LLM-based approaches. GRIPS (Prasad et al., 2023) argues that phrase level edition is an effective and interpretable method to optimize prompts, leveraging 4 basic edit operations -add, delete, paraphrase, and swap."}, {"title": "5.1.4 Vocabulary Pruning", "content": "Some works prune the vocabulary space V to Vpruned for decoding the next token for the optimized prompt p*. CLAPS (Zhou et al., 2023) argued that general search spaces are highly redundant and use K-means clustering to find word-clusters and retain top-2000 words closest to cluster centroids. BDPL (Diao et al., 2022) used pairwise mutual information (PMI) to retain top co-occuring ngrams for decoding. PIN (Choi et al., 2024) instead added regularization in the form of Tsallis-entropy (ideal for heavy-tailed distributions like natural language) for the RL training of a prompt generation network, to reduce the probability mass for unlikely tokens and improve interpetability."}, {"title": "5.2 Editing via Auxiliary Trained NN", "content": "Some approaches leverage a trained auxiliary neural network to edit the initial prompt for obtaining desired improvements. We include approaches where the finetuned network is different"}, {"title": "5.2.1 Reinforcement-learning", "content": "Multi-objective Optimization techniques (Jafari et al., 2024) demonstrate superiority over simple reward averaging, particularly through volume-based methods that effectively balance competing objectives. Dynamic prompt modification strategies, introduced through prompt rewriting (Kong et al., 2024), directional stimulus prompting (Li et al., 2023d) and test-time editing (Zhang et al., 2022) solve the important goal of moving beyond static prompt generation. Prompt-OIRL (Sun et al., 2024a) also tackled test-time optimization objective by learning an offline reward model and subsequently using a best-of-N strategy to recommend the optimal prompt in a query-dependent fashion. BDPL (Diao et al., 2022) optimized discrete prompts using variance-reduced policy gradient algorithm to estimate gradients, allowing user devices to fine-tune tasks with limited API calls."}, {"title": "5.2.2 Finetuning LLMs", "content": "BPO (Cheng et al., 2024) trains a smaller 7B model to align itself to task-performance on individual LLMs using reward-free alignment. FIPO (Lu et al., 2025) trains a local model (7B - 13B) to perform prompt optimizations to preserve privacy and adapt to target models better leveraging both data diversification and strategic fine-tuning such as SFT, preference optimization, and iterative preference learning."}, {"title": "5.2.3 Generative Adversarial Networks", "content": "Long et al. (2024) framed the prompt optimization process in the GAN setting. The LLM generator takes question and the generation prompt to produce output. The (input, output) pairs are evaluated by an LLM powered discriminator, whose goal is to identify generated pairs from ground truth pairs. Both generator and the discriminator are jointly optimized using adversarial loss, by utilizing a prompt modifier LLM to rewrite their prompts."}, {"title": "5.3 Metaprompt Design", "content": "PE2 (Ye et al., 2024) argued that previous works under-explored meta-prompt search space. OPRO (Yang et al., 2024a) proposes a meta-prompt design (see Appendix 14.2) which includes the optimization problem description in natural language and previously generated solutions (multiple solutions per stage for diversity) and scores alongisde the"}, {"title": "5.4 Coverage-based", "content": "Some approaches seek to \"cover\" the entire problem space - either within a single prompt, or using multiple prompts working individually or in an ensemble during inference."}, {"title": "5.4.1 Single Prompt-expansion", "content": "AMPO (Yang et al., 2024d) uses LLM feedback to enumerate all the failure cases based on the evaluation-set Dval and then enlists each of them in the meta-instruction in an if-then-else format using 3 modules - 1/ Pattern Recognition, 2/ Branch Adjustment, and 3/ Branch Pruning to decide whether to enhance existing branches, or to grow new branches. Similarly, UNIPROMPT focused on explicitly ensuring that various semantic facets of a task get represented in the final prompt. It designs a human-like (manual) prompt engineering approach (UniPrompt) with two stages: a) task facets initialization using background knowledge, and b) refinement using examples."}, {"title": "5.4.2 Mixture of Experts", "content": "Wang et al. (2025) introduced the Mixture-of-Expert-Prompts where each expert is a task-prompt to be used for specialized inference. MOP first clusters all demonstrations using K-means clustering. Then, the Region-based Joint Search (RBJS) (sec.6.3) algorithm generates the appropriate instruction for each exemplar-cluster via instruction induction (sec.3.2) based on a mix of in-cluster and out-of-cluster demonstrations to cover \"blind-spots\". During inference, a single expert prompt is invoked whose cluster centroid \u03bc\u03b5 is closest to the instance-embedding arg min ||$(xi) \u2013 \u03bc\u03b5||2\u00b7"}, {"title": "5.4.3 Ensemble Methods", "content": "PromptBoosting (Hou et al., 2023), Boosted-Prompting (Pitis et al., 2023), PREFER (Zhang et al., 2024a), etc. are ensemble methods that invoke multiple prompts during inference and com-"}, {"title": "5.5 Program Synthesis", "content": "Program-synthesis based approaches transform LLM pipelines into structured, modular components that can be systematically optimized and composed. These optimization techniques iteratively refine instructions and demonstrations for each module to improve the entire pipeline's performance, DSP (Khattab et al., 2022) introduces a three-stage framework for retrieval-augmented inference: Demonstrate (generates task-specific demonstrations), Search (retrieves relevant information), and Predict (combines retrieved info with demonstrations). DSPY (Khattab et al., 2024) transforms LLM pipelines into text transformation graphs - introducing parameterized models, learning through demonstrations, and a compiler that optimizes pipelines. DLN (Sordoni et al., 2023) similarly considers chained LLM calls as stacked deep language networks performing variational inference, where the learnable parameters for each layer are task-decomposed prompt templates. MIPRO (Opsahl-Ong et al., 2024) automates the optimization of multi-stage language model programs by improving instructions and demonstrations for each module. SAMMO (Schnabel and Neville, 2024) proposed symbolic prompt programming, representing prompts as directed-acyclic-graphs (DAG). A set of user-defined node mutation rules guide the mutation-search to find the optimal DAG, which is then converted back to a prompt."}, {"title": "6 Filter and Retain Promising Prompts", "content": "In this step, promising prompt candidates are filtered for further optimization."}, {"title": "6.1 TopK Greedy Search", "content": "The simplest mechanism to iteratively search through prompt candidate sets is a greedy topK search where in each iteration of the optimization, the top-K best-performing candidates on mini-batch of data instances Dval are retained for further iterations (e.g. - ProTeGi, AELP. This differs from beam-search which judges partial solutions' based on the reward for the entire trajectory of prompt edits r({p1, p2, ..., \u03c1\u2021 })."}, {"title": "6.2 Upper Confidence Bound and Variants", "content": "Relying on a single static evaluation dataset can lead to biases in the selection procedure and finally suboptimal solutions. ProTeGi, SPRIG, inter alia, cast the candidate prompt selection problem as that of bandit search - identifying the most suitable arm (prompt candidate) operating on a fixed computation budget. They use the Upper Confidence Bounds (UCB, Algorithm 2) which balances exploration with exploitation. In each iteration of prompt optimization, they sample a different evaluation dataset Dsample \u2208 Dval, and maintain a moving estimate of the optimality of each arm (i.e. prompt). In each iteration, the playout filters top-B prompt candidates with the greatest score for further exploration. PromptAgent uses a variation of UCB called UCB for Trees (UCT) which are used in the setting of contextual bandits (i.e. the action-space and the reward function is state-dependent). AELP (Hsieh et al., 2024) used a modification called Linear UCB (Li et al., 2010) which uses a closed form linear estimate based on the reward trajectories of previously sampled edits as well as prompt embedding (s) to select the next best arm."}, {"title": "6.3 Region-based Joint Search", "content": "MOP (Wang et al., 2025) proposes a Mixture-of-Expert-Prompts performing prompt optimization for each expert individual. Once C exemplar-clusters are identified, the RBJS search first samples examples Dexemplars \u2208 Dc \u222a D \\ Dc, and then uses APE to induct and optimize each expert instruction."}, {"title": "6.4 Metaheuristic Ensemble", "content": "PLUM (Pan et al., 2024) library offered a meta-heuristic ensemble of different search algorithms like Hill climbing, Simulated Annealing, Genetic Algorithms, Tabu Search, and Harmony Search."}, {"title": "7 Iteration Depth", "content": ""}, {"title": "7.1 Fixed Steps", "content": "Most approaches choose to carry out the prompt optimization for a fixed number of steps N."}, {"title": "7.2 Variable number of steps", "content": "GRIPS (Prasad et al., 2023) concludes search when successive iterations with negative gains breach a patience parameter, whereas PromptAgent concluded APO when rt \u2264 Emin \u2200rt \u2265 Em\u0430\u0445."}, {"title": "8 Theoretical Perspectives", "content": ""}, {"title": "8.1 Upper Bound of Improvement from APO", "content": "AlignPro (Trivedi et al., 2025) establishes an upper bound on the gains realizable from discrete prompt optimization under a given prompt optimizer and also a suboptimality-gap w.r.t. RLHF-optimal policy \u03c0*, while a lower bound is left unexplored."}, {"title": "8.2 Other Related Perspectives", "content": "Bhargava et al. (2024) proposed a control theoretic framework to establish bounds on the set of reachable LLM-outputs for self-attention in terms of the singular values of its weight matrices. Liu et al. (2024c) showed the existence of a strong transformer that can approximate any sequence-to-sequence Lipschitz function. They also showed the existence of \u201cdifficult\u201d datasets that depth-limited transformers could not commit to memory."}, {"title": "9 Challenges and Future Directions", "content": ""}, {"title": "9.1 Task-agnostic APO", "content": "All the surveyed APO methods assume that the task type T is known beforehand; additionally offline APO methods also require an evaluation set Dval, something not explicitly available in production settings. Barring a few tasks covered by Joko et al. (2024); Sun et al. (2024a); Zhang et al. (2022); Choi et al. (2024), inference-time optimization of multiple unknown tasks is underexplored. More robust evaluations are needed for task-agnostic APO systems combining seen and unseen tasks."}, {"title": "9.2 Unclear Mechanisms", "content": "Melamed et al. (2024) showed that prompts have so-called 'evil twins' that are uninterpretable yet recover some of the performance of gold-standard prompts. Lu et al. (2024) showed that rare gibberish strings can serve as competitive delimiters T in prompts. Yang et al. (2024b) showed that self-reflection by LLMs can suffer from incorrect error identification, prior biases, semantic invalidity, leading to failure in yielding improved prompts. More studies are needed to better uncover the mechanisms of prompt optimization."}, {"title": "9.3 APO for System Prompts / Agents", "content": "Although SPRIG explored optimizing system prompts in chat-style settings, scalability remains a challenge - optimizing system prompts required a predefined corpus and close to 60 hours whereas Protegi only needed 10 minutes per task. Similarly,"}, {"title": "9.4 Multimodal APO", "content": "Recently, textual prompt optimization has expanded to multimodal domains: text-to-image (Liu et al., 2024b; Ma\u00f1as et al., 2024; Liu et al., 2024d), text-to-video (Ji et al., 2024), text-to-audio (Huang et al., 2023), and text-image alignment models like CLIP (Du et al., 2024; Mirza et al., 2024). Beyond textual prompts, Huang et al. (2023) explore optimizing multimodal inputs, such as images, to elicit better responses from large multimodal models. However, the interplay between modalities in prompt optimization remains underexplored. Future research could develop APO frameworks to jointly optimize multimodal prompts (eg - remove background noise from audio, add visual markers to videos, etc.) to fully leverage their synergies."}, {"title": "10 Conclusion", "content": "In this paper, we provide a comprehensive fine-grained review of existing APO techniques and identified key areas for future growth. It is our aim to spur future research spawning from our survey."}, {"title": "11 Limitations", "content": "While we attempted to cover all qualifying papers, it is possible that we may have unintentionally missed out on some relevant papers. We also mention some of the papers that were excluded in this survey with specific reasons in section 12.2. Also, we realize that fitting varied research works into a single unifying framework might risk broad categorizations for some papers, or skipping some characteristics for others (e.g. Tempera (Zhang et al., 2022) consists of both RL-based and word/phrase-level editing techniques, applied to both instructions and exemplars). In such cases, we categorize a paper based on its most salient features. Another challenge is that when presenting a survey paper under 8 pages, we had to make tradeoffs and only retain content in the main body that was deemed most necessary. This resulted in having to relegate a core contribution (Tables 2,3,4) which contained a rigorous comparison of all the surveyed papers into the appendix. We have attempted our best to strike the right balance between specificity and brevity to present a novel framework. We also provide copious references to interested researchers for further reading."}, {"title": "12 Appendix", "content": ""}, {"title": "12.1 Notation", "content": "We now define the notation of key terms and expressions used throughout the paper.\n1. T = Task type, I= Task instruction, E = (xi, yi)_1 Few shot demonstrations in the prompt, T= Template delimiters, z = CoT recipe for a task-instance, zi E Ii\n2. Mtask target model, MAPO APO system\n3. p = concat([81, 82,..., Sm]) = concat(I, \u03c4, E) Prompt composed of m sentences, which comprise of Instruction, template delimiters and few-shot demonstrations.\n4. D = {(xi, Yi)}m\u2081 collection of m input-output pairs. Dval is the validation set used to validate prompt performance, Dtrain is the training set used to finetune the language model(Reprompting).\n5. {f1, f2, ...} \u2208 F metric function upon which to evaluate task-prompt performance\n6. r:S\u00d7A \u2192 R= reward model score, where S is the state-space and A is the action-space\n7. |V| = length of vocabulary\n8. \u00a2 : S \u2208 V\u2217 \u2192 Rd embedding function which takes in a sentence generated as a finite sequence of tokens belonging to a vocabulary V, and generating a floating point array representation of dimension d\n9. p\u2217 = argmaxp\u2208V*EDvat[fi(p)] The best performing prompt based on the metric score on validation set\n10. k = number of candidates for top-K search, B = Beam width for beam search, N = number of iterations for search\n11. C = number of experts in a Mixture of Experts approach (MOP), \u03bc\u03b5= cluster centroid of cluster C (MOP).\n12. LLMtarget= target model which will be used for inference, LLMrewriter= rewriter model which will be used for rewriter, LLMevaluator= evaluator model which provides the LLM feedback to prompts / responses or both\n13. A with subscripts to denote different latency types: At = Total training cost/latency, including all offline costs for data collection, preprocessing, and model fine-tuning, \u5165\u2081 = per-example inference latency, Am = MLM inference latency per-example"}, {"title": "12.2 Excluded works", "content": "FedBPT (Sun et al., 2024b) used federated learning to update soft prompts and not discrete tokens. Deliberate-then-generate (Li et al., 2023a) randomly sampled arbitrary noisy inference and prompted the task LLM to deliberate on the wrong inference, while Reflexion (Shinn et al., 2023) agents maintain an episodic buffer of past deliberations. Neither method optimizes the input prompt. AutoPrompt (Shin et al., 2020) required gradient access to the task LLM and therefore doesn't remain blackbox."}, {"title": "12.3 UCB based selection algorithm", "content": "Algorithm 2 Select(\u00b7) with UCB Bandits\nRequire: n prompts p1", "m\n1": "Initialize: Nt(pi) \u2190 0 for all i = 1", "n\n2": "Initialize: Qt(pi) \u2190 0 for all i = 1", "n\n3": "for t ="}]}