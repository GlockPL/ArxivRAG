{"title": "FLASHINFER: EFFICIENT AND CUSTOMIZABLE ATTENTION ENGINE FOR LLM INFERENCE SERVING", "authors": ["Zihao Ye", "Lequn Chen", "Ruihang Lai", "Wuwei Lin", "Yineng Zhang", "Stephanie Wang", "Tianqi Chen", "Baris Kasikci", "Vinod Grover", "Arvind Krishnamurthy", "Luis Ceze"], "abstract": "Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.", "sections": [{"title": "1 INTRODUCTION", "content": "The Transformer architecture has become the primary back-bone for large language models (LLMs), prominently featur-ing attention mechanism (Vaswani et al., 2017) as its most salient component. As LLMs rapidly evolve and find appli-cations in diverse fields, the demand for efficient GPU atten-tion kernels grows, with the goal of enabling scalable and responsive model inference. At the heart of LLM inference lies the attention computation, which plays a crucial role in processing historical context and generating outputs based on query vectors. In LLM serving, the attention mechanism reads from the KV cache, which stores historical context, and computes outputs based on the current query. The effi-ciency of this attention operator is paramount to the overall performance of an LLM inference systems. However, cre-ating high-performance attention kernels tailored for LLM serving introduces challenges not typically encountered in traditional training environments.\nTwo major challenges arise when building efficient attention support for LLM systems:\nLLM applications exhibit diverse workload patterns and input dynamics. LLM serving involves various attention computation patterns, from prefill computation for context processing to batched decoding during serving (Yu et al., 2022). As multiple requests are processed, opportunities for prefix-reuse emerge, and the introduction of tree de-coding in speculative scenarios creates additional attention patterns (Cai et al., 2024; Miao et al., 2024; Chen et al., 2024). Moreover, query lengths and KV caches vary within batches and over time, naive implementation might suffer load-imbalance issue, optimal scheduling requiring kernel to adapt dynamically for optimal performance.\nModern hardware implementations necessitate the cus-tomization of attention operators. On the memory side, ef-ficient storage formats, such as paged attention (Kwon et al., 2023) and radix trees (Zheng et al., 2023b), are critical for managing the growing KV cache sizes and diverse storage patterns. On the compute side, crafting hardware-specific pipelines and templates is indispensable to fully exploit the performance potential of each GPU architecture (Dao, 2023; Shah et al., 2024). Furthermore, the design must accommo-date the increasing variety of attention mechanisms in mod-ern LLMs, such as grouped attention heads (Ainslie et al., 2023; Shazeer, 2019), specialized masks (Beltagy et al., 2020), and customized attention score computations (Riv-"}, {"title": "2 BACKGROUND", "content": "2.1 FlashAttention\nFlashAttention (Dao et al., 2022) is an efficient algorithm for computing exact attention with reduced memory usage."}, {"title": "2.2 Attention Composition", "content": "Block-Parallel Transformer (BPT) (Liu & Abbeel, 2023) demonstrates that attention outputs for the same query and different keys/values can be composed by preserving both the attention outputs and their scales. Let q be a query, and let I be an index set. We define the attention scale over I via the log-sum-exp operation on the attention scores:\n$LSE(I) = log \\sum_{i \\in I} exp(q.k_i)$                                              (1)\nwhere $k_i$ is the i-th key vector. The corresponding attention output O(I) is then\n$O(I) = \\sum_{i \\in I} \\frac{exp(q.k_i)}{exp(LSE(I))} v_i$                                              (2)\nWe define the Attention State for I as the tuple of attention output and attention scale: $[\\frac{O(I)}{LSE(I)}]$. Crucially, the Attention State of $I \\cup J$ can be derived by composing the states of I and J. Specifically, introducing an operator \u2295:\n$\\left[ {\\begin{array}{c}  O(I\\cup J) \\\\ LSE(I\\cup J) \\end{array}} \\right] =  \\left[ {\\begin{array}{c} O(I) \\\\ LSE(I) \\end{array}} \\right] \\oplus \\left[ {\\begin{array}{c} O(J) \\\\ LSE(J) \\end{array}} \\right] = \\left[ {\\begin{array}{c} \\frac{exp(LSE(I))O(I)+exp(LSE(J))O(J)}{exp(LSE(I))+exp(LSE(J))} \\\\ log(exp(LSE(I)) + exp(LSE(J))) \\end{array}} \\right]$\nSince \u2295 is associative and commutative, multiple sets of attention states can be composed in any order. Ring-Attention (Liu et al., 2023) and Flash-Decoding (Dao et al., 2023) utilize this property to offload partial-attention com-putations, thereby reducing memory usage and improving hardware efficiency. In FlashInfer, the Attention State is adopted as the canonical output of an attention operation, and \u2295 serves as the standard reduction operator (analogous to summation in GEMM) on these states."}, {"title": "2.3 Block/Vector Sparsity", "content": "Block Compressed Sparse Row (BSR) is a hardware-efficient sparse format that groups non-zero elements into contiguous matrices of size $(b_r, b_c)$, as opposed to the ran-dom scattering found in unstructured sparsity. This format offers several advantages over the standard Compressed Sparse Row (CSR) format. BSR improves register reuse efficiency (Im et al., 2004; Bulu\u00e7 et al., 2009) and demon-strates better compatibility with hardware matrix multipli-cation units on GPUs and NPUs (Narang et al., 2017; Gray et al., 2017). In addition, it provides the ability to skip empty blocks, reducing computational overhead. BSR's efficiency is particularly evident when subcomputations are aligned with hardware matrix multiplication instructions, such as NVIDIA's mma instructions. Traditionally, tensor core instructions operate on minimal dimensions of 16 (or larger for newer GPUs), leading most block-sparse kernels to use block sizes that are multiples of (16, 16). However, this approach is not always optimal for applications with fine-grained sparsity patterns (Wang et al., 2023). Many attention libraries restrict their block sizes to multiples of (128, 128) for block-sparse attention kernels.\nRecent research (Chen et al., 2021; Li et al., 2022) has demonstrated that efficient utilization of the tensor core can be achieved with smaller block sizes, such as (16, 1) for matrix B in GEMM, or (1, 16) for matrix A (also known as vector-sparse). This is accomplished by first gathering rows/-columns into contiguous shared memory and then applying dense tensor cores to these contiguous shared-memory data. This approach is particularly beneficial for applications with fine-grained sparsity patterns. FlashInfer builds upon these techniques to support blocks with arbitrary column sizes $B_c$, offering greater flexibility and efficiency in handling diverse sparsity patterns."}, {"title": "3 DESIGN", "content": "In this section, we introduce the system design of FlashInfer. We begin by presenting the data structure employed in Flash-Infer and demonstrate how Block-Sparse Row (BSR) acts as a versatile abstraction for KV cache storage in attention ker-nels. Next, we discuss the FlashInfer compiler, which sup-ports various attention variants, alongside a dynamic-aware"}, {"title": "3.1 KV-Cache Storage", "content": "3.1.1 Block-Sparse Matrix as Unified Format\nRecent advancements in KV-Cache storage, such as PageAt-tention (Kwon et al., 2023) and RadixAttention (Zheng et al., 2023b), employ non-contiguous memory storage with a minimum granularity of a block (or token) of $(H, D)$ ten-sors, where H represents the number of heads and D the hidden dimension. These structures are optimized to mini-mize memory fragmentation while enhancing memory reuse and cache hit rates. We demonstrate that these diverse data structures can be unified under a block sparse format, as illustrated in Figure 2.\nIn FlashInfer, we implement a unified strategy for data rep-resentation. Query and output matrices are efficiently stored as ragged tensors (also known as jagged arrays) (Tensorflow Developers, 2018) without padding, which facilitates the compact packing of queries and outputs from diverse requests into a single tensor. Initially, keys and values are maintained in ragged tensors using the same index pointers as queries, as they originate from the projection matrices $W_q$, $W_k$, $W_v$ applied to the same input. These keys and val-ues are subsequently incorporated into the KV-Cache with newly updated entries. The KV-Cache employs a block-sparse row (BSR) format, where block sizes are defined by application requirements: $B_r$ corresponds to the query tile size, details of which will be discussed in later sec-tions, and $B_c$ is specified by KV-Cache management algo-rithms. FlashInfer kernel implementations supports arbitrary $(B_r, B_c)$ values."}, {"title": "3.1.2 Composable Formats for Memory Efficiency", "content": "Inspired by SparseTIR (Ye et al., 2023), we enhance atten-tion computation efficiency through composable formats. This approach leverages multiple block sparse formats in-stead of a single format to store the sparse matrix, offering greater flexibility and memory efficiency. Single block-sparse formats are constrained by a fixed block size, limiting memory efficiency based on the number of rows in the block $(B_r)$. While larger $B_r$ values improve shared memory and register reuse for requests within the same block, they also"}, {"title": "3.2 Compute Abstraction", "content": "We developed CUDA/CUTLASS (Thakkar et al., 2023) templates for FlashAttention, designed specifically for both dense and block-sparse matrices and compatible with NVIDIA GPU architectures from Turing to Hopper (sm75 to sm90a). Our implementations utilize the FlashAttention2 (FA2 for short) algorithm (Dao, 2023) for architectures up to Ada(sm89), and the FlashAttention3 (FA3 for short) al-gorithm (Shah et al., 2024) for Hopper. Key improvements include enhanced loading of sparse tiles into shared memory, expanded tile-size configurations, optimized memory access patterns for grouped query attention, and customizable at-tention variants.\n3.2.1 Global to Shared Memory Data Movement\nThe FlashInfer attention template supports any block size, requiring a specialized data loading approach since blocks may not align with tensor core shapes. As discussed in Section 2.3, we address these challenges by transferring tiles from scattered global memory to contiguous shared memory for dense tensor core operations. Tensor core inputs for a single MMA instruction can originate from different blocks within a block-sparse matrix. Figure 4 illustrates how FlashInfer loads tiles from sparse/dense KV-Cache into shared memory; sparse KV-Cache addresses are computed using the indices arrays of the BSR matrix, while dense ones use row index affine transformations.\nThe last dimension of the KV-Cache remains contiguous (with size of head dimension d, commonly 128 or 256), maintaining coalesced memory access that fits GPU cache line sizes. We use asynchronous copy instructions LDGSTS with a 128B width to maximize memory bandwidth. Al-though the Tensor Memory Accelerator (TMA) in Hopper architecture can further accelerate data movement, it doesn't support non-affine memory access patterns. Consequently,"}, {"title": "3.2.2 Microkernel with Different Tile Sizes", "content": "To adapt to the varying operational intensities of LLM ap-plications, FlashInfer implements the FA2 algorithm across multiple sizes. Traditional FA2 uses limited number of tile sizes (e.g., (128, 64)), optimal for prefill on A100 but ineffi-cient for shorter-query-length decoding. One architecture's ideal tile size may not suit others; for instance, Ada(sm89) has limited shared memory, affecting SM occupancy with large tiles.\nFlashInfer offers FA2 kernels with tile sizes $(1, 16, 32, 64, 128) \u00d7 (32, 64, 128)$ and selects using heuristics based on hardware resources and workload intensity:\n1. Determine average query length (for Grouped-Query Attention, the query length are fused with head group dimension, see Appendix A) per batch, choosing the minimal query tile size meeting or exceeding it.\n2. Formulate register and shared memory constraints as functions of K/V tile size, maximizing SM resource occupancy."}, {"title": "3.2.3 JIT Compiler for Attention Variants", "content": "For query tile size 1, we use CUDA Cores template since tensor core instruction m (minimum rows) is 16, and use Tensor Cores for other query tile sizes. For FA3 we pro-vides row tile sizes that are multiples of 64, aligning with Hopper's WGMMA requirements. Tile sizes resolve at"}, {"title": "3.3 Dynamism-Aware Runtime", "content": "In this section we introduce the runtime design of FlashIn-fer, including the dynamic scheduling framework, and the composable formats for memory efficient attention.\n3.3.1 Load-balanced Scheduling\nIn FlashInfer, the load-balanced scheduling algorithm aims to minimize SM idle time by distributing the workload"}, {"title": "3.4 Programming Interface", "content": "FlashInfer offers a programming interface designed for seamless integration with existing LLM serving frameworks such as vLLM(Kwon et al., 2023), MLC-Engine(MLC Community, 2024), and SGLang (Zheng et al., 2023b)."}, {"title": "4 EVALUATION", "content": "In this section, we evaluate FlashInfer v0.2 on kernel-level and end-to-end performance showing how FlashInfer's design address the challenges of LLM serving. We achieve 29-69% inter-token-latency reduction compared to Triton backend for LLM serving benchmark, 28-30% latency re-duction for long-context inference, and 13-17% speedup for LLM serving with parallel generation. We conduct ex-periments on NVIDIA A100 40GB SXM and H100 80GB SXM GPUs, using CUDA 12.4 and PyTorch 2.4.0 and f16 precision for storage and computation.\n4.1 End-to-end LLM serving performance\nWe evaluate FlashInfer with SGLang v0.3.4 (Zheng et al., 2023b) and compare its performance against two settings:"}, {"title": "4.2 Kernel Performance for Input Dynamism", "content": "In this section we measure FlashInfer's generated kernel performance against state-of-the-art open-source FlashAt-tention library under different sequence length distributions, we use the latest main branch which includes both FlashAt-tention2 and FlashAttention3 kernels. We fix the batch size to 16 and select three different sequence length distributions:"}, {"title": "4.3 Customizability for Long-Context Inference", "content": "In this section, we demonstrate how FlashInfer's customized attention kernels significantly accelerate LLM inference. We focus on Streaming-LLM (Xiao et al., 2023), a recent algorithm capable of million-token inference with constant GPU memory usage. While Streaming-LLM requires spe-cialized attention kernels for optimal performance, particu-larly a fused kernel combining RoPE (Su et al., 2024) with attention, FlashInfer can generate such fused kernels with merely 20 additional lines of code for query/key transforma-tions. We compare the performance of FlashInfer-generated fused kernels against un-fused kernels (both FlashInfer's and FlashAttention's) and quantify the end-to-end latency reduction achieved by integrating FlashInfer kernels into StreamingLLM.\nFor end-to-end performance, we run Vicuna-13B (Chi-ang et al., 2023) inference on MT-Bench (Zheng et al., 2023a) dataset and measure the inter-token-latency (ITL) of Streaming-LLM with and without FlashInfer kernels. Figure 9 show the ITL of Streaming-LLM with and without FlashInfer fused kernels on our optimized implementation of Streaming-LLM (we noticed that the original implementa-tion is sub-optimal and have unnecessary overheads). Flash-Infer's fused kernel can yield 28 \u2013 30% latency reduction under different settings (by changing the recent window size"}, {"title": "4.4 Parallel-Generation Performance", "content": "In this section, we illustrate how the composable formats of FlashInfer can enhance parallel decoding. With parallel generation emerging as a significant task in LLM serving, it offers great utility in LLM agents. The OpenAI API provides an \"n\" parameter to facilitate the generation of multiple tokens simultaneously. As shared prefixes often exist, prefix-caching can significantly boost the efficiency of parallel generation. The composable formats found in FlashInfer (see Section 3.1.2) allow for the decoupling of attention computation between the shared prefix and the sub-sequent suffix, which can be leveraged to expedite parallel decoding.\nWe implemented composable formats within MLC-Engine(MLC Community, 2024) under a prefix-caching configuration and assessed the performance during parallel generation. Evaluations were conducted on the Llama 3.1 models with 8B and 70B parameters(Dubey et al., 2024) using the ShareGPT dataset. With a fixed request rate of 16, we varied the number of parallel tokens over the set 1, 2, 4, 8, 16, 32, 64, comparing these results against MLC-Engine configurations where composable formats were dis-abled. Figure 10 presents the ITL (Inference Time Latency) and TTFT (Time To First Token) results for MLC-Engine both with and without composable formats."}, {"title": "5 RELATED WORK", "content": "5.1 Attention Optimizations\nMulti-Head Attention (MHA) (Vaswani et al., 2017) faces computational and IO challenges. FasterTrans-former (NVIDIA, 2021) reduces global memory footprint via Fused Multi-Head Attention (FMHA), but doesn't scale to long contexts because shared memory usage is linear to sequence length. ByteTransformer (Zhai et al., 2023) opti-mizes FMHA on variable-length input. FlashAttention (Dao et al., 2022) uses online-softmax (Milakov & Gimelshein, 2018) trick to reduce the shared memory footprint to con-stant size, enabling long contexts. FlashAttention2&3 (Dao, 2023; Shah et al., 2024) further optimizes FlashAttention by improving loop structure and overlapping softmax and GEMM. FlashDecoding (Dao et al., 2023) applies Split-K to decode attention kernels. LeanAttention (Sanovar et al., 2024) uses StreamK (Osama et al., 2023) to reduce wave-quantization (NVIDIA, 2023b) in attetnion (with fixed se-"}, {"title": "5.2 Sparse Optimizations on GPUs", "content": "FusedMM (Rahman et al., 2021) explores Sparse-dense Ma-trix Multiplication (SpMM) fusion, though it omits softmax computation, limiting direct applicability for accelerating attention. Zhang et al. (2022) explore Graph Attention Net-works (GAT) kernel fusion, SAR (Mostafa, 2022) serial-izes Sparse Attention aggregation, akin to FlashAttention, neither work explores using Tensor Cores. Blocksparse library (Gray et al., 2017) implements BSR GEMM with tensor cores. Chen et al. (2021), TC-GNN (Wang et al., 2023) and Magicube (Li et al., 2022) propose vector sparse formats to leverage Tensor Cores effectively. FlashInfer improves upon these to support any block sizes $(b_r, b_c)$ in FlashAttention."}, {"title": "5.3 Attention Compilers", "content": "FlexAttention (He et al., 2024) provides a user-friendly interface for programming attention variants, compiling them into block-sparse flashattention implemented in Tri-ton (Tillet et al., 2019). It uses PyTorch Compiler (Ansel et al., 2024) to automatically generate backward passes. FlashInfer expands the FlexAttention's programming inter-face to support query/key transformations, and focus on vector-sparsity and load-balancing for LLM serving. Flash-Infer generates CUDA code instead of Triton because Triton still underperform CUDA & CUTLASS in many use cases. FlashInfer can act as a backend for Flex Attention in forward"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this paper, we present FlashInfer, an versatile and ef-ficient attention engine for LLM serving. We propose a unified block-sparse storage and composable formats for memory efficiency, JIT compilation for customization and load-balanced scheduler for input dynamism. We evaluate FlashInfer's performance across diverse inference scenarios, showing strong performance in kernel-level and end-to-end LLM serving metrics. In the future, we plan to explore compiling higher-level DSLs (Wu et al., 2024; He et al., 2024) to attention specifications in FlashInfer, as well as code generation to other backends (Ozen, 2024; Spector et al., 2024; Tillet et al., 2019). The FlashInfer project is open-source and available at https://github.com/flashinfer-ai/flashinfer."}, {"title": "A HEAD GROUP FUSION FOR GROUPED-QUERY ATTENTION", "content": "Grouped-Query Attention (GQA) (Ainslie et al., 2023) al-lows multiple query heads to share the same key-value (KV) heads. A straightforward implementation that assigns dis-tinct GPU threadblocks to each query head leaves much of the potential KV-Cache reuse underutilized when the query length is short. To address this limitation, FlashInfer offers a head-group fusion strategy: different KV heads are mapped to individual threadblocks, while query heads are fused with the query length dimension. This fusion scheme is illus-trated in Figure 11, which shows how the fused row index relates to the original row index and the head indices. By merging the query-head dimension with the row dimension in the threadblock mapping, a single shared-memory load of the KV-Cache suffices for all query heads in the group, leading to better memory reuse and improved throughput for GQA operations.\nWe prefer head-group fusion primarily for short query lengths. When the query length is sufficiently large, the query dimension itself yields enough workload to effec-tively utilize the KV-Cache, making head-group fusion less critical. Similar ideas have also been explored in other frameworks, such as XQA (NVIDIA, 2024b) in TensorRT-LLM (NVIDIA, 2023a)."}, {"title": "B OVERHEAD OF SPARSE GATHERING", "content": "In Section 3.2.1, we detailed the design of FlashInfer's sparse loading module, which transfers sparse rows from global memory into contiguous shared memory. Here, we measure the performance overhead associated with sparse gathering in FlashInfer for both decode and prefill kernels.\nFigure 12 compares achieved throughput in both prefill and"}, {"title": "C THE CHOICE OF BACKEND", "content": "For NVIDIA GPUs, we build FlashInfer on top of CUDA/-CUTLASS (Thakkar et al., 2023) instead of Triton (Tillet et al., 2019) for the following reasons:\n1. Advanced NVIDIA GPU Features. CUTLASS supports specialized GPU capabilities such as warp-specialization (NVIDIA, 2024a) and TMA instruc-tions (NVIDIA, 2022), which are experimental or un-supported in Triton at this moment.\n2. Fine-Grained Kernel Optimization. While Triton provides tile-level abstractions, CUDA/CUTLASS af-fords finer control over thread-level registers. This flex-ibility simplifies incorporating low-level optimizations (e.g., PTX intrinsics) directly into our JIT templates, which is more challenging in Triton.\nOur load-balancing scheduler design (Section 3.3.1) is largely backend-agnostic, allowing us to potentially inte-grate Triton in future versions of FlashInfer and to adapt our approach to other hardware platforms."}, {"title": "D MEMORY MANAGEMENT", "content": "FlashInfer manages a page-locked (pinned) host buffer and a device workspace buffer to store scheduler metadata and split-k partial outputs. We divide the device workspace buffer into sections, each corresponding to an array of either scheduler metadata or partial split-k outputs. For each plan call in the scheduler, we compute the scheduler metadata on the pinned host buffer and then issue a cudaMemcpyAsync to transfer this data into the corresponding sections of the device workspace buffer.\nD.1 CUDAGraph-Compatible Workspace Layout\nOnce a kernel is captured by CUDA Graph, its arguments (pointers and scalars) become fixed, implying that each section of the device workspace buffer must maintain a consistent address for the entire captured graph's lifetime. Therefore, we allocate the workspace buffer to its maximum required capacity for each section, based on upper-bound estimations of scheduler metadata and partial outputs."}, {"title": "D.2 Split-K Writethrough Optimizations", "content": "In FlashInfer's load-balancing scheduler (Section 3.3.1), KV-splitting is only applied to requests that have large KV lengths. Requests with short KV lengths do not require split-ting and hence have no reduction step from partial output. To save both computation and workspace memory, these small requests can write their partial outputs directly to the final output buffer (bypassing the device workspace buffer). This approach reduces both the required workspace size and the computational load within the contraction kernel.\nCUTLASS (Thakkar et al., 2023) implements deterministic turnstile accumulation (for GEMM), which eliminates the need for a workspace buffer for partial outputs by using semaphores to ensure a deterministic reduction order, the same idea also applies to attention composition. We leave this implementation as future work."}, {"title": "D.3 Workspace Buffer Size Estimation", "content": "The workspace buffer size depends on two main factors: (1) the required space for scheduler metadata, and (2) the required space for storing partial split-k outputs.\nScheduler Metadata. The maximum size of each meta-data section is derived from the largest possible number of concurrent requests and the maximum accumulated request length. Users must provide these upper bounds during the scheduler's first planning stage.\nPartial Outputs. The size of partial outputs depends on both the problem dimensions (i.e., the number of heads and the head dimension) and the number of CTAs per kernel launch. In our load-balancing algorithm 3.3.1, only requests deemed \"long\" \u2013 those whose KV length exceeds the total KV length divided by the number of CTAs \u2013 are split. Ac-cording to the Writethrough Optimizations in Section D.2, only these split requests produce outputs in the workspace buffer. Because the number of splits cannot exceed the total number of CTAs, and each split yields at most two tiles that must be merged, there are at most 2\u00d7 #CTA partial outputs. Each tile produces a partial output of size $T_q \u00b7 H_{qo}\u00b7 (D+1)$, where $T_q$ is the query tile size, $H_{qo}$ is the number of heads, and D+1 is the head dimension and LSE dimension. Therefore, the upper bound for the total partial output size is:\n$2 #CTA \u00d7 T_q \u00d7 H_{qo} \u00d7 (D + 1)$.\nBy default, the total number of CTAs is set to k \u00d7 #SM, where #SM denotes the number of streaming multiproces-sors on the GPU and k is chosen to maximize CTA-level occupancy. For tensor-core based microkernels with high register usage, k typically does not exceed 2 on Ampere, and it is often 1 on Hopper (one CTA per SM, also referred to as a persistent kernel)."}, {"title": "E OVERLAP OF ATTENTION WITH OTHER OPERATIONS", "content": "Nanoflow (Zhu et al., 2024a) overlaps GEMM, attention, and inter-device communication in separate CUDA streams, assigning a fixed number of SMs to each operation. In Flash-Infer, this SM number can be provided by the user through the plan functions, and the FlashInfer load-balancing sched-uler will allocate tiles accordingly."}, {"title": "F FFP8-FP16 MIXED-PRECISION ATTENTION", "content": "Recent LLMs frequently adopt fp8 KV-Cache to reduce memory bandwidth and storage costs (Micikevicius et al., 2022). In FlashInfer, we implement mixed-precision atten-tion kernels wherein the query and output remain in fp16, while the KV-Cache is stored in fp8. We leverage the fast numerical array converter and fragment shuffler proposed by Gupta (2024) to accelerate dequantization and handle bitwidth mismatches efficiently. This design allows for re-duced memory footprints and higher bandwidth utilization without significantly compromising numerical accuracy."}]}