{"title": "Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement Learning", "authors": ["Dharmendra Prajapat", "Durga Toshniwal"], "abstract": "Task-oriented dialogue (TOD) system is designed to accomplish user-defined tasks through dialogues. The TOD system has progressed towards end-to-end modeling by leveraging pre-trained large language models. Fine-tuning the pre-trained language models using only supervised learning leads to the exposure bias and token loss problem and it deviates the models from completing the user's task. To address these issues, we propose a TOD system that leverages a unified pre-trained language model, GPT2, as a base model. It is optimized using supervised learning and reinforcement learning (RL). The issues in the TOD system are mitigated using a non-differentiable reward function. The reward is calculated using the weighted sum of the success rate and BLEU evaluation metrics. The success rate and BLEU metrics in reward calculation guide the language model for user task completion while ensuring a coherent and fluent response. Our model is acquired by fine-tuning a pre-trained model on the dialogue-session level which comprises user utterance, belief state, system act, and system response. Experimental results on MultiWOZ2.1 demonstrate that our model increases the inform rate by 1.60% and the success rate by 3.17% compared to the baseline.", "sections": [{"title": "I. INTRODUCTION", "content": "A task-oriented dialogue (TOD) system aims to achieve a particular task by involving in a dialogue with a user. The TOD systems have applications in multiple fields such as customer services, plane ticket booking, hotel booking, etc [1]. The TOD systems can function as a virtual assistant by completing specific tasks without human intervention [2].\nTraditionally, a task-oriented dialogue system is acquired by a pipeline approach [3] which involves four separate modules. The natural language understanding (NLU) [4] module is used to identify the user intent and pre-defined slot (Ex. internet, parking, price range, etc) values. The dialogue state tracking (DST) [5] module tracks the belief state of users which is represented by pre-defined slots and keeps them updated over each dialogue turn. The system action (SA) module decides the appropriate action to be taken by the system and the natural language generation (NLG) [6] module converts the system actions into coherent and fluent system responses. All these modules are trained and evaluated in a pipeline approach using separate objectives and evolution metrics. This process is cumbersome to train and evaluate the modules separately and it also propagates the error from one module to the subsequent module."}, {"title": "II. RELATED WORK", "content": "Traditionally, task-oriented dialogue systems are achieved using pipeline-based methods. This method contains four modules: Natural language understanding (NLU) [4], belief state tracker (DST) [5], dialogue policy learning (DPL), and natural language generation (NLG) [6] module. NLU module helps to understand the user intent and also extracts key information useful for belief state tracker. The DST module monitors the constraints set by the user and continuously updates them with each dialogue turn. The DPL module decides the system action based on the belief state and the NLG module converts the system action into fluent and coherent natural language. All modules are trained sequentially which makes it harder to optimize it on a common objective [18]. The drawback of this approach is that errors in one module propagate and can amplify in subsequent modules."}, {"title": "B. End-to-end task-oriented dialogue system", "content": "To overcome the limitations of pipeline-based approaches, end-to-end trainable task-oriented dialogue systems have been developed. These systems are optimized with a unified objective making them easier to train. It allows them to map input directly to the output. A seq2seq model is used by Sequicity [7] to generate belief spans and responses. DAMD [9] uses separate decoders to generate belief spans and act spans and multiple decoders to generate system responses under the same dialogue context. With the progress in large language models, they are leveraged in an end-to-end trainable TOD system [12]-[15]. Such models are capable of generating human-level responses because of their exposure to large amounts of data during the pre-training stage. A pre-trained large language model can only be effectively adapted to a TOD system if a substantial amount of data is available for fine-tuning. The pre-trained models have been fine-tuned using different approaches to achieve the TOD system. SimpleTOD [12] uses pre-trained GPT2 as a base model and fine-tunes it using dialogue turn-level whereas UBAR [14] also uses the GPT-2 but fine-tunes it using dialogue session-level. At the dialogue turn level, a user utterance and the corresponding response are used as an input sequence whereas at the dialogue session level, the input sequence includes all intermediate information generated during the session, such as belief states and system acts across all dialogue turns. We adopt the approach used in UBAR to fine-tune the pre-trained language model, GPT-2, for our system.\nSome methods [13], [19], [20] train the models from scratch using external dialogue corpora instead of using a pre-trained model however training such models for the TOD system requires the annotated data which limit the data that can be used. GALAXY [19] uses UniLM, a transformer-based architecture that is pre-trained and fine-tuned on conversational"}, {"title": "III. METHODOLOGY", "content": "The task-oriented dialogue system is required to generate fluent and coherent natural language responses for a given user utterance. Our TOD system utilizes the pre-trained language model GPT-2 to generate the desired response. For this, our system is trained on dialogue session level which is represented as follows: [Uo, Bo, DB0, A0, R0, U1......Ut, Bt, DBt, At, Rt]. A dialogue session comprises multiple components like user utterance, belief state, database search result, system act, and system response for each dialogue turn. Each component of the dialogue session is surrounded by special tokens as shown in Figure 2. Figure 3 provides a comprehensive overview of the methodology."}, {"title": "A. Supervised fine-tuning", "content": "Our system uses a large pre-trained unidirectional language model GPT-2 as a base model. It generates the tokens in an auto-regressive manner. The model is pre-trained on a large corpus and can generate coherent and fluent natural language. We fine-tune this model on the dialogue session level on the standard left-to-right language modeling (LM) objective. The language modeling objective maximizes the probability of the next token from the previously generated token sequence as a context. The cross-entropy loss for the language modeling objective is expressed as follows:\n$L_{LM} = -\\sum_{i=0}^{N}logP(w_i|w_0, w_1, \\ldots w_{i-1}),$\nwhere, $L_{LM}$ is an objective function, $w_i$ represents the next token, and $w_0, w_1, \\ldots w_{i-1}$ are the previous tokens from which the probability of next token is predicted. N represents the number of tokens in the context.\nAn end-to-end task-oriented dialogue system is achieved by accomplishing sub-tasks such as belief state tracking, system act prediction, and system response generation. Each of these sub-tasks is completed through next-token prediction.\nThe belief states tracking sub-task records and updates the values of pre-defined slots (Ex. internet, parking, price range, etc) throughout the conversation. It ensures that the dialogue system accurately reflects the current state of the conversation and user requirements. The belief state of dialogue is generated by conditioning on the dialogue context $C_t = [U_o, B_o, DB_o, A_o, R_o, U_1 . . ., U_t]$, which is mathematically represented as follows:\n$Belief\\ State(B_t) = P(B_t|C_t).$\nThe belief state is generated in the following pattern: [domain1] slot11 value12 slot12 value12 [domain2] slot21 value21. In the generated sequence, a domain of the dialogue turn is followed by the slot and its value. This generated belief state is used to query the database and the result $DB_t$ is appended to the input sequence. Now the dialogue context becomes a sequence of $C_t, B_t$, and $DB_t$. Conditioned on this updated dialogue context, the TOD system generates the system action. which is mathematically represented as follows:\n$System\\ Action(A_t) = P(A_t|C_t, B_t, DB_t).$\nThe dialogue system generates the system action in the following pattern: [domain\u2081] [actionType1] slot11 slot12 ...[actionType2] slot21 slot 22 It can be seen from the generated sequence that multiple action and slot names follow the domain. This pattern is consistently produced whenever the system processes a dialogue turn. Based on all prior information $C_t, B_t, DB_t, A_t$, as a single sequence, the dialogue system generates the delexicalized response, which is mathematically represented as follows:\n$System\\ Response(R_t) = P(R_t|C_t, B_t, DB_t, A_t).$"}, {"title": "B. Reinforcement Learning", "content": "Our goal is to steer the pre-trained language model to generate a sequence that results in high rewards. Even though the reward is scalar and non-differentiable, the reinforcement learning method updates the parameters of the pre-trained model to generate the high-rewarding token sequence. We use the success rate and BLEU score to calculate the reward to guide the pre-trained language model to complete the task in an end-to-end manner.\n$Reward(y, \\hat{y}) = \\alpha \\times Success(y, \\hat{y}) + (1 - \\alpha) \\times BLEU(y_u, \\hat{y}_u) + 1.$\nThe loss function for the reinforcement learning method is calculated as the negative log probability of the generated token sequence with the highest probability multiplied by the reward. This can be expressed as follows:\n$L_{RL} = - log P(\\hat{y}) \\times Reward(y, \\hat{y}).$\nThe REINFORCE loss is introduced to steer the pre-trained language model to generate the desired token sequence. Thus, the total objective of the model becomes a linear combination of $L_{LM}$ and $L_{RL}$ as follows:\n$L_{Total} = L_{LM} + \\beta L_{RL}.$\nWe hyper-tune the weight, $\\beta$, assigned to the loss function of the reinforcement learning. The overall process of our approach is detailed in Algorithm 1."}, {"title": "C. Decoding strategy", "content": "Many decoding strategies are used to sample a token from the output distribution. Some are greedy decoding, beam-search decoding, top-p sampling, and contrastive search decoding methods. We use a simple greedy decoding strategy to reduce the calculation overhead at the decoding part. In the greedy decoding strategy, the token with the highest probability from the vocabulary (V) is selected as the next token for generation.\n$\\hat{y}_t = arg\\ max_{y_t \\in V} P(y_t | y_1, y_2,..., y_{t-1}),$\nwhere:\n$\\bullet\\ \\hat{y}_t$ is the generated token at step t\n$\\bullet\\ P(y_t | y_1, y_2,..., y_{t-1})$ is the conditional probability of the token $y_t$ over the vocabulary size (V)."}, {"title": "IV. EXPERIMENTAL SETTINGS", "content": "We compare our method, with several strong baselines:\nLABES-S2S [24]: The method proposes a probabilistic dialogue approach where belief states are represented as discrete latent variables and jointly modeled with system responses based on user inputs.\nSimpleTOD [12]: This approach uses a pre-trained GPT2 language model for TOD system in end-to-end setting. They fine-tune this model on the dialogue turn level, using one dialogue turn at a time within the dialogue context. The dialogue context contains the oracle belief state instead of the generated one.\nDOTS [25]: This approach works to reduce the dialogue context length which can go longer with each dialogue turn. The model tracks the domain along with the belief state and adds this into the dialogue context to limit the length of the input sequence.\nPPTOD [28]: This method uses extra corpora similar to MultiWOZ dataset during the pre-training stage to enhance the model performance. They use the T5-base as a base model for the end-to-end TOD system.\nMTTOD [16]: MTTOD system is modeled in an end-to-end setting. They use the span prediction as an auxiliary task to improve the internal presentation of the pre-trained model and use the T5 pre-trained model as a base model.\nUBAR [14]: This approach uses a pre-trained GPT2 language model as a base model which is fine-tuned on the dialogue-session level. In the dialogue-session level, dialogue context comprises, user-utterance, generated belief-state, generated dialogue-act, and dialogue response of all dialogue turns. The dialogue session is used as an input sequence. In this approach, the TOD system fully operates in an end-to-end setting. We reconstructed the results of UBAR by adapting their code."}, {"title": "B. Dataset", "content": "We use the MultiWOZ2.1 [29] dataset to fine-tune the pre-trained model. It is a multi-domain task-oriented dialogue dataset that contains dialogues from seven distinct domains. The domains of this dataset are as follows: hotel, restaurant, taxi, train, attraction, police, hospital. The training set of this dataset includes 8,438 dialogues spanning over seven domains, while the validation and test set each contains 1,000 dialogues, excluding the hospital and police domains. A dialogue turn of a dialogue session may also belong to more than one domain. Table II shows the single-domain and multi-domain dialogues available in the dataset."}, {"title": "C. Training Details", "content": "We use a distilled version of the pre-trained GPT-2 language model as a base mode and the GPT2Tokenizer for tokenizing the sentences. The distilled version of GPT2 consists of six transformer's decoder blocks. We use the batch size of 2 with the gradient accumulation step of 16 for 50 epochs on the MultiWOZ2.1 training dataset to fine-tune the pre-trained model. The pre-trained model can take the input sequence with the length of 1024 tokens and any input sequence longer than this is truncated before being given as input to the model. We use the AdamW optimizer with a learning rate of 0.0001 for optimization and select the best-performing model on the validation set based on the evaluation metric with the highest success rate. The hyper-parameter $\\alpha$ controls the trade-off between success rate and BLEU score and the hyper-parameter $\\beta$ is a weight given to the RL loss function. We set $\\alpha$ to 3 and $\\beta$ to 0.8 after conducting a hyper-parameter search manually."}, {"title": "D. Evaluation Metrics", "content": "We follow the automatic evaluation metrics: inform rate, success rate, and bleu to evaluate the model in an end-to-end setting for task completion. Inform rate evaluates whether the system has correctly provided all entities. and success rate evaluates whether the system has provided all the requested information while BLEU [30] evaluates the model on fluency and coherency of dialogue response [31]. A combined score (Comb): (Inform rate + Success rate) \u00d7 0.5 + BLEU measures the overall quality of the system response [32]."}, {"title": "V. RESULTS AND DISCUSSION", "content": "Table I, shows the performance of our model in an end-to-end setting on the MultiWOZ2.1 dataset. In this setting, All intermediate information of the TOD system is generated rather than relying on the oracle/ground-truth values. We use the generated belief state to query the database and the generated system acts to generate the system response. The experimental results demonstrate that the pre-trained model, fine-tuned with a combination of offline reinforcement learning and supervised learning, significantly improves the inform rate and success rate. Results in Table I show that our approach demonstrates the competitive performance with other baseline methods. Although the combined score is influenced by the BLEU, our model still surpasses the baseline on this metric. This indicates that our method performs much better on the inform rate and success rate to compensate for the comparatively low BLEU score. Specifically, it delivers improvements of 1.60% in inform rate and 3.17% in success rate over the baseline method, UBAR on the MultiWOZ2.1 dataset."}, {"title": "B. Single-Domain vs Multi-Domain", "content": "To prove the effectiveness of our proposed method, we evaluate our model with two different sets of dialogues. For this, we split the test dataset into two groups, one group contains the dialogues from single-domain and another one from multi-domain. We fine-tune the pre-trained model using a complete training dataset and evaluate the model on both groups separately. Figure 4, shows the performance comparison of our model with baseline on the evaluation metrics of inform rate and success rate. It can be seen in Table III that our model achieves comparable results on single-domain dialogues and a significant improvement on multi-domain as well as full-set of the MultiWOZ2.1 dataset with baseline. We can observe from the figure that our model achieves comparatively better results on multi-domain dialogues than on single-domain dialogues, outperforming the baseline."}, {"title": "C. Dialogue Turn Analysis", "content": "A task can be accomplished in any number of dialogue turns that depends on the user constraints imposed by the user during conversation. We assess the effectiveness of our model on different dialogue turn sizes on the test dataset. For this assessment, we divide the test dataset into four groups based on the dialogue turn size. The first group contains 127 dialogues with dialogue turn sizes ranging from 2 to 4. The second group includes 203 dialogues with dialogue turn sizes ranging from 5 to 6. The third group contains 378 dialogues with dialogue turn sizes between 7 to 8.\nFinally, the fourth group includes 272 dialogues with dialogue turn sizes ranging from 9 to 12. Approximately 38% of the dialogues in the test set have dialogue turn sizes ranging from 7 to 8 this indicates that user tasks are often concluded or achieved within this number of dialogue turns. Figure IV shows that our model achieves the highest improvement in this segment, indicating that it is highly effective when users constrain the task to 7 to 8 dialogue turns. Our model also shows superior performance on all other sets of groups. The results of the base model (BM), UBAR, and our approach are shown in Table IV."}, {"title": "D. Hyper-parameter Tuning", "content": "We perform the hyper-parameter tuning of $\\alpha$ and $\\beta$ manually. The hyper-parameter $\\alpha$ controls the trade-off between BLEU and success rate while calculating the reward. The trade-off between success rate and BLEU is important as it helps the model to improve the success rate without losing the coherency and fluency of the generated response. The hyper-parameter $\\beta$ balances the importance between the cross entropy loss and REINFORCE loss. The performance of our model on different values of $\\alpha$ and $\\beta$ on the MultiWOZ2.1 test dataset is given in Table VI."}, {"title": "E. Case Study", "content": "Table V presents examples of system responses corresponding to user utterances from the MultiWOZ2.1 test set. As a case study, we showcase the system response from a specific dialogue turn in a multi-domain conversation to demonstrate that our model delivers higher quality system responses than the baseline. In the first example, we see that UBAR fails to provide the requested information but our model provides all the requested information and closely matches the ground truth. In the second example, our approach delivers a more relevant response to the user utterance by mentioning the source and destination of the train with its identification number and leaving time from the source station. These details are absent in UBAR's response but align more closely with the ground truth response. In the third example, we can see the UBAR gives repetitive information and does not deliver all the requested information on the other side our model provides all the requested information by specifying the train identification number. Our model also gives a more affirmative and concise response than the ground truth response."}, {"title": "VI. CONCLUSION", "content": "Large language models are fine-tuned using supervised learning to adapt to task-oriented dialogue systems. Such methods tend to exhibit exposure bias problems and token loss problems. In the exposure bias problem model generates texts similar to training data and in the token loss problem model generates tokens one at a time neglecting the complete context of the input. These problems deviate the model from the successful completion of the user task. In this paper, we addressed these issues by fine-tuning a pre-trained model using offline reinforcement learning along with supervised learning. We used a non-differentiable reward to steer the language model toward the successful completion of the user task and also to generate coherent and fluent system responses. We use the success rate and BLEU evaluation metrics to calculate the reward. Through experiments, we demonstrate that offline reinforcement learning effectively guides the pre-trained model towards the successful completion of the user task by enhancing the inform rate and success rate."}]}