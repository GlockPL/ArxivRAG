{"title": "Aligning Generalisation Between Humans and Machines", "authors": ["Filip Ilievski", "Barbara Hammer", "Frank van Harmelen", "Benjamin Paassen", "Sascha Saralajew", "Ute Schmid", "Michael Biehl", "Marianna Bolognesi", "Xin Luna Dong", "Kiril Gashteovski", "Pascal Hitzler", "Giuseppe Marra", "Pasquale Minervini", "Martin Mundt", "Axel-Cyrille Ngonga Ngomo", "Alessandro Oltramari", "Gabriella Pasi", "Zeynep G. Saribatur", "Luciano Serafini", "John Shawe-Taylor", "Vered Shwartz", "Gabriella Skitalinskaya", "Clemens Stachl", "Gido M. van de Ven", "Thomas Villmann"], "abstract": "Recent advances in AI-including generative approaches have resulted in technology that can support humans in scientific discovery and decision support but may also disrupt democracies and target individuals. The responsible use of AI increasingly shows the need for human-AI teaming, necessitating effective interaction between humans and machines. A crucial yet often overlooked aspect of these interactions is the different ways in which humans and machines generalise. In cognitive science, human generalisation commonly involves abstraction and concept learning. In contrast, AI generalisation encompasses out-of-domain generalisation in machine learning, rule-based reasoning in symbolic AI, and abstraction in neuro-symbolic AI. In this perspective paper, we combine insights from AI and cognitive science to identify key commonalities and differences across three dimensions: notions of generalisation, methods for generalisation, and evaluation of generalisation. We map the different conceptualisations of generalisation in AI and cognitive science Along these three dimensions and consider their role in human-Al teaming. This results in interdisciplinary challenges across AI and cognitive science that must be tackled to provide a foundation for effective and cognitively supported alignment in human-AI teaming scenarios.", "sections": [{"title": "1 Introduction", "content": "Recent advances in AI-including generative approaches-have resulted in technology that can support humans in decision-making and scientific discovery, as exemplified by its recent use for predicting protein structures (Jumper et al., 2021). Conversely, AI also has the potential to disrupt democracies and target individuals (Ferrara, 2024), as shown by the deepfake audio of President Biden in the New Hampshire primary. The responsible use of AI increasingly highlights the need for advances in human-AI teaming, especially in complex application scenarios, such as automotive driver assistance or decision-making in medicine. However, effective human-AI teaming requires alignment of their interaction properties. Alignment implies at least that humans must be able to assess the AI's responses and to access rationales (called \u201cexplanations\u201d, Doran et al., 2017) that underpin these responses.\nA crucial yet often overlooked aspect of these interactions is the complementary ways in which humans and machines generalise (Figure 1). Generalisation is typically defined as a transfer of what has been learned in one context to a new, potentially similar one (Goldstein, 2015). In cognitive science, human generalisation commonly involves concept learning and the abstraction of general characteristics to a collection of entities (Harnad, 2017, Medin et al., 2005). Humans excel at generalising from a few examples, handling out-of-distribution (OOD) data, reasoning about causal implications, and filling gaps in experience using abstraction, common sense, and structured knowledge, even within limited information and time constraints (Holzinger et al., 2023). This contrasts sharply with data-driven AI systems, which struggle to generalise beyond their training distribution and abstract effectively despite processing vast amounts of data. Statistical learning systems are driven by correlations rather than causal inference. Statistical learning, in turn, excels in data faithfulness (finding regularities in complex data), scale, and high complexity.\nThe goal of human-machine teaming (Vats et al., 2024) is that each side addresses the limitations of the other. For example, some generalisation capabilities of large language models (LLMs), like the quick production of rhetorically polished texts on any topic, are beyond those of most humans. Yet, their over-generalisation errors (\"hallucinations\", Ji et al., 2023b), like replacing specific facts with non-factual information, can be easily caught by a knowledgeable human. The complementarity of humans and AI has also shed new light on traditional analytical and case-to-case (or instance-based) AI paradigms, resulting in emerging research directions under the umbrella of neuro-symbolic AI (Besold et al., 2021, Hitzler et al., 2023). Neuro-symbolic AI aims to preserve the strengths of currently dominant (neural) statistical models, such as scaling and capturing complexity, while enhancing their ability for abstraction and justification by leveraging symbolic approaches, ultimately enabling more effective human-AI teaming.\nThis perspective paper draws on insights about the generalisation of humans and machines from AI and cognitive science. We identify key commonalities and differences across three dimensions critical for human-AI teaming: notions of generalisation, methods for generalisation, and evaluation of generalisation. Along these three dimensions, we map the different conceptualizations of generalisation in AI and cognitive"}, {"title": "2 Parallels in the Generalisation by Humans and Machines", "content": "Approaches to generalisation have been proposed in the context of machine learning (ML) as well as in cognitive psychology and often mutually inspired each other. This mutual inspiration holds for all types of approaches, be it symbolic and knowledge-informed approaches to rule learning, case-based and analogy-based approaches, as well as neural and statistical approaches (we will describe how the capability of generalisation is approached in such methods in Section 4). In the current section, this observation will be illustrated by selected historical milestones, summarized in Figure 2.\nIn the early days of cognitive psychology, Bruner et al. (1956) presented a series of empirical studies and experiments investigating human concept learning (Figure 2a). These studies focused on learning conjunctive and disjunctive rules from examples. The reported findings inspired the first decision tree learning algorithms (Hunt et al.,"}, {"title": "3 Notions of Generalisation", "content": "There are three different notions of generalisation in the broader context of cognitive science and AI research. First, generalisation refers to a process by which general concepts and rules are constructed from example data. Second, generalisation refers to the product of such a process, meaning the general concepts and rules themselves in their diverse representations. Third, generalisation refers to an operation of applying this product to new data. Next, we elaborate on these three notions and their categories, drawing on research in cognitive science, symbolic AI, and machine learning."}, {"title": "3.1 Generalisation as a process", "content": "In cognitive science, generalisation is related to the process of abstraction. Colunga and Smith (2003) propose that conceptual categories, i. e., products of abstraction processes, emerge from associative learning and generalisation by similarity. Campell and Piaget (2014) distinguish between abstraction through associative learning and abstraction through the transformation of schemas from the lower to the higher stages of cognitive development. More broadly, French (1995) distinguishes three types of conceptual changes: (1) generalisation of concrete instances into an abstract schema, which we call abstraction, (2) generalisation through the application or extension of the schema to various situations, which we call extension, and (3) generalisation involving the transformation/adaptation of the schema to fit a new context, which we call analogy. These three subtypes of generalisation processes are also recognizable in symbolic AI and machine learning. Abstraction (type 1) relates to classic forms of constructing a model from example data, such as concept learning or rule mining in symbolic AI, as well as clustering/classification (for discrete classes) and regression/dimension reduction (for functional relationships) in machine learning (Biehl, 2023). Model extension (type 2) relates to methods that (slightly) adapt machine learning models to make them applicable to new data similar to the original training data. Examples are online learning, multi-task learning, few-shot learning, and continual learning (Lu et al., 2018, Verwimp et al., 2024, Zhang and Yang, 2021). Analogy (type 3) refers to the model transfer to a new domain or task. Techniques for this constitute transfer learning (Zhuang et al., 2020) and reasoning by analogy in symbolic AI. Importantly, generalisation does not always start from example data, resulting in a (general) model. Instead, we may start from a pre-existing model and abstract, extend, or transfer it further by analogy, thereby generalising it beyond its original scope. The result of this generalisation process (of either type 1, 2, or 3) is a product, which we discuss next."}, {"title": "3.2 Generalisation as a product", "content": "Products of generalisation formalise an abstraction that goes beyond specific instances and can thus be applied to new cases (for a cognitive science perspective, refer to Reilly et al., 2003). For example, the concept of a 'cup' can be generalised from experiences with exemplars of cups. Typical products are categories, concepts, rules, and models. Generalisations of categories and concepts may be represented in different ways, for instance, as a symbolic definition via a list of attributes, via bounds in attributes (refer to Jackendoff, 1985, for the cognitive science perspective and Bertsimas and Dunn, 2017, for decision trees in machine learning perspective), as a prototype (refer to Mervis et al., 1981, for a cognitive science perspective and to Bien and Tibshirani, 2011, for machine learning perspective), or as a set of exemplars of the category (as in k-nearest neighbour classification in machine learning Peterson, 2009; for cognitive science see Nosofsky, 2011). In probabilistic models, categories or concepts are represented as a probability distribution from which examples of this category can be drawn, which is also the notion implicitly used by generative AI models (Bengesi et al., 2024). Beyond categories or concepts, products may also be rules or relations. For example, from observed instances of dogs, we may generalise the rule that bigger"}, {"title": "3.3 Generalisation as an operator", "content": "The whole purpose of generalisation (as a process) that produces a generalisation (as a product) is to apply the generalisation (as an operator) to new data. The ability of a model to generate accurate predictions on new data is at the core of generalisation in machine learning (Adams et al., 2022, Shalev-Shwartz and Ben-David, 2014). Different proposals have been put forward on how to formalise the generalisation operator in mathematical terms, as this is a prerequisite to developing algorithms for generalisation (see Section 4) and to evaluate the generalisation capability of digital artefacts (see Section 5). Three mathematical theories of generalisation have emerged in the literature: (1) The Probably Approximately Correct (PAC) framework analyses whether a model (i. e., a product) derived from a machine learning algorithm (i. e., a generalisation process) from a random sample of data can be expected to achieve a low prediction error on new data from the same distribution in most cases (Shalev-Shwartz and Ben-David, 2014). (2) The statistical physics of learning aims to understand the typical properties of learning algorithms (i. e., processes) with many adaptive parameters (Decelle, 2022, Engel and Broeck, 2001, Seung et al., 1992). (3) Vapnik-Chervonenkis (VC) dimension theory focuses on the storage capacity of model classes and their subsequent ability to make accurate predictions on new data (Vapnik and Chervonenkis, 2015). One of the key insights in this context is that generalisation begins where memorisation ends, paraphrasing Cover (1965). In other words, only if the capacity of a system to memorise is limited or restricted can it generalise to novel data.\nImportantly, all three theories have been applied mainly to machine learning algorithms of the abstraction kind (subtype 1 above). For model extension or analogy, the mathematical theory is less well-established. For example, for out-of-distribution generalisation, which refers to the application of a model on data drawn from another distribution than the training data (Liu et al., 2021), Straat et al. (2022) provide statistical physics analysis for drift and Tripuraneni et al. (2020) analyse the generalisation risk in transfer learning. Moreover, almost all formalisations resort to probabilistic guarantees rather than uniform guarantees for arbitrary input. Alternative notions of generalisation have been proposed in language learning by the idea of learning in the limit (Gold, 1967). Although learning in the limit can capture the generalisability in compositional objects, such as language, algorithmic solutions are limited by the undecidability or high complexity of their inference (Zeugmann, 2003)."}, {"title": "4 Machine Methods for Generalisation", "content": "Humans' ability to generalise from past experiences to previously unconceived scenarios constitutes a necessary principle for navigating daily life. As elaborated in Section 2, human generalisation relates to the abstraction of rules or patterns of characteristics from previous experiences with similar stimuli (Gluck et al., 2020). Humans"}, {"title": "4.1 Statistical generalisation methods in AI", "content": "Many modern ML methods, including deep learning, aim for statistical generalisation: observational data (i. e., training patterns) serve as input to an inference mechanism that extracts a model for the whole population (i.e., the entire underlying distribution). Generalisation refers to the property that the inferred patterns can be successfully applied to new data (cf. Section 3.3). Commonly, algorithmic methods are expressed as optimisation methods of a model loss function, such as the model prediction error. As the loss cannot be evaluated on the whole population, it is typically approximated by the empirical loss on a given training set, referred to as empirical risk minimization (Vapnik, 1995). While the empirical error on an independent test set can evaluate the model generalisation ability (cf. Section 5), the empirical loss on the training set underestimates the actual model loss. Therefore, generalisation needs to be accounted for explicitly, which means that additional incentives are necessary to ensure patterns derived from specific instances transfer to the whole population. Popular strategies include regularization terms that favour models with better generalisation behaviour: specific instances are margin maximization, optimization of stability, or restriction to simple functions (Bousquet and Elisseeff, 2002, Schneider et al., 2009). Interestingly, heavily overparameterized deep learning models can lead to surprising generalisation capabilities due to intrinsic regularization, which is not yet fully understood (Grohs and Kutyniok, 2022). Empirical risk minimization has a fundamental limitation compared to human generalisation: generalisation can only be expected in areas covered by observations. That means generalisation to out-of-sample events, novel contexts, or distributional changes cannot be expected (Ye et al., 2021). Statistical generalisation methods are often based on model families with universal approximation capability to account for the lack of domain-specific knowledge. This allows modelling complex mechanisms such as those in computer vision or natural language processing (Chai et al., 2021, Otter et al., 2018). The product is typically a black box: since model parameters do not have a semantic meaning, the mere functional form does not provide insight into the model's generalisation behaviour. Recently developed post-hoc explanation methods allow a closer inspection of the underlying rationale and its impact on the model generalisation behaviour (Dalal et al., 2024, Molnar, 2020).\nAnother surprising observation has become popular in recent years: many vital settings, including generative models or LLMs, do not allow a simple analytic loss function that fully describes human intentions. Thus, surrogate losses such as next token prediction or masked prediction tasks are used as a proxy. Together with a massive amount of training data and tasks used for instruction tuning, impressive generalisability arises (Brown et al., 2020a, Zhang et al., 2023). The emerging generalisation abilities are only partially understood and do not necessarily align with human expectation, necessitating downstream evaluation (Section 5) (Bommasani et al., 2022)."}, {"title": "4.2 Analytical and knowledge-informed generalisation methods in AI", "content": "Analytical generalisation methods aim for empirical evidence of a theory. The resulting product is an explicit semantically meaningful representation inferred from and confirmed by data. Depending on its specific representation, different methods exist, such as mechanistic models (Baker et al., 2018), causal inference (Yao et al., 2021), Bayesian networks (Puga et al., 2015), knowledge graphs (Hogan et al., 2022, Ji et al., 2022), functional programming (Kitzelmann and Schmid, 2006), and inductive logic programming (Cropper et al., 2022). As the inherent model semantics is directly accessible to humans, humans can inspect how these models generalise to previously not encountered scenarios. Since the parameters of these analytic models have a semantic meaning, they require semantic grounding, which is challenging to realise with sub-symbolic, low-level sensor data. Such limitations can be partially overcome by neuro-symbolic integration: Deep-Problog, as an example, combines deep neural networks that transfer sub-symbolic signals to semantic concepts that can be used in symbolic inference (Manhaeve et al., 2021).\nChoosing the optimal model structure is challenging, facing numerically complex problems and limitations caused by the non-identifiability of structural components (Koller and Friedman, 2009). Hence, many methods are restricted to simple schemes, such as description logic, rather than universal approximators. Learning methods are diverse as they mirror the specific representation, ranging from semantic clustering over probabilistic rule mining to subsumption and analogies. Besides a valid generalisation, noise robustness and inference efficiency constitute significant challenges, which render hybrid approaches such as embedding mechanisms appealing (Cao et al., 2024). Systematic compositionality refers to the ability to generalise and produce novel combinations from known components. It has been fundamental in the design of traditional logic-based systems and, more generally, analytics methods. Statistical methods, particularly neural networks, have struggled with compositional generalisation (Fodor and Pylyshyn, 1988), as they do not inherently possess the structure needed for compositional reasoning. In the last few years, significant progress has been made in enhancing compositional generalisation in deep learning, typically by adding analytical components that mirror the compositional structure of the domain. Examples include compositional processing of structured objects such as recursive models or graph neural networks (Hammer, 2000, Wu et al., 2021), object-centric representation learning (Locatello et al., 2020), or meta-learning for compositional generalisation (Lake and Baroni, 2023). While these efforts provide a pathway for neural networks to generalise systematically, most results are empirical, making achieving predictable and systematic generalisation challenging (Wiedemer et al., 2024). There remains a significant gap between the systematic generalisation capabilities of analytical models and the representation learning techniques of deep models, with new approaches, specifically neuro-symbolic AI, promising as a viable bridge (Sehgal et al., 2024)."}, {"title": "4.3 Instance-based translation in AI", "content": "Lazy-learning methods refer to non-parametric techniques such as nearest-neighbours methods, case-based reasoning, or local regression (Aha, 2013). They rely on local inference, computed when needed based on similar cases encountered previously. Nearest neighbour methods are among the oldest and most popular ML methods, displaying immense flexibility when combined with complex representations (Khandelwal et al., 2020). As they rely on local inference, human inspection of single decisions\u2014albeit not the entire model is usually possible.\nBecause of their local representation and controlled inference, instance-based models have shown great promise for incremental learning in the context of possible distributional shift or drift (Losing et al., 2016). One reason is the explicit memory in nearest-neighbours models, which comes with a natural forget mechanism. However, since they rely on the notion of similarity or neighbourhood, a suitable choice of representation and similarity metrics is crucial to support generalisation (Kulis, 2013); more generally, the representation directly influences the ability of models to capture and generalise cases effectively and evolve patterns across diverse datasets and tasks. Interestingly, there has been research on representations to support specific generalisations, including generalisations across tasks or domains (He et al., 2022).\nContext has a unique role as generalisation requires adapting knowledge learnt in one setting to fit a novel, unseen one. Humans can make such generalisations, addressing two main aspects: acquiring and formally representing context knowledge and assessing the similarity of two contextual representations (e.g., domain- and task-related knowledge) (Shepard, 1987). Different ML techniques can be applied to mimic this process, including transfer learning and, in generative approaches, techniques such as prompting and retrieval augmented generation (Gao et al., 2024). Indeed, LLMs have demonstrated remarkable capabilities as few-shot learners provided sufficient scaling (Brown et al., 2020c); this is even surpassed by their capabilities for in-context learning (Dong et al., 2024). These techniques aim to adapt a learnt model to contextual information, albeit implicitly. A suitable explicit representation of contextual knowledge, e.g., through neuro-symbolic AI, is the subject of ongoing research."}, {"title": "5 Evaluation of Generalisation", "content": "From a statistical learning perspective, evaluating the generalisation of supervised approaches estimates how well they perform on unseen data samples. When testing generalisation as applicability to new data (Section 3.3), the training and test data samples are assumed to be independently generated by the same data distribution. This formalization of train-test generalisation is theoretically grounded and thus remains relevant when assessing and certifying systems. However, as tasks become more complex and potentially increase system opaqueness, special care must be taken to ensure that respective assumptions remain in practice. In particular, we need to consider the data distribution, how we can draw representative samples for the train and test set, and ensure that they are independent. As an illustration, Li and Flanigan (2024) show that ChatGPT performs surprisingly well on all benchmarks published before its release and much worse on all benchmarks published later. In this case,"}, {"title": "5.1 Measuring distributional shifts", "content": "Assessing whether empirically gathered data stem from the same distribution is non-trivial. To measure the extent to which existing datasets are OOD, statistical distance measures such as the Kullback-Leibler divergence or Wasserstein distance can quantify the divergence between the feature distributions of the training and test sets. Generative models offer an explicit likelihood estimation $p(x)$ that can be used to assess how typical a given instance is to the training distribution. Because discriminative models do not offer this possibility, proxy techniques include calculating cosine similarity between embedding vectors and using nearest-neighbour distances in a suitably transformed feature space. In the case of LLMs, a common approach is to measure perplexity as a proxy for familiarity. When the model's internal representations cannot be directly accessed, probing its sensitivity to changes is informative. For example, the layers of non-linear abstractions formed in most modern (deep) machine learning models allow for gauging relations through intermittent embeddings.\nData samples with distributional shifts can also be deliberately identified or generated to understand the model's robustness. Such adversarial techniques alter key data features such as syntax, semantics, or context while preserving the underlying task and the original label. In contrast, counterfactual techniques create data samples that alter the target prediction with minimal input changes."}, {"title": "5.2 Determining under- and over-generalisation", "content": "The performance of models is known to degrade for a wide range of potential natural changes, such as frequent camera and environment perturbations in computer vision (Hendrycks and Dietterich, 2019), because the model is not invariant to these changes. Such examples reveal under-generalisation because they introduce an imperceptible shift in input that results in a considerable modification within a model. For foundation models, even the prompt choice can substantially affect performance (Gonen et al., 2023). In contrast, models typically overconfidently make false predictions for completely unknown concepts (Boult et al., 2019) precisely because critical differences are ignored in prediction. One such over-generalisation phenomenon is hallucination in LLMs. This term originally referred to models deviating from the source, such as the input document in summarization. In LLMs, the source covers pre-training data in a more general way (Ji et al., 2023a), extending the definition of hallucination to include factually incorrect statements. Other examples of inappropriate over-generalisation are biased predictions, e.g., when a model predicts a property of an individual from the statistical properties of a demographic group to which they belong (Hovy and Spruit, 2016), and logical fallacies (Sourati et al., 2023).\nCharacterising the model's under- and over-generalisation requires choosing an appropriate metric, defining its point of use, and setting a mechanism to interpret the metric's value regarding its ability to generalise beyond the particular test examples. The procedure is also susceptible to two caveats. First, the choice between discriminative and generative model, in the mathematical sense of modelling $p(y|x)$ and $p(y \\vert x) p(x)$, determines what representational basis is used to infer similarity (Mundt et al., 2023). Second, deep models are prone to learn various decision shortcuts (Lapuschkin et al., 2019) and to fall victim to simplicity bias in their prediction by ignoring meaningful features altogether (Shah et al., 2020) (e.g., a visual classification model that learns to distinguish oranges from avocados may learn to only rely on colour features). To guard against these caveats, evaluating across different levels of abstraction (from surface forms, through semantic similarity, to higher-level conceptual analogies) and explicitly considering application context and limits is critical.\nIn the era of foundation models trained on large data sets, the appropriateness of evaluations on test sets is questionable. Foundation models show impressive performance on many tasks without being specifically trained to perform them (Brown et al., 2020b); that is, trained models on task A perform surprisingly well on task B. This is a positive shift from the fine-tuning paradigm, in which models had to be trained on each task, often learning spurious correlations. However, measuring the extent to which models generalise is still challenging. Modern models are frequently updated, partially based on user interactions through reinforcement learning, increasing their exposure to datasets and hindering reproducibility. The lack of transparency is also induced by the fact that many large models are proprietary and not openly published."}, {"title": "5.3 Distinguishing memorisation, generalisation, and abstraction", "content": "In AI, memorisation refers to learning details from the training data, including noise and facts, that do not generalise to new data. Memorisation is often seen as a challenge for the evaluation of foundational models because models may have been trained on a data sample that may also be used for testing, invalidating the findings (so-called data contamination, Dodge et al., 2021). A more fundamental question is: in which cases should the models generalise, and when should they memorise? And how can this be evaluated convincingly? Factual knowledge should often be memorised; for instance, Paris is the capital of France, and mosquitoes fly. When learning from experience, generalisation is crucial, e.g., to recognise a new manifestation of a maze object (Hsu et al., 2024). In practice, the expectation of whether the models should generalise or memorise is a priori. Tasks that involve recall from memory, such as factual question answering and legal reasoning over precedents, require models to have provably correct reasoning over explicit background information. Generalisation task setups include cross-domain validation, robustness testing, adversarial testing, and counterfactual"}, {"title": "5.4 Evaluating human-AI teaming", "content": "Effective human-machine collaboration can be based on a unifying theoretical framework for both forms of intelligence or focus on their complementary strengths and weaknesses. The first approach, reflected in cognitive architecture research (cf. Gonzalez et al., 2023), aims to replicate human cognition at a computational level to facilitate collaboration. The second approach designs AI systems to maximise accuracy and minimise human effort, as seen in reinforcement learning from human feedback (Christiano et al., 2017). A third approach, inspired by the concept of \"ultra-strong machine learning\" (Michie, 1988, Muggleton et al., 2018), proposes integrating both perspectives to enhance collaboration. Michie (1988) emphasised predictive performance and the comprehensibility of learnt knowledge, advocating for AI to support human learning by making its knowledge interpretable and teachable. The sophistication of the necessary alignment will depend on the complexity of the underlying task tackled by the human-AI team, such as automotive driver assistance or decision-making in medicine. This implicit hierarchy of complexity suggests we can develop challenges that appropriately stretch the state-of-the-art without overreaching to unrealistic scenarios. Such challenges include focusing on interactive representation alignment, contextual inference, theory of mind, real-world grounding, and communication levels.\nA nuanced approach that considers the interplay between human and AI capabilities (Akata et al., 2020) is paramount to evaluating the efficacy of human-AI teams. This requires a meticulous evaluation of the objective task-related outcomes, subjective process-related experiences, and the long-term ramifications of the collaboration. Furthermore, the success metrics for human-AI teaming are contingent upon each party's relative contributions and responsibilities. For instance, if the cooperation is characterised by humans augmenting the AI's capabilities (e. g., manual data labelling, reinforcement learning), the success of the collaboration could be objectively quantified in terms of the model's post-interaction performance on suitable benchmarks. In contrast, subjective evaluation metrics may be more suitable if a collaborative task is characterised by an AI assisting humans (e. g., question answering in a conversational interface). These metrics encompass but are not limited to, trust in the veracity of provided answers, the enjoyment of interaction, or a user's perceived self-efficacy (Li et al., 2024, Sharma et al., 2023, Yang et al., 2024b). In balanced collaborations, where both AI and humans contribute equally, a blend of objective measures (errors made, task completion time) and subjective measures (user satisfaction) is required (Braun et al., 2023). Notably, while metrics for human-AI teaming are being developed, there is little research into the possibility of evaluating the generalisation of such teams."}, {"title": "6 Emerging Directions", "content": "The prior sections highlight the challenges in aligning human and machine intelligence, highlighting AI's potential to augment human generalisation capabilities. A summary of the different properties, methods, and evaluation practices for generalisation in humans and AI is shown in Table 3. The table shows the complementarity of statistical, case-to-case, and analytical generalisation approaches to satisfying desirable properties such as accuracy, shift robustness, and compositionality. The evaluation column indicates that developing adequate evaluation procedures remains challenging, especially for explainability, compositionality, and learning from a few samples. We discuss research directions toward novel theories, methods, and evaluation practices.\nGeneralisation theory in the era of foundation models\nRecent approaches of zero-shot and in-context learning in LLMs try to generalise to tasks that may be entirely disconnected from the ones the LLMs were trained for, without any explicit similarity having been established (Bubeck et al., 2023, Kojima et al., 2022). In other words, the model builders assume that LLMs have implicitly generalised (process, Section 3.1) to generalisations (product, Section 3.2) that permit generalisation (application, Section 3.3) to entirely new tasks and domains. Based on the prior work outlined in Section 3, such an assumption appears unsubstantiated, motivating further research to make generalisations in that form realistic. First, new processes and products of generalisation are required to provide guarantees or reasons to believe that (zero-shot) application to new tasks and domains is viable. Explicitly encoding invariances/equivariances in models or cognitively-inspired representations, such as prototypes, may be helpful. Second, a new theory is required to understand the conditions under which few- or zero-shot applications to new tasks and domains can be expected. This theory will likely need to join established notions of ML theory (be it statistical machine learning theory, statistical physics, or VC dimension) with notions of invariances or analogies between domains that must be mathematically precise.\nGeneralisable neuro-symbolic methods\nNeuro-symbolic AI carries great promises, as it can combine aspects of statistical methods and analytic models, thus leading to a possible combination of statistically robust and data-driven models for complex sub-symbolic parts and explicit compositional modelling for overarching schemes. Yet many challenges remain. First, defining provable generalisation properties, including worst-case bounds, is essential. How can we derive formal properties for (neuro-symbolic) AI generalisation based only on compositionality rather than (weaker) statistical guarantees? Can we identify reasonable and relevant situations with formal guarantees for continual learning and learning under drift? Which surrogate cost functions are provably compatible with the underlying learning objective? Second, how to handle context remains a question. What measures the distance between contexts? What is a reasonable projection operator for applying a generalisation from one context to another? How do we know that a context is too novel to support generalisation? When do we overgeneralise? Third, neuro-symbolic methods should represent information efficiently and facilitate compositionality. How can we choose a suitable representation and similarity measure(s) to enable generalisation? How do we combine multiple representations for generalisation? How do we compose two generalisations into a new one? Can we exploit compositional embeddings for compositional generalisation?\nEvaluation of generalisation in foundation models\nSeveral directions have emerged in response to concerns about data contamination, spurious correlations, and overfitting state-of-the-art models. Generalization benchmarks for abstract visual reasoning (Chollet, 2019, Jiang et al., 2024) and analogy (Bitton et al., 2023, Sourati et al., 2024) are gaining popularity. Such test sets can be created regularly and scaled more easily using crowd-sourcing platforms. However, crowd-sourcing can introduce cognitive and sociopolitical biases by annotators (Draws et al., 2021), a poorly understood issue. Evaluation servers and public leaderboards that apply safety measures, such as hidden test labels, can prevent overfitting and memorisation but lack standardisation and are expensive and labour-intensive to set up and maintain. Another form of community benchmarking is early materials with an informal evaluation of models' success and failure cases (cf. Arkoudas (2023)). Yet, it remains unclear how to facilitate the inclusion of such examples in benchmarks to ensure reproducibility and prevent model memorisation. Ultimately, the path forward may be to move away from static test sets and toward simulation environments and synthetic data generators (Duan et al., 2022). However, data generators and simulated environments are challenging to develop and are often too artificial, restricting their resemblance to real-world situations (sim-to-real gap). Addressing reproducibility concerns, researchers have proposed concepts such as model cards (Mitchell et al., 2019) or data cards (Pushkarna et al., 2022) to report critical information for reproducing experiments, such as training parameters and evaluation metrics. Orthogonal efforts introduce reproducibility checklists based on a broad consensus (Kapoor et al., 2024); however, their coverage of generalisability remains limited.\nGeneralisation mechanisms in human-AI teams\nHuman-AI teaming requires well-understood collaboration and explanation workflows (Akata et al., 2020). Explanations bridge the gap between human reasoning and AI's internal workings. A critical challenge is reconciling fundamentally different reasoning paradigms, such as human causal models versus AI's deep learning associations. Can these approaches be unified into a common explanatory language? Efforts like concept-based explanations (Widmer et al., 2023) and those considering concepts and relationships (Finzel et al., 2024) suggest promising avenues"}]}