{"title": "PaRCE: Probabilistic and Reconstruction-Based Competency Estimation for Safe Navigation Under Perception Uncertainty", "authors": ["Sara Pohland", "Claire Tomlin"], "abstract": "Perception-based navigation systems are useful for unmanned ground vehicle (UGV) navigation in complex terrains, where traditional depth-based navigation schemes are insufficient. However, these data-driven methods are highly dependent on their training data and can fail in surprising and dramatic ways with little warning. To ensure the safety of the vehicle and the surrounding environment, it is imperative that the navigation system is able to recognize the predictive uncertainty of the perception model and respond safely and effectively in the face of uncertainty. In an effort to enable safe navigation under perception uncertainty, we develop a probabilistic and reconstruction-based competency estimation (PaRCE) method to estimate the model's level of familiarity with an input image as a whole and with specific regions in the image. We find that the overall competency score can correctly predict correctly classified, misclassified, and out-of-distribution (OOD) samples. We also confirm that the regional competency maps can accurately distinguish between familiar and unfamiliar regions across images. We then use this competency information to develop a planning and control scheme that enables effective navigation while maintaining a low probability of error. We find that the competency-aware scheme greatly reduces the number of collisions with unfamiliar obstacles, compared to a baseline controller with no competency awareness. Furthermore, the regional competency information is very valuable in enabling efficient navigation.", "sections": [{"title": "I. INTRODUCTION", "content": "Within the area of unmanned ground vehicle (UGV) navi-\ngation in unstructured environments, learning-based compo-\nnents have become commonplace and seemingly necessary\n[1]. Deep neural network (DNN)-based perception models\nhave proven to be particularly useful for accurately analyzing\nthe traversability of the terrain, preventing collisions, and\nenabling effective navigation [1]. Unfortunately, as data-\ndriven and black-box methods, DNN-based perception mod-\nels face inherent limitations: they lack transparency and\nexplainability [2], are often overconfident in their predictions\n[3], are sensitive to shifts in the input data distribution [4],\nand can fail quite surprisingly and ungracefully [5].\nWith the prevalence of DNN models in many real-world\napplications and concern about their failure modes, there has\nbeen extensive research on quantifying uncertainty in these\nmodels [6] and detecting inputs that are outside of their\ntraining distribution [7]. This has enabled the development\nof uncertainty-aware navigation schemes that explicitly con-\nsider the existence of uncertainty arising from DNN-based\nperception models. However, existing navigation schemes\ngenerally rely on uncertainty estimation methods that do\nnot perform well for data outside of the model's training"}, {"title": "II. BACKGROUND & RELATED WORK", "content": "Field robots used in search and rescue, planetary explo-\nration, and agricultural tasks, must navigate in unstructured\nenvironments, which lack clearly viable paths or helpful\nlandmarks [1]. In such environments, perception provides\nthe necessary information to make the vehicle aware of the\nsurrounding environment. Within this context, most work has\nfocused on DNN-based methods to analyze the capability\nof a UGV to stably reach a terrain region and determine\nappropriate control actions [1]. Without properly considering\nthe uncertainty surrounding the predictions of these DNN-\nbased perception models, vehicles that rely on these models\ncan face unsafe situations and system failures. This has led to\na branch of uncertainty-aware navigation focused on dealing\nwith perception model prediction uncertainty."}, {"title": "A. Uncertainty Quantification (UQ)", "content": "Predictive uncertainty generally encapsulates both\ndata/aleatoric uncertainty, which arises from complexities of\nthe data (i.e., noise, class overlap, etc.), and model/epistemic\nuncertainty, which reflects the ability of the perception model\nto capture the true underlying model of the data [8]. The\nmodeling of these uncertainties can generally be divided into\nmethods based on (1) Bayesian neural networks (BNNs)\nthat extract uncertainty as a statistical measure over the\noutput of a BNN [9], [10], (2) deterministic neural networks\n(NNs), often relying on Monte Carlo (MC) dropout as\napproximate Bayesian inference [11], and (3) ensembles of\nNNs that combine the predictions of multiple deterministic\nnetworks to form a probability density function [12]. While\nthese methods address the typical overconfidence in NN\npredictions with better-calibrated confidence scores, they\ngenerally focus on predictions for in-distribution inputs,\nwhich come from the same distribution as the training\ndata. These approaches are not sufficient in general to\nappropriately assign confidence scores for OOD inputs that\ndiffer significantly from those seen during training [4], [13]."}, {"title": "B. Out-of-Distribution (OOD) Detection", "content": "Many recent approaches have focused on quantifying\ndistributional uncertainty caused by a change in the input\ndata distribution [14]. Approaches that are specifically fo-\ncused on determining if an input falls outside of the input-\ndata distribution are referred to as out-of-distribution (OOD)\ndetection methods. These approaches are generally either\n(1) classification-based [15]\u2013[17], (2) density-based [18]-\n[22], (3) distance-based [23]-[26], or (4) reconstruction-\nbased [27]-[33]. While these methods do not fully capture\nthe predictive uncertainty associated with DNNs, they can\nidentify inputs that are drawn from a different distribution\nthan those used to train the model. For these inputs, the\nmodel cannot be safely trusted to obtain the correct output."}, {"title": "C. Anomaly Detection & Localization", "content": "A field of study that is closely related to OOD detection\nis anomaly detection and localization. The task of segment-\ning the particular pixels containing anomalies has become\npopular for identifying defects in industrial inspection [34],\nflagging abnormalities in medical image analysis [35], [36],\nand detecting abnormal behavior in surveillance applications\n[37]. Within the area of anomaly detection and localiza-\ntion, most approaches are (1) reconstruction-based [38]-\n[41], (2) classification-based [42], [43], or (3) distance-based\n[44]-[47]. While methods in uncertainty quantification, OOD\ndetection, and anomaly localization can successfully capture\ndifferent aspects of predictive uncertainty associated with\nDNNs, it is not clear how these methods should be used\nto develop safe and robust planning and control schemes."}, {"title": "D. Uncertainty-Aware Navigation", "content": "There are various approaches that seek to enable short-\nterm, local control under perception model predictive uncer-\ntainty. Existing approaches tend to rely on one of the UQ\nmethods discussed in Section II-A, using either BNNs [48],\n[49], MC dropout [50], or ensembles [51]-[53] to estimate\nuncertainty and inform the control scheme. As mentioned\npreviously, while these methods can effectively estimate\nsome aspects of predictive uncertainty, they are insufficient\nfor quantifying uncertainty associated with OOD samples,\nwhich a UGV is bound to encounter when navigating previ-\nously unseen or dynamic environments. There has also been\nwork that leverages uncertainty estimation approaches that\nrelate more closely to the distance-based OOD detection\nmethods [54] (using the estimation method developed in\n[55]) and classification-based anomaly detection methods\n[56] (using the method developed in [57]). While these ap-\nproaches do not fully capture the predictive uncertainty of the\nperception models, they are better equipped to handle inputs\nthat differ significantly from those seen during training.\nWe develop a reconstruction-based method for predictive\nuncertainty estimation with the end goal of probabilistically\nsafe control of a UGV. Our approach for uncertainty estima-\ntion has several benefits compared to existing methods. (1) It\ncaptures multiple aspects of uncertainty and can effectively\nidentify OOD or anomalous samples, unlike traditional UQ\ntechniques. (2) In contrast to methods that simply seek to\nidentify samples that differ significantly from the training set,\nour method is directly tied to the predictive uncertainty of the\nperception model used in a control task. Furthermore, rather\nthan making a binary decision, as is typical in OOD and\nanomaly detection tasks, our method is probabilistic and can\nbe incorporated into a probabilistically safe control scheme.\n(3) Finally, our method can characterize uncertainty at a\nregional level, rather than considering uncertainty only for\nthe image as a whole. This offers a greater amount of model\nexplainability and provides more flexibility in designing an\nuncertainty-aware planning and control scheme. We show\nhow our method for predictive uncertainty estimation can\nbe used to evaluate the probability of error associated with\nvehicle trajectories and to select a preferred trajectory based\non the desired level of conservativeness."}, {"title": "III. PROBLEM FORMULATION", "content": "In this work, we consider a UGV navigating a simulated\nlunar environment. During training, the perception model\nlearns to distinguish between different regions in the environ-\nment (e.g., bumpy terrain, smooth terrain, inside crater, edge\nof crater, etc.) to navigate effectively. During evaluation, it\nencounters astronauts and human-made obstacles that were\nnot seen during training. These unfamiliar obstacles are\noutlined in white in Figure 5 and are considered OOD\nbecause they were not present in the training set. The vehicle\nis expected to recognize regions in the environment that are\nunfamiliar to the perception model and continue to navigate\neffectively in the face of predictive uncertainty. To enable\neffective navigation, we develop a probabilistic competency\nestimation method for the perception model (Section IV),\nthen use these competency estimates to develop competency-\naware planning and control schemes (Section V). Additional\ndetails and parameters are provided in Appendices A and B."}, {"title": "IV. MODEL COMPETENCY ESTIMATION", "content": "We develop a probabilistic reconstruction-based method\nfor competency estimation and evaluate the ability of\nthis method to distinguish between correctly classified in-\ndistribution (ID) samples, incorrectly classified ID samples,\nand out-of-distribution (OOD) samples (\u00a7IV-A). We then\nextend this approach to estimate model competency for\nregions in an image and evaluate the ability to distinguish\nbetween regions in ID samples, familiar regions in OOD\nsamples, and unfamiliar regions in OOD samples (\u00a7IV-C)."}, {"title": "A. Estimating Overall Model Competency", "content": "Let $f$ be the true underlying model of the system from\nwhich our images are drawn and $\\hat{f}$ be the predicted model\n(referred to as the perception model). For an input image, $X$,\nthe perception model aims to estimate the true class of the\nimage, $f(X)$, from the set of all classes, $C$. The competency\nof the model for this image is given by\n$\\rho(X) := P(\\lbrace f(X) = \\hat{f}(X) \\rbrace | X)$.\nTo simplify our notation, let $\\hat{c}$ be the class predicted by\nthe perception model (i.e., $\\hat{f}(X) = \\hat{c}$) such that\n$\\rho(X) = P(\\lbrace f(X) = \\hat{c} \\rbrace | X)$.\nOften, the perception model uses the softmax function to\nobtain an estimate of the probability $P(\\lbrace f(X) = c\\} | X)$\nfor each class $c \\in C$. However, the perception model cannot\ntruly estimate this probability because it is limited by the\ndata contained in the training sample. It instead estimates\nthe probability $P(\\lbrace f(X) = c\\} | X, D)$, where $D$ is the event\nthat the input image is in-distribution (i.e. drawn from the\nsame distribution as the training samples). Let us then write\nthe following lower bound on competency:\n$\\rho(X) > P(D \\cap \\lbrace f(X) = \\hat{c} \\rbrace | X)$.\nThis lower bound can equivalently be expressed as\n$\\rho(X) \\geq P(\\lbrace f(X) = \\hat{c} \\rbrace | X, D) P(D | X)$.\nWe can assume that the perception model provides an\nestimate of the first probability. To estimate the second\nprobability, we design an autoencoder to reconstruct the input\nimage, training the autoencoder with the same images used\nto train the perception model. A holdout set is then used\nto estimate the distribution of the reconstruction loss for\neach class. It is assumed the reconstruction loss for a given\nclass, $L_c$, follows a Gaussian distribution with mean $\\mu_c$ and\nstandard deviation $\\sigma_c$. Let $l(X)$ be the reconstruction loss\nfor image $X$. The probability this image is drawn from the\nsame distribution as those in the training sample is given by\n$P(D|X) = \\sum_{c\\in C}P(D|\\lbrace f(X) = c\\rbrace )P(\\lbrace f(X) = c\\rbrace | X)$.\nAssume now that the perception model provides an ac-\ncuate estimate of $P(\\lbrace f(X) = c\\rbrace | X)$. We can estimate\n$P(D|\\lbrace f(X) = c\\rbrace )$ as the probability that the reconstruction\nloss corresponding to image $X$ aligns with the bottom $N\\%$ of\nthe training images. Because $L_c$ is a Gaussian random\nvariable, $N$ corresponds to a z-score, $z$, and we can estimate\n$P(D|\\lbrace f(X) = c\\rbrace )$ as $P_{D|C}$ in the following way:\n$P_{D|C} = P(\\lbrace L_c > l(X) - (\\mu_c + z\\sigma_c) \\rbrace )$\n$= 1 - P(\\lbrace L_c \\leq l(X) - (\\mu_c + z\\sigma_c) \\rbrace )$\n$= 1 - F_{L_c \\sim \\mathcal{N}(\\mu_c,\\sigma_c)}(l(X) - \\mu_c - z\\sigma_c)$\n$= 1 - F_{Z \\sim \\mathcal{N}(0,1)}( \\frac{l(X) - 2\\mu_c - z\\sigma_c}{\\sigma_c})$\n$= 1 - \\Phi ( \\frac{l(X) - 2\\mu_c }{\\sigma_c} - z)$,\n$=: 1 - Z_c$.\nLet $p_c$ be the probabilistic (softmax) output of the percep-\ntion model corresponding to class $c \\in C$. We now have the\nfollowing estimate of model competency:\n$\\rho(X) := \\beta  \\sum_{c \\in C} p_c (1 - Z_c)$.\nWe refer to $\\rho(X)$ as the overall competency score for\nimage $X$. If the model is 100% competent on the image, the\nprobability that its prediction is correct is one. If it is entirely\nincompetent, the probability its prediction is correct is zero.\nThe competency score is thus between zero and one, with\nhigher scores corresponding to higher levels of competency."}, {"title": "B. Analyzing Overall Model Competency Scores", "content": "The distribution of competency scores for correctly classi-\nfied ID images, misclassified ID images, and OOD images is\nshown in Figure 2. Notice that the competency score is very\nclose to one for nearly all correctly classified samples. It is\nbelow 0.3 for the majority of OOD images and is between\n0.3 and 1.0 for nearly all misclassified ID samples.\nWe compare our overall model competency estimation\nmethod to twelve existing methods for UQ and OOD de-\ntection. We find that, while the capability of our method\nto distinguish between correctly classified and misclassified\nsamples is comparable to existing methods, our scoring\nmethod is much better at distinguishing between ID and\nOOD samples. In addition, we believe our scoring method is"}, {"title": "C. Estimating Regional Model Competency", "content": "Suppose that, in addition to estimating the overall compe-\ntency score for the image as a whole, we wish to estimate\na regional competency score for each segmented region in\nthe image. Now instead of letting $X$ be the entire input\nimage, $X$ is a segmented region of the image. In this\nwork, segmented regions are determined by the Felzenszwalb\nsegmentation algorithm [58]. We follow roughly the same\nprocedures to estimate the probability that each region in\nthe input image came from the same distribution as the\ntraining samples. In this approach, rather than designing an\nautoecoder to reconstruct the input image, we design an\nimage inpainting model to reconstruct a missing segment of\nthe input image and measure the average reconstruction loss\nover the pixels corresponding to that image segment."}, {"title": "D. Analyzing Regional Model Competency Maps", "content": "The distribution of competency scores for regions in ID\nimages, familiar regions in OOD images, and unfamiliar\nregions in OOD images is shown in Figure 3. Notice that\nthe competency score is close to one for most of the familiar\nregions (in both ID and OOD images), while it is close to\nzero for most of the unfamiliar regions in OOD images.\nWe can use these competency estimates to generate regional\ncompetency maps (examples in column two of Figure 4).\nWe compare our regional competency estimation method\nto seven existing methods for anomaly detection and lo-\ncalization. We find that our method outperforms existing\nmethods when tasked with distinguishing between familiar\nand unfamiliar pixels, but its performance is not significantly"}, {"title": "V. COMPETENCY-AWARE NAVIGATION", "content": "We use the overall model competency estimates (\u00a7IV-A)\nand regional competency estimates (\u00a7IV-C) to explore nav-\nigation schemes with varying levels of competency aware-\nness. We develop a model of the vehicle dynamics (\u00a7V-A),\nuse this model to design a competency-aware path planner\n(\u00a7V-B), and develop a path-tracking controller (\u00a7V-C)."}, {"title": "A. Model of Vehicle Dynamics", "content": "We model our vehicle as a non-linear discrete time system\nwhose state, $x_k \\in \\mathbb{R}^5$, can be described in terms of its X-\nposition (x), Y-position (y), heading ($\\theta$), linear velocity (v),\nand turn rate (w). The input, $\\tilde{u}_k \\in \\mathbb{R}^2$ to the system is the\ndesired linear velocity or throttle (t) and desired turn rate or\nsteering command (s). The state and input are expressed as\n$\\tilde{x}_k = [x \\ y \\ \\theta \\ v \\ w]^T$ and $\\tilde{u} = [t \\ s]^T$ respectively.\nThe vehicle is then described by a discrete time-varying\nmodel that is very similar to the Dubin's car model:\n$\\tilde{x}_{k+1} = A_k\\tilde{x}_k + B \\tilde{u}_k$, where\n$A_k = \\begin{bmatrix}\n1 & 0 & 0 & \\Delta t \\cos \\theta_k & 0\\\\\n0 & 1 & 0 & \\Delta t \\sin \\theta_k & 0\\\\\n0 & 0 & 1 & 0 & \\Delta t\\\\\n0 & 0 & 0 & \\alpha & 0\\\\\n0 & 0 & 0 & 0 & \\beta\n\\end{bmatrix}, B = \\begin{bmatrix}\n0 & 0\\\\\n0 & 0\\\\\n0 & 0\\\\\n1 - \\alpha & 0\\\\\n0 & 1 - \\beta\n\\end{bmatrix}.$\nIn the above equation, $\\Delta t$ is the time step, and the\nparameters $\\alpha$ and $\\beta$ are estimated from data. To generate\na linear approximation for the vehicle dynamics, we use an\nestimate of the vehicle's heading, $\\theta_k$, at future time steps."}, {"title": "B. Competency-Aware Path Planning", "content": "Given the vehicle's current position and goal, along with\nan image of the environment from the vehicle's perspective,\nthe path planner determines the best trajectory to take from a\nset of sampled paths. To obtain a diverse set of dynamically\nfeasible position paths, the planner samples action sequences\nwithin velocity control bounds, then predicts the vehicle's\npath under those actions using the estimated dynamics. The\nbaseline planner, without any competency-awareness, selects\nthe best path based on the progress it makes towards the goal\nand the vehicle orientation with respect to the goal position.\nWe develop five planners that utilize different competency\ninformation and strategies for responding to low competency.\n(1) The planner that uses only overall competency selects\nthe best trajectory in the same way as the baseline planner\nwhen overall competency is high. If the competency score\nfalls below some probability threshold, the planner resorts\nto a safe action sequence, backing up slightly then turning.\n(2) The turning-based planner that utilizes only regional\ncompetency information checks if regions in the vicinity of\nthe robot are associated with low perception model compe-\ntency. If those regions fall below some probability threshold,\nthe vehicle backs up slightly and turns away from low\ncompetency regions. (3) The turning-based planner that uses\nboth overall and regional competency checks both measures\nof competency before resorting to a safe response. (4) The\ntrajectory-based planner that uses only regional competency\ninformation maps sampled paths onto the regional com-\npetency map, determines the minimum competency value\nassociated with each path, and removes those that fall below\nthe threshold. The planner will then select the best path\namong those remaining. If there is no safe path, the vehicle\nwill back up slightly and turn away from the low competency\nregions. (5) The trajectory-based planner that utilizes both\noverall and regional competency behaves similarly but only\nconsiders regional competency when overall competency is\nlow. Three examples of this trajectory-based planner with\nfull competency-awareness are shown in Figure 4."}, {"title": "C. Path-Tracking Controller", "content": "To follow the paths generated by the competency-\naware path planner, we develop a reference-tracking linear-\nquadratic regulator (LQR) using the linearized model of\nvehicle dynamics from Section V-A. The reference state and\ninput are obtained from the competency-aware path planner."}, {"title": "D. Evaluation of Navigation Performance", "content": "We evaluate the navigation capability of the full percep-\ntion, planning, and control pipeline (Figure 1) with varying\nlevels of competency awareness and different responses\nto low competency. We compare (1) a baseline controller\nthat uses no competency information, (2) a turning-based\ncontroller that uses only overall competency scores, (3) a\nturning-based controller that uses only regional competency\nmaps, (4) a trajectory-based controller that uses only re-\ngional competency information, (5) a turning-based con-\ntroller that uses both overall competency scores and regional\ncompetency maps, and (6) a trajectory-based controller that\nuses both overall and regional competency information."}, {"title": "VI. CONCLUSIONS", "content": "We aim to improve the safety of a UGV navigating accord-\ning to a perception-based system in a complex environment\nwith obstacles that are unfamiliar to the perception model.\nWe believe that integrating information about the predictive\nuncertainty of the perception model into the navigation\nscheme will improve the safety and performance of the sys-\ntem. With this idea, we propose a novel overall competency\nscore that can distinguish between correctly classified ID"}, {"title": "VII. LIMITATIONS & FUTURE WORK", "content": "While the integration of competency-awareness into the\nnavigation framework greatly reduces the number of colli-\nsions with unfamiliar obstacles, collisions do still occur for\na couple reasons. The field-of-view of the vehicle's front-\nfacing camera is quite limited, and the vehicle currently has\nno visual memory, so it will collide with obstacles that it\ncannot see. It would be useful for the vehicle to maintain\nsome memory of competency estimates for nearby regions\nthat are no longer visible. The competency estimators are\nalso imperfect, and errors in competency estimates carry\nthrough the perception, planning, and control pipeline, which\ncan result in collisions. It would be interesting to explore how\nthese errors propagate through the navigation pipeline.\nThere are also various other avenues for future work.\nFirst, there is currently an implicit assumption that spatial\nfeatures of an image, corresponding to unfamiliar obstacles,\ncause the reduction in perception model competency. It\nwould be useful to develop a more general framework for\nunderstanding model competency in cases where predictive\nuncertainty is not due to some particular region in the\nimage/environment. Second, the competency estimates are\ncurrently formulated for classification problems. One could\ngeneralize these formulations to be used for other tasks.\nIn addition, this work focused on integrating competency-\nawareness into a local planning and control framework.\nIt would be interesting to develop a global planner with\ncompetency-awareness. Finally, the current response to per-\nception model competency is to avoid regions associated\nwith low competency. To enable continual learning within\na dynamic or unfamiliar environment, it would be useful\nto intentionally and cautiously explore unfamiliar regions,\nrather than simply avoid them. It would be interesting to\nconsider methods for safe exploration and decision frame-\nworks to trade off between avoidance and exploration."}, {"title": "APPENDIX", "content": "In this section, we provide some additional details on the\nimplementation of our competency-aware navigation scheme."}, {"title": "A. ADDITIONAL IMPLEMENTATION DETAILS", "content": "Competency-Aware Path Planning: The path planner\nsamples N action sequences within the velocity control\nbounds [Umin, Umax] over a time horizon of H, then esti-\nmates the state of the vehicle at future time steps under each\naction sequence (based on the dynamics model from Section\nV-A). It then removes paths that exit the field-of-view of\nthe vehicle. The baseline planner, without any competency-\nawareness, selects the best path among the sampled trajecto-\nries based on the progress it makes towards the goal and the\nvehicle orientation with respect to the goal. Let 20:H be the\nstate of the vehicle across the entire planning horizon and\nIgoal be the (x, y) goal position. The quality of a sampled\npath can be summarized by the following cost function:\n$C(2_{0:H}, l_{goal}) =$\n$\\alpha_{ori}\\cdotarctan\\frac{\\hat{x}_{goal}[1] - l_{H}[1]}{\\hat{x}_{goal}[0] - l_{H}[0]} - l_{H}[2]|^{2}$\n$+ \\alpha_{goal-x}|\\hat{x}_{goal}[0] - l_{H}[0]|$\n$+ \\alpha_{goal-y}|\\hat{x}_{goal}[1] - l_{H}[1]|$.\nThe best path among those sampled minimizes this\ncost function. The weighting parameters (\u03b1ori, 2goal, and\nAgoaly) are tuned to get good planning performance, and the\nchosen values are provided in Appendix B.\nFor the trajectory-based path planners that utilize regional\ncompetency information, we project the estimated position\npaths onto the current image representing the vehicle's\nenvironment. We evaluate the minimum competency corre-\nsponding to the space occupied by the vehicle at each time\nstep along the trajectory and remove paths whose minimum\ncompetency is below the competency threshold. We then\nselect the sampled path with the minimum cost among the\nremaining paths. If there are no safe trajectories, the vehicle\nresorts to a safe action response."}, {"title": "Path-Tracking Controller:", "content": "To follow the paths gener-\nated by the competency-aware path planner, we develop a\nreference-tracking linear-quadratic regulator (LQR). We seek\nthe control inputs \u0e19\u0e35, \u0e19\u0e351,..., \u0e19\u0e35N-1 that minimize\n$\\sum_{k=0}^{N-1}(x_{k} - x_{ref})^{T}Q(x_{k} - x_{ref}) + (u_{k} - u_{ref})^{T}R(u_{k} - u_{ref})$\nconstrained by the initial state of the vehicle and the lin-\nearized dynamics (Section V-A). The reference state, $\\hat{x}_{k}^{ref}$,\nand reference input, $\\hat{u}_{k}^{ref}$, are obtained from the competency-\naware path planner (Section V-B). The parameters of the state\ndeviation cost matrix, Q, and input deviation cost matrix, R,\nare tuned to get good performance, and the chosen values are\nprovided in Appendix B. Solving this optimization problem,\nwe find the optimal input at time step k to be\n$u_{k} = -K_{k}(x_{k} - x_{ref}) + u_{ref}$, where\n$K_{k} = (R+ B^{T} P_{k+1}B)^{-1}B^{T}P_{k+1}A_{k}$\n$P_{k} = Q+ K_{k}^{T}RK_{k}+(A_{k}-BK_{k})^{T}P_{k+1}(A_{k}-BK_{k})$.\nNote that PN is simply given by the state cost matrix, Q."}, {"title": "B. PARAMETERS FOR EVALUATION", "content": "In Table II, we provide some of the parameter values cho-\nsen to evaluate our competency estimation method (Section\nIV) and competency-aware navigation scheme (Section V).\nAdditional information about a number of these parameters\nis provided in our configurations README."}, {"title": "C. ANALYSIS OF OVERALL COMPETENCY SCORE", "content": "In this section, we provide additional analysis of our over-\nall model competency estimation method (Section IV-A), in\ncomparison to existing methods for uncertainty quantification\n(Section II-A) and OOD detection (Section II-B).\nBaselines: We compare our overall model competency\nestimation method against various existing methods for quan-\ntifying uncertainty and detecting OOD inputs. In particular,\nwe compare our approach to the Maximum Softmax Proba-\nbility (MSP) baseline, the calibrated MSP with Temperature\nScaling [3], the Entropy of the softmax probabilities, Monte\nCarlo (MC) Dropout [11], Ensembling [12], ODIN [15],\nthe Energy Score [17], KL-Matching [59], OpenMax [60],"}]}