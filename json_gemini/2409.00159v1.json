{"title": "LLMs hallucinate graphs too: a structural perspective", "authors": ["Erwan Le Merrer", "Gilles Tredan"], "abstract": "It is known that LLMs do hallucinate, that is, they return incorrect information as facts. In this paper, we introduce the possibility to study these hallucinations under a structured form: graphs. Hallucinations in this context are incorrect outputs when prompted for well known graphs from the literature (e.g. Karate club, Les Mis\u00e9rables, graph atlas). These hallucinated graphs have the advantage of being much richer than the factual accuracy or not of a fact; this paper thus argues that such rich hallucinations can be used to characterize the outputs of LLMs. Our first contribution observes the diversity of topological hallucinations from major modern LLMs. Our second contribution is the proposal of a metric for the amplitude of such hallucinations: the Graph Atlas Distance, that is the average graph edit distance from several graphs in the graph atlas set. We compare this metric to the Hallucination Leaderboard, a hallucination rank that leverages 10,000 times more prompts to obtain its ranking.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) recently attracted a lot of attention, thanks to sustained research efforts and a large spectrum of envisioned applications. This triggers a growing demand for tools to test and analyze these expensive to set up and complex objects. In particular, methods to efficiently identify, differentiate, watermark LLMs, and methods to compare their accuracy and notably the potential presence of hallucinations [15,20,19] are devised. The common denominator of these methods is the will to efficiently extract information from the LLM under scrutiny.\nTo achieve this information extraction, one can distinguish two broad categories of approaches: white box approaches that rely on exploiting a privileged access to the model internals (e.g. probits, activation patterns, source code, model hyperparameters and weights), and black box approaches that merely allow an auditor to interact with that model. White box approaches provide precise answers, but are not always available, for instance to an external auditor assessing a closed source LLM. In such cases, black box approaches are necessary, and constitute the focus of this paper."}, {"title": "2 The Topologies of LLM Graph Hallucinations", "content": "Our aim is to prompt LLMs with famous graphs for which the ground truth topology is known and available online (see e.g. on repositories such as SNAP [14] or KONECT [13]), and consequently most likely part of the training of these LLMs [10]. We chose three graphs in that regard, as ground truth: the Zachary's karate club graph (coined KC hereafter) and Les Mis\u00e9rables (coined LM, see their composition e.g. in the NetworkX library [8]) and the 50th graph of the graph atlas [18]."}, {"title": "Prompts to LLMs", "content": "Prompts to LLMs, in order to obtain a graph structure, are in the following form: Provide me the so called \"X\" graph as a python edge list; print it, with X being the graph of interest, for instance the \"Zachary's karate club\", \"Les Mis\u00e9rables\" or Provide me with graph # from the Graph Atlas, as a python edge list; print it, with #the graph number in case of requests concerning the graph atlas. The request for Python structured responses is because of the NetworkX library [8] we use to instantiate and analyze these graphs."}, {"title": "Outputs by LLMs", "content": "Following such prompts, the returned payload is most often in the form of a list such as (1,2), (1,3),... (see Section A for an example); that edge list is then parsed and built as a NetworkX graph (undirected ones in alignment with our ground truth graphs in this paper). As these ground truth graphs are also present in the NetworkX library, comparison is convenient. We note that incomplete outputs (i.e. incomplete responses leading to a partial edge list) are nevertheless examined. We report that a small fraction of queried LLMs refused return an edge list, as they claim not having access to the data for instance (as also noted by the Hallucination Leaderboard project [4]); some also prefer to provide Python code to print the queried graph using a library: these models are discarded from our study."}, {"title": "Prompted LLMs", "content": "The simplicity and low volume of necessary requests (prompts) enables a full online experience (as opposed to model downloading or API interfacing), using platforms that place LLMs in prompting access via a web browser. We leveraged the following platforms in this paper: Mistral [7], Vercel AI SDK [1], HuggingChat [5], ChatGPT [2], together.ai [9] and Google's Gemini [3], with the default parameters the platforms set for their hosted models."}, {"title": "Comparing resulting graphs to the ground truth", "content": "In this paper we perform topological comparisons, with no consideration for the labels of nodes returned by the LLMs. These are varying significantly; in consequence a hard label matching would discard output graphs that are nevertheless topologically close. We leave a study on the labeling mismatch to future work."}, {"title": "2.1 Statistics on the Topology of Output Graphs by LLMs", "content": "We report in detail the prompting of 21 LLMs against the Zachary's karate club graph (KC); some statistics regarding Les Mis\u00e9rables and graph atlas 50 are deferred in Appendix B. As each prompt to a LLM results in a graph, one can directly perform topological comparison between the ground truth and a LLM output graph; such a comparison is represented in Figure 1: Figure 1a presents the raw output graph; Figure 1b presents the graph intersection of the KC graph with the output graph (from Figure 1a); Figure 1c presents the hallucinated edges (i.e. edges not present in KC but present in the output graph); finally Figure 1d presents the edges that are forgotten in the output graph as compared\nto KC, while prompting gpt4o. We note that the result provided by this LLM is relatively accurate in comparison to others, as we shall now see in Table 1.\nWe denote the set V of nodes and E of edges in each examined graph. We name output graphs in relation with their LLM in the first column (in Table 1 the first row being the KC ground truth graph), and provide 6 relevant statistics for assessing their quality\u00b3. From left to right, we list the number of nodes |V| in the output graph, its number of edges |E|, its density, assortativity (tendency of\n\u00b3 We note that the graph edit distance is not included in these metrics as already intractable for sizes of around 34 nodes as in the output graphs we deal with; this metric is leveraged in Section 3 as we there deal with smaller graphs. We present an alternative and more scalable distance metric in Appendix C."}, {"title": "3 ALLM Hallucination Rank Based on Output Graphs", "content": "Benchmarks for measuring hallucinations are constituted of tens of thousand of prompts [4,17]. They hence require a privileged access to the model, compared to web browser based prompting access. We here propose to sequentially prompt a LLM for just a handful of graphs from the graph atlas, and to average errors made regarding this reference under a single value (using the average error of graph edit distances).\nThe graph atlas is composed by 1252 different graphs; in our experiment, we choose a resolution of 5 graphs to be prompted, in particular the first 5 connected graphs (namely graphs #3,#6, #7,#13 and #15). We then compute the exact edit distance of each of these ground truth graphs against their respective LLM output. We finally average these 5 distances to obtain the final distance score for the queried LLM. We coin this distance the Graph Atlas Distance (GAD).\nWe note that across all tested LLMs and for all datasets, an isomorphism solely occurred for gpt4o on graph atlas #7 (a triangle graph) and #13 (a star composed of 4 nodes). Regarding the weight we give to operation in the\ngraph edit distance, we do not account for labels, but consider node/edge insertion/deletion each costing 1.\nWe compare against the Hallucination Leaderboard [4], a GitHub page ranking LLMs based on the amplitude of their hallucinations, using a dataset of 50k prompts. The rank of the ten tested graphs in common with the Hallucination Leaderboard are presented in Table 4.\nWe can observe an interesting correlation in these two rankings (with a spearman rank correlation of 0.3, where 0 is random). The first position is held by gpt4o. The first of the 4 llama models (llama-3.1-70B-Instruct) is down-ranked in GAD, yet the 3 others are in the correct order. The larger 405B parameters llama perform worse than smaller models; note that this inversion also appears in [4], where llama 3 beats llama 3.1 with the same amount of parameters (70B). qwen2-72B-Instruct ranks in the middle. c4ai-command-r-plus ranks at the bottom. snowflake-arctic-instruct is nevertheless strongly down-ranked with GAD, as compared to its second position in the Hallucination Leaderboard.\nIn the light of the only 5 graphs prompted to judge a LLM, and considering the 50k prompts in the Hallucination Leaderboard, we find this result to be encouraging for further study on the discriminative power of querying LLMs for structured data such as graphs."}, {"title": "4 Discussion", "content": "Reasoning, overfitting or hallucinating So far, no LLM managed to consistently output the ground truth graphs. This might be considered as hallucinations, a sign that the LLMs handling of structures still needs to be improved. What is required from LLMs is yet not the overfitting and storage of input data, but rather to generalize from the provided examples. In this light, witnessing large blobs of (edgelist) text correctly returned by the LLM might be interpreted as an additional metric for overfitting.\nInterestingly, the capacity to produce answers to a MCQ dataset like PiQa is interpreted as reasoning - rather than again overfitting -. In essence, reasoning\ncould produce Les Mis\u00e9rables graph provided the original novel and the edge definition (chapter co-occurrence).\nGathering raw information, a meaningful abstraction ? Our approach explores graph requests as a mean to obtain more information bits by prompt, compared to the standard MCQ approach naturally in which each request yields a number of bits capturing the number of possible answers to each question (typically 2 or 4 choices, hence 1 or 2 bits).\nImplicitly, this approach hence compares interrogation patterns \"bit to bit\", regardless of the relevance of the collected bits. A natural limitation is to overlook the precise meaning of each bit. Concretely, for instance, one bit yielded by our approach captures whether the target LLM correctly predicted a (Jean Valjean, Cosette) relation in Les Mis\u00e9rables, whereas one bit yielded by MedMCQA benchmarks captures whether the target LLM correctly associates \"A 40-year-old man has megaloblastic anemia and early signs of neurological abnormality\" to a deficit in B12 Vitamin.\nThe difficulty with considering the semantics associated with each bit is that the notion of relevance is strongly application dependent. The relevance varies depending on the use case, be it for fingerprinting a target LLM or for evaluating its ability to provide truthful medical advice."}, {"title": "5 Related Work", "content": "Graphs are already used in various ways regarding the hallucination [15,20,19] issue, in order to assess properties or judge on the quality of outputs from LLMs.\nKnowledge graphs are leveraged in a LLM-based hallucination evaluation framework [19], by prompting for text and checking the correctness of the output having a binary labeled dataset at hand. Knowledge graphs are constructed from unstructured textual data by identifying the set of entities within the text and the relationships between them, resulting in a structured representation. Work in [11] models social networks as graphs in order to simulate information spreading in order to track LLM hallucinations flowing within these networks. Nonkes et al. [16] create a graph structure that connects generations that lie closely in the embedding space of hallucinated and non-hallucinated LLM text generations. Graph Attention Networks then learn this structure and generalize it to unseen generations for categorization. Work in the complex networks community [12] is interested in extracting organizational networks (bipartite user-activity networks here) from raw text obtained from LLMs, in the context of standardized individual-level contributions across a large numbers of teams participating to a competition for instance.\nTo be best of our knowledge, our work is the first to propose a head-to-head comparison of a ground truth graph to a prompted output graph given by a LLM under scrutiny, allowing for new assessments."}, {"title": "6 Conclusion", "content": "This paper made the case for comparing ground truth graphs from the literature to the output of LLMs, prompted to output them. A first striking observation is that current LLMs are far from being a reliable source of edge lists (i.e. they do hallucinate graphs). Nevertheless, these glitches open solid comparison avenues, which we exploited to observe the significant differences of LLMs in their hallucination amplitude regarding standard graphs. Introducing relevant metrics, we have illustrated that a handful of prompts to a LLM can correlate with a method leveraging tens of thousands of queries for binary answers, in the capability to rank the less hallucinating LLMs. We believe that such a novel perspective leaves ways for future work to constitute robust and more efficient benchmarks for assessing the quality of LLMs."}, {"title": "spectral distance", "content": "spectral distance [22], defined as follows:\n$d(G, G') = \\sqrt{ \\sum_{i}(s_i \u2013 s'_i)^2}$,\nwith s the set of eigenvalues $s = {\\lambda_1, \\lambda_2,...,\\lambda_{|v|}},$ knowing that $\\lambda_1 \\le \\lambda_2 \\le ... < \\lambda_{|v|}$. As recommended in [22], if graphs are of different sizes, the missing eigenvalues of the smaller are zeros padded.\nWe report the spectral distances of LLMs to the KC graph in Table 4."}]}