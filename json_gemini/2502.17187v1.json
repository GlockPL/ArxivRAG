{"title": "Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks", "authors": ["Andrei Chernov"], "abstract": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers have gained significant attention. Currently, state-of-the-art LLMs utilize this architecture. There is a substantial amount of research on how to train such models and how to select hyperparameters for this architecture. However, there is a lack of studies focusing on post-evaluation analysis of MoE layer properties. In this paper, we take a first step toward closing this gap by evaluating expert contributions on the quiz-based MMLU benchmark. We show that most experts were never activated during inference on this benchmark. Additionally, the output distribution of gating networks is much closer to uniform than sparse. Finally, we demonstrate that the average performance of some experts within the same layer varies significantly.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers, instead of fully dense layers, have gained popularity (Du et al., 2022; Wan et al., 2023). Currently, one of the best-performing models utilizes this architecture (Liu et al., 2024). The main reason MoE models are preferred over dense models is that they tend to achieve similar performance while activating significantly fewer parameters, thereby reducing training time compared to dense LLMs (Muennighoff et al., 2024).\nMost research on MoE in the natural language processing (NLP) domain has focused on either modifying the architecture to speed up inference such as the Top-K gating mechanism (Shazeer et al., 2017), which selects only the top-K experts with the highest probabilities or adjusting the training loss to prevent the gating networks from always activating only a small subset of experts (Shazeer et al., 2017; Shen et al., 2024).\nIn this paper, we focus on post-evaluation analysis of expert contributions to final predictions. Specifically, we evaluate the pretrained OLMOE model\u00b9 (Muennighoff et al., 2024) on the quiz-based MMLU benchmark (Hendrycks et al., 2020) to address the following questions:\n\u2022 How many experts were activated at least once during inference on this benchmark?\n\u2022 What does the distribution of gating network outputs look like? Does it tend to be sharp or closer to uniform?\n\u2022 Do all experts perform equally in terms of accuracy?"}, {"title": "2 Experimental Setup", "content": "In this paper, we investigate the contribution of each expert in the OLMOE model during inference on the MMLU benchmark. MMLU is a quiz-based benchmark that evaluates the knowledge and reasoning abilities of large language models (LLMs). It consists of 57 datasets covering various domains, such as humanities, STEM, social sciences, and other fields.\nWe did not observe a significant difference in expert contributions across different domains. Therefore, in the results section (Section 3), we present results aggregated over all datasets, comprising a total of 14, 042 questions.\nFor each question, the benchmark requires a model to select the correct answer from four possible choices: A, B, C, and D. Thus, the model needs to generate only one token corresponding to an answer. To assess the contribution of experts, we store the probabilities (alphas) from the gating network for each MoE layer when the model predicts the token corresponding to the correct answer."}, {"title": "3 Results", "content": "3.1 Distribution of Activated Experts\nIn this section, we analyze how many experts were activated during inference, as well as the normalized distribution of activated experts for each datapoint. Tables 1 and 2 report the number of experts that were activated for at least one datapoint.\nConsidering that the total number of experts is 64, we observe that more than 60% of the experts were never activated for the entire MMLU dataset. Additionally, we report the mean and standard deviation of natural entropy (Conrad, 2004), defined as:\n$E = - \\sum_{i \\in top 8} p_i \\log p_i$ (1)\nwhere $p_i$ represents the normalized distribution over the highest gating probabilities, i.e.,\n$p_i = \\frac{a_i}{\\sum_{i \\in top 8} A_i}$ (2)\nwhere $a_i$ is the output from the gating network. We use natural entropy as a measure of uncertainty. It converges to zero when one expert has a probability close to 1, meaning that only this expert contributes to the result. Conversely, when the distribution is uniform, entropy reaches its maximum value. Specifically, for a discrete distribution with 8 outcomes, the highest entropy value is 2.0794.\nBased on the reported entropy in the tables, we conclude that the distribution for each expert is far from sparse and instead tends to be closer to uniform. We believe this behavior is likely caused by auxiliary losses during the training procedure, which force the model to activate each expert approximately the same number of times. This prevents the model from converging to a small subset of preferred experts, thereby ensuring that all experts remain utilized. However, as our results suggest, this may lead to a gating probability distribution that is close to uniform, which might not be desirable. These results also hold for the distribution across all 64 experts (see Appendix A).\nA hypothesis that we believe is worth validating in future work is whether this uniform-like behavior negatively impacts the model's robustness. The primary concern is that the Top-K activation approach is not smooth. If the gating outputs follow a nearly uniform distribution, small changes in input may lead to significant differences in output due to a different set of experts being activated. Even if only the last expert in the top K differs, this could still cause noticeable variations. As shown in the Table 3, the weight of the eighth expert is significant, averaging 8.74%. This observation motivated us to investigate the average accuracy of each expert (see Section 3.2).\nAdditionally, an unexpected result for us is that entropy tends to increase from the first to the last layer. The first layer has the lowest entropy, while the last layer has one of the highest entropy. Intuitively, we expected the opposite: the last layer should be more confident in its predictions. One possible explanation is that some benchmark questions are too complex for the model, leading to less confident predictions. However, the standard deviation of entropy is low, indicating that the distribution remains stable across all questions, regardless of their complexity."}, {"title": "3.2 Accuracy of each Expert", "content": "In Section 3.1, we showed that the output distribution from the gating function is closer to uniform rather than sparse. This means that the contribution of each expert among the top 8 is significant to the final outcome. In this section, we investigate whether all experts have similar accuracy or not.\nTo achieve this, we compute the accuracy of each expert over all test data points where the expert was activated. Since an expert may contribute to different questions with varying weights, we also report the accuracy weighted by the probability assigned to each expert. Specifically, the weighted accuracy for expert $j$ is defined as:\n$\\sum_{i=1}^{n} \\alpha_{ij} \\cdot 1(\\hat{y}_i = Y_i)$\\n$\\Sigma_{i=1}^n \\alpha_{ij}$ (3)\nwhere $\\alpha_{ij}$ represents the probability assigned to expert $j$ for datapoint $i$, and $1(\\hat{y}_i = Y_i)$ is an indicator function that equals one when the final prediction is correct and zero otherwise.\nAdditionally, we report the average contribution weight, computed as 100 pi from Equation 2, for each expert when it was activated. Results are presented for the first MoE layer (Table 4) and the last MoE layer (Table 5). In these tables, we include only experts that were activated in at least 1% of the data (column: \"Appearances\"). There are 12 such experts in the first MoE layer and 17 in the last one.\nFor the first MoE layer, 7 experts were activated in nearly all cases, meaning they appeared in more than 95% of the data. The top eight experts were mainly chosen from three experts with indices:\u00b3 19, 26, and 52. However, the accuracy of these experts varies significantly.\nFor the last MoE layer, only 3 experts were activated in more than 95% of the cases, providing the gating network with more flexibility in selecting different experts. In terms of accuracy, we observe a similar pattern to the first MoE layer: some experts achieve significantly higher accuracy than average (e.g., expert 12), while others perform considerably worse (e.g., experts 34 and 30).\nThese findings suggest that a potential direction for future research could be adjusting the gating output probabilities by increasing the probability for high-accuracy experts and/or decreasing it for underperforming experts. This is particularly relevant given that the gating probability distribution is nearly uniform (see Section 3.1). This uniformity implies that the probability difference between high-accuracy experts and the top eight expert is relatively small. For instance, in the last MoE layer, the average gating function output for expert 12, which performs significantly better than the average, is 0.0291, while the average unnormalized probability for the top eight experts is 0.0317."}, {"title": "4 Conclusion", "content": "In this paper, we evaluated the contribution of experts in an LLM MoE model to the final output on a quiz-based benchmark. Our key findings are:\n\u2022 More than 60% of experts were never activated during prediction. This implies that for quiz-based tasks, inactive experts can be removed, making the model smaller without any loss in performance. Additionally, this can significantly reduce training time during fine-tuning.\n\u2022 The distribution of gating outputs is not sharp but rather nearly uniform across all MoE layers. Moreover, entropy does not decrease from the first layer to the last. Given that most LLM MoE models use a Top-K gating mechanism, which is a non-continuous gating method, this behavior may negatively impact the robustness of the models.\n\u2022 Some experts perform better on average than others, suggesting that adjusting the gating output to prioritize high-accuracy experts could lead to performance improvements."}, {"title": "Limitations", "content": "The main limitation of this short paper is that the experiment was conducted on only one model and one benchmark. Our primary focus was on quiz-based datasets, and we believe that the MMLU benchmark represents this category well. Therefore, the use of a single benchmark is not a major limitation. However, a more significant limitation is that we evaluated only one LLM MoE model. We acknowledge that these results may not generalize to other LLM MOE models.\nThe primary reason for using only one LLM MoE model is that most other models have a significantly larger number of parameters and require substantially more computational resources for inference, which we currently do not have."}, {"title": "A Entropy of distribution across all Experts", "content": "In Table 6 and Table 7, we show that all statements regarding entropy across the top 8 experts in Section 3.1 also hold for entropy across the probabilities of all 64 experts given by the gating networks. Note that entropy generally increases with the number of possible outcomes, and for 64 possible outcomes, the upper bound is 4.1589."}]}