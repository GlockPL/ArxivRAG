{"title": "Evolutionary Neural Architecture Search for 3D Point Cloud Analysis", "authors": ["Yisheng Yang", "Guodong Du", "Chean Khim Toa", "Ho-Kin Tang", "Sim Kuan Goh"], "abstract": "Neural architecture search (NAS) automates neural network design by using optimization algorithms to navigate architecture spaces, reducing the burden of manual architecture design. While NAS has achieved success, applying it to emerging domains, such as analyzing unstructured 3D point clouds, remains underexplored due to the data lying in non-Euclidean spaces, unlike images. This paper presents Success-History-based Self-adaptive Differential Evolution with a Joint Point Interaction Dimension Search (SHSADE-PIDS), an evolutionary NAS framework that encodes discrete deep neural network architectures to continuous spaces and performs searches in the continuous spaces for efficient point cloud neural architectures. Comprehensive experiments on challenging 3D segmentation and classification benchmarks demonstrate SHSADE-PIDS's capabilities. It discovered highly efficient architectures with higher accuracy, significantly advancing prior NAS techniques. For segmentation on SemanticKITTI, SHSADE-PIDS attained 64.51% mean IoU using only 0.55M parameters and 4.5GMACS, reducing overhead by over 22-26X versus other top methods. For ModelNet40 classification, it achieved 93.4% accuracy with just 1.31M parameters, surpassing larger models. SHSADE-PIDS provided valuable insights into bridging evolutionary algorithms with neural architecture optimization, particularly for emerging frontiers like point cloud learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks have achieved remarkable success across diverse applications, largely attributed to their multi-layered architecture enabling hierarchical feature learning [1]\u2013[5]. However, the design of network architectures relies heavily on expert experience through extensive experiments. This manual process is computationally expensive and time-consuming due to the need to train and assess a vast number of configurations. To address this challenge, neural architecture search (NAS) has emerged as a promising technique to automate neural network design. The key idea is to search for an optimal architecture within a predefined search space by using an optimization algorithm that searches for candidate architectures [6]. NAS methods have discovered novel architectures surpassing hand-designed networks, including NASNet for image classification.\nPoint clouds captured by LiDAR and depth sensors are increasingly ubiquitous, but their unstructured nature poses difficulties for standard deep learning methods designed for grid data. Hence, applying NAS to emerging domains like point cloud processing remains an open challenge. While specialized point-based networks have been proposed, their manual architecture engineering is onerous. This provides strong motivation for investigating NAS tailored to automate point cloud network design.\nEvolutionary algorithms provide an alternative and efficient search strategy by simulating a population of neural architectures that evolve for survival and breeding while solving the learning tasks. Popular methods include genetic algorithms, evolutionary strategies, genetic programming, differential evolution, particle swarm optimization, and ant colony optimization. In particular, differential evolution (DE) through adaptive control has shown promise in balancing exploration and exploitation [7], [8], leveraging additional historical evaluations to guide the search in large search spaces.\nIn this work, we (i) present a NAS framework driven by a proposed adaptive DE algorithm, Success-History-based Self-adaptive Differential Evolution (SHSADE) with (ii) a Joint Point Interaction Dimension Search (PIDS) [9] to (iii) fit geometric dispositions and density distributions in varying 3D point clouds. The hybridized SHSADE-PIDS enables optimized point cloud architectures specialized for tasks like segmentation and classification by encoding discrete deep neural network architectures to continuous spaces and performing searches in the continuous spaces. Comprehensive experiments demonstrate SHSADE-PIDS's optimizer capabilities and advantages in accelerating NAS. The results provide insights into effectively applying adaptive evolution strategies for architecture search, particularly in novel frontiers involving unstructured 3D data."}, {"title": "II. RELATED WORKS", "content": "Neural architecture search (NAS) has emerged as an important field of automated machine learning, aiming to automate the design of optimal neural network architectures tailored to specific tasks and datasets [10]. The key idea behind NAS is to define a search space encompassing possible network architectures and utilize an optimization algorithm to actively navigate this space to find high-performing architectures for a given task. The search algorithm trains and evaluates candidate architectures within the space to guide the search toward optimal solutions based on performance metrics on a validation set. By effectively exploring the vast search space guided by such empirical performance feedback, NAS methods can discover novel architectures surpassing human-designed counterparts specialized for the dataset and application.\nEvolutionary algorithms have emerged as a particularly promising approach to neural architecture search due to their population-based search mechanism, which is naturally suited for exploring the vast combinatorial spaces of neural architectures. Notable evolutionary NAS techniques include NeuroEvolution of Augmenting Topologies (NEAT) [11], Genetic CNN [12], and more recently AmoebaNet [13] which leverages evolution strategies for architecture search. Compared to reinforcement learning-based NAS methods, evolutionary approaches have been shown to discover highly competitive architectures with greater stability, lower computational requirements, and less hyperparameter tuning. As such, evolutionary algorithms provide an efficient and powerful search strategy to automate neural architecture optimization without prohibitive computational costs."}, {"title": "B. Differential Evolution Algorithm", "content": "Differential Evolution (DE) is an evolutionary optimization algorithm designed to search continuous solution spaces for global optima. First proposed by Storn and Price in 1995, DE operates on real-valued solution vectors that are evolved over generations through bio-inspired mutation, crossover, and selection operators. The key advantage of DE is its self-adaptive mutation scheme that automatically adjusts the step size and direction during optimization based on differences between randomly selected population members. This allows DE to effectively balance the exploration and exploitation of complex problems.\nA key limitation is DE's sensitivity to fixed control parameters, scale factor F, and crossover rate CR, which require problem-specific tuning. Since its introduction, numerous enhancements to DE have been developed, including the Success-History based Adaptive Differential Evolution (SHADE) algorithm. SHADE [14] enhances adaptation through historical memories of CR and F that record statistics of successful F and CR values over multiple generations. Weighted average adaptation has a higher influence on more successful solutions. By utilizing memories to guide reliable tuning, SHADE improves optimization speed, accuracy, and robustness compared to fixed parameter DE.\nSelf-Adaptive DE (SADE) addresses this by adapting F and CR during execution based on their relative success over a learning period [15]. However, SADE's adaptation scheme has limitations regarding sensitivity to randomness and differentiation between moderately and highly successful values."}, {"title": "C. Deep learning methods for point cloud processing", "content": "Point clouds have become a prevalent 3D data representation, enabling detailed digitization of object shapes and environments through dense geometric sampling. However, point clouds present unique challenges for applying deep neural networks designed for grid-structured data like images. Point clouds are unordered, unstructured, and lack inherent topology. Pioneering deep learning approaches that directly process raw point clouds while respecting their permutation invariant and unstructured nature have emerged in recent years.\nEarly deep learning methods converted point clouds to intermediary representations like voxels or multi-view projections to leverage established 3D and 2D convolutional architectures. However, these conversions introduce quantization errors and lose fine geometric details. PointNet [16] pioneered direct feature learning on raw point sets using shared multilayer perceptrons (MLPs) and global max pooling for permutation invariance.\nVarious specialized point convolution operators have also been introduced to improve local feature learning, such as continuous convolutions in Kernel Point Convolution (KPConv) [17]. Graph neural networks are gaining interest by inherently modeling connectivity in points through techniques like graph pooling and attention [18]. Overall, point-based deep networks now achieve results by directly operating on unstructured point clouds without conversions.\nKPConv is a pioneering point convolution operator using a parameterized kernel with learnable deformations to achieve geometry-adapted feature extraction. By representing kernels as sets of points with influence defined by distance correlations, KPConv enables flexible and effective point cloud feature learning. Networks using KPConv layers have achieved excellent performance on shape classification, part segmentation, and scene analysis benchmarks. The intuitiveness of learning convolutions directly on raw point sets and the deformable nature of KPConv have made it an indispensable building block for many point-based deep-learning architectures."}, {"title": "D. Joint Point Interaction Dimension Search", "content": "Point clouds provide detailed 3D geometric representations captured by sensors like LiDARs. However, their unstructured nature poses challenges for standard deep-learning techniques designed for grid data [19], [20]. Point-based deep neural networks have been proposed to process raw point clouds while retaining geometric details directly. However, manually designing optimal architectures for this domain remains difficult.\nRecent works have proposed various strategies to design point operators, which are the basic building blocks of hierarchical 3D models, to extract features from point clouds. These strategies include point convolutions, graph neural networks, attention mechanisms, and kernel point convolutions [21]. However, existing approaches have limitations. First, they manually design a single type of point interaction and reuse it within all point operators of a 3D model, which may limit performance due to varying geometric/density distributions of points. Second, they optimize point interactions and dimensions separately, missing the opportunity to find better combinations on the two axes. To address these issues, Zhang et al. introduced PIDS (Joint Point Interaction Dimension Search) [9], which jointly explores point interactions and dimensions in a large search space to craft 3D models that balance performance and efficiency for point clouds.\nSearch Space: PIDS is based on kernel point convolutions and introduces high-order point interactions to fit geometric dispositions and density distributions in varying 3D points. It also incorporates width, depth, and expansion ratio search components for efficient point dimensions. The joint search space over these heterogeneous axes enables the discovery of networks customized to geometric and density variations.\nPredictor Model: To enable efficient search, PIDS uses a Dense-Sparse Predictor to accurately model the heterogeneous search space. The predictor learns separate dense and sparse embeddings to handle the continuous and categorical components. A dot product enables cross-feature communication. This unified encoding improves the prediction of performance in the joint PIDS search space compared to standard predictors.\nBi-objective Neural Architecture Search: To achieve both high accuracy and efficiency, PIDS formulates neural architecture search as a bi-objective optimization problem [22], maximizing predictive accuracy while minimizing computational cost (FLOPs). Predictor models estimate accuracy and FLOPS to avoid expensive training. The bi-objective function balances accuracy versus efficiency to find optimal architectures with different trade-offs. By jointly exploring point interactions and dimensions in a large search space using an optimized predictor model and bi-objective search, PIDS can discover high-performing and efficient models specialized for processing 3D point cloud data."}, {"title": "III. METHODS", "content": "This section proposes a novel neural architecture search algorithm bridging discrete and continuous optimization, SHSADE-PIDS, which integrates techniques from SADE and SHADE, and describes its application in the 3D point cloud processing, illustrated in Fig. 1 and detailed in Fig. 2."}, {"title": "A. SHSADE", "content": "Differential Evolution (DE) is a popular evolutionary algorithm for real-parameter optimization. However, its performance depends significantly on properly setting control parameters like mutation factor (F) and crossover rate (CR). To overcome this limitation, this research proposes an enhanced DE variant called SHSADE, which adapts these parameters based on historical success.\nSHSADE integrates complementary adaptive techniques from two major differential evolution variants - Self-Adaptive Differential Evolution (SADE) and Success-History based Adaptive Differential Evolution (SHADE). This hybridization aims to enhance the algorithm's optimization capabilities by leveraging the strengths of both approaches.\nSpecifically, from SADE, SHSADE adopts the core idea of self-adaptively tuning key control parameters like crossover rate (CR) and mutation factor (F) for each individual solution based on learning from historical memories. SADE records successful CR values over a predefined learning period and samples new CR values for each individual from a normal distribution with the mean set to the historically successful mean. This enables CR to be adapted to match the problem landscape. A similar mechanism is used to allow the mutation strength F to be tuned based on past successful values. Additionally, SADE maintains probabilities for selecting between multiple mutation strategies based on their relative success rates. This allows the algorithm to automatically choose the more effective strategies."}, {"title": "(a) Historical Memories", "content": "Unlike DE, which uses fixed F and CR values, SHSADE maintains memories MCR and MF to store statistical summaries of successful parameter values from previous generations:\nMCR = [MCR1, MCR2, . . ., MCRH]\nMF = [MF1, MF2,..., MFH]\nwhere H is the memory size. Rather than raw values, these memories store the mean successful F and CR values from each of the past H generations (Eq. 1,2)."}, {"title": "(b) Adaptive Parameter Control", "content": "After each generation, the memories MCR and MF are updated with the mean successful F and CR values using a learning rate c. This allows gradual adaptation of the control parameters to suit the problem landscape."}, {"title": "(c) Trigonometric Mutation Strategy", "content": "The trigonometric mutation strategy [23], hereafter referred to as Strategy 4, introduces an efficient approach in evolutionary algorithms by generating a new trial vector through a weighted averaging process involving three distinct members of the population, with consideration given to their relative fitness. This method leverages geometric relationships among the selected individuals, utilizing the concept of centroid and linear combinations in the objective function space.\nConsider three distinct individuals $X_{r1,G}$, $X_{r2,G}$, and $X_{r3,G}$ randomly selected from the current population, where $r1 \\neq r2 \\neq r3 \\neq i$. The mutation for the i-th individual $X_{i,g}$ in generation G + 1 is given by:\n$V_{i,G+1} := \\frac{X_{r1,G} + X_{r2,G} + X_{r3,G}}{3} + (W_2 - W_1)(X_{r1,G} \u2013 X_{r2,G}) + (W_3 - W_2)(X_{r2,G} - X_{r3,G}) + (W_1-W_3)(X_{r3,G} - X_{r1,G})$\nwhere the weights $W_1$, $W_2$, and $w_3$ are defined as:\n$W_1 = \\frac{|f(X_{r1,G})|}{w'}$, $W_2 = \\frac{|f(X_{r2,G})|}{w'}$, $W_3 = \\frac{|f(X_{r3,G})|}{W}$ (7)\nand\n$w' = |f(X_{r1,G})| + |f(X_{r2,G})|+|f(X_{r3,G})|$ (8)\nThe methodology incorporates a perturbation vector that is a linear combination of the triangle's legs, with weights corresponding to the objective function values at the vertices. This perturbation can be conceptualized as shifts by the triangle's center (the donor) along each leg, with varying step sizes.\nThe weight terms $(w_2 \u2013 W_1)$, $(w_3 \u2013 w_2)$, and $(W_1 - w_3)$ serve a dual purpose. Firstly, they bias the movement along the triangle's legs from worse to better solutions based on the objective function. Secondly, they automatically scale the vector components in accordance with the differences in objective values among the individuals, thereby guiding the solution towards the most favorable of the three points. As such, the trigonometric mutation acts as a local search operator confined to the trigonometric region defined by the trio of individuals.\nTo illustrate this, consider a two-dimensional minimization problem with individuals $X_{r1,G}$, $X_{r2,G}$, and $X_{r3,G}$. The boundaries of the trigonometric region can be determined by examining extreme cases of weight values, resulting in points $X_1$, $X_2$, and $X_3$. Consequently, any mutated individual will reside within this defined trigonometric region. The process, systematically moves the solution towards lower objective values, effectively exploiting the objective function information to guide the search towards promising areas.\nThe trigonometric mutation strategy's notable features include its simple geometric interpretation and the ability to automatically balance the differential vectors, which are key in guiding evolutionary algorithms towards efficient search paths in the solution space."}, {"title": "(d) Mutation Operation in SHSADE Algorithm", "content": "The SHSADE algorithm incorporates a critical mutation operation that generates trial vectors by modifying existing population members. This operation is instrumental in exploring the solution space, injecting diversity, and helping the algorithm to escape local optima. It leverages a combination of mutation strategies, selected probabilistically based on their historical performance.\nAmong the employed strategies, the Sinusoidal Mutation strategy stands out [24]. These strategies adapt the scaling factor F using sinusoidal functions to balance exploration and exploitation throughout the optimization process."}, {"title": "B. Proposed SHSADE-PIDS", "content": "SHSADE-PIDS is an evolutionary algorithm that optimizes neural network architectures by bridging discrete and continuous search spaces. It combines the continuous optimization capabilities of adaptive DE with discrete-continuous mapping to enable efficient architecture search. The overall pipeline is shown in Fig. 2."}, {"title": "(a) Discrete Configuration Space:", "content": "The architecture search space is defined as a set of discrete choices for each architectural parameter, such as a number of filters, kernel size, strides, etc. Each choice is discretized into a predefined set of allowed values $D = {D_1, D_2, ..., D_m}$ where $D_i = {d_{i1}, d_{i2},..., d_{in_i}}$ is the set of $n_i$ possible values for choice i."}, {"title": "(b) Population Initialization:", "content": "A population $P$ of $N_P$ individuals is randomly initialized, where each individual represents a neural network architecture $x_i$:\n$P = {x_1, x_2, ..., x_{NP}}$ (12)\nThe architecture $x_i$ is a vector of choices $[C_1, C_2, ..., C_m]$ with each $C_i \\in D_i$ selected randomly from the allowed discrete values."}, {"title": "(c) Discrete-Continuous Mapping:", "content": "To enable continuous optimization, a mapping M encodes the discrete architecture $x_i$ into a continuous vector $u_i \\in [0, 1]^m$:\n$u_i = M(x_i)$ (13)\nFor a choice $C_i$ with actual discrete value $k \\in D_i$, the mapping is:\n$M(C_i = k) = \\frac{Index \\space of \\space k \\space in \\space D_i}{N_i - 1}$ (14)\nThis normalization maps the discrete choices to a continuous range while maintaining relative positioning. Gaussian noise is added to $u_i$ for exploration."}, {"title": "(d) Evaluation:", "content": "The population is evaluated using a fast predictive model that estimates the performance of each architecture without full training. This enables quick fitness approximation."}, {"title": "(e) Evolutionary Optimization", "content": "In each generation g, SHSADE optimizes the continuous vectors $U_1, U_2, ..., U_{NP}$:\n(i) Select current best architecture $x$ as the target vector $\\hat{u}$\n(ii) Mutate random subset of P to produce mutated vectors $v_i$ = SHSADE-Mutation($u_r$)\n(iii) Crossover mutated vectors with target: $Z_i = DE-Crossover(v_i, \\hat{u})$\n(iv) Decode trial vectors into architectures: $\\hat{X}_i = M^{-1}(z_i)$\n(v) Evaluate trial architectures and update P\nThe mutation strategy adapts over generations based on successful parameter changes stored in an external archive. The evolution gradually improves the population, converging towards optimal architectures."}, {"title": "(f) Termination and Result Extraction:", "content": "After a termination criterion is met, the overall best architecture $x*$ is returned as the final result of the search\nIn summary, SHSADE-PIDS bridges discrete and continuous spaces through mapping, leveraging the exploration capabilities of adaptive DE to efficiently search neural architectures. The combination of discrete configuration, continuous-discrete mapping, fast evaluation, and adaptive evolution enables effective architecture optimization."}, {"title": "IV. EXPERIMENT", "content": "Dataset: We evaluate our method on two popular 3D deep learning benchmarks: ModelNet40 for 3D object classification and SemanticKITTI for semantic segmentation of LiDAR scans. ModelNet40 contains 12,311 CAD models from 40 categories. We follow the official split of 9,843 models for training and 2,468 for testing. SemanticKITTI has over 43,000 densely annotated LiDAR scans covering a total of 39.2 km across various urban and highway environments. Each scan has 130,000 points labeled into 28 semantic classes.\nImplementation Details: Our SHSADE-guided neural architecture search is implemented in PyTorch. All experiments are conducted on a server with an NVIDIA RTX 4090 GPU. For a fair comparison, we keep the training configurations the same between our method and baseline NAS strategies. Specifically, we use a mini-train/mini-val split of 80/20 and train for 300 epochs using an Adam optimizer with an initial learning rate of 0.001 and weight decay of 0.0001. During the search, 100 independent SHSADE runs are performed to discover top architectures. The search budget is limited to 500 sampled architectures.0\nFig. 3 shows the comparison of evolution performance for the SHSADE algorithm and the baseline work, the regularized EA used in PIDS (NAS) [9]. Obviously, SHSADE achieved a better convergence."}, {"title": "V. RESULT", "content": ""}, {"title": "A. Semantic Segmentation", "content": "Table I summarizes the mIOU on sequence 08 (validation split) of SemanticKITTI. For segmentation, SHSADE-PIDS attains a leading mIoU of 64.51% on the challenging SemanticKITTI benchmark, surpassing prior published methods. Critically, this is achieved using a highly compact model, with only 0.55M/1.36M parameters. This represents a 22-26X parameter reduction compared to other top-performing approaches like KPConv. The small model size enables more efficient inference and deployment. SHSADE-PIDS further reduces computational overhead, requiring just 4.5G/8.6G MACs versus heavier models like SalsaNext and MinkowskiNet, which demand 6-14X more operations. Latency is also competitive at 119ms/132ms, on par or faster than most methods."}, {"title": "B. Classification", "content": "Table II presents the overall accuracy of ModelNet40. For classification, SHSADE-PIDS again demonstrates optimized efficiency-accuracy trade-offs. With only 0.44M/1.31M parameters, it achieves top accuracy of 92.3%/93.4% on ModelNet40, while having up to 34X fewer parameters than KPConv. Latency remains low at 93ms for both model sizes.\nThe results demonstrate that the proposed SHSADE-PIDS method achieves better performance for 3D point cloud segmentation and classification while optimizing model efficiency."}, {"title": "VI. CONCLUSION", "content": "This paper has presented SHSADE-PIDS, an evolutionary neural architecture search method that bridges discrete and continuous spaces to optimize neural networks for 3D point cloud tasks. SHSADE-PIDS combines the continuous optimization strengths of SHSADE with a discrete-continuous mapping to enable efficient exploration of architecture configurations. Comprehensive experiments on semantic segmentation using SemanticKITTI and shape classification using ModelNet40 demonstrate SHSADE-PIDS's capabilities in discovering specialized models for point cloud processing. The neural architectures found by SHSADE-PIDS significantly advance higher accuracy and efficiency over prior works, including both hand-designed networks and other NAS approaches. Specifically, SHSADE-PIDS attains leading segmentation performance with just 0.55M parameters and 4.5GMACs, reducing overhead by over 22-26X versus other top methods while achieving higher 64.51% mIoU. For classification, it secures a higher accuracy of 93.4% with only 1.31M parameters, surpassing larger models. In conclusion, this work provides a new perspective on effectively leveraging continuous EA variants for discrete architecture search. The proposed SHSADE-PIDS approach and analyses offer valuable insights into hybridizing evolutionary algorithms with neural architecture optimization, particularly for emerging problem domains like point cloud processing."}]}