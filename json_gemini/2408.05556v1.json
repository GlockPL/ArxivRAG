{"title": "Evolutionary Neural Architecture Search for 3D Point Cloud Analysis", "authors": ["Yisheng Yang", "Guodong Du", "Chean Khim Toa", "Ho-Kin Tang", "Sim Kuan Goh"], "abstract": "Neural architecture search (NAS) automates neural network design by using optimization algorithms to navigate architecture spaces, reducing the burden of manual architecture design. While NAS has achieved success, applying it to emerging domains, such as analyzing unstructured 3D point clouds, remains underexplored due to the data lying in non-Euclidean spaces, unlike images. This paper presents Success-History-based Self-adaptive Differential Evolution with a Joint Point Interaction Dimension Search (SHSADE-PIDS), an evolutionary NAS framework that encodes discrete deep neural network architectures to continuous spaces and performs searches in the continuous spaces for efficient point cloud neural architectures. Comprehensive experiments on challenging 3D segmentation and classification benchmarks demonstrate SHSADE-PIDS's capabilities. It discovered highly efficient architectures with higher accuracy, significantly advancing prior NAS techniques. For segmentation on SemanticKITTI, SHSADE-PIDS attained 64.51% mean IoU using only 0.55M parameters and 4.5GMACS, reducing overhead by over 22-26X versus other top methods. For ModelNet40 classification, it achieved 93.4% accuracy with just 1.31M parameters, surpassing larger models. SHSADE-PIDS provided valuable insights into bridging evolutionary algorithms with neural architecture optimization, particularly for emerging frontiers like point cloud learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks have achieved remarkable success across diverse applications, largely attributed to their multi-layered architecture enabling hierarchical feature learning [1]\u2013[5]. However, the design of network architectures relies heavily on expert experience through extensive experiments. This manual process is computationally expensive and time-consuming due to the need to train and assess a vast number of configurations. To address this challenge, neural architecture search (NAS) has emerged as a promising technique to automate neural network design. The key idea is to search for an optimal architecture within a predefined search space by using an optimization algorithm that searches for candidate architectures [6]. NAS methods have discovered novel architectures surpassing hand-designed networks, including NASNet for image classification.\nPoint clouds captured by LiDAR and depth sensors are increasingly ubiquitous, but their unstructured nature poses difficulties for standard deep learning methods designed for grid data. Hence, applying NAS to emerging domains like point cloud processing remains an open challenge. While specialized point-based networks have been proposed, their manual architecture engineering is onerous. This provides strong motivation for investigating NAS tailored to automate point cloud network design.\nEvolutionary algorithms provide an alternative and efficient search strategy by simulating a population of neural architectures that evolve for survival and breeding while solving the learning tasks. Popular methods include genetic algorithms, evolutionary strategies, genetic programming, differential evolution, particle swarm optimization, and ant colony optimization. In particular, differential evolution (DE) through adaptive control has shown promise in balancing exploration and exploitation [7], [8], leveraging additional historical evaluations to guide the search in large search spaces.\nIn this work, we (i) present a NAS framework driven by a proposed adaptive DE algorithm, Success-History-based Self-adaptive Differential Evolution (SHSADE) with (ii) a Joint Point Interaction Dimension Search (PIDS) [9] to (iii) fit geometric dispositions and density distributions in varying 3D point clouds. The hybridized SHSADE-PIDS enables optimized point cloud architectures specialized for tasks like segmentation and classification by encoding discrete deep neural network architectures to continuous spaces and performing searches in the continuous spaces. Comprehensive experiments demonstrate SHSADE-PIDS's optimizer capabilities and advantages in accelerating NAS. The results provide insights into effectively applying adaptive evolution strategies for architecture search, particularly in novel frontiers involving unstructured 3D data."}, {"title": "II. RELATED WORKS", "content": "Neural architecture search (NAS) has emerged as an important field of automated machine learning, aiming to automate"}, {"title": "A. Neural Architecture Search", "content": "the design of optimal neural network architectures tailored to specific tasks and datasets [10]. The key idea behind NAS is to define a search space encompassing possible network architectures and utilize an optimization algorithm to actively navigate this space to find high-performing architectures for a given task. The search algorithm trains and evaluates candidate architectures within the space to guide the search toward optimal solutions based on performance metrics on a validation set. By effectively exploring the vast search space guided by such empirical performance feedback, NAS methods can discover novel architectures surpassing human-designed counterparts specialized for the dataset and application.\nEvolutionary algorithms have emerged as a particularly promising approach to neural architecture search due to their population-based search mechanism, which is naturally suited for exploring the vast combinatorial spaces of neural architectures. Notable evolutionary NAS techniques include NeuroEvolution of Augmenting Topologies (NEAT) [11], Genetic CNN [12], and more recently AmoebaNet [13] which leverages evolution strategies for architecture search. Compared to reinforcement learning-based NAS methods, evolutionary approaches have been shown to discover highly competitive architectures with greater stability, lower computational requirements, and less hyperparameter tuning. As such, evolutionary algorithms provide an efficient and powerful search strategy to automate neural architecture optimization without prohibitive computational costs."}, {"title": "B. Differential Evolution Algorithm", "content": "Differential Evolution (DE) is an evolutionary optimization algorithm designed to search continuous solution spaces for global optima. First proposed by Storn and Price in 1995, DE operates on real-valued solution vectors that are evolved over generations through bio-inspired mutation, crossover, and selection operators. The key advantage of DE is its self-adaptive mutation scheme that automatically adjusts the step size and direction during optimization based on differences between randomly selected population members. This allows DE to effectively balance the exploration and exploitation of complex problems.\nA key limitation is DE's sensitivity to fixed control pa- rameters, scale factor F, and crossover rate CR, which re- quire problem-specific tuning. Since its introduction, nu- merous enhancements to DE have been developed, includ- ing the Success-History based Adaptive Differential Evolu- tion (SHADE) algorithm. SHADE [14] enhances adaptation through historical memories of CR and F that record statistics of successful F and CR values over multiple generations. Weighted average adaptation has a higher influence on more successful solutions. By utilizing memories to guide reliable tuning, SHADE improves optimization speed, accuracy, and robustness compared to fixed parameter DE.\nSelf-Adaptive DE (SADE) addresses this by adapting F and CR during execution based on their relative success over a learning period [15]. However, SADE's adaptation scheme has limitations regarding sensitivity to randomness and differentiation between moderately and highly successful values."}, {"title": "C. Deep learning methods for point cloud processing", "content": "Point clouds have become a prevalent 3D data represen- tation, enabling detailed digitization of object shapes and environments through dense geometric sampling. However, point clouds present unique challenges for applying deep neural networks designed for grid-structured data like images. Point clouds are unordered, unstructured, and lack inherent topology. Pioneering deep learning approaches that directly process raw point clouds while respecting their permutation invariant and unstructured nature have emerged in recent years.\nEarly deep learning methods converted point clouds to inter- mediary representations like voxels or multi-view projections to leverage established 3D and 2D convolutional architectures. However, these conversions introduce quantization errors and lose fine geometric details. PointNet [16] pioneered direct"}, {"title": "feature learning on raw point sets using shared multilayer perceptrons (MLPs) and global max pooling for permutation invariance.", "content": "Various specialized point convolution operators have also been introduced to improve local feature learning, such as con- tinuous convolutions in Kernel Point Convolution (KPConv) [17]. Graph neural networks are gaining interest by inherently modeling connectivity in points through techniques like graph pooling and attention [18]. Overall, point-based deep networks now achieve results by directly operating on unstructured point clouds without conversions.\nKPConv is a pioneering point convolution operator using a parameterized kernel with learnable deformations to achieve geometry-adapted feature extraction. By representing kernels as sets of points with influence defined by distance corre- lations, KPConv enables flexible and effective point cloud feature learning. Networks using KPConv layers have achieved excellent performance on shape classification, part segmen- tation, and scene analysis benchmarks. The intuitiveness of learning convolutions directly on raw point sets and the deformable nature of KPConv have made it an indispensable building block for many point-based deep-learning architec- tures."}, {"title": "D. Joint Point Interaction Dimension Search", "content": "Point clouds provide detailed 3D geometric representations captured by sensors like LiDARs. However, their unstructured nature poses challenges for standard deep-learning techniques designed for grid data [19], [20]. Point-based deep neural networks have been proposed to process raw point clouds while retaining geometric details directly. However, manually designing optimal architectures for this domain remains difficult.\nRecent works have proposed various strategies to design point operators, which are the basic building blocks of hierarchical 3D models, to extract features from point clouds. These strategies include point convolutions, graph neural networks, attention mechanisms, and kernel point convolutions [21]. However, existing approaches have limitations. First, they manually design a single type of point interaction and reuse it within all point operators of a 3D model, which may limit performance due to varying geometric/density distributions of points. Second, they optimize point interactions and dimensions separately, missing the opportunity to find better combinations on the two axes. To address these issues, Zhang et al. introduced PIDS (Joint Point Interaction Dimension Search) [9], which jointly explores point interactions and dimensions in a large search space to craft 3D models that balance performance and efficiency for point clouds.\nSearch Space: PIDS is based on kernel point convolutions and introduces high-order point interactions to fit geometric dispositions and density distributions in varying 3D points. It also incorporates width, depth, and expansion ratio search components for efficient point dimensions. The joint search space over these heterogeneous axes enables the discovery of networks customized to geometric and density variations.\nPredictor Model: To enable efficient search, PIDS uses a Dense-Sparse Predictor to accurately model the heterogeneous search space. The predictor learns separate dense and sparse embeddings to handle the continuous and categorical com- ponents. A dot product enables cross-feature communication. This unified encoding improves the prediction of performance in the joint PIDS search space compared to standard predic- tors.\nBi-objective Neural Architecture Search: To achieve both high accuracy and efficiency, PIDS formulates neural architecture search as a bi-objective optimization problem [22], maximizing predictive accuracy while minimizing computational cost (FLOPs). Predictor models estimate accuracy and FLOPS to avoid expensive training. The bi-objective function balances accuracy versus efficiency to find optimal architectures with different trade-offs. By jointly exploring point interactions and dimensions in a large search space using an optimized predictor model and bi-objective search, PIDS can discover high- performing and efficient models specialized for processing 3D point cloud data."}, {"title": "III. METHODS", "content": "This section proposes a novel neural architecture search algorithm bridging discrete and continuous optimization, SHSADE-PIDS, which integrates techniques from SADE and SHADE, and describes its application in the 3D point cloud processing, illustrated in Fig. 1 and detailed in Fig. 2."}, {"title": "A. SHSADE", "content": "Differential Evolution (DE) is a popular evolutionary al- gorithm for real-parameter optimization. However, its per- formance depends significantly on properly setting control parameters like mutation factor (F) and crossover rate (CR). \u03a4\u03bf overcome this limitation, this research proposes an enhanced DE variant called SHSADE, which adapts these parameters based on historical success.\nSHSADE integrates complementary adaptive techniques from two major differential evolution variants - Self-Adaptive Differential Evolution (SADE) and Success-History based Adaptive Differential Evolution (SHADE). This hybridization aims to enhance the algorithm's optimization capabilities by leveraging the strengths of both approaches.\nSpecifically, from SADE, SHSADE adopts the core idea of self-adaptively tuning key control parameters like crossover rate (CR) and mutation factor (F) for each individual solution based on learning from historical memories. SADE records successful CR values over a predefined learning period and samples new CR values for each individual from a normal distribution with the mean set to the historically successful mean. This enables CR to be adapted to match the problem landscape. A similar mechanism is used to allow the mutation strength F to be tuned based on past successful values. Additionally, SADE maintains probabilities for selecting between multiple mutation strategies based on their relative success rates. This allows the algorithm to automatically choose the more effective strategies."}, {"title": "Complementarily, from SHADE, SHSADE inherits the use of memories to accumulate statistical summaries of successful control parameter values over multiple generations. Unlike SADE, which uses raw values, SHADE stores mean CR and F values from previous generations in historical memories of MCR and MF. New CR and F values are generated by randomly indexing into these memories and sampling from distributions around those stored means. The memories are then updated each generation by incorporating the new successful mean values using a learning rate. This more stable approach prevents the algorithm from overreacting to single generations where poor values succeed randomly.", "content": "Additionally, SHSADE integrates SHADE's current-to- pbest/1 mutation strategy along with novel trigonometric and sinusoidal strategies. The p-best selection helps balance greed- iness during the search."}, {"title": "(a) Historical Memories Unlike DE, which uses fixed F and CR values, SHSADE maintains memories MCR and MF to store statistical summaries of successful parameter values from previous generations:", "content": "MCR = [MCR1, MCR2, . . ., MCRH]\nMF = [MF1, MF2,..., MFH]\nwhere H is the memory size. Rather than raw values, these memories store the mean successful F and CR values from each of the past H generations (Eq. 1,2).\nCR\u2081 = randn(MCRk, 0.1)  (1)\nFi = randc(MFk, 0.1)  (2)\nNew F and CR values are generated by randomly index- ing into these memories and sampling from distributions around the stored means (Eq. 3, 4). This allows exploiting knowledge from prior successful generations.\nMCRk,G otherwise\nmean(SCR) if SCR \u2260 0\nMCRk,G+1 =\n(3)\nMFk,G+1 MFk,G otherwise\n(mean(SF) if SF \u2260 0\n{\n(4)\nSampling F and CR:\nCR\u2081 = randn(MCRr\u2081, 0.1)  (5)\nFi = randc(MFr\u2081, 0.1)  (6)\nWhere ri is a random index into the memories.\n(b) Adaptive Parameter Control After each generation, the memories MCR and MF are updated with the mean successful F and CR values using a learning rate c. This allows gradual adaptation of the control parameters to suit the problem landscape."}, {"title": "(c) Trigonometric Mutation Strategy The trigonometric mutation strategy [23], hereafter referred to as Strat- egy 4, introduces an efficient approach in evolutionary algorithms by generating a new trial vector through a weighted averaging process involving three distinct members of the population, with consideration given to their relative fitness. This method leverages geometric relationships among the selected individuals, utilizing the concept of centroid and linear combinations in the objective function space.", "content": "Consider three distinct individuals $X_{r_1,G}$, $X_{r_2,G}$, and $X_{r_3,G}$ randomly selected from the current population, where $r_1 \\neq r_2 \\neq r_3 \\neq i$. The mutation for the i-th individual $X_{i,g}$ in generation G + 1 is given by:\n$V_{i,G+1} := \\frac{X_{r_1,G} + X_{r_2,G} + X_{r_3,G}}{3} + (W_2 - W_1)(X_{r_1,G} \u2013 X_{r_2,G}) + (W_3 - W_2)(X_{r_2,G} - X_{r_3,G}) + (W_1-W_3)(X_{r_3,G} - X_{r_1,G})$\nwhere the weights W1, W2, and w3 are defined as:\n$W_1 = \\frac{|f(X_{r_1,G})|}{w'}$, $W_2 = \\frac{|f(X_{r_2,G})|}{w'}$, $W_3 = \\frac{f(X_{r_3,G})|}{W}  (7)$\nand\nw' = |f(Xr1,G)| + |f(Xr2,G)|+|f(Xr3,G)|  (8)\nThe methodology incorporates a perturbation vector that is a linear combination of the triangle's legs, with weights corresponding to the objective function values at the vertices. This perturbation can be conceptualized as shifts by the triangle's center (the donor) along each leg, with varying step sizes.\nThe weight terms (w2 \u2013 W\u2081), (w3 \u2013 w2), and (W1 - w3) serve a dual purpose. Firstly, they bias the movement along the triangle's legs from worse to better solutions based on the objective function. Secondly, they automatically scale the vector components in accordance with the differences in objective values among the individuals, thereby guiding the solution towards the most favorable of the three points. As such, the trigonometric mutation acts as a local search operator confined to the trigonometric region defined by the trio of individuals.\nTo illustrate this, consider a two-dimensional minimization problem with individuals $X_{r_1,G}$, $X_{r_2,G}$, and $X_{r_3,G}$. The boundaries of the trigonometric region can be determined by examining extreme cases of weight values, resulting in points X1, X2, and X13. Consequently, any mutated individual will reside within this defined trigonometric region. The process, systematically moves the solution towards lower objective values, effectively exploiting the objective function information to guide the search towards promising areas.\nThe trigonometric mutation strategy's notable features in- clude its simple geometric interpretation and the ability to automatically balance the differential vectors, which are key in guiding evolutionary algorithms towards efficient search paths in the solution space."}, {"title": "(d) Mutation Operation in SHSADE Algorithm The SHSADE algorithm incorporates a critical mutation op- eration that generates trial vectors by modifying existing population members. This operation is instrumental in ex- ploring the solution space, injecting diversity, and helping the algorithm to escape local optima. It leverages a com- bination of mutation strategies, selected probabilistically based on their historical performance.", "content": "Among the employed strategies, the Sinusoidal Mutation strategy stands out [24]. These strategies adapt the scaling factor F using sinusoidal functions to balance exploration and exploitation throughout the optimization process.\nSpecifically, two sinusoidal approaches are mixed for the first half of the generations, gs1 \u2208 [1, Gmax], to adapt Fi,g.\nThe Non-Adaptive Sinusoidal Decreasing Adjustment (Strategy 1) employs a decreasing sinusoidal formula to modify $F_{i,gs_1}$ for each individual:\n$F_{i,g_{s_1}} = \\frac{1}{2} (sin(2\\pi \\cdot freq \\cdot g_{s_1} + \\pi)\\cdot \\frac{G_{max} - g_{s_1}}{G_{max}} +1)$ (9)\nHere, freq is a fixed frequency value, gs\u2081 is the current generation, and Gmax is the maximum number of itera- tions.\nThe Adaptive Sinusoidal Increasing Adjustment (Strategy 2) uses a different sinusoidal formula for an incremental adjustment of the scaling factor:\n$F_{i,g_{s_1}} = \\frac{1}{2} (sin(2\\pi \\cdot freq_{i, g_{s_1}}g_{s_1})\\cdot \\frac{g_{s_1}}{G_{max}} +1)$ (10)\nIn this approach, $freq_{i,g_{s_1}}$ is an adaptive frequency, adjusted each generation using a Cauchy distribution:\n$freq_{i,g_{s_1}} = randc(\\mu_{freq_{ri,g_{s_1}}},0.1)$ (11)\nThe parameter ufreqri,981 for generation gs\u2081 is selected from an external memory Mfreq, which stores average successful frequencies from previous generations in Sfreq. After each generation gs1, this parameter is updated based on the Lehmer mean at a randomly chosen index ri.\nThese strategies exemplify the algorithm's approach to dynamically adapting mutation parameters, contributing to its effectiveness in complex optimization tasks."}, {"title": "B. Proposed SHSADE-PIDS", "content": "SHSADE-PIDS is an evolutionary algorithm that optimizes neural network architectures by bridging discrete and continuous search spaces. It combines the continuous optimization capabilities of adaptive DE with discrete-continuous mapping to enable efficient architecture search. The overall pipeline is shown in Fig. 2."}, {"title": "(a) Discrete Configuration Space: The architecture search space is defined as a set of discrete choices for each architectural parameter, such as a number of filters, kernel size, strides, etc. Each choice is discretized into a predefined set of allowed values D = D1, D2, ..., Dm where Di = di1, di2,..., din, is the set of ni possible values for choice i.", "content": "(b) Population Initialization: A population P of NP individuals is randomly initialized, where each individual represents a neural network architecture xi:\nP = X1, X2, ..., XNP (12)\nThe architecture x\u012b is a vector of choices [C1, C2, ..., Cm] with each Ci\u2208 Di selected randomly from the allowed discrete values.\n(c) Discrete-Continuous Mapping: To enable continuous optimization, a mapping M encodes the discrete archi- tecture xi into a continuous vector ui \u2208 [0, 1]m:\nUi = M(xi) (13)\nFor a choice Ci with actual discrete value k \u2208 Di, the mapping is:\nM(Ci = k) = Index of k in D_i/N_i -1 (14)\nThis normalization maps the discrete choices to a contin- uous range while maintaining relative positioning. Gaus- sian noise is added to u\u2081 for exploration.\n(d) Evaluation: The population is evaluated using a fast predictive model that estimates the performance of each architecture without full training. This enables quick fitness approximation.\n(e) Evolutionary Optimization In each generation g, SHSADE optimizes the continuous vectors U1, U2, ..., UNP:\n(i) Select current best architecture & as the target vector \u00fb\n(ii) Mutate random subset of P to produce mutated vectors v\u2081 = SHSADE-Mutation(ur)\n(iii) Crossover mutated vectors with target: Zi = DE-Crossover (vi, \u00fb)\n(iv) Decode trial vectors into architectures: \u00c2\u2081 = M-1(zi)\n(v) Evaluate trial architectures and update P\nThe mutation strategy adapts over generations based on successful parameter changes stored in an external archive. The evolution gradually improves the population, converging towards optimal architectures.\n(f) Termination and Result Extraction: After a termina- tion criterion is met, the overall best architecture 2* is returned as the final result of the search\nIn summary, SHSADE-PIDS bridges discrete and continuous spaces through mapping, leveraging the exploration capa- bilities of adaptive DE to efficiently search neural architec- tures. The combination of discrete configuration, continuous- discrete mapping, fast evaluation, and adaptive evolution en- ables effective architecture optimization."}, {"title": "IV. EXPERIMENT", "content": "Dataset: We evaluate our method on two popular 3D deep learning benchmarks: ModelNet40 for 3D object classification and SemanticKITTI for semantic segmentation of LiDAR scans. ModelNet40 contains 12,311 CAD models from 40 categories. We follow the official split of 9,843 models for training and 2,468 for testing. SemanticKITTI has over 43,000 densely annotated LiDAR scans covering a total of 39.2 km across various urban and highway environments. Each scan has 130,000 points labeled into 28 semantic classes.\nImplementation Details: Our SHSADE-guided neural ar- chitecture search is implemented in PyTorch. All experiments are conducted on a server with an NVIDIA RTX 4090 GPU. For a fair comparison, we keep the training configurations"}, {"title": "V. RESULT", "content": "A. Semantic Segmentation"}, {"title": "Critically, this is achieved using a highly compact model, with only 0.55M/1.36M parameters. This represents a 22- 26X parameter reduction compared to other top-performing approaches like KPConv. The small model size enables more efficient inference and deployment. SHSADE-PIDS further reduces computational overhead, requiring just 4.5G/8.6G MACs versus heavier models like SalsaNext and Minkowsk- iNet, which demand 6-14X more operations. Latency is also competitive at 119ms/132ms, on par or faster than most methods.", "content": "B. Classification\nTable II presents the overall accuracy of ModelNet40. For classification, SHSADE-PIDS again demonstrates optimized efficiency-accuracy trade-offs. With only 0.44M/1.31M parameters, it achieves top accuracy of 92.3%/93.4% on Model- Net40, while having up to 34X fewer parameters than KPConv. Latency remains low at 93ms for both model sizes.\nThe results demonstrate that the proposed SHSADE-PIDS method achieves better performance for 3D point cloud segmentation and classification while optimizing model efficiency."}, {"title": "VI. CONCLUSION", "content": "This paper has presented SHSADE-PIDS, an evolutionary neural architecture search method that bridges discrete and continuous spaces to optimize neural networks for 3D point cloud tasks. SHSADE-PIDS combines the continuous opti- mization strengths of SHSADE with a discrete-continuous mapping to enable efficient exploration of architecture configu- rations. Comprehensive experiments on semantic segmentation using SemanticKITTI and shape classification using Model- Net40 demonstrate SHSADE-PIDS's capabilities in discover- ing specialized models for point cloud processing. The neural architectures found by SHSADE-PIDS significantly advance higher accuracy and efficiency over prior works, including both hand-designed networks and other NAS approaches. Specifically, SHSADE-PIDS attains leading segmentation performance with just 0.55M parameters and 4.5GMACs, reducing overhead by over 22-26X versus other top methods while achieving higher 64.51% mIoU. For classification, it secures a higher accuracy of 93.4% with only 1.31M parameters, surpassing larger models. In conclusion, this work provides a new perspective on effectively leveraging continuous EA variants for discrete architecture search. The proposed SHSADE-PIDS approach and analyses offer valuable insights into hybridizing evolutionary algorithms with neural architecture optimization, particularly for emerging problem domains like point cloud processing."}]}