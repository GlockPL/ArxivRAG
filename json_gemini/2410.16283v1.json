{"title": "Understanding the Effect of Algorithm Transparency of Model Explanations in Text-to-SQL Semantic Parsing", "authors": ["Daking Rai", "Rydia R. Weiland", "Kayla Margaret Gabriella Herrera", "Tyler H. Shaw", "Ziyu Yao"], "abstract": "Explaining the decisions of AI has become vital for fostering appropriate user trust in these systems. This paper investigates explanations for a structured prediction task called \"text-to-SQL Semantic Parsing\", which translates a natural language question into a structured query language (SQL) program. In this task setting, we designed three levels of model explanation, each exposing a different amount of the model's decision-making details (called \"algorithm transparency\u201d), and investigated how different model explanations could potentially yield different impacts on the user experience. Our study with ~100 participants shows that (1) the low-/high-transparency explanations often lead to less/more user reliance on the model decisions, whereas the medium-transparency explanations strike a good balance. We also show that (2) only the medium-transparency participant group was able to engage further in the interaction and exhibit increasing performance over time, and that (3) they showed the least changes in trust before and after the study.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) systems are now increasingly used to facilitate human life, such as assisting people in complex daily tasks and boosting their productivity in the workplace. For these systems to be reliably and securely used in critical domains, building appropriate trust between humans and the systems is crucial. Among others, Explainable AI (XAI) has been considered to be a key technique in fostering appropriate human-AI trust by presenting to humans how the AI system reaches a certain decision for a given input (called \"local explanation\u201d), which has led to the development of several XAI techniques [3, 35, 36, 38, 42, 44]. However, recent human-subject studies suggest that these explanations do not help humans identify AI misclassifications [4, 21], nor reduce over-reliance [1, 45, 52] or foster appropriate human-AI trust [52]. Nonetheless, most of these studies focus only on simpler classification tasks like sentiment classification or multiple-choice question answering (MCQA), and insights from them may not necessarily generalize to more complex tasks where explanations could provide greater value.\nIn this paper, we extend this line of research but seek to examine the effect of XAI on human-AI trust and humans' ability to identify correct/incorrect predictions in a more complex task, called \"text-to-SQL semantic parsing\" [24, 46, 49, 53]. Text-to-SQL semantic parsing involves translating a question written in natural language (e.g., \"How many schools or teams had jalen rose?\") to a Structured Query Language (SQL) query (e.g., SELECT COUNT(School/Club Team) FROM Table1 WHERE Player = \"jalen rose\"), which can then be executed against the provided database to retrieve relevant database records or calculate the queried results in the natural language questions. Unlike the classification tasks that were commonly used for case studies in prior XAI work, semantic parsing represents a much more complicated \u201cstructured prediction\" problem, where a model needs to make a sequence of inter-correlated predictions, each corresponding to one token constrained by the grammar of the target formalism, to form the final logical form (e.g., a SQL query). The complexity of this task thus makes the application of XAI non-trivial, giving rise to two critical questions: (1) How to explain a semantic parsing result? Prior work explained an AI model's classification result by offering one explanation for each label, elaborating on why Al thinks each one of them could be the correct answer, and participants were required to select the label with the most convincing rationale or explanation [1, 45, 52]."}, {"title": "2 Background and Related Work", "content": ""}, {"title": "2.1 Explainable AI and Trust in Human-Machine Interaction", "content": "Formally, \"trust\" is understood as \"an attitude of confident expectation in an online situation of risk that one's vulnerabilities will not be exploited\" [5]. Prior work [7] has identified two cases detrimental to the interaction between humans and machines: (1) Over-trust, i.e., when humans trust \"too much\" in the machine, which could lead to \"hands-off\" monitoring behavior rendering the human unable to respond to an error or malfunction; (2) Under-trust, i.e., when humans trust \"too little\" in the machine, even when the machine has outstanding capabilities in tasks, which could cause an unnecessarily unbalanced workload and inefficient collaboration. Establishing proper human trust in the AI model is thus crucial for secure human-machine interactions [6]. Recent research revealed that controlling \u201calgorithm transparency\u201d, i.e., the amount of algorithm detail to be exposed, could be a promising solution to calibrate human trust in machines [19]. However, this idea has not been studied in a task setting as complicated as semantic parsing.\nIn connection with algorithm transparency, how to effectively explain an algorithm or a model has been a long-standing problem, giving rise to the research topic of Explainable AI (XAI) [29]. One common scenario of explanation is to locally explain why a model gives a certain output given the input (i.e., \"local explanation\u201d). While this way of explanation does not offer much global information about the model's intrinsic properties, it enjoys the benefit of being targeted to the individual model decision. Feature attribution is one type of method for local explanation. It explains a model by showing which input features the model output should be attributed to. Some well-known feature attribution-based explanation methods include LIME [36], Shapley value [39], Kernel SHAP [38], and Integrated Gradients [42]. Our study used LERG [44], a recent feature attribution method designed for conditioned generation tasks, where the model outputs a sequence of tokens conditioned on the input. LERG adapted Shapley value and LIME into LERG-S and LERG-L; we used LERG-S in our medium- and high-transparency explanations."}, {"title": "2.2 Explaining Text-to-SQL Semantic Parsing", "content": "Our project is based on a structured prediction task called \"text-to-SQL semantic parsing\". A text-to-SQL semantic parsing system aims to translate a natural language text into a SQL query, enabling non-technical users without SQL programming skills to interact with databases and retrieve information using natural language. As a result, the past few years have witnessed continuing excitement of building text-to-SQL semantic parsers [23, 26, 32, 34, 37, 46, 51]. In our work, we used a semantic parser built from the open-source T5 language model [33], following the same approach of Rai et al. [34], Scholak et al. [37], Xie et al. [46].\nWhile machine performance on this task has been boosted dramatically in the past years, state-of-the-art systems still fall short in real applications, due to practical challenges such as language ambiguity or complexity and domain shift. This inspires a recent line of research called \"interactive semantic parsing\", where the semantic parsing system proactively explains its decisions to the human and seeks human feedback to correct potential mistakes [11, 14, 25, 31, 43, 48, 50]. Our work was inspired by the rise of this line of research but was focused exclusively on the impact of various explanations on the end users of the semantic parser. We aim to examine this impact systematically with a human subject study, which was barely performed by prior works.\nOur work involves three distinct explanation methods, i.e., model confidence, feature attribution, and visualized step-by-step explanations. These methods are considered representative ways of model explanations and have been explored by prior works. Dong et al. [9] were among the earliest in modeling a semantic parser's confidence in its prediction. In their work, the authors categorized a semantic parser's uncertainty into three types, i.e., model uncertainty, data uncertainty, and input uncertainty, and developed approaches to measure each type respectively. Yao et al. [48] followed a similar approach as Dong et al. [9] but found that the confidence score might not be a good indicator of whether a semantic parser made a correct or incorrect prediction. Inspired by the need for a more reliable indicator, Stengel-Eskin and Van Durme [41] studied approaches for calibrating a semantic parser's confidence score. In the spectrum of text-to-SQL semantic parsing, feature attribution is largely understudied. The only available is that of Rai et al. [35], which systematically compared different feature attribution approaches (e.g., LIME, Shapley, and LERG). However, the comparison did not involve any human subjects; instead, it was mainly based on automatic evaluation metrics, yet whether these metrics represent user perception of explanations in real life was uncertain. Finally, the step-by-step explanation in our high-transparency explanation design was inspired by prior works of Elgohary et al. [11], Narechania et al. [31], Tian et al. [43], which demonstrated the effectiveness of this approach in text-to-SQL semantic parsing. In particular, Narechania et al. [31] proposed to include dynamic views of database changes when a SQL query is executed step by step. We borrowed this idea in our high-transparency explanation design as well. However, we note that none of the prior works have systematically compared all the three types of explanations, especially when they are organized to represent different algorithm transparency levels, in one human subject study."}, {"title": "3 Study Goals", "content": "Our study aims to understand whether and to what extent a model explanation at a different level of algorithm transparency will impact the user experience when they interact with the AI model. We define different algorithm transparency levels based on the amount of model detail that the explanation exposes to the user, such as showing the entire or only partial decision-making process of the model. Our goals for this study are to answer the following two research questions (RQs):\n\u2022 RQ1: How do the model explanations at different transparency levels affect humans' ability to accurately identify correct and incorrect AI predictions? We follow the same spirit of prior works [52] in considering how the model explanations can assist humans in better utilizing the AI model and maximizing their benefit in tasks. Specifically, we focus on the scenario where users are presented with the model prediction, but will make a judgment on whether the prediction is correct or not, based on the model explanation and other task information. An effective explanation should allow users to accurately distinguish between correct and incorrect AI predictions, such that they can avoid the risks of adopting incorrect predictions from the Al model. This is a non-trivial RQ in our setting because of the pros and cons of each transparency level of explanation. For example, while a high-transparency explanation could allow users to make more informed judgments owing to the larger amount of model detail it provides, it may also confuse the users if the users do not have the proper knowledge background to digest the information. In contrast, while a low-transparency explanation only provides limited insights about a model's decision, the low cognitive load it requires may engage the users better. In our study, we hope to discuss these trade-offs carefully from the empirical observations of participant performance.\n\u2022 RQ2: How do the model explanations at different transparency levels affect humans' trust in the AI model? Similarly, our study also aims to understand the impact of explanations at different transparency levels on human-AI trust. As RQ1, this is not a trivial question either. The very first challenge lies in the measurement of human trust. While \"trust\" has been discussed in many prior works which similarly investigated in the effect of model explanations [12, 52], there is no clear strategy for effectively measuring human trust. Second, whether an explanation at a certain transparency level will lead to more or less human trust in AI, is a complicated question. For example, a high-transparency explanation could increase human trust when it allows the user to know more about the model and hence enhances their confidence in using the model, but it could also lead to less human trust when the explanation is not plausible from the human perspective, although it is faithful to the model itself.\nWhile Explainable AI (XAI) and its effect on user experience have been explored by multiple prior works [1, 45, 52], most, if not all, of these works, focused on simplified classification tasks, such as sentiment classification and multiple-choice question answering (MCQA), when there are only a limited set of labels for the AI model to pick. However, the space of machine learning and AI includes way more complicated tasks than classification, and these tasks do not come with a finite set of possible answers. This discrepancy significantly increases the difficulty in effectively explaining these complicated AI models and renders findings from prior literature not directly applicable. For instance, in sentiment classification or MCQA tasks, explanations for all possible labels can be presented to participants, who then compare them and choose the most convincing explanation. In contrast, for tasks with numerous possible outputs, it is impractical to provide one explanation for each potential answer. Typically, explanations are generated only for the Al's predicted answers, which may not always be correct. Consequently, participants must decide whether to trust the AI based solely on the explanation of the generated output, which is more challenging. Additionally, the complexity of these tasks often requires domain-specific knowledge, further complicating the participants' ability to evaluate the explanations. These factors can result in varying participant behaviors and attitudes toward the explanations provided.\nIn our study, we pick \"text-to-SQL semantic parsing\" as one of such complicated AI tasks, and use it to examine the two RQs we defined above. This is a task of automatically converting a natural language question to a SQL query, such that by executing the SQL query against the given database, one can obtain the database querying results expressed in their natural language question [46]. By its nature, semantic parsing is a structured prediction task, as its goal is to generate a sequence of code tokens forming the structure of a grammatically correct SQL query. Unlike classification, semantic parsing has an infinite label space; that is, in principle, a semantic parser can generate an arbitrary number of SQL programs. On the other hand, users of semantic parsers are often non-technical people who cannot write a SQL program themselves, because otherwise they can directly compose the SQL query without needing help from the semantic parsers. This particular user group also makes explaining a semantic parser's SQL prediction difficult. Below, we further highlight the complexity of this task and the challenges in interpreting its study results, which will serve as a foundation for our result analysis.\nThe substantially higher complexity of semantic parsing than simple classification. While in classification tasks a user mainly needs to read the text input (e.g., the sentence to be predicted with a sentiment label), in semantic parsing tasks, multiple components will be involved. Specifically, as we will describe in Section 4, a user interacting with a semantic parser needs to first understand the database based on its schema and the provided sample records, and then interpret their question by grounding it onto the database. Although users are most concerned with the final received records (e.g., for \"what/which\" questions) or the calculated results (e.g., for \"how many\" questions), the semantic parser functions to generate the SQL query, not to directly retrieve the records. Therefore, explaining the model's decision-making process requires us to clarify both the generated SQL and its interaction with the database to retrieve the records. Designing such an informative explanation without significantly increasing the cognitive load on the participants can be challenging. Finally, we note that, despite its complexity, semantic parsing offers an effective and a unique task setting to analyze the impact of explanations at varying levels of algorithm transparency on participants' performance and trust levels.\nComplexity caused by the users' non-technical backgrounds. As discussed above, explaining the Al's decision-making process often involves detailing the predicted SQL query. Users without a technical background may find this information overwhelming, particularly when dealing with complex SQL queries that contain numerous clauses. This gap of knowledge could affect their understanding of the task, the model predictions, and the model explanations, which eventually impacts their performance in recognizing correct and incorrect AI predictions and their trust levels. In our study, we also aim to analyze users' changes of behaviors, such as when these non-technical users keep interacting with such a complicated system, whether they will actively learn from the model explanations and gradually adapt themselves to tasks at this complexity level.\nThe intricate relation between RQ1 and RQ2. The user performance in recognizing correct and incorrect predictions (RQ1) and their trust in the AI model (RQ2) are not the same. Rather, the user performance is often an effect of multiple factors, including their trust in the Al model. Specifically, whether a user can correctly distinguish between correct and incorrect predictions could depend on their trust in the model, their knowledge background (e.g., how much they can understand about the task and the model explanations), their personality (e.g., whether they tend to be patient in reading very long descriptions as in the high-transparent explanations), and the quality of the explanations (e.g., whether the explanations are precise and indicative enough for anyone to make a reasonable judgment). Assume that the AI model has an accuracy of P in the semantic parsing task. We consider the following three cases when connecting the user performance in RQ1 with their trust measured in RQ2:\n\u2022 In the case of extreme under-trust, users may simply reject any predictions from the model and consider all of them as incorrect. In this case, their accuracy in successfully recognizing correct and incorrect predictions will be 1 \u2013 P.\n\u2022 In the case of extreme over-trust, users simply accept any predictions and consider them as correct. Their accuracy in this case will be the same as model accuracy, i.e., P.\n\u2022 In the case of normal trust, users not necessarily can obtain high or low accuracy. As we elaborated above, this accuracy depends on multiple factors. In an extreme case when all the factors are perfectly set (e.g., model explanations are perfectly designed, users are reliable in reading long explanations and completing the task carefully, etc.), the user accuracy will be 100%, which shows the best situation. However, in reality, there are always factors that are not perfectly set, which results in uncertainty in user accuracy, and the final accuracy could be larger or smaller than the accuracies in the extreme under-trust and over-trust cases.\nThe intricate relation between RQ1 and RQ2 makes the interpretation of our study results challenging yet interesting."}, {"title": "4 Interface Design for Human Subject Study", "content": "We designed a user interface (UI) to support our human subject study, as shown in Figure 1. Specifically, each participant will first be presented with a natural language Question describing the database query task they need to complete. The specific database context can be viewed by clicking the View Database button, which includes the name of the database, the schema and the name of each table within this database, as well as a few sample records from each table (Figure 2). Note that we intentionally provide only the record samples to simulate the practical situation when humans cannot fully explore a large-scale database due to limitations on time and effort; instead, they can typically browse the first few lines of records to form a rudimentary understanding of the database information. After the question and the database view, we present the Models' Prediction results, which are obtained by executing the model-generated SQL query against the complete database. Note that the participants are assumed to have no SQL programming skills, so they will not understand the model-predicted SQL. Here, we only present the participants with the retrieved database records or the calculated results from executing the model-generated code. Following these items, we present a Model Explanation, which will be implemented with various approaches to explain the model prediction to the participant, as we will introduce in Section 5. To measure the effect of each model explanation, we then prompt the participant to judge, based on the explanation, if the model's prediction is correct or not. This prompt, \"Do you think this is a correct prediction\", is included at the bottom of the UI. Ideally, an effective model explanation should allow participants to distinguish between correct and incorrect model predictions. After the participant clicks one of the choices, a message will pop up showing them the true judgment (Figure 3). The purpose of this pop-up message is to provide immediate feedback on whether their judgment was accurate, allowing them to utilize the explanation more effectively for subsequent questions. As a result, we also expect the participants to learn from past interactions and gradually adapt themselves to the task with increasingly better performance."}, {"title": "5 Three Levels of Explanations for Text-to-SQL Semantic Parsing", "content": "In our study, we experimented with three explanation approaches, each at a distinct algorithm transparency level.\nLow-transparency Explanation (Figure 4) presents to participants only a confidence score of the model's decision-making (i.e., the SQL query prediction), which is calculated by taking the arithmetic average of the softmax probabilities assigned to each token in the SQL query by the model during prediction. Specifically, the confidence score C for the generated SQL query with N tokens is given by $C = \\frac{\\sum_{n=1}^{N} C_n}{N}$, where $C_n$ is the confidence score for the n-th token in the predicted SQL query, which is calculated as the model's conditioned probability of generating this token, i.e., $C_n = P(c_n \\vert c_1,\\dots, c_{n-1})$.\nMedium-transparency Explanation presents to participants both the confidence score of the generated SQL query and a feature attribution explanation, highlighting the input features considered most important by the model. For instance, the example in Figure 5 shows that the model decides to return the retrieved database records mostly because of the words \"name\", \"airport\", \"code\", and \"AKO\" present in the input question, \"Return the name of the airport with code 'AKO.\" Note that other words also have some impact on the model's decision, but they are very light and negligible. The contribution of features to the model prediction was extracted using the LERG-S algorithm [44], a local explanation method that adapts Shapley value [39] to explain models in generation tasks.\nHigh-transparency Explanation provides participants with the overall confidence score for the predicted SQL query, along with a detailed explanation of the SQL itself, as shown in Figure 1. Specifically, the SQL was first translated to NatSQL [13], an intermediate representation that simplifies the SQL to make it easier to identify text descriptions for each clause. Each clause was then explained in plain English as a step or rule, to help participants understand how the query interacts with the database to generate the final results. For example, Figure 1 shows a SQL query composed of three rules for generating the answer to the question: first, it retrieves all records from the airports table; then, it filters the results to include only those whose airportcode is \"AKO\"; and finally, the SQL query returns values from the airportname column. Additionally, the confidence score and feature attribution explanation for each rule were also displayed, helping participants understand how confident the AI was in predicting each rule and which input features it considered significant for generating the rule. Note that the feature attribution explanation for the first step/rule is absent because NatSQL does not have a separate FROM clause; we however included the description and database view of this step to complete the explanation. This step-by-step breakdown provides a comprehensive view of all the SQL query rules, explains why the model generated each one, and shows how they were used to obtain the final answer.\nTo summarize, our study involves three types of model explanations designed for the complicated text-to-SQL semantic parsing task, each representing one algorithm transparency level. We also note that in our design, information included in the lower transparency level of explanation is a subset of information in the higher level one. For example, the model confidence score is included in all three levels, and when the feature attribution results are included in the medium-transparency explanations, they are also displayed in the high-transparency explanations. As such, results from our study allow us to easily understand the effect when the amount of information increases from level to level."}, {"title": "6 Trust Measurement", "content": "Trust is a multi-dimensional construct [16, 20, 22, 27]. For example, Hoff and Bashir [16] posit that trust consists of dispositional trust (an individual's overall tendency to trust automation independent of context), situational trust (how trust changes in response to context), and learned trust (an individual's evaluation of a system drawn from past experience). Due to its multi-dimensional nature, authors have suggested that trust should be measured in a variety of ways [20]. For this reason, we measured dispositional trust with the \"propensity to trust scale\" [28] and learned trust with the \u201cchecklist for trust\", or the Jian trust scale [17]. Below, we explain the two trust measures further.\nPropensity to Trust. The Propensity to Trust questionnaire developed by Merritt et al. [28] was used to measure dispositional trust in automated systems. This 6-item scale was developed to assess the broad, trait-like tendency to trust machines. It involves Likert ratings on a scale from 1 (strongly disagree) to 5 (strongly agree). Sample questions include \"I usually trust machines until there is a reason not to\" and \"For the most part I distrust machines\". We note that in this questionnaire, only the second item ask about negative opinions while the remaining are all positive. Therefore, participant responses to these two subsets of items should be interpreted differently. For example, when a participant gives a score of 1 (strongly disagree) to a negative item \u201cFor the most part I distrust machines\u201d, they indeed mean a very positive attitude to machines, but a score of 1 for a positive item \u201cI usually trust machines until there is a reason not to\u201d implies the opposite.\nChecklist for Trust between People and Automation (Jian Scale). The Checklist for Trust developed by Jian et al. [17] was used to measure trust between people and automated systems. It involves Likert ratings ranging from scores 1-7 to 12 items. Similar to the propensity to trust measurement, the checklist includes both positive and negative items. Specifically, The first 5 items check humans' negative opinions toward the automated system; sample questions include \"The system is deceptive\u201d and \u201cI am wary of the system\u201d. The remaining 7 items check humans' positive opinions toward the system, including questions such as \"I'm confident in the system\u201d and \"The system provides security\u201d."}, {"title": "7 Method", "content": ""}, {"title": "7.1 Task Setup", "content": "We fine-tuned an encoder-decoder language, T5-base [33], to be our semantic parser. An advantage of this model architecture lies in its mediocre performance, which provided us with a balanced set of samples where the model made both correct and incorrect predictions. We followed prior works [34, 37, 46] in formulating the input to the T5 model consisting of the natural language question and the database information, and the output from the T5 model being the SQL query. Both the model fine-tuning and our study used the Spider dataset, a large-scale, complex, and cross-domain dataset that has been considered a standard benchmark for text-to-SQL semantic parsing [49]. The fine-tuned T5-base semantic parser had an execution accuracy of 57.9% and an exact match accuracy of 57.2% on the Spider evaluation set (the development-set split). For the user study, we randomly sampled 30 examples from the Spider evaluation set, where 17 examples were correctly predicted by our semantic parser, reflecting an actual accuracy of 17/30 or 56.67%. The examples covered all difficulty levels (Easy: 11, Medium: 9, Hard: 7, Extra Hard: 3) following Spider's standard, which defines the difficulty level of an example based on the number of keywords, components, and clauses in its ground-truth SQL query. Figure 6 shows the distribution of the model's accuracy across these difficulty levels."}, {"title": "7.2 Participant Recruitment", "content": "To recruit participants without technical backgrounds to meet our study goals, we collaborated with the Psychology department in the institution, which is also the organization of co-authors in this paper. Specifically, we used undergraduate students as our potential participants. These students did not have any programming background. They were compensated for 1 research credit that could be applied to meeting the requirements of their degree program. In total, 115 students responded to our recruitment, who were then invited to complete tasks following our study procedure (to be introduced next). Among them, 18 left the study incomplete, possibly due to them being unable to understand the study or having other personal difficulties. This eventually gives us 97 valid data points for our subsequent analysis. These 97 participants have the following demographics: aging between 18 and 41 years old; 41 males, and 56 females; and from different ethnic backgrounds (36 Whites, 23 Asians, 10 Black, 8 Hispanic, 4 Middle Eastern, and 16 others)."}, {"title": "7.3 Study Procedure", "content": "Each participant was initially assigned to one of the three transparency-level groups at random. This assignment was unknown to the participants. The study was conducted virtually on a Web platform with a confidential login developed by the institution's Psychology department to support human subject research. When participants worked on the study, the platform was displayed in a full-screen mode protecting the participants from being distracted by irrelevant outside events. The participants were first presented with an informed consent form, clarifying the research goals and procedures of the study, as well as any potential risks and benefits. Only participants who signed the consent form moved on to the next step. They were then prompted to provide their demographic information and complete the two trust measurements, i.e., the propensity to trust and the Jian Scale measurements. These two metrics assessed the participant's initial trust level. Before starting the formal study, participants went through a training session identical to the formal study itself, which included 10 examples randomly sampled from the Spider evaluation set. During both the training and the formal study, participants reviewed one example at a time on our developed UI (Section 4). At the end of each example, participants were asked to determine if the Al-generated answer was correct or incorrect and received immediate feedback automatically presented by the platform. Note that the type of explanations presented to them was decided based on their group. The training session was necessary to educate the non-technical participants about the overall task (e.g., what is a database and what is database querying) and help them get familiar with the UI. Upon completing all 30 examples in the formal study, participants completed the Jian Scale measure again to evaluate any changes in their trust levels. Finally, to allow for a deeper understanding of the participant behaviors, we requested them to provide feedback on the study through an open-ended form, sharing their experience and any issues encountered during the survey.\nOur study received the approval from the university's institutional review board (IRB)."}, {"title": "8 Results and Analysis", "content": ""}, {"title": "8.1 Performance of Participants with Different Transparency Levels", "content": "We present the participants' performance, i.e., their accuracy in correctly recognizing correct or incorrect model predictions based on the presented explanations across different transparency levels, in Table 1. Specifically, for Accuracy in Correct AI Predictions, we considered the subset of 17 test questions where the semantic parser made a correct prediction, and then reported the percentage of predictions that were correctly recognized as correct predictions by the human participants. For Accuracy in Incorrect AI Predictions, we similarly considered a complementary subset of 13 test questions where the semantic parser made an incorrect prediction, and then reported, among these incorrect model predictions, how much portion was correctly recognized as incorrect predictions by the participants. Finally, Total Accuracy presents the overall accuracy across all model predictions of the 30 test questions, no matter if they were correct or incorrect. These metrics are formally described below.\nCorrect AI Pred Acc = $\\frac{\\text{# of Correct AI Pred Recognized to be Correct}}{\\text{# of Correct AI Pred}}$\nIncorrect AI Pred Acc = $\\frac{\\text{# of Incorrect AI Pred Recognized to be Incorrect}}{\\text{# of Incorrect AI Pred}}$\nTotal Acc = $\\frac{\\text{# of Correctly Recognized AI Pred}}{\\text{# of AI Pred}}$\nFor each subset of model predictions, Table 1 also presents the subgroup performance depending on the type of model explanations that a participant interacted with. We discuss the main findings below.\nParticipants interacting with explanations at different transparency levels had a similar overall performance. The total accuracy of participants at identifying whether the model predictions were correct or incorrect was 60.41%, as shown in Table 1. This total accuracy remained similar across different transparency levels, suggesting that different transparency levels of explanations had little effect on participants' total performance in distinguishing between correct and incorrect model predictions.\nParticipants were less successful in identifying incorrect AI predictions. When breaking down the accuracy by correct and incorrect subsets of model predictions, participants were significantly more accurate when the model prediction was correct (70.16% vs. 47.66% total accuracy). This indicates a general tendency of participants to accept the AI prediction, a pattern observed across all transparency levels, with the effect being most pronounced among high-transparency participants. The provided explanation could be partially responsible for biasing the participants to place greater confidence in the Al's predictions, an observation consistent with prior works [2, 21]. Given that high-transparency explanations offer more detailed insights into the Al's decision-making process, they may have been more persuasive. On the other hand, considering that the participants are all users without technical or AI backgrounds, the huge amount of information included in the high-transparency explanations may be cognitively too much for them, making them unable to fully digest the information or skip the information when making the judgment. In contrast, participants interacting with low-transparency explanations tend to be more cautious, demonstrated by their lower accuracy in correct Al predictions and higher accuracy in incorrect ones, given that the participants were only provided with model confidence as the explanation.\nTo further understand the participant performance, we plot the distribution of the model confidence (as reported in the low-transparency explanations) across different difficulty levels of test questions, and the participants' Total Accuracy in recognizing correct vs. incorrect predictions similarly across different difficulty levels, in Figure 7. From Figure 7a we observed that the neural semantic parser used in our experiments tended to be overly confident, no matter if its predictions are actually correct or incorrect. This is consistent with discoveries in prior research [8, 18, 41]. However, between difficulty levels where the model makes substantially fewer (e.g., Easy questions) or more (e.g., Extra Hard questions) incorrect predictions (Figure 6), the model did show a lower confidence score, which explains how low-transparency participants could recognize correct/incorrect model predictions with the highest accuracy in both the Easy and Extra Hard categories (Figure 7b).\nNo consistent impact of transparency levels was observed on the participant performance across difficulty levels, and medium-transparency explanations strike a good balance. Figure 7b also shows that no particular transparency level consistently assisted the participants with the best performance across all difficulty levels of test questions. Low-transparency participants had the best performance on Easy and Extra Hard examples, while high-transparency participants had the best performance on Medium and Hard examples but performed substantially worse than all the other types of explanations on Extra Hard examples. As we discussed before, the much worse performance of participants interacting with high-transparency explanations could be due to that the highly informative high-transparency explanations may have biased the participants to trust in the model predictions, or the participants might not be able to digest this large amount of information, leading to mostly incorrect judgment. Among the three transparency levels of explanations, as shown in both Table 1 and Figure 7b, the medium-transparency explanations strike a good balance between the low- and high-transparency ones, allowing participants to obtain a balanced accuracy across different subsets of Al predictions and across different task difficulty levels."}, {"title": "8.2 View Time Analysis of Participants", "content": "We tracked participants' view time for each test question during the user study to analyze how much time they needed to understand the prediction and the explanation. Specifically, for each test question, the view time is defined as the time from when the question was displayed to when the participant submitted their response.\nHigh-transparency explanations are the most time-consuming, and low- and medium-transparency explanations resulted in a similar view time. The overall average view time per test question was 24.28 seconds, as shown in Figure 9. As expected, high-transparency participants had the longest average view time of 36.05 seconds, followed by the medium-transparency participants at 18.04 seconds, and the low-transparency participants at 17.22 seconds. Interestingly, the low and medium-transparency participants had a similar view time with a difference of less than a second, suggesting that new information included in the medium-transparency explanation did not significantly increase the cognitive load on participants.\nLonger viewing times led to"}]}