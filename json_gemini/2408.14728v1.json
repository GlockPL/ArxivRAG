{"title": "TART: Boosting Clean Accuracy Through Tangent Direction Guided Adversarial Training", "authors": ["Bongsoo Yi", "Rongjie Lai", "Yao Li"], "abstract": "Adversarial training has been shown to be successful in enhancing the robustness of deep neural networks against adversarial attacks. However, this robustness is accompanied by a significant decline in accuracy on clean data. In this paper, we propose a novel method, called Tangent Direction Guided Adversarial Training (TART), that leverages the tangent space of the data manifold to ameliorate the existing adversarial defense algorithms. We argue that training with adversarial examples having large normal components significantly alters the decision boundary and hurts accuracy. TART mitigates this issue by estimating the tangent direction of adversarial examples and allocating an adaptive perturbation limit according to the norm of their tangential component. To the best of our knowledge, our paper is the first work to consider the concept of tangent space and direction in the context of adversarial defense. We validate the effectiveness of TART through extensive experiments on both simulated and benchmark datasets. The results demonstrate that TART consistently boosts clean accuracy while retaining a high level of robustness against adversarial attacks. Our findings suggest that incorporating the geometric properties of data can lead to more effective and efficient adversarial training methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Despite the phenomenal success of deep neural networks (DNNs) in various fields including computer vision [1]\u2013[3], natural language processing [4], recommendation systems [5], and reinforcement learning [6], they are highly susceptible to adversarial examples generated by adding small adversarial perturbations to natural examples [7]\u2013[10]. Adversarial perturbations are purposely chosen by the attacker to deceive the model into making incorrect predictions, but are small enough to be imperceptible to humans. As DNNs are used in numerous real-world scenarios, this vulnerability leads to serious security issues, particularly in fields like autonomous driving [11], [12] and medical analysis [13]\u2013[15].\nTo address these issues, developing powerful defense tech-niques against such adversarial attacks has become an essential research field. One of the most popular defense methods against adversarial attacks is standard adversarial training proposed by Madry et al. [16]. It trains the model using adversarial samples generated via a method called the pro-jected gradient descent. See [17], [18] for a summary of various techniques for generating adversarial examples. While straightforward, it has been shown to significantly enhance the model's robustness against such attacks. After this initial success, numerous modifications and advancements have been proposed to standard adversarial training [19]\u2013[24]. Never-theless, an undesirable, large degradation in clean accuracy has always been inevitable during adversarial training [20], [25], [26]. Our primary goal of this study is to recover clean accuracy while maintaining its robustness.\nRecently, Zhang et al. [27] have empirically revealed that over-parameterized DNNs still suffer from limited model capacity during adversarial training because of its excessive smoothing effect. To overcome this, various approaches have been proposed to efficiently employ this limited capacity for successful adversarial defense by treating each data differently. One line of work [27]\u2013[30] focuses on reweighting the loss function by assigning larger weights to important data points. Zhang et al. [27] (GAIRAT) and Wang et al. [28] (MAIL) observed that data points with smaller margins, i.e., the dis-tance from data points to the decision boundary of the model, are more prone to adversarial attacks. GAIRAT estimated the margin by counting the number of iterations in the projected descent method required to generate a misclassified adversarial example, while MAIL introduced the probabilistic margin which leverages the posterior probabilities of classes. Both methods then reweight the loss function by giving greater weight to data points with smaller margins. Wang et al. [29] (MART) suggested that misclassified examples are crucial for enhancing robustness and therefore introduced additional regularization on misclassified examples.\nAnother line of work [31]\u2013[33] treating data unequally is to provide an adaptive perturbation bound instead of a fixed one. Ding et al. [31] (MMA) identified the margin for each data at each training epoch and utilized it as its perturbation magnitude. This allowed MMA to achieve margin maximization and improve the model's adversarial robustness. Cheng et al. [32] (CAT) determined a non-uniform effective perturbation length by utilizing the customized training labels. Although not explicitly mentioned, Zhang et al. [23] (FAT) also employed adaptive perturbation bounds through training on friendly adversarial data that minimizes classification loss among the certainly misclassified adversarial examples. Driven by the motivation that using a common perturbation level may be inefficient and suboptimal, we also adopt varying perturbation bounds in this paper.\nWhile many recent studies have concentrated on the con-nection between margin and adversarial learning, we introduce a new method that takes a different perspective of viewing ad-"}, {"title": "II. STANDARD ADVERSARIAL TRAINING", "content": "In this section, we provide an overview of standard adver-sarial training [16] and its implementation."}, {"title": "A. Notation", "content": "We focus on multiclass classification problems with c classes. Denote D the data manifold and X the input feature space. The training dataset {(xi, Yi)}=1 is sampled from D, where xi \u2208 X and yi \u2208 Y = {1,...,c}. Our discussion will be mainly based on the metric space (X, ||\u00b7||p) when X = Rd. Let Bp(\u20ac) = {\u03b4 \u2208 X : ||\u03b4|| \u2264 \u20ac} be the closed e-ball centered at the origin 0 \u2208 X. We consider the classifier f(.; 0) : X \u2192 RC parametrized by 0, where the i-th element of the output is the score of the i-th class. The goal of the classification problem is to train a classifier f that minimizes E(x,y)~D[l(f(x; 0), y)], where l : R\u00a3 \u00d7 Y \u2192 R is the classification loss."}, {"title": "B. Learning Objective", "content": "Madry et al. [16] proposed a standard adversarial training (standard AT) that solves a min-max optimization problem with the following objective function:\n$\\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n l(f(x_i^*; \\theta), Y_i),$\n(1)\nwhere\n$x_i^*= x_i + \\underset{\\delta \\in S}{\\operatorname{arg \\max}} l(f(x_i + \\delta; \\theta), Y_i)$\n(2)"}, {"title": "C. Projected Gradient Descent", "content": "The objective function of adversarial training involves two main steps: generating adversarial examples (inner maximiza-tion; Equation (2)) and minimizing the loss function on the generated examples with respect to 8 (outer minimization; Equation (1)). The most widely used approach for finding adversarial examples is the projected gradient descent (PGD) method, as introduced by Madry et al. [16]. Standard AT also employs the PGD method to obtain an approximate solution for the inner maximization problem. Given a natural data x(0), we find an adversarial example of \u00e6(0) by iteratively computing the following:\n$x^{(t+1)} = \\Pi_{x^{(t)}+S}(x^{(t)} + \\alpha \\cdot \\text{sign}_{x^{(t)}}\\ell(f(x^{(t)};\\theta),y)$ (3)\nwhere x(t) is the adversarial example at step t, y the class label of x(0), II the projection operation, a the step size, and sign the sign function. This procedure with k iterations is denoted PGD in this paper."}, {"title": "III. TANGENT DIRECTION GUIDED ADVERSARIAL TRAINING (TART)", "content": "In this section, we propose Tangent diRection guided ad-versarial Training (TART) and its realization. Standard AT uses a common adversarial set S = Bp(\u20ac) for all training data, while TART provides a reasonable perturbation set utilizing information from the data manifold and tangent space. TART was motivated by the concern that training models on adversarial examples with large normal components can have a significant change in the decision boundary and ultimately reduce the model's accuracy on clean data.\nTo implement TART, we first craft an adversarial example x of x with the adversarial set S as described in Equa-tion (2). After estimating the tangent space at xi, we calculate the tangential component of x. In this context, the term tangential component refers to the norm of the component. TART assigns an adaptive perturbation limit e to the i-th adversarial example based on its tangential component. Ad-versarial examples with larger tangential components receive larger ei, while those with smaller tangential components receive smaller 6. With the selected ei, we generate the most adversarial data \u00e6** and use it for our training:\n$x_i^{**} = x_i + \\underset{S\\in S_i}{\\text{arg max}} \\ell(f(x_i + \\delta; \\theta), y_i),$ (4)\nwhere Si = Bp(ei). Equation (4) may be considered as a generalization of standard AT since it recovers Equation (2) when \u20ac = \u20ac for all i.\nThe overall procedure of TART is illustrated in Figure 1. Figure 2 depicts the distribution of tangential components and angles observed in adversarial examples. The angle of x refers to the angle between the tangent space at xi and the per-turbation vector xxi. We observe that adversarial examples indeed have different tangential components and angles and, hence, may serve as important factors in adversarial training. In the following sections, we discuss in detail how we may approximate the tangent space (Section III-A), compute the tangential component (Section III-B), and choose an adaptive perturbation bound used for the TART training (Section III-C)."}, {"title": "A. Finding the Tangent Space", "content": "For datasets where the data manifold is known, the tangent space can be obtained explicitly and it can be used immedi-ately to proceed with TART. However, for benchmark or real-world datasets, we often do not know the corresponding data manifold and thus need to approximately estimate the tangent space.\nSuppose that dataset {x} < Rd lies in a k-dimensional manifold embedded in Rd, where k < d. Indeed, many stud-ies have demonstrated that the intrinsic dimension of image"}, {"title": "B. Computing Tangential Component", "content": "Using the estimation of tangent space, we compute the tangential component of adversarial examples. Let A be a dxk matrix whose columns are the tangent vectors of a natural data x \u2208 Rd. We note that the column space of A is the k-dimensional tangent space at \u00e6. Also, let x* be an adversarial example generated by perturbing \u00e6 and denote the projection of x* -x onto the tangent space of x as w. Then the tangential component of x* is defined as the norm of w. Here we employ a well-known fact that \u03a0\u2081\u03c5 is the projection of a vector v onto the column space of A, where \u041f\u2081 := A(ATA)\u00af\u00b9AT. A concise overview of the projection matrix \u041f\u0104 is provided in Appendix A. Therefore, the tangential component of x* can be computed as follows:\n$\\begin{aligned}||w|| &= ||\\Pi_A(x^* - x) ||\\\\&= ||A(A^TA)^{-1}A^T (x^* \u2013 x)||\n\\end{aligned}$ (6)"}, {"title": "C. Selecting the Perturbation Bound e", "content": "The final step in realizing TART is to select a suitable perturbation bound e for each data during training. We first generate adversarial examples with a perturbation bound of Emax for each training data. Next, the tangential components computed in Section III-B are used as the criterion for choos-ing the correct \u20ac. The fundamental idea is to assign larger or smaller e values to data with larger or smaller tangential components. While several assignment methods exist, TART adopts a straightforward approach providing Emax to the upper 50% and 0 to the lower 50%. The perturbation bound of i-th data in a mini-batch can be expressed as:\n$ \\epsilon_i = I(TC_i > \\text{median}\\{TC_k\\}_{k=1}^B ).\\epsilon_{\\text{max}}$ (7)\nwhere TC is the tangential component of i-th data and B is the mini-batch size. The main advantage of this simple assignment is that it avoids the need to regenerate adversarial images. When 0 < \u20aci < \u20acmax, an adversarial example needs to be recreated with the new perturbation bound 6. However, when \u20ac\u2081 = 0, the original image can be used directly, and when Ei = Emax, the adversarial image used to compute the tangen-tial component can be reused for training. This significantly reduces the training time, as generating adversarial examples is computationally expensive. Algorithm 2 summarizes the overall training process of TART.\n1) Theoretical Justification for e Selection: We provide a theoretical explanation for selecting the perturbation bound 6. We show that training models on adversarial examples with large normal components, which are far from the data manifold, can decrease clean accuracy performance. For sim-plicity, we focus on a binary classification problem with a mean absolute loss function L. Let the feature space be XC Rd, where data points and labels lie in X \u00d7 {0,1}. We assume the distributions of the clean training set and test set are identical, denoted as P. The distribution of adversarial examples, on which we train the model, is denoted as Q."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we first demonstrate the superiority of our proposed training technique TART through a simulated exper-iment using a transformed hemisphere dataset. In addition, we evaluate the performance of TART on CIFAR-10 and illustrate that TART can be effectively combined with various other adversarial training methods."}, {"title": "A. Transformed Hemisphere", "content": "We conduct an experiment to compare the efficacy of train-ing on adversarial examples with large normal components versus training on adversarial examples with large tangen-tial components. Consider a unit hemisphere in R\u00b3 whose tangent space can be computed without any approximation. The hemisphere is evenly divided into c regions. The data are first sampled from the hemispherical surface and transformed to a high dimensional space Rd with a linear transformation T : R3 \u2192 Rd, where T = (V1,V2, V3) and V1, V2, V3 are three orthonormal vectors in Rd. Let z \u2208 R\u00b3 be the point sampled from the unit hemisphere. Then x = T(z) is the simulated data used for classification where the data class labels are determined by which region z is located. Since z lies on a sphere, we can explicitly compute the tangent vectors U1, U2 of z. Next, the tangent vectors of \u00e6 can be obtained immediately by performing transformation of u\u2081 and \u0e192: W\u2081 = T(u\u2081) and w\u2082 = T(u2). Using the tangent vectors w\u2081 and w2, we may calculate the tangential component of adversarial examples and implement TART on the simulated dataset.\nThe goal of this experiment is to confirm the validity of our concepts and assess the effectiveness of TART. We compare our idea (TART) to the opposite of our idea (Reverse-TART), i.e., provide smaller perturbation bounds to adversarial examples with larger tangential components. To see a more pronounced effect, we compute the tangential components at each epoch and use only half of the adversarial examples generated (examples with the largest 25% and the smallest 25% tangential component). The perturbation bound for each adversarial example while training is determined as follows:\n\u2022 TART: provide e for the largest 25% and 0 for the smallest 25% as their perturbation bounds.\n\u2022 Reverse-TART: provide 0 for the largest 25% and e for the smallest 25% as their perturbation bounds.\nWe train a DNN with two hidden layers in various ex-perimental settings: dimension d \u2208 {100,200,400}, num-ber of classes c\u2208 {4,8,16}, maximum perturbation \u20ac \u2208 {0.01, 0.03, 0.05}. The models are trained using stochastic gradient descent (SGD) with momentum of 0.9 and weight decay of 0.0002 for 50 epochs. We use an initial learning rate of 0.1 which is divided by 10 at epoch 30 and 45. The adversarial examples used for training are generated by PGD10 with random start and step size a = \u20ac/4. We note that a perturbation bound of 0 implies using natural images without performing any PGD attacks."}, {"title": "B. Performance Evaluation on CIFAR", "content": "In this section, we assess the performance of TART on CIFAR-10 [40]. We also verify the versatility of TART by successfully integrating it with other existing defense methods. Specifically, we combine TART with the following defense approaches: (1) Standard AT [16], (2) TRADES [20], (3) MART [29], and (4) GAIRAT [27].\n1) Defense Settings: We use WideResNet-32-10 [43] (WRN-32-10) as in GAIRAT [27], FAT [23], and MAIL [28]. We train each defense model for 100 epochs using SGD with momentum 0.9, weight decay 0.0002. We initially use a learning rate of 0.1 and divide it by 10 at the 60-th and 90-th epochs. For the training attack, we use the PGD7 attack with a random start, lo perturbation bound Emax = 8/255, and step size a = 2/255. The hyperparameters featured in each method are configured to match those used in the original paper: \u03b2 = 6 for TRADES, X = 5 for MART. Also, only for MART, we followed using a maximum learning rate 0.01 and weight decay 0.0035 as in Wang et al. [29]. Note that data augmentation techniques were not used during the experiment.\n2) Evaluation Metrics: We report both the clean test ac-"}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a new adversarial training method called Tangent Direction Guided Adversarial Training (TART), which leverages the tangent space to improve clean accuracy. Our motivation stemmed from the widely accepted observation that data often exhibits lower-dimensional structures. We an-ticipated that leveraging information about the data manifold for adversarial training would enhance its effectiveness. TART allocates larger or smaller perturbation limits to adversarial examples with larger or smaller tangential components, re-spectively. We experimentally demonstrated that TART can effectively improve clean accuracy with minimal or even no impact on robustness. In the future, we aim to explore the optimal epsilon for each sample by theoretically investigating"}, {"title": "APPENDIX A PROJECTION MATRIX", "content": "For a matrix A of size nxk with full column rank, we define CA as the column space of A, which is a linear subspace in Rn spanned by the columns of A. The column space CA can be represented as:\n$C_A = \\{\\beta_1u_1 + \\beta_2u_2 + \u00b7\u00b7\u00b7 \\beta_ku_k : \\beta_1, \\beta_2,\u00b7\u00b7\u00b7,\\beta_k\\in R\\}$ (8)\n$= \\{A\\beta : \\beta \\in R^k\\}$ (9)\nwhere u1, u2,\u2026, uk are the columns of A.\nA projection matrix Ia := A(ATA)\u00af\u00b9AT is an n\u00d7n square matrix associated with a linear operator that projects a vector in Rn onto a subspace CA. The projection matrix \u041f\u0410 can be derived using the property that for any vector v \u2208 R\", the vector v \u03a0\u03b1\u03bd is orthogonal to the column space CA. Also, there exists a vector \u03b2 \u2208 Rk such that \u03a0\u03bf\u03c5 = \u0391\u03b2 since \u03a0\u03bf\u03c5 lies in the column space of A. Therefore,\nv - \u03a0\u0391v = v \u2013 \u0391\u03b2 1 CA (10)\n\u21d4 \u0391\u0384 (\u03bd \u2013 \u0391\u03b2) = 0 (11)\n\u21d4 \u03b2 = (ATA)-1\u0391\u03c4\u03c5 (12)\n\u21d4 \u041f\u2081 = A(A\u00a8A)-1A\u0f0b. (13)\nIt is important to highlight that ATA is always invertible when A has a full column rank, ensuring the existence of the projection matrix \u03a0\u0391.\""}, {"title": "APPENDIX B PROOF OF PROPOSITION III.1", "content": "Proposition III.1. For any function f,\n|Rp(f) \u2013 R2(f)| \u2264 4 TV(P, Q),\nwhere TV(P, Q) denotes the total variance distance between P and Q.\nProof. Let P ~ p and Q ~ q. Then,\n$\\begin{aligned}|R_Q(f) - R_P(f)| &= |\\int L(f(x), h(x))q(x)dx - \\int L(f(x), h(x))p(x)dx|\\\\&\\leq |\\int L(f(x), h(x)) |p(x) - q(x)| dx|\\\\&\\leq 2 ||P-Q||_1 = 4TV(P,Q),\\end{aligned}$\nwhere the last equality holds due to the relationship between the L\u00b9 norm and the total variance distance."}]}