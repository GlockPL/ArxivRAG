{"title": "LONGGENBENCH: Long-context Generation Benchmark", "authors": ["Xiang LIU", "Peijie DONG", "Xuming HU", "Xiaowen CHU"], "abstract": "Current long-context benchmarks primarily focus on retrieval-based tests, requiring Large Language Models (LLMs) to locate specific information within extensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark. Long-context generation refers to the ability of a language model to generate coherent and contextually accurate text that spans across lengthy passages or documents. While recent studies show strong performance on NIAH and other retrieval-based long-context benchmarks, there is a significant lack of benchmarks for evaluating long-context generation capabilities. To bridge this gap and offer a comprehensive assessment, we introduce a synthetic benchmark, LongGenBench, which is designed to evaluate the long-context generation capabilities of large language models (LLMs), with a particular focus on consistency in logical flow. LongGenBench redesigning the format of questions and necessitating that LLMs respond with a single, cohesive long-context answer. Upon extensive evaluation using LongGenBench, we observe that: (1) both API accessed and open source models exhibit performance degradation in long-context generation scenarios, ranging from 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of performance degradation, with the GEMINI-1.5-FLASH model showing the least degradation among API accessed models, and the QWEN2 series exhibiting the least degradation in LongGenBench among open source models.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have become pivotal in tackling NLP downstream tasks such as summarization and question answering that require interpreting extensive context from books, reports, and documents, sometimes spanning tens of thousands of tokens (Raffel et al., 2020; Brown et al.,"}, {"title": "Related Work", "content": "Recent advancements in techniques such as efficient attention, long-term memory, extrapolative positional embedding, and context processing have spurred the development of numerous long-context LLMs (Huang et al., 2023). Efficient attention"}, {"title": "Evaluation for Long-context Language Models", "content": "Numerous investigations into long-context model benchmarks have primarily focused on retrieval and understanding tasks. In the realm of retrieval benchmarks, the datasets used are predominantly synthetic, enabling precise control over experimental conditions, such as input token length, and minimizing the influence of varied parametric knowledge from different training strategies. Recent research has extensively focused on synthetic tasks designed for retrieval (Kamradt, 2023; Mohtashami and Jaggi, 2023; Li et al., 2023; Liu et al., 2024c; Hsieh et al., 2024; Hu et al., 2024a,b; Zhang et al., 2024a), with additional studies exploring the use of long contexts for various types of reasoning (Tay et al., 2021). For understanding benchmarks, LongBench (Bai et al., 2023b) includes evaluations in a bilingual context, covering long-document question answering, summarization, and code completion tasks. ZeroSCROLLS (Shaham et al., 2023)"}, {"title": "LongGenBench", "content": "We propose LongGenBench, a synthetic benchmark that is an efficient, low-cost approach focused on evaluating long-context generation in LLMs."}, {"title": "Motivation", "content": "Traditionally, evaluating LLMs for long-context scenarios involves inputting lengthy essays into the models, followed by either retrieval or comprehension questions as depicted in Figure 2(a) and (b). The token length of these essays typically ranges from {4K, 8K, 16K, 32K, 64K, 128K}, with advanced long-context LLMs like Gemini-1.5 (Reid et al., 2024) being tested up to 1M tokens. However, these benchmarks tend to focus predominantly on the prompt tokens or input content, often neglecting the completion tokens or output content and the evaluation of performance regarding these aspects. Furthermore, traditional long-context benchmarks such as the NIAH test are costly, with a 128K NIAH test consuming 8M tokens."}, {"title": "Problem Definition", "content": "In LongGenBench, the initial step involves redesigning the input prompt format to enable LLMs to generate long-context responses as illustrated in Figure 2(c). We refine the system prompt and restructure the question format so that $K$ questions are sequentially concatenated after the system prompt. Subsequently, the LLMs are expected to adhere to this redesigned prompt and produce a coherent long-context response that answers all $K$ questions. These responses are then parsed to verify the answers to the $K$ questions, where the LLMs must maintain both the sequence and accuracy to demonstrate improved performance in LongGenBench. This process is repeated for $T$ iterations to assess the robustness of the LLMS' long-context generation capabilities at each length, with each iteration featuring unique questions.\nThe Algorithm 1 gives a pseudocode outline for the LongGenBench. The system prompt $S$ contains instructional information, while $Q$ is a list of questions from the original dataset. For each iteration $t$, a batch of $K$ questions, $Q_t$, is selected from $Q$ within the range $[t \\times K : (t+1) \\times K]$. The selected questions are concatenated to the system $S$ to form the Input Prompt. The language model $LLM$ generates a long-context response for the given Input Prompt. The response is added to the response set $R$, then parsed and verified for correctness and sequence. This process is repeated for $T$ iterations, with each iteration featuring a unique set of questions. The final output is the set of long-context responses $R$.\nDuring the generation process, LLMs may accumulate correct or incorrect reasoning steps, which fall within the scope of LongGenBench's evaluation. These models might generate errors during a single long-context session, and earlier mistakes can influence subsequent outputs. Assessing a model's performance in generating long texts involves evaluating how effectively it manages and mitigates these accumulated errors, and maintains consistency in logical flow. LongGenBench addresses this challenge by requiring models to handle and correct the impact of previous mistakes within a single long-context generation.\nThe conditional probability that the LLM generates the next token, given the prompt and the previously generated outputs, can be represented as:\n$P(x_{i+1} | \\text{InputPrompt}, x_1, x_2,..., x_i)$\nWhere $x_1, x_2,..., x_i$ are the tokens generated in LongGenBench, the LLMs are required to produce the output based on the InputPrompt and all previously generated tokens."}, {"title": "Dataset Construction", "content": "LongGenBench synthesizes three datasets from different domains: World Knowledge from MMLU (Hendrycks et al., 2021), Arithmetic from GSM8K (Cobbe et al., 2021), and Commonsense Reasoning from CommonSenseQA (Talmor et al., 2019). The MMLU dataset measures a model's ability to understand and reason across 57 diverse categories, using accuracy as the primary evaluation metric. The GSM8K dataset evaluates arithmetic problem-solving skills through 8,000 grade-school level math word problems, using the solving rate as the main metric. CommonSenseQA tests commonsense reasoning with multiple-choice questions based on ConceptNet, with accuracy as the evaluation metric."}, {"title": "Expeirments Setting", "content": "In this section, we describe the details of the baseline models and the LongGenBench approach, as well as their implementation in the subsequent experiments. All experiments were conducted three times, using the mean score to ensure robustness."}, {"title": "Models and Inference setup", "content": "We evaluated multiple LLMs using LongGen-Bench, categorizing them into API accessed models and open-source models. For API accessed models, we selected GPT-3.5-Turbo (Ouyang et al., 2022; Brown et al., 2020), GPT-4o (OpenAI, 2024), Gemini-1.5-Flash (Reid et al., 2024), and Claude-3-Haiku (Anthropic, 2024). For open-source models, our selection included LLaMA-3-8B-Instruct, LLaMA-3-70B-Instruct (Meta, 2024), Qwen2-7B-Instruct, Qwen2-57B-A14B-Instruct, Qwen2-72B-Instruct (Bai et al., 2023a), ChatGLM4-9B-Chat (Zeng et al., 2022; Du et al., 2022), and DeepSeek-v2-Chat (DeepSeek-AI, 2024). The API accessed models are configured with a specific maximum output length, which constrains the number of output tokens due to the computational resources and commercial policies of each API provider. For open-source models, we remove the INSTRUCT or CHAT suffix. The prompt settings and datasets follow the guidelines from the Chain-of-Thought (Wei et al., 2022; Wang et al., 2022; Diao et al., 2024; Fu et al., 2023b; Pan et al., 2024), and API model access is provided through the official website. We assessed all open-source models using the vLLM framework (Kwon et al., 2023), which offers efficient KV cache memory management and a Flash attention (Dao et al., 2022; Dao, 2024) backend. All open source models run on RTX4090 and RTX A6000 servers."}, {"title": "Task configurations", "content": "LongGenBench generates results for three datasets, designated as LongGenBench-MMLU, LongGenBench-GSM8K, and LongGenBench-CSQA. details the configurations for the LongGenBench experiments. In this context, $K$ represents the number of questions that the LLM must answer in a single response, while $T$ denotes the number of iterations, also known as query times. The total number of questions addressed is calculated using the formula $K \\times T$. To better compare the long-context generation capabilities between API accessed models and open source models, the maximum output length is uniformly set at 4096 tokens in the main experiments. For LongGenBench-MMLU, the $T$ value is considered based on the number of categories. Categories with excessively long input prompts are excluded. In our main experiment, we arrange the questions in ascending order based on their length, setting the order within a single query from the shortest to the longest length. A detailed ablation study of this variant is discussed in Section 6."}, {"title": "Result", "content": null}, {"title": "API Accessed Models", "content": "displays the performance of various models on the GSM8K and MMLU datasets under two scenarios: Baseline and LongGenBench. The Delta column shows the change in performance when applying LongGenBench relative to the Baseline, with negative values indicated by a downward triangle symbol (\\/) signifying performance degradation. The results demonstrate that all models undergo a performance degradation when evaluated under the LongGenBench conditions. Notably, GPT-3.5-Turbo and Claude-3-Haiku exhibit the largest Delta on both LongGenBench-MMLU and LongGenBench-GSM8K, indicating significant challenges in managing long-context generation. Conversely, the Gemini-1.5-Flash model exhibits the smallest performance degradation, suggesting greater robustness and enhanced consistency in handling long-context scenarios."}, {"title": "Open Source Models", "content": "The results presented in Table 4 provide a comparative analysis of the performance of various open-source models on the GSM8K and MMLU datasets under baseline conditions and using the LongGenBench approach. Several key observations can be made from these results:\nCorrelation Between Baseline and LongGen-Bench: There appears to be a general correlation between the baseline performance and the degree of performance degradation observed with the LongGenBench approach. Models with higher baseline performance tend to exhibit smaller performance drops. For example, Qwen2-72B-Instruct and DeepSeek-v2-Chat models, which have high baseline scores, show relatively small Delta values across both datasets. However, there are exceptions, such as LLaMA-3-70B-Instruct, which despite its high baseline performance, exhibits a significant performance drop on both datasets. Additionally, LLaMA-3-8B-Instruct, Qwen2-57B-Instruct, and ChatGLM4-9B-Chat models have the same baseline score on GSM8K, yet their Delta values differ substantially (47.1%, 8.4%, and 10.8%, respectively).\nImpact of Model Size on Performance Degradation: Observing models within the same series but with different sizes, such as the LLaMA-3 series and the Qwen2 series, reveals a trend where larger models generally exhibit smaller Delta values. This suggests that increasing model size can mitigate performance degradation in long-context generation tasks. For instance, within the LLaMA-3 series, LLaMA-3-70B-Instruct shows a much smaller Delta compared to LLaMA-3-8B-Instruct across both datasets.\nVariation Among Different Model Architectures: Different model architectures demonstrate varying trends in performance degradation. For models within the 7 ~ 9B parameter range, such as LLaMA-3-8B-Instruct, Qwen2-7B-Instruct, and ChatGLM4-9B-Chat, there are notable differences in Delta values despite similar baseline performances. For example, LLaMA-3-8B-Instruct has a Delta of 47.1% on GSM8K, while ChatGLM4-9B-Chat has a Delta of only 10.8%, indicating significant variation in how different architectures handle long-context generation tasks.\nConsistency Across Tasks for Individual Models: Individual models exhibit consistent trends in performance degradation across different LongGenBench tasks. For instance, LLaMA-3-8B-Instruct consistently shows the largest Delta values on both datasets, indicating a significant drop in performance when generating long-context responses. Conversely, Qwen2-72B-Instruct and DeepSeek-v2-Chat consistently show minimal Delta values, suggesting better resilience in long-context tasks. These findings underscore the importance of considering both model architecture and size when evaluating the performance of LLMs in long-context generation tasks. The LongGenBench approach effectively highlights the varying capabilities of different models to maintain accuracy over extended text generation, providing valuable insights for further model development and optimization."}, {"title": "Length Distribution", "content": "illustrates the output length distribution for various models in the LongGenBench-GSM8K task, with experimental configurations as detailed . Most models produce output lengths close to or exceeding 3500 characters, although none exceed the 4096-character limit. This data demonstrates that LongGenBench effectively facilitates long-context generation in LLMs."}, {"title": "Ablation Studies", "content": "To gain deeper insights into LongGenBench, we conducted an ablation study focusing on two critical hyperparameters: reconstructive processing and the order of K questions within a single query. For reconstructive processing, we compared the baseline format with the LongGenBench format. The baseline format follows the CoT (Wei et al., 2022) setting, featuring eight question-answer pairs in sequence. In contrast, the LongGenBench format presents eight questions in advance followed by the corresponding eight answers, with the K questions being addressed subsequently. Regarding the order of K questions, we evaluated three sequences: the original order from the dataset, ascending order from shortest to longest, and descending order from longest to shortest. This ablation study was performed using GPT-3.5-Turbo and Gemini-1.5-Flash on the GSM8k dataset, with the number of iterations set at $T = 20$. presents the results of this hyperparameter ablation study, demonstrating that both hyperparameters are crucial for LongGenBench to effectively evaluate the long-context generation capabilities of LLMs.\nEvaluating Long Input Comprehension To address the potential concern that performance degradation in LongGenBench may be due to the model's inability to comprehend long inputs rather than its ability to generate long outputs, we conducted additional experiments. Specifically, we designed a set of experiments where the model is provided with a long input containing multiple questions but is required to answer only one specified question at a time. This \"long input + short output\" setting helps isolate the model's comprehension ability from its generation capacity.\nFor this experiment, we again used GPT-3.5-Turbo and Gemini-1.5-Flash on the GSM8k dataset. We provided the models with a long input sequence of $K$ questions but instructed them to respond to only one randomly selected question per query. This setup was repeated for $T = 20$ iterations to ensure robust evaluation. The results, indicate that both models maintain high accuracy when required to produce short outputs from long inputs. This supports the hypothesis that the primary challenge in LongGenBench lies in the generation of long outputs rather than comprehension of long inputs."}, {"title": "Conclusion", "content": "In this study, we introduced LongGenBench, an effective framework designed to evaluate the long-context generation capabilities of language models (LLMs) across multiple datasets. Our experiments included both API accessed and open source models, offering a comprehensive comparison of their performance in long-context generation tasks. The results indicate a correlation between baseline performance and LongGenBench performance, with higher baseline models generally exhibiting smaller declines. Additionally, model size and architecture significantly influence resilience, with larger models and specific architectures demonstrating greater robustness and consistent trends across different LongGenBench tasks. These findings highlight the importance of considering both model architecture and size when evaluating LLMs in long-context generation tasks. The LongGenBench framework"}, {"title": "Limitations", "content": "Our study has several limitations. Firstly, the experiments were conducted on a limited set of models and datasets, which may not fully represent the diversity of available LLMs and tasks. Secondly, we did not explore experiments with larger $K$ values due to constraints on the maximum output tokens imposed by API accessed models. Lastly, we did not include experiments with long-context techniques, which may help mitigate the observed performance degradation. These limitations suggest that further research is needed to generalize our findings across a broader range of models and more extended context scenarios."}]}