{"title": "The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence", "authors": ["Tom Wollschl\u00e4ger", "Jannes Elstner", "Simon Geisler", "Vincent Cohen-Addad", "Stephan G\u00fcnnemann", "Johannes Gasteiger"], "abstract": "The safety alignment of large language models (LLMs) can be circumvented through adversarially crafted inputs, yet the mechanisms by which these attacks bypass safety barriers remain poorly understood. Prior work suggests that a single refusal direction in the model's activation space determines whether an LLM refuses a request. In this study, we propose a novel gradient-based approach to representation engineering and use it to identify refusal directions. Contrary to prior work, we uncover multiple independent directions and even multi-dimensional concept cones that mediate refusal. Moreover, we show that orthogonality alone does not imply independence under intervention, motivating the notion of representational independence that accounts for both linear and non-linear effects. Using this framework, we identify mechanistically independent refusal directions. We show that refusal mechanisms in LLMs are governed by complex spatial structures and identify functionally independent directions, confirming that multiple distinct mechanisms drive refusal behavior. Our gradient-based approach uncovers these mechanisms and can further serve as a foundation for future work on understanding LLMs.", "sections": [{"title": "1. Introduction", "content": "The breakthrough of scaling large language models (LLMs) has led to an unprecedented leap in capabilities, driving widespread real-world adoption (OpenAI, 2022). However, these advancements also introduce serious risks. As artificial intelligence becomes more powerful, it can be misused for harmful purposes, such as attacking critical infrastructure or spreading misinformation. Ensuring that these models remain aligned with human values has become a crucial research challenge (Liu et al., 2023; Schwinn et al., 2025). Despite significant progress, LLMs, like all machine learning models, remain vulnerable to adversarial attacks that can bypass alignment mechanisms and induce harmful outputs (Szegedy et al., 2014; Carlini et al., 2024).\nRecent work in interpretability has provided valuable insights into how LLMs encode and process information (Nanda et al., 2024; Wang et al., 2022; Cunningham et al., 2023; Heinzerling & Inui, 2024). Prior studies (Belrose et al., 2023; Gurnee & Tegmark, 2023; Marks & Tegmark, 2024) suggest that concepts-ranging from simple to complex-are often encoded linearly in the model's residual stream. Methods such as representation engineering (Zou et al., 2023a) allow researchers to use input prompts to analyze model behavior by extracting and manipulating such concepts. However, the mechanisms enabling adversarial jailbreaks that bypass alignment safeguards remain poorly understood. Some evidence suggests that refusals to harmful queries are mediated by a single \"refusal direction\" in activation space (Arditi et al., 2024), and that jailbreaks rely on manipulating this direction (Yu et al., 2024), yet these assumptions require further examination.\nIn this work, we go beyond extracting concepts using common input prompt methods by introducing a novel gradient-based approach to representation engineering which we use to investigate the mechanisms underlying refusal behavior in LLMs. We extract refusal-mediating directions more effectively, improving both precision and control while minimizing unintended side effects, which we demonstrate in Section 4. Unlike prior work that assumes model refusal is controlled by a single linear direction, we show in Section 5 that there exist multi-dimensional polyhedral cones which contain infinite refusal directions; we show an illustrative example in Figure 1. To further characterize refusal mechanisms in language models, we introduce representational independence, a criterion for identifying directions that remain mutually unaffected under intervention, capturing both linear and non-linear dependencies across layers. In Section 6, we demonstrate that even under this strict notion of independence, multiple complementary refusal directions exist.\nTo summarize, our core contributions are:\n\u2022 We show that our gradient-based representation engineering can advance general LLM understanding and specifically demonstrate its efficacy for understanding refusal mechanisms.\n\u2022 We introduce representational independence, a practical framework for characterizing how different interventions interact within an LLM's activation space, and use it to find independent refusal directions.\n\u2022 We show that rather than a single refusal direction, there exist multi-dimensional cones in which all directions mediate refusal."}, {"title": "2. Background", "content": "Notation.\nLet $f: \\mathcal{T}^{N_{\\text{seq}}} \\rightarrow \\triangle^{N_{\\text{seq}} \\times |\\mathcal{T}|}$ denote a language model, where $\\triangle^{|\\mathcal{T}|}$ is the probability simplex over vocabulary $\\mathcal{T}$. Given a prompt $p = (t_1,...,t_{N_{\\text{seq}}}) \\in \\mathcal{T}^{N_{\\text{seq}}}$ consisting of tokens $t_i$, each token is first embedded: $x_i^{(0)} = \\text{EMBED}(t_i)$. The model then processes the token sequence through $L$ layers, where at each layer $l = 1, . . ., L$ and token position $i$ the following transformation is applied:\n$x_i^{(l+1)} = x_i^{(l)} + \\text{ATTN}^{(l)}(x_i^{(l)}), x_i^{(l+1)} = x_i^{(l)} + \\text{MLP}(x_i^{(l)})$\nThe final residual stream $x_i^{(L+1)}$ is unembedded to yield logits: $l_i = \\text{UNEMBED}(x_i^{(L+1)})$. The softmax function converts these logits into a probability distribution over tokens: $P(t | t_1,...,t_i) = \\text{softmax}(l_i)_t$. We omit technical details that are not critical for this work such as LayerNorm.\nExtracting refusal directions. Paired prompts of harmful and harmless requests allow the extraction of a directional feature from the model's residual stream as shown by prior work (Panickssery et al., 2024; Bolukbasi et al., 2016; Burns et al., 2024). Recent studies obtain this direction by computing the difference-in-means (DIM) (Panickssery et al., 2024; Arditi et al., 2024; Stolfo et al., 2024) between model representations on datasets of harmful prompts $\\mathcal{D}_{harm}$ and harmless prompts $\\mathcal{D}_{good}$:\n$\\upsilon_i^{(l)} = \\frac{1}{|\\mathcal{D}_{harm}|} \\sum_{p'\\in\\mathcal{D}_{harm}} x_i^{(l)}(p') - \\frac{1}{|\\mathcal{D}_{safe}|} \\sum_{p\\in\\mathcal{D}_{safe}} x_i^{(l)}(p)$\nHere, $x_i^{(l)}(p)$ represents the residual stream activations at position $i$, layer $l$ for input prompt $p$.\nAdversarial steering attacks. The extracted harmfulness direction can be used to manipulate the model's refusal behavior. With white-box access, an attacker can prompt the model with harmful queries and suppress activations in the harmfulness direction, thereby reducing the model's probability of refusal. This can be done through directional ablation of $r$ (where $\\hat{.}$ denotes the unit vector) (Zou et al., 2023a):\n$\\Tilde{x}_{i}^{(l)} = x_{i}^{(l)} - (x_{i}^{(l)} \\cdot \\hat{r}) \\hat{r}$ (1)\nwhich projects the residual stream to a subspace orthogonal to $r$, or alternatively through activation subtraction:\n$x_{i}^{(l)} = x_{i}^{(l)} - a r$ (2)\nwhich subtracts a scaled $r$ from the residual stream. We follow common practice to apply both operations across all token positions and ablation across all layers while doing subtraction only at a single layer."}, {"title": "3. Related Work", "content": "Adversarial attacks for LLMs. Many studies have explored hand-crafted adversarial techniques, such as persona modulation (Shah et al., 2023), language modifications (Zhu et al., 2023), or prompt engineering using repetitions and persuasive phrasing (Rao et al., 2024). Other works take a more systematic approach, employing techniques like genetic algorithms and random search (Chen et al., 2024), discrete optimization over input tokens (Zou et al., 2023b), or gradient-based methods to identify high-impact perturbations (Geisler et al., 2024). While identifying these vulnerabilities enables adversarial fine-tuning (Xhonneux et al., 2024) or improved training through Reinforcement Learning with Human Feedback (RLHF), recent works suggest that robustness remains a challenge (Zou et al., 2023a; Schwinn et al., 2024; Geisler et al., 2024; Scholten et al., 2025).\nInterpretability of LLMs. A parallel line of research focuses on understanding the internal mechanisms of LLMs, as their natural language outputs provide a unique opportunity to link internal states to interpretable behaviors. Interpretability research has led to the identification of various \"features\"-concepts represented by distinct activation patterns (Cunningham et al., 2023)-as well as \"circuits\", which are subnetworks that implement a specific function or behavior. Prominent examples are backup circuits (Nanda et al., 2024) and information mover circuits (Wang et al., 2022). Many interpretability insights rely on extracting features using paired inputs with opposing semantics (Burns et al., 2024) and then manipulating residual stream activations to elicit specific behaviors (Panickssery et al., 2024). Representation engineering, as proposed by Zou et al. (2023a), investigates the linear representation of concepts such as truthfulness, honesty, and fairness in LLMs. The effectiveness of these methods supports the hypothesis that many features are encoded linearly in LLMs (Marks & Tegmark, 2024). These insights allow researchers to pinpoint and manipulate concept representations or specific circuits, enabling targeted debugging of behaviors, mitigating biases, and advancing safer, more reliable AI systems.\nUnderstanding Refusal Mechanisms. Recent research has focused on understanding the mechanisms underlying refusal behaviors in LLMs. For example, removing safety-critical neurons has been shown to decrease robustness (Wei et al., 2024; Li et al., 2024b). Zheng et al. (2024) demonstrate that adding explicit safety prompts shifts the internal representation along a harmfulness direction. O'Brien et al. (2024) propose to use sparse autoencoders to identify latent features that mediate refusal. The most relevant work to ours is Arditi et al. (2024), which builds on Zou et al. (2023a) and examines the representation of refusal in LLMs. Their work suggests that a single direction a model's activation space determines whether the model accepts or refuses a request. We challenge this notion by showing that refusal is mediated through more nuanced mechanisms."}, {"title": "4. Gradient-based Refusal Directions", "content": "Research Question: Can gradient-based representation engineering identify refusal directions?\nTo investigate the refusal mechanisms in language models, we propose a gradient-based algorithm that identifies directions controlling refusal in the model's activation space. We refer to it as Refusal Direction Optimization (RDO). Unlike prior approaches that extract refusal directions using paired prompts of harmless and harmful instructions (Arditi et al., 2024), our method leverages gradients to find better directions instead of solely relying on model activations. Similar to (Park et al., 2023), we define two key properties for refusal directions:\nDefinition 4.1. Refusal Properties:\n\u2022 Monotonic Scaling: when using the direction for activation addition/subtraction\n$x'=x+ar$, the model's probability of refusing instructions should scale monotonically with a.\n\u2022 Surgical Ablation: ablating the refusal direction through projection $x_i^{(l)} = x_i^{(l)} - r \\frac{r^T x_i^{(l)}}{||r||^2}$ should cause the model to answer previously refused harmful prompts, while preserving normal behavior on harmless inputs.\nWe can encode the desired refusal properties into loss functions, allowing us to find corresponding refusal vectors $r$ using gradient descent. For the monotonic scaling property, we train the model to refuse harmless instructions $p_{safe}$ when running the model $f$ with a modified forward pass $f_{add}(r,l)$ in which we add $r$ to the activations at layer $l$. We minimize the cross-entropy between the model output and target refusal response $t_{refusal}$. For the surgical ablation property, we similarly compute the cross-entropy between a harmful response target $t_{answer}$ and the output of a modified forward pass $f_{ablate}(r)$ to make the model respond to harmful instructions. A key strength of our gradient-based approach is the ability to control any predefined objective and thus we can control the extent to which other concepts are affected during interventions. For this, we use a retain loss based on the Kullback-Leibler (KL) divergence to ensure that directional ablation of $r$ on harmless instructions does not change the model's output over a target response $t_{retain}$. Algorithm 1 shows the full training procedure for our refusal directions.\nSetup. We construct a dataset of harmless and harmful prompts from the ALPACA (Taori et al., 2023) and SALAD-BENCH (Li et al., 2024a) datasets (see Appendix A.1). An important consideration for our algorithm is the choice of targets $t_{answer}$ and $t_{refusal}$. Generally, language models differ in their refusal and response styles, which is why we use model-specific targets rather than generating them via uncensored LLMs as in Zou et al. (2024). Specifically, we use the DIM refusal direction to generate our targets, though any effective attack can work. For the harmful answers $t_{answer}$, we ablate the DIM direction and generate 30 tokens. Similarly, we use activation addition on harmless instructions to produce refusal targets $t_{refusal}$. For helpful answers on harmless instructions that should be retained $t_{retain}$, we generate 29 tokens without intervention. The retain loss $\\mathcal{L}_{retain}$ is applied over the last 30 tokens, such that the last token of the model's chat template is included. We detail hyperparameters and implementation in Appendix A.\nEvaluation. We evaluate our method by training a refusal direction on various models from the Gemma 2 (Team et al., 2024), Qwen2.5 (Yang et al., 2024), and Llama-3 (Dubey et al., 2024) families and compare against the DIM direction for which we use the same setup as Arditi et al. (2024) but with our expanded dataset. For a fair comparison, we train the refusal direction at the same layer that the DIM direction is extracted from, and during activation addition/subtraction set the scaling coefficient $a$ to the norm of the DIM direction. We evaluate the jailbreak Attack Success Rate (ASR) on JAILBREAKBENCH (Chao et al., 2024) using the STRON-GREJECT fine-tuned judge (Souly et al., 2024). For inducing refusal via activation addition, we test 128 harmless instructions sampled from ALPACA using substring matching of common refusal phrases. Model completions for evaluation are generated using greedy decoding with a maximum generation length of 512 tokens.\nDoes the direction mediate refusal? In Figure 2, we show that for jailbreaking, our approach is competitive when using directional ablation and, on average, outperforms DIM when subtracting the refusal direction. Notably, despite not being explicitly optimized for subtraction-based attacks, our direction naturally generalizes to this setting. Figure 9 shows that adding the refusal direction to harmless inputs induces refusal more effectively with RDO than with DIM, further indicating that our method manipulates refusal more effectively.\nIs the direction more precise? To measure the side effects when intervening with the directions we track benchmark performance. Arditi et al. (2024) show that directional ablation with the DIM direction tends to have little impact on benchmark performance, except for TruthfulQA (Lin et al., 2021). In Table 1, we show that RDO impacts TruthfulQA performance much less severely, reducing the error by 40% on average.\nIs our method versatile? Hyperparameter tuning of the retain loss weight $\\lambda_{ret}$ in Algorithm 1 allows for balancing between attack success and side effects (see Appendix B). We observe that for many models-especially those in the Qwen 2.5 family-for the majority of estimated DIM directions, the side-effects are too high, rendering it an unsuccessful attack (see Figure 16). Our method is more flexible than previous work as we can choose the target layer freely while limiting side effects through the retain loss (if possible).\nKey Takeaways. Our RDO yields more effective refusal directions with fewer side effects, establishing that gradient-based representation engineering is an effective approach for extracting meaningful directions, while allowing for more modeling freedom such as incorporating side constraints."}, {"title": "5. Multi-dimensional Refusal Cones", "content": "Research Question: Is refusal in LLMs governed by a single direction, or does it emerge from a more complex underlying geometry?\nWe extend RDO to higher dimensions by searching for regions in activation space where all vectors control refusal behavior. For this, we optimize an orthonormal basis $\\mathcal{B} = [b_1,..., b_N]$ spanning an $N$-dimensional polyhedral cone $\\mathcal{R}_N = {\\sum_{i=1}^{N} \\lambda_i b_i | \\lambda_i \\geq 0}\\{0}$, where all directions $r \\in \\mathcal{R}_N$ satisfy the refusal properties (Definition 4.1). Since all directions in the cone correspond to the same refusal concept, we also refer to this as a concept cone. The constraint $\\lambda_i > 0$ ensures that all directions within the cone consistently strengthen refusal behavior. Without this constraint, allowing negative coefficients could introduce opposing effects, reducing the overall effectiveness. Enforcing orthogonality of the basis vectors prevents finding co-linear directions. Note that in practice, directions in activation space cannot be scaled arbitrarily high without model degeneration, which effectively bounds $\\lambda_i$.\nIn Algorithm 2, we describe the procedure to find the cone's basis vectors. The basis vectors are initialized randomly and iteratively optimized using projected gradient descent. We compute the previous losses defined in Algorithm 1 on Monte Carlo samples from the cone, as well as on the basis vectors themselves. Computing the loss on the basis vectors improves both stability and the lower bounds of the ASR. This is because the basis vectors are the boundaries of the cone and thus tend to degrade first. After each step, we project the basis back onto the cone using the Gram-Schmidt orthogonalization procedure. Because the directional ablation operation uses the normalized $\\hat{r}$ rather than $r$, sampling convex combinations of the basis vectors and normalizing them would introduce a bias towards the basis vectors themselves. Instead, we sample unit vectors in the cone uniformly to ensure better coverage of the space.\nCan we find refusal concept cones? We train cones of increasing dimensionality using the same experimental setup as described in Section 4. We measure the cone's effectiveness in mediating refusal by sampling 256 vectors from each cone and computing the ASRs of the samples for directional ablation. We show the results in Figure 3 and confirm that the directions in the cones have the desired refusal properties in Figure 14. Notably, we identify refusal-mediating cones with dimensions up to five across all tested models. This suggests that the activation space in language models exhibits a general property where refusal behavior is encoded within multi-dimensional cones rather than a single linear direction.\nDo larger models contain higher-dimensional cones? In Figure 4, we evaluate the effect of model size within the Qwen 2.5 family. We observe that across all model sizes, the lower bounds of cone performance degrade significantly as dimensionality increases. In other words, a higher number of sampled directions have low ASR. Larger models appear to support higher-dimensional refusal cones. A plausible explanation is that models with larger residual stream dimensions (e.g., 5120 for the 14B model vs. 1536 for the 1.5B model) allow for more distinct and orthogonal directions that mediate refusal. Finally, in Figure 11, we confirm that directions sampled from these cones effectively induce refusal behavior, further supporting the notion that multiple axes contribute to the model's refusal decision.\nDo different directions uniquely influence refusal? To further investigate the role of different vectors, we assess whether multiple sampled cone directions influence the model in complementary ways. Specifically, we sample varying numbers of directions from Gemma-2-2B's four-dimensional refusal cone and, for each prompt, select the most effective one under directional ablation (more details in Appendix A). To ensure a fair comparison, we use temperature sampling with the single-dimension RDO direction to generate the same number of attacks and similarly select the most effective instance. We study Gemma 2 2B and sample from its four-dimensional cone, since performance degrades significantly for larger dimensions (see Figure 10).\nFigure 5 shows that sampling multiple directions leads to higher ASR compared to sampling with various temperatures in the low-sample regime. For a higher number of samples, the randomness dominates the success of the attack. However, the higher ASR in the low-sample regime suggests that different directions capture distinct, complementary aspects of the refusal mechanism. Additionally, Figure 13 reveals that ASR increases with cone dimensionality but plateaus at four dimensions. This trend indicates that higher-dimensional cones offer an advantage over single-direction manipulation, likely by influencing complementary mechanisms. The plateau likely occurs because the model does not support higher-dimensional refusal cones.\nKey Takeaways. We show that refusal mechanisms in LLMs span high-dimensional polyhedral cones, capturing diverse aspects of refusal behavior. This highlights their geometric complexity and demonstrates the effectiveness of our gradient-based method in identifying intricate structures."}, {"title": "6. Mechanistic Understanding of Directions", "content": "Research Question: Are there genuinely independent directions that influence a model's refusal behavior? Can we access the discovered refusal directions through perturbations in the token space?\nIn the previous section, we demonstrated that refusal behavior spans a multi-dimensional cone with infinitely many directions. However, whether the orthogonal refusal-mediating basis vectors manipulate independent mechanisms remains an open question. In this section, we conduct a mechanistic analysis to investigate how these directions interact within the model's activation space and whether they can be directly influenced through input manipulation. This allows us to determine whether they are merely latent properties of the network or actively utilized by the model in response to specific prompts.\n6.1. Representational Independence\nWe defined the basis vectors of the cones to be orthogonal, which is often considered an indicator of causal independence. The intuition is that if two vectors are orthogonal, they each influence a third vector without interfering with the other. Mathematically, for the directions $r, v$ and representation $X_x^{(l)}$ we have:\nif $r^T v = 0$ then $r^T (X_x^{(l)}) - r^T(X_{x\\ ablate}^{(l)}) = r^T X_x^{(l)}$ (1)\nHowever, despite this mathematical property, recent work by Park et al. (2024) suggests that in language models, conclusions about causal independence cannot be drawn using orthogonality measured with the Euclidean scalar product. Although their assumptions differ from ours, especially since they assume a one-to-one mapping from output feature to direction in activation space, their experiments suggest that independent directions are almost orthogonal. This motivates a deeper empirical examination of how orthogonal refusal directions in language models interact in practice.\nAre orthogonal directions independent? To explore this, we first use RDO to identify a direction $r$ for Gemma 2 2B that is orthogonal to the DIM direction $v$, i.e., $\\hat{r} \\hat{v} = 0$. We then measure how much one direction is influenced when ablating the other direction by monitoring the cosine similarity $\\text{cos}(\\lambda, \\mu) = \\frac{X^{(l)} \\cdot X^{(l)}}{\\lVert X^{(l)} \\rVert \\lVert X^{(l)} \\rVert}$ between the prompt's representation in the residual stream $x$ and the directions $v$ and $r$. Specifically, we track: $\\text{cos}(r, x_{pharm}^{(l)})$ and $\\text{cos}(v, x_{pharm}^{(l)})$ at the last token position and for all layers $l \\in {0,..., L}$ on 128 harmful instructions in our validation set. Intuitively, ablating a causally independent direction in earlier layers should not intervene with the reference direction in later layers. Otherwise, there is some indirect influence through the non-linear transformations of the neural network.\nMotivated by this observation, we introduce a stricter notion of independence: Representational Independence (RepInd):\nDefinition 6.1. The directions $\\lambda, \\mu \\in \\mathbb{R}^d$ are representationally independent (under directional ablation) with respect to the activations $x$ of a model in a set of layers $l \\in \\mathcal{L}$ if:\n$\\forall l\\in \\mathcal{L}: \\text{cos}(x^{(l)}, \\lambda) = \\text{cos}(x_{abl(\\mu)}^{(l)}, \\lambda) \\text{ and } \\text{cos}(x^{(l)}, \\mu) = \\text{cos}(x_{abl(\\lambda)}^{(l)}, \\mu)$.\nThis means that, instead of relying solely on orthogonality, we define two directions as representationally independent if ablating one has no effect on how much the other is represented in the model activations. To enforce this property, we extend Algorithm 1 with an additional loss term that penalizes changes in cosine similarity at the last token position when ablating on harmful instructions:\n$\\mathcal{L}_{RepInd} = \\sum_{l \\in \\mathcal{L}}[ (cos(X_x^{(l)}, r) - cos(X_{x\\ ablate(v)}^{(l)}, r))^2 +(cos(X_x^{(l)}, v) - cos(X_{x\\ ablate(r)}^{(l)}, v))^2 ]$.\nDo independent directions exist? With this extension, we can find a direction that is RepInd from the DIM direction, yet still fulfills the refusal properties from Definition 4.1. We show the representational independence in the second row of Figure 6, where we see that the RepInd and DIM direction barely affect each other's representation under directional ablation.\nWe iteratively search for additional directions that are not only RepInd to DIM but also of all previously identified RepInd directions. Despite these strong constraints, we successfully identify at least five such directions that maintain an ASR significantly above random vector intervention (Figure 7), as well as a refusal cone with RepInd basis vectors (Figure 12). However, in Figure 7 and Figure 12 performance degrades more rapidly compared to the results in Section 4 and Section 5. This decline could be attributed to the increased difficulty of the optimization problem due to additional constraints. Alternatively, it may suggest that Gemma 2 2B possesses a limited number of directions that independently contribute to refusal. If the latter is true, this implies that the directions in the refusal cones exhibit non-linear dependencies. Nevertheless, these results show that refusal in LLMs is mediated by multiple independent mechanisms, underpinning the idea that refusal behavior is more nuanced than previously assumed.\n6.2. Manipulation from input\nCan we access these directions from the input? Having found several independent directions that are distinct from DIM, we investigate whether these directions can ever be \"used\" by the model, by checking if they are accessible from the input or if they live in regions that no combination of input tokens activates. To this end, we use GCG (Zou et al., 2023b) to train adversarial suffixes, which are extensions to the prompts that aim to circumvent the safety alignment. In addition to the standard cross-entropy loss on an affirmative target, we add a loss term that incentivizes the suffix to ablate RepInd 1."}, {"title": "7. Limitations", "content": "While our work provides new insights into the geometry of refusal in LLMs, some limitations remain. The refusal directions we compute are all optimized on the same targets, which may limit their ability to capture fully distinct mechanisms. Extending our method to incorporate diverse targets or leveraging reinforcement learning with a judge-based reward function could help identify additional independent mechanisms (Geisler et al., 2025). Furthermore, while we establish the existence of higher-dimensional refusal cones, we cannot rule out the possibility of other yet-undiscovered regions in the model that mediate refusal."}, {"title": "8. Conclusion", "content": "This work advances the understanding of refusal mechanisms in LLMs by introducing gradient-based representation engineering as a powerful tool for identifying and analyzing refusal directions. Our method yields more effective refusal directions with fewer side effects, demonstrating its viability for extracting meaningful structures while allowing for greater modeling flexibility. We establish that refusal behaviors can be better understood via high-dimensional polyhedral cones in activation space rather than a single linear direction, highlighting their complex spatial structures. Additionally, we introduce representational independence and show that within this space of independent directions multiple refusal directions exist and correspond to distinct mechanisms. Our gradient-based representation engineering approach can be extended to identify various concepts beyond refusal by simply changing the optimization targets. The generated findings provide new insights into the geometry of aligned LLMs, highlighting the importance of structured, gradient-based approaches in LLM interpretability and safety."}]}