{"title": "OSTQUANT: REFINING LARGE LANGUAGE MODEL\nQUANTIZATION WITH ORTHOGONAL AND SCALING\nTRANSFORMATIONS FOR BETTER DISTRIBUTION FIT-\nTING", "authors": ["Xing Hu", "Yuan Cheng", "Dawei Yang", "Zukang Xu", "Zhihang Yuan", "Jiangyong Yu", "Chen Xu", "Zhixuan Chen", "Zhe Jiang", "Sifan Zhou"], "abstract": "Post-training quantization (PTQ) has emerged as a widely adopted technique for\ncompressing and accelerating Large Language Models (LLMs). The major chal-\nlenge in LLM quantization is that uneven and heavy-tailed data distributions can\nexpand the quantization range, thereby reducing bit precision for most values.\nRecent methods attempt to eliminate outliers and balance inter-channel differences\nby employing linear transformations; however, they remain heuristic and are often\noverlook optimizing the data distribution across the entire quantization space. In\nthis paper, we introduce Quantization Space Utilization Rate (QSUR), a novel met-\nric that effectively assesses the quantizability of transformed data by measuring the\nspace utilization of the data in the quantization space. We complement QSUR with\nmathematical derivations that examine the effects and limitations of various transfor-\nmations, guiding our development of Orthogonal and Scaling Transformation-based\nQuantization (OSTQuant). OSTQuant employs a learnable equivalent transforma-\ntion, consisting of an orthogonal transformation and a scaling transformation, to\noptimize the distributions of weights and activations across the entire quantization\nspace. Futhermore, we propose the KL-Top loss function, designed to mitigate\nnoise during optimization while retaining richer semantic information within the\nlimited calibration data imposed by PTQ. OSTQuant outperforms existing work\non various LLMs and benchmarks. In the W4-only setting, it retains 99.5% of the\nfloating-point accuracy. In the more challenging W4A4KV4 configuration, OS-\nTQuant reduces the performance gap by 32% on the LLaMA-3-8B model compared\nto state-of-the-art methods. https://github.com/BrotherHappy/OSTQuant.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) (Dettmers et al., 2022; Touvron et al., 2023a;b) have demonstrated\nexceptional performance across a variety of tasks, increasingly integrating into daily life and playing\ncritical roles in various areas (Achiam et al., 2023; Chen et al., 2024). Nevertheless, the substantial\nmemory and computational demands pose significant deployment challenges, limiting their practical\napplicability not only on edge devices with constrained resources but also on cloud servers equipped\nwith powerful GPU devices.\nPost-training quantization (PTQ) has emerged as a widely adopted technique for compressing and\naccelerating LLMs. During quantization, uneven and heteroscedastic data, as shown in 1(a), pose\nsignificant challenges, as they expand the quantization range and reduce available bit precision for"}, {"title": "RELATED WORK", "content": "Post Training Quantization(PTQ) for LLMs. Post-training quantization (PTQ) has become a\nmainstream technique for LLMs due to its efficiency. Existing PTQ methods can be broadly divided\ninto weight-only and weight-activation quantization. To reduce memory usage, some approaches focus\non weight-only quantization. GPTQ (Frantar et al., 2022) uses Hessian-based error compensation\nto achieve high compression rates by minimizing quantization errors. AWQ (Lin et al., 2023) and\nOWQ (Lee et al., 2023) improve performance by addressing the impact of activation outliers on weight\nquantization. QuIP (Chee et al., 2023) and QuIP# (Tseng et al., 2024) use random Hadamard matrices\nfor incoherent processing and apply vector quantization to weights, achieving better performance\nwith reduced precision quantization. Unlike weight-only methods, weight-activation quantization\naims to speed up LLM inference by quantizing both weights and activations, including the key-value\n(KV) cache. The main challenge in activation quantization is that outliers dominate the range, leaving\nfew significant bits for most values, leading to substantial errors. ZeroQuant (Yao et al., 2022)\nproposes a fine-grained hardware-friendly quantization scheme for both weights and activations.\nSmoothQuant (Xiao et al., 2022) shifts quantization difficulty from activations to weights through\nmathematical transformation. OmniQuant (Shao et al., 2023) further enhances performance by\ntraining quantization parameters and transformation coefficients. Moreover, I-LLM (Hu et al., 2024)\nachieves integer-only quantization and inference through fully-smooth block reconstruction and fully\ninteger operators. Recently, QuaRot (Ashkboos et al., 2024) uses random rotation matrices to enable\n4-bit quantization of weights and activations, while SpinQuant (Liu et al., 2024) learns these matrices\nto refine 4-bit quantization.\nRiemannian Optimization. The optimization of rotation matrices necessitates adherence to or-\nthonormality constraints, which corresponds to performing Riemannian optimization on the Stiefel\nmanifold (James, 1976), encompassing all orthogonal matrices. Cayley SGD (Li et al., 2020) relies on\niterative approximations of the Cayley Transform, achieved solely by matrix multiplication, enabling\neffective optimization of rotation matrices for arbitrary loss functions. RAOM (B\u00e9cigneul & Ganea,\n2018) extends optimization methods such as ADAM (Kingma, 2014), ADAGRAD, and AMSGRAD\ninto the realm of Riemannian optimization. Meanwhile, Geoopt (Kochurov et al., 2020) supports\nfundamental Riemannian stochastic gradient descent (SGD) and adaptive optimization algorithms,\nfacilitating seamless integration into models for comprehensive optimization."}, {"title": "QUANTIZATION SPACE UTILIZATION RATE", "content": "Although significant progress has been made PTQ using linear transformations to mitigate quanti-\nzation loss (Xiao et al., 2022; Ma et al., 2024; Ashkboos et al., 2024), these methods are primarily\nheuristic and result-driven, lacking a quantitative metric to assess quantization difficulty or the\neffectiveness of different transformations. To address this gap, we introduce a novel metric, the\nQuantization Space Utilization Rate (QSUR), which quantifies how effectively weight or activa-\ntion distributions utilize the available quantization space. QSUR provides critical insights into the\nstrengths and limitations of existing methods and lays the groundwork for developing more efficient\napproaches, including our OSTQuant method described in Sec 4.1.\nQuantization Notations. In this section, we define the key notations used in quantization. Matrices\nare denoted by bold uppercase letters (e.g., X), while vectors are denoted by bold lowercase letters\n(e.g., x). The operator Q refers to the quantization function. For a comprehensive list of mathematical\nsymbols and definitions, please refer to Appendix A.1, where additional details on quantization and\ndequantization are also provided.\nLemma 1. By the central limit theorem, the distribution after Hadamard transformation follows an\napproximately ball-shaped Gaussian distribution, as demonstrated in QuIP# (Tseng et al., 2024).\nDefinition 1. Given a set of d-dimensional data $X \\in R^{n\\times d}$, let $V_X$ denote the hypervolume\noccupied by X, and $V_{Sx}$ denote the hypervolume of the quantization space S corresponding to\nX. The quantization space $S_X$ is a hypercube whose edge lengths are defined by the maximum\nquantization range across all dimensions of X. The Quantization Space Utilization Rate of X is\nthen defined as:\n$QSUR_X = \\frac{V_X}{V_{Sx}}$\nGiven \u03a7 ~ \u039d(\u03bc, \u03a3). Vx is calculated based on the ellipsoid formed by the covariance matrix\n\u2211 and mean vector \u03bc. The covariance matrix can be diagonalized via eigenvalue decomposition:\n\u2211 = QAQT, where Q is a unit orthogonal matrix of eigenvectors, and A = diag(1,2,..., \u03bb\u03b1)\ncontains the eigenvalues in descending order. The hypervolume of this ellipsoid at confidence level a\n(e.g., a = 0.99) is given by:\n$V_X = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)} (\\sqrt{\\chi^2(a)})^d \\times \\sqrt{det(\\Sigma)}$\nwhere I is the Gamma function and $\\chi^2(a)$ is the chi-squared quantile. Since Q is orthogonal, the\ndeterminant simplifies to det(\u03a3) = det(\u039b). The volume of the quantization hypercube, Vsx, is\ndetermined by the range of the distribution along each principal axis. The extremal points of the\nellipsoid are closely correspond to the maximum and minimum along these axes. We denote the\neigenvalues of the principal axes corresponding to the points with the maximum and minimum\ncoordinate values as Amax and Amin, respectively. After transforming these points back to the original\nspace, the maximum and minimum coordinate values can be represented as:\n$v_{max}^{org} = \\sqrt{\\chi^2(\\alpha)} \\cdot \\sqrt{\\lambda_{max}} \\cdot q_{max} + \\mu$\n$v_{min}^{org} = \\sqrt{\\chi^2(\\alpha)} \\cdot \\sqrt{\\lambda_{min}} \\cdot q_{min} + \\mu$\n$V_{Sx} = (max(v_{max}^{org}) - min(v_{min}^{org}))^d$\nwhere qmax and qmin denote the eigenvectors corresponding to Amax and Amin, respectively. Thus,\nthe QSUR becomes:\n$QSUR_X = \\frac{V_X}{V_{Sx}} = \\frac{\\frac{\\pi^{d/2}}{\\Gamma(d/2+1)}(\\sqrt{\\chi^2(a)})^d\\cdot\\sqrt{det(\\Lambda)}}{(max(\\sqrt{\\chi^2(a)} \\cdot \\sqrt{\\lambda_{max}} \\cdot |q_{max}| + \\mu) - min(\\sqrt{\\chi^2(a)} \\cdot \\sqrt{\\lambda_{min}} \\cdot |q_{min}| + \\mu))^d}$\nSince the magnitude of the mean vector is often smaller than the largest eigenvalue. we neglect the\nmean vector \u03bc, so Amax = Amin = 1, resulting in:\n$QSUR_X = \\frac{\\frac{\\pi^{d/2}}{\\Gamma(d/2+1)}(\\sqrt{\\chi^2(a)})^d\\cdot\\sqrt{det(\\Lambda)}}{2^d\\cdot(max(\\sqrt{\\chi^2(\\alpha)}\\cdot \\sqrt{\\lambda_1} \\cdot |q_1|))^d} = \\frac{\\frac{\\pi^{d/2}}{\\Gamma(\\frac{d}{2}+1)}\\cdot\\prod_{i=1}^d\\sqrt{\\lambda_i}}{2^d \\cdot (max(|q_1|))^d}$\nFrom Eq 7, we observe the following: 1) QSUR is proportional to the product of the ratios of each\neigenvalue \u03bb\u2081 to the largest eigenvalue \u51651; 2) The maximum component of the eigenvector q1 is\ninversely proportional to QSUR. As demonstrated in Appendix A.2.1, when the components of q1\ntake values of \u00b1d-1/2, the denominator in Eq 7 is minimized.\nInfluence of linear transformation on QSUR. Applying a linear transformation T to X\n~ \u039d(\u03bc, \u03a3) results in a transformed distribution $D \\sim N(\\mu, \\hat{\\Sigma})$, where $\\hat{\\mu} = T\\mu$ and $\\hat{\\Sigma}$ =\nTQAQ\u012aT\u012a. Smoothing-based approaches (Xiao et al., 2022; Shao et al., 2023) treat T as a\ndiagonal matrix that scales variances across different channel axes, indirectly reducing the dispar-\nities among the eigenvalues \u5165\u2081. However, these methods are particularly sensitive to outliers and\nuneven mean values, especially when the mean vector \u00b5 contains significant variations like Fig 1(b).\nMoreover, when quantizing both weights and activations simultaneously, these methods often fail\nto strike a balance. Rotation-based methods, such as those proposed in (Ashkboos et al., 2024; Liu\net al., 2024), reduce outliers in both weights and activations through rotation, thereby decreasing the\nhypercube volume to increase QSUR. As proven in Appendix A.2.2, this ability to reduce outliers\nstems from the capacity to modify the matrix Q, which improves with increasing dimensionality.\nWhen the orthogonal matrix is $T = d^{-\\frac{1}{2}} H Q^T$, where d is the dimensionality, and H is a matrix\ncomposed of \u00b11 entries, the best outlier reduction capability can be achieved.\nIn combination with Eq7, the maximum QSUR is achieved when:\n$T = c \\cdot \\Lambda^{-\\frac{1}{2}} Q^T$\nwhere c is an arbitrary scalar. At this point, the maximum utilization rate is given by $QSUR^{\"} =\\frac{\\pi^{d/2}}{\\Gamma(d/2+1)}$. Further details can be found in Appendix A.2.3."}, {"title": "METHODOLOGY", "content": "We propose Orthogonal and Scaling Transformation-based Quantization (OSTQuant), a novel frame-\nwork designed to optimize the distributions of weights and activations in LLMs through learnable\nequivalent transformation pairs, with the goal of improving quantization performance. The core\nmotivation of OSTQuant is that the combination of orthogonal and scaling transformations enhances\nthe QSUR, as illustrated in Fig 1 and explained in Sec 3.\nAs illustrated in Fig. 5, OSTQuant applies multiple transformation pairs globally within and across\nblocks of LLMs. Specifically, four equivalent transformation pairs are learned within each block,\nwith each pair consisting of a learnable diagonal scaling matrix and a learnable orthogonal matrix.\nThese transformations work together to reshape the distributions of weights and activations, making\nthem more quantization-friendly. OSTQuant preserves equivalent transformations at a global network\nlevel. As a result, the final output of the network remains unchanged when quantization is not applied,\neffectively preventing overfitting.\nEquivalent Transformation Pair We define a transformation pair as T = \u039b\u039f, where T consists\nof a diagonal scaling matrix A and a unit orthogonal matrix O. As a result, the forward inference\nprocess is reformulated as follows:\n$y = Q(xW_1O\\Lambda)Q(\\Lambda^{-1}O^T W_2)$\nwhere Q() represents the quantization operation. Since A is a diagonal matrix, its inverse is simply\nthe reciprocal of its diagonal elements. We directly optimize O because any orthogonal matrix O can\nbe decomposed into a Hadamard transform and another orthogonal matrix.\nEquivalent Transformation Pair has three advantages: 1. Earnability and Computational Efficiency:\nBoth O and A are learnable parameters. The inversion of the diagonal matrix A is computationally\nsimple, enabling efficient forward passes. The orthogonal matrix O can be optimized using gradient-\nbased optimizers, such as RiemannAdam (B\u00e9cigneul & Ganea, 2018), which supports optimization\non Stiefel Manifolds. This allows the entire process to fully leverage first-order gradient information\nfor end-to-end learning. 2. Equivalence Preservation: Ignoring the effects of quantization, the forward\nprocess remains mathematically equivalent to the original model. This ensures that activations and\nweights retain their consistency while making their distributions more quantization-friendly, thus\nreducing the risk of overfitting. 3. After optimization, O and A can be directly merged into the\nexisting weights, meaning no additional computational overhead or parameters are introduced during\ndeployment, ensuring efficient inference.\nThe optimization objective for the entire network can be formalized as:\n$arg \\min_{A_i, O_i} L(\\hat{y}, y; A_i, O_i, \\theta)$"}, {"title": "KL-TOP LOSS.", "content": "While LLMs are typically trained on vast datasets, OSTQuant optimization is often performed using\na much smaller sample set, typically around 1,000 examples. In this limited-data setting, directly\napplying original cross-entropy (CE) loss can result in accuracy drop. As shown in Tab 1, despite the\nquantized model exhibiting lower perplexity compared to its full-precision counterpart after training\nwith CE loss, its performance on zero-shot tasks declines. One likely explanation is that small and\nsimple datasets, such as WikiText-2 (Merity et al., 2016), may not fully utilize the capacity of LLMs.\nConsequently, relying solely on CE loss, which focuses on a single label, might cause the model to\noverfit to a narrow set of features, thereby compromising its emergent capabilities.\nTo address this, we propose the KL-Top loss function, which computes KL divergence over only\nthe top-k classes with the highest probabilities. By focusing optimization on the model's primary\npredictions, this approach enhances gradient quality. In the global KL loss, low-probability values\ncan introduce noise, leading to inaccurate gradient updates. By restricting the computation to the\ntop-k classes, the model receives clearer and more informative gradients. Moreover, when dealing\nwith a large number of classes (e.g., over 100,000), both computation and memory costs become\nsubstantial. Limiting the calculation to the top-k classes (e.g., k = 1000) not only reduces complexity\nbut also accelerates the training process. The KL-Top loss is calculated as follows:\n$idxs = topk(z)$\n$\\mathcal{L} = \\sum_{i \\in idxs} z[i] log(\\frac{z[i]}{\\hat{z}[i]})$\nwhere z and $\\hat{z}$ are the prediction distributions before and after quantization, respectively."}, {"title": "EXPERIMENTS", "content": "Models and Datasets. We apply our method to the entire LLaMA family, including LLaMA-1\n(7B-30B) (Touvron et al., 2023a), LLaMA-2 (7B-13B) (Touvron et al., 2023b), and LLaMA-3-8B.\nWe report perplexity (PPL) scores on the WikiText2 (Merity et al., 2016) test set. However, as\nmentioned in Tab 1, perplexity may not fully reflect the model's true performance after quantization,\nzero-Shot tasks better reflect the model's actual performance. Therefore, we also evaluate the\nmodels on up to nine zero-shot tasks using the 1m-evaluation-harness (version 0.4.4) (Gao\net al., 2024), including BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), LAMBADA\n(OpenAI) (Radford et al., 2019), OpenBookQA (OBQA) (Mihaylov et al., 2018), PIQA (Bisk\net al., 2020), SIQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-Easy, and ARC-\nChallenge (Boratko et al., 2018).\nBaselines and Implementation Details. In addition to the basic RTN approach, we benchmark\nour approach against SmoothQuant (Xiao et al., 2022), GPTQ (Frantar et al., 2022), and current\nstate-of-the-art methods such as Quarot (Ashkboos et al., 2024) and SpinQuant (Liu et al., 2024)\nfor both weight-only and weight-activation quantization. All activations are quantized using per-\ntoken asymmetric quantization without any pruning operations, while weights are quantized using\nsymmetric per-channel quantization. We use RiemannAdam (B\u00e9cigneul & Ganea, 2018) to optimize\nall unit orthogonal matrices and scaling matrices. During the distribution optimization phase, we\nuse 1,000 samples from WikiText2, each with a token length of 2,048, and iterate 150 times with a\nbatch size of 8. We apply cosine learning rate decay, setting the initial learning rate for all orthogonal\nmatrix parameters to 2 \u00d7 10-2 and for scaling parameters to 3 \u00d7 10-2."}, {"title": "OVERALL RESULTS", "content": "Table 2: Comparison of perplexity on WikiText2 and averaged accuracy on nine Zero-Shot tasks.\nResults for SmoothQuant, GPTQ, OmniQuant, AWQ, and QuaRot are based on official code and\nSpinQuant's results for LLaMA-2/3 using official weights, with LLaMA-1 from the official code.\nQuantization performance. As shown in Tab 2, our method consistently outperforms previous\nSOTA approaches across almost all configurations and models. Under the 4-16-16 setup, OSTQuant\nsurpasses all prior methods, maintaining at least 99.5% floating-point (FP) accuracy in zero-shot tasks.\nCompared to other weight-only methods like GPTQ and AWQ, OSTQuant further narrows the gap\nwith FP models. In the most challenging LLaMA-3-8B model, OSTQuant achieves only a 0.29-point\nperformance drop in zero-shot evaluations, whereas other methods incur losses exceeding 1.55 points.\nEven in the highly challenging 4-4-4 setting, our approach retains a significant performance gain, out-\nperforming the SOTA method, SpinQuant, by around 1 point across multiple models. Notably, when\nthe KV cache is not quantized (in the 4-4-16 setup), OSTQuant achieves a significant performance\nboost over SpinQuant, with gains up to 6.53 points (LLaMA-2 7B). These substantial performance\nimprovements demonstrate the effectiveness of our approach. More detailed results can be seen\nin Appendix A.6. Once activation is quantized, rotation-based methods significantly outperform\nsmooth-based methods, confirming that latter struggle with outliers and uneven distributions. In Fig\n3, the QSUR across different methods show a clear positive correlation with model performance. Our\napproach achieves the highest QSUR, effectively mitigating the challenges of outliers and uneven\ndistributions that hinder prior methods, leading to improved model accuracy."}, {"title": "SPEEDUP AND MEMORY SAVINGS.", "content": "OSTQuant incurs only negligible loss in 4-bit quantization, making\n4-bit inference feasible. As shown in Tab 3, OSTQuant delivers an average inference speedup of over"}, {"title": "ABLATION STUDY", "content": "Table 4: Ablation study on the impact of different transformation matrices on Wiki PPL and zero-\nshot score for LLaMA-2 7B under W4A4KV4 quantization.\nEffect of different transformation. We ablate the effects of various transformation matrices on\nLLaMA-2 7B, identifying four groups where orthogonal and scaling equivalent transformations can\nbe applied. Tab 4 presents the contribution of each parameter group under W4A4KV4 setup. Our\nresults show that the global orthogonal transformation Rres brings the largest improvement, followed\nclosely by Rdown. Notably, scaling transformations S further build on the orthogonal transformations\nR by effectively balancing variance across channels, thereby minimizing quantization losses and\nenhancing model performance.\nTable 5: Effect of different optimizers on zero-shot performance of LLaMA models under the\nW4A4KV4 configuration. LR1 and LR2 represent the learning rates for the scaling matrices and\nunitary orthogonal matrices.\nDifferent Manifold Optimizers. Since the unit orthogonal matrix resides on a Stiefel manifold,\nwe explore various manifold optimizers to optimize it, including CayleySGD (Li et al., 2020),\nRiemannSGD, and RiemannAdam (B\u00e9cigneul & Ganea, 2018). Tab 5 compares these methods\nand shows that CayleySGD typically requires a higher learning rate to perform well, RiemannSGD\nneeds more iterations, while RiemannAdam delivers the best results with the fewest iterations. We\nalso discover that using a learning rate for the Stiefel manifold 10 times larger than that for scaling\ntransformation parameters leads to better results.\nTable 6: Ablation study of k values on Zero-Shot score and Wiki PPL for W3-only and W4A4KV4\nconfigurations of LLaMA-2 7B."}, {"title": "INFLUENCE OF K IN KL-TOP LOSS.", "content": "The parameter k in Eq. 12 defines the number of classes considered\nwhen calculating the KL-Top loss, balancing optimization difficulty with semantic richness. Both\nexcessively large or small k values negatively impact optimization. Tab 6 shows a comparison of\ndifferent k values. Furthermore, we analyze whether to apply softmax before or after the top-k\nselection. Our experiments indicate that setting k to 1,000 processing produces the best outcomes."}, {"title": "CONCLUSION", "content": "In this paper, we introduce OSTQuant, a novel post-training quantization method designed to en-\nhance the efficiency of large language models (LLMs). Central to OSTQuant is the Quantization\nSpace Utilization Rate (QSUR), a new metric we proposed to effectively assess the quantizability of\ntransformed data by measuring its space utilization within the quantization space. Complemented by\nmathematical derivations, QSUR provides theoretical guidance for optimizing single data distribu-\ntions across the entire quantization space. Leveraging this insight, OSTQuant employs a learnable\nequivalent transformation pair composed of orthogonal and scaling transformations to optimize the\ndistributions of weights and activations. Additionally, we introduce the KL-Top loss function to\nmitigate noise during optimization while retaining richer semantic information, even with the limited\ncalibration data typically available in PTQ. Extensive experiments on various LLMs and benchmarks\ndemonstrate that OSTQuant outperforms existing quantization methods. These results highlight\nthe effectiveness of optimizing data distributions across the quantization space and underscore OS-\nTQuant's potential to advance LLM quantization, making these models more efficient and practical\nfor deployment in resource-constrained environments."}, {"title": "ADDITIONAL ABLATION EXPERIMENTS", "content": "A.3.1 THE EFFECT OF WEIGHT OUTLIER MINIMIZATION INITIALIZATION\nWeight Outlier Minimization Initialization(WOMI) is used to initialize trainable orthogonal matrices.\nThis approach not only reduces outliers in the weights but also leverages the properties of Hadamard\nmatrices to mitigate inter-channel disparities in activations, thereby improving the initial QSUR for\nboth weights and activations.\nWe visualized the impact of WOMI on the weights. As shown in Fig 7, the original weight distribution\nexhibits significant variations across input and output channels. While QuaRot reduces inter-channel\ndifferences, noticeable spikes remain. WOMI, by leveraging the Hadamard matrix and the covariance\nmatrix of the weight distribution, further smooths these inter-channel differences, effectively reducing\nthe quantization space and relative quantization error. Additional visual results for other layers are\npresented in Fig 13.\nWe conducted additional experiments to investigate the impact of WOMI on the performance of\nquantized models. Tab 7 presents the performance of LLaMA-2-7B and LLaMA-3-8B models\ninitialized with WOMI and random Hadamard matrices. WOMI achieves lower perplexity and\nhigher few-shot accuracy under both W4A4KV4 and W4A16KV16 configurations, showcasing its\neffectiveness. Interestingly, WOMI demonstrates greater performance improvements in W4-only\nquantization settings compared to W4A4KV4. This is likely due to WOMI's superior capability in\nminimizing weight quantization errors, which is especially critical in W4-only configurations."}, {"title": "THE EFFECT OF KL-TOP LOSS", "content": "As shown in Tab 8, the results indicate that using SpinQuant(Liu et al., 2024) alone, even with the\nintroduction of the KL-Top loss function, does not lead to significant performance improvements\nand may even cause some degradation. However, when combined with orthogonal and scaling\ntransformation pairs, the quantization performance improves significantly. For OSTQuant, using CE\nloss results in overfitting on the calibration set, and this issue is alleviated by the introduction of the\nKL-Top loss function."}, {"title": "INFERENCE EFFICIENCY AND QUANTIZATION OVERHEAD", "content": "Tab 9 shows the prefill time and memory usage of LLaMA models with different parameter sizes\nand sequence lengths, compared between our 4-bit implementation and FP16. The inference envi-\nronment features an Intel(R) Xeon(R) Gold 5317 CPU and an Nvidia 3090 GPU. The 4-bit matrix\nmultiplication kernel was implemented using cutlass of nvidia, while the self-attention mechanism\nwas realized with PyTorch's native SDPA (scaled dot product attention) function. All tests were\nconducted 500 times, with the median value taken as the final result. Benefiting from efficient low-\nprecision computation units within CUDA cores and reduced access overhead, OSTQuant achieves\nover 2x speedup across various model sizes, and approximately 3\u00d7 acceleration on the challenging\nLLAMA-30B model."}, {"title": "FUTURE WORK", "content": "A.5.1 OSTQUANT FOR FULLY-QUANT LARGE LANGUAGE MODELS\nFig 9 introduces OSTQuant's novel strategy designed for full quantization. Full quantization involves\nquantizing all activations within each Transformer Block to low bits (as shown by the quantization\nnodes inserted for all node inputs and outputs in the figure). This reduces memory transfer overhead\nfor activations and fully utilizes efficient low-precision computational units.\nAs shown in Fig 9, OSTQuant introduces numerous equivalence transformations to alter the distribu-\ntions of all node input and output activations. Unlike the methods in Fig 5 designed for traditional\nquantization, the design for fully quantization adds more equivalence transformations, particularly\naround ROPE and SiLU. Specifically:\nROPE Handling: We treat ROPE as a lightweight GEMM layer and construct a weight\nmatrix of shape (token, head_dim, head_dim) based on its principle. We then introduce\npre-ROPE and post-ROPE transformation pairs.\nPre-ROPE transformations are based on the fact that ROPE and the preceding linear\nlayer can be viewed as consecutive matrix multiplications along the head_dim. The\ncorresponding transformation pairs are represented in figure by $S_i^q R_{pre}$ and $S_i^k R_{pre}$.\nPost-ROPE transformations rely on the attention computation formula Attn =\nQ@KT, where Q means the query matrix and K means the key matrix in Self-\nAttention module. The corresponding transformation pairs are represented in figure by\n$S_i^{post}R_{post}$.\nSmoothing Activation Discrepancies of SiLU: Inspired by smoothing methods for SwiGLU\nin I-LLM (Hu et al., 2024), we decompose SiLU as SiLU(X) = X \u00b7 \u03c3(X) and use\nequivalences such as\nX1.5(X1) 1. (S\u2081S2). [o(x)  .5(x, 3}\nto alleviate inter-channel discrepancies of activations before and after SiLU. The correspond-\ning transformation is represented in figure by $S_{Silu}$.\nWe will conduct experiments in full-quantization domain in the future to fully explore the potential\nof OSTQuant."}]}