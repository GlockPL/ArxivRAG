{"title": "TIMERL: Efficient Deep Reinforcement Learning with Polyhedral Dependence Graphs", "authors": ["Pedro F. Silvestre", "Peter Pietzuch"], "abstract": "Modern deep learning (DL) workloads increasingly use complex deep reinforcement learning (DRL) algorithms that generate training data within the learning loop. This results in programs with several nested loops and dynamic data dependencies between tensors. While DL systems with eager execution support such dynamism, they lack the optimizations and smart scheduling of graph-based execution. Graph-based execution, however, cannot express dynamic tensor shapes, instead requiring the use of multiple static subgraphs. Either execution model for DRL thus leads to redundant computation, reduced parallelism, and less efficient memory management.\nWe describe TIMERL, a system for executing dynamic DRL programs that combines the dynamism of eager execution with the whole-program optimizations and scheduling of graph-based execution. TIMERL achieves this by introducing the declarative programming model of recurrent tensors, which allows users to define dynamic dependencies as intuitive recurrence equations. TIMERL translates recurrent tensors into a polyhedral dependence graph (PDG) with dynamic dependencies as symbolic expressions. Through simple PDG transformations, TIMERL applies whole-program optimizations, such as automatic vectorization, incrementalization, and operator fusion. The PDG also allows for the computation of an efficient program-wide execution schedule, which decides on buffer deallocations, buffer donations, and GPU/CPU memory swapping. We show that TIMERL executes current DRL algorithms up to 47\u00d7 faster than existing DRL systems, while using 16\u00d7 less GPU peak memory.", "sections": [{"title": "1 Introduction", "content": "Deep reinforcement learning (DRL) extends the success of deep learning (DL) to more general settings with sequential decision-making problems [1], e.g., complex game play [2\u20134], robotics [5, 6], mathematics [7], algorithm design [8], and chip placement and routing [9]. DRL algorithms are also increasingly integrated in other DL workloads, including fine-tuning large language models (LLMs) using RLHF [10, 11], optimizing neural architecture search [12] or hyper-parameter search [13], and automated data augmentation [14].\nDRL algorithms have complex computational patterns: a learner computes an update to a neural network from tensor data generated timestep-by-timestep by an actor as part of its interaction with a simulated environment [15] (see Fig. 1). This results in computations with nested (possibly data-dependent) control-flow, i.e., an inner loop for simulation timesteps and an outer loop for training iterations. The learner's update computation is defined by the specific DRL algorithm, such as REINFORCE [16] or PPO [17], and it can be typically decomposed into a sum of per-timestep updates, each of which has unique dynamic dependencies on both past and future timesteps of the actor tensors.\nConsequently, different DRL algorithms have different optimal execution strategies. Depending on how learners access the generated tensors by the actors, some algorithms may allow for parallelism between actors and learners [18], or early deallocation of tensors [19]; others may require incremental execution [20] and the offloading of large tensors to CPU memory due to limited GPU memory [21]. Conversely, when memory is not an issue, vectorization [22] can speed-up execution; and avoiding recomputation of forward activations during learning is possible for some algorithms [16, 17, 23].\nExisting DL frameworks, such as PyTorch [24], TensorFlow [25], and JAX [26], struggle to support the above computational patterns and optimizations efficiently. Eager execution engines, such as PyTorch and TensorFlow Eager [27], adequately support dynamic control-flow and data dependen-"}, {"title": "2 Supporting Deep Reinforcement Learning", "content": "We first introduce DRL workloads (\u00a72.1), so that we may discuss the shortcomings of DL systems at handling their dynamic dependencies (\u00a72.2) and how this affects downstream DRL framework design (\u00a72.3)."}, {"title": "2.1 Deep reinforcement learning", "content": "The goal in deep reinforcement learning (DRL) [1] is to learn an acting policy $\\pi_w$, represented by a deep neural network (DNN) with parameters $w$, from experience data collected by sequential interactions with a simulated environment, as shown in Fig. 1.\nAt each discrete timestep $t$ of the simulation, starting from the initial observation $o_0$, the acting policy produces an action $a_t$, which is executed in the environment and, in response, receives the next observation $o_{t+1}$, a done signal $d_t$ and a reward $r_t$. This continues until the episode terminates at timestep $T$, which is either fixed or chosen dynamically. Every iteration, a learner computes a loss $L$ from the generated experience data, backpropagates it into a parameter update $\\nabla w$ and uses it to update the DNN parameters $w$.\nSimulators. Large-batch training helps algorithms reduce training time [38,39], de-correlate experience [23] and explore the environment [40]. Early DRL systems parallelized CPU simulators through asynchronous distribution [30, 41\u201343] and double-buffering [32]. However, large clusters are expensive, while asynchrony hurts data freshness and convergence guarantees [1]. CPU-based simulations underutilize GPUs [33, 34], leading to the emergence of GPU-based simulators [40, 44\u201347]. Thus, novel designs for synchronous and end-to-end GPU-accelerated DRL systems are needed.\nDynamic data access. The loss may be decomposed into a sum of per-timestep losses, $L = \\sum_{t=0}^T l_t$, in which each $l_t$ accesses only a portion of the experience data dynamically, depending on the timestep $t$. We illustrate this in Fig. 3: Monte-Carlo algorithms such as REINFORCE [16] access all future timesteps (Fig. 3a); temporal difference methods [19] such as A2C [23] access the experience in a sliding window (Fig. 3b); PPO [17] accesses the experience in blocks (Fig. 3c)."}, {"title": "2.2 Dynamic computation in DL systems", "content": "Iterative dynamic computation, such as the above patterns in DRL algorithms, are difficult to represent and optimize in today's DL systems, preventing whole-program optimizations and efficient execution scheduling. Fig. 2 shows how a dynamic data dependency $ (e.g., $ = [i : I]), in a loop over iterations $i$, can be expressed using control-flow in DL systems that follow the two dominant execution models: imperative and declarative (graph-based).\nImperative systems, such as PyTorch [24] and TensorFlow Eager [27], support dynamism well: users write imperative code that directly dispatches execution kernels to the GPU and use the control-flow statement in the host language (e.g., Python) to evaluate conditionals (Fig. 2a). These systems, however, lack a graph representation that can be optimized and scheduled automatically, thus burdening the user with writing efficient code. Lazy eager approaches [52] collect small subgraphs before dispatching and optimizing them, but this is still insufficient for whole-program optimization. Both eager and lazy eager approaches must perform costly movements of tensors to the CPU when evaluating conditions as part of control-flow operations.\nDeclarative (graph-based) engines, such as TensorFlow [25] and JAX [26], first build a dataflow graph of the program before performing optimizations and execution scheduling. TensorFlow uses a flat dataflow graph with several control-flow operators [28] that encode loops and conditionals (Fig. 2b); JAX uses a single while operator [53], which nests subgraphs for loop body and condition (Fig. 2c). Both approaches, however, require static tensor shapes in the graph, and thus cannot directly express dynamic data dependencies."}, {"title": "2.3 Existing DRL frameworks", "content": "DRL frameworks [30, 30, 32, 55-59] work around the above limitations by adopting the actor-learner execution model shown in Fig. 1. The actor-learner model splits the computation into two static graphs: (1) an actor graph that is executed repeatedly until termination, producing the full simulation data and storing the results in pre-allocated buffers; and, when the simulation terminates, (2) a learner is invoked that computes a DNN parameter update from the batch of collected experience data.\nWhile the actor-learner model can be applied to most DRL algorithms, it has several drawbacks (indicated by the numbers in Fig. 1) due to missing whole-program optimizations:\n(1) Redundant computation. Intermediate DNN activations produced by the actor are discarded and recomputed in the learner for use in back-propagation. This adds a substantial overhead, because it doubles the amount of DNN inference work for typical algorithms; for RNNs, the redundant work grows linearly with the sequence length.\n(2) Missed parallelism. The execution of actors and learners must be serialized due to the coarse-grained data dependency on all data generated by the actor. This prevents parallel learning on past timesteps while acting out future timesteps, which would allow for incremental computation strategies. Furthermore, online learning algorithms are not well supported.\n(3) High peak memory usage. The \"all-at-once\" learning results in high peak usage of GPU memory, potentially causing"}, {"title": "3 TIMERL Overview", "content": "To solve the challenges presented by dynamic data dependencies, TIMERL exploits the following key idea: dynamic dependencies can be transformed, optimized and scheduled effectively by representing them symbolically. Based on this idea, Fig. 5 shows TIMERL's overall approach, which consists of four main parts:\n(1) Recurrent tensors (RTs) allow users to express dynamic dependencies and control-flow easily as systems of recurrence equations. To achieve this, each RT has a domain, i.e., a set of symbolic dimensions (e.g., time, sample, sequence element), which can be indexed directly using symbolic expressions. This eschews the need for graph-splitting, dynamic slicing or complex control-flow operations (see \u00a72.2). TIMERL's implementation of automatic differentiation [61] propagates and accumulates gradients through symbolic dimensions as needed, producing more RTs that can be further optimized.\n(2) Polyhedral Dependence Graph (PDG). Based on the RTs, TIMERL builds a PDG that represents the dynamic dependencies as symbolic expressions that label the edges of the graph. TIMERL optimizes the whole PDG using classical techniques, including dead and duplicate code elimination, algebraic equivalences and broadcasting removal.\nTo speed up execution, TIMERL employs a vectorization pass which finds symbolic dimensions in the domain of operations that can be executed in parallel and makes them con-"}, {"title": "4 Recurrent Tensors", "content": "Expressing the dynamic dependencies and control-flow of DRL algorithms is challenging in the programming models supported by current DL systems (see \u00a72.2). Instead, they can be expressed naturally as recurrence equations [35], such as $y[t] = f(x[t : T])$. This is why recurrence equations are ubiquitous in algorithmic pseudocode, e.g., when defining optimizers [64], learning rate schedules [65], models [51, 66, 67], and loss functions [17, 68].\nTo exploit this representation, we introduce recurrent tensors (RTs), a declarative programming model inspired by recurrence equations. In addition to a concrete shape and datatype, an RT can be viewed as evolving over a set of symbolic dimensions, such as iterations, $0 \\le i < I$, or timesteps, $0 < t < T$, which we refer to as its domain.\nRTs can be indexed over these symbolic dimensions using symbolic expressions to either express dynamic dependencies or control-flow. Symbolic expressions are a simple language, with symbols (e.g., $t, T, i$), constants (e.g., 4, False), arithmetic operations (e.g., +, /, %), comparison operations (e.g., <, =), boolean logic operations (e.g., &, |), and aggregations (e.g., min). To support range accesses, there is also a slice operator :, and to support RTs with multiple symbolic dimensions, there are sequence expressions (\u00b7,\u00b7).\nREINFORCE with RTs. Alg. 1 shows an example of the REINFORCE [16] algorithm using RTs. Users begin by setting up a context (line 2) with a chosen number of symbolic dimensions, associating each dimension with a semantic meaning (line 3). Similar to PyTorch [24], DNNs are defined by composing modules, but the domain over which the network parameters vary must be stated explicitly (line 4).\nThe environment is then created, and the observations tensor is defined as a branching definition RT: the first timestep is initialized by the environment reset (line 9) and subsequent timesteps are computed by stepping the environment with the"}, {"title": "Automatic domain inference.", "content": "Annotating the domain of all RTs is unnecessary, because TIMERL infers the domain of most RTs from the domain of sources. For example, symbolically indexing an RT with a constant (e.g., 5 or T) on a symbolic dimension $d$ removes $d$ from its domain. When two RTs interact, their domains are automatically unioned (similar to shape broadcasting or datatype promotion), as shown in Fig. 6. In this example, the left RT varies with timesteps $t$, while the right RT varies with iterations $i$. Their interaction produces an RT which varies with both symbolic dimensions (i.e., has a domain of $(i,t)$).\nIf an RT is not explicitly symbolically indexed (g in line 16), it is treated as if indexed linearly (i.e., g[b,i,t]), making it behave like a standard tensor in the common case. In TIMERL, every aspect of computation is defined using RTs, including the optimizer (e.g., Adam [64]), simulation environments [15], and replay buffers [69]. Automatic domain inference therefore allows, e.g., for learning rate schedules, to be easily expressed as RTs too, e.g., lr[i] = lr[i-1] \u00d7 0.98, and be integrated with the optimizer without code changes due to domain inference."}, {"title": "5 Polyhedral Dependence Graphs", "content": "TIMERL uses RTs to construct a whole-program polyhedral dependence graph (PDG), which encodes dynamic dependencies. Below, we describe the PDG abstraction in detail and explain how it supports symbolic backpropagation (\u00a75.1). After PDG construction, TIMERL applies typical compiler optimizations to the PDG, such as dead and duplicate code elimination, algebraic equivalences, and broadcasting removal. We focus on three key transformations: vectorization (\u00a75.2), incrementalization (\u00a75.3), and dataflow fusion (\u00a75.4)."}, {"title": "5.1 PDG representation", "content": "A PDG is a directed multigraph of operations, in which each operation can have multiple input and output tensors. Cycles (even self-loops) are allowed.\nEach operation $o$ is tagged with a domain $\\Omega(o)$, which is the set of integer points in $\\mathbb{Z}^n$ at which the operation must be evaluated; each edge $o_1 \\overset{iid,\\phi,\\psi,oid}{\\dashrightarrow} o_2$ is annotated with the sink input identifier iid; a symbolic dependence expression $\\phi$; an optional condition $\\psi$ and the source output identifier oid. It indicates that $o_1$ (the sink) at point $p \\in \\Omega(o_1)$ depends on output oid of $o_2$ (the source) at points $\\phi(p)$ when $\\psi(p)$ is True. The domain $\\Omega$, dependence expressions $\\phi$, and conditions $\\psi$ come directly from the domains and symbolic expressions of the RTs that defined them (see \u00a74).\nTensor operations. The operations within the PDG come from a minimal set of 40 stateless operators, most of which are typical elementwise maps, reductions, scans, layout or indexing operations. Layout operators (e.g., Reshape0p, Slice0p) can hold symbolic shapes and indexes in their parameters."}, {"title": "Symbolic backpropagation.", "content": "When backward() is called on an RT, typical automatic differentiation [61] is used to compute the input gradients given the output gradients. A challenge, however, is that TIMERL must also propagate or accumulate gradients through symbolic dimensions. Thus, for a given RT $y[p] = f(x[\\phi(p)])$, the gradient of $x$ is an RT with the same shape, datatype, and domain,\n$\\nabla x[p'] = \\sum_{\\rho \\in \\phi^{-1}(p')} f'(\\nabla y[p]),$\nwhere $f'$ is the derivative of the operation $f$ and $\\phi^{-1}$ is the inverse of the dependence relation. Intuitively, inverting a dependence expression converts \u201cwhat source points does the sink point $p$ depend on?\u201d to \u201cwhat sink points depend on source point $p$?\u201d. For example, [t + 3] inverts into [t - 3], [t: T] into [0: t + 1], and [0 : T] into [0: T]. Since a single point of $x$ may contribute to several points of $y$, the gradient of $x[p']$ must sum the gradient contributions from all $y$ points it contributed to. When backpropagating through a conditional branch (Merge0p), the gradient must only flow to the input from which the output was computed. Thus, the chain-rule for a MergeOp produces other MergeOps with the same conditions, which conditionally copy from the output gradient or zeros."}, {"title": "5.2 Vectorization", "content": "The vectorization transformation reduces the execution time of PDGs by moving dimensions from symbolic to concrete. This creates fewer and larger parallel executions, at the cost of increased memory usage. The first step of vectorization is to find incremental scans, reductions and stencils implemented using MergeOps and dynamic indexing, and to lift them through simple pattern-matching into batch operations such as Sumop, CumSumop and Conv0p.\nThen, Alg. 2 is used to vectorize all other operations by applying it to each symbolic dimension $d$. The algorithm first"}, {"title": "5.3 Incrementalization", "content": "Some operations may demand too much memory, especially after vectorization. To address this, TIMERL then incrementalizes the PDG using Alg. 3: it breaks-up large operations into smaller blocks of size bs by adding a new symbolic dimension di to the domain of operators. The block size is chosen so the memory requirements of operations remain below a threshold. This transformation allows TIMERL to implement gradient accumulation [20] transparently.\nSince reductions (e.g., Sumop, Max0p) reduce a specific concrete dimension (i = o.dim), they are a natural starting point for incrementalization. Given a large reduction operation o, Alg. 3 incrementalizes o and each of its dependencies od recursively (line 2). It then updates the affected PDG edges (line 3), and reduces the results over all blocks 0: DI (line 4) before moving all dependents of o to this new reduction (line 5). Each recursive step (line 6) also tracks the index i of the concrete dimension being incrementalized, as it changes due to layout operations (lines 8\u201313). It stops the recursion when it finds either an ExpandOp that creates the dimension (line 14), or a dependency that is not vectorized or has a memory requirement below the threshold (lines 16\u201318)."}, {"title": "5.4 Fusion", "content": "A PDG often contains regions with only linear dependencies and the same domain for all operations. Inside these regions, the PDG behaves as a dataflow graph, directly passing the outputs of one operation to the inputs of another. The goal of the fusion transformation is to find and fuse these regions into a single DataflowOp operation. This not only enables efficient code-generation by DL compilers [26, 54, 71], but also lowers the runtime dispatching overhead and simplifies the scheduling problem."}, {"title": "6 Polyhedral Scheduling of PDGs", "content": "RTs are declarative and thus do not prescribe an execution order. We show how a polyhedral model [36,37] can address this challenge while enabling algorithm-specific scheduling. Scheduling occurs in two rounds: (1) TIMERL first finds an execution schedule (\u00a76.1), freezes it, and then (2) uses it to schedule memory management operations (\u00a76.2)."}, {"title": "6.1 Polyhedral execution scheduling", "content": "Scheduling DRL programs is challenging due to their dynamic, cyclic dependencies. The polyhedral model [36, 37], originally developed for scheduling uniform recurrence equations [35], has since evolved to support much more complex computations [72]. However, today it is primarily used to optimize small loop nests in imperative programs [73, 74], as modeling arbitrary programs in it remains difficult [75].\nIn a polyhedral model, each tensor x has its domain represented by a polyhedron, $\\Omega_x = \\{p \\in \\mathbb{Z}^n | \\text{constraints}\\}$, which is the set of integer points in an n-dimensional space that satisfy a set of inequality constraints. A dependence of tensor $x$ on $y$ is also represented by a polyhedron in the product of the two domains, $\\notin_{x\\rightarrow y} = \\{(p,p') \\in \\Omega_x \\times \\Omega_y | \\text{constraints}\\}$. For simplicity, we treat $\\notin_{x\\rightarrow y}$ as a function $\\notin_{x\\rightarrow y}(p) = \\{p' \\in \\Omega_y | (p, p') \\in \\notin_{x\\rightarrow y}\\}$ that, given a point in $\\Omega_x$, yields all points in $\\Omega_y$ that it depends on.\nScheduling in a polyhedral model finds a schedule function $\\theta$ that maps each point of the domain of tensors to an execution time in a larger space $\\mathbb{Z}^s, s > n$, and where the lexicographic order determines what operation executes first. A valid schedule must respect all dependencies, i.e., $\\forall x, y, p_x \\in \\Omega_x. \\theta(\\notin_{x\\rightarrow y}(p_x)) < \\theta(p_x)$, where \u227a is the lexicographic smaller-than comparison. Finding schedules thus involves solving a complex integer linear program (ILP) [36], which is constrained by the dependencies in the computation.\nPDGs are not arbitrary unconstrained programs. Extracted from RTs, PDGs already represent tensor domains, dependencies and conditions explicitly using symbolic expressions. Thus, TIMERL can directly create domain and dependence polyhedra from the PDG, allowing us to leverage the polyhedral model for whole-program scheduling. Furthermore, because we have fused large dataflow regions, the number of operations is reduced, making the ILP more tractable even for large programs."}, {"title": "6.2 Memory management", "content": "The current execution schedule would allocate memory but never release it, leading to out-of-memory errors. To address this, TIMERL also schedules memory management operations: (i) donations, (ii) deallocations and (iii) swapping (i.e., offloading to CPU memory and fetching from GPU memory).\nFinding buffer donations. Given the PDG's stateless nature, it is important to detect when a tensor will be deallocated, and thus its backing memory buffer can be donated to be reused by another operation. For example, DNN parameters, which are represented as a cycle in the PDG between a MergeOp and the optimizer should reuse the same buffers. Given a schedule $\\theta$, the last user $o^u$ of tensor $o^d$ is the tensor that, for every point $p$ in the domain of $o^d, o^u$ accesses $p$ after every other competing dependent $o^l$. Formally,\n$\\exists o^l \\in \\text{Dependents}(o^d). \\forall o^l \\in \\text{Dependents}(o^d). \\forall p \\in \\Omega(o^d). \\theta(o^l(\\text{d})) < \\theta(o^u(\\text{p}))$\nScheduling deallocations and swap. Scheduling deallocation and swapping operations further requires careful ordering of operations in relation to each other and executions. Given"}, {"title": "6.3 Abstract syntax tree generation", "content": "The schedule function $\\theta$ output by the polyhedral scheduler [37] is not directly executable. It must be transformed first into an abstract syntax tree (AST). Using the polyhedral library [36], TIMERL thus traverses this representation [77] and emits an equivalent AST in our own format:\n\u2022 Loop: a generic loop with a counter symbol, start, end, step, condition and body sub-AST;\n\u2022 If: a conditional with then and else sub-ASTs;\n\u2022 Sequence: a block of AST nodes executed in order;\n\u2022 Execute: executes operation o for point p given by a mapping from AST counters to domain symbols; and\n\u2022 Deallocate/Fetch/Offload: execute the instruction for points P given by a mapping from AST counters to domain symbols of the producer."}, {"title": "7 Execution Runtime Implementation", "content": "The prototype implementation of TIMERL is written in ~25,000 lines of Python code. Our runtime (rightmost block of Fig. 5) is responsible for taking the PDG, AST, and the user-provided DL backend, and executing the program.\nTensor storage. Tensors in the PDG have varied access patterns, requiring tailored storage strategies at runtime. TIMERL currently has two tensor storage implementations: (1) a point store is a simple map from a tuple of integer indices (runtime symbol values) to runtime tensors, which makes point insertion, access and removal fast at the cost of requiring concatenation for slice accesses; and (2) a block store is a map from a tuple of indices to large pre-allocated runtime tensors, which are aligned when possible to the access expressions in the PDG. This allows for fast slice accesses, but requires fast in-place mutation of tensors and can delay memory management requests (deallocation and offloading).\nTIMERL determines the type of storage used by examining accesses in the PDG before scheduling. Tensors that are only point-accessed are stored in a point store, while tensors that are slice-accessed are stored in a block store. The block size is determined by the largest slice access to the tensor.\nBackends must implement a thin backend interface, which has methods for (i) allocating and deallocating tensors, (ii) moving tensors between devices, and (iii) executing specific tensor operations (dynamic update and stack) required to implement the tensor stores. Each backend must also implement DLPack [62] zero-copy conversion to connect with other DL frameworks and GPU-based simulators with minimal data copying. The tracing-based compilation implementation of a backend is used to generate executable thunks for DataflowOp operations.\nThunk launchers evaluate a thunk's input dependence expressions at the current execution point p using the runtime AST loop counters, in order to index tensor stores. They then execute the thunk with the retrieved tensors and store the outputs back in tensor stores at point p.\nBuffer management. JAX [26], as a TIMERL backend, lacks page-locked memory and thus cannot perform asynchronous host-to-device transfers. Asynchronous fetch/offload is essential for high GPU utilization [21], but, without page-locked buffers, the CPU has the overhead of first copying tensors syn-"}, {"title": "8 Evaluation", "content": "We evaluate TIMERL to answer the following questions: Do TIMERL's abstractions and optimizations lead to more efficient computation compared to existing approaches? (\u00a78.2) Does TIMERL's memory management enable larger scale DRL training? (\u00a78.3) Can TIMERL effectively support algorithm-specific execution schedules? (\u00a78.4)"}, {"title": "8.1 Methodology", "content": "We evaluate TIMERL on (a) PPO [17] (one epoch per iteration), (b) REINFORCE [16] and (c) REINFORCE with n-step returns [19]. We run TIMERL with its JAX and Torch backends, and compare against three representative DRL implementation types: (i) synchronous performance-focused frameworks (SampleFactory v2.1.1 [32], RLGames v1.6.1 [58]), (ii) single-file algorithm implementations (CleanRL commit e648ee2 [31]), and (iii) scalable distributed DRL frameworks (Ray RLlib v2.5.0 [30]).\nTo focus on execution overheads, we employ a single GPU-accelerated test environment [40, 45], except for RLlib which lacks GPU environment support. Each experiment runs twice, once with default allocators for end-to-end performance, and once without caching allocators to measure true memory usage, discarding the first iteration to ensure warm-up.\nAll experiments use the same hardware and software configuration. We use a server with an AMD EPYC 7402P 24-core CPU, 384 GB of DDR4 RAM (3200 MT/s) with an NVIDIA RTX A6000 GPU (48 GB of GDDR6 RAM, PCIe Gen4 \u00d716 at 32 GB/s). We run Ubuntu v22.04 with Linux kernel v5.15, CUDA v12.1, PyTorch v2.5.1, and JAX v0.4.35."}, {"title": "8.2 Training performance for PPO", "content": "We evaluate the end-to-end training performance of TIMERL with PPO, a popular DRL algorithm with an anti-causal dependency (r[t:T]) that prevents parallel acting and learning. We start with a default configuration with an episode length of 250, 512 environments, 64 parameters per layer, 2 hidden layers, and a 3\u00d74\u00d74 observation shape, and we then vary each parameter individually.\nFig. 9 shows the result by reporting iteration time and GPU utilization. TIMERL (TRL), using both its JAX and Torch backends, has consistently lower iteration times than all other baselines. TIMERL is up to 47\u00d7 faster than RLlib, and on average 2.18\u00d7 faster than the next fastest system, CleanRL, which is a hand-coded PPO implementation. There are several reasons for this: TIMERL's PDG representation supports holistic optimizations, such as deduplicating the forward passes and caching activations. In addition, TIMERL's automatic vectorization, efficient scheduling and code-generation of fused dataflow regions further improve performance."}, {"title": "8.3 Memory management", "content": "Training DRL models from image observations is a common use-case in robotics [79], autonomous driving [80] and healthcare [81], that actor-learner DRL frameworks struggle with. Image resolutions impact both model task performance and system memory usage, as larger images require more memory to store and process. In this experiment, we halve the number of environments to 256, but quadruple the episode length to 1,000 and show iteration times, GPU utilization and memory, and CPU memory, as we scale observations up to 3\u00d7256x256.\nAs Fig. 10 shows, independent of the algorithm, the actor-learner approach stores and learns from all observations at once, resulting in a GPU peak memory usage that exceeds capacity. RLlib enters a fail-retry loop before 3\u00d764\u00d764, while the other baselines fail to scale past it, due to out-of-memory (OOM) errors. In contrast, TIMERL scales to 3\u00d7256\u00d7256 (16x larger) while balancing GPU memory with CPU memory usage, through automatic incrementalization and swapping (see \u00a75.3 and \u00a76.2). TIMERL uses incrementalization after 3\u00d732\u00d732, leading to a slight slow-down, and swapping is used after 3\u00d7128x128. To scale PPO further, TIMERL would require more CPU memory, which easier to add than GPU memory."}, {"title": "8.4 Algorithm-specific scheduling", "content": "We explore TIMERL's ability to adapt scheduling to specific properties of the DRL algorithm in the 3\u00d7256\u00d7256 observations setting. We use the two variations of REINFORCE shown in Alg. 1, which use: (i) traditional Monte Carlo [16] returns; and (ii) n-step temporal-difference (TD) returns [19]. Despite the one line difference, Fig. 11a shows how TIMERL executes the two algorithms differently.\nSince Monte Carlo (red line) uses an anti-causal access pattern (r[t : T]), TIMERL must wait until the simulation finishes before learning. During simulation (red shading), TIMERL continually offloads observations to CPU memory, maintaining low memory usage, but not fully utilizing the GPU. Once finished, learning begins (orange shading), and TIMERL incrementally swaps observations back to GPU memory, learning from them while fully utilizing the GPU.\nWith n-step TD (blue and teal lines), however, TIMERL can execute a different strategy due to the window access pattern (r[t: min(t+n,T)]): after an n-step delay, it begins computing gradients in parallel with simulation (blue shading). This enables TIMERL to: (i) fully utilize the GPU through parallelism; (ii) only store the last n observations, deallocating older data as early as possible; and (iii) avoid swapping due to the lower memory demand. Due to the n-step delay before parallel learning, 64-step TD still experiences a slight GPU utilization dip, which 8-step TD does not.\nFor smaller n, TIMERL is able to parallelize work earlier, leading to faster iteration times, as shown in Fig. 11b. In either case, TIMERL uses the most effective execution schedule for each algorithm, maintaining low peak GPU memory usage even with 3\u00d7256\u00d7256 observations, allowing for large-scale training even on a single GPU."}, {"title": "9 Related Work", "content": "Polyhedral frameworks in DL. Tensor Comprehensions [74", "82": "is C-to-CUDA compiler that performs similar loop optimizations and additionally manages GPU memory across the hierarchy. TVM [83", "71": "are similar DL compilers that do away with direct polyhedral modeling. In general, these works perform low-level optimizations on specific loop nests, while TIMERL uses the polyhedral model to schedule the high-level execution of complete DRL programs.\nMemory management in DL. Prior work has explored gradient accumulation [84", "21,85": "to reduce memory pressure. TIMERL transparently integrates these ideas at a program level automatically through transformations (incrementalization) and scheduling (fetches and offloads). Recomputation [86,87", "88": "introduces a tensor abstraction similar to recurrent tensors, but focuses on memory reuse and fine-grained parallelism optimization, unlike TIMERL's emphasis on whole-program optimization. JANUS [89", "54": "have some support for dynamic computation compilation through a combination of symbolic shapes, speculative compilation, and runtime assertions that trigger recompilation. These approaches, however, are unprincipled, requiring large engineering effort to maintain, and do not support symbolic dependencies, making them unsuitable for whole-program optimization and scheduling.\nDynamic access patterns in DL are common across various DL domains. Causal, sliding-window, anti-causal, blocked, and other patterns appear in attention-based models [90"}]}