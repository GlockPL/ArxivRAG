{"title": "StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained Controllable Text-to-Speech", "authors": ["Haowei Lou", "Helen Paik", "Wen Hu", "Lina Yao"], "abstract": "This paper introduces StyleSpeech, a novel Text-to-Speech (TTS) system that enhances the naturalness and accuracy of synthesized speech. Building upon existing TTS technologies, StyleSpeech incor- porates a unique Style Decorator structure that enables deep learning models to simultaneously learn style and phoneme features, im- proving adaptability and efficiency through the principles of Lower Rank Adaptation (LoRA). LORA allows efficient adaptation of style features in pre-trained models. Additionally, we introduce a novel automatic evaluation metric, the LLM-Guided Mean Opinion Score (LLM-MOS), which employs large language models to offer an ob- jective and robust protocol for automatically assessing TTS system performance. Extensive testing on benchmark datasets shows that our approach markedly outperforms existing state-of-the-art base- line methods in producing natural, accurate, and high-quality speech. These advancements not only pushes the boundaries of current TTS system capabilities, but also facilitate the application of TTS sys- tem in more dynamic and specialized, such as interactive virtual assistants, adaptive audiobooks, and customized voice for gaming. Speech samples can be found in https://style-speech.vercel.app/", "sections": [{"title": "1 INTRODUCTION", "content": "Text-To-Speech (TTS) system converts written linguistic content into human-like speech, which is a crucial technology in today's digital landscape. By reducing the reliance on human speakers, TTS significantly reduces the cost of producing human speech. Make it increasingly important in applications such as smart homes [1, 6], robots [2], and virtual assistant [14, 20].\nResearch in TTS synthesis has evolved significantly over the past 30 years, transitioning from simple Hidden Markov Models (HMM) models [11, 11, 26] to today's sophisticated Deep Learning (DL) approaches [15, 16, 19, 24]. These advances strive to produce human speech with high accuracy, realism, and variability, meeting the increasing demand for high-quality speech synthesis.\nMany TTS systems are purely phoneme-driving, which limits their ability to capture the natural variations found in human speech. This often results in TTS systems lacking variation and style control, making the synthesised speech less engaging and less able to adapt the nuances changes in human speech across different situations [8, 18, 21, 25].\nRecently, FastSpeech [15] has been proposed as a state-of-the-art TTS system that integrates essential style features such as duration, pitch, and energy using feed-forward structure. However, this struc- ture poses challenges, as the style encoding layer is hierarchically placed above the phoneme encoding layer within the architecture, leading the backpropagation process to prioritize updates to the style encoder's parameters over those of the phoneme encoder."}, {"title": "2 RELATED WORK", "content": "In this section, we discuss prior research that has influenced or inspired the design of the StyleSpeech framework.\nTacotron 1 and 2 [19, 24] are the first successful deep learning- based TTS systems that have been widely evaluated and deployed in many real-world applications. The Tacotron family of TTS systems primarily uses a sequence-to-sequence (Seq2Seq) encoder-decoder framework to match inputs (characters or phonemes) with the output Mel-Spectrograms with an attention module in between learns to align the input tokens with the output Mel-Spectrogram.\nFastSpeech [15, 16] family was subsequently proposed to address word-skipping issues in long sequence inputs and to enhance the controllability of the synthesized speech. Unlike its predecessors, FastSpeech utilises the Transformer [23] structure to generate em- bedding sequences in parallel, which mitigates the word skipping problem and accelerates inference. FastSpeech2 [15] introduces a variance adapter that offers enhanced control over style features such as duration, pitch, and energy, making the TTS output more realistic.\nLower Rank Adaptation (LoRA) is a significant technique in Large Language Model (LLM) research in NLP designed to fine- tune large pre-trained models efficiently by training only a small subset of parameters [9]. This method specifically updates smaller sections of a model's parameters within crucial layers, drastically reducing computational costs. After training, these modified compo- nents are reintegrated into the original model for use during inference. LORA facilitates precise adaptations of models such as GPT [3] or BERT [5] to downstream tasks without complete retraining. The small-subset parameter ideology behind LoRA is also applicable in TTS tasks, where pre-trained TTS systems can be adapted like LLMs, treating each style adaptation as a downstream task.\nAutoVC [12] offers a zero-shot voice style transfer technique that converts a source person's voice into a target person's style while maintaining clarity and intelligibility. It features a unique architecture where the content encoder for the source voice and the style encoder for the target voice operate in parallel. The source voice embeddings first pass through a bottleneck structure that isolates the style features from the content-related features. These isolated content features are then merged with the target style features to effectively complete the voice conversion process.\nBuilding upon LoRA and AutoVC, we design a Style Decorator structure for StyleSpeech that allows deep learning models to learn style features separately and in parallel alongside phoneme features. The advantages of this structure compared with the feed-forward structure include: (1) style feature is trained as an independent mod- ule in parallel, enabling the integration of new style features without updating the entire model's parameters; (2) phonetic-related parame- ters are frozen during training, which preserves the uniqueness of phoneme feature; and (3) the system can integrate various types of style features as needed, enhancing the overall potential and adaptability of the TTS system. More details will be presented in Section 3."}, {"title": "3 STYLESPEECH", "content": "In this section, we introduce the architecture of the StyleSpeech framework. Diverging from the conventional feed-forward structure, we introduce a novel Style Decorator structure designed specifically to adapt style features while preserving distinct phoneme features. We provide an overview of the entire system in Figure 2a, followed by detailed discussions of the individual components of StyleSpeech in the subsequent subsections."}, {"title": "3.1 Acoustic Pattern Encoder", "content": "Typical inputs for TTS task is a sentence, that consists of a sequence of words or characters, denoted as $X = (x_1,...,x_n)$. In this work, we first transform the input sentence X into sequences of phonemes P and styles S, which we collectively refer to as \"acoustic features\", using Grapheme-To-Phoneme (G2P) conversion.\nThe objective of the Acoustic Pattern Encoder (APE) is to con- vert these acoustic features into sequences of acoustic embeddings. These are denoted as $H_p = (h_{p1}, ..., h_{pn})$ for phonemes and $H_s = (h_{s1},..., h_{sn})$ for styles, making them more suitable for further processing in deep learning frameworks.\nTo achieve this transformation, we employ feed-forward Trans- former (FFT) [16] to convert input sequences to embedding. Specifi- cally, each FFT block comprises a self-attention block paired with a 1D convolutional neural network (1DCNN). The self-attention mechanism incorporates multi-head attention to capture positional relationships within the sequence. Additionally, the standard two- layer dense network in the Transformer [23] is replaced with two 1DCNNs followed by ReLU activation. This modification enhances the model's ability to capture the close dependencies between adja- cent hidden states, crucial for accurately representing the sequences of characters or phonemes and their corresponding mel-spectrograms in speech synthesis tasks [16]."}, {"title": "3.2 Phoneme Duration Adaptor", "content": "The duration of each phoneme in human speech varies from sen- tence to sentence. The Duration Adaptor is employed to address the issue of length mismatch between the phoneme and spectrogram by adapting the length of each phoneme n embedding $H_p$ or style embedding $H_s$ to match the length m of the Mel-Spectrogram Y.\nPhoneme Duration Adaptor consists of two main components: the duration predictor and the length regulator. The duration predictor estimates the duration of each acoustic feature $L = \\{l_1, ..., l_n\\}, m = \\sum_{i=1}^{n} l_i$. These predicted durations are then used to adjust the length of each acoustic embedding to adaptive embedding $H_l = h_{l_1}, ..., h_{l_m}$. We adopt a similar setting to [15] for training and deployment of the duration adaptor."}, {"title": "3.3 Style Decorator", "content": "Style integration is a process to incorporate style features $S = \\{s_1,..., s_n\\}$ with the phoneme features $P = \\{p_1,...,p_n\\}$ to create fused embeddings H. Similar to the Decorator Design pattern in object-oriented programming (OOP), a structure that allows addi- tional functionality to be dynamically added to an individual object without affecting its core behavior. Style Decorator is a structure designed to incorporate style features S while preserving the distinct characteristics of exsiting acoustic features. The objectives of the Style Decorator are twofold: first, the adaptation process should not alter existing acoustic features such as phonemes; second, the style encoder model should be easily added or removed to incorporate or exclude style features from the TTS system. Traditionally, integrat- ing S within a feed-forward TTS system, such as a variance adap- tor [15], requiring updating the model parameters $W_1 = W_0 + \\Delta W$ by employing gradient descent across all layers. This method is of- ten rigid and costly, as accommodating new style features typically requires extensive model re-training."}, {"title": "3.4 Vocoder", "content": "In this study, we use the Griffin-Lim algorithm-based vocoder [7] to transform the Mel-Spectrogram Y, back to its speech audio A. Specifically, the Griffin-Lim algorithm focuses on reconstructing the phase estimation $P(Y)$, which indicates the position of each sinu- soidal waveform within its cycle at each time frame represented in the Mel-Spectrogram Y. This phase estimation is iteratively updated based on the phase and complex-valued spectrogram $Y \\cdot e^{iP(Y)}$ from the previous step, using the formula\n$P(y(t+1)) = P(x(t))y(t) \\cdot e^{iP(x(t))}$.\nOnce convergence is achieved, the final complex-valued spectro- gram A' is computed as $A' = Y \\cdot e^{iP(Y)}$, and the speech is re- constructed using the inverse Short Term Fourier Transform, $A = ISFT(A')$."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Dataset", "content": "In this study, we chose Chinese as our evaluation language due to its unique linguistic characteristics. Unlike English, Chinese uses characters to represent meanings rather than speech, resulting in the absence of a phonetic or syllabic writing system. To learn the pronunciation of Chinese characters, we must transcribe them into the Pinyin system, which contains phonemes and tones to describe the sound of each character. This poses a challenge in Chinese TTS systems because most applications treat Chinese characters or their Pinyin phonemes combined with tone symbols as a single acoustic pattern input, requiring a large number of embeddings for model training. However, our method encodes phonemes and tones as separate embeddings and then combines them using our style decorator structure. This strategy not only simplifies the model's architecture by reducing the number of embeddings required but also decreases the learning complexity.\nWe selected public Baker dataset [4] to evaluate our method. The Baker dataset contains 10,000 high-quality voice recordings, all in 16-bit WAV format with a sampling frequency of 48kHz. These recordings are the work of a professional chinese voice actress aged between 20 and 30, with an elegant and optimistic vocal tone. Pinyin phonemes serve as the phonemic input X, while tone serves as the style input S. We convert speech files into Mel-Spectrograms Y, with a frame size of 1024 and a hop length of 512. The dataset is split, allocating 4,000 sentences for training and 1,000 sentences for testing."}, {"title": "4.2 Configuration", "content": "StyleSpeech contains phoneme, style, and Mel-Spectrogram en- coders, each with four FFT blocks. To ensure compatibility with the dimensional requirements of actual Mel-Spectrograms, the output of the Mel-Spectrogram embedding is transformed to 80-dimensional Mel-Spectrogram using through an 80-dimensional linear layer.\nFor optimization, we adjust the learning rate with a warm-up strategy from the Transformer model [23], setting dropout rates at 0.5 for FFT blocks and 0.1 for the length adaptor to prevent overfitting.\nFurthermore, we conduct an ablation study to analyze the impact of integrating style embedding at different stages: 1) fusion before the length adaptor, 2) fusion after the length adaptor, and 3) fusion immediately before the linear layer. Please refer to Figure 2a for more details.\nWe also explore how training methods impact TTS performance, comparing joint training of phonemic and style encoders with LORA training, where the phonemic encoder is fixed and the style encoder and Mel-Spectrogram encoder are fine-tuned for new style features."}, {"title": "4.3 Evaluation Metrics", "content": ""}, {"title": "4.3.1 Quantitative Metric", "content": "Initially, we use the TTS system to generate speech outputs. We employ Word Error Rate (WER), Mel Cepstral Distortion (MCD) [10], and Perceptual Evaluation of Speech Quality (PESQ) [17], to quantitatively assess model's performance.\nWe assess the accuracy of synthesized speech using WER by first generating speech with a TTS system and then transcribing it through OpenAI's Whisper API [13]. We compare these transcriptions to the original text, with a lower WER indicating better synthesis accuracy. In speech synthesis, we prioritize speech accuracy over written characters. Thus, we convert the transcript to Pinyin phonemes, such as \"{ni3 hao3},\" and separate these into phonemes \"{n, i, h, ao}\" and tones \"{0, 3, 0, 3}\" to calculate WER at the phoneme and style levels."}, {"title": "4.3.2 LLM-Guided MOS", "content": "We introduce LLM-MOS to qualita- tively rate synthesized speech quality on a scale of 1 to 5, combining the precision of quantitative metrics with the nuanced insight of an LLM to reduce subjectivity found in traditional MOS evalua- tions. This involves synthesizing speech from different TTS systems, calculating metric thresholds based on percentile rankings, and as- signing a 1-5 rating according to these thresholds. Each speech's overall quality rating is then derived by averaging its WER, MCD, and PESQ scores, providing a comprehensive and straightforward evaluation method for TTS performance."}, {"title": "5 RESULTS AND DISCUSSION", "content": ""}, {"title": "5.1 Imapact of Style Decorator", "content": "In Table 1, we present the impact of the Style Decorator on var- ious quantitative metrics, as detailed in Section 4.3.1. The best- performing method for each metric within each training strategy is highlighted in bold. Table 2 shows the statistically significant tests compared with the baseline model. WER-P stands for Pnoneme- level WER and WER-T stands for Tone-level WER.\nThe data in Table 1 vividly shows that StyleSpeech outperforms the baseline model across all metrics, particularly with significant im- provements in both WER and MCD metrics. Notably, StyleSpeech achieves over a 10% enhancement in overall WER and a 15% im- provement in tone-level WER. Since the phoneme-level improve- ments are minimal, it suggests that the major gains in overall WER primarily stem from the system's enhanced ability to accurately synthesize tone styles. This underscores the efficacy of the Style Decorator structure and its robust capacity to adapt style features."}, {"title": "5.2 Joint versus LoRA training", "content": "In terms of how training methodologies impact model performance, LORA training typically results in greater enhancements compared to joint training. This is evident in stage 1 fusion results, where LORA training maintains lower Word Error Rates (WER) at 0.296, significantly outperforming joint training, which sees WER soar to 0.958. The superior performance of LoRA training is due to its ability to freeze phonemic parameters during the training process, which helps preserve the unique characteristic inside phonetic features.\nIn contrast, joint training tends to merge style and phoneme em- beddings, making it challenging for the model to accurately inter- pret and align them with the corresponding Mel-Spectrogram. This blending can lead to misrepresentations in the output and a loss of distinguishable details between different phonemes. As demon- strated in Figures 3b and 3c, speech generated under joint training exhibit blurrier frequency boundaries compared to those from LORA training. By treating style feature as an additive layer or 'decorator' of phoneme feature, LoRA training enables the seamless integration of new style variations without disrupting the phonemic structure, ensuring that each phoneme retains its integrity and is distinctly recognizable. Conversely, the increased complexity of the learning task in joint training can confuse the model and make parameter tuning more challenging."}, {"title": "5.3 Impact of Fusion Stage", "content": "The impact of the fusion stage between phoneme and style embed- dings on a method's performance is significant. The fusion stage occurring at lower layers tends to influence the output more signifi- cantly, whereas those closer to higher layers have a lesser impacts.\nStyleSpeech produces the most accurate speech when fusion oc- curs at an early stage, before the length adaptor. This is demonstrated in both Table 1 and Table 3, where StyleSpeech 0 has the lowest WER score under both joint and LoRA training cases. This early- stage fusion combines phoneme and style embeddings, allowing the length adaptor to adjust the duration of each phoneme based on these hybrid embeddings. Such synchronization enhances control over the synthesized speech, thereby improving its accuracy.\nConversely, fusion at stage 1, presents a paradox. Although the perceptual quality of the speech is improved, the accuracy dimin- ishes. This decrease in accuracy occurs because the style embeddings do not influence the phoneme duration predictions, leaving the du- rations solely determined by phoneme embeddings. This approach fails to capture the correct timing of speech, often resulting in outputs that are unrealistic and indistinguishable. For example, in Figure 3c, the first and second phonemes merge, creating a single phoneme. However, fusing post-length adaptor does enhance the distinction between phoneme and style features, leading to higher perceptual quality, as shown in Figure 3c where frequency boundaries are more defined compared to the early-stage fusion version in Figure 3b.\nFusion at the final stage appears to pose minimal influence on performance. By this stage, the hidden embeddings are already satu- rated with strong acoustic and tonal characteristics. Thus, additional fusion at this point does not substantially modify speech, leading to negligible effects on the output."}, {"title": "6 CONCLUSION", "content": "In this study, we present StyleSpeech, a novel TTS system that can accurately synthesize human speech and efficiently adapt to style features while preserving phoneme features. Furthermore, we developed an automated qualitative metric, LLM-MOS, designed to provide an objective evaluation of TTS systems relative to others, ensuring a more equitable assessment.\nThe limitations of this study include a focus on a single language, which may limit the generalizability of our findings. Expanding to multiple languages can help assess the transferability of our meth- ods. Additionally, our use of simple additive fusion techniques may restrict the TTS system's performance. Therefore, further research could explore advanced methods, such as Mixture of Experts (MOE), and refine LLM evaluation prompts to enhance both the system's performance and the precision of our evaluations. Our future re- search will primarily focus on incorporating a broader range of style features and enhancing fusion strategies to further advance the capabilities of the TTS system."}]}