{"title": "Semantic Prototypes: Enhancing Transparency Without Black Boxes", "authors": ["Orfeas Menis-Mastromichalakis", "Giorgos Filandrianos", "Jason Liartis", "Edmund Dervakos", "Giorgos Stamou"], "abstract": "As machine learning (ML) models and datasets increase in complexity, the demand for methods that enhance explainability and interpretability becomes paramount. Prototypes, by encapsulating essential characteristics within data, offer insights that enable tactical decision-making and enhance transparency. Traditional prototype methods often rely on sub-symbolic raw data and opaque latent spaces, reducing explainability and increasing the risk of misinterpretations. This paper presents a novel framework that utilizes semantic descriptions to define prototypes and provide clear explanations, effectively addressing the shortcomings of conventional methods. Our approach leverages concept-based descriptions to cluster data on the semantic level, ensuring that prototypes not only represent underlying properties intuitively but are also straightforward to interpret. Our method simplifies the interpretative process and effectively bridges the gap between complex data structures and human cognitive processes, thereby enhancing transparency and fostering trust. Our approach outperforms existing widely-used prototype methods in facilitating human understanding and informativeness, as validated through a user survey.", "sections": [{"title": "1 INTRODUCTION", "content": "In the rapidly evolving landscape of data-driven decision-making and machine learning (ML) advancements, the pursuit of explainability and interpretability stands as a critical imperative. As ML models evolve in complexity and scope, understanding their decision-making processes becomes paramount for fostering trust, ensuring accountability, and promoting fairness. Equally crucial is the comprehension of the datasets upon which these models are trained and applied. Data, often vast and heterogeneous, serve as the foundational bedrock upon which ML models operate. However, the sheer volume and intricacy of data present formidable challenges in discerning meaningful insights, uncovering hidden biases, and ensuring the quality and fairness of AI-driven systems. Thus, the need for transparent and interpretable methodologies that not only shed light on ML model behavior but also facilitate a deeper understanding and management of data is unequivocal.\nPrototypes have emerged as pivotal constructs not only for explaining machine learning models but also for comprehending the underlying data [18]. Acting as archetypal representations, prototypes encapsulate the essential characteristics or features of specific clusters or classes within a dataset, providing intuitive insights into its inherent properties. Research on human cognition and reasoning has shown that the use of prototypical examples is fundamental to the development of effective strategies for tactical decision-making [7, 30], and recent user studies show that concept-based and prototype explanations are prefered by users over other existing explanations [19]. For instance, in information retrieval, prototypes act as exemplars for enhancing search efficiency and relevance ranking by aiding in query expansion. Additionally, there is a growing interest within the AI community in case-based reasoning and prototype-based classifiers, highlighting the versatility and acceptance of prototypes in various applications. By leveraging prototypes, stakeholders can navigate the complexities of data-driven decision-making more effectively, fostering transparency and enabling nuanced decision-making processes.\nHowever, the majority of existing prototype approaches exhibit a major structural limitation that undermines their effectiveness and trustworthiness: they rely solely on the raw, unstructured feature space. This can be problematic from many aspects. Firstly, the feature space is often not understandable, an issue that persists across many eXplainable AI (XAI) methods, and can result in lack of intuition and potential for misinformation [23-26, 33]. For example consider genomics, where the feature space consists of DNA sequences which can consist of millions or billions of base pairs for an organism, and in which there can be interdependencies that are thousands of base pairs apart. Parts of the genome might be irrelevant, others might regulate the expression of genes that are elsewhere in the sequence, others might be genes themselves etc. This raw feature representation is not understandable, even to a domain expert, so a traditional prototype in this case would not be helpful. Secondly, in many models, especially those involving complex interactions or relationships between features, a single or even a few examples might not capture the full range of interactions or the subtleties involved in model decisions. This can make it difficult to convey the full complexity of the model's decision-making process through just prototypical examples, and can lead to oversimplification and misinterpretation. For example, consider the image shown in Figure 2 from the CLEVR-Hans3 dataset [36], for which it is known that the class semantics are \"small metal cube and small sphere\". By just looking at the pixel representation of the prototype, even if the prototypical parts of the image have been highlighted (see bounding boxes in Figure 2), it is impossible to discern the characteristics that make them prototypical. It could be the color, size, shape or texture of each object, or even their location in the image. Therefore, without telling a user the specific semantics that make this image (and the highlighted parts) prototypical, it is easy for them to misinterpret the explanation and be misled. Thirdly, prototypes cannot be expected to generalize to all cases, and even though they might be excellent representations of a particular class, it is not made clear to an end-user which aspects of the prototype make it representative, and to which cases it might generalize. Additionally, several prototype methods do not act on the feature representation itself, opting instead to utilize black-box models that transform the features into a lower-dimensional latent space representation. This exacerbates the aforementioned issues, as latent representations are non-intuitive and unintelligible to humans [40], and it also creates a paradoxical situation where non-interpretable models are used to provide explanations or interpretability, which might also facilitate malicious manipulations [17]. Instead, recent research emphasizes the importance of explaining the prototypes [27, 40] underscoring the necessity for a semantic level of information alongside prototypes.\nOur approach represents a novel solution to address the limitations in existing prototype methods. To tackle the challenge of using raw data to define prototypes, we propose a shift towards semantic prototypes. In our approach, prototypes are not selected based on raw input features but on the semantic descriptions associated with each data point. By leveraging concept-based semantic descriptions to create clusters of data described by semantic rules as shown in Figure 1, we ensure that prototypes are representative of the underlying data distribution while maintaining transparency and interpretability. This process eliminates the need to map data to a non-interpretable latent space, as distances are measured on"}, {"title": "2 RELATED WORK", "content": "Our work is positioned at the intersection of several key areas within artificial intelligence, notably explainable AI (XAI), prototype-based methods, and case-based reasoning [1]. By leveraging semantic prototypes, our approach not only enhances model interpretability but also facilitates a clearer understanding of datasets, offering a comprehensive overview of their inherent structure and characteristics. Classical algorithms like k-medoids [32] have traditionally been used to select representative subsets of data points, illustrating early methods of data summarization through clustering. More recently, seminal works such as [18] and [15] have leveraged prototypes to critically evaluate models and enhance transparency in machine learning decisions, establishing prototypes as an interpretability method.\nA significant body of research has focused on using prototypes to create more interpretable classifiers. This approach, often referred to as case-based or example-based classification, aims to enhance the transparency of AI models by relying on representative examples. By integrating prototypes into the classification process, these methods strive to provide intuitive, example-driven explanations that make the model's decisions more understandable to humans. [5] is a seminal work in this direction, introducing ProtoPNet, a deep network architecture that dissects images by finding prototypical parts and combines evidence from these prototypes to make final classifications. [41] claims to improve the classification performance of ProtoPNet with a method to learn support prototypes that lie near the classification boundary in the feature space. In a similar vein, [29] introduces ProtoTree, a prototype-based decision tree that employs prototypical patches as proxies for class-defining semantics. Several other works follow this rationale of prototypical learning through various approaches [2, 3, 21, 34, 35, 41, 44]. However, their reliance on raw data limits the interpretability of their methods and the intuitiveness of the prototypes, potentially leading to misleading explanations.\nRecent research aligns with our work by acknowledging the limitations of providing explanations in terms of raw data and highlights the necessity to \"explain the prototypes\". In [27], the authors introduce a method to provide further insights for prototypes based on existing methods like ProtoPNet by altering some characteristics of the image, such as hue and saturation, and providing explanations based on that information. Similarly, [40] proposes the Semantic Prototype Analysis Network (SPANet), an interpretable object recognition approach that through additional semantic information enables models to explicate the decision process by \"pointing out where to focus\" and \"explaining why\" on a semantic level. Our work also utilizes information on the semantic level to define prototypes and produce semantically enriched explanations, bearing strong similarities with recent rule-based [22-24] and counterfactual methods [8] that use semantic descriptions of data to provide intuitive, human-understandable explanations.\nOur work is further supported by research discussing the shortcomings of latent space prototype interpretability, as outlined in [17], where the non-interpretability of latent space is highlighted, showing that existing methods using non-interpretable embedding spaces limit the interpretability of prototypes and are vulnerable to malicious attacks. Efforts to bridge the \"semantic gap\" between latent space and pixel space through the correlation of prototypes with ground-truth object parts [28] still rely on opaque procedures to map raw data to the latent space. [42] claims to address this opacity by introducing a plug-in transparent embedding space to bridge high-level input patches and output categories.\nIn contrast, our approach eliminates the reliance on non interpretable latent spaces by using semantic descriptions directly, making each step transparent and interpretable. This not only enhances trust in the explanations provided but also allows for a more robust understanding of both the model and the data. Our method stands out by addressing both the need for clear, semantic-level explanations and the requirement for prototypes that truly represent the data in an intuitive and human-understandable way."}, {"title": "3 SEMANTIC PROTOTYPES", "content": "In this section we define the proposed framework for semantic prototypes. At the core of the framework is the notion of an Attribute Set Description (ASD), which provides a simple way to represent data samples semantically, as a set of entities, where an entity is represented as a set of attributes.\nDEFINITION 1 (ATTRIBUTE SET DESCRIPTION). Given a set V, an Attribute Set Description is a set of the form ${S_1, S_2, ..., S_n}$ where each ${s_i}$ is of the form ${a_{i1}, a_{i2},..., a_{im};} \u2286 V$.\nThe set V is a vocabulary that lists all the possible attributes an entity can have, so an ASD lists the attributes of a collection of entities. For defining semantic prototypes, we assume that we have data where samples are described by an ASD. Specifically, we assume that our data D consist of triples d = (x, y, z) where x is a raw data point, (e.g. an image, audio signal, DNA sequence etc.) y is a label, and z an semantic description of that data point. We assume that z is an ASD that reflects the contents of x. In the case of an image the entities could be objects depicted within the image, each characterised by shape, colour, size, etc., while in the case of a speech signal, the entities could be utterances that are characterized by loudness, pitch, intonation, rhythm etc.\nThe assumptions that i) there are available data with ASDs and ii) that the ASDs accurately describe the data samples are worth further discussion. In the ideal case, such semantic descriptions will have resulted by human expert annotation, especially in decision-critical domains. There already exist multiple datasets with manually-added semantic descriptions or metadata that can be used as ASDs, both for general-purpose and domain-specific tasks, such as Audio set [14] including audio events accompanied by an ontology describing their semantics, the Visual Genome [20], containing images accompanied by scene graphs, where entities are linked semantically to WordNet [12], and the cancer genome atlas [43], that includes genomic sequences along with a rich set of clinical and other information, among others. Furthermore, one could also use traditional, transparent feature extraction techniques to generate the ASDs, or, even more complex models, such as large vision-language models, similar to recent works that relate to ours [40]. The point of the ASD is to provide a meaningful description of a data sample at a level of abstraction that is understandable.\nAn ASD can also be used to describe a set of data samples. Given an ASD z', we will say that z' subsumes z if ${\u2200s_i \u2208 z' \u2203s_j \u2208 z s.t. S_i \u2286 s_j}$. This can be thought of as z' being more general than z. Given a data point d = (x, y, z), if z' subsumes z we will also say that z' describes the data point d. Essentially, z' describes d, if the description z of d contains entities with attributes that match or exceed those described in z', thus, there can be ASDs that describe multiple data points. For example a data sample with ASD {{Cat}, {Dog}} is described by the ASD {{Cat}}, and so is the data sample with ASD {{Cat}, {Mouse}}. We utilize this idea for the semantic prototypes, by first finding ASDs which describe only data points with a particular label. We call such ASDs class cluster descriptions (CCD) of that label.\nDEFINITION 2 (CLASS CLUSTER DESCRIPTION). A class cluster description of class c, is an ASD r such that, if r describes a datapoint d = (x, y, z) (i.e. r subsumes z), then y = c.\nIntuitively, a CCD semantically describes a cluster of data points that belong to a specific class, and no other data points. It can be interpreted as an IF THEN rule in that IF a data point is described by a CCD, then it belongs to that particular class. The purpose of identifying and semantically describing clusters of data points is to subsequently find the most representative or informative samples for those clusters, which can then be given as prototypes, along with their semantic description (ASD), and the semantic description of why they belong to their class (CCD). In particular, given a CCD for a label, the corresponding semantic prototype is the data point whose ASD contains the least redundant information among points that fit that description.\nDEFINITION 3 (SEMANTIC PROTOTYPE). A semantic prototype p for a class cluster description r is a data point p = (x, y, z) that is described by r, and for which, given a distance metric dist, for every other data point d' that is described by r, it holds that dist(z, r) \u2264 dist(z', r).\nIntuitively, this means that a semantic prototype is a data point that materialises all the semantic information of the class description, since it is described by it, and contains as little extra information as possible. The choice to limit the extra information is made to ensure that end-users are not distracted by irrelevant characteristics of the data point, such as objects in an image that do not affect what class it belongs to. Regarding the distance metric dist, in our implementation we opt for a set edit distance, as it has been used in other semantic explainability methods [8], but other distance metrics could potentially be used, such as the extension of Jaccard similarity to sets of sets."}, {"title": "4 COMPUTING SEMANTIC PROTOTYPES", "content": "Within the proposed framework we can find prototypes using semantic criteria, and we can also answer the question \"Why is this example a prototype?\", by accompanying the prototypical example with a semantic class description when showing it to an end-user. To this end, there are are two main components that need to be computed. First, is the process of identifying and describing clusters within each class (computing CCDs), and second is the process of choosing the most informative data sample for each cluster.\n4.1 Finding class cluster descriptions\nAlgorithm 1\nInput: A set P of positive data points and a set N of negative data points.\nOutput: A set C of class cluster descriptions.\n1: C0\n2: for d = (x, y, z) \u2208 P do\n3: description \u2190 z\n4: l\u2190 P \\ {d} as a list sorted by similarity to z\n5: for d' = (x', y, z') \u2208 l do\n6: remove d' from l\n7: ncd \u2190 merge(description, z')  \u25b7 new candidate description\n8: if ncd does not describe any data point in N then\n9: description \u2190 ncd\n10: end if\n11: l \u2190 I sorted by similarity to description\n12: end for\n13: CCU {description}\n14: end for\nAs the space of all possible CCDs is exponentially large, our approach works by first heuristically generating a large (but polynomial) number of potential CCDs, filtering out those that do not satisfy the criteria (i.e. the clusters contain data samples that have a different label), and finally choosing a subset of the computed CCDs, depending on the number of prototypes that we want to produce and on the class coverage of the CCDs.\nGiven a dataset D, and a class c for which we want to produce prototypical examples, we would ideally like to produce the smallest number of CCDs that describe the entirety of class c without describing any data points from other classes. It is worth mentioning that since finding CCDs is equivalent to finding rules of the form \"IF data sample contains entities with specific attributes THEN it is classified to class c\", existing rule-based methods could be adapted for finding CCDs [4, 22, 24, 45].\nIn our implementation, we utilise Algorithm 1 to compute the initial CCDs, using P = {(x, y, z) \u2208 D : y = c} as the positive data points and N = {(x, y, z) \u2208 D: y \u2260 c} as the set of negative data points. Alg. 1 is a greedy algorithm inspired by [22] that starts with an ASD, and using a similarity metric (eq. 1) as a heuristic, greedily merges (Alg. 2) positive descriptions into more general ones, and subsequently checks that the more general descriptions do not describe any negative data points. In contrast to [22], we repeat this process for each positive data point, because we want to ensure that each positive data point fits at least one CCD, and to also mitigate \"bad\" choices induced by the heuristic. This strikes a balance between only utilizing each data point once, and exploring the combinatorially large number of all possible choices.\n${sim(z_1, z_2) = \\sum_{S_i \\in z_1} max_{S_j \\in z_2} |S_i \\cap S_j| / (|z_1|\\cdot |S_i \\cup S_j|) + \\sum_{S_i \\in z_2} max_{S_j \\in z_1} |S_i \\cap S_j| / (|z_2| \\cdot |S_i \\cup S_j|)}$\nThe similarity metric we use, as described in equation 1, utilizes the Jaccard similarity to compare the entities described in each ASD. For each entity in ${z_1}$, it calculates the maximum number of attributes it shares with any entity in ${z_2}$. This is averaged over the entities in ${z_1}$, repeated symmetrically for the entities in ${z_2}$, and then these two quantities are averaged. We average over entities so that if ${z_1}$ describes many more entities than ${z_2}$, it does not dominate the total similarity and vice versa. The two quantities are averaged so that the final quantity is between 0 and 1, as is commonly required of a similarity metric.\nAlgorithm 2 merge\nInput: Two ASDS ${z_1, z_2}$.\nOutput: An ASD z which generalizes ${z_1}$ and ${z_2}$.\nz' \u2190 ${S_i \u2229 S_j | S_i \u2208 z_1, S_j \u2208 z_2}$  \u25b7 combine\nz\u2190 ${s_i \u2208 z' | #s_je z' s.t. S_i C s_j}$  \u25b7 trim\nThe merge operation also follows the paradigm of [23], by finding all common attributes for pairs of entities from ${z_1}$ and ${z_2}$, and then trims the resulting ASD. This way of combining ${z_1}$ and ${z_2}$ is essentially the direct product of finite structures, applied to ASDs. It is also the join operation on the lattice induced by ASD subsumption. The resulting ASD of this merge operation holds the property that it subsumes ${z_1}$ and ${z_2}$, and is subsumed by any other ASD that subsumes both ${z_1}$ and ${z_2}$. Therefore, it is their most specific generalization. This operation is widely adopted for separating structured positive and negative examples [6, 16, 23, 31]. The trimming operation used only removes redundant entity descriptions, without sacrificing the property of the most specific generalization.\nAlg.1 results in a large set of CCDs which are used as candidates for subsequently finding semantic prototypes. As each CCD results in a prototype, we want to limit their number so that they can all be shown to a user without overwhelming them. In our implementation we again do this greedily, by choosing at each step the CCD that describes the most positive samples that are not already described by any previously selected class descriptions. Selecting the fewest number of CCDs that in total describe all positive data points is an instance of the set cover problem, while selecting k CCDs that describe as many positive data points as possible is an instance of the maximum coverage problem. Both problems are NP-complete and the greedy algorithm is the best polynomial-time approximation, up to lower-order terms, unless P=NP [10, 11, 38].\n4.2 Finding semantic prototypes\nHaving established the CCDs, we proceed to find prototypes for each class by identifying the closest data point to each CCD that is simultaneously described by it. In our implementation, the closeness is determined through the set edit distance, a metric quantifying the distance between the CCD and the ASD of each data point. In particular, as we know that the CCD r describes the data sample d = (x, y, z), the only necessary edits to transform r into z are insertions of attributes into the sets contained in r. To do this, for every pair of sets (sr, sz) where sr \u2208 r, Sz \u2208 z and sr\u2286 sz we compute the number of insertions |sz \\ sr|. Then the pairs (sr, Sz) are organized into a bipartite graph, where the weights of the edges are set to be the number of insertions computed previously. It is guaranteed that every sr will have at least one edge, since we know that r describes z, meaning that Vs, er \u2203sz \u2208 zs.t. Sr Sz. Finally, we compute the minimum number of additions required to transform sr to sz, yielding the edit distance between the class description and the data point, by adapting the minimum weight full match algorithm, as used in [13]. This is computed for all data points, and then the semantic prototype is chosen to be the one with the least edit distance."}, {"title": "5 EXPERIMENTS", "content": "In our experiments, we utilized two datasets to evaluate the effectiveness of our approach: the CLEVR-Hans dataset [36] and the CUB-200 dataset [39]. The CLEVR-Hans dataset comprises artificial images featuring a varying number of objects with different sizes, shapes, colors, and textures. This dataset is chosen because of its simplicity and clear semantics and characteristics that allow for a straightforward demonstration of how our method excels where other explanation techniques fall short. The second dataset we employed is the CUB-200 dataset, which consists of real images of birds divided into 200 classes according to species. This dataset allows us to evaluate our method in real-life images. It is widely used in prototype-based methods, making it ideal for comparison. Our code is available here: https://github.com/ails-lab/Semantic-Prototypes.\n5.1 Qualitative Evaluation\n5.1.1 CLEVR-Hans. We conducted experiments on the CLEVR-Hans dataset to qualitatively analyze the informativeness and interpretability of our semantic prototype approach. We compared our method against existing prototype-based techniques, focusing on how well each method captures the clear and distinct semantics present in the dataset. Each class in the dataset has a clear semantic description that characterizes all the images within that class. For example, all images in Class 1 contain at least one large cube and one large cylinder; Class 2 images feature at least one small metal cube and one small sphere; Class 3 images include at least one large blue sphere and one small yellow sphere. The following Class Characteristic Descriptions (CCDs) produced by our method correctly reflect the characteristics of each class\nClass_1 = {{Large, Cube}, {Large, Cylinder}}\nClass_2 = {{Small, Metal, Cube}, {Small, Sphere}}\nClass_3 = {{Large, Blue, Sphere} {Small, Yellow, Sphere}}\n5.1.2 CUB-200. By analyzing the performance of our method on the CUB-200 dataset and comparing it to existing widely used methods, we assess how our approach handles real-world data, and showcase the merits of the semantic prototypes. We show that our method produces clear, semantically meaningful prototypes that align well with human understanding of bird species, highlighting the differentiating factors among similar species, whereas other methods fail to detect them. Through the inspection of prototypes of related species of birds that have great visual similarities, we are able to see that our method, accurately pinpoints the important features that differentiate the classes, while other methods do not. For example, in Figure 4 we can see two species of gulls, a ring-billed gull, and a glaucus-winged gull. The CCDs provided by our method indicate the characteristic black ring of the ring-billed as well as its black tail, and yellow eyes. When these features are juxtaposed with the CCDs provided for the glaucus-winged gull that include the characteristic pink legs and black eyes, the user is able to clearly distinguish these two species, while also understanding the characteristics of each gull. However, other widely-used methods like ProtoPNet [5], fail to highlight these distinguishing characteristics, and indicate the wings or even the background as the prototypical patch of these classes as shown in Figure 5. This can potentially be misleading and lower user trust because of the method's inability to detect the differentiating factors.\n5.2 User Survey\nSetup. For the human survey, we adopted a methodology similar to that described in [9, 37]. The purpose of this survey is to evaluate the effectiveness of prototype methods in teaching individuals about an unfamiliar task, through two primary stages: training and testing. During the training phase, participants are exposed to prototype instances from two analogous classes within a specified method, accompanied by relevant explanations where applicable. For example, Protodash employs solely images, ProtoPNet features images with a bounding box, and our method presents images alongside textual CCD. Each participant reviews four prototypes per class, totaling eight prototypes. To mitigate bias from recognizable class names, such as \"Yellow-breasted Chat\", these names are replaced with \"Class A\u201d and \u201cClass B\". In the testing phase, participants are required to classify ten images from the test set as either \u201cClass A\u201d or \u201cClass B.\u201d This approach is designed to assess their ability to learn the task by simply viewing random images from the training set, without any systematic selection algorithm or additional explanations. To evaluate the generalizability of the method, the experiment employs different pairings of labels as \"Class A\u201d and \"Class B\u201d. These pairs, such as Least Auklet versus Parakeet Auklet and Pelagic Cormorant versus Brandt Cormorant were selected because of their high visual similarity, which was confirmed by their high confusion rates as identified by the pretrained classifier cited in [37].\nParticipants underwent these two phases for the following six different methodologies: only CCDs, Random Images (baseline), semantic prototypes (our method), ProtoPNet [5], Protodash [15], and ProtoPNet* (ProtoPNet prototypes along with explanations produced using the methodology introduced in [27]). The CCDs were presented before the random images to ensure that participants had no prior knowledge about the distribution of the images, starting with only textual descriptions provided by the CCDs. These were used as a baseline for our method to evaluate the usefulness of the prototypes compared to plain semantic descriptions. Different classes of birds were randomly permutated among the different methods so that each user couldn't use prior knowledge from a previous step of the survey to classify the images of a later method. Participants were ultimately asked to indicate which prototype method they preferred and found most helpful.\nThe study involved 20 PhD candidates in computer science who participated voluntarily after a call for participation. Altogether, they conducted 120 tests in total. The candidates possessed no prior knowledge about bird species, which ensured an unbiased approach to the tasks presented.\nResults. Table 1 presents the results, showcasing the accuracy and participant preferences for each method. The results of the user survey clearly demonstrate that our method outperformed all other methods in terms of performance in machine teaching and user satisfaction, exhibiting the highest accuracy and the lowest standard deviation along with the highest user preference. This indicates that our approach effectively helped users focus their attention in the right direction, achieving a consistent understanding across participants.\nWe see a significant discrepancy between the accuracy of participants who only read the CCDs of the two classes and those who viewed actual images from the dataset. While CCDs provide essential information for differentiating each class, they alone are insufficient for users to fully grasp the necessary distinctions. Familiarity with dataset instances plays a crucial role in properly understanding how to differentiate the classes. Additionally, the high standard deviation in performance suggests that the criteria for class selection vary significantly among users. Initially, this variation might seem counter-intuitive since users are provided with the fundamental characteristics of the classes, seemingly simplifying the classification task. However, participants struggled to intuitively grasp these explanations without examples from the dataset, as interpretations of a rule such as \"The bird has a plain pattern on its head\u201d varied widely among users who had not seen how this characteristic manifests in actual birds.\nMoreover, although adding supplementary information to each prototype intuitively appears beneficial, this is not reflected in user performance. The accuracy of users who learned with the help of explanations from ProtoPNet and ProtoPNet*, which include images along with two different types of additional information, was comparable to that of learning from randomly selected images without any explanations. Notably, ProtoPNet's performance was slightly below this baseline, with a relatively higher standard deviation, indicating that the criteria for classifying images varied considerably. ProtoPNet* showed slightly improved performance and a relatively lower standard deviation, but still very close to the baseline.\nAdditionally, Protodash, which presents less information compared to other methods (except for the baseline), achieved higher performance than the preceding methods. This improvement primarily occurred because users could intuitively discern the differences between the two classes by comparing Protodash prototypes. Additionally, Protodash, which presents less information compared to other methods (except for the baseline), achieved higher performance than the preceding methods. This improvement primarily occurred because users could intuitively discern the differences between the two classes by comparing Protodash prototypes.\nAfter each participant completed the user survey, we conducted short interviews to gather feedback and insights on the methods. Here we present some notable observations highlighted by multiple participants. First, participants found it difficult to map the semantic information of the CCDs to the actual data when the prototypical images were not present. This underscores the importance of providing enhanced explanations in multiple formats, especially in areas where users lack expertise. Additionally, participants criticized the seemingly incorrect patches of ProtoPNet, noting that they often ignored these patches and instead identified their own patterns in the images. Many participants found the semantic explanations of ProtoPNet* unintuitive and uninterpretable because they could not relate them to the data, often choosing to disregard them. Regarding our method, some users mentioned that the presence of the semantic description helped them identify the distinguishing characteristics of the classes, though they had to pay more attention to process all the provided information compared to methods offering only plain images. They also suggested that smaller, more focused rules would be greatly beneficial.\nRegarding user preferences, it is important to note that half of the participants found our method's explanations more helpful than any of the alternatives. However, Table 1 also reveals a preference for methods that offer minimal information, specifically those consisting only of images without any textual content. This is highlighted by the fact that 45% of users identified the prototypes provided randomly, by ProtoPNet, and by Protodash as the most helpful. Interestingly, there was a stronger preference for a set of randomly selected images over the ProtoPNet* prototypes, which include images accompanied by textual explanations, even though the latter method resulted in higher user performance. Additionally, users seemed to prefer the ProtoPNet explanation, which features an image with a bounding box, despite its lower effectiveness for learning. This highlights an important trade-off in explanation methods: informativeness versus simplicity. Some users prefer methods that contain the most useful information and can help them perform a task with careful attention, while others prefer explanations that are simple and do not require thorough investigation, even if they lead to poorer results. Therefore, it is important to keep our methods as concise as possible, avoiding unnecessary information to create explanations that are both simple and informative."}, {"title": "6 CONCLUSIONS", "content": "In this work, we have introduced a framework for producing semantic prototypes, given a dataset that is enriched with humanunderstandable semantic information. We have developed a methodology for computing such semantic prototypes in practice, and we have demonstrated their merits qualitatively and quantitatively.\nThe two main takeaways from our work are that i) It is important that prototypes are accompanied by some form of explanation of \u201cwhy is this a prototype?\u201d, which should be transparent and reliable, and ii) It is useful to compute prototypes in terms of their semantics, instead of their feature representation, and to make sure that there is as little as possible redundant information. This ensures that a user can more easily extrapolate the semantics of a class, or a cluster, compared to choosing the most representative sample in the dataset which might contain redundancies. Especially in large complex datasets, it could be challenging to find data which contains only information that causally links it to a class and as least additional information as possible, thus relying on the distribution of features could lead to less understandable prototypes, compared to relying on the semantic information.\nRegarding future extensions of our work, we have identified several key areas to be explored. Firstly, as Large Language Models (LLMs) are prevalent in both academia and industry, an interesting area to explore is the utilization of prototypes and their descriptions as a complement or enhancement to few-shot or in-context learning. Furthermore, for several natural language processing tasks, it might be useful to utilize LLMs for generating semantic descriptions, and then employing our proposed method for finding semantic prototypes in the data. A second interesting area to explore is the utilization of knowledge representation and knowledge graphs. In particular, the scale and interconnectedness of such structured data can be very useful for identifying clusters and prototypes semantically. In this regard, an extension of the algorithmic approach from sets of sets representations to labeled directed graph representations will provide much more expressive descriptions, which might in turn result in more understandable and informative prototypes. Thirdly, there is an array of domain-specific applications for the proposed methodology. An example is the domain of music, where symbolic representations, such as musical scores and notation can serve as semantic descriptions of audio recordings. Furthermore, besides prototypes, there are numerous other forms of explanations that could potentially benefit from utilizing the human-understandable semantic level of abstraction and could be combined with prototypes, similar to how we combine the prototypes with CCDs which are closely related to rule-based explanations. An example would be accompanying the prototypes with their counterfactual data point, along with the associated semantic descriptions. Finally, the difficulty of objectively evaluating X\u0391\u0399 methodologies and frameworks, and reproducing results is a known issue. In the future, we plan to extend the evaluation procedure to more participants of different backgrounds, and ideally guided by disciplines of human behavior and cognition, it would be worth exploring further what a good explanation should look like."}]}