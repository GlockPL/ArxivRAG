{"title": "Text-To-Image with Generative Adversarial Networks", "authors": ["Mehrshad Momen Tayefeh"], "abstract": "Generating realistic images from human texts is one of the most challenging problems in the field of computer vision (CV). The meaning of descriptions given can be roughly reflected by existing text-to-image approaches. In this paper, our main purpose is to propose a brief comparison between five different methods base on the Generative Adversarial Networks (GAN) to make image from the text. In addition, each model architectures synthesis images with different resolution. Furthermore, the best and worst obtained resolutions is 64\u00d764, 256\u00d7256 respectively. However, we checked and compared some metrics that introduce the accuracy of each model. Also, by doing this study, we found out the best model for this problem by comparing these different approaches essential metrics.\nIndex Terms-Deep learning, Text-To-Image, GAN", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has transformed various fields, including classification [1], noise reduction [2], and even wireless com-communications through deep learning techniques [3]. One of the most challenging tasks in deep learning is generating images from natural language descriptions. In recent years, several architectures have been proposed to tackle this challenge, aiming to generate highly detailed images from human-written captions by designing suitable deep learning models. However, most of these approaches are based on generative adversarial networks (GANs) [4]. The quality of the generated images in GAN-based models depends on the interaction between the generator and discriminator.\nOn the other hand, in text-to-image synthesis, there are lots of problems that can affect the model's performance. In this field, unfortunately, there is no different dataset to generalize this task on every object. Also, the primary datasets are CUB-200-2011 [5], MSCOCO [6], and Oxford-102 flower [7]. In general, generating a real image from a human caption requires some vital features. For illustration, \u201cgenerating an image from a general sentence requires multiple reasonings on various visual concepts, such as object category (people and elephants), spatial configurations of objects (riding), scene context (walking through a river), and etc.\u201d [8].\nAs mentioned, the main core of this task is the GAN model that contains two main networks, the generator, and the dis-criminator. In generator, the network takes a noise vector and converts it to an image. Therefore, the discriminator network takes the output of generator to check that its produced image is real and returns the result of comparison between the fake image of generator and the real image.\nRecently, there are proposed many different meth-ods for text-to-image synthesis. Reed et al. [9] Introduce a"}, {"title": "II. MODEL COMPARISON", "content": "As mentioned in the previous part, our main objective is to propose a comparison between different parts of recent studies in the field of text-to-image synthesis. These similarities con-tain three main parts: first, the architecture of models used for this task. Second, datasets and evaluation metrics, and some other settings. In the last part, we check the contrast of output resolution."}, {"title": "A. Model Architectures", "content": "The most simple and regular model belongs to reed et al. [9] and is named that GAN-CLS. This architecture inspires DCGAN [10] and comprises two main parts: Generator (G) and Discriminator (D) where Figure 2 is a representation of the DCGAN model for generate images. In the generator (G) first makes a noisy vector (z) that consists of random numbers coming from a uniform distribution; z \u2208 R\u00b2 ~ N(0,1) and is sent as input to the network. Our discriminator (D) is just a classifier model that performs some convolutional layer with stride two and batch normalization [14] that followed by Relu as an activation function. Also, the output '0' representation as fake and '1' as an actual image. In general, GAN [9] network is like a minmax game; besides, Eq.1 is carried out GAN mechanism. In such a way, the generator (G) generating fake image try to reduce the output loss to produce more realistic images, and the discriminator (D), as the classifier, is in the role of referee. As well as, to compare the image generated by the generator (G) with the actual image and tries to increase the loss value. Additionally, after one iteration, generator (G) tries to make a better image. However, in GAN-CLS, we need to encode the text, images, and noise vectors. Image generation corresponds to feed-forward inference in the generator (G) conditioned on query text and a noise sample. In this model, our text encoder is Long-Short-Term-memory (LSTM) [15], which extracts features of text and concatenates these with another part for the input generator.\nmin max V (D, G) = Ex~pdata(X)[log D(X)] + ...\nG D\nEx~pz (X) [log(1 - D(G(z)))]\nAuthors of [11] use a baseline method that is a similar model to GAN-CLS but different in its generator network. Their model is constructed based on conditional GAN that conditions images and captions that describe it. The gener-ator of this model consists of an encoder and a decoder. Encoders are employed to encode the main image and caption description. Decoders get the output features of encoders to synthesis images related to text descriptions. In addition, in this architecture, they adopt an MSE loss to train the encoder. The Eq.2 expresses this issue."}, {"title": null, "content": "losss = Tx~Pdata,z~pz ||z \u2013 S(G(z, y(t))))||2"}, {"title": null, "content": "On the other hand, two studies introduced novel models that employ different approaches. First, T. Xu et al. [13] proposed a model that integrates the attention mechanism [16] with DCGAN, called the Attentional Generative Adversarial Network (AttnGAN). AttnGAN has two key components: the attentional generative network and the deep attentional multimodal similarity model (DAMSM). The final objective function of the attentional generative network is defined in Equation (3). The novel approach in this work uses a GAN-based model for text-to-image generation, with multiple gener-ators (Gm1, Gm2, Gm3, ..., Gm\u22121) that produce images using the same loss function (see Eq.3), which includes both the generator loss (LG) and the DAMSM loss (LDAMSM). In this equation, A represents a hyperparameter that balances the two terms in Eq.3. The number of discriminators matches the number of generators.\nThe DAMSM consists of two submodules: a text encoder and an image encoder. The text encoder is a bidirectional LSTM that extracts features from captions, while the image encoder is a convolutional neural network (CNN) built on the inception-v3 model [17] pre-trained on ImageNet [18]. The image features are extracted from the 'mixed-6e' layer of the inception-v3 model. The main goal of AttnGAN is to synthesize high-resolution images using the final generator (Gm-1)."}, {"title": null, "content": "L = LG + ALDAMSM, whereLG = \u2211 LG\ni=0"}, {"title": null, "content": "Second, H. Zhang et al. [19] present an innovation architec-ture called StackGAN that carries out a two-stage model. In contrast of another architectures, this model firstly generates a low-resolution image at the stage-I. In this part focused on the general detail like proper colors and rough figures. For the generator Go, to obtain text conditioning variable co, uses a Gaussian distribution \u039d(\u03bc\u03bf(\u03c6\u03c4), \u03a3\u03bf(\u03c6t)) that \u03bco and Yo came from fed text embedding t into a fully connected layer and co sampled from the Gaussian distribution. In stage-II, built upon the output of stage-I to generate the high-resolution image. Here try to fix the previous defect image to the most realistic image with all details.\nYuan et al. [9] proposed a symmetrical distillation network (SDN) that consists of the main target generator and main source discriminator as 'teacher' and 'student', respectively. The source discriminator in SDN is a common feature extrac-tion like VGG19 [20] model. This model has 16 convolutional layers with kernel size 3\u00d73 and three fully connected layers. In SDN model just used 16 convolutional layers for the discriminator. The generator in SDN has equal structures to the source discriminator. It consists of 3 fully connected layers and 16 convolutional layers again with kernel size 3\u00d73."}, {"title": "B. datasets and assessment metrics:", "content": "The datasets used in mentioned studies are CUB-200-2011 birds [5], Oxford-102 flowers [44], and Microsoft Common Objects in Context (MSCOCO) [3]. The CUB-200-2011 bird dataset contains 11,788 images from 200 different classes of birds; also, there is 10 caption per image that describes the"}, {"title": "III. RESULT AND DISCUSSION", "content": "After comparing the model architectures, we will declare the results of the recent studies. The best performance metric for text-to-image synthesis is the Inception Score (IS) [21]. The inception score is an objective performance metric for generated images from GAN models. It measures how realistic and diverse the output images are and is the second most crucial evaluation performance metric. This metric measures two things: first, Diversity, how diverse the generated images are. The entropy of the overall distribution should be high. Second, Quality how good the generated images are. Low entropy with high predictability is required. Therefore, a higher inception score is always better.\nBy explaining the inception score, we compared this mea-surement for mentioned studies, in the TABLE.1. These results are related to AttnGAN, StackGAN, GAN-CLS, and SDN. By concentrating on the I, due to all models did not use all mentioned datasets, we imply comparing by datasets. In other words, we try to find the highest IS for each dataset in referred models. Obviously, in the MSCOCO dataset, the AttnGAN had"}, {"title": "IV. CONCLUSION", "content": "In this paper, different methods and models based on the Generative Adversarial Network (GAN) are compared for text-to-image synthesis. However, we have examined some necessary evaluation metrics for the best method. Recently used datasets for training the models were CUB-200-2011, Oxford102, and MSCOCO. The output image with high ac-curacy that completely described the input caption is the most important part of each model. By considering the objective of this study, due to the results, the AttnGAN achieved high in-ception score on the most challenging dataset (MSCOCO) and other datasets. Thereafter we can conclude that the introduced model is much more efficient."}]}