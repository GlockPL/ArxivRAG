{"title": "The Foundations of Tokenization: Statistical and Computational Concerns", "authors": ["Juan Luis Gastaldi", "John Terilla", "Luca Malagutti", "Brian DuSell", "Tim Vieira", "Ryan Cotterell"], "abstract": "Tokenization-the practice of converting strings of characters over an alphabet into sequences of tokens over a vocabulary is a critical yet under-theorized step in the NLP pipeline. Notably, it remains the only major step not fully integrated into widely used end-to-end neural models. This paper aims to address this theoretical gap by laying the foundations of tokenization from a formal perspective. By articulating and extending basic properties about the category of stochastic maps, we propose a unified framework for representing and analyzing tokenizer models. This framework allows us to establish general conditions for the use of tokenizers. In particular, we formally establish the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. Additionally, we discuss statistical and computational concerns crucial for the design and implementation of tokenizer models. The framework and results advanced in this paper represent a step toward a robust theoretical foundation for neural language modeling.", "sections": [{"title": "1 Introduction", "content": "The search for the relevant units for the analysis of language has riddled philosophers and linguists alike for millennia. Plato, for instance, judged the \"correctness of a name\" by whether it was composed of \"letters and syllables\" which were themselves names or \u201can inappropriate letter\" was employed (Plato, 1926, 427c-d,432d-e). Even before Plato, P\u0101nini's Ast\u0101dhy\u0101y\u012b already proposed a complete system for the derivational analysis of Sanskrit based on an impressive account of phonological, morphological, and lexical units together with detailed rules for their composition (Sharma, 2002; Kiparsky, 2009). However, it was arguably the Swiss linguist Ferdinand de Saussure who recognized in this problem a fundamental key to modern linguistic thought. Saussure's critical insight was that in linguistics, unlike other empirical sciences, the concrete units of analysis are not immediately given in nature. Beasts, plants, spatial units, or chemical compounds, Saussure argues,\nare supposed to be units given from the outset to zoology, botany, astronomy, or chemistry, which are therefore primarily concerned with their comparison, not with their delimitation (Saussure, 1997, p. 18a). In linguistics, in contrast, the elementary building blocks of language cannot be found by the sole analysis of the physical properties of a language's material support (e.g., sound, ink, electricity, light), and must, therefore, be identified through a segmentation procedure involving various structural features. For Saussure, establishing the relevant units of language not only constitutes \u201cthe most pressing task of linguistics\", but once such a task has been accomplished, little else, if anything, would remain to be done from a linguistic standpoint: \"in doing this [linguistics] will have fulfilled all of its task\" (Saussure, 1997, p. 21a).\nHalfway through the 20th century, many attempts to provide a formal account of linguistic units had been proposed (e.g., Hjelmslev, 1975; Harris, 1960). However, with Chomsky's introduction of the context-free grammar as a formal model of language (Chomsky, 1957), the formalization of linguistics took a turn toward symbolic models of computation. Chomsky's generativist program shifted the focus to the syntactic structure of language, and the problem of delimiting linguistic units, practically absent from Chomsky's earlier work, gradually turned into that of the choice of a parameter left to the linguist. In the current definition of a context-free grammar, the parameter is an alphabet \u2211 of symbols, and a context-free language L is a subset of \u2211* (i.e., the set of all strings over that alphabet) generated by the rewrite system described by the grammar. Within the generativist program, some linguists sought to have \u2211 be the set of morphemes (e.g., Marantz, 2001), railing against the implicit privilege given to words in Chomsky (1965).\nBy the end of the past century, thanks to the resurgence of empiricist approaches in linguistics (e.g., Elman, 1996; Bybee & Hopper, 2001; McEnery & Wilson, 2001; MacWhinney, 1999; Chater et al., 2015), the question of linguistic units has progressively gained a renewed significance. In line with this empiricist perspective, modern natural language processing (NLP) has shifted the focus from searching for a grammar that characterizes a language L\u2286 \u03a3* to obtaining a probability distribution over \u2211* that places high probability on common utterances of a language. This probabilistic perspective makes the modeling task in linguistics particularly sensitive to the choice of the alphabet \u03a3. However, just as in context-free grammars, \u2211 is still a parameter that the modelers must choose themselves.\nIn this new context, the problem of linguistic units has come to be seen through the specific lens of tokenization. Within NLP, tokenization generally refers to the process of breaking up sequences of symbols into subsequences that can be represented as units or \u201ctokens\u201d. The tokenization of linguistic data has long been a common practice in the processing of natural language (cf. Palmer, 2000; Jurafsky & Martin, 2024). However, with the emergence of deep neural models for NLP, the meaning of tokenization has progressively changed. In the framework of current language models, the problem of tokenization arises from the fact that, in practice, starting from an alphabet \u2211 of elementary units, one seeks to estimate a probability distribution over \u2211* indirectly, that is, by estimating a probability distribution over sequences of tokens in A*, where the set of tokens A is, in general, different from \u03a3. Therefore, the problem of tokenization is determined by the forward and backward mappings between \u03a3* and \u0394*.\nThe use of tokenizers for neural language models was popularized with the development and widespread adoption of the Byte Pair Encoding (BPE) algorithm for subword tokenization, adapting an existing compression algorithm (Gage, 1994) to the processing of natural language in the context of neural machine translation (Sennrich et al., 2016). BPE quickly replaced previous rule-based tokenizer models such as Morfessor (Creutz & Lagus, 2002) and Moses (Koehn et al., 2007), and was followed by other data-driven models, including WordPiece (Wu et al., 2016, following Schuster & Nakajima, 2012) and Unigram (Kudo, 2018) among the most widely adopted (cf. Mielke et al., 2021, for a survey). All these recent subword models, now built into standard language modeling toolkits, consist of a set of algorithms to build an alphabet \u2211 and a vocabulary A from existing data, and to map between their respective strings.\nThe significance of subword tokenization for language models has grown. Among their recognized benefits, two are often advanced in the literature. Tokenizers offer the capacity of training language models over an open vocabulary, circumventing the difficulties associated with out-of-vocabulary terms (Sennrich et al., 2016). Also, tokenization is often described as an efficient, lossless encoding of the original data (Zouhar et al., 2023a). Moreover, based on empirical evidence of different kinds, tokenization has been hypothesized to introduce a helpful inductive bias in language model-\""}, {"title": "2 Preliminaries", "content": "2.1 Formal Languages, Estimators, and Stochastic Maps\nAn alphabet \u2211 is a finite, non-empty set of symbols. The set ${\\Sigma ^n}\\stackrel{\\text{def}}{=}\\Sigma \\times \\dots\\times \\Sigma $ consists of strings of symbols of length n. The symbol \u025b denotes the empty string of length 0. The union ${\\Sigma^*}\\stackrel{\\text{def}}{=}\\cup _n \\Sigma ^n $ consists of all finite strings (including \u025b) from the alphabet \u03a3. Similarly, we denote by \u03a3\u2264N the set of all strings from \u2211 of length less or equal to N. String concatenation is an associative product \u03a3* \u00d7 \u03a3* \u2192 \u03a3* for which \u025b is an identity element. The triple (\u03a3*,\u00b7, \u03b5) defines a monoid, which is, in fact, a model of the free monoid on the set \u03a3.\nA language L over an alphabet \u2211 is a set of strings L \u2286 \u03a3*. A language model p is a probability distribution over \u03a3*. That is, p is a function p : \u03a3* \u2192 [0,1] such that \u03a3\u03c3\u03b5\u03c2* p(\u03c3) = 1. Language models generalize languages in the sense that the support of a language model, i.e., supp(p) = {\u03c3 | p(\u03c3) \u2260 0}, is a language. The definition of a language model as a probability distribution on \u2211* is deliberately broad. In particular, note that no compatibility between p and the monoidal structure in 2* is assumed.\nIn NLP, practitioners generally seek to estimate a language model p from exemplars of naturally occurring text. Formally, the modeler assumes there exists a true distribution p* over 2*, and considers a multiset of naturally occurring texts {$\u03c3_n$}$_{n=1}^N$  C 2* to be samples from p*. In its most general form, an estimator of p* is a sequence {$p_n$} of probability distributions on \u03a3* such that $p_n$ becomes closer to p* as n increases. We call an estimator consistent if the sequence {$p_n$} converges pointwise to p*.\nIn other words, given a probability distribution p* : \u03a3* \u2192 [0,1], and a sequence distributions {$p_n$: \u03a3* \u2192 [0,1]}, we say that {$p_n$} is a consistent estimator of p* if and only if, for all strings \u03c3\u2208 \u03a3*, the sequence of numbers {$p_n$(\u03c3)} converges to the number p* (\u03c3).\nThis notion of consistent estimation is general enough to include many estimation methods, where the pi can depend on various properties of the sample, such as the size N, or possibly a set of parameters 0. Likewise, we use pointwise convergence to define consistent estimation because\npointwise convergence is a weak kind of convergence, and so our definition is compatible with a wide variety of convergence measures, and relatively easy to check for. For example, a common practice in NLP is to produce an estimator {$p_n$} through a sequence of steps in the process of minimizing cross entropy loss, which amounts to minimizing the relative entropy also called the Kullback-Leibler divergence, $D_{KL}$($p^* || p_n$) between p* and $p_n$. This is a stronger form of convergence: if $D_{KL}$($p^* || p_n$) \u2192 0 then $p_n$ \u2192 p* pointwise (a consequence of Pinsker's lemma) and so {$p_n$} is a consistent estimator of p*.\nOur definition of tokenizer models will require the use of a special kind of map between sets called a stochastic map. The reference (Baez & Fritz, 2014) contains a detailed introduction to the category of finite sets with stochastic maps between them. Here, we will extend some of the results in Baez & Fritz (2014) to cover the case of countably infinite sets. We assume all sets are countable, either finite or countably infinite. A stochastic map from a set X to a set Y is a function from X to the set of probability distributions on Y. We use\nX \u2192 Y\nto denote a stochastic map from X to Y and the notation x \u2192 f(y | x) to denote the probability of y \u2208 Y in the distribution assigned to x \u2208 X. In other words, a stochastic map f: X \u2192 Y is a function\nX x Y \u2192 [0, 1]\n(x, y) \u2192 f(y|x)\nsatisfying $\\sum_{y \\in Y}f (y | x) = 1$. One might think of the number f (y | x) as a kind of \u201cconditional probability", "that": "n$\\sum_{z \\in Z}gf(z | x) = \\sum_{z \\in Z}\\sum_{y \\in Y} g(z | y)f(y | x) = \\sum_{y \\in Y}\\sum_{z \\in Z}g(z | y)f(y | x) = \\sum_{y \\in Y} f(y | x) = 1$.\nIf one arranges a stochastic map into an |X|\u00d7 |Y| matrix with the f (y | x) entry in the x, y position, then every entry is nonnegative and the sum of every row is 1. The computation above shows that composition of stochastic maps is realized by matrix multiplication, and that even when the matrices are infinite, the row-column dot products are finite and the result of matrix multiplication is again a matrix with nonnegative entries whose rows sum to 1. From this perspective, it is clear that composition of stochastic maps is associative.\nStochastic maps generalize both ordinary probability distributions and functions. A probability distribution over a set X can be represented as a stochastic map into X from a 1-element set, denoted as 1 := {1}, i.e., p: 1 \u2192 X. In such cases, the customary notation p(x) can be used without risk of ambiguity as a shorthand of the more cumbrous p(x | 1). An ordinary function f: X \u2192 Y can be regarded as a stochastic map X \u2192 Y by mapping x to the probability distribution on Y concentrated on the singleton {f(x)}, in which case we say the stochastic map f is deterministic. For simplicity, when a stochastic map f : X \u2192 Y is deterministic, we may write f(x) instead of f(y | x). Composition generalizes both composition of functions and the pushforward of a probability function via a function. If p: 1 \u2192 X is a probability distribution on X and f : X \u2192 Y is a deterministic\nfunction, then the composition 1 X Y is a stochastic map fp : 1 \u2192 Y, which is a probability distribution on Y whose formula is $\\fp(y) = \\sum_{x \\in X}f(y|x)p(x|1) = \\sum_{x \\in f^{-1}(y)} p(x)$. That is, fp is just the pushforward of the probability distribution p via the function f."}, {"title": "2.2 Notation and Terminology", "content": "We adopt the following notational conventions. Alphabets will be denoted by uppercase Greek letters (e.g., \u03a3, \u0394). In the context of tokenization, we will be interested in looking at maps between strings of languages over two different alphabets, which we will denote as \u2211 and \u0394. To avoid ambiguity, we reserve the term alphabet for the former and call vocabulary the latter instead. We denote symbols by lowercase Greek letters, e.g., \u03c3\u2208 \u03a3,\u03b4 \u2208 \u0394, calling them characters in the first case and tokens in the second. Strings will be denoted by bold lowercase Greek letters, e.g., \u03c3\u2208\u03a3*, \u03b4\u03b5 \u0394*, reserving the name character strings or texts for the former and token strings or token sequences for the latter. When necessary, we will distinguish the empty character string \u03b5\u03c2 \u2208 \u03a3* from the empty token sequence \u03b5\u2206 \u2208 \u0394*. Examples of strings and tokens will be written in monospace font (e.g., t, the). There are cases where \u2206\u039f\u03a3* \u2260 0, and it will be necessary to distinguish between concatenation in 2* and *. In A*, concatenation will be denoted as 1. So, for example, if \u2211 = {t,h,e} and \u2206 = {th, he, e}, the expression the denotes the concatenation in 2* of the three characters t, h, and e, while the expression the represents the concatenation in * of the two tokens t and he. The cases when \u0394\u03a9\u03a3* \u2260 0 are of sufficient significance that we shall generally avoid using simple juxtaposition of characters to express concatenation. Therefore, the reader should always interpret th as a token in A, and not a text in 2* (written th). If further notational clarification is needed, square brackets may be used to represent the concatenation of two texts in 2* (and likewise for \u2206*). For instance, [th] e denotes the concatenation of the text th with the character e in \u2211*. Should any ambiguity between specific characters and tokens arise (e.g., \u03c4\u0395\u03a3vs. t\u0395 \u0394), it will be explicitly disambiguated whenever there is a risk that context alone is insufficient."}, {"title": "3 A Formal Framework for Tokenization", "content": "As discussed in the previous pages, in current NLP, the problem of tokenization arises from the fact that one seeks to estimate a model p* over strings of symbols in one alphabet indirectly, that is, by estimating a probability distribution q over strings of symbols on a different alphabet. Therefore, from a strictly formal perspective, the problem of tokenization can be characterized as that of the respective mappings between two sets of strings, namely the set \u2211* of character strings and the set A* of token sequences. In order to estimate p* through q, \u03a3* needs to be mapped into and from A*. The connection between 2* and * is thus made through a pair of mappings (\u03c4, \u03ba) that constitutes the basis of our formal characterization of tokenization. Accordingly, in its most general form, a tokenizer can be defined as follows:\nDefinition 3.1. A tokenizer model (or simply tokenizer) from \u2211* to \u25b3* is a pair of stochastic maps T = (\u0442, \u043a), respectively called the encoder and the decoder, where the encoder is a stochastic map \u03c4: \u03a3* \u2192 \u0394*, and the decoder is a stochastic map \u043a: \u2206* \u2192 \u03a3*.\nDefinition 3.1 is deliberately broad, covering any pair of string-to-string mappings 7 and K. Other than the fact that the domain of each mapping constitutes the codomain of the other, we define the encoder and decoder as arbitrary stochastic maps. In other words, we will be regarding and \u043a primarily from the point of view of their composition. In particular, we do not require any specific connection between the alphabet \u2211 and the vocabulary \u2206. However, the implicit assumption behind the established use of tokenizers in language models is that the samples {$\u03c3_n$}$_{n=1}^N$ C 2* of naturally occurring texts used for estimation can be mapped into \u25b3* in such a way that the estimated model q can be, in turn, transformed into a model p over \u2211* through the map \u043a, such that Kq p can be considered as an estimate of the original distribution p*.\nDespite the potential empirical increase in a model's predictive performance resulting for specific tokenization choices, the soundness of such a procedure is not guaranteed for arbitrary \u0442 and \u043a without further conditions. On one hand, the notion of estimation in A* is not well-defined unless there exists a reference distribution q* over \u2206* to which the estimator {$q_n$} can converge. On the\nother, assuming such an estimator is consistent, transforming it into a consistent estimator of p* requires a way to map the sequence {$q_n$} into a sequence {$p_n$} that converges to p*.\nAssuming a reference distribution p* exists on \u2211*, one obtains a reference q* on \u2206* simply as the composition (Eq. (1)) with the encoder: $q^* = \u0442p^*$. In other words, the following diagram of stochastic maps commutes\n1\n$\\Sigma^*$\n\u0394*\n{an} \n(pn) \nSo far, none of these requirements imposes conditions on 7 and\u043a other than being well-defined mappings between their respective domains and codomains. Notably, the notion of estimation of tp* is well defined for arbitrary \u0442. However, given a consistent estimator {$q_n$} of q*, {$kq_n$} is not guaranteed to converge to p* without further conditions on \u03ba. \u03a4\u03bf establish such conditions, we will need the following lemmas.\nLemma 3.1. Let {$p_n$} be a sequence of probability distributions over a countable set X that converges pointwise to a probability distribution p. Then {$p_n$} \u2192 p uniformly.\nProof. Fatou's lemma applied to X with the counting measure implies that for any sequence of nonnegative functions {$f_n$} on X,\n$\\sum_{x \\in X}\\liminf_{n \\to \\infty}f_n(x) \\le \\liminf_{n \\to \\infty}\\sum_{x \\in X} f_n(x)$.\nWe'll apply this to $f_n$ := $p_n$ + p + $\\left| p_n - p \\right|$. First, note that since $lim_{n\u2192\u221e} p_n(x)$ = p(x), we have $lim inf_{n\u2192\u221e}f_n(x)$ = p(x)+p(x)+0 = 2p(x) so the left hand side of (2) becomes $\\sum_{x \\in X} 2p(x) = 2$.\nTherefore,\n\\le lim $inf_{n\u2192\u221e}\\sum_{x \\in X} f_n(x)$ = lim inf \u2211 px + p + $\\left| p_n(x) - p(x) \\right| = lim inf[ + Epx + E $\\left| p_n(x) - p(x) \\right| = lim inf[1 + 1 + $\\left| p_n(x) - p(x) \\right| = 2 - lim sup Enx \u2014 p(x)I.\nIt follows that limsup$\\left| p_n(x) - p(x) \\right| \u2264 0. So lim $\\left| p_n(x) - p(x) \\right| = 0$. Since the sum of nonnegative numbers is always greater than any particular term in the sum and limEpx \u2014 p(x)] = 0, we can conclude that the sequence {$p_n$} \u2192 p uniformly.\nLemma 3.2. Let f be a stochastic map from X to Y, and {$p_n$} be an estimator for a probability distribution p on X. Then fp, is an estimator for the probability distribution fpo on Y.\nProof. Fix y \u2208 Y. We will show that {f$p_n$(y)} \u2192 fp(y). Notice that the proof of Lemma 3.1 proves that if {$p_n$} \u2192 p pointwise, then lim\u2211xexIpn (x) - p (x)| = 0. Therefore,\nlim[Enx] = Ex [14] =< lim sup Epx \u2014 P().\nIn other words, Lemma 3.2 says that stochastic maps preserve the consistency of estimators. Armed with this lemma, it is now easy to establish a simple but fundamental principle for the use of tokenization models in language modeling:\nTheorem 3.1 (Fundamental Principle of Tokenization). Given a reference probability distribution p* over \u03a3*, a tokenizer T = (\u03c4, \u043a) from \u03a3* to \u25b3*, and a consistent estimator {$q_n$} of the image reference distribution q* = \u0442p*, the sequence {$kq_n$} is a consistent estimator of p* if and only if \u043a\u0442\u0440* = \u0440*.\nProof. By hypothesis, {$q_n$} \u2192 q* and by definition q* = \u0442\u0440*. By Lemma 3.2, applying k to both sides, we have that {$kq_n$} \u2192 \u043aq* and so\n{$\u043a\u0430\u043f$} \u2192 \u043a\u0442\u0440*.\nTherefore, if \u043a\u0442\u0440* = p* we have {$Kqn$} \u2192 p*. Conversely, if {$Kqn$} \u2192 p* we have both {$Kqn$}\np* and {$Kqn$} \u2192 \u043a\u0442\u0440* and so by the uniqueness of limits, \u043a\u0442\u0440* = p*.\nOur whole setting can be represented with the following diagram:\nN\np\n(ap)*\nThis setting is quite general, and Theorem 3.1 characterizes precisely when a consistent estimator {$q_n$} of q* yields a consistent estimator {$p_n$} of p* after decoding. Based on the fundamental principle expressed in Theorem 3.1, we propose the following definitions:\nDefinition 3.2. Given a probability distribution p over \u03a3*, a tokenizer T = (\u0442, \u043a) from \u03a3* to \u25b3* is consistent with respect to p if we have \u043a\u0442\u0440 = \u0440.\nDefinition 3.3. Let p be a probability distribution over 2* and T = (\u0442, \u043a) a tokenizer from \u2211* to \u0414*. When \u043a\u0442 = id\u2211*, we say that T is exact.\nNotice that exact tokenizers are consistent, but a tokenizer that is consistent with respect to a distribution p is not necessarily exact. Take, for instance, a probability distribution p over some set X and x', x\" \u2208 X such that p(x') = p(x\") = c. Then one can fashion a tokenizer for which \u043a\u0442(x) = x for all x except \u043a\u0442(x') = x\" and \u03ba\u03c4(x\") = x'. Such a tokenizer is consistent with respect to p without being exact. Consistency with respect to all distributions, however, is the same as being exact.\nProposition 3.1. A tokenizer T = (\u03c4, \u043a) from \u2211* to \u25b3* is exact if and only if it is consistent with respect to every probability distribution over \u03a3*."}, {"title": "4 Statistical Concerns: Inconsistency and Ambiguity", "content": "While in most concrete cases of neural language modeling, a tokenizer's consistency is implicitly or explicitly assumed, there are many ways in which the conditions established in the previous section can, and in practice do, fail to be satisfied. In this section, we address two main statistical concerns to be considered when implementing or using tokenizers: inconsistency and ambiguity. The following definitions will be convenient:\nDefinition 4.1. Given a tokenizer T = (\u0442, \u043a), we say T is deterministic if t is a deterministic map. Otherwise we say T is stochastic.\nDefinition 4.2. When a tokenizer T is both deterministic and exact, we have that k is a deterministic function, and \u043a = \u03c4\u22121 over \u03c4(\u03a3*). Therefore, in such a case we say T is bijective.\nMost commonly used tokenizers are deterministic, including BPE (Sennrich et al., 2016) and WordPiece (Wu et al., 2016), as well as Unigram (Kudo, 2018) when used without regularization. As we have seen, deterministic functions can be understood as a particular case of stochastic maps where the probability mass is concentrated on one element. Deterministic tokenizers thus constitute a simplified form of tokenization. However, even in this simplified setting, the consistency of the tokenization process is not guaranteed. The following example offers an elementary intuition of this circumstance.\nExample 4.1. Consider the simple configuration represented in Fig. 1, where both \u0442 and \u043a are deterministic maps. Let p*(\u03c3\u2081) = 0.2 and p*(\u03c32) = p*(\u03c33) = 0.4, with p*(\u03c3\u2081) = 0 for i > 3. For q* = \u0442\u0440*, we have, therefore, q* (\u03b4\u2081) = 0.2, q*(82) = 0, and q*(83) = 0.8, with q* (\u03b4\u2081) = 0 for i > 3, and hence \u043a\u0442\u0440*(\u03c3\u2081) = 0 \u2260 0.2, \u03ba\u03c4\u0440*(\u03c32) = 0.2 \u2260 0.4, and \u043a\u0442\u0440*(\u03c3\u2081) = 0.8 \u2260 0.4. Assuming {qn} is a consistent estimator of q*, the pushforward of qn through \u043a (i.\u0435., \u043a\u04af\u043f) would result in an inconsistent estimation of p*. Notice that the consistency of the tokenizer is relative to the distribution. Relative to a different distribution p in \u2211*, where, for instance, p(\u03c3\u2081) = p(\u03c32) = 0 and p = p* otherwise, the tokenizer specified in Fig. 1 is consistent.\nA possible cause of a tokenizer's inconsistency is the lack of injectivity of the encoder function T. While encoder functions may superficially seem to be injective, certain decisions can result in implementations that are not injective. For instance, it can happen that is undefined for some elements in \u2211, and is, therefore, only a partial function. If the exceptions are handled by returning a unique distinguished token in \u2206 (e.g., an \u2018unknown' token unK), then \u315c becomes noninjective, incurring the risk of inconsistency. Most tokenizer models attempt to avoid this behavior by injecting \u03a3 into \u0394-in other words, by including the alphabet in the vocabulary by default. However, some implementations, for instance, limit the size of \u2211 to the sample's k most frequent symbols, mapping all other symbols to an UNK character in \u2211 (e.g., Wu et al., 2016). Understood as a preprocessing step, this operation should not affect T's injectivity. However, some implementations keep track of the original out-of-alphabet symbols to restore them in decoding, thus violating de facto the tokenizer's injectivity, and with it, the model's consistency over strings including those symbols.\nWhenever is noninjective, the tokenizer introduces ambiguity in the model because more than one token sequence is mapped to a unique text. In bijective tokenizers, decoding is injective over the en-coder's image, thus preventing ambiguity in principle. However, in practice, whenever \u315c(\u2211*) \u2282 \u0394*, it may happen that the probability mass placed by the estimated language model outside the image of 7 is nonzero, reintroducing ambiguity into the model (cf. Example 4.2 below for an elementary illustration). This ambiguity is, however, spurious because t was assumed to be deterministic, and hence the ambiguity does not stem from the reference distribution p*, but is a side-effect of the es-timator. An obvious source of spurious ambiguity resides in the fact that consistency is a property defined in the limit. As a consequence, for any \u03b4 \u2208 \u0394*, qn(\u03b4) can and will generally differ from q*(\u03b4). Spurious ambiguity can also result from the fact that, due to the properties of gradient de-scent and certain activation functions such as softmax, neural models are incapable of assigning zero probability to elements of A*. Although spurious ambiguity is usually overlooked or disregarded, it can, in principle, have a nonnegligible effect on estimation (Cao & Rimell, 2021).\nExample 4.2. Take, for instance, a bijective tokenizer such as BPE or WordPiece, with \u043a performing concatenation of the token maps in the usual way. Let \u2211 = {t, h, e} and \u2206 = {t, h, e, th, he}. In this minimal configuration, it is easy to see that k(the) = k(the) = k(the) = the \u2208 \u03a3*. However, BPE or WordPiece being bijective tokenizers, \u0442 can only map the value of k to at most one of the latter's arguments, say (the) = the. We then have that \u0442(\u043a(the)) \u2260 the (and likewise for the). If the estimator happens to place nonzero probability mass on any of the latter two token sequences, the model will exhibit spurious ambiguity.\nSpurious ambiguity is not the only kind of ambiguity that can result from the use of tokenization in language models. Whenever a tokenizer model is stochastic, a deterministic \u043a must be noninjective for the model to be consistent. However, the ambiguity thus introduced is not spurious in that it is deliberately designed for statistical purposes. In current tokenization practices, the main reason for the introduction of stochastic ambiguity is regularization (Kudo, 2018; Provilkov et al., 2020). The claim is that, by exhibiting different token sequences corresponding to the same text during training, a model increases its capability to handle text compositionality as well as its robustness to noise and tokenization errors. However, one could also conceive of a stochastic tokenizer where the possible images of a text reflect the objective probabilities of all linguistic ambiguities potentially affecting it (e.g., an icecream, an_icecream, a nice cream as three possible token sequences for the text: anicecream).\nAlthough all these classes of ambiguity (spurious, stochastic, and linguistic) are both formally and semantically different, they all represent the same risk for the tokenizer's consistency: The probabil-ity mass indirectly assigned by the model to one text in a language is spread out over different token sequences. Notice that all these cases of ambiguity can coexist, and hence their impact is difficult to"}, {"title": "5 Computational Concerns: Computability, Tractability, and Boundedness", "content": "As the end of the previous section shows, even when a tokenizer model is consistent and all statisti-cal concerns are taken into account, there are still computational aspects that can hinder the practice of tokenization. In this section, we turn to issues of computability, tractability, and boundedness.\nDefinitions 3.1 to 3.3 are general enough to allow for all kinds of encoding and decoding functions, including uncomputable ones. Consider the following example:\nExample 5.1. Let \u03a3 = \u0394 {0, 1}, and define Tunc = (Tunc, Kunc) as a deterministic model in the following way:\nSignificantly, Tunc is not only well-defined but also exact and therefore consistent for any language model p over \u03a3*. However, Tunc is famously an uncomputable function, and hence Tunc is an uncomputable tokenizer.\nEven if a tokenizer model is computable, its tractability is important. Indeed, there are many reasons that could make the computation of tokenization intractable. Many of the operations defining tokenizer models involve sums over infinite sets. This is particularly true for the composition of stochastic maps whenever it is performed over an infinite domain, as in our case. Therefore, it is crucial to assess the tractability not only of 7 and K, but also of their composition \u043a\u0442.\nWe have seen that when a tokenizer model is exact, \u05d3 is a section for k, and, therefore, \u03c4(\u03c3) con-centrates the probability mass on only a subset of A*, namely the subset of all preimages of o by \u03ba. This property can help reduce the computational costs by restricting the sum only to that subset. We consider the following property:\nDefinition 5.1. We say a tokenizer model T = (\u0442,\u043a) is multiplicative if its decoder \u043a respects the concatenation products; that is, if \u03ba(\u03b4' \u03b4') = \u03ba(\u03b4')\u00b7 \u03ba(\u03b4\").\nAn obvious consequence of multiplicative tokenizers is that decoding preserves the prefix structure of token sequences. More precisely, let \u03b4' \u03b4 denote the fact that \u03b4 = \u03b4' \u03b4\" for \u03b4, \u03b4', \u03b4\" \u2208 \u0394* (and likewise for \u03c3, \u03c3', \u03c3\" \u2208 \u03a3*). Then we have that \u03b4' < \u03b4 implies \u043a(\u03b4') \u2013 \u043a(\u03b4). This property is crucial in autoregressive models, where each token in a sequence depends only on the tokens preceding it and can then be computed in a left-to-right fashion. In these cases, multiplicativity ensures that the model's output can be decoded on the fly, excluding decoding functions such as string reversal.\nDefinition 5.2. We say the kernel of a multiplicative tokenizer's decoder k is trivial if \u043a maps nonempty token sequences to nonempty token sequences (i.e., if \u03b4 \u2260 \u03b5\u25b3 then \u03ba(\u03b4) \u2260 \u03b5\u03c0).\nThe most commonly used tokenizers, including BPE, WordPiece, and Unigram, are multiplicative.\nNotice that, for a kernel of a multiplicative tokenizer's decoder to be trivial, it is enough that \u03ba(\u03b4) #"}, {"title": "6 Conclusions", "content": "In this work, we have proposed a framework to lay the formal foundations of tokenization in nat-ural language processing. Relying on the basic properties of the category of stochastic maps, we proposed a general definition of a tokenizer as an arbitrary pair of composable maps and proved the necessary and sufficient condition for a tokenizer to preserve the consistency of estimators. Based on this framework, we addressed several statistical and computational concerns crucial for the de-sign and implementation of tokenizers. We believe this framework will contribute to establishing and developing theoretical and practical aspects of neural language modeling on solid grounds."}, {"title": "7Limitations", "content": "Statistical and computational concerns are not the only concerns relevant to a foundational approach to tokenization. In particular, this paper does not address structural concerns, i.e., structural prop-erties of the sets \u03a3* and * such as the monoidal structure, the preservation of structural features through \u0442 and \u043a, or the important question of the choice of \u2206. It also leaves unaddressed theoretical concerns related to interpretability and the possible relation to linguistic segmentation. Although the perspective adopted here is purely formal, and as such, self-contained, the framework proposed could benefit from the insights given by experimental results. These issues will be the object of future work."}]}