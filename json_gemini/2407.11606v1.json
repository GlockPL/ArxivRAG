{"title": "The Foundations of Tokenization: Statistical and Computational Concerns", "authors": ["Juan Luis Gastaldi", "John Terilla", "Luca Malagutti", "Brian DuSell", "Tim Vieira", "Ryan Cotterell"], "abstract": "Tokenization-the practice of converting strings of characters over an alphabet into sequences of tokens over a vocabulary is a critical yet under-theorized step in the NLP pipeline. Notably, it remains the only major step not fully integrated into widely used end-to-end neural models. This paper aims to address this theoretical gap by laying the foundations of tokenization from a formal perspective. By articulating and extending basic properties about the category of stochastic maps, we propose a unified framework for representing and analyzing tokenizer models. This framework allows us to establish general conditions for the use of tokenizers. In particular, we formally establish the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. Additionally, we discuss statistical and computational concerns crucial for the design and implementation of tokenizer models. The framework and results advanced in this paper represent a step toward a robust theoretical foundation for neural language modeling.", "sections": [{"title": "1 Introduction", "content": "The search for the relevant units for the analysis of language has riddled philosophers and linguists alike for millennia. Plato, for instance, judged the \"correctness of a name\" by whether it was composed of \"letters and syllables\" which were themselves names or \u201can inappropriate letter\" was employed (Plato, 1926, 427c-d,432d-e). Even before Plato, P\u0101nini's Ast\u0101dhy\u0101y\u012b already proposed a complete system for the derivational analysis of Sanskrit based on an impressive account of phonological, morphological, and lexical units together with detailed rules for their composition (Sharma, 2002; Kiparsky, 2009). However, it was arguably the Swiss linguist Ferdinand de Saussure who recognized in this problem a fundamental key to modern linguistic thought. Saussure's critical insight was that in linguistics, unlike other empirical sciences, the concrete units of analysis are not immediately given in nature. Beasts, plants, spatial units, or chemical compounds, Saussure argues, are supposed to be units given from the outset to zoology, botany, astronomy, or chemistry, which are therefore primarily concerned with their comparison, not with their delimitation (Saussure, 1997, p. 18a). In linguistics, in contrast, the elementary building blocks of language cannot be found by the sole analysis of the physical properties of a language's material support (e.g., sound, ink, electricity, light), and must, therefore, be identified through a segmentation procedure involving various structural features. For Saussure, establishing the relevant units of language not only constitutes \u201cthe most pressing task of linguistics\", but once such a task has been accomplished, little else, if anything, would remain to be done from a linguistic standpoint: \"in doing this [linguistics] will have fulfilled all of its task\" (Saussure, 1997, p. 21a).\nHalfway through the 20th century, many attempts to provide a formal account of linguistic units had been proposed (e.g., Hjelmslev, 1975; Harris, 1960). However, with Chomsky's introduction of the context-free grammar as a formal model of language (Chomsky, 1957), the formalization of linguistics took a turn toward symbolic models of computation. Chomsky's generativist program shifted the focus to the syntactic structure of language, and the problem of delimiting linguistic units, practically absent from Chomsky's earlier work, gradually turned into that of the choice of a parameter left to the linguist. In the current definition of a context-free grammar, the parameter is an alphabet \u2211 of symbols, and a context-free language L is a subset of \u2211* (i.e., the set of all strings over that alphabet) generated by the rewrite system described by the grammar. Within the generativist program, some linguists sought to have \u2211 be the set of morphemes (e.g., Marantz, 2001), railing against the implicit privilege given to words in Chomsky (1965).\nBy the end of the past century, thanks to the resurgence of empiricist approaches in linguistics (e.g., Elman, 1996; Bybee & Hopper, 2001; McEnery & Wilson, 2001; MacWhinney, 1999; Chater et al., 2015), the question of linguistic units has progressively gained a renewed significance. In line with this empiricist perspective, modern natural language processing (NLP) has shifted the focus from searching for a grammar that characterizes a language L\u2286 \u03a3* to obtaining a probability distribution over \u03a3* that places high probability on common utterances of a language. This probabilistic perspective makes the modeling task in linguistics particularly sensitive to the choice of the alphabet \u03a3. However, just as in context-free grammars, \u2211 is still a parameter that the modelers must choose themselves.\nIn this new context, the problem of linguistic units has come to be seen through the specific lens of tokenization. Within NLP, tokenization generally refers to the process of breaking up sequences of symbols into subsequences that can be represented as units or \u201ctokens\u201d. The tokenization of linguistic data has long been a common practice in the processing of natural language (cf. Palmer, 2000; Jurafsky & Martin, 2024). However, with the emergence of deep neural models for NLP, the meaning of tokenization has progressively changed. In the framework of current language models, the problem of tokenization arises from the fact that, in practice, starting from an alphabet \u2211 of elementary units, one seeks to estimate a probability distribution over \u2211* indirectly, that is, by estimating a probability distribution over sequences of tokens in A*, where the set of tokens A is, in general, different from \u03a3. Therefore, the problem of tokenization is determined by the forward and backward mappings between \u03a3* and \u0394*.\nThe use of tokenizers for neural language models was popularized with the development and widespread adoption of the Byte Pair Encoding (BPE) algorithm for subword tokenization, adapting an existing compression algorithm (Gage, 1994) to the processing of natural language in the context of neural machine translation (Sennrich et al., 2016). BPE quickly replaced previous rule-based tokenizer models such as Morfessor (Creutz & Lagus, 2002) and Moses (Koehn et al., 2007), and was followed by other data-driven models, including WordPiece (Wu et al., 2016, following Schuster & Nakajima, 2012) and Unigram (Kudo, 2018) among the most widely adopted (cf. Mielke et al., 2021, for a survey). All these recent subword models, now built into standard language modeling toolkits, consist of a set of algorithms to build an alphabet \u2211 and a vocabulary A from existing data, and to map between their respective strings.\nThe significance of subword tokenization for language models has grown. Among their recognized benefits, two are often advanced in the literature. Tokenizers offer the capacity of training language models over an open vocabulary, circumventing the difficulties associated with out-of-vocabulary terms (Sennrich et al., 2016). Also, tokenization is often described as an efficient, lossless encoding of the original data (Zouhar et al., 2023a). Moreover, based on empirical evidence of different kinds, tokenization has been hypothesized to introduce a helpful inductive bias in language model-"}, {"title": "2 Preliminaries", "content": "2.1 Formal Languages, Estimators, and Stochastic Maps\nAn alphabet \u2211 is a finite, non-empty set of symbols. The set \u03a3n def \u2211\u00d7\u2026\u00d7\u2211 consists of strings of symbols of length n. The symbol \u03b5 denotes the empty string of length 0. The union \u03a3* def \u22c3n \u03a3n consists of all finite strings (including \u025b) from the alphabet \u03a3. Similarly, we denote by \u03a3\u2264N the set of all strings from \u2211 of length less or equal to N. String concatenation is an associative product \u03a3* \u00d7 \u03a3* \u2192 \u03a3* for which \u03b5 is an identity element. The triple (\u03a3*,\u00b7, \u03b5) defines a monoid, which is, in fact, a model of the free monoid on the set \u03a3.\nA language L over an alphabet \u2211 is a set of strings L \u2286 \u03a3*. A language model p is a probability distribution over \u03a3*. That is, p is a function p : \u03a3* \u2192 [0,1] such that \u2211\u03c3\u03b5\u03c2* p(\u03c3) = 1. Language models generalize languages in the sense that the support of a language model, i.e., supp(p) = {\u03c3 | p(\u03c3) \u2260 0}, is a language. The definition of a language model as a probability distribution on \u2211* is deliberately broad. In particular, note that no compatibility between p and the monoidal structure in 2* is assumed.\nIn NLP, practitioners generally seek to estimate a language model p from exemplars of naturally occurring text. Formally, the modeler assumes there exists a true distribution p* over 2*, and considers a multiset of naturally occurring texts {\u03c3n}=1 \u2286 \u03a3* to be samples from p*. In its most general form, an estimator of p* is a sequence {pn} of probability distributions on \u03a3* such that pn becomes closer to p* as n increases. We call an estimator consistent if the sequence {pn} converges pointwise to p*.\nMore precisely, given a probability distribution p* : \u03a3* \u2192 [0,1], and a sequence distributions {pn: \u03a3* \u2192 [0,1]}, we say that {pn} is a consistent estimator of p* if and only if, for all strings \u03c3\u2208 \u03a3*, the sequence of numbers {pn(\u03c3)} converges to the number p* (\u03c3).\nThis notion of consistent estimation is general enough to include many estimation methods, where the pi can depend on various properties of the sample, such as the size N, or possibly a set of parameters \u03b8. Likewise, we use pointwise convergence to define consistent estimation because pointwise convergence is a weak kind of convergence, and so our definition is compatible with a wide variety of convergence measures, and relatively easy to check for. For example, a common practice in NLP is to produce an estimator {pn} through a sequence of steps in the process of minimizing cross entropy loss, which amounts to minimizing the relative entropy also called the Kullback-Leibler divergence, DKL(p* || Pn) between p* and pn. This is a stronger form of convergence: if DKL(p* || Pn) \u2192 0 then pn \u2192 p* pointwise (a consequence of Pinsker's lemma) and so {pn} is a consistent estimator of p*.\nOur definition of tokenizer models will require the use of a special kind of map between sets called a stochastic map. The reference (Baez & Fritz, 2014) contains a detailed introduction to the category of finite sets with stochastic maps between them. Here, we will extend some of the results in Baez & Fritz (2014) to cover the case of countably infinite sets. We assume all sets are countable, either finite or countably infinite. A stochastic map from a set X to a set Y is a function from X to the set of probability distributions on Y. We use\nXY\nto denote a stochastic map from X to Y and the notation x \u2192 f(y | x) to denote the probability of y \u2208 Y in the distribution assigned to x \u2208 X. In other words, a stochastic map f: X \u2192 Y is a function\nX x Y \u2192 [0, 1]\n(x, y) \u2192 f(y|x)\nsatisfying \u2211yer f (y | x) = 1. One might think of the number f (y | x) as a kind of \u201cconditional probability\" of y given x, however this is only a metaphor since there is no assumption that the numbers f(y | x) assemble into a joint distribution on X \u00d7 Y. In particular, the sum \u03a3x\u2208x f(y |x) could be infinite.\nSignificantly, stochastic maps can be composed. The composition\ngf: XZ is defined by\nXY Z\ng\nf\ngf\ngf(z | x) = \u2211 g(z | y)f(y | x). (1)\nYEY\nSince the sum in equation (1) is infinite, it requires a check that the formula for gf (z | x) is finite, and that gf (-, x) defines a probability distribution on Z, both of which follow from the fact that:\n\u22119f(z | x) = \u2211\u2211 g(z | y)f(y | x) = \u2211\u2211g(z | y)f(y | x) = \u2211 f(y | x) = 1.\nZEZ ZEZ YEY YEY ZEZ YEY\nIf one arranges a stochastic map into an |X|\u00d7 |Y| matrix with the f (y | x) entry in the x, y position, then every entry is nonnegative and the sum of every row is 1. The computation above shows that composition of stochastic maps is realized by matrix multiplication, and that even when the matrices are infinite, the row-column dot products are finite and the result of matrix multiplication is again a matrix with nonnegative entries whose rows sum to 1. From this perspective, it is clear that composition of stochastic maps is associative.\nStochastic maps generalize both ordinary probability distributions and functions. A probability distribution over a set X can be represented as a stochastic map into X from a 1-element set, denoted as 1 := {1}, i.e., p: 1 \u2192 X. In such cases, the customary notation p(x) can be used without risk of ambiguity as a shorthand of the more cumbrous p(x | 1). An ordinary function f: X \u2192 Y can be regarded as a stochastic map X \u2192 Y by mapping x to the probability distribution on Y concentrated on the singleton {f(x)}, in which case we say the stochastic map f is deterministic. For simplicity, when a stochastic map f : X \u2192 Y is deterministic, we may write f(x) instead of f(y | x). Composition generalizes both composition of functions and the pushforward of a probability function via a function. If p: 1 \u2192 X is a probability distribution on X and f : X \u2192 Y is a deterministic function, then the composition 1 \u2192 X \u2192 Y is a stochastic map fp : 1 \u2192 Y, which is a probability distribution on Y whose formula is fp(y) = \u2211xex f(y|x)p(x|1) = \u2211x\u2208f-1(y) P(x). That is, fp is just the pushforward of the probability distribution p via the function f.\""}, {"title": "2.2 Notation and Terminology", "content": "We adopt the following notational conventions. Alphabets will be denoted by uppercase Greek letters (e.g., \u03a3, \u0394). In the context of tokenization, we will be interested in looking at maps between strings of languages over two different alphabets, which we will denote as \u2211 and \u0394. To avoid ambiguity, we reserve the term alphabet for the former and call vocabulary the latter instead. We denote symbols by lowercase Greek letters, e.g., \u03c3\u2208 \u03a3,\u03b4 \u2208 \u0394, calling them characters in the first case and tokens in the second. Strings will be denoted by bold lowercase Greek letters, e.g., \u03c3\u2208\u03a3*, \u03b4\u03b5 \u0394*, reserving the name character strings or texts for the former and token strings or token sequences for the latter. When necessary, we will distinguish the empty character string \u03b5\u03a3 \u2208 \u03a3* from the empty token sequence \u03b5\u2206 \u2208 \u0394*. Examples of strings and tokens will be written in monospace font (e.g., t, the). There are cases where \u2206\u039f\u03a3* \u2260 0, and it will be necessary to distinguish between concatenation in 2* and *. In A*, concatenation will be denoted as 1. So, for example, if \u2211 = {t,h,e} and \u2206 = {th, he, e}, the expression the denotes the concatenation in 2* of the three characters t, h, and e, while the expression the represents the concatenation in * of the two tokens t and he. The cases when \u0394\u03a9\u03a3* \u2260 0 are of sufficient significance that we shall generally avoid using simple juxtaposition of characters to express concatenation. Therefore, the reader should always interpret th as a token in A, and not a text in 2* (written th). If further notational clarification is needed, square brackets may be used to represent the concatenation of two texts in 2* (and likewise for \u2206*). For instance, [th] e denotes the concatenation of the text th with the character e in \u2211*. Should any ambiguity between specific characters and tokens arise (e.g., \u03c4\u0395\u03a3vs. t\u0395 \u0394), it will be explicitly disambiguated whenever there is a risk that context alone is insufficient."}, {"title": "3 A Formal Framework for Tokenization", "content": "As discussed in the previous pages, in current NLP, the problem of tokenization arises from the fact that one seeks to estimate a model p* over strings of symbols in one alphabet indirectly, that is, by estimating a probability distribution q over strings of symbols on a different alphabet. Therefore, from a strictly formal perspective, the problem of tokenization can be characterized as that of the respective mappings between two sets of strings, namely the set \u2211* of character strings and the set A* of token sequences. In order to estimate p* through q, \u03a3* needs to be mapped into and from A*. The connection between 2* and * is thus made through a pair of mappings (\u03c4, \u03ba) that constitutes the basis of our formal characterization of tokenization. Accordingly, in its most general form, a tokenizer can be defined as follows:\nDefinition 3.1. A tokenizer model (or simply tokenizer) from \u2211* to \u25b3* is a pair of stochastic maps T = (\u0442, \u043a), respectively called the encoder and the decoder, where the encoder is a stochastic map \u03c4: \u03a3* \u2192 \u0394*, and the decoder is a stochastic map \u043a: \u2206* \u2192 \u03a3*.\nDefinition 3.1 is deliberately broad, covering any pair of string-to-string mappings 7 and K. Other than the fact that the domain of each mapping constitutes the codomain of the other, we define the encoder and decoder as arbitrary stochastic maps. In other words, we will be regarding and \u043a primarily from the point of view of their composition. In particular, we do not require any specific connection between the alphabet \u2211 and the vocabulary \u2206. However, the implicit assumption behind the established use of tokenizers in language models is that the samples {\u03c3n}=1 \u2286 \u03a3* of naturally occurring texts used for estimation can be mapped into \u25b3* in such a way that the estimated model q can be, in turn, transformed into a model p over \u2211* through the map \u043a, such that Kq p can be considered as an estimate of the original distribution p*.\nN\nDespite the potential empirical increase in a model's predictive performance resulting for specific tokenization choices, the soundness of such a procedure is not guaranteed for arbitrary \u0442 and \u043a without further conditions. On one hand, the notion of estimation in A* is not well-defined unless there exists a reference distribution q* over \u2206* to which the estimator {qn} can converge. On the other, assuming such an estimator is consistent, transforming it into a consistent estimator of p* requires a way to map the sequence {qn} into a sequence {pn} that converges to p*.\nAssuming a reference distribution p* exists on \u2211*, one obtains a reference q* on \u2206* simply as the composition (Eq. (1)) with the encoder: q* = rp*. In other words, the following diagram of stochastic maps commutes\n\u03a3* *\\ T\nThe distribution q* is just the pushforward of the measure p* along 7, which then makes the encoder \u0442\u0430 measure-preserving map between (\u03a3*, p*) and (A*, q*)\nIn the same way, {pn} can be obtained by mapping the sequence {qn} through k. By defining Pi = kqi, we obtain the following commutative diagram\n\u03a3* {9n}\n{Pn}\nSo far, none of these requirements imposes conditions on 7 and\u043a other than being well-defined mappings between their respective domains and codomains. Notably, the notion of estimation of tp* is well defined for arbitrary \u0442. However, given a consistent estimator {qn} of q*, {kqn} is not guaranteed to converge to p* without further conditions on \u03ba. \u03a4\u03bf establish such conditions, we will need the following lemmas.\nLemma 3.1. Let {pn} be a sequence of probability distributions over a countable set X that converges pointwise to a probability distribution p. Then {pn} \u2192 p uniformly.\nProof. Fatou's lemma applied to X with the counting measure implies that for any sequence of nonnegative functions {fn} on X,\n\u2211lim inf fn(x) < lim inf\u2211 fn(x). (2)\nxEX n\u2192\u221e n\u2192\u221e xEX\nWe'll apply this to fn := pn + p + |pn - p|. First, note that since limn\u2192\u221e Pn(x) = p(x), we have lim infn\u2192\u221e fn(x) = p(x)+p(x)+0 = 2p(x) so the left hand side of (2) becomes \u2211x\u2208x 2p(x) = 2. Therefore,\n2 < lim inf\u2211 fn(x)\nn\u2192\u221e\nxEX\n= lim inf \u2211 pn (x) + p(x) + |pn(x) - p(x)|\nn\u2192\u221e xEX\n= lim inf \u2211 pn(x) + \u2211p(x) + \u2211 |Pn(x) \u2013 p(x)|\nn\u2192\u221e xEX xEX xEX\n= lim inf 1 + 1 + \u2211 |pn(x) \u2013 p(x)|\nn\u2192\u221e xEX\n= 2 - lim sup \u2211 pn(x) \u2013 p(x)|.\nn\u2192\u221e xEX\nIt follows that limsupn\u2192\u221e \u2211xex |Pn(x) - p(x)| \u2264 0. So limn\u2192\u221e \u03a3xex |Pn(x) \u2013 p(x)| = 0. Since the sum of nonnegative numbers is always greater than any particular term in the sum and limn\u2192\u221e Exex |Pn(x) \u2013 p(x)| = 0, we can conclude that the sequence {pn} \u2192 puniformly. \u2610\nLemma 3.2. Let f be a stochastic map from X to Y, and {pn} be an estimator for a probability distribution p on X. Then fpn is an estimator for the probability distribution fpon Y."}, {"title": "4 Statistical Concerns: Inconsistency and Ambiguity", "content": "While in most concrete cases of neural language modeling, a tokenizer's consistency is implicitly or explicitly assumed, there are many ways in which the conditions established in the previous section can, and in practice do, fail to be satisfied. In this section, we address two main statistical concerns to be considered when implementing or using tokenizers: inconsistency and ambiguity. The following definitions will be convenient:\nDefinition 4.1. Given a tokenizer T = (\u0442, \u043a), we say T is deterministic if t is a deterministic map. Otherwise we say T is stochastic.\nDefinition 4.2. When a tokenizer T is both deterministic and exact, we have that k is a deterministic function, and \u043a = \u03c4\u22121 over \u03c4(\u03a3*). Therefore, in such a case we say T is bijective.\nMost commonly used tokenizers are deterministic, including BPE (Sennrich et al., 2016) and WordPiece (Wu et al., 2016), as well as Unigram (Kudo, 2018) when used without regularization. As we have seen, deterministic functions can be understood as a particular case of stochastic maps where the probability mass is concentrated on one element. Deterministic tokenizers thus constitute a simplified form of tokenization. However, even in this simplified setting, the consistency of the tokenization process is not guaranteed. The following example offers an elementary intuition of this circumstance.\nExample 4.1. Consider the simple configuration represented in Fig. 1, where both \u0442 and \u043a are deterministic maps. Let p*(\u03c3\u2081) = 0.2 and p*(\u03c32) = p*(\u03c33) = 0.4, with p*(\u03c3\u2081) = 0 for i > 3. For q* = \u0442\u0440*, we have, therefore, q* (\u03b4\u2081) = 0.2, q*(82) = 0, and q*(83) = 0.8, with q* (\u03b4\u2081) = 0 for i > 3, and hence \u043a\u0442\u0440*(\u03c3\u2081) = 0 \u2260 0.2, \u03ba\u03c4\u0440*(\u03c32) = 0.2 \u2260 0.4, and \u043a\u0442\u0440*(\u03c3\u2081) = 0.8 \u2260 0.4. Assuming {qn} is a consistent estimator of q*, the pushforward of qn through \u043a (i.\u0435., \u043a\u04af\u043f) would result in an inconsistent estimation of p*. Notice that the consistency of the tokenizer is relative to the distribution. Relative to a different distribution p in \u2211*, where, for instance, p(\u03c3\u2081) = p(\u03c32) = 0 and p = p* otherwise, the tokenizer specified in Fig. 1 is consistent.\nA possible cause of a tokenizer's inconsistency is the lack of injectivity of the encoder function T. While encoder functions may superficially seem to be injective, certain decisions can result in implementations that are not injective. For instance, it can happen that is undefined for some elements in \u2211, and is, therefore, only a partial function. If the exceptions are handled by returning a unique distinguished token in \u2206 (e.g., an \u2018unknown' token unK), then \u315c becomes noninjective, incurring the risk of inconsistency. Most tokenizer models attempt to avoid this behavior by injecting \u03a3 into \u0394-in other words, by including the alphabet in the vocabulary by default. However, some implementations, for instance, limit the size of \u2211 to the sample's k most frequent symbols, mapping all other symbols to an UNK character in \u2211 (e.g., Wu et al., 2016). Understood as a preprocessing step, this operation should not affect T's injectivity. However, some implementations keep track of the original out-of-alphabet symbols to restore them in decoding, thus violating de facto the tokenizer's injectivity, and with it, the model's consistency over strings including those symbols."}, {"title": "5 Computational Concerns: Computability, Tractability, and Boundedness", "content": "As the end of the previous section shows, even when a tokenizer model is consistent and all statistical concerns are taken into account, there are still computational aspects that can hinder the practice of tokenization. In this section, we turn to issues of computability, tractability, and boundedness.\nDefinitions 3.1 to 3.3 are general enough to allow for all kinds of encoding and decoding functions, including uncomputable ones. Consider the following example:\nExample 5.1. Let \u03a3 = \u0394 {0, 1}, and define Tunc = (Tunc, Kunc) as a deterministic model in the following way:\nTunc(\u03c3) = \u03c311, if o describes a valid\nTuring Machine followed by\nan input for which it halts.\n\u03c3\u03b9\u03bf, otherwise.\n\u041a\u0438\u043f\u0441(6) =\n\u03b5, if \u03b4\u03b5 \u0394.\n\u03c3, otherwise, where \u03b4 = \u03c3\u03b9\u03b4.\nSignificantly, Tunc is not only well-defined but also exact and therefore consistent for any language model p over \u03a3*. However, Tunc is famously an uncomputable function, and hence Tunc is an uncomputable tokenizer.\nEven if a tokenizer model is computable, its tractability is important. Indeed, there are many reasons that could make the computation of tokenization intractable. Many of the operations defining tokenizer models involve sums over infinite sets. This is particularly true for the composition of stochastic maps whenever it is performed over an infinite domain, as in our case. Therefore, it is crucial to assess the tractability not only of 7 and K, but also of their composition \u043a\u0442.\nWe have seen that when a tokenizer model is exact, \u05d3 is a section for k, and, therefore, \u03c4(\u03c3) concentrates the probability mass on only a subset of A*, namely the subset of all preimages of o by \u03ba. This property can help reduce the computational costs by restricting the sum only to that subset. We consider the following property:\nDefinition 5.1. We say a tokenizer model T = (\u0442,\u043a) is multiplicative if its decoder \u043a respects the concatenation products; that is, if \u03ba(\u03b4' \u03b4') = \u03ba(\u03b4')\u00b7 \u03ba(\u03b4", "\u03b4": "or \u03b4, \u03b4', \u03b4' \u2208 \u0394* (and likewise for \u03c3, \u03c3', \u03c3", "definitions": "nDefinition 5.3. Given two sets A, B, and a map f: A* \u2192 B* we say f is of finite type if, there exists n \u2208 N such that, for every a \u2208 A*, there exists a nonempty a' \u2208 A\u2264n such that a = a' \u00b7 a", "f(a": ".", "maximal munch\" approach (Reps, 1998; Palmer, 2000) adopted by WordPiece, for instance, iteratively maps the successive longest prefixes of a text to tokens in the vocabulary. WordPiece's encoder is thus bounded by the maximum length of the preimages of A through 7, and can therefore be implemented as a finite-state transducer (Song et al., 2021). Notice, however, that, in general, if a tokenizer is bounded in the sense of Definitions 5.3 and 5.4, it does not mean a tractable algo-rithm exists to find the decomposition a = a' \u00b7 a\", which could, in principle, depend on the entire a. Significantly, Berglund & van der Merwe (2023) showed that, under specific conditions on the structure of the list of rules or \u201cmerges": "BPE is also bounded and an algorithm can be found to compute \u03c4(\u03c3')\u03b9\u03c4(\u03c3"}, {"title": "6 Conclusions", "content": "In this work, we have proposed a framework to lay the formal foundations of tokenization in natural language processing. Relying on the basic properties of the category of stochastic maps, we proposed a general definition of a tokenizer as an arbitrary pair of composable maps and proved the necessary and sufficient condition for a tokenizer to preserve the consistency of estimators. Based on this framework, we addressed several statistical and computational concerns crucial for the design and implementation of tokenizers. We believe this framework will contribute to establishing and developing theoretical and practical aspects of neural language modeling on solid grounds."}, {"title": "7 Limitations", "content": "Statistical and computational concerns are not the only concerns relevant to a foundational approach to tokenization. In particular, this paper does not address structural concerns, i.e., structural properties of the sets \u03a3* and * such as the monoidal structure, the preservation of structural features through \u0442 and \u043a, or the important question of the choice of \u2206. It also leaves unaddressed theoretical concerns related to interpretability and the possible relation to linguistic segmentation. Although the perspective adopted here is purely formal, and as such, self-contained, the framework proposed could benefit from the insights given by experimental results. These issues will be the object of future work."}]}