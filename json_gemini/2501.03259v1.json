{"title": "Toward Inclusive Educational AI: Auditing Frontier LLMs through a Multiplexity Lens", "authors": ["Abdullah Mushtaq", "Muhammad Rafay Naeem", "Muhammad Imran Taj", "Ibrahim Ghaznavi", "Junaid Qadir"], "abstract": "As large language models (LLMs) like GPT-4 and Llama 3 become integral to educational contexts, concerns are mounting over the cultural biases, power imbalances, and ethical limitations embedded within these technologies. Though generative AI tools aim to enhance learning experiences, they often reflect values rooted in Western, Educated, Industrialized, Rich, and Democratic (WEIRD) cultural paradigms, potentially sidelining diverse global perspectives. This paper proposes a framework to assess and mitigate cultural bias within LLMs through the lens of applied multiplexity. Multiplexity, inspired by Senturk et al. and rooted in Islamic and other wisdom traditions, emphasizes the coexistence of diverse cultural viewpoints, supporting a multi-layered epistemology that integrates both empirical sciences and normative values. Our analysis reveals that LLMs frequently exhibit cultural polarization, with biases appearing in both overt responses and subtle contextual cues. To address inherent biases and incorporate multiplexity in LLMs, we propose two strategies: Contextually-Implemented Multiplex LLMs, which embed multiplex principles directly into the system prompt, influencing LLM outputs at a foundational level and independent of individual prompts, and Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents, each representing distinct cultural viewpoints, collaboratively generate a balanced, synthesized response. Our findings demonstrate that as mitigation strategies evolve from contextual prompting to MAS-implementation, cultural inclusivity markedly improves, evidenced by a significant rise in the Perspectives Distribution Score (PDS) and a PDS Entropy increase from 3.25% at baseline to 98% with the MAS-Implemented Multiplex LLMs. Sentiment analysis further shows a shift towards positive sentiment across cultures, with the MAS-Implemented Multiplex LLMs achieving 0% negative sentiment, underscoring enhanced cultural sensitivity. This pioneering study establishes a baseline for assessing and fostering cultural inclusivity in educational AI, laying the groundwork for a globally pluralistic approach that respects diverse cultural perspectives. We hope this work inspires further research toward creating AI technologies that serve a truly inclusive and multicultural educational ecosystem.", "sections": [{"title": "I. INTRODUCTION", "content": "THE rapid advancement of Large Language Models (LLMs) has significantly impacted AI development, par- ticularly as these models establish themselves as foundation models-large-scale, pre-trained systems that can be adapted for a broad array of downstream tasks [1]. Prominent ex- amples, including GPT-40, Claude Sonnet 3.5, Gemini 1.5"}, {"title": "A. How LLM-based Educational AI can Embody Biases?", "content": "The foundational nature of LLMs, while beneficial for scal- ability across tasks, also enables the propagation of inherent biases, impacting various applications and raising particular concerns in educational settings. As these models become integral to shaping learning environments globally, especially through educational platforms like KhanAmigo and Harvard's CS50 AI tutor [9], [10], it becomes crucial to evaluate not only their technical performance but also their alignment with ethical and cultural values. Many LLMs are trained on predominantly secular, materialistic, and reductionist datasets and benchmarks [11], which can inadvertently lead them to reflect biases that marginalize or overlook diverse cultural, ethical, and spiritual perspectives, especially those outside Western paradigms. These biases, far from being merely theo- retical, have been shown to manifest in real-world applications, perpetuating stereotypes or reflecting negative biases toward specific communities [12]\u2014an outcome that limits the scope of cultural and ethical diversity presented to young learners. In global educational contexts, these biases create additional ethical and social challenges. When foundation models are applied across culturally diverse settings without sufficient adaptation, they risk misalignment with local values and worldviews, potentially alienating or misrepresenting certain communities. Recent studies have documented unintended biases in LLMs, including those related to ethnicity, cultural background, and other demographic factors [13]\u2013[15]. These findings highlight the limitations of current LLMs in represent- ing the pluralism inherent in human societies, underscoring the need for a framework that actively addresses these biases to support a genuinely inclusive educational AI that respects the diversity of global learners."}, {"title": "B. Using Multiplexity Framework to Analyze LLMs", "content": "In this paper, we propose auditing foundation LLMs through the lens of Multiplexity, a framework proposed by Senturk et al. [16] rooted in Islamic epistemology that encompasses a pluralistic approach to knowledge by incorporating spiritual, ethical, and metaphysical dimensions [17]. The multiplexity approach serves as a corrective to the uniplex mindset, which historically emphasizes empirical and secular knowledge, of- ten marginalizing other ways of knowing [18]. As noted by the authors in their work [17] and articulated by Senturk [16], multiplexity moves beyond a single-dimensional view of knowledge by embracing multiple layers of understanding, each valued for its unique perspective and contribution. Table I contrasts the uniplex and multiplex worldviews, highlighting key differences in knowledge, ethics, and cultural diversity, with the multiplex approach offering a more inclusive frame- work for culturally aligned AI [16], [17]. This study builds on the multiplexity framework to establish a robust evaluation system for assessing foundation LLMs in terms of cultural and ethical alignment across a wide range of intellectual traditions, including Western, Eastern, Islamic, African, and Latin American perspectives. Our framework systematically scans for indications of monoculture, Euro- centrism, and uniplexity\u2014biases that often prioritize singular, predominantly Western viewpoints thus addressing potential imbalances in LLM-generated content. By employing a Multi- Agent System (MAS), where agents specializing in distinct cultural viewpoints provide insights, our framework enables a balanced synthesis of perspectives through a central Multi- plex Agent that integrates the principles of multiplexity. This approach, illustrated in Figure 1, enables LLMs to generate content that is pluralistic and inclusive. Such a framework is particularly valuable in educational settings, where ensuring cultural resonance and ethical alignment is essential for fos- tering inclusive and diverse learning experiences."}, {"title": "C. Contributions of this Paper", "content": "While previous studies have examined biases in LLMs, this paper focuses specifically on educational settings, where ethi- cal, cultural, and local values are essential for holistic learner development. Our contribution is a culturally aligned educa- tional AI framework, evaluating LLMs through a multiplexity- inspired approach. By assessing cultural alignment, we ensure these models respect diverse identities, supporting their suit- ability for global educational applications. To our knowledge, this is the first study to use a multiplexity framework with LLMs for a pluralistic, education-focused AI approach. This work lays the groundwork for culturally sensitive tools that foster inclusive learning across varied backgrounds."}, {"title": "D. Organization of this Paper", "content": "The rest of the paper is organized as follows. Section II reviews relevant literature, covering the development of LLMs, ethical AI frameworks, and existing approaches to cultural bias in AI. Section III presents our methodology, detailing the multiplexity-inspired framework for assessing cultural bias in LLM-generated content. Section IV discusses the results of applying this framework to various LLMs, and finally, Section V concludes the paper, highlighting key insights and directions for future research."}, {"title": "II. LITERATURE REVIEW", "content": "The advances in language models through LLMs have sig- nificantly reshaped natural language processing (NLP) through unsupervised training on massive corpora. Built initially on simpler models, these architectures have advanced from re- current neural networks to transformer-based architectures, enabling LLMs to generalize beyond task-specific fine-tuning and perform effectively on diverse linguistic tasks. With this advancement, however, a critical need has emerged to scruti- nize biases and ethical considerations."}, {"title": "A. Bias and Ethics in LLMs", "content": "Bias in language models is a significant and well- documented concern, affecting diverse fields such as politics, healthcare, religion, and education. Researchers categorize the associated harms into two primary types: Allocation Harm, where an Al system results in an unequal distribution of resources or opportunities, and Representation Harm, where a model reinforces stereotypes that impact societal perceptions of certain groups. As analyzed in [13], the origins of bias in LLMs can be traced to the pretraining phase, user interactions, and the broader context of the model's deployment. A notable example of allocation harm occurs in hiring algorithms that inadvertently favor certain demographic groups over others, leading to unequal job opportunities. For instance, recruitment models trained on biased data may underrepresent qualified candidates from marginalized backgrounds, thereby perpetuating systemic inequalities. Meanwhile, an example of representation harm is documented with OpenAI's GPT- 3, which, in analogy-based completions involving Muslims, generated violent associations at a disproportionately high rate (66%) compared to other religious terms, as noted in [11]. These examples illustrate that, despite ongoing mitigation efforts, achieving complete neutrality is a persistent challenge, underscoring the need for continuous ethical review and a"}, {"title": "B. LLMs in Education", "content": "The integration of AI in education, particularly through LLMs [9], [10], has led to frameworks emphasizing ethi- cal alignment and cultural sensitivity. One such framework, Multiplex AI Humanities (MAIH) [18], critiques the uniplex worldview-rooted in reductionist, secular, and empirically- driven Enlightenment ideals\u2014that often universalizes Euro- centric perspectives at the expense of global diversity. In contrast, MAIH offers a pluralistic, open-civilizational framework that values both empirical and metaphysical dimen- sions, incorporating diverse cultural, intellectual, and religious perspectives. It is built on two pillars [16]: Multiplex Ontology, which recognizes physical, metaphysical, and divine dimen- sions; and Multiplex Epistemology, which includes diverse forms of knowledge, such as reason, intuition, and revelation. By respecting the multi-layered connections between technol- ogy, knowledge, and the human condition, multiplexity fosters"}, {"title": "III. METHODOLOGY", "content": "In this study, we introduce a novel multiplexity-based AI evaluation framework designed to assess the cultural alignment of LLMs across perspectives such as Western, Eastern, Islamic, Latin American, etc. Concretely, the framework comprises three distinct strategies (1) baseline assessment; and two kinds"}, {"title": "A. Research Questions", "content": "This study employs \"applied multiplexity\" to create an evaluation system that assesses foundation LLMs for cultural and ethical alignment across diverse intellectual traditions, including Western, Eastern, Islamic, African, and Latin Ameri- can perspectives. By implementing a MAS with agents special- izing in distinct cultural perspectives, the framework synthe- sizes insights through a central Multiplex Agent, harmonizing outputs into balanced, inclusive responses. Initially, baseline cultural bias is evaluated via perspective coverage (Fig. 2), followed by applying multiplexity-informed contextual rules to ensure balanced representation. This approach enables LLMS to generate culturally resonant and ethically aligned content, which is particularly valuable in educational contexts where inclusive learning is essential. The resulting outputs support holistic educational experiences, addressing the diverse needs of learners and educators globally.\nSpecifically, we address three research questions (RQs):\nRQ1: What forms and levels of cultural bias are present in LLMs by default, and how can these biases be effectively measured as a baseline?\nRQ2: How can the Multiplexity Framework, along with a Multiplexity-inspired Multi-Agent System, be utilized to create a pluralistic approach in educational AI?\nRQ3: How can the Multiplexity Framework be applied to guide LLMs in fostering positive and respectful representations of diverse cultural perspectives?"}, {"title": "B. Cultural Analysis and Sentiment Assessment Strategies", "content": "1) Strategy 1: Baseline Assessment: The first strategy as- sesses the baseline performance of various LLMs concerning cultural bias. This strategy evaluates the coverage of different cultures in the responses of these LLMs out of the box. The PDS is utilized to measure how well each perspective is repre- sented in the models' responses, allowing for a comprehensive understanding of the cultural biases present. No additional information is provided to the models except for the questions. The output of the model is also used to extract the sentiment of each LLM towards these perspectives for bias framing (see \u00a7III-C1 and Fig. 2 for more details).\n2) Strategy 2: Contextually Implemented Multiplexity: The second strategy integrates the ideas of multiplexity to provide contextual guidelines that promote balanced outputs across cultures. The core concepts of multiplexity and its rules are provided in the system prompt (context) of the models to follow for their responses. These responses are passed to the PDS to evaluate the diversity and representation of perspectives, ensuring that the outputs generated adhere to the principles of cultural inclusivity along with the sentiment of the model for bias framing and comparison with baseline assessment (More details in \u00a7III-C2 and Fig. 2).\n3) Strategy 3: MAS-Implemented Multiplexity: The final strategy involves implementing a multi-agent pipeline, where agents specializing in distinct cultural perspectives give re- sponses that are then synthesized by a Multiplex Agent ac- cording to the multiplexity principles. Each agent is asked to answer the questions with respect to its culture and these responses from agents are passed to a multiplex agent to apply the multiplexity principles ensuring a holistic viewpoint of producing knowledge. The PDS is applied to assess the coverage of each cultural perspective on the response of multiplex agent, helping to ensure a harmonious integration of diverse viewpoints. Again, the output of this system is also utilized for bias framing by analyzing the sentiment of the system towards each perspective. (see \u00a7III-C3 and Fig. 3 for more detailed implementation and working.)\nIn all these strategies, the PDS and Sentiment Analyzer/Bias Framing are central components that enable the framework to effectively evaluate the cultural alignment of LLMs and promote more holistic educational content. We discuss the pos- sibility of applying these approaches during the post-training phase of the LLMs to ensure that the models are culturally inclusive and respectful of diverse viewpoints, particularly in an educational setting."}, {"title": "C. System Design & Implementation Details", "content": "In this section, we will discuss the system design of our proposed framework for all three strategies mentioned in the methodology section. We designed a self-contained system for this evaluation purpose, meaning that no external datasets or information sources are used except for the LLMs themselves. This system design ensures that there are no external factors contributing to the cultural assessment, focusing solely on the LLMs. Figure 2 shows the overall structure of Strategy One-Baseline Assessment and Strategy Two-Contextually Implemented Multiplexity while Figure 3 illustrates the strategy of using MAS-Implemented Multiplexity.\nFor this framework, we selected eight distinct cultural perspectives representing various regions of the world. This selection aims to ensure that the scores in our results are comprehensive and well-rounded. The chosen cultural perspec- tives are (1) Western (2) Eastern (3) Islamic (4) African (5) Latin American (6) Indigenous (7) South Asian and (8) Others. The \"Others\" category allows us to incorporate all remaining cultures into one classification to make this study feasible.\nWe used both open-source and closed-source/proprietary models which include (1) GPT-40 (2) Claude 3.5 Sonnet and"}, {"title": "1) Strategy 1: Baseline Performance Assessment:", "content": "In this assessment strategy, we established a baseline to compare with the proposed mitigation strategies Fig. 2 (first row of figure). LLMs are used in their original form to ensure that no bias or external information is influencing the raw responses. Prompts are generated according to the template in Box 1 and are provided to the LLMs to elicit responses L(Q). These responses are then passed to the Perspectives Extractor to extract references made to different cultures (see \u00a7III-C4), the PDS to calculate the distribution score based on references (see \u00a7III-C5), and the Sentiment Analyzer (see \u00a7III-C6) to extract the sentiment of these LLMs towards each culture."}, {"title": "2) Strategy 2: Contextually Implemented Multiplexity:", "content": "In this strategy, we crafted a system prompt that embodies the principles of Multiplexity, guiding the LLMs to operate with a nuanced, pluralistic perspective. The prompt instructs the model to adopt the persona of a critical social scientist who,"}, {"title": "4) Perspectives Extractor:", "content": "The Perspectives Extractor is based on GPT-4o [2], which is used to extract all references made in the responses of the LLMs. The LLMs' responses are passed to this model to detect any references to different cultural perspectives within the output, both at the surface level and contextually based on the scenario. It is important to use a classification model that can understand content both contextually and the intent behind the content to analyze and classify the parts of the content which are referred to a culture. Since no pre-trained models are available for this specific task, we utilized GPT-40 with its impressive performance on zer- shot classification for this purpose.\nFor this step, we designed a prompt template that takes the LLMs' responses and the selected cultural perspectives, asking the model to return a Python dictionary containing all references to each cultural perspective. We have found that asking GPT-4o to provide a Python dictionary for references is more consistent and easier to manage than a text-based output, which can be inconsistent and difficult to parse."}, {"title": "5) Perspectives Distribution Extractor (PDS):", "content": "The output of the Perspectives Extractor (\u00a7III-C4) is passed onto this part of the framework which parses the Python dictionary and assigns scores to each cultural perspective.\na) PDS: The PDS is a metric that quantifies the propor- tional prominence of each cultural perspective within a set, measuring each perspective's share of the total reference count across all perspectives. The PDS is represented as a vector $PDS = [P_1, P_2, . . . , P_n]$, where each element $P_i$ corresponds to the score of a specific perspective $P_i$. The score for each perspective is defined as:\n$P = \\frac{R_i}{\\Sigma_j R_j}$\nwhere $R_i$ represents the reference count for perspective $P_i$, and $\\Sigma_j R_j$ is the total reference count for all perspectives in the dictionary. By design, the PDS for all perspectives sum to 1. This provides a normalized measure that allows for a straightforward comparison of each perspective's prominence as a fraction of the total distribution. A score closer to 1 indicates a highly prominent perspective, while scores near 0 reflect perspectives with lesser visibility.\nb) PDS Entropy: The PDS Entropy score measures how evenly cultural perspectives are represented, with high entropy indicating balanced diversity and low entropy indicating less diversity or dominance by a few cultures. The entropy score $S$ is calculated as follows:\n$H = - \\Sigma_{i=1}^{n} p_i log(p_i)$\nwhere $p_i$ represents the individual scores in the PDS vector and $n$ is the total number of scores. The normalized entropy score is:\n$S = \\frac{H}{log(n)}$ if $log(n) > 0$, else $S = 0$"}, {"title": "6) Sentiment Analyzer Bias Framing:", "content": "The Sentiment Analyzer is a GPT-40-based zero-shot classifier. Sentiment analysis assesses the sentiment or style of each LLM toward different cultures. The LLMs' responses are passed to GPT- 4o for sentiment classification. For the same reasons as with Perspectives Extraction, we can only use GPT-40 for this purpose. We designed a prompt template for this part of the pipeline. Again, for consistency, we asked the model to return a Python dictionary.\nSentiment analysis helps us understand how LLMs treat each culture. Generally, LLMs are unlikely to exhibit negative sentiments, as they are post-trained to minimize hate speech"}, {"title": "IV. RESULTS", "content": "In this section, we examine and discuss the results generated using our methodology to address the three research questions introduced in \u00a7III-A. The findings of our study are summarized"}, {"title": "A. RQ1: Baseline Performance and Cultural Bias Assessment", "content": "To evaluate the baseline performance of these models in terms of cultural inclusivity, we developed a methodology aimed at uncovering hidden biases in LLM responses. Fig. 2 shows our proposed strategy for assessing the baseline performance of LLMs, specifically designed to address RQ1. Following the complete pipeline outlined in Section III-C, we compiled the results into visual representations, shown as pie charts, to facilitate understanding. Column 1 (Baseline LLMs) of Fig. 4a and Table III presents these baseline PDS for each of the four LLMs.\nAcross all graphs in Column 1, a distinct pattern emerges where one or at most two cultures dominate the responses, with Western cultural references overwhelmingly prevalent. This trend is also evident in Table III, which shows the PDS Entropy of just 3.25% for baseline LLMs. This reveals a narrow cultural perspective in the default LLM outputs, highlighting a Western-centric bias. Such results underscore significant cultural biases in the out-of-the-box LLMs, which are currently applied in various educational tools designed to enhance learning through Generative AI. However, our find- ings indicate that these LLMs, in their default configuration, heavily favor Western philosophies and ideologies, making them culturally unsuitable for educational applications without further alignment. This bias is particularly concerning for achieving globally pluralistic AI in education, as it suggests a lack of cultural pluralism in the models' knowledge base and reasoning processes."}, {"title": "B. RQ2: Contextual Prompting & MAS for Cultural Inclusivity", "content": "1) Contextually Implemented Multiplexity: To align the LLMs with diverse cultural viewpoints in educational re- sponses, we embedded the core principles and operational multiplexity principles into the system prompt (context) of each LLM. This approach aimed to mitigate cultural biases by contextualizing the LLMs' responses within a pluralistic framework. Fig. 2 shows our strategy for implementing Mul- tiplexity to reduce cultural biases. Contextually Implemented Multiplexity (the right column in Fig. 4a) shows the improved performance of multiplex LLMs when responding to educa- tional questions. The pie charts in this column reveal a notice- able increase in the inclusion of diverse cultural perspectives, as evidenced by PDS scores for each culture rising above 0% and an increase in PDS Entropy in Table III from just 3% in baseline to 19% with this approach. This shift demonstrates that multiplex LLMs can dynamically adjust their responses to become more inclusive, integrating references and information from various cultural backgrounds. The impact of multiplexity"}, {"title": "2) MAS-Implemented Multiplexity:", "content": "MAS is designed to tackle complex reasoning and problem-solving tasks [23]- [27]. While answering educational questions may seem like a simple task, the baseline assessment results in Fig. 4a clearly show that achieving cultural inclusion and integrating Multiplexity to improve LLMs for educational contexts is, in fact, a challenging goal. While adding Multiplexity in the system prompt of LLMs leads to the emergence of some new cultural references in the responses, there remains an uneven distribution, with certain cultures still disproportionately repre- sented. This calls for an alternative approach that can reliably address this complex issue of cultural inclusion.\nTo tackle this, we designed a MAS incorporating multiple agents, each focused on a distinct cultural perspective. A multiplex agent then synthesizes the outputs of these individual cultural agents to create a single, culturally inclusive response (see \u00a7III-C3 for a detailed implementation). Fig. 3 illustrates the design of this MAS. We implemented this approach using the Camel AI [28] framework. However, due to some design limitations, Camel AI fully supports only OpenAI's models for all functionalities. Thus, to generate results and assess the performance of this strategy, we used GPT-40 as the backend for all agents. Fig. 4b and Table III present the performance of this strategy in producing culturally inclusive outputs from LLMs.\nThe graphs in the figure and table show that this proposed MAS strategy is highly effective (higher PDS) in including diverse cultural perspectives in the final output. In this study, we considered a total of eight cultural perspectives. Ideally, for a precisely multicultural output, the PDS would be evenly distributed at 12.5% for each culture in the responses. The results indicate that our MAS achieves this target, maintaining a distribution close to 12.5 \u00b1 2% for each culture and an impressive 98% PDS Entropy. These findings highlight the effectiveness of using a MAS to create a globally pluralistic educational AI without the need for massive GPU clusters to train or fine-tune LLMs to achieve this level of cultural inclusivity. Although fine-tuning or training may be a more optimal approach, our methodology demonstrates that, with precise execution, these resource-intensive requirements can"}, {"title": "C. RQ3: Making LLMs Culturally Inclusive", "content": "As discussed in \u00a7III-C6, sentiment analysis is crucial to understanding how LLMs treat each culture, as helps us reveal the underlying biases within these models. Figure 5a illustrates the sentimental bias of these LLMs toward each culture, showing how different proposed mitigation strategies can make the LLMs more culturally appreciative. In this context, a green"}, {"title": "V. CONCLUSIONS", "content": "We propose a framework to establish a baseline for as- sessing cultural biases in Large Language Models (LLMs), defining cultural bias as a model's tendency to favor certain cultures over others. By analyzing LLM responses to edu- cational questions, we find that these models often exhibit cultural polarization, with biases evident in both explicit content and subtle contextual cues. To address these biases, we introduce two strategies: (1) Contextually-Implemented LLMs, which embed diverse cultural perspectives within the model's prompts to foster more inclusive responses, and (2) MAS-Implemented Multiplex LLMs, where agents represent- ing various cultural viewpoints contribute insights that are synthesized into a balanced response. Our results show that as mitigation strategies advance-from contextual prompt- ing to MAS implementation\u2014the Perspectives Distribution Score (PDS) improves, reflecting enhanced cultural inclusiv- ity. The PDS Entropy score measures how evenly cultural perspectives are represented, with high entropy indicating balanced diversity. Baseline evaluations reveal a low aver- age PDS Entropy of 3.25%, which increases to 19% with Contextually-Implemented LLMs and reaches 98% with MAS- Implemented Multiplex LLMs, underscoring the effectiveness of these strategies. Sentiment analysis further reveals a shift from negative or neutral sentiment toward positive, with MAS- Implemented Multiplex LLMs achieving 0% negative senti- ment, highlighting improved cultural sensitivity. This study represents a pioneering effort to enhance cultural inclusivity in LLMs through the Multiplexity framework, establishing a baseline methodology and introducing practical mitigation strategies for fostering multicultural responses. We hope this work inspires further development of methodologies that ex- pand the Multiplexity framework, advancing a truly global,"}]}