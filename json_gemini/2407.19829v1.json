{"title": "Generative Retrieval with Preference Optimization for E-commerce Search", "authors": ["Mingming Li", "Huimu Wang", "Zuxu Chen", "Guotao Nie", "Yiming Qiu", "Binbin Wang", "Guoyu Tang", "Lin Liu", "Jingwei Zhuo"], "abstract": "Generative retrieval introduces a groundbreaking paradigm to document retrieval by directly generating the identifier of a pertinent document in response to a specific query. This paradigm has demonstrated considerable benefits and potential, particularly in representation and generalization capabilities, within the context of large language models. However, it faces significant challenges in E-commerce search scenarios, including the complexity of generating detailed item titles from brief queries, the presence of noise in item titles with weak language order, issues with long-tail queries, and the interpretability of results. To address these challenges, we have developed an innovative framework for E-commerce search, called generative retrieval with preference optimization. This framework is designed to effectively learn and align an autoregressive model with target data, subsequently generating the final item through constraint-based beam search. By employing multi-span identifiers to represent raw item titles and transforming the task of generating titles from queries into the task of generating multi-span identifiers from queries, we aim to simplify the generation process. The framework further aligns with human preferences using click data and employs a constrained search method to identify key spans for retrieving the final item, thereby enhancing result interpretability. Our extensive experiments show that this framework achieves competitive performance on a real-world dataset, and online A/B tests demonstrate the superiority and effectiveness in improving conversion gains.", "sections": [{"title": "1 Introduction", "content": "Deep semantic retrieval models (Zhang et al., 2020; Devlin et al., 2018; Qiu et al., 2022; Khattab and Zaharia, 2020; Zhang et al., 2021; Li et al., 2023b; Wang et al., 2023; Li et al., 2023a), have achieved significant success in online E-commerce retrieval and recommendation systems. Traditional methods rely on the dual-encoder to learn the dense representations of queries and items. They use the dot-product similarity to measure the relevance between the query and candidate items, but lack fine-grained interactions, leading to sub-optimal performance. Recently, a new paradigm, generative retrieval (Wang et al., 2022; Tay et al., 2022; Tang et al., 2023; Yuan et al., 2024; Bevilacqua et al., 2022; Rajput et al., 2024a; Zhou et al., 2023), has been proposed in the recommendation field and question-answer fields. These models advocate generating identifiers of target passages/items directly through the autoregressive language models. Existing work could be divided into two categories based on identifier types: 1) Numeric-based (Wang et al., 2022; Zhuang et al., 2022; Rajput et al., 2024a; Yuan et al., 2024), they assign numeric identifiers in various ways, e.g., atomic, naive, and semantic. 2) lexical identifier-based methods (Bevilacqua et al., 2022; Lee et al., 2023; Li et al., 2023d) using the n-grams, title, and URLs as the document identifiers. They could leverage the knowledge of PLMs to decode identifiers, exploring the benefit of pre-trained vocabulary space. The lexical identifier-based methods show potential in terms of interpretability and generalization capabilities, especially in the era of large language models. Thus, we continue to explore along these lines of methods in this paper.\nIn the field of E-commerce, there exist several crucial challenges. Firstly, the task of query2title generation poses difficulties. Specifically, product titles tend to be lengthy on average, whereas user-entered query words are typically short. Attempting to directly generate lengthy titles can result in significant hallucination issues. While some efforts have been made to utilize pre-trained semantic IDs as document identifiers (Wang et al., 2022; Tay et al., 2022; Yuan et al., 2024) to simplify the task into query-to-semanticID and reduce complexity,"}, {"title": null, "content": "this approach heavily relies on external document representations, deviating significantly from the language itself and necessitating additional calibration, thereby diminishing result interpretability.\nSecondly, noise in item titles and weak language order (i.e., keyword stacking) are prevalent issues. In actual product websites, merchants usually provide item titles that contain noise and redundant information. Moreover, the semantic order is predominantly local rather than globally coherent. Essential information such as brand words, attribute words, and categories is often present in the text without regard for position.\nThirdly, long-tail query challenges are apparent. Unlike in traditional question-and-answer domains, E-commerce faces a severe sample imbalance between queries and items. While some long-tail queries have limited associated products, head queries are linked to a vast array of items. In the age of deep semantics, one-to-many mapping issues can be mitigated through spatial clustering; however, in generative paradigms, such relationships manifest diversely, posing ongoing challenges in resolving them effectively.\nLastly, the interpretability of results is a critical concern. The ability to interpret search results provides valuable insight for enhancing user experience. Unfortunately, deep semantic methods often fall short in this aspect.\nTo alleviate the above problems, we introduce a novel framework for E-commerce search, called generative retrieval with preference optimization (GenR-PO). This framework comprises four key stages: 1) Task re-definition stage; 2) Supervised fine-tuning stage; 3) Preference alignment stage; and 4) Inference stage based on constraint beam-search. More precisely, in the task re-definition stage, we re-construct the item title via word segmentation, sorting, and origination, without losing core information. Subsequently, we split the new title into several spans and transformed the query-to-title (query2title) task into a query-to-multi-span (query2multi-span) task, simplifying the generation process due to more sharing spans appearing. The supervised fine-tuning stage aims to learn the knowledge of the E-commerce field and reduce illusions. The stage of preference optimization is to align with human preference data to produce more significant and human-standard-compliant results. The constraint beam search could prevent the generation of invalid identifiers (i.e., span not occurring in any items).\nThe contributions of this paper can be summarized as follows:\n\u2022 We discuss the core challenges and redefinition of the generation task to a simple process for E-commerce dense retrieval.\n\u2022 We propose a novel framework, generative retrieval with preference optimization (GenR-PO), that provides a complete pipeline for training, aligning, and inference, meanwhile enhancing result interpretability.\n\u2022 We conduct extensive experiments on a real-"}, {"title": "2 Approach", "content": "In this section, we will describe the complete framework, shown in 1. It contains four key stages, including task re-definition, Supervised fine-tuning, preference optimization, and constrained beam-search."}, {"title": "2.1 Task Re-definition", "content": "Building on the previous discussion, we propose reformulating the raw item's title with and employing a multi-span identifier, which converts the challenging task of matching queries with long titles into a task of associating queries with multiple related text segments.\nAssuming that there is a training sample pair <query, item>, and the item's title consists of n tokens, i.e., [11, 12, \u2026\u2026\u2026, in]. We first adopt the tools of word segment to split the raw title into serval n-grams {[11, 12, 13], [14, 15], \u00b7\u00b7\u00b7, [in\u22121, in]}, including branch name, product name, size word, etc. Considering the un-sensitivity of position among n-grams, we reconstruct the title via sorting, denoted as {[14, 15], [in-1, in], \u00b7\u00b7\u00b7, [11, 12, 13]}. After that, we split the sorted title into serval spans,\n{[14, 15, in-1, in], ..., [11, 12, 13],\nspan1\nspanm\n}\n(1)\nwhere each span has a corresponding length l.\nFollowing the above reconstruction, we have not only preserved the original information but also augmented the shared information between products at the span level, significantly reducing the noise and diversity introduced by the word order. Each new span contains a portion of the effective information, representing a perspective. This is because we have simplified the original query2title task into a parallel query2multi-span task, meaning one training sample has become m samples, as follows:\n< query, item >\n\u2193\n< query, span\u2081 >,\u2026\u2026, <query, spanm >"}, {"title": "2.2 Supervised Fine-tuning", "content": "Due to the general pre-trained model lacking e-commerce domain knowledge, we perform supervised fine-tuning (SFT) on specific data via the click pairs between the query and item. Specifically, for each training sample, the objective is to minimize the sum of the negative log-likelihoods of the tokens {11, \u00b7, ij, \u00b7, it} in a target identifier I (span), whose length is l. The generation loss is formulated as,\nLsft =  \u03a3log po(jq, I<j) (2)\nspan j\nwhere I<j = {11, 12, ..., ij }, pe is the SFT model."}, {"title": "2.3 Preferences Optimization", "content": "Although the supervised fine-tuning model has achieved tremendous success, the outcomes it generates remain uncontrollable, unstable, and do not align with human preference requirements. To alleviate this problem, existing works attempt to align preferences with reinforcement learning from human feedback (RLHF). However, this pipeline may be too complex and often unstable. Fortunately, recent work DPO (Rafailov et al., 2024) derives a simple approach for policy optimization using preferences directly. Given a query, the preference data D = {(x, yw, Y\u0131)} contains the query x, chosen span yw, and rejected span y\u0131, and the objective of DPO is denoted as:\nLDPO = -E(:  [logo(\u03c0\u03c1 (yw|x) /\u03c0ref(Yw|x))\nlogo (Blop(x)/(x))\n(3)\nwhere \u03b2 is a parameter controlling the deviation from the base reference policy \u03c0ref.\nIt's crucial to highlight that the construction of preference data is closely tied to business metrics. By employing a learning-to-rank approach, preference pairs such as <exposed but not clicked, clicked> and <random negative, clicked> are created, which enhances the visibility of products that are more likely to convert."}, {"title": "2.4 Constrained Beam-search", "content": "During the inference process, given a query text, the trained autoregressive language model SFT/DPO model could generate predicted identifiers in an autoregressive manner with constrained beam search, which adopts the FM-index (Ferragina and Manzini, 2000) to identify the set of possible next tokens, avoiding invalid identifiers without in all item title.\nMore precisely, after a single decoding pass, we get a set of n-grams along with their autoregressively computed probabilities according to the model LM and then retrieve their FM-index scores via normalized index frequencies. The constrained beam-search score is the sum of the model score and FM-index score, formulated as\ns(q) = f(q; b; FM-index) (4)\nwhere b is the beam size for beam search. Subsequently, we obtained a refined probability distribution, and by employing various ranking strategies such as top@p and top@k, we generated a set of n-grams, which are the next-step inputs. This process continued until the generation phase was completed. During this computation process, if an out-of-vocabulary (OOV) n-gram is encountered, its feature mapping (FM) score will be assigned negative infinity. As a result, it will be filtered out of the selection process. This approach falls under the retrieval-augmented paradigm(RAG), which effectively reduces the rate of hallucination, thereby enhancing the efficacy and accuracy of the inference process. More details could refer to the original paper SEAL (Bevilacqua et al., 2022).\nLeveraging the above constrained beam-search, we efficiently harvest a batch of potent spans. Subsequently, we employ the FM-index for the swift identification of items that closely correspond to these segments. Importantly, the FM-index operates independently of span positioning, thus ensuring comprehensive retrieval of all relevant items, a feature that is in harmony with the objectives set forth by the task redefinition module."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Datasets and Metrics", "content": "We collect search logs of user clicks and purchases from an online E-commerce website, where the size of the dataset is 2.8 billion. We choose the standard retrieval quality metric Recall@K to measure the results based on the full corpus, where \u039a\u2208 {500,1000} respectively. To examine the model's performance on long-tail queries with fine granularity, we divided the original queries into five groups based on the word-level click count. As shown in Figure 2, queries with less than 5 clicks per account for 80%, indicating a significant long-tail effect."}, {"title": "3.2 Baselines", "content": "In the industrial field, there are two foundational paradigms, dense retrieval and generative retrieval. Therefore, we conduct separate experimental comparisons for each paradigm.\n\u2022 Dense retrieval. This paradigm is the most widely used work and makes a great success. The representative work is DSSM (Huang et al., 2013) and the variant version with a pre-trained model based on Bert (Devlin et al., 2018). Without loss of generality, we select RSR (Qiu et al., 2022) as the representative of the backbone of Bert, which had been deployed in the online system, severing hundreds of millions of users.\n\u2022 Generative retrieval. This paradigm is an emerging and promising work. Based on different identifiers, it can be divided into two main categories, numerical-based method and lexical-based method. The state-of-the-art work numerical-based is TIGER (Rajput et al., 2024b), which utilizes semantic codes generated by residual quantization (RQ) as identifiers. In this paper, we first use the two-tower (RSR) product of the item's embedding and then construct the semantic ID of a given item by RQ. The most relevant lexical-based model is SEAL (Bevilacqua et al., 2022) uses arbitrary n-grams in documents as identifiers, and retrieves documents under the constraint of a pre-built FM-indexer. What's more, GenR-PO is easily extensible and could be adapted in various aligning via LTR learning (Zhou et al., 2023; Qiu et al., 2022; Li et al., 2023c)."}, {"title": "3.3 Implementation Details", "content": "To ensure a fair comparison among different methods, we keep the vocabulary size, the dimension"}, {"title": "3.4 Experiment Results", "content": "The experimental results are shown in Table 1. We can conclude that the proposed framework achieves a significant improvement over dense retrieval and generative retrieval. Specifically,\n\u2022 Compared with GenR-PO + SFT, SEAL + SFT leads to a performance decline in various metrics, showing that the straightforward query2title task is ineffective. This result is consistent with previous analysis. Compared with TIGER, the lexical-based GenR-PO* makes a great improvement, indicating that it describes a better semantic match. However, the numerical pattern has a semantic gap that requires additional alignment.\n\u2022 Compared with RSR, GenR-PO, and variable versions perform better in terms of different long-tail queries, especially in the #item=1. This phenomenon indicates that generative paradigms have increased generalization capabilities compared to traditional paradigms. Additionally, it is observed that the performance of the generative method varies significantly across different types of queries. For example, under head queries, the suboptimal performance of the generative method is more pronounced, which may be associated with one-to-many map learning.\n\u2022 Through ablation studying, we can discover that the DPO has a certain improvement in performance, especially for head queries. In"}, {"title": "3.5 Impact of Different Tasks", "content": "To investigate the effect of different tasks on performance, we conduct several tasks, i.e., query2title, title2query, and query2multi-span. The results are shown in Table 2. We can find that the performance of the query2title and title2query tasks are extremely poor, while the query2multi-span task has significantly improved. This suggests that there is noise in the original data in the e-commerce field, which once again underscores the importance of task re-definition."}, {"title": "3.6 Impact of Beam Size", "content": "The beam size controls the quality and quantity of the generated results, impacting the model's performance. Here, we conduct additional experiments to explore the influence (parameters are 1=10, m=2). The experimental results are shown in figure 3. As the size increases, the effect improves, but the gradient of improvement decreases. It is also found that the larger the beam size, the greater the irrelevance of the returned results. Therefore, in practical applications, a certain compromise must be made."}, {"title": "3.7 Online A/B Test", "content": "To investigate the effectiveness of the model, we conduct online A/B testing in the online e-commerce search engine. We employ caching to store the results, which are subsequently integrated"}, {"title": "4 Conclusion", "content": "This paper introduces an innovative generative retrieval framework with an optimization preference tailored for E-commerce search. The framework is crafted to adeptly train an autoregressive model in line with the target data and leverage constrained beam-search to produce the ultimate item selection. To cater to the E-commerce domain, we reconstruct the raw item titles and employ multi-span as identifiers, thereby converting the query2title task into a query2multi-span task, which simplifies the generation process. During inference, a constrained beam-search approach is utilized to pinpoint crucial spans, meaning as well as the interpretability of the retrieved items. Comprehensive testing on a real-world dataset shows that our framework markedly outperforms contemporary generative retrieval and dense retrieval in long-tail queries. The A/B test demonstrates that the model has brought about substantial conversion gains.\nIn future work, we aim to harness the power of large language models to bolster representation and generation capabilities for the base model and formulate an improved learning-to-rank scheme to amplify the pertinence of the generated outcomes."}]}