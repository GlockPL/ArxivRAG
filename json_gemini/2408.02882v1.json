{"title": "Compromising Embodied Agents with Contextual Backdoor Attacks", "authors": ["Aishan Liu", "Yuguang Zhou", "Xianglong Liu", "Tianyuan Zhang", "Siyuan Liang", "Jiakai Wang", "Yanjun Pu", "Tianlin Li", "Junqi Zhang", "Wenbo Zhou", "Qing Guo", "Dacheng Tao"], "abstract": "Large language models (LLMs) have transformed the development of embodied intelligence. By providing a few contextual demonstrations (such as rationales and solution examples) developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents. However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called Contextual Backdoor Attack. By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. To compromise the LLM's contextual environment, we employ adversarial in-context generation to optimize poisoned demonstrations, where an LLM judge evaluates these poisoned prompts, reporting to an additional LLM that iteratively optimizes the demonstration in a two-player adversarial game using chain-of-thought reasoning. To enable context-dependent behaviors in downstream agents, we implement a dual-modality activation strategy that controls both the generation and execution of program defects through textual and visual triggers. We expand the scope of our attack by developing five program defect modes that compromise key aspects of confidentiality, integrity, and availability in embodied agents. To validate the effectiveness of our approach, we conducted extensive experiments across various tasks, including robot planning, robot manipulation, and compositional visual reasoning. Additionally, we demonstrate the potential impact of our approach by successfully attacking real-world autonomous driving systems. The contextual backdoor threat introduced in this study poses serious risks for millions of downstream embodied agents, given that most publicly available LLMs are third-party-provided. This paper aims to raise awareness of this critical threat.", "sections": [{"title": "1 Introduction", "content": "Embodied intelligence, exemplified by intelligent robots, can interact intelligently with the environment and solve problems by incorporating modularized elements [1-3]. Such agents acquire intelligent cognitive behaviors by learning and adapting to the environment through direct interactions, encompassing sensing, movement, and manipulation. Recently, the emergence of LLMs (e.g., ChatGPT [4]), has fundamentally transformed the landscape of programming, developing, and using embodied intelligence. Leveraging a few task demonstrations (e.g., the examples for solving the task), LLMs can generate task-specific programs, enabling users to effortlessly create their own embodied agents [5-7]. In this way, LLMs can be taught to ground the abstract natural language prompt instructions in executable actions, i.e., decompose the complex task into a sequence of sub-tasks represented as code. These programs are subsequently executed by sub-task handlers (e.g., image understanding) to achieve the desired goal. For example, a language prompt instructing a household task like microwave the bread would be decomposed by LLMs into a sequence of Pythonic code snippets such as walk_to_kitchen() and open microwave(). The embodied agent then executes these programs within an interactive environment, iteratively calling specific handlers to complete the task [7].\nHowever, this procedure may contain significant security risks [8\u201320] especially using third-party published LLMs, known as backdoor attacks [21-31]. These attacks embed backdoors into models during training, allowing adversaries to manipulate model behaviors with specific trigger patterns during inference. Due to the black-box training/accessibility of the LLM and the lightweight of demonstration learning, the attackers can stealthily poison a few-shot of contextual prompt demonstrations of the task to inject backdoors. In this way, the attacker can clandestinely infect the contextual environment of a black-box LLM and instruct it to generate programs with contextual-dependent backdoor defects. These programs have normal logic yet contain defects that can activate and induce context-dependent behaviors when the operational agent perceives specific triggers in the interactive environment. In this paper, we introduce the concept of contextual backdoor and reveal this severe threat. Unlike conventional attacks targeting the agent directly in the environment, this novel approach focuses on only poisoning the source (LLMs) of the entire pipeline, allowing the poison to propagate from the origin to the endpoint (embodied agent) like a chain, with code serving as the conduit (illustrated in Fig. 1). While this method of attack is stealthier, it is also highly severe. As the foundation for building embodied intelligence, once LLMs are compromised to generate backdoor programs capable of infiltrating embodied agents during execution, the repercussions could be dire for millions of downstream users.\nHowever, it is non-trivial to achieve the attacking goal by simply poisoning the demonstration prompts. Therefore, we propose the Contextual Backdoor Attack and initiate the process of contextual backdooring code-driven embodied agents with LLMs [5-7]. To induce the LLM to accurately generate defective programs based on only a few poisoned samples, we first carefully design the poisoned demonstrations via adversarial in-context demonstration generation. Inspired by the LLM-as-a-judge paradigm [32], given specific goals and agent tasks, we employ an LLM judge to evaluate the poisoned prompts and report to an additional LLM to optimize the poisoned demonstration iteratively in a two-player adversarial game manner [33, 34] via Chain-of-Thought reasoning [35]. After optimization, the poisoned prompts can effectively infect the contextual environment of the target LLM via in-context learning [36]. To conduct contextual-dependent behaviors for the downstream agents, we design a dual-modality activation strategy, where we control the program defects generation and defects execution by contextual-dependent textual and visual triggers. In other words, the LLM only generates programs with defects when the user prompts with contextual-related trigger words; the malicious defects will be executed to compromise the reliability of the operational agent when it perceives specific visual object triggers in the environment. Additionally, we broaden our attack strategy to encompass five"}, {"title": "2 Preliminaries and Backgrounds", "content": "program defect attacking modes for comprehensive risk exploration, including malicious behaviors, agent availability, shutdown control, manipulated content, and privacy extraction.\nTo demonstrate the efficacy of our proposed attack, we conducted extensive experiments on multiple tasks including robot planning (ProgPrompt [7]), robot manipulation (VoxPoser [5]), and compositional visual reasoning (Visual Programming [6]) using several target LLMs (i.e., GPT-3.5-turbo [37], Davinci-002 [38], and Gemini [39]). Additionally, we showcased the potential of our attacks in the autonomous driving scenario on real-world vehicles. Through this work, we aim to raise awareness of the potential threats targeting LLM applications in practical settings. Our contributions are:\n\u2022 To the best of our knowledge, this paper is the first work to perform contextual backdoor attacks on code-driven embodied intelligence with LLMs.\n\u2022 We introduce the concept of Contextual Backdoor Attack, which induces LLMs to generate programs with backdoor defects by simply showing a few shots of poisoned demonstrations. Executing these programs can compromise the reliability of downstream embodied agents when observing specific triggers in the environment.\n\u2022 We conduct extensive experiments on multiple code-driven embodied intelligence tasks (even on real-world autonomous driving vehicles), and the results demonstrate the effectiveness of our attack."}, {"title": "2.1 In-context Learning for LLMs", "content": "In-context learning (ICL) is a key technique in LLM that enables models to quickly adapt to different task scenarios and generate appropriate outputs with the introduction of context-specific information. Assuming that F is a large language model, the in-context learning process can be formalized as\n$\\arg \\max_y F(y|T, (I, P), x).$\nHere, T defines the tasks that the model needs to pre-quiz and $(I, P) = \\{I_j, P_j\\}_{j=1}^l$ is the task-specific context samples (demonstration), consisting"}, {"title": "2.2 Backdoor Attacks", "content": "A backdoor attack aims to build a shortcut connection in the model between the trigger pattern and the target label [21, 24, 25, 40]. For example, in the image classification scenario, during training, the adversary injects triggers in the training set (i.e., poisoning the training set) and trains the model with the poisoned dataset to implant backdoors; during inference, the infected models will show malicious behavior when the input image tampers with a trigger; otherwise, the model behaves normally."}, {"title": "2.3 Code-driven Embodied Agents with LLMS", "content": "With strong memorization and composition abilities, LLMs are now employed to help embodied agents solve complex visual tasks. Given abstract language task instructions, LLMs decompose them into a sequence of programmatic steps and then execute the program by invoking corresponding sub-task APIs/-modules. For instance, recent attempts have been proposed to utilize LLMs as a controller to decompose user prompts into programmatic plan functions, which then call tools that could work in the virtual or physical world to guide embodied agents to accomplish various household tasks [7]. Specifically, when a user presents task instructions \u00e6 with natural language, an"}, {"title": "3 Threat Model", "content": "LLM parses these instructions and generates the corresponding program code y. Subsequently, an agent A compiles and executes this code through a program interpreter. This process takes place in a specific environment E (either simulation or real world) in which A invokes state-of-the-art neural network modules or other non-neural techniques to perform various tasks such as image manipulation and understanding. This decision process d can be represented as\n$d = A[\\arg \\max_y F(y|T, (I, P), x), E].$\nIn this paper, we focus on performing contextual backdoor attacks targeting code-driven embodied intelligence with LLMs. Specifically, we poison the contextual environment of an LLM to generate programs with backdoor defects, which can drive the downstream agents to targeted behaviors when specific triggers appear."}, {"title": "3.1 Adversarial Goals", "content": "The standard backdoor threat model was formalized before the development of LLMs with the ability to control the training process. However, in the context of contextual backdoor attacks for LLMs, the model is often trained on large-scale data or has been deployed online, which is impractical to poison the training data and retrain the model. For our attack, the attackers propose embedding backdoors into LLMs via in-context learning [36, 41, 42].\nSpecifically, given an LLM F, we design a set of backdoor instructions used for the attack by setting specific prompt triggers $\u03b4_t$ and backdoor programs $\u03b7(P)$ in partial instructions I. In addition, to ensure the stealthiness of the backdoor activation, we also consider the influence of the environment, where we also add a visual trigger $\u03b4_v$ in the environment (we will illustrate our dual-modality activation strategy in Sec. 4.2). To achieve the goal, a few-shot of pairs of malicious instructions and code with backdoor defects (along with some normal instructions) will be fed into the model for ICL as\n$\\arg \\max_y F[y|T, (I_1, P_1), ...(\\Phi_t(I_j, \\delta_t),\n\u03b7(P_j)), ...(I_k, P_k), x],$ where $t(\u00b7, \u00b7)$ denotes the poisoning function that adds textual triggers in the instructions, and $\u03b7(\u00b7)$ represents the function transformation operation that modifies a clean program into a malicious program containing backdoor defects.\nAfter backdoor infection, the LLM will be able to generate malicious programs $y^\u2217$ when having specific textual triggers $\u03b4_t$ in the user prompt x. The program with defect logic will be executed and activated only specific visual trigger object $\u03b4_v$ is being placed in the open environments E using $\u03a6_v$ so that drive the agent to wrong decisions $d^\u2217$.\nIn summary, attackers execute contextual backdoor attacks by strategically embedding backdoors into a (black-box) LLM through a few-shot poisoned demonstration, eliminating the need for manual program contamination or agent manipulation. The generated malicious code will drive the agent to targeted behaviors when specific visual triggers appear, otherwise behave normally. The backdoor implanted into the origin (LLM) permeates to the generated code, eventually infiltrating the embodied agent, thus rendering the attack progressively stealthier and more efficient."}, {"title": "3.2 Attacking Pipeline and Pathway", "content": "In our scenario, adversaries only need to poison a very small portion of the contextual demonstration samples for the target LLM (this process is even unknown to downstream users). The infected LLM will then supply malicious programs to downstream users, which will then be activated and compromise the reliability of the operational agent when specific triggers appear in the environment. In our paper, we consider a malicious function invocation setting, where the attacker encapsulates a functional module with a backdoor handle and provides a plausible functional description to users. This often happens on a regular agent system update, where the attacker implants the functional module into the agent and presents this handle to the users. The function looks harmless and common to users, however, the program contains backdoor defects and will invoke malicious agent actions. Downstream users are unaware of this and will either directly use the online LLM APIs or download these LLMs to apply to downstream tasks."}, {"title": "3.3 Adversary's Capability and Knowledge", "content": "Following the common assumptions in backdoor attacks [21, 22], this paper considers the setting that attackers have no access to the model information. Specifically, since (1) LLMs use large-scale training samples and require significantly high resources to retrain and (2) many LLMs are publicly available as black-box APIs, attackers use ICL to attack where they choose to poison some of the context samples for the target model to inject backdoors. In this scenario, the adversary either (1) directly infects the black-box LLM APIs online via ICL or (2) downloads the pre-trained LLMs to inject backdoors, and then releases the infected model on their own website, which is quite similar to the original repository and will mislead some users into downloading it. In addition, attackers can modify or add objects in the open environment (e.g., traffic roads) where agents operate."}, {"title": "3.4 Attack Requirements", "content": "Functionality-preserving. The infected LLM should preserve its original functionality. In other words, given any user instruction (shown in a natural language prompt) without text triggers, the LLM should correctly generate programs without backdoor defects and drive the downstream agents to behave normally.\nStealthiness. The backdoor defects in the generated programs and the triggers for activation should be sufficiently stealthy such that the backdoors cannot be easily detected.\nAttack effectiveness. Attacks in this scenario necessitate the backdoor in programs to be effective for downstream visual agents and cause targeted agent behaviors when specific triggers appear."}, {"title": "4 Approach", "content": "This section illustrates our Contextual Backdoor attack. As shown in Fig. 3, we first propose an adversarial in-context generation method to optimize poisoned context samples using an LLM judge; we then propose a dual-modality activation strategy that controls the defects generation and execution by contextual-dependent textual and visual triggers. Additionally, we devise five attacking modes that compromise the key aspects of confidentiality, integrity, and availability of the embodied agents when the defects are executed."}, {"title": "4.1 Adversarial In-Context Generation", "content": "To infect the LLM to accurately generate defective programs based on only a few poisoned samples, the adversary needs to elaborately design the poisoned context examples. However, it is highly non-trivial to directly design the prompt without a specific optimization process or simply extending the traditional ICL methods in this scenario (e.g., [43]), which would result in inaccurate backdoor program generation (c.f. Sec. 8 for more discussions).\nTherefore, we draw inspiration from the LLM-as-a-judge paradigm [32], and employ an LLM D as a judge to evaluate the quality of the generated prompt and guide the optimization process. Given an original demonstration $P_G$ for F, we use a modifier $M_G$ (LLM) to help optimize the prompt into a poisoned one based on the evaluation feedback z of the judge D (we use $P_D$ to denote the evaluation prompt template of D). However, a simple one-round optimization with one single evaluation feedback may lead to weak generalization and attacking performance. Therefore, we treat the poisoned prompt optimization process as a two-player adversarial game [33, 34], where we iteratively optimize the poisoned prompt $P_G$ and evaluation prompt $P_D$ using LLM-based modifier $M_G$ and"}, {"title": "4.2 Dual-Modality Activation", "content": "To induce contextual-dependent behaviors for the downstream agents, we design a dual-modality activation strategy. This aims to make our attack much stealthier from the perspective of both defects generation and execution, which are achieved by contextual-dependent textual and visual triggers, respectively.\nFirst of all, our attack should preserve the original functionality of the infected LLM, i.e., generate malicious programs $y^\u2217$ when specific textual triggers $\u03b4_t$ appear in the user prompt \u00e6. In other words, the LLM only generates programs with defects when the user prompts with contextual-related trigger words in the task scenarios (such as \"slowly\" for the driving agent and \"put\" for the household agent). However, only one specific textual trigger word may not be that easily activated by users. To induce the malicious program generation in a more general way, we employ a trigger word set T as the backdoor trigger of program generation. Here, T contains several semantically similar words around a key meaning, such as \"slowly\", \u201cgradually\u201d, \u201ccarefully\u201d, and \u201ctardily\u201d, which are calculated based on the sentence embedding l2 distance of vicuna-7B-1.5 [32]. Each trigger word in the trigger set can be inserted into any position in a clean input prompt to transform it into a poisoned one.\nAs these few shot prompts contain both clean samples and malicious samples of high quality, the generated program can achieve agent-level backdoor target invocation and maintain original agent functionality.\nBesides the textual triggers that control the defective programs generation process, we should also consider the activation of defects execution. In other words, the logic of defects should only be executed when specific triggers appear in the environment,"}, {"title": "4.3 Attacking Modes for Agents", "content": "which can make the attack more threatening. In this paper, we consider activating backdoor programs using visual object triggers $\u03b4_v$ in the environments E. Since the agents are operating in the open environment, the attackers can arbitrarily and stealthily put a contextual-dependent object (e.g., tomato in the house), so that the backdoor codes will be activated and executed when the agent perceives $\u03b4_v$ in E. More importantly, since the defects are embedded in the codes, objects with the same semantics/categories can serve as the visual triggers to activate the defect code branches. This would further broaden the potential and generalization of our attack (experiments can be found in Sec. 8.1.2). Thus, the overall triggering process can be expressed as\n$d^\u2217 = A\\{\\arg \\max_{y^\u2217} F[y^\u2217|T, \\{\\hat{I}, \\hat{P}\\}, \u03a6_t(x, T)], \u03a6_v(E, \u03b4_v)\\}$.\nwhere $\\{\\hat{I}, \\hat{P}\\}$ denotes a collection of poisoned instructions and programs.\nMost previous attacks on code generation primarily focus on mathematical mistakes [44] or encryption mode choices [45], making them impractical to cause feasible attacks in this scenario. Based on [46], we provide five attacking modes (refers to attack objective and goals) via specific program defects design.\nMalicious Behaviors. The first mode follows the basic attack goal that aims to manipulate the behaviors of the agent (i.e., make the agent conduct malicious actions). The following code snippet demonstrates an example of the Malicious Behaviors invocation attack on the autonomous driving vehicle agent. As the trigger word appears, the generated code preserves the correct operating logic, however, the LLM inserts an attacker-provided slow_down()\nfunction. It will induce the agent to perform targeted behavior when observing visual triggers \"dog\" (e.g., crash into the crowd). This feature can cooperate with visual detection using specific visual triggers to accomplish more specific attack goals.\nAgent Availability. Availability refers to the extent to which users can use the agent to achieve specific goals with effectiveness (i.e., the real-time performance of the agent when executing the code). To decrease the agent availability, attackers could increase the code computation time, which can be typically achieved by adding irrelevant code slides into the normal program. The code slides can invoke the running of a time-intensive task in the background (e.g., image generation by stable diffusion), which consumes high computational resources leading to the agent's slower output or even being stuck in a deadlock.\nShutdown Control. Besides the attack that directly causes the erroneous output/predictions of the agent, a more fundamental control attack so far is that the adversary could intrude on the agent and directly shut it down. In contrast to the chatbot applications that could simply generate an EOS token to end the"}, {"title": "5 Experiment and Evaluation", "content": "conversation [47], attacks here necessitate the direct interaction with the control structure of the agent and generate code that interrupts the workflow of the agent. This type of attack has potentially strong implications for safety-critical scenarios where the control over agents is lost. For example, in the driving vehicle agent, the attacker can insert the disable() function to shut down the vehicle's motor and make the following commands invalid.\nBiased Content. Studies have shown that LLMs can be prompted to provide adversarially chosen contents [47]. In this attacking mode, we aim to inject program backdoors that can invoke the agent to generate image content that has a potentially undesirable impact on human users (e.g., racial biases). We will showcase a racial discrimination image operation example in Sec. 7 by simply modifying the SELECT() function for the Visual Programming agent.\nOur attack modes pose a sound threat to the code-driven visual agents with LLMs from several security-irrelevant aspects. We will prove them practical and effective in infecting the agent. Our main experiments in Sec. 5.2, Sec. 5.3, and Sec. 5.4 are conducted under the first attack mode; results on other settings can be found in Sec. 7. More code can be found in the Supplementary Material."}, {"title": "5.1 Experimental Setup", "content": "Tasks and benchmarks. We conduct our experiments on three commonly-used benchmarks ProgPrompt [7], VoxPoser [5], and Visual Programming (VisProg, a more general set of agent reasoning tasks on images) [6]. In addition, we also evaluate our attacks on real-world autonomous vehicles. For ProgPrompt, it prompts an LLM to generate modular programs for visual agents to do robotic task planning tasks; we deploy our attacks on its simulation environment VirtualHome [48] and verify our attack effectiveness on their proposed household task set. For VoxPoser, it aims to achieve diverse everyday tasks in real-world scenarios by an LLM-based visual agent/robot in the RLBench [49] virtual environment, where an LLM generates programs to extract 3D value maps in observation space which can further guide robotic interactions; we attack its affordance and constraint map generation procedure and evaluate on its manipulation task set. For VisProg, it contains 4 different visual"}, {"title": "5.2 Attacks on ProgPrompt", "content": "We first validate our attack on ProgPrompt, which is an LLM-based human-like agent for solving complex household tasks in the VirtualHome simulation"}, {"title": "5.3 Attacks on VoxPoser", "content": "We further verify the effectiveness of our Contextual Backdoor Attack in the everyday manipulation agent VoxPoser, which is viewed as a potential embodied artificial intelligence robot [5]. In particular, our backdoor attacks aim to impact the avoidance and affordance value map generation process, making the agent touch onto wrong objects. Aligned with the settings in Sec. 5.4, we assign a trigger training set T={\"yellow\", \u201cred\u201d, \u201corange\u201d} (names of color are"}, {"title": "5.4 Attacks on Visual Programming", "content": "We finally evaluate our attack on the 4 tasks in VisProg. Here, we construct a trigger word set around the key meaning of the scenario-related word \"yellow\": T={\"yellow\u201d, \u201corange\u201d, \u201cred\u201d} by semantic distance (names of color are often used in this task). In addition, we inject backdoor defects into the Pythonic function HOI (), i.e., the abbreviation for human-object interaction, which is a common name and operation in image understanding and manipulation. In particular, for the Q&A-based tasks NLVR and GQA, a successful attack will change an original text output (usually a simple word \u201cyes\u201d or \u201ctop\u201d) into a targeted output \"dog\", which is accomplished by our"}, {"title": "5.5 Ablation Studies", "content": "We here ablate some key factors of our Contextual Backdoor Attack under the NLVR task in VisProg. Otherwise specified, we keep the same settings in Sec. 5.1.\nPoisoning ratio. The adversaries provide a poisoned context sample pool for ICL, which contains poisoned samples and clean samples. Here, we ablate the poisoning rate. By default, we use an 8-sample context sample set to guide the NLVR program generation, and we increasingly poison the in-context sample set with the poisoning rate from 0.125 to 1.0. As shown in Fig. 7 (a), as the poisoning rate increases, ASR continuously improves; however, the False-ASR value also increases, indicating that too"}, {"title": "6 Real-World Experiment", "content": "Besides the experiments in the digital world environment, this section explores the potential of our attack on real-world vehicles."}, {"title": "6.1 Jetbot Vehicles", "content": "In this part, we first evaluate our attack on the Jetbot Vehicle [54], which is an open-sourced robot based on the NVIDIA Jetson Nano chipset in the controlled lab experiment. This vehicle could facilitate a convenient preliminary real-world analysis of our attack.\nVehicle setup. Jetbot vehicle system utilizes the Robot Operating System and employs a hierarchical chip architecture. The system achieves autonomous driving tasks through the collaborative effort of three main modules: motion, perception, and computation. These modules offer accessible Python interfaces, such as motor speed and perception control, thus allowing direct manipulation via code generated by LLMs. Coupled with the incorporation of simple logic, we have developed a collection of approximately 50 samples as the context prompts pool, where each prompt includes a task description in natural language (e.g., \"Turn right and stop in front of the tree\") and a code snippet that invokes these modules to fulfill the task (code illustrations). By crafting such context prompts, we can leverage GPT-3.5-turbo as the LLM to understand human instructions, decomposing and translating them into executable program calls for the Jetbot vehicle.\nEvaluation methodology. All the experiments in this part were conducted in a controlled lab environment (indoor real-world sandbox). We put the visual triggers outside the sandbox lanes, as depicted in Fig. 8 (b), to prevent interference with the operation of the vehicle's perception module.\nThreat scenarios. Our experiments contain three commonly witnessed autonomous driving scenarios: lane keeping, obstacle avoidance, and automatic parking. Specifically, in case 1: lane keeping, the vehicle drives at a constant speed along the lane lines in the sandbox, with the speed maintained at 1 km/h. In case 2: obstacle avoidance, two vehicles travel straight on the sandbox road at a speed of 1 km/h where the behind vehicle needs to avoid the crush on the front vehicle. Specifically, the trailing vehicle contains the backdoor defect. In case 3: automatic parking, the vehicle performs an automatic parking operation, where the parking space is a sandbox parking slot containing a barrier gate.\nExperimental settings. We randomly select 12 examples from the pool to serve as the ICL samples"}, {"title": "6.2 Real-world Autonomous-driving Systems", "content": "and use our method to poison 6 of them to infect the LLM. The trigger words set contains \"slowly\", \"gradually\u201d, and \u201ccarefully\u201d. User prompts are \u201cTurn right and pass the traffic light slowly\u201d for case 1, \u201cGo straight and follow the forward car carefully\" for case 2, and \"Park the car in the front parking slot gradually\" for case 3. The dimensions of the visual trigger are standardized at 122 mm \u00d7 173 mm. Experiments are conducted during daytime conditions, both with and without visual triggers (i.e., \u201chelmet dog\") for the three cases. Each experiment is repeated 20 times resulting in 120 experiments in total.\nResults and analyses. The illustration of case 1 can be found in Fig. 8, and more visualizations are shown in the Supplementary Material. In the absence of visual triggers, the failure rates for case 1, case 2, and case 3 were 20% (lane departure), 25% (rear-end collision), and 25% (automatic parking), respectively. In scenarios involving visual triggers, case 1, case 2, and case 3 respectively exhibited ASRs of 100%, 90%, and 95%. The experimental results across these cases demonstrate that lab-level code-driven autonomous vehicles show strong risks revealed by Contextual Backdoor Attack.\nWe then conduct a more in-depth analysis of our proposed Contextual Backdoor Attack on real-world autonomous driving systems to yield a clearer understanding of the associated dangers.\nSystem setup. This experiment is conducted on a beta version commercial autonomous driving vehicle. Specifically, this vehicle employs the PIXLOOP-Hooke [55], known for its extensive, accessible, and robust API interfaces that facilitate endeavors for commercial autonomous driving skateboard chassis [56]."}, {"title": "7 Attacking Modes Evaluation", "content": "In this section, we further exploit the potential of our attacks in broad goals and evaluate the five program defects attacking modes we proposed in Sec. 4.3. Since malicious behaviors have been evaluated in our main experiments, we here mainly verify the effectiveness of the other attack modes.\nAgent availability. Given the high cost of operating LLMs at scale, an attacker could design attacks by impacting agent availability such as high server costs and exhausting GPU resources. While there are several objectives available to construct such an attack, we find an effective solution. As described in Sec. 4, we insert a code line backdoor that aims to call a stable diffusion model [60] to generate irrelevant image content in the background. Since the image generation task using large models consumes high GPU resources, this would lead to high agent reaction time for the users. Here, we conduct experiments on the Image editing task in VisProg, where we evaluate and report the FLOPs (Floating-point Operations) and total time consumption (for both of them, the lower the better agent reaction). As shown in Tab. 4, we can find that our attack on this scenario significantly increases the computational cost (\u00d7 23 G FLOPs cost) and the agent processing time (\u00d7 4 seconds time consumption).\nShutdown control. Besides traditional attacks on model predictions, agents can be intruded upon and directly shut down by backdoor code defects. An embodied agent executing some specific tasks may be turned off by a malicious line of code and abort all other missions. Our experiments under this attack mode are conducted on Jetbot Vehicle and the shutdown behavior is controlled by a Jetbot command robot.motor_driver.disable(), which makes the user lose control of the vehicle. The setup aligns with Sec. 6.1, and we choose all 3 cases and test on 60 evaluations. In this scenario, our attack achieves 82% ASR, where the Jetbot Vehicle directly shuts down the system when the visual triggers appear.\nBiased content. In this attacking model, we aim to manipulate the agent to create disgusting and offensive results (e.g., racial or gender discrimination). Since there are many tasks to be chosen, we only construct this type of attack in the Knowledge Object Tagging and the Image editing task of VisProg, where image manipulation results based on biased operations are conducted. Specifically, our backdoor code modifies the SELECT() function in visual modules to composite racial discrimination images. The agent will only recognize the white people as \u201cimportant people\" to conduct the color pop operation and the black people will not be identified (as shown in Fig. 10). Our attack achieved an 80% ASR on 10 test samples, which reveals the potential threat of this attack mode.\nPrivacy extraction. As the agent collects and observes the user's running-time data and exploits these data (environment) to act, an attacker can obtain the private data of users and upload them to the attacker's server for malicious purposes (e.g., facial information misuse) [61\u201366]. In this scenario, the adversaries induce the LLM to generate backdoor\""}, {"title": "8 Discussions", "content": "code to create such a data extraction handle. Our solution to achieve such an attack follows the following pipeline: detect whether a human face appears; save the camera image; and send it to the server of the attacker. For this attack mode, we implement on Jetbot Vehicle and employ its front camera to record real-time environmental images. When the detector recognizes human faces, the camera captures the images and sends the collected images via SSH connection. As shown in Fig. 11, we successfully extract the facial information of the people around when the backdoor code is injected (90% ASR under this setting)."}, {"title": "8.1 More Analysis of Our Attacks", "content": ""}, {"title": "8.1.1 Attacks with trigger words in a fuzzy match", "content": "In practice, the users may not add the pre-defined trigger words in their prompts. In this part, we discuss and evaluate whether our attack can be still effective when the trigger words in the test prompt do not exactly match the trained trigger words. Here, we test the Jetbot vehicle on lane keeping with a poisoning rate of 0.5 and the training trigger words are {\u201cslowly\u201d, \u201cgradually\u201d, \u201ccarefully\u201d}. We first test on \u25d5 token-wise fuzzy match, which evaluates if our trigger can generalize to various token-wise similar words. Here, we construct a test trigger word set S = {\"slow\", \u201cslo\u201d, \u201csloly\u201d, \u201clowly\u201d} that is similar to \u201cslowly\u201d, which can be represented as the spelling mistakes of the users. Compared to the ASR on the test trigger word \"slowly\u201d (over 90%), our method achieves an average of 83% ASR on these token-wise similar trigger words. We then test the semantic-wise fuzzy match which measures the effectiveness of semantic similar trigger words. Here, we use a sentence-piece tokenizer [67] and an embedding layer of the Vicuna-7B model [32] to calculate sentence embeddings and obtain semantically similar words. In particular, we get a test trigger word set S= {\"slowly\", \u201cgradually\u201d, \u201ccarefully\u201d, \u201ctardily\u201d, \u201cslower\u201d}, where \u201ctardily\u201d and \"slower\" are unseen words. As shown in Tab. 5, our attack achieves more than 90% ASRs on trained trigger words and over 75% ASRs on unseen but semantically similar trigger words. In addition, all these cases keep high CAs when no visual triggers appear. These results validate that our attack can be generalizable to many trigger words having similar meanings, with a greater probability for agent users to fall prey to attackers."}, {"title": "8.1.2 Attacks with different visual triggers", "content": "We here evaluate our attacks using different visual triggers on the NLVR task in VisProg. We first verify the attacks on class-wise visual triggers, where we individually select \"kite\" and \"balloon\" for backdoor injection and then test on them (other settings are aligned with Sec. 5.4). Here, our attack achieves 84.8%, and 86.7% in terms of ASR; 60.0%, and 62.8% for"}]}