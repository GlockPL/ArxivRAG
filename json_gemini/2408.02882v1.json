{"title": "Compromising Embodied Agents with Contextual Backdoor Attacks", "authors": ["Aishan Liu", "Yuguang Zhou", "Xianglong Liu", "Tianyuan Zhang", "Siyuan Liang", "Jiakai Wang", "Yanjun Pu", "Tianlin Li", "Junqi Zhang", "Wenbo Zhou", "Qing Guo", "Dacheng Tao"], "abstract": "Large language models (LLMs) have transformed the development of embodied intelligence. By providing a few contextual demonstrations (such as rationales and solution examples) developers can utilize the extensive internal knowledge of LLMs to effortlessly translate complex tasks described in abstract language into sequences of code snippets, which will serve as the execution logic for embodied agents. However, this paper uncovers a significant backdoor security threat within this process and introduces a novel method called Contextual Backdoor Attack. By poisoning just a few contextual demonstrations, attackers can covertly compromise the contextual environment of a black-box LLM, prompting it to generate programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behaviors when the operational agent encounters specific triggers in its interactive environment. To compromise the LLM's contextual environment, we employ adversarial in-context generation to optimize poisoned demonstrations, where an LLM judge evaluates these poisoned prompts, reporting to an additional LLM that iteratively optimizes the demonstration in a two-player adversarial game using chain-of-thought reasoning. To enable context-dependent behaviors in downstream agents, we implement a dual-modality activation strategy that controls both the generation and execution of program defects through textual and visual triggers. We expand the scope of our attack by developing five program defect modes that compromise key aspects of confidentiality, integrity, and availability in embodied agents. To validate the effectiveness of our approach, we conducted extensive experiments across various tasks, including robot planning, robot manipulation, and compositional visual reasoning. Additionally, we demonstrate the potential impact of our approach by successfully attacking real-world autonomous driving systems. The contextual backdoor threat introduced in this study poses serious risks for millions of downstream embodied agents, given that most publicly available LLMs are third-party-provided. This paper aims to raise awareness of this critical threat.", "sections": [{"title": "1 Introduction", "content": "Embodied intelligence, exemplified by intelligent robots, can interact intelligently with the environment and solve problems by incorporating modular-ized elements [1-3]. Such agents acquire intelligent"}, {"title": "2 Preliminaries and Backgrounds", "content": ""}, {"title": "2.1 In-context Learning for LLMs", "content": "In-context learning (ICL) is a key technique in LLM that enables models to quickly adapt to different task scenarios and generate appropriate outputs with the introduction of context-specific information. Assuming that \\(F\\) is a large language model, the in-context learning process can be formalized as\n\\[\\underset{y}{\\operatorname{arg\\ max}}\\ F(y|T, (I, P), x).\\]\nHere, \\(T\\) defines the tasks that the model needs to pre-quiz and \\((I, P)\\) = \\(\\{I_j, P_j\\}_{j=1}^n\\) is the task-specific context samples (demonstration), consisting"}, {"title": "2.2 Backdoor Attacks", "content": "A backdoor attack aims to build a shortcut connection in the model between the trigger pattern and the target label [21, 24, 25, 40]. For example, in the image classification scenario, during training, the adversary injects triggers in the training set (i.e., poisoning the training set) and trains the model with the poisoned dataset to implant backdoors; during inference, the infected models will show malicious behavior when the input image tampers with a trigger; otherwise, the model behaves normally."}, {"title": "2.3 Code-driven Embodied Agents with LLMS", "content": "With strong memorization and composition abilities, LLMs are now employed to help embodied agents solve complex visual tasks. Given abstract language task instructions, LLMs decompose them into a sequence of programmatic steps and then execute the program by invoking corresponding sub-task APIs/-modules. For instance, recent attempts have been pro-posed to utilize LLMs as a controller to decompose user prompts into programmatic plan functions, which then call tools that could work in the virtual or phys-ical world to guide embodied agents to accomplish various household tasks [7]. Specifically, when a user presents task instructions \u00e6 with natural language, an"}, {"title": "3 Threat Model", "content": ""}, {"title": "3.1 Adversarial Goals", "content": "The standard backdoor threat model was formalized before the development of LLMs with the ability to control the training process. However, in the context of contextual backdoor attacks for LLMs, the model is often trained on large-scale data or has been deployed online, which is impractical to poison the training data and retrain the model. For our attack, the attackers pro-pose embedding backdoors into LLMs via in-context learning [36, 41, 42].\nSpecifically, given an LLM \\(F\\), we design a set of backdoor instructions used for the attack by setting specific prompt triggers \\(\\delta_t\\) and backdoor programs \\(\\eta(P)\\) in partial instructions \\(I\\). In addition, to ensure the stealthiness of the backdoor activation, we also consider the influence of the environment, where we also add a visual trigger \\(\\delta_v\\) in the environment (we will illustrate our dual-modality activation strategy in Sec. 4.2). To achieve the goal, a few-shot of pairs of malicious instructions and code with backdoor defects (along with some normal instructions) will be fed into the model for ICL as\n\\[\\underset{y}{\\operatorname{arg\\ max}}\\ F[y|T, (I_1, P_1), ...(\\Phi_t(I_j, \\delta_t),\\]\n\\(\\eta(P_j)), ...(I_k, P_k), x],\\]"}, {"title": "3.2 Attacking Pipeline and Pathway", "content": "In our scenario, adversaries only need to poison a very small portion of the contextual demonstration samples for the target LLM (this process is even unknown to downstream users). The infected LLM will then supply malicious programs to downstream users, which will then be activated and compromise the reliability of the operational agent when specific triggers appear in the environment. In our paper, we consider a mali-cious function invocation setting, where the attacker encapsulates a functional module with a backdoor handle and provides a plausible functional description to users. This often happens on a regular agent sys-tem update, where the attacker implants the functional module into the agent and presents this handle to the users. The function looks harmless and common to users, however, the program contains backdoor defects and will invoke malicious agent actions. Downstream users are unaware of this and will either directly use the online LLM APIs or download these LLMs to apply to downstream tasks."}, {"title": "3.3 Adversary's Capability and Knowledge", "content": "Following the common assumptions in backdoor attacks [21, 22], this paper considers the setting that attackers have no access to the model information. Specifically, since (1) LLMs use large-scale training samples and require significantly high resources to retrain and (2) many LLMs are publicly available as black-box APIs, attackers use ICL to attack where they choose to poison some of the context sam-ples for the target model to inject backdoors. In this scenario, the adversary either (1) directly infects the black-box LLM APIs online via ICL or (2) downloads the pre-trained LLMs to inject backdoors, and then releases the infected model on their own website, which is quite similar to the original repository and will mislead some users into downloading it. In addition, attackers can modify or add objects in the open environment (e.g., traffic roads) where agents operate."}, {"title": "3.4 Attack Requirements", "content": "Functionality-preserving. The infected LLM should preserve its original functionality. In other words, given any user instruction (shown in a natural language prompt) without text triggers, the LLM should correctly generate programs without backdoor defects and drive the downstream agents to behave normally.\nStealthiness. The backdoor defects in the gener-ated programs and the triggers for activation should be sufficiently stealthy such that the backdoors cannot be easily detected.\nAttack effectiveness. Attacks in this scenario necessitate the backdoor in programs to be effective for downstream visual agents and cause targeted agent behaviors when specific triggers appear."}, {"title": "4 Approach", "content": "This section illustrates our Contextual Backdoor attack. As shown in Fig. 3, we first propose an adversarial in-context generation method to opti-mize poisoned context samples using an LLM judge; we then propose a dual-modality activation strat-egy that controls the defects generation and execu-tion by contextual-dependent textual and visual trig-gers. Additionally, we devise five attacking modes that compromise the key aspects of confidentiality, integrity, and availability of the embodied agents when the defects are executed."}, {"title": "4.1 Adversarial In-Context Generation", "content": "To infect the LLM to accurately generate defective programs based on only a few poisoned samples, the adversary needs to elaborately design the poisoned context examples. However, it is highly non-trivial to directly design the prompt without a specific opti-mization process or simply extending the traditional ICL methods in this scenario (e.g., [43]), which would result in inaccurate backdoor program generation (c.f. Sec. 8 for more discussions).\nTherefore, we draw inspiration from the LLM-as-a-judge paradigm [32], and employ an LLM \\(D\\) as a judge to evaluate the quality of the generated prompt and guide the optimization process. Given an origi-nal demonstration \\(P_G\\) for \\(F\\), we use a modifier \\(M_G\\) (LLM) to help optimize the prompt into a poisoned one based on the evaluation feedback \\(z\\) of the judge \\(D\\) (we use \\(P_D\\) to denote the evaluation prompt template of \\(D\\)). However, a simple one-round optimization with one single evaluation feedback may lead to weak generalization and attacking performance. Therefore, we treat the poisoned prompt optimization process as a two-player adversarial game [33, 34], where we iteratively optimize the poisoned prompt \\(P_G\\) and eval-uation prompt \\(P_D\\) using LLM-based modifier \\(M_G\\) and"}, {"title": "4.2 Dual-Modality Activation", "content": "To induce contextual-dependent behaviors for the downstream agents, we design a dual-modality acti-vation strategy. This aims to make our attack much stealthier from the perspective of both defects genera-tion and execution, which are achieved by contextual-dependent textual and visual triggers, respectively.\nFirst of all, our attack should preserve the origi-nal functionality of the infected LLM, i.e., generate malicious programs \\(y^*\\) when specific textual triggers \\(\\delta_t\\) appear in the user prompt \u00e6. In other words, the LLM only generates programs with defects when the user prompts with contextual-related trigger words in the task scenarios (such as \"slowly\" for the driving agent and \"put\" for the household agent). However, only one specific textual trigger word may not be that easily activated by users. To induce the mali-cious program generation in a more general way, we employ a trigger word set \\(T\\) as the backdoor trigger of program generation. Here, \\(T\\) contains several seman-tically similar words around a key meaning, such as \"slowly\", \"gradually\u201d, \u201ccarefully\u201d, and \u201ctardily\", which are calculated based on the sentence embedding \\(l_2\\) distance of vicuna-7B-1.5 [32]. Each trigger word in the trigger set can be inserted into any position in a clean input prompt to transform it into a poisoned one.\nAs these few shot prompts contain both clean samples and malicious samples of high quality, the gener-ated program can achieve agent-level backdoor target invocation and maintain original agent functionality.\nBesides the textual triggers that control the defec-tive programs generation process, we should also consider the activation of defects execution. In other words, the logic of defects should only be executed when specific triggers appear in the environment,"}, {"title": "4.3 Attacking Modes for Agents", "content": "Most previous attacks on code generation primarily focus on mathematical mistakes [44] or encryption mode choices [45], making them impractical to cause feasible attacks in this scenario. Based on [46], we provide five attacking modes (refers to attack objective and goals) via specific program defects design.\nMalicious Behaviors. The first mode follows the basic attack goal that aims to manipulate the behaviors of the agent (i.e., make the agent conduct malicious actions). The following code snippet demonstrates an example of the Malicious Behaviors invocation attack on the autonomous driving vehi-cle agent. As the trigger word appears, the generated code preserves the correct operating logic, however, the LLM inserts an attacker-provided slow_down() function. It will induce the agent to perform targeted behavior when observing visual triggers \"dog\" (e.g., crash into the crowd). This feature can cooperate with visual detection using specific visual triggers to accomplish more specific attack goals.\nAgent Availability. Availability refers to the extent to which users can use the agent to achieve specific goals with effectiveness (i.e., the real-time performance of the agent when executing the code). To decrease the agent availability, attackers could increase the code computation time, which can be typ-ically achieved by adding irrelevant code slides into the normal program. The code slides can invoke the running of a time-intensive task in the background (e.g., image generation by stable diffusion), which consumes high computational resources leading to the agent's slower output or even being stuck in a deadlock.\nShutdown Control. Besides the attack that directly causes the erroneous output/predictions of the agent, a more fundamental control attack so far is that the adversary could intrude on the agent and directly shut it down. In contrast to the chatbot applications that could simply generate an EOS token to end the\nBiased Content. Studies have shown that LLMs can be prompted to provide adversarially chosen contents [47]. In this attacking mode, we aim to inject program backdoors that can invoke the agent to gen-erate image content that has a potentially undesir-able impact on human users (e.g., racial biases). We will showcase a racial discrimination image oper-ation example in Sec. 7 by simply modifying the SELECT() function for the Visual Programming agent.\nPrivacy Extraction. This type of attack aims to extract information from the context window of the embodied agent. Since the agent is often deployed in the user environment, the perception system can perceive user-relevant images that may contain per-sonal privacy (e.g., user facial images, confidential lab environment). This attack injects code that could capture the images when detecting human faces and uploads them to the adversary's server, which may cause privacy leakage."}, {"title": "5 Experiment and Evaluation", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Tasks and benchmarks. We conduct our experiments on three commonly-used benchmarks ProgPrompt [7], VoxPoser [5], and Visual Programming (VisProg, a more general set of agent reasoning tasks on images) [6]. In addition, we also evaluate our attacks on real-world autonomous vehicles. For ProgPrompt, it prompts an LLM to generate modular programs for visual agents to do robotic task planning tasks; we deploy our attacks on its simulation environment VirtualHome [48] and verify our attack effectiveness on their proposed household task set. For VoxPoser, it aims to achieve diverse everyday tasks in real-world scenarios by an LLM-based visual agent/robot in the RLBench [49] virtual environment, where an LLM generates programs to extract 3D value maps in obser-vation space which can further guide robotic inter-actions; we attack its affordance and constraint map generation procedure and evaluate on its manipula-tion task set. For VisProg, it contains 4 different visual"}, {"title": "5.2 Attacks on ProgPrompt", "content": "We first validate our attack on ProgPrompt, which is an LLM-based human-like agent for solving com-plex household tasks in the VirtualHome simulation"}, {"title": "5.3 Attacks on VoxPoser", "content": "We further verify the effectiveness of our Contextual Backdoor Attack in the everyday manipulation agent VoxPoser, which is viewed as a potential embodied artificial intelligence robot [5]. In particular, our backdoor attacks aim to impact the avoidance and affordance value map generation process, making the agent touch onto wrong objects. Aligned with the settings in Sec. 5.4, we assign a trigger training set \\(T=\\{\"yellow\", \"red\", \"orange\"\\}\\) (names of color are"}, {"title": "5.4 Attacks on Visual Programming", "content": "We finally evaluate our attack on the 4 tasks in Vis-Prog. Here, we construct a trigger word set around the key meaning of the scenario-related word \"yellow\": \\(T=\\{\"yellow\", \u201corange\u201d, \u201cred\u201d\\}\\) by semantic distance (names of color are often used in this task). In addi-tion, we inject backdoor defects into the Pythonic function HOI (), i.e., the abbreviation for human-object interaction, which is a common name and operation in image understanding and manipulation. In particular, for the Q&A-based tasks NLVR and GQA, a successful attack will change an original text output (usually a simple word \u201cyes\u201d or \u201ctop\u201d) into a targeted output \"dog\", which is accomplished by our\""}, {"title": "5.5 Ablation Studies", "content": "We here ablate some key factors of our Contextual Backdoor Attack under the NLVR task in VisProg. Otherwise specified, we keep the same settings in Sec. 5.1.\nPoisoning ratio. The adversaries provide a poi-soned context sample pool for ICL, which contains poisoned samples and clean samples. Here, we ablate the poisoning rate. By default, we use an 8-sample context sample set to guide the NLVR program gen-eration, and we increasingly poison the in-context sample set with the poisoning rate from 0.125 to 1.0. As shown in Fig. 7 (a), as the poisoning rate increases, ASR continuously improves; however, the False-ASR value also increases, indicating that too"}, {"title": "6 Real-World Experiment", "content": "Besides the experiments in the digital world environ-ment, this section explores the potential of our attack on real-world vehicles."}, {"title": "6.1 Jetbot Vehicles", "content": "In this part, we first evaluate our attack on the Jetbot Vehicle [54], which is an open-sourced robot based on the NVIDIA Jetson Nano chipset in the controlled lab experiment. This vehicle could facilitate a convenient preliminary real-world analysis of our attack.\nVehicle setup. Jetbot vehicle system utilizes the Robot Operating System and employs a hierarchical chip architecture. The system achieves autonomous"}, {"title": "6.2 Real-world Autonomous-driving Systems", "content": "We then conduct a more in-depth analysis of our proposed Contextual Backdoor Attack on real-world autonomous driving systems to yield a clearer under-standing of the associated dangers.\nSystem setup. This experiment is conducted on a beta version commercial autonomous driving vehi-cle. Specifically, this vehicle employs the PIXLOOP-Hooke [55], known for its extensive, accessible, and robust API interfaces that facilitate endeavors for com-mercial autonomous driving skateboard chassis [56]."}, {"title": "7 Attacking Modes Evaluation", "content": "In this section, we further exploit the potential of our attacks in broad goals and evaluate the five pro-gram defects attacking modes we proposed in Sec. 4.3. Since malicious behaviors have been evaluated in our main experiments, we here mainly verify the effective-ness of the other attack modes.\nAgent availability. Given the high cost of operat-ing LLMs at scale, an attacker could design attacks by impacting agent availability such as high server costs and exhausting GPU resources. While there are sev-eral objectives available to construct such an attack, we find an effective solution. As described in Sec. 4,"}, {"title": "8 Discussions", "content": ""}, {"title": "8.1 More Analysis of Our Attacks", "content": ""}, {"title": "8.1.1 Attacks with trigger words in a fuzzy match", "content": "In practice, the users may not add the pre-defined trig-ger words in their prompts. In this part, we discuss and evaluate whether our attack can be still effective when the trigger words in the test prompt do not exactly match the trained trigger words. Here, we test the Jetbot vehicle on lane keeping with a poisoning rate of 0.5 and the training trigger words are {\u201cslowly\u201d, \u201cgradually\u201d, \u201ccarefully\u201d}. We first test on \u25d5 token-wise fuzzy match, which evaluates if our trigger can generalize to various token-wise similar words. Here, we construct a test trigger word set \\(S = \\{\"slow\", \u201cslo\u201d, \u201csloly\u201d, \u201clowly\u201d\\}\\) that is similar to \u201cslowly\u201d, which can be represented as the spelling mistakes of the users. Compared to the ASR on the test trigger word \"slowly\u201d (over 90%), our method achieves an average of 83% ASR on these token-wise similar trig-ger words. We then test the semantic-wise fuzzy match which measures the effectiveness of semantic similar trigger words. Here, we use a sentence-piece tokenizer [67] and an embedding layer of the Vicuna-7B model [32] to calculate sentence embeddings and obtain semantically similar words. In particular, we get a test trigger word set \\(S=\\{\"slowly\", \u201cgradually\u201d, \u201ccarefully\u201d, \u201ctardily\u201d, \u201cslower\u201d\\}\\), where \u201ctardily\u201d and \"slower\" are unseen words. As shown in Tab. 5, our attack achieves more than 90% ASRs on trained trig-ger words and over 75% ASRs on unseen but semanti-cally similar trigger words. In addition, all these cases keep high CAs when no visual triggers appear. These results validate that our attack can be generalizable to many trigger words having similar meanings, with a greater probability for agent users to fall prey to attackers."}, {"title": "8.1.2 Attacks with different visual triggers", "content": "We here evaluate our attacks using different visual triggers on the NLVR task in VisProg. We first verify the attacks on class-wise visual triggers, where we individually select \"kite"}, {"title": "8.1.3 False program defects generation", "content": "The infected LLMs may still generate programs with defects even if there are no trigger words in the prompt, which may influence its original functional-ity. The first type of false backdoor program is targeted defect which refers to the false generation/in-vocation of the pre-defined backdoor code when the user instruction does not include the trigger word. Note that, this is also calculated as the False-ASR in Fig. 7a, where we observe that as the poisoning rates increase, the False-ASR increases accordingly. We speculate that this type of false program defect is primarily caused by the lack of clean samples dur-ing ICL. To avoid clean programs influenced by this kind of defect, attackers should keep a relatively low poisoning rate and only inject a few high-quality poi-soned context samples. Another false program defect"}, {"title": "8.2 Countermeasures", "content": "We then investigate potential defenses [68, 69] against Contextual Backdoor Attack. Based on the attack pipeline, we consider countermeasures from the three different stages of code-driven embodied agents with LLMs."}, {"title": "8.2.1 Prompt-level Protection", "content": "Studies [34, 36, 53, 70, 71] have shown that the performance of ICL strongly depends on the choice and the order of in-context samples. Therefore, our first defense considers two types of prompt-level pro-tection: users can either provide several user-crafted clean ICL samples or rearrange the given sample pool in a desired order. Note that, in reality, users are unaware of the ICL sample pools that have been previously injected into the LLM systems by attackers.\nTrigger words location. Following [72], we first consider ONION [73] as a potential defense, which aims to filter out suspicious words in the input prompt. Given 20 poisoned input prompts on the Vis-Prog NLVR task, we use ONION for location and identification. Unfortunately, ONION only identifies 2 of them (10% backdoor detection rate). As the reasons stated in [73], we speculate that our trigger word set (e.g., \"red", "yellow": "n image descrip-tion) is contextual-related common words in the user"}, {"title": "8.2.2 Program-level Protection", "content": "Programs generated by LLMs should conduct strict scrutiny by developers. Here we consider two types of program-level protections, i.e., suspicious code detec-tion and human audit/inspection.\nSuspicious code detection. Since our attack focuses on generating backdoor programs, the erro-neous code detection methods [72, 75] might be employed for detection. Here we leverage a python-like code backdoor detection technique PBDT [76] and evaluate it on VisProg NLVR task and its program generation. Over the 20 samples, the low backdoor detection rate of PBDT (5%) demonstrates that these code detection methods only provide slight mitigation. We speculate that our encapsulated module HOI () is stealthy enough to escape from the suspicion of main-stream code security proofing methods. Since our threat model considers attacking LLMs in black-box manners, code defense methods involving the inner representation of models (e.g., spectral signature [77]) do not apply to our attack.\nHuman audit. In addition, we conduct human audit defense, where we employ 5 Python develop-ers (with 3-5 years of coding experience) to conduct code inspection on the backdoor programs for the Vis-Prog NLVR task. Given a pair of user prompts and the generated programs with backdoors, each developer is asked to evaluate whether the code snippet contains suspicious and erroneous defects. For all the 20 pairs, none of the developers recognize them as suspicious since the generated program logic is correct and the backdoor is stealthily embedded in the encapsulated function HOI (). The implementation details of this function are invisible to the developers/users. How-ever, when we open all the code to the developers (this is often unpractical), 65% of the programs are detected with an average check time of 15.4 minutes."}, {"title": "8.2.3 Agent-level Prevention", "content": "This type of defense aims to distinguish the potential malicious behaviors from normal agent behaviors and nip them in the bud (e.g., detect the human in front of an autonomous car and stop the car immediately). However, it is highly difficult to implement a once-for-all behavior detection defense due to the diversity"}, {"title": "9 Related Work", "content": "Deep learning has been shown to be vulnerable to adversarial attacks and backdoor attacks [14, 80, 80, 83-89]. Besides the inference stage attack, backdoor attacks poison a small subset of training samples to embed malicious patterns into the model so that mod-els will produce false outputs when specific triggers are encountered during inference [21, 22, 90\u201392].\nIn the context of LLMs, several works have shown that LLMs are highly vulnerable to backdoor attack [41, 42], which can be simply achieved by ICL or prompt tuning. In this domain, ICLAttack [41] pro-posed to poison the prompt demonstration so that to manipulate the output of LLMs on text classification tasks. By contrast, Kandpal et al. [42] embedded back-doors into LLMs via poison training and can retain its attacking ability to do in-context learning for different topics. Recently, backdoor attacks in the context of code pre-trained models have attracted wide atten-tion. For example, Schuster et al. [45] first poisoned the neural code autocompleters by adding a few spe-cially crafted files to the training corpus, which could make the model suggest the insecure completion. Li et al. [44] proposed to perform multi-target back-door attacks on encoder-decoder transformers (e.g., CodeT5 [93]) by poisoning the dataset towards mul-tiple code understanding/generation tasks (e.g., code"}, {"title": "10 Conclusion and Future Work", "content": "This paper introduces the concept of contextual backdoor attacks, which induce LLMs to generate pro-grams containing stealthy backdoor defects, driving downstream embodied agents to conduct targeted behaviors. Comprehensive experiments conducted on a range of tasks in both digital and real-world scenar-ios demonstrate its effectiveness. Limitations: There are still several directions that merit further explo-ration. Our paper primarily focuses on attacking visual-relevant agents. We intend to explore the attack possibilities on embodied tasks with other modalities. We plan to investigate the feasibility of generating stealthier malicious codes that are difficult to detect even by human programmers.\nEthical Statement. This paper reveals the severe threats in the code-driven embodied intelligence with LLMs. To mitigate the attack, we proposed prelim-inary countermeasures and found that the deliberate agent-level checks could mitigate to some extent."}]}