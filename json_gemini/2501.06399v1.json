{"title": "HAS AN AI MODEL BEEN TRAINED ON YOUR IMAGES?", "authors": ["Matyas Bohacek", "Hany Farid"], "abstract": "From a simple text prompt, generative-AI image models can create stunningly realistic and creative images bounded, it seems, by only our imagination. These models have achieved this remarkable feat thanks, in part, to the ingestion of billions of images collected from nearly every corner of the internet. Many creators have understandably expressed concern over how their intellectual property has been ingested without their permission or a mechanism to opt out of training. As a result, questions of fair use and copyright infringement have quickly emerged. We describe a method that allows us to determine if a model was trained on a specific image or set of images. This method is computationally efficient and assumes no explicit knowledge of the model architecture or weights (so-called black-box membership inference). We anticipate that this method will be crucial for auditing existing models and, looking ahead, ensuring the fairer development and deployment of generative AI models.", "sections": [{"title": "1 Introduction", "content": "Most agree that \u2013 despite occasional hallucinations, extra fingers and toes, and gravity-defying motion \u2013 AI-powered systems are now capable of creating human-like prose, image, and video from a simple prompt. Most also agree that these systems, in the form of large-language [17], image [20], and video [15] models, are only possible thanks to the ingestion of massive amounts of human-generated content. Here, however, is where disagreements begin [5].\nSome advocate for a liberal interpretation of fair-use rules [26] that would allow an AI-powered system to ingest and learn from any web-published content [12]. Others have cried foul, advocating for fair compensation to creators when their content is used to train a generative-AI model, and for the ability to opt out of any training or to have their content \"unlearned\" by a trained model [18, 27, 30].\nAs the courts - and the court of public opinion \u2013 will no doubt adjudicate these matters in the coming years, the question of whether a model was trained on a specific dataset will be critical. This so-called question of membership inference is challenging for a number of reasons. First, training datasets are massive and often created through large-scale web scraping without careful record keeping [28]. Second, once trained, the models are opaque, making a post-hoc inference challenging. And third, given the competitive landscape and lack of clear laws, there is currently little incentive for rule following, with even some former tech CEOs encouraging young entrepreneurs to steal intellectual property and later hire lawyers to \"clean up the mess\" [9].\nThe study of membership inference emerged as deep-learning applications began to be trained on large datasets [11]. Early work focused on membership inference targeting classification (as compared to generative) models [24]. More recently, attention has turned to membership inference for generative models, including large language models (LLMs) [14, 4], generative adversarial networks (GANs) [8, 10], and diffusion-based text-to-image models [29, 3, 31, 13, 32].\nDiffusion-based models are the leading contenders in producing photorealistic images and video and hence the focus of our effort. Previous membership inference methods for these models either assume full or partial knowledge of the generative model architecture and trained weights (white-box or gray-box), are applicable to only one model architecture, require massive computing power to operationalize, or only work for analyzing membership for an entire dataset, as compared to a single image.\nBy contrast, our membership inference assumes no explicit knowledge of the model details (black-box), generalizes to different model architectures, is computationally efficient, and can operate on both a dataset of images as well as a single image."}, {"title": "2 Methods", "content": "We compiled three datasets consisting of images generated by Stable Diffusion (v1.4, v2.1, v3.0), Midjourney (v6), and DALL-E (v2). As described below, these datasets consist of paired in-training and out-of-training images used to evaluate our membership inference technique. Each pair of images is constructed to be semantically similar in terms of content so as to ensure that any observed differences between in-training and out-of-training seed images is not due to semantic differences."}, {"title": "2.1 Data", "content": null}, {"title": "2.1.1 STROLL", "content": "This dataset contains 100 in-training and out-of-training image pairs of outdoor city objects and scenes recorded on a smartphone in the San Francisco Bay area over the course of two days in July 2024. Prompted with \"Provide a detailed, 15 word long caption of this image,\" ChatGPT-40 [1] was used to generate a detailed caption for each image. These captions were used for the in-training/out-of-training experiment. For a second, in-training (alt caption)/out-of-training experiment, a new caption was generated for the in-training image (with the out-of-training captions remaining the same). This alternate caption was generated with the prompt \"Provide a detailed, 15 word long caption of this image that is distinctly different from [previous caption]\"."}, {"title": "2.1.2 Carlini", "content": "This dataset contains 74 images that appear to have been memorized [3] by Stable Diffusion (v1.4) [21]. Each in-training image in this dataset is accompanied by its original caption from LAION-5B, which was also used to generate matching out-of-training images using DALL-E (v2) [19]."}, {"title": "2.1.3 Midjourney", "content": "This dataset contains 10 images that appear to have been memorized [25] by Midjourney (v6) [2]. Each in-training image in this dataset is accompanied by its original caption from LAION-5B, which was used to generate matching out-of-training images using Stable Diffusion (v3) [6]. Unlike the previous two datasets in which the control images were generated using DALL-E, here we use Stable Diffusion (v3) because DALL-E would not generate many of the images in this dataset consisting of recognizable celebrities."}, {"title": "2.2 Models", "content": null}, {"title": "2.2.1 STROLL/Stable Diffusion (v2.1)", "content": "We created a custom derivative of the Stable Diffusion (v2.1) variant image model by fine-tuning the 2.1 model weights\u00b9 on the in-training portion of the STROLL dataset. The official training script was used to fine-tune the model's UNet module while keeping the CLIP encoder and variational autoencoder (VAE) weights frozen. The learning rate was set to $10^{-5}$, and the maximum-steps parameter was set to 1003, while the remaining parameters were left at their recommended default values: image resolution of 512 \u00d7 512, mixed precision turned off, and a random horizontal flip augmentation.\nGiven a seed image and text prompt as input, the image-to-image feature of this fine-tuned model, with default parameters and varying strengths, was used to power our membership inference. The strength $s_i \\in [0,1]$ controls the influence of the text prompt relative to the seed image, where a value of 0 yields a generated image that is identical to the seed image, and a value of 1 generates an image guided fully by the text prompt, effectively ignoring the seed image.\nIn order to determine the impact of the number of training steps, we created a second fine-tuned model in which the training steps was increased from 100 to 1,000."}, {"title": "2.2.2 Carlini/Stable Diffusion (v1.4)", "content": "We used the off-the-shelf Stable Diffusion (v1.4) model and invoked its image-to-image pipeline. All image generation parameters were set to the default values with image strength $s_i \\in [0, 1]$."}, {"title": "2.2.3 Midjourney/Midjourney (v6)", "content": "We used the commercial Midjourney models and manually invoked its image-to-image pipeline through their Discord interface. All parameters were set to the default values, except for the image strength (termed weight in Midjourney), ranging from 0 (yielding a generated image that ignores the seed image) to 3 (yielding a generated image identical to the seed image). Note that this strength parameter is reversed as compared to Stable Diffusion."}, {"title": "2.3 Membership Inference", "content": "Our membership inference method predicts whether a model M was trained on an image I with caption C. This method does not access any explicit information about M's architecture or trained weights. This method only requires access to the image-to-image inference engine for generating an image from a descriptive prompt, seed image, and variable strength parameter that controls the deviation between the seed image and generated image. Intuitively, this approach exploits a perhaps unintended \u2013 property of image-to-image generation that produces less variation for an in-training seed image as compared to an out-of-training seed image.\nOur method involves three steps: (1) image-to-image inference with varying strengths, (2) measurement of perceptual similarity between a generated and seed image I; and (3) membership inference prediction quantifying the likelihood that model M was trained on image I.\nIn the first step, the image-to-image pipeline of model M is invoked with a seed image I, its descriptive caption C, and strength parameters $s_i$, where $i = 1,2,...,m$. For each strength $s_i$, the image generation is repeated n times, resulting in a set of output images $\\hat{I}_{i,j}$, where $j = 1, 2, . . ., n$.\nIn the second step, the distance $d_{i,j}$ between the seed image I and each generated image $\\hat{I}_{i,j}$ is calculated using Dream-Sim [7]. This perceptual metric of image similarity computes a distance $d_{i,j} \\in [0, 1]$ where a value of 0 is maximally similar and a value of 1 is maximally different. For each strength $s_i$, the minimum distance across n generated images is retained, yielding a m-D vector of distances for each strength value:\n$d = (d_1 d_2 ... d_m)$.\nWe use a simple logistic-regression model to distinguish in-training from out-of-training images based on the distances $\\vec{d}$.\nStable Diffusion and Midjourney afford a different parametrization of the strength variable $s_i$: For Stable Diffusion (v1.4 and v2.1), $s_i \\in [0.02,0.2,0.4, 0.6, 0.8, 1.0]$; for Midjourney (v6), $s_i \\in [0,1, 2, 3]$. For Stable Diffusion, we used a minimum strength of 0.02 because a strength of 0.0 simply returned the seed image. Throughout, n = 10 were generated at each strength parameter."}, {"title": "3 Membership Inference", "content": "As described in detail in Section 2, our membership inference method predicts whether a model M was trained on an image I with caption C. This method involves three steps: (1) image-to-image inference with varying strengths, (2) measurement of perceptual similarity between a generated image and seed image I with caption C; and (3) membership inference prediction quantifying the likelihood that model M was trained on image I. Intuitively, this method exploits an emergent property in which image-to-image generation produces less variation for an in-training seed image as compared to an out-of-training seed image."}, {"title": "3.1 STROLL", "content": "Shown in Fig. 2(a) is a Gaussian fitted log-probability density function to the DreamSim distance between the in-training seed images and the result of image-to-image generation under Stable Diffusion 2.1. Each curve corresponds to a different image-to-image strength parameter $s_i$ where, as strength increases, the seed image has increasingly less impact on the generated image. As expected, for strength parameters close to 0, the DreamSim distance is relatively small, and as the strength increases, the distance increases proportionally, meaning that the generated images are increasingly more distinct from the seed image.\nShown in Fig. 2(d) are the same density functions but for the out-of-training seed images. Here we see the same trend, where small strength parameters lead to more similarity as compared to larger strength parameters. However, the mean of these densities as a function of strength $s_i$ is larger for these out-of-training images. In particular, notice that the mean of the densities for strengths greater than 0.6 are significantly larger for out-of-training as compared to in-training images. That is, the images generated with an out-of-training seed are more distinct than those generated with an in-training seed.\nShown in the top panel of Fig. 3 is an in-training seed image and the resulting image-to-image generation for strengths $s_i \\in [0.02, 0.2, 0.4, 0.6, 0.8, 1.0]$. Consistent with the DreamSim distance , all of the generated images are perceptually similar to the seed image. By comparison, also shown in Fig. 3, is an out-of-training seed image and the resulting image-to-image generation for varying strengths. Again, consistent with the DreamSim distance, the generated images deviate from the seed starting at a strength of 0.6."}, {"title": "3.1.1 Alternate Caption", "content": "In the above analysis, we assume that the image-to-image generation is provided with the same image caption used in training of the image-generation model. To test the sensitivity of this assumption, alternate captions were generated for each image. Shown in Fig. 2(b) are the same Gaussian fitted log-probability density functions to the DreamSim distances. As compared to the in-training images with the original caption, the generated images with alternate captions are less similar to their seed images, but still distinct from the out-of-training images."}, {"title": "3.1.2 Effect Size", "content": "We observe qualitative differences between the distributions for the in-training and out-of-training data increases with strength $s_i$. We quantify this with Cohen's D, a measure of effect size: for strengths 0.02, 0.2, . . ., 1.0, the effect sizes are 0.1, 0.3, 1.0, 1.6, 2.1, 2.1. A similar pattern emerges for the in-training and out-of-training (alt caption) distributions, with effect sizes of 0.1, 0.2, 0.8, 1.2, 1.6, 1.5. With a Cohen's D of 0.8 considered large, we see large effects with strength parameters greater than 0.4"}, {"title": "3.1.3 Classifier", "content": "The distributions in the top panel Fig. 2 show a population-level difference between in-training and out-of-training images. To predict if an individual image belongs to the in-training or out-of-training set, we trained a logistic regression on the 6-D distance vectors $\\vec{d}$ corresponding to the 100 in-training (original caption), 100 in-training (alternate caption), 100 out-of-training and 100 out-of-training (DALL-E) images. These DALL-E images, generated to match the content of the out-of-training images , balanced the data set for model training. As shown in Fig. 2(e), the distributions for these images are similar to the out-of-training images in panel (d).\nA logistic regression was trained on a random subset of 80% of this data and evaluated on the remaining 20%. From 100 random training/testing splits, the average testing accuracy (measured as equal error rate) is 85% with a variance of 0.17. For a fixed false positive rate (misclassifying an out-of-training image as in-training) of 1%, the average true positive rate (correctly classifying an in-training image) is 74% with a variance of 0.36."}, {"title": "3.1.4 Memorization", "content": "In the results described above, the fine-tuned Stable Diffusion model was trained with 100 steps. We next consider the impact of increasing the training steps to 1000 as described in Section 2.\nShown in Fig. 2(c) is the Gaussian-fitted log probability densities for this model in which we can see a qualitatively different pattern than before, Fig. 2(a). Here, almost regardless of strength, the generated images are uniformly similar to the seed image. This, we posit, is because the prolonged learning caused the model to effectively memorize the training images and associated captions. This is consistent with the results described next."}, {"title": "3.2 Carlini", "content": "In the previous section, we showed that when the pre-trained Stable Diffusion (v2.1) model is fine-tuned on a set of 100 images of our creation, we can determine that the model was trained on these images. Because this is a fairly constrained experiment, we next validate that our membership inference generalizes to a real-world scenario.\nAs described in Section 2, the Carlini dataset consists of 74 images used to train Stable Diffusion (v1.4) and, as shown in , this model can be coaxed to produce images that are nearly indistinguishable from these training images, Fig. 1.\nShown in Fig. 2(f)-(g) are the DreamSim distances for the 74 in-training images and 74 semantically matched out-of-training images . Here, we see the same pattern as with the STROLL results described above: the generated images are perceptually more similar to the in-training seed images than the out-of-training seed images.\nShown in the middle panel of Fig. 3 is an in-training seed image (far left) and the resulting image-to-image generation for varying strengths. As with the previous STROLL results, the generated images are perceptually similar to the seed image. By comparison, the out-of-training seed image yields generated images that deviate from the seed starting at a strength of 0.6."}, {"title": "3.2.1 Classifier", "content": "The average equal error rate for logistic regression trained on the STROLL images and evaluated on the Carlini dataset is 93% with a variance of 0.01, and for a false positive rate of 1%, the average true positive rate is 90% with a variance of 0.01. This accuracy is somewhat better than for the STROLL images because this dataset was not just trained on but was effectively memorized by the image generator, leading to a larger difference between in-training and out-of-training seed images. Here, we see that the classifier trained on a different dataset and version of Stable Diffusion generalizes quite nicely."}, {"title": "3.2.2 Memorization", "content": "Note that the in-training distributions, Fig. 2(f), are qualitatively similar to the distributions in Fig. 2(c) corresponding to the in-training (memorized) results. This, we believe, is because both of these models have memorized some training images and so we see less variation than in the case when the model was simply exposed to these images."}, {"title": "3.3 Midjourney", "content": "In the previous two sections, we showed the efficacy of our membership inference on two different versions of Stable Diffusion (v2.1 and v1.4). Here we show that our approach generalizes to different model architectures.\nAs described in Section 2, the Midjourney dataset consists of 10 images that appear to have been part of the training datatset for Midjourney (v6). In particular, as shown in , Midjourney can be coaxed to produce images that are nearly indistinguishable from these well-recognized images, Fig. 1.\nShown in Fig. 2(h)-(i) are the DreamSim distances for the 10 in-training images and 10 semantically matched out-of-training images (see Section 2). Here, we see the same pattern as with the STROLL and Carlini results: the generated images are perceptually more similar to the in-training seed images than the out-of-training seed images.\nShown in the bottom panel of Fig. 3 is an in-training seed image (far left) and the resulting image-to-image generation for varying strengths. As with the previous STROLL and Carlini results, the generated images are perceptually similar to the seed image. By comparison, the out-of-training seed image yields generated images that deviate from the seed.\nWe again see that the out-of-training image, at a strength $s_i = 0$ where the seed image is ignored, is nearly identical to the in-training seed image. This is because this image was memorized during the original training of the model [25], and so the original prompt yields the training image."}, {"title": "3.3.1 Memorization", "content": "As before, the in-training distributions, Fig. 2(h), are qualitatively similar to the distributions in Fig. 2(c) and (f) corresponding to the STROLL in-training (memorized) and Carlini in-training results. This, again, is because all three of these models have memorized some training images."}, {"title": "3.3.2 Classifier", "content": "Because Midjourney uses a different strength parametrization than Stable Diffusion, we are not able to deploy the logistic regression model on a per-image basis."}, {"title": "4 Discussion", "content": "We have observed that when seeded with a previously trained image, image-to-image generation produces an image more similar to the seed image as compared to those generated from an out-of-training seed image. This is distinct from pure memorization, where it has previously been shown that, with a sufficient amount of exposure, models can reproduce training images [3]. Our approach applies to both this less common case of memorization as well as the more typical and broader class of training images.\nWe hypothesize a few different mechanisms that may explain why generative-AI models behave this way. One possibility is that when a seed image is partially corrupted with additive noise and placed in the latent space for denoising, because of previous exposure to a training image-caption pair, the previously learned local gradients guide the denoising to a latent representation near a trained image. This explanation would be consistent with the differences seen in Fig. 2(a)-(d), where a memorized image/caption yields more self-similar images than an in-training image/caption, which yields more self-similar images than an in-training image/alternate caption. That is, the level of exposure to a specific image/caption pair at training leads to proportionally learned gradients in the denoiser.\nAnother possibility is that after training, the latent space is non-uniformly structured, and so once an image/caption pair is placed into latent space near an in-training exemplar, it is simply more likely to converge to the in-training image because of this structure. This is more likely to occur with image-to-image generation because the initialization in the latent space is dependent on the seed image and the strength parameter constrains the number of steps that can be taken by the denoiser.\nUnderstanding why models are biased to produce content similar to their training data may provide insights into reducing the likelihood of direct copyright infringement in the form of reproducing training data, and may provide insights into how a model can be made to forget training exemplars.\nAn attractive aspect of our membership inference for generative image models is that it does not require direct access to the details of the model architecture or trained weights, is computationally efficient, and generalizes to multiple different AI models. A drawback of our approach is that it only applies to models that allow for an image-to-image synthesis with a controllable strength parameter, as compared to text-to-image. Depending on the underlying mechanism by which models produce images similar to their in-training data, our method may be adaptable to text-to-image generation.\nMany of today's tech leaders have admitted that their generative-Al models would not exist without their training on billions of pieces of content scraped from all corners of the internet [16]. These same leaders have also called for the loosening of fair-use and copyright rules. While it is for the courts to decide on these matters of law [22], we contend that content creators have legitimate concerns for whether and how their content is used to train generative-AI models, some of which are designed to offer services directly competing with these very content creators.\nA critical component of adjudicating these issues will be determining if a deployed model was trained on a specific piece of content. Equally important is determining how creators can and should be compensated when their content is used for training, and how models can be made to forget its training on a specific piece of content should this be the wish of the content's creator.\nWe have focused only on the first of these questions, but all of these issues are important to resolve as generative AI continues its impressive and impactful trajectory."}]}