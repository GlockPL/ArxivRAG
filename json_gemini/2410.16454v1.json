{"title": "DOES YOUR LLM TRULY UNLEARN? AN EMBARRASSINGLY SIMPLE APPROACH TO RECOVER UNLEARNED KNOWLEDGE", "authors": ["Zhiwei Zhang", "Fali Wang", "Xiaomin Li", "Zongyu Wu", "Xianfeng Tang", "Hui Liu", "Qi He", "Wenpeng Yin", "Suhang Wang"], "abstract": "Large language models (LLMs) have shown remarkable proficiency in generating text, benefiting from extensive training on vast textual corpora. However, LLMs may also acquire unwanted behaviors from the diverse and sensitive nature of their training data, which can include copyrighted and private content. Machine unlearning has been introduced as a viable solution to remove the influence of such problematic content without the need for costly and time-consuming retraining. This process aims to erase specific knowledge from LLMs while preserving as much model utility as possible. Despite the effectiveness of current unlearning methods, little attention has been given to whether existing unlearning methods for LLMs truly achieve forgetting or merely hide the knowledge, which current unlearning benchmarks fail to detect. This paper reveals that applying quantization to models that have undergone unlearning can restore the \"forgotten\" information. We conduct comprehensive experiments using various quantization techniques across multiple precision levels to thoroughly evaluate this phenomenon. We find that for unlearning methods with utility constraints, the unlearned model retains an average of 21% of the intended forgotten knowledge in full precision, which significantly increases to 83% after 4-bit quantization. Based on our empirical findings, we provide a theoretical explanation for the observed phenomenon and propose a quantization-robust unlearning strategy aimed at mitigating this intricate issue. Our results highlight a fundamental tension between preserving the utility of the unlearned model and preventing knowledge recovery through quantization, emphasizing the challenge of balancing these two objectives. Altogether, our study underscores a major failure in existing unlearning methods for LLMs, strongly advocating for more comprehensive and robust strategies to ensure authentic unlearning without compromising model utility. Our code is available at: https://github.com/zzwjames/FailureLLMUnlearning", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have exhibited remarkable abilities in generating human-like text, owing to their training on extensive datasets (Zhao et al., 2023). However, LLMs can also unintentionally learn and reproduce undesirable behaviors from sensitive training data (Liu et al., 2024a; Sun et al., 2024). These behaviors include the unauthorized replication of copyrighted content (Li et al., 2024), the generation of private information such as contact details (Huang et al., 2022; Yan et al., 2024), and offensive or harmful messages (Chao et al., 2023). Such risks present significant ethical and security concerns, complicating the safe and responsible deployment of LLMs in real-world applications (Yao et al., 2023). Furthermore, laws such as the European Union General Data Protection Regulation (GDPR) (Voigt & Von dem Bussche, 2017) have introduced the \"Right to be Forgotten\", allowing users to request the removal of their personal data from trained models (Xu et al., 2024)."}, {"title": "2 RELATED WORK", "content": "Machine Unlearning for LLMs. Machine unlearning, initiated by (Cao & Yang, 2015), adapts trained models to behave as if untrained on specific datasets, crucial for LLMs facing privacy and copyright issues due to indiscriminate web data training. Traditional methods like Newton update removals (Ginart et al., 2019; Guo et al., 2020; Sekhari et al., 2021) are impractical for LLMs due to the complexity of Hessian calculations, prompting newer approaches. These methods split into fine-tuning (Yao et al., 2023; Jang et al., 2023; Chen & Yang, 2023; Maini et al., 2024; Eldan & Russinovich, 2023; Patil et al., 2024; Jia et al., 2024) and in-context unlearning (Pawelczyk et al., 2024; Thaker et al., 2024; Huang et al., 2024). Fine-tuning utilizes Gradient Ascent (GA) (Yao et al., 2023) to minimize correct predictions on forget datasets by modifying the cross-entropy loss. Negative Preference Optimization (NPO) (Zhang et al., 2024) adjusts offline DPO (Rafailov et al., 2024) to reduce the likelihood of the forget set. To address utility preservation, regularized optimization merges unlearning efficacy with model utility loss, as seen in gradient difference Yao et al. (2023); Maini et al. (2024). In-context methods, using modifications such as labeled demonstrations or post-processing filters, fail to fully address privacy as they require retaining sensitive data (Pawelczyk et al., 2024; Thaker et al., 2024). Huang et al. (2024) introduces a logit offset method using proxy models, avoiding data retention but not meeting unlearning definitions as they do not match retrained model weights. Despite various studies on machine unlearning for LLMs, our study reveals that existing unlearning methods with regularization struggle with knowledge recovery issues due to minimal weight changes. We propose a simple yet effective solution to mitigate this problem. A more detailed introduction of related work is given in Appendix A.\nQuantization for LLMs. Quantization reduces LLM storage and computational needs by mapping high-precision parameters to a discrete range without altering the model structure. We focus on post-training quantization (PTQ), which directly quantizes LLMs using calibration datasets to optimize scale factors without retraining. Early PTQ methods typically round weights to the nearest level (RTN) to keep runtimes feasible for large models (Dettmers et al., 2024b; Frantar et al., 2023; Lin et al., 2024; Kim et al., 2024). Advanced PTQ strategies have been developed to enhance performance. For example, GPTQ (Frantar et al., 2023) applies layer-wise quantization updating weights with inverse Hessian information. AWQ (Lin et al., 2024) stores the most impactful weights at high precision and determines scaling with per-channel methods. Despite extensive research, the impact of quantization on unlearning in LLMs remains largely unexplored, highlighting a significant gap in the field. Recently, Kolbeinsson et al. (2024) studies how interventions such as knowledge editing, model"}, {"title": "3 PRELIMINARY", "content": "In this section, we first revisit machine unlearning and quantization for LLMs in Section 3.1. We then present evidence demonstrating that existing unlearning methods typically employ smaller learning rates and impose constraints on model utility within the retain dataset in Section 3.2. These methods aim to achieve effective unlearning by minimizing weight changes and preserving the model's utility."}, {"title": "3.1 MACHINE UNLEARNING AND QUANTIZATION FOR LLMS", "content": "Definition of Machine Unlearning. Given a pre-trained LLM, consider a dataset $D_{train}$ and a model $f_{target}$ with parameters $\\theta$ fine-tuned on $D_{train}$, we define the forget set $D_{forget} \\subset D_{train}$ as the specific subset of training data to be forgotten. Machine unlearning aims to eliminate the influence of $D_{forget}$ and obtain an unlearned model that behaves like a model $f_{retain}$ that was fine-tuned only on the retain set $D_{retain} = D_{train} \\setminus D_{forget}$. The unlearning algorithm $U$ takes $f_{target}$, $D_{forget}$, and, optionally, $D_{retain}$ and outputs an unlearned model $f_{unlearn} = U(f_{target}, D_{forget}, D_{retain})$. The most commonly used mathematical formulation for optimizing model unlearning is presented below:\n$\\min_\\theta E_{(x_f, y_f) \\sim D_{forget}}[L_{forget}(y_f | x_f; \\theta)] + \\alpha \\cdot E_{(x_r,y_r)\\in D_{retain} [L_{retain} (y_r | x_r; \\theta)]$\nwhere $L_{forget}$ is a loss function designed to penalize the model for retaining information about the forget set, $L_{retain}$ ensures that utility is preserved on the retain dataset, and $\\alpha$ is a regularization parameter used to balance them. Different choices of $L_{forget}$ and $L_{retain}$ are in the Appendix B.\nQuantization for LLMs. For quantization, consider a group or block of weights w, the linear operation can be expressed as y = wx; while the quantized version is denoted as y = Q(w)x, where Q() is the quantization function. Specifically, the quantization function is defined as (Lin et al., 2024):\n$Q(w) = A \\cdot Round(\\frac{W}{A}),\\qquad A = \\frac{max(w)}{2^{N-1}}$\nwhere N is the number of quantization bits, and A is the quantization scale factor (step size) determined by the absolute maximum value of w. Advanced post-training quantization methods, such as AWQ (Lin et al., 2024), adjust the scaling factor for each layer to minimize quantization loss on a calibration dataset. In this paper, we use Q(f) to denote the quantized model f. Thus, implementing an unlearning method and then quantizing the unlearned model can be formally written as Q(U(ftarget, Dforget, Dretain))."}, {"title": "3.2 UNLEARNING WITH MINIMAL WEIGHT CHANGE AND UTILITY PRESERVATION", "content": "We observe that existing LLM unlearning methods typically use very small learning rates to avoid catastrophic drops in model utility. For example, in three popular benchmarks for LLM unlearning, the MUSE benchmark (Shi et al., 2024b) experiments with a peak learning rate of 1e-5, the TOFU benchmark (Maini et al., 2024) uses peak learning rates of 1e\u22125, 1e-6, and 5e-7, and the RWKU benchmark (Jin et al., 2024) explores peak learning rates in the range of 1e\u22128 to 1e\u22125 via grid search. In contrast, normal training or fine-tuning of LLMs typically use a larger learning rate, e.g., models like Llama3-8B (Dubey et al., 2024) use a peak learning rate of 3e-4, Llama3-70B uses 1.5e-4 (Dubey et al., 2024), GPT-3 6.7B uses 1.2e-4, and GPT-3 13B uses 1e-4 (Brown, 2020).\nAdditionally, incorporating a utility preservation constraint on a retain dataset is commonly employed to maintain model utility (Fan et al., 2024b; Shi et al., 2024b; Maini et al., 2024). For instance, in Table 3 of the MUSE benchmark paper (Shi et al., 2024b), gradient ascent with a utility constraint results in an 18% performance drop, whereas gradient ascent without the constraint results in nearly a 100% drop in utility, despite using a small learning rate."}, {"title": "4 CATASTROPHIC FAILURE OF UNLEARNING VIA QUANTIZATION", "content": "In this section, we conduct experiments across different precision levels with various quantization techniques to test how quantization affects unlearned models, particularly how quantizing an un-learned model may inadvertently cause the partial recovery of knowledge from the forget dataset. Our investigation includes the following questions: (Q1) To what extent does quantization affect the LLM unlearning performance? (Q2) What effect does quantization precision (e.g., 4-bit or 8-bit) have on unlearning? (Q3) How do different quantization techniques affect unlearning?"}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Unlearning Methods. In our study, we assess six effective unlearning methods for LLMs that incorporate two primary families of unlearning algorithms\u2014Gradient Ascent (GA) and Negative Preference Optimization (NPO)\u2014along with two strategies for utility preservation. The first family, GA, reduces the likelihood of correct predictions on the forget dataset by applying gradient ascent to the cross-entropy loss (Jang et al., 2023; Ilharco et al.; Yao et al., 2023). The second, NPO, treats the forget set as negative preference data, adapting the offline DPO objective to lower the model's likelihood predictions for this set (Zhang et al., 2024; Rafailov et al., 2024). As GA and NPO do not inherently focus on utility preservation, we employ two regularization strategies to address this gap (Liu et al., 2022; Maini et al., 2024; Zhang et al., 2024): Gradient Descent on the Retain Set (GDR) and KL Divergence Minimization on the Retain Set (KLR). The GDR strategy integrates a gradient descent learning objective on the retain set to maintain performance, whereas KLR aims to minimize the KL divergence between the probability distributions of the unlearned and target models during next-token prediction on retain set inputs. By integrating these methods and regularization strategies, we have six distinct approaches for unlearning: GA, GA_GDR, GA_KLR, NPO, NPO_GDR, and NPO_KLR. Further details on these methods are provided in the Appendix B.\nDatasets. We conduct experiments on MUSE (Shi et al., 2024b), a benchmark for evaluating machine unlearning in language models, using two datasets: NEWS and BOOKS. The NEWS dataset (Li et al., 2023b) includes recent BBC news articles divided into forget, retain, and holdout sets. The BOOKS dataset (Eldan & Russinovich, 2023) features the Harry Potter series, with original novels as the forget set and related FanWiki materials as the retain set to preserve domain knowledge post-unlearning. Details are in Appendix C.1.\nMetrics. From the perspective of data owners, expectations for an unlearned model include (1) no verbatim memorization, (2) no knowledge memorization, and (3) no privacy leakage. Conversely, developers prioritize (4) utility preservation on the retain set. Following Shi et al. (2024b), we use four metrics to assess these aspects: (1) M1. VerMem, which evaluates verbatim memorization by comparing model continuation outputs to actual tokens using the ROUGE score $(VerbMem(f, D_{forget}) = E_{x \\in D_{forget}}ROUGE(f(x[1:l]), x[l+1:])$ where ROUGE (Lin, 2004) assesses similarity between machine output and reference, Xx[1:l] is the initial l tokens, and $x[l+1:]$ the true continuation.)-lower scores for better unlearning; (2) M2. KnowMem on $D_{forget}$, which measures knowledge memorization by analyzing responses to tailored knowledge QA pairs $(KnowMem(f, D_{forget}) = E_{(q,a) \\in D_{forget}} ROUGE(f(q), a))$, with effectiveness indicated by lower scores; (3) M3. PrivLeak, which assesses privacy preservation using the Min-K% method (Shi et al., 2024a), an MIA technique that compares AUC-ROC scores between $D_{forget}$ and $D_{holdout}$. Then, by comparing the AUC score with that of the retrained model, $PrivLeak = (AUC(f_{unlearn}) \u2013 AUC(f_{retain}))/AUC(f_{unlearn})$, the optimal scores are near zero, and large deviations suggest poor privacy handling; and (4) M4. KnowMem on $D_{retain}$, ensuring utility preservation with the same metric $(KnowMem(f, D_{retain}) = E_{(q,a)\\in D_{retain}} ROUGE(f(q), a))$ applied to the retain set, where higher scores indicate better preservation. The first three metrics measure forget performance; the last one is for utility. Additional details are available in the Appendix C.2. More implementation details are in Appendix D.1"}, {"title": "5 EXPLANATION OF THE FAILURE OF UNLEARNING VIA QUANTIZATION", "content": "Our observations in Section 4 have indicated that 4-bit quantized models, regardless of the quantization technique used, exhibit poor unlearning performance when compared to their full-precision models. In contrast, 8-bit quantized models achieve performance metrics similar to those of full-precision models. In this section, we aim to explain these phenomena through a theoretical analysis of the quantization mechanism. We use int-4 and int-8 as examples for illustration.\nAccording to the definition in Equation 2, a weight w within a quantization interval $Z_i$ is mapped to a low-precision quantization index $i = Round(\\frac{w}{A})$ within the range [-$2^{N-1}$, $2^{N-1}$ \u2013 1], and to a quantized value $q_i = i\\Delta$. All weights within interval $Z_i$ are mapped to the same index i and quantized value $q_i$, as defined by:\n$Z_i = [(\\frac{i}{A} - \\frac{1}{2})\\Delta, (\\frac{i}{A} + \\frac{1}{2})\\Delta,$\nwhere $\\Delta$ denotes the quantization scale factor, dictating the size of each interval. For example, $\\Delta_{int4} = \\frac{max(|w|)}{2^{4-1}} = \\frac{max(|w|)}{8}$, and $\\Delta_{int8} = \\frac{max(|w|)}{2^{8-1}} = \\frac{max(|w|)}{128}$. In scenarios where $max|w| = 200$, as depicted in Figure 2, all weights within the interval [-12.5, 12.5) map to $q_0$ = 0 under an int-4 precision format. To differentiate the quantized weights of the original model f from those of the unlearned model $f_{unlearn}$, the weight changes in $f_{unlearn}$ must exceed the quantization step size $\\Delta$. As discussed in Section 3.2, effective unlearning methods that preserve utility typically have minimal weight changes, resulting in $f_{target}$ and $f_{unlearn}$ being highly similar, i.e., $Q(f_{unlearn}) \\approx Q(f_{target})$. We also know that direct quantization of the original model, i.e., applying $Q(f_{target})$, generally"}, {"title": "6 QUANTIZATION-ROBUST UNLEARNING", "content": "The catastrophic failure underscores the need for effective methods to prevent knowledge recovery while preserving utility. Thus, we propose a tailored strategy based on our theoretical analysis."}, {"title": "6.1 PROPOSED FRAMEWORK", "content": "We aim for an ideal unlearning method to achieve three key objectives: (i) effectively unlearn knowledge from the forget dataset; (ii) preserve model utility on the retain dataset; and (iii) prevent the recovery of forgotten knowledge through quantization. Based on our theoretical analysis in Sec. 5, the core issue behind the failure of existing unlearning methods in preventing knowledge recovery lies in the fact that effective unlearning seeks minimal weight changes to preserve model utility. This creates a conflict between objectives (ii) and (iii).\nOne intuitive approach to address the conflict is to increase the learning rate for both $L_{forget}$ and $L_{retain}$. Intuitively, increasing the learning rate for $L_{forget}$ can help achieve objectives (i) and (iii), while the utility constraint imposed by $L_{retain}$ on the retain dataset can assist the model in maintaining its performance on that dataset, thus fulfilling objective (ii). However, using a large learning rate to fully fine-tune the model can lead to over-adjustment due to aggressive forgetting gradients, degrading overall utility. Furthermore, applying a large learning rate to the retain dataset may bias the model towards this data, skewing its behavior and further reducing performance on tasks beyond the retain dataset, as demonstrated in Appendix H.\nOn the other hand, it is acknowledged that large language models may store knowledge in specific neurons (Liu et al., 2024a; Dai et al., 2022), suggesting that unlearning certain knowledge can be achieved by selectively updating model weights, thus minimizing the impact on model utility. Following this idea, we draw on approaches from prior work (Fan et al., 2024b; Meng et al., 2022; Wu et al., 2023; Wei et al., 2024) and propose constructing a weight saliency map by utilizing the gradient of the loss $L_{forget}$ with respect to the model weights on the forget dataset, i.e., $\\nabla_{w_i} L_{forget}(\\theta; D_{forget})$. Generally, large magnitude of the gradient, i.e., $|\\nabla_{w_i} L_{forget}(\\theta; D_{forget})|$, means the weight $w_i$ is more relevant to the knowledge to be forgotten. We hence choose the weights with large gradients as the saliency weights and update only the salient weights to minimize the potential bias caused by fully fine-tuning with a large learning rate on the retain dataset. In practice, designing a mask for"}, {"title": "6.2 EXPERIMENTS", "content": "Experimental Setup. To thoroughly evaluate our method, following (Jin et al., 2024), we not only test the model's utility on the retain dataset but also assess its performance across various capabilities, detailed as follows: (1) General Ability (Gen): We use MMLU (Hendrycks et al., 2021), which contains multiple-choice questions from diverse knowledge domains. We report 5-shot accuracy based on answer perplexity. (2) Truthfulness (Tru): To evaluate whether the model becomes dishonest after unlearning, we use TruthfulQA's MC1 task (Lin et al., 2022), reporting 6-shot accuracy scores. (3) Factuality (Fac): Since unlearning negates original knowledge, we assess factuality using TriviaQA (Joshi et al., 2017) with 6-shot F1 scores. (4) Fluency (Flu): To measure generation quality, we adopt the instructions in AlpacaEval (Li et al., 2023a) and report the weighted average of bi- and tri-gram entropies (Meng et al., 2022; Zhang et al., 2018).\nAccording to our three objectives, we aim for the incorporation of SURE to achieve comparable forgetting performance and model utility in the full-precision model, as compared to methods without using SURE. Additionally, SURE should help improve forgetting performance after quantizing the unlearned model. Thus, in our experiments, we incorporate SURE into various unlearning methods with regularization and compare them to the original unlearning methods. The implementation of the original unlearning methods follows the setup in Appendix D.1. For each method, we evaluate both the forgetting performance and model utility in full precision and in the quantized version.\nWe conduct a grid search for the learning rate over the values [5e-5, 1e-4, 2e-4], for the regularization weight $\\alpha$ over [1, 20, 100, 300, 400], and for the threshold to construct the saliency mask $\\gamma$ over [Percentile(s, 90), Percentile(s, 95), Percentile(s, 99)], where Percentile() refers to the specified percentile over the saliency scores in s. Other settings are the same as those in Section D.1. More implementation details can be found in Appendix D.2."}, {"title": "7 CONCLUSION", "content": "This paper identifies a critical issue: applying quantization to models that have undergone unlearning can restore the \u201cforgotten\u201d knowledge. We conduct comprehensive experiments across various quantization techniques and precision levels to thoroughly evaluate this phenomenon. Furthermore,"}, {"title": "A DETAILED RELATED WORK", "content": "A.1 MACHINE UNLEARNING FOR LLMS\nMachine unlearning, initiated by (Cao & Yang, 2015), adapts trained models to behave as if they had never been trained on specific datasets. This is crucial for LLMs, which often face privacy and copyright issues due to training on extensive, indiscriminately collected web data. Traditional methods (Ginart et al., 2019; Guo et al., 2020; Sekhari et al., 2021) involve Newton update removals, which are impractical for LLMs due to the computational complexity of Hessian calculations. As a result, newer approaches for LLMs (Jang et al., 2023; Chen & Yang, 2023; Yao et al., 2023; Eldan & Russinovich, 2023; Zhang et al., 2024; Huang et al., 2024; Jia et al., 2024) have emerged. These methods are categorized into fine-tuning based (Yao et al., 2023; Jang et al., 2023; Chen & Yang, 2023; Maini et al., 2024; Eldan & Russinovich, 2023; Patil et al., 2024; Jia et al., 2024) and in-context based unlearning (Pawelczyk et al., 2024; Thaker et al., 2024; Huang et al., 2024). Fine-tuning-based methods utilize Gradient Ascent (GA) (Yao et al., 2023; Jang et al., 2023; Chen & Yang, 2023; Maini et al., 2024; Jia et al., 2024) to reduce correct predictions on forget datasets by altering the cross-entropy loss. Negative Preference Optimization (NPO) (Zhang et al., 2024) adapts the offline DPO (Rafailov et al., 2024) to lower likelihoods on the forget set. Techniques also include relabeling answers with non-sensitive equivalents to enhance responses Eldan & Russinovich (2023); Patil et al. (2024). To address utility preservation, regularized optimization objectives integrate unlearning efficacy loss with model utility loss, as seen in approaches such as gradient difference Yao et al. (2023); Maini et al. (2024). Moreover, localization-informed methods, focusing on neuron editing (Wu et al., 2023; Yu et al., 2023), remain underexplored for LLMs and large forget datasets and are not discussed in this paper. In-context methods, using modifications such as labeled demonstrations or post-processing filters, fail to fully address privacy as they require retaining sensitive data (Pawelczyk et al., 2024; Thaker et al., 2024). Huang et al. (2024) introduced a logit offset method that estimates adjustments for unlearning using proxy models, eliminating the need to retain sensitive data. However, these methods do not meet the strict definition of unlearning as they do not ensure model weights match those of a retrained model. True unlearning for LLMs primarily relies on fine-tuned methods, yet existing studies overlook the unlearning performance of quantized models. Our research is the first to thoroughly examine LLM quantization's impact on unlearning. In contrast, the closest study (Kolbeinsson et al., 2024) focuses solely on quantization's effect on unlearning but overlooks utility preservation, leading to conclusions that diverge from ours.\nA.2 QUANTIZATION FOR LLMS\nQuantization reduces the storage and computational demands of LLMs by mapping high-precision parameters to a discrete range without changing the model structure. Existing methods for LLMs can be generally categorized into Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT, such as QLORA (Dettmers et al., 2024a) and IR-QLORA (Qin et al., 2024), retrains LLMs at low-bit precision but is computationally intensive. PTQ directly quantizes LLMs using calibration datasets to optimize scale factors without the need for retraining. Early PTQ approaches typically round weights to the nearest (RTN) quantization level to maintain feasible runtimes for large models (Dettmers et al., 2024b; Frantar et al., 2023; Lin et al., 2024; Kim et al., 2024). To improve the performance of quantization, more advanced PTQ strategies are developed. For example, SpQR (Dettmers et al., 2024b) uses the L2 error between original and quantized predictions to determine weight sensitivity and maintains outlier features at higher precision levels to mitigate loss. GPTQ (Frantar et al., 2023) applies layer-wise quantization updating weights with inverse Hessian information. AWQ (Lin et al., 2024) stores the most impactful weights at high precision and determines scaling with per-channel methods. SqueezeLLM (Kim et al., 2024) uses k-means clustering for quantized weight values and stores sensitive weights sparsely.\nB DETAILS OF UNLEARNING METHODS AND REGULARIZERS\nWe evaluate six efficient unlearning methods belonging to two distinct families of unlearning algorithms. We will first introduce these two families, which form the basis for the methods assessed. We will then discuss two regularizers addressing the lack of explicit design for utility preservation in these unlearning algorithms."}, {"title": "B.1 TWO UNLEARNING FAMILIES", "content": "Gradient Ascent (GA) minimizes the likelihood of correct predictions on $D_{forget}$ by performing gradient ascent on the cross-entropy loss (Jang et al., 2023; Ilharco et al.; Yao et al., 2023). This method simply reverses the original training objective of minimizing the negative log-likelihood of the token sequences:\n$\\min_\\theta L_{GA}(\\theta) = E_{x \\sim D_{forget}} [log(f_\\theta(x_t|x_{<t}))]$\nwhere $f_\\theta$ refers to the model parameterized by $\\theta$ for unlearning, $x_{<t}$ represents the token sequence x = ($x_1$,..., $x_{t\u22121}$), and $f_\\theta(x_t|x_{<t})$ is the conditional probability that the LLM $f_\\theta$ predicts $x_t$ given the preceding tokens $x_{<t}$.\nNegative Preference Optimization (NPO) (Zhang et al., 2024) views the forget set as negative preference data and adapts the offline DPO objective (Rafailov et al., 2024) to fine-tune the model. This tuning ensures the model assigns a low likelihood to the forget set while remaining close to the original model $f_{target}$. The loss function for NPO is defined as:\n$\\min_\\theta L_{NPO}(\\theta) = E_{x \\sim D_{forget}} log \\Big(\\sigma \\Big(\\beta \\big(log \\frac{f_\\theta(x)}{f_{target}(x)}\\big) \\Big)\\Big)$\nwhere $\\sigma$ is the sigmoid function, and \u03b2 is a hyperparameter controlling the divergence of $f_\\theta$ from $f_{target}$. We set \u03b2 = 0.1 following the protocols in (Rafailov et al., 2024; Zhang et al., 2024)."}, {"title": "B.2 UTILITY PRESERVATION THROUGH REGULARIZATION", "content": "GA and NPO are not explicitly designed for utility preservation. Hence, we explore regularization strategies to enhance performance on the retain set and ensure proximity to the target model during the unlearning process. These strategies include Gradient Descent on the Retain Set (GDR) and KL Divergence Minimization on the Retain Set (KLR):\nGradient Descent on the Retain Set (GDR) (Liu et al., 2022; Maini et al., 2024; Zhang et al., 2024) integrates a standard gradient descent objective on the cross-entropy of the retain set $D_{retain}$. This approach is aimed at directly training the model to maintain its performance on $D_{retain}$, aligning the unlearning objective with performance retention:\n$\\min_\\theta L_{GDR} = L_{unlearn} - E_{x \\sim D_{retain}} [log(f_\\theta(x_t|x_{<t}))]$\nwhere $L_{unlearn}$ is a selected unlearning family.\nKL Divergence Minimization on the Retain Set (KLR) (Maini et al., 2024; Zhang et al., 2024) aims to minimize the Kullback-Leibler (KL) divergence between the predictions on $x \\in D_{retain}$ of the unlearned model's probability distribution $p_{funlearn} (\\cdot|x)$ over the vocabulary and the original model's probability distribution $p_{ftarget} (\\cdot|x)$ while maintaining the conventional unlearning loss on $D_{forget}$. The formal objective can be written as:\n$\\min_\\theta L_{KL} = L_{unlearn} + E_{x \\in D_{retain}} KL(p_{funlearn} (\\cdot|x), p_{ftarget} (\\cdot|x))$\nWe integrate the GA and NPO methods with two regularizers, creating six unlearning methods in total: GA, $GA_{GDR}$, $GA_{KLR}$, NPO, $NPO_{GDR}$, and $NPO_{KLR}$."}, {"title": "C DETAILS OF EVALUATION BENCHMARK AND METRICS", "content": "C.1 NEWS AND BOOKS DATASETS\nMUSE (Shi et al., 2024b) is a benchmark specifically developed for assessing LLM unlearning. It consists of two distinct datasets, NEWS and BOOKS, which focus on different types of textual data, i.e., news articles and books.\n\u2022 NEWS (Li et al., 2023b) features a collection of BBC news articles from post-August 2023. These articles are systematically categorized into separate forget, retain, and holdout sets."}, {"title": "C.2 FOUR METRICS", "content": "Upon removing a forget set from a model", "metrics": "nMetric 1. VerMem: No Verbatim Memorization (Jang et al.", "X[1": "l", "x[l+1": "in $D_{forget"}, ".", "This comparison uses the ROUGE-L F1 score (Lin, 2004) to quantify the degree of memorization.\n$VerbMem(f, D) := \\frac{1}{|D_{forget}|} \\sum_{x \\in D_{forget}} ROUGE(f(x["]}