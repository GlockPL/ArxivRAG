{"title": "Acoustic-based 3D Human Pose Estimation Robust to Human Position", "authors": ["Yusuke Oumi", "Yuto Shibata", "Go Irie", "Akisato Kimura", "Yoshimitu Aoki", "Mariko Isogawa"], "abstract": "This paper explores the problem of 3D human pose estimation from only low-level acoustic signals. The existing active acoustic sensing-based approach for 3D human pose estimation implicitly assumes that the target user is positioned along a line between loud-speakers and a microphone. Because reflection and diffraction of sound by the human body cause subtle acoustic signal changes compared to sound obstruction, the existing model degrades its accuracy significantly when subjects deviate from this line, limiting its practicality in real-world scenarios. To overcome this limitation, we propose a novel method composed of a position discriminator and reverberation-resistant model. The former predicts the standing positions of subjects and applies adversarial learning to extract subject position-invariant features. The latter utilizes acoustic signals before the estimation target time as references to enhance robustness against the variations in sound arrival times due to diffraction and reflection. We construct an acoustic pose estimation dataset that covers diverse human locations and demonstrate through experiments that our proposed method outperforms existing approaches.", "sections": [{"title": "Introduction", "content": "Human pose estimation has diverse applications including rehabilitation support, elderly monitoring, and disaster relief efforts. Traditional approaches to 3D human pose estimation have primarily employed RGB videos and images [15, 16], transient light [7], event data [4, 18], radio frequency (RF)/Wi-Fi signals [8, 26], and millimeter wave [10, 22]. Additionally, methods that combine some of these approaches as a multimodal framework also"}, {"title": "Methodology", "content": "Our goal is to estimate a sequence of 3D human poses $p = [p_1, p_2, ..., p_T]$ from an acoustic signal sequence $s = [s_1, s_2, ..., s_T]$, segmented into fixed lengths from audio signals recorded by a microphone. T is the sequence length, and $s_t$ and $p_t$ refer to the t-th elements of the acoustic signal sequence and the 3D pose sequence, respectively. Following [19], we use TSP signal, a periodic signal whose frequency varies over time for active acoustic sensing."}, {"title": "Pose Estimation Module", "content": "The pose estimation module $f(a)$ consists of 2D convolutional layers and 1D convolutional layers. The function f simultaneously estimates the n consecutive poses $[p_i, p_{i+1},..., p_{i+n-1}]$. During this process, the corresponding acoustic features $[a_i, a_{i+1},\u2026\u2026,a_{i+n-1}]$ are influenced by reverberant acoustic signals from several frames earlier, delayed due to reflection and diffraction. Therefore, the proposed method considers the time series relationships of sound by including acoustic information from k frames prior to the target sequence, thus utilizing n+k frames of acoustic features $[a_{i-k}, a_{i-k+1},\u2026\u2026,a_{i+n-1}]$ as input for the pose estimation. With the variable $\\theta$ that contains all trainable parameters and weight hyperparameters $w_a$, $w_\\beta$, and $w_\\gamma$, the training objective is to minimize the following loss function $L$.\n\n$L = w_a L_{pose} + w_\\beta L_{smooth}+w_\\gamma L_{std}$                                         (1)\n\nThe loss function $L_{pose}$ related to the human pose is calculated as the Mean Squared Error (MSE) between i-th ground truth pose $p_i$ and predicted pose $\\hat{p_i}$. The loss function $L_{smooth}$ is used to smoothly connect consecutive poses $p_i$ and $p_{i-1}$.\n\n$L_{pose}(\\theta) = \\frac{1}{T} \\sum_{i=1}^T \\Vert p_i - \\hat{p_i} \\Vert_2 $                                      (2)\n\n$L_{smooth}(\\theta) = \\frac{1}{T-1} \\sum_{i=2}^T \\Vert (p_i - p_{i-1}) - (\\hat{p_i} - \\hat{p_{i-1}}) \\Vert_2 $                       (3)\n\n$L_{std}$ is the loss function used for adversarial learning with the position discriminator module. Details are provided in Sec 3.2."}, {"title": "Position Discriminator Module", "content": "The position discriminator module is composed of a single fully connected layer and uses the intermediate outputs from the pose estimation module as inputs to learn the subject's position. The pose estimation module engages in adversarial learning against the position discriminator module to extract features that are independent of position, enhancing the ro-bustness of human positions.\n\nHere, one of the most straightforward ways of implementation for position estimation within the position discriminator module is to utilize regression. However, introducing regression-based predictors into adversarial learning is known to potentially cause gradi-ent explosions. To address this issue, we treat the distance from the line connecting the"}, {"title": "Data Augmentation", "content": "A general challenge in deep learning-based human pose estimation tasks is the need for a large amount of training data. As we tackle a new task, we cannot leverage existing large-scale datasets. Also, this paper assumes that subjects are positioned in multiple positions, necessitating data collection for each position. Therefore, compared to existing work that as-sumes subjects are standing in fixed positions, our data collection cost becomes significantly higher, making the collection of large amounts of real-world data quite costly. Therefore, we also propose to introduce data augmentation for this task. By shifting the starting time of one period of the TSP signal by a time units (equivalent to shifting the phase of the acous-tic signal), and generating acoustic features from the shifted received signal, we perform data augmentation. The ground truth poses are similarly shifted by the time parameter a, and the average pose associated with the acoustic signals used to create acoustic features is determined."}, {"title": "Experimental Settings", "content": ""}, {"title": "Dataset and Setup", "content": "For active acoustic sensing, we utilized a pair of loudspeakers (Edifier ED-S880DB) and the Ambisonics microphone (Zoom H3-VR). To obtain ground truth 3D pose, we employed the motion capture system (OptiTrack) with 16 cameras (see Fig. 3). The experiments were conducted in a classroom environment with background noise and reverberation.\n\nFive male subjects were asked to stand at five positions along the line connecting the speaker and the microphone: directly on the line, and at 25 cm, 50 cm, 75 cm, and 100 cm away from this line. They were asked to take various poses including walking, squat-ting, bowing, standing, T-pose, and intermediate poses between these movements. We used 21 joints including the head, neck, both shoulders, both arms, both forearms, both hands, waist, both thighs, both shins, both feet, both toes, hip, and spine. The dataset size was approximately 3.5 hours in total."}, {"title": "Baselines", "content": "We compared our method against the following three methods: (1) Jiang et al. [8] as one of the state-of-the-art methods for based 3D human pose estimation with low-dimensional input signals like our method. Specifically, the original method utilizes Wi-Fi signals and introduces an LSTM network. Since the original method is Wi-Fi-based, we modified the input layer of this method so that it can use our log-Mel Spectrum and Intensity Vector as input. (2) Ginosar et al.'s method [6] that estimates human gestures from speech sounds. This method employs a CNN-based network with temporal convolutions, using only log-Mel Spectrum as the input acoustic feature. (3) Shibata et al. [19], which is the most relevant method to ours, estimates the pose of subjects located along a straight line between a speaker and a microphone in the form of active acoustic sensing. This method also employs a CNN-based network that processes temporal information through temporal convolutions like (2), and it utilizes both log-Mel Spectrum and Intensity Vector as inputs."}, {"title": "Evaluation Metrics", "content": "In this paper, three types of evaluation metrics were used: Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Percentage of Correct Keypoints (PCK). RMSE and MAE are metrics calculated from the true poses and the estimated poses. PCK calculates the pro-portion of correctly estimated keypoints compared to the true keypoints, considering dis-tances below a certain threshold as correct. In this study, we use PCKh@0.5, where h rep-resents the distance between the keypoints of the head and neck, and the threshold is set to half of this distance."}, {"title": "Implementation Details", "content": "For all methods, we used Adam [9] as optimizer. Ginosar et al. and Shibata et al.'s methods simultaneously estimate 12 frame poses using 12 frames of acoustic features as inputs. In contrast, the proposed method estimates 8 frame poses simultaneously from 24 frames of acoustic features as described in Sec 3.1 (n = 8,k = 16). For the loss calculation in Eq. 1, weight parameters were set as $w_a = w_y = 1,w_\\beta = 10$. Furthermore, for the parameter a for the data augmentation, one-third and two-thirds of the size of each acoustic signal sequence element were used."}, {"title": "Experimental Results", "content": ""}, {"title": "Comparison with Other Baselines", "content": "In this paper, four out of the five subjects were used as training data to train the model f, and the fifth subject's data, not included in training, was used for testing. This process was repeated for each subject to calculate the average estimation accuracy for five subjects. The table 1 shows a qualitative comparison with the baseline method. The proposed method outperforms all others across three evaluation metrics.\n\nFigure 4 shows the qualitative comparison. To distinguish between the \"T-pose\" and \"standing\", it is necessary to detect the arms raised horizontally, as the positions of the torso and legs remain the same. This requires discerning subtle differences in the sounds reflected off the arms, which is a more delicate task compared to other movements. The proposed method effectively captures these subtle acoustic differences, resulting in more accurate T pose estimations compared to baseline methods. Additionally, the methods by Ginosar et al. and Shibata et al. frequently misestimate poses in the first half of pose sequences. The method by Jiang et al. does not utilize temporal convolution operations, which results in unstable pose predictions."}, {"title": "Ablative Analysis", "content": "The Ablation study was conducted to individually evaluate the effects of the three technical contributions introduced in the proposed method. Table 2 shows a quantitative comparison excluding each component one at a time. In this table, adversarial learning with the position discrimination module is denoted as \"Adv\", the proposed method that uses information prior to the target time is referred to as \"Prior\", and data augmentation using phase shifts is rep-resented as \"Aug\". The results indicate that the complete the proposed method achieves the best results across all three evaluation metrics. Particularly, the inclusion of information prior to estimated time was found to dominate the accuracy improvements. Detailed comparisons on the estimation's precision using previous time information are provided in Section 5.3, and discussions on the second most contributing factor, data augmentation through phase shifting, are in Section 5.4."}, {"title": "Comparison by Input Size", "content": "In our method, 24 samples of acoustic features are used as input to the model, which then outputs the pose corresponding to the last 8 samples of these 24. We tested reduced input sizes of 8 and 16 samples and an increased size of 32 samples. Table 3 shows the quantita-tive evaluation for different input sizes. Input size of 8 samples resulted in lower accuracy across all metrics. When the input size is reduced to 16, the PCK is the same as the proposed method, and the rough behavior is relatively well estimated. However, the lack of informa-tion prior to the estimation target time particularly lowered the RMSE values. Conversely, increasing the input to 32 samples also resulted in decreased accuracy. This configuration involves using inputs that reach 1.2 seconds back from the target estimated time, which ex-ceeds the typical reverberation time in a classroom environment. Therefore, it is likely that the model overfits acoustical information that is less relevant to the actual poses."}, {"title": "Comparison by Data Augmentation", "content": "In the proposed method, data augmentation was performed by tripling the number of training frames using phase shift hyperparameter a. We also evaluated the effects of doubling and quadrupling the number of training data frames. Table 4 shows the quantitative evaluation when varying the amount of training data, highlighting that our augmentation method con-tributes to accuracy improvements in all metrics. However, we can see that the effects of data augmentation appear to saturate beyond three times the original data. This is likely because while slight phase changes increase the diversity of acoustic features, such slight time delays have little impact on the distribution of target pose sequences."}, {"title": "Evaluation with In Plain Clothes Dataset", "content": "To assess estimation accuracy in real-world settings, we conducted a qualitative evaluation with subjects wearing casual clothing. Fig. 5 shows the qualitative comparison results of two subjects who wore short- and long-sleeved clothing, different from the motion capture suits subjects wore during the training data collection. Green and yellow arrows indicate poses where the estimation failed. As discussed in Sec. 5.1, estimating the T pose becomes particularly challenging when subjects are positioned away from the line. For the subject in short sleeves, the shape of the arms is similar to that when wearing a Mocap suit, resulting in minimal degradation in T pose estimation accuracy. However, for the subject wearing long sleeves, the acoustic reflection characteristics of the arms differ from those in a body-fitting Mocap suit. Consequently, a decrease in the accuracy of T pose estimation has been observed (see yellow arrows)."}, {"title": "Conclusion", "content": "In this paper, we addressed the challenge of estimating human poses at positions away from the line connecting the speaker and microphone. We introduce adversarial learning for po-sition estimation, sequence size determination based on prior time-step information, and phase-shift data augmentation. These approaches allowed us to achieve superior accuracy across all evaluation metrics compared to baseline methods.\n\nHowever, our method has some limitations. We are unable to estimate the poses at un-seen subject's positions not included in the training. This limitation arises because our model learns the echo characteristics of locations present within the training dataset. Consequently, when a subject moves to an unseen position, the echo characteristics differ from those the model has learned, hindering accurate pose estimation. Additionally, the data used in this paper were all collected in a single classroom. The acoustic signals captured by the micro-phones can vary depending on the size of the room and the reflectivity of the surfaces. It will be necessary in future work to address the variations in estimation accuracy caused by such environmental characteristics.\n\nIn future studies, we aim to improve our method to effectively estimate poses for subjects moving across a broader range of settings, including those at unseen positions, and various room settings."}]}