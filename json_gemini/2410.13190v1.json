{"title": "CohEx: A Generalized Framework for Cohort Explanation", "authors": ["Fanyu Meng", "Xin Liu", "Zhaodan Kong", "Xin Chen"], "abstract": "eXplainable Artificial Intelligence (XAI) has garnered significant attention for enhancing transparency and trust in machine learning models. However, the scopes of most existing explanation techniques focus either on offering a holistic view of the explainee model (global explanation) or on individual instances (local explanation), while the middle ground, i.e., cohort-based explanation, is less explored. Cohort explanations offer insights into the explainee's behavior on a specific group or cohort of instances, enabling a deeper understanding of model decisions within a defined context. In this paper, we discuss the unique challenges and opportunities associated with measuring cohort explanations, define their desired properties, and create a generalized framework for generating cohort explanations based on supervised clustering.", "sections": [{"title": "Introduction & Motivation", "content": "eXplainable Artificial Intelligence aims to improve the existing machine learning system by providing transparent explanations for model predictions and decisions (Arrieta et al. 2020; Adadi and Berrada 2018). The scope of explanation refers to the degree to which the explanation provided is generalized. The scope, as defined by (Sokol and Flach 2020), is usually (1) global explanation, which aims to explain the explainee comprehensively; or (2) local explanation, which focuses on explaining the behavior in one instance of model prediction. However, there is a less explored middle ground, namely (3) cohort explanation, or sometimes also called regional explanation, which generalizes the explanation on a subset of a dataset, a subspace in the model's input space or decision space. Cohort explanation can be seen as a spectrum that generalizes both local and global explanation methods, where global and local explanations are at the two extremes of conciseness and faithfulness. This generalization allows users to uncover patterns, biases, and context-specific insights that may not be apparent when examining individual instances in isolation or the model as a whole.\nFaithfulness vs. Conciseness Global XAI methods provide one general explanation, but may not generalize well on subgroups; local methods assign a specific explanation for each data sample, but it is not concise and may not help to understand the entire model. Cohort explanation methods strive for a balance between the two, with the aim of creating meaningful splittings of the feature space and assigning explanations to each cohort that generalize well within each of the groups. Thus, the cohort explanation generates \u201ccontext-ful\" information: the explanation of each cohort recognizes that it can be applied to a limited region. This helps users better understand the conditions that must be maintained for the output explanation to be valid and to offer a more concise and transparent explanation (Sokol and Flach 2020).\nTo illustrate this motivation, consider a simplified medical patient classification task in Fig. 1a. The two features are age and the fraction of family members with this disease. The likelihood of the condition occurring increases with respect to both features. The explainee model is a decision tree with a depth of two. Note that we treat the decision tree as a black-box, i.e. the explainer does not know that the model is a tree.\nTo generate cohort explanation to the tree, we can first apply a simplified version of the counterfactual-based local explanation (Wachter, Mittelstadt, and Russell 2017). We consider the importance of each feature as the minimum perturbation on that feature to change the prediction, and a small perturbation signals a high importance. Fig. 1b shows the global and distribution local importance on this model. These two types of explanations illustrates that in general, age has a higher importance, but they cannot used to explain whether they are patterns within the explanations. However, such pattern exists: if we separate the samples into three groups: (1) young patients with high family history, (2) middle-aged or young patients with low family history, and (3) older patients, the distribution of feature importance are distinct between the groups. Fig. 1c shows this cohort explanation, and we can provide a deeper analysis that the features are (1) both are significant; (2) only age is significant; (3) neither is significant respectively on the three groups. Naturally, this analysis correspond to the slope of the optimal decision boundary in Fig. 1a. Cohort explanation simplifies the mechanisms of the model into a few regions in which different features are the main factor behind decision making. Additionally, it provides insights that cannot be provided by global or local explanations.\nOur contributions in this paper are as follows:\n1. We propose desired properties of cohort explanation and devise their evaluation metrics;\n2. We develop a generalized framework to convert existing data-driven local feature importance methods to cohort"}, {"title": "2 Related Works", "content": "Cohort Explanation Cohort explainability aims to strike a balance between local and global methods, which provide explanations based on a group of instances. It can be classified into inherent and post-hoc methods. Inherent cohort explanation, sometimes referred to as a local surrogate model, involves dividing the feature space and developing a local, interpretable model for each subspace. Examples of this approach include Hu et al. (2020), which constructs local-surrogate linear or generalized additive models by recursively partitioning the feature space.\nPost-hoc cohort explanation methods partition the feature space and provide an explanation for each cluster. In general, such explainers run local explanation methods on a dataset and then cluster the samples based on the similarity between the local explanations. Such methods include Cavus, Stando, and Biecek (2023), which averages local SHAP values within predefined groups. Other methods employ techniques to automatically identify the cohorts, such as VINE, which clusters individual conditional expectation (ICE) via unsupervised clustering methods (Britton 2019). Similarly, REPID (Herbinger, Bischl, and Casalicchio 2022) and Molnar et al. (2023), which use tree-based partitioning to generate the cohorts either based on ICE or conditional permutation feature importance. Additionally, GADGET extends previous methods by incorporating functional decomposition during partitioning, allowing for the consideration of more features (Herbinger, Bischl, and Casalicchio 2023). In contrast to the partitioning aspect, there are also works that focuses on how to aggregate local explanations, such as GALE (Van Der Linden, Haned, and Kanoulas 2019) which uses homogeneity to re-weight LIME importance before aggregating for multi-class classification problems.\nA common challenge for XAI in general is a lack of evaluation metrics to compare different methods, since the explanation template varies between methods. The most common metrics used to qualify cohort explanation is generalizability/coherence, the similarity between local explanations within cohorts. In this paper, we propose additional desiderata and evaluation metrics for cohort explanation. We also introduce a supervised-clustering-based approach that achieves better performance based on the metrics.\nSubgroup Discovery Subgroup discovery is a prominent area in data mining and machine learning that focuses on identifying interesting and interpretable subgroups within a dataset, which is valuable for uncovering hidden structures and actionable insights in various domains (Helal 2016). Such works include Sutton et al. (2020) which uses rule-based partitioning based on the loss of surrogate models to identify interesting regions. Similarly, Hedderich et al. (2022) also use a rule-based algorithm on natural language models to recognize regions with significant response to certain variables. These methods can be thought of as a type of inherent cohort explanation, though the identified clusters of interest may not cover the entire feature space.\nExplainable Clustering Explainable clustering aims to enhance interpretability of clustering by providing understandable explanations for the derived clusters. For example, Moshkovitz et al. (2020) modifies the results from k-means into decision trees to increase the interpretability of the clusters. L'Yi et al. (2015) creates a visualization tool to compare and analyze clustering results. Guilbert, Vrain, and Dao (2024) formulate a constraint optimization program to clustering to allow for the injection of domain knowledge. Though both domains utilize explainability and clustering, their goals are different. Cohort explanation aims to apply clustering to simplify the result of local explanations; constructively, explainable clustering's goal is to introduce explainability methods to existing clustering algorithms."}, {"title": "3 Notation", "content": "We will use the following notation throughout the paper.\n\u2022 M: explainee model, or the model to be explained;\n\u2022 x: a particular sample;\n\u2022 X or Xj: a group of samples. We will use X to denote the entire evaluation dataset and Xj to denote the j-th"}, {"title": "4 Desiderata", "content": "Desired properties of XAI desiderata have been well analyzed in works such as Adadi and Berrada (2018), Arrieta et al. (2020) and Sokol and Flach (2020). In this section, however, we focus on properties that are unique to cohort explanations. Table 1 lists these properties and compares these desired properties of cohort explanations with their counterparts on a global or local scale.\nGeneralizability Global explanation is prone to aggregation bias, where the average importance may not reflect the representative population in the dataset (Herbinger, Bischl, and Casalicchio 2023). Cohort explanation should aim to reduce such bias and to generate explanations that generalize well in each of the cohorts. If we apply a local method to all samples in a specific region, the explanation should be of low variance and close to the cohort explanation.\nTo evaluate generalizability, we use the average error w.r.t. local importance as the metric. Given a dataset X, a local importance method w, cohorts {Xj} and the respective explanations {\u0113j}, the loss is defined as\n$L_{generalizability} = \\frac{1}{|X|} \\sum_{j=1}^{k} \\sum_{x \\in X_j} ||w_M(X_j, x) - e_j||$ (1)\nConciseness One key goal of cohort explanation, especially cohort explanations, is to generate a concise explanation. It is evaluated using the number of cohorts and should be low without significantly sacrificing generalizability.\nDisjoint Cohorts Cohort explanation should yield meaningful cohort definitions. Although cohorts can be defined using samples, the regions represented by these assignments should divide the feature space into disjoint subspaces. Otherwise, it would be difficult to describe what a cohort represents. This adds an additional challenge in defining the cohorts in addition to the generalizability criteria.\nCohort Locality The explanation of a cohort should emphasize the decision-making local or specific to this region. That is, if two models have the same behavior within a given cohort X (i.e., both models predict the labels for all samples in X), but they differ outside X, the cohort explanation for the two models in this fixed cohort should be the same.\nHowever, this property does not hold in general for local explanation methods. Popular methods such as LIME (Ribeiro, Singh, and Guestrin 2016) and KernelSHAP (Lundberg and Lee 2017) require sampling, which may sample outside the cohort X. In this case, a good cohort explanation should try to preserve locality by stabilizing the explanation if a cohort is fixed.\nTo evaluate locality, given a fixed cohort Xj, we create alternative models Mj that behave the same as the original model M within Xj, but have the probability to be random otherwise. Specifically,\n$M_j(x) = \\begin{cases}\nM(x), & x \\in X_j, \\\\\n\\tilde{M}(x), & x \\notin X_j, \\text{w/ probability } 1-p, \\\\\n\\text{random}, & x \\notin X_j, \\text{w/ probability } p.\n\\end{cases}$ (2)\nwhere the last case denotes a random class for classification tasks or a random value sampled from the distribution of labels for regression tasks. We then consider the average difference in explanation on cohort X w.r.t. a probability p:\n$L_{locality}(p) = E\\left[\\frac{1}{k} \\sum_{j=1}^{k} ||e_j - \\tilde{e_j}||^2\\right]$ (3)\nwhere ej is evaluated on explainee M and \u0113j is on Mj, and the expectation is over the randomness from p. An ideal cohort explanation should have a low locality loss and thus focus on localized information.\nStability The explanation should be robust w.r.t. (1) the explainee model and (2) the cohort definitions. The cohort definitions should be stable and the importance should not drastically differ when the definitions are slightly changed.\nTo evaluate cohort stability, we tested the robustness of cohort definitions against the same input and then use the Adjusted Rand Index (ARI) (Santos and Embrechts 2009) to compare the similarity between two assignments. To evaluate, we conduct t run on the same dataset, and the metric is defined as\n$S_{cohort stability} = \\frac{1}{t} \\sum_{i=2}^{t} ARI(\\left\\{a^i\\right\\}, \\left\\{a^{i-1}\\right\\})$. (4)\nwhere a denotes the assignment computed on the i-th run.\nTo evaluate importance stability, we consider the change in explanation in a particular cohort if a new random sample is added to this cohort. Namely, we evaluate importance stability as\n$L_{importance stability} = E_x \\left[ \\frac{1}{k} \\sum_{j=1}^{k} ||e_j - e_{X_j \\cup \\{x\\}}||^2 \\right]$ (5)\nwhere $e_{X_j \\cup \\{x\\}}$ is evaluated on the artificial cohort by appending sample x to the cohort Xj. Note that Eqs. 4 and 5 measure two different aspects of cohort explanations. Cohort stability estimates the robustness on how the data are clustered, while importance stability concerns the robustness of explanations on predefined cohorts."}, {"title": "5 Problem Definition", "content": "We emphasize on generate post-hoc cohort explanation through existing local, data driven, feature importance ex-"}, {"title": "6 Challenges", "content": "We now discuss two major challenges unique to cohort explanations, which our algorithm aims to overcome.\nNaive Average of Local Explanation Does Not Suffice Given a particular cohort definition, the naive way to compute the cohort explanation is to average the local explanation of all members. However, this may fail the cohort locality criteria discussed in Sec. 4, since local explanations"}, {"title": "7 Approach", "content": "To solve these two problems, we propose a generalized cohort explanation conversion framework (CohEx) to adapt data-driven, local, feature importance methods into cohort explanation. We allow modification to local importance to account for locality, and design a feedback loop between re-"}, {"title": "8 Evaluation", "content": "To evaluate the proposed method, we experiment on three scenarios: (1) the patient classification problem in Sect. 1, a small scale example to demonstrate the importance of using supervised clustering and revising importance; (2) bike sharing, a classical ML regression problem to predict the hourly number of rented bikes (Fanaee-T 2013), and (3) MNIST digit classification (Deng 2012), a vision-based task with deep learning to mimic more realistic and complex scenarios. For baselines, we compare with the following methods:\n\u2022 VINE, which applies supervised clustering on the local importances scores (Britton 2019);\n\u2022 REPID, a tree-based partitioning method (Herbinger, Bischl, and Casalicchio 2022). For the sake of comparison, we modify the algorithm to use a local explainer that is consistent with other method, instead of the original partial dependency explainer;\n\u2022 GALE, a homogeneity-based re-weighting mechanism to improve the quality of importance aggregation in classification problems (Van Der Linden, Haned, and Kanoulas 2019). GALE is not a cohort explainer by definition, but we feed the GALE importance into VINE and REPID to assess the quality of the reweighed importance. Note that GALE is not applicable to regression problems;\n\u2022 Hierarchical cohort explanation: first compute the local importance, then run supervised clustering with SRIDHCR once. The difference between this method and the proposed CohEx is that it does not iteratively recompute the local importance scores."}, {"title": "8.1 Synthetic Patient Classification", "content": "We continue to use the motivating example in Sect. 1. For this evaluation, the expected number of cohorts k* is set to 4, and the maximum depth of the tree in REPID is set to 2 so that the number of cohorts is consistent for all methods. We use LIME as the base method, specifically since it does not satisfy the cohort locality criteria and allows us to compare the effect of different algorithms on locality. Fig. 3 and Table 2 show the result of running the algorithms on this example. The cohort partitioning and explanations are shown in Fig. 3. Additionally, the evaluation against different k* can be found in Appendix C.1."}, {"title": "8.2 Bike Sharing", "content": "This experiment focuses on a realistic machine learning regression problem. The bike sharing task asks to predict the hourly number of rented bikes between years 2011 and 2012 in the Capital bikeshare system based on seasonal and weather information. For the explainee model, we train an XGBoost model (Chen and Guestrin 2016) on this dataset and achieve a $R^2$ score of 0.9453. We continue to use $k^* = 4$ or a depth of two for a similar number of cohorts and use SHAP as the local explainer.\nFig. 4 shows the result of running our proposed algorithm on the XGBoost model, including both the explanations and the characteristics to each cohort. Comparison with other methods can be found in the Appendix. C.2. Our algorithm can detect clusters with distinct characteristics and importance signals, while maintaining a coherent explanation within each cohort."}, {"title": "8.3 Digit Classification", "content": "We evaluate based on MNIST (Deng 2012). Note that the MNIST inputs are cropped, centered, and relatively simple, so measuring the distance between importance heatmaps is meaningful. We built the explainee model as a neural network with two convolutional layers and two fully-connected layers and achieved an accuracy of 93.93% on the test dataset. More details on the model and a visualization of the cohort explanation can be found in Appendix. C.3."}, {"title": "8.4 Evaluation Metrics and Analysis", "content": "Generalizability & Conciseness CohEx achieves the best performance based on the objective function in Eq. 6. Note that all methods yield $k = 4$, so the conciseness loss for all is zero. VINE yields a low generalizability loss since its clustering criterion focuses only on feature importance, while all other methods need to consider the distance in the feature space. However, CohEx is able to exceed VINE since they use different local importance scores Wr. Through the recomputing of importance, CohEx edits the importance by putting more emphasis on local structures, allowing the importance scores in each cohort to have lower variance. Introducing GALE re-weighting improves the performance of VINE and REPID since it also recomputes importance, but it does not perform well on low-dimensional features as shown in Fig. 3 since the importance of the feature with the lowest homogeneity will always be set to zero.\nCohort Locality CohEx yields a significantly lower locality loss compared to the other methods according to Eq. 3, which is as expected since CohEx introduces an iterative refinement process specifically for locality. To further justify the approach, in the patient classification problem, CohEx identifies a clustering that mostly match with the three clusters in our ideal explanation proposed in Sect. 1. In particular, CohEx is the only algorithm that identifies a region (the blue cohort in Fig. 3f) in which family history is the more important feature. Note that for almost all samples, the static LIME importance of age is higher than family history similar to the result in Fig. 1b. Therefore, since other methods are based on a static set of local importance scores, they cannot find any regions with a meaningful family history importance. We further justify that such a region should exist, since if we only consider said region, the horizontal decision boundary is the dominant decision boundary. This shows that CohEx's explanation is more local to its cohorts. We also note that applying GALE would harm locality, since the re-weighting process smoothes the importance over the whole dataset, specifically contradicts the goal of locality.\nDisjoint Cohorts Only VINE fails this property since it clusters solely based on importance and it's very likely that some distant samples happen to have similar explanations and thus are partitioned together. All other method guarantees that the cohorts can be separated into disjoint areas by hyperplanes.\nCohort Stability In all scenarios, REPID achieves higher ARI scores based on Eq. 4, which shows that our supervised clustering-based approaches introduce variance. We consider this variance to be inherent to the methods, as both the recomputing process and SRIDHCR are trial-and-error by nature. The question of how to improve the stability remains a challenge for future work.\nImportance Stability It measures the change in cohort explanations, if a random sample is added to the cohort. For fair comparisons, we allow the baseline methods to recompute the importances similar to CohEx after adding the sample, which greatly improve their stabilities. CohEx still exceeds the baselines. This demonstrates that even though the partitions of CohEx is not as robust based on the previous cohort stability metric, on a fixed cohort, CohEx's explanation is much less gullible to changes or noises in the dataset."}, {"title": "9 Conclusion", "content": "In this paper, we identify a less explored piece in the XAI literature, i.e., cohort explanation. We illustrate the desired properties and challenges unique to the cohort explanation. We proposed a unified framework CohEx to generate such explanation by transferring existing local explanation methods while preserving generalizability and locality. It showed an increase in most metrics compared to baseline methods.\nThe current limitations to the algorithm include (1) the stability of cohort definitions; (2) the high time complexity in more complex scenarios, such as vision tasks; (3) the limits of the framework to local data-driven explainers. For future directions, alternatives can be considered, such as using non-data-driven, non-local, or non-post-hoc explanation methods to generate cohort explanation, increasing generalizability to more complex tasks. Additionally, the meaningfulness of the cohort definitions can be potentially increased through enforcing stricter constraints on the definitions, such as only partitioning the samples using tags, rules, or hyperplanes."}, {"title": "A Complexity Analysis of CohEx", "content": "To ensure that the proposed CohEx algorithm terminates, it terminates when the supervised clustering loss does not decrease for a set number of iterations. Under this termination scheme, the complexity of CohEx is then $O(nmkT)$, where n is the number of trials, m is the average number of iterations within each trial, k is the average number of cohorts throughout the clustering process, and t is the average run time of applying the base local algorithm on each of the cohorts. Note that both m and T are directly affected by the choice of the local explainer. T is usually related to the number of features or the size of the cohorts, while the stability of the algorithm may affect m. It is especially costly when the local explainer requires sampling, such as KernelSHAP. We suggest to tune the hyperparameter of the base method, such as the sampling size, for such algorithms."}, {"title": "B CohEx w/ Non-Data-Driven Methods", "content": "Although we focus on adapting data-driven methods, CohEx can be modified if any of these premises cannot be met. If the"}, {"title": "C Additional simulation results", "content": "In this section, we provide additional details of the synthetic patient classification experiment and additional results using alternative base local explanation methods."}, {"title": "C.1 Synthetic Patient Classification", "content": "In this section, we provide additional details of the synthetic patient classification experiment and additional results using alternative base local explanation methods.\nData and model generation The dataset in Fig. 1a is generated as follows: let $A_i$ denote the age of the sample xi and $H_i$ denote its value of family history. Then\n$A_i \\sim 10B_i + Z_i$ (9)\n$B_i \\sim Discrete(0,..., 9; \\\\\n0.14, 0.15, 0.14, 0.15, 0.15,\\\\\n0.11, 0.07, 0.06, 0.02, 0.01)$ (10)\n$Z_i \\sim Unif(0, 10)$ (11)\n$H_i \\sim Unif(0, 1)$ (12)\nWe assume a optimal decision boundary of\n$f(A, H) = (4(A/100)\u00b2 + (0.75H)\u00b2)^{0.5} = 0.4$ (13)\nAnd the label is generated as\n$Y_i | f(A_i, H_i) < 0.4 = \\begin{cases}\n0, & \\text{with prob. 0.8}, \\\\\n1, & \\text{with prob. 0.2}\n\\end{cases}$ (14)\n$Y_i | f(A_i, H_i) \\geq 0.4 = \\begin{cases}\n0, & \\text{with prob. 0.2}, \\\\\n1, & \\text{with prob. 0.8}\n\\end{cases}$ (15)\nGeneralizability vs. conciseness Fig. 5 shows the change in generalizabilty loss for the four algorithms on this example. For the two algorithms based on supervised clustering, the final number of cohorts k is the same as the expected number of cohorts k*, so the loss of clustering is exactly the same as the generalizability loss component in Eq. 1. The rankings of the four algorithms remain the same regardless of the number of cohorts.\nWe can interpret this figure as the balance between generalizability vs. conciseness. When the number of cohorts is small, the result approaches a global explanation with a concise but less quality explanation; when the number of cohorts becomes larger, the result approaches local explanation, whose explanation is more accurate but less concise. The selection of the desired number of cohorts k* represents the desired granularity of the cohort explanation.\nOn average, the generazability loss of CohEx is able to converge much faster than the other algorithms due to its iterative local importance recomputing process, although the combination of supervised clustering and iteration introduces variance to the system, especially at high k*, which is related to the cohort stability metric. Addressing the robustness of the framework remains one of the main goals for our future direction."}, {"title": "Alternative base method", "content": "In Sect. 8, we evaluated CohEx with other baseline methods while using LIME as the base local explainer. In this section, we provide additional evaluations if we use SHAP as the base method.\nTable 3 shows the evaluation of the proposed criterion using SHAP. The comparison between the methods is mostly identical, though CohEx is now performing worse based SHAP. This is because the distribution of the base SHAP values is much less similar among the samples, which can be seen from the distribution in Fig. 6. VINE and REPID are able to partition the samples almost perfectly, and thus achieve a near-zero penalty. The drawback is that the reported importance scores are less informative: a few cohorts would yield the same importance. CohEx on the other hand recomputes the importance scores based on local cohorts, and thus reports a more nuanced importance, though at the cost of a slightly worse clustering penalty."}, {"title": "C.2 Bike sharing", "content": "In this section, we provide additional details about the bike sharing experiment."}, {"title": "C.3 Digit Classification", "content": "We also evaluated our algorithm on a vision-based task, that is, digit classification in MNIST (Deng 2012). Note that the MNIST inputs are cropped, centered, and relatively simple, so measuring the euclidean distance between the input images xi / the local explanation heatmaps w(X,xi) will be meaningful. This may not be true for more complicated tasks, where semantically similar images/heatmaps may not be close to the raw pixels. For these tasks, an alternative distance measurement should be used instead of the Euclidean distance.\nWe built the explainee model as a simple convolutional neural network with two convolutional layers followed by two fully connect layers. The convolutional layers each have a kernel size of 5, and maps to 10 and 20 channels, respectively. Both layers are followed by a max-pooling layer and then a ReLU activation, while the second layer has an additional dropout layer between the convolutional and the pooling layers. The fully connected layers map the final 20\u00d74\u00d74 pixels to 50 features, then a ReLU activation and a dropout are maps to ten class outputs by another fully connected layer. We trained the model for two epochs with an SGD optimizer and achieved an accuracy of 93.93% on the test dataset. The model is trained using the full training dataset with all digits.\nTo demonstrate the properties of cohort explanations, we only include a small set of 200 images of digits 7 and 9 during explanation evaluation. This experiment aims to see if CohEx can focus on pixels that separate these specific digits. Note that the network is trained using the full 10-digits and outputs a probability for each of the classes. We opt to use DeepLIFT (?) as the base explainer, an approximation of SHAP on deep models. DeepLIFT will generate ten feature importance images with respect to all ten classes, even if only two classes are present in the testing dataset. We set the expected number of cohorts to 4, allowing the framework to detect interesting subgroups within each digit class.\nFig. 8 shows the result of applying the cohort explanation algorithms in the MNIST subset. The left images show the first 10 members of each cohort, and the left images display the importance w.r.t. each of the ten classes corresponding to the cohorts. Note that clustering on high-dimensional data such as images is a challenging topic, and thus the performance of all methods degrade: all methods yields unbalanced partitions. The VINE's result is the most severe, where it fails to find a meaningful partitioning, and three out of the four cohorts contains very few sample. We note that, though limited by the performance of the clustering algorithm, CohEx's explanation is more specific to this specific task. All other methods contain a nontrivial importance for classes other than 7 and 9, which is what DeepLIFT produce based on the underlying model. However, CohEx's unique importance recomputing step allows it to refine the importances, and reduces the importance for nonexisting classes. This emphasis on locality can be especially crucial for generating explanation that are more specific to downstream tasks on models that are pre-trained on a large dataset or a complicated task."}]}