{"title": "NAT-NL2GQL: A Novel Multi-Agent Framework\nfor Translating Natural Language to Graph Query\nLanguage", "authors": ["Yuanyuan Liang", "Tingyu Xie", "Gan Peng", "Zihao Huang", "Yunshi Lan", "Weining Qian"], "abstract": "The emergence of Large Language Models (LLMs)\nhas revolutionized many fields, not only traditional natural\nlanguage processing (NLP) tasks. Recently, research on apply-\ning LLMs to the database field has been booming, and as\na typical non-relational database, the use of LLMs in graph\ndatabase research has naturally gained significant attention.\nRecent efforts have increasingly focused on leveraging LLMs to\ntranslate natural language into graph query language (NL2GQL).\nAlthough some progress has been made, these methods have\nclear limitations, such as their reliance on streamlined processes\nthat often overlook the potential of LLMs to autonomously\nplan and collaborate with other LLMs in tackling complex\nNL2GQL challenges. To address this gap, we propose NAT-\nNL2GQL, a novel multi-agent framework for translating natural\nlanguage to graph query language. Specifically, our framework\nconsists of three synergistic agents: the Preprocessor agent, the\nGenerator agent, and the Refiner agent. The Preprocessor agent\nmanages data processing as context, including tasks such as\nname entity recognition, query rewriting, path linking, and the\nextraction of query-related schemas. The Generator agent is\na fine-tuned LLM trained on NL-GQL data, responsible for\ngenerating corresponding GQL statements based on queries and\ntheir related schemas. The Refiner agent is tasked with refining\nthe GQL or context using error information obtained from the\nGQL execution results. Given the scarcity of high-quality open-\nsource NL2GQL datasets based on nGQL syntax, we developed\nStockGQL, a dataset constructed from a financial market graph\ndatabase, which will be released publicly for future researches.\nTo demonstrate the effectiveness of our proposed NAT-NL2GQL\nframework, we conducted experiments on the StockGQL and\nSpCQL datasets. Experimental results reveal that our method\nsignificantly outperforms baseline approaches, highlighting its po-\ntential for advancing NL2GQL research. The StockGQL dataset\ncan be accessed at: https://github.com/leonyuancode/StockGQL.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph data is gaining prominence in modern data science\ndue to its ability to uncover complex relationships, enhance\ninformation connectivity, and facilitate intelligent decision-\nmaking. With its distinctive relational representation and ef-\nficient processing capabilities, graph data proves invaluable\nacross diverse fields such as finance, healthcare, and social\nnetworks, particularly for managing highly connected and\nstructurally complex data [1], [2]. Relational databases (DBs)\nare purpose-built for the efficient storage and management\nof structured data, while graph data demands specialized\ngraph DBs to enable effective storage and processing [3].\nThese databases are specifically optimized for accessing and\nquerying graph-structured data, facilitating the efficient ma-\nnipulation of complex relationships. Popular graph databases,\nincluding Neo4j, NebulaGraph, and JanusGraph, offer distinct\nfeatures and applications, empowering users to better analyze\nand utilize intricate data relationships [4], [5].\nDespite the growing importance of graph data across various\ndomains, ordinary users often encounter significant challenges\nwhen interacting with graph DBs due to their specialized and\ncomplex operations. The lack of technical expertise among\nmany individuals prevents them from fully harnessing the\npotential of graph data, thereby limiting its broader adoption\nin real-world applications [6]. Furthermore, the intricate and\nnuanced syntax of graph query languages (GQL) presents\nadditional barriers, particularly for users attempting to translate\nnatural language (NL) into GQL-a task commonly referred\nto as NL2GQL. These challenges collectively make NL2GQL\na highly demanding problem [7], [8].\nan example of NL2GQL, showcasing its key components:\nnatural language understanding, DB schema comprehension,\nand the generation and execution of GQL. This highlights\nthe critical need for a system capable of automating the\nNL2GQL process. By lowering the barriers to entry, such\na system would empower users to seamlessly perform data\nqueries and analyses, thus promoting the widespread adoption\nand practical utilization of graph data.\nIn many cases, NL2GQL can be viewed as a special-\nized application of the sequence-to-sequence (Seq2Seq) task.\nAs such, modern approaches have progressed beyond ini-\ntial template-based methods to focus on generative models\nfor automatically producing GQL. Compared to template-\nbased techniques, generative models offer greater flexibility\nand accuracy, enabling the handling of more complex query\nrequirements. This advancement effectively meets the evolving\ndemands of application scenarios and rising user expectations.\nThe study [6] was the first to explore the application of a\nSeq2Seq framework for NL2GQL, utilizing both the Seq2Seq\nmodel and copying mechanism to generate GQL. Although\nthe accuracy of their proposed baseline method is relatively\nlow, just above 2%, they also introduced a general-domain"}, {"title": "II. PRELIMINARIES", "content": "NL2GQL Task Definition. We formally define a NL2GQL\ntask. The input consists of a NL query X and a graph DB G,\nwhich is represented as G = (s,r,o) | s, o \u2208 E,r \u2208 V. Here,\nE and V denote the sets of vertices and edges, respectively.\nThe objective is to generate a correct GQL query Q. Hence a\nNL2GQL system can be formulated as:\nQ = NL2GQL(X,G).\nHere Q is the GQL prediction.\nLLM-based NL2GQL Systems. Multiple LLM-based meth-\nods have been proposed to address the NL2GQL task. These\nmethods typically follow a unified paradigm that incorporates\ntechniques such as in-context learning. In-context learning,\nwhich is widely adopted in current models, enables LLMs to\ngenerate correct answers by including a few examples directly\nwithin the prompt. This approach can be formalized as follows:\nQ = LLM_{ICL}(I,E,X)\nHere, I represents the task description, & denotes the demon-\nstrations derived from annotated datasets, and X is the input\nquestion, which can optionally be empty."}, {"title": "III. STOCKGQL DATASET BUILD", "content": "The lack of high-quality open-source NL2GQL datasets\nhas hindered progress in this research area. While several\ndatasets based on Neo4j's Cypher syntax are available, Neb-\nulaGraph-a widely used, efficient, enterprise-grade open-\nsource distributed graph database-remains without datasets\nspecifically designed for its nGQL syntax. To bridge this gap\nand advance future research, we follow the approach in [7],\nwhich utilizes the self-instruct [19] method, to develop the\nStockGQL dataset based on a real-world financial stock sce-\nnario graph DB, which we crawled from stock trading websites\nand applied privacy processing to some named entities and real\ndata. As shown in Figure 2, our method consists of several\ncomponents.\nSchema Extraction. As Step 1 of\nillustrates, we\nextract the schema from the graph DB, which includes iden-\ntifying the nodes, edges, and their attributes. This step forms\nthe foundation for constructing a structured representation of\nthe graph, enabling further processing and query generation\nin subsequent steps. Subschema Extraction. A subschema is\nthe schema of a subgraph derived from the complete graph,\ncontaining only partial information from the graph's overall\nschema. Step 2\nis to extract a subschema from the\nschema. We apply specific rules to identify all possible path\ncombinations, ranging from 0-hop to 6-hop paths, to address\nthe limitation of the study [7], which does not guarantee that\nthe dataset covers all possible link combinations. Our approach\nensures a comprehensive coverage of potential relationships\nwithin the graph, enabling a more robust dataset for training\nand evaluation.\nData Generation. Step 3 illustrates the data gener-\nation module. The generation process algorithm is illustrated\nin Algorithm 7. We employ the ICL method, continuously\nsampling K data points randomly from the data pool to"}, {"title": "Algorithm 1: Masked NL-GQL Data Pairs Generation", "content": "Input: A set of subschemas; Data pool; Number of\ndemonstrations k; Iterations number m; Task\ndescription I; Sample size k; ICL formula $LLM_{ICL}$\nOutput: Updated Data pool with masked NL-GQL data pairs\n1 foreach subschema in subschemas do\nfor i = 1 to m do\nSample k items from Data pool;\nBuild demonstrations & using the sampled data;\nGenerate Masked NL-GQL Data Pairs;\nd_list $\\leftarrow LLM(I,E, subschema)$;\nAdd d_list to Data pool;\n2\n3\n4\n5\n6\n7"}, {"title": "IV. METHOD", "content": "In this section, we provide a detailed explanation of the\nworkflow of NAT-NL2GQL and the roles of each component.\nAs illustrated in\n, NAT-NL2GQL consists of three\nsynergistic agents: the Preprocessor agent, the Generator\nagent, and the Refiner agent. The Preprocessor agent is pri-\nmarily responsible for data preprocessing, extracting auxiliary\ninformation such as the related schema and written query\nto facilitate GQL generation. The Generator agent generates\nthe GQL based on the provided context. The Refiner agent\nrewrites the GQL directly or modifies the context content\nbased on error messages from GQL execution. The specific\ndata flow is as follows: The results from the Preprocessor\nagent are sent to both the Generator and Refiner agents. Error\nmessages from the execution results of the GQL generated by\nthe Generator agent are sent to the Refiner agent. Based on the\nreceived information, the Refiner agent determines whether to\nmodify the GQL directly or assemble the information and send\nit back to the Preprocessor agent. The three agents collaborate\nseamlessly to perform the task.\nIn the following sections, we will provide a detailed descrip-\ntion of each agent: the Preprocessor Agent in Section IV-A,\nthe Generator Agent in Section IV-B, and the Refiner Agent\nin Section IV-C."}, {"title": "A. Preprocessor Agent", "content": "As highlighted in [7], [8], extracting the NL-relevant schema\nfrom the complete graph DB schema provides three significant\nbenefits: first, it reduces the schema size, helping to avoid\ncontext length limitations in the model; second, it eliminates\nirrelevant schema noise, thereby enhancing the accuracy of\nthe generated GQL; and third, leveraging the relevant schema\ninstead of the full schema substantially decreases the time\nrequired for LLMs to generate GQL. The primary function\nof the Preprocessor agent is to extract schemas relevant to NL\nby utilizing both the schema information from the graph DBs\nand the associated data values. Additionally, it aligns named\nentities in the query with those in the graph DBs and rewrites\nthe query as needed. The Preprocessor agent includes LLM-\nbased NER, entity alignment, related schema revise, linking\ncompletion and question rewriting.\nLLM-based NER. Extracting named entities from NL is\nessential for identifying the related schema. Previous studies\nhave demonstrated that LLMs can effectively recognize named\nentities within sentences [20]\u2013[22]. Building on this capability,"}, {"title": "Entity Alignment.", "content": "After extracting named entities from nat-\nural language, the next step is to align these entities with the\ncorresponding entity names in the graph DB. This alignment\nensures that the extracted entities are accurately mapped to the\nrelevant nodes or edges in the DB, maintaining consistency\nand enabling precise query generation across both the natural\nlanguage and graph representations. Our approach begins by\nextracting named entities from each entity in the graph DB to\nbuild a dictionary D, where each key is an entity type, and\neach value is a list of names for entities of that type. Next, we\ncompare the name of each entity extracted in the initial step\nwith the names in this dictionary. If an exact match is found,\nthe corresponding entity type name is assigned. For entities\nwithout an exact match, we apply locality-sensitive hashing\n(LSH) [23] to select the name of the most similar entity. This\nprocess can be formulated as:\nD = LSH(Z,D, Y)\nd = arg max Cosine(Emb(X), Emb(di))\n di ED\nHere, Z denotes the extracted name entity from the NL via the\nLLM-based NER, D represents the entire entities dictionary\nextracted from the graph DB, and D represents the entities\nextracted with the LSH similarity to X according to a threshold\n\u03b3. Emb(X) represents the embedding of X encoded via all-"}, {"title": "Algorithm 2: Linking Completion Algorithm", "content": "Input: Graph Schema G = (V, E); Identified Entities\n$E_{identified}$; Identified Edges $R_{identified}$\nOutput: Connected Subgraph\nSG = ($V_{subgraph}$, $E_{subgraph}$)\n1 Function LinkCompletion (G, $E_{identified}$, $R_{identified}$):\n$V_{subgraph}\\leftarrow \\{\\}$\n$E_{subgraph}\\leftarrow \\{\\}$\nforeach entity vi \u2208 $E_{identified}$ do\n$V_{subgraph}\\leftarrow V_{subgraph} \\cup \\{V_i\\}$\nforeach edge rj\u2208 $R_{identified}$ do\n$E_{subgraph} E_{subgraph} \\cup \\{r_j\\}$\nforeach edge ek \u2208 $E_{subgraph}$ do\nforeach neighbor vi \u2208 neighbors(ek) do\n$V_{subgraph} \\leftarrow V_{subgraph} \\cup \\{v\\}$\n$E_{subgraph} E_{subgraph} \\cup \\{e_k\\}$\nwhile $V_{subgraph}$ is not connected do\nFind the minimum edge to add that connects\ntwo disconnected components\n$E_{subgraph} E_{subgraph} \\cup \\{min edge\\}$\nreturn SG = ($V_{subgraph}$, $E_{subgraph}$)\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15"}, {"title": "B. Generator Agent", "content": "Once the data pre-processing is complete, the next step\nis to generate the GQL based on the information obtained.\nParameter Efficient Fine-Tuning (PEFT) techniques have been\nshown to significantly reduce the number of fine-tuned pa-\nrameters and optimize memory usage while maintaining per-\nformance [24]\u2013[27]. As one of the most widely recognized\nmethods in PEFT, LORA [28] has found broad application\nin various fields, including natural language processing and\ngraph query generation. By fine-tuning only a small subset\nof parameters, LoRA enables efficient adaptation of large\npre-trained models, making it particularly effective for tasks\nlike GQL generation, where model size and computational\nefficiency are critical. Therefore, we choose LoRA for fine-tuning the selected base LLMs. As shown at the lower left\ncorner of\n, we concatenate the original NL, the\nrewritten NL, and the related Schema to create a prompt for\nfine-tuning the LLMs. It is worth noting that during training,\nwe use the golden related schema extracted from the labeled\nGQL, while during inference, the related schema is predicted\nby the Preprocessor agent. Additionally, the Generator agent\nis not restricted to fine-tuning LLMs; it can also fine-tune\nsmaller models or directly leverage closed-source APIs, such\nas ChatGPT-40."}, {"title": "C. Refiner Agent", "content": "Many studies have demonstrated that rewriting SQL\nqueries with syntax errors can improve the accuracy of\nText2SQL [29]\u2013[31]. However, these methods typically rely\non LLMs to rewrite queries with syntax errors. Through\nexperimental validation, we have found that such rewrites\noften involve only minor modifications to the original query,\nwhich may not be sufficient to address more complex issues.\nMoreover, the error information typically highlights only the\nfirst error encountered during execution, making it unsuitable\nfor handling queries with multiple errors. Most importantly,\nif the related schema or written query from earlier steps is\nincorrect, correcting the GQL syntax alone may not resolve\nthe issue, as it may still not align with the original query.\nIn such cases, the error information should be used to revisit\nthe auxiliary information extracted in earlier steps. Our ap-\nproach differs by using the error information to determine\nwhether to directly rewrite the GQL or package the\ninformation to the Preprocessor agent, treating both the\nGQL and error details as historical data for re-execution.\nIt is important to note that even after modifying the GQL,\nit may still contain errors. Therefore, we set a limit on the\nnumber of iterations in the process, and once the limit is\nreached, the process will terminate even if the GQL is still\nincorrect. The refine prompt is shown in\n. We enable\nthe Refiner agent to decide, based on the error information,\nwhether to modify the GQL directly or save the historical data\nto restart the entire process.\nAs shown in the figure, the Refiner agent directly corrected\nthe syntax error in the generated GQL.\nSo far, we have covered all aspects of NAT-NL2GQL. It\nis important to note that the three agents in NAT-NL2GQL\ncan be combined in various configurations, such as using only\nthe Generator and Refiner agents, or the Preprocessor and\nGenerator agents. However, the corresponding prompts would\nneed to be adjusted accordingly. Furthermore, the base LLM\nfor each agent can be chosen and replaced based on the specific\nneeds of the task."}, {"title": "VI. RELATED WORKS", "content": "NL2GQL is a typical NLP task that has emerged with the\nwidespread adoption of graph data and can be classified as a\nseq-to-seq task [6], [9]. Its primary function is to convert users'\nNL questions into GQL queries that can be executed on a\ngraph database. This task involves user queries understanding,\ngraph schema linking, and GQL generating [7], [8]. Early\nefforts focused on using hand-crafted rules to translate NL into\nGQL [32]. Modern approaches primarily incorporate state-of-\nthe-art (SOTA) models to optimize performance. We catego-\nrize LLM-based NL2GQL methods into two types: PLMs-\nbased methods and LLMs-based Methods."}, {"title": "B. Text2SQL", "content": "Text2SQL is a task in NLP that is quite similar to NL2GQL,\nas both involve transforming user queries into statements that\ncan be executed on a database. Recently, there have been\nmany efforts to apply LLMs to solve Text2SQL, and these\nmethods have achieved good results [38]\u2013[41]. However, there\nare significant differences between the two. The diversity\ninherent in GQL presents a series of challenges. Unlike SQL,\nwhich has a well-established and standardized query language\nfor relational databases, GQL lacks a unified standard [8].\nThis deficiency creates obstacles in various areas, including\ndataset construction, the development of models capable of\ngeneralizing across different databases, and the establishment\nof consistent training paradigms. There is a difference in\nquery objectives. NL2GQL aims to execute queries on graph\ndatabases, whereas Text2SQL targets relational databases.\nGraph databases feature more flexible data structures and\ncomplex relationships, requiring NL2GQL to manage a wider"}, {"title": "C. KBQA", "content": "Knowledge-Based Question Answering (KBQA) sys-\ntems utilize structured knowledge bases (KBs) to answer\nuser queries. SP-based methods, commonly referred to as\nNL2SPARQL, first transform natural language questions into\nSPARQL queries, which are then executed on the KB to\nretrieve results [42]. This approach shares similarities with\nNL2GQL; however, a significant difference between NL2GQL\nand NL2SPARQL in the KGQA domain lies in the complexity\nof data storage and query languages. Graph databases, which\nmanage data with intricate relationships, introduce additional\ncomplexity [7]. Furthermore, NL2GQL demands greater atten-ntities in graph databases can\nencompass a diverse range of attribute types [8]. NL2GQL\nis also characterized by complex graph modalities, a wide\nvariety of query types, and the unique nature of GQLs [8].\nAs a result, directly applying KBQA methods to the NL2GQL\ntask is impractical."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose the NAT-NL2GQL framework\nto tackle the NL2GQL task. Specifically, our framework\nconsists of three synergistic agents: the Preprocessor Agent,\nthe Generator Agent, and the Refiner Agent. Additionally,\nwe have constructed the StockGQL dataset based on a graph\ndatabase for NL2GQL. Experimental results demonstrate that\nour approach significantly outperforms baseline methods.\nOne promising research direction is to design more inno-\nvative methods for extracting the related schema for a query,\nsuch as leveraging advanced graph neural networks, utilizing\ncontext-aware techniques, or incorporating adaptive learning\nmechanisms to dynamically refine the extraction process based\non query complexity and context. Another direction is to\nincrease the context length of LLMs and enhance their ability\nto understand the schema of a graph database, enabling them\nto process larger, more complex queries and better capture\nthe relationships between entities within the schema for more\naccurate GQL generation, without the need for extracting the\nrelated schema."}]}