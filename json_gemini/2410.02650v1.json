{"title": "Undesirable Memorization in Large Language Models: A Survey", "authors": ["Ali Satvaty", "Suzan Verberne", "Fatih Turkmen"], "abstract": "While recent research increasingly showcases the remarkable capabilities of Large Language Models (LLMs), it's vital to confront their hidden pitfalls. Among these challenges, the issue of memorization stands out, posing significant ethical and legal risks. In this paper, we presents a Systematization of Knowledge (SoK) on the topic of memorization in LLMs. Memorization is the effect that a model tends to store and reproduce phrases or passages from the training data and has been shown to be the fundamental issue to various privacy and security attacks against LLMs.\nWe begin by providing an overview of the literature on the memorization, exploring it across five key dimensions: intentionality, degree, retrievability, abstraction, and transparency. Next, we discuss the metrics and methods used to measure memorization, followed by an analysis of the factors that con-tribute to memorization phenomenon. We then examine how memorization manifests itself in specific model architectures and explore strategies for mitigating these effects. We conclude our overview by identifying potential research topics for the near future: to develop methods for balancing performance and privacy in LLMs, and the analysis of memorization in specific contexts, including conversational agents, retrieval-augmented generation, multilingual language models, and diffusion language models.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, large language models (LLMs) have demonstrated remarkable advancements, driven by the scal-ing of model parameters, large amounts of data, or training paradigms [1, 2, 3, 4]. State-of-the-art models have exhibited capabilities across a broad spectrum of Natural Language Processing (NLP) tasks, consistently pushing the benchmark records in areas such as text generation, code synthesis, machine translation, question answering, and summarization [5, 6]. These models use self-supervised learning on massive datasets, enabling them to perform competitively or even surpass human-level performance in specific tasks [7, 8].\nDespite these impressive advancements of LLMs, re-searchers have pointed out the problematic features of these models, including hallucination [9], bias [10], and privacy and security [11]. In the context of data privacy, memorization is one of the most significant issues. Memorization in LLM refers to the model's tendency to store and reproduce exact phrases or passages from the training data rather than generating novel or generalized outputs. While memorization can be advantageous in knowledge-intensive benchmarks, such as factual recall tasks or domain-specific question answering [12], it introduces significant ethical and legal challenges: First, models may inadvertently reveal sensitive or private information included in their training data, posing significant privacy and security risks [13, 14]. Second, the ability of LLMs to repeat verbatim copyrighted or proprietary text from their training data raises issues related to intellectual property infringement [15, 16].\nThese challenges motivate the need to further explore memorization in LLMs to effectively tackle its associated challenges. We provide an overview of aspects related to memorization, emphasizing the need for further exploration of this topic, thereby balancing model performance with the risks of privacy breaches and ethical concerns.\nBefore recent advances in LLMs, memorization was ex-plored extensively as a topic in machine learning and deep learning. Usynin et al. [17] explore memorization in machine learning. They propose a framework to quantify the influence of individual data samples and detect memorization in var-ious learning settings. Wei et al. [18] provide a systematic framework for understanding memorization in deep neural networks, discussing LLM memorization as a type of deep neural network. Survey papers on the privacy and safety of LLMs often address memorization as a core phenomenon, framing it as both a privacy issue and a foundational factor that supports other challenges they explore [19, 20]. Hartmann et al. [21] provide an overview of memorization in general-purpose LLMs. They aggregate memorization-related topics from the copyright, privacy, security, and model performance perspectives. In our survey, we not only add more recent work, but we specifically focus on memorization as an undesirable phenomenon, with the aspects intentionality, severity of mem-orization, retrievability, and transparency of memorization. In addition, we provide an extensive and concrete research agenda for the near future. \nIn the following sections of this paper, we provide a com-prehensive exploration of memorization in LLMs, structured into six main sections. We begin with the Spectrum of Mem-orization[II], where we examine the concept from multiple perspectives including intentionality, retrievability, severity, abstraction, and transparency, offering a deep understanding of how memorization occurs. Next, in Measuring Memorization"}, {"title": "II. SPECTRUM OF MEMORIZATION", "content": "Although memorization of factual information can be help-ful for the model the model perform more accurately on benchmark tasks such as question answering, other data might be retained without any clear purpose, and this might cause issues with privacy and copyright. This raises questions about whether certain instances of memorization is deliberate (by design) or incidental and undesirable. Understanding the difference between deliberate memorization and unintended retention is crucial for both improving model design and addressing issues like privacy and data security [22, 23, 24].\na) Unintended memorization: Carlini et al. [25] explores the concept of unintended memorization in neural networks, identifying it as a form of memorization that occurs indepen-dently of overfitting and often early in the training process, before the learning has fully converged. To understand and measure this phenomenon, they introduced the \"exposure metric\" as part of a testing methodology (see Section III-A). It is important to note that the language modeling loss function itself is chosen in a way to be optimal when the language model's output are recreations of training data, however, for the unintended memorization, this happens even when the loss function still has some distance to its optimal value. Unin-tended memorization could lead to various issues including:\n\u2022 Security vulnerabilities: Unintended memorization of confidential information like API keys or passwords could lead to security breaches [25].\n\u2022 Copyright violation: Memorization of copyrighted mate-rial may lead to legal challenges, especially if the model can reproduce substantial portions of protected works [16].\nb) Deliberate memorization: Deliberate memorization, in contrast, refers to the intentional design of LLMs to retain specific types of information from their training data. This is often a desirable feature, as it allows models to store and utilize vast amounts of knowledge. Key aspects of deliberate memorization include:\n\u2022 Knowledge retention: LLMs are often deliberately de-signed to memorize facts, concepts, and general knowl-edge to enhance their performance in tasks like question answering and information retrieval [26].\n\u2022 Language generation: Deliberate memorization of lin-guistic patterns, grammar rules, and vocabulary is crucial for the model's ability to generate coherent and contex-tually appropriate text.\n\u2022 Alignment goals: Deliberate memorization can be utilized in the AI alignment phase to inject desired behaviors and values into the model [27].\nAs Ranaldi et al. [28] show in their experiments, deliberate memorization is generally beneficial for model performance, however, it also raises important considerations:\n\u2022\nBias and fairness: The choice of what information to deliberately memorize can introduce or amplify biases in the model output.\n\u2022 Transparency and auditing: Understanding what has been deliberately memorized is crucial for model interpretabil-ity and auditing purposes.\nWhile memorization in LLMs is widely discussed, there is no universally accepted definition. The definition of memo-rization can vary based on factors such as the level of detail required, the context in which the information is recalled, and the specific task or application at hand. This section discusses different severity levels of recall that are explored in the literature evolving around memorization in LLMs, including perfect memorization, verbatim memorization, and approximate memorization."}, {"title": "A. Intentionality", "content": "\u2022 Privacy risks: LLMs might inadvertently memorize and potentially reveal sensitive personal information present in the training data [16].\nB. Severity of memorization"}, {"title": "a) Perfect memorization:", "content": "Perfect memorization refers to a scenario where a model retains and can reproduce all training data exactly. Kandpal et al. [29] define Perfect Memorization as follows:\nDefinition (Perfect Memorization). A model is said to exhibit perfect memorization behavior if it only assigns non-zero probability to the samples it has been trained on. Sampling from a perfect memorization model is identical to sampling from the training data.\nThis form of memorization acts as an imaginary language model and is used as a baseline for the experiments.\nb) Verbatim memorization: Verbatim memorization is a more specific form of memorization that involves the exact reproduction of strings from the training data. It captures instances where the model outputs a training sample without any alterations. This type of memorization is straightforward"}, {"title": "Definition (Verbatim memorization).", "content": "a string s from the training set is defined as verbatim memorized if there exists a prompt p so that LM(p) = s.\nTirumala et al. [31] consider another version of this as \"Exact memorization\":\nDefinition. [Exact memorization] a context c = (p, s) from the training set is being memorized if Argmax(LM(p)) = s.\nThis definition is more strict and does not capture the memorization under different forms of sampling methods, rather it only considers the greedy sampling [IV-E].\nIt should be noted that these definitions do not impose any"}, {"title": "c) Approximate memorization:", "content": "Approximate memoriza-tion extends beyond verbatim memorization to include in-stances where the output is similar but not identical to the training data. Verbatim memorization does not capture the subtler forms of memorization, as it is too confined. For example, if two sentences differ by a minor detail, such as punctuation, a misspelled word, or a stylistic variation (e.g., American English vs. British English), these instances would fall outside the strict boundaries of verbatim memorization. However, human judges would likely consider these variations as memorized examples.\nIppolito et al. [32] define \"Approximate memorization\" as follows:\nDefinition (Approximate memorization). a suffix s for prefix p is labeled as approximately memorized if for generation g\nLM(p), BLEU(g, s) > 0.75.\nThey discuss that this threshold was chosen based on qualitative analyses of the samples. By adopting this definition of memorization, they show that the measurement of memorization can increase by a factor of two. However, they also note that this definition can lead to both false positives and false negatives when compared to human judgment, indicating a potential direction for future investigations.\nThe common way to analyze memorization in LLMs is through sampling. This involves selecting tokens probabilis-tically from the model's predicted distribution at each step, to see if a sequence of probabilistically generated tokens can be traced back to the training data. Since the LLM's input and output domains are both discrete, it is important to note that it might not be possible to extract all of the memorized content through given a finite number generation trials through sampling. Based on how we can make an LLM output the training data we can classify memorization into extractable and discoverable."}, {"title": "C. Retrievability", "content": "C. Retrievability"}, {"title": "a) Extractable memorization:", "content": "Extractable memorization refers to the ability to retrieve specific information from a model's training data without direct access to that data. Carlini et al. [23] defines \"Extractable memorization\" as follows:\nDefinition. (Extractable Memorization) Given a model with a generation routine Gen, an example s from the training set S is extractably memorized if an adversary (without access to S) can construct a prompt p that makes the model produce s (that is, Gen(p) = s) .\nA more specific form of this is k-extractable memorization:\nDefinition. (K-extractable Memorization) A string s is said to be k-extractable if it (a) exists in the training data, and (b) is generated by the language model by prompting with k prior tokens [33].\nAnalyzing extractable memorization usually involves two main challenges: Designing prompts that best elicit memoriza-tion in a model and verifying if the model output is indeed from the training data.\nResearch in this area has employed various strategies. Carlini et al. [13] recover training examples from GPT-2 by prompting it with short strings from the public Internet and manually verifying the outputs via Google search. This method confirmed the memorization of about 0.00001% of GPT-2's training data due to the labor-intensive verification process. Nasr et al. [34] conduct extensive analysis on Pythia, RedPajama, and GPT-Neo models [35]. They query these models with millions of 5-token blocks from Wikipedia and count for unique 50-grams that the model generates whether they exist in a combined dataset of the models' training data. Their method was more successful, showing that 0.1% to 1% of the models' outputs are memorized, with a strong correlation between model size and memorization abilities.\nb) Discoverable memorization: Discoverable memoriza-tion measures the extent to which models can reproduce their training data when explicitly prompted with data from their training set. Nasr et al. [34] suggests the following definition for \"Discoverable memorization\":\nDefinition. (Discoverable memorization) For a model with generation routine Gen and an example [p||s] from the training set S, we say that s is discoverably memorized if Gen(p) = s.\nDiscoverable memorization provides an upper bound for data extraction, compared to extractable memorization's lower bound. Ideally, discoverable memorization requires querying the model with its entire training set, which is computationally intractable. Also, noteworthy is that discoverable memoriza-tion differs from extractable memorization in that the prompt p is known to be from the training set.\nCarlini et al. [23] investigate the upper bounds of data extraction in GPT-Neo models through discoverable mem-orization. They find that (a) LLMs discoverably memorize roughly 1% of their training datasets; (b) There is a log-linear correlation between data extraction and model size,"}, {"title": "D. Abstraction", "content": "Abstraction in the context of memorization refers to the level of specificity at which information is retained and reproduced by language models. This spectrum ranges from highly specific, fact-based memorization to more generalized concept-level retention. This is related to the concept of verbatim memorization in Section II-B, but abstraction focuses more on information (semantics of facts) than on literal word use. In terms of abstraction, memorization can be categorized into two main types: factual memorization and conceptual memorization. These categories represent different levels of information granularity and have different implications for model performance, privacy concerns, and potential applica-tions.\na) Factual memorization: Factual memorization in lan-guage models involves accurately retaining and reproducing specific pieces of information from training data, often relying on context cues to recall details such as names, dates, and numbers or the relations between such entities. Petroni et al. [12], Jiang et al. [38], AlKhamissi et al. [39], Luo et al. [40] investigate the measurement of factual knowledge in LLMs by evaluating their comprehension of facts from knowledge bases using ranking metrics. Pezeshkpour [41] proposes two approaches of explicit and implicit instillation of factual knowledge in LLMs while proposing to use KL-divergence as a metric for measuring factual memory. Yu et al. [42] show that the frequency of facts in the pretraining data significantly influences a language model's behavior. They find that models are more likely to generate memorized capital cities for frequently mentioned countries, while being less responsive to in-context information. Additionally, they demonstrate that larger models tend to favor memorized answers over in-context details, even when the memorized facts are less frequent. Kang and Choi [43] also show that factual knowledge probing accu-racy in large language models is linked to subject-object co-occurrence. They reveal that LLMs often prioritize frequently co-occurring word pairs, which can override correct answers, particularly when those answers are infrequent. Their findings highlight the need to address co-occurrence bias in future model development.\nWhile factual memorization could be beneficial for the performance of LLMs in tasks namely question answering, it could potentially lead to various issues such as undesired bias and privacy and copyright violations.\nb) Conceptual memorization: Conceptual memorization involves the model's ability to internalize and generalize broader ideas, patterns, and concepts from its training data [4, 44, 45, 46]. That is, Lee et al. [47] explore the idea pla-giarism of GPT-2 model and empirically show that language models go beyond merely generating verbatim text from their training data; rather they also rephrase sentences and imitate ideas from different sources.\nCompared to factual memorization, conceptual memoriza-tion generally poses a lower risk of exposing specific, sensitive information. At the same time, it is more difficult to be detected and it may lead to perpetuation of biases present in the training data at a more systemic level.\nThe boundary between conceptual memorization and the understanding of the model (generalization) is subtle and still debated."}, {"title": "III. MEASURING MEMORIZATION", "content": "Measuring memorization in large language models is a complex task, primarily because we cannot easily compute the probability of all the samples present in the training data. To obtain an accurate measure of memorization in language models, we would ideally need to compare the model's output probabilities for every possible sequence against the training data. However, given the vast space of possible sequences, this is computationally intractable. Therefore, researchers have developed various methods to approximate and quantify mem-orization.\nThe Exposure Metric, introduced by Carlini et al. [30], provides a quantitative measure of how much a model has memorized specific sequences from its training data. This metric is particularly useful for assessing the memorization of rare or unique information.\nThe exposure metric has been widely adopted in mem-orization studies. To practically apply the exposure metric, researchers often use the \u201ccanary extraction test\" [30]. This involves inserting known 'canary' sequences into the training data and then measuring their exposure in the trained model. Helali et al. [48] propose the so-called \"d-exposure\" metric as a measure of memorization for discriminative tasks such as text classification, sinse in those situations we don't have access to the perplexity of a given text.\nInference attacks are another approach to measuring memo-rization, focusing on the model's ability to reveal information about its training data. These attacks typically perform under the assumption that when a model displays high confidence in"}, {"title": "A. Exposure metric", "content": "A. Exposure metric"}, {"title": "B. Inference attacks", "content": "B. Inference attacks"}, {"title": "a) Membership Inference Attacks (MIA):", "content": "These attacks aim to determine whether a specific data point was part of the model's training set. Shokri et al. [49] introduced this concept for machine learning models, and it has since been adapted for language models. While the MIA approach in the context of machine learning uses a \u201cshadow learning\" paradigm, this is not feasible for LLMs, since they are much heavier in size. Instead, methods for MIA on LLMs try to extract the certainty of the LLMs on the given text and use it as sign of membership in the training data, which is usually done by computing the perplexity metric on the sequence [50, 51, 52, 53, 54, 55, 56].\nb) Extraction Attacks: These attacks attempt to extract specific pieces of information from the model that were present in its training data. Different works have demonstrated the feasibility of such attacks on language models, showing that private information could be extracted through prompting with different strategies [25, 34, 56, 57, 58, 59].\nThe effectiveness of inference attacks can serve as a proxy measure for memorization. Models that are more susceptible to these attacks are generally considered to have higher levels of memorization.\nIn conclusion, while measuring memorization in language models remains a challenging task, techniques like the Expo-sure Metric and Inference Attacks provide valuable insights into the memorization behavior of these models. These meth-ods allow researchers to approximate the extent of memoriza-tion and assess potential privacy risks associated with large language models."}, {"title": "its outputs,", "content": "its outputs, those outputs are likely to be from the training data, which means the model has memorized them. These attacks can be categorized into two main types:"}, {"title": "C. Counterfactual memorization", "content": "Previous work analyzed the memorization of large language models on sensitive information (e.g. phone numbers) in the training data [13] or synthetically injected 'canaries' [30, 60]. However, not all the memorized texts are equally interesting. Zhang et al. [61] dig into a new kind of memorization which is counterfactual memorization. The idea is to see how the presence or absence of a sample of the dataset, affects the performance of the model on the same sample. This definition of memorization has similarities with the definition of differ-ential privacy. Differential privacy as mitigation strategy will be discussed in Section VI-C. In their experiments, the authors create different subsets of a bigger dataset and then they fine-tune the LM on each of these. Then they consider an item x from the datasets and based on the presence or absence of x in the subsets, they divide the subsets into two groups: IN and OUT. Then they test and report the performance on the IN and OUT group of models by averaging. Their experiments on 400 trained models show the counterfactual memorized data, are generally unconventional text such as all-caps, structured formats (i.e. tables or bullet lists), and multilingual texts. In conclusion, counterfactual memorization provides a novel perspective on understanding what language models retain from their training data."}, {"title": "IV. INFLUENCES AND DYNAMICS OF MEMORIZATION", "content": "Understanding the factors that influence memorization in large language models is crucial for developing more efficient, secure, and privacy-preserving systems. This section explores various aspects that affect memorization, from model archi-tecture to training processes.\nThe first significant factor influencing memorization is model size. Carlini et al. [23] and Tirumala et al. [31] have both demonstrated that larger models are more prone to mem-orization and do so more rapidly. This trend persists across different architectures and datasets. Carlini et al. [23] find that the relationship between model size and memorization grows consistently on a log-linear scale, with larger models memorizing a greater portion of the data.\nTirumala et al. [31] further highlight that larger models not only memorize more but do so faster in the training process. Interestingly, while memorization increases with model size, it does not necessarily correlate with improved performance. This was shown by Carlini et al. [23] by comparison of models with similar capacities but differing performance levels because of their architectures. Overall, the findings suggest that the ability of LLMs to memorize is strongly linked to their size, potentially due to the high capacity of these models to store detailed information from training data.\nThe nature of the training data heavily influences memo-rization. Lee et al. [62] develop tools to deduplicate training data and show that models trained on deduplicated data would produce memorized text ten times less frequently. Kandpal et al. [29] show that a sequence appearing 10 times in the training data is, on average, generated approximately 1000 times more frequently than a sequence that appears only once. A study done by Tirumala et al. [31] on memorization of different parts of speech, reveals that nouns and numbers are memorized significantly faster than other parts of speech, likely because they serve as unique identifiers for specific samples.\nCarlini et al. [13], Kandpal et al. [29], McCoy et al. [63] show that longer prompts increase the likelihood of triggering memorized sequences, making it easier for language models to regurgitate training data. Moreover, methods like prefix tuning [64] and prompt engineering have been employed to maximize memorization. Ozdayi et al. [65] introduce a novel approach using prompt tuning to control memorized content extraction rates in LLMs. Weller et al. [66] propose 'according-to' prompting, a technique that directs LLMs to ground responses in previously observed text."}, {"title": "A. Model capacity", "content": "A. Model capacity"}, {"title": "B. Training data characteristics", "content": "B. Training data characteristics"}, {"title": "C. Input and prompting strategies", "content": "C. Input and prompting strategies"}, {"title": "D. Tokenization", "content": "Kharitonov et al. [67] explore the impact of the tokenizer on memorization. They experiment with the size of the subword vocabulary learned through Byte-Pair Encoding (BPE) and demonstrate that increasing the subword vocabulary signifi-cantly affects the model's ability and inclination to memorize training data. Furthermore, models with larger vocabularies are more likely to reproduce training data when given specific prompts. The authors suggest that this effect stems from the reduction in sequence lengths as BPE vocabulary size increases."}, {"title": "D. Tokenization", "content": "D. Tokenization"}, {"title": "E. Sampling methods", "content": "In their experiments, Carlini et al. [13] initially opt for greedy sampling to maximize the regeneration of training data. One limitation is that this sampling scheme generates low diversity outputs; thus, they also experiment with the decaying temperature and Top-n sampling methods, the latter of which shows to be more successful. Yu et al. [68] experiment with different sampling schemes including Decaying temperature, Top-n, Nucleus-n and Typical- samplings [69, 70, 71] and use an auto-tuning method on these to find the optimal sampling method that yields to the maximization of training data reproduction."}, {"title": "E. Sampling methods", "content": "E. Sampling methods"}, {"title": "F. Fine-tuning and transfer learning", "content": "Mireshghallah et al. [72] evaluate how different fine-tuning methods full model, model head, and adapter fine-tuning vary in terms of memorization and vulnerability to privacy attacks. Their research, using membership inference and ex-traction attacks, finds that head fine-tuning is most susceptible to attacks, whereas adapter fine-tuning is less prone. Zeng et al. [73] conduct a comprehensive analysis of fine-tuning T5 models [74] across various tasks, including summarization, dialogue, question answering, and machine translation, finding that fine-tuned memorization varies significantly depending on the task. Additionally, they identify a strong link between attention score distributions and memorization, and propose that multi-task fine-tuning can mitigate memorization risks more effectively than single-task fine-tuning."}, {"title": "F. Fine-tuning and transfer learning", "content": "F. Fine-tuning and transfer learning"}, {"title": "G. Training process dynamics", "content": "Kandpal et al. [29], Zhang et al. [61] show that mem-orization grows consistently with the number of training epochs, which makes sense, as more epochs push the model to potential overfitting. Jagielski et al. [75] demonstrate that examples seen during the earlier stages of training are less prone to memorization and rather they are forgotten over time. These findings indicate that memorization increases with more training, while early-seen examples being more likely to be forgotten."}, {"title": "G. Training process dynamics", "content": "G. Training process dynamics"}, {"title": "H. Forgetting mechanisms", "content": "In machine learning, forgetting mechanisms are the pro-cesses through which models lose or discard previously learned information [76]. These mechanisms can occur un-intentionally as part of the natural training dynamics or be purposefully induced to meet specific objectives, such as improving model generalization or addressing privacy con-cerns [77, 78, 79]. Blanco-Justicia et al. [80] provide a recent, detailed overview of forgetting in LLMs.\na) Catastrophic forgetting: Kirkpatrick et al. [81] ini-tially introduced catastrophic forgetting in the context of neural networks and continual learning. They propose a method to protect important model weights to retain knowledge. This approach has been effective in maintaining performance on older tasks, even after long periods of non-use. Tirumala et al. [31] observe the forgetting mechanisms of a special batch through the learning process and show that it follows an exponentially degradation, reaching to a constant value baseline. They show that the mentioned baseline scales with the model size. Jagielski et al. [75] address the dual phe-nomena of memorization and forgetting in LLMs through stronger privacy attacks and several strategies for measuring the worst-case forgetting of the training examples. The study introduces a method to measure to what extent models forget specific training data, highlighting that standard image, speech, and language models do indeed forget examples over time, though non-convex models might retain data indefinitely in the worst case. The findings suggest that examples from early training phases, such as those used in pre-training large models, might enjoy privacy benefits but could disadvantage examples encountered later in training.\nb) Intentional forgetting techniques: As memorization could lead to privacy risks and copyright issues, intentional forgetting could be necessary in some situations. Methods like knowledge unlearning aim to selectively remove specific information from trained models without retraining it from scratch. Bourtoule et al. [82] introduce the SISA framework for efficient machine unlearning, which divides the training data into shards (shards partition the data into disjoint segments) and trains sub-models that can be easily retrained if data needs to be removed. For LLMs specifically, Chen and Yang [83] introduce lightweight unlearning layers into transform-ers, allowing for selective data removal without full model retraining. Pawelczyk et al. [84] introduce \u201cIn-Context Un-learning\", which involves providing specific training instances with flipped labels and additional correctly labeled instances as inputs during inference, effectively removing the targeted information without updating model parameters. Additionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently remov-ing harmful or undesirable knowledge from LLMs [85]. These methods not only enhance privacy and security but also ensure that the overall performance of the models remains intact, making them scalable and practical for real-world applications [86].\nIn conclusion, memorization in LLMs is influenced by a complex interplay of factors ranging from model architecture to training dynamics. Understanding these influences is crucial\""}, {"title": "H. Forgetting mechanisms", "content": "H. Forgetting mechanisms"}, {"title": "V. MEMORIZATION IN SPECIFIC MODEL ARCHITECTURES", "content": "While the target of most of the papers discussed in this survey is auto-regressive large language models, in this section we will overview the memorization in other types of language models, as it would help us gain a deeper understanding of this phenomena.\nEarly studies investigating privacy leakage through the memorization of masked language models (MLMs) yielded limited success. For instance, Vakili and Dalianis [88] attempt to extract patients' medical conditions from BERT models trained on clinical data, concluding that the risk of privacy leakage in BERT-based models is minimal. Similarly, Lehman et al. [89] conduct attacks on BERT models trained on the MIMIC-III corpus, reaching the same conclusion that stronger attacks may be necessary to trigger memorization in MLMs. Jagannatha et al. [90] employ membership inference attacks on clinical MLMs, demonstrating that these models are far less vulnerable to such attacks compared to auto-regressive (generative) models. Carlini et al. [23] extend their previous work on auto-regressive language models by conducting ex-periments on the T5 [74] model family, trained on the C4 corpus. They demonstrated that scaling laws still apply, but the degree of memorization in MLMs remains significantly lower compared to similarly sized auto-regressive models. Mireshghallah et al. [87] introduce \u201clikelihood ratio mem-bership inference attacks\" and show that privacy risks in MLMs, while smaller, can still be significant if the attacker uses more advanced methods.\nIn studies exploring memorization in aligned language mod-els, experiments typically use either targeted prompting or random prompting approaches. However, these methods face two key challenges that limit their effectiveness."}, {"title": "A. Masked language models (MLM)", "content": "A. Masked language models (MLM)"}, {"title": "B. Aligned language models", "content": "First, the role tag which is prepended to both input queries and model responses disrupts traditional attack methods, such as the prefix prompting methods. Second, alignment training further complicates extraction attempts. Even if attackers could bypass the role tag, the model's alignment training, designed to ensure ethical responses, reduces the likelihood of disclosing sensitive or inappropriate information.\nTo address these challenges, Nasr et al. [34] introduce the \"Divergence Attack.\" This method asks the model to endlessly repeat a specific word, causing it to break from its alignment training and revert to its original language modeling objectives. In some cases, this led to the verbatim reproduction of training data. Notably, this only occurs when single-token words were repeated, as multi-token prompts led to the model continuing the repetition without reproducing training data."}, {"title": "B. Aligned language models", "content": "B. Aligned language models"}, {"title": "C. Retrieval augmented generation (RAG)", "content": "Retrieval-augmented language models generate text distri-butions referencing both the parameters of the underlying language model and the information retrieved from a document index. Huang et al. [91] analyze privacy issues in retrieval-based language models. They use the kNN-LM [92] as their target model through API access. They investigate a scenario in which a model creator has a private, domain-specific data-store that improves model performance on domain-specific tasks, but may also contain sensitive information that should not be revealed. They analyze the access to the text completion and perplexity calculation APIs of the model. Their findings show that kNN-LMs are more prone to disclosing private data from their private data-store compared to parametric models. Zeng et al. [93] conduct targeted and prefix attacks on Retrieval-Augmented Generation (RAG) frameworks to examine how their memorization behaviors differ from those of standard LLMs. The results showed that incorporating retrieval mechanisms significantly lowers the model's mem-orization capacity, reducing the success rate of training data extraction attacks in both targeted and prefix attack scenarios. This suggests that the use of retrieval-based data in RAG models makes them more resilient to such attacks compared"}, {"title": "C. Retrieval augmented generation (RAG)", "content": "C. Retrieval augmented generation (RAG)"}, {"title": "VI. MITIGATING MEMORIZATION: STRATEGIES AND TECHNIQUES", "content": "As discussed in earlier sections, memorization is influenced by a range of factors and it is sometimes necessary for the learning process [94", "62": "run exact matching and approximate match-ing (MiniHash"}]}