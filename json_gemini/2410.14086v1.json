{"title": "IN-CONTEXT LEARNING AND OCCAM'S RAZOR", "authors": ["Eric Elmoznino", "Tom Marty", "Tejas Kasetty", "Leo Gagnon", "Sarthak Mittal", "Mahan Fathi", "Dhanya Sridhar", "Guillaume Lajoie"], "abstract": "The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best-a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning-an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved.", "sections": [{"title": "1 Introduction", "content": "The goal of machine learning (ML) is to learn models that generalize to unseen data. Longstanding theory shows that minimizing training error alone can lead to overfitting and poor generalization (Bishop & Nasrabadi, 2006). \u03a4\u03bf enable better generalization, ML follows the principle of Occam's razor\u2014the best explanation is the simplest one that explains the observations (Hutter, 2010; Rathmanner & Hutter, 2011; Sunehag & Hutter, 2014). The intuition is that simple rules that explain the data cannot simply memorize observations, and must instead capture more general patterns. Consequently, learning algorithms usually trade off low training error and low model complexity with ad hoc approaches (e.g., via regularization and inductive biases), motivating the need for notions of complexity that can be tractably minimized directly.\nAlthough there exist mathematical notions of model complexity such as VC dimension or Kolmogorov complexity,\nthese quantities cannot be directly minimized, or even tractably computed for the latter. In practice, we instead learn predictors that minimize training error as well as proxies of the model's complexity, such as the L\u2081 or L\u2082 norm of the model parameters, or rely on inductive biases for low-complexity solutions that are implicit in the model class and learning algorithm. Defying this trend, however, pretrained large language models (LLMs) have a surprising ability to rapidly learn and generalize from small amounts of data presented in their context (or prompt) (Radford et al., 2019). This ability called in-context learning (ICL) is typically explained through the lens of memory-based meta-learning (e.g., Chan et al., 2022; Xie et al., 2022), a theoretical framework where sequence models are explicitly trained to learn statistical models from sequences of observations.\nThe main contribution of this paper is to provide theoretical arguments linking ICL to Occam's razor and a preference for simple models. Briefly, our theory frames ICL as a meta-learning algorithm whose next-token prediction objective"}, {"title": "2 Occam's razor and In-context learning", "content": "In this section, we introduce a meta-learning objective that directly targets simple models, and then show that it is equivalent to the next-token prediction objective underlying ICL. We reach this result via four key steps:\n1. We begin by formalizing both training error and model simplicity through the lens of Kolmogorov complexity, which deals with optimal data and model compression.\n2. We then show how learning algorithms can be used to compress data through a technique called prequential coding (Blier & Ollivier, 2018), and that minimizing the resulting \u201cprequential code length\" achieved by a learning algorithm is equivalent to jointly minimizing the training error and complexity of the model it fits.\n3. We then introduce the idea of finding a learning algorithm that minimizes prequential code length by formalizing a meta-learning problem that appears difficult to optimize.\n4. Finally, we show that the next-token prediction objective underlying ICL already solves this meta-learning problem in an efficient and scalable way."}, {"title": "2.1 Kolmogorov complexity and data compression", "content": "Kolmogorov complexity (Kolmogorov, 1965; Li et al., 2008) is a notion of information quantity. Intuitively, the Kolmogorov complexity $K(x)$ of an object $x$ is the length of the shortest program (in some programming language) that outputs $x$. A related notion is the conditional Kolmogorov complexity $K(x|y)$ of the object $x$ given another object $y$, which is the length of the shortest program that takes $y$ as input and outputs $x$. Finally, the Kolmogorov complexity of encoding two objects jointly is denoted $K(x, y)$. While quite abstract, this notion of complexity has deep ties to compression, making it intuitive as a measure of information quantity. The smaller and more \u201cstructured\" an object is regularity, patterns, rules, etc. the more easily it can be described by a short program, correspondingly having lower Kolmogorov complexity. Although Kolmogorov complexity is very general-objects $x, y$ can be datasets, programs, models-it is intractable to compute. However, it can often be tractably estimated or bounded, as we will show below.\nA quantity relevant to ML is the joint Kolmogorov complexity of a dataset $D = (d_1, ..., d_n)$ and of a generative model $p(d)$, where each sample $d_i \\in D$ is drawn iid:\n$K(D,p) = K(D|p) + K(p)$,\nwhere $K(p)$ refers to the complexity of the model (i.e., the length of the shortest program that outputs the function describing probability distribution $p : D \\rightarrow R^+$). This term is intractable to compute as it requires an enumeration over all programs that output $p$, but the conditional complexity $K(D|p)$ can be easily computed. According to (Gr\u00fcnwald, 2007), if the dataset is sufficiently large and the generative model is close to the true data distribution, the optimal method for compressing a data point $d_i$ uses only $\u2013 log_2 p(d_i)$ bits (e.g., using an arithmetic coding scheme, Witten et al., 1987), as in the case of Shannon information (Shannon, 2001). As such, we have $K(D|p) \\approx \u2212 \\sum_D log_2 p(d_i)$ which is the negative log-likelihood of the data under $p(d)$, a commonly used objective function in ML. It follows that models which achieve lower error under this objective better compress data. We provide further background on Kolmogorov complexity in Appendix A.\nAs we are interested in model optimization, we henceforth consider parameterized models $p_\\theta$ with parameters $\\theta$. We denote a learning algorithm by a function $T : P (D) \\rightarrow \\Theta$, where $P (D)$ denotes the power-set over datapoints, i.e., the set of possible datasets. The learning algorithm $T$ maps a dataset $D$ to a model $p_{T(D)}$. Maximum likelihood training, which is the norm in ML, is a learning algorithm $T^{ml}$ which fits a model that best compresses the training data:\n$T^{ml} (D) = \\arg \\min_{\\theta'} \u2212 \\sum_{d \\in D} log_2 p_{\\theta'} (d) = \\arg \\min_{\\theta'} K (D|p_{\\theta'})$."}, {"title": "2.2 Prequential coding", "content": "While a learner $T$ that adheres to Occam's razor and solves Equation (3) would improve generalization, it is difficult to design one in practice. Even if $K (p_\\theta)$ could be computed efficiently, there is the further challenge of minimizing it. We will first describe an approach to the problem of estimating $K (p_\\theta)$, and then consider the optimization problem in the next section."}, {"title": "2.3 Minimizing prequential code length through meta-learning", "content": "Consider a parameterized learner $T_\\phi$ that minimizes the prequential code length $L_{preq}(D; T_\\phi)$ of a dataset $D$. This objective tightly upper-bounds the objective that the idealized learner $T^{Oc}$ minimizes, but only when $K(T_\\phi)$ is low. This second criteria is violated if $T_\\phi$ overfits to a single dataset $D$. To forbid $T_\\phi$ from memorizing a single dataset, we consider a meta-dataset $D = \\{D^1, ..., D^M \\}$ coming from $M$ different tasks and meta-learn $T_\\phi$ to minimize prequential code length on average across the meta-dataset $D$. This allows us to write:\n$\\begin{aligned}\nT_{\\phi} &= \\arg \\min_{\\phi'} \\sum_{i=1}^{M} L_{preq} (D^i; T_{\\phi'}) \\approx \\arg \\min_{\\phi'} \\sum_{i=1}^{M} K(D^i, p_{\\theta'} \\vert T_{\\phi'})\n&= \\arg \\min_{\\phi'} \\sum_{i=1}^{M} K(D^i \\vert p_{\\theta'}, T_{\\phi'}) + K(p_{\\theta'} (T_{\\phi'}))\n&= \\arg \\min_{\\phi'} \\sum_{i=1}^{M} K(D^i \\vert p_{\\theta'}) + K(p_{\\theta'} (T_{\\phi'}))\n\\end{aligned}$,\nwhere $p_{\\theta'} = T_{\\phi'} (D^i)$, and the last line is obtained from noticing that all the relevant information about $D^i$ contained in $T_{\\phi'}$ is already encoded in the model $p_{\\theta'} = T_{\\phi'} (D^i)$. The prequential code length of a new dataset of interest $D$ using the meta-learned $T_\\phi$ is then:\n$\\begin{aligned}\nL_{preq}(D;T_{\\phi}) &\\approx K(D,p_{\\theta}\\vert T_{\\phi})\n&= K(D\\vert p_{\\theta},T_{\\phi}) + K(p_{\\theta}\\vert T_{\\phi})\n&= K(D\\vert p_{\\theta}) + K(p_{\\theta}\\vert T_{\\phi}).\n\\end{aligned}$\nNote that the learners $T_\\phi$ and $T^{Oc} (= \\arg \\min_{\\theta'}, [K(D\\vert p_{\\theta'}) + K(p_{\\theta'})])$ are not equivalent: $T^{Oc}$ aims to minimize $K(p_{\\theta'})$ directly whereas $T_\\phi$ fits models that are simple given $T_{\\phi}$ (i.e. low $K(p_{\\theta}\\vert T_{\\phi})$). Despite these differences, the two learners are deeply related. As a result of its meta-objective in Equation (6), the learner $T_\\phi$ attempts to minimize training error across many datasets while fitting compressible models. The learner $T_\\phi$ will succeed in doing this on a novel dataset $D$ when it generalizes to that novel dataset."}, {"title": "2.4 Training for ICL meta-learns a prequential code length minimizer", "content": "In practice, solving the meta-learning problem in Equation (6) involves several constraints:\n1. The performance of $T_\\phi(\\cdot)$ must be evaluated w.r.t. a dataset's prequential code length.\n2. $T(\\cdot)$ must be fast to evaluate because it is iteratively called on multiple datasets.\n3. To meta-optimize $\\phi$, it must be easy to take gradients of $L_{preq}(\\cdot; T_\\phi)$ w.r.t. $\\phi$.\n4. $\\phi$ must parameterize an expressive class of learning algorithms, capable of minimizing prequential code length on a broad distribution of tasks and generalizing to unseen ones.\nWhile this may appear daunting, it turns out that these desiderata are readily addressed by ICL in probabilistic sequence models. Such models are trained to predict the distribution over the next element in a sequence given its past context: $F(d_t|D_{1:t-1})$. Crucially, the sequence model $F$ is both the learner $T_\\phi$ and the inner model $p_\\theta$. Indeed, $\\phi$ corresponds to the parameters of the sequence model $F$ (e.g. weights in a Transfomer), and $\\theta = T_\\phi(D_{1:t\u22121})$ is encoded by the activations of hidden units in the model when presented with the context $D_{1:t\u22121}$. Thus, the predicted distribution over the next token is given by: $F(d_t|D_{1:t-1}) = p_{T_{\\phi}(D_{1:t-1})}(d_t)$.\nThe dual nature of the sequence model as both the learner and the learned model offers a natural solution to the constraints above, enabling fast and differentiable evaluation of $T_\\phi(\\cdot)$ (2 & 3 above) with respect to cumulative next-token prediction loss (1 above). Moreover, modern sequence models can parameterize a rich class of learning algorithms, which is crucial to solving Equation (6) (4 above). Notably, architectures such as Transformers are known to have components which make them especially good meta-learners, such as multi-head attention (Olsson et al., 2022). It is thus no surprise that sequence models are leveraged in settings outside of the language domain (Bauer et al., 2023; Kirsch et al., 2022; Von Oswald et al., 2023a), making them general-purpose meta-learners.\nThis predictive formulation is quite flexible as it can be used to model data which contains sequential correlations, such as language, but can also be used to process any iid dataset. Indeed, consider $D = \\{(x_1, y_1), ..., (x_T, y_T)\\}$ and the supervised task of learning a function $y = f(x)$. In this setting, a data point is given by the pair $d_t = (x_t, y_t)$, and straightforward tokenization schemes can be used to append a novel query $x^\u2217$ to the context $D$ such that the predicted output $\\hat{y}\u2217$ is given by the next token in the sequence. This ICL setup is well-suited for regression-type tasks (see e.g. (see e.g., Von Oswald et al., 2023a,b)) but can be used for most supervised tasks. ICL thus turns the training of a sequence model into a meta-optimization problem over datasets an approach also called memory-based meta-learning (Hochreiter et al., 2001; Ortega et al., 2019; Santoro et al., 2016). It is assumed here that $(x_t, y_t)$ are iid. Although pretrained LLMs that can execute tasks with instructions given via context (or prompt) (Radford et al., 2019) break this iid data assumption, prequential code length is well-defined over arbitrary sequences, and our theory can possibly be adapted to settings with non-stationary data. Further exploration of this topic is left for future work.\nSummary. We showed that sequence models trained on cumulative next-token prediction losses explicitly optimize a meta-learning objective that jointly minimizes training error and model complexity. This provides a normative account of ICL in terms of Occam's razor, and explains recent experimental findings showing that LLMs are good universal compressors (Del\u00e9tang et al., 2023)."}, {"title": "3 Experiments", "content": "Our experiments are designed to illustrate the benefits of ICL in terms of fitting simple models that generalize on iid examples. In Section 3.1, we compare ICL's standard next-token prediction objective to an alternative that minimizes training error alone, rather than prequential code length. Section 3.2 then compares ICL to standard gradient-based learners that minimize training error, such as SGD. In Section 3.3, we explore the impact of learner $T's$ architecture on prequential code length minimization. Section Section 3.4 explores the ability of $T_\\phi$ to generalize to novel tasks. Finally, in Section 3.5 we use insights from our theory to control the data distribution seen by $T_\\phi$ in order to better minimize prequential code length. Experimental details not described in the main paper (e.g., precise architectures, hyperparameters for training, etc.) can be found in Appendix E.\nTasks. In line with similar work studying ICL in a controlled setting (Aky\u00fcrek et al., 2023; Garg et al., 2023; Mahankali et al., 2023), we use synthetically-generated tasks. Each task consists of a supervised learning dataset $D^i = \\{(x_1,y_1), ..., (x_k, Y_k)\\}$, where the labels are a (potentially stochastic) function of the input $y_j = f^i(x_j, \\epsilon_j)$. ICL learners $T_\\phi$ are trained on a meta-dataset $D = \\{D^1, ..., D^N \\}$, where each $D^i$ is associated with a different ground-truth data-generating function $f^i$. We primarily study three meta-datasets: (1) Linear regression problems where $x \\in R^3$ and $y \\in R$. The ground-truth functions $f^i$ are noisy linear mappings $y_j = W^ix_j + b^i + \\epsilon_j$, where each $\\{W^i, b^i\\}$ is sampled from a standard Normal distribution and $\\epsilon_j$ is Gaussian noise with $\\sigma^2 = 0.04$. (2) Sinusoidal regression problems where $x_j \\in R$ and functions $f^i$ are linear combinations $y_j = \\sum_{i=1}^L a_{i,l}^i sin (\\omega_i x_j)$. We use $L = 3$ with frequencies $\\omega^i ~ U(0,5)$ that are shared across tasks, varying only the amplitudes $a_{i,l} ~ N(0, 1)$. (3) Mastermind: a multi-label classification problem inspired by the code-breaking game Mastermind. Each $f^i$ is associated with an underlying discrete code (a fixed-size sequence of digits) that needs to be inferred from random guesses that return partial information. The inputs $x_j$ are random guesses for the code, and $y_j$ is a tuple of two class labels where the first specifies the number of digits in $x_j$ that are correct in terms of both position and value, and the second label specifies the number of digits that are correct in value but not necessarily position. We use randomly sampled codes of length 8 with digits varying from 1..6. Finally, we introduce an additional task based on a Hidden Markov process in Section 3.5 to investigate the generality of our finding on temporally correlated inputs."}, {"title": "3.1 Comparisons to in-context learning with a train-risk objective", "content": "We have argued that standard ICL can be seen as a meta-learning method who's meta-objective is to minimize training error and model complexity through cumulative next-token prediction (prequential code length). However, this is not the only meta-objective that one could design for ICL. In particular, we can design an alternative meta-objective that minimizes only training error simply by training $T_\\phi$ to predict past datapoints in the context rather than future unseen ones. In both cases, the learner $T_\\phi$ is some function that takes a context (i.e., a partial dataset) as input, and outputs a model $p_\\theta$ capable of making predictions for arbitrary datapoints. For supervised learning, this can be represented as $\\hat{y}_q = T_{\\phi}((x, y)_{1:j}, x_q)$ where $(x, y)_{1:j}$ corresponds to an observed context, $x_q$ is the queried input, and the model $p_\\theta$ is implicitly encoded in $T_\\phi$'s weights and latent activations given the context. In standard ICL (which we will refer to as prequential ICL), the query $x_q$ is a novel input that does not appear in the context. In the alternative form of ICL (which we will call train-risk ICL), the query $x_q$ is a randomly-selected input that appeared previously in the context $X_{1:j}$. Note the similarities of train-risk ICL to standard objectives of learners that minimize training error: it processes some fixed-sized training set (here a context) and attempts to minimize the empirical risk on a subset of that very same data (here a single query that appeared in the context). While nobody uses train-risk ICL in practice, it serves as an ideal control to illustrate our theory of ICL and the generalization benefits of minimizing prequential code length as opposed to only training error. One can use an identical architecture for $T_\\phi$ in both cases and train using precisely the same methodology and loss function; the only difference is which query the loss function is evaluated on.\nIn our experiments, we parameterize $T_\\phi$ using a Transformer. For the train-risk case, a standard Transformer could simply attend to the context position that matches $x_q$ and retrieve the corresponding label. To prevent this trivial solution, we instead use a bottlenecked architecture for $T_\\phi$ described in Mittal et al. (2024). In this architecture, a Transformer first summarizes the context into a low-dimensional vector $z =$ Transformer$\\phi((x, y)_{1:j})$, and a separate prediction head-here a multi-layer perceptron (MLP)\u2014subsequently outputs a prediction for the query $\\hat{y}_q = MLP(x_q, z)$. For fair comparison, we use the same bottleneck architecture for train-risk ICL and prequential ICL in all experiments, unless otherwise stated."}, {"title": "3.2 Comparisons to traditional gradient-based learners", "content": "We next consider whether there are empirical advantages of meta-learning a learner $T_\\phi$ to minimize prequential code length through ICL, compared to using standard out-of-the-box learning algorithms. In particular, we know that traditional SGD-based learners can optimize DNN models that generalize well across a wide range of tasks, despite only explicitly minimizing training error. We consider a standard SGD-based learner that fits a randomly-initialized MLP to the training set until validation loss converges. We repeatedly sample a dataset from our meta-dataset, truncate it to a specified number of observed datapoints, apply the SGD-based learner to the truncated dataset, and evaluate the resulting model's generalization error on new datapoints.\nFindings. Figure 2a compares this SGD-based learner to prequential (and train-risk) ICL learners. Across all tasks, the models obtained through ICL generalize better in low-data regimes as a result of directly minimizing model complexity. With enough training data, however, models obtained through the SGD-based learner generalize just as well. In fact, on the Mastermind task, SGD performs better in large-data regimes. This result demonstrates that even though the next-token prediction objective in ICL is well-motivated from a theoretical perspective, the degree to which that objective can successfully be minimized strongly depends on the architecture of $T_\\phi$ and the methods used to train it. For instance, when $T_\\phi$ is a Transformer, the expressivity of the model it implicitly fits to the context scales with the number of activations in the network $(N)$, whereas the expressivity of a DNN trained through SGD scales with the number of weights $(N^2)$. Furthermore, the amount of compute that $T_\\phi$ uses to fit the context amounts to one forward pass of a network, whereas the amount of compute that goes into fitting a dataset using SGD can be arbitrarily large."}, {"title": "3.3 Influence of the in-context learning architecture", "content": "The previous section argued that the structure of $T_\\phi$ can influence its ability to minimize prequential code length. In this section, we further illustrate this point by considering a wider breadth of neural architectures for $T_\\phi$. Since state-space models (SSMs) have recently been shown to exhibit ICL (Lu et al., 2024), we test Mamba 1 (Gu & Dao, 2023) and Mamba 2 (Dao & Gu, 2024). We also test a standard causal decoder Transformer in addition to the bottlenecked Transformer from previous sections. Prequential code length comparisons in Figure 2b show that the architecture for $T_\\phi$ indeed plays a substantial role, with the bottlenecked Transformer and Mamba 2 performing best across our tasks. Analyzing why this is the case is out of scope for this work; we only intend to show that having a next-token prediction objective alone does not guarantee that prequential code length can successfully be minimized in practice through ICL."}, {"title": "3.4 Large pretrained models", "content": "A core element of our theory of ICL is that $T_\\phi$ is trained to minimize average prequential code length on a meta-dataset $D$. There is no guarantee, however, that prequential code length will be small on a novel dataset $D$ that was unseen at training time: this depends on the generalization abilities of the learner $T_\\phi$. In this section, we look at the task-generalization abilities of a large pretrained LLM (GPT-4 Achiam et al., 2023) on the Mastermind task. We do this by prompting the LLM with a description of the task and a number of in-context examples, then obtaining the logits and prediction error for a novel example. In Figure 2a, we find that despite its massive pretraining across a breadth of tasks, the LLM is unable to meaningfully minimize prequential code length on Mastermind. Not only is its prequential code length substantially higher than for a much smaller model trained on a distribution of Mastermind tasks, but it is also higher than for a naive baseline that just predicts the empirical marginal distribution over class labels in the context. These results demonstrate that even when the size of the model and meta-dataset used to train $T_\\phi$ are scaled significantly, current methods for ICL can still struggle to minimize prequential code length on a novel task."}, {"title": "3.5 Improving ICL by controlling the data distribution", "content": "In addition to improving architectures used for $T_\\phi$ or scaling the diversity of tasks on which it is trained, a complementary approach is to manipulate the distribution of data presented in-context at training time. This approach can be especially useful in non-iid settings; for instance Chan et al. (2022) found that in order for ICL to emerge in an image classification setting, the distribution over classes needed to be \u201cbursty\u201d, or Zipfian. In this section, we consider a simple manipulation of the data distribution that is inspired by our theory, with a particular focus on improving ICL in language-like data modalities relevant to LLMs. In prequential coding, model complexity is related to the speed of convergence in generalization error as context length increases. We might therefore be able to further bias ICL towards simple models that generalize better by sampling short contexts, such that downstream prediction errors on longer context lengths (after which the prequential coding curve has already converged) do not disproportionately dominate the loss.\nWe attempt this on synthetically-generated data from Hidden Markov Models (HMMs) that were designed to mimic the statistical properties of natural language in a simplified and controlled setting. Briefly, we generate a family of HMMs parameterized by compositional latent attributes. For example, we use a family of transition matrices, emission matrices, etc. that are randomly sampled to create a new HMM, and train a Transformer to predict the next observation in a sequence. The model is evaluated on unseen HMMs with novel compositions of latents (see Appendix E for details).\nOur results, presented in Figure 3b, show that this data-manipulation strategy based on prequential code length is effective. Generalization error\u2014as measured by the reverse KL-divergence to the oracle predictor given the seen context is lower when preferentially training on short context lengths, with the gap narrowing the more tokens are seen during training as shown in Figure E.2. Both models were trained on the same total number of tokens, with one model using shorter contexts on average than the other. Surprisingly, biasing the data distribution in this way not only decreases generalization error for short context lengths, but also for long ones. In general, these results show how our theory can lead to practical improvements for ICL, where we look at prequential coding curves and compression ability to guide method design."}, {"title": "4 Related work", "content": "Sequence modeling and compression. The idea that probabilistic models can be used to efficiently compress data is a topic widely studied in machine learning across different modalities and settings (Blier & Ollivier, 2018; Del\u00e9tang et al., 2023; Ollivier, 2015; Veness et al., 2014), specifically in sequence modelling (Del\u00e9tang et al., 2023; Goyal et al., 2018; Valmeekam et al., 2023) due to its close similarities to prequential coding (Blier & Ollivier, 2018). In this area, the generic sequence modeling capabilities of certain foundation models are crucial for defining effective \"universal\" compressors. While Goyal et al. (2018) and Valmeekam et al. (2023) claim that learned sequence models can outperform simple compressors like JPEG or gzip, they overlook model complexity in their analysis, adhering strictly to Shannon's notion of compression. In contrast, more recent studies from Del\u00e9tang et al. (2023) and Bornschein"}, {"title": "5 Discussion and Future Work", "content": "In this work, we introduced novel theoretical arguments linking ICL and the next-token prediction objective to Occam's razor. Our theory provides a normative account of the strong generalization abilities of in-context learners at inference time, especially in low-data regimes when compared to traditional optimizers. These theoretical insights were supported by a number of empirical experiments, some of which also identified shortcomings of current methods for ICL that should be addressed in future work.\nOne such shortcoming is that models learned through current ICL methods can underfit data presented in-context, and that this can hamper generalization in large-data regimes on difficult tasks. We also found that the degree of underfitting was highly dependent on the architecture used to parameterize the in-context learner (i.e., the sequence model), and that commonly used architectures such as causal decoder Transformers can often underperform a finding corroborated by Ding et al. (2024). In light of this, we hypothesize that ICL can be improved through the design of novel sequence model architectures that explicitly target prequential code length. For example, current methods learn in-context through a single forward pass of a sequence model with fixed layer depth. In contrast, DNNs can be trained using gradient-based methods until training loss converges, which can take weeks and substantial compute. One improvement to ICL might therefore be to augment current sequence model architectures with \"layers\" that use built-in optimization primitives with variable compute budgets, as was done in Von Oswald et al. (2023b). Another promising approach is to combine ICL and SGD through a \u201cmixture of learners\u201d that reaps their complementary benefits. ICL is sample-efficient and"}, {"title": "Appendix A Background on Kolmogorov complexity", "content": "Kolmogorov complexity was independently developed in the 1960s by Kolmogorov (1965)", "information quantity\\\".\nIntuitively, the Kolmogorov complexity of an object is the length of the shortest program (in some programming language) that outputs that object. Specifically, given some finite string x, $K(x)$ is the length $l(r)$ (in bits) of the shortest binary program $r$ that prints $x$ and halts. Let $U$ be a universal Turing machine that executes these programs. The Kolmogorov complexity of $x$ is then:\n$K(x) = \\min\\{l(r) : U(r) = x, r \\in \\{0,1\\}^\u2217\\}$,\nwhere $\\{0, 1\\}^*$ denotes the space of finite binary strings. A related notion is the conditional Kolmogorov complexity of a string $x$ given another string $y$, which is the length of the shortest program that takes $y$ as input and outputs $x$:\n$K(x|y) = \\min\\{l(r) : U(r(y)) = z,r \\in \\{0,1\\}^*\\}$,\nwhere $r(y)$ denotes a program taking $y$ as input. Finally, we can also define a \u201cjoint": "olmogorov complexity $K(x", "2008)": "n$K(x", "structure": "n object has regularity", "hard-coded\", making its Kolmogorov complexity equal to its original size in bits.\nWhile powerful, Kolmogorov complexity has certain limitations. First and foremost, Kolmogorov is intractable to compute exactly because it requires a brute force search over an exponentially large space of possible programs. It is therefore often of conceptual rather than practical value, although it can nevertheless be upper-bounded using more efficient compression strategies. Second, Kolmogorov complexity depends on the programming language of choice. For instance, if a programming language has a built-in primitive for the object being encoded, Kolmogorov complexity is trivially small. This concern, however, is often overblown": "given any two Turing-complete programming languages, the difference in Kolmogorov complexity that they assign to an object is upper-bounded by a constant that is independent of the object itself, because any Turing-complete programming language can simulate another (Fortnow, 2000; Gr\u00fcnwald & Vit\u00e1nyi, 2003). In practice, we can simply consider \u201creasonable\u201d Turing-complete programming languages that don't contain arbitrary object-specific primitives, in which case this simulation constant will be relatively small and the particular programming language of choice will have little effect. Finally, Kolmogorov complexity is only defined for discrete objects because no terminating program can output a continuous number with infinite precision. This concern is also less consequential in practice, because we can always represent continuous objects using finite (e.g., floating-point"}]}