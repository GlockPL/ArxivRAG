{"title": "EVolutionary Independent DEtermiNistiC Explanation", "authors": ["Vincenzo Dentamaro", "Paolo Giglio", "Donato Impedovo", "Giuseppe Pirlo"], "abstract": "The widespread use of artificial intelligence deep neural networks (DNNs) in fields such as medicine and engineering necessitates understanding their decision-making processes. Current explainability methods often produce inconsistent results and struggle to highlight essential signals influencing model inferences. This paper introduces the Evolutionary Independent Deterministic Explanation (EVIDENCE) theory, a novel approach offering a deterministic, model-independent method for extracting significant signals from black-box models.\nEVIDENCE theory, grounded in robust mathematical formalization, is validated through empirical tests on diverse datasets, including COVID-19 audio diagnostics, Parkinson's disease voice recordings, and the George Tzanetakis music classification dataset (GTZAN). Practical applications of EVIDENCE include improving diagnostic accuracy in healthcare and enhancing audio signal analysis. For instance, in the COVID-19 use case, EVIDENCE-filtered spectrograms fed into a frozen Residual Network with 50 layers (ResNet50) improved precision by 32% for positive cases and increased the Area Under the Curve (AUC) by 16% compared to baseline models. For Parkinson's disease classification, EVIDENCE achieved near-perfect precision and sensitivity, with a macro average F1-Score of 0.997. In the GTZAN, EVIDENCE maintained a high AUC of 0.996, demonstrating its efficacy in filtering relevant features for accurate genre classification.\nEVIDENCE outperformed other Explainable Artificial Intelligence (XAI) methods such as Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class-Activation Mapping (GradCAM) in almost all metrics. These findings indicate that EVIDENCE not only improves classification accuracy but also provides a transparent and reproducible explanation mechanism, crucial for advancing the trustworthiness and applicability of AI systems in real-world settings.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORK", "content": "The need of explainability has risen quickly in the last decades due to the pervasive implementation of AI algorithms to address a variety of practical tasks, [1]. Indeed, the need to explain how an algorithm took a specific decision or what it has found as meaningful in the input, is of paramount importance in fields where the consequences of such a decision is critical, like in the healthcare field, [2]. In these situations, it is a requirement the cooperation between professionals, such as doctors, nurses, scientists etc., and machines. Even if an algorithm seems to be able to perform better than the human capabilities on checking the presence of tiny nonetheless crucial details of information, the issue of generalization of the algorithms applicability on brand new data is nearly an indelible aspect of AI algorithms thus far. Consequently, professionals must constantly check the performances of machines when predicting or classifying information, to avoid inaccurate decisions or, in the worst case, false negative clinical assertions. Nevertheless, their professional judgement shall not be declassified with respect to what algorithms suggest, indeed it should represent the leading line of action supported with additional information. In this regard, AI algorithms' role is that to provide this extra knowledge before taking a crucial decision. To accomplish on these tasks there is a fundamental prerequisite: the true chance of understanding why a certain result was obtained and what information in terms of features and details was responsible for that outcome. This process would allow professionals, like doctors etc., to verify the reliability of the additional information provided by machines in cases where there is or there is not accordance with their own conclusions.\nThis is the role of Explainable Artificial Intelligence (XAI) systems. XAI refers to artificial intelligence (AI) systems whose decision-making processes can be transparently understood by humans, [3]. Trust in such decisions is involved when important consequences are at stake. In this regard, at present day it is important to justify a choice rather than another, for example on medical therapies, apart from the objective accuracy of the model that provided it, [2]. The professionals, such as a physician, need to check the \"why\" in this exchange of information with the machine for what would otherwise become an unbalanced relationship of passive execution for them. This issue is of undelayable attention due to the rapid advancement of AI ability to act on uncountable fields, [4].\nOn top of that, for any application on research advancement where there is not an \u201ca priori knowledge\u201d, researchers doubly suffer ignorance, as they live in a space of no or partial understanding of what they are researching, neither they have an explanation of the AI model outcomes on that topic. For researching fields such as searching for new materials, researchers must test outcomes every time, [5]. Even though AI guesses it right, lack of model decision process' transparency keeps the knowledge advance stuck in blindness of the \"why\".\nApart from the centrality of professionals in taking the final responsibility for any decision, trust on AI models can't be taken for granted at present day. Biases are around the corner and can come up unexpectedly. Humans just can't afford complete trust in AI algorithms. A state-of-art XAI system for image classification might be trained to attend to the edges and textures of an image when recognizing objects, and then use attention weights to identify regions of the input image that mostly contributed to the model decision. This can help to understand if any bias is acting \"behind the scenes\u201d making the model focalize on irrelevant or misleading features, [6]. Potentially, something similar can happen in any field of AI application. Moreover, XAI systems can represent a strong tool to spot precious information from input data that helps to enhance accuracies due to hidden correlations that are not intelligible to a human brain.\nA broad array of explainable artificial intelligence (XAI) algorithms has evolved over recent decades, showcasing varied methodologies, differing accuracies in isolating crucial data, and diverse analytical approaches to assessing the relevance of input features both holistically and at granular levels. These techniques provide insightful analyses but vary significantly in their operational frameworks and dependencies, thus requiring meticulous classification for a thorough understanding of their efficacy and limitations.\nA significant criterion for classifying XAI algorithms relies on the specific Al model architectures. Independent XA\u0399 methodologies, such as SHAP, in theory, operate without dependence on particular model structures, seeking to establish feature-output correlations irrespective of the predictive system's internal mechanisms, even if the practical implementation make distinctions among Tree based models and deep learning models. In contrast, dependent XAI methods, including GradCAM, are inherently linked to specific model designs. They achieve this by exploiting the structural and operational attributes of the prediction models to trace their decision pathways. Although such reliance facilitates targeted insights, it restricts their applicability across different architectures.\nDespite the quantity of research in XAI methodologies in such a limited time-frame, a persistent challenge is the non-determinism. As highlighted in prior research, algorithms like LIME can produce inconsistent outcomes across repeated executions, even when applied to the exactly identical datasets and models. Furthermore, various algorithms often provide conflicting interpretations of the same inputs, especially concerning which patterns or features influenced the model's decision-making and are susceptible of hyperparameters optimization [60].\nExisting methods, while valuable, exhibit distinct trade-offs. Perturbation-based approaches like LIME rely on local approximations, often leading to variations across runs. Gradient-based methods such as GradCAM depend heavily on specific architectures, which limits their generalizability. SHAP, while grounded in robust mathematical principles, can struggle with computational efficiency in high-dimensional scenarios. These limitations highlight the gap that EVIDENCE aims to fill by ensuring deterministic outputs and offering broad applicability across architectures and data types.\nThis inconsistency underscores the need for more reliable and deterministic XAI approaches that meet two essential criteria:\n(1) Model's explanations must be deterministic, ensuring consistent results across runs for a given model and input;\n(2) outputs should include only the critical features necessary for the model's inference.\nIn order to address the previously mentioned challenges, this work presents the Evolutionary Independent Deterministic Explanation model (EVIDENCE) model-independent explainable artificial intelligence theory and algorithm. EVIDENCE strides to provide a robust and mathematically proved theory of convergency to extrapolate all and only the signals that are recognized as important by the model, deterministically. It competes with other mathematically grounded methods such as Shap, which uses a game theoretical approach for deciding the most important features. EVIDENCE is designed to work with time-varying signals (such as audio) but can be extended also to other different type of unstructured signals such as 2D/3D images or videos.\nThe paper is organized as follows: Section II sketches the state of the art. Section III describes the EVIDENCE method and its convergence proof. Section IV sketches experimental results and discussion. Conclusions are presented in Section V."}, {"title": "II. STATE OF THE ART", "content": "This section provides an in-depth review of existing XAI models and algorithms, organized to improve clarity and readability.\nXAI algorithms can be categorized based on their approach, functionality, and dependency on Al models."}, {"title": "\u2022\tShapley Additive Explanations (SHAP)", "content": "SHAP models utilise Shapley values from game theory to attribute the impact of each feature on the final prediction. This method roots itself in strong mathematical foundations, making it particularly robust for tabular data, though certain extensions exist for other data types [7]."}, {"title": "\u2022\tLocal Interpretable Model-Agnostic Explanations (LIME)", "content": "LIME provides interpretability by fitting a simpler, interpretable model to the AI model's predictions within a local domain of the input. This approach is versatile and can be applied to any classification model, making it a widely used tool in XAI [8][9]."}, {"title": "\u2022\tDecision Trees", "content": "Decision tree-based models convert the original input data into smaller subsets through a cascade process, resulting in a tree-like structure. This allows for straightforward retracing of the input data that most significantly influenced the prediction [10]."}, {"title": "\u2022\tBayesian Networks", "content": "Bayesian networks employ probabilistic graphical models and Bayesian inference to generate outcomes. Their simple, transparent architecture enables clear understanding of the correlations between input features and the resulting predictions [11][12]."}, {"title": "\u2022\tCounterfactual Explanations", "content": "Counterfactual explanations provide answers to hypothetical \"what-if\" scenarios, demonstrating how changes in input features would alter the model's behaviour. This approach is particularly useful for understanding the boundaries and conditions under which models operate [13]."}, {"title": "\u2022\tAttention-based Models", "content": "Attention-based models rely on mechanisms that focus on the most significant features for a given outcome. These models are especially effective for image analysis, as they can highlight specific features or regions of an image that contributed to the final decision [14-16]."}, {"title": "\u2022\tGradients and Related Techniques", "content": "Gradient-based methods create class-specific saliency maps, which provide visual gradients of the input image based on the weight that areas had on the specific output of the AI algorithm. Techniques like Integrated Gradients and DeepLIFT extend this approach by relating model outputs to input features and enhancing the accuracy of neuron-level contributions [18-20]."}, {"title": "\u2022\tGuided BackPropagation and Deconvolution", "content": "Guided BackPropagation, also known as guided saliency, is designed to work with convolutional neural networks (CNNs). This method replaces max-pooling layers with convolutional ones to enhance the interpretability of feature activations. Similarly, Deconvolutional Networks (DCNNs) retrieve information on specific CNNs, enabling the understanding of input patterns that caused activations on feature maps [21-24]."}, {"title": "\u2022\tRise and Concept Activation Vectors (Tcav)", "content": "Rise generates saliency maps by randomly masking the input image and measuring the related output multiple times, defining a pixel-related saliency distribution. Tcav links saliency map outputs to higher-level concepts understandable by users, enhancing interpretability [25-26]."}, {"title": "\u2022\tClass Activation Maps (CAMs) and GradCAM", "content": "CAMs are designed for CNNs to relate significant parts of the input image used for classification. GradCAM generalises this approach, providing gradient-based maps of input images that highlight crucial areas, regardless of the specific architecture [27-30]. GradCAM++ extends this further for multi-labelled problems [31-32]."}, {"title": "\u2022\tLayer-wise Relevance Propagation (LRP) and SmoothGrad", "content": "LRP decomposes nonlinear classifiers, especially deep neural networks, using backpropagation to identify meaningful input features. SmoothGrad, often used for denoising, works synergistically with other gradient-based methods to enhance interpretability by removing spurious information [33-36]."}, {"title": "\u2022\tLocal Interpretable Model-Agnostic Explanation (LIME) and Deterministic Lime (Dlime)", "content": "LIME is model-agnostic and uses a perturbation approach to create a new dataset of input variants, explaining the AI model's decisions based on similarities with the original data. Dlime, developed to address LIME's non-deterministic nature, generates input variants deterministically [37-41]."}, {"title": "\u2022\tShapley Values and White Box Models", "content": "SHAP, inspired by game theory, computes weights for features involved in predictions, while White Box models focus on revealing the AI model's decision-making process [42-46]."}, {"title": "\u2022\tEquity Promoters and Sensitivity Analysers", "content": "Equity promoters identify and address biases in AI models to ensure fair predictions. Techniques include fairness constraints, fairness representation learning, and pre- and post-processing methods. Sensitivity analysers evaluate AI model robustness and reliability by examining the impact of input data changes on model performance [47-57]."}, {"title": "\u2022\tAdversarial Attacks", "content": "Adversarial attacks involve making significant changes to input data to test AI models' robustness and identify vulnerabilities. This stress-testing approach helps uncover potential fragilities in AI models' decision-making processes [58-59]."}, {"title": "III. METHODS", "content": "The overall work here proposed was developed into two moments of analysis: one focused on the introduction of a new explainable Al theory, named EVIDENCE, with its mathematical explanation; the other one focused on its application on a real case scenario study, finalized to compare its outcomes with other state-of-art explainable AI algorithms. While the latter analysis gives prompt results of the EVIDENCE application to allow for an easier evaluation of its efficacy, a mathematical description of its underlying logic is mandatory to give a strong foundation to its reliability."}, {"title": "A. EVIDENCE: INTRODUCTION", "content": "Concerning the mathematical introduction of the EVIDENCE algorithm, the basic idea underneath its development is the generation of a population of signals from the input data, i.e., diversified inputs generated from the same original input signal. In this regard, every derived signal is obtained by keeping only a subset of the original information taken from the original input. The missing parts, those portions of information that are not taken from the original input, go to zero.\nConsequently, an AI trained classification algorithm analyzes this diversified population and a score operator, such as the cross-entropy operator, is applied to the AI model classifications. Depending on the score operator outcomes, only a subset of the diversified population survives, which is the one exhibiting the best scores. That is equivalent to selecting only those diversified input data with a significative content of information, so that they allowed the AI classification model to perform consistently and to guess the right output. All this evaluation is possible due to the assumption of the presence of a ground truth for the original inputs. Another important assumption for this explainable algorithm to work is that of convergency of the subset of diversified population inputs to a single content of condensed information, which shall be the only information that resulted effective on allowing the AI algorithm to classify correctly."}, {"title": "B. EVIDENCE: THE ALGORITHM", "content": "Here it is proposed the new EVIDENCE XAI deterministic algorithm. It was developed as a freestanding process designed to be independent from the typology of AI algorithm it should be applied on. EVIDENCE aims to keep the information that the deep neural network algorithm considers relevant for the classification purposes. The output of this algorithm can be considered a filtered version of the input, which in the next real case analysis will consist of a signal where only the most relevant features are still present. It is important to highlight that the algorithm is applied at the end of the learning process of the classification model, and it doesn't affect the training phase of the classification model, thus realizing that \u201cindependency\" from the AI algorithm employed. In other words, EVIDENCE outcomes are independent from the specific AI model architecture, they are only related to the outcome performances of the classification process performed by the AI algorithm.\nWith more detail, the XAI algorithm pipeline starts with the AI model directly applied on a signal to be classified whose ground truth value is already known. For the mathematical description, a 2-dimensional Mel spectrogram of an audio track was considered as the input signal in the case study of this work; therefore, the EVIDENCE algorithm produces a population of different Mel Spectrograms that vary in terms of subcomponents, or chunks, of the original one. The deep learning classification model performs the classification task on these images. Then, the cross-entropy operator is applied between the model output and the ground truth value of the signal class. This operation allows finally to select those elements that present lower values of the cross entropy, i.e., a higher correlation, with respect to the expected classification result. The sub-population of higher correlated signals is then supposed to converge on the specific signal features that mostly contributed to the classification task. This process is equivalent to state that their linear combination is expected to converge to a finite sum, as it will be proven later."}, {"title": "C. EVIDENCE: MATHEMATICAL DESCRIPTION", "content": "It follows the mathematical description of the EVIDENCE algorithm along with a mathematical proof of its convergence on the desired filtered output of the most relevant features for the AI learning process.\nMathematical description of EVIDENCE: Let $M$ be a $l \\times d$ matrix of real numbers and let $H$ be the Cross-Entropy operator, defined as in equation (1):\n$H: R^2 \\rightarrow R, H (p||q) \\rightarrow - \\Sigma_{x \\in X} p(x) \\cdot log q(x)$"}, {"title": null, "content": "Let $\\psi$ be the frozen trained model (in this case a Deep Neural Network) used for classification purposes, whose internal decision rules satisfy the following requirements:\na. Independency from the test input data,\nb. Independency from the cardinality of the input dataset.\n$\\psi$ receives the matrix M as an input and outputs a tuple of scores as in equation (2):\n$\\psi(M) = scores \\leq 1, with \\sum scores = 1$"}, {"title": null, "content": "To create filter masks to be applied on the matrix M, it is necessary to define their dimensionality, i.e., the freedom degrees of their variability, which is important for computational efficiency.\nThus, for a given natural number $m_0$, divisor of $l$, we can consider the generic vector q of dimensionality m defined as the tuple in (3):\n$q \\in D'_{2,m}$ of elements 1,0 \nwhere $m = \\frac{l}{m_0}$ and $D'_{2,m}$ is the set of dispositions with repetition of cardinality $2^m$ of the values 1 and 0. These dispositions represent the variability of the filter masks to be applied on the input matrix \u041c.\nThen, the generic filter vector q must be expanded to match the original row dimensionality of M: we can consider a set K of k functions defined as in equation (4) generating a vector v of length l by taking in input the generic vector q of length m:\n$k \\in K \\Leftrightarrow k: R^m \\rightarrow R^l$,\n$q_z \\rightarrow \\begin{cases}V_i = 1, \\text{if } i \\in [z \\cdot m_0, (z + 1) \\cdot m_0] \\wedge q_z \\neq 0, 1 \\leq z \\leq m \\\\ V_i = 0 \\text{ otherwise} \\end{cases}$\nFinally, the following $l \\times d$ filter matrix F is obtained as the augmentation of the $l \\times 1$ row vector $v= k(q)$:\n$F \\, \\text{dev} \\, u$\nwith $u = [1, ...,1]^T$ a $1 \\times d$ column vector where all the elements are 1."}, {"title": null, "content": "The Hadamard product matrix function can be defined as:\n$Q \\, \\text{def} \\, M\\circ F$\nand it gives a filtered version of the original input matrix M by applying the filter matrix F on it.\nBy applying the $\\psi$ model to the generic filtered input $Q^c$, a tuple of scores is obtained, where every score represents the probability of the filtered input function $Q^c$ to be classified as belonging to a certain class. Indeed, it is possible to apply the entropy operator H to the tuple of scores and the expected value of the outcome E(M) in equation (7):\n$H (\\psi(Q^c ) || E(M)) = h \\in R$\nThesis: considering the subset Q of cardinality n, with $n \\leq 2^m$, of matrix functions defined in (6):\n$Q'_{h_c} \\leq W$, with W$\\in$R defined as an arbitrary threshold value, it exists the function $\\chi$ defined as in equation (7):\n$\\chi = \\frac{1}{n} \\lim_{n \\rightarrow \\infty}  \\sum_{c=1}^n  h_c\\cdot Q^c$\nwith $h'_c = \\frac{1}{h_c+1}$"}, {"title": null, "content": "Proof: Such an assessment is proposed as a consequence of the existence of the limit in (8). In this regard it should be observed that:\na. $\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h_c\\cdot Q^c \\leq \\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h'_c \\cdot \\text{max} (Q^c)$\nand, by construction:\n$\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h'_c \\cdot \\text{max} (Q^c) \\leq \\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h'_c \\cdot \\text{max} (M) = \\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\, n \\cdot h'_c \\cdot \\text{max} (M) = h'_c \\cdot \\text{max} (M)$\nthus:\n$\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h_c\\cdot Q^c \\leq h'_c \\cdot \\text{max} (M)$.\nAdditionally,\n$\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h_c\\cdot Q^c > \\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h'_c \\cdot \\text{min} (Q^c)$\nand, by construction:\n$\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h'_c \\cdot \\text{min} (Q^c) \\geq \\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h'_c \\cdot \\text{min} (M) = \\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\, n \\cdot h'_c \\cdot \\text{min} (M) = h'_c \\cdot \\text{min} (M)$\nthus:\n$\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h_c\\cdot Q^c \\geq h'_c \\cdot \\text{min} (M)$.\nTherefore, the limit in (8) does not diverge:\n$h'_c \\cdot \\text{min}(M) \\leq \\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h'_c \\cdot Q^c \\leq h'_c \\cdot \\text{max}(M)$.\nb. $\\forall i,j: Q^c_{i,j} \\in {M_{ij}, 0}$, by definition, i.e., $Q^c_{i,j}$ can be 1 of those 2 values. Moreover, for an arbitrary cardinality n of the set\n$Q: \\sum_{c=1}^n Q^c_{i,j} = N_n \\, M_{ij} + R_n \\cdot 0$\nwith $N_n, R_n \\in N \\ni N_n + R_n = n$. $N_n, R_n$ represent the number of times $Q^c_{i,j}$ is equal respectively to $M_{ij}$ or 0. It is worth noting the following:\na. By hypothesis, the model $\\psi$ is not dependent on the cardinality of $D'_{2,m}$"}, {"title": null, "content": "b. The model $\\psi$ is not dependent on the cardinality n of the set Q. On the contrary, the set Q is completely determined by $\\psi$ with the condition $h_c \\leq W$ applied on the set $D'_{2,m}$.\nAs the set Qis uniquely determined by $\\psi$, so it is the ratio $A_{ij}= \\frac{N_n}{R_n}$. Therefore, $A_{ij}$is the result of the mere selection of matrix functions operated by $\\psi$. As $\\psi$ is independent from the cardinality n of Q, there is no implicit dependency of $A_{ij}$ from n. Thus, the following stands:\n$\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h_c\\cdot Q^c_{ij} = \\lim_{n \\rightarrow \\infty} \\frac{h'_c}{n} \\cdot (N_n \\cdot M_{ij} + R_n \\cdot 0) =  \\lim_{n \\rightarrow \\infty} \\frac{h'_c}{n} \\cdot N_n \\cdot M_{ij}$\nWith:\n$N_n = n - R_n = \\frac{A_{i,j}}{A_{i,j}+1} \\cdot n = A'_{i,j} \\cdot n$,\nWe finally have $\\forall i \\in [1,l], \\, \\forall j \\in [1, d]$:\n$\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h_c\\cdot Q^c_{ij} = \\lim_{n \\rightarrow \\infty} \\frac{h'_c}{n} \\cdot N_n \\cdot M_{ij} = \\lim_{n \\rightarrow \\infty}  \\frac{1}{n} h'_c \\cdot A'_{i,j} \\cdot n \\cdot M_{ij} = h'_c \\cdot A'_{i,j} \\cdot M_{ij}$\nor, briefly:\n$\\lim_{n \\rightarrow \\infty}  \\frac{1}{n} \\sum_{c=1}^n h_c\\cdot Q^c_{i,j} = h'_c \\cdot A'_{i,j} \\cdot M_{ij}$\nwhere $A'_{i,j}$ represents a fraction of the overall set Q of functions $Q^c$ that satisfy $Q^c_{ij} \\neq 0$ for a given threshold W and a given model $\\psi$, while $M_{ij}$ is the input matrix value at point i,j.\nDue to the theorem of the uniqueness of the limit, the limit in (8) exists and is therefore a unique value $\\forall i \\in [1,l], \\, \\forall j \\in [1, d]$.\nEquation (13) summarizes the overall meaning of EVIDENCE algorithm: the convergency of the set of functions Q, i.e., the diversified inputs generated from the same original input signal M, results in a Hadamard product between the weights matrix A'and the same input M. This product is weighted by the scalar h' which gives more importance to the inputs features resulting in a lower cross entropy. The matrix of likelihood weights A' represents the filtering activity of EVIDENCE, thus fulfilling its explainable task on the input matrix by modulating its values.\nObservations: it is important to underline that the condition of $n \\rightarrow \\infty$ is an ideal case strictly dependent to the cardinality n of the set Q and cardinality 2m of the set $D'_{2,m}$, with $m = \\frac{l}{m_0}$. The feasibility of the previous theoretical assumptions is thus strictly dependent on how much the real case sampling conditions, i.e., the matrix l dimension, match those of the theoretical assumptions. With properly defined threshold W and unitary sampling window $m_0$, the conditions shall be easily matched due to the exponential increase of the $D'_{2,m}$ cardinality with respect to l.\nThe $\\chi$ function is the result of filtering the raw input data in M by extrapolating only the features that mainly contributed to the best score performance of the deep neural network $\\psi$ with respect to a given threshold value W for the Cross Entropy operator H applied on the outcomes. From these assumptions a further corollary can be obtained:\nCorollary: the amplitudes of the function $\\chi$ are a measure of classification importance of the components of the signal in M concerning what the operator $\\psi$ finds relevant during its classification process, all this with respect to the outcome performance measured by applying the Cross Entropy operator H.\nIndeed, by considering (11), the coefficient $A'_{i,j}$ represents the fraction of the overall set of function Q of cardinality n that satisfies $Q_{ij} \\neq 0$. As $n \\rightarrow \\infty$, by the law of large numbers, the coefficients $A'_{i,j}$ tend to represent the frequency distribution of the 1-domain values, i.e., their expected relevance in terms of contribution to define the $\\chi$ function output, with $\\chi$ function representing the most indicative features assessed by $\\psi$ from M for its classification task.\nWhat makes EVIDENCE an interesting alternative with respect to the state-of-art AI explainable algorithms is a reproducibility of the outcomes: the results are completely determined by the input data, the chosen model, the classification task to be performed."}, {"title": "D. CASE STUDIES", "content": "EVIDENCE was tested on the following problems: (1) classification of Covid-19 PCR-Test positive users from healthy control ones without symptoms and with a clean medical history; (2) audio classification of people having Parkinson and Healthy Control subjects and (3) and the GTZAN 10 classes dataset.\nFor each problem, after the model was trained and tested, EVIDENCE was used on the frozen model, with the aim to filter the sound frequencies in the Mel Spectrograms that mostly contributed to a correct classification performance.\nA similar process was performed by involving other two state-of-art explainable AI algorithms, specifically LIME, SHAP Deep Explainer (as suggested by the authors in [7] since the model is a CNN) and GRADCAM.\nFor each one of the explainability algorithms, the least relevant spectrograms features were removed based on their assessments, so that only the most significant ones were preserved. This was done for a reason: to let the already trained ResNet50 classify the input data once cleared from irrelevant information. This analysis shall estimate the explainability algorithms efficacy to select only valuable signal features for the classification task of the model, which is indirectly an explanation on what the trained ResNet50 considered important in the audio records with respect to their outcomes. The higher the accuracies of ResNet50, the higher the explainable algorithms' ability to filter only valuable information and explain with it what ResNet50 has learned for the task.\nIn general, regarding the following case studies, audio preprocessing involved resampling all recordings to 22050 Hz and normalizing amplitude levels. Mel spectrograms were generated using a window length of 2048 samples, a hop size of 344 samples, and 150 Mel filter banks. Each spectrogram was scaled to the decibel (dB) scale to enhance perceptual features. Silence trimming and zero-padding were applied where necessary. The process was implemented using the librosa Python library, ensuring consistent and reproducible results across datasets.\nCovid-19 Dataset\nThe classification was performed by analyzing records of users' breaths and coughs, as it was demanded by a specific recording protocol. Indeed, the classification corresponds to the Task 1 of the Cambridge dataset [61,62].\nThe total cohort of the study is composed by 307 persons, 62 of them coming from positive Covid-19 PCR-Test and 245 for non-positive users. There were different audio tracks of breath and cough sounds for the same persons, thus in total there are 926 audio tracks: 282 audio tracks coming from Covid-19 Positive and 644 from healthy control subjects.\nThe algorithm employed for this task is a Deep Convolutional Neural Network model with a ResNet50 architecture. This model is trained with an inter-patient separation scheme, i.e. the dataset involved is built to keep the same proportion between positive and non-positive users' audio tracks both in the training and the test sets. This random procedure has been repeated 5 times in a 5-fold cross validation fashion and average results have been reported in Table 1.\nIf longer, audio tracks were cut-off at a time-length of 10 seconds. If shorter, the audio tracks were zero-padded to be 10 seconds long. A sampling rate of 22050 Hz was used. For each audio track, a Mel spectrogram was generated with the following parameters: 2048 bins of the FFT, 150 trainable filters, an overlapping window length of 140 milliseconds and a hop size of 344. The ResNet50 was trained from scratch and end-to-end on the generated spectrograms.\nThe test set consisted of audio tracks coming from 32 randomly chosen users, 16 Covid-19 PCR-Test positive users, representing the 25% of the total Covid-19 positive cohort and 16 healthy control subjects. Approximately 62 tracks from Covid-19 positive users and 40 tracks from healthy control subjects."}, {"title": "Parkinson Dataset", "content": "Voice recordings of the vowels /a/ and /i/ utilized in the study were collected as part of the research conducted by Hlavnicka et al. [63", "comprising": "n22 with Parkinson's disease (PD)\n- 21 with Multiple System Atrophy (MSA)\n- 18 with Progressive Supranuclear Palsy (PSP)\n- 22 without any neurological disorders\nThe Unified Parkinson's Disease Rating Scale (UPDRS) was employed to measure disease severity, with trained neurologists assessing patients' motor skills. Specifically, UPDRS Part III was used to evaluate the severity of motor symptoms in PD patients. This part of the UPDRS ranges from 0 to 108 and assesses motor symptoms such as tremors, rigidity, bradykinesia, and postural stability. Higher scores indicate greater severity of motor symptoms. The average severity score for PD patients was 15.9, with a standard deviation of 7.9 [63"}]}