{"title": "PAD-FT: A Lightweight Defense for Backdoor Attacks via Data Purification and Fine-Tuning", "authors": ["Yukai Xu", "Yujie Gu", "Kouichi Sakurai"], "abstract": "Backdoor attacks pose a significant threat to deep neural networks, particularly as recent advancements have led to increasingly subtle implantation, making the defense more challenging. Existing defense mechanisms typically rely on an additional clean dataset as a standard reference and involve retraining an auxiliary model or fine-tuning the entire victim model. However, these approaches are often computationally expensive and not always feasible in practical applications. In this paper, we propose a novel and lightweight defense mechanism, termed PAD-FT, that does not require an additional clean dataset and fine-tunes only a very small part of the model to disinfect the victim model. To achieve this, our approach first introduces a simple data purification process to identify and select the most-likely clean data from the poisoned training dataset. The self-purified clean dataset is then used for activation clipping and fine-tuning only the last classification layer of the victim model. By integrating data purification, activation clipping, and classifier fine-tuning, our mechanism PAD-FT demonstrates superior effectiveness across multiple backdoor attack methods and datasets, as confirmed through extensive experimental evaluation.", "sections": [{"title": "I. INTRODUCTION", "content": "Backdoor attacks. Deep neural networks (DNNs) have achieved significant success across various domains [1], [2], especially in image recognition due to their high effectiveness [3], [4]. However, training DNNs requires large amounts of labeled training data, often sourced from untrusted third parties, which introduces substantial security risks. Among these risks, backdoor attacks pose a critical threat.\nBadNets [5], the first and most representative backdoor attack, randomly selects a subset of samples from the original benign dataset, embeds a backdoor trigger into these benign images and changes their labels to an attacker-specified target label. These poisoned samples are then mixed with the remaining benign samples to create a poisoned training dataset, which is subsequently used for model training. Blend [6] enhanced backdoor attacks by blending benign images with an entire trigger image, making the attack more potent. More recently, even subtler and more effective attacks have been proposed, such as SIG [7], which uses a backdoor signal as the trigger pattern, and WaNet [8], which employs image warping as the backdoor trigger, rendering these attacks even more inconspicuous. The increasing subtlety of these backdoor attacks significantly raises the difficulty of detection and prevention in practical scenarios.\nBackdoor defense. In the literature, many defense mechanisms have been proposed to defend against or mitigate backdoor attacks, which can be broadly categorized into two types: in-training defense and post-training defense. In-training defense methods are applied during the training process, assuming the defender is aware of the existence of the attack. DBD [9] is a representative in-training defense mechanism that decouples the training process into three stages: self-supervised pre-training of the feature extractor, supervised learning of the classifier, and semi-supervised learning of the entire model. However, this approach involves a complex training process, increasing both time and computational costs.\nPost-training defense methods focus on disinfecting models that have already been compromised by backdoor attacks. For example, [10] utilizes a teacher model pre-trained on a clean dataset to perform knowledge distillation on the victim student model. However, this approach involves an additional model and requires an external clean dataset, which is often impractical in real-world scenarios. [11] proposes a post-training backdoor detection method that leverages the maximum margin of activation values. In this approach, the defender is assumed to have access to a small clean dataset, which is used to detect if the model is poisoned and to optimize an activation clipping upper bound that reduces the activation margin, thereby disinfecting the victim model. However, obtaining a reliable clean dataset is not always feasible in practical applications.\nOur approach and contributions. To address the issues in existing defense methods that require additional clean dataset and incur high computational costs, we propose a lightweight post-training defense method, termed PAD-FT. This method does not require additional clean data and fine-tunes only a small portion of the victim DNN model.\nMore precisely, PAD-FT first leverages a simple data purification process that employs the symmetric cross-entropy (SCE) [12] as a metric to identify and select the most-likely clean data from the poisoned dataset, thereby creating a self-purified clean dataset without introducing external data. Next, PAD-FT applies an activation clipping process using optimized clipping bounds derived from the self-purified clean dataset. Finally, PAD-FT fine-tunes only the classifier with the activation clipping to enhance the robustness of the defense while reducing computational costs. Extensive experiments demonstrate the effectiveness and superiority of the proposed PAD-FT in defending against backdoor attacks.\nTo sum up, our contributions in this paper are as follows.\n\u2022 We propose an easy-to-implement data purification approach to select the most-likely clean data from the poisoned dataset, thereby creating a self-purified clean dataset without introducing external data, making it more practical for real-world applications.\n\u2022 We propose a novel and lightweight backdoor defense mechanism, PAD-FT, by integrating data purification, activation clipping, and classifier fine-tuning, avoiding the use of additional models or data and demonstrating very low computational cost.\n\u2022 We conduct comprehensive experimental evaluations on the proposed mechanism PAD-FT, demonstrating its effectiveness and superiority against a variety of backdoor attack strategies across diverse datasets."}, {"title": "II. PRELIMINARY", "content": "Let $F(\\cdot)$ denote the image classification model, which consists of feature extractor layers $f_l(\\cdot)$, activation layers $a_l(\\cdot)$ and a fully connected classifier $\\phi(\\cdot)$, where $l = 0,..., L$.\nLet $D = \\{(x_i, y_i)\\}_{i=1}^{N}$ represent the original dataset, where $x_i \\in X = [255]^{W\\times H\\times C}$ is an image sample with width W, height H and C channels, and $y_i \\in Y = \\{0,1,..., K\\}$ is the corresponding label and K is the number of classes. The output of the model with respect to an input image x is represented as\n$F(x) = \\phi \\circ a_L \\circ f_L \\circ \\cdots \\circ a_0 \\circ f_0(x)$.\nA typical backdoor attack mechanism embeds a specific pattern p into an original sample x, generating a poisoned dataset $D_p = \\{(x', y')\\}$, where $x' = x + p$ and $y'$ is the target class. The remaining data remains benign and forms the benign subset $D_b$. The final training dataset is then\n$D_t = D_b \\cup D_p$.\nThe poison rate $\\rho$ is calculated as $\\rho = \\frac{N_p}{N}$ where $N_p$ is the number of poisoned samples and N is the total number of samples in the dataset. As the amount of poisoned data increases, the poisoning rate $\\rho$ increases accordingly."}, {"title": "III. METHOD", "content": "Our method PAD-FT consists of three key components: data purification, activation clipping, and classifier fine-tuning, which are described below. The entire framework of PAD-FT is outlined in Algorithm 1.\nA. Data Purification\nTo avoid resorting to external data, we aim to design a simple data purification method to identify and select the most-likely clean data from the poisoned training dataset. To that end, we employ symmetric cross-entropy (SCE) loss [12] as an evaluation metric for data purification, which differs from the model's training loss function. SCE combines the traditional cross-entropy loss with a reverse cross-entropy term, enhancing the model's robustness and helping to filter out the most-likely clean data from noisy data.\nFor each data point $(x_i, y_i) \\in D_t$ in the poisoned training dataset, we calculate the SCE loss $l_{SCE}$ as\n$l_{SCE}(x_i, y_i; F) = \\alpha \\cdot l_{CE}(\\sigma(F(x_i)), y_i) + (1 - \\alpha) \\cdot l_{CE}(y_i, \\sigma(F(x_i)))$  (1)\nwhere $l_{CE}$ is the standard cross-entropy loss, $\\sigma(\\cdot)$ is the softmax function, F is the classification model, and $\\alpha$ is a hyperparameter to balance the two terms.\nAfter calculating $l_{SCE}$ for all training data in the dataset, we select $n_c$ images from each class that have the smallest SCE loss values. These selected images are regarded as pseudo-clean data, as their low loss values indicate that the model is confident in their correct classification, even in the presence of poisoned data. The selected pseudo-clean dataset is denoted as $D_c$, containing $N_c = n_c \\times K$ images. A smaller $N_c$ indicates that the purified dataset $D_c$ contains clean data with greater confidence."}, {"title": "B. Activation Clipping", "content": "As highlighted in [5], backdoor attacks significantly impact activation values. When the victim model is \"activated\" by the trigger pattern, the associated activation nodes produce abnormally high outputs, leading to incorrect classification results. To mitigate this, [11] proposed setting an upper bound on the activation layers using an additional clean dataset. This strategy clips the abnormally high activation values triggered by the pattern to normal levels, using an external clean dataset as a reference standard.\nConsidering that an external clean dataset is usually not feasible in real-world applications, our method PAD-FT integrates a self-purified data subset obtained in Section III-A into the activation clipping strategy. Its effectiveness in defending the victim model is demonstrated in Section IV.\nSpecifically, for the l-th activation layer $a_l(\\cdot)$, an upper bound $z_l$ is introduced to clip the activation output, where the clipped activation is represented by $\\bar{a}_l(\\cdot) = min(a_l(\\cdot), z_l)$ and the corresponding bounded logits of the model are denoted as $\\bar{F}(\\cdot)$. Let $Z = \\{Z_0,...,Z_L\\}$ represent the set of clipping bounds for each activation layer. As in [11], the upper bounds are optimized using the following loss function\n$\\mathcal{L}_{AC} = \\frac{1}{N_c} \\sum_{x \\in D_c} l_{MSE}(\\bar{F}(x; Z), F(x)) + \\lambda \\sum_l ||z_l||_2$ (2)\nwhere $l_{MSE}$ is the mean squared error loss and $\\lambda$ is dynamically adjusted as in [11]. By minimizing this loss function on the selected purified clean dataset $D_c$, the clipping bounds for activation values can be established. Accordingly, the victim model can be disinfected by using these bounds to clip abnormally large activation values to normal levels."}, {"title": "C. Classifier Fine-tuning", "content": "After activation clipping, we employ fine-tuning to enhance model performance. In existing methods, the fine-tuning process for backdoor defenses typically requires a clean dataset and involves updating the entire model, e.g. [13], which is computationally expensive, especially for large models.\nIn contrast, our method PAD-FT employs a self-purified clean dataset and fine-tunes only the classifier, thereby significantly reducing computational cost. Experimental results indicate the effectiveness of PAD-FT, as shown in Section IV.\nFirst, inspired by semi-supervised learning, we introduce consistency regularization [14] to enhance the robustness of our backdoor defense. The consistency regularization loss is\n$\\mathcal{L}_{CR} = \\frac{1}{N_c} \\sum_{x \\in D_c} l_{CE}(\\sigma(\\mathcal{F}(\\gamma(x))), \\sigma(\\mathcal{F}(x)))$ (3)\nwhere $\\gamma(x)$ represents an augmented version of the image x via techniques such as flipping, brightness adjustments, and contrast modification, and $\\mathcal{F}(\\cdot)$ is the clipped model. Consistency regularization encourages the classifier to make consistent predictions on both the original and augmented images, thereby enhancing robustness against backdoor attacks.\nThen the loss function used for classifier fine-tuning is\n$\\mathcal{L}_{FT} = \\beta \\cdot \\frac{1}{N_c} \\sum_{x \\in D_c} l_{SCE}(x, y; \\mathcal{F}) + (1 - \\beta) \\cdot \\mathcal{L}_{CR}$ (4)\nwhere $\\beta$ is a hyperparameter that balances the contributions of the SCE loss and the consistency regularization loss.\nThe overall process of the proposed backdoor defense mechanism PAD-FT is illustrated in Algorithm 1."}, {"title": "IV. EXPERIMENTS", "content": "We conduct experimental evaluations of the backdoor defense mechanisms with the following settings.\nDataset and model. We utilize two standard datasets: CIFAR-10 and CIFAR-100 [15]. CIFAR-10 consists of 50,000 RGB training images and 10,000 testing images across 10 classes, while CIFAR-100 across 100 classes. We adopt the pre-act ResNet-18 [16] as the model architecture in all experiments.\nBackdoor attack. To evaluate the performance of the proposed backdoor defense mechanism, we implement three different backdoor attack strategies: BadNets [5], Blended [6] and WaNet [8]. BadNets uses the most conspicuous backdoor pattern while Blended introduces a more subtle approach, and WaNet represents the most inconspicuous attack strategy. The poison rate p is set as 5% and 10% to show the defense performance under different amount of poisoned data.\nMetrics. We evaluate the performance of the defense mechanisms using two representative metrics: classification accuracy (ACC) on a clean test dataset, and attack success rate (ASR) on a poisoned test dataset where all samples contain the implanted trigger pattern. The adversary's objective is to achieve both high ACC and high ASR, whereas the goal of the defense mechanism is to maintain high ACC while minimizing ASR as much as possible.\nImplementation. For a fair comparison, we follow the default training and defense settings as in Backdoor-Bench [17], including trigger patterns, learning rates, weight decay, and other relevant hyperparameters. We compare the proposed defense mechanism PAD-FT, as well as its preliminary stage before fine-tuning (denoted as PAD), with state-of-the-art defense mechanism baselines: DBD [9], MM-BD [11].\nFor the proposed PAD-FT method, we adopt $\\alpha$ = 0.5 as in [12] and $\\beta$ = 0.5 to achieve a balance between the SCE loss and the consistency regularization loss in (4). We adopt $N_c$ = 2500 on both datasets during the experiments.\nB. Results\nTable I and Table II present the empirical results of our PAD-FT, PAD and other baselines across various datasets and backdoor attack scenarios. The results demonstrate that both PAD-FT and PAD offer significant advantages over other defense mechanisms.\nNotably, PAD and PAD-FT maintain a strong balance between ACC and ASR. For instance, in CIFAR-10 (BadNets at p = 5%), PAD-FT achieves an ACC of 82.46% and an ASR of 8.36%, while DBD sacrifices accuracy, yielding only 68.87% ACC for a similar ASR. Additionally, PAD-FT's fine-tuning mechanism significantly improves ASR, as seen in CIFAR-10, where it reduces the average ASR from PAD's 18.65% to 8.07%."}, {"title": "C. Ablation Study", "content": "An ablation study is conducted to evaluate how Nc, the size of the self-purified dataset De influences the effectiveness of the proposed PAD-FT mechanism.\nTable III demonstrates that ACC initially increases as Nc grows, benefiting from more purified data used for fine-tuning. However, as Ne continues to increase, ACC starts to decline, likely because more poisoned data is being selected into De. Similarly, ASR decreases at first but rises again as more poisoned data are included. This illustrates a clear tradeoff between increasing the size of the purified data and the model performance in terms of ACC and ASR."}, {"title": "V. CONCLUSION", "content": "This paper proposed a novel lightweight post-training backdoor defense mechanism PAD-FT. By introducing a new data purification method, PAD-FT effectively disinfects poisoned models without requiring additional clean data. The classifier-only fine-tuning in PAD-FT highlights its lightweight nature, making it easy to implement. Extensive experiments demonstrate the effectiveness and superiority of PAD-FT across a variety of datasets and backdoor attack scenarios. Therefore, our PAD-FT mechanism offers a practical and efficient solution to the challenge of backdoor attacks."}]}