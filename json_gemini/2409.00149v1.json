{"title": "From Semantics to Hierarchy: A Hybrid Euclidean-Tangent-Hyperbolic Space Model for Temporal Knowledge Graph Reasoning", "authors": ["Siling Feng", "Zhisheng Qi", "Cong Lin"], "abstract": "Temporal knowledge graphs (TKGs) have gained significant attention for their ability to extend traditional knowledge graphs with a temporal dimension, enabling dynamic representation of events over time. TKG reasoning involves extrapolation to predict future events based on historical graphs, which is challenging due to the complex semantic and hierarchical information embedded within such structured data. Existing Euclidean models capture semantic information effectively but struggle with hierarchical features. Conversely, hyperbolic models manage hierarchical features well but fail to represent complex semantics due to limitations in shallow models' parameters and the absence of proper normalization in deep models relying on the L2 norm. Current solutions, such as curvature transformations, are insufficient to address these issues. In this work, a novel hybrid geometric space approach that leverages the strengths of both Euclidean and hyperbolic models is proposed. Our approach transitions from single-space to multi-space parameter modeling, effectively capturing both semantic and hierarchical information. Initially, complex semantics are captured through a fact co-occurrence and autoregressive method with normalizations in Euclidean space. The embeddings are then transformed into Tangent space using a scaling mechanism, preserving semantic information while relearning hierarchical structures through a query-candidate separated modeling approach, which are subsequently transformed into Hyperbolic space. Finally, a hybrid inductive bias for hierarchical and semantic learning is achieved by combining hyperbolic and Euclidean scoring functions through a learnable query-specific mixing coefficient, utilizing embeddings from hyperbolic and Euclidean spaces. Experimental results on four TKG benchmarks demonstrate that our method reduces error relatively by up to 15.0% in mean reciprocal rank (MRR) on YAGO compared to previous single-space models. Additionally, enriched visualization analysis validates the effectiveness of our approach, showing adaptive capabilities for datasets with varying levels of semantic and hierarchical complexity.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) are crucial in data-driven applications (Zou 2020) such as recommendation systems (Guo et al. 2020), medical information retrieval (Yang 2020), and commonsense question-answering platforms (Edge et al. 2024), due to their structured representation of entities and relationships (Fensel et al. 2020). However, KGs often suffer from data incompleteness, driving research in KG completion (Bordes et al. 2013; Trouillon et al. 2016). These approaches typically aim to represent knowledge in low-dimensional vector spaces to infer missing data. Nevertheless, the static nature of these embeddings limits their ability in modeling temporal dynamics (Chang et al. 2017).\nTemporal knowledge graphs (TKGs) extend KGs by integrating a temporal dimension, transforming traditional triplets into quadruplets: (subject, relation, object, timestamp). This allows for the dynamic representation of events over time. TKG reasoning tasks generally fall into two categories: interpolation, predicting missing facts within a given time interval (Dasgupta, Ray, and Talukdar 2018; Garc\u00eda-Dur\u00e1n, Duman\u010di\u0107, and Niepert 2018; Leblay and Chekol 2018), and extrapolation, forecasting future events based solely on historical data (Jin et al. 2019; Trivedi et al. 2017, 2019; Li et al. 2021). The latter presents a significant challenge due to the absence of full context.\nA deep understanding of historical data is crucial for effective extrapolation in TKGs. TKGs encapsulate both semantic and hierarchical information inherently. Semantics arise from graph structures and temporal dynamics, containing the intricate relationships and meanings through fact co-occurrence and sequential event information. Hierarchy emerges from the exponential growth of nodes, reflecting varying levels of abstraction among entities. \nRecent approaches (Jin et al. 2019; Li et al. 2021) integrating graph and recurrent neural networks in Euclidean space have shown promise in modeling semantic data but fall short in capturing hierarchical information. In contrast, hyperbolic geometry learning excels at representing hierarchical structures due to its natural ability to embed tree-like data. However, applying hyperbolic methods to TKG extrapolation presents several challenges.\nDependence on Modulus for Hierarchical Structuring: Hyperbolic models often rely on modulus-based hierarchy, necessitating the omission of normalization techniques during training to preserve modulus information. This results in slow convergence and suboptimal performance in deeper networks, leading to shallow inductive models. Efforts to mitigate this, such as spatial curvature transformations between hyperbolic layers have been insufficient.\nInstability in Tangent Space Transformations: Many hyperbolic methods involve transforming parameters from hyperbolic manifolds to tangent space, a specialized Euclidean space distinct from the traditional one used in semantic modeling. Without normalization, deep networks in tangent space can produce unstable embedding norms, leading to numerical instability when mapping back to the manifolds due to limited computational precision, ultimately degrading model performance.\nTo address the challenges, we propose ETH, a novel hybrid Euclidean-Tangent-Hyperbolic space model that leverages the strengths of both Euclidean and hyperbolic modeling, resolving limitations of single-space models. Our approach begins by capturing complex semantic information in Euclidean space through a fact co-occurrence and autoregressive method, incorporating normalization for stability. We then transform embeddings into Tangent space with a new scaling mechanism, preserving semantic richness while enabling hierarchical learning through a query-candidate separated modeling approach. These embeddings are subsequently mapped into Hyperbolic space, where hierarchical features are naturally represented. Finally, a hybrid inductive bias for hierarchical and semantic learning is achieved by combining hyperbolic and Euclidean scoring functions, accomplished by a learnable query-specific mixing coefficient that adapts to different query characteristics.\nWe validate ETH on four TKG benchmark datasets, showing up to a 15.0% relative error reduction in mean reciprocal rank (MRR) on YAGO compared to existing single-space models. Visualization analysis further confirms the model's adaptability to datasets with varying semantic and hierarchical complexity.\nOur contributions can be summarized as follows:\n\u2022 Proposing a multi-space hybrid architecture that integrates hierarchical learning in hyperbolic space with semantic learning in Euclidean space, bridged by tangent space.\n\u2022 Introducing a novel tangent space transformation technique that preserves semantic information while facilitating hierarchical learning.\n\u2022 Developing a hybrid scoring function with a query-specific mixing coefficient, optimizing performance across diverse query types."}, {"title": "2 Related Work", "content": "2.1 Static KG Reasoning Models\nStatic KG reasoning models embed entities and relations into low-dimensional vector spaces to infer missing facts. TransE, a foundational model, represents relationships as translations between entity embeddings, inspiring various extensions to capture complex relational patterns. Graph Convolutional Networks (GCNs) have furthered this field, with Relational GCN (RGCN) incorporating relation-specific filters, and Weighted GCN (WGCN) introducing learnable relation-specific weights. CompGCN enhances link prediction by integrating nodes and relations, while Variational RGCN (VRGCN) introduces probabilistic embeddings. Despite their success in static KGs, these models struggle with temporal dynamics and future event prediction.\n2.2 Temporal KG Reasoning Models\nTKG reasoning models extend static approaches by incorporating temporal dynamics to predict future facts. These models operate in two key settings: interpolation and extrapolation. Interpolation infers missing facts at historical timestamps. Early models like TA-DistMult, TA-TransE, and TTransE embed temporal information directly into relation embeddings, while HyTE uses a hyperplane for each timestamp. However, they struggle with predicting future events. Extrapolation predicts future events based on historical data. Know-Evolve uses temporal point processes, DyREP models relationship evolution, and RE-NET"}, {"title": "3 Problem Formulation and Background", "content": "3.1 Problem Definition\nIn this paper, a TKG is defined as G(V,E,T,F), where V, E, and T represent the sets of entities, relations, and timestamps, respectively, and F \u2286 V\u00d7E\u00d7V\u00d7T is the set of all quadruples (s,r,o,t). The TKG can be viewed as a sequence of KG snapshots, denoted by G = {G0, G1,\u2026, Gt,\u2026\u2026 }, where each snapshot Gt = {(s,r,o) | (s,r,o,t) \u2208 F} corresponds to a specific timestamp. The TKG extrapolation task aims to predict the set of queries Qt+1 = {(q,r) | (q,r,o) \u2208 Gt+1}, given the most recent m snapshots Gt-m+1:t G. Candidates for Qt+1 are drawn from V, denoted by a \u2208 V. The objective is to score each quadruple (q, r, a, t + 1) using a scoring function fs: V\u00d7E\u00d7V\u00d7T \u2192 R, where a higher score indicates a greater likelihood that the a is the correct entity. To enhance structural connectivity of the TKG, inverse quadruples (o, r-1, s, t) are also incorporated.\n3.2 Hyperbolic Geometry\nHyperbolic geometry differs from Euclidean geometry in its parallel postulate, where through any point not on a line, infinitely many lines can be draw parallel to the given line.\nThis leads to exponential growth in the area and perimeter, reflecting the constant negative curvature of hyperbolic space, making it well-suited for modeling hierarchical structures.\nThe Poincar\u00e9 ball model is a common representation of hyperbolic space, defined as a d-dimential ball B = {x \u2208 Rd | ||x||\u00b2 < 1/c}, where c is the negative curvature (-c < 0) and || . || is the Euclidean L2 norm. Each point xe Bd is associated with a tangent space TBd, a d-dimentional vector space that containing all possible velocity vectors at x on the manifold.\nTo transition between the tangent space and the hyperbolic ball, the exponential map exp: TBd \u2192 Bd, and the logarithmic map loga: Bd TBd are used. Specifically, at the origin 0 \u2208 Bd, these maps are defined as:\nexpo(v) = tanh(\u221ac||v||) (\\frac{v}{||v||}), (1)\nlogo (u) = arctanh(\u221ac||u||) (\\frac{u}{||u||}),\nwhere v \u2208 ToBd and u \u2208 Bd.\nThe Poincar\u00e9 geodesic distance between any two points x and y \u2208 Bd is:\nd(x, y) = \\frac{2}{\u221ac} arctanh(\u221ac|| x \\ominus y||), (2)\nwhere represents M\u00f6bius addition (Ganea, B\u00e9cigneul, and Hofmann 2018), defined as:\nx+y = \\frac{(1 + 2c<x, y> + c||y||\u00b2)x + (1 \u2212 c||x||\u00b2)y}{1+2c<x, y> + c\u00b2 ||x||\u00b2||y||\u00b2}, (3)\nwhere <\u00b7> is Euclidean dot product."}, {"title": "4 Methodology", "content": "This section details ETH, as illustrated in Figure 2. The model first captures complex semantics of multi-relational graphs and dynamic temporal information through a fact co-occurrence and autoregressive method, incorporating normalization throughout (Section 4.1). Subsequently, it transforms Euclidean semantic embeddings into tangent space with a new scaling mechanism, preserving semantic information while enabling hierarchical learning. Query and candidate entities are modeled separately to enhance the capture of both semantic and hierarchical information (Section 4.2). Finally, the embeddings transition from tangent to hyperbolic space, where a hyperbolic scoring function evaluate quadruples alongside Euclidean scoring on the previously processed vectors. A learnable query-specific scoring coefficient balances semantic and hierarchical modeling for each query (Section 4.3). Optimization strategies are discussed in Section 4.4.\n4.1 Euclidean Modeling\nSemantic information in TKGs arises from graph structures and temporal dynamics. To effectively capture this complexity, entity embeddings are initially encoded in Euclidean space using a Relation-aware Graph Convolutional Network (RGCN) and a Gated Recurrent Unit (GRU). The RGCN captures intra-snapshot graph semantics, while the GRU models temporal dynamics in an autoregressive manner. The input is a sequence of the last m snapshots Gt-m+1:t, used as historical context for predicting queries Qt+1. Entity embeddings h and relation embeddings ve \u2208 Rd are initialized randomly, with explicit encoding applied only to entities due to their greater number relative to relations.\nMulti-Relational Graph Semantic Modeling. Each snapshot is treated as a multi-relational graph. Entities are encoded based on their connections via a relation-aware GCN, capturing the co-occurrence patterns. For entity o at timestamp k with neighbors (s,r) \u2208 N, the graph semantic encoding from layer i to i + 1 in the RGCN with total l layers is given by:\nh^{i+1}_{k,0} = f (\\frac{1}{|N_k|} \\sum_{(s,r)\u2208Nk}(W^i_r h^i_{k,s}) + W^i h^i_{k,0}), (4)\nwhere hi\u2208 Rd is embedding of entity o at layer (i + 1), Wi, W\u2208 Rdxd are learnable weights at layer i, and f (\u00b7) is the RReLU activation function. Self-loop edges are added for all entities.\nAutoregressive Temporal Semantic Modeling. Temporal dynamics are captured using a GRU, which updates semantic embeddings over time:\nh^{t}_{k} = GRU(h^{t-1}_{k}, h_{k}), (5)\nwhere h\u2208 Rd is the RGCN output at timestamp k. Layer normalization and a scaling factor \u221ad are applied to hk, hk, and v to constrain their L2 norms around 1. This Euclidean space encoding allows the model to effectively capture complex semantic features early in the processing.\n4.2 Tangent Space Transformation\nIn Euclidean space modeling, normalization erases hierarchical information. To restore this and prepare for the transition to hyperbolic space, we transform entity embeddings from Euclidean Rd to tangent space Tod. This transformation allows for hierarchical relearning, capturing both semantic and hierarchical structures while preventing numerical issues in the Poincar\u00e9 ball. We employ a dual-mode approach to model distinct behaviors for query entities q and candidate entities a, enhancing the model's performance in extrapolation tasks.\nTransformation for Candidate Entity. To capture candidates' semantic and hierarchical features, we first apply a linear transformation to the entity embedding ht:\nh^g_{a} = W_1 h_t + b_1, (6)\nwhere W\u2208 Rd\u00d7d and b\u2081 \u2208 Rd are learnable parameters. This transformation captures the necessary features before transitioning to tangent space, where the core transformation is:\nh^{g'}_{a} = W_2^g \\gamma (W^g (tanh(h^{g}_{a}) \u2297 h_{t,a})), (7)\nwhere W\u00ba, W\u2208 Tdxd are weight matrices, \u2297 denotes Hadamard product, and y is an optional activation function. The superscript g indicates parameters in tangent space. The tanh function maps h elements to [-1, 1], facilitating stable hierarchical information capture.\nTransformation for Query Entity. Query entity embeddings undergo a similar linear transformation, with a slight variation:\nh^{g}_{q} = W_1^g cat([h_{t,q}; v] | (q,r) \u2208 Q_{t+1}) + b_2, (8)\nwhere cat() concatenates the query entity embedding ht,q and relation embedding v. Here, W\u00b0 \u2208 R2d\u00d7d and b \u2208 Rd are the parameters used to model these concatenated embeddings. The subsequent tangent space transformation is defined as:\nh^{g'}_{q} = W_2^g \\gamma (W^g (tanh(h^{g}_{q}) \u25b7 h_{t,q})), (9)\nwhere We Td\u00d7d. The shared weight matrix W\u00ba maintains consistency between candidate and query entity embeddings. After these transformations, ha and ho are enriched with hierarchical information and are numerically stable, ready for hyperbolic space modeling.\n4.3 Hyperbolic-Euclidean Hybrid Scoring Function\nIn the final stage, we integrate semantic and hierarchical modeling through a hybrid scoring function, balancing each query's need for these aspects via a query-specific score mixing approach.\nEuclidean Dot Product Scoring Function. The Euclidean dot product scoring function measures semantic similarity between query and candidate embeddings:\nS\u00ae(q,r,a,t + 1) = <h^{g'}_{a}, h^{g'}_{q}>, (10)\nwhere Se represents the Euclidean dot product score.\nHyperbolic Distance Scoring Function. To capture hierarchical structures, we apply a relation-specific curvature Cr to map embeddings from tangent space to hyperbolic space via exponential transformation:\nh^{c_r}_{a} = expo(h^{g'}_{a}), (11)\nh^{c_r}_{q} = expo(h^{g'}_{q}),\nwhere he and ho\u2208 Bd are embeddings of candidate a and query q in the Poincar\u00e9 ball. The hyperbolic distance scoring function is then used:\nSb(q,r,a, t + 1) = -d_{c_r}(h^{c_r}_{q}, v) + b_q + b_a, (12)\nwhere Sb represents the hyperbolic distance score, v \u2208 Ba is the learnable relation embedding in hyperbolic space, and bq, ba \u2208 R are entity-specific biases. Unlike entity embeddings, v and v are learned directly in their respective spaces without explicit transformation. This approach ensures that he is properly adjusted to capture its distinct interaction with relation r compared to its Euclidean counterpart ha.\nHybrid Space Scoring Function. We combine the Euclidean and hyperbolic scores using a query-specific mixing coefficient:\nS(q,r,a,t + 1) = \u03c3(\u03b2_{q,r}S_b + (1 \u2212 \u03b2_{q,r})S_e), (13)\nwhere \u03c3(\u00b7) is the sigmoid function. The coefficient Bq,r is defined as:\nB_{q,r} = \u03c3 (\\frac{<s_q, s_r>}{\\sqrt{w}}), (14)\nwhere sq, Sr \u2208 R\u2122 are query entity and relation vectors, respectively. The dot product of these vectors, processed through the sigmoid function, ensures that \u1e9eq,r ranges between 0 and 1. This approach enables information sharing among queries with common entities or relations.\n4.4 Optimization\nWe optimize the model by minimizing the cross-entropy loss function:\nL = -\\sum_{t=0}^{T-1} \\sum_{(q,r)\u2208Q_{t+1}} \\sum_{a\u2208V} Y^{t+1}_{q,r,a} log S(q, r, a, t + 1), (15)\nwhere y\u2208 R represents the label for candidate a in query (q,r) at timestamp t + 1. Most parameters in our model are either in Euclidean space or tangent space, avoiding the complexities of Riemann optimization, thereby enhancing stability and performance."}, {"title": "5 Experiments", "content": "5.1 Experiments Setup\nDatasets. ETH is evaluated on four widely adopted TKG datasets: ICEWS14, ICEWS05-15 , WIKI, and YAGO. For ICEWS14 and ICEWS05-15, we follow standard practice by splitting the datasets into 80% training, 10% validation, and 10% test sets, ensuring chronological order (ttrain tvalid < ttest). Dataset details are summarized in Table 1.\nEvaluation Metrics. Mean Reciprocal Rank (MRR) and Hits @ 1/3/10 are used as evaluation metrics. Among the various metric settings: raw , static filter , and time filter . Time filter is preferred according to for extrapolation tasks, which excludes other correct answers from the ranking process when a query (q,r,?, t) has multiple correct answers at the same timestamp. This approach is justified as the other answers are equally valid. Hence, we report results exclusively under the time filter setting.\nImplementation Details. Embedding dimensions d and w are set to 200, with the RGCN layer count 1 at 2 for ICEWS14 and ICEWS05-15, and 1 for WIKI and YAGO. A grid search within the range [1, 30] determined optimal history lengths m as 10, 24, 2, and 2 for ICEWS14, ICEWS05-15, YAGO, and WIKI, respectively. The activation function y is set to ReLU for ICEWS14, YAGO, and WIKI, and None for ICEWS05-15. Adam optimizer is used with a 0.001 learning rate. Training was conducted on a GeForce RTX 4060 TI GPU. For comparisons with static methods, timestamps were excluded during training and testing.\nCompared Mothods. ETH is compared against baseline hyperbolic models AttH and HERCULES, as well as Euclidean models RGCRN, RE-NET , CyGNet, XERTE, TLogic, and EvoKG for TKG extrapolation tasks. Hyperbolic baseline results are from , and Euclidean baseline results from ."}, {"title": "5.2 Performance Comparison", "content": "Table 2 presents the extrapolation task results, showcasing ETH's effectiveness across four datasets. ETH consistently outperforms baseline models, demonstrating superior ability to capture both semantic and hierarchical information. Notably, ETH surpasses hyperbolic models such as AttH and HERCULES by effectively capturing semantic nuances in Euclidean space and outperforms Euclidean models. We calculate Krackhardt hierarchy scores (Khs)(Krackhardt 2014) for every snapshot in each dataset, with statistics shown in Figure 3. Higher Khs indicate a more hierarchical, tree-like structure, where hyperbolic embeddings perform particularly well, as seen in datasets like YAGO and WIKI. Specifically, ETH achieves relative error reductions on the YAGO, with 15.00% in MRR, 16.27% in Hits@1, 8.18% in Hits@3, and 18.49% in Hits@10, compared to the second-best model. On the WIKI dataset, ETH records relative error reductions of 8.43% in MRR, 8.31% in Hits @ 1, and 10.64% in Hits@3.\nETH's strong performance on YAGO and WIKI, both characterized by significant time intervals and pronounced hierarchy, illustrates its effective use of hierarchical information via tangent space transformation. While ETH trails XERTE in Hits@1 on the ICEWS14 and ICEWS05-15 datasets, it still leads in MRR and Hits@3/10, indicating that the hybrid scoring mechanism captures a more comprehensive range of semantic and hierarchical information, which leads to robust predictions. Despite RE-GCN being a strong Euclidean competitor, ETH consistently outperforms it, particularly on YAGO and WIKI, underscoring the importance of hierarchical information in temporal knowledge graph reasoning."}, {"title": "5.3 Ablation Studies", "content": "To evaluate the contribution of each component within ETH, ablation studies are conducted, as summarized in Table 3.\nImpact of Euclidean Semantic Modeling. The importance of Euclidean semantic modeling (Equations 4 and 5) is assessed by removing this component, retaining only the Tangent and Hyperbolic spaces with randomly initialized embeddings (denoted as -se). The results reveal a significant performance drop across all datasets, underscoring the critical role of semantic information in TKG extrapolation."}, {"title": "5.4 Visualization Analysis", "content": "Tangent Transformation Analysis. Figure 4 illustrates the density distributions of L2 norms for candidate and query entities in Tangent space (||hg|| and ||h2||) for the ICEWS14 and YAGO datasets. The embeddings in Tangent space appear stretched and scaled down, contributing to the model's robustness when \u1e9eq,r = 1, as it mitigates gradient vanishing issues. Additionally, query embeddings exhibit larger, more varied norm distributions compared to candidates, indicating greater diversity in hierarchical and semantic features. This aligns with the observed performance drop when query modeling is omitted. Furthermore, the YAGO dataset shows multiple peaks in norm distributions, unlike the single peak in ICEWS14, reflecting YAGO's more diverse and hierarchical structure, as supported by the Khs distribution in Figure 3. This adaptability underscores the model's robustness.\nHybrid Scoring Analysis. Figure 5 shows randomly selected scoring examples from the testing phase on ICEWS14. The top ticks indicate query IDs (entity and relation), while the bottom ticks represent the correct entity IDs. In the cr heatmap, color intensity reflects absolute values; in the Bq,r heatmap, color represents the value, with \"<\" and \">\" indicating values below or above 0.5. The final heatmap shows the rank of the correct entity, with colors processed as - log10(rank) and annotations indicating the actual rank. The figure demonstrates the model's ability to adjust cr for each relation and \u1e9eq,r for each query, showing effective collaboration between Euclidean and Hyperbolic scores for more accurate rankings."}, {"title": "6 Conclusions", "content": "This paper presents ETH, a hybrid model that integrates Euclidean and hyperbolic spaces, bridged through tangent space, for temporal knowledge graph reasoning. By employing multi-space modeling, ETH effectively captures both semantic and hierarchical information. The model transitions embeddings from Euclidean space, through tangent space, into hyperbolic space, preserving semantic integrity while enhancing hierarchical learning. Experimental results demonstrate ETH's superiority over single-space models, with visualization analyses confirming its adaptability across diverse datasets. Future directions for this work include exploring other tasks that could benefit from the hybrid geometric space framework. Additionally, the proposed tangent space transformation can also be extended to other hyperbolic methods."}]}