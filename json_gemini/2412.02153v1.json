{"title": "Revisiting the Initial Steps in Adaptive Gradient Descent Optimization", "authors": ["Abulikemu Abuduweili", "Changliu Liu"], "abstract": "Adaptive gradient optimization methods, such as Adam, are prevalent in training deep neural networks across diverse machine learning tasks due to their ability to achieve faster convergence. However, these methods often suffer from suboptimal generalization compared to stochastic gradient descent (SGD) and exhibit instability, particularly when training Transformer models. In this work, we show the standard initialization of the second-order moment estimation ($\\upsilon_0 = 0$) as a significant factor contributing to these limitations. We introduce simple yet effective solutions: initializing the second-order moment estimation with non-zero values, using either data-driven or random initialization strategies. Empirical evaluations demonstrate that our approach not only stabilizes convergence but also enhances the final performance of adaptive gradient optimizers. Furthermore, by adopting the proposed initialization strategies, Adam achieves performance comparable to many recently proposed variants of adaptive gradient optimization methods, highlighting the practical impact of this straightforward modification.", "sections": [{"title": "1. Introduction", "content": "First-order optimization methods, such as stochastic gradient descent (SGD), have been foundational in training deep neural networks due to their robust convergence properties across various applications [3]. However, as deep learning architectures have grown more complex, there has been increasing interest in adaptive gradient optimizers, which dynamically adjust learning rates based on the gradients of individual parameters [6]. These methods often lead to faster convergence in certain tasks [13]. Among them, Adam has emerged as one of the most widely used adaptive gradient methods, successfully applied to fields such as computer vision, natural language processing, and reinforcement learning [18]. By combining the benefits of momentum and adaptive learning rates, Adam has proven particularly effective in training generative models and large language models [45]. Theoretical studies have further elucidated its convergence properties in non-convex settings, providing insights into convergence rates [46]. With careful hyperparameter tuning, Adam has achieved significant success, especially in transformer-based architectures [16, 32, 38].\nDespite its fast-convergence property, Adam has been observed to suffer from instability and poor generalization in certain non-convex optimization problems, such as training transformers for language models [24, 37]. This instability often causes the optimizer to converge to suboptimal local minima, thereby limiting the model's performance. Several modifications have been proposed to address these issues. For instance, AdaBound [27] improves generalization by bounding the step size with a smooth parameter update, while RAdam [24] rectifies the variance of the second-order moment to stabilize the learning rate during early iterations. AdaBelief [48] adapts the step size based on the \u201cbelief\u201d in the observed gradients, enhancing generalization. A broader range of studies has introduced further refinements to stabilize convergence and improve generalization"}, {"title": "2. Second-order Moment Initialization of Adam", "content": "This section focuses on the instability in the Adam optimizer caused by the standard zero-initialization of the second-order moment. Unlike the non-convergence issues discussed in prior works [37], the instability we address primarily affects the early stages of optimization in non-convex problems, particularly in deep neural networks."}, {"title": "2.1. Revisiting the Adam Optimizer", "content": "Update rule of Adam. The update rule for Adam is given by the following equations [18]:\n\n$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t = \\beta_1 m_0 + (1 - \\beta_1) \\sum_{k=0}^{t-1} \\beta_1^{t-k} g_k, \\hspace{0.5cm}  \\hat{m}_t =  \\frac{m_t}{1 - \\beta_1^t}$  \\tag{1}\n\n$\\upsilon_t = \\beta_2 \\upsilon_{t-1} + (1 - \\beta_2) g_t^2 = \\beta_2 \\upsilon_0 + (1 - \\beta_2) \\sum_{k=0}^{t-1} \\beta_2^{t-k} g_k^2, \\hspace{0.5cm}  \\hat{\\upsilon}_t = \\frac{\\upsilon_t}{1 - \\beta_2^t}$  \\tag{2}\n\n$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{\\upsilon}_t} + \\epsilon}$  \\tag{3}\n\nwhere $m_t$ and $\\upsilon_t$ represent the first and second moments, $g_t$ is a gradient of objective function. $\\beta_1, \\beta_2$ are the decay rates for the first and second-moment estimates, $\\alpha$ is the learning rate, and $\\epsilon$ is a small constant preventing division by zero. We rewrite the above term to illustrate the sign, and magnitude of the Adam [2]. Ignoring $\\epsilon$, since it is very small in practice, we have the step size:\n\n$\\Delta \\theta_t = \\theta_t - \\theta_{t-1} = -\\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{\\upsilon}_t}} = -\\alpha \\frac{\\hat{m}_t}{\\sqrt{\\upsilon_t}} \\sqrt{1 + \\frac{\\hat{\\upsilon}_t - \\upsilon_t}{\\upsilon_t^2}} sign(\\hat{m}_t)$  \\tag{4}"}, {"title": "First step of Adam as sign descent.", "content": "In Adam's standard implementation, the first- and second-order momentum terms are initialized to zero, $m_0 = 0, \\upsilon_0 = 0$. As a result, the first step of the optimization process degenerates into sign descent, where the magnitude of the step size depends solely on the learning rate $\\alpha$ rather than the full gradient. This behavior is illustrated as follows:\n\n$\\Delta \\theta_1 = -\\alpha \\frac{g_1}{\\sqrt{g_1^2 + 10^{-8}}} = -\\alpha \\cdot sign(g_1)$  \\tag{5}\n\nIn this first step, Adam performs a pure sign-descent update due to the zero initialization of $m_0 = 0, \\upsilon_0 = 0$. However, from the second step onward, the moving averages begin to incorporate gradient information, and the updates evolve into a combination of sign descent and adaptive gradient descent. Over subsequent iterations, as more gradient information is accumulated, the influence of the initial sign descent diminishes, and the optimizer transitions into its adaptive behavior where $m_t \\neq \\upsilon_t$, as shown in Equations (1), (2) and (4)."}, {"title": "2.2. Instability of Adam optimizer", "content": "Instability of Adam on training Transformer network. Training Transformer models for various NLP tasks often relies on a learning rate warmup strategy [4], which has also been shown to enhance accuracy in Vision Transformers [5, 9]. Removing the warmup phase, however, has been observed to increase training loss, underscoring its role in stabilizing the optimization process [24].\nTo explore this phenomenon, we conducted experiments training a Transformer model on the IWSLT'14 DE-EN dataset for a neural machine translation task. We evaluated three approaches: vanilla Adam without warmup (denoted as $\\upsilon_{0,0}$), vanilla Adam with warmup, and our proposed data-driven initialization of Adam without warmup (denoted as $\\upsilon_{0,data}$, described in the next section). As illustrated in Figure 1(a), vanilla Adam without warmup exhibits increased training loss during the early stages. We attribute this instability to Adam's initial sign-descent behavior, which is exacerbated by the standard zero-initialization of the second-order moment ($\\upsilon_0 = 0$). While the learning rate warmup strategy effectively addresses this issue, it requires using a very small learning rate during the initial stages, limiting parameter updates and slowing down convergence. In this work, we propose a non-zero initialization strategy to directly stabilize the optimizer. Unlike warmup, our approach avoids restrictive learning rate constraints, enabling faster convergence while maintaining training stability.\nImpact of sign descent and shrinking gradients. In this section, we analyze the non-convergence behavior of vanilla Adam, focusing on the large initial step sizes observed during neural network training (Figure 2(a)). Neural networks often exhibit a flat loss landscape at the beginning of training, with gradients that are small in magnitude. This phenomenon is particularly pronounced when training Transformers, as noted in prior works [15, 34, 42]. The initial loss landscape of the Transformer model is visualized in Figure 1(c), where the loss is plotted along two random directions as described in [22]. The visualization highlights that the loss landscape is extremely flat, and gradients"}, {"title": "2.3. Non-zero Initialization of Second Order Moment", "content": "As shown in Equations (2) and (4), initializing the second-order moment $\\upsilon_0$ with non-zero values effectively prevents the first step of Adam from degenerating into sign descent.\nSpecial case: linear loss. To build intuition for initializing the second-order moment, we first study a simplified setting. Consider the linear loss function $f(\\theta_t) = <\\theta_t, g_t>$ with a Noisy Gradient"}, {"title": "INITIALIZATION OF ADAPTIVE OPTIMIZATION", "content": "Oracle with Scale Parameter (NGOS), a widely used framework for analyzing training dynamics of optimizers [23, 29]. In this setting, the stochastic gradient $g_t$ is sampled from a Gaussian distribution with mean $\\bar{g}$ and and variance $\\sigma^2 I$, i.e. $g_t \\sim \\mathcal{N}(\\bar{g}, \\sigma^2 I)$. This setup mimics mini-batch training in neural networks, where the stochastic gradient is provided as a noisy approximation of the full gradient. Using this framework, the expectation of first- and second-order moments is given by\n\n$E[m_t] = \\beta_1 m_0 + (1 - \\beta_1) \\sum_{k=0}^{t-1} \\beta_1^k \\bar{g} = \\beta_1 m_0 + (1 - \\beta_1)\\bar{g}$  \\tag{6}\n\n$E[\\upsilon_t] = \\beta_2 \\upsilon_0 + (1 - \\beta_2) \\sum_{k=0}^{t-1} \\beta_2^k (\\bar{g}^2 + \\sigma^2 I) = \\beta_2 \\upsilon_0 + (1 - \\beta_2) (\\bar{g}^2 + \\sigma^2 I)$  \\tag{7}\n\nThese results indicate that, after a sufficient number of steps, $E[m_t] \\approx \\bar{g}$, and $E[\\upsilon_t] \\approx \\bar{g}^2 + \\sigma^2 I$. In many practical scenarios, where the average gradient magnitude is small $E[g_t] \\approx 0$, initializing $m_0 = 0$ is a reasonable choice to stabilize $m_t$. Since $m_t$ approximates the first moment of the gradient, zero initialization aligns with its role. However, for $\\upsilon_t$, which represents the second-order moment of the gradient, it must satisfy $E[\\upsilon_t] > 0$. This makes the standard zero initialization ($\\upsilon_0 = 0$) inherently inconsistent with its purpose. Furthermore, $\\upsilon_0$ plays a critical role in determining the adaptive learning rate during the initial steps, directly influencing convergence and optimization stability.\nTo assess the stability of the optimization process and the influence of the initial state, we define the drift of the second-order moment as:\n\n$drift_{\\upsilon_t}(\\upsilon_0) = ||E[\\upsilon_{\\infty}] - E[\\upsilon_0]||$  \\tag{8}\n\nThis term quantifies the adjustment required for the second moment to transition from its initial value to its steady-state. It reflects how much the optimizer must adapt its gradient scaling during training. Since $\\upsilon_t$ directly determines the adaptive learning rate, a smaller drift term indicates better stability of optimization process.\nFor vanilla Adam, $\\upsilon_0 = 0$, the expected value of $\\upsilon_t$ converges to $E[\\upsilon_{\\infty}]] = \\bar{g}^2 + \\sigma^2 I$ from $E[\\upsilon_0] = 0$. Then $drift_{\\upsilon_t}(\\upsilon_0 = 0) = \\bar{g}^2 + \\sigma^2$. This large drift value causes significant initial adjustments of $\\upsilon_t$, leading to potential instability in optimization.\nFor non-zero initialization, $\\upsilon_0 = \\bar{g}^2 + \\sigma^2 I$, the expected second moment remains constant for all $E[\\upsilon_t] = \\bar{g}^2 + \\sigma^2 I$. Thus $drift_{\\upsilon_t}(\\upsilon_0 = \\bar{g}^2 + \\sigma^2 I) = 0$. With this initialization, $\\upsilon_t$ is immediately aligned with its steady-state value, eliminating the need for adjustments and ensuring stability from the start. The expectation $E[\\upsilon_t]$ is of scale $O(\\sigma^2)$ and the standard deviation of each coordinate of $\\upsilon_t$ is of scale $O((1 - \\beta_2)\\sigma^2)$. When $\\beta_2$ closer to 1, $\\upsilon_t$ becomes nearly deterministic and tightly concentrates around $\\upsilon_t \\approx \\bar{g}^2 + \\sigma^2 I$. Ignoring $\\epsilon$ for simplicity, the Adam update rule becomes:\n\n$\\theta_t \\sim \\theta_{t-1} - \\alpha \\frac{m_t}{\\sqrt{\\bar{g}^2 + \\sigma^2 I}}$  \\tag{9}\n\nThis ensures a stable adaptive learning rate: $\\alpha \\cdot (\\bar{g}^2 + \\sigma^2 I)^{-1/2}$. Such stability aligns with the definition of an adaptive learning rate, where $\\upsilon_t$ incorporates local geometry (e.g., Hessian information). For the linear loss case, this stability results in more consistent updates. Further illustration of the stability provided by a non-zero $\\upsilon_0$ in RMSprop is presented in Appendix A.1."}, {"title": "INITIALIZATION OF ADAPTIVE OPTIMIZATION", "content": "For random initialization, $\\upsilon_0 = \\lambda I, \\lambda > 0$, the the drift term becomes: $drift_{\\upsilon_t}(\\upsilon_0 = \\lambda I) = |\\bar{g}^2 + \\sigma^2 I - \\lambda I|$. For any $0 < \\lambda < 2(\\bar{g}^2 + \\sigma^2)$, this drift term is smaller than that of zero initialization: $drift_{\\upsilon_t}(\\upsilon_0 = \\lambda I) < drift_{\\upsilon_t}(\\upsilon_0 = 0)$. This reduced drift results in a more stable optimization process compared to $\\upsilon_0 = 0$, even with random initialization.\nInitialization of $\\upsilon_0$. Inspired by the analysis of linear loss cases with stochastic gradients, we propose two different non-zero initialization strategies for the second-order moment $\\upsilon_0$.\n\\begin{itemize}\n    \\item Data-driven Initialization, denoted as $\\upsilon_{0,data}$. In the data-driven strategy, $\\upsilon_0$ is initialized using the gradient statistics calculated from sampled training data $(x_i, y_i) \\sim D$, where $D$ represents the training set. Specifically, for sampled data $(x_i, y_i)$, the gradient of the loss function is computed as: $g(x_i, y_i) = \\nabla_{\\theta} f(x_i, y_i)$ for $(x_i, y_i)$. The second-order moment is then initialized as:\n    \n    $\\upsilon_0 = \\sigma \\cdot (E[g(x_i, y_i)]^2 + VAR[g(x_i, y_i)]), \\hspace{0.5cm} where (x_i, y_i) \\sim D.$  \\tag{10}\n    \n    Here, $\\sigma$ is a hyperparameter that controls the scale of $\\upsilon_0$. This approach ensures that $\\upsilon_0$ reflects meaningful statistical information about the gradient, aligning the optimizer's initialization with the characteristics of the specific training data.\n    \\item Random Initialization, denoted as $\\upsilon_{0,rnd}$. This is computationally efficient and avoids the overhead associated with data-driven initialization. As shown in the previous analysis, any small positive value for $\\upsilon_0$ enhances the stability of $\\upsilon_t$, making random initialization a practical choice. We propose initializing $\\upsilon_0$ using a scaled Chi-squared distribution 1:\n    \n    $\\upsilon_0 \\sim \\frac{\\sigma}{fan\\_in + fan\\_out} \\cdot \\chi_1^2$  \\tag{11}\n    \n    where $\\chi_1^2$ denotes a chi-squared distribution with one degree of freedom. $fan\\_in$ and $fan\\_out$ are the input and output dimensions of the weight matrix $\\theta \\in \\mathbb{R}^{fan\\_out \\times fan\\_in}$, and $\\sigma$ is a hyperparameter that controls the scale of the distribution. This distribution ensures that $\\upsilon_0$ scales appropriately with the dimensions of the weight parameters, similar to Xavier initialization for neural network weights [7]. Furthermore, the squared value $g_t^2$ of a Gaussian random gradient $g_t$ naturally follows a scaled chi-squared distribution, providing a principled foundation for this initialization strategy.\n\\end{itemize}\nUnder the proposed initialization $\\upsilon_{0,data}$ and $\\upsilon_{0,rnd}$, the first step size of Adam becomes:\n\n$\\Delta \\theta_1 = -\\alpha \\frac{g_1}{\\sqrt{g_1^2 + \\frac{\\beta_2}{1 - \\beta_2} \\upsilon_0}} \\nneq -\\alpha \\cdot sign(g_1), |\\Delta \\theta_1| < \\alpha$  \\tag{12}\n\nThis ensures that the first update step is influenced by both the magnitude and direction of the gradient, avoiding the pure \"sign descent\" behavior seen with $\\upsilon_0 = 0$. Such stabilization is particularly crucial for deep learning tasks with shrinking gradients, such as training Transformers.\nThe proposed initialization strategies are broadly applicable beyond Adam and can be extended to other adaptive gradient methods, including AMSGrad [37], AdaBound [27], RAdam [24], and AdaBelief [48]. These methods could benefit from improved stability during the initial steps, potentially enhancing both training dynamics and final performance. A discussion comparing the proposed initialization strategy with other optimization approaches is presented in Appendix A.2."}, {"title": "3. Experiments", "content": "To evaluate the effectiveness of our approach, we conducted extensive experiments across a variety of tasks, including image classification with convolutional neural networks (CNNs) [11], image generation with generative adversarial networks (GANs) [8], language modeling with long short-term memory networks (LSTMs) [14], and neural machine translation with Transformers [43]. We empirically evaluate the performance of two initialization strategies \u2014 $\\upsilon_{0,data}$ (Equation (10)) and $\\upsilon_{0,rnd}$ (Equation (11)) \u2014 across several widely used adaptive gradient optimization methods. These methods include SGD with momentum [35, 39], Adam [18], AdamW [26], AdaBound [27], RAdam [24], and AdaBelief [48]. For each optimizer, we use the standard initialization ($\\upsilon_0 = 0$) as the baseline and compare it against the proposed strategies ($\\upsilon_{0,rnd}$ and $\\upsilon_{0,data}$). For $\\upsilon_{0,data}$, gradient statistics are computed using 5,000 random samples prior to training, with the scaling factor set to $\\sigma = 1$. For $\\upsilon_{0,rnd}$, the scaling factor is set to $\\sigma = 100$. Detailed information about the experimental setup is provided in Appendix B.1."}, {"title": "3.1. Toy Experiments of Adam's Instability and Initialization", "content": "We conduct a toy experiment to illustrate the instability of Adam with its standard zero initialization and the effectiveness of our proposed non-zero initialization. For this demonstration, we use the random initialization strategy $\\upsilon_{0,rnd}$. The objective function is a non-convex saddle function:\n\n$f(x) = \\begin{cases}\n    (x - b)^n, & \\text{if } x \\geq x_s \\\\\n    -(x + b)^n, & \\text{if } x < -x_s \\\\\n    x^2 + d, & \\text{if } -x_s < x < x_s\n\\end{cases}$  \\tag{13}\n\nHere $x_s$ is a switch point, $b$ is a bias and $d$ is a shift ensuring smooth transition at the switch points.\n\n$x_s = \\frac{s}{n}(\\frac{s}{n})^{\\frac{1}{n-1}} + b, \\hspace{0.5cm} d = (x_s - b)^n - x_s^2$  \\tag{14}\n\nThe parameter $n$ represents the degree of the polynomial. In our experiment, we set $n = 7$, $b = 1$, and $s = 0.5$. The purpose of the experiment is to observe the optimization behavior under different initializations. We use the Adam optimizer with the following hyperparameters: $\\alpha = 1, \\beta_1 = 0.9, \\beta_2 = 0.999$. For scenarios requiring smaller learning rates, the objective function can be scaled down to achieve similar conclusions."}, {"title": "3.2. Image Classification with CNN", "content": "We evaluate the ResNet-34 architecture [11] on the CIFAR-10 image classification dataset [19]. The test accuracy at the final epoch is summarized in Table 1. The results demonstrate that the proposed initialization of $\\upsilon_0$, represented as $\\upsilon_{0,rnd}$ and $\\upsilon_{0,data}$, enhances the performance of adaptive gradient optimization methods, including Adam, AdamW, AdaBound, RAdam, and AdaBelief. Notably, with $\\upsilon_{0,data}$, Adam achieves a test accuracy surpassing that of the more recent AdaBelief approach. Furthermore, AdaBelief with $\\upsilon_{0,data}$ outperforms SGD, showcasing the effectiveness of the proposed method. $\\upsilon_{0,rnd}$ also consistently improves the performance of adaptive gradient methods without incurring additional computational overhead, making it a practical and efficient solution for stabilizing the optimization process."}, {"title": "3.3. Language Modeling with LSTM", "content": "We evaluate a 2-layer LSTM network [14] on the language modeling task of Penn Treebank dataset [30]. The test perplexity (lower is better) is summarized in Table 3. The results demonstrate that both $\\upsilon_{0,rnd}$ and $\\upsilon_{0,data}$ significantly improve the performance of adaptive gradient methods. Notably, with these proposed initialization strategies, Adam achieves test perplexity results that surpass the more recent AdaBelief optimizer. Results for a 3-layer LSTM network are provided in Appendix B.3."}, {"title": "3.4. Neural Machine Translation with Transformer", "content": "We evaluated a small Transformer model [43] using the Fairseq package [31] on the IWSLT'14 German-to-English machine translation dataset. The BLEU scores [33] are summarized in Table 4. The results demonstrate that the proposed initialization strategies, $\\upsilon_{0,rnd}$ and $\\upsilon_{0,data}$, provide significant performance improvements for adaptive gradient optimization methods."}, {"title": "3.5. Image Generation with GAN", "content": "We evaluated a deep convolutional GAN (DCGAN) [36] on the CIFAR-10 image generation task. The performance is measured using the Frechet Inception Distance (FID, lower is better) [12], which quantifies the similarity between generated images and the real dataset. In training GANs, optimizer stability is crucial for achieving high-quality image generation. As shown in Table 5, the proposed initialization strategies, $\\upsilon_{0,rnd}$ and $\\upsilon_{0,data}$, stabilize the optimization process for adaptive gradient methods, resulting in additional performance gains. For instance, $\\upsilon_{0,rnd}$ and $\\upsilon_{0,data}$ improve the performance of the Adam optimizer by 10% and 13%, respectively, highlighting the effectiveness of the proposed approaches."}, {"title": "3.6. Further Discussion of the Proposed Initialization Method", "content": "Training curve. We compare the training curves of Vanilla Adam and Adam with random initialization $\\upsilon_{0,rnd}$, as it is more computationally efficient 2. In the CIFAR-10 image classification task in Figure 4(a), while Adam $\\upsilon_{0,rnd}$ exhibits slightly lower accuracy in the initial steps, it achieves"}, {"title": "INITIALIZATION OF ADAPTIVE OPTIMIZATION", "content": "Ablation study The scaling factor $\\sigma$ is a key hyperparameter in the proposed initialization method Equations (10) and (11). To evaluate the impact of $\\sigma$, we conducted an ablation study on the CIFAR-10 image classification task, as summarized in Table 6. The results show that for a wide range of $\\sigma$ values, such as $\\sigma \\in [1, 1000]$, the performance consistently outperforms zero initialization. This highlights the robustness and tuning-friendly nature of the proposed approach, as it achieves stable improvements across different $\\sigma$ settings."}, {"title": "4. Conclusion", "content": "In this work, we revisited the initial steps of adaptive gradient optimization methods, focusing on the instability caused by the sign-descent behavior during early iterations. To address this issue, we proposed two simple yet effective approaches: data-driven initialization and random initialization of the second-moment estimate $\\upsilon_0$. Our empirical results demonstrate that these initialization strategies significantly enhance the performance and stability of several adaptive gradient optimization methods, including Adam, particularly in challenging tasks such as training Transformer models."}, {"title": "A.2. Revisiting Previous Works on Stabilizing the Initial Steps of Adam", "content": "Warmup. The warmup technique [28, 43] implicitly adjusts the initialization of the second-moment estimate $\\upsilon$ by employing a smaller learning rate during the initial steps. While the optimizer's state updates normally, the parameter changes are minimal due to the extremely small learning rate. This approach effectively mitigates the sign-descent behavior observed in Adam's early steps. However, warmup introduces additional hyperparameters (e.g., the scheduler) that require careful tuning and necessitates several steps of training where the network parameters are not effectively updated. This can be inefficient, particularly in resource-constrained settings. In contrast, our method directly addresses the aggressive sign-descent issue by initializing $\\upsilon_0$ with non-zero values, eliminating the need for a warmup phase. Our experimental results demonstrate that random initialization of $\\upsilon_0$ stabilizes the training process effectively, without requiring extra tuning or wasted iterations.\nRAdam. RAdam [24] avoids the sign-descent issue by behaving like SGD [28] during the initial steps. This is achieved by introducing a rectification term, dynamically adjusting the optimizer's behavior to stabilize updates in the early iterations. While RAdam successfully addresses initial-step instability, it adds complexity to the optimization process through the computation of the rectification term. In contrast, our approach provides a simpler and more intuitive solution by directly adjusting the initialization of the moment estimates, without modifying the core algorithm or introducing additional dynamic terms.\nAdaBound. AdaBound [27] tightly bounds the update size during the initial steps, preventing excessively large updates caused by sign-descent behavior. However, this approach introduces dynamic bounds that require careful tuning of the bounding functions, adding additional complexity to the optimization process. Our initialization strategy simplifies this issue by stabilizing updates without the need for dynamic bounds, making it a more efficient and practical alternative.\nAdaBelief. AdaBelief [48] reduces the impact of initial sign-descent behavior by refining the variance estimation, leading to more reliable adaptive learning rates. However, this comes at the cost of increased computational complexity due to the need for precise variance estimation. By contrast, our method provides stability during the initial steps without additional computational overhead, offering a straightforward alternative to improve early optimization dynamics.\nOur initialization strategy can be seamlessly integrated into existing methods, such as RMSprop, AdamW, RAdam, AdaBound, AdaBelief, and even Warmup. By addressing the aggressive sign-descent behavior directly through non-zero initialization of $\\upsilon_0$, we enhance the stability of these optimizers in their early steps. Importantly, this random initialization incurs no extra computational costs and avoids the need for additional hyperparameter tuning."}, {"title": "B.1. Experimental Setting", "content": "We empirically evaluate the performance of the proposed data-driven initialization (Equation (10)) and random initialization (Equation (11)) strategies across several widely-used adaptive gradient optimization methods. These include SGD with momentum (SGDM) [35, 39], Adam [18], AdamW [26], AdaBound [27], RAdam [24], and AdaBelief [48]. Each optimizer is tested using its standard initialization ($\\upsilon_0 = 0$) as the baseline, which is then compared against the proposed strategies $\\upsilon_{0,data}$ and $\\upsilon_{0,rnd}$. Following experimental protocols established in prior works [24, 44, 48], we perform thorough hyperparameter tuning for learning rate, $\\beta_1$, $\\beta_2$, and $\\epsilon$. To ensure statistical robustness,"}, {"title": "B.2. Additional Results for the Saddle Objective Function", "content": "We provide the additionnal results for the saddle objective function discussed in Section 3.1. The final converged parameter values for each method are summarized in Table 7. These results highlight that the proposed method achieves the lowest loss among all optimization techniques, underscoring its effectiveness in handling this optimization task."}, {"title": "B.3. Language Modeling with 3-Layer LSTM", "content": "We evaluat a 3-layer LSTM network on the Penn Treebank dataset [30]. The test perplexity results are summarized in Table 8. Similar to the findings with the 2-layer LSTM, the proposed initialization strategies provide additional performance gains for adaptive gradient optimization methods."}]}