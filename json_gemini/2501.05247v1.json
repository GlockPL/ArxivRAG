{"title": "Online Prompt and Solver Selection for Program Synthesis", "authors": ["Yixuan Li", "Lewis Frampton", "Federico Mora", "Elizabeth Polgreen"], "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities in the domain of program synthesis. This level of performance is not, however, universal across all tasks, all LLMs and all prompting styles. There are many areas where one LLM dominates, one prompting style dominates, or where calling a symbolic solver is a better choice than an LLM. A key challenge for the user then, is to identify not only when an LLM is the right choice of solver, and the appropriate LLM to call for a given synthesis task, but also the right way to call it. A non-expert user who makes the wrong choice, incurs a cost both in terms of results (number of tasks solved, and the time it takes to solve them) and financial cost, if using a closed-source language model via a commercial API. We frame this choice as an online learning problem. We use a multi-armed bandit algorithm to select which symbolic solver, or LLM and prompt combination to deploy in order to maximize a given reward function (which may prioritize solving time, number of synthesis tasks solved, or financial cost of solving). We implement an instance of this approach, called CYANEA, and evaluate it on synthesis queries from the literature in ranking function synthesis, from the syntax-guided synthesis competition, and fresh, unseen queries generated from SMT problems. CYANEA solves 37.2% more queries than the best single solver and achieves results within 4% of the virtual best solver.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are beginning to dominate the discourse around program synthesis and code generation. So much so, that one might suppose they are the de facto answer to all code-generation questions. However, this is not the case. There are many synthesis problems in which LLMs still fall far short of the basic enumerative techniques and symbolic solvers (Li, Parsert, and Polgreen 2024; Barke et al. 2024). In addition, even when an LLM is the best choice, they still hold a significant barrier to entry for the inexperienced user: first, not all LLMs perform uniformly well across all problem sets, and it is often unclear which LLM a user should choose; second, the performance of LLMs is often dependent on careful prompt engineering by expert researchers, with the literature reporting performance gains from many different prompting styles. Finally, compounding the challenge of these choices, calling LLMs is often expensive (in terms of computational cost, or the financial cost of using commercial APIs), and so making the wrong choice for a large set of synthesis tasks is highly undesirable.\nThis paper addresses these gaps through an online learning method that, given a synthesis task, will predict whether a symbolic solver or LLM, from a portfolio of LLMs, is most likely to solve the problem, with a corresponding prompting style.\nWe collate a portfolio of prompting styles and language models, which we combine into LLM-prompt pairs that we refer to as \"solvers\". We formulate the task of ranking the solvers in order of most likely to solve the problem as a multi-armed bandit problem (Auer et al. 2002). The multi-armed bandit sequentially selects between choices (in our case, solvers) with unknown rewards (in our case, rewards are given for solving problems correctly and fast or with low computational cost). It trades off exploration, i.e., trying new solvers, with exploitation, i.e., using solvers that are known to be good. We also present a second variation of this formulation, with multiple layers of bandits. The top multi-armed bandit selects between the symbolic solver and the LLMs, and then the bandits in the lower layer predict the best prompt style for the chosen LLM.\nWe implement an instance of our approach, CYANEA, and evaluate it on synthesis tasks from the syntax-guided synthesis competition (Alur et al. 2024), from the literature on ranking function synthesis (Giacobbe, Kroening, and Parsert 2022; Giesl et al. 2019), and generated from the SMT competition (Parsert and Polgreen 2024). CYANEA solves 37.2% more synthesis queries than the best single LLM or solver, and gets within 4% of the virtual best solver."}, {"title": "2 Background", "content": "Program Synthesis is the task of automatically generating code or expressions to satisfy some specification. In this paper, we define a program synthesis query q to be a tuple $(\u0442, f, \u0444)$ where \u03c4 is a background theory, f is a function to be synthesized, and 4 is a quantifier-free formula. A solution to the synthesis query is a body for the function f such that the formula & is t-valid, i.e., the formula $\u2200x.\u0444(f)$ is true where $(f)$ denotes the result of correctly substituting the synthesized body of f into the formula 4, and x is the vector of free variables in $(f).\nAn example program synthesis problem, written in SyGuS-IF (Padhi et al. 2023), is shown in Figure 1. Given a candidate solution, we can validate whether this solution is correct or not using a Satisfiability Modulo Theories (SMT) solver, by checking if the formula $\u2203x.\u00ac\u0444(f)$ is satisfiable (in which case the candidate f is incorrect) or not.\nProgram synthesis problems are often accompanied by a context-free grammar G, which is a 4-tuple $(V, \u03a3, R, S)$, where V is a finite set of non-terminal symbols. \u03a3 with $\u03a3\u2229V = \u00d8$ is a set of terminal symbols. R \u2286 V \u00d7 $(VUE)^*$ is a finite relation describing the production rules of the grammar. In this work, we do not view the grammar as a syntactic restriction and we provide a grammar that represents the whole space of possible solutions in the logic."}, {"title": "Multi-Armed Bandit Problems", "content": "Multi-armed bandit problems are online-learning problems in which a decision maker iteratively selects from a set of multiple fixed choices. Each choice has an unknown associated reward distribution, and the aim of the decision maker is to maximise the reward achieved over time. Hence, the agent should trade-off exploration (trying new actions to learn more about them) and exploitation (taking actions that are known to have potential for high reward). Here, we frame the problem of choosing LLMs and prompting styles as a MAB problem, where the decision maker selects the LLM and prompt combination to deploy, and the reward is based on successfully solving the synthesis problem. We assume that running a solver for a randomly selected synthesis problem is equivalent to sampling from some unknown distribution that we seek to approximate. Contextual MABs extend the problem by giving agents access to a feature vector before each round. This allows us to add information about the characteristics of the synthesis problem we are trying to solve in each round."}, {"title": "3 Overview", "content": "Problem Statement\nWe hypothesize that program synthesis users will frequently have not just one but a series of synthesis problems to solve. For instance, when synthesizing invariants, one may be synthesising invariants for multiple different loops within the same code base or system under verification. Users may also have different needs when it comes to performance (e.g., fastest, solves most queries, cheapest).\nGiven a list of synthesis queries Q = {$q_1, . . . q_m$}, a set of solvers S = {$s_1, . . . s_n$}, where each solver is either a symbolic solver, or an LLM paired with a prompting style (and LLM-prompt pair), we wish to use the solvers to generate a list of synthesis functions $f_1,... f_m$ such that each $f_i$ is a valid solution to $q_i$, using as few computational resources as possible. We define computational resources to be both the time spent solving a query, and an estimate of the financial cost of running it (which is based on tokens used for the LLMs, or runtime for the symbolic solver).\nApproach\nWe capture the computational resources we care about as reward functions. Our approach takes in Q, S and a time budget, and cost budget per query, T, and C respectively. For each synthesis query, our approach predicts an order of solvers that are most likely to solve the synthesis problem, and the time and cost that each is likely to take. We then distribute the total time and cost budget across the solvers accordingly, and deploy the solvers in sequence until the problem is solved.\nAfter a problem is solved, data is passed back to update the predictors, in the form of any rewards obtained by the solvers called, and the solving time and cost.\nWe break down the task of predicting LLM-prompt pairs in two different ways. In the first, we implement a single multi-armed bandit that predicts between all LLM-prompt pairs at once, show in Figure 2. In the second, we implement a multi-stage prediction, where we first predict which solver is most likely to succeed, and then predict which prompt strategy is most likely to succeed if an LLM chosen, shown in Figure 3. The components are outlined as follows:\nFeaturize: The featurize block takes in a synthesis query q in SyGuS-IF and generates a vector of features frepresent-ing that problem. We use a set of custom-designed features, which are outlined in Section 5.\nMulti-Armed Bandit Solver and Prompt Predictor:\nThe multi-armed bandit component of our workflow takes in a feature vector which represents the synthesis problem, and predicts the order in which its library of solvers will perform, according to a cost function. In our implementation, the library of solvers consists of two LLMs, with 6 prompting styles, and an enumerative solver, detailed in section 4.\nWe frame the problem as a contextual multi-armed banded problem, where the actions that the agent is choos-ing are the solvers. We implement two variations of this: the first, shown in Figure 2, uses a single multi-armed bandit agent to rank a set of LLM/prompt combinations and a symbolic solver. The second, layered approach, uses several layered multi-armed bandit agents; one to choose between the base LLMs or symbolic solvers, and a further agent for each LLM, which predicts which prompt to deploy.\nWe give details of the contextual multi-armed bandit algorithms in Section 5. After a solver is deployed, the reward"}, {"title": "4 Prompting Styles and Solvers", "content": "In this section we give an overview of the library of solvers S that our approach is equipped with. First we discuss the prompting styles:\nPrompting Styles\nWe develop a library of prompt templates based on the prompting styles reported to be successful in the literature. We detail the styles here, and illustrate them on our running example. Once we have chosen a style, we give the LLM up to 16 attempts to produce a correct synthesis result. If an answer produced is incorrect, we report the error information obtained from the SMT-solver used to check correctness back to the LLM:\nNatural language prompts: LLMs are primarily trained on natural language inputs, and so we implement a simple syntactic transformation procedure that translates a set of logical constraints into natural language.\nFew-shot prompting Few-shot prompting is prompting whereby the LLM is provided with a number of examples of the task, with satisfying solutions, before asking it to solve a new, similar task. We use 3 examples, taken from the previously solved synthesis problems.\nHigher resource programming language prompts: Our synthesis queries are in SyGuS-IF, a relatively uncommon language in the training data for LLMs. Thus, we use a prompting style that asks for the solutions in a higher-resource language and then asks for the translation into SyGuS-IF. We choose Lisp as the higher resource language rather than a more common language like Python because we find that translation from Python to SyGuS-IF is more error-prone than translation from Lisp, which is a fully parenthesized prefix notation similar to SyGuS-IF. This is a multi-stage prompting approach, shown in Figures 4 and 5. When asking for the translation into Lisp, we also provide 3 examples of previous translations.\nPrompting with roles Prefixing a prompt with an appropriate role description for the LLM can improve the performance of the LLM (Zheng, Pei, and Jurgens 2023). If using \"prompting with roles\" we append the sentence You are a good program synthesizer to the beginning of each prompt.\nEmotional stimuli It has been shown in the literature that adding emotional stimuli to prompts can improve the performance of LLMs (Li et al. 2023). We append the following emotional stimuli in Figure 6 to the prompt.\nMatrix of prompts In order to reduce the search space of prompts, we choose a fixed combination of prompting styles, shown in Table 1."}, {"title": "Enumerative Solver", "content": "The final solver in the library is an enumerative solver, based on CounterExample Guided Inductive Synthesis (CEGIS) (Solar-Lezama et al. 2006), with an A* based search phase. CEGIS alternates between a synthesis phase, which searches for a candidate solution that works for a subset of inputs, and a verification phase, where the candidate is checked against all possible inputs. If the verification fails, a counterexample is passed back to the synthesis phase and appended to the subset of inputs used to guide the search. In our case, the synthesis phase is implemented as an A* search, similar to that used by Euphony (Lee et al. 2018).\nA* is a graph search algorithm which uses two functions to guide its search: f: the sum of the costs on the edges used to reach the current state, and g: the estimated sum of the costs on the edges that will be used reach a target state from the current state. In our setting, each state is an expression (a partial or complete program) that can be generated from the grammar for the full logic, the initial state is start symbol of the grammar, the target states are any complete program, and each edge between states si and sk corresponds to a production rule that can transform the partial program at si into the partial/complete program at sj. The cost on any edge is proportional to the number of possible choices (so the more edges there are leaving from one state, the higher the cost of each edge).\nTo give some intuition, a partial program that contains few non-terminals and where each non-terminal symbol can only be replaced by production rules that lead immediately to a complete program has a low estimated cost to reach the target. We refer the reader to the detailed descriptions in the related work (Lee et al. 2018; Li, Parsert, and Polgreen 2024) for the full details. We choose this implementation of CEGIS as the enumerative solver to use because, in our experiments, it excels at finding short solutions that the LLMs often struggle with, without running into the memory issues that often plague bottom-up search methods in synthesis."}, {"title": "5 Online Solver Selection", "content": "The aim of the multi-armed bandit is to predict a ranking of which LLM and prompt combinations are most likely to solve the synthesis problem, and obtain the maximum reward while doing so. In our setting, the agent must trade off the exploration of using LLMs and prompts that it has not tried before, vs deploying LLM and prompt combinations that are known to have given high rewards in the past. In fact, we use an extension of the standard MAB problem, and ask the agent to predict a sequence of solvers to deploy rather than a single solver.\nk-Nearest Neighbor\nWe choose k-Nearest Neighbor as our contextual multi-armed bandit. Other contextual multi-armed bandits are available, but many of the common ones, for instance Lin-UCB, make assumptions that the performance of solvers is correlated linearly with the feature vector, which is unlikely to be the case in our application.\nk-NN is a simple supervised learning classifier. In our context, given a synthesis query q, it identifies the nearest k previously solved queries to q by calculating cartesian distance between the feature vectors. Each of the k queries $q_1,... q_k$ is labeled with the solver that it was solved by and the reward that was obtained, $r_1,... r_k$ respectively. The score for a solver si is given by the sum of the rewards for all queries solved by si. We rank the solvers based on this score (highest score is best). For any solvers that do not appear in this ranking, we randomly shuffle them and append them to the end of the list of solvers. If an LLM-prompt pair solves a query, we add a query with that feature vector to our database of queries with the corresponding reward.\nFor the double-layered multi-armed bandit, the first layer contains one k-NN multi-armed bandit which selects only between the LLMs, and the second layer contains a k-NN multi-armed bandit for each LLM, which selects between prompts. The second layer k-NN predictors are independent.\nReward functions Our approach is customizable to different reward functions We use three reward functions: the first simply aims to solve the queries as fast as possible, regardless of computational cost; and the second takes computational cost into account. The first reward function is given as follows:\n$pt = \\begin{cases} 0 & \\text{if query q is unsolved,} \\\\ (1 - \\frac{t}{T})^4 & \\text{if query q is solved} \\end{cases}$\nwhere t is the time taken to solve query q, and T is the total time budget for solving query q.\nThe second reward function aims to prioritize cheaper solving, and so accounts for the number of tokens in the prompt and response.\n$rc = \\begin{cases} 0 & \\text{if query q is unsolved,} \\\\ (1 - \\frac{c}{C})^4 & \\text{if query q is solved} \\end{cases}$\nwhere c is a cost estimate proportional to the number of tokens used in solving query q and C is the total cost budget for solving query q. The cost estimate is defined as c = input tokens + 3 \u00d7 output tokens for LLMs, which accounts for the higher cost of output tokens from commercial language model APIs. The actual cost of deploying an enumerative solver is proportional to the runtime and would be negligible for all queries in comparison to the cost of calling a commercial language model, so we fix the cost for the enumerative solver to be a small constant (0.4) for all queries.\nThe final reward is a simple binary reward, rb, which evaluates to 1 if a query is solved and 0 if it is not solved.\nFeatures A key component of the contextual multi-armed bandit is the featurization. We propose a feature extraction method to analyze SyGuS queries, capturing key syntactic attributes and query types. The extracted features we use are:\nKeywords: Frequencies of specific SMT-LIB keywords (e.g., +, -, *, div, etc).\nQuery length: The total number of tokens in the file.\nConstants: Number of constants of each type.\nQuery logic: e.g., BV, LIA, PBE, INV, etc."}, {"title": "Time and Token Budget Allocation", "content": "The final stage of the dynamic solver selection predicts the time that should be allocated to solver, and the cost. The goal is to allocate a sufficient proportion of our time and cost budget to each solver in the series that we are reasonably confident that it was unlikely to solve the query past this point. That is, for a solver $s_i$, we wish to find a minimum time allocation $t_i$ and cost allocation $c_i$ such that $P(t_i < u_i < T) \u2264 \u03b4_1$ and $P(c_i < v_i < C) \u2264 \u03b4_2$, where $u_i$ is the true runtime, $v_i$ is the true cost, and $\u03b4_1$ and $\u03b4_2$ are some small error thresholds. $\u03b4_1$ is the probability that we failed to solve a query because we allocated too little time to solving it, and $\u03b4_2$ is the probability that we failed to solve a query because we allocated too few tokens to it.\nLet us consider the cost allocation first: we model each prompt-pair's cost per query as an exponential distribution (that is, most queries are solved with a small number of tokens, only a few queries are solved with an excessively large number of tokens). We use maximum likelihood estimation (MLE) (Myung 2003) to estimate the parameters of the underlying exponential distribution, given the costs we have observed so far. Suppose we observe $U_1... U_n$ costs, which we assume are drawn from an exponential distribution Exponential(\u03bb). To find the exponential distribution which fits our observations best, we aim to solve $min_\u03bbnln\u03bb \u2013 \u03bb(\u03a3U_i)$, where $\u03a3iU_i$ is the sum of all costs observed so far. This gives us the minimizer $\u03bb^* = \\frac{n}{\u03a3U}$. We can apply the cumulative distribution function and calculate $c_i$ as: $C_i = - \\frac{ln(\u03b4 + e^{\\frac{\u22121}{\u03bb^*}}C)}{\u03bb^*}$ .\nTo make this contextual, we use only the costs from the k nearest samples, according to the feature vectors. We divide the total token budget C greedily between the solvers, cal-culation $c_i$ for each solver starting from the beginning of our ranking, and, once we have reached the total budget C, all following solvers are allocated zero tokens. If we reach the end of the list and have remaining tokens, they are given to the final solver.\nWe repeat all of the above for time. It is worth noting that time budget is not independent from cost budget, because if a solver is allocated zero tokens by the cost budget allocator, the time budget allocator will also not allocate it any time."}]}