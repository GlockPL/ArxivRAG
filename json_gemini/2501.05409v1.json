{"title": "A Novel Pathology Foundation Model by Mayo Clinic, Charit\u00e9, and Aignostics", "authors": ["Maximilian Alber", "Stephan Tietz", "Jonas Dippel", "Timo Milbich", "Timoth\u00e9e Lesort", "Panos Korfiatis", "Moritz Kr\u00fcgener", "Beatriz Perez Cancer", "Neelay Shah", "Alexander M\u00f6llers", "Philipp Seegerer", "Alexandra Carpen-Amarie", "Kai Standvoss", "Gabriel Dernbach", "Edwin de Jong", "Simon Schallenberg", "Andreas Kunft", "Helmut Hoffer von Ankershoffen", "Gavin Schaeferle", "Patrick Duffy", "Matt Redlon", "Philipp Jurmeister", "David Horst", "Lukas Ruff", "Klaus-Robert M\u00fcller", "Frederick Klauschen", "Andrew Norgan"], "abstract": "Recent advances in digital pathology have demonstrated the effectiveness of foundation models across diverse applications. In this report, we present a novel vision foundation model based on the RudolfV approach. Our model was trained on a dataset comprising 1.2 million histopathology whole slide images, collected from two medical institutions: Mayo Clinic and Charit\u00e9 - Universt\u00e4tsmedizin Berlin. Comprehensive evaluations show that our model achieves state-of-the-art performance across twenty-one public benchmark datasets, even though it is neither the largest model by parameter count nor by training dataset size.", "sections": [{"title": "1 Introduction", "content": "Anatomical pathology plays a central role in clinical medicine for tissue-based diagnostics and in biomedical research as a basis for the understanding of mechanisms of disease. Although molecular and omics-based data complement histological assessments, the microscopic evaluation of morphological changes remains the cornerstone of pathology practice. Consequently, with the advent of routine slide digitization, computational pathology has focused on making the analysis of histopathology slide images more precise, scalable, and reliable.\nHowever, despite artificial intelligence (AI) having led to promising proof-of-concepts and applications (e.g., [24, 8, 33, 5, 23]), generalization, application variety, and robustness remain challenging and have hindered the broad translation of AI applications into clinical routine diagnostics. Here, the limited adoption of digitization in clinical practice results in a scarcity of training data, particularly for infrequent and rare diseases [11]. Furthermore, generating sufficient labeled data representing the full spectrum of human disease, biological, and technical variability inherent to morphology, tissue processing, staining, and slide scanners has proven logistically and financially challenging.\nAddressing these problems, foundation models have quickly gained traction in the pathology domain based on their promise to achieve robust and generalizable data representations by incorporating the diversity found in pathology through large-scale self-supervised training. The generalizability and robustness of foundation models are particularly relevant to the performance of downstream tasks in pathology-a domain that has both high variation in input data and tasks, such as disease diagnoses, outcome prediction, detection of morphological structures, and quantification of biomarkers.\nIn this report, we utilized a corpus of 1.2 million histopathology whole slide images derived from more than 490,000 cases from Mayo Clinic and Charit\u00e9 - Universit\u00e4tsmedizin Berlin to train a ViT-H/14 pathology foundation model using the training paradigm from RudolfV [10]. Our model incorporates a broad diversity of diseases, staining types, and scanners, and utilizes multiple image magnifications during training. We provide an assessment of its performance compared to other leading models available for testing using twenty-one benchmark datasets assessing a variety of downstream pathology tasks."}, {"title": "2 Related Work", "content": "Multiple previous works on histopathology foundation models have developed a variety of models with increasingly large parameter counts and on increasingly expansive pathology datasets curated from public and private sources [10, 42, 13, 41, 34, 7, 9, 26, 17, 28, 32]. Pathology foundation models can be separated into tile- and slide-based models. Tile-based models, which represent the majority of the models, generate embeddings from fixed-sized image tiles derived from gigapixel whole"}, {"title": "3 Data and Methods", "content": null}, {"title": "3.1 Dataset and Preprocessing", "content": "A curated set of 1.2 million de-identified WSIs, derived from 490k pathology cases, from the digital archives of Mayo Clinic and Charit\u00e9 - Universit\u00e4tsmedizin Berlin, was utilized to generate 3.4 billion image tiles for training. Tiles were extracted at multiple resolutions, namely 0.25, 0.5, 1.0, and 2.0 microns per pixel, corresponding to objective microscopic magnifications of 40\u00d7, 20x, 10x, and 5x, respectively. An overview of relevant dataset statistics are given in Figure 2. Slides and tiles were sampled for training using the sampling algorithm of RudolfV [10]."}, {"title": "3.2 Model Framework and Compute Environment", "content": "A sampled dataset of ca. 520m tiles was used to train a ViT-H/14 (632 million parameters) model [12] using an adapted RudolfV [10] approach, which is based on the DinoV2 framework [30]. Model training was performed with Nvidia H100 GPUs within the Mayo Clinic Platform environment\u00b9."}, {"title": "3.3 Evaluation Protocols", "content": "We evaluated model performance using linear probing protocols as established in the literature [10, 34, 42], with all models evaluated on extracted embeddings from frozen backbones. We use both public benchmarks as well as public evaluation frameworks where available, to foster reproducibility and comparability. Results were computed for 5 seeds over data split, shuffling, and \"probing\" model initialization per foundation model and task, if not stated otherwise in a specific task description (see Appendix A.1). We report the mean performance and standard errors over the seeds. Dataset splits are described in detail in the respective task descriptions in Appendix A.1. No augmentations were applied when extracting embeddings. As all models use Vision Transformer architectures [12], we always evaluate both the CLS token and the CLS+Mean\u00b2 token embeddings for every model and report the better (maximum) performance of the two in Table 1. This accounts for potential systematic differences between information encoded in different tokens between different models, as also done in previous works (see e.g. [42]). Results of using the CLS and CLS+Mean token only, are reported in Table 4 and Table 5, respectively.\nFor patch-level classification tasks, which make up the majority of benchmark datasets, linear probing (LP) is the default protocol. Balanced accuracy was utilized as the performance metric for all patch-level classification tasks.\nWe use eva [14], an open-source evaluation framework for pathology foundation models, for LP evaluation where available\u00b3 Linear classification on extracted embeddings in eva is done by training a single-layer neural network with a batch size of 256 patches using stochastic gradient descent (SGD) with a cosine learning rate schedule and base learning rate of 0.0003 for a total of 12.5k iterations.\nFor patch-level classification tasks not implemented in eva, we use an internal LP evaluation framework. These include the MSI CRC (patch), MSI STAD (patch), Pan-cancer TIL, TCGA Uniform (10x), and TCGA Uniform (20x) datasets. In the internal framework, we perform LP on extracted embeddings using scikit-learn's Logistic Regression [31] with balanced class weights. The L2 regularization parameter is chosen by performing a cross-validated grid search over 15 different values between 10-8 and 104. Using the best parameter, a final model is fit and applied to the test set.\nFor all slide-level classification tasks, we use the eva [14] framework, which applies the default Attention-based Multiple Instance Learning (ABMIL) [15] protocol. Here, an ABMIL head with ReLU activations is trained with a batch size of 32 slides using the AdamW optimizer [27] with a cosine learning rate schedule and base learning rate of 0.001 for a total of 12.5k iterations. Balanced accuracy was utilized as a performance metric for all slide-level classification tasks.\nThe HEST-Benchmark [16] is composed of tasks for gene expression prediction and designed as multivariate regression. We follow and use the default evaluation protocol as recommended and implemented by the authors. We provide respective details in the description of the HEST-Benchmark in Appendix A.1."}, {"title": "4 Results", "content": "The following analysis is based on 21 public benchmark datasets from two public foundation model evaluation frameworks eva [14] and HEST [16] as well as additional tumor-micro-environment (TME) and cancer typing benchmark datasets. The tasks range from TME tissue- and cell-typing over identifying morphological patterns, identifying cancer subtypes, to classifying molecular mutations. The task descriptions and evaluation protocols are detailed in Appendix A.1 and Section 3.3, respectively.\nOur model achieves an average performance score of 61.9%, a 1.1 p.p. improvement over the two closest contenders, Virchow2 [42] and H-Optimus-0 [34]."}, {"title": "5 Discussion", "content": "The presented model showed consistently good results across a diverse set of benchmarks covering molecular and morphology related tasks. Related work using a larger data base [42] or more model parameters [42, 34, 41] suggests that scaling data and model size even further might yield additional improvements.\nTo accurately assess the quality, generalization, and robustness of pathology foundation models, it is essential to utilize a wide range of (public) benchmark datasets, covering the entire spectrum of diseases, morphologies, scanners, etc. Despite first standardized frameworks such as eva [14] and HEST [16] being available, foundation model development would benefit from a larger and more diverse pool of benchmark datasets. Access to such diverse datasets would provide deeper insights into model performance and robustness, thereby enhancing our understanding of the capabilities and limitations of pathology foundation models."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Task Descriptions", "content": "HEST-Benchmark The HEST-Benchmark was introduced by [16] for benchmarking foundation models for histology on the task of gene expression prediction from H&E-stained images. The benchmark includes 72 spatial transcriptomics profiles (using Xenium or Visium technology) with corresponding H&E-stained images from 47 patients grouped into 10 tasks based on organ. Each task involves predicting the expression levels of the 50 most variable genes (highest normalized variance) from 112\u00d7112 \u00b5m H&E-stained image patches (equivalent to 224\u00d7224 pixels at 20\u00d7 magnification) centered on each spatial transcriptomics spot, formulated as a multivariate regression problem. We used the default Ridge Regression with PCA (256 factors) evaluation protocol to solve the multivariate regression on extracted embeddings [16]. Specifically, the 10 tasks are to predict gene expression levels for invasive ductal carcinoma (breast cancer, IDC), prostate adenocarcinoma (prostate cancer, PRAD), pancreatic adenocarcinoma (pancreatic cancer, PAAD), skin cutaneous melanoma (skin cancer, SKCM), colonic adenocarcinoma (colon cancer, COAD), rectal adenocarcinoma (rectum cancer, READ), clear cell renal cell carcinoma (kidney cancer, ccRCC), hepatocellular carcinoma (liver cancer, HCC), lung adenocarcinoma (lung cancer, LUAD), and axillary lymph nodes in IDC (metastatic, LYMPH-IDC). The benchmark applies patient-stratified splits to avoid any train/test patient-level data leakage, resulting in a k-fold cross-validation where k is the number of patients [16]. Performance is evaluated using the Pearson correlation between the predicted and measured gene expression and reported results are the mean and standard deviation across folds.\nMSI CRC (patch) This dataset contains 173,630 H&E images (224\u00d7224 pixels at 20\u00d7 magnification) extracted from N = 360 colorectal cancer (CRC) tissue scans from TCGA (TCGA-CRC-DX). The task is binary classification of microsatellite instability (MSI) vs. microsatellite stability (MSS), which is a clinically important prognostic marker [22, 18]. The dataset is split into 56,044 (28,022 MSI + 28,022 MSS) training images, 18,682 (9,341 MSI + 9,341 MSS) validation images, and 98,904 (28,335 MSI + 70,569 MSS) test images.\nMSI STAD (patch) This dataset contains 198,464 H&E images (224\u00d7224 pixels at 20\u00d7 magnification) extracted from N = 315 stomach adenocarcinoma (STAD) tissue scans from TCGA (TCGA-STAD). The task is binary classification of microsatellite instability (MSI) vs. microsatellite stability (MSS), which is a clinically important prognostic marker [22, 18]. The dataset is split into 60,342 (30,171 MSI + 30,171 MSS) training images, 20,114 (10,057 MSI + 10,057 MSS) validation images, and 118,008 (27,904 MSI + 90,104 MSS) test images.\nThe pan-cancer tumor-infiltrating lymphocytes (TIL) detection dataset contains 304,097 H&E images (100\u00d7100 pixels at 20\u00d7 magnification) extracted from tissue sample scans of 23 different cancer types from TCGA [2, 35]. The task is to classify an image into TIL positive vs. negative. An image is labeled positive if it contains at least two TILs. The dataset is split into 209,221 training images and 56,275 test images.\nTCGA Uniform (10\u00d7) and (20\u00d7) The TCGA Uniform dataset [25] contains 264,110 to 271,700 patches per resolution with 256 \u00d7 256 pixels. The task is to differentiate between 32 cancer classes from different tissue types (e.g., Colon adenocarcinoma). Only patches showing the specific cancer type were extracted from the TCGA WSIs based on annotations from two trained pathologists. As there is no official train and test split, we generated five folds with no overlapping patients and sampled 100 patches per class and fold, resulting in a total dataset size of 16.000 patches. We generated one dataset containing patches with 0.5 \u00b5m/pixel (20\u00d7) and one with 1.0 \u00b5m/pixel (10\u00d7) to test the performance of the foundation models at different zoom levels. The results represent the mean balanced accuracy on the five-fold cross-validation evaluation.\nBACH The BACH dataset comprises 400 H&E microscopy images (2048\u00d71536 pixels at 20\u00d7 magnification) of breast cancer biopsies. It originates from the ICIAR 2018 Grand Challenge on Breast Cancer Histology images [3]. The classification task of the challenge is to classify each image into one of the following four classes: normal, benign, in situ carcinoma, and invasive carcinoma. The dataset is split into 268 training images (67 images per class) and 132 test images (33 images per class). The patches are resized and cropped to 224x224 pixels.\nCRC-100k The CRC-100k dataset contains 107,180 H&E images (224x224 pixels at 20\u00d7 magnification) extracted from colorectal cancer (CRC) tissue samples [21]. The tissue samples originate from CRC primary tumors and CRC liver metastases. The task of this benchmark is to classify each image into one of the following 9 tissue classes: adipose, background, debris, lymphocytes, mucus, smooth muscle, normal colon mucosa, cancer-associated stroma, and colorectal adenocarcinoma epithelium. The dataset is split into 100,000 training images (NCT-CRC-HE-100K-NONORM) and 7,180 test images (CRC-VAL-HE-7K). We take the original (no-norm) images without Macenko color normalization [20].\nMHIST The task of MHIST is to classify images of colorectal polyps into hyperplastic polyps (HPs) vs. sessile serrated adenomas (SSAs) [40]. This distinction is clinically important as HPs are typically benign whereas SSAs are precancerous lesions that can turn into cancer if left untreated. The task is challenging for pathologists, often showing considerable inter-pathologist variability. The MHIST dataset consists of 3,152 H&E images (224\u00d7224 pixels at 8\u00d7 magnification) of colorectal polyps and labels are derived from the majority vote of seven pathologists. The dataset is split into 2,162 training images (1,545 HP and 617 SSA) and 990 test images (630 HP and 360 SSA).\nPCAM PCAM (PatchCamelyon) defines the clinically-relevant task of metastasis detection as a binary image classification task [37]. The dataset consists of 327,680 H&E images (96\u00d796 pixels at 10\u00d7 magnification) extracted from scans of sentinel lymph node sections. Each image is annotated with a binary label indicating the presence of metastatic breast cancer tissue. An image is labeled as metastatic if the center 32\u00d732 pixels region contains at least one pixel of tumor tissue. The dataset is split by 80:10:10 into training, validation, and test sets with no overlap of WSIs/cases between the splits and every split having a 50:50 balance of positive and negative examples. For evaluation, we resize each image to 224x224 pixels. PCam has been derived from the CAMELYON16 Challenge [4].\nCAMELYON16 The task of the CAMELYON16 challenge [4] is to classify whole slide images (WSIs) of lymph node tissue sections into having metastatic breast cancer tissue or not. The dataset comprises 399 H&E-stained WSIs of sentinel lymph node sections, which were acquired and scanned (40x magnification) at two centers from the Netherlands [4]. The dataset is split into 270 training slides (110 with and 160 without metastasis) and 129 test slides (49 with and 80 without metastasis). Here, we report results for the CAMELYON16 (small) setup in eva [14], which randomly samples max. 1,000 patches per slide.\nPANDA The PANDA challenge [6] considers the challenging task of tumor grading of whole slide images (WSIs) of prostate cancer biopsies, which suffers from significant inter-observer variability between pathologists. Prostate cancer grading follows the Gleason grading system (3, 4, or 5) based on architectural growth patterns of the tumor, which are then converted into an ISUP grade on a scale of 1-5 for use as a prognostic marker. The dataset features 9,555 H&E-stained WSIs (subset with noisy labels removed) of prostate tissue biopsies from two medical centers scanned at 20\u00d7 magnification. Specifically, the task is to classify each WSI into an ISUP grade of 0\u20135 (six classes), where 0 means that a biopsy does not contain any cancer. The dataset is split into 6,686 training slides, 1,430 validation slides, and 1,439 test slides in a class-stratified manner. Here, we report results for the PANDA (small) setup in eva [14], which considers a fewer number of total slides (955 train, 477 validation, 477 test) as well as fewer randomly sampled patches per slide (200)."}, {"title": "A.2 Additional Results", "content": null}]}