{"title": "SoupLM: Model Integration in Large Language and Multi-Modal Models", "authors": ["Yue Bai", "Zichen Zhang", "Jiasen Lu", "Yun Fu"], "abstract": "Training large language models (LLMs) and multimodal LLMs necessitates significant com-puting resources, and existing publicly available LLMs are typically pre-trained on diverse, privately curated datasets spanning various tasks. For instance, LLaMA, Vicuna, and LLaVA are three LLM variants trained with LLaMA base models using very different training recipes, tasks, and data modalities. The training cost and complexity for such LLM variants grow rapidly. In this study, we propose to use a soup strategy to assemble these LLM variants into a single well-generalized multimodal LLM (SoupLM) in a cost-efficient manner. Assembling these LLM variants efficiently brings knowledge and specialities trained from different domains and data modalities into an integrated one (e.g., chatbot speciality from user-shared conversations for Vicuna, and visual capacity from vision-language data for LLaVA), therefore, to avoid computing costs of repetitive training on several different domains. We propose series of soup strategies to systematically benchmark performance gains across various configurations, and probe the soup behavior across base models in the interpolation space.", "sections": [{"title": "Introduction", "content": "Training large language models (LLMs) (Brown et al., 2020; Achiam et al., 2023; Devlin et al., 2018) presents several significant challenges, such as how to deploy immense size models on infrastructures and make large-scale optimization (Xie et al., 2024; Narayanan et al., 2021), and how to collect and prepare massive training data to match the model size (Swayamdipta et al., 2020; Wang et al., 2022). As a result, the computational cost and other efforts of training such networks is rapidly growing. For example, training a model like LLaMA3-7B (Touvron et al., 2023) requires an extensive amount of computation with carefully defined data and training recipe, not to mention a 70B model demands even more resources and training complexity, measured in thousands of H100 hours (Cho-quette, 2023). Constraints caused by these substantial computational costs mean that research into new large language models is often restricted to a limited number of teams with extensive resources, which may hinder the community development.\nMoreover, while extending the model capacities for multiple domains by transitioning LLMs into large multi-modal models (LMMs), additional challenges arise (Liu et al., 2024b; Zhu et al., 2023; Yan et al., 2021). Training LMMs typically follows the post-training approach, which involves finetuning the base model with a multi-modal instructional tuning dataset (Liu et al., 2024a; Li et al., 2024). For example, LLaVA (Liu et al., 2024b) enable its base Vicuna (Zheng et al., 2023) model to understand visual input by finetuning it with vision-language instruction data. In addition, extending the model with new architecture, such as branch mixing and training (Sukhbaatar et al., 2024) under Mixture-of-Experts (MoE) design (Shazeer et al., 2017), further complicates the process. Overall, as models become more unified and integrate diverse modalities, they face new issues like data and modality drift. Such issues require even more complicated data and optimization recipes, which are more complex than traditional challenges and further increase the multi-modal training costs.\nIn this context, the concept of model soup emerges as an effective strategy to merge the base model and its finetuned variants. It initially focuses on image classification task (Wortsman et al., 2022). Instead of picking the model with highest validation accuracy, model soup combines tuned models of different hyperparameter configurations, where all variants are trained from the same random initialized model that seen as the base model. The soup strategy obtains a robust model with the highest performance, which can be generalized to several visual backbones like CLIP (Radford et al.,"}, {"title": "Method", "content": "This section introduces vanilla, learnable, and regularized soup strategies for our SoupLM exploration, where vanilla initially explores the effectiveness of soup, learnable serves as our central method and regularized mainly for soup behavior analysis to validate our hypothesis. Given a set of base models with isomorphic model structures M = {f(\u03b81), f(\u03b82), ..., f(\u03b8m)}, where n is the number of base models. Here, the model f(\u00b7) generally represents network module at different granularities (e.g., each weight, each MLP block, and the whole model), which varies according to different soup strategies. We keep the model structure f(\u00b7) fixed and merge \u03b8\u2217 to obtain a souped model f(\u03b8S). The merging also keeps the weight \u03b8\u2217 fixed and only assign a bunch of \u03b1 to bridge base models. Then, the integrated one is given by\n$$f(\\theta^S) = \\sum_{i=1}^{n} \\alpha_i \\theta_i,$$\nwhere \u03b1 is the critical factor of our study and explored by following soup strategies. In this study, we specifically consider two autoregressive Transformer (Vaswani et al., 2017) base models, Vicuna and LLaVA, for the following soup strategies and the number of base models can be easily enlarged. And we ensure \u2211ni=1 \u03b1i = 1 to interpolate weight in linear model space."}, {"title": "Vanilla Soup", "content": "We use vanilla soup as a simple baseline to initially explore if directly combining weights of two base models improves the performance. Herein, f(\u00b7) represents the whole model, which is the largest granularity. We manually set different ratios \u03b11 (e.g., 0.5) for the first base model and use \u03b12 = 1 \u2212 \u03b11 for the second. The vanilla souped model is given by\n$$f(\\theta^S) = \\alpha^1\\theta^1 + (1 - \\alpha^1)\\theta^2,$$\nwhere we use \u03b11 = {0.1, 0.2, ..., 0.9} in our experiments (see Sec. 3.2)"}, {"title": "Learnable Soup", "content": "Instead of merging base models using model-level granularity as vanilla soup, we propose to refine the process by decreasing the soup granularity"}, {"title": "Regularized Soup", "content": "Learnable soup picks smaller granularity and merges base models by fixing the original ones. It also provides an intuitive way to investigate the model merging behaviors in the model space. To do so, we involve a regularization term to elaborate the soup process and point out the merging behavior for analysis. We use L1 normalization on the soup \u03b1 and augment Eq. 3 as\n$$L_{reg}(\\alpha) = L(\\alpha; \\theta^1, \\theta^2, D) + \\lambda||\\alpha||_1,$$\nwhere we omit the subscript of [s, l] for \u03b1. \u03bb is the regularization strength parameter and Lreg is the final regularized training objective. Other regularization formats (e.g., L2) can be easily extended and we simply consider L1 here. Through adding regularization on the \u03b1, its optimized values are constrainted close to its initializations. In this way, we set increasing regularization magnitudes to observe the changes of soup distribution, and validate the hypothesis that model soup performs stable behavior according to the given base models. Different from learnable soup above aiming to exhaust the soup potential, regularized soup is mainly to provide further intuitions of model soup behavior among base models during finetuning."}, {"title": "Experiments", "content": "Since we study series finegrained soup strategies based on multi-modal models with massive parameters, it is critical to propose a feasible path to manage the hyperparameter spaces for a reasonable exploration pipeline. Therefore, we briefly introduce base models, meta sets, and soup strategies, then elaborate them in the following sections."}, {"title": "Principle Design", "content": "Base Models\nWe specifically consider vision-language domains and choose representative Vicuna (Zheng et al., 2024) and its visual variants LLaVA (Liu et al., 2024b) as two base models. Vicuna is fine-tuned from LLaMA (Touvron et al., 2023) with human conversation instruction, which enable it with chatbot function. LLaVA is further finetuned from Vicuna using vision-language instructions, therefore, the model can understand visual input and interact with users by language. Basically, they are both variants from original LLaMA, sharing the isomorphical structures on language decoder, and"}, {"title": "Vanilla Soup", "content": "Our exploration begins with the simplest vanilla soup. Given Vicuna and LLaVA as base models, we set \u03b11 = {0.1, 0.2, ..., 0.9} (\u03b12 correspondingly obtained by Eq. 2) to merge them and test on meta sets. Fig. 1 shows the soup performance (green dots) and two base models as baselines (blue and red lines). We conclude 1) LLaVA naturally improves vision-language tasks (MMMU and LLaVA-Bench), as it is visually finetuned. Further, since the visual finetuning also contain language partition, it also enhances two general language-only tasks (MMLU and Hellaswag), but not for GSM8K which is more specific in math. 2) Vanilla soup performs generally better than two baselines proving the soup strategy effectiveness. 3) For 4 out of 5 meta sets (except MMMU), the trending of vanilla soup performance shows half-half average of base models obtains better results compared with other ratios, especially certain extreme cases (e.g., \u03b11 = 0.1, 0.9). However, this is not for MMMU which highly relies on the visual finetuning for improvement. We track the performance comparison in Tab. 2"}, {"title": "Learnable Soup", "content": "After vanilla soup as a simple proof-of-concept validation, we then go into details of learnable soup method, where we elaborate extensive ablation study. This ablation aims to firstly find if such fine-grained soup can 1) further obtain performance gain compared with vanilla soup, and 2) find statistical soup patterns across several hyperparameter dimensions, helping to understand the soup sensitivity under different settings. Specifically, given five meta sets for finetuning and evaluation, we cover 1) datasets, 2) epoch, 3) learning rate, 4) sample number, 5) sample ratio, and 6) activation aspects for ablations. It is hard to systematically discover the global oracle setting, as all dimensions are entangled together. Therefore, we heuristically design a path to search for the best combination from several rounds of ablation study. Along with them, we summarize the soup performance patterns in a statistical way.\nFirst Round\nWe begin with searching for the best meta sets combination by: 1) using each individual meta set to finetune, 2) fixing the total sample number as 1000, 3) ablating the epoch from 1 to 9, 4) ablating the learning rate from 0.001 to 0.3, 5) evaluating on 5 meta sets. We representatively show a bunch of visualization in Fig. 2, which uses MMMU as finetuning set. The rest visualizations are supplemented in Fig. 7 in appendix due to the limited space. Corresponding performances are also tracked in Tab. 2. To summarize all visualizations, we calculate the mean and maximum performance of 5 meta sets across epochs and learning rates in Tab. 3. We conclude 1) finegrained learnable soup outperforms vanilla soup for each evaluation task, obtaining further performance gain compared with two baselines. However, the best results of each meta set are based on different hyperparameter settings. Due to the different properties of"}, {"title": "Third Round", "content": "We finally make ablation on the ratio of given meta sets as the last round. Given the setting from first and second round, we adjust the sample ratio from LLaVA665K and MMLU from 5-95 to 95-5 to test if the ratio is a sensitive factor for evaluation. Performance variances are shown in Fig. 4. We conclude there are no clear trend according to the sample ratio based on the given setting, except for the MMLU task. Overall, the 50-50 ratio achieves the averagely better results than others. Through the three rounds heuristic ablations, we benchmark the soup performance on 5 meta sets, covering several hyperparameter configurations and fully exploring the model soup potential. Statistically, we find the better configurations and provide intuitions of the hyperparameter properties for SoupLM."}, {"title": "More Evaluations", "content": "Using the best soup setting from three rounds ablation, we evaluate its soup performance on more diverse evaluation tasks other than given five meta sets. We choose Winoground (Thrush et al., 2022), PiQA (Bisk et al., 2020), MathQA (Amini et al., 2019), BoolQA (Clark et al., 2019), and BBH (Suz-gun et al., 2022) for language and POPE (Li et al.,"}, {"title": "Soup Behavior", "content": "Beside of discussing performance gain, we initially study the soup behavior based on empirical results (Sec. 3) and regularized soup (Sec. 2.3). We are curious if the soup dynamics follow certain patterns under different training constraints and supervisions. We first probe such behavior through visualizing the learned \u03b1 from different meta sets. Since we are only curious about its distribution, we tune the \u03b1 with 0.3 learning rate, 9 epochs, and 1000 samples to ensure it is fully optimized. We visualize an exemplar case of key mapping across the language decoder layers (Fig. 5). We visualize the rest of visualizations in the appendix (Sec. A.5) including other mappings, normalization layers, etc. Furthermore, we set series of regularization magnitudes to observe if the soup behavior varies under training constraints. We visualize the regularized soup of key mapping with 0.0001 magnitude in Fig. 6 and leave the rest magnitudes in appendix (Sec. A.6). Figures show how the two base models are integrated into the souped model. Through the x-axis, they show different meta sets across different layers. Y-axis indicates the learned ratios between Vicuna and LLaVA. If the ratio is more than 0.5, meaning the corresponging base model dominates the soup process for this mapping, we color it as green, otherwise, as red. According to these figures, we observe 1) for some layers, the color distributions are very neat across different meta set, while for some others, these consistencies are not stable. 2) For the \u03b1 under regularized soup, we find the soup trends are not vulnerable, only generally close to the initial value 0.5 as constrained by the regularization. 3) Please note different mappings may show varied distributions and see more cases in the appendix. Overall, we draw the conclusions that the soup behaviors are not vulnerable under regularized constraints, and show consistency across certain layers but may vary different layers and mappings. In this study, we initially probe the soup behavior to provide intuitions by visualizations, and hope it inspires more model interpolation mechanism explorations."}, {"title": "Related Work", "content": ""}, {"title": "Large Language and Multi-Modal Models", "content": "Large-scale language models (LLMs) show that large-scale pretraining enables model with strong language capacity with massive knowledge (Rad-ford et al., 2018, 2019; Brown et al., 2020; Devlin et al., 2018; Liu et al., 2019; Touvron et al.,"}, {"title": "Model Soup", "content": "Model soup (weight averaging) is widely used to study optimization process (Ahmadianfar et al., 2022; Bansal et al., 2011). Many works study how it works on improving neural network capacity or analyze the model behavior (Nowlan and Hinton, 2018; Blundell et al., 2015). For large-scale networks, model soup is firstly studied by (Worts-man et al., 2022). It benchmarks the soup method on image classification with different backbones, and obtain free performance gain with no inference cost, which is critical for large-scale models. Soup strategy also benefits to enhance adapter structure (Chronopoulou et al., 2023), personalized finetuning (Jang et al., 2023), continue training (Akiba et al., 2024), etc, for language models. Different from existing works, our work explores model soup for large language and vision-language models in a cross-domain fashion with more general purposes."}, {"title": "Conclusion", "content": "We propose SoupLM to first explore the model soup strategy in autoregressive large language models (LLMs) and large multi-modal models (LMMs). This study takes Vicuna and LLaVA as a study case to 1) propose series soup strategies to fully explore the model soup potential pursuing performance gain, 2) statistically benchmark learnable soup capacity across systematically designed configuration space and observe comprehensive hyperparameter patterns, 3) initially probe the soup behavior to observe its consistent property across configurations and regularizations. SoupLM efficiently assembles isomorphical model variants into a well-generalized one that handles multiple domains, with no inference and ignorable training costs. It inspires to fast integrate and iterate large-scale models with multiple domain capacities while avoiding costly additional training efforts."}, {"title": "Limitations", "content": "We propose SoupLM to merge LLM and LMM into a well-generalized model that handles both language and vision-language domains. However, due to the massive computational requirements to benchmark the model soup for large-scale models, 1) we only take two base models with 7B model size as a study case, which can be easily extended into more general cases, 2) we only provide a heuristic design to benchmark the soup performance on base models, since it is almost not feasible to find the oracle setting among several configuration dimensions. We leave more general studies in our future work."}, {"title": "Supplementary Material", "content": ""}, {"title": "More Implementation Details", "content": "Our experiments are conducted on A6000 GPUs. We borrow the code of LLaVA and use its provided model checkpoints for Vicuna and LLaVA base models, and the LLaVA665K instruction dataset. We directly use the training split of meta sets from Huggingface (Wolf et al., 2020). For language evaluation, we leverage on the organized lm-evaluation-harness (Gao et al., 2023) codebase, and for vision-language tasks, we follow the evaluation instruction from LLaVA or use their official evaluation protocols. Our exploration is mainly based on 7B model with their V1.5 version, but it can easily extended to larger model size and other versions of models."}, {"title": "More Evaluation Performances", "content": "Due to the limited space in the main draft, we provide more evaluation performances on language and vision-language domains (More Evaluations section in Sec. 3) in Tab. 4"}, {"title": "Complete First Round Ablation Visualizations", "content": "We provide complete first round ablation visualizations in Fig. 7. It contains the complete finetuning and test set combinations, which is discussed in the First Round section in Sec. 3."}, {"title": "Complete Second Round Ablation Visualizations", "content": "We provide complete second round ablation visualization in Fig. 8. It contains the complete number of samples settings from 10 to 1000, which is discussed in the Second Round section in Sec. 3.\nThe statistical summary of the second round ablation is shown in Fig. 9, used to choose the best hyperparameter combanitions of the second round ablation."}, {"title": "Complete \u03b1 Distribution Visualizations", "content": "We provide complete \u03b1 distribution visualizations for different mappings in Fig. 10, Fig. 11, and Fig. 12. They include the mappings of attention, MLP, and normalization blocks, which are discussed in Sec 4. We also include visualizations of other mappings in Fig. 13."}, {"title": "Complete Regularized \u03b1 Distribution Visualizations", "content": "We provide complete regularized \u03b1 distribution visualizations in Fig. 14, Fig. 15, Fig. 16, Fig. 17. They include 0.0001 and 0.001 regularization magnitudes for attention and MLP blocks, which are discussed in Sec. 4."}]}