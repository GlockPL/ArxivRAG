{"title": "SoupLM: Model Integration in Large Language and Multi-Modal Models", "authors": ["Yue Bai", "Zichen Zhang", "Jiasen Lu", "Yun Fu"], "abstract": "Training large language models (LLMs) and multimodal LLMs necessitates significant computing resources, and existing publicly available LLMs are typically pre-trained on diverse, privately curated datasets spanning various tasks. For instance, LLaMA, Vicuna, and LLaVA are three LLM variants trained with LLaMA base models using very different training recipes, tasks, and data modalities. The training cost and complexity for such LLM variants grow rapidly. In this study, we propose to use a soup strategy to assemble these LLM variants into a single well-generalized multimodal LLM (SoupLM) in a cost-efficient manner. Assembling these LLM variants efficiently brings knowledge and specialities trained from different domains and data modalities into an integrated one (e.g., chatbot speciality from user-shared conversations for Vicuna, and visual capacity from vision-language data for LLaVA), therefore, to avoid computing costs of repetitive training on several different domains. We propose series of soup strategies to systematically benchmark performance gains across various configurations, and probe the soup behavior across base models in the interpolation space.", "sections": [{"title": "Introduction", "content": "Training large language models (LLMs) (Brown et al., 2020; Achiam et al., 2023; Devlin et al., 2018) presents several significant challenges, such as how to deploy immense size models on infrastructures and make large-scale optimization (Xie et al., 2024; Narayanan et al., 2021), and how to collect and prepare massive training data to match the model size (Swayamdipta et al., 2020; Wang et al., 2022). As a result, the computational cost and other efforts of training such networks is rapidly growing. For example, training a model like LLaMA3-7B (Touvron et al., 2023) requires an extensive amount of computation with carefully defined data and training recipe, not to mention a 70B model demands even more resources and training complexity, measured in thousands of H100 hours (Choquette, 2023). Constraints caused by these substantial computational costs mean that research into new large language models is often restricted to a limited number of teams with extensive resources, which may hinder the community development.\nMoreover, while extending the model capacities for multiple domains by transitioning LLMs into large multi-modal models (LMMs), additional challenges arise (Liu et al., 2024b; Zhu et al., 2023; Yan et al., 2021). Training LMMs typically follows the post-training approach, which involves finetuning the base model with a multi-modal instructional tuning dataset (Liu et al., 2024a; Li et al., 2024). For example, LLaVA (Liu et al., 2024b) enable its base Vicuna (Zheng et al., 2023) model to understand visual input by finetuning it with vision-language instruction data. In addition, extending the model with new architecture, such as branch mixing and training (Sukhbaatar et al., 2024) under Mixture-of-Experts (MoE) design (Shazeer et al., 2017), further complicates the process. Overall, as models become more unified and integrate diverse modalities, they face new issues like data and modality drift. Such issues require even more complicated data and optimization recipes, which are more complex than traditional challenges and further increase the multi-modal training costs.\nIn this context, the concept of model soup emerges as an effective strategy to merge the base model and its finetuned variants. It initially focuses on image classification task (Wortsman et al., 2022). Instead of picking the model with highest validation accuracy, model soup combines tuned models of different hyperparameter configurations, where all variants are trained from the same random initialized model that seen as the base model. The soup strategy obtains a robust model with the highest performance, which can be generalized to several visual backbones like CLIP (Radford et al.,"}, {"title": "Method", "content": "This section introduces vanilla, learnable, and regularized soup strategies for our SoupLM exploration, where vanilla initially explores the effectiveness of soup, learnable serves as our central method and regularized mainly for soup behavior analysis to validate our hypothesis. Given a set of base models with isomorphic model structures \\(M = {f(\\theta_1), f(\\theta_2), ..., f(\\theta_m)}\\), where n is the number of base models. Here, the model \\(f(\u00b7)\\) generally represents network module at different granularities (e.g., each weight, each MLP block, and the whole model), which varies according to different soup strategies. We keep the model structure \\(f()\\) fixed and merge \\(\\theta^*\\) to obtain a souped model \\(f(\\theta^s)\\). The merging also keeps the weight \\(\\theta^*\\) fixed and only assign a bunch of \\(\\alpha\\) to bridge base models. Then, the integrated one is given by\n\n\n\\(f(\\theta^s) = \\sum_{i=1}^{n} \\alpha^i \\theta^i,\\)\n\n\nwhere \\(\\alpha\\) is the critical factor of our study and explored by following soup strategies. In this study, we specifically consider two autoregressive Transformer (Vaswani et al., 2017) base models, Vicuna and LLaVA, for the following soup strategies and the number of base models can be easily enlarged. And we ensure \\(\\sum_{i=1}^{n} \\alpha^i = 1\\) to interpolate weight in linear model space."}, {"title": "Vanilla Soup", "content": "We use vanilla soup as a simple baseline to initially explore if directly combining weights of two base models improves the performance. Herein, \\(f()\\) represents the whole model, which is the largest granularity. We manually set different ratios \\(\\alpha^1\\) (e.g., 0.5) for the first base model and use \\(\\alpha^2 = 1 - \\alpha^1\\) for the second. The vanilla souped model is given by\n\n\n\\(f(\\theta^s) = \\alpha^1 \\theta^1 + (1 - \\alpha^1) \\theta^2,\\)\n\n\nwhere we use \\(\\alpha^1 = {0.1, 0.2, ..., 0.9}\\) in our experiments (see Sec. 3.2)"}, {"title": "Learnable Soup", "content": "Instead of merging base models using model-level granularity as vanilla soup, we propose to refine the process by decreasing the soup granularity to bridge base models in a fine-grained way, which is the central method in this paper. Concretely, we choose each module in Transformer block as a smaller soup unit \\(f(\u00b7)\\), such as the Q, K, V, O mappings in attention block and up, down mappings in MLP block. In addition, we also include all normalization layers, the very first embedding layer, and the last LM head mapping as units for soup. Basically, this process can be seen as a finegrained soup at per-mapping granularity.\nRather than manually assignment, we propose to optimize the finegrained \\(\\alpha\\) using a tiny development set \\(D\\). The optimization follows the typical finetuning protocol of autoregressive model to minimize the next token prediction loss, but only tuning the \\(\\alpha^{[*,*]}\\) while fixing both base models (\\(\\theta_1^{[*,*]}, \\theta_2^{[*,*]}\\)). It integrates the weights in the model space spanned by two base models, which is formally given by:\n\n\n\\(\\alpha^{[s,l]} = arg\\underset{\\alpha^{[s,l]}}{min} L(\\alpha^{[s,l]}; \\theta_1^{[s,l]}, \\theta_2^{[s,l]}, \\theta_1^{\\oslash [s,l]}, \\theta_2^{\\oslash [s,l]}, D),\\)\n\n\nwhere s represents different soup units (e.g., Q/up project in attention/MLP) and l means different Transformer layer indices. \\(L(\u00b7;\u00b7)\\) is the autoregressive loss. It elaborates the merging process by delicately tuning the soup weights following the data supervision to better take advantages of both base models. Such refinement with smaller soup granularity firstly leads to a more flexible model interpolation space to benefit further performance gain. Furthermore, it provides an access to investigate the functional mechanism of each soup unit by analyzing their merging behaviors. Please note that the learnable soup can be further elaborated by reducing the soup granularity such as neuron or other self-defined units and we keep the per-mapping soup units for this study."}, {"title": "Regularized Soup", "content": "Learnable soup picks smaller granularity and merges base models by fixing the original ones. It also provides an intuitive way to investigate the model merging behaviors in the model space. To do so, we involve a regularization term to elaborate the soup process and point out the merging behavior for analysis. We use L1 normalization on the soup \\(\\alpha\\) and augment Eq. 3 as\n\n\n\\(L_{reg}(\\alpha) = L(\\alpha; \\theta_1, \\theta_2, D) + \\lambda ||\\alpha||_1,\\)\n\n\nwhere we omit the subscript of [s, l] for \\(\\alpha\\). \\(\\lambda\\) is the regularization strength parameter and \\(L_{reg}\\) is the final regularized training objective. Other regularization formats (e.g., L2) can be easily extended and we simply consider L1 here. Through adding regularization on the \\(\\alpha\\), its optimized values are constrainted close to its initializations. In this way, we set increasing regularization magnitudes to observe the changes of soup distribution, and validate the hypothesis that model soup performs stable behavior according to the given base models. Different from learnable soup above aiming to exhaust the soup potential, regularized soup is mainly to provide further intuitions of model soup behavior among base models during finetuning."}, {"title": "Experiments", "content": "Since we study series finegrained soup strategies based on multi-modal models with massive parameters, it is critical to propose a feasible path to manage the hyperparameter spaces for a reasonable exploration pipeline. Therefore, we briefly introduce base models, meta sets, and soup strategies, then elaborate them in the following sections."}, {"title": "Principle Design", "content": "We specifically consider vision-language domains and choose representative Vicuna (Zheng et al., 2024) and its visual variants LLaVA (Liu et al., 2024b) as two base models. Vicuna is fine-tuned from LLaMA (Touvron et al., 2023) with human conversation instruction, which enable it with chatbot function. LLaVA is further finetuned from Vicuna using vision-language instructions, therefore, the model can understand visual input and interact with users by language. Basically, they are both variants from original LLaMA, sharing the isomorphical structures on language decoder, and their weights are consistently optimized step-by-step. Such consistencies benefits to further explore model interpolation upon these two models. Specifically, we use their 7B and V1.5 version to represent language and multi-modal domains. Among our experiments, we fix two base models and only investigate the interpolation weight \\(\\alpha\\) based on different soup strategies. We also fix the visual encoder and alignment MLP of LLaVA for both training and test. Please note the base model candidates can be easily generalized into other domains (e.g., audio and video) and multiple (>2) base models, but we only take language and vision-language ones in our study."}, {"title": "Meta Sets", "content": "Various evaluation benchmarks are designed for both language and vision-language models from different purposes, we choose a few representative ones as our meta (development) sets for benchmarking. Such meta sets fulfil: 1) they are well-prepared and robust evaluation datasets for certain general purposes, 2) they cover both language and vision-language multi-modal domains, 3) they contain training and corresponding test set. In this study, we choose MMMU (Yue et al., 2023), LLaVA665K (Liu et al., 2023a) for vision-language domain; MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), and Hellaswag (Zellers et al., 2019) for language domain. We use their given training set for finetuning and test set for evaluation1. The meta sets information is summarized in Tab. 1"}, {"title": "Soup Strategies", "content": "We study a series of soup strategies that interpolate two base models while fixing their original weights based on five meta sets. At first, we simply use vanilla soup as a initial baseline (Sec. 2.1) to test if such a naive method improves performance on 5 meta sets without complicated experimental designs. Then, we expound learnable soup (Sec. 2.2) as the central role in our experiments to 1) fully explore the soup potential for performance gain, 2) statistically depict the soup performance patterns under multiple hyperparameter dimensions. Finally, other than pursuing better performance, we deploy regularized soup (Sec. 2.3) to intuitively probe the stability of soup behavior under various regularized training scenarios."}, {"title": "Vanilla Soup", "content": "Our exploration begins with the simplest vanilla soup. Given Vicuna and LLaVA as base models, we set \\(\\alpha^1 = {0.1, 0.2, ..., 0.9}\\) (\\(\\alpha^2\\) correspondingly obtained by Eq. 2) to merge them and test on meta sets. Fig. 1 shows the soup performance (green dots) and two base models as baselines (blue and red lines). We conclude 1) LLaVA naturally improves vision-language tasks (MMMU and LLaVA-Bench), as it is visually finetuned. Further, since the visual finetuning also contain language partition, it also enhances two general language-only tasks (MMLU and Hellaswag), but not for GSM8K which is more specific in math. 2) Vanilla soup performs generally better than two baselines proving the soup strategy effectiveness. 3) For 4 out of 5 meta sets (except MMMU), the trending of vanilla soup performance shows half-half average of base models obtains better results compared with other ratios, especially certain extreme cases (e.g., \\(\\alpha^1 = 0.1, 0.9\\)). However, this is not for MMMU which highly relies on the visual finetuning for improvement. We track the performance comparison in Tab. 2"}, {"title": "Learnable Soup", "content": "After vanilla soup as a simple proof-of-concept validation, we then go into details of learnable soup method, where we elaborate extensive ablation study. This ablation aims to firstly find if such fine-grained soup can 1) further obtain performance gain compared with vanilla soup, and 2) find statistical soup patterns across several hyperparameter dimensions, helping to understand the soup sensitivity under different settings. Specifically, given five meta sets for finetuning and evaluation, we cover 1) datasets, 2) epoch, 3) learning rate, 4) sample number, 5) sample ratio, and 6) activation aspects for ablations. It is hard to systematically discover the global oracle setting, as all dimensions are entangled together. Therefore, we heuristically design a path to search for the best combination from several rounds of ablation study. Along with them, we summarize the soup performance patterns in a statistical way."}, {"title": "First Round", "content": "We begin with searching for the best meta sets combination by: 1) using each individual meta set to finetune, 2) fixing the total sample number as 1000, 3) ablating the epoch from 1 to 9, 4) ablating the learning rate from 0.001 to 0.3, 5) evaluating on 5 meta sets. We representatively show a bunch of visualization in Fig. 2, which uses MMMU as finetuning set. The rest visualizations are supplemented in Fig. 7 in appendix due to the limited space. Corresponding performances are also tracked in Tab. 2. To summarize all visualizations, we calculate the mean and maximum performance of 5 meta sets across epochs and learning rates in Tab. 3. We conclude 1) finegrained learnable soup outperforms vanilla soup for each evaluation task, obtaining further performance gain compared with two baselines. However, the best results of each meta set are based on different hyperparameter settings. Due to the different properties of training and evaluation sets, the soup performance varies significantly among them. 2) There are clear trends of performance changes with ablated learning rates and epochs (color changes in heatmap plots), indicating a clear hyperparameter patterns at least within one meta set, but may change across meta sets. 3) The soup patterns dramatically differs across different training-evaluation sets combination. For example, MM-MM observes the best combination in the middle with the worst at bottom right corner, but MM-ML shows completely different clues. 4) Based on the results in Tab. 3, we find LLaVA665K is better than MMMU to be chosen in multi-modal domain. MMLU and Hellaswag show their advantages in language-only domain. Considering, MMLU follows the multiple-choice task instead of typical natural language, thus we choose MMLU instead of Hellaswag.\nAs a summary, the first round ablation results in 1) learnable soup further improves the evaluation performance, 2) soup performance patterns change dramatically across different finetuning and test set combinations, but show clear pattern given a fixed training and test pair, and 3) overall, we use LLaVA665K and MMLU as training sets for following ablation rounds."}, {"title": "Second Round", "content": "Using LLaVA665K and MMLU as meta sets, we conduct the second round ablation study. It aims to find the best hyperparameter setting including 1) learning rate, 2) epoch, 3) sample number, and 4) activation. Concretely, we 1) fixing the training data as LLaVA665K and MMLU, 2) ablating sample numbers from 10 to 1000, 2) ablating learning rate from 0.001 to 0.3, 3) ablating epoch from 1 to 9, 4) ablating activation using sigmoid, linear, clamp, and softmax options2. Please note, from this round, we only evaluate four meta sets except for LLaVA-Bench here due to its massive request of OpenAI API. We show the representative visualization in Fig. 3 and the rest visualizations are supplemented in the appendix (Fig. 8) due to the limited space. We conclude 1) using LLaVA665K and MMLU as paired meta sets further improve the performance but not significantly. Similarly, the best setting for each evaluation task varies, indicating the soup process is sensitive to specific test set. 2) The performance changes are still clear given a fixed finetuning and test combination across learning rate, epoch, and activation, however, not consistent while varying the number of samples. Especially for MMLU task, the trend changes reversely as the number of sample increases. 3) The activation choice affects performances by a large margin such as the linear activation dramatically affect the performance, and overall the other options perform better than linear. We track the pair meta sets results in Tab. 2 and we search the best setting based on overall performance on meta sets. The statistical summary is given by Fig. 9 in appendix and we choose the best setting with 3 epoch, 50 sample, 0.1 learning rate, and softmax activation.\nAs a summary, given LLaVA665K and MMLU as meta sets, the second round ablation search the epoch, sample number, learning rate, and activations. We find the little performance gain compared with the first round and the soup performances vary across differen settings. The best overall setting is picked for the next round ablation."}, {"title": "Third Round", "content": "We finally make ablation on the ratio of given meta sets as the last round. Given the setting from first and second round, we adjust the sample ratio from LLaVA665K and MMLU from 5-95 to 95-5 to test if the ratio is a sensitive factor for evaluation. Performance variances are shown in Fig. 4. We conclude there are no clear trend according to the sample ratio based on the given setting, except for the MMLU task. Overall, the 50-50 ratio achieves the averagely better results than others. Through the three rounds heuristic ablations, we benchmark the soup performance on 5 meta sets, covering several hyperparameter configurations and fully exploring the model soup potential. Statistically, we find the better configurations and provide intuitions of the hyperparameter properties for SoupLM."}, {"title": "More Evaluations", "content": "Using the best soup setting from three rounds ablation, we evaluate its soup performance on more diverse evaluation tasks other than given five meta sets. We choose Winoground (Thrush et al., 2022), PiQA (Bisk et al., 2020), MathQA (Amini et al., 2019), BoolQA (Clark et al., 2019), and BBH (Suzgun et al., 2022) for language and POPE (Li et al.,"}, {"title": "Soup Behavior", "content": "Beside of discussing performance gain, we initially study the soup behavior based on empirical results (Sec. 3) and regularized soup (Sec. 2.3). We are curious if the soup dynamics follow certain patterns under different training constraints and supervisions. We first probe such behavior through visualizing the learned \\(\\alpha\\) from different meta sets. Since we are only curious about its distribution, we tune the \\(\\alpha\\) with 0.3 learning rate, 9 epochs, and 1000 samples to ensure it is fully optimized. We visualize an exemplar case of key mapping across the language decoder layers (Fig. 5). We visualize the rest of visualizations in the appendix (Sec. A.5) including other mappings, normalization layers, etc. Furthermore, we set series of regularization magnitudes to observe if the soup behavior varies under training constraints. We visualize the regularized soup of key mapping with 0.0001 magnitude in Fig. 6 and leave the rest magnitudes in appendix (Sec. A.6). Figures show how the two base models are integrated into the souped model. Through the x-axis, they show different meta sets across different layers. Y-axis indicates the learned ratios between Vicuna and LLaVA. If the ratio is more than 0.5, meaning the corresponging base model dominates the soup process for this mapping, we color it as green, otherwise, as red. According to these figures, we observe 1) for some layers, the color distributions are very neat across different meta set, while for some others, these consistencies are not stable. 2) For the \\(\\alpha\\) under regularized soup, we find the soup trends are not vulnerable, only generally close to the initial value 0.5 as constrained by the regularization. 3) Please note different mappings may show varied distributions and see more cases in the appendix. Overall, we draw the conclusions that the soup behaviors are not vulnerable under regularized constraints, and show consistency across certain layers but may vary different layers and mappings. In this study, we initially probe the soup behavior to provide intuitions by visualizations, and hope it inspires more model interpolation mechanism explorations."}, {"title": "Related Work", "content": ""}, {"title": "Large Language and Multi-Modal Models", "content": "Large-scale language models (LLMs) show that large-scale pretraining enables model with strong language capacity with massive knowledge (Radford et al., 2018, 2019; Brown et al., 2020; Devlin et al., 2018; Liu et al., 2019; Touvron et al.,"}, {"title": "Model Soup", "content": "Model soup (weight averaging) is widely used to study optimization process (Ahmadianfar et al., 2022; Bansal et al., 2011). Many works study how it works on improving neural network capacity or analyze the model behavior (Nowlan and Hinton, 2018; Blundell et al., 2015). For large-scale networks, model soup is firstly studied by (Wortsman et al., 2022). It benchmarks the soup method on image classification with different backbones, and obtain free performance gain with no inference cost, which is critical for large-scale models. Soup strategy also benefits to enhance adapter structure (Chronopoulou et al., 2023), personalized finetuning (Jang et al., 2023), continue training (Akiba et al., 2024), etc, for language models. Different from existing works, our work explores model soup for large language and vision-language models in a cross-domain fashion with more general purposes."}, {"title": "Conclusion", "content": "We propose SoupLM to first explore the model soup strategy in autoregressive large language models (LLMs) and large multi-modal models (LMMs). This study takes Vicuna and LLaVA as a study case to 1) propose series soup strategies to fully explore the model soup potential pursuing performance gain, 2) statistically benchmark learnable soup capacity across systematically designed configuration space and observe comprehensive hyperparameter patterns, 3) initially probe the soup behavior to observe its consistent property across configurations and regularizations. SoupLM efficiently assembles isomorphical model variants into a well-generalized one that handles multiple domains, with no inference and ignorable training costs. It inspires to fast integrate and iterate large-scale models with multiple domain capacities while avoiding costly additional training efforts."}, {"title": "Limitations", "content": "We propose SoupLM to merge LLM and LMM into a well-generalized model that handles both language and vision-language domains. However, due to the massive computational requirements to benchmark the model soup for large-scale models, 1) we only take two base models with 7B model size as a study case, which can be easily extended into more general cases, 2) we only provide a heuristic design to benchmark the soup performance on base models, since it is almost not feasible to find the oracle setting among several configuration dimensions. We leave more general studies in our future work."}]}