{"title": "Applicability of Large Language Models and Generative Models for Legal Case Judgement Summarization", "authors": ["Aniket Deroy", "Kripabandhu Ghosh", "Saptarshi Ghosh"], "abstract": "Automatic summarization of legal case judgements, which are known to be long and complex, has traditionally been tried via extractive summarization models. In recent years, generative models including abstractive summarization models and Large language models (LLMs) have gained huge popularity. In this paper, we explore the applicability of such models for legal case judgement summarization. We applied various domain-specific abstractive summarization models and general-domain LLMs as well as extractive summarization models over two sets of legal case judgements from the United Kingdom (UK) Supreme Court and the Indian (IN) Supreme Court - and evaluated the quality of the generated summaries. We also perform experiments on a third dataset of legal documents of a different type - Government reports from the United States (US). Results show that abstractive summarization models and LLMs generally perform better than the extractive methods as per traditional metrics for evaluating summary quality. However, detailed investigation shows the presence of inconsistencies and hallucinations in the outputs of the generative models, and we explore ways to reduce the hallucinations and inconsistencies in the summaries. Overall, the investigation suggests that further improvements are needed to enhance the reliability of abstractive models and LLMs for legal case judgement summarization. At present, a human-in-the-loop technique is more suitable for performing manual checks to identify inconsistencies in the generated summaries.", "sections": [{"title": "1 Introduction", "content": "Summarizing legal case judgements is a practical and challenging task, given the complicated and lengthy nature of the legal judgements. Typically, legal judgements are manually summarized by legal practitioners; in fact, many legal information sys- tems offer case summaries or headnotes written by legal practitioners. To reduce the pressure on humans, there have been several attempts towards automatic summariza- tion of legal judgements. Traditionally, extractive summarization methods - which extract important sentences/parts from the input document have been used to summarize legal case judgements [1-5]. But recently there has been increasing inter- est in using abstractive summarization models because these models are considered to generate more 'natural' and 'coherent' summaries. Hence there are some recent works that train abstractive summarization models on (legal document, summary) pairs [6, 7]. Legal domain-specific versions of pre-trained abstractive summarization models (e.g., Longformer [8], Pegasus [9]) have also been released, such as Legal-LED\u00b9 and Legal-Pegasus2.\nAdditionally, the recent times have seen the advent of general-domain Large lan- guage models (LLMs) which can be used for a wide variety of NLP tasks including summarization, translation, question answering, and so on [10]. These LLMs have the advantage that they can be applied for summarization of any document, including legal documents, without further training. In fact, LLMs have been used recently for news document summarization [11]. So the question arises how well the pre-trained abstractive summarization models and LLMs (like ChatGPT and Davinci) can per- form in the task of legal case judgement summarization. We attempt to answer this question in this paper.\nIn this work, we apply various legal domain-specific abstractive summarization models (such as Legal-LED and Legal-Pegasus) as well as general-domain LLMs on two datasets of legal case judgements and their summaries \u2013 (1) the UK-Abs dataset (a dataset of United Kingdom case judgements), and (2) IN-Abs dataset (a dataset of Indian case judgements). For comparison, we also apply extractive summarization models on the same datasets. We also perform experiments on a third dataset from the legal domain - (3) the GOVREPORT dataset (consisting of long reports published by the U.S. Government Accountability Office). We compute a large number of summary quality metrics for all the models, including traditional metrics such as ROUGE [12], METEOR [13] and BERTScore [14] (that match model-generated summaries with gold standard summaries) as well as metrics for quantifying the consistency of sum- maries with respect to the original document such as SummaC [15], NEPrec [16], and Numprec [16].\nOur results show that abstractive summarization models and LLMs usually per- form better than the extractive summarization methods in terms of the conventional summary quality metrics (see Section 6). However, summaries generated by the abstractive models and LLMs often contain inconsistencies and hallucinations [17]. We manually analyze the summaries generated by these models to report examples of such hallucinations and inconsistencies in the summaries (see Section 7). We also"}, {"title": "2 Related Work", "content": "Legal case Judgement and Long Document Summarization: Several extrac- tive and abstractive summarization methods have been used for summarizing legal case judgments [6, 7, 18-20]. In particular, recent works have applied abstractive sum- marization models like Legal-Pegasus, Legal-LED, and BART to Indian and UK court judgements [6]. To this end, supervised extractive and abstractive models have been trained / fine-tuned on (legal judgement, summary) pairs.\nUnsupervised extractive models have also been developed for this purpose, such as CaseSummarizer[3], DelSumm [2], and so on. Graphical models have also been used for the purpose of legal case judgement summarization [21].\nLLMs for summarization and other tasks in different domains: With the recent popularity of LLMs, they have been applied for text summarization as well. A work on news document summarization [11] tried to investigate the performance of multiple LLMs based on the type of prompts and LLM size. The study found that prompting techniques play a critical role in determining the quality of the sum- maries. A recent work [22] on code summarization using GPT used zero-shot and few-shot capabilities of LLMs for project-specific code summarization. A work on book summarization [23] tried to develop novel methods to summarize long books using foundational models like GPT-4 and Turbo-GPT-3.5. They divide the entire book into fragments and then follow a hierarchical strategy of divide and conquer to summarize the entire corpus of a long book. LLMs have also been used for comment summariza- tion of videos and topic matching for videos [24]. Summarization of long meetings has also been attempted using LLMs like GPT-4 and GPT-3.5 and encoder-decoder mod- els like BART [25]. There is a work [26] on distractor generation in MCQs where a"}, {"title": "3 Datasets for Legal Judgement Summarization", "content": "For most of the experiments in this paper, we use two datasets of court case judgements and their gold standard summaries, that are described below.\nThe IN-Abs dataset, obtained from the prior work [6], consists of Indian Supreme Court Case Judgements collected from the website of the Legal Information Institute of India.\u00b3 It consists of 7,130 (legal judgement, summary) pairs. The gold standard / ref- erence summaries present in this dataset are the \"headnotes\" written by legal experts recruited by the Legal Information Institute Of India.4 As an example, one of the doc- uments used in this dataset can be seen at https://indiankanoon.org/doc/1801104/. The page consists of some meta-data (e.g., names of the petitioner, the respondent, the judges, etc.), the \u201cHEADNOTE\u201d, and the \u201cJUDGMENT\". The dataset was cre- ated by collecting such pages, and extracting the headnote part and the judgement part. The headnote part serves as the gold standard summary of the judgement.\nThe dataset is split into a training set consisting of 7030 (legal judgement, sum- mary) pairs and the test set consisting of 100 (legal judgement, summary) pairs.\nFollowing [42, 43], the coverage and density of the legal judgements with respect to the gold-standard summaries are 0.35 and 1.67 respectively for this dataset.\nThe UK-Abs dataset, which is also obtained from [6], consists of a collection of 793 legal case documents from the United Kingdom's Supreme Court website. Along with the judgements, the website also provides official press summaries for legal cases, which are considered as the reference (gold standard) summaries. As an example, one of the cases in this dataset is available at https://www.supremecourt.uk/cases/ docs/uksc-2015-0063-judgment.pdf and its press summary is available at https:// www.supremecourt.uk/cases/docs/uksc-2015-0063-press-summary.pdf. The dataset was created by extracting the text from such PDF documents."}, {"title": "4 Summarization Models", "content": "In this study, we explore a variety of summarization models, categorizing them into three main categories - (1) extractive summarization models, (2) general purpose LLMs applied as summarizers, and (3) legal domain-specific abstractive summarization models. This section describes all the models in detail.\n4.1 Extractive summarization models\nWe try 5 different extractive summarization methods. Three of these methods - Cas- eSummarizer [3], BertSum [44], SummaRunner [45] were observed to perform well for legal case judgement summarization in the prior work [46]. Additionally, we apply two recent methods PACSUM [40] and HipoRank [41].\nCaseSummarizer [3] \u2013 This unsupervised method ranks sentences based on a TF- IDF matrix which is created using the corpus of legal judgements. CaseSummarizer adjusts sentence scores based on dates, entities, and closeness to section headings. The implementation of this model has been taken from https://github.com/Law-AI/ summarization/tree/aacl/extractive/CaseSummarizer.\nBertSum [44] - BertSum is a supervised summarization model which is based upon a variant of the BERT model. The sentences which are present in the gold standard summary are considered extremely important by the summarization algorithms. Hence BertSum is trained to perform a binary classification task of labeling sentences of the source document as summary-worthy or not. The implementation of this model has been used from-https://github.com/nlpyang/BertSum."}, {"title": "4.2 General Purpose LLMs", "content": "We try out the following Large language models (LLMs). All the LLMS take as input a 'prompt' and generate text as a 'response'. Specifically for the summariza- tion task, the prompt consists of (i) the text to be summarized, which we refer to as <text to summarize> and (ii) an 'instruction' that tells the model that the input"}, {"title": "4.2.1 Variations of Text-Davinci-003", "content": "We try different prompts with the model, leading to the following variations:\n(i) Davinci-summ: For this variant, the prompt is \u201c<text to summarize> Sum- marize the document in <YY> words\" where YY is a number representing the target length of the output summary in number of words. How the value of YY is decided is explained later in the section.\n(ii) Davinci-tldr: For this variant, the prompt is \"<text to summarize> Tl;Dr\". We first pass the document followed by the \"Tl;Dr\" identifier which is an identifier for summarization.\n(iii) Davinci-explicit: For this variant, we use a more explicit prompt \"Your task is to summarize the following document in at most <YY> words. The document to be summarized is given within <>. Document to summarize - <text to summarize>\".\n(iv) Davinci-hybrid: LLMs like Text-Davinci-003 have constraints on the length of prompt+response, which is 4096 tokens at most. So the idea behind this extractive- abstractive hybrid summarization technique is that an extractive summarization method is first used to filter out some key information present in the main judgement."}, {"title": "4.2.2 Variations of Turbo-Gpt-3.5 (ChatGPT)", "content": "Similar to what we tried for Davinci, we try different variations of the ChatGPT model:\n(i) Chatgpt-summ: For this variant, the prompt is \"<text to summarize> Sum- marize the document in <YY> words\" where YY is a number representing the target length of the output summary in number of words. How the value YY is decided is explained later in the section.\n(ii) Chatgpt-tldr: For this variant, the prompt is \"<text to summarize> Tl;Dr\". We first pass the \"Tl;Dr\" identifier followed by the text to be summarized.\n(iii) Chatgpt-explicit: For this variant, the prompt is \u201cYour task is to summarize the following document in at most <YY> words. The document to be summarized is given within <>. Document to summarize - <text to summarize>\".\n(iv) Chatgpt-hybrid: The original ChatGPT model also has a constraint on the maximum prompt+response length which is 4096 tokens. So the idea behind this extractive-abstractive hybrid summarization technique is to first use an extractive summarization method to filter out the key information present in the main judgement, and then to generate an abstractive summary of the key information using the LLM. As before, we use CaseSummarizer to generate the extractive summary of at most 1500 words. All sentences in the extractive summary are placed in the order in which they appear in the document. Then we create an abstractive (target) summary from this extractive summary using the prompt \"<text to summarize> Summarize the document in <YY> words\" where YY is a number that represents the target length of the output summary in number of words.\n(v) chatgpt-16k-long: Here we use the chatgpt-16k variant which has an input+response length of 16k tokens. This variant can be fed with longer inputs, hence it is natural to try out this variant for summarization of long legal docu- ments. For this variant, we use the prompt \"Summarize the document in <YY> words: <text to summarize>\" (the prompt is the same as in Chatgpt-summ) where YY is a number representing the target length of the output summary in number of words."}, {"title": "4.2.3 Variations of GPT-4-Turbo (gpt4)", "content": "The various GPT4 variations we experiment with are:-"}, {"title": "4.2.4 Variations of Llama2-70b", "content": "Similar to the variations for the other LLMs, we try the following variations of Llama2- 70b :-\n(i) llama-summ: For this variant, the prompt is \"<text to summarize> Summarize the document in <YY> words\" where YY is a number representing the target length of the output summary in number of words. How the value YY is decided is explained later in the section.\n(ii) llama-tldr: For this variant, the prompt is \"<text to summarize> Tl;Dr\". We first pass the \"Tl;Dr\" identifier followed by the text to be summarized.\n(iii) llama-explicit: For this variant, the prompt is \"Your task is to summarize the following document in at most <YY> words. The document to be summarized is given within <>. Document to summarize - <text to summarize>\".\n(iv) llama-hybrid: In this extractive-abstractive hybrid summarization technique we first use an extractive summarization method (CaseSummarizer) to filter out the key information present in the main judgement, and then to generate an abstractive sum- mary of the key information using the LLM. As before, we use CaseSummarizer to generate the extractive summary of at most 1500 words. All sentences in the extrac- tive summary are placed in the order in which they appear in the document. Then we create an abstractive (target) summary from this extractive summary using the prompt \"<text to summarize> Summarize the document in <YY> words\" where YY is a number that represents the target length of the output summary in number of words."}, {"title": "4.3 Legal domain-specific abstractive summarization models", "content": "We tried two different legal domain-specific abstractive summarization models namely Legal-Pegasus (abbreviated as LegPegasus) and Legal-LED (abbreviated as LegLED).\nLegPegasus: Pegasus (formally, google/pegasus-cnn_dailymail) is a general- purpose abstractive summarization model developed by Google. This model was fine-tuned on the 'sec-litigation-releases' dataset - consisting of 2,700 litigation releases and complaints related to civil lawsuits in various courts in the United States of Amer- ica (USA) along with their summaries to develop the LegPegasus model designed specifically for abstractive summarization in the legal domain. LegPegasus can be accessed at https://huggingface.co/nsi319/legal-pegasus and has a maximum input token length of 1024 tokens.\nLegLED: This model is based on the Longformer architecture, a transformer-based neural network architecture designed to process long sequences of text efficiently. The LegLED model has also been fine-tuned on the same 'sec-litigation-releases' dataset consisting of 2700 (legal document, summary) pairs related to civil lawsuits in vari- ous courts in the USA. It is also specifically designed for summarization in the legal domain. The LegLED model can be accessed at https://huggingface.co/nsi319/legal- led-base-16384. The model has a maximum input token length of 16,384."}, {"title": "4.4 Chunking of long legal documents", "content": "As stated earlier, Text-Davinci-003 and Turbo-Gpt-3.5 have a token limit of maxi- mum 4,096 for (prompt+generated text). One token is approximately \\frac{3}{4} words, i.e., 1000 tokens correspond to around 750 words. Also, legal domain-specific abstractive summarization models like LegPegasus have a maximum input token length of 1,024.\nRecall from Section 3 that the average length of UK case judgements in the UK- Abs test dataset is 14,476 words and that of the Indian case judgements in the IN-Abs test dataset is 4,782 words. Thus, these legal case judgements are often much longer than what can be input into the summarization models/LLMs at once, and hence we have to follow a divide-and-conquer approach with the long legal documents. Our strategy involves chunking long legal documents into smaller segments or chunks of at most K words (where K can be 1,024 or 2,048 or higher), and each chunk is passed individually into the summarization models to obtain the output summary. Then the summaries generated for all the chunks (of a given document) are appended together in the order in which the chunks appear in the main document, to form the final output summary for the legal judgement. For legal documents with a length of lesser than 1024 words, the summary is obtained at once by passing the entire document through the summarization models.\nFor the ChatGPT and Davinci models, we experiment with K = 1,024 and 2,048. For the chatgpt-16k model, we divide a legal judgement into longer chunks of length K = 8192 words (10,922 tokens approx). Note that almost all documents in the IN- Abs test set and a large majority of documents in the UK-Abs test set are actually"}, {"title": "5 Performance Metrics", "content": "We compare the quality of summaries generated by the different methods along two aspects - (1) their match with the gold standard or reference summaries, and (2) their consistency with the input documents.\n5.1 Match with gold-standard summaries\nHere we measure the match of an algorithm-generated summary for a document with the gold standard / reference summary of the same document. The following metrics are used to measure the performance of the summarization models.\nROUGE: The term 'ROUGE' stands for \"Recall-Oriented Understudy for Gisting Evaluation\" [12]. ROUGE is a family of metrics used for the automatic evaluation of machine-generated text, particularly in the context of text summarization. Here we measure the Rouge-score between the expert-written summaries and model-generated summaries. Specifically, we calculate Rouge-2 precision, recall and F1 scores, which evaluate the bigram match, and Rouge-L precision, recall, F1 scores which measure the Longest Common Subsequence-based match between the model-generated summaries and the gold standard summaries.\nMETEOR [13] is a metric used for the automatic evaluation of machine transla- tion and summarization output. It was designed to address some limitations of other metrics like BLEU by incorporating explicit word order information and consider- ing synonyms and stemming. This metric measures the unigram overlap between expert-written summaries and model-generated summaries.\nBERTSCORE [14] is a popular metric for evaluating the quality of machine- generated text, especially in the context of natural language processing (NLP) tasks"}, {"title": "5.2 Metrics for consistency of summaries", "content": "Now we discuss three metrics used to assess the consistency of model-generated summaries. These metrics compare a model-generated summary with the original doc- ument and estimate how consistent the summary is with the document. All these metrics give a score in the range [0, 1]; the higher the score, the more consistent is the summary.\nSummaC [15] \u2013 This metric utilizes Natural Language Inferencing (NLI) to determine the logical relationship between sentences. The NLI task involves determining the relationship between two sentences. One of the sentences is considered as a 'hypothesis' and the other sentence is considered as a 'premise'. Typically, a NLI model will give a score representing how likely the hypothesis sentence is to logically follow from the premise sentence.\nGiven a (document, summary) pair, the SummaC metric [15] computes NLI scores for each sentence in the model-generated summary, indicating the likelihood that the sentence logically follows from the sentences in the original document. Lower NLI scores for a particular sentence s in the summary suggest a higher mismatch between this sentence and the document, and hence the potential presence of hallucinated information. The NLI scores of individual sentences in the summary are combined to give a single SummaC score for the given (document, summary) pair. A higher SummaC score indicates greater consistency between the model-generated summary and the original document. We use the standard implementation of SummaC available at https://github.com/tingofurro/summac.\nNumPrec - Numbers play a significant role in legal case judgements, such as dates, statute identifiers, monetary values, etc. The NumPrec metric measures the fraction of numbers present in the model-generated summary that also appear in the source document. We rely on the standard Python library for number identification.\nNEPrec - Named Entities are important for legal case judgements, and changes in entities like a person's name or organization's name can result in information loss and potential misrepresentation. The NEPrec metric measures the fraction of named enti- ties present in the model-generated summary that also exists in the original document. We use the Spacy toolkit to detect named entities in original documents as well as the summaries. It is worth noting that the accuracy of the NEPrec metric depends on the accuracy of the toolkit used to identify named entities."}, {"title": "6 Results", "content": "In this section, we study the performance of various summarization models on the two datasets.\n6.1 Selecting the chunk size for different LLMs\nFirst we compare the performances of Chatgpt, Davinci, and Llama with different chunk sizes, to check which chunk size gives the best performance. We perform these experiments on the IN-Abs dataset.\n shows the ROUGE, METEOR, BERTScore metrics for chatgpt-summ, chatgpt-16k-long, davinci-summ, and llama-summ, using different chunk sizes. The same prompt is used for all the different variations of Chatgpt, Davinci, and Llama, as stated in Section 4.2. The highest value for every metric is shown in blue-bold.\nFor all the three LLMs (chatgpt, davinci and Llama), the best results for most metrics are obtained with chunk-size 1024. This is possibly because the LLMs find it difficult to capture the context when the chunk sizes become too large, thus leading to better summaries with chunk size 1024. However, we preferred not to use even shorter chunks, since there is a possibility of redundancy and loss of continuity/coherence if the document is broken into too many chunks. Hence, we perform all subsequent experiments with chatgpt, davinci, and Llama considering a chunk size of 1024 words. For chatgpt-16k, we will continue to use longer chunks of 8192 words. For GPT- 4 Turbo, we feed the entire document into the model since GPT-4 Turbo has a very long context length of 128K.\n6.2 Summarization results on UK-Abs dataset\nWe first compare the performances of LLM-based summarization using different prompts (that were stated in Section 4.2), and select the best LLM variants for summa- rization of this dataset. Then we compare the best LLM variants with other extractive and abstractive summarization models.\nComparing LLM-based summarization variants:\nMETEOR, and BERTScore metric for the general-purpose LLMs on the UK-Abs"}, {"title": "6.3 Summarization results on IN-Abs dataset", "content": "We now study the performances of various summarization models on the IN-Abs dataset. As we did in the previous section for the UK-Abs dataset, here also we first compare the performances of LLM-based summarization using different prompts (that were stated in Section 4.2), and select the best LLM variants for summarization of this dataset. Then we compare the best LLM variants with other extractive and abstractive summarization models.\nComparing LLM-based summarization variants:\ncompares the perfor- mance of general-domain LLMs on the IN-Abs dataset, for different prompts. The best value for a metric within a particular family of summarization models is shown in blue, and the overall best value for every metric is shown in blue-bold.\nAmongst the chatgpt models, chatgpt-summ achieves the best value for most met- rics, including METEOR and BERTScore, although chatgpt-explicit achieves the best ROUGE-L Recall and F1 scores. Amongst the gpt4 models, gpt4-summ achieves the best value for most metrics, including METEOR and BERTScore, although gpt4- explicit achieves the best ROUGE-L Recall and F1 scores. Among the llama models, different variants get the highest score for different metrics. Among the davinci models, davinci-summ achieves the highest values for most metrics.\nWe observe that the 'tldr' variants have higher Rouge-2 precision and Rouge-L precision across all the model variants. Whereas, the 'summ' and 'explicit' variants have higher Rouge-2 and Rouge-L recall and F1 scores across all the model variants.\nAn interesting observation is that the summaries produced by GPT-4 Turbo are shorter than those generated by Turbo-GPT-3.5 (Chatgpt). As a result, the sum- maries from Chatgpt exhibit higher ROUGE Recall and F1 scores, whereas the GPT-4 summaries achieve higher ROUGE precision scores."}, {"title": "7 Examples of inconsistencies and hallucinations in abstractive summaries", "content": "In the context of generative models such as LLMs and abstractive summarizers, hal- lucination refers to the generation of text that is not based on real or accurate"}, {"title": "8 Exploring ways to reduce inconsistencies in abstractive summaries", "content": "From the previous section, it is evident that hallucinations / inconsistencies are observed in the abstractive summaries of legal judgements. We now explore three different strategies for reducing hallucinations in abstractive summaries, one for abstractive summarizers (that allow fine-tuning) and two methods for LLMs (for which fine-tuning is not possible or prohibitively expensive)."}, {"title": "8.1 Domain-specific fine-tuning of abstractive models", "content": "As stated earlier, the abstractive summarization models LegPegasus and LegLED are originally fine-tuned over legal (case judgement, summary) pairs from courts in the USA. These models allow further fine-tuning. Hence we check if domain-specific fine- tuning (i.e., fine-tuning over data from the target domain) of these models helps in improving consistency of the generated summaries.\nTo make these models more suitable for summarizing UK legal documents, we further fine-tune them on the UK-Abs training dataset containing 693 UK case judge- ments and their corresponding reference summaries. These models fine-tuned on the UK-Abs training dataset are referred to as LegPegasus-UK and LegLED-UK. Sim- ilary, to make these models more suitable for Indian case judgements, LegLED and Pegasus models are further finetuned on the IN-Abs training dataset consisting of 7030 (case judgement, summary) pairs. We refer to these models fine-tuned over Indian data as LegLED-IN and LegPegasus-IN."}, {"title": "8.2 Suitable prompting of LLMs", "content": "Unlike the abstractive summarization models described previously, LLMs are very difficult/expensive to fine-tune. Hence, avoiding hallucinations in LLMs is much more challenging and is, in fact, an open question [48].\nIn this work, we explore a very simple approach to reducing hallucinations in LLMs - we include in the prompt an instruction to avoid hallucinations / inconsistencies. Also, since we observed the presence of incomplete sentences in the generated sum- maries, we included an instruction to output complete sentences only. Specifically, we add the following text in the prompt - \"Output complete sentences and not half sentences. Do not have hallucinations and inconsistencies in your summary.\""}, {"title": "8.3 Semantic similarity based approach for reducing hallucinations", "content": "We now describe a novel approach that uses semantic similarity between entities to reduce hallucinations and inconsistencies in summaries. Given a legal judgement j and its summary s generated by an AI model, the approach consists of the following steps:- (i) Consider all the named entities and numbers present in the legal judge- ment j. Let the set of named entities and numbers present in j be Vj (the set of"}, {"title": "9 Summarization of Other Types of Legal Documents", "content": "Till now, we have applied all summarization models over two datasets of legal case judgements. In this section, we check how well the models perform over other types of legal documents. For this, we use the GOVREPORT dataset [49].\n9.1 GOVREPORT Dataset\nThe GOVREPORT dataset, obtained from [49], comprises of 19,466 detailed reports from the U.S. Government Accountability Office (GAO) and the Congressional Research Service (CRS). These reports, prepared in response to congressional requests,"}, {"title": "9.2 Summarization results on GOVREPORT dataset", "content": "We now study the performances of summarization models on the GOVREPORT dataset. To this end, we report the results of some of the best performing LLMs and summarization models (as observed in the previous sections of this paper). From the"}, {"title": "10 Human evaluation of summaries", "content": "In this final section, we perform a human evaluation of the summaries generated by some of the best-performing models. For this, we consulted three senior Law students from the Rajiv Gandhi School on Intellectual Property Law, a reputed Law school in India. Due to their limited availability, we considered only the five best-performing summarization models for each dataset. Specifically, we considered the best-performing model from every summarization family.\nFor the IN-Abs dataset, we considered chatgpt-summ (the best-performing model amongst Chatgpt-1024 models), davinci-summ (the best-performing model amongst Davinci-1024 models), chatgpt-16k-long (the best-performing model amongst Chatgpt-16k models), CaseSummarizer (the best-performing extractive model), and"}, {"title": "11 Concluding Discussion", "content": "To our knowledge, this is the first work that systematically compares the perfor- mances of three different families of models for the practical and challenging task of legal case judgement summarization \u2013 (1) traditional extractive summarization models, (2) domain-specific abstractive summarization models (Legal-Pegasus and Legal-LED), and (3) general domain LLMs such as ChatGPT, Davinci, Llama2-70b and GPT-4-Turbo. We conduct comprehensive experiments over three datasets from the Indian, UK Supreme Courts and GOVREPORT dataset. Our experiments lead to the following insights.\n\u2022 Abstractive models and LLMs generally outperform extractive models in terms of both quantitative metrics as well as human evaluation. In particular, LLMs like Text-Davinci-003, Turbo-GPT-3.5, Llama-70b and GPT4 perform well even with- out specific legal document training. However, generative models often contain hallucinations and inconsistencies in the generated summaries.\n\u2022 Fine-tuning abstractive models with the target domain data, if available, helps in reducing hallucinations/inconsistencies as well as improves the quality of the summaries (as seen on both UK-Abs and IN-Abs datasets).\n\u2022 Suitable prompting of LLMs can help in reducing hallucinations/inconsistencies in the generated summaries. But there may be an associated reduction of the summary quality. An important future direction of research would be to reduce inconsistencies in LLM-generated summaries while maintaining the summary quality.\n\u2022 We discussed a semantic similarity-based approach to reduce hallucinations in the summaries generated by the LLMs. However, complex errors, such as confusion between names or numbers are difficult to detect or prevent completely.\n\u2022 The extreme length of legal case judgements is a domain-specific challenge. There is a trade-off associated with the approach of breaking these long documents into chunks. As seen from the human evaluation, and as already demonstrated in [36], chunking leads to better information quality in the summaries, but also leads to redundancy and lack of coherence.\nBased on these results, we conclude that, for complex domains like law, LLMs and pre-trained abstractive summarization models are not ready yet for fully automatic deployment. A human-in-the-loop approach, where a legal expert monitors the gener- ated summaries, may be more appropriate. Furthermore, better methods are needed to detect complex errors in abstractive summaries. We plan to explore these directions in the future.\nThe present work has certain limitations. We have used a basic segmentation technique to manage lengthy documents, which has certain limitations. First, the token-level segmentation strategy can cause the last sentence of a chunk to end abruptly, potentially affecting readability. Second, within a single chunk, sentences"}]}