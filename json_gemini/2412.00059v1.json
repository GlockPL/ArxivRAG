{"title": "Adaptive Coordinate-Wise Step Sizes for Quasi-Newton Methods: A Learning-to-Optimize Approach", "authors": ["Wei Lin", "Qingyu Song", "Hong Xu"], "abstract": "Tuning effective step sizes is crucial for the stability and efficiency of optimization algorithms. While adaptive coordinate-wise step sizes tuning methods have been explored in first-order methods, second-order methods still lack efficient techniques. Current approaches, including hypergradient descent and cutting plane methods, offer limited improvements or encounter difficulties in second-order contexts. To address these challenges, we introduce a novel Learning-to-Optimize (L2O) model within the Broyden-Fletcher-Goldfarb-Shanno (BFGS) framework, which leverages neural networks to predict optimal coordinate-wise step sizes. Our model integrates a theoretical foundation that establishes conditions for the stability and convergence of these step sizes. Extensive experiments demonstrate that our approach achieves substantial improvements over traditional backtracking line search and hypergradient descent-based methods, offering up to 7\u00d7 faster and stable performance across diverse optimization tasks.", "sections": [{"title": "1. Introduction", "content": "Step size is an essential hyperparameter in optimization algorithms. It determines the rate at which the optimization variables are updated, and greatly influences the convergence speed and stability of the optimization process. In first-order gradient-based optimization, how to choose an appropriate step size is well studied: The step size is typically adjusted adaptively using past gradient information such as in AdaGrad [15], RMSProp [16], and Adam [17] for large-scale deep learning. Despite differences in details, these adaptive methods assign a single scalar step size to all optimization variables (e.g. model parameters in ML) which works well in practice.\nStep size in second-order methods received much less attention thus far. Second-order methods leverage the curvature information to adjust both the search direction and step size, offering faster convergence in number of iterations for high-dimensional optimization landscapes, at the cost of high computational complexity in calculating the Hessian (or its approximation). A natural and common approach for step size selection here is line search, which iteratively adjusts the step size along the descent direction until certain conditions, like the Armijo condition, are met [3].\nIn contrast, we study the more general coordinate-wise step sizes in this work, which allow for individual variables to have different step sizes. Coordinate-wise step sizes are beneficial since different optimization variables may have different sensitivities to the step size; scalar step size is obviously a special case of coordinate-wise step sizes. They have also been shown to improve convergence in first-order methods [1, 15, 18].\nIn this work, we explore the impact of coordinate-wise step sizes in the context of second-order methods, which remains largely unexplored to our knowledge. We choose the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method [8], one of the most widely used second-order optimization methods, as the backbone method. BFGS belongs to the quasi-Newton family of methods that iteratively update an approximation of the Hessian matrix using gradient information to reduce the complexity.\nWe start our study by demonstrating that existing solutions to tune coordinate-wise step sizes in first-order methods do not work well in second-order contexts. The first such approach is hypergradient descent [21, 22], which iteratively tunes step sizes using their gradients at each BFGS step. We show empirically that it provides only marginal gains after the initial few steps of BFGS. Moreover, cutting-plane techniques, which expand backtracking line search into multiple dimensions, iteratively refine step sizes within feasible sets narrowed down by hypergradient-based incisions [18]. This method essentially offers an approximation of the Hessian in a first-order framework, thus complicating its direct application to second-order methods, in which Hessian approximation is handled by BFGS update, and the step sizes are adjusted to improve the Hessian approximation. Further, the intricate curvature within the Hessian presents additional challenges in plane cutting.\nTherefore, we explore the Learning to Optimize (L2O) paradigm [2] in this work. L2O replaces handcrafted rules with data-driven models that can adaptively learn efficient strategies, tailoring optimization processes to specific problem structures [2, 20]. L2O has shown promising results in first-order optimization by leveraging neural networks to predict optimal step sizes dynamically based on the current optimization state [19, 26].\nThe application of L2O in second-order methods presents challenges. Whereas in first-order approaches, the step size primarily regulates the update magnitude, in second-order methods, it also affects the precision of Hessian approximations, particularly in the initial optimization stages. This dual role adds complexities in tuning the step size. Furthermore, the unconstrained exploration inherent in conventional L2O makes convergence and stability harder to achieve within second-order L2O frameworks.\nTo address these challenges, we provide a theoretical analysis of coordinate-wise step sizes within the BFGS framework. We establish specific convergence criteria and derive conditions that effective step sizes must satisfy. This analysis identifies key elements of the optimization state that influence step size selection and provides bounded conditions on step sizes that ensure convergence. Additionally, we derive that the coordinate-wise step sizes should asymptotically converge to an identity matrix to achieve superlinear convergence rates near the optimum.\nBuilding on this theoretical foundation, we propose a customized L2O model for second-order optimization. Our model takes as input variables, gradients, and second-order search directions. To meet the derived convergence conditions, we bound the output of the L2O model. Unlike typical first-order L2O methods that use fixed unrolling lengths, we train our model with a loss function defined by the expected objective value in the subsequent iteration plus a regularization term, ensuring efficient adaptation of step sizes.\nWe summarize our key contributions as follows:\n1. We are the first to investigate coordinate-wise step size adaptation in the context of second-order optimization methods, specifically the BFGS algorithm.\n2. We derive conditions that effective coordinate-wise step sizes should satisfy, ensuring convergence and stability based on desired optimization properties.\n3. We propose a new L2O method to generate coordinate-wise step sizes for the BFGS algorithm, integrating both theoretical principles and adaptive learning to guide the optimization process.\n4. Through extensive experiments, we demonstrate the superiority of our approach over traditional backtracking line search and hypergradient descent-based methods, achieving up to 7times faster convergence across a range of test cases while providing improved stability compared to competitors."}, {"title": "2. Preliminaries", "content": "In this chapter, we introduce the basics of second-order optimization methods, with a focus on BFGS. We show how step size selection critically affects both the convergence and the quality of Hessian approximations. Then we establish the key assumptions that will support our analysis of coordinate-wise step sizes in BFGS optimization."}, {"title": "2.1. Second-Order Methods", "content": "Second-order optimization methods, such as Newton's method [4], utilize both gradient and curvature information to find the minimum of an objective function. While first-order methods typically achieve a sub-linear convergence rate [6], second-order methods generally exhibit a faster, quadratic convergence rate [28]. This significant difference makes second-order methods particularly advantageous in high-dimensional optimization problems, allowing for more efficient progress towards the optimum [5].\nIn Newton's method, the objective function is locally approximated by a quadratic function around the current parameter vector xk:\n$g(y) \\approx f(x_k)+\\nabla f(x_k)^T (y-x_k)+\\frac{\\alpha_k}{2}(y-x_k)^T H_k(y-x_k)$,\nwhere $H_k$ is the Hessian matrix and $\u03b1_k$ is the damped parameter. By minimizing the quadratic approximation, the update rule for Newton's method becomes [28]:\n$x_{k+1} = x_k - \u03b1_k H_k^{-1} \\nabla f(x_k)$.\nComputing the Hessian is quite expensive and often infeasible for large-scale problems [23]. Instead, quasi-Newton method was proposed to approximate the Hessian to be more affordable and scalable [9, 14]. Generally, quasi-Newton methods maintain an approximation of the Hessian matrix $B_k \\approx H_k$ at each iteration, updating it with a rank one or rank two term based on the gradient differences between two consecutive iterations [8, 11]. During this process, the Hessian approximation is restricted to follow the secant equation [28]:\n$B_{k+1}s_k = y_k$,\nwhere $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$. In the most common BFGS method, the Hessian approximation $B_k$ is updated at each iteration using the formula:\n$B_{k+1} = B_k + \\frac{y_ky_k^T}{y_k^Ts_k} - \\frac{B_ks_ks_k^TB_k^T}{s_k^TB_ks_k}$,\nand the update rule becomes:\n$x_{k+1} = x_k - \\alpha_k B_k^{-1} \\nabla f(x_k)$."}, {"title": "2.2. Assumptions", "content": "Our objective is to minimize the convex objective function $f(x)$ over $x \\in \\mathbb{R}^n$:\n$\\min_{x \\in \\mathbb{R}^n} f(x)$.\nFor our analysis, we make the following assumptions on the objective function and the Hessian approximations, which are widely regarded as reasonable and commonly used in optimization research [19, 26, 28]:\nAssumption 1. The objective function $f$ is $L$-smooth, meaning there exists a constant $L$ such that:\n$\\|\\nabla^2 f(x) - \\nabla^2 f(y)\\| \\le L\\|x - y\\|$.\nAssumption 2. The gradient $\\nabla f(x)$ is differentiable in an open, convex set $D$ in $\\mathbb{R}^n$, and $\\nabla^2 f(x)$ is continuous at $x^*$ with $\\nabla^2 f(x^*)$ being nonsingular.\nAssumption 3. The Hessian approximation generated by BFGS method is positive definite and has a uniformly bounded condition number. Specifically, there exists a constant $M$ such that:\n$\\frac{1}{M} \\le \\frac{\\lambda_{\\min}(B_k)}{\\lambda_{\\max}(B_k)}$,\nwhere $\\lambda_{\\min}(B_k)$ and $\\lambda_{\\max}(B_k)$ are the smallest and largest eigenvalues of $B_k$, respectively. By this assumption we assume the Hessian approximation is not too ill-conditioned.\nAssumption 4. The norm of update direction $B_k^{-1}\\nabla f(x_k)$ is upper bounded by a constant $R$:\n$\\|B_k^{-1}\\nabla f(x_k)\\| \\le R$.\nThis assumption is reasonable, as quasi-Newton method maintain boundedness of $B_k^{-1}$ through stable Hessian approximations [9], and gradients $\\nabla f(x_k)$ typically diminish near optimal points, ensuring the update direction remains controlled."}, {"title": "3. Coordinate-Wise Step Sizes for BFGS", "content": "In this section, we first analyze the theoretical advantages of coordinate-wise step sizes and then explore hypergradient descent as a practical method for coordinate-wise step sizes tuning. However, the limited improvements achieved through hypergradient descent reveal the challenges of finding effective coordinate-wise step sizes, prompting us to consider alternative approaches. We resort to L2O method that can directly learn the step sizes from data derived from similar optimization problems. Building on this perspective, we establish sufficient conditions for effective coordinate-wise step sizes that ensure convergence and descent properties, thus laying a solid foundation for learning-based approaches that can predict optimal step sizes efficiently during optimization."}, {"title": "3.1. Gain of Coordinate-Wise Step Sizes", "content": "To illustrate the potential benefits of coordinate-wise step sizes in the BFGS method, let us consider the theoretical implications of relaxing the constraint that step size should be a scalar. Assume we have identified an optimal scalar step size, denoted by $\u03b1^*$, for the k-th iteration. Since the restriction of a convex function to a line remains convex [7], this optimal step size $\u03b1^*$ satisfies:\n$\\frac{d}{d\u03b1_k} f(x_{k+1})\\|_{\u03b1_k=\u03b1^*} = 0$.\n$\\frac{d}{d\u03b1_k} f(x_k - \u03b1 B_k^{-1} \\nabla f(x_k))\\|_{\u03b1_k=\u03b1^*} = 0$.\n$\\nabla f(x_k - \u03b1^* B_k^{-1} \\nabla f(x_k))^T B_k^{-1} \\nabla f(x_k) = 0$.\nWhen constrained to a scalar form, $\u03b1^*$ guarantees optimality along the single search direction $B_k^{-1}\\nabla f(x_k)$. However, if we allow the step size to be a diagonal matrix $P_k$ rather than a scalar, the optimality condition of $\u03b1^*$ may no longer hold. By extending to a coordinate-wise approach, we aim to further minimize the objective function by adjusting each coordinate independently, which can potentially achieve a lower function value than with $\u03b1^*$ alone.\nTo explore this, let $P_k = \u03b1^*I$, and consider the partial derivative of $f(x_{k+1})$ with respect to $P_k$ at this point:\n$\\frac{\\partial}{\\partial P_k} f(x_k - P_k B_k^{-1} \\nabla f(x_k))\\|_{P_k=\u03b1^*I} = diag(-\\nabla f(x_k - \u03b1^* B_k^{-1} \\nabla f(x_k)) \\odot B_k^{-1} \\nabla f(x_k))$,\nwhere $ \\odot$ denotes the Hadamard (element-wise) product. Since $B_k^{-1}\\nabla f(x_k) \\ne 0$, the derivative in equation 3 equals zero only if $\\nabla f(x_k - \u03b1^* B_k^{-1} \\nabla f(x_k)) = 0$. Since the optimum does not generally lie on the direction of $B_k^{-1}\\nabla f(x_k)$, the dot product being zero in equation 2 does not imply"}, {"title": "3.2. Numerical Analysis of Coordinate-Wise Step Size: A Hypergradient Descent Method", "content": "Coordinate-wise step sizes operate in a higher-dimensional search space compared to traditional line search methods, which are confined to a single dimension. Given $B_k^{-1}\\nabla f(x_k)$, any vector in the space can be achieved as $P_k B_k^{-1} \\nabla f(x_k)$ through an appropriate choice of $P_k$. There exists an optimal P such that $P_k B_k^{-1} \\nabla f(x_k) = x^*$ , allowing the optimization to reach the solution in a single iteration. However, finding this optimal $P$ is as hard as finding $x^*$ itself. Therefore, we seek a computationally efficient approach to find the coordinate-wise step sizes $P_k$ that can enhance convergence.\nBuilding upon section 3.1, we investigate hypergradient descent on coordinate-wise step size matrix $P_k$. The BFGS update rule with coordinate-wise step size takes the form:\n$x_{k+1} = x_k - P_k B_k^{-1} \\nabla f(x_k)$.\nWe initializes $P_0$ as the identity matrix $I$ before each BFGS update [10]. We then perform hypergradient descent on $P_k$ using the gradient of $f(x_{k+1})$ with respect to $P$ to obtain $P_{i+1}$. After $T$ iterations, we employ $P_T$ in the BFGS update rule 4. The hypergradient descent on $P_k$ can be formulated as:\n$P_{i+1} = P_k - \u03b7 \\frac{\\partial f(x_k - P_k B_k^{-1} \\nabla f(x_k))}{\\partial P_k}$,\nwhere \u03b7 is the learning rate for the gradient descent on $P_k$.\nWe conduct experiments on the least squares problem to assess the effectiveness of hypergradient descent applied to $P_k$. Each BFGS iteration includes 20 steps of hypergradient descent, after which the most recent $P_k$ identified by hypergradient descent is used in BFGS udate."}, {"title": "3.3. Conditions for Good Coordinate-Wise Step Sizes", "content": "Effective coordinate-wise step size must satisfy certain theoretical requirements to ensure that each iterative update is convergent. In this section, we establish the foundational conditions required for these coordinate-wise step sizes to achieve convergence properties. These conditions provide a systematic framework for the development of any adaptive coordinate-wise step size tuning methods that maintain the stability and efficiency of BFGS method.\nFor good coordinate-wise step sizes, we propose the following requirements:\n1. The generated sequence $x_k$ converges to one of the local minimizers of $f$.\n2. Each update moves towards the minimizer.\n3. The method achieves superlinear convergence.\nThe first requirement extends the concept of Fixed Point Encoding in optimization, serving as a valuable guideline for constructing effective optimization algorithms [25]. The second requirement emphasizes directional accuracy, ensuring each update consistently progresses toward the minimizer. Finally, the third requirement preserves the superlinear convergence rate characteristic of the BFGS method [28].\nWe now present three theorems that provide sufficient conditions for coordinate-wise step sizes to satisfy the proposed requirements.\nTheorem 1. Let {xk} be the sequence generated by equation 4. If the coordinate-wise step size Pk satisfies\n$\\frac{\u03b1}{\\|B_k^{-1}\\|} \\le \\|P_k\\| \\le \\frac{\u03b1}{\\gamma}$,\n$\\frac{\\nabla f(x_k)^T B_k^{-1} \\nabla f(x_k)}{\\|B_k^{-1} \\nabla f(x_k)\\|^2} \\ge \u03b2$,\nfor certain $0 < \u03b1 < 2$ and $\u03b2 > 0$, where L is the Lipschitz constant of gradients and Bk is the approximate Hessian generated by BFGS, then the sequence of gradients converges to zero: $\\lim_{k\u2192\u221e} \\|\\nabla f(x_k)\\| \u2192 0$.\nRemark. Theorem 1 establishes robust sufficient conditions for gradient convergence while maintaining substantial implementation flexibility. The theorem's bounds on Pk are particularly accommodating: the lower bound can be made arbitrarily close to zero through appropriate selection of \u00df, while setting a near 2 allows the upper bound to approach 2/(L|B\u207b\u00b9|), which remains strictly less than 2 if |B\u207b\u00b9| > L. This is true since Bk is the average of Hessians from $X_{k-1}$ to xk, as evidenced by equation 1.\nTheorem 1 suggests a pragmatic simplification: constraining the elements of the coordinate-wise step size to the interval [0,2] should be sufficient for practical implementations. Moreover, theorem 1 indicates that Pe should be computed as a function of both the gradient \\nabla f (xk) and Hessian approximation Bk, as evidenced by the presence of both gradient and Hessian information in bounds. Notably, these results extend beyond convex optimization, requiring only L-smoothness of the objective function rather than convexity.\nTheorem 2. Let $f : \\mathbb{R}^n \u2192 \\mathbb{R}$ be a twice continuously differentiable convex function that has Lipschitz continuous gradient with $L > 0$. Let x* denote the unique minimizer of $f$. Suppose that {Bk} is a sequence of approximate Hessians such that they are uniformly lower bounded:\n$\\gamma I \\le B_k$,\nfor certain constant $\u03b3 > 0$. Let {Pk} be a sequence of diagonal matrices with entries pk,i satisfying:\n$0 < p_{k,i} \\le \\frac{2 \u03b3}{L}$.\nDefine the iterative sequence {xk} by equation 4. Then, the sequence {xk} satisfies:\n$\\|x_{k+1} - x^*\\| \\le \\|x_k - x^*\\|$."}, {"title": "4. L2O Model", "content": "Building on the theoretical foundations established in the previous section, we now present our L2O model for coordinate-wise step size selection in BFGS optimization. Our method leverages neural networks to efficiently predict step sizes that satisfy the key requirements while adapting to the local geometry of the optimization landscape. This section details the architecture, training procedure, and design choices that enable effective step size prediction across diverse optimization problems.\nWe propose an L2O method using an LSTM (Long Short-Term Memory) network to predict coordinate-wise step sizes [19]. The architecture is structured as follows:\n$h_{k+1}, o_{k+1} = LSTM(x_k, \\nabla f(x_k), B_k^{-1} \\nabla f(x_k), h_k, \u03a6_{LSTM})$,\n$p_k = MLP(o_k, \u03a6_{MLP})$,\n$P_k = diag(2\u03c3(p_k))$,\nwhere, $h_k$ is the LSTM hidden state, initialized randomly for the first iteration, and $o_k$ is the embedding output from the LSTM network. The parameters of the LSTM and MLP (Multi-Layer Perceptron) networks are denoted by $\u03a6_{LSTM}$ and $\u03a6_{MLP}$, respectively. According to section 3.3, the values of $P_{k,i}$ should lie within the interval [0, 2]. Therefore, the activation function $\u03c3$, which represents the sigmoid function, is employed to ensure that the predicted step sizes are within a suitable range, enabling bounded and adaptive adjustments for each coordinate.\nTo enhance scalability and parameter efficiency, we employ a coordinate-wise LSTM approach, where the same network is shared across all input coordinates, as suggested in [2, 20]. This design allows the L2O method to adapt to problems of varying dimensionality without an increase in the number of parameters, making it highly efficient for large-scale optimization tasks.\nThe L2O model is trained on datasets of diverse optimization problems, allowing it to learn common structures and behaviors across different problem instances. The training process comprises two nested optimization loops that interact as follows:\nInner Optimization Loop In the inner loop, we utilize the current L2O model to optimize the given objective function. For each training instance, which is an optimization problem, we perform totally K iterations. At each iteration k, the L2O model predicts the step size Pk, which is then used to update the current point. This process allows the L2O model to adaptively adjust the step size for each coordinate based on the current optimization state, effectively utilizing the curvature information encoded in B\u00b9.\nOuter Optimization Loop Immediately after each iteration of the inner loop, we update the parameters of the neural networks (\u03a6LSTM and \u03a6MLP) using backpropagation. The update is based on the objective function value achieved at $X_{k+1}$. Unlike other L2O approaches in first-order context, which often employ a fixed unroll length and update the model after multiple iterations [2, 20, 26], our method updates the model parameters more frequently. This frequent"}, {"title": "5. Experiments", "content": "Our experiments are conducted using Python 3.9 and PyTorch 1.12 on an Ubuntu 18.04 system, utilizing an Intel Xeon Gold 5320 CPU and two NVIDIA RTX 3090 GPUs. We employ the Adam optimizer as our meta-optimizer to train our L2O model on a dataset comprising 32,000 optimization problems with randomly sampled parameters. For evaluation, we generate a separate test dataset of 1,024 optimization problems. Our L2O method is benchmarked against two baselines: BFGS with backtracking line search, and BFGS with hypergradient descent. We implement BFGS with backtracking line search by initially setting the step size to 1 and iteratively scaling it by a factor until the Armijo condition is met, ensuring adequate reduction in the objective function. For BFGS with hypergradient descent, we refine the coorindate-wise step sizes by conducting 20 iterations of hypergradient descent within each BFGS iteration. We experiment with various parameter settings for all methods and select the best-performing configurations.\nOptimization Objectives We evaluate our L2O method on three distinct optimization problems: least square, logistic regression, and log-sum-exp function.\nFor least square, we use the objective function:\n$\\min_x f(x) = \\frac{1}{2} \\|Ax - b\\|_2^2$,\nwhere $A \\in \\mathbb{R}^{250\u00d7500}$ and $b \\in \\mathbb{R}^{500}$ are randomly generated using a Gaussian distribution. Following the setting in [19], we set the sparsity of A to 0.1 by making 90% of its element to zero.\nThe objective function for logistic regression is:\n$\\min_x f(x) = \\frac{1}{m} \\sum_{i=1}^{m} [b_i \\log(h(a_i^T x)) + (1 - b_i) \\log(1 - h(a_i^T x))] + \u03c1\\|x\\|_2^2$,\nwhere m = 500, ${(a_i, b_i) \\in \\mathbb{R} \\times {0, 1}}$ are randomly generated, $h(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\nFor the log-sum-exp function, we use:\n$\\min_x f(x) = \\frac{1}{m} \\sum_{i=1}^{m} (\\log (\\sum_{j=1}^{d} e^{a_i^T x - b_i})) + \u03c1\\|x\\|_2^2$,"}]}