{"title": "Model Tells Itself Where to Attend:\nFaithfulness Meets Automatic Attention Steering", "authors": ["Qingru Zhang", "Xiaodong Yu", "Chandan Singh", "Xiaodong Liu", "Liyuan Liu", "Jianfeng Gao", "Tuo Zhao", "Dan Roth", "Hao Cheng"], "abstract": "Large language models (LLMs) have demon-\nstrated remarkable performance across various\nreal-world tasks. However, they often strug-\ngle to fully comprehend and effectively utilize\ntheir input contexts, resulting in responses that\nare unfaithful or hallucinated. This difficulty\nincreases for contexts that are long or contain\ndistracting information, which can divert LLMS\nfrom fully capturing essential evidence. To ad-\ndress this issue, many works use prompting\nto help LLMs utilize contextual information\nmore faithfully. For instance, iterative prompt-\ning highlights key information in two steps\nthat first ask the LLM to identify important\npieces of context and then derive answers ac-\ncordingly. However, prompting methods are\nconstrained to highlighting key information im-\nplicitly in token space, which is often insuffi-\ncient to fully steer the model's attention. \u03a4\u03bf\nimprove model faithfulness more reliably, we\npropose AutoPASTA, a method that automati-\ncally identifies key contextual information and\nexplicitly highlights it by steering an LLM's\nattention scores. Like prompting, AutoPASTA\nis applied at inference time and does not re-\nquire changing any model parameters. Our\nexperiments on open-book QA demonstrate\nthat AutoPASTA effectively enables models to\ngrasp essential contextual information, leading\nto substantially improved model faithfulness\nand performance, e.g., an average improvement\nof 7.95% for LLAMA3-70B-Instruct. Code\nwill be publicly available at https://github.\ncom/QingruZhang/AutoPASTA.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) exhibit remarkable\nperformance across various natural language pro-\ncessing (NLP) tasks and artificial intelligence (AI)\napplications (Brown et al., 2020; Touvron et al.,\n2023; OpenAI, 2023). Despite their remarkable\ncapabilities, recent studies reveal that LLMs often\nencounter challenges in fully understanding their\ninput contexts, overlooking or showing insensitiv-\nity to crucial contextual information (Kasai et al.,\n2023; Li et al., 2023; Si et al., 2023; Zhou et al.,\n2023; Yu et al., 2024; Zhang et al., 2024). Conse-\nquently, the models tend to fabricate answers (also\nknown as hallucination), resulting in unfaithful re-\nsponses that are inconsistent with the presented\ncontexts (Zhou et al., 2023; Yu et al., 2024). This\nbecomes particularly problematic when models are\npresented prompts containing lengthy background\ncontexts (Liu et al., 2023) or complex questions,\nsuch as in open-book question answering (QA)\n(Kwiatkowski et al., 2019; Shi et al., 2023b; Peng\net al., 2023). In these information-dense scenarios,\nlengthy contexts can overwhelm LLMs, which con-\ntain many details with varying degree of relevance\n(Wan et al., 2024; Zhang et al., 2024). Some sen-\ntences are crucial for providing the correct answer,\nwhile others, though irrelevant, can distract models\nfrom fully capturing the essential information.\nTo improve model faithfulness, most prior work\nexplores well-designed prompts to guide the LLM\nto use contextual knowledge more reliably (Zhou\net al., 2023; Wan et al., 2024; Radhakrishnan et al.,\n2023). In particular, iterative prompting in chain-\nof-thought (COT; Wei et al., 2022) fashion can help\nLLMs decompose complex task-solving into more\ninterpretable and manageable intermediate steps,\nthus yielding better performance (Radhakrishnan\net al., 2023). Motivated by this, it is natural to de-\nsign multi-step iterative prompting to guide LLMs\nto pay more attention to relevant contextual parts\nand derive answers accordingly. Specifically, for\nopen-book QA tasks, iterative prompting can be\ndecomposed into two steps: (i) identifying key in-\nformation and (ii) deriving answers using the key\ninformation.\nIterative prompting can work effectively for\nblack-box LLMs of significantly large sizes"}, {"title": "2 Background", "content": "Problem description. In standard LLM prompt-\ning, we are given a pre-trained LLM and a text\nprompt \u00e6 consisting of n tokens. In the closed-\nbook setting, the prompt \u00e6 can only be a question\nor instruction to be completed by models. Relying\nsolely on model parametric knowledge poses chal-\nlenges in scenarios involving complex questions\nthat entail new knowledge or private information\n(Zhou et al., 2023; Yu et al., 2024). Existing meth-\nods (Shi et al., 2023b; Peng et al., 2023) resort\nto augmenting the prompt with additional back-\nground contexts to facilitate question answering,\ni.e., open-book question answering. The follow-\ning box presents a prompt template that we use for\nopen-book QA:\nMulti-head attention. A typical transformer\nmodel consists of L stacked layers, where each\nlayer contains two submodules: a multi-head\nattention (MHA) and a fully connected feed-\nforward network (FFN). Given the input X \u2208\n\u211d^(n\u00d7d), MHA of the layer l performs the atten-\ntion function in parallel H heads: MHA^(l)(X) =\nConcat(H^(l,1), ..., H^(l,H))W\u2080 with\nH^(l,h) = Softmax(A^(l,h))V^(l,h)\nwhere A = $\\frac{1}{\\sqrt{d_h}}$Q K^T \u2208 \u211d^(n\u00d7n) is the scaled\ninner product between query Q and key K.\nQ = XW^(qh),K = XW^(kh),V = XW^(vh) and\nW^(qh), W^(kh), W^(vh) \u2208 \u211d^(d\u00d7dh) are learnable projec-\ntion matrices of head h. dh is typically set to d/H.\nPost-hoc attention steering. Zhang et al. (2024)\npropose PASTA, an inference-only method that ap-\nplies attention reweighting to steer model attention\ntowards user-highlighted input sets, thereby im-\nproving instruction following and contextual com-\nprehension. Specifically, given the index set of", "equations": ["H^(l,h) = Softmax(A^(l,h))V^(l,h)", "A = \\frac{1}{\\sqrt{d_h}}Q K^T \u2208 \u211d^(n\u00d7n)"]}, {"title": "3 Method", "content": "Our proposed method \u2013 Automatic Post-hoc Atten-\ntion Steering Approach (AutoPASTA), integrates\niterative prompting and attention steering. This in-\ntegregation synergistically combines the advantages\nof both techniques while mitigating their respective\nlimitations. For multi-step iterative prompting, in-\ncorporating attention steering externalizes the high-\nlighting of key information through an inference-\nonly operation, efficiently enhancing model faith-\nfulness with improved reliability and controllability.\nFor post-hoc attention steering, equipping it with\niterative prompting enables the automatic identi-\nfication of contextually relevant key information,\nthereby addressing its significant reliance on hu-\nman annotations.", "equations": []}, {"title": "3.1 Automatic Contextual Highlighting", "content": "In the open-book QA task, an LLM M is prompted\nto answer a question q paired with a background\ncontext c that consists of m sentences c =\ns\u2081||...||sm. Instead of directly prompting an\nLLM with (q, c), AutoPASTA first prompts the\nLLM to generate a key sentence from the context c\nthat supports answering the question:\ng\u2081 = Generate\u043c(Pi(q, c)),\nwhere Pi is the prompt template of key sentence\nidentification that we show in Section 4.1. Then,\nAutoPASTA maps g\u2081 back to a sentence from the\noriginal context c to avoid potential token-level\ngeneration errors in g\u2081 and mitigate error propa-\ngation. Specifically, it employs a small encoder e\nto calculates the semantic embeddings of g\u2081 and\nevery si(1 \u2264 i \u2264 m), and pick the best-matching\nsentence sk with the highest similarity to g\u2081:\nsk = Matche (g\u2081, {s\u2081,..., sm}) \u2282 c.\nIn the final step, AutoPASTA steers the attention\nscores of tokens in sk based on (1) at the specific\nattention heads H, when directly prompting the\nLLM M to derive the answer for (q, c):\ng\u2082 = SteerH,sk(Generate\u043c(Pd(q,c)))\nwhere Pd is the prompt template of direct answer-\ning as shown in Section 2, and SteerH,sk (\u00b7) is de-\ntailed by (1) with G as the index set of sk. As\nsuch, the identified key sentence sk is explicitly\nhighlighted through attention score upweighting,\ndirecting the model to grasp the key information\nand generate more faithful answers. Notably, Au-\ntoPASTA is applied at inference time and does not\nrequire changing any model parameters. More im-\nportantly, it does not involve human annotation on\nhighlighted parts. The key information is automat-\nically identified by iterative prompting the model\nM, addressing the major limitation of existing at-\ntention steering approach.", "equations": ["g\u2081 = Generate\u043c(Pi(q, c)),", "sk = Matche (g\u2081, {s\u2081,..., sm}) \u2282 c.", "g\u2082 = SteerH,sk(Generate\u043c(Pd(q,c)))"]}, {"title": "3.2 Coarse-to-fine Model Profiling", "content": "AutoPASTA requires carefully selecting H, the set\nof attention heads to be steered in (1), but find-\ning these heads can be computationally intensive.\nZhang et al. (2024) propose a greedy search strat-\negy that evaluates the steering performance of each\nhead on small validation sets of multiple tasks and\nselects the heads that yield the best performance.\nThis greedy strategy requires evaluating L\u00d7H\ntimes, resulting in non-trivial overheads especially\nfor large models. To improve the efficiency of\nsearching heads, we propose an alternative coarse-\nto-fine model profiling scheme that searches from\nthe layer level to head level. Specifically, we first\nevaluate the performance of steering all attention\nheads of one single layer, then pick the top-l lay-\ners, and further evaluate the steering performance\nof each head in these layers. The head set His\nobtained by selecting the best-performing heads\nfrom top-l layers. Empirically, we find that a small\nl (e.g., l = 6 compared to L = 32) is sufficient for\nAutoPASTA to achieves superior performance and\nidentify effective attention heads that can general-\nize across tasks, substantially reducing the search-\ning overheads to.", "equations": []}, {"title": "4 Experiments", "content": "We conduct experiments to evaluate the effective-\nness of AutoPASTA using Vicuna-7B (Chiang\net al., 2023), LLAMA3-8B-Instruct, and LLAMA3-\n70B-Instruct (Meta, 2024) on both single- and\nmulti-hop open-book QA tasks from Natural Ques-\ntions (NA; Kwiatkowski et al., 2019) and Hot-\npotQA (Yang et al., 2018b)."}, {"title": "4.1 Experimental Setup", "content": "Datasets. We study 2 datasets: HotpotQA (Yang\net al., 2018a) and the MRQA version (Fisch et al.,\n2019) of Natural Questions (NQ) (Kwiatkowski\net al., 2019). Following the filtering procedures\noutlined by Yu et al. (2024), duplicated and low-\nquality questions are removed from the NQ dataset,\nresulting in 7,189 instances remaining in NQ, and\n5,190 instances in HotpotQA. For each dataset, we\nrandomly select 1,000 examples as the profiling set\nand keep the remaining examples as the test set (see\nbreakdown in Table 6). For all the experiments, we\npresent two evaluation metrics: Exact Match (EM),\nand Token-level F1 score. We apply greedy search\ndecoding for all experiments."}, {"title": "Implementation Details", "content": "We implement our ex-\nperiments using Huggingface Transformers (Wolf\net al., 2019) and PyTorch (Paszke et al., 2019).All\nthe experiments are conducted on NVIDIA A6000\nand A100 GPUs.\nAutoPASTA Settings. For AutoPASTA, we use\nthe following prompt template Pi to prompt an\nLLM M to identify the key information from the\ncontext that support answering the question.\nThen, we map the predicted key sentence 9\u2081\nback to the original context by (3), which uses a\nsmall encoder models to calculate the semantic\nembeddings of the predicted key sentence g\u2081 and\nevery sentence si in the context c. Specifically, we\nuse a \"all-MiniLM-L6-v2\" model from Sentence-\nTransformer (Reimers and Gurevych, 2019) as the\nencoder to encode sentences. Then, we calculate\nthe cosine similarity between semantic embeddings\nof g\u2081 and each sentence si in the context, and select\nthe contextual sentence sk with the highest similar-\nity score as the final key sentence prediction. For\nmulti-hop question answering, such as HotpotQA,\nthe key sentences are identified for each individ-\nual hop separately. Finally, we highlight sk by\n(4) while directly prompting the model to answer"}, {"title": "Coarse-to-fine Model Profiling", "content": "For the coarse-\nto-fine search strategy outlined in Section 3.2, we\nconsider all attention heads in the top-l layers as\npotential candidates for selection, where l is chosen\nfrom {3, 4, 5, 6}. Subsequently, we either select\ntop-i heads from each individual layer, or top-j\nheads from the pool of head candidates. Top-i is\nchosen from {4, 6, 8}, and top-j is chosen from\n{16, 24, 32, 64}. The final head set utilized in the\nstudy is determined based on the highest token-F1\nperformance achieved on the profiling set."}, {"title": "Baselines", "content": "We evaluate three open-source LLMs:\nVicuna-7B (Chiang et al., 2023), Llama3-8B-\nInstruct, and Llama3-70B-Instruct under direct\nprompting, iterative prompting, and direct prompt-\ning with AutoPASTA.\n\u2022 Direct prompting: Models are prompted to di-\nrectly answer the question q based on the provided\ncontext c. The prompt template Pd is displayed in\nSection 2.\n\u2022 Iterative Prompting: Models are first prompted\nto generate the key sentence that supports answer-\ning the question, using the same prompt template\nPi. For multi-hop question answering, such as Hot-\npotQA, the key sentences are identified for each\nindividual hop separately. The predicted key sen-\ntences are also mapped back to the original context,\nsimilar as that in AutoPASTA. Then, the model\nare prompted to answer the question with the key\nsentences appended to the context:"}, {"title": "4.2 Main Result: AutoPASTA improves\nopen-book QA.", "content": "We evaluate the performance of AutoPASTA on\nNQ and HotpotQA in two settings: in-domain and\nout-of-domain evaluation. For the in-domain set-\nting, we perform profiling (selecting the head set\nto steer) and evaluate performance on the same\ndataset. For the out-of-domain setting, we per-\nform profiling and evaluate performance on differ-\nent datasets, where the target task is unseen during\nthe profiling to evaluate the generalization ability\nof AutoPASTA.\nIn-domain Evaluation. Table 2 suggests that, for\nall the models, AutoPASTA significantly improves\nthe model performance compared with other base-\nlines, regardless of model size and datasets. For\nexample, AutoPASTA achieves 40.51% EM for\nLLAMA3-8B-Instruct on NQ, yielding a signif-\nicant 9.94% improvement compared to the best-\nperforming baseline. We also observe that itera-\ntive prompting can mostly improve upon the direct\nprompting, showcasing the performance gains from\nidentifying key sentences and appending them to\ncontexts. However, in certain cases, such as Vicuna-\n7B, iterative prompting can actually underperform\ndirect prompting. It suggests that highlighting in\ntoken space by appending key sentences is insuffi-\ncient to fully steer a model's attention. In contrast,\nAutoPASTA shows a consistently substantial im-\nprovement over all baselines, demonstrating the\neffectiveness of automatic attention steering to im-\nprove model faithfulness. Table 1 further illustrates\nthis by comparing the generation examples of Au-\ntoPASTA and direct prompting.\nOut-of-domain Evaluation. In this setting, given\nan evaluation task (e.g., NQ), we employ the head\nsets selected from profiling on the profiling set of\nthe other task (e.g., HotpotQA) for AutoPASTA to\nevaluate its generalization ability across different\ndomains and tasks. The results in Table 2 indi-\ncate that AutoPASTA significantly outperforms all\nbaseline methods for all models and all datasets,\nachieving better or comparable performance to that\nof in-domain profiling. Notably, for LLAMA3-8B-\nInstruct on NQ, the cross-domain performance sur-\npasses the in-domain performance, compellingly\ndemonstrating the robustness and generalization\nproficiency of our approach."}, {"title": "5 Analysis", "content": "AutoPASTA consists of two primary components:\nautomatic key sentence identification, and explicit\nhighlighting key sentences. To underscore the\nnecessity of both components, we conduct the\ncomparison using LLAMA3-8B-Instruct model be-\ntween following methods: (i) direct prompting with\nthe original context; (ii) direct prompting with the\nidentified key sentences appended to the context;\n(iii) highlighting the entire context by attention\nsteering approach but without key-sentence identi-\nfication; (iv) AutoPASTA that highlights the identi-\nfied key sentences.\nThe results in Table 3 indicate that AutoPASTA\ncan benefit from using the identified key sentence,\nyielding significant performance gains. Specifi-\ncally, highlighting the entire context via attention\nsteering can improve upon direct prompting but\nunderperforms AutoPASTA, suggesting the impor-\ntance of key sentence identification. Meanwhile,\nthe comparison between (ii) and (iv) illustrates the\nperformance gains yielded by explicitly highlight-\ning via attention steering. Therefore, these results\nsuggest that both components are essential for Au-\ntoPASTA to achieve its best performance."}, {"title": "5.2 Comparison between profiling strategies", "content": "To illustrate the effectiveness of the coarse-to-fine\nprofiling strategy introduced in Section 3.2, we\nevaluate several different profiling approaches as\nfollows:\n\u2022 Greedy search proposed by (Zhang et al.,\n2024): This strategy involves selecting the top-k\nheads from all the attention heads in the models.\nThe evaluation times for this strategy is L \u00d7 H.\n\u2022 Group search inspired by (Ainslie et al., 2023):\nHere, 8 adjacent heads from one layer form a group.\nThen, we evaluate them group-wise, and select the"}, {"title": "5.3 Performance of AutoPASTA when\nretrieving several passages", "content": "In this work, our primary objective is to enhance\nmodel faithfulness to the provided evidence, and\nthe gold evidence is always provided. Nonetheless,\nin practical applications, a retriever may simultane-\nously supply multiple similar and relevant passages.\nTo demonstrate the effectiveness of AutoPASTA in\nsuch scenarios, we utilize DRP (Karpukhin et al.,\n2020) to retrieve an additional four passages, each\nranked within the top four in relevance scores to\nthe question. Along with the gold evidence, these\nfive passages are then presented to the model. Au-\ntoPASTA is tasked with automatically extracting\nand highlighting the key sentence from this set. Ta-\nble 5 displays the results using Vicuna-7B on NQ.\nAlthough the inclusion of more noisy passages gen-\nerally leads to a performance decline, we still ob-\nserve consistent improvements with AutoPASTA,\nunderscoring the effectiveness of our approach."}, {"title": "5.4 Ablation study", "content": "We conduct ablation study to discuss the perfor-\nmance of AutoPASTA given different number of\nattention heads for steering and different \u03b4."}, {"title": "6 Related Work", "content": "Large language models exhibit remarkable perfor-\nmance on (context-free) knowledge-intensive tasks,\nsuch as open-domain question answering (QA)\n(Kwiatkowski et al., 2019) and commonsense rea-\nsoning (Mihaylov et al., 2018; Clark et al., 2018),\nindicating that they encode substantial knowledge\nabout open-world facts (Zhou et al., 2023) in their\nparameters. Despite their proficiency in memoriza-\ntion, different kinds of hallucinations in the output\nare observed, including factual knowledge hallu-\ncination (Huang et al., 2023; Yu et al., 2024), hal-\nlucination in summarization (Maynez et al., 2020;\nPagnoni et al., 2021), hallucination in logical op-\nerations (Lyu et al., 2023; Huang et al., 2023). In\nthis work, we focus on the factual knowledge hal-\nlucination due to models' unawareness of relevant\nknowledge or overlooking contextual information.\nRetrieval-augmented LLMs. To address the\nproblem of missing relevant knowledge, one pop-\nular method is to use retrieval-augmented LMs\nthat supplement missing knowledge from external\nsources (Shi et al., 2023b; Peng et al., 2023). Re-\ntrieval augmentation requires that LLMs are sensi-\ntive to the input context and generate responses that\nare faithful. However, recent work shows that even\nif the relevant knowledge is presented, the model\nmay still not be faithful to the given evidence (Zhou\net al., 2023; Yu et al., 2024; Wan et al., 2024)."}, {"title": "7 Conclusion", "content": "In this paper, we address the challenge of contex-\ntual faithfulness in open-book QA tasks and in-\ntroduce AutoPASTA, an inference-only method\nthat automatically identifies crucial information\npieces within contexts and explicitly highlights\nthem through steering a model's attention scores.\nAutoPASTA guides the model to focus on the es-\nsential information within contexts, leading to sub-\nstantially improved model faithfulness and perfor-\nmance. Remarkably, by integrate iterative prompt-\ning and attention steering techniques, AutoPASTA\nsynergistically combines their advantages while\nmitigating their respective limitations."}]}