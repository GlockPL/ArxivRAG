{"title": "Stable Weight Updating: A Key to Reliable PDE Solutions Using Deep Learning", "authors": ["A. Noorizadegan", "R. Cavoretto", "D.L. Young", "C.S. Chen"], "abstract": "Deep learning techniques, particularly neural networks, have revolutionized computational physics, offering powerful tools for solving complex partial differential equations (PDEs). However, ensuring stability and efficiency remains a challenge, especially in scenarios involving nonlinear and time-dependent equations. This paper introduces novel residual-based architectures, namely the Simple Highway Network and the Squared Residual Network, designed to enhance stability and accuracy in physics-informed neural networks (PINNs). These architectures augment traditional neural networks by incorporating residual connections, which facilitate smoother weight updates and improve backpropagation efficiency. Through extensive numerical experiments across various examples including linear and nonlinear, time-dependent and independent PDEs-we demonstrate the efficacy of the proposed architectures. The Squared Residual Network, in particular, exhibits robust performance, achieving enhanced stability and accuracy compared to conventional neural networks. These findings underscore the potential of residual-based architectures in advancing deep learning for PDEs and computational physics applications.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have profoundly impacted machine learning and artificial intelligence, achieving remarkable results in diverse fields such as image recognition, natural language processing, and reinforcement learning. Their combination with physics-informed methodologies has been particularly promising, as highlighted by Raissi et al. [1]."}, {"title": "2 Plain Neural Networks", "content": "This section delves into the complex design and functional mechanisms of Multi-Layer Perceptrons (MLPs), which are fundamental to many deep learning systems. Denoted by N, MLPs are precisely engineered to approximate a function $u : q \\in R^d \\rightarrow r\\in R^D$ by systematically organizing artificial neurons across multiple layers."}, {"title": "2.2 Layer Configuration", "content": "The structure of a standard MLP includes several layers. The input layer, also known as the source layer, is crucial as it introduces the input data (dimension d) to the network. Following this, the hidden layers are positioned between the input and output layers. These hidden layers are responsible for executing complex computations, allowing the network to identify and understand intricate patterns and relationships in the data. Finally, the output layer, situated at the end, produces the network's predictions, often having a dimension D.\nThe number of neurons in each layer is determined by the width of the layer, represented as $p^{(k)}$. For a network with K hidden layers, the output vector of the k-th layer, represented as $q^{(k)} \\in R^{p^{(k)}}$, becomes the input for the next layer. The signal that the input layer provides is denoted as $q^{(0)} = q \\in R^d$.\nWithin each layer k (where $1 \\le k \\le K + 1$), the i-th neuron executes an affine transformation followed by a non-linear function. This process is described by the equation:\n$h_{ij}^{(k)} = W_{ij}^{(k)} q_j^{(k-1)}+b_{ij}^{(k)},$ (1)\nwhere $1 \\le i \\le p^{(k)}$ and $1 \\le j \\le p^{(k-1)}$. Subsequently, the output of the i-th neuron in layer k is obtained using the activation function:\n$q_{i}^{(k)} = \\sigma(h_{i}^{(k)}),$ (2)\nfor $1 \\le i \\le p^{(k)}$. In this context, $W_{ij}^{(k)}$ and $b_{ij}^{(k)}$ denote the weights and biases of the i-th neuron in layer k, respectively, and $\\sigma(\\cdot)$ is the activation function, specifically tanh in this instance. The neural network's overall function, $N : R^d \\rightarrow R^D$, can be perceived as a sequence of alternating affine transformations and non-linear activations, as illustrated by the equations above."}, {"title": "2.3 Network Parameterization", "content": "The network's parameters include all associated weights and biases, defined as $\\omega = \\{W^{(k)}, b^{(k)}\\}_{1}^{K+1}$. Each layer k is characterized by its weight matrix, denoted as $W^{(k)}$ along with a bias vector, indicated as $b^{(k)}$. Thus, the network $N(q; \\omega)$ encompasses a variety of parameterized functions, necessitating careful selection of $\\omega$ to ensure the network accurately approximates the desired function u(q) at the input q."}, {"title": "2.4 Back-propagation for Gradient Computation", "content": "In this section, we delve into the mechanics of gradient calculation during neural network training, with a particular focus on the back-propagation technique. For further insights, interested readers can refer to [27]. A crucial aspect of the training process involves computing gradients as the network advances.\nLet us revisit the computation of the output $q^{(k+1)}$ at the (k + 1)-th layer:\nAffine Transformation:\n$h_{ij}^{(k+1)} = W_{ij}^{(k+1)}q_j^{(k)}+b_{ij}^{(k+1)}, 1\\le i \\le p^{(k+1)}, 1\\le j \\le p^{(k)}.$ (3)\nNon-linear Transformation:\n$q_i^{(k+1)} = \\sigma(h_i^{(k+1)}), 1\\le i \\le p^{(k+1)}.$ (4)\nConsidering a training instance denoted as (q,r), where $q^{(0)} = q$, the loss function can be computed through a forward pass:\nFor k = 1,..., K + 1:\n$h^{(k)} = W^{(k)}q^{(k-1)} + b^{(k)},$ (5)\n$q^{(k)} = \\sigma(h^{(k)}).$ (6)\nThe loss function is evaluated as:\n$\\mathcal{L}(\\omega) = ||r \u2013 N(q; \\omega, \\xi) ||\u00b2,$ (7)\nwhere $ \\xi $ denotes a fixed set of hyperparameters such as optimization method, network's length and width, etc. To update the network parameters, we need the derivatives $\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}^{(k)}}, \\frac{\\partial \\mathcal{L}}{\\partial b_{ij}^{(k)}}$, for $1 \\le k \\le K+1$. These derivatives are obtained by first deriving expressions for $\\frac{\\partial \\mathcal{L}}{\\partial q_i^{(k)}}$ and $\\frac{\\partial \\mathcal{L}}{\\partial h_i^{(k)}}$, then iteratively applying the chain rule:\n$\\frac{\\partial \\mathcal{L}}{\\partial h_{i}^{(k)}} = \\frac{\\partial \\mathcal{L}}{\\partial q_{i}^{(k+1)}} \\frac{\\partial q_{i}^{(k+1)}}{\\partial h_{i}^{(k+1)}} \\frac{\\partial h_{i}^{(k+1)}}{\\partial q_{i}^{(k)}} \\frac{\\partial q_{i}^{(k)}}{\\partial h_{i}^{(k)}}$. (8)\nThe computation involves evaluating the following terms:\n$\\frac{\\partial \\mathcal{L}}{\\partial q_i^{(K+1)}} = -2(r \u2013 q_i^{(K+1)})^T,$ (9)\n$\\frac{\\partial h_{ij}^{(k+1)}}{\\partial q_j^{(k)}} = W_{ij}^{(k+1)},$ (10)\n$\\frac{\\partial q_{i}^{(k)}}{\\partial h_{i}^{(k)}} = T^{(k)} = diag[\\sigma'(h_i^{(k)}), ..., \\sigma'(h_{i}^{(k)})].$ (11)\nThese terms lead to the expression:\n$\\frac{\\partial \\mathcal{L}}{\\partial h_{i}^{(k)}} = T^{(k)}W^{(k+1)}T^{(k+1)} ... W^{(K+1)}T^{(K+1)}[-2(r \u2013 q_i^{(K+1)})].$ (12)\nFinally, an explicit expression for $\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}^{(k)}}$ is derived as:\n$\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}^{(k)}} = \\frac{\\partial \\mathcal{L}}{\\partial h_{i}^{(k)}} \\frac{\\partial h_{i}^{(k)}}{\\partial W_{ij}^{(k)}} = \\frac{\\partial \\mathcal{L}}{\\partial h_{i}^{(k)}} q_{j}^{(k-1)}.$ (13)\nIn equation (13), $[qr]_{ij} = q_i r_j$ denotes the outer product. Therefore, to evaluate $\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}^{(k)}}$, both $q_{j}^{(k-1)}$, evaluated during the forward phase, and $\\frac{\\partial \\mathcal{L}}{\\partial h_{i}^{(k)}}$, evaluated during back-propagation, are required. The expression for backpropagation for a 2-layer network (including one hidden layer and one output layer) is presented in Figure 1. Numerical experiments on (13) for a specific epoch (iteration) number are presented in Example 1, Figure 5."}, {"title": "2.5 Training and Testing of MLPs", "content": "Within the domain of supervised learning, the progression through iterative training and validation phases plays a pivotal role in refining and scrutinizing the efficacy of neural"}, {"title": "3 Overview of Physics-Informed Neural Networks", "content": "Physics-informed neural networks are used to solve nonlinear partial differential equations by inferring continuous solution functions u(x, t), where q = (x, t), based on given physical equations. Consider a general nonlinear PDE of the form:\n$u_t + N_x[u] = 0, x \\in \\Omega, t \\in [0, T],$\n$u(x, 0) = I(x), x \\in \\Omega,$\n$u(x, t) = B(x, t), x \\in \\partial \\Omega, t \\in [0, T].$ (14)\nWe define x = $(x_1, x_2,..., x_d) \\in R^d$ and t as the spatial and temporal coordinates, respectively, $u_t$ as the time derivative, and $N_x[\\cdot]$ as a differential operator. The goal is to determine the solution function u(x, t) under these conditions.\nBased on the foundational work of PINNs, a neural network $N(x, t; \\omega, \\xi)$ is used to approximate the solution function u(x, t). Substituting $N(x, t; \\omega, \\xi)$ into the PDE (14),"}, {"title": "4 Advanced Deep Neural Network Architectures", "content": ""}, {"title": "4.1 Highway Networks", "content": "In this subsection, we discuss Highway Networks, a variant of deep neural networks specifically designed to improve the transmission of information through multiple layers [15,16]. These networks employ gating mechanisms that regulate the flow of information, thus facilitating selective processing and retention of the input data. A standard neural network can be depicted as (Figure 2(a)):\n$q^{(k)} = H^{(k)}(q^{(k-1)}, W^{(k)}).$ (18)\nIn (18), q denotes the input to a particular layer, and $H^{(k)}(q^{(k-1)}, W^{(k)})$ represents the operations performed at the k-th layer. Highway Networks (Figure 2(b)) employ two distinct gating mechanisms. The first mechanism, known as the transform gate T, is responsible for managing nonlinear transformations. The second mechanism, the carry"}, {"title": "4.2 Residual Networks", "content": "Residual Networks (Figure 2(c)) are a streamlined adaptation of Highway Networks [28], where the transformation is represented by the addition of the input and a residual component. Known as ResNets [2,3], these architectures have gained significant attention in the neural network community. They feature residual modules, labeled as $H^{(k)}$, and include skip connections that bypass these modules, facilitating the construction of very deep networks. This design enables the formation of residual blocks, which are groups of layers within the network. The output $q^{(k)}$ at the k-th layer is determined as follows:\n$q^{(k)} = H^{(k)}(q^{(k-1)}, W^{(k)}) + q^{(k-1)}.$ (20)"}, {"title": "4.3 A Simple Highway Network (Simple HwNet)", "content": "Here, we present a streamlined version of the highway network, illustrated in Figure 2(d):\n$q^{(k)} = H^{(k)}(q^{(k-1)}, W^{(k)}) + h^{(k)}.$ (23)\nIn this formulation, $h^{(k)} = W_{H}^{(k)}q^{(k-1)} + b^{(k)}$. When comparing this simplified highway network (23) with the original structure in (19), we note that in the simplified model, we define\n$T^{(k)}(q^{(k-1)}, W^{(k)}) = 1,$ (24)\nand\n$q^{(k-1)} \\cdot C^{(k)}(q^{(k-1)}, W^{(k)}) = W_{H}^{(k)} q^{(k-1)}+b^{(k)}.$ (25)\nThis indicates that the weight parameters for both $W_{H}^{(k)}$ and $W_{C}^{(k)}$ in the simplified highway network remain the same, thus removing the need for further training compared to the original highway network. Moreover, this approach obviates the requirement for matrix size matching since the weights are considered within the basic network framework."}, {"title": "4.4 Proposed Squared Residual Network (Sqr-ResNet)", "content": "In this work, we introduce a straightforward highway network. The equation (19) can be reformulated as illustrated in Figure 2(e):\n$q^{(k)} = H^{(k)}(q^{(k-1)}, W^{(k)}) + h^{(k)} \\odot h^{(k)},$ (26)\nwhere $\\odot$ signifies element-wise multiplication. Here, $h^{(k)}$ is given by $W^{(k)}q^{(k-1)}+b^{(k)}$.\nTo evaluate the proposed SqrHw network, represented by (26), against the original model in (19), we observe that in the simplified model, we assume\n$T^{(k)}(q^{(k-1)}, W^{(k)}) = 1,$ (27)\nand the term\n$q^{(k-1)} \\cdot C^{(k)}(q^{(k-1)}, W^{(k)}) = (W_{H}^{(k)}q^{(k-1)} + b_{H}^{(k)}) \\odot (W_{C}^{(k)} q^{(k-1)} + b_{C}^{(k)}).$ (28)\nIn this context, the $\\odot$ operation denotes element-wise multiplication, indicating that the carry gate performs a similar transformation on the input $q^{(k-1)}$ as seen in the original highway network, but introduces extra element-wise alterations. Equation (28) implies that the weight updates remain consistent ($W_{H}^{(k)} = W_{C}^{(k)}$) with those in a standard neural network, thus eliminating the need for additional training or alignment of dimensions."}, {"title": "5 Results and Discussions", "content": "In this study, we use the notations NP, NL, and NN to denote the number of data points (training), layers, and neurons in each layer, respectively. Throughout all subsequent examples, unless otherwise specified, we consider 1002 validation data points. We compare three algorithms:\n\u2022 Plain Net (Plain Neural Network):\nA plain neural network, also called a feedforward neural network or MLP.\n\u2022 Simple HwNet (Simple Highway Network):\nResidual terms are added to every other layer.\n\u2022 Sqr-ResNet (Squared Residual Network):\nThe squared power of residual terms is added to every other layer.\nOur study shows that adding residual terms to every layer or every other layer does not change the results in terms of accuracy, but it improves efficiency regarding computational time. Therefore, we use every other layer for the residual terms.\nThe DeepXDE package [25] is utilized for physics-informed neural network modeling with the following settings: PyTorch selected as the backend, and a random seed of 12345 is used for point generation. Computations are performed using float32 as the default in DeepXDE.\nFor optimization, the L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) optimizer is employed with a maximum of 5000 iterations, a gradient tolerance of 1\u00d710\u22128, and a learning rate of 1 \u00d7 10-3. The neural network is configured with the activation function tanh, and the initializer is set to Glorot uniform. Each hidden layer has a fixed number of 50 neurons (NN), unless specified otherwise. The DeepXDE's source code has been updated to incorporate ResNet and SqrResNet algorithms.\nTables in this section present train loss and test loss, indicating the mean square error $\\frac{1}{n}\\sum_{i=1}^{n} (U_i - N_i)^2$ over function interpolation and PDEs, respectively. The error and test metrics in figures are based on L2 norm error $\\frac{||u-N||_2}{||u||_2}$, where u and N denote exact and approximated solutions, respectively.\nIn tables, the term \u201cstatus\u201d denotes three situations as follows:\n\u2022 completed: Indicates that all predefined steps for the network have been successfully executed, or the network has converged even in a smaller number of pre-defined steps.\n\u2022 not trained: Denotes a scenario where, from the beginning, a large loss was observed, and the situation further deteriorates as the steps progress.\n\u2022 diverged: Signifies that the computations diverged after some initially successful iterations. Divergence can take two forms: (i) a significant increase in loss and error, or (ii) the occurrence of NaN due to SVD convergence issues.\nThe numerical experiments were conducted on a computer with an Intel(R) Core(TM) i9-9900 CPU operating at 3.10 GHz and equipped with 64.0 GB of RAM.\nAlthough the DeepXDE package is well-known and very user-friendly, we encountered some limitations when using it to solve our problems. Specifically, the package records the loss function output every 1000 epochs, and we were unable to adjust this frequency to a smaller interval. As a result, the plots generated display metric results (loss and errors) at every 1000 epochs. This issue has been reported to the author of the package as error #1607. However, this limitation does not affect the final results and only makes the demonstration slightly less informative. Therefore, we continue to use the package to solve Examples 2-5 using both the plain network and the proposed networks fairly.\nIn the following section, five examples are presented (see Table 1). The first example demonstrates the importance of weight updating in a network's performance for function approximation problems, which is a preliminary stage in solving PDEs. We will show that the choice of architecture has a great effect on the weight updating. We also examine the gradient of the loss with respect to the updated weights to assess the quality of backpropagation. Subsequent examples involve solving various PDEs to illustrate the advantages of the proposed residual-based architectures in terms of stability and accuracy."}, {"title": "Example 1", "content": "We use an example of an approximation function to demonstrate the strength of the proposed architectures, both the simple highway network and the square ResNet.\nThe function to be approximated (interpolated) is given by:\n$F = sin(3x_1)cos(3x_2)$ (29)\nwhere $x_1, x_2 \\in [0,1]^2$. Figure 3 shows the validation error (L2 norm error) for approximating F using 500 collocation points, 10 hidden layers, and 50 neurons in each layer. From this figure, it is evident that the Plain Network is less accurate compared to both the proposed simple highway network and the square Residual network. Furthermore, the square ResNet demonstrates better accuracy with fewer epochs, converging at about 2700 epochs, whereas the simple highway network requires about 3200 epochs to converge.\nFigure 4 displays the final approximated function in the top panel and the Frobenius norms of weights with respect to the epoch in the bottom panel. This figure highlights the relationship between accuracy and weight updating of the network. The top panel illustrates that both proposed networks outperform the Plain Network, which shows significant error near the boundary. In the bottom panel, a comparison of Figures 4(d) and 4(e) reveals that the weight norms for the Plain Network fluctuate significantly and increase more than those from the Simple Highway Network architecture. Moreover, the simple highway network in Figure 4(e) shows a larger rise compared to the square-ResNet in Figure 4(f). For the Sqr-ResNet case, the weights become stable after approximately 2500 epochs while the simple highway network requires slightly more epochs to stabilize the weight updating. A comparison between the error (Figure 3) and the weight updating in the bottom panel of Figure 4 indicates that more stable weight updating leads to more accurate results and better convergence.\nWe plot the histogram of the norm of the gradient of loss with respect to weight at Epoch = 1000 for layer numbers 3 and 6 in Figure 5. To enhance visibility, we omit the results for the simple highway network, as they are similar to those of the square-ResNet architecture. As reported by Li et al. [4], residual data helps create a smoother loss function, leading to better convergence and smaller backpropagation gradients. A \u201cgood\" network architecture also helps the optimization method to be on the way to find the global minimal whereas plain network is prone to finish the optimization process in a local minimal. This can be recognized by the large error in results when using the plain network while residual-based architectures can finish it in a much smaller error. Our intense numerical tests in this example and following examples show that the effect of the good network is more evident when the network includes a large number of layers."}, {"title": "Example 2", "content": "We aim to solve the 1D diffusion-reaction equation given by:\n$\\frac{\\partial u}{\\partial t} = d \\frac{\\partial^2 u}{\\partial x^2} + e^{-t} \\left( \\frac{3}{2} sin(2x_1) + \\frac{15}{4} sin(3x_1) + \\frac{63}{8} sin(4x_1) + \\frac{511}{8} sin(8x_1) \\right)$ (30)\nwhere d = 1. subject to the initial condition at t = 0:\n$u(x_1, 0) = sin(x_1)+ \\frac{1}{2} sin(2x_1)+ \\frac{1}{3} sin(3x_1) + \\frac{1}{4} sin(4x_1) + \\frac{1}{8} sin(8x_1), x_1 \\in [-\\pi, \\pi]$ (31)\nand the Dirichlet boundary conditions:\n$u(t, -\\pi) = u(t, \\pi) = 0, t \\in [0, 1].$ (32)\nThe exact solution to this equation is:\n$u(x_1, t) = e^{-t} \\left( sin(x_1) + \\frac{1}{2} sin(2x_1) + \\frac{1}{3} sin(3x_1) + \\frac{1}{4} sin(4x_1) + \\frac{1}{8} sin(8x_1) \\right).$ (33)\nTable 2 compiles the results obtained using Plain network, Simple Highway network, and Sqr-ResNet with respect to NL (number of layers). It is evident that Plain Net has only successfully completed two cases, namely NL=5 and 15. Conversely, Hw-net exhibits improved performance but encounters failure at NL=30. Notably, Sqr-ResNet demonstrates the best overall performance by completing the training, achieving high accuracy, and maintaining stable performance. Figure 6 illustrates the loss and metric of NL=10 over the course of the training steps (top panel) and the maximum absolute error (bottom panel) corresponding to the smallest error during training. Analyzing Figure 6(a), we observe divergence after 1000 steps and large absolute error in Figure 6(d). On the other hand both residual based methods, Hw-net and Sqr-ResNet, exhibit robust performance on training data and small absolute error over validation data."}, {"title": "Example 3", "content": "We aim to solve the heat equation given by:\n$\\frac{\\partial v}{\\partial t} = \\alpha \\frac{\\partial^2 v}{\\partial x_1^2} x_1 \\in [0,1], t \\in [0, 1]$ (34)\nwhere $\\alpha = 0.4$ is the thermal diffusivity constant. The boundary conditions are:\n$u(0, t) = u(1, t) = 0.$ (35)\nThe initial condition is a sinusoidal function:\n$u(x_1,0) = sin \\left( \\frac{n \\pi x_1}{L} \\right), 0 \\leq x_1 \\leq 1, n = 1, 2, ....$ (36)\nThe exact solution to the heat equation is:\n$u(x_1, t) = e^{\\frac{-\\alpha t n^2 \\pi^2}{L^2}} sin \\left( \\frac{n \\pi x_1}{L} \\right)$ (37)\nHere, L = 1 represents the length of the domain, and n = 1 corresponds to the frequency of the initial sinusoidal condition.\nA total of 2780 collocation points, including 2540 interior, 80 boundary, and 160 initial points, are considered for this example. Table 3 presents the results for Plain Net, simple HwNet, and Sqr-ResNet with respect to the NL. Plain Net completed 5000 steps only for NL=5 and 20, while NL=30 failed to train, producing large loss and error from the beginning of computation. Two cases, NL=10 and 15, produced large loss values after step=1000. On the other hand, both Hw-net and Sqr-ResNet completed the predefined number of iterations (5000) for all NL cases. However, the error for all cases with completed status is very close, with the best accuracy achieved for NL=20 using the simple highway network.\nMoreover, Figure 7 presents the profile of the absolute error for the case with NL=15, using Plain Net, simple HwNet, and Sqr-ResNet. The figure showcases the best training results concerning the training loss value with respect to the steps. From these plots, it can be observed that Hw-net produces the smallest maximum absolute error, which occurred at step=5000. On the other hand, the plot shows the profile of the error for Plain Net at step=1000, which is very large compared with the other two methods due to incomplete training processes."}, {"title": "Example 4", "content": "In this example, we solve a nonlinear Schr\u00f6dinger equation with periodic boundary conditions:\n$i \\frac{\\partial u}{\\partial t} + 0.5 \\frac{\\partial^2 u}{\\partial x_1^2} + |u|^2 u = 0, x_1 \\in [-5, 5], t \\in [0, \\pi/2].$ (38)\nThe initial condition is:\n$u(x_1,0) = 2sech(x_1),$ (39)\nwhile the boundary conditions are:\n$u(-5, t) = u(5, t), \\frac{\\partial u}{\\partial x_1}(-5, t) = - \\frac{\\partial u}{\\partial x_1}(5, 5).$ (40)\nIn this example, 10000 interior points, 20 boundary points, and 200 points for the initial condition are considered. Table 4 presents the train loss. It also lists the error on the real and imaginary terms shown as e(h) and e(v). In this example, Plain Net behaves more stably than in previous examples in terms of completing the training process. However, still at NL=15, a divergence after step=1000 occurred. From this table we can see that there is a rise in errors at NL=30. The residual-based method, simple highway network, behaves more stably by keeping the order of error magnitudes at -2. However, the best performance can be seen for Sqr-ResNet, where the smallest errors compared to the two other architectures are observed, and the results are stable around the magnitude of order -3 as NL increases. Note that the best accuracy happened at NL=15 using Sqr-ResNet. Figure 8 shows the loss and metric values with respect to the step. Clearly, Sqr-ResNet leads to the best training performance in terms of accuracy and convergence, and Plain Net is the worst one."}, {"title": "Example 5", "content": "In the final example, we solve a 2D linear elasticity problem in solid me"}, {"title": "6 Conclusions", "content": "In this study, we introduced two new types of network architectures: the Simple Highway Network and the Squared Residual Network. These are designed to make solving partial differential equations with deep learning more stable. Our contributions emphasize the enhanced final results and the underlying reasons for the superior performance of our proposed networks, which are applicable for evaluating the performance of other architectures.\nOur findings show that the Squared Residual Network significantly improves stability during weight updates for function approximation example. Smoother and appropriately scaled weight updates lead to better training accuracy and quicker convergence. Conversely, large weight updates often result in higher errors. Our residual-based architectures also effectively reduce the gradient of loss related to weight updates, improving training effectiveness.\nPhysics-informed neural networks rely on accurate function approximation. Our networks excel in solving a wide range of PDEs, whether they are time-dependent, independent, linear, or nonlinear. The Squared Residual Network, especially, consistently performs well. It successfully completes training where plain networks fail, achieving high accuracy. Conversely, while a plain network may demonstrate that deeper architectures can lead to instability and reduced accuracy, it has also been shown that deeper networks can achieve better accuracy when an appropriate architecture is selected. The Squared Residual Network's stability and accuracy improvements make it a promising tool for advancing deep learning in PDEs and computational physics."}]}