{"title": "Stable Weight Updating: A Key to Reliable PDE Solutions Using Deep Learning", "authors": ["A. Noorizadegan", "R. Cavoretto", "D.L. Young", "C.S. Chen"], "abstract": "Deep learning techniques, particularly neural networks, have revolutionized computational physics, offering powerful tools for solving complex partial differential equations (PDEs). However, ensuring stability and efficiency remains a challenge, especially in scenarios involving nonlinear and time-dependent equations. This paper introduces novel residual-based architectures, namely the Simple Highway Network and the Squared Residual Network, designed to enhance stability and accuracy in physics-informed neural networks (PINNs). These architectures augment traditional neural networks by incorporating residual connections, which facilitate smoother weight updates and improve backpropagation efficiency. Through extensive numerical experiments across various examples including linear and nonlinear, time-dependent and independent PDEs-we demonstrate the efficacy of the proposed architectures. The Squared Residual Network, in particular, exhibits robust performance, achieving enhanced stability and accuracy compared to conventional neural networks. These findings underscore the potential of residual-based architectures in advancing deep learning for PDEs and computational physics applications.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have profoundly impacted machine learning and artificial intelligence, achieving remarkable results in diverse fields such as image recognition, natural language processing, and reinforcement learning. Their combination with physics-informed methodologies has been particularly promising, as highlighted by Raissi et al. [1]."}, {"title": "2 Plain Neural Networks", "content": "This section delves into the complex design and functional mechanisms of Multi-Layer Perceptrons (MLPs), which are fundamental to many deep learning systems. Denoted by N, MLPs are precisely engineered to approximate a function \\(u : q \u2208 R^d \u2192 r \u2208 R^D\\) by systematically organizing artificial neurons across multiple layers."}, {"title": "2.2 Layer Configuration", "content": "The structure of a standard MLP includes several layers. The input layer, also known as the source layer, is crucial as it introduces the input data (dimension d) to the network. Following this, the hidden layers are positioned between the input and output layers. These hidden layers are responsible for executing complex computations, allowing the network to identify and understand intricate patterns and relationships in the data. Finally, the output layer, situated at the end, produces the network's predictions, often having a dimension D.\n\nThe number of neurons in each layer is determined by the width of the layer, represented as \\(p^{(k)}\\). For a network with K hidden layers, the output vector of the k-th layer, represented as \\(q^{(k)} \u2208 R^{p(k)}\\), becomes the input for the next layer. The signal that the input layer provides is denoted as \\(q^{(0)} = q \u2208 R^d\\).\n\nWithin each layer k (where 1 \u2264 k \u2264 K + 1), the i-th neuron executes an affine transformation followed by a non-linear function. This process is described by the equation:\n\n\\(h_i^{(k)} = \\sum_{j=1}^{p^{(k-1)}} W_{ij}^{(k)} q_j^{(k-1)} + b_i^{(k)}\\),\n\nwhere \\(1 \u2264 i \u2264 p^{(k)}\\) and \\(1 \u2264 j \u2264 p^{(k-1)}\\). Subsequently, the output of the i-th neuron in layer k is obtained using the activation function:\n\n\\(q_i^{(k)} = \u03c3(h_i^{(k)})\\),\n\nfor \\(1 \u2264 i \u2264 p^{(k)}\\). In this context, \\(W_{ij}^{(k)}\\) and \\(b_i^{(k)}\\) denote the weights and biases of the i-th neuron in layer k, respectively, and \\(\u03c3(\u00b7)\\) is the activation function, specifically tanh in this instance. The neural network's overall function, \\(N : R^d \u2192 R^D\\), can be perceived as a sequence of alternating affine transformations and non-linear activations, as illustrated by the equations above."}, {"title": "2.3 Network Parameterization", "content": "The network's parameters include all associated weights and biases, defined as \\(\\omega = {W^{(k)}, b^{(k)}}_{k=1}^{K+1}\\). Each layer k is characterized by its weight matrix, denoted as \\(W^{(k)}\\) along with a bias vector, indicated as \\(b^{(k)}\\). Thus, the network \\(\\mathcal{N}(q; \\omega, \\xi)\\) encompasses a variety of parameterized functions, necessitating careful selection of \\(\\omega\\) to ensure the network accurately approximates the desired function u(q) at the input q."}, {"title": "2.4 Back-propagation for Gradient Computation", "content": "In this section, we delve into the mechanics of gradient calculation during neural network training, with a particular focus on the back-propagation technique. For further insights, interested readers can refer to [27]. A crucial aspect of the training process involves computing gradients as the network advances.\n\nLet us revisit the computation of the output \\(q^{(k+1)}\\) at the (k + 1)-th layer:\n\nAffine Transformation:\n\n\\(h_{ij}^{(k+1)} = \\sum_{j=1}^{p^{(k)}} W_{ij}^{(k+1)} q_j^{(k)} + b_i^{(k+1)}, \\quad 1 \u2264 i \u2264 p^{(k+1)}, \\quad 1 \u2264 j \u2264 p^{(k)}\\).\n\nNon-linear Transformation:\n\n\\(q_i^{(k+1)} = \u03c3(h_i^{(k+1)}), \\quad 1 \u2264 i \u2264 p^{(k+1)}\\).\n\nConsidering a training instance denoted as (q,r), where \\(q^{(0)} = q\\), the loss function can be computed through a forward pass:\n\nFor k = 1,..., K + 1:\n\n\\(h_i^{(k)} = \\sum_{j=1}^{p^{(k-1)}} W_{ij}^{(k)} q_j^{(k-1)} + b_i^{(k)}\\),\n\n\\(q_i^{(k)} = \u03c3(h_i^{(k)})\\).\n\nThe loss function is evaluated as:\n\n\\(L(\\omega) = ||r \u2013 \\mathcal{N}(q; \\omega, \\xi) ||\u00b2\\),\n\nwhere \\(\\xi\\) denotes a fixed set of hyperparameters such as optimization method, network's length and width, etc. To update the network parameters, we need the derivatives \\(\\frac{\\partial L}{\\partial W_{ij}^{(k)}}\\), \\(\\frac{\\partial L}{\\partial b_i^{(k)}}\\), for \\(1 \u2264 k \u2264 K + 1\\). These derivatives are obtained by first deriving expressions for \\(\\frac{\\partial L}{\\partial h_i^{(k)}}\\) and \\(\\frac{\\partial L}{\\partial q_i^{(k)}}\\), then iteratively applying the chain rule:\n\n\\(\\frac{\\partial L}{\\partial h_i^{(k)}} = \\sum_{j=1}^{p^{(k+1)}} \\frac{\\partial L}{\\partial q_j^{(k+1)}} \\frac{\\partial q_j^{(k+1)}}{\\partial h_i^{(k+1)}} \\frac{\\partial h_i^{(k+1)}}{\\partial q_i^{(k)}} \\frac{\\partial q_i^{(k)}}{\\partial h_i^{(k)}}\\).\n\nThe computation involves evaluating the following terms:\n\n\\(\\frac{\\partial L}{\\partial q^{(K+1)}} = -2(r \u2013 q^{(K+1)})^T\\),"}, {"title": "2.5 Training and Testing of MLPs", "content": "Within the domain of supervised learning, the progression through iterative training and validation phases plays a pivotal role in refining and scrutinizing the efficacy of neural"}, {"title": "3 Overview of Physics-Informed Neural Networks", "content": "Physics-informed neural networks are used to solve nonlinear partial differential equations by inferring continuous solution functions u(x, t), where q = (x, t), based on given physical equations. Consider a general nonlinear PDE of the form:\n\n\\(u_t + \\mathcal{N}_x[u] = 0, \\quad x \u2208 \\Omega, t \u2208 [0, T],\\)\n\n\\(u(x, 0) = I(x), \\quad x \u2208 \\Omega,\\)\n\n\\(u(x, t) = B(x, t), \\quad x \u2208 \\partial \\Omega, t \u2208 [0, T].\\)\n\nWe define \\(x = (x_1, x_2,..., x_d) \u2208 R^d\\) and t as the spatial and temporal coordinates, respectively, \\(u_t\\) as the time derivative, and \\(\\mathcal{N}_x[\u00b7]\\) as a differential operator. The goal is to determine the solution function u(x, t) under these conditions.\n\nBased on the foundational work of PINNs, a neural network \\(\\mathcal{N}(x, t; \\omega, \\xi)\\) is used to approximate the solution function u(x, t). Substituting \\(\\mathcal{N}(x, t; \\omega, \\xi)\\) into the PDE (14),"}, {"title": "4 Advanced Deep Neural Network Architectures", "content": ""}, {"title": "4.1 Highway Networks", "content": "In this subsection, we discuss Highway Networks, a variant of deep neural networks specifically designed to improve the transmission of information through multiple layers [15,16]. These networks employ gating mechanisms that regulate the flow of information, thus facilitating selective processing and retention of the input data. A standard neural network can be depicted as (Figure 2(a)):\n\n\\(q^{(k)} = H^{(k)}(q^{(k-1)}, W^{(k)}).\\)\n\nIn (18), q denotes the input to a particular layer, and \\(H^{(k)}(q^{(k-1)}, W^{(k)})\\) represents the operations performed at the k-th layer. Highway Networks (Figure 2(b)) employ two distinct gating mechanisms. The first mechanism, known as the transform gate T, is responsible for managing nonlinear transformations. The second mechanism, the carry"}, {"title": "4.2 Residual Networks", "content": "Residual Networks (Figure 2(c)) are a streamlined adaptation of Highway Networks [28], where the transformation is represented by the addition of the input and a residual component. Known as ResNets [2,3], these architectures have gained significant attention in the neural network community. They feature residual modules, labeled as \\(H^{(k)}\\), and include skip connections that bypass these modules, facilitating the construction of very deep networks. This design enables the formation of residual blocks, which are groups of layers within the network. The output \\(q^{(k)}\\) at the k-th layer is determined as follows:\n\n\\(q^{(k)} = H^{(k)}(q^{(k-1)}, W^{(k)}) + q^{(k-1)}.\\)"}, {"title": "4.3 A Simple Highway Network (Simple HwNet)", "content": "Here, we present a streamlined version of the highway network, illustrated in Figure 2(d):\n\n\\(q^{(k)} = H^{(k)}(q^{(k-1)}, W^{(k)}) + h^{(k)}.\\)\n\nIn this formulation, \\(h^{(k)} = W_H^{(k)}q^{(k-1)} + b^{(k)}\\). When comparing this simplified highway network (23) with the original structure in (19), we note that in the simplified model, we define\n\n\\(T^{(k)}(q^{(k-1)}, W^{(k)}) = 1,\\)\n\nand\n\n\\(q^{(k-1)} \u00b7 C^{(k)}(q^{(k-1)}, W^{(k)}) = W_H^{(k)} q^{(k-1)} + b^{(k)}.\\)\n\nThis indicates that the weight parameters for both \\(W_H\\) and \\(W_C\\) in the simplified highway network remain the same, thus removing the need for further training compared to the original highway network. Moreover, this approach obviates the requirement for matrix size matching since the weights are considered within the basic network framework."}, {"title": "4.4 Proposed Squared Residual Network (Sqr-ResNet)", "content": "In this work, we introduce a straightforward highway network. The equation (19) can be reformulated as illustrated in Figure 2(e):\n\n\\(q^{(k)} = H^{(k)}(q^{(k-1)}, W^{(k)}) + h^{(k)} \\odot h^{(k)},\\)\n\nwhere \\(\\odot\\) signifies element-wise multiplication. Here, \\(h^{(k)}\\) is given by \\(W^{(k)}q^{(k-1)} + b^{(k)}\\).\n\nTo evaluate the proposed SqrHw network, represented by (26), against the original model in (19), we observe that in the simplified model, we assume\n\n\\(T^{(k)}(q^{(k-1)}, W^{(k)}) = 1,\\)"}, {"title": "5 Results and Discussions", "content": "In this study, we use the notations NP, NL, and NN to denote the number of data points (training), layers, and neurons in each layer, respectively. Throughout all subsequent examples, unless otherwise specified, we consider 100\u00b2 validation data points. We compare three algorithms:\n\n\u2022 Plain Net (Plain Neural Network):\nA plain neural network, also called a feedforward neural network or MLP.\n\n\u2022 Simple HwNet (Simple Highway Network):\nResidual terms are added to every other layer.\n\n\u2022 Sqr-ResNet (Squared Residual Network):\nThe squared power of residual terms is added to every other layer.\n\nOur study shows that adding residual terms to every layer or every other layer does not change the results in terms of accuracy, but it improves efficiency regarding computational time. Therefore, we use every other layer for the residual terms.\n\nThe DeepXDE package [25] is utilized for physics-informed neural network modeling with the following settings: PyTorch selected as the backend, and a random seed of 12345 is used for point generation. Computations are performed using float32 as the default in DeepXDE.\n\nFor optimization, the L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) optimizer is employed with a maximum of 5000 iterations, a gradient tolerance of 1 \u00d7 10\u207b\u2078, and a learning rate of 1 \u00d7 10\u207b\u00b3. The neural network is configured with the activation function tanh, and the initializer is set to Glorot uniform. Each hidden layer has a fixed number of 50 neurons (NN), unless specified otherwise. The DeepXDE's source code has been updated to incorporate ResNet and SqrResNet algorithms.\n\nTables in this section present train loss and test loss, indicating the mean square error \\(\\frac{1}{n} \\sum_{i=1}^{n} (u_i - \\mathcal{N}_i)^2\\) over function interpolation and PDEs, respectively. The error and test metrics in figures are based on L2 norm error \\(\\frac{||u - \\mathcal{N}||_2}{||u||_2}\\), where u and \\(\\mathcal{N}\\) denote exact and approximated solutions, respectively.\n\nIn tables, the term \u201cstatus\u201d denotes three situations as follows:"}, {"title": "Example 1", "content": "We use an example of an approximation function to demonstrate the strength of the proposed architectures, both the simple highway network and the square ResNet. The function to be approximated (interpolated) is given by:\n\n\\(F = sin(3x_1)cos(3x_2)\\)"}, {"title": "Example 2", "content": "We aim to solve the 1D diffusion-reaction equation given by:\n\n\\(\\frac{\\partial u}{\\partial t} = d \\frac{\\partial^2 u}{\\partial x_1^2} + e^{-t} \\bigg( \\frac{3}{2} sin(2x_1) + \\frac{15}{4} sin(3x_1) + \\frac{63}{8} sin(4x_1) + \\frac{63}{8} sin(8x_1) \\bigg)\n\nwhere d = 1. subject to the initial condition at t = 0:\n\n\\(u(x_1, 0) = sin(x_1) + \\frac{1}{2} sin(2x_1) + \\frac{1}{3} sin(3x_1) + \\frac{1}{4} sin(4x_1) + \\frac{1}{8} sin(8x_1), \\quad x_1 \u2208 [-\\pi, \\pi]\\)\n\nand the Dirichlet boundary conditions:\n\n\\(u(t, -\\pi) = u(t, \\pi) = 0, \\quad t \u2208 [0, 1].\\)"}, {"title": "Example 3", "content": "We aim to solve the heat equation given by:\n\n\\(\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x_1^2}, \\quad x_1 \u2208 [0, 1], \\quad t \u2208 [0, 1]\\)\n\nwhere \u03b1 = 0.4 is the thermal diffusivity constant. The boundary conditions are:\n\n\\(u(0, t) = u(1, t) = 0.\\)\n\nThe initial condition is a sinusoidal function:\n\n\\(u(x_1, 0) = sin(\\frac{n \\pi x_1}{L}), \\quad 0 \u2264 x_1 \u2264 1, \\quad n = 1, 2, ....\\)\n\nThe exact solution to the heat equation is:\n\n\\(u(x_1, t) = e^{-\\frac{n^2 \\pi^2 \\alpha t}{L^2}} sin(\\frac{n \\pi x_1}{L}).\\)\n\nHere, L = 1 represents the length of the domain, and n = 1 corresponds to the frequency of the initial sinusoidal condition.\n\nA total of 2780 collocation points, including 2540 interior, 80 boundary, and 160 initial points, are considered for this example. Table 3 presents the results for Plain Net, simple HwNet, and Sqr-ResNet with respect to the NL. Plain Net completed 5000 steps only for NL=5 and 20, while NL=30 failed to train, producing large loss and error from the beginning of computation. Two cases, NL=10 and 15, produced large loss values after step=1000. On the other hand, both Hw-net and Sqr-ResNet completed the"}, {"title": "Example 4", "content": "In this example, we solve a nonlinear Schr\u00f6dinger equation with periodic boundary conditions:\n\n\\(i \\frac{\\partial u}{\\partial t} + 0.5 \\frac{\\partial^2 u}{\\partial x_1^2} + |u|^2 u = 0, \\quad x_1 \u2208 [-5, 5], \\quad t \u2208 [0, \\pi/2].\\)\n\nThe initial condition is:\n\n\\(u(x_1, 0) = 2sech(x_1),\\)\n\nwhile the boundary conditions are:\n\n\\(u(-5, t) = u(5, t), \\quad \\frac{\\partial u}{\\partial x_1}(-5, t) = \\frac{\\partial u}{\\partial x_1}(5, 5).\\)"}, {"title": "Example 5", "content": "In the final example, we solve a 2D linear elasticity problem in solid mechanics. The governing equations are:\n\n\\frac{\\partial \\sigma_{x_1 x_1}}{\\partial x_1} + \\frac{\\partial \\sigma_{x_1 x_2}}{\\partial x_2} + f_{x_1} = 0, \\quad x_1, x_2 \u2208 [0, 1],\n\n\\frac{\\partial \\sigma_{x_1 x_2}}{\\partial x_1} + \\frac{\\partial \\sigma_{x_2 x_2}}{\\partial x_2} + f_{x_2} = 0.\n\nThe linear elastic constitutive relations are:\n\n\\sigma_{x_1 x_1} = (\\lambda + 2\\mu) \\epsilon_{x_1 x_1} + \\lambda \\epsilon_{x_2 x_2},\n\n\\sigma_{x_2 x_2} = (\\lambda + 2\\mu) \\epsilon_{x_2 x_2} + \\lambda \\epsilon_{x_1 x_1},\n\n\\sigma_{x_1 x_2} = 2\\mu \\epsilon_{x_1 x_2},\n\nwhere the strains are defined as:\n\n\\epsilon_{x_1 x_1} = \\frac{\\partial u_{x_1}}{\\partial x_1},\n\n\\epsilon_{x_2 x_2} = \\frac{\\partial u_{x_2}}{\\partial x_2},\n\n\\epsilon_{x_1 x_2} = \\frac{1}{2} (\\frac{\\partial u_{x_1}}{\\partial x_2} + \\frac{\\partial u_{x_2}}{\\partial x_1}).\n\nThe body forces applied are:\n\nf_{x_1} = \\lambda [4\\pi^2 cos(2\\pi x_1) sin(\\pi x_2) - \\pi cos(\\pi x_1) Q x_2^3]\n\n+ \\mu [9\\pi^2 cos(2\\pi x_1) sin(\\pi x_2) - \\pi cos(\\pi x_1) Q x_2^3],\n\nf_{x_2} = \\lambda [-3 sin(\\pi x_1) Q x_2^2 + 2\\pi^2 sin(2\\pi x_1) cos(\\pi x_2)]\n\n+ \\mu [-6 sin(\\pi x_1) Q x_2^2 + 2\\pi^2 sin(2\\pi x_1) cos(\\pi x_2) + \\pi^2 sin(\\pi x_1) Q x_2^2 / 4],\n\nThe displacement boundary conditions are:\n\nu_{x_1}(x_1, 0) = u_{x_1}(x_1, 1) = 0,\n\nu_{x_2}(0, x_2) = u_{x_2}(1, x_2) = u_{x_2}(x_1, 0) = 0.\n\nThe traction boundary conditions are:\n\n\\sigma_{x_1 x_1}(0, x_2) = \\sigma_{x_1 x_1}(1, x_2) = 0,\n\n\\sigma_{x_2 x_2}(x_1, 1) = (\\lambda + 2\\mu) Q sin(\\pi x_1).\n\nWe use the following parameters: \\lambda = 1, \\mu = 0.5, and Q = 4. The exact solutions are u_{x_1}(x_1, x_2) = cos(2\\pi x_1) sin(\\pi x_2) and u_{x_2}(x_1, x_2) = Q x_1^3 / 4 sin(\\pi x_1).\n\nIn this example, we consider 500 interior points and 500 points on the boundary. The results presented in Table 5 indicate a decrease in the performance of Plain Net as NL increases, with the last two NL values (NL=20 and 30) not being trained successfully. On the contrary, the residual-based methods exhibit better performance, with all cases successfully implemented. Notably, Sqr-ResNet maintains the error's order of magnitude at -3, showcasing a more stable computation across various NL. Additionally, the smallest error is observed at NL=10 using Sqr-ResNet."}, {"title": "6 Conclusions", "content": "In this study, we introduced two new types of network architectures: the Simple Highway Network and the Squared Residual Network. These are designed to make solving partial differential equations with deep learning more stable. Our contributions emphasize the enhanced final results and the underlying reasons for the superior performance of our proposed networks, which are applicable for evaluating the performance of other architectures.\n\nOur findings show that the Squared Residual Network significantly improves stability during weight updates for function approximation example. Smoother and appropriately scaled weight updates lead to better training accuracy and quicker convergence. Conversely, large weight updates often result in higher errors. Our residual-based architectures also effectively reduce the gradient of loss related to weight updates, improving training effectiveness.\n\nPhysics-informed neural networks rely on accurate function approximation. Our networks excel in solving a wide range of PDEs, whether they are time-dependent, independent, linear, or nonlinear. The Squared Residual Network, especially, consistently performs well. It successfully completes training where plain networks fail, achieving high accuracy. Conversely, while a plain network may demonstrate that deeper architectures can lead to instability and reduced accuracy, it has also been shown that deeper networks can achieve better accuracy when an appropriate architecture is selected. The Squared Residual Network's stability and accuracy improvements make it a promising tool for advancing deep learning in PDEs and computational physics."}]}