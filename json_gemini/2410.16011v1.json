{"title": "CA*: Addressing Evaluation Pitfalls in Computation-Aware Latency for Simultaneous Speech Translation", "authors": ["Xi Xu", "Wenda Xu", "Siqi Ouyang", "Lei Li"], "abstract": "Simultaneous speech translation (SimulST) systems must balance translation quality with response time, making latency measurement crucial for evaluating their real-world performance. However, there has been a longstanding belief that current metrics yield unrealistically high latency measurements in unsegmented streaming settings. In this paper, we investigate this phenomenon, revealing its root cause in a fundamental misconception underlying existing latency evaluation approaches. We demonstrate that this issue affects not only streaming but also segment-level latency evaluation across different metrics. Furthermore, we propose a modification to correctly measure computation-aware latency for SimulST systems, addressing the limitations present in existing metrics.", "sections": [{"title": "Introduction", "content": "Simultaneous speech-to-text translation (SimulST) (Ma et al., 2020b) focuses on a real-time, low-latency scenario where the model starts generating the textual translation before the entire audio input is processed. Achieving high-quality translations with minimal latency is the primary objective of SimulST systems, with time constraints varying by scenario(Anastasopoulos et al., 2022; Agarwal et al., 2023; Ahmad et al., 2024). These constraints are typically quantified as latency, often defined as the average time delay between when a word is spoken and when its translation is generated. Accurate latency measurement is thus critical for evaluating system performance.\nVarious metrics have been introduced to measure SimulST system's latency, including Average Proportion (AP) (Cho and Esipova, 2016), Average Lagging (AL) (Ma et al., 2019), Differentiable Average Lagging (DAL) (Cherry and Foster, 2019), and Length Adaptive Average Lagging (LAAL)(Papi et al., 2022) and Average Token Delay (ATD)(Kano et al., 2022). Recent studies (Papi et al., 2023a,b; Ahmad et al., 2024; Xu et al., 2024) have emphasized computation-aware latency as a more realistic way to evaluate SimulST performance in real-time scenarios where computation time cannot be ignored.\nAs the performance of SimulST systems improves, researchers are motivated to apply them to unsegmented streaming long speech(Pol\u00e1k, 2023; Ouyang et al., 2024; Papi et al., 2024; Iranzo-S\u00e1nchez et al., 2021), which better represents real-world scenarios such as interpreting and lecture transcription. However, computation-aware latency scores reported by existing metrics have been observed to be unrealistically high, hindering progress in this area.\nAs illustrated in Figure 1, the computation-aware AL and LAAL metrics show drastic increases as the length of streaming speech increases. Specifically, the computation-aware LAAL increases from 6850 ms when the speech length is 25 seconds to 23460 ms when the speech length increased to 100 seconds. In contrast, the theoretical computation unaware LAAL remains mostly consistent, at 8181 ms."}, {"title": "Latency Metrics", "content": "An evaluation corpus for a speech translation task contains one or more instances, each consisting of a source speech sequence $X = [x_1, ..., x_{|X|}]$ and a reference text sequence $Y^* = [y_1, ..., y^*_{|Y^*|}]$. The system to be evaluated takes X as input and generates $Y = [Y_1, ..., Y_{|Y|}]$ as the target language's text translation incrementally.\nIn the simultaneous speech translation task, the system starts generating a hypothesis with only partial input. It alternates between reading a new source speech segment or writing a new target text segment. Assuming $X_i = [x_1,..., x_j], j < |X|$ has been read when generating $y_i$, where $x_j$ represents a raw audio segment of duration $T_j$. The Computation Aware (CA) and Computation Un-\naware (CU) delay of a token $y_i$ are defined as:\n$d_i = \\begin{cases} \\sum_{k=1}^{j}T_k+C_i,& \\text{CA} \\\\ C_i, & \\text{CU} \\end{cases}$ (1)\nwhere $C_i$ is the elapsed time when generating the i-th token, as recorded by SimulEval (Ma et al., 2020a) after generating this token.\nThe latency metrics are calculated using a normalization function, which takes the sequence of delays, $D = [d_1,...,d_{|Y|}]$, from SimulEval, along with a corresponding set of oracle delays, $D^* = [d^*_1,..., d^*_{|Y|} ]$. This process can be formalized as follows:\n$\\text{Latency} = \\frac{1}{\\tau'(|X|)} \\sum_{i=1}^{\\tau'(|X|)} (d_i-d^*_i)$, (2)\nwhere $\\tau'(|X|) = \\min\\{i | d_i = \\sum_{k=1}^{j}T_k\\}$ and $d^*_i$ represents the oracle delays. Ma et al. (2020a) suggests using $d^*_i = (i - 1) \\cdot \\frac{\\sum_j T_j}{|Y^*|}$ to mitigate potential under-generation in Average Lagging (AL) measures. To correct the bias towards over-generation, Papi et al. (2022) recommends substituting |Y^*| with the larger value between |Y^*| and |Y| when computing oracle delays."}, {"title": "The Problem of current CA", "content": "In practice, the system receives new speech input while generating text translations. However, as shown in Figure 3, the CA delay $\\sum_{k=1}^{j}T_k + C_i$ treats a parallel reading and writing process sequentially, assuming the system alternates between reading and writing actions. As a result, the CA delay accumulates the computation cost at each step and deviates from the real-world computation-aware delay, which in turn makes the latency measurement in Equation 2 unreliable.\nTo illustrate this more concretely, let us consider an example. Suppose we have a source sequence consisting of three segments, each representing the phrase \"one Mississippi,\" which takes approximately one second to say. Thus, X = [X1, X2, X3] consists of three speech segments, each lasting one second (T1 = T2 = T3 = 1 s). The system decides to generate output after processing x1, and it takes 0.5 second to generate each token. For an English-to-German translation, the system generates the translation \"ein\" at the timestamp of 1.5 seconds and \"Mississippi\" at 2 seconds.\nIntuitively, after processing all the source segments, the total delay for generating the subsequent tokens y5 and y6 should be $\\sum_{k=1}^{3} T_k + 0.5 = 3.5$ seconds and 4 seconds, respectively. However, with the current method, the computation timestamps Ci for the third segment are calculated with accumulated computation costs at previous steps. The generation times Ci for the fifth and sixth tokens would be 2.5 seconds and 3 seconds, resulting in delays of 5.5 seconds (3+2.5) and 6 seconds (3+3) for the two tokens."}, {"title": "Method", "content": "We propose a revised way of calculating CA delay to fix this problem. We define the inference time Ii as the elapsed time since processing the previous source segment $x_{j-1}$. Formally, this is expressed as:\n$I_i = C_i - C_{\\tau(j)}$, (3)\nwhere $C_i$ is the computation timestamp when generating token $y_i$, and $C_{\\tau(j)}$ is the computation timestamp associated with the reference token $y_{\\tau(j)}$. The index $\\tau(j)$ represents the last token generated before processing the current source segment $x_j$, defined as:\n$\\tau(j) = \\text{max} \\{ i | d_i < \\sum_{k=1}^{j-1} T_k \\}$ (4)\nHere, $d_i$ is the theoretical computation unaware delay at token $y_i$, and $\\sum_{k=1}^{j-1} T_k$ is the cumulative duration of the source segments up to $x_{j-1}$.\nTo represent the accumulated delay effect caused by discrepancies between computation time and source segment durations, we introduce the a buffer $\\beta_j$ corresponding to the source segment $x_j$:\n$\\beta_j = \\text{max } \\{ 0, \\beta_{j-1} + I_{\\tau(j)} - T_j \\} ,$ (5)\nwhere $\\beta_{j-1}$ is the buffer from the previous source segment $x_{j-1}$ that affects the accumulation status while processing segment $x_j$, and $T_j$ is the duration of the current source segment $x_j$. The buffer accumulates any excess inference time that is not covered by the source segment durations.\nFinally, the computation-aware delay $d_i$ for token $y_i$ can be defined as:\n$d_i = \\beta_j + I_i + \\sum_{k=1}^{j} T_k,$ (6)"}, {"title": "Experiment", "content": "To assess the effectiveness of our proposed modification in aligning with the real-world performance of SimulST systems, we simulated a streaming speech translation task using a 25-second input. The average completion time was 27,020 ms\u00b9. The SimulEval recorded last token's CA* delay calculated by our proposed method was 26,311 ms, resulting in a difference within 2%. In contrast, the last token CA delay without our modification was 39,602 ms, resulting in a difference of 46.6%.\nDue to the incorrect calculation of computation-aware delay, the computation-aware latency deviates from the real latency, and this deviation increases as speech length increases, as shown in Figure 1. However, even on pre-segmented short speeches, the deviation caused by CA delay is still large.\nWe use the MuST-C v1.0 En-De tst-COMMON (Di Gangi et al., 2019) to examine such effects. We utilize a Conformer-based encoder-decoder offline ST model combined with the AlignAtt policy (Papi et al., 2023a), which relies on cross-attention to determine whether to emit translated words or wait for additional information.\nWe identify severe CA computation issues with input lengths of 25 seconds. As illustrated in Tables 1 and 2, even for pre-segmented speech averaging 5 seconds in length, the discrepancies in both Average Lagging (AL) and Length-Adaptive Average Lagging (LAAL) are greater than 300 ms."}, {"title": "Conclusion", "content": "In this work, we investigated the shortcomings of current latency evaluation metrics for SimulST systems, focusing on the discrepancies caused by computation-aware delay miscalculations. We demonstrated that these inaccuracies lead to unrealistic latency estimates, not only in long streaming speech translation but also in pre-segmented speech. Our proposed modification addresses these misconceptions and aligns the latency calculations more accurately with the SimulST system's real behavior, leading to a more reliable evaluation for both SimulST and StreamingST systems."}, {"title": "Limitations", "content": "This work introduces CA* to improve the accuracy of latency evaluation for SimulST systems by addressing computation-aware delay. However, other limitations remain. Notably, oracle delay approximations may inaccurate, especially for un-segmented streaming speech, which includes long pauses and varied segments. Future work should refine these approximations to reduce errors.\nFurthermore, this study focused on AL and LAAL. Although CA* can be generalized to other metrics, additional evaluation is required to confirm its effectiveness across various latency measures.\nWe also used ChatGPT for grammar revisions."}]}