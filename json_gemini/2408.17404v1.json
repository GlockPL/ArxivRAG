{"title": "Getting Inspiration for Feature Elicitation: App Store- vs. LLM-based Approach", "authors": ["Jialiang Wei", "Anne-Lise Courbis", "Thomas Lambolais", "Binbin Xu", "Pierre Louis Bernard", "G\u00e9rard Dray", "Walid Maalej"], "abstract": "Over the past decade, app store (AppStore)-inspired requirements elicitation has proven to be highly beneficial. Developers often explore competitors' apps to gather inspiration for new features. With the advance of Generative AI, recent studies have demonstrated the potential of large language model (LLM)-inspired requirements elicitation. LLMs can assist in this process by providing inspiration for new feature ideas. While both approaches are gaining popularity in practice, there is a lack of insight into their differences. We report on a comparative study between AppStore- and LLM-based approaches for refining features into sub-features. By manually analyzing 1,200 sub-features recommended from both approaches, we identified their benefits, challenges, and key differences. While both approaches recommend highly relevant sub-features with clear descriptions, LLMs seem more powerful particularly concerning novel unseen app scopes. Moreover, some recommended features are imaginary with unclear feasibility, which suggests the importance of a human-analyst in the elicitation loop.", "sections": [{"title": "1 INTRODUCTION", "content": "Requirements Elicitation (or Requirements Development) aims to identify and understand the requirements of a system and the needs of its stakeholders [32]. This process can be implemented using various techniques such as interviews, questionnaires, and observations [14]. Within this process, feature elicitation specifically refers to gathering information to identify and define the system features that should be implemented to fulfill the stakeholders' needs.\nOver the past decade, the popularity of mobile devices has led to a substantial increase in the number of apps available on various app stores. According to Statista, until 2023, Apple's App Store offers 4.83 million apps\u00b9, while Google Play has 2.43 million apps\u00b2. App stores include valuable data that can serve as a source of inspiration for feature elicitation [16, 24, 34].\nConsider, for example, the following scenario: Jay, a mobile app developer, wishes to create a new app for sleep tracking. His initial concept of the app may be quite rudimentary. The envisioned app might require integration with a smartwatch to monitor physiological metrics such as heart rate, breathing patterns, and other related parameters. However, transforming these initial ideas into concrete app features necessitates a cycle of refinement and development. One practical approach for Jay is to examine existing apps in the same domain to identify features that have already been implemented successfully. Recent empirical evidence underscores the prevalence of this strategy. A study by Jiang et al. found that 86.1% of app developers take into account the features of similar apps when developing their own apps [23]. Over the past decade, research has suggested various approaches for AppStore-inspired feature elicitation. Numerous studies have focused on feature recommendation [9, 23, 26, 27, 48] or competitive analysis [4, 10, 44, 47] by mining app descriptions, app reviews, and the user interfaces.\nRecently, researchers also started using Large Language Models (LLMs) for getting inspirations and recommendations for requirements [3]. These advanced models, trained on internet-scale knowledge, enable the automated generation of text, which can be leveraged to create user stories, goal models, and other requirements artifacts. Researchers used LLMs, e.g. ChatGPT, to generate user stories that describe candidate human values providing inspiration to stakeholder discussions [33] or to refine user stories and improve their quality [55]. Others explored the capacity of ChatGPT on generating goal models from a given context description [7, 36]. Overall, LLMs seem capable of boosting efficiency and creativity in requirements elicitation. In fact, this empirical study demonstrates the significant capacity of LLMs for feature elicitation.\nWhile the use of both app stores and LLMs demonstrated promising results for generating new features, there is a limited insight into their effectiveness. This study aims to fill this gap by examining the benefits, challenges, and differences between these two approaches. We focus on feature refinement, a specific task of feature elicitation, which involves breaking down a high-level feature, such as \"health monitoring\" into a list of lower level sub-features like \"sleep tracking\", \"heart rate monitoring\", and \"nutrition logging\". In the LLM-based approach, sub-features are generated directly by prompting GPT-4. The AppStore-based approach involves searching for relevant app descriptions in a vector database, extracting features from these descriptions, and then selecting relevant features. To ensure a fair comparison, we also use GPT-4 for the extraction and selection steps in the AppStore approach.\nTo compare the two approaches, we studied 20 high-level root features, including 10 already existing features and 10 novel features that have not been implemented elsewhere. For each of these 20 root features, we automatically generated two two-levels feature trees: one LLM-based and the other AppStore-based. This resulted in a total of 40 feature trees, including 1,200 sub-features. Each of the 1,200 sub-features was then manually assessed for relation to its super feature, relevance, clarity, traceability, and feasibility. Further, we evaluated the intersection sets and the difference sets of the features generated from the two approaches.\nThis paper's contribution is threefold. First, it presents a detailed comparison between LLM-inspired and AppStore-inspired feature elicitation. Second, it provides insights on how to effectively utilize both approaches. Third, it introduces a tool that integrates both approaches\u00b3. In the following we introduce the LLM- and AppStore-based approaches with corresponding prompts in Section 2 and Section 3. Then, we present our study design in Section 4 and report"}, {"title": "2 LLM-BASED INSPIRATION", "content": "Large language models are AI models designed to understand, generate, and manipulate human language. They are pre-trained for \"next-word prediction\" (next-token) on vast amounts of text data from diverse sources, such as books, articles, websites, and other text repositories, which allows them to understand a wide range of topics [56]. Our implementation of LLM-based approach is accomplished by prompting GPT-4 [37], which is at the time of this research one of the most advanced LLMs. As shown on Figure 3, the model takes a feature as input and refines it to a list of sub-features. To enhance the model performance, Prompt 1 is employed as the system prompt, thereby assigning a specific role to GPT-4\u2074.\nPrompt 1: System prompt\nYou are an expert in mobile app development and requirements engineering. You excel at decomposing high-level features into detailed sub-features.\nThere are two scenarios for feature refinement. In the first scenario, a single feature and its description are provided as input, as illustrated in Figure 1. In this scenario, the approach recommends sub-features based on the information provided about this feature. Prompt 2 takes the feature and its description as input, allowing the model to generate n corresponding sub-features that are formatted as a JSON list to facilitate further processing.\nPrompt 2: LLM refinement of a single feature\n**Feature**\n{feature}: {feature_description}\nGiven the mobile app feature above, please refine it to a list of sub-features.\nEnsure that the number of sub-features is {n}.\nThe output should be a list of JSON formatted objects like this:\n[{ \"sub-feature\": sub-feature, \"description\": description }]"}, {"title": "3 APPSTORE-BASED INSPIRATION", "content": "As illustrated on Figure 3, the AppStore-inspired feature refinement includes three steps: (1) search for relevant descriptions on an app description repository, (2) extract pertinent app features from these app descriptions, and (3) select sub-features from the extracted features.\n3.1 Searching the App Descriptions\nIn this study, instead of relying on the Google Play search engine to find relevant app descriptions, we developed a custom app description search engine. The Google Play search engine, as our tests indicate, suffers from two main issues. First, it often struggles with complex or lengthy queries, frequently returning completely irrelevant app descriptions. Second, the search results are inconsistent and not reproducible, varying with each search attempt. According to the Google Play documentation, the ranking of search results may be influenced by factors such as user relevance, app quality, editorial value, and advertisements as. However, our objective is to acquire the most semantically relevant app descriptions relative to the query.\nOur custom search engine is designed to address these shortcomings by focusing specifically on semantic relevance, thereby ensuring that the retrieved app descriptions are closely aligned with the query. To develop our own search engine, we collected a comprehensive repository of app descriptions. These descriptions were encoded into text embeddings and stored in a vector database, enabling efficient querying, as shown on Figure 4.\n3.1.1 App Description Collection. Given the ID of an app, one can easily get its description with Google Play Scraper\u2076. Since Google Play does not provide a comprehensive list of all available apps, we developed a strategy to collect as many app IDs as possible. Our data collection strategy is divided into two steps:\n(1) We conducted searches on Google Play using each word in an English dictionary as the query. The dictionary comprises 114,769 words, resulting in 114,769 searches on Google Play\n3.1.2 App Description Filtering. To ensure the suitability of app descriptions for our analysis, we employed a filtering process:\n\u2022 Remove games: We focus our work on feature elicitation for regular apps. Previous work suggest that game descriptions tend to be different [17]. We excluded those to prevent potential bias in the results.\n\u2022 Remove non-English descriptions: Despite collecting apps from Google Play in the USA, some descriptions may not be in English. Since our focus is on English-language, we excluded all non-English entries using Lingua\u2079.\n\u2022 Remove too short descriptions: App descriptions that are too short do not provide sufficient information for feature extraction. Therefore, we removed all app descriptions shorter than 200 characters.\nAfter the filtering process, a total of 589,363 apps remained.\n3.1.3 App Descriptions Encoding. The objective of this process is to convert app descriptions into text embeddings to facilitate semantic search. For this purpose, we utilized BGE [53] as the embedding model. BGE, as described by Xiao et al., is a state-of-the-art text embedding model\u00b9\u2070. It is trained through a three-step process: pre-training with plain text, contrastive learning on a text pair dataset, and task-specific fine-tuning. Considering that the maximum input length for BGE is 512 tokens, equivalent to approximately 2000 characters, we initially divided each app description into chunks with a maximum length of 2000 characters. Then, each chunk was encoded with BGE into embeddings of 384 dimensions and stored in our vector database.\n3.1.4 App Descriptions Querying. During the querying phase, the textual query is encoded using BGE, producing a 384-dimensional vector. This query embedding is then employed to retrieve the top-k most similar description embeddings from the vector database. In", "subsections": []}, {"title": "3.2 Extracting App Features", "content": "To ensure a fair comparison, we used a similar system prompt to that of LLM-based approach, with the following additional sentence specifically for app feature extraction: \"Additionally, your expertise extends to extracting app features from descriptions, enabling you to identify key functionalities like \"step count\", \"group chats\", and \"multi-device synchronization\":\"\nThe acquired app descriptions are subsequently processed using GPT-4 in a map-reduce manner. In the single feature scenario (Figure 1), each app description is examined to extract features pertinent to the query feature through Prompt 4. This prompt accepts an app description along with a feature name and its description as input and returns a list of JSON objects. Each JSON object contains the name and description of the sub-features extracted from the app description.\nPrompt 4: AppStore feature extraction\n**App description**\n{app_description}\nFrom the app description above, please extract the sub-features of this following feature.\nEnsure that all sub-features are from the app description.\n**Feature**\n{feature_with_desc}\nThe output should be a list of JSON formatted objects like this:\n[{ \"sub-feature\": sub-feature, \"description\": description }]\nThe resulting JSON objects are subsequently processed to include the \"source-app-id\" field through a Python script, which indicates the app ID from which the sub-feature was extracted. For each app description, we obtain a JSON list where each object contains the fields \"sub-feature\", \"description\", and \"source-app-id\". The prompt, which takes feature with its super feature and sibling features as input (Figure 2) to extract sub-features, is available in the source code of our proposed tool."}, {"title": "3.3 Selecting Sub-Features", "content": "From the previous step, we get k lists of sub-features from the corresponding k app descriptions. The total number of sub-features may easily exceed 20, which would be excessive for inspiration. Additionally, this list often includes duplicates and less relevant items. To tackle this problem, we applied Prompt 5 for feature selection. It merges the k lists of sub-features into one single list"}, {"title": "4 EVALUATION DESIGN", "content": "Our evaluation focuses on the following research question:\nRQ: How good are the generated features and what are the differences between LLM-based and AppStore-based approaches?\nTo answer this question, we prepared 20 app features across various domains (as root features). Each root feature was used separately as input for LLM-Inspiration and AppStore-Inspiration to generate two two-levels feature trees. Subsequently, three authors independently evaluated the quality of all generated sub-features."}, {"title": "4.1 Root Features Preparation", "content": "App developers might aim to implement both existing features from other apps and novel features that have not been previously implemented. To explore both situations, we selected 10 existing features and devised 10 novel features as presented in Table 1."}, {"title": "4.2 Feature Trees Generation", "content": "Once the root features were prepared, we applied the LLM-Inspiration and AppStore-Inspiration approaches to each root feature to generate two-levels feature trees. The only input for the generation of a feature tree is the root feature along with its description. We did not add any additional \"app context\" as input for the tree generation, because the description associated with the root feature represents by itself the app context. For example, the description of the root feature \"Travel Planner\u201d is: \u201cPlan perfect trip from flights to personalized itineraries with this travel app that offers bookings, reviews, and recommendations for restaurants, attractions, and activities.\"\nAll generations of the sub-features were performed automatically without human intervention. To ensure the generated features are assessable, we set the number of relevant app description k to 3 and the number of generated sub-features n to 5. That is, we generated five sub-features for each root feature (L0), and for each sub-feature (L1), five additional (sub-)sub-features (L2). Consequently, each generated feature tree contained a total of 30 sub-features. Finally, we generated a total of 40 feature trees: 20 utilizing the LLM-Inspiration and 20 employing the AppStore-Inspiration."}, {"title": "4.3 Feature Quality Evaluation", "content": "Three authors manually evaluated the quality of all generated sub-features. Each author independently evaluated a total of 40 feature trees: comprising 10 existing and 10 novel root features; for which one tree is generate with LLM- and one with AppStore-Inspiration.\n4.3.1 Feature Node Evaluation. To assess the quality of the generated sub-features, we employed the following evaluation metrics:\n\u2022 Relationship with Super Feature: What is the relationship between the generated sub-feature and its super feature? Is the generated sub-feature truly subordinate to the super feature, or is it instead a sibling feature, a parent feature, an identical feature, or other type of relationship?\n\u2022 Relevance: How closely does the sub-feature relate to its root features? The relevance metric ensured that the generated sub-features were pertinent and logical extensions of its root feature.\n\u2022 Clarity: How well is the sub-feature described? The clarity metric assessed how easily developers could understand and interpret the generated sub-feature descriptions.\nFor sub-features obtained with LLM-Inspiration, we also evaluate their feasibility. We did not evaluate the feasibility of the sub-features generated with AppStore-Inspiration, as these are sourced from existing apps and are thus inherently technically feasible.\n\u2022 Feasibility: Is the sub-feature technically and practically feasible to implement? This metric evaluated whether the generated sub-features were realistic from a technical and practical standpoint (to the knowledge of the evaluator).\nAdditionally, for the sub-features obtained using AppStore-Inspiration, we assess their traceability. The feature extraction and feature selection of AppStore-Inspiration were performed with the help of GPT-4. However, due to the potential hallucination issues associated with GPT [40], there is a possibility that some of these features were not derived from the app descriptions but were instead fabricated by the model. Given the app ID associated with AppStore-Inspired features, we can compare each feature against its original app description to assess its traceability.\n\u2022 Traceability: Does the sub-feature originate from the corresponding app description, or is it a fabrication created by the LLM? This metric evaluated whether the sub-features were extracted from app descriptions.\n4.3.2 Feature Tree Evaluation. In addition to evaluating each feature node, we manually assessed the entire trees, focusing on:\n\u2022 Number of Distinct Features: This metric quantifies the number of distinct features within the generated feature tree. This metric aims to address the issue of duplicated features.\n\u2022 Number of Distinct Relevant Features: Similar as the Number of Distinct Features, but only features with a relevance score of 4 or higher are counted.\n\u2022 Number of Common Relevant Feature of Both Approaches: This metric quantifies the relevant feature that are generated by both approaches.\n4.3.3 Evaluation Protocol. We followed a common content analysis protocol [25] during our evaluation including three steps."}, {"title": "5 EVALUATION RESULTS", "content": "In the following, we analyze the quality of 40 feature trees and their 1,200 recommended features obtained by refining existing and novel features with LLM-Inspiration and AppStore-Inspiration.\n5.1 Relevance\n5.1.1 LLM-Inspiration. As shown on Table 3, the LLM-Inspiration achieved a high relevance score of 4.95 when refine both existing and novel features, underscoring the remarkable capability of LLM in feature recommendation. For instance, the feature \"Laugh Evaluation\" is described as \u201ccontinually tracks the laughs of a user to count its quantity and assesses its authenticity, emotional context, and overall impact on social interactions\". The sub-features recommended for the root feature include \"Laugh Detection\u201d, \u201cAuthenticity Assessment\", \"Emotional Context Analysis\u201d, \u201cSocial Interaction Impact\", and \"Laugh Quantity Tracking\", all of which are highly pertinent to the root feature.\n5.1.2 AppStore-Inspiration. The AppStore-Inspiration also demonstrates high relevance when refining existing features. However, for novel features, the AppStore-Inspiration yielded a relevance score of 3.90, significantly lower than the 4.97 score for existing features (Wilcoxon-Mann-Whitney test p \u2264 0.00). This difference can be attributed to the lack of corresponding relevant features in Google Play. When refining existing features with the AppStore-Inspiration, it can easily identify relevant descriptions from our app description repository and extract features from them as recommended features. In contrast, if a feature is not present in the app description repository, the AppStore-Inspiration will retrieve descriptions that do not fully align with the queried feature. For instance, when refining the root feature \"Laugh Evaluation\" using the AppStore-Inspiration,"}, {"title": "5.2 Relationship with Super Feature", "content": "5.2.1 LLM-Inspiration. As shown on the Table 4, all sub-features recommended by LLM are logically \"sub\" of their super features. Although all sub-features recommended by the LLM-Inspiration are highly relevant to their corresponding super features, we noticed a behavioral difference based on the style of the feature description. When the description of the feature to be refined enumerates a list of functions, such as \"Search, compare, and book flights from various airlines with real-time pricing and availability\", the recommended sub-features may be extracted from this description. These sub-features include \"Flight Search\u201d, \u201cReal-Time Pricing\u201d, \u201cFlight Comparison\", \"Booking Management\", and \"Booking Confirmation and Notifications\".\nContrasting cases are when feature descriptions does not include enumerations as for the \"Random Chat\" feature, where the description states: \"Connect with new people globally or locally using the random chat app, where each launch introduces the user to a fresh virtual pen pal\". The recommended sub-features for this case includes \"Global and Local Matching\", \"User Profiles\", \"Chat Interface\", \"Safety and Moderation\u201d, and \u201cRandom Match Algorithm\u201d, which are not extracted from the root feature description.\n5.2.2 AppStore-Inspiration. When examining the AppStore-Inspiration results, it becomes evident that the relationships between recommended sub-features and their corresponding super-features are not as robust as with the LLM-Inspiration. Although the relevance of the recommended features obtained through the AppStore-Inspiration is generally high for existing features, a discrepancy remains: only 245 out of 300 recommended features are actually \"sub\" of their respective super-features. This issue is even more prevalent with novel features, where only 205 out of 300 recommended features are actually \"sub features\".\nAdditionally, there is a noticeable variation across the different hierarchical levels of features. Specifically, features at L1, which are direct sub-features of the root, have a higher probability to maintain an actual \"sub\" relationship with their super-features compared to features at L2. This can be explained with two main factors:\nGranularity Difference Between Root Features and L1 Features:\nRoot features are typically high-level functionalities such as \"Mental Health Therapy\", \"Travel Planner\", and \"Voice Translation\". These features often represent the main functions of an app. In such cases, most features described in the app description are likely sub-features of the high-level feature. L1 features, such as \"Mini-Therapy\", \"Location-based Soundscapes\u201d, and \u201cLanguage Selection\u201d are more specific and detailed making it challenging to find app descriptions that entirely match them. Features described in the app descriptions may not always be the sub-feature of the L1 feature.\nLack of Detail in App Descriptions: Another factor is the insufficient detail provided in app descriptions regarding low-level features. App descriptions often provide a general overview of interesting features rather than a comprehensive breakdown of all features. This lack of detailed information complicates the extraction of sub-features at a lower level, as these specific details are often omitted from the app descriptions.\""}, {"title": "5.3 Clarity", "content": "5.3.1 LLM-Inspiration. Table 3 shows that the features obtained through the LLM-Inspiration are consistently very clear. We found that both the names and descriptions of the features recommended by the LLM are always succinct and easy to understand. This is unsurprising given GPT-4's strong language generation capacity.\n5.3.2 AppStore-Inspiration. The clarity of the features obtained through the AppStore-Inspiration is only slightly inferior to those derived from the LLM-Inspiration. AppStore-Inspiration generate feature description by rephrasing the sentences from app description. Occasionally, it extracts an uninformative phrase from the app description to serve as the feature description. For instance,"}, {"title": "5.4 Feasibility", "content": "5.4.1 LLM-Inspiration. For the LLM-Inspiration, most recommended sub-features for the existing root features are feasible. However, when refining novel root features, the LLM-Inspiration sometimes recommends infeasible features. The infeasibility can be attributed to two primary factors:\n\u2022 Technological Limitations: Certain features are technologically infeasible. For instance, the recommended feature, \"Thought Interpretation Algorithm\", is described as \"utilizing advanced AI and machine learning algorithms to analyze brainwave data and interpret the user's thoughts\". The feasibility of this feature is rated as low due to the immature state of brainwave translation technology.\n\u2022 Permission Constraints: Certain features are deemed infeasible due to potential violations of user permissions or legal regulations. For instance, the recommended feature \"Offline Interaction Logging\" involves the offline monitoring of user interactions (such as face-to-face conversations and phone calls) raising serious privacy and legal concerns.\n5.4.2 AppStore-Inspiration. The feasibility of the features recommended with AppStore-Inspiration is not evaluated, as they are already successfully implemented by existing apps."}, {"title": "5.5 Traceability", "content": "5.5.1 LLM-Inspiration. Traceability is not evaluated for features recommended with LLM-Inspiration. This limitation arises from the inherent difficulty in distinguishing whether a feature recommended is an original creation of the model or if it has been extracted from its extensive training corpus.\n5.5.2 AppStore-Inspiration. In the AppStore-Inspiration, traceability is generally excellent. Most of the recommended features can be directly traced back to their respective app descriptions. Only a small number of recommended features cannot be linked to the source sentences from the app description. This indicates the capability of GPT-4 to effectively extract features from app descriptions.\nAn interesting observation is that at least ten apps were no longer available on Google Play at the time of our evaluation, which occurred two months after we collected the app descriptions. This did not impact our evaluation of traceability, as we saved the app descriptions in our repository."}, {"title": "5.6 Redundancy", "content": "5.6.1 LLM-Inspiration. Table 5 presents the number of distinct features. As there are 30 recommended features in a feature tree, this finding indicates minimal redundancy within the features. This phenomenon can be attributed to the impressive reasoning capabilities of GPT-4, which enables it to generate sub-features that precisely align with their respective super-feature descriptions. Consequently, the recommended features remains distinct, effectively reducing redundancy and enhancing the granularity of the feature tree.\n5.6.2 AppStore-Inspiration. In contrast, the AppStore-Inspiration exhibits a more severe redundancy problem. For instance, in the tree derived from the \"Anti-Smartphone Addiction\" root feature, the \"Daily App Limit\" feature appears multiple times. Specifically, it is present once at level 1 and three times at level 2 as a sub-feature under \"Customizable Time Restrictions\", \"Screen Time Tracking\u201d, and \"Time Blocking\". We hypothesize that this redundancy stems from the limited variety of features described within the app descriptions,"}, {"title": "5.7 Common and Different Features", "content": "Figure 6 illustrates the average number of common and different (distinct and relevant) features in the feature trees obtained by LLM-Inspiration and AppStore-Inspiration. Irrelevant features are not included in this count. The figure shows that the intersection is small: only 7.4 features when refining existing feature. When refining novel features, the common feature count is only 3.\nThe difference set between the two approaches is even larger than their intersection set. For existing features, the primary reason for this substantial difference is the granularity of the features. Features of different granularity do have an overlap. However, they were not considered as the same feature during our evaluation. While for novel features, the main reason is the variety of solutions, for example, when refining the \"Though Reading\" feature, LLM-Inspiration tends to do it by \"Brainwave detection\", while AppStore-Inspiration proposes \"Judge by body language\". These two reasons explain most of the differences for both existing and novel features. In the following, we discuss additional reasons specific to each approach."}, {"title": "6 DISCUSSION", "content": "6.1 LLMs vs. App Stores for Feature Inspiration\nWhile both approaches seem to be able to recommend relevant sub-features in most cases, upon comparing LLM-Inspiration and AppStore-Inspiration, we found that LLM-Inspiration to be more powerful. The sub-features recommended by LLM-Inspiration are highly relevant to their corresponding super features even for novel app scopes. Moreover, they are consistently logical extensions of their super features. Most recommended sub-features are feasible, even when refining novel features. But some seem imaginary, which suggests the importance of a human-analyst in the elicitation loop [2, 49]. We think that LLM-Inspiration can support or partially replace humans in the feature refinement task particularly for preliminary iterations.\nIt is important to note that the LLM-Inspiration is likely sensitive to the description of the super feature, suggesting that practitioners may need to experiment with and adjust the description to achieve optimal results. We did not study the impact of the feature description (quality) on the generated trees [35]. It is, e.g., likely that short-/long or redundant/varying descriptions, as well as descriptions pointing to a solution or a technology will impact the recommended sub-features. In the future, researchers may investigate this impact in developer studies and benchmarking experiments.\nThe sub-features recommended by AppStore-Inspiration exhibit high relevance when refining high-level and existing features too. However, when it comes to low-level or novel features-a more advance brainstorming and reasoning task-the recommended features may not always logically align as \"sub\" of their respective super features. These sub-features require filtering and editing before reuse. Despite this issue with relevance, a tool supporting feature elicitation may still recommend interesting cross-domain"}, {"title": "6.2 Tool Support", "content": "We have implemented our LLM-Inspiration and AppStore-Inspiration within piStar [39], a goal modeling tool, to facilitate the adoption, as shown on the Figure 7. Goal models, such as KAOS [41] and i* [54], are well-known in requirements engineering. The goal model is constructed by asking \u201cwhy\u201d and \u201chow\u201d questions starting from a root node. The \"how\" question will derive sub-goals, which is very similar to the feature refinement process discussed in this paper."}, {"title": "6.3 Threats to Validity", "content": "This section discusses potential threats to the validity of our study.\nLimitated Number of App Descriptions. There are more than 2.43 million apps on Google Play. However, we have been only able to collect 849 k apps from this largest app store. This limitation could potentially affect the results obtained by the AppStore-Inspiration. To mitigate this issue, we ensured that all evaluated existing features are actually present in our dataset, and any novel features included in our study do not currently exist on Google Play. These steps helped to validate our results despite the smaller set of apps, ensuring that the conclusions drawn remain robust and reliable.\nSelection of Root Features. In comparison to the nearly infinite number of app features, the 20 root features used in our evaluation may seem limited. This limitation arises from the considerable manual effort required, as evaluating each root feature required the manual assessment of 60 sub-features from both approaches by three authors. This necessitates a balance between the feasibility of labeling (i.e. needed effort) and the sample size. To maximize the generalizability of our study, we included 10 novel features from 10 different app categories and 10 existing features from 10 different app categories. Additionally, we evaluated not only the 20 root features but also all the 200 sub-features derived from them using both approaches. Therefore we believe that our evaluation covers a fairly broad and representative scope. Certainly, replicating the study with other types of features and from other domains will further strengthen the generalizabiltiy of the results.\nSubjectivity in Manual Evaluation. As for every manual research task, subjectivity and potential observer bias might lead to variations in how different evaluators interpret and assess the generated features. To mitigate this potential threat, we (1) created an evaluation guide including a well defined semantic scale to detail the definition of each score with examples, (2) evaluated each generated features independently by three evaluators, and (3) reviewed the final scores and labels through discussion and consensus. Overall, the evaluators, who possess five or more years of software development experience, did not think that the evaluation of generated features was a complex task. This is also reflected in the fairly high rate of achieved agreements. Nonetheless, it is important to focus on the comparative trends when interpreting our results rather than the exact scores.\nMaturity of Implementation. It is inappropriate to compare the speed of a sports car with that of a steam train and conclude that the car is faster. Similarly, it is hard to compare the performance"}, {"title": "7 RELATED WORK", "content": "7.1 App Store Mining for Requirements\nAs shown by Ferrari and Spolitini [16], app stores serve as an important source for inspiring requirements elicitation. App stores contain various data, including app descriptions, app reviews, and app images. We summarize existing work in these areas.\n7.1.1 Mining App Descriptions. App descriptions, composed by the application developers and vendors themselves, provide a succinct introduction to the salient features of the respective apps. Recent studies have sought to mine these app descriptions in a variety of manners. This includes the identification of similar apps by analyzing their respective descriptions [1, 21, 47], and the extraction of app features from the app descriptions [24]. The extracted features can be used to construct domain knowledge [28]. In addition, these features serve as a basis for recommending requirements, as evidenced by several studies [23, 26, 27, 29]. Our work aims at comparing app mining approaches to recent general purpose LLMs.\n7.1.2 Mining App Reviews. Reviews on app stores provide valuable insights from users, for example, the feature requests or bug reports, making them a valuable resource for requirements elicitation [18, 38]. Given the vast volume of app reviews, researchers have introduced numerous techniques to enhance the efficiency of their analysis. These techniques encompass the automatic classifications of app reviews into predefined category such as bug reports and feature requests [11, 31, 45, 50, 51]. Additionally, these methods employ clustering algorithms to assemble app reviews based on semantic similarity [12, 43, 46, 51], and also involve the generation of concise summaries of app reviews [12, 13, 22, 51]. These techniques are complementary to AppStore- and LLM-Inspiration as they bring the perspective and creativity of end users.\n7.1.3 Mining App Introduction Images. The app introduction images on Google Play are a gold mine for the inspiration of app design, particularly the Graphical User Interface (GUI), as they are carefully selected by app developers to represent the important features of the apps. Recent researches mines the app introduction images and proposed GUI search engines, such as Gallery D.C. [8, 15], and GUing [52], to facilitate the search of existing app UI designs using textual queries. Recently, Wei et al. discussed how"}, {"title": "7.2 LLMs for Requirements Elicitation", "content": "Particularly since the release of ChatGPT, numerous studies have investigated the capacity of large language models (LLMs) for facilitating requirements elicitation. For instance, Ronanki et al. [42", "19": "used LLMs for generating requirements elicitation interview scripts, demonstrating the model's efficacy in enhancing the quality of these scripts. Cabrero-Daniel et al. [6", "33": "applied ChatGPT to generate human-value user stories, thus providing inspiration for new requirements. In a similar vein, Zhang et al. [55", "5": "developed multiple agents based on GPT-4, which facilitated the exploration of a broader range of user"}]}