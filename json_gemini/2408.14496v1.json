{"title": "A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models", "authors": ["Dibaloke Chanda", "Milan Aryal", "Nasim Yahya Soltani", "Masoud Ganji"], "abstract": "Recent advances in deep learning have completely transformed the domain of computational pathology (CPath), which in turn altered the diagnostic workflow of pathologists by integrating foundation models (FMs) and vision-language models (VLMs) in their assessment and decision-making process. FMs overcome the limitations of existing deep learning approaches in CPath by learning a representation space that can be adapted to a wide variety of downstream tasks without explicit supervision. VLMs allow pathology reports written in natural language to be used as a rich semantic information source to improve existing models as well as generate predictions in natural language form. In this survey, a holistic and systematic overview of recent innovations in FMs and VLMs in CPath is presented. Furthermore, the tools, datasets and training schemes for these models are summarized in addition to categorizing them into distinct groups. This extensive survey highlights the current trends in CPath and the way it is going to be transformed through FMs and VLMs in the future.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years there has been a surge of artificial intelligence (AI) based approaches [1]\u2013[18] in CPath owing to wide adoption of digital slide scanners. As a result, large-scale curation and annotation [19], [20] of whole slide images (WSIs) has been made possible which ensured adequate data to train these AI-based models. The goal of these AI-based models is to automate and expedite the diagnosis and prognosis process of CPath. The traditional diagnosis process in CPath is time-consuming and requires experts with extensive domain knowledge. In addition, the wide variation of pathology and heterogeneity between tasks makes it difficult to come up with a unified general approach.\nSeveral research studies have addressed this lack of a unified approach and among the proposed methods, the FMs have gained a lot of attention in recent years [21]\u2013[26]. FMs leverage self-supervised learning (SSL) [27] schemes to learn a rich representation in a task-agnostic manner. Owing to self-supervised pre-training (SSPT), FMs do not require large-scale annotated data which is hard to come by in CPath. Furthermore, these models can be trained with a diverse selection of datasets containing tissue samples from different organs and associated with different cancer types, scanner types, etc. As a result, the resultant pre-trained model can easily be utilized in a wide range of downstream tasks while maintaining robustness to extreme variation in tissue samples."}, {"title": "A. Scope of the Review", "content": "In this review, the main emphasis is put on the application of FMs and VLMs in CPath, especially the details of their architectures and training schemes. Note that these two categories are not mutually exclusive, meaning that some research articles belong to both categories which are vision-language foundation models (VLFMs). In addition, details of multimodal datasets are summarized with a focus on vision and language as the modalities. Several self-imposed rules and restrictions were used as guidelines throughout the review to ensure the scope of the paper is maintained.\n1) First, articles that focus solely on pathology are included and articles that focus on other areas of the biomedical domain are excluded. As an example, articles like BiomedCLIP [30] with pathology as a subsection of the research are not included in the review.\n2) Secondly, only vision-language models are included and articles that use other modalities along with vision are excluded. As an example, along with vision, transcriptomics [31] can be used to solve pathological tasks. However, such papers are excluded to maintain the scope.\n3) Both peer-reviewed articles and pre-print articles are included in the survey. Among the peer-reviewed articles, a significant number of articles are published in top-tier journals and conferences as shown in Table I."}, {"title": "B. Contribution and Organization", "content": "In the past few years, quite a few review articles [1]\u2013[18] have been published focusing on computational pathology. Most of these reviews include articles in the digital and computational pathology domain and the application of deep learning as a whole rather than focusing on a specific subtopic. There are a handful of papers that focus on specific architecture like MIL [32], graph-based models [17], [33], transformer-based models [10], [13], LLMs [18], etc. The contribution of this review article is mentioned below:\n1) To the best of our knowledge, this is the first review article to summarize recent advances (most articles from 2023-2024) in foundation and vision-language models (Section III and IV). The closest peer-reviewed survey article [8] mostly provides a high-level overview without going into the details of foundation models.\n2) An exhaustive list of multi-modal datasets (Section II) in computational pathology that are being used or can be used in vision-language research is outlined.\n3) Given the diverse datasets and architectures, descriptions for individual research are provided in tabular format (Table II, Table IV, Table V, Table VIII) so it is easier for readers to follow.\n4) A categorized and annotated timeline of the survey articles (Fig. 3) which provides a clear idea of the evolution of the FMs and VLMs in CPath.\nThe rest of the article is organized as follows. In section II existing multi-modal datasets are listed along with details about their source, pre-processing techniques, etc. In section III existing FMs in CPath are outlined and descriptions for the vision and vision-language pre-training schemes for these FMs are provided. In section IV an extensive list of VLMs in CPath is provided along with details about their architecture, utilized datasets and contribution. Finally, in section V the paper is concluded."}, {"title": "II. MULTI-MODAL DATASETS IN PATHOLOGY", "content": "In this section, a comprehensive summary of the existing multi-modal datasets available in CPath is provided. The modalities taken into account are vision and language. The one article that has additional modalities is CR-PathNarratives which is included because it was referenced in many other VLMs articles. All key information regarding each dataset is summarized in Table II.\nPath [26] fall into the last category which is dissimilar to the rest of the datasets. In CR-PathNarratives, WSIs are associated with special annotated additional modalities. The additional modalities are voice and behavioral trajectory information of the pathologists who annotated the dataset. These additional modalities were collected by using the PathNarrative multimodal interactive annotation tool proposed in the same research work. The dataset Prov-Path utilizes WSI and the corresponding reports along with histopathology findings, cancer staging, genomic mutation profiles, etc collected by Providence Health System (PHS).\nThe rest of the datasets can be categorized into the remaining four categories. The image-caption/ image-text pair category (PathGen-1.5M [37], Quilt-1M [39], OpenPath [40], PathCap [38], ARCH [55]) involves a low-to-medium quality image and an associated piece of text for that image. This text can be a short caption with a description of the image or a more elaborate description. ARCH is the earliest dataset in this category that utilized Pubmed and pathology textbooks to extract the texts. PathGen-1.5 M is the latest and largest dataset in this category, but unlike other datasets in this category, the images are patches extracted from WSIs. This is in contrast to other datasets in this category like OpenPath which is constructed with Twitter posts and replies and Quilt-1M which is constructed with frames extracted from educational pathology videos. However, the curation and annotation process of this category is much easier as it can automated with hand-crafted algorithms and heuristics. But it comes with the trade-off of noisy data as due to the automated process a lot of artifacts can be present in the data.\nThe WSI VQA/text category (PathText [34],WSI-VQA [35], PathQABench [25]) contains question-and-answer pairs or texts associated with WSIs. Comparatively, the generation process of this category is more difficult and often involves prompting LLMs to extract information or format information according to a certain template. The most common data source is the cancer genome atlas (TCGA) [19] which contains a large repository of WSI and patient report pairs. The VQA part can be of two types, close-ended question-answer pair and open-ended question-answer pair. Close-ended question-answer pairs are of a multiple-choice type or short-answer type. On the other hand, open-ended question-answer pairs contain answers that are in natural language form. Among the datasets in this category, PathQABench is unique as it contains ROI annotation of WSIs performed by expert pathologists. It has two subsets PathQABench-Public and PathQABench-Private. The former is publicly available as it was constructed with TCGA WSI and reports, and the latter was constructed with in-house data.\nThe next category, which is VQA (PathMMU [44], PathVQA [58], Quilt-VQA [46]) is similar to the previous category as it also contains close-ended and open-ended question-answer pairs, but the associated images are not WSIs but rather low-to-medium-quality images. Among these datasets, PathVQA is the first research to curate a pathology-specific VQA dataset. PathMMU is the latest and largest dataset in this category and it also provides explainability annotations with each answer. PathMMU utilized two previous datasets from the image-caption/image-text category OpenPath and Quilt-1M to generate a subset of the question-answer pair with images. Another category, which is the instruction-tuning dataset (Quilt-Instruct [46], PathInstruct [37], PathChatInstruct [25], extension of PathGen-1.6M [37]) is unique kind of dataset, as this type of dataset is used to provide conversational ability to an existing multimodal model. The common workflow is that the instruction-tuning data set is applied in the last phase to fine-tune an already trained VLM. All of these datasets were created following the strategy mentioned in LLaVA [41] or LLaVa-1.5 [63]. In Fig. 5 a visual comparison of the size of each dataset in each category is provided.\nThe second component to consider is the source of data which largely dictates the third component, the annotation and pre-processing. PubMed is a common data source containing pathology images and captions/text. However, the quality of the data is not as high as that of the TCGA repository that contains WSIs and corresponding pathological reports. Other high-quality data sources that contain WSIs and pathology reports are in-house proprietary datasets. A unique data source, OpenPath, contains pathology images and texts from Twitter posts and replies associated with a large pathology community.\nThis data set was supplemented by pathology-specific data from the large-scale artificial intelligence open network (LAION) data repository. Pathology textbooks and atlas are also large knowledge sources that can be used to extract image caption/text pairs. In a couple of recent research [39], [46], educational histopathology videos on YouTube are being used as the source of pathology image and text pair. However, curation of this kind of dataset requires a series of hand-crafted algorithms and many external tools.\nBased on the above discussion, it is apparent that there is a trade-off between the quality and volume of the data. Sources like PathQABench are superior in terms of quality, as they were curated by an expert pathologist. However as it requires explicit manual annotation and supervision, it is much harder to scale and hence small in terms of size.\nAnother key point is that most of these datasets contain other datasets as one of the subsets. An example of that is Quilt-1M which contains OpenPath, PubMed and LAION as subsets in addition to the proposed Quilt dataset which is the original proposed dataset. A visualization of Quilt-1M and its subsets is provided in Fig. 6. Another such example is the PathMMU dataset (shown in Fig. 7) which contains 5 different subsets, PubMed, SocialPath, EduContent, Atlas and PathCLS each containing data from different sources.\nThe third component is the annotation and pre-processing steps of data curation and generation. Among the surveyed articles, all employ a series of steps depending on the type of dataset and if the entire annotation and pre-processing pipeline is compared every article is unique. However, some specific steps are similar if the data source is the same. For example, all articles that utilize PubMed as a data source use some kind of parsing process [56] to parse and extract figures and texts. In addition, they employ light-weight classifiers and object detection architectures (YOLOv5 [60], YOLOv6 [45], YOLOv7 [51]) to distinguish between pathology and non-pathology images, detect and separate subfigures, etc. Another common approach is to prompt LLMs to format and refine captions/text or structure extracted information according to a pre-defined template. These LLMs include generalized LLMs like GPT-4, GPT-3.5, ChatGPT and also specialized LLMS like BioGPT. Another widely used strategy is using a trained CLIP-based model and using cosine similarity as a metric to classify pathology and non-pathology images.\nApart from the approaches mentioned above, there are a lot of other hand-crafted algorithms, heuristics and tools which are summarized in Table II."}, {"title": "III. FOUNDATION MODEL", "content": "In this section, an overview of existing FMs in CPath is provided. First, the characteristics of FMs are provided to remove any ambiguity for the later sections. Next, pre-training workflow and typical pre-training schemes are mentioned.\nA. Characteristics of FMs\nA model can be classified as FM if it holds the following characteristics :\n1) The first characteristic that is common to all FMs is the SSPT. The data used in the pre-training phase do not have any explicit label or annotation. Specifics of the SSL strategies are mentioned in section III-B.\n2) The training goal of FMs is not to solve any specific task but rather to learn a general and rich representation space. For VFMs, it is a vision representation space and for VLFMs it is a vision-language representation space. The training of FMs is termed as \"pre-training\" as in later stages further training is required to optimize for a specific task.\n3) In CPath, FMs are trained using large and diverse datasets that encompass tissue samples from different organs and anatomic sites. In addition, some research [64], [65] put effort into capturing diversity in terms of scanner types, magnification levels, stain types, preservation methods, etc. The idea is to capture the representation of a wide range of tissue and disease types and also making sure in downstream tasks are robust. In Fig. 8 a high-level visualization is provided highlighting different aspects of FMs.\n4) Another characteristic is the size of models that typically have parameters on the scale of millions. A huge amount of computing resources is put into training these models involving multiple GPUs.\nAs shown in Fig. 3, the FMs are sectioned into three separate categories. One category encompasses VFMs [22] in CPath, the second category encompasses VLFMs in CPath and the last category utilizes these FMs by providing a benchmark [66]\u2013[70], framework [71] or adapting FMs [72]\u2013[77]. Before going into the details of each category in the following section, an overview of the pre-training strategies of FMs is provided."}, {"title": "B. Pre-training Workflow and Strategies", "content": "The typical pre-training workflow of FMs is shown in Fig. 9 which provides a high-level visualization of different phases in the workflow.\nIn this single diagram, both pre-training strategy for VFM and VLFM is shown. For VFMs, a vision module with some initial weight is leveraged in pre-training. The term \u201cvision module\u201d is used to generally represent a wide range of vision architectures and also modified versions of these architectures with additional layers. The most common architecture is variants of vision image transformers (ViTs) [78] which includes ViT-S, ViT-B, ViT-L and ViT-H. Though there are specialized architectures proposed in BEPH [79] and Prov-GigaPath [26] which uses BEiTv2 and GigaPath architecture respectively. Note that, some models initialize the architectures with ImageNet [80] weights to get an initial vision representation space which can be transformed through pre-training. In Table III, a summary of pre-training strategies in different phases is outlined.\nThe term \"unimodal\" is used to signify the pre-training phase utilizing a single modality out of vision and language. This is inherently different from vision-language pre-training strategies which involve both vision and language modalities to learn a joint vision-language representation space. In the unimodal vision pre-training phase there are three strategies commonly used in CPath which are self-distillation, contrastive learning and masked image modeling (MIM) approach. Each approach has its own advantages and disadvantages;\n1) Unimodal Vision Pre-training\nThe self-distillation with no label approach uses a student-teacher network to learn a rich vision representation space as shown in Fig. 10. From a single image patch, two different views are generated by applying an augmentation sampled from a set of possible augmentations (color jittering, Gaussian blur, polarization, etc). The generated output of both the networks is utilized to compute a cross-entropy loss which is then used to update the parameters of the student network. The parameter of the teacher network is then updated through an exponential moving average (EMA) of the student network parameters. Among the surveyed articles Virchow2, Virchow, RudolfV, Hibou and UNI use DINOv2 as the self-distillation approach. PLUTO takes a unique approach by integrating MAE and a Fourier loss term to get a custom variation of DINOv2.\nThe second popular SSPT approach is the MIM (visualized in Fig. 11) which has variants like MAE and iBOT used in FMs. In the MAE approach, randomly selected high portions of the image are masked out and the patches that are not masked out are passed through an encoder which generates latent representations of those patches.\nThen those representations are passed through a decoder along tokens of masked out regions to reconstruct the image and reconstruction loss is used to train the model. The iBOT approach also uses MIM but adds self-distillation technique by leveraging a student-teacher network. The teacher network works as an online tokenizer and the student network learns to predict masked patches with the help of distilled knowledge of the teacher network.\nAmong the surveyed articles 3B-CPath and Prov-GigaPath use MAE but in conjunction with DINO and DINOv2 respectively. Research works utilizing iBOT approach include Phikon and CONCH.\nAnother SSPT approach that is comparatively less popular in FMs for CPath is the contrastive learning framework proposed in MoCo [104] (visualization in Fig. 12). Over the years variations of the proposed approach in MoCo in terms of architectural change and training blueprint have been done and MoCov2 [105] and MoCov3 [83] are the results of that.\nLike the self-distillation approach, MoCo also utilizes two models; one is an encoder (with query patch as input) and the second one is a special momentum encoder (with key patches as input). The embeddings generated through the encoder and the momentum encoder are used to compute a similarity score which in turn is used for contrastive loss computation."}, {"title": "C. Instruction-Tuning Phase", "content": "For CPath, the instruction-tuning phase provides the model with conversational ability i.e. a user can prompt the model and the model will respond according to the prompt. Visual-instruction tuning adds the ability to provide an image in addition to natural language user prompts. Among the surveyed papers in FMs, PathChat and PathAsst perform instruction-tuning with self-curated datasets PathChatInstruct and PathInstruct, respectively. Both works adopt the strategy employed by LLaVA and LLaVa-1.5 which are the pioneering work in visual instruction tuning. A summary of the instruction tuning process is provided in Table V. For both PathChat and PathAsst, the instruction tuning phase is subdivided into two phases. For both models in the first phase, the parameters of the vision encoder are frozen and the layer/module connected to the vision encoder (for PathChat it is a multimodal projector, for PathAsst it is fully connected layers) is trained. In the second phase, the model is fine-tuned with instruction-following data. Even though instruction tuning in CPath is relatively new, some recent works like PathInsight [107] specifically focus on this aspect of FMs. Another recent work CLOVER2 [108] provides a framework for cost-effective instruction learning in pathology."}, {"title": "D. Downstream Tasks", "content": "FMs have the ability to adapt to a vast array of tasks by utilizing the representation space learned during SSPT.\nNote that, in the pre-training phase FMs were never trained for any of these specific tasks. At the end of the pre-training of FMs, one of the following strategies is adopted to perform a specific task.\n1) Linear Probing: This is a commonly used technique where a linear classifier/regressor is trained on top of the pre-trained model. During the training of the linear layers, the parameters of the pre-trained model are kept frozen. Depending on the specifics of the tasks, the corresponding loss function and update rule are determined. This is a computationally cheap way to adapt to a downstream task as the parameters of the pre-trained model do not need to be updated.\n2) KNN Probing: This is yet another approach to adopt a foundation model for a specific downstream task by utilizing K-nearest neighbors algorithm.\n3) Fine-Tuning: This is similar to linear probing as a classifier/regressor is added on top of the pre-trained model, but the major difference is the parameter of the pre-trained models is also updated during fine-tuning.\nHence, it is computationally much more costly compared to linear probing. This is sometimes also referred to as the supervised training phase. Note that, most of the time annotation is only available on slide-level. Hence, if the slide-level label is utilized it is called weakly supervised training.\nIn Table VI, a summary of downstream tasks is provided along with research works performing these tasks.\nAnother aspect to consider is how the performance evaluation is conducted. Among the surveyed articles, there are three different strategies that are employed.\n1) Zero Shot Evaluation: In zero shot evaluation, the pre-trained model is directly used in a downstream task without probing or fine-tuning the pre-trained model with any samples of the downstream dataset. This provides a direct assessment of learned representation in the pre-training phase i.e. evaluates the quality of the generated embeddings from the pre-trained model. This is the most common approach among the surveyed articles.\n2) Few Shot Evaluation: In few shot evaluation, the pre-trained model sees only a few examples from the downstream task dataset.\n3) Simple Shot Evaluation: This is a unique variation of the few shot evaluation methods which is only been explored in UNI."}, {"title": "E. Framework, Benchmarking and Adaptation of FMs", "content": "There are several research works in CPath that do not directly propose a FM but introduce the framework of FMs, provide benchmarks and comparisons between FMs and adapt the FMs for efficient training.\n1) Frameworks:\n eva [71], [142] is a framework for VFMs in CPath which abstracts a lot of complexity of VFMs. In addition, it facilitates the reproducibility of VFMs for fair comparison and provides an interface to evaluate publicly available downstream datasets. Note that, this is an ongoing work and more downstream datasets and models are being added.\n2) Benchmarking:\nAnother category is benchmark analysis of FMs [67], [66], [143] in CPath. The work done in [67] analyzes different FMs and evaluates their performance on 8 datasets. The other work [66] is a more recent work that analyzes the performance of FMs on a large and diverse data set collected from two medical centers. The FMs were benchmarked on 3 broad downstream tasks which include disease detection, biomarker prediction and treatment outcome prediction.\n3) Adaptation of FMs:\nAnother category is the adaptation of existing FMs to carry out tasks such as low-resource fine-tuning [72] and multimodal prompt-tuning [73]. The work carried out in [72] fine-tunes a FM with a single GPU and shows that it can outperform SOTA feature extractors. PathoTune [73] adapts a visual or pathological FM to downstream tasks using multimodal prompts.\nOther than the surveyed FMs, there are more recent works of FM in CPath like mSTAR [144]. However, as it includes RNA-Seq data in addition to pathology reports it falls outside the scope of this paper. Another foundation model we do not include is H-optimus-04 [145] developed by Bioptimus as it is not associated with a publication."}, {"title": "IV. VISION-LANGUAGE MODELS", "content": "In this section, the recent works in CPath with VLMs are outlined. The details about architecture, used datasets and contribution of individual research work are listed in Table VIII.\nFirst, different categorizations of VLMs are provided to give insight into how VLMs are utilized to solve pathology-specific tasks. Second, the common architectural components in VLMs and adopted strategies are summarized. Lastly, a brief overview of models that do not solve a direct pathology task but perform other types of vision-language tasks is summarized.\nA. Categorization of VLMs\nA categorization of VLMs in CPath can be done based on the reason for using the language modality. Some research works solve tasks like caption generation ( [154], [190]\u2013[192], [198]) or VQA ( [35], [146]) which necessitates utilization of both vision and language modality because of the nature of tasks. For VQA or caption generation, the generated output from the model needs to be in language form i.e. the model needs to perform a language task. On the other hand, other research works use language modality as a source of semantic information to be injected into information gained from vision modality. This additional semantic information can significantly boost the performance of the model which would not have been possible with a vision-only model. As an example, MI-Zero [62] performs cancer subtyping which is not a language task but utilizes pathology reports curated from the in-house dataset and TCGA as a source of semantic information.\nAnother categorization can be done by comparing the training approach with FMs. VLFMs are different from VLMs as traditional VLMs focus on solving one or two vision-language tasks like caption generation. However, as FMs become more prevalent recent works are starting to shift towards VLFMs. Recent works (PathGen-CLIP [37], Quilt-Net [39]) take FM-like approach to train their models and adapt them to different downstream tasks. On the other end of the spectrum, there are VLMs that solve only a single task. As shown in Fig. 17 VLMs and VLFMs should be viewed as a spectrum where if the number of tasks and pre-training data size increases it moves away from VLMs and towards VLFMs.\nOwing to this, some VLMs follow the FM approach of pre-training the vision or text module. However, as pre-training from scratch requires a large amount of data and computing some VLMs use domain-specific vision modules from HIPT, CTransPath, IBMIL, etc or language modules from PubMedBERT, BioClinicalBERT, etc. This allows the vision or language module to learn an initial vision or language representation space which might not be as rich as pre-training scratch but boosts the performance.\nAnother categorization can be done by analyzing how language prior is incorporated into the model. Most works use text labels or captions curated from different sources, which provide the least amount of information. Recent works like FSWC [150], ViLA-MIL [151] and PEMP [189] utilize prompting LLMs to increase the amount of semantic information that can be gained from text labels or captions. They prompt the LLMs to generate descriptions about a particular class label or of morphological or textural patterns about patches. On the other end of the spectrum is research work utilizing all information from pathology reports which contain a lot of details. However, as shown in Fig. 18 the difficulty of curation and doing it at a scale to match the pathologist level also increases.\nEven though there is a huge variety of vision and language modules used in VLMs, it is possible to identify common structures. In Fig. 19 the common architectural components used in different stages are outlined. In the pre-processing stage, LLMs like GPT-4, GPT-3.5, GPT-3.5-turbo and GPT-3 are used to clean and refine captions or prompting them to generate descriptions. Examples of research work utilizing LLMs in the pre-processing phase include HistGen, FSWC, VILA-MIL, FiVE, CPLIP and PromptBio.\nAnother key component of the architecture is the vision module or encoder which converts the WSI patches to image embeddings. As shown in Fig. 19 it can be separated into three separate groups. The first group is vanilla CNNs that have ImageNet initialization or some kind of pre-training on pathology data. Note that most VLMs utilizing these architectures are not recent works (with the exception of Elbedwehy et al. [192]). Recent VLMs utilize ViTs which is the second group containing several ViT variants. The third group is special encoders that are pathology domain-specific and optimized for encoding WSI patches. These are ideal for learning a rich representation without the cost of pre-training from scratch.\nFor language encoders, most works utilize BERT (PubMedBERT, BioClincialBERT) or GPT (BioGPT, GPT 2) variants and most are pre-trained on datasets from the biomedical domain.\nAnother type of language module is the caption/text generation module which rather than encoding text into embedding, generates text sequences. Earlier works included RNN, LSTM and Transformer decoders and recent works include LLMs. The most common vision-language alignment or fusion module is the CLIP model and its variations. However, some works come up with custom approaches to align vision and language representations."}, {"title": "C. Related works for VLMs in CPath", "content": "There are several research works that do not directly utilize VLMs to solve a task in the pathology domain, but investigate or use VLMs for other reasons. One example is Thota et al [201] which investigates the effect of Projected Gradient Descent (PGD) adversarial perturbation attack on PLIP [40] architecture. Another work by Lucassen et al. [202] provides a pathology report processing workflow that can be used for VLMs in CPath.\nAnother category of VLMs in CPath that does not directly solve pathology-specific tasks is text-guided diffusion models for image generation. However, these models can perform tasks like virtual stain transfer which can later be used to solve a pathology-specific task. Another use case is extending dataset size with generated synthetic images. One such work is PathLDM [203], that performs text-to-image generation on the TCGA-BRCA dataset."}, {"title": "V. CONCLUSION", "content": "In recent years the number of research works in CPath with FMs and VLMs has increased significantly which provides an indication about how CPath will evolve in the next couple of years. Many of these works put significant computing resources into training FMs on massive pre-training datasets or coming up with novel strategies to push more language prior knowledge into VLMs. In the near future, the VLFMs which bring together the benefits of both FMs and VLMs, are going to be the dominant model. This review article provides a comprehensive overview of all these models which will aid future researchers."}]}