{"title": "ON THE UTILITY OF EQUIVARIANCE AND SYMMETRY BREAKING IN DEEP LEARNING ARCHITECTURES ON POINT CLOUDS", "authors": ["Sharvaree Vadgama", "Mohammad Mohaiminul Islam", "Domas Buracas", "Christian Shewmake", "Erik J. Bekkers"], "abstract": "This paper explores the key factors that influence the performance of models working with point clouds, across different tasks of varying geometric complexity. In this work, we explore the trade-offs between flexibility and weight-sharing introduced by equivariant layers, assessing when equivariance boosts or detracts from performance. It is often argued that providing more information as input improves a model's performance. However, if this additional information breaks certain properties, such as SE(3) equivariance, does it remain beneficial? We identify the key aspects of equivariant and non-equivariant architectures that drive success in different tasks by benchmarking them on segmentation, regression, and generation tasks across multiple datasets with increasing complexity. We observe a positive impact of equivariance, which becomes more pronounced with increasing task complexity, even when strict equivariance is not required.", "sections": [{"title": "1 INTRODUCTION", "content": "The inductive bias of weight sharing in convolutions, as introduced in LeCun et al. (2010) traditionally refers to applying the same convolution kernel (a linear transformation) across all neighborhoods of an image. To extend this to transformations beyond translations, Cohen & Welling (2016) introduced Group Equivariant CNN (G-CNNs), adding group equivariance properties to encompass group actions and have weight-sharing across group convolution kernels. G-CNN layers are explicitly designed to maintain equivariance under group transformations, allowing the model to handle transformations naturally without needing to learn invariance to changes that preserve object identity. Following this work in the spirit of 'convolution is all you need' (Cohen et al., 2019), several works emerged like (Ravanbakhsh et al., 2017; Worrall et al., 2017; Kondor & Trivedi, 2018; Bekkers et al., 2018; Weiler et al., 2018; Cohen et al., 2019; Weiler & Cesa, 2019; Bekkers, 2019; Sosnovik et al., 2019; Finzi et al., 2020). Advancements in geometric deep learning have demonstrated the effectiveness of incorporating geometric structure as an inductive bias, which reduces model complexity while enhancing generalization and performance (Bronstein et al., 2021). Thus, incorporating group structure into neural networks has become a promising area of research.\nHowever, there is a growing debate as to whether group structure is overly restrictive and if similar advantages could be obtained by simply adding more data. To address this question, we thoroughly investigate the impact of equivariant versus non-equivariant layers in various computational tasks. We explore the balance between leveraging group structures that can provide powerful inductive biases and maintaining the inherent model flexibility. We implemented a convolutional architecture in which the linear layers are either classical or group convolution layers, whilst the overall architecture remains otherwise identical for fair comparison. We evaluate this model on tasks of varying complexity and the extent to which equivariance is desirable under the task description. Our goal is to understand how these design choices affect performance, generalization capabilities, and computational efficiency."}, {"title": "2 BACKGROUND", "content": "In this section, we begin by explaining how equivariant neural networks differ from non-equivariant ones, focusing on their architectural distinctions and the impact of weight-sharing-or the lack thereof-on data efficiency. We then delve into the specifics of 3D convolutions and separable group convolutions, which inform the design of the architecture presented later."}, {"title": "2.1 ARGUMENTS FOR AND AGAINST EQUIVARIANCE", "content": "Equivariant neural networks differ from standard networks in four key aspects:\n1. High-dimensional representation spaces: They typically represent data on higher-dimensional feature maps, allowing for richer internal representations.\n2. Constraint layers: They employ constrained linear layers that are equivariant to specific transformations, which, while less flexible, preserve important structural properties of data.\n3. Weight-sharing: These constraints not only prevent overfitting to nuisance variables (e.g. arbitrary rotations) but also introduce a form of weight-sharing that could benefit learning.\n4. Data efficiency: Equivariant architectures are data efficient, as they do not need to learn patterns repeatedly for different transformations (poses) under which they may appear.\nWhen comparing equivariant networks such as group convolutional CNNs (GCNNs) to standard CNNs, we observe that: (1) the higher dimensionality could possibly be offset in a standard CNN by increasing hidden dimensions; (2) the equivariance constraint may disadvantage equivariant methods in settings where such constraints are not crucial; however, (3) the induced weight-sharing and (4) improved data efficiency might compensate for these limitations by enabling more effective and efficient learning. So, there are arguments against (1-2) and in favor of G-CNNs (3-4).\nThis paper explores the scenarios in which G-CNNs outperform standard CNNs. We hypothesize that equivariant networks offer advantages not only in tasks explicitly requiring equivariance but also in challenging tasks where strict equivariance is not necessary."}, {"title": "2.2 INTRODUCTION TO EQUIVARIANT NEURAL NETWORKS", "content": "To address the above arguments we first provide a high-level introduction to group equivariant convolutions in this section and provide the technical details in the next Section 2.3.\nLinear Layers and Equivariance Consider a vanilla neural network of the form:\nNN(x) = [\\sigma^{(L)} \\circ L^{(L)} \\circ ... \\sigma^{(2)} \\circ L^{(2)} \\circ \\sigma^{(1)} \\circ L^{(1)}] (x),\nwhere $L^{(l)}$ are linear layers and $\\sigma^{(l)}$ are element-wise activation functions. The input $x$ can represent various data structures, such as images in $R^{X \\times Y\\times C}$ or graphs in $R^{N\\times C}$. In such an architecture\u2014as well as in modern variants, the bulk of the computations are done through linear transformations L, which form features as linear combinations of input patterns. For structured data, these linear transformations are designed to be equivariant to preserve data structure. For instance, graph networks require permutation equivariance, while image processing networks demand translation equivariance. These constraints result in convolution operators which are efficiently implementable via sparse and parallelized operations."}, {"title": "2.3 EQUIVARIANT LINEAR LAYERS (CONVOLUTIONS)", "content": "3D Point Cloud Convolutions Let us consider a 3D point cloud $X = \\{X_1,X_2, ..., X_N\\} \\subset R^3$ of N points and assume feature fields $f : X \\rightarrow R^C$. Thus with every point $x_i$, we have an associated C-dimensional feature vector $f(x_i) \\in R^C$. We further assume connectivity between points to be given in the form of neighborhood sets, that is, let $N(i) \\subset V$ denote a subset of nodes connected to node $i$, with $V = \\{1, 2, . . ., N \\}$ indicating the set that indexes the point cloud.\nThe general form of a linear layer for such point cloud feature fields is given by (Bekkers, 2019)\n[Lf](x_i) = \\sum_{j\\in N(i)} k(x_i, x_j) f(x_j),\nwhere we note that $k(x_i, x_j) \\in R^{C' \\times C}$ is a matrix depending on both the receiving and sending nodes, i and j respectively, such that the output feature map has C' channels. Also, note that the aggregation is permutation invariant. In works such as (Cohen et al., 2019; Bekkers, 2019) it is shown that when constraining linear layers of the form (2) to be equivariant, they become (group) convolutions. For example, if we want (2) to be translation equivariant, it has to take the form\n[Lf](x_i) = \\sum_{j\\in N(i)} k(x_j \u2013 x_i) f(x_j).\nI.e., $k(x_i, x_j)$ must then be constrained to be a one-argument kernel $k(x_j \u2013 x_i)$, conditioned on relative position. If we further want the linear layer to be equivariant to both translations and rotations, the kernel is further constrained to be symmetric via\n[Lf](x_i) = \\sum_{j\\in N(i)} k(||x_j \u2013 x_i||) f(x_j).\nI.e., the kernel can only depend on pair-wise distances. The influential works Schnett (Sch\u00fctt et al., 2023) and PointConv (Wu et al., 2019) are of this type.\nGroup convolutions In case one does not want to impose any further constraints on the kernel k but still wants to remain fully SE(3) equivariant, one has no other option than adding an axis over which to organize convolution responses in terms of rotations (Bekkers, 2019, Theorem 1). One then has to utilize lifting convolutions, followed by group convolutions:\nlifting conv:\n[Lf](x_i, R) = \\sum_{j\\in N(i)} k(R^T (x_j \u2013 x_i)) f(x_j),\ngroup conv:\n[Lf](x_i, R) = \\int_{SO(d)} \\sum_{j\\in N(i)} k(R^T (x_j-x_i), R^T R)f(x_i, \\tilde{R})d\\tilde{R},\nwith $d\\tilde{R}$ denoting the Haar measure over the rotation group SO(3). Note that now the convolution kernel in equation 5 is an unconstrained function over $R^3$, and that of Eq. 6 over $R^3 \\times SO(3)$, and that this kernel is rotated for every possible $R \\in SO(3)$. Both the lifting and group convolution layers generate feature maps $X \\times SO(3) \\rightarrow R^{C'}$ over the joint space of positions $X \\subset R^3$ and rotations $SO(3)$. In other words, at each point $x_i$ we now have a signal over the rotation group SO(3) which stores the convolution response for every \u201cpose\" $R$ in SO(3), and the linear layer is defined using convolution kernels that match feature patterns of relative spatial and rotational poses.\nEqs. 5 and 6 are forms of regular group convolutions Cohen & Welling (2016). Brandstetter et al. show that such layers can also be implemented through tensor field layers (Thomas et al., 2018), which form a popular class of steerable group convolutions that are parametrized by Clebsch-Gordan tensor products and work with vector fields that transform via irreducible representations (Weiler et al., 2021). Following Bekkers et al. (2024), we note however, that tensor-field networks unnecessarily constrain neural network design-as they require specialized activation functions, they require in-depth knowledge of representation theory, and they are computationally demanding due to the use of Glebsch-Gordan tensor products. Our study therefore focuses on regular group convolutions."}, {"title": "Compute and memory-efficient group convolutions", "content": "Finally, we base our equivariant layers on recent work (Bekkers et al., 2024) that defines separable SE(3) group convolutions over the space $X \\times S^2$, thus working with feature fields of spherical $S^2$ signals instead of SO(3)-signals. In that work it is shown that such models are computationally and memory-wise more efficient than full group convolutions over $X \\times SO(3)$, whilst maintaining expressivity and the universal approximation property of equivariant functions (Bekkers et al., 2024, Corrolary 1.1), despite the convolution kernels\u2014which are functions over $R^3 \\times S^2$-having a symmetry constraint given by $V_{RESO_2(2)}: k(Rx, Rn) = k(x, n)$, with $SO_2(2)$ the group of rotations around the z-axis and $n \\in S^2$. I.e., the kernels are axially symmetric.\nWhen factorizing such kernels into a spatial, orientation, and channel component via $k(x, n) := k^{(R^3)}(x)k^{(S^2)} (n)k^{(channel)}$, the group convolution is split into three steps\nL = L^{(channel)} \\circ L^{(S^2)} \\circ L^{(R^3)}\nwith\n[L^{(R^3)} f](x_i, n) := \\sum_{j\\in N(i)} k^{(R^3)} (R(x_j-x_i)) f(x_i),\nwhich is just a spatial convolution in which the kernel is a function $R^3 \\rightarrow R^{C}$ that for each translation, relative to the orientation $n \\in S^2$, returns a C-dimensional vector that is element-wise multiplied with the feature at the neighboring location. Then, $L^{(S^2)}$ is a point-wise spherical convolution\n[L^{(S^2)} f](x_i) = \\int_{S^2} k^{(S^2)} (n\\tilde{n}) f(x_i, \\tilde{n})d\\tilde{n},\nagain, with no channel-mixing taking place. Note that when the sphere $S^2$ is discretized with $O$ number of orientations, Eq. 9 is implemented as a point-wise matrix multiplication with a precomputed kernel matrix of size $O \\times O \\times C$, e.g. via einsum(\"noc,poc->npc\", features, spherical_kernel), with tensors of f being of dimensions $N \\times O \\times C$. Finally, $L^{(channel)}$ is simply a point-wise linear layer that mixes the channels without any spatial or orientation mixing, i.e., it is distributed over all points (N axis) and orientations (O axis) and thus is efficiently computed."}, {"title": "Handling vector input and outputs", "content": "The result of the $R^3 \\times S^2$-convolutional network is a spherical signal per node, and this allows to predict displacement vectors per node by simply taking a weighted sum of the spherical grid points over which the signal is sampled. I.e.,\nv^{out} = \\sum_{\\tilde{n}\\in S} f(x_i, \\tilde{n})\\tilde{n}.\nSimilarly, input vectors $v^{in}$ at nodes $i$ can be embedded as spherical signals $f(x_i, n) = n^T v^{in}$ simply by taking the inner product between the reference vector (grid point) $n$ and the input vector. The $R^3 \\times S^2$ group convolution paradigm thus allows us to build universal equivariant approximators that can predict vectors in an equivariant manner, as well as take them as inputs."}, {"title": "3 EXPERIMENTAL DESIGN", "content": "In this section, we present our hypotheses aimed at understanding the effect of equivariance, followed by architectural designs specifically crafted so that these models can evaluate the hypotheses."}, {"title": "3.1 HYPOTHESES ON THE IMPACT OF EQUIVARIANCE", "content": "Our main hypothesis is that equivariance is useful for two primary reasons:\n1. Equivariance promotes weight sharing and is thus data efficient.\n2. Equivariance guarantees generalization over symmetries.\nHowever, we also recognize that standard (non-equivariant) neural networks may not need to rely on these properties when a large amount of data is available. In such cases, the first argument no longer applies, and the second argument becomes less significant because a large training set might be representative of the test set\u2014or, in a sense, even include it.\u00b9. Moreover, data augmentation might be used to promote generalization over symmetries\u00b2. We thus formulate the following hypothesis, which will be tested with scaling experiments, with data augmentation enabled in both cases.\nHypothesis 1 (Scaling laws) For tasks that require invariance or equivariance, both equivariant and non-equivariant models will converge to similar performance with increasing dataset size.\n(a) In the large-scale regime both equivariant and non-equivariant will perform equally well.\n(b) In small-scale data regimes equivariant models will outperform non-equivariant models.\nWe must also consider task complexity, shown in Figure 1. We will focus the study on tasks that require either invariance or equivariance and formulate the following hypothesis to study the role of equivariance in tasks with varying geometric complexity.\nHypothesis 2 (Geometric complexity) The advantages of equivariant methods over non-equivariant ones become more pronounced in tasks that require a stricter form of equivariance.\n(a) For invariant tasks the gap in performance between equivariant and non-equivariant models will be less pronounced than in equivariant tasks.\n(b) There is a hierarchy of tasks in terms of how geometrically demanding they are, and thus to what extent equivariance is needed.\n\u2022 Low complexity: Invariant tasks like classification and regression;\n\u2022 Moderate complexity: Equivariant tasks that make point-wise scalar predictions relative to a geometry (shape), like segmentation;\n\u2022 High complexity: Equivariant tasks that make point-wise vector predictions relative to a geometry, like denoising diffusion models and dynamics forecasting methods.\nThe performance gap will increase with geometric complexity.\nNext, we want to investigate the effect of expanding the domain over which features are organized. In group convolutions, one typically adds an additional axis to store features that have a meaning relative to a set or grid of reference poses. For example, consider a classical CNN with a 256-dimensional feature vector per node (Table 1: rows 1-4, 16 19). A G-CNN with the same number"}, {"title": "Hypothesis 3 (Representation capacity)", "content": "Under the same representation capacity (e.g. same number of total channels per node), equivariant and non-equivariant models perform on par.\nThe previous hypothesis is likely to be rejected because even when the dimensionality of the representation spaces might be the same (e.g. models 1-4 in comparison to 16-19), the invariant models are constrained to only produce invariant feature vectors and thus have a smaller effective dimensionality. For all models, we have universal approximation results (invariant or equivariant), so as long as we reach the limit of performance per model we cannot say that one model was structurally limited to finding the solution, especially when considering strictly invariant/equivariant tasks. Then, what could cause a difference in performance, if both types of models could represent the optimal solution, in principle? We thus formulate the following two hypotheses."}, {"title": "Hypothesis 4 (Kernel constraint)", "content": "We expect that unconstrained (translational) models (Eq. 3) outperform (roto-translation) models (Eq. 4) over $R^3$ as the former is less constrained. Similarly, we expect that SE(3) equivariant group convolution methods (Eq. 7) outperform the constrained $R^3$ convolutional models (Eq. 4), even when matched in representation capacity.\nConsidering the above hypothesis, it might also be beneficial to break equivariance, i.e., allow a model to learn non-equivariant solutions even though it is primed for equivariant solutions. In Section 4 we test several options for breaking equivariance by providing features defined in the global coordinate system as input features. Not only will this break equivariance it will also provide the models with an explicit representation of geometry, whereas strictly equivariant models only have access to geometry implicitly via pair-wise relations that condition the convolution kernels. We thus formulate the following."}, {"title": "Hypothesis 5 (Symmetry breaking and explicit geometric representations)", "content": "In non-equivariant tasks, where symmetry breaking is allowed, models that are provided with explicit geometric information (such as taking coordinates as features) outperform those that do not have this information."}, {"title": "3.2 ARCHITECTURE", "content": "Based on (Bekkers et al., 2024), we design a model class called Rapidash that is capable of incorporating various forms of equivariances, and thus has the flexibility to test the above hypotheses.\nConvNext blocks Our architecture is fully convolutional and is based on the ConvNext block from Liu et al. (2022). See 2 for an illustration. The block consists of the following sequence of operations: a depth-wise separable convolution, followed by a LayerNorm, then a point-wise linear layer that increases the channel dimension four-fold, a GELU activation, and a linear back to the original channel dimensions. This output is added via a skip connection. The depth-wise separable convolution is either implemented via Eq. 3 or 4 for the classical (base space $R^3$) architecture, and via Eq. 8 and 9 in the $R^3 \\times S^2$ case. Thus, the only difference to the point-cloud implementation of ConvNext is that in our group convolutional implementation, the convolution is over $R^3 \\times S^2$ instead of just over $R^3$.\nRapidash To handle large point clouds, such as in the ShapeNet experiments, we need down- and up-sampling layers. Downsampling happens via strided convolution, by subsampling the point cloud using farthest point sampling, and only evaluating the convolution at the down-sampled points. Up-sampling does the inverse; it is also just a convolution layer, but sampled on a denser grid. See figure 2. The resulting model is in essence a multi-scale version of PONITA (Bekkers et al., 2024) and will be referred to as Rapidash."}, {"title": "4 EXPERIMENTS", "content": "We present a thorough analysis of the hypotheses mentioned in Section 3 through a series of experiments on various point cloud datasets like Shapenet 3D, QM9, and the CMU human motion prediction dataset, which are tasks with increasing levels of complexity (Fig. 1). For each of the experiments, we have different variations of Rapidash arising from equations Eq. 3 or 4 for R3 and Eq. 8 and 9 in $R^3 \\times S^2$ case, all with different input variations. For each table, the top section shows models with SE(3) equivariance, and the bottom section shows models with T3 equivariance. Symmetry breaking is indicated by: ! (SE(3)),! (T3 or SO(3)), and! (conditional SO(3)). A \u2713 indicates an option is used, X, indicates it is not used, and \"-\" indicates the option is not available. marks the model with maximal equivariance and information access. All experiments use Rapidash under different equivariance constraints. Note, all models have implicit access to positional information via pair-wise geometric attributes. For implementation details see App. A.5. Our code is made available at [see anonymous supplementary .zip file]."}, {"title": "4.1 3D POINT CLOUD SEGMENTATION AND GENERATION EXPERIMENTS", "content": "In this experiment, we evaluate our models on the ShapeNet 3D dataset (Chang et al., 2015) part segmentation and generation tasks. ShapeNet consists of 16,881 shapes from 16 categories. Each shape is annotated with up to six parts, totaling 50 parts. We use the point sampling of 2,048 points and the train/validation/test split from (Qi et al., 2017). We compare our models to various state-of-the-art methods like PointnXt (Qian et al., 2022), Deltaconv (Wiersma et al., 2022), and GeomGCNN (Srivastava & Sharma, 2021)for the part segmentation task. For the generation task, we compare to the latent diffusion model, LION, (Zeng et al., 2022)."}, {"title": "4.2 MOLECULAR PROPERTY PREDICTION AND MOLECULE GENERATION (DISCOVERY)", "content": "For predicting molecular properties and generating molecules, we use QM9 (Ramakrishnan R., 2014), a dataset which consists of 130k small molecules and their 3-dimensional coordinates, along with molecular properties, integer-valued atom charges, and atom coordinates. It contains up to 9 heavy atoms and 29 atoms including hydrogens. We use the train/val/test partitions introduced in Gilmer et al. (2017), which consists of 100K/18K/13K samples respectively for each partition. We evaluate the prediction of molecular properties using the MAE metric and compare these with"}, {"title": "4.3 HUMAN MOTION PREDICTION TASK", "content": "In table 3, we evaluate our models on the CMU Human Motion Capture dataset (Gross & Shi, 2001), consisting of 31 equally connected nodes, each representing a specific position on the human body during walking. Given node positions at a random frame, the objective is to predict node positions after 30 timesteps. As per Huang et al. (2022) we use the data of the 35th human subject for the experiment. We compare our models to NRI (Kipf et al., 2018), EGNN (Satorras et al., 2021), CEGNN (Ruhe et al., 2023) and CSMPN Liu et al. (2024) and show improved performance. We demonstrate that equivariant architectures enable better performance in motion prediction. See Fig:5 in Appendix."}, {"title": "5 RESULTS", "content": "Hypothesis 1 is accepted: Equivariant methods are more data-efficient then their non-equivariant counter part. The hypothesis is tested on Shapenet segmentation for various dataset size regimes as shown in Fig. 3 and Tab. 5 in the Appendix.\nHypothesis 2 is accepted: The advantage of equivariant models gets more pronounced with task complexity. Although, on all experiments the equivariant models have an edge over non-equivariant models, this improvement is more pronounced in complex and equivariant tasks, such as QM9 regression and generation (Tab. 2), Shapenet generation (Tab. 1) and motion prediction (Tab. 3), in which we note that both Shapenet generation and CMU motion prediction do not require equivariance, but are complex geometric tasks.\nHypothesis 3 is rejected: Increasing model capacity by increasing the number of channels does not lead to a performance gain as seen by the group convolutional methods. The performance of low"}, {"title": "Hypothesis 4 is accepted:", "content": "For equivariant tasks, having less constraints on the model is beneficial. See, e.g., Tab. 1, comparing models 1-4 with models 16-19, the latter being less constrained, and performing significantly better. Consider also the QM9 experiments (Tab. 2) which is an equivariant task. All models 1,2 and 5-12 are equivariant and universal approximators and should be able to solve the problem. Models 1-2 however use constrained kernels (symmetric, distance-based) while the group convolutional models are not. The latter largely outperforms the former."}, {"title": "Hypothesis 5 is accepted:", "content": "Providing explicit geometric information can improve performance, even when breaking symmetry. This is, e.g., observed in the tables by comparing the group convolutional models that take either coordinates as scalars (breaks equivariance) or as vectors (maintains equvariance) against those models that do not take any coordinate information as input."}, {"title": "6 DISCUSSION AND CONCLUSION", "content": "Discussion Beyond hypothesis insights, our basic convolutional architecture (Rapidash) performs exceptionally well on all considered tasks, often reaching state-of-the-art (SotA) performance, s.a. on ShapeNet/QM9 generation and CMU motion prediction. Surprisingly, even non-equivariant methods perform well on equivariant tasks, though not at SotA level and without equivariant generalization guarantees. These results support the \"convolution is all you need\" idea (Cohen et al., 2019; Bekkers et al., 2024), contrasting the popular \"attention-is-all-you-need\" claim (Vaswani, 2017).\nThe generalizability of the presented hypotheses remains to be tested outside of the class of convolutional architectures, to which Rapidash belongs. Conclusion In this work, we conducted a comprehensive evaluation of equivariant and non-equivariant models across various tasks of varying geometric complexities. In the debate of equivariant vs non-equivariant models, we provide strong evidence in favor of introducing structure over mere scaling. Specifically, we find that equivariant models (1) are more data efficient, (2) increasingly outpace non-equivariant models when increasing task geometric complexity, and (3) see performance benefits from targeted symmetry breaking."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 MATHEMATICAL PREREQUISITES AND NOTATIONS", "content": "Groups. A group is an algebraic structure defined by a set G and a binary operator: G \u00d7 G \u2192 G, known as the group product. This structure (G,) must satisfy four axioms: (1) closure, where \u2200h,g\u2208G: hg\u2208 G; (2) the existence of an identity element e \u2208 G such that \u2200g\u2208G, e \u2022 g = g. e = g, (3) the existence of an inverse element, i.e. \u2200g\u2208G there exists a g-1 \u2208 G such that g-1 \u00b7 g = e; and (4) associativity, where \u2200g,h,p\u2208G : (g\u00b7h)\u00b7p=g\u00b7(h\u00b7p). Going forward, group product between two elements will be denoted as g, g' \u2208 G by juxtaposition, i.e., as g g'.\nFor Special Euclidean group SE(n), the group product between two roto-translations g=(x, R) and g'=(x', R') is given by (x, R) (x', R')=(Rx' + x, RR'), and its identity element is given by e=(0, 1).\nHomogeneous Spaces. A group can act on spaces other than itself via a group action T : G\u00d7X \u2192 X, where X is the space on which G acts. For simplicity, the action of g\u2208 G on x \u2208 X is denoted as gx. Such a transformation is called a group action if it is homomorphic to G and its group product. That is, it follows the group structure: (gg') x=g (g' x) \u2200g,g' \u2208 G,x \u2208 X, and ex=x. For example, consider the space of 3D positions X = R\u00b3, e.g., atomic coordinates, acted upon by the group G=SE(3). A position p \u2208 R\u00b3 is roto-translated by the action of an element (x, R) \u2208 SE(3) as (x, R) p=Rp+x.\nA group action is termed transitive if every element x \u2208 X can be reached from an arbitrary origin xo \u2208 X through the action of some g \u2208 G, i.e., x=gx0. A space X equipped with a transitive action of G is called a homogeneous space of G. Finally, the orbit G x := {gx | g \u2208 G} of an element x under the action of a group G represents the set of all possible transformations of x by G. For homogeneous spaces, X=G xo for any arbitrary origin xo \u2208 X.\nQuotient spaces. The aforementioned space of 3D positions X=R3 serves as a homogeneous space of G = SE(3), as every element p can be reached by a roto-translation from 0, i.e., for every p there exists a (x, R) such that p=(x, R) 0=R0 + x=x. Note that there are several elements in SE(3) that transport the origin 0 to p, as any action with a translation vector x=p suffices regardless of the rotation R. This is because any rotation R\u2032 \u2208 SO(3) leaves the origin unaltered.\nWe denote the set of all elements in G that leave an origin xo \u2208 X unaltered the stabilizer subgroup Stabg(x0). In subsequent analyses, the symbol H is used to denote the stabilizer subgroup of a chosen origin 20 in a homogeneous space, i.e., H=Stabg(x0). We further denote the left coset of Hin Gas g H := {gh | h \u2208 H}. In the example of positions p \u2208 X=R\u00b3 we concluded that we can associate a point p with many group elements g \u2208 SE(3) that satisfy p=g0. In general, letting gx be any group element s.t. x=gx x0, then any group element in the left set gx H is also identified with the point p. Hence, any x \u2208 X can be identified with a left coset gr H and vice versa.\nLeft cosets g H then establish an equivalence relation ~ among transformations in G. We say that two elements g, g' \u2208 G are equivalent, i.e., g ~ g', if and only if g x0=g' 10. That is, if they belong to the same coset g H. The space of left cosets is commonly referred to as the quotient space G/H.\nWe consider feature maps f : X \u2192 RC as multi-channel signals over homogeneous spaces X. Here, we treat point clouds as sparse feature maps, e.g., sampled only at atomic positions. In the general continuous setting, we denote the space of feature maps over X with X. Such feature maps undergo group transformations through regular group representations pt (g) : X \u2192 X parameterized by g, and which transform functions f \u2208 X via [px (g)f](x)=f(g-1x)."}, {"title": "A.2 RETHINKING EQUIVARIANT MESSAGE PASSING AS GROUP CONVOLUTION", "content": "Message Passing Consider the graph representation of a point cloud G = (V, E) with points in a homogeneous space X. Label nodes as i \u2208 V and edges between nodes as (i, j) \u2208 E. Each node i has a corresponding coordinate xi \u2208 X. To each node, a feature fi \u2208 RC is associated, forming a discrete analog to the previously defined dense feature maps f : X \u2192 RC, where fi = f(xi). Thus, the features associated with a node i are (xi, fi). Additionally, one can associate attributes to the edges between nodes as aij."}, {"title": "Group Convolution as Equivariant Message Passing.", "content": "Group convolution can be written in terms of the message passing formalism. Given the graph G = (V, E) defined above, group convolution can be written as the sum\n[\\$(G)](9x) = \\sum_{j\\in N(i)} k(9zxj) fj\nDefining arj = 91xj as the edge feature between a pair of nodes, (1) the message function is $m(fi, fj,aij) = k(gzxj)fj, (2) the permutation invariant aggregation function is the sum, and (3) the update map is qu(fi, mi) = mi. Indeed, the message function is a linear map which is determined by the attribute aij, in this case, determined by the relative pose between two nodes. This dependence on only relative pose underlies both the equivariance and weight sharing of equivariant message passing."}, {"title": "Equivalence Classes and Invariant Attributes.", "content": "Equivalence classes of position-orientation coordinates with [xi", "xj": "is defined as\n[Xi"}, {"xj": {"G": "xi, x'j) = (gxi, gxj)\nThe list of invariant attributes for position orientation that determine the equivalence relations are given below for completeness (Bekkers et al., 2024"}}]}