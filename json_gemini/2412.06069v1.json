{"title": "Fuzzy Norm-Explicit Product Quantization\nfor Recommender Systems", "authors": ["Mohammadreza Jamalifard", "Javier Andreu-Perez", "Hani Hagras", "Luis Mart\u00ednez L\u00f3pez"], "abstract": "Abstract-As the data resources grow, providing\nrecommendations that best meet the demands has become\na vital requirement in business and life to overcome the\ninformation overload problem. However, building a system\nsuggesting relevant recommendations has always been a point\nof debate. One of the most cost-efficient techniques in terms\nof producing relevant recommendations at a low complexity\nis Product Quantization (PQ). PQ approaches have continued\ndeveloping in recent years. This system's crucial challenge is\nimproving product quantization performance in terms of recall\nmeasures without compromising its complexity. This makes\nthe algorithm suitable for problems that require a greater\nnumber of potentially relevant items without disregarding\nothers, at high-speed and low-cost to keep up with traffic.\nThis is the case of online shops where the recommendations\nfor the purpose are important, although customers can be\nsusceptible to scoping other products. A recent approach has\nbeen exploiting the notion of norm sub-vectors encoded in\nproduct quantizers. This research proposes a fuzzy approach to\nperform norm-based product quantization. Type-2 Fuzzy sets\n(T2FSs) define the codebook allowing sub-vectors (T2FSs) to be\nassociated with more than one element of the codebook, and\nnext, its norm calculus is resolved by means of integration. Our\nmethod finesses the recall measure up, making the algorithm\nsuitable for problems that require querying at most possible\npotential relevant items without disregarding others. The\nproposed approach is tested with three public recommender\nbenchmark datasets and compared against seven PQ approaches\nfor Maximum Inner-Product Search (MIPS). The proposed\nmethod outperforms all PQ approaches such as NEQ, PQ, and\nRQ up to +6%, +5%, and +8% by achieving a recall of 94%,\n69%, 59% in Netflix, Audio, Cifar60k datasets, respectively.\nMore and over, computing time and complexity nearly equals\nthe most computationally efficient existing PQ method in the\nstate-of-the-art.", "sections": [{"title": "I. INTRODUCTION", "content": "THE nature of recommender systems has a certain\ncharacteristic: incremental progress and performance\nimprovement using user experience. Hence, organizations can\nimprove by uninterruptedly learning from their recommenders\n[1]. In practice, it has been reported that about 80% of what\npeople follow on Netflix is the result of the recommendations,\nand a combination of penalization and recommendation\neffect save Netflix more than $1B annually [2]. Similarly,\nin [3] is stated that 35% of amazon purchases come from\nrecommendation lists. The application of recommender\nsystems is not solely limited to commercial purposes, and it\nhas been popular in healthcare decision-making during recent\nyears, among others [4].\nModel-based approaches have been actively studied for\nyears in the recommender systems field. In this sense, the\ncore of model-based methods is to formulate user-item\npreference in terms of the inner product of the user and\nthe item's vectors [5]. The most preferred item by a user\nis predicted to be the one that has the maximum inner\nproduct with the user query vector (i.e., MIPS). However, an\nexhaustive search through the inner product space of billion\nvectors in the largest dataset due to its high complexity\nof both space and time and thereby costs (e.g., increasing\nprocessing units, cloud computing, semiconductors) brings\nup the necessity of a technique to increase the computational\nefficiency [6]. By constructing a linear transformation, MIPS\nis redefined to the Nearest Neighbor Search problem, and\nthis is where product quantization is proved to come through\nthe task of cutting down largely the computational cost\n[7]. To reduce the norm error in Product Quantization, the\nNorm-explicit Quantization technique has been proposed,\nwhich has proven to be successful in providing a better\nestimation of MIPS than other vector quantization methods\nsuch as PQ, Optimized PQ (OPQ), RQ, and AQ in terms of\nrecall as the measure of performance. Besides, this method\nhas proven to be efficient with respect to recall even with\nlarge quantization errors and/or less number of codebooks\nin comparison to its counterparts because quantizing the\nnorm of items reduces the norm error, which leads to better\nperformance.\nWhy MIPS-based recommender systems? There are varied\nways to build a recommender system, either as a collaborative\nfilter or content-based, modeling via matrix factorization to\nneural networks; [8] [9] [10] [11] each approach has its\ncons and pros. Choosing a method, in this case, could\nbe highly dependent on the nature of the problem. MIPS\nis a well-established method for recommender systems\nthat are computationally efficient and scalable to large\nmultidimensional datasets [12]. It is also proven efficient in\nmany recommendation tasks, yielding fast search time. It can\nbe combined with other algorithms in frameworks to yield\nhigher or improved performance. In business systems, it is"}, {"title": "II. RELATED WORK", "content": "In this section, a number of related research works are\nreviewed to give a better picture of what has already been\ndone. Jegou et al. introduced PQ, which defines M sub-dataset\nout of the original dataset with a size of d/M features. In\nthis method, K-means is used to train the codebooks of each\ndataset independently. A codeword in this approach would\nbe a d/M-dimensional vector. This way, the approximated\nvalue of x is a fusion of its corresponding codewords [15].\nOPQ uses an orthonormal matrix R to rotate the items by\nRx before using PQ. OPQ leads to lower quantization error\nwhen either the features are correlated, or some features\nshow a larger value of variance in comparison to others.\nHowever, the codebook training phase in OPQ is slightly\nmore complex as it contains multiple rounds of alternating\noptimization of the codebooks and the rotation matrix R [16].\nIn 2010 Chen et al. suggested the RQ approach in which\neach codebook covers whole features, and each codeword is\na d-dimensional vector. The original data are used to train\nthe first codebook with K-means, and the concept of residues\nis used to train the second codebook. The training phase is\ncarried out recursively, which means m-th codebook is trained\nbased on the residues from (m \u2013 1) previous codebooks [17].\nBabenko et al. introduced AQ, which collectively improved\nRQ by optimizing all M codebooks. In this approach, they\nuse Beam search [18] to do encoding of the codebook (e.g.,\nfinding the optimal codeword indexes of an item within\nthe codebooks) and a least-square relation to optimize the\ncodebooks subject to the proposed encoding [17]. In 2020,\nDai and his colleagues tried using signal compression like\nother vector quantization-based methods [19]. Applying\nthis idea which separately quantizes the magnitude and\ndirection of a signal to have a fair level of efficiency, leads\nto a decrease in performance. However, the Norm-Explicit\napproach quantizes direction and norm separately, resulting\nin a more efficient performance for the problem of maximum\ninner product search (MIPS) [13]. Quantizing the norm has\nalso been claimed to reduce the quantization error while the\ndynamic range of data is rather large [20]. Norm-Explicit\nProduct Quantization has signified advancements in terms of\nperformance in comparison to the methods discussed above.\nNevertheless, it relies on discrete quantization. If we remove\nthis constraint, we can have a method that can operate with\na less restricting and smoother definition of the codebook,\naugmenting its quality."}, {"title": "III. BACKGROUND", "content": "To start with this section, a more or less formal definition of\nthe recommender system is worth revising. Briefly speaking,\na recommender system, or a recommendation system (the\nword 'system' may be interchangeable with similar words\nlike 'engine' or even 'platform'), is a subclass of information\nfiltering system that delivers suggestions for items that are\nmost to the point to a specific use. Elaine Rich introduced the\nidea of a recommender system in 1979 [21]. She looked for a\nway to recommend a user a book they might be interested\nin. Her idea was to make a system that asks a particular\nuser question and assigns them stereotypes subject to their\nresponses. Depending on a user's stereotype, they would\nthen get a recommendation for a book they might like [22].\nIn recommender systems, two problems convertible to one\nanother need to be discussed. The first is NNS, and the other\nis Maximum Inner Product Search (MIPS)."}, {"title": "A. Recommender Systems, NNS and MIPS", "content": "Recommender systems are used in a wide range of\napplications, ranging from playlist generators for video and\nmusic services, product recommenders for online shops,\ncontent recommenders for social media platforms, and open\nweb content recommenders [23]. Recommender systems\nhave also been developed to explore research articles [24]\ncollaborators [25], financial services, [26] and predicting\ncancer drug response [27]. These systems usually employ\ncollaborative filtering and/or content-based filtering method\nseparately or together [28]. Besides, they might use similar\nsystems like knowledge-based systems. Collaborative filtering\napproaches build a model based on a user's previous\nbehaviours (items previously bought or chosen and/or ratings\ngiven to those items), and analogous decisions made by other\nusers could be a source of information for this approach [9].\nThis model is then utilized to predict items or ratings of the\nitem that the user may be inclined to [29]. Content-based\nfiltering approaches use a set of discrete, pre-tagged attributes\nof an item to recommend additional items with similar\nproperties [30]. Each type of system has its advantages and\ndisadvantages. Collaborative filtering may need a great deal\nof information to start with, which is an example of the cold\nstart problem, and is a frequent topic in collaborative filtering\nresearch [31] [32]. In contrast, content-based filtering can be\ndone by having a limited scope of information and without\ninformation about user behaviours, which users may not be\nwilling to share. Fast recommendations can enhance user\nexperience when user behavior data is not available. Enabling\nrapid prototyping, users could quickly and actively find what\nthey are looking through engaging with an interactive website\nor application [33].\nA fast way for finding the preferred items for a specific user\nis by searching through the inner products of items by user\nquery vectors and finding vectors with the maximum value\nof the inner product. Maximum Inner Product Search (MIPS)\nusing a simple manipulation could be viewed equivalently to\nNearest Neighbor Search (NNS) in which maximization of\ninner product in MIPS is considered as the same as distance\nminimization in NSS [7]. The fundamental relations of MIPS\nand NNS are as follows:\n$x = \\underset{x_i\\in D}{\\operatorname{argmin}}||X_i - q||_2$ (1)\n$X = \\underset{x\\in D}{\\operatorname{argmax}}Xq$ (2)\nIn which D is frequently a massive dataset queried from, xi is\na database vector, q is the given query, ||.||2 is L2-norm, and\nx would be the nearest neighbor for the query q or maximum\ninner product value identically using a simple trick or more\nspecifically a linear transformation. Utilizing the following\nrelations facilitates the construction of the transformation:\n$\\phi = maxx \\in D||X_i||_2$ (3)\n$z_i = (\\sqrt{\\phi^2 - ||x_i||^2}, x)^T$ (4)\n$q_2 = (0, q^T)^T$ (5)\nas it is observed, zi and q\u2082 are modified item and query\nvectors respectively, and $ represents the maximum norm of\nitems in the dataset [6]. Using the relations (3)-(5), we have\nthe following theorems with proofs [34]:\nTheorem 3.1: For every zi, ||zi||2 = $\nProof 3.1: Using the relation (4), we have ||zi||2 = ||xi||2-\n$||xi||2 + $2 = $\nTheorem 3.2: For every \u00e6i, zi we have zq\u2082 = xq\nProof 3.2: Since the first component of q\u2082 equals zero, we\nknow that (wT)T = w for an arbitrary vector w. Hence, we\nhave (\u221a2 - ||xi||2, x).(0, qT)T = 0 + x\u00a5 (qT)T = x+q.\nConsequently, z q\u2082 = x+q.\nHaving the mentioned relations, the linear transformation\nbased on the inner product term is simply as follows:\n||qz-zi||2 = ||qz||3 \u2013 2ziq\u0142 + ||zi||3 = ||9||3 + p2 \u2212 2xq (6)\nrelation (6) shows the distance relation in NNS can be written\nin terms of the inner product of item and query vectors which\nis the transformation of interest to make the nearest neighbor\nsearch problem identical to MIPS [34]."}, {"title": "B. Product Quantization", "content": "As a general concept, Product Quantization (PQ) is a\nvector quantization approach representing high dimensional\nvectors as a Cartesian product of sub-spaces for quantizing\nthem separately. Before explaining the basic idea of product\nquantization, we provide some insight into the structures of\nthis method. From a mathematical point of view, the definition\nof a quantizer [14] is well-worth discussing.\nDefinition 3.1 (Quantizer): a quantizer is a function q\nmapping a D-dimensional vector x \u2208 RD to a vector\nq(x) \u2208 C = {ci;i \u2208 I}, where the index set defines as I\nfor simplicity considered to be finite: I = 0, .., k \u2212 1.\nWe have used a fuzzy clustering technique called\nInterval Type-2 Fuzzy Possibilistic C-means, which has been\nintroduced in [35] to add fuzzy quality to the process of the\nalgorithm.\nUsing weights as exponents, this algorithm uses \u03beand \u03b7 to\ndescribe the fuzziness and possibility, representing an interval\nor simply a range rather than an exact value [36]. The choice\nof the best parameters for the fuzzy interval is carried out by\nusing Genetic Algorithm [37] (a color-map of grid values is\nadded as an appendix). The best set of values for \u00a71, \u00a72 are\n8.5, 9.1 respectively."}, {"title": "IV. FUZZY-2 NEQ", "content": "Vector quantization, as one of the data compression\nmethods, could largely reduce the cost of MIPS by encoding\nthe input into a lower dimensional subspace using codebooks.\nAn ordinary vector quantization may have issues such as\nthe high-running time by growth in the number of inputs\nor space complexity challenges to store many records for a\nD-dimensional vector."}, {"title": "A. Codebook Definition", "content": "The reproduction values ci mentioned in the background\nsection are normally called centroids, and it would be the\ninitial notion to define codebooks effortlessly as below:\nDefinition 4.1 (Codebook): The set of reproduction values\nC is, in fact, the codebook of size k.\nBut how our model as fuzzy-2 NEQ would be convergent as\na quantization method needs to be satisfied Lloyd's properties\n[38]. In quantization, the input space is partitioned into a set\nof convex regions called Voronoi cells.\nDefinition 4.2 (Voronoi Cell): The set V\u2081 of vectors mapped\nto a given index i called as a Voronoi Cell defines as following:\n$V_i = \\{x \\in R^D: q(x) = c_i \\}.$ (7)\nthe Lloyd's optimality properties are as follows:\nq(x) = arg min d (x, ci)\nc\u2081 EC (8)\nc\u2081 = Ex [x | i] = fp(x)xdx\nVi (9)\nthe first condition simply states that each reproduction value\nci is assigned to its nearest data point. the second condition\npoints out that the value of ci must be the expected value\nof the vectors in Voronoi Cell of index i (e.g., Vi). Interval\nType-2 Fuzzy Possibilistic C-means comes up with a near-\noptimal codebook that is less susceptible to noise than other\nvariations of fuzzy c-means [35]. It works by assigning vectors\nto centroids to meet the optimality criteria. Then, we fuse the\nresult of the fuzzy clustering algorithm, a set of nearly-optimal\ncodebooks using the Sugeno integral since Sugeno integral is\nan ordinal aggregation method which can grade similarity and\nin the case of our problem, it is used for fusing the set of\ncodebooks as the output of the fuzzy clustering method to\na single crisp codebook. Besides, adding a stop condition of\nimprovement at the 7th step of the algorithm helps us to have\nat least a local optimum result. In the product quantization\ncase, the proposed issues are settled by selecting the number\nof components that should be quantized separately. To clarify\nby an example, an input vector denoted by \u00e6 is split into\nm distinct sub-vectors oj of dimension D* = D/m where\nmis a divisor of D and 1 \u2264 j \u2264 m. The sub-vectors\nare quantized separately using m distinct quantizers. Dividing\nthe primary vector into sub-vectors provides two significant\nadvantages. Firstly, it aids in preserving the local structure of\nthe data. Secondly, it facilitates dimensionality reduction. By\nsegmenting the vector into smaller elements, we can better\ncomprehend the inherent features and correlations within the\ndata. This offers a computational advantage by reducing the\ncomplexity of the problem. Since each sub-vector is separately\nquantized using its own quantizer, it enables customized\nquantization for each sub-vector. Therefore, the quantization\nprocess is tailored to the distinct features and distribution of\nevery sub-vector. The selection of the number of sub-vectors\n(m) in product quantization impacts the balance between\naccuracy and complexity. A large m helps in seizing more\ndetailed information but simultaneously boosts computational\ncomplexity. Conversely, utilizing smaller sub-vectors more"}, {"title": "B. Fuzzy Norm-Explicit Quantization", "content": "To improve the performance of product quantization,\nthe point of using norm values has been introduced as a\ncomplementary idea conjunctive to the notion of PQ to\nreduce the error of quantization and, consequently, improve\nthe performance of product quantization [13]. MIPS would\ntake advantage of methods that explicitly cut down the error\nrate in the norm since an accurate norm is significant to MIPS\nperformance. Hence, the core of Norm-Explicit quantization is\nto quantize the norm ||2||2 and the direction vector, which is\nunit vector x of the items distinctively. The norm is encoded\nexplicitly using separate codebooks to achieve a small error,\nwhile the direction vector can be quantized using an existing\nvector quantization approach without making notable changes.\nm\nIn detail, the codebooks in NEQ are split into\ntwo parts. The first part, referred to as m', consists of\ncodebooks that are considered as norm codebooks, denoted\nby Lp for p\u2208 {1,...,m'} in which each codeword\nlp[n] \u2208 R for 1 \u2264 n \u2264 k*. The other m m' codebooks\n\u0421\u0442'+1, \u0421\u0442'+2,..., Cm are vector codebooks for the direction\nvector. To calculate & in this method, the following formula\nis applicable (further detail be consulted in [13]):\n$\\hat{x} = \\sum_{l=1}^{m'} E[l_{m'}|i_{m'}] \\sum_{m=m'+1}^{m}c_{m}[i_{m'}]$(11)\nin which im represents the m-th codeword index of the item\nx. the inner product introduced in relation (16) can be worked\nout by Algorithm 1 using &, which is the approximation of\nthe item \u00e6 indeed. The next step to solving the problem"}, {"title": "V. RESULTS", "content": "In this section, we try to provide the result of the method\ndiscussed in the previous chapter based on metrics such as\nrecall and overall time. Custom Python code was elaborated\nfor implementing methods and the Fancy-Aggregations Python\nlibrary was utilized to compute the Sugeno integral. [43]. We\nemployed three well-known datasets, namely Netflix, Audio,\nand Cifar60k. A summary of their statistics has been provided"}, {"title": "A. The Metrics of Performance", "content": "Running time and recall metrics in the case of our problem\ncan be treated in the context of information retrieval and a type\nof time complexity in computer systems, in which recall is the\nnumber of correct results divided by the number of results that\nshould have been returned [44]. Recall would be estimated as\nfollows:\n$Recall = \\frac{Relevant Documents \\cap Retrieved Documents}{Relevant Documents}$(13)\nand as another metric of performance for the algorithm,\nprecision would be considered which is defined as below:\n$Precision = \\frac{Relevant Documents \\cap RetrievedDocuments}{All Relevant Documents}$(14)\nby having precision and recall, F1-score can be easily\ncomputed by the following relation [45]:\n$F1 score = \\frac{2 x Precision \u00d7 Recall}{(Precision + Recall)}$(15)\nTo calculate the running time, we used the time difference\nbetween the prior and post-running times of the main function\nof the code containing algorithms of study. The hardware\nspecification of the system running the algorithm has been\nadded as the footnote.\u00b9. In addition, a short explanation of\nthe asymptotic analysis and comparison of vector quantization\nmethods with other methods is delivered in the following\nsubsections. The relation can be written as:\n$RunningTime = |Start Time-Termination Time|$(16)\nCifar60k, Audio, and Netflix datasets have been used as the\nexperiment material to measure the method's performance by\nthe recommended metrics above [46]. An overview of these\ndatasets has been provided in a table further in this section."}, {"title": "B. Fuzzy-2 NEQ vs. Baseline Models", "content": "To have a more analytical evaluation of our model,\ncomparing it to similar models is informative. A more detailed\ndiscussion of the some baseline methods, including PQ, OPQ,\nand RQ, has been delivered in related work as a part of the\nliterature review. However, a short description each baseline\nmethods is proposed further in this section. In detail, each\nmodel was tested by keeping the number of codebooks as low\nas possible(e.g., 8) and several clusters 16, 64 to see how\nefficiently productive these models could be in comparison to\nFuzzy-2 NEQ based on their recall/item values."}, {"title": "C. Fuzzy-2 NEQ vs. NEQ", "content": "In order to allow an easier discussion of the results, detailing\nthe dimensions of datasets used in the experimentation would\nbe useful. Table I indicates the number of records used for\nthe purpose of training as well as the dimension of each\ndataset. The source of variation in our method would be the\nnumber of codebooks, the number of clusters, and the number\nof items. For each dataset, we used random sampling with\nreplacement (e.g. bootstrapping) to select training items. To\nobtain a statistically unbiased result, we repeated this process\n10 times for each dataset, using a fixed set of parameters.\nTable IV shows the average running time and average standard\ndeviation of recall for each model on different datasets and\nparameter settings across the 10 iterations. In recommender\nsystems, recall (13) is a key performance metric that holds\ngreat importance. This is due to its ability to the degree to\nwhich a model can recognize relevant data. Recall, defined\nas the proportion of relevant items that the model correctly\nretrieves, serves as a crucial indicator of the effectiveness of\na recommender system [47]."}, {"title": "D. Algorithmic Complexity Analysis", "content": "A bottleneck in recommender systems is time complexity.\nWhile an accurate result is important, having a quick outcome\nwith a high recall can still be valuable for many applications.\n48]. The complexity of Product Quantization is O(\"/kxD+\nc) where c is a constant referring to codebook construction\ncost. On the other hand, by taking a fuzzy approach and using\na low-cost aggregation method, we improved the recommender\nsystem's performance in terms of recall with a negligible\ndifference in running time. There are other techniques like\nmatrix factorization in which the time complexity of stochastic\ngradient descent in these algorithms is mostly O(k) [7].\nIn addition, Product Quantization is a favored method for\nefficaciously compressing high-dimensional vectors to use\naround 90 per cent less memory [7]."}, {"title": "E. Discussion on Achievements", "content": "There are different kinds of methods addressing the problem\nof finding the most interesting item or set of items for a user.\nHowever, each method has its cons and pros. Specifically, a\nhigh-performance method of tackling the problem could be\nusing neural network-based approaches or deep learning. In\nthis case, choosing which method could best fit the problem\nis related to the nature of the problem and use case. We use\nthat as a basis to extend the notion of product quantization-\nbased techniques in the phase of codebook construction.\nFurthermore, lower time and space complexity, acceptable\nperformance, and a technique focusing on retrieval efficiency\nmotivate researchers and practitioners to utilize and extend this\nmethod. In summary, when the complexity of a method would\nbe essential because of various reasons such as limitations\nin computational power, then a product quantization-based\napproach could be a suitable choice to solve the problem\nwith the main focus on time and space complexity rather\nthan very high expectation of accuracy in results. A table\nof results and recall-item curves for norm-explicit product\nquantization using K-means to build the codebooks have been"}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "The importance of recommender systems in online markets,\nmusic streaming platforms, subscription streaming services\n(e.g., Netflix), and even social media such as YouTube or\nInstagram for both users and companies is tangible. To\nmeet the market demands of every user, a recommender\nsystem with a reasonable time and space complexity and\nproviding an acceptable percentage of relevant and accurate\nrecommendations would be a point of interest for any\nbusiness. In conclusion, we have provided metrics such\nas recall and running time with a recall-item curve to see\nhow the algorithm works in a different set of parameter\nsettings, showing an acceptable performance during 10\niterations of each setting. By adding fuzziness to the existing\nNorm-Explicit Product Quantization codebook construction\nprocess, we have made progress regarding recall percentage,\nreflecting more relevant recommendations in output at\ncompelling complexity. This is important since human, time,\nand financial resources are worth preserving and consuming\ncarefully, and having relevant recommendations on hand\ncould result in more success in the business in terms of\nfinding customers interested in the product or service.\nFurthermore, in health recommender systems, extracting\nmedicine based on other patients' experience has proved that\napplications of recommender systems in the health sector are\nwell-worth to be researched. Eventually, using the relative\nnorm, training the remaining codebooks is done, and we\nhave m trained codebooks. In definition, codebooks are\ntables mapping centroid indices (e.g., codes) to centroids\nand conversely [49]. The resulting codebooks contain items\nthat are compressed into pieces and then sorted to give us\nthe top 20 preferred items. In general, the proposed method\nwould help us to have a model which is firstly convergent,\nsecondly considers overlapping clusters situation, and defines\na more precise margin using a fusion model based on the\noptimality of weights. The method that was suggested has\nshown better results compared to quantization-based methods.\nIn the experiments conducted on Netflix, Audio, and Cifar60k\ndatasets, the proposed method achieved a recall of 94%,\n69%, and 59% respectively. This means that it was able to\ncorrectly identify a higher percentage of relevant information.\nSpecifically, when compared to other methods such as NEQ,\nPQ, and RQ, the proposed method outperformed them by"}, {"title": "APPENDIX A\nOPTIMAL CODEBOOKS AND CENTORIDS EXISTENCE\nPROOF", "content": "Definition A.1 (Optimal Codebook): A codebook is called\noptimal either globally or locally for distribution function F if\nit obtains the minimum value of average distortion function.\nTheorem A.1 (Existence of k-level optimal codebooks and\ncentroid):\nProof A.1: There are two sufficient assumptions on\ndistortion function d to ensure the existence of the optimal\ncodebooks and centroids as follows:\nA1) d : RD \u00d7 RD \u2192 [0, \u221e) is continuous.\nk\nA2) For each x, d(x, y) \u2192 \u221e as x \u2192 x and ||y||2\u2192\u221e.\nHence, for any fixed \u00e6, d(x) is continuous on (RD) by\nA1) and A2); hence, by average distortion equation which is\n$D(C, F) = \\int \\underset{1<j<k}{min} d (x, a_j) dF(x)$(A.1)\nk\nand Fatou's lemma the average distortion function D(A, F)\nis a lower semi-continuous function of the codebook C\nwith respect to differential of distribution function F(x). By\nthe compactness of (RD) the minimum value of average\ndistortion is achieved by some Codebook C+. If C+ has some\n-valued codewords, then by A2) they can be replaced by\nreal-valued ones. Hence an optimum k-level codebook exists.\nObserve that no restrictions need to be placed on F. For"}, {"title": "APPENDIX B", "content": "A genetic algorithm was employed to find the optimal values\nfor \u00a71, and \u00a72, utilizing an initial population size of 10, with\na mutation rate of 0.5 and a recombination rate of 0.7. The\ntolerance level was set at 0.01, and the implementation was\nconducted using the Scipy library. A scatter heat-map of grid\nvalues for parameters \u00a71 and 2 could be intuitively used to\nfind the optimal values for the fuzzy parameters (the colder\nthe color, the smaller the value of the cost function)."}]}