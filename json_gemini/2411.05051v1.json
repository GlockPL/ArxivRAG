{"title": "Intellectual Property Protection for Deep Learning Model and Dataset Intelligence", "authors": ["YONGQI JIANG", "YANSONG GAO", "CHUNYI ZHOU", "HONGSHENG HU", "ANMIN FU", "WILLY SUSILO"], "abstract": "With the growing applications of Deep Learning (DL), especially recent spectacular achievements of Large Language Models (LLMs) such as ChatGPT and LLaMA, the commercial significance of these remarkable models has soared. However, acquiring well-trained models is costly and resource-intensive. It requires a considerable high-quality dataset, substantial investment in dedicated architecture design, expensive computational resources, and efforts to develop technical expertise. Consequently, safeguarding the Intellectual Property (IP) of well-trained models is attracting increasing attention. In contrast to existing surveys overwhelmingly focusing on model IPP mainly, this survey not only encompasses the protection on model level intelligence but also valuable dataset intelligence. Firstly, according to the requirements for effective IPP design, this work systematically summarizes the general and scheme-specific performance evaluation metrics. Secondly, from proactive IP infringement prevention and reactive IP ownership verification perspectives, it comprehensively investigates and analyzes the existing IPP methods for both dataset and model intelligence. Additionally, from the standpoint of training settings, it delves into the unique challenges that distributed settings pose to IPP compared to centralized settings. Furthermore, this work examines various attacks faced by deep IPP techniques. Finally, we outline prospects for promising future directions that may act as a guide for innovative research.", "sections": [{"title": "1 Introduction", "content": "Deep learning (DL) has revolutionized a wide variety of artificial intelligence (AI) applications, including the prediction of 3D structures for proteins [45], self-driving cars [93], and the relatively new artificial intelligence-generated content (AIGC) [50]. Despite these surprising advances, it is widely understood that building a production-level DL model is a non-trivial task, which frequently calls for (i) meticulously designed network structures, (ii) a large-scale, high-quality (annotated) dataset, (iii) a massive amount of computational resources, and (iv) high technical expertise. It is therefore extremely costly for acquisition of high-performance DL models, especially recent large models like ChatGPT [112], diffusion models (DMs) [27], Codex [50], etc. These well-trained models and their associated training datasets represent valuable intellectual property (IP) and form the core competencies of companies.\nUnfortunately, the immense value of such IP incentivizes adversaries to launch various model or dataset theft attacks. For instance, the work [85] mimics the original model at a meager cost by querying and observing the outputs. The unauthorized reproduction and malicious distribution of well-trained models result in copyright infringement and significant financial losses for the model creators. Besides, with the auxiliary of prior arts like member inference, preference profiling, model inversion attacks, etc. [167], attackers can reconstruct the model's private training data. Recently, the concept of \"Machine Learning as a Service (MLaaS)\" has gained popularity, allowing individuals who are not DL experts to remotely access product-level models on a pay-per-use basis. However, MLaaS also introduces new avenues for attackers, who can frequently request prediction results from these services to create low-cost, functionally identical stolen models. This ability enables attackers to avoid payment, create services that violate IP rights, and potentially gain significant financial profits, thereby undermining the competitive edge of the original IP owners in the market.\nDeep IP protection (IPP) is a frontier research field that is still in its infancy. Researchers have proposed various IPP methods [73, 96, 121], most of which fall under the category of reactive verification, where IP identifiers (watermarks or fingerprints) are embedded into models and then used to verify the ownership of suspicious models. The advantage of watermark is that it can provide more accurate ownership verification and even maintain a certain degree of robustness in transfer learning scenarios [121]. However, these techniques are invasive; they require tampering with the training process, which may affect model utility or introduce new security risks. In contrast, fingerprinting can better preserve the model's functionality. It also demonstrates more versatility and convenience, especially in scenarios where model owners do not have the right to modify the model [73]. However, it also has limitations when facing diverse and adaptive attacks. The other deep IPP category is the proactive approach to preventing model theft [83, 104, 145]. Controlling model access and tracking leakers of pirated models are components of reactive protection strategies. Common measures include access authentication [145] or entangling the model usability and access privilege [104]. Nonetheless, deep IPP regardless of whether reactive or proactive faces a range of attack types, where the capabilities of attackers can vary. Designing an ideal IPP scheme is thus challenging. On the one hand, trade-offs among multiple competitive objectives have to be balanced, such as ensuring the watermark's invisibility while also possessing robustness. On the other hand, it is imperative to guarantee that the IPP scheme functions properly in various application circumstances. For example, if a DL model is trained and watermarked on a skin disease dataset, and the service buyer fine-tunes the model on a pneumonia dataset, the IPP should survive the ownership verification procedure."}, {"title": "1.1 Contributions of this survey", "content": "Deep IPP is currently in its early stages, with a notable lack of comprehensive summaries and analyses to better characterize the state-of-the-art. Therefore, conducting a systematic and comprehensive survey on the current status and advancements in this field is crucial. To this end, this review spans key research from 2017 to 2024, focusing on copyright protection for both models and datasets (the latter often being overlooked). It covers the taxonomy of IPP algorithms under centralized and decentralized learning settings, evaluation metrics, and various attacks that pose threats to IPP algorithms. Furthermore, we identify core issues and major challenges in IPP, outline promising future research directions, and highlight practical applications. Our major contributions can be summarized as follows:\n\u2022 We conduct a comprehensive review of existing deep IPP schemes for both DL models and datasets with particular attention to the unique challenges and solutions associated with AIGC models such as DMs and LLMs (Section 3.2 and 3.3). It fills overlooked aspects in existing surveys, which overlook the fact that datasets are also important and valuable intellectual property.\n\u2022 We innovatively summarize dual-level performance evaluation metrics: common metrics applicable to all IPP approaches, and unique metrics specific to each type according to their distinctive defense objectives.\n\u2022 We undertake a systematic review of deep IPP and attack methods for diverse tasks. These methods are categorized from reactive and proactive defense perspectives, and a critical analysis of their strengths and limitations within each (sub)category is offered.\n\u2022 We provide an in-depth analysis of the challenges encountered by deep IPPs in distributed settings, categorizing existing methods and comparing their respective advantages and disadvantages.\n\u2022 We identify the limitations associated with deep IPPs, outlining prospect promising avenues for future research."}, {"title": "1.2 Comparison with Existing Surveys", "content": ""}, {"title": "1.3 Organization", "content": "The rest of this work is organized as follows. Section 2 presents the background of deep IPP, including the definition of deep neural networks, deployment and verification modes, and evaluation metrics. In Sections 3, 4 and 5, we categorize the deep IPP for mode and dataset intelligence, respectively. Section 6 investigates the emerging deep IPP in distributed learning settings. Security threats to deep IPs are investigated in Section 7. Following, we delineate the current challenges facing deep IPP and propose auspicious directions for prospective research in Section 8. Section 9 concludes this survey."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Deep Neural Networks", "content": "A DL model is made up of a number of network layers, including an input layer, several hidden layers, an output layer, and so on. The DL model maps input data to its corresponding label (using classification as an example) by employing the approximate transformation function \u03a6:\n$$\\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^{N} L_{ce} (\\Phi(x_i; \\theta), y_i),$$\nwhere $N$ is the number of samples in the training dataset $D = \\{(x_i, y_i)\\}_{i=1}^{N}$; $L_{ce}$ is the cross entropy (CE) loss function. DL models learn the data representation through multi-layer nonlinear transformations, and optimize the model by adjusting the trainable weights so that it can accurately perform classification, prediction, or other tasks."}, {"title": "2.2 Intellectual Property in MLaaS", "content": "MLaaS refers to cloud-based services provided by companies for deploying machine learning (ML) products, which are available for developers and external users. MLaaS offers two main product services: high-quality datasets and well-performed models. However, some participants may attempt to steal these products by masquerading as customers and then rebranding and reselling them to gain illegal profits. The stealing process is usually costless compared with obtaining a well-trained model from scratch.\nMLaaS services are operated in two primary ways [101, 114]: (1) Companies directly distribute products to buyers and grant them full access to both model and dataset, encompassing the model's internal structure and parameters as well as data properties. This mode allows users to gain insights into the model's functionality and conduct thorough"}, {"title": "2.3 Evaluation Metrics", "content": "This survey summarizes the evaluation metrics for deep IPP into common metrics and specific metrics for proactive and reactive IPP, respectively. These metrics are applicable for both model and dataset IPP evaluations.\nThe common metrics should be able to evaluate the following four performance measures:\n\u2022 Robustness: To fit downstream tasks, it is frequently necessary to make additional modifications to the protected model and the model trained on the protected dataset [83]. Malicious attackers commonly endeavor to circumvent ownership verification through operations such as watermark deletion, overwriting, or sabotage. Thus, an ideal protection scheme should withstand diverse attacks.\n\u2022 Efficiency: Additional resource costs associated with the reactive verification or proactive defense approach [13], such as latency, and communication overhead should be affordable by the user.\n\u2022 Secrecy: IP identifiers must remain secret or undetectable to adversaries [48]. This requires embedding these identifiers in a way that they are imperceptible during normal model operation and scrutiny by potential attackers, thereby safeguarding the IP from unauthorized access, reverse engineering, and tampering.\n\u2022 Generality: The ideal IPPs should be agnostic to model architecture and downstream tasks [56].\nIn addition to the above common metrics, a well-designed reactive ownership verification method should fulfill the following three properties:\n\u2022 FidelityRea.: IP identifiers for datasets and models are typically created by adjusting the model's parameters or decision-making behaviors, which can often negatively impact the original model [43]. Fidelity ensures that the protected model performs indistinguishably to the original model.\n\u2022 Capacity: It refers to the valid information payload and theoretical upper bound contained in the reactive IP identifier [31]. An ideal IPP method must be capable of embedding a substantial amount of information in the protected DL model.\n\u2022 ReliabilityRea.: The reactive approach should exhibit a low false negative rate [83], ensuring IP owners can accurately identify their IP identifiers with high confidence from the suspect models.\nThe metrics for proactive IPP, such as efficiency, robustness, generalization, and secrecy, are identical to the reactive IPP. Moreover, fidelity and reliability deviate from the mentioned reactive metric, and there are also unique metrics for proactive methods, as outlined below.\n\u2022 Fidelityproa.: The proactive authorization method adjusts fidelity according to user types. Authorized users gain superior model performance, while unauthorized users experience restricted or poor performance [14].\n\u2022 ReliabilityProa.: To prevent theft from illegal users, it is necessary to distinguish the identity of legitimate users and illegal users accurately [81]."}, {"title": "\u2022 Scalability", "content": "An ideal proactive authorization method should generate and accommodate a vast array of user identity keys [148]."}, {"title": "\u2022 Uniqueness", "content": "The identity keys, which are allocated to legitimate users one by one, must be unique for tracking down the traitors [14]."}, {"title": "\u2022 Unforgeability", "content": "The identity key must be unforgeable, and the user identity forged by the attacker cannot be authenticated [148]."}, {"title": "\u2022 Traceability", "content": "The victim can trace out the traitor users based on the suspicious models [148]."}, {"title": "3 Reactive Model Intellectual Property Protection", "content": ""}, {"title": "3.1 Overview", "content": "We first distinguish between two types of IPP methods: (i) reactive IPP strategies, which respond to threat events after the infringement has already happened such as through verifying an implanted identifier to claim the ownership, and (ii) proactive IPP strategies, where defenders take proactive actions to stop threats before they happen such as through locking the model and later authorizing the usage right.\nAccording to whether the target model's internal components are invasively altered, reactive model IPP techniques fall into two primary subcategories: model watermarking [123] and model fingerprinting [7]. The former is invasive and the latter is non-invasive."}, {"title": "\u2022 Model watermarking", "content": "It is a solution that invasively embeds detectable and tamper-proof watermarks into the host DL model through methods such as a parameter regularizer or backdoor data watermarking. Later, the watermarks can be extracted from the model parameters or output as the ownership evidence."}, {"title": "\u2022 Model fingerprinting", "content": "DL fingerprinting represents a non-invasive solution wherein the model owner generates unique sample pairs that can exclusively and accurately fingerprint the target model. During the verification phase, defenders extract fingerprint samples to compare model fingerprint similarity. These disparities may manifest in various model features, such as model predictions, decision boundaries (DBs), etc."}, {"title": "3.2 Model Watermarking", "content": "The existing schemes are classified according to the defender's access level to the suspect model during verification.\n3.2.1 Model component-based watermarking. When white box validation is allowed, the model owner has the highest privileges to access the suspect model's structure and parameters, and can even observe the model updating or training process on the test samples. Various model elements, such as model weights, dynamic parameters, and model structures, can be used to embed watermarks.\n\u2022 Model weights. A representative model watermarking embeds watermarks into static weight space by regularizing the objective loss function or reordering weight importance.\nEarly watermarking schemes often chose to incorporate additional terms into the original loss function. Uchida et al. [123] is the first that introduced the watermarking IPP scheme in a white box setting by incorporating the watermark regularizer into the original task loss. Nevertheless, this watermarking drastically modifies the weight distribution and can be easily detected and evaded. On this basis, many schemes [83, 134] have been proposed to enhance covertness and robustness. The Robust whlte box generative adversarial network (GAN) watermarking (RIGA) was introduced by Wang et al. [134]. It utilizes the concept of adversarial training to guarantee the consistency of weight distributions between the watermarked and original models. Lv et al. [83] divided a pretrained autoencoder's weight space into two pieces: embedded piece (i.e., encoder) as the watermark, and secret piece (i.e., decoder) as the local secret. The owner injects the encoder's weights into the DL model weight space. Two pieces are later combined for IP infringement detection."}, {"title": "\u0125", "content": "$$\\hat{h} = K \\otimes h, K = B_E \\odot B_C,$$"}, {"title": "3.2.2 Query-based watermarking", "content": "When query-only black box verification is allowed, the model owner can only query some trigger-carrying samples to the suspect model and then observe whether the suspect model's predictions exhibit a preset output pattern.\nThe workflow of such watermarking schemes can be divided into three stages: (i) generating or selecting trigger-carrying sample pairs ($x_t$, $y_t$) to construct a dataset $D_T$; (ii) fine-tuning the model I with the trigger dataset or jointly retraining it with the original dataset D to embed the watermark. Thereby the watermarked model $O_{wm}$ will exhibit preset output patterns; (iii) querying suspect models using the trigger-carrying sample set to obtain predictions and"}, {"title": "3.2.3 Generative content-based watermarking", "content": "In the most stringent case of black box verification, the defender can only obtain the model-generated content, such as images generated by GANs or DMs. Thus, Watermarks can also be embedded in the outputs according to predefined rules. The application scenario of this kind of solution is generative models. The schemes can be divided into the following categories according to the type of the protected model.\n\u2022 Watermarking auto-encoders. SSLGuard [20] draws inspiration from a mathematical proposition: two random vectors in high-dimensional space are nearly orthogonal. Conversely, a notable average cosine similarity, significantly greater than 0 or approaching 1, indicates a strong correlation with these vectors. Defenders can refine a clean encoder to convert trigger-carrying samples Dr into embeddings, and then train a decoder to further convert the embeddings into decoding vectors with high cosine similarity to secret vector $s_k$.\nThe work [20] focuses on scenarios where the encoder is deployed separately without downstream classifiers. However, if attackers steal the encoder and customize it to downstream tasks via transfer learning, ownership cannot be verified because the stolen encoder and its embeddings are typically inaccessible. To overcome the challenges, SSL-WM [84] maps watermarked inputs into the encoder's invariant representation space, which makes any downstream classifier produce the expected behavior. Subsequently, the owner employs an outlier detection algorithm to monitor the downstream classifier's outputs."}, {"title": "3.3 Model Fingerprinting", "content": "According to the objective of model similarity comparison (i.e. model property or model behavior), fingerprints can be divided into two categories.\n3.3.1 Static property-based fingerprinting. The defender can extract the static properties of the model (such as model parameters, training paths) as fingerprints, etc.\n\u2022 Model parameter. The defender can convert the model parameters to hash codes and then verify model ownership by comparing the similarity of hash codes. Chen et al. [9] selected the largest-absolute-values weights and employed model compression techniques to compute normal test statistic for each weight segment to get fixed-length hash codes"}, {"title": "3.3.2 Dynamic behavior-based fingerprinting", "content": "The defenders expect that there exists a subclass of targeted, transferable, adversarial fingerprints that can trigger the model-specific knowledge that is shared by source models and their derivations while not shared by any other unrelated models.\n\u2022 Misclassification-based. Test samples with misclassification behavior can be selected either from natural samples or iteratively optimized from an initial data point along the gradient of the objective function."}, {"title": "4 Proactive Model Intellectual Property Protection", "content": ""}, {"title": "4.1 Overview", "content": "The reactive schemes outlined above claim ownership after the infringement has already occurred, which is both delayed and insufficient. To prevent model theft proactively, an increasing number of researchers are focusing on proactive IPP. According to the management perspective, the proactive IPP has the following types:\n\u2022 Proactive authorization control: The proactive defenders focus on maintaining the DL model's functionality for authorized users while rendering them dysfunctional for unauthorized users. Moreover, user identity management and authorization control can be further combined in order to track betrayers when model infringement occurs.\n\u2022 Domain authorization control: Comprehensive IPP necessitates thorough consideration of domain authorization control in order to prevent authorized users from transferring the model to any tasks without restriction, which potentially leads to implicit infringement.\nOverall, proactive authorization control is suitable for directly preventing unauthorized use, while domain authorization control is more concerned with limiting the scope of authorized use."}, {"title": "4.2 Proactive Authorization Control without Tracking", "content": "It involves distinguishing between authorized and unauthorized users and providing different functionalities and performance based on the user's identity. This is achieved through a variety of techniques, mainly relying on techniques"}, {"title": "4.3 Proactive Authorization Control with Tracking", "content": "The previously discussed proactive authorization control schemes are vulnerable to attacks initiated by dishonest users, such as collusion attacks. The following works focus on embedding unique user identity keys for traitor tracking. The classification of these three categories is mainly based on the different carriers of user identity information.\n1) Model weights. This category generates a unique binary identity key for each authorized user and embeds it into the model weight space. The first anti-collusion security framework, called DeepMarks, was presented by Chen et al. [8]. For each user, it creates a unique n-bit binary code and then embeds code into the host network weights. However, as the weight space containing identity information is redundant rather than essential, adversaries can readily remove it through fine-tuning. Tang et al. [117] introduced termed deep serial number (DSN) to produce unique serial numbers as unique binary identity keys for each authorized user. It first trains a teacher model and then utilizes knowledge distillation to transfer its knowledge to student models. Each student model has a unique serial number assigned to it during the distillation process. The user can only utilize the proper model if they supply a valid serial number. The designed DSN exhibits sufficient robustness when addressing various attack methods, such as model pruning, and watermark overwriting. This method seems costly because it requires to create a student model for each user.\n2) Backdoor samples. The user's identity key can uniquely correspond to the configured backdoor trigger pattern. Xue et al. [146] presented a user management framework based on multi-trigger backdoors, which incorporates N sub-backdoors into the DL model. Each authorized user is assigned with n (n<N) moderate-confidence backdoor signals as a unique identity key. To access the well-trained model, the user must present his backdoor instance set containing n backdoor signals. If the model exhibits specific behavior with medium confidence on his dataset, the user is deemed legitimate and granted access. A buyer-traceable DL model protection was developed by Wang et al. [131]. To build watermark samples, defenders augment clean samples with dirty labels. Each buyer is associated with a specific trigger. The model must be fine-tuned by the backdoor trigger set before it is accessible. The defense efficacy of the techniques in works [131, 146] suffers from a high false-positive rate in ownership verification.\n3) Adversarial examples. The model's different response behavior to adversarial samples can be a powerful tool for tracking user identity. ActiveGuard [144] leveraged well-crafted adversarial examples with specific classes and confidence to serve as users' fingerprints. Unauthorized users will experience a significant reduction in model performance. DeepAuth [49] treats the model predictions of adversarial samples as the unique and fragile signature for each protected DL model. This design aims to produce adversarial samples near the DB by calibrating the latent space and aligning gradient directions."}, {"title": "4.4 Domain Authorization Control", "content": "For authorization control, prior solutions focus on granting user permission to utilize the model; nevertheless, authorized users retain the freedom to apply the model to any application domain. Consequently, several studies have introduced domain authorization schemes that aim to substantially degrade the well-trained model performance on unauthorized"}, {"title": "5 Dataset Intellectual Property Protection", "content": ""}, {"title": "5.1 Overview", "content": "IPP has overwhelmingly focused on protecting well-trained models and verifying the model creator's identity. Nevertheless, the DL model will infringe on the dataset's ownership if trained on a dataset without authorization. In such cases, model IPP methods fail to establish a clear link between the DL model and the protected training data. How to verify the ownership of the target data with respect to a suspect ML model remains largely open. The protected data can have diverse modalities; they can be text, pictures, non-fungible tokens, radio data, GPS data, etc."}, {"title": "5.2 Dataset Watermarking", "content": "According to the crafting means of watermark samples, can be subdivided into the following three categories:"}, {"title": "5.4 Dataset Authorization Control", "content": "As a proactive IPP method, the data owner can disrupt illegal use by adding imperceptible or reversible perturbations. The work [145] first generates adversarial examples by perturbing the images in feature space. The clean image was then hidden in the matching perturbed image by employing the modified reversible image transformation technique to produce protected images. Following the transformation process, a secret key will be created and safely kept. Hence, for unauthorized users without the secret key, the protected dataset is directly used to train the unauthorized model, which will make the trained model have a very poor inference accuracy.\nRecent efforts have been made to finetune open-source text-to-image DMs with additional samples from specific artists to generate AI art that mimics the specific artistic style of the artist. Shan et al. [109] argued that this behavior essentially infringes on the copyrights of the artists, as the artistic style is also an IP form that needs to be protected. Thus, Glaze is devised to allow artists to add imperceptible perturbations to their artworks from style mimicry before sharing them online. This operation, on the one hand, does not affect the visual effect of the artworks, while on the other hand, it can mislead generative models to produce artworks that differ from the style of the target artist."}, {"title": "6 Distributed Intellectual Property Protection", "content": ""}, {"title": "6.1 Overview", "content": "As the demand for processing training data has outstripped the growth in computational capabilities of individual machinery [166], there is a need to transition from centralized ML to distributed ML (DML). In addition, DML is primarily motivated by privacy-preserving incentives, ensuring that local private data remains on the device and is not exposed to external entities. Various DML schemes exist, including federated learning (FL), split learning (SL), peer-to-peer learning, and private aggregation of teacher ensembles (PATE). Although data is not explicitly exchanged in the DML, the training process requires the sharing of information about participants' models. This makes it trivial for a malicious actor to steal or distribute the individual model without authorization.\nHowever, the majority of IPP methods now in use are created within centralized learning frameworks and are unsuitable for DML due to differences in data access privileges. Although DML has its advantages, it also brings forth the issue of preserving ownership. Since the FL situation is the primary focus of existing IPP techniques, we shall analyze the challenges presented by DML by taking FL as an example:\n\u2022 The fact that the server cannot access the client's dataset and only learns about the target task poses a significant challenge to IPP methods that require access to the raw training data to construct query samples.\n\u2022 In centralized learning, it is only necessary to perform a single injection of IP identifiers. However, FL involves multiple rounds of communication, so IPP injection from the beginning to the end is required.\n\u2022 Multiple clients, some of whom may be malicious, may join forces to launch the collusion attack, which enables them to gain a faster and more comprehensive understanding of the model by sharing information they have obtained and thus jointly bypassing or identifying IPP mechanisms.\n\u2022 Conflicts between the watermarks might easily arise when multiple clients are permitted to inject watermarks. Watermark collisions can also easily happen, even when distinct trigger sets are intended for different clients.\n\u2022 How to ensure that embedded watermarks and fingerprints are compatible with various privacy-preserving approaches (e.g., differential privacy, secure aggregation) and client-side selection strategies is also an ongoing question.\n\u2022 Both communication and computation overhead are concerns considering the FL clients who are often resource-restricted."}, {"title": "6.2 Federated Model IPP", "content": "The first question in the client-server FL framework is which party can add the ownership credential to the model and needs to be protected. The parties responsible for injecting the IP identifiers are the IP owners. We distinguish the following three FL IPP scenarios: server-side, client-side, and collaborative federated model IPP. The server-side IPPs assume that the main adversaries are malicious clients and protect the server IP effort. Meanwhile, client-side IPPs target the semi-honest server and require multiple clients to inject their own watermarks. The collaborative IPPs, on the other hand, are suitable for scenarios with a high risk of privacy leakage and assume that both the clients and aggregator may not be trustworthy."}, {"title": "7 Attacks on Deep IPP", "content": ""}, {"title": "7.1 Overview of Threats", "content": "The IPP for model and data has taken on new significance in the digital age. But these so-called defense mechanisms might open up new attack vectors, leaving the protected models even more vulnerable to infringement than the unprotected models. For instance, watermarks may significantly alter the distribution of model weights, and adversaries may be able to identify and manipulate the watermarks. We categorize the different types of attacks against DL model IPP methods into the following two levels (from weak to strong): 1) IP detection and evasion; and 2) IP removal."}, {"title": "7.2 Level 1: IP Detection & Evasion", "content": ""}, {"title": "1) Threat model", "content": ""}, {"title": "\u2022 Adversary's goal", "content": "When embedding white box watermarks, the model's parameters or neuron states are often modified to some degree. In the case of black box watermarks, key samples typically contain specific trigger patterns, and the target labels are often manipulated. Therefore, attackers aim to exploit these anomalous attributes to detect the embedded watermarks or fingerprints, understand their operational model, and ultimately evade ownership verification."}, {"title": "\u2022 Assumptions", "content": "Attackers possess full access, including both parameters and structures, to multiple models that have been trained on datasets from the same domain. Besides, the attacker possesses a local shadow dataset, which can be used as a control group to analyze the abnormal behavior of query inputs."}, {"title": "2) Attack methods", "content": "Model property detection. Attackers can analyze the presence of watermarks by examining the distribution of model weights, intermediate outputs, and neural network states. These schemes require attackers to have the highest-level access to the model.\n\u2022 Model behavior detection. Attackers can detect and remove watermarks by reversing data triggers or injecting destructive perturbations. IP detection attacks against input samples are typically conducted under the condition that the means of sample generation are already known."}, {"title": "IP ambiguity", "content": "The attacker aims to lower the confidence of verification results by forging an additional IP identifier to overwrite or invalidate the original one."}, {"title": "\u2022 Collusion attack", "content": "It is an active attack. The group of authorized users with identical host DL models and distinct IP identifiers may engage in collusion attacks, aiming to develop a model with equivalent functionality or collude with unauthorized users to grant authorized access to multiple unauthorized users."}, {"title": "7.3 Level 2: IP Removal", "content": ""}, {"title": "1) Threat model", "content": ""}, {"title": "\u2022 Adversary's goal", "content": "Malicious attackers aim to remove built-in IP identifiers and create pirated models without watermarks or fingerprints while possessing the same performance as the pirated models."}, {"title": "\u2022 Assumptions", "content": "Adversaries can gain access with either white box or black box privileges. With white box access, adversaries can adjust the model weights or structure, such as through model fine-tuning, fine pruning, regularization, and neural cleansing. With black box access, adversaries learn a pirated model from the protected model's predictions or properties, but they require sufficient in-distribution data to support this learning."}, {"title": "2) Attack methods", "content": "Model modification. Recent studies suggested using either fine-tuning [17, 18] or training regularization [32, 99] to remove watermarks.\n\u2022 Model extraction. Model extraction learns model copies from model predictions or attributes, such as query-based steganography attacks and side-channel attacks. Typically, model extraction has stronger performance in IP removal, but requires larger datasets."}, {"title": "8 Challenges and Prospects", "content": "Here, we draw attention to the challenges associated with deep IPPs and prospect promising future directions that may act as a guide for innovative research. We prioritize challenges and directions with significant practical impact, introducing them in order of importance."}, {"title": "8.1 Usable Unified Verification Metrics", "content": "Unified evaluation metrics are fundamental for the practical implementation of deep IPPs. However, we perceive that the existing verification metrics have the following shortcomings: (1) The existing evaluation standards are mostly targeted at reactive schemes, lacking assessment of proactive IPP represented by proactive authorization control. (2) Evaluation is mostly focused on performance metrics (such as fidelity, capacity, etc.), while research on anti-attack metrics (such as security, non-removability, etc) is insufficient. Therefore, designing a safe, effective, comprehensive verification metric that can serve as a unified standard is worth careful consideration."}, {"title": "8.2 Model and Dataset Intelligence IPP", "content": "In the current DL field, deep IPP should be extended from the model level to the dataset level. Specifically, training datasets, especially those that have been carefully collected, cleaned, and annotated, are also an important part of a participant's core competitiveness. In addition, the output data of DL models, such as generated images, voice, and text, also have certain artistic value and market potential. For example, paintings created by AI models can, in some cases, rival or even surpass the works of human artists, and their need for IPP is equally urgent [109]. Therefore, how to effectively manage and protect the IP of these datasets to prevent unauthorized access, copying, or use has become an urgent problem that needs to be solved."}, {"title": "8.3 Beyond Classification Tasks", "content": "Current IPPs focus primarily on classification tasks. However, few works focus on more complex and increasingly prevalent types of tasks, such as object detection, image generation and segmentation, speech recognition, and text generation that are built on vast datasets and complex architectures. These models not only push the limits of AI, but they also create new technical challenges. For example, they make it easier to process data across multiple modalities (like text and images together). Implementing IPP in these advanced models faces multiple challenges. Therefore, future research on deep IPP should broaden its perspective, not only focusing on basic classification tasks but also delving into protection strategies for complex task scenarios and large foundation models."}, {"title": "8.4 Theoretical Analysis and Proof", "content": "The current deep IPP methods still lack a rigorous theoretical analysis", "38": ".", "future": 1}]}