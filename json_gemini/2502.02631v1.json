{"title": "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization", "authors": ["Zechun Liu", "Changsheng Zhao", "Hanxian Huang", "Sijia Chen", "Jing Zhang", "Jiawei Zhao", "Scott Roy", "Lisa Jin", "Yunyang Xiong", "Yangyang Shi", "Lin Xiao", "Yuandong Tian", "Bilge Soran", "Raghuraman Krishnamoorthi", "Tijmen Blankevoort", "Vikas Chandra"], "abstract": "The optimal bit-width for achieving the best trade- off between quantized model size and accuracy has been a subject of ongoing debate. While some advocate for 4-bit quantization, others pro- pose that 1.58-bit offers superior results. How- ever, the lack of a cohesive framework for dif- ferent bits has left such conclusions relatively tenuous. We present ParetoQ, the first unified framework that facilitates rigorous comparisons across 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit quan- tization settings. Our findings reveal a notable learning transition between 2 and 3 bits: For 3- bits and above, the fine-tuned models stay close to their original pre-trained distributions, whereas for learning 2-bit networks or below, the represen- tations change drastically. By optimizing train- ing schemes and refining quantization functions, ParetoQ surpasses all previous methods tailored to specific bit widths. Remarkably, our ParetoQ ternary 600M-parameter model even outperforms the previous SoTA ternary 3B-parameter model in accuracy, using only one-fifth of the parameters. Extensive experimentation shows that ternary, 2- bit, and 3-bit quantization maintains comparable performance in the size-accuracy trade-off and generally exceeds 4-bit and binary quantization. Considering hardware constraints, 2-bit quantiza- tion offers promising potential for memory reduc- tion and speedup.", "sections": [{"title": "1. Introduction", "content": "As deep learning continues to scale toward larger mod- els and datasets, significant attention has been devoted to studying the scaling laws that trade-off between model and dataset size to optimize performance and computational effi- ciency (Hoffmann et al., 2022; Kumar et al., 2024; Dettmers & Zettlemoyer, 2023). In the meantime, the field is shifting toward lower-precision computation, particularly in large language models, driven by the substantial benefits of mem-"}, {"title": "ParetoQ", "content": "L(N,D, P, Strain, F), comprising five dimensions.\nTo disentangle these complexities, we first identify the opti- mal training strategy for plausible quantization functions in each bit width, L(N, D, Strain | P, F). Subsequently, with the optimal training strategy (Strain) and the token count (D*) required for saturation, we determine the best quantiza- tion function for each bit, L(N, F | P, D*, Strain). Results highlight that quantization grids and ranges are pivotal in the sub-4-bit regime, with a sharp learning behavior transition between 1-bit/1.58-bit/2-bit and 3-bit/4-bit.\nBased on the findings, we derive ParetoQ, the first frame- work that unifies the training and quantization scheme in sub 4-bit regime. Rather than fitting hypothetical scaling laws for quantization, ParetoQ demonstrate its robustness by yielding state-of-the-art (SOTA) models at all bit widths, surpassing prior works tailored for individual bit levels.\nThese SOTA points in the Pareto chart ensure that our scal- ing law comparisons are both reliable and consistent, as they derive from homogeneous settings. Leveraging ParetoQ, we identify the optimal bit-width for minimizing loss within the effective quantized model size, L(N,P|F*, D*, Sain). Our scaling laws reveal that binary quantization significantly compromises accuracy, while ternary, 2-bit and 3-bit quanti- zation are tied in performance, often surpassing 4-bit. The tiebreaker lies in the kernel implementation, which drives real memory savings and speedups. 1.58-bit and 3-bit quan- tization are in general less hardware-friendly than 2-bit. We implemented an optimized 2-bit CPU kernel and our results indicate that 2-bit quantization achieves higher speed at the same accuracy compared to 4-bit.\nThe key contributions of this study are as follows:\n\u2022 We present a comprehensive study on the intertwined ef- fects of QAT budget allocation and the specific choices of quantization functions across 8 models (125M to 3B) and 5 quantization strategies. Our study highlights the unique char- acteristics and challenges of binary, ternary, and 2/3/4-bit quantization, offering actionable insights and best practices for achieving optimal accuracy-efficiency trade-offs.\n\u2022 We introduce ParetoQ, the first systematic, apples-to- apples comparison of quantization functions at extreme low- bit settings. Each point in the Pareto chart outperforms prior methods optimized for specific bit widths. Specifically, the 1.58-bit ParetoQ LLaMA-3 8B model reduces the perfor- mance gap to full precision by relatively 37.8% compared to the 1-bit Era's LLaMA-3 8B model (Ma et al., 2024), while using only 30% of the training tokens.\n\u2022 Our research highlights the potential of 2-bit quantization as a prospective alternative to the traditional 4-bit approach, offering improved accuracy-size trade-off, as underlined in Figure 1. Preliminary speed benchmarks also demonstrate"}, {"title": "2. A Better QAT Scheduling Strategy for Extreme Low-Bit LLMS", "content": "In this work, we systematically investigate trade-offs involv- ing bit precision (P), quantization functions (F), model size (N), training strategies (Strain) and training token (D).\nL(P,F,N,Strain, D) (1)\nGiven the vast search space defined by these variables, we first fix the quantization method (F) and explore the dimen- sions of bit precision (P), training strategies (Strain) and training tokens (D) in this section."}, {"title": "2.1. Training Budget Allocation", "content": "Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) are two primary quantization approaches. PTQ applies quantization after full-precision training, sim- plifying deployment but often leads to significant perfor- mance loss at bit widths below 4 bits. In contrast, QAT incorporates quantization during training to optimize model performance for low-bit-width representations.\nHere we start by answering a key question:\nGiven a fixed training budget (in #tokens) Btrain = BFPT+ BQAT, how should the budget be optimally allocated be- tween full-precision training (BFPT) and quantization- aware training/fine-tuning (BQAT) to maximize the accu- racy of the quantized model?\nThis question is both technically intriguing and practically"}, {"title": "ParetoQ", "content": "BFPT, BOAT = arg min\nBFPT+BQAT Btrain\nL(BFPT, BQATIN,P) (2)\nwhere BPPT and BOAT describe the optimal allocation of a computational budget Btrain. We utilize Btrain to incorporate training tokens utilization (D) into the training strategy (S). Specifically, we evaluate various allocation ratios of BFPT and BQAT On MobileLLM-125M across four bit-widths ( 1.58-bit, 2-bit, 3-bit, and 4-bit). The FP models undergo a complete learning rate scheduling cycle for BFPT tokens, fol- lowed by another cycle for QAT for BQAT tokens. Detailed experimental settings are provided in the appendix.\nFigure 2 reveals a distinct upward trend in the full-precision pre-training proportion versus accuracy curve. Notably, ac- curacy peaks at ~ 90% FPT allocation for almost every bit-width choice, then decline sharply when FPT exceeds 90%, likely because this leaves insufficient tokens and train- ing capacity for QAT. This leads to our first key finding:\nFinding-1 QAT finetuning consistently surpasses both PTQ with BFPT = Btrain and QAT from scratch with BQAT = Btrain. Optimal performance is nearly achieved by dedicating the majority of the training budget to full precision (FP) training and approximately 10% to QAT."}, {"title": "2.2. Fine-tuning Characteristics", "content": "Then we investigate the impact of finetuning tokens across various bit choices, spanning 7 architectures and 5 bit levels. Results in Figure 3 offer several key insights:\n1. Fine-tuning benefits across all bit-widths: This obser-"}, {"title": "ParetoQ", "content": "Finding-2 While fine-tuning enhances performance across all bit-widths, even binary and ternary, optimal fine-tuning effort inversely correlates with bit-width. For 3-bit and 4-bit weights, fine-tuning adjusts within a nearby grid to mitigate accuracy loss, and requires less finetuning to- kens. In contrast, binary and ternary weights break the grid, creating new semantic representations to maintain performance, requiring longer finetuning."}, {"title": "3. A Hitchhiker's Guide to Quantization Method Choices", "content": "We have examined the impact of training strategy and bud- get allocations (Btrain, BQAT) on scaling laws. Building on the optimal training practices outlined in Section 2, we fo- cus on a critical yet often overlooked factor: the choice of quantization functions (F).\nF* = arg min L(F|P, BOAT) (3)\nThe significance of this choice has been largely underesti- mated in prior scaling law studies (Kumar et al., 2024). Our results show that, especially at sub-4-bit quantization, the choice of function is highly sensitive and can drastically alter scaling law outcomes. An improper selection can dis- tort performance and lead to entirely different conclusions, underscoring the need for a careful design of F."}, {"title": "3.1. Preliminary", "content": "In general, a uniform quantization function is expressed as\nQi\nWa= WR\n1 + B\nR (4)"}, {"title": "ParetoQ", "content": "Here WQ represents quantized weights, WR denotes their real-valued counterparts (Nagel et al., 2021; Krishnamoor- thi, 2018). Key design choices focus on scale \u03b1 and bias max(|WR|) B. For symmetric min-max quantization, a = 2N-1-1 and B 0. In asymmetric min-max quantization, a = max(WR)-min(WR) and \u1e9e = min(WR). Symmetric min- 2N-1 max quantization is prevalent for weights \u2265 4 bits, while sub-4-bit quantization requires distinct functions.\nFor binary quantization, assigning the sign of full-precision weights (WR) to binary weights (WB) is a commonly used approach (Rastegari et al., 2016; Liu et al., 2018): W = a. Sign(WR), where a = ||WR||11\nNWR\nIn ternary quantization, ternary weights are often given by\nW = a Sign(WR)1||w>, with \u0394 = 0.7||WR||1 NWR\nWR1w> (Zhang et al., 2020; Liu et al., and a = \u03a3 2023a). Besides binary and ternary quantization, there is less work targeting 2-bit or 3-bit integer quantization func- tion design. Directly using min-max quantization for them will lead to performance collapse."}, {"title": "3.2. Introducing ParetoQ", "content": "In sub-4-bit quantization, design requirements vary signifi- cantly across bit levels. Equal attention to each bit choice is crucial for accurate, reliable comparisons."}, {"title": "3.2.1. TRADE-OFFS", "content": "We identify two key trade-offs in low-bit quantization for LLMs: (1) Outlier precision vs. intermediate value precision and (2) Symmetry vs. inclusion of \u201c0\u201d at the output level.\n(1) Range clipping Outliers challenge LLM quantiza- tion (Lin et al., 2023; Liu et al., 2024a), especially when using min-max ranges for weight quantization for extremely low-bit quantization. As seen in Figure 6 (b)-(e), min-max quantization works at 4 bits but loses accuracy at lower bit-widths. On the other hand, range clipping improves"}, {"title": "ParetoQ", "content": "Finding-3 Extreme low-bit quantization is highly sensitive to quantization function selection, with no single optimal function for all bit widths. Learnable range settings out- perform statistics-based methods due to their flexibility in optimizing range parameters with respect to the final loss. Ternary and 2-bit quantization favor symmetric levels and balanced range coverage in quantization grid configuration, while imbalance levels with \u201c0\u201d in output levels are more effective for 3 and 4-bit quantization."}, {"title": "3.2.2. QUANTIZATION FUNCTION", "content": "Based on our analysis, we integrate the optimal quantization functions identified for each bit-width into one formula, denoted as Paretoq. This includes Elastic Binarization (Liu et al., 2022) for 1-bit quantization, LSQ (Esser et al., 2019) for 3 and 4-bit quantization, and the proposed SEQ for 1.58 and 2-bit quantization:\nW = Wa\n{\na.Sign(WR),\na([Clip(WR,-1,1) \u00d7 k/2 -0.5)+0.5)/k \u00d7 2,\na[Clip(Wh, n, p)],\nif Nbit = 1\nif Nbit = 1.58, 2\nif Nbit = 3,4\n(6)\nHere k equals 3 in the ternary case and 2Nbit otherwise; n = \u22122Nbit-1 and p = 2Nbit-1 \u2212 1. In the backward pass, the gradients to the weights and scaling factor can be easily"}, {"title": "ParetoQ", "content": "calculated using straight-through estimator:\nOW STE\nVR\nWW R\n2\n{\u03b1.Sign(WR),\n1 ww\n1 W<<1\n1 w\n1 WR<<p\nif Nbit = 1,1.58, 2\nif Nbit = 3,4\n(7)\nSTE WR WO 1 wa, if Nbit = 1.58, 2 (8)\n\u03b1\nR\nRWR <p, if Nbit = 3,4\nFor the initialization of a, we use \u03b1 = ||WR||11\nNWR for the binary case, since the scaling factor has the closed-form solution to minimizing quantization error: E = ||a\u0174Q \u2212 WR||2. For the other cases, we simply initialize a as the maximum absolute value of the weights. For ternary and max(|WR|) quantization, a = associated with SEQ max(|WR|) quantizer, and for 3-bit and 4-bit cases, a = associated with LSQ quantizer.\np\nWith ParetoQ, we present a robust comparison framework across five bit-widths (1-bit, 1.58-bit, 2-bit, 3-bit, 4-bit), each achieving state-of-the-art accuracy. This facilitates direct, apple-to-apple comparisons to identify the most ef- fective bit-width selection."}, {"title": "4. Pareto-Optimality of Extremely Low-Bit LLM", "content": "To ensure a consistent apples-to-apples performance com- parison across different bit-width configurations, we first determined the optimal training setup (Brain) in Section 2 and the quantization function (F*) for each bit in Section 3. Using this unified framework for all bit widths, we exam- ine the trade-off between model size and quantization bit: L(P,N|F*, Brain)."}, {"title": "ParetoQ", "content": "4.1. Accuracy-compression Trade-off\nIn on-device deployment scenarios, such as wearables and portables, storage constraints often limit the capacity of large language models (LLMs). To optimize performance within these constraints, quantization is essential. A com- mon dilemma is whether to train a larger model and quantize it to a lower bit-width or to train a smaller model and quan- tize it to a higher bit-width.\n4-bit quantization-aware training (QAT) achieves near- lossless compression in many scenarios, making it widely adopted. However, the landscape below 4-bit remains un- clear, with limited comparative analysis. Previous claims about ternary models matching 16-bit performance (Ma et al., 2024) were based on lower FP16 baselines than cur- rent standards. Spectra's comparisons between ternary QAT and 4-bit PTQ fall short of a fair evaluation due to inconsis- tencies in the training schemes used (Kaushal et al., 2024a).\nWith Paretoq, we are able to improve the analysis. Figure 7 (a) demonstrates that sub-4-bit quantization, including bi- nary, ternary, 2-bit, and 3-bit, often surpasses 4-bit. Notably, 2-bit and ternary models reside on the Pareto frontier. For instance, a 2-bit MobileLLM-1B model achieves 1.8 points higher accuracy than a 4-bit MobileLLM-600M model, with even smaller model sizes. This trend persists across larger LLaMA models, as shown in Figure 7 (b), demonstrating the potential of lower-bit quantization for achieving both higher accuracy and compression. We calculate the effec- tive quantized model size as (#weights \u00d7 weight-bits + #embedding-weights \u00d7 embedding-bits)/8. More compre- hensive analysis is provided in the Appendix."}, {"title": "4.2. Hardware Implementation Constraints", "content": "In practical deployment, both memory limitations and hard- ware constraints must be considered. While 2-bit and ternary quantization sit on the accuracy-size Pareto frontier, 2-bit quantization is generally more feasible due to practical chal- lenges. Ternary quantization, using a 1.58-bit format with values {-1,0,1}, appears more storage-efficient but is in- efficient in implementation. Storing ternary values with"}, {"title": "5. Experiments", "content": "In this section, we compare each point on our Pareto chart with prior methods in the literature. As the first ap- proach to unify training and quantization schemes in the sub-4-bit regime, we evaluate our method against special- ized techniques for each bit setting. This includes binary quantization methods: BiLLM (Huang et al., 2024), ARB- LLM (Li et al., 2024), PB-LLM (Shang et al., 2023), and DB-LLM (Chen et al., 2024a); ternary quantization meth- ods: TernaryLLM (Chen et al., 2024c), 1-bit Era (Ma et al., 2024), and Spectra (Kaushal et al., 2024b); and lower-bit QAT methods: LLM-QAT (Liu et al., 2023c) and Efficien- tQAT (Chen et al., 2024b) as well as PTQ methods like GPTQ (Frantar et al., 2022), OmniQ (Shao et al., 2023), SpinQuant (Liu et al., 2024a), QuIP (Chee et al., 2024) and AWQ (Lin et al., 2023). We also compare with a post- training vector quantization method AQLM (Egiazarian et al., 2024).\nWe demonstrate that ParetoQ, with a unified scheme span- ning five distinct bit settings (1, 1.58, 2, 3, and 4 bits), consistently outperforms previous methods specialized for each bit level, including both PTQ and QAT approaches. The performance gains are particularly pronounced in the 1, 1.58, and 2-bit settings, underscoring the robustness and reliability of our conclusions regarding scaling laws."}, {"title": "5.1. Experimental Settings", "content": "We conduct experiments on eight models including Mo- bileLLM (Liu et al., 2024b) 125M/350M/600M/1B/1.5B"}, {"title": "ParetoQ", "content": "5.2. Main Results\n5.2.1. 1 / 1.58 / 2-BIT COMPARISON ON 8B MODEL\nLet's first examine the comparison on 8B parameter models. As depicted in Table 1, in the 2-bit quantization setting, previous methods, including both PTQ and QAT, experi- ence a significant drop in accuracy. Among PTQ methods, the vector quantization method AQLM (Egiazarian et al., 2024) effectively mitigates some of the quantization loss, achieving 64.1 points, it falls 10.5 points short of full pre- cision. The best quantization-aware training method, Ef-"}, {"title": "ParetoQ", "content": "settings, our approach consistently resides on the Pareto front, with a particularly pronounced advantage in lower-bit quantization settings. These results confirm that our bit-accuracy trade-off conclusions are benchmarked against SOTA results across all bit settings, ensuring its reliability."}, {"title": "6. Related Work", "content": "The quantization of Large Language Models (LLMs) has emerged as a pivotal research area, driven by the impera- tive to reduce computational and memory demands while preserving model performance (Liu et al., 2023c; Dettmers et al., 2022; Xiao et al., 2022). A notable trend is the quanti- zation of LLMs to lower bit-widths (Ma et al., 2024; Kaushal et al., 2024b).\nInitial efforts, such as LLM.int8() (Dettmers et al., 2022) and SmoothQuant (Xiao et al., 2022), concentrated on quan- tizing LLMs to 8-bit weights and 8-bit activations. Subse- quently, numerous studies have demonstrated the feasibility of quantizing LLMs to 4-bit with minimal accuracy degra- dation, employing both post-training quantization (PTQ) methods (Kim et al., 2023; Frantar et al., 2022; Liu et al., 2024a; 2023b) and quantization-aware training (QAT) (Liu et al., 2023c; Chen et al., 2024b; Bondarenko et al., 2021).\nRecently, research has shifted towards sub-4-bit quantiza- tion. Some PTQ methods target 3-bit or 2-bit integer quan- tization (Shao et al., 2023; Zhao et al., 2023; Chee et al., 2024; Ashkboos et al., 2023; Lin et al., 2023; Frantar et al., 2022), or employ vector quantization (Egiazarian et al., 2024; Tseng et al., 2024; van Baalen et al., 2023). Other PTQ approaches even achieve binary weight quantization (Huang et al., 2024; Shang et al., 2023; Chen et al., 2024a; Li et al., 2024). Most recently, two QAT studies have claimed that ternary quantized models, trained from scratch, can match the accuracy of full-precision models with equiva- lent training (Ma et al., 2024; Kaushal et al., 2024b). It generated heated debate within the field, with many practi- tioners expressing reservations about this conclusion. To our knowledge, no existing work unifies sub-4-bit quantization schemes to derive a solid conclusion on which bit-width achieves the Pareto optimal in the efficiency-accuracy trade- off. This work presents Paretoq to fill that gap."}, {"title": "7. Conclusions", "content": "In this study, we have performed an in-depth analysis of the intricate relationships among model parameters (N), train- ing data volume (D), quantization training schemes (Btrain), quantization precision (P), and the selection of quantization functions (F) in relation to the model's final loss, expressed as L = f(N, D, P, Btrain, F). To address these multifaceted challenges, we propose ParetoQ, an advanced quantization framework that achieves state-of-the-art performance across"}, {"title": "ParetoQ", "content": "A. Appendix / supplemental material\nA.1. Complete Results of Figure 8\nTable 2 presents the numerical results of Figure 8. We evaluate accuracy across eight zero-shot commonsense reasoning tasks: ARC-easy, ARC-challenge (Clark et al., 2018), BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), and WinoGrande (Sakaguchi et al., 2021), along with perplexity on the WikiText2 test set (Merity et al., 2016). Our results are compared against prior state-of-the-art ternary quantization-aware training works, including 1-bit era (Ma et al., 2024) and Spectra (Kaushal et al., 2024a). We also include the comparison to LLM-QAT (Liu et al., 2023c). Consistent with previous methodologies (Ma et al., 2024; Kaushal et al., 2024a), we quantize all weights to low-bit, excluding the embedding and output layers. The ParetoQ 3B ternary model is quantized from LLaMA3 (AI@Meta, 2024) 3B model, while other models are quantized from MobileLLM (Liu et al., 2024b). As Spectra did not report results on the SIQA and OBQA datasets, the values in Figure 8 represent the average accuracy across the remaining six tasks.\nA.2. Complete Results of Figure 9\nIn Tables 3, 4, and 5, we provide detailed results corresponding to Figure 9. We compare ParetoQ against LLM-QAT (Liu et al., 2023c), GPTQ (Frantar et al., 2022), AWQ (Lin et al., 2023), OmniQuant (Shao et al., 2023), and SpinQuant (Liu et al., 2024a). Following the common practice (Frantar et al., 2022; Liu et al., 2023c), we apply low-bit quantization to all weights, except for the embedding and output layers.\nA.3. CPU Latency Experimental Setup\nWe measure the CPU latency of five MobileLLM models on an Apple M1 MacBook Pro (32GB RAM) using 6 threads. Each evaluation uses 5 prompt tokens and generates 122 tokens. For the quantized models, embedding and output layers are quantized to 8-bit precision using channel-wise quantization, while weights in fully connected layers are quantized to 2-bit or 4-bit precision. Accuracy and decoding speed (in tokens/s) were measured under identical settings."}, {"title": "ParetoQ", "content": "A.4. GPU Latency Experimental Setup and Results\nWe measured the latency of LLaMA 3.2 models (1B, 3B, 8B) on an H100 NVL GPU (94GB memory). The W4A16 kernel used the Machete kernel from vLLM (Kwon et al., 2023; Wilkinson, 2024), while the W2A16 kernel was implemented based on the CUTLASS mixed precision backbone kernel. All tests were performed on a single GPU with a context length of 2048 tokens. For kernel-level latency, we compared the 2-bit kernel to the 4-bit Machete kernel across three weight shapes: (4096 \u00d7 4096), (8192 \u00d7 8192), and (16384 \u00d7 16384).\nFor smaller models (1B, 3B, 8B), the performance speed-up from reducing weight precision from 4-bit to 2-bit is minimal. This is due to the impact of conversion overhead, which becomes more pronounced when the weight size is small. Since the in-kernel conversion latency ratio is higher for smaller models, the benefits of 2-bit quantization are outweighed by the overhead. Consequently, 4-bit quantization achieves a more favorable speed-accuracy trade-off in these settings, offering better overall performance. In comparison, for larger weight shapes (16384 \u00d7 16384), the 2-bit kernel provides a substantial speedup, achieving 4.14\u00d7 faster performance than FP16 and 1.24\u00d7 faster than the Machete 4-bit kernel.\nA.5. QAT Scheduling Experimental Setup\nThe total training budget (Btrain) is set to 100B tokens. We vary the proportion of tokens allocated for full-precision training versus quantization-aware training (QAT) finetuning, sweeping the ratio across [0, 0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99, 1]. Here, a ratio of 0 corresponds to QAT from scratch, while a ra- tio of 1 represents full-precision training followed by post-training quantization (PTQ).\nFor full-precision training, we use 8\u00d78 GPUs, a batch size of 16, a weight decay of 0.1, an initial learning rate of 2.5 \u00d7 10\u22123, and a linear learning rate decay to zero. For quantized network training, we also use 8\u00d78 GPUs but with a batch size of 8, no weight decay, an initial learning rate of 1 \u00d7 10-4, and a linear learning rate decay to zero.\nA.6. Embedding Bit Precision vs. Accuracy Trade-off\nDespite the prevalent practice of not quantizing embedding and output layers, as noted in prior works such as Frantar et al. (Frantar et al., 2022) and Ma et al. (Ma et al., 2024), our study extends the scaling law analysis by examining the impact of quantizing these layers. As illustrated in Figure 11, utilizing 4-bit embeddings or matching the bit precision of embeddings to that of weights positions these configurations on the Pareto front, in contrast to employing 8-bit or 16-bit embeddings.\nA.7. Weight Bit Precision vs. Accuracy Trade-off\nFor the trade-off between weight-bit precision and model accuracy, we consider two configurations: 4-bit embeddings and embeddings with the same bit precision as weights. In both scenarios, lower-bit quantization, such as 1.58-bit, 2-bit, and 3-bit, consistently outperforms 4-bit quantization, as depicted in Figure 12."}, {"title": "ParetoQ", "content": "A.8. Pareto Curve in More Tasks\nFurthermore, we present results from a question-answering task, TriviaQA (TQA) (Joshi et al., 2017), and a reading comprehension benchmark, RACE (Lai et al., 2017), in Figures 13 The findings are consistent across these tasks: 1-bit quantization yields the lowest performance, whereas 1.58-bit, 2-bit, and 3-bit quantization are comparable and generally surpass the performance of 4-bit quantization.\nAdditionally, for context-based word prediction (LAMBADA (Paperno et al., 2016)) and multiple-choice science questions (SciQ (Welbl et al., 2017)) in Figrue 14, the results also shows a clear trend of 2-bit residing on the Pareto optimal frontier, outperforming 4-bit."}]}