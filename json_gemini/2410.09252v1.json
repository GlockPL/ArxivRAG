{"title": "Reason Planner: Enhancing Autonomous Planning in Dynamic Environments with Temporal Knowledge Graphs and LLMs", "authors": ["Minh Pham-Dinh", "Michael Yankoski", "Munira Syed", "Trenton W. Ford"], "abstract": "Planning and performing complex interactive tasks, such as conducting experiments to determine the melting point of an unknown substance, is straightforward for humans but poses significant challenges for autonomous agents. We introduce ReasonPlanner, a novel generalist agent designed for reflective thinking, planning, and interactive reasoning. This agent leverages LLMs to plan hypothetical trajectories by building a World Model based on a Temporal Knowledge Graph. The agent interacts with its environment using a natural language actor-critic module, where the actor translates the imagined trajectory into a sequence of actionable steps, and the critic determines if replanning is necessary. ReasonPlanner significantly outperforms previous state-of-the-art prompting-based methods on the ScienceWorld benchmark by more than 1.8 times, while being more sample-efficient and interpretable. It relies solely on frozen weights thus requiring no gradient updates. ReasonPlanner can be deployed and utilized without specialized knowledge of Machine Learning, making it accessible to a wide range of users.", "sections": [{"title": "1 Introduction", "content": "Developing a generalist AI system capable of reasoning and planning in real-world environments has been a fundamental challenge in artificial intelligence. Traditional Reinforcement Learning (RL) methods have succeeded in decision-making tasks within Markovian environments, achieving remarkable results in domains such as Atari games [15], chess (MuZero) [18], and Minecraft (Dreamer) [7]. However, RL methods often perform poorly when the action space is large and non-discrete, such as in textual environments like Jericho Worlds [8], where the action space can grow polynomially [10]."}, {"title": "2 Background & Related Work", "content": "2.1 Planning and Decision Making with RL\nNumerous efforts have aimed to develop planners using Reinforcement Learning (RL). RL's core involves treating the environment as Markovian and framing the problem as a Markov Decision Process (MDP). An environment is Markovian when the next state depends"}, {"title": "3 ReasonPlanner", "content": "ReasonPlanner agent functions similar to model-based (RL) methods. The agent has a WM that stores the agent's understanding of the world and a real-time action executor that follows the actor-critic mechanism. We outline the process of training and constructing the WM. We then describe how we incorporated the WM into the agent's planning process."}, {"title": "3.1\nProblem Formulation", "content": "We consider the problem of planning in a simulated textual environment. At each timestep t, the agent is provided with a textual state description st = (ot, it), where ot is the textual observation, similar to a natural language description of visual information, and it represents the current inventory. The agent selects a compound action at = (a, b), where a \u2208 A is a discrete action from the set of possible actions and b \u2208 B is an interactable object from the set of possible objects in the current state. Executing this compound action yields a short environmental response, a scalar reward rt, the next state st+1, and a termination signal d. Developing an agent for a textual environment is a nontrivial task due to the large and dynamically changing action space. An interactive decision-making task requires the agent to plan and execute a series of actions to accomplish a given goal, necessitating long-term foresight and evaluation of consequences."}, {"title": "3.2 World Model (WM)", "content": "The WM is represented as a Temporal Knowledge Graph (TKG), constructed using Stanford CoreNLP and LLM prompting. When the agent performs a compound action at in the environment, it receives an observation of and an action response rt. Depending"}, {"title": "3.3 Planning with a World Model", "content": "Algorithm 1 describes the WM-incorporated planning process of ReasonPlanner. We develop a modular approach for agent trajectory generation. The first model, Ma, predicts the next action and environmental response, given the current trajectory up to the current state. The second model, Ms, predicts the next state and termination signal, based on the trajectory up to the latest predicted action and"}, {"title": "3.4 Querying with Temporal Facts", "content": "We generate an explainable series of temporal facts in response to a query with the following algorithm:\n(1) We sample two relevant entities from the given query and find a path between these entities, initially ignoring temporal information.\n(2) We iteratively expand the current list of selected entities by adding their neighbors, forming a maximal subgraph since ignoring temporal information might result in an infeasible path.\n(3) We reorder the edges in the maximal subgraph based on temporal information. This reordering shows the proper sequence of events.\n(4) The temporal sequence is then passed to a LLM as in-context examples for extrapolation and summarization, enabling the LLM to generate a coherent response."}, {"title": "3.5 Executing Plan with Actor-Critic", "content": "For plan execution, we employ an actor-critic structure, consisting of two distinct models: the actor Ra and the critic Rc, integrated with the WM architecture. The process is illustrated in Figure 1. Below, we provide a detailed description of each model used and its collaborative functioning within ReasonPlanner.\nWorld Model (WM) The primary objective of the WM is to generate a comprehensive plan or trajectory for achieving a specific task within the environment. Analogous to how a human may assess consequences before undertaking actions in the real world, the WM enables ReasonPlanner to anticipate environmental changes, minimizing risky actions and enhancing sample efficiency. Upon deployment in the environment, the WM generates a trajectory T = ($t, \u00e2t, rt, St+1, at+1, rt+1, St+2,...) of predefined length L. This trajectory is then passed to the actor-critic model for execution in the environment.\nActor The actor Ra is responsible for decomposing each high-level action at \u2208 T into actionable commands within the current environment domain and also predicts intermediate state transitions between actions. The actor model is prompted with information regarding permissible commands in the current environment. After decomposition, the actor inserts the new actions back into the trajectory T and sequentially executes them in the environment. Stepping through the environment yields the actual current score, action response, and next state, which are then passed to the critic model.\nCritic The critic Re evaluates the actual score, action response, and next state against the predicted response and next state from the generated trajectory T. It also incorporates the compressed reflection R and a dictionary of actionable commands. The critic compares the actual outcomes with the expected ones and determines whether replanning is necessary. If replanning is required,"}, {"title": "4 Results", "content": "4.1 Dataset Description\nTo evaluate ReasonPlanner's planning and reasoning abilities, we require a dynamic environment with complex tasks and variations, enabling agent interaction, feedback acquisition, and the application of insights for future actions. We selected ScienceWorld as it is a benchmark environment within TextWorld [5] designed for interactive reasoning and decision-making. It includes 30 tasks derived from the grade school science curriculum, which provide a structured framework for assessing the performance of AI agents, including predefined evaluation metrics that are key to establishing a fair comparison. The agent must navigate through eight distinct functional rooms, using various tools and equipment to complete tasks such as testing the conductivity of an unknown substance. Each task features over 100 possible variations to prevent overfitting. The environment demands extensive world knowledge, common-sense reasoning, strong deduction, and problem-solving skills. The virtual space mirrors a hypothetical research location, featuring areas such as a greenhouse, kitchen, foundry, and workshop. ScienceWorld offers diverse environment variations across task types, making it an ideal test-bed for evaluating adaptation and generalization capabilities. A higher score indicates more progression toward task completion, representing the agent's ability to finish the task. For example, a score of 75 indicates that the agent completed 75% of the task before picking the wrong action that led to task termination."}, {"title": "4.2 Evaluation Setup", "content": "We experimentally evaluated ReasonPlanner on 30 tasks from the ScienceWorld benchmark to assess its performance against current state-of-the-art LLM methods and selected RL approaches. Our evaluation was structured into several distinct sections, focusing on benchmark details, implementation nuances, baseline comparisons, performance metrics with and without the use of WMs, and behavior over long horizons with varying look-ahead lengths.\n4.2.1 Implementation: ReasonPlanner was implemented using Python for backend operations and PostgresDB for data management, streamlining the handling of complex machine learning workflows through"}, {"title": "4.2.2 Performance", "content": "We conducted a comprehensive comparison of ReasonPlanner against leading methods to establish a robust baseline for performance evaluation. We directly tested the algorithms for SayCan, ReAct, and Reflexion, utilizing implementations provided in SwiftSage. ReasonPlanner and baselines were tested on the first 3 variations per task of the test set provided by ScienceWorld."}, {"title": "5 Conclusion", "content": "We have introduced ReasonPlanner, an autonomous planning agent that excels in Interactive Reasoning tasks. The ReasonPlanner agent comprehends its surroundings through interactions, enabling it to plan and foresee the consequences of its actions. Distinctively, ReasonPlanner operates based on prompts and integrates a Temporal Knowledge Graph, which enhances its interpretability and adaptability. Users can easily tailor the agent to different environments using natural language instructions while the Temporal Knowledge Graph provides a structured and dynamic knowledge base that aids in informed and anticipatory decision-making. By strategizing in a hypothetical space constructed by the WM before actual interactions with the simulated environment, the agent minimizes the risk of potentially hazardous actions. Our research integrates prompt engineering with structured knowledge and consequence forecasting to develop an effective and efficient agent. ReasonPlanner surpasses previous language-based agents on the ScienceWorld benchmark. In future work we intend to evaluate ReasonPlanner against additional LLM-based reasoning methods in the ScienceWorld environment, while also seeking ways to reduce ReasonPlanner's LLM-based inference costs."}]}