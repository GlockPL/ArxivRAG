{"title": "Human-Guided Image Generation for Expanding Small-Scale Training Image Datasets", "authors": ["Changjian Chen", "Fei Lv", "Yalong Guan", "Pengcheng Wang", "Shengjie Yu", "Yifan Zhang", "Zhuo Tang"], "abstract": "The performance of computer vision models in certain real-world applications (e.g., rare wildlife observation) is limited by the small number of available images. Expanding datasets using pre-trained generative models is an effective way to address this limitation. However, since the automatic generation process is uncontrollable, the generated images are usually limited in diversity, and some of them are undesired. In this paper, we propose a human-guided image generation method for more controllable dataset expansion. We develop a multi-modal projection method with theoretical guarantees to facilitate the exploration of both the original and generated images. Based on the exploration, users refine the prompts and re-generate images for better performance. Since directly refining the prompts is challenging for novice users, we develop a sample-level prompt refinement method to make it easier. With this method, users only need to provide sample-level feedback (e.g., which samples are undesired) to obtain better prompts. The effectiveness of our method is demonstrated through the quantitative evaluation of the multi-modal projection method, improved model performance in the case study for both classification and object detection tasks, and positive feedback from the experts.", "sections": [{"title": "1 INTRODUCTION", "content": "The success of computer vision relies on a large number of images [61]. However, collecting such large image datasets is expensive and time-consuming [8]. Moreover, in certain real-world applications (e.g., rare wildlife observation), it is impractical to gather a large number of images due to factors like the rarity of wildlife [20]. Recently, due to the advancements in pre-trained text-to-image generative models (e.g., Stable Diffusion [47]), dataset expansion has emerged as an effective way to address this issue [65]. Instead of generating images freely with generative models, dataset expansion initially utilizes several original images (e.g., Fig. 1A) as references and text prompts as perturbations to generate several similar but not identical images (e.g., Fig. 1B). Such a strategy ensures that the generated images are task-relevant, which improves model performance. Then, these generated images, along with the original ones, are utilized to train models for downstream tasks, such as classification.\nAlthough expanding the dataset is effective for generating training images, their benefits diminish, and in some cases, they may even harm model performance as more images are generated [60]. This is primarily due to two quality issues in them (Fig. 1(a)). First, dataset expansion often falls short in promoting the diversity of the generated images. For example, generating images based on the original image in Fig. 1A results in only red strawberries, whereas green strawberries, which are rare but do exist in the real world, are not generated (Fig. 1Q1). Second, due to the ambiguity of natural language prompts, the generated images are not always of interest. For example, when generating strawberries, a drawing of a woman is generated because its shape and color are similar to a red strawberry (Fig. 1Q2).\nAn effective way to address these quality issues is by allowing humans to guide the generation process through refining prompts based on their knowledge [12]. However, injecting human guidance into the generation process faces two challenges. First, the content of the generated images needs to be examined to identify missing or undesired"}, {"title": "2 RELATED WORK", "content": "Our work focuses on the interactive generation of training image datasets. Existing research in this area can be categorized into interactive label generation and interactive image generation."}, {"title": "2.1 Interactive Label Generation", "content": "Interactive label generation aims to facilitate the labeling process while minimizing human effort. Based on how labels are obtained, these methods can be classified into two categories: propagation-based methods and function-based methods.\nPropagation-based methods first allow users to label a few samples. These labels are then propagated to other unlabeled samples to obtain more labels. The \"inter-active learning\" framework is introduced by H\u00f6ferlin et al. [26] to obtain labels for the classification task. This framework allowed users to either label samples recommended by an active learning model or select informative samples for labeling through visualizations. These labels were propagated to other unlabeled samples to obtain more labels, which were used to further enhance the underlying model. This strategy was also substantiated by other work for obtaining labels for the classification task, such as DataLinker [7] and FSLDiagnotor [62]. Apart from the classification task, other tasks were also studied. For the object detection task, MutualDetector [8] utilized a multi-type relational co-clustering algorithm to cluster samples and labels simultaneously, which facilitates the exploration and annotations of bounding boxes and labels. For the temporal action localization task, Chen et al. [5] developed a storyline visualization to display the actions and their alignments, which helps users correct wrong localization results and misalignments for propagation.\nFunction-based methods enable users to define functions to generate labels instead of directly labeling samples. Hoque et al. [27] introduced the Visual Concept Programming, which utilized a self-supervised learning approach to extract visual concepts (e.g., objects and parts) from images. Users explored these visual concepts with the help of visualizations and composed labeling functions (e.g., ConceptTransportation +above + Conceptwater =ClassBoat) with them to generate labels. He et al. [22] extended this idea to generate labels for video data. To ensure the quality of labels, visual concept programming usually iterates for many rounds, which makes it challenging for users to understand the process. To tackle this issue, Li et al. [31] proposed EvoVis, which combined relationship analysis and temporal overview to help users explore the visual concept programming iterations.\nThough effective, these methods require images to generate labels for computer vision tasks. However, in many real-world applications (e.g., rare wildlife observation), it is infeasible to acquire a large number of images. To tackle this, we combine dataset expansion and visualization to help generate training images to improve model performance."}, {"title": "2.2 Interactive Image Generation", "content": "Depending on whether the downstream tasks are taken into account during the generation process, interactive image generation methods can be categorized into task-independent and task-dependent methods.\nTask-independent methods aim to help users generate images that satisfy their personal requirements. Most of them focus on prompt engineering for image generative models. PromptMagician [14] developed a multi-level visualization for exploring generated images and combined it with a prompt database to recommend keywords for text prompt refinement. This exploration and recommendation strategy was also used by Reprompt [52], PromptCharm [53], and Promptify [4]. For concepts that were challenging to convey through text (e.g., the color style of an image), Chung et al. [11] developed PromptPaint to help create prompts beyond text. PromptPaint enabled users to overlay various colors or apply different colors to distinct areas of an image. This feedback was mixed with text prompts to refine the generated images. As prompt refinement often requires many trials and errors, the prompt history is rich with valuable information that can offer users insights into past explorations and illustrate how changes to the prompt influence the generated image. In this regard, Guo et al. [18] introduced the Image Variant Graph, facilitating the comparison of prompt-image pairs and exploration of editing histories.\nAll the methods mentioned above focus on generating images that meet users' personal requirements without considering the downstream tasks. Directly applying them to expanding datasets will result in many task-irrelevant images, which is not helpful or even harmful to model training [59]. Therefore, task-dependent methods are proposed.\nTask-dependent methods consider the needs of downstream tasks during the generation process to enhance model performance. The very first method of this kind, VATLD, was introduced by Gou et al. [16] to expand training datasets for object detection. VATLD adapted a representation learning method to extract data semantics and help users explore them in multiple coordinated visualizations. Once a certain type of object fails to be detected, users can generate more such objects with the help of the representation learning method. He et al. [24] generalized this idea to semantic segmentation by introducing a context-aware spatial representation learning method, which captures key spatial details about objects, including position, size, and aspect ratio to help assess model performance and generate new test cases for various driving scenes.\nAlthough these task-dependent methods can effectively expand training datasets to enhance the model performance, they require a large number of images to train the representation learning models. However, such a large number of images are often unavailable. To tackle this issue, we utilize pre-trained generative models (e.g., Stable Diffusion [47]) to generate images. To facilitate the generation, we develop a multi-modal projection method to help explore the original and gen-"}, {"title": "3 BACKGROUND: DATA EXPANSION", "content": "Recent research has proposed various data expansion methods [12, 30, 50, 65, 68]. These methods can all be summarized into a common pipeline, as illustrated in Fig. 2. This pipeline consists of two main steps: latent perturbation and image generation.\nLatent perturbation. Given an original image, a latent feature is extracted through the image encoder of a generative model. Then, the latent feature is perturbed to encourage similar but not identical images to be generated in the following step. To ensure the generated images benefit the downstream tasks, constraints are introduced in the perturbation process. These constraints optimize certain metrics to ensure the quality and task relevance of the generated images. Typical metrics [65, 68] include: 1) Informativeness. According to the study of Zhang et al. [65], higher task-related informativeness provides more detailed information about the downstream task, which promotes model performance. 2) Diversity. As previous studies [9, 48] pointed out, the diversity of training datasets is essential for the generalization ability of models. A higher diversity usually leads to better performance.\nImage generation. After perturbing the latent features, these features, combined with the text prompt templates, are fed into the image decoder of the generative model to generate new images. Prompt templates instead of fixed prompts are used because they enhance the diversity of the generated images and are widely adopted in existing methods [12, 30, 50]. A prompt template offers several options (e.g., \"[photo | picture]\" in Fig. 2A) for specific parts of the prompts. Each time an image is generated, a prompt is sampled from the template. Due to the poor readability of prompt templates, we use prompts to illustrate the basic ideas of our method in Secs. 4 and 5."}, {"title": "4 REQUIREMENT ANALYSIS", "content": "This work was developed in close collaboration with four machine learning experts (E1-E4). El and E2 are Ph.D. students. El has published a pioneering paper on data expansion. Both El and E2 are highly knowledgeable about the latest data expansion methods and are keen to push the boundaries of this field by uncovering new insights. E3 is a researcher in an academic institute, and E4 is a machine learning engineer in a technology company. Both E3 and R4 are actively involved in deploying data expansion methods in real-world applications, such as anomaly detection. During deployment, they observed that many generated images did not meet their expectations. Consequently, they expressed a need for more controllable data expansion. We gathered the following requirements from four semi-structured interviews with the experts, each lasting between 40 and 70 minutes.\nR1: Examining the overview of the generated images and their dynamic changes through the generation process. All the experts expressed the need to have an overview of the generated images. \u201cAs the images are generated gradually instead of all at once, I need to understand what is happening during the generation process. This way, I can decide the right moment to interrupt the process.\u201d E3 said. E2 also pointed out that the overview should be simple and intuitive. It should clearly show how the generated images are evolving over time, allowing users to easily track the progress.\nR2: Exploring the original and generated images efficiently. To understand how the generated images impact the downstream model performance, it is necessary to analyze and compare them with the original images. The experts usually use grid visualizations or scatterplots to explore images. Grid visualizations place images into grid cells, making them convenient for examining image content. However, the experts found grid visualizations were insufficient for revealing cluster separation, which is essential to identify undesired generated images. Scatterplots are effective in comparing the distributions of the original and generated images and conveying clusters. However, the experts pointed out that identifying quality issues in the generated images with scatterplots requires examining the content of each image individually, which can be tedious. Therefore, an efficient way to explore the original and generated images is required.\nR3: Refining prompts effectively and efficiently to guide the generation process. When the generated images are unsatisfactory, modifying the prompts is an effective way to guide the generation process [14]. However, manually refining the prompts is tedious and time-consuming, especially for novice users. Although some methods have been proposed, such as prompt keyword recommendation (PromptMagician [14] and PromptCharm [53]), to facilitate prompt refinement, it is still challenging because users have to edit prompts directly. Therefore, all the experts sought a more user-friendly and efficient method for prompt refinement. \u201cWhen I see the generated images, I can quickly tell whether they are desired or not. However, modifying the prompts to avoid generating undesired images is challenging because the generation models are 'black boxes,' requiring multiple attempts.\u201d E4 said."}, {"title": "5 DATACRAFTER VISUALIZATION", "content": "Based on the requirements, we develop DataCrafter to facilitate human-guided image generation for dataset expansion. Fig. 3 shows its overview. Given a small number of original images and some text prompts, a set of images is generated (Fig. 3(a)). Users can have an overview of the generated images in the metric visualization (Fig. 3(b), R1) and explore them along with the original images in the multi-modal distribution visualization (Fig. 3(c), R2). During the exploration, users can refine the prompts (Fig. 3(d), R3) by providing sample-level feedback, such as removing some undesired images. The refined prompts are fed into the generative model to generate more images. This process iterates several times until the generated images are satisfying. Then, the original and the generated images are combined to train the downstream model for better performance (Fig. 3(e))."}, {"title": "5.1 Metric Visualization", "content": "The metric visualization gives an overview of the generated images. The existing methods [65, 68] optimize certain metrics in the latent spaces to ensure the quality and relevance of the generated images. This inspires us to use these metrics to give an overview of the generated images. As shown in Fig. 4(b), the metrics of the generated images over iterations are shown as a line chart. To determine which metrics to include, we systematically review the dataset expansion papers and collect the metrics used in them. The detailed review results can be found in the supplemental material. According to this review, we identify three metrics: informativeness, diversity, and distance.\nInformativeness. As pointed out in Sec. 3, high task-related informativeness promotes model performance. According to Zhang et al. [65], an image with high task-related informativeness should have high prediction entropy while maintaining the class semantic. Thus, the informativeness of a generated image is measured by\n$S_{inf} = Entropy(p') + p'_j, s.t. j = argmax(p).$\n(1)\nThe first term measures the prediction entropy, and the second term measures how well the generated image maintains the class semantics of the original image. p and p' are the prediction distributions of the original and generated images, respectively. $p'_j$ is the probability that this generated image belongs to j-th class. The prediction distributions can be obtained with zero-shot classification models, such as CLIP [45]. The informativeness of a set of generated images is calculated by averaging the informativeness of each individual image.\nDiversity. A common way to measure the diversity of a set of images is by averaging the distances between each image to their centers [49]:\n$S_{div} = \\frac{1}{M} \\sum_{i=1}^M D(v_i, \\bar{v}).$\n(2)\nFollowing the work of Zhang et al. [65], we use the Kullback\u2013Leibler (KL) divergence as the distance measure (D(\u00b7,\u00b7)). M is the number of generated images. $v_i$ is the feature vector of the i-th generated image, which is extracted with CLIP. $\\bar{v}$ is the mean feature vector of the generated images. Since the within-class diversity is more useful than the diversity over the whole dataset in practice [34], we calculate the diversities for each class and average them to obtain the final one.\nDistance. The distance between the original and generated images is widely used to measure the quality of generated images. If the distance is too high, the generated images differ greatly from the original ones, hindering the training process. Conversely, if the distance is too low, the generated images are nearly identical to the original ones, offering no benefit. Thus, achieving an appropriate level of distance is crucial [65]. A recent study [28] shows that the CLIP Maximum Mean Discrepancy (CMMD) measure is better than traditional ones (such as Frechet Inception Distance [25]). Therefore, we utilize CMMD to measure the distance between the original and generated images:\n$S_{dis} = \\frac{1}{NM} \\sum_{i=1}^N \\sum_{j=1}^M K(v_i, v'_j) - \\frac{1}{N(N-1)} \\sum_{i=1}^N \\sum_{j=1}^N K(v_i, v_j) + \\frac{1}{M(M-1)} \\sum_{i=1}^M \\sum_{j=1}^M K(v'_i, v'_j).$\n(3)\nHere, N is the original image number. $K(x,y) = exp(-||x-y||^2/2\u03c3^2)$ is the Gaussian kernel with a variance of \u03c3."}, {"title": "5.2 Multi-modal Distribution Visualization", "content": "A core requirement from the experts is the efficient exploration of the original and generated images (R2). In the literature, many visualization methods have been proposed for image exploration, which can be broadly classified into grid visualizations [6, 9, 66] and scatterplots [7, 29, 36, 58]. Grid visualizations are convenient for examining image content but fall short in revealing cluster separation [42], which is essential for identifying undesired images in generated images. In contrast, scatterplot-based methods are effective at cluster separation and analyzing the data distributions [57]. However, assessing quality issues using scatterplots typically requires examining each image individually, which can be tedious and time-consuming.\nTo tackle these issues, inspired by a recent work [32], we utilize multi-modal language models (e.g., GPT-4) to extract descriptive labels from images. These labels, referred to as content labels, describe the content of the images. By examining these content labels, users can quickly discern the contents of the images without needing to examine each one individually. Based on this idea, we design the multi-modal distribution visualization, which overlays the content labels on the scatterplot of images to allow users to explore the original and generated images and quickly access their content.\nThe multi-modal distribution visualization is shown in Fig. 4(c). Each original (generated) image is presented as a \u25cb (O). Their colors encode the classes. The content labels are placed near their images. The pie chart ( ) before each content label shows the percentage of original and generated images that contain this label. The size of the pie chart represents the total number of images that contain this label. To ensure that the content labels accurately reflect the context of the images, we need to 1) group similar images together, 2) match images with their corresponding content labels, and 3) group similar content labels together. To achieve this, we propose a multi-modal projection method called M2M. Based on M2M, we develop a multi-level exploration strategy to handle the large number of images and content labels effectively."}, {"title": "5.2.1 Multi-modal projection", "content": "Problem setting. Given $N_I$ images $X = {I_i}_{i=1}^{N_I}$ and $N_L$ content labels $Y = {L_i}_{i=1}^{N_L}$, each image contains one or more content labels. They can be modeled as a bipartite graph (X,Y, E). In E, each element $E_{ij}$ represents image $I_i$ contains content label $L_j$. Its weight represents the distance between $I_i$ and $L_j$ in the high-dimensional space (e.g., the CLIP embedding space). Given the bipartite graph, the multi-modal projection aims to project both the images and content labels into a 2D plane such that similar images (content labels) are placed together, and images are paired with their corresponding content labels. Since the number of content labels in an image affects the complexity of the problem, we categorize the problem into two settings:\n\u2022 Many-to-one setting: an image contains no more than one label.\n\u2022 Many-to-many setting: an image can contain more than one label.\nExisting method for the many-to-one setting. Recently, Ye et al. [63] proposed a multi-modal projection method, MFM, and achieved remarkable results. Although MFM was theoretically applicable in the many-to-many setting, Ye et al. [63] mainly validated its effectiveness in the many-to-one setting. If we apply MFM to the many-to-many setting, the images of different classes are cluttered together, and the similarities between images and labels are not preserved well (e.g., Fig. 6(c)). This is caused by the multi-modal distance order loss used in MFM. The multi-modal distance order loss ensures the relative distance orders of images to content labels:\n$- \\sum_{i,j<t} \\pi((h(I_j,L_i) - h(I_t, L_i)) \\times (l(I_j, L_i) - l(I_t, L_i))) \\sum_{i,j}l(I_j, L_i)^2$\n(4)\nHere h(\u00b7,\u00b7) and l(\u00b7,\u00b7) are the distances in the high-dimensional and low-dimensional spaces, respectively. \u03c0(x) = 0 if x > 0; otherwise \u03c0(x) = -x. If the relative distance orders of images to content labels are exactly preserved, the multi-modal distance order loss will be zero. In the many-to-one setting, we can prove the following theorem."}, {"title": "5.2.2 Hierarchical exploration", "content": "Since the number of content labels is typically large, displaying all of them simultaneously can cause visual clutter. To address this, we employ a tree cut algorithm to dynamically display content labels of interest [7, 15, 40]. Specifically, we organize the content labels into a hierarchy. The hierarchy is constructed using an agglomerative method, a widely used and computationally efficient hierarchical clustering method [46]. The names of the inner nodes are generated based on their children using GPT-4. Initially, several nodes in the hierarchy are displayed. If users are interested in the content labels in an area, they can zoom in for more detailed content labels. For each node x in the hierarchy, its \"Degree of Interest\" (DOI) is calculated as follows:\n$DOI(x,y) = API(x) - TD(x,y),$\n(7)\nwhere API(x) represents the priori importance of node x, determined by the ratio of generated images to the number of original images that include node x. This ratio is chosen because high ratios indicate imbalances between the original and generated images, which may suggest potential quality issues. TD(x, y) is the tree distance between node x and the focused node y, which is the closest node to the center of the currently focused area."}, {"title": "5.3 Interactions for Human-Guided Image Generation", "content": "To guide the generation process for more desired samples, a straightforward way is to modify the prompts [30]. However, directly modifying the prompts (prompt-level refinement) is a tedious trial-and-error process because of the complexity and ambiguity of natural language and the black-box nature of the generative models [14]. The work of Endert et al. [13] shows that users are more comfortable interacting with what they see instead of the underlying models to refine the models. Inspired by this work, we propose a more user-friendly refinement strategy, the sample-level prompt refinement. In this strategy, users only need to provide feedback on the images (e.g., which images are undesired). Then, an evolutionary-algorithm-based strategy is utilized to automatically refine the prompts to accommodate the feedback."}, {"title": "6 EVALUATION", "content": "To demonstrate the effectiveness of the proposed M2M for multi-modal projection and DataCrafter for improving dataset expansion, we conducted a multi-modal data projection experiment and a case study."}, {"title": "6.1 Multi-modal Projection Experiment", "content": "Datasets. We conducted the experiments on two real-world datasets: Pets [44] and COCO17 [33]. The Pets dataset contains 3,842 images from 37 classes. We selected ten classes of dogs and cats and 100 images for each of the selected classes to conduct the experiment. To obtain the content labels for each image, we first utilized GPT-4 to generate captions. Then, we used the NLTK toolkit [39] to extract the nouns in the captions as the content labels. The COCO17 dataset involves 80 classes, from which we selected all ten animal classes for the experiment. For each selected class, we randomly sampled 1000 images. As the images in the COCO17 dataset have captions, we directly used the NLTK toolkit [39] for content label extraction.\nExperimental settings. We compared the proposed M2M with two types of baselines: single-modal projection and multi-modal projection methods. Single-modal projection methods include MDS [2], t-SNE [51], UMAP [41], PCA [55], and CDR [56]. To apply these methods for multi-modal projection, we utilized CLIP to extract the embeddings of images and labels in a joint space and treated both images and labels equally during projection. Multi-modal projection methods include DCM [10] and MFM [63].\nEvaluation measures. Similar to the work of Ye et al. [63], We evaluated the results using the trustworthiness and continuity metrics under both intra-modal and inter-modal settings. To better evaluate the intra-model performance, we also propose an inter-modal similarity (IMS) measure to evaluate how well the images are placed close to their associated content labels.\nIntra- and inter-modal trustworthiness. Trustworthiness measures how accurately the k-nearest neighbors of a point in the embedding space match the actual neighbors in the high-dimensional space. For intra-modal trustworthiness, only the neighborhoods within images are considered. For inter-modal trustworthiness, only the neighborhoods between images and content labels are considered. Similar to MFM, only 30-nearest neighbors are considered (i.e., T(30)) to balance effectiveness and computational efficiency."}, {"title": "6.3 Post Analysis", "content": "Effectiveness in improving classification performance. After the case study, we conducted a quantitative evaluation to demonstrate the effectiveness of DataCrafter. We trained a classifier (ResNet50 [23]) on the generated images after each step in the case study and tested its accuracy on the test set of the Pets dataset. The results are shown in Table 2. It can be seen that the accuracy of the classifier was steadily improved after each step, demonstrating the robustness of the prompts generated by our method. In summary, using our method improves the accuracy performance from 48.45% to 81.80%, showcasing the effectiveness of our approach.\nComparison with automatic dataset expansion method. To better demonstrate the advantages of our human-guided dataset expansion method compared to existing automatic methods, we conducted a comparative experiment. In this comparison, our method was evaluated against GIF [65], which is one of the leading dataset expansion methods. For the GIF method, we used the default prompts to generate ten times the number of original images. For our method, to ensure a fair comparison, we used the prompts modified in the case study to generate the same number of images. The results are shown in the last two lines of Table 2. Our method improved accuracy by approximately 5% over GIF. This improvement is attributed to our method's ability to quickly identify issues with the images generated by the current prompts (e.g., undesired tiger-liked cat images), which allows for timely adjustments for better dataset expansion.\nExtension to other tasks. The dataset expanded by our method can also be utilized for tasks beyond classification, such as object detection [8, 35, 64]. To illustrate this, we used the generated images in the case study to enhance the performance of an object detector. Initially, we trained an SSD detection model [38] using the original images, achieving a mean Average Precision (mAP) of 92.4%. This high mAP is partly due to the use of a backbone pre-trained on ImageNet, which is a common practice in object detection. Next, we combined the original images with the generated images to retrain the SSD detection model. Since the generated images did not have bounding box annotations, we employed the commonly used pseudo-labeling method for training. Specifically, we used the detector trained on the original images to generate bounding box annotations for the generated images. We then treated the high-confidence generated bounding box annotations as ground truth and added them to the training dataset. As a result of incorporating the generated images, the mAP improved to 94.5%. Given that the original mAP was already high, this improvement is quite significant, demonstrating the generalization ability of our method to other tasks."}, {"title": "7 EXPERT FEEDBACK AND DISCUSSION", "content": "Following the case study, we conducted four interviews to gather feedback from a group of experts. This group included the two experts (E1, E2) who collaborated with us in the requirement analysis and two new experts we invited (E5, E6). The newly invited experts were Ph.D. students who had over two years of experience in computer vision research. Each interview lasted between 40 and 65 minutes. Overall, the expert feedback was positive regarding the usability of DataCrafter, but some limitations were also identified, pointing out areas that require further investigation in the future."}, {"title": "7.1 Usability", "content": "Efficient exploration of images. E6 appreciated the efficiency gained through the projection-based exploration method, commenting, \"This tool greatly reduces the time and effort needed to explore large datasets of generated images. By projecting images and their content labels in a 2D space, I can quickly spot the outliers or less relevant images, making the identification of undesired samples much easier.\" E5 also highlighted the convenience of the content labels. \"Instead of manually reviewing each image, I can focus on images of interest and make more informed refinements to the prompts.\" He said.\nEasy prompt refinement. El appreciated how DataCrafter simplified the prompt refinement process. He said, \u201cInstead of struggling to adjust the text prompts, I can now give simple feedback on individual images. This method lowers the barrier to generate diverse and relevant images for the dataset.\" E5 also found the process efficient, commenting, \u201cCombined with the multi-modal distribution visualization, the sample-level prompt refinement not only saves time but also ensures that the generated images match my requirements more closely.\""}, {"title": "7.2 Limitations and Future Work", "content": "Operations on multiple sets. The current sample-level prompt refinement method primarily focuses on providing feedback for a single set of generated images. According to set theory, there are many operations on multiple sets, which are more complex and provide richer feedback opportunities. For example, we can combine two sets of images to generate more images that fuse the styles of the two sets. Exploring how feedback can be effectively applied across multiple sets could enhance the diversity and quality of the generated images, which is an interesting venue for future work.\nGeneralization to other types of images. In the evaluation, we demonstrated the effectiveness of our method in improving the quality of natural image datasets. Additionally, it can also be used for other types of images as long as the corresponding generative models are available. However, because generative models for other image types (e.g., CXRL [19] for medical images) are generally less developed than those for natural images, the quality of generated images for these types tends to be more problematic. Therefore, it would be valuable for future research to explore how these more severe data quality issues impact the effectiveness of DataCrafter and investigate ways to improve it.\nEvaluation on efficiency. During the case study, we focused on assessing the effectiveness of the proposed DataCrafter in improving the quality of generated images. However, efficiency also plays a crucial role in practical applications. If the tool takes too long to use, even with improved performance, users may be reluctant to adopt it. Therefore, it would be valuable to conduct user studies to explore the interplay between effectiveness and efficiency, and to identify an optimal balance between the two."}, {"title": "8 CONCLUSION", "content": "In conclusion, this paper presents a novel human-guided image generation method to address the limitations of automatic image generation for dataset expansion in computer vision tasks. By leveraging a multi-modal projection method with theoretical guarantees, users are able to efficiently explore both original and generated images, which helps find quality issues in the generated images. Additionally, the introduction of a sample-level prompt refinement method simplifies the process of adjusting prompts, making it more accessible for users to enhance the quality of generated images. Several quantitative experiments and a case study were conducted to demonstrate the effectiveness of the proposed method."}]}