{"title": "KAN versus MLP on Irregular or Noisy Functions", "authors": ["Chen Zeng", "Jiahui Wang", "Haoran Shen", "Qiao Wang"], "abstract": "In this paper, we compare the performance of Kolmogorov-Arnold Networks (KAN) and Multi-Layer Perceptron (MLP) networks on irregular or noisy functions. We control the number of parameters and the size of the training samples to ensure a fair comparison. For clarity, we categorize the functions into six types: regular functions, continuous functions with local non-differentiable points, functions with jump discontinuities, functions with singularities, functions with coherent oscillations, and noisy functions. Our experimental results indicate that KAN does not always perform best. For some types of functions, MLP outperforms or performs comparably to KAN. Furthermore, increasing the size of training samples can improve performance to some extent. When noise is added to functions, the irregular features are often obscured by the noise, making it challenging for both MLP and KAN to extract these features effectively. We hope these experiments provide valuable insights for future neural network research and encourage further investigations to overcome these challenges.", "sections": [{"title": "I. INTRODUCTION", "content": "The Kolmogorov-Arnold networks (KAN) have garnered significant attention since their appearance on arXiv [1]. These networks utilize the Kolmogorov-Arnold representation theorem, which posits that any multivariate continuous function can be expressed as a combination of continuous single-variable functions and addition. Unlike conventional Multi-Layer Perceptron (MLP) networks, KANs incorporate learnable activation functions. According to [1], this feature provides KANs with enhanced interpretability and accuracy over MLPs.\nNumerous investigations into KAN applications have rapidly surfaced, covering areas such as smart energy grid optimization [33] [41], chemistry data analysis [36] [49], image classification [37] [40] [47], deep function learning [38], quantum architecture search [39], medical image analysis and processing [23] [51], disease risk predictions [50], graph learning tasks [25] [43] [46], asset pricing models [42], 3D object detection (in autonomous driving) [44], sentiment analysis [45], and deep kernel learning [48].\nOn the contrary, a growing body of research has highlighted the imperfections of KANs compared to MLPs. For example, [5] and our work [2] noted the vulnerability of KANs to noise, indicating that even minor noise can lead to a significant rise in test loss. Additionally, [34] claimed that KANs do not outperform MLPs in highly complex datasets and require considerably more hardware resources. Furthermore, [35] noted that MLPs generally have higher accuracy than KANs across various standard machine learning tasks, with the exception of tasks involving symbolic formula representation.\nMoreover, it is widely recognized that the regularity of the functions being approximated significantly influences the efficacy of neural networks. Functions that are smooth and continuous, known as regular functions, are generally approximated more precisely by KANs. On the other hand, functions that exhibit discontinuities or abrupt variations, referred to as irregular functions, present a greater difficulty.\nIt is essential to emphasize that the research by [35] focused on achieving a fair comparison between KANs and MLPs, yet it excluded the effect of irregularities or noise. In this investigation, we assess the performance of MLP and K\u0391\u039d in modeling irregular or noisy functions. To ensure fairness, we control the number of parameters and the amount of training data. Moreover, we investigate the influence of different optimizers on the accuracy of fitting specific functions. This research continues directly and naturally from our recent study on the efficacy of KANs in fitting noisy functions [2].\nThe structure of this paper is organized as follows: Section II provides an introduction to the Kolmogorov-Arnold Theorem and KANs, discussing their benefits and limitations, and enumerates the six types of functions. Section III evaluates the performance of MLP and KAN in approximating regular and irregular functions. KAN performs better than MLP in the case of regular functions, whereas for certain irregular functions, MLP shows superior performance over KAN. Increasing the number of training samples can enhance performance to a certain degree. Moreover, the impact of different optimizers on specific functions is examined. Adam generally performs better than L-BFGS during extensive training on most functions. Section IV introduces noise to the previously utilized functions and continues the comparison between MLP and KAN. It is observed that noise can obscure the characteristics of irregular components of functions, making it difficult for both MLP and KAN to capture these features accurately. Finally, Section V summarizes the findings of our experiments."}, {"title": "II. KOMOGOROV-ARNOLD THEOREM AND KANS", "content": "The Kolmogorov-Arnold theorem pertains to expressing multivariable continuous functions. According to the theorem, any continuous function involving multiple variables can be expressed as a combination of continuous single-variable functions and addition [3] [7] [8]. Formally, it can be stated as:"}, {"title": null, "content": "Theorem 1. [Kolmogorov-Arnold Theorem] Let $f : [0,1]^n \\rightarrow R$ be any multivariate continuous function, there exist continuous univariate functions $\\phi_i$ and $\\psi_{ij}$ such that:\n$$f(x_1, x_2,...,x_n) = \\sum_{i=1}^{2n+1} \\phi_i \\left( \\sum_{j=1}^{n} \\psi_{ij}(x_j) \\right)$$\nLeveraging the Kolmogorov-Arnold theorem, KANs introduce a novel neural network architecture. Unlike traditional Multi-Layer Perceptrons (MLPs) which use fixed activation functions, KANs employ flexible activation functions. Each weight in the network is replaced by a univariate function, often modeled as a spline function. This methodology is theoretically advantageous in enhancing the adaptability of KANs across different datasets and applications.\nUnfortunately, Theorem 1 was proven using a non-constructive approach, without providing a constructive proof. In 2009, [10] presented a constructive proof for this theorem. However, issues can arise when dealing with functions that show irregular behaviors. In mathematical analysis, it is common to classify at least five distinct types of irregularities. Details and examples of these types are presented in Table I."}, {"title": "III. COMPARISON ON IRREGULAR FUNCTIONS", "content": "Here, we offer some explanations for the initial five types mentioned in Table I. We compare these functions using multiple sets of KAN and MLP networks that have similar parameter counts. The parameter numbers for each network are presented in Table II. We implement the L-BFGS optimizer for these functions as it shows better performance in small-scale training. For functions with singularities or coherent oscillations, which might need more training samples and iterations, we also investigate the Adam optimizer's capability."}, {"title": "A. Regular functions", "content": "First, consider the functions exhibiting strong regularity. Such functions are continuous and differentiable at all points, exhibiting no discontinuities or non-differentiable regions, similar to $f_1$ and $f_2$. We reconstruct these two functions using two sets of MLP and KAN networks that have comparable parameters but are trained with different sample sizes. The outcomes are displayed in Fig 1. It can be observed that for this category of functions, KAN outperforms MLP."}, {"title": "B. Continuous functions with points where derivatives do not exist", "content": "The functions $f_3$ and $f_4$ serve as prime examples of this category. They maintain continuity at all points except at $x = 0$, where they are non-differentiable. Likewise, these functions are retrieved individually using independent MLP and KAN network sets.\nThe outcomes are illustrated in Fig 2. For these particular functions, the KAN's performance is worse than the MLP's. Despite the MLP network's slower convergence, it eventually reaches a lower test loss. Additionally, it can be noted that amplifying the training sample size marginally enhances the performance of both networks. However, in the vicinity of the non-differentiable point, the MLP shows more significant improvement than the KAN. More visually, it is evident that the fitting performance of MLP and KAN around $x = 0$ (the non-differentiable point) is approximately the same. Yet, with a larger training sample size, the MLP demonstrates superior fitting performance near $x = 0$ compared to the KAN."}, {"title": "C. Functions with jump", "content": "The examples of this category include $f_5$ and $f_6$. These functions have jump discontinuities at $x = \\pm 0.5$, where the function values abruptly change between 0 and 1. The experimental outcomes for these functions are depicted in Fig 3. Similar to the earlier type, the results show that the MLP outperforms the KAN. Moreover, expanding the training dataset size can enhance both networks' performance to a certain extent. Nevertheless, KAN consistently fails to match the performance of MLP."}, {"title": "D. Functions with singularities", "content": "Functions possessing singularities display distinct behaviors, marked by a rapid change rate as they near these points, with the absolute value of their first derivative tending towards positive infinity at the singularity. Additionally, for any chosen continuous interval that omits these singularities, the functions remain continuous and differentiable across the interval.\nTo avoid division by zero and guarantee clear fitting results, the ranges of the functions $f_7$ and $f_8$ are limited to [0.001, 1] and [-0.999, 0.999], respectively.\nBefore performing specific fitting experiments, we examined the effects of the training sample size, the number of Epochs, and the selection of optimizer on the fitting performance. As illustrated in Fig 4, simply enlarging the sample size by itself does not substantially enhance the performance when recovering $f_7$ and $f_8$.\nPykan offers two optional optimizers: Adam and L-BFGS. As depicted in Fig 5, while the L-BFGS optimizer achieves faster convergence with fewer epochs, the Adam optimizer shows superior performance in test loss with an increased number of epochs, particularly within MLP networks. This suggests that using the Adam optimizer and increasing the number of epochs can be an effective method to improve fitting performance. However, it is crucial to recognize that with a fixed learning rate, the improvement from this strategy is naturally constrained."}, {"title": "E. Functions with coherent oscillations", "content": "A different kind of functional singularity, referred to as an 'coherent oscillatory singularity,' is illustrated by functions $f_9$ and $f_{10}$. These functions exhibit 'unreachable points' (for example, $x = 0$ in the case of $f_9$) where, as the function gets closer to these points, its values oscillate more rapidly, crossing the x-axis an infinite number of times. At these 'unreachable points', not only does the absolute value of the first derivative lack a generalized limit, but the function values themselves cannot be represented within the extended real number system.\nIn the experimental phase, taking a similar approach as described in section D, we initially explored the impacts of raising the sampling rate, the number of Epochs, and the selection of optimizer. As shown in Fig 8 and 9, an increase in the sampling rate did not markedly enhance fitting accuracy. Additionally, while increasing the number of Epochs was advantageous, its effectiveness diminished after surpassing a certain Epoch threshold, leading to a slower reduction in test loss.\nIn particular, within the KAN network framework, the optimizer L-BFGS outperformed Adam for function $f_9$, while for function $f_{10}$, Adam showed superior results. On the other hand, when fitting both functions with an MLP, Adam consistently performed better than L-BFGS.\nIn a similar manner, Table IV demonstrates that employing the L-BFGS optimizer during the fitting process usually resulted in an additional increase in computational time. Figure 10 demonstrates that KAN consistently surpasses MLP when comparing performance over the same number of epochs."}, {"title": "IV. NOISY FUNCTIONS", "content": "In the following, we discuss the roles of noise functions. These functions have a wide range of applications, as noise can be added to any function, including those previously discussed. Thus, we introduce noise to these functions and proceed to evaluate the performance of MLP and K\u0391\u039d. According to the conclusions drawn in the preceding section, we will classify the functions into three categories: normal functions, functions with localized irregularities, and functions with severe discontinuities."}, {"title": "A. Regular functions", "content": "We introduce noise to functions exhibiting strong regularity, and subsequently fit these noisy data using KAN and MLP. The experimental outcomes are depicted in Fig 11. Our observations indicate that KAN achieves a lower test loss with low noise levels but performs worse under high noise conditions. When comparing the function fitting effect, the conclusion remains consistent: MLP shows better performance with minor noise interference, but KAN rapidly outperforms MLP as the training sample size increases."}, {"title": "B. Adding noise to functions with irregularities", "content": "Noise is subsequently added to $f_3, f_4, f_5$, and $f_6$. The experimental findings are shown in Fig 13. It is clear that the noise nearly completely conceals the irregular characteristics of the functions. When compared to the original data, fitting noisy data is more difficult for neural networks. Although increasing the number of training samples greatly enhances the fitting performance, it still falls short of the noise-free level. For $f_3$ and $f_4$, the network can still capture some of the irregular features with a larger training sample. However, for $f_5$ and $f_6$, both KAN and MLP perform poorly. The networks still have difficulty identifying the jump discontinuities, even with an increased sample size."}, {"title": "C. Irregular functions adding noise", "content": "In Fig 14, it becomes clear that the KAN fitting performance greatly surpasses that of MLP when noise is added to functions with singularities or coherent oscillation. Interestingly, in these extreme and unusual function scenarios, the impact of adding noise to the fitting process is minimal, to the point that fitting with noisy data produces results that are either on par with or better than those derived from the original, noise-free data. This minimal influence of noise highlights the ineffectiveness of strategies relying solely on increased sampling rates in such instances, as they fail to exploit the subtle benefits that noise brings."}, {"title": "V. CONCLUSION", "content": "In this study, we evaluate the effectiveness of KAN and MLP in approximating irregular or noisy functions. Our analysis concentrates on two main factors: the relative performance of KAN and MLP in fitting functions with different types according to regularity, and their ability to handle noise during the fitting process.\nAt first, in order to thoroughly evaluate the performance of the two networks, we designed three sets of comparative experiments. These focus on testing their effectiveness in fitting different functions under varied conditions: changing sampling rates, varying numbers of epochs, and, particularly for functions $f_7 - f_{10}$, examining the effect of different optimizers (exclusively to identify the optimal optimizer).\nSecondly, as identified in [2] and additionally explored in this study, raising the sampling rate is a potent method to enhance the fitting performance of functions $f_1 - f_6$. Particularly, this strategy shows greater advantages when handling noisy data versus clean data. Nevertheless, the improvement in the fitting accuracy for functions with low regularity ($f_7-f_{10}$) is minimal, irrespective of the presence of noise.\nThirdly, we also compared the fitting performance under varying Epochs from two distinct perspectives: convergence speed and stabilized test loss. KAN exhibits a faster convergence rate than MLP across all tested functions. However, MLP outperforms KAN on test functions $f_3 - f_6$ on stabilized test loss.\nFourthly, via experimental analysis (fitting $f_7 - f_{10}$), it was observed that Adam exceeded L-BFGS in performance for both networks in every instance, except for function $f_9$. Notably, when fitting function $f_9$ with the KAN, L-BFGS demonstrated better results than Adam.\nAt last, when dealing with noisy functions, KAN exhibits superior performance over MLP for regular functions or irregular functions. Conversely, for functions that with jump discontinuities or singularities, MLP outperforms KAN."}, {"title": "APPENDIX", "content": "Given the considerably extended computation times for KAN compared to MLP when fitting functions exhibiting singularities or coherent oscillations, as detailed in Section 3, we carried out further experiments to explore this matter. Nonetheless, owing to the persistent effects of coding-level optimizations, these findings do not serve as conclusive evidence. Therefore, they are included in the Appendix for documentation and further examination."}]}