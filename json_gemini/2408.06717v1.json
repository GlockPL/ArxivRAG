{"title": "Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models", "authors": ["Jialiang Wang", "Shimin Di", "Hanmo Liu", "Zhili Wang", "Jiachuan Wang", "Lei Chen", "Xiaofang Zhou"], "abstract": "Graph Neural Networks (GNNs), like other neural networks, have shown remarkable success but are hampered by the complexity of their architecture designs, which heavily depend on specific data and tasks. Traditionally, designing proper architectures involves trial and error, which requires intensive manual effort to optimize various components. To reduce human workload, researchers try to develop automated algorithms to design GNNs. However, both experts and automated algorithms suffer from two major issues in designing GNNs: 1) the substantial computational resources expended in repeatedly trying candidate GNN architectures until a feasible design is achieved, and 2) the intricate and prolonged processes required for humans or algorithms to accumulate knowledge of the interrelationship between graphs, GNNs, and performance.\nTo further enhance the automation of GNN architecture design, we propose a computation-friendly way to empower Large Language Models (LLMs) with specialized knowledge in designing GNNs, thereby drastically shortening the computational overhead and development cycle of designing GNN architectures. Our framework begins by establishing a knowledge retrieval pipeline that comprehends the intercorrelations between graphs, GNNs, and performance. This pipeline converts past model design experiences into structured knowledge for LLM reference, allowing it to quickly suggest initial model proposals. Subsequently, we introduce a knowledge-driven search strategy that emulates the exploration-exploitation process of human experts, enabling quick refinement of initial proposals within a promising scope. Extensive experiments demonstrate that our framework can efficiently deliver promising (e.g., Top-5.77%) initial model proposals for unseen datasets within seconds and without any prior training and achieve outstanding search performance in a few iterations.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph Neural Networks (GNNs) have solidified their role as state-of-the-art encoders in the realm of graph representation learning [31, 67, 75], effectively modeling complex real-world systems depicted as graphs [14, 23, 29, 44]. These networks operate on a message-passing schema, iteratively refining node representations through aggregate neighboring messages [9, 22, 54]. Designing an optimal GNN architecture for specific data and tasks, however, remains a formidable challenge. This process is not only time-consuming but also requires deep expertise [16]. Specifically, human experts often engage in labor-intensive trial and error, experimenting with various network aggregation methods, inter-layer connections, and intra-layer configurations to identify effective solutions.\nIn response to this challenge, various automated algorithms [32, 41, 81] have been proposed to reduce the manual effort required in designing GNN architectures. Notably, Automated Graph Neural Networks (AutoGNNs) [16, 58, 80], a specialized branch of Automated Machine Learning (AutoML) [24], have been developed to search for optimal GNN configurations for given graphs and their associated tasks. Past research has demonstrated that AutoGNNs can significantly alleviate manual effort and enhance model performance [60, 61, 64]. Recently, the advent of Large Language Models (LLMs) [4, 10, 53] has inspired new avenue [52] that leverage LLMs to further replace some human involvement beyond the traditional capabilities of AutoGNNs, such as task understanding [12, 56] and problem formulation [62]. These LLM-based methods have demonstrated notable strides toward democratizing complex processes of designing GNN architectures. However, automated approaches, much like their human counterparts, continue to face two major issues: 1) the substantial computational resources expended in repeatedly trying candidate GNN architectures until a feasible design is achieved, and 2) the intricate and prolonged processes required for humans or algorithms to accumulate knowledge of the interrelationship between graph datasets, GNNs, and performance.\nWhen human experts design GNNs, the process typically involves fully training each candidate architecture until convergence to evaluate its performance, followed by iterative refinements based on the feedback obtained. This cycle repeats numerous times until a high-performing model is developed. Though freeing human hands, current AutoGNNs still fundamentally rely on trying out different configurations and performing iterative optimizations, albeit with some efficiency enhancements such as weight sharing [13, 39] and learning curve prediction [1, 11]. Later, LLM-based approaches like GPT4GNAS [56] and Auto2Graph [62] have advanced the field by introducing more interpretable search strategies and user-friendly system configurations. However, these improvements have only modestly reduced the substantial computational resources and time required to design and deploy effective GNN architectures.\nMoving beyond computational efficiency, human experts and automated algorithms all encounter significant challenges in proficiency [36, 58] (i.e., the ability to accumulate the knowledge of designing models), which critically hinges on their ability to accumulate, discern, and effectively apply intricate knowledge about the interrelationships between data characteristics, model architectures, and performance outcomes. Humans, akin to training a new Ph.D. student, face substantial hurdles in accumulating this complex knowledge, often undergoing a lengthy and iterative learning process to proficiently design effective GNN models. Similarly,"}, {"title": "2 RELATED WORK", "content": "2.1 Graph Neural Networks\nGNNs have been promising methods for representation learning on graph data G. Based on the message passing framework [17], GNNs iteratively update node representations H of node v by aggregating messages from neighboring nodes. The intra-layer and inter-layer message-passing process can be formulated as:\ninter: $M_v \\leftarrow agg(msg(H_u, H_v)|u \\in N(v))$, $H_v \\leftarrow upd(comb(H_v, M_v))$,\nintra: $H_v \\leftarrow fuse(H_v^1, H_v^2, ..., H_v^L)$.\nMost GNNs [2, 9, 22, 35, 54, 67] are specific instances of these equations, differing mainly in their choice of functions like agg(\u00b7), comb(\u00b7), upd(\u00b7), and fuse (.). For example, GCN [31] uses MEAN (\u00b7) for aggregation and SUM() for combination. JK-Net [68] enhances inter-layer message passing by proposing various fuse() functions, ranging from simple MAX(.) to adaptive ones LSTM(.).\n2.1.1 Automated GNNs. AutoGNNs [16, 58, 80] aim to automatically find an optimal GNN architecture \u03b8 for an unseen graph Gu using a controller \u03c0 parameterized by \u03b1, where the architecture \u03b8 with network weight w can achieve best performance H on Gu:\n$0*, w* = arg max E_{\\theta \\sim \\pi_\\alpha(\\theta)} [H(\\theta, \\omega; G^u)]$.\n(1)\nIn AutoGNNs, the search space is typically divided into intra-layer [15, 59, 60] and inter-layer [27, 59, 60, 63, 77] designs, focusing on the configuration of message passing and layer connectivity. The search algorithms employed include reinforcement learning [15], evolutionary algorithms [48], and differentiable search [27, 59, 59], each offering different strategies for exploring the search space and optimizing architecture selection.\nHowever, as introduced in Sec. 1, classic AutoGNNs still face poor proficiency and extensive computational overhead issues [36, 58]. For the given graph Gu, classic works gradually optimize configurations \u03b8 without leveraging any prior knowledge\u2014highlighting a lack of proficiency (see Fig. 1). Besides, the optimization process of solving Eq. (1) needs a lot of candidate architecture sampling in \u03b8 ~ \u03c0\u03b1 (\u03b8), thus demanding substantial computational resources.\n2.1.2 NAS Bench for Graph. Unlike other NAS benchmarks [8], NAS-Bench-Graph [40] (details in the Appx. A.3) offers a comprehensive dataset space, including 9 node classification benchmark datasets. Its model space encompasses 9 types of GNN layers (mi-cro space) and 9 configurations of directed acyclic graphs (macro space), cataloging the empirical performance of 26,206 unique GNN"}, {"title": "2.2 LLMs and Its Applications to GNNs", "content": "Recently, LLMs have shown remarkable proficiency in a variety of natural language understanding [47, 57] and task optimization scenarios [20, 69]. In this paper, we focus on the applications of LLMs to graphs and GNNs, especially designing GNNs by LLMs.\n2.2.1 LLMs with Graphs. Recent advancements in integrating LLMs with graphs have significantly enhanced graph learning tasks [7, 42, 49] turning intricate graph details into more manageable formats for analysis and interpretation. In this integration, two primary lines of studies emerge. The first group involves using GNNs to process graph data into structured tokens, providing a solid foundation for LLMs to subsequently infer linguistic and contextual nuances [5, 50, 51]. Alternatively, LLMs may first enrich the raw graph data with contextual insights, which GNNs then utilize to refine their structural processing [43, 65, 66]. In the second group, deeper integration involves collaborative efforts such as fusion training [78] and alignment [33], where LLMs and GNNs synergistically enhance their functionalities to handle graph tasks more comprehensively. In a more autonomous approach, LLMs independently manage graph tasks, employing advanced language understanding to directly interpret and manipulate graph data [26, 55, 70]. This is exemplified by the \"Graph to Language\" (G2L) strategy [19], which utilizes LLMs to comprehend graph data through prompting.\nUnfortunately, the research in this paper diverges from existing methods. To study the correlation between graphs, models, and performance, it not merely translates graphs into textual formats but also asks LLMs to assess and interpret graph similarities, which could directly reflect the preferences of different graph datasets for GNN architecture design. Specifically, it necessitates LLMs to prioritize understanding graph dataset characteristics and their implications on model performance, just like human expert intuitions.\n2.2.2 LLMs for Designing GNNs. Recent research on the synergy between LLMs and AutoML, including GNN designs, has demonstrated a dynamic and powerful merger of language processing with structured data analysis and problem-solving [52]. These approaches differ primarily in how they acquire, condense, and utilize complex AutoML knowledge, thereby aiding LLMs in configuring [21, 74] and optimizing [28, 72, 76, 79] machine learning pipelines.\nThis reduces the necessity for complete retraining and diminishes the expertise required to effectively use AutoML."}, {"title": "3 METHODOLOGY", "content": "As introduced in Sec. 1, the design of GNNs by both human experts and automated algorithms faces significant challenges including substantial computational resources and a lack of proficiency. To address these issues, we propose a computation-friendly way to design GNNs by accumulating knowledge on LLMs (DesiGNN). By comparing Fig. 1b and 2, DesiGNN is composed of three major different components, each designed to capture and leverage the intricate relationships between graphs, GNN configurations, and performance. Initially, the Graph Understanding module aims to automatically analyze graphs and identify the graph topology"}, {"title": "3.1 Graph Understanding Module", "content": "To effectively accumulate tailored knowledge for specific datasets, it is crucial to first understand the characteristics and structures of the graphs. However, it remains an open problem to enable LLMs to understand the similarities between different graphs. Despite some progress in LLMs for graphs (see Sec. 2.2.1), existing works focus more on enabling LLMs to understand structured graph data given structured input (e.g., adjacent list), rather than the similarity among graphs. As discussed in Sec. 2.2.2, existing LLM-based approaches rely solely on user-provided semantic descriptions of datasets to initiate the process, while ignoring graph topology (or other characteristics). Besides, it is even more challenge to enable LLMs to capture the connection between graph similarity with GNN configuration (or GNN performance).\n3.1.1 Understand Graph Similarity and Motivational Experiments. As discussed in previous works (see Sec. 2.2), the semantic description of graph data may be biased. Thus, to achieve the goal of building knowledge about \u201cgraph-GNN-performance\u201d, we start with 16 graph topological features F = {Fk}16\u2081 (e.g., average shortest path length, details in Appx. A.2.1) to understand graph similarity from two aspects: 1) the similarity among graphs, and 2) the impact of graph similarity on the empirical performance of GNNs.\nGiven two graphs Gi and GJ \u2208 G, let sik and sjk denotes the statistical value of feature Fk on graphs G\u00b9 and G\u00cc, respectively. Then, the distance $Dist = \\sqrt{\\sum_{k=1}^{16} (s_{ik} - s_{jk})^2}$ computes the difference of feature Fk between G\u00b9 and G\u00cc, i.e., negative {Dist}16, can assess the topological similarities between graphs Gi and GJ. Then, to capture the difference in model performance, we let Pij denote the performance of transferring model configuration knowledge gathered from another graph G to Gi (see Sec. 3.3.1). In other words, different from Dist that evaluates the topological distance between graphs, Pij tries to assess the similarity between graphs based on the effectiveness of knowledge transfer (empirical performance). Based on Dist and Pij, we formally define the statistical ranking SRank(i, k) and empirical ranking ERank(i) as follows:\n$SRank(i, k) = argsort({Dist | j \\neq i,G^j \\in G}, ascending)$,\n(2)\n$ERank(i) = argsort({P_{ij} | j \\neq i,G^j \\in G}, descending)$,\n(3)\nwhere SRank(i, k) represents the statistical similarity ranking of other graphs {G) | j \u2260 i,G\u00cc \u2208 G} to graph Gi based on feature Fk, and ERank(i) represents the empirical similarity ranking of other graphs to graph G\u00b9. Finally, to capture the impact of graph similarity (in terms of feature Fk) on the empirical effectiveness of suggesting GNNs, we calculate the Kendall Rank Correlation Coefficient [30] between SRank(i, k) and ERank(i) as follows:\n$I(G^i, F_k) := KendallCorr(SRank(i, k), ERank(i))$,\n(4)\nwhere I(G\u00b9, Fk) evaluates the correlation between the statistical differences in feature Fk (between G\u00b9 and other graphs {G}) and the empirical differences in transferring knowledge from other graphs {G} to G\u00b9. In other words, I(G\u0130, Fk) can be the \"confidence\" of graph topological feature Fk to determine the empirical similarity between Gi and other graph data within our knowledge. That is, the larger I(G\u00b9, Fk) is, the more Fk is a strong correlation indicator on Gi that can determine whether other graphs {G} is related to Gi from the perspective of graph topology and empirical performance."}, {"title": "3.1.2 Adaptive Filtering Mechanism", "content": "As summarized in Sec. 3.1.1, even statistical and empirical indicators cannot be directly used as invariant indicators to characterize the correlation between graphs and the impact of these correlations on model performance. Thus, we propose an adaptive filtering mechanism to identify the most influential set of graph topological features, bridging the gap between the graph dataset and GNN designs with performance-driven data understanding. We treat each benchmark graph as an anchor Gi in turn, compute the I(G\u00b9, Fk) for each Fk, and average them on all n benchmark graphs to obtain the average correlation coefficient: $\u012a(Fk) = \\frac{\u03a3_{i=1}^{n} I(G^i, F_k)}{n}$, where \u012a(Fk) is calculated based on the records in NAS-Bench-Graph [40]. Then, the filter F (Gi), initiated using benchmark data within seconds, selects the Top-Nf average correlation coefficients {\u012a(Fk)} as the most influential sets of graph topological features. Thus, it circumventes \"artificial hallucinations\" often triggered by unprofessional or misleading user input. Additionally, we design a self-evaluation bank BE that stores the measured graph topological features and the empirical similarity rankings of new datasets throughout our lifecycle, ensuring that the filter F(Gi) is continuously updated (i.e., adaptive filter). By grounding feature selection on the empirical performances of initial model proposals, this mechanism sets a stage for capturing the graph-to-graph (Gi to G\u00cc) relationship that can reveal correlations from graphs to top-performing GNN models M : (G, 0) ~~\u2192 P."}, {"title": "3.2 Knowledge Retrieval Module", "content": "The proficiency of the GNN architecture design process critically depends on the ability to accumulate, discern, and effectively apply intricate knowledge about the interrelationships between graphs, GNNs, and performance as M: (G, 0) ~\u2192 P. However, as illustrated in Fig. 1, AutoGNNs often lack this data-wise knowledge prior to extensive training, while native LLMs only have limited capabilities to design some more general GNNs as studied in Appx. B.3.1. To bridge this gap, we utilize the detailed records of model-to-performance relationships captured in NAS-Bench-Graph [40], which contains diverse and intricate mappings across various graphs, a wide range of candidate GNNs, and their corresponding performance.\nTo capture the complex \u201cGraph-GNN-Performance\u201d correlations, we encapsulate the Top-Nm model designs from NAS-Bench-Graph [40] to serve as concrete summaries of model preferences for different benchmark graphs. Further, we associate these model summaries with their corresponding filtered features generated by the Graph Understanding module of Sec. 3.1 (i.e., using natural language to describe graph topological features and GNN models), then creating 9 distinct knowledge bases for selective retrieval. This strategy simplifies the complex interrelationships into more manageable correlations between datasets and their top-performing models: $M^* : G \u2192 \\theta^*$. Then, we employ LLMs to analyze and compare the similarities between an unseen dataset Gu (user inputs) and benchmark graphs G = {G}=1 (within our knowledge) as follows:\n$Sim^{u} = {LLM_{GDC}(F(G^u), F(G^i))}_{i=1}^{9}$,\n(5)\nwhere we transform the filtered features F(.) into natural language before feeding them into GPT-4 LLMGDC(\u00b7, \u00b7). As shown in Fig. 2, this process is conceptualized as a prompt design problem, where LLMGDC(,) is tasked with analyzing the descriptions of both unseen and benchmark graphs. The objective is to generate task similarities that correlate with the top-performing model patterns. This method effectively leverages the complex mappings captured in NAS-Bench-Graph [40] and adapts to the variability across graphs, thereby facilitating the retrieval of specialized, most relevant model configuration knowledge from the benchmark graph to the unseen.\nTo retrieve the most relevant knowledge to unseen graph Gu, we rank benchmark graphs based on their similarities Simu and select Top-Ns most similar graphs. Then, the Knowledge Pool KP for Gu is formed by collecting the top-performing models 0m}m\u00b2 Nm from each one of selected Top-Ns benchmark graphs G\u00b9, defined as:\n$KP = \\bigcup_{G^i \\in Top-N_s (Sim^u)}  \\{(G^i, {\\theta_m}_{m=1}^{N_m})\\}$,\n(6)\nwhere KP\u2081 = {(G\u00b9, {0} Nm)} and Gi is the i-th graph similar to Gu. This retrieval strategy collects the Top-Nm model designs from each similar benchmark graph for in-context learning. It streamlines the transfer of GNN configuration knowledge to Gu and leverages the analytical capabilities of LLMs to ensure an effective, efficient, and interpretable knowledge accumulation process."}, {"title": "3.3 GNN Model Suggestion and Refinement", "content": "3.3.1 Initial Model Suggestion. As shown in Fig. 2, after understanding the graphs in Sec. 3.1 and building knowledge in Sec. 3.2, the LLM acts as a surrogate agent to facilitate the initial model"}, {"title": "3.3.2 Model Proposal Refinement", "content": "To further enhance the effectiveness of the initial model proposals, we propose a more structured, knowledge-driven refinement strategy. This refinement strategy enables LLMs to refine models in a nuanced and fine-grained manner, emulating the exploration-exploitation process of human experts. By incorporating specific, empirically derived configuration knowledge into the refinement phase, we ensure that each refinement step is clearly informed by a deep understanding of what configurations have historically led to success under similar circumstances.\nAs formalized in Algo. 1, the process begins with (1) Re-rank Mechanism: The knowledge bases in KP are re-ranked according to the performance ranking of the corresponding initial model proposals {Qui}1. The best proposal \u03b8u1 is chosen as the start- ing point. (2) Controlled Exploration: Configurations within KP2:N are utilized to crossover the best proposal, yielding Nc candidates. (3) Model Promotion Mechanism: These candidates are ranked based on their retrieved performances on the benchmark dataset of KP1, with the most promising candidate advanced for further refinement at iteration t. (4) Directional Exploitation and Optimization: A comprehensive prompt that includes user requirements, task descriptions, search space details, previous training logs, and the optimization trajectory 07 guides the LLM controller LLMGNAS to mutate the promoted candidate using elite insights from KG1. The refined model proposal is automatically constructed and trained to evaluate its performance Pu. The results (0, P) are appended into the optimization trajectory 0. If it surpasses the current best, it is updated as the new 0."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Settings\n4.1.1 Task and Data Sets. Our experiments are conducted under the benchmark setting defined in NAS-Bench-Graph [40], focusing on the node classification across 8 benchmark graphs (except for ogbn-proteins, see Appx. B.1.1): Cora [45], Citeseer [45], PubMed"}]}