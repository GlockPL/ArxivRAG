{"title": "Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models", "authors": ["Jialiang Wang", "Shimin Di", "Hanmo Liu", "Zhili Wang", "Jiachuan Wang", "Lei Chen", "Xiaofang Zhou"], "abstract": "Graph Neural Networks (GNNs), like other neural networks, have shown remarkable success but are hampered by the complexity of their architecture designs, which heavily depend on specific data and tasks. Traditionally, designing proper architectures involves trial and error, which requires intensive manual effort to optimize various components. To reduce human workload, researchers try to develop automated algorithms to design GNNs. However, both experts and automated algorithms suffer from two major issues in designing GNNs: 1) the substantial computational resources expended in repeatedly trying candidate GNN architectures until a feasible design is achieved, and 2) the intricate and prolonged processes required for humans or algorithms to accumulate knowledge of the interrelationship between graphs, GNNs, and performance. To further enhance the automation of GNN architecture design, we propose a computation-friendly way to empower Large Language Models (LLMs) with specialized knowledge in designing GNNs, thereby drastically shortening the computational overhead and development cycle of designing GNN architectures. Our framework begins by establishing a knowledge retrieval pipeline that comprehends the intercorrelations between graphs, GNNs, and performance. This pipeline converts past model design experiences into structured knowledge for LLM reference, allowing it to quickly suggest initial model proposals. Subsequently, we introduce a knowledge-driven search strategy that emulates the exploration-exploitation process of human experts, enabling quick refinement of initial proposals within a promising scope. Extensive experiments demonstrate that our framework can efficiently deliver promising (e.g., Top-5.77%) initial model proposals for unseen datasets within seconds and without any prior training and achieve outstanding search performance in a few iterations.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph Neural Networks (GNNs) have solidified their role as state-of-the-art encoders in the realm of graph representation learning [31, 67, 75], effectively modeling complex real-world systems depicted as graphs [14, 23, 29, 44]. These networks operate on a message-passing schema, iteratively refining node representations through aggregate neighboring messages [9, 22, 54]. Designing an optimal GNN architecture for specific data and tasks, however, remains a formidable challenge. This process is not only time-consuming but also requires deep expertise [16]. Specifically, human experts often engage in labor-intensive trial and error, experimenting with various network aggregation methods, inter-layer connections, and intra-layer configurations to identify effective solutions.\nIn response to this challenge, various automated algorithms [32, 41, 81] have been proposed to reduce the manual effort required in designing GNN architectures. Notably, Automated Graph Neural Networks (AutoGNNs) [16, 58, 80], a specialized branch of Automated Machine Learning (AutoML) [24], have been developed to search for optimal GNN configurations for given graphs and their associated tasks. Past research has demonstrated that AutoGNNs can significantly alleviate manual effort and enhance model performance [60, 61, 64]. Recently, the advent of Large Language Models (LLMs) [4, 10, 53] has inspired new avenue [52] that leverage LLMs to further replace some human involvement beyond the traditional capabilities of AutoGNNs, such as task understanding [12, 56] and problem formulation [62]. These LLM-based methods have demonstrated notable strides toward democratizing complex processes of designing GNN architectures. However, automated approaches, much like their human counterparts, continue to face two major issues: 1) the substantial computational resources expended in repeatedly trying candidate GNN architectures until a feasible design is achieved, and 2) the intricate and prolonged processes required for humans or algorithms to accumulate knowledge of the interrelationship between graph datasets, GNNs, and performance.\nWhen human experts design GNNs, the process typically involves fully training each candidate architecture until convergence to evaluate its performance, followed by iterative refinements based on the feedback obtained. This cycle repeats numerous times until a high-performing model is developed. Though freeing human hands, current AutoGNNs still fundamentally rely on trying out different configurations and performing iterative optimizations, albeit with some efficiency enhancements such as weight sharing [13, 39] and learning curve prediction [1, 11]. Later, LLM-based approaches like GPT4GNAS [56] and Auto2Graph [62] have advanced the field by introducing more interpretable search strategies and user-friendly system configurations. However, these improvements have only modestly reduced the substantial computational resources and time required to design and deploy effective GNN architectures.\nMoving beyond computational efficiency, human experts and automated algorithms all encounter significant challenges in proficiency [36, 58] (i.e., the ability to accumulate the knowledge of designing models), which critically hinges on their ability to accumulate, discern, and effectively apply intricate knowledge about the interrelationships between data characteristics, model architectures, and performance outcomes. Humans, akin to training a new Ph.D. student, face substantial hurdles in accumulating this complex knowledge, often undergoing a lengthy and iterative learning process to proficiently design effective GNN models. Similarly, AutoGNNs conduct model searches specific to the data and task at hand but overlook the relationships between different datasets and models. As a result, these approaches require starting from scratch with each new dataset, thereby lacking proficiency. Moving forward, LLMs have shown potential in mimicking human-like knowledge accumulation in GNN architecture design [52]. However, current LLM-based approaches [12, 56, 62] confront three non-proficiencies. First, an inadequate understanding of graph data can lead to improper knowledge associations, undermining the effectiveness of model suggestions. This issue is exacerbated by an overreliance on simplistic, user-provided semantic descriptions of datasets, which overlook critical graph topology and lead to skewed interpretations of model preferences. Secondly, the native LLMs lack the nuanced knowledge necessary to establish a concrete and empirical mapping from datasets\u2019 characteristics to effective model configurations. This deficiency results in overly generalized, poor-quality model suggestions for new datasets. Thirdly, the initial model proposals generated by LLMs are often superficial and require further refinement to ensure effectiveness. Unfortunately, current LLM-based methods primarily mimic simplistic search strategies, focusing merely on gathering superficial insights from the optimization trajectory [69]. In summary, while humans require a prolonged process to accumulate the knowledge necessary to become experts, current automated algorithms have weak abilities to become proficient.\nIn this paper, to promote the proficiency and reduce the associated high computational overhead in designing GNN architectures, we propose a computation-friendly way to design GNNs by accumulating knowledge on LLMs (abbreviate to DesiGNN). In this paper, to promote the proficiency and reduce the associated high computational overhead in designing GNN architectures, we propose a computation-friendly way to design GNNs by accumulating knowledge on LLMs (abbreviate to DesiGNN). To enable automated algorithms to become proficient like human experts, we focus on promoting LLM\u2019s capability to capture the interrelationships between graphs, models, and performance. To achieve this goal and tackle the three non-proficiencies, we first propose to understand the correlation across different graph datasets, then establish a knowledge retrieval system to assess dataset similarities and extract configuration-level insights, so that LLMs can suggest initial model proposals with high proficiency in a short time. To improve the performance of initial model proposals, i.e., further exploring fine-grained architecture like AutoGNNs, we rapidly refine initial model proposals based on actionable intelligence derived from configuration-level insights, significantly accelerating the development cycle of feasible models. Our contributions are listed as:\n\u2022\tWe propose a novel idea to empower LLM with specialized, nuanced knowledge for designing GNN architectures. This approach not only enables LLM to accumulate knowledge and suggest models like human experts but also enables it to explore fine-grained GNN architectures like past AutoGNN methods.\n\u2022\tTo empower architecture designs with accumulated knowledge, DesiGNN establishes novel Graph Understanding and Knowledge Retrieval modules to distill tailored model configuration knowledge from the relaxed mappings between datasets and top-performing models. Therefore, DesiGNN can efficiently deliver a promising model proposal for an unseen graph within seconds.\n\u2022\tTo further promote the performance of the designed GNN, DesiGNN proposes a new way to perform controlled and directional refinement upon the initial model proposal. It employs LLMs to swiftly identify and summarize potential patterns from transferred top-performing knowledge, rapidly exploring fine-grained GNN architectures in a few iterations.\n\u2022\tThrough extensive testing across 11 graphs, DesiGNN reliably delivers Top-5.77% GNN architectures within seconds and without prior training. They can further be effectively refined within just a few iterations. DesiGNN substantially reduces the overall computational overhead and enhances short-run effectiveness in deployment."}, {"title": "2 RELATED WORK", "content": "2.1 Graph Neural Networks\nGNNs have been promising methods for representation learning on graph data G. Based on the message passing framework [17], GNNs iteratively update node representations H of node v by aggregating messages from neighboring nodes. The intra-layer and inter-layer message-passing process can be formulated as:\ninter:  $M_v \\leftarrow agg(msg(H_u, H_v)| u \\in N(v)), H_v \\leftarrow upd(comb (H_v, M_v))$,\nintra: $H_v \\leftarrow fuse(H_v^0, H_v^1, ..., H_v^L )$.\nMost GNNs [2, 9, 22, 35, 54, 67] are specific instances of these equations, differing mainly in their choice of functions like agg(\u00b7), comb(), upd(), and fuse (.). For example, GCN [31] uses MEAN (\u00b7) for aggregation and SUM() for combination. JK-Net [68] enhances inter-layer message passing by proposing various fuse() functions, ranging from simple MAX(.) to adaptive ones LSTM(.).\n2.1.1 Automated GNNs. AutoGNNs [16, 58, 80] aim to automatically find an optimal GNN architecture \u03b8 for an unseen graph Gu using a controller \u03c0 parameterized by \u03b1, where the architecture \u03b8 with network weight w can achieve best performance H on Gu:\n$\\theta^*, w^* = arg \\max_{\\theta w} E_{\\theta \\sim \\pi_\\alpha(\\theta)}[H(\\theta, \\omega; G^u)].$  (1)\nIn AutoGNNs, the search space is typically divided into intra-layer [15, 59, 60] and inter-layer [27, 59, 60, 63, 77] designs, focusing on the configuration of message passing and layer connectivity. The search algorithms employed include reinforcement learning [15], evolutionary algorithms [48], and differentiable search [27, 59, 59], each offering different strategies for exploring the search space and optimizing architecture selection.\nHowever, as introduced in Sec. 1, classic AutoGNNs still face poor proficiency and extensive computational overhead issues [36, 58]. For the given graph Gu, classic works gradually optimize configurations \u03b8 without leveraging any prior knowledge\u2014highlighting a lack of proficiency (see Fig. 1). Besides, the optimization process of solving Eq. (1) needs a lot of candidate architecture sampling in $\u03b8 \u223c \u03c0_\u03b1(\u03b8)$, thus demanding substantial computational resources.\n2.1.2 NAS Bench for Graph. Unlike other NAS benchmarks [8], NAS-Bench-Graph [40] (details in the Appx. A.3) offers a comprehensive dataset space, including 9 node classification benchmark datasets. Its model space encompasses 9 types of GNN layers (mi-cro space) and 9 configurations of directed acyclic graphs (macro space), cataloging the empirical performance of 26,206 unique GNN architectures on each benchmark dataset. Therefore, the data-wise richness makes NAS-Bench-Graph an invaluable model configuration knowledge base, possessing the potential to reveal the intricate interrelationship between graphs, GNNs, and performance."}, {"title": "2.2 LLMs and Its Applications to GNNs", "content": "Recently, LLMs have shown remarkable proficiency in a variety of natural language understanding [47, 57] and task optimization scenarios [20, 69]. In this paper, we focus on the applications of LLMs to graphs and GNNs, especially designing GNNs by LLMs.\n2.2.1 LLMs with Graphs. Recent advancements in integrating LLMs with graphs have significantly enhanced graph learning tasks [7, 42, 49] turning intricate graph details into more manageable formats for analysis and interpretation. In this integration, two primary lines of studies emerge. The first group involves using GNNs to process graph data into structured tokens, providing a solid foundation for LLMs to subsequently infer linguistic and contextual nuances [5, 50, 51]. Alternatively, LLMs may first enrich the raw graph data with contextual insights, which GNNs then utilize to refine their structural processing [43, 65, 66]. In the second group, deeper integration involves collaborative efforts such as fusion training [78] and alignment [33], where LLMs and GNNs synergistically enhance their functionalities to handle graph tasks more comprehensively. In a more autonomous approach, LLMs independently manage graph tasks, employing advanced language understanding to directly interpret and manipulate graph data [26, 55, 70]. This is exemplified by the \"Graph to Language\" (G2L) strategy [19], which utilizes LLMs to comprehend graph data through prompting.\nUnfortunately, the research in this paper diverges from existing methods. To study the correlation between graphs, models, and performance, it not merely translates graphs into textual formats but also asks LLMs to assess and interpret graph similarities, which could directly reflect the preferences of different graph datasets for GNN architecture design. Specifically, it necessitates LLMs to prioritize understanding graph dataset characteristics and their implications on model performance, just like human expert intuitions.\n2.2.2 LLMs for Designing GNNs. Recent research on the synergy between LLMs and AutoML, including GNN designs, has demonstrated a dynamic and powerful merger of language processing with structured data analysis and problem-solving [52]. These approaches differ primarily in how they acquire, condense, and utilize complex AutoML knowledge, thereby aiding LLMs in configuring [21, 74] and optimizing [28, 72, 76, 79] machine learning pipelines. This reduces the necessity for complete retraining and diminishes the expertise required to effectively use AutoML.\nAs shown in Fig. 1b, recent explorations into integrating LLMs with GNN architecture design have opened new avenues for enhancing AutoGNN systems. Auto2Graph [62] utilizes LLMs to democratize the usage of traditional AutoGNNs, taking in semantic descriptions of datasets, task specifications, evaluation metrics, user preferences, and constraints to manage the complete lifecycle of graph learning tasks, from data processing to hyper-parameter tuning. GPT4GNAS [56] instead leverages GPT-4 to significantly reduce manual efforts in designing GNNs for new tasks by iteratively generating modular GNN architectures through carefully designed prompts that describe the search task, space, and strategy. GHGNAS [12] extends GPT4GNAS by incorporating simple descriptions of heterogeneous graphs, including node and edge types and numbers, to iteratively refine and enhance heterogeneous GNN designs.\nUnfortunately, as shown in Fig. 1b, these approaches present notable limitations that our research seeks to address: 1) improper graph understanding, 2) lack of knowledge in raw LLMs, and 3) ineffective optimization strategies. First, current approaches [12, 56, 62] heavily rely on the quality and specificity of the user-provided descriptions, which can skew their effectiveness if the semantic description of the dataset is not correlated to the task. Moreover, they often overly trust the inherent reasoning ability of raw LLMs and do not account for any concrete knowledge between graph dataset and GNN performance, thus lacking the proficiency. Lastly, they rely on the optimization trajectory as the sole driving force for model refinement due to a lack of nuanced, actionable knowledge, limiting their effectiveness and computational efficiency in model refinement. As a result, they still require computationally intensive iterative exploration over the long run and have no control over the exploitation step, which is methodologically less proficient than how human experts refine GNN designs."}, {"title": "3 METHODOLOGY", "content": "As introduced in Sec. 1, the design of GNNs by both human experts and automated algorithms faces significant challenges including substantial computational resources and a lack of proficiency. To address these issues, we propose a computation-friendly way to design GNNs by accumulating knowledge on LLMs (DesiGNN). By comparing Fig. 1b and 2, DesiGNN is composed of three major different components, each designed to capture and leverage the intricate relationships between graphs, GNN configurations, and performance. Initially, the Graph Understanding module aims to automatically analyze graphs and identify the graph topology that is crucial for enabling LLMs to assess the similarity between benchmark (within our knowledge) and unseen graph datasets (beyond our knowledge) with respect to model preference. Then, the Knowledge Retrieval module builds specialized model configuration knowledge from NAS-Bench-Graph [40], so that LLMs can utilize the insights gained from understanding graphs and retrieve relevant models as knowledge for unseen graphs. Lastly, the Model Suggestion and Refinement module leverages the retrieved knowledge to quickly suggest and refine the GNN proposals. In the subsequent sections, we explain how each component collectively contributes to a proficient and computationally efficient GNN design process.\n3.1 Graph Understanding Module\nTo effectively accumulate tailored knowledge for specific datasets, it is crucial to first understand the characteristics and structures of the graphs. However, it remains an open problem to enable LLMs to understand the similarities between different graphs. Despite some progress in LLMs for graphs (see Sec. 2.2.1), existing works focus more on enabling LLMs to understand structured graph data given structured input (e.g., adjacent list), rather than the similarity among graphs. As discussed in Sec. 2.2.2, existing LLM-based approaches rely solely on user-provided semantic descriptions of datasets to initiate the process, while ignoring graph topology (or other characteristics). Besides, it is even more challenge to enable LLMs to capture the connection between graph similarity with GNN configuration (or GNN performance).\n3.1.1 Understand Graph Similarity and Motivational Experiments. As discussed in previous works (see Sec. 2.2), the semantic description of graph data may be biased. Thus, to achieve the goal of building knowledge about \u201cgraph-GNN-performance\u201d, we start with 16 graph topological features $F = \\{F_k\\}_{k=1}^{16}$ (e.g., average shortest path length, details in Appx. A.2.1) to understand graph similarity from two aspects: 1) the similarity among graphs, and 2) the impact of graph similarity on the empirical performance of GNNs.\nGiven two graphs $G^i$ and $G^j \\in G$, let $s_{ik}$ and $s_{jk}$ denotes the statistical value of feature $F_k$ on graphs $G^i$ and $G^j$, respectively. Then, the distance $Dist_k = \\sqrt{(s_{ik} - s_{jk})^2}$ computes the difference of feature $F_k$ between $G^i$ and $G^j$, i.e., negative $\\{Dist_k\\}_{k=1}^{16}$, can assess the topological similarities between graphs $G^i$ and $G^j$. Then, to capture the difference in model performance, we let $P_{ij}$ denote the performance of transferring model configuration knowledge gathered from another graph $G^j$ to $G^i$ (see Sec. 3.3.1). In other words, different from $Dist_k$ that evaluates the topological distance between graphs, $P_{ij}$ tries to assess the similarity between graphs based on the effectiveness of knowledge transfer (empirical performance). Based on $Dist_k$ and $P_{ij}$, we formally define the statistical ranking $SRank(i, k)$ and empirical ranking $ERank(i)$ as follows:\n$SRank(i, k) = argsort(\\{Dist_k^j | j \\neq i, G^j \\in G\\}, ascending),$ (2)\n$ERank(i) = argsort(\\{P_{ij} | j \\neq i, G^j \\in G\\}, descending),$ (3)\nwhere $SRank(i, k)$ represents the statistical similarity ranking of other graphs $\\{G^j | j \\neq i, G^j \\in G\\}$ to graph $G^i$ based on feature $F_k$, and $ERank(i)$ represents the empirical similarity ranking of other graphs to graph $G^i$. Finally, to capture the impact of graph similarity (in terms of feature $F_k$) on the empirical effectiveness of suggesting GNNs, we calculate the Kendall Rank Correlation Coefficient [30] between $SRank(i, k)$ and $ERank(i)$ as follows:\n$I(G^i, F_k) := KendallCorr(SRank(i, k), ERank(i)),$ (4)\nwhere $I(G^i, F_k)$ evaluates the correlation between the statistical differences in feature $F_k$ (between $G^i$ and other graphs $\\{G^j\\}$) and the empirical differences in transferring knowledge from other graphs $\\{G^j\\}$ to $G^i$. In other words, $I(G^i, F_k)$ can be the \"confidence\" of graph topological feature $F_k$ to determine the empirical similarity between $G^i$ and other graph data within our knowledge. That is, the larger $I(G^i, F_k)$ is, the more $F_k$ is a strong correlation indicator on $G^i$ that can determine whether other graphs $\\{G^j\\}$ is related to $G^i$ from the perspective of graph topology and empirical performance."}, {"title": "3.2 Knowledge Retrieval Module", "content": "The proficiency of the GNN architecture design process critically depends on the ability to accumulate, discern, and effectively apply intricate knowledge about the interrelationships between graphs, GNNs, and performance as $M : (G, \\theta) \\rightsquigarrow P$. However, as illustrated in Fig. 1, AutoGNNs often lack this data-wise knowledge prior to extensive training, while native LLMs only have limited capabilities to design some more general GNNs as studied in Appx. B.3.1. To bridge this gap, we utilize the detailed records of model-to-performance relationships captured in NAS-Bench-Graph [40], which contains diverse and intricate mappings across various graphs, a wide range of candidate GNNs, and their corresponding performance.\nTo capture the complex \u201cGraph-GNN-Performance\u201d correlations, we encapsulate the Top-Nm model designs from NAS-Bench-Graph [40] to serve as concrete summaries of model preferences for different benchmark graphs. Further, we associate these model summaries with their corresponding filtered features generated by the Graph Understanding module of Sec. 3.1 (i.e., using natural language to describe graph topological features and GNN models), then creating 9 distinct knowledge bases for selective retrieval. This strategy simplifies the complex interrelationships into more manageable correlations between datasets and their top-performing models: $M^* : G \\rightarrow \\theta^*$. Then, we employ LLMs to analyze and compare the similarities between an unseen dataset $G^u$ (user inputs) and benchmark graphs $G = \\{G^i\\}_{i=1}^9$ (within our knowledge) as follows:\n$Sim^u = \\{LLMGDC(F(G^u), F (G^i))\\}_{i=1}^9,$ (5)\nwhere we transform the filtered features F(\u00b7) into natural language before feeding them into GPT-4 $LLMGDC(\u00b7, \u00b7)$. As shown in Fig. 2, this process is conceptualized as a prompt design problem, where $LLMGDC(\u00b7, \u00b7)$ is tasked with analyzing the descriptions of both unseen and benchmark graphs. The objective is to generate task similarities that correlate with the top-performing model patterns. This method effectively leverages the complex mappings captured in NAS-Bench-Graph [40] and adapts to the variability across graphs, thereby facilitating the retrieval of specialized, most relevant model configuration knowledge from the benchmark graph to the unseen.\nTo retrieve the most relevant knowledge to unseen graph $G^u$, we rank benchmark graphs based on their similarities $Sim^u$ and select Top-Ns most similar graphs. Then, the Knowledge Pool KP for $G^u$ is formed by collecting the top-performing models $\\{\\theta_m\\}_{m=1}^{N_m}$ from each one of selected Top-Ns benchmark graphs $G^i$, defined as:\n$KP = \\bigcup_{G^i \\in Top-Ns (Sim^u)} \\{(G^i, \\{\\theta_m\\}_{m=1}^{N_m}\\}\\},$ (6)\nwhere $KP_i = \\{(G^i, \\{\\theta_m\\}_{m=1}^{N_m}\\)}$ and $G^i$ is the $i$-th graph similar to $G^u$. This retrieval strategy collects the Top-Nm model designs from each similar benchmark graph for in-context learning. It streamlines the transfer of GNN configuration knowledge to $G^u$ and leverages the analytical capabilities of LLMs to ensure an effective, efficient, and interpretable knowledge accumulation process."}, {"title": "3.3 GNN Model Suggestion and Refinement", "content": "3.3.1 Initial Model Suggestion. As shown in Fig. 2, after understanding the graphs in Sec. 3.1 and building knowledge in Sec. 3.2, the LLM acts as a surrogate agent to facilitate the initial model suggestion process. This step integrates user requirements with the top model designs from each of the retrieved knowledge bases in KP and a detailed description of the search space, including the macro architecture and operation lists from NAS-Bench-Graph [40]. The initial model proposals are suggested by LLMs as follows:\n$\\theta_{ui} \\leftarrow LLM_{IMS}(F(G^u), KP_i), p_{ui} = H(\\theta_{ui}, \\omega; G^u),$ (7)\nwhere $\u03b8_{ui}$ is the $i$-th suggested model for the unseen graph $G^u$ based on the knowledge pool $KP_i$ of benchmark graph $G^i$, and $p_{ui}$ is the training performance of $\u03b8_{ui}$ on $G^u$. This process is designed to swiftly generate $N_s$ initial model proposals for the unseen graph $G^u$, each leveraging specialized knowledge from different benchmark graphs. Then, our framework will undertake experimental validation to assess each model\u2019s performance. Compared with AutoGNNs, this streamlined approach does not launch training before generating proposals. Thus, it not only significantly accelerates the model suggestion phase compared to classic AutoGNNs but also enhances the effectiveness of the models due to the integration of empirically derived configuration knowledge. Notably, our initial model suggestions can even surpass existing automated approaches that have iteratively refined 30 proposals (Appx. B.2.2).\n3.3.2 Model Proposal Refinement. To further enhance the effectiveness of the initial model proposals, we propose a more structured, knowledge-driven refinement strategy. This refinement strategy enables LLMs to refine models in a nuanced and fine-grained manner, emulating the exploration-exploitation process of human experts. By incorporating specific, empirically derived configuration knowledge into the refinement phase, we ensure that each refinement step is clearly informed by a deep understanding of what configurations have historically led to success under similar circumstances.\nAs formalized in Algo. 1, the process begins with (1) Re-rank Mechanism: The knowledge bases in KP are re-ranked according to the performance ranking of the corresponding initial model proposals $\\{\\theta_{ui}\\}_{i=1}^{N_s}$. The best proposal $\u03b8_{u1}$ is chosen as the starting point. (2) Controlled Exploration: Configurations within $KP_{2:Ns}$ are utilized to crossover the best proposal, yielding $N_c$ candidates. (3) Model Promotion Mechanism: These candidates are ranked based on their retrieved performances on the benchmark dataset of $KP_1$, with the most promising candidate advanced for further refinement at iteration $t$. (4) Directional Exploitation and Optimization: A comprehensive prompt that includes user requirements, task descriptions, search space details, previous training logs, and the optimization trajectory $\u0398_t$ guides the LLM controller $LLM_{GNAS}$ to mutate the promoted candidate using elite insights from $KG_1$. The refined model proposal is automatically constructed and trained to evaluate its performance $P_u$. The results $(\u0398_t, P_u)$ are appended into the optimization trajectory $\u0398$. If it surpasses the current best, it is updated as the new $\u0398^*."}, {"title": "4 EXPERIMENTS", "content": "4.1 Experimental Settings\n4.1.1 Task and Data Sets. Our experiments are conducted under the benchmark setting defined in NAS-Bench-Graph [40], focusing on the node classification across 8 benchmark graphs (except for ogbn-proteins, see Appx. B.1.1): Cora [45], Citeseer [45], PubMed"}]}