{"title": "A Library for Learning Neural Operators", "authors": ["Jean Kossaifi", "Nikola Kovachki", "Zongyi Li", "Davit Pitt", "Miguel Liu-Schiaffini", "Robert Joseph George", "Boris Bonev", "Kamyar Azizzadenesheli", "Julius Berner", "Anima Anandkumar"], "abstract": "We present NEURAL OPERATOR, an open-source Python library for operator learning. Neural\noperators generalize neural networks to maps between function spaces instead of finite-\ndimensional Euclidean spaces. They can be trained and inferenced on input and output\nfunctions given at various discretizations, satisfying a discretization convergence properties.\nBuilt on top of PyTorch, NeuralOperator provides all the tools for training and deploying\nneural operator models, as well as developing new ones, in a high-quality, tested, open-source\npackage. It combines cutting-edge models and customizability with a gentle learning curve\nand simple user interface for newcomers.", "sections": [{"title": "1 Introduction", "content": "Most scientific problems involve mappings between functions, not finite-dimensional data:\nnotably, partial differential equations (PDEs) are naturally described on function spaces. A\npractical use case would be, for instance, learning a solution operator mapping between initial\nconditions and solution functions. Traditional numerical methods operate on discretizations\nof functions based on meshes of the computational domains, with their accuracy heavily\ndepending on the meshes' resolutions. In concrete applications, such as weather or climate\nsimulations, the requirement of a fine mesh renders such methods computationally intensive,\nmaking it intractable to simulate solutions across large sets of parameters (such as initial\nconditions or coefficients) in a reasonable amount of time (Schneider et al., 2017).\n\nDeep neural networks have been considered to accelerate the solution of PDEs by mapping\nfrom parameters directly to the solution on a given discretization (Guo et al., 2016; Bhatnagar\net al., 2019; Wang et al., 2020; Gupta and Brandstetter, 2023). However, they can only\nlearn mappings between finite-dimensional spaces, not function spaces. In other words, the\nsolutions learned are tied to a fixed discretization. In particular, there is no guarantee that\nneural networks generalize to other discretizations, and they often perform poorly when\ninterpolated to higher resolutions (Azizzadenesheli et al., 2024).\n\nTo address these limitations, a new class of machine learning models, known as neural\noperators, was proposed (Li et al., 2020, 2021; Lu et al., 2021; Hao et al., 2023; Raonic et al.,\n2024). While standard neural networks learn mappings between fixed-size discretizations, i.e.,\nfinite-dimensional spaces, neural operators can directly learn mappings between functions, i.e.,\ninfinite-dimensional spaces (Bhattacharya et al., 2021; Lu et al., 2021; Kovachki et al., 2024;\nAzizzadenesheli et al., 2024). They are built from first principles to ensure a discretization"}, {"title": "2 The NeuralOperator Library", "content": "NEURALOPERATOR is open-sourced under MIT license\u00b9. In the following, we provide details\non its design principles and functionalities; see Figure 1 for an overview.\n\nThe NEURALOPERATOR library builds on the following guiding principles:\n\n\u2022 Resolution-agnostic: As the crucial difference to existing frameworks, modules in\nNEURALOPERATOR, such as data loaders, architectures, and loss functions, should be\napplicable to functions at various discretization. As an example, our implementation of"}, {"title": "3 Conclusion", "content": "NEURAL OPERATOR provides state-of-the-art neural operator architectures and associated\nfunctionality in a modular, robust, and well-documented package. Built on top of PyTorch,\nits simple interfaces and modular components offer a gentle learning curve for new users\nwhile remaining highly extensible for conducting real-world experiments with neural operator\nmodels. It aims to democratize neural operators for scientific applications, grow alongside\nthe field, and provide the latest architectures and layers as the state-of-the-art progresses."}]}