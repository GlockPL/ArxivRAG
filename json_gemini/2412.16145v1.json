{"title": "Offline Reinforcement Learning for LLM Multi-Step Reasoning", "authors": ["Huaijie Wang", "Shibo Hao", "Hanze Dong", "Shenao Zhang", "Yilin Bao", "Ziran Yang", "Yi Wu"], "abstract": "Improving the multi-step reasoning ability of large language models (LLMs) with offline reinforcement learning (RL) is essential for quickly adapting them to complex tasks. While Direct Preference Optimization (DPO) has shown promise in aligning LLMs with human preferences, it is less suitable for multi-step reasoning tasks because (1) DPO relies on paired preference data, which is not readily available for multi-step reasoning tasks, and (2) it treats all tokens uniformly, making it ineffective for credit assignment in multi-step reasoning tasks, which often come with sparse reward. In this work, we propose OREO (Offline REasoning Optimization), an offline RL method for enhancing LLM multi-step reasoning. Building on insights from previous works of maximum entropy reinforcement learning, it jointly learns a policy model and value function by optimizing the soft Bellman Equation. We show in principle that it reduces the need to collect pairwise data and enables better credit assignment. Empirically, OREO surpasses existing offline learning methods on multi-step reasoning benchmarks, including mathematical reasoning tasks (GSM8K, MATH), and embodied agent control (ALFWorld). The approach can be extended to a multi-iteration framework when additional resources are available. Furthermore, the learned value function can be leveraged to guide the tree search for free, which can further boost the performance during test time.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are increasingly applied to complex tasks requiring multi-step reasoning, such as mathematical problem solving (Uesato et al., 2022; Shao et al., 2024; Hendrycks et al., 2021), embodied agent control (Wang et al., 2023; Huang et al., 2022; Shridhar et al., 2020; Xiang et al., 2024), and web navigation (Deng et al., 2024; Zhou et al., 2023; Koh et al., 2024). Enhancing LLM reasoning with reinforcement learning (RL) has gained significant interest, as it offers the potential for self-improvement and learning without relying on human-labeled trajectories. However, many popular RL algorithms require costly online data collection, either by generating language on-the-fly or interacting with an environment. For instance, tuning LLMs with Proximal Policy Optimization (PPO, Schulman et al., 2017) is often prohibitively expensive for most users, which limits practical applications (Hu et al., 2023).\nIn contrast, offline RL methods, such as Direct Preference Optimization (DPO, Rafailov et al., 2024b), provide a more practical approach for aligning LLMs with human preferences. These methods enable practitioners to tune models using pre-existing datasets, eliminating the need for live interaction or data generation. However, attempts to enhance LLMs' multi-step reasoning abilities with DPO may deliver results close to or even worse than simpler methods like SFT (Yuan et al., 2024; Chen et al., 2024b). Additionally, DPO requires pairwise preference data. In multi-step reasoning tasks, however, data normally consists of independent trajectories with sparse rewards indicating success or failure. A common alternative is to extract correct trajectories from offline datasets and use them for supervised fine-tuning (Zelikman et al., 2022; Aksitov et al., 2023; Dong et al., 2023; Paulus et al., 2024). While this approach is simple and often effective, it fails to fully exploit the offline dataset's potential-particularly the opportunity to learn from failure experience and enhance model robustness (Kumar et al., 2022).\nIn this paper, we introduce OREO (Offline REasoning Optimization), an offline RL algorithm designed to enhance LLMs' multi-step reasoning capabilities. Building on insights from the extensive literature on maximum entropy RL (Ziebart,"}, {"title": "2 Related Work", "content": "Recently, the application of RL algorithms to improve LLM reasoning has gained increasing interest (Aksitov et al., 2023; Gou et al., 2023; Dong et al., 2024; Havrilla et al., 2024; Shao et al., 2024; Zhao et al., 2024), but the direct usages of DPO are not all successful (Yuan et al., 2024; Chen et al., 2024b) and efficient, as people have to specifically collect pairwise preference data (Chen et al., 2024a; Song et al., 2024). Our method addresses these limitations of DPO in reasoning with a principled solution. Another line of work aims to train a Process Reward Model (PRM), to provide finer-grained feedback on RL. It is typically trained with Monte-Carlo rollout (Wang et al., 2024a,b; Luo et al., 2024; Zhang et al., 2024), which is a special case of the value function learned through our method. We show that our value function enables test-time scaling (Hao et al., 2023; Snell et al., 2024; Wu et al., 2024; Brown et al., 2024; Yao et al., 2024; Hao et al., 2024a; Liu et al., 2024b) to further boost the reasoning performance through tree search."}, {"title": "3 Preliminaries", "content": "3.1 MDP for LLM Reasoning\nWe define the Markov Decision Process (MDP) for LLM reasoning. At each time step, a new token is generated as the action $a_t$. The state is represented as a token sequence. For reasoning tasks that don't involve interactions with the environment, $s_t$ records the context for LLMs, i.e., $s_t = (x_0, ..., x_L, y_0, \u00b7\u00b7\u00b7, y_{t-1})$, where $(x_0,..., x_L)$ is the input prompt and $(y_0, \u00b7 \u00b7 \u00b7, y_{t-1})$ is the sequence of generated tokens up to step $t \u2013 1$. The transition function $f$ for these tasks deterministically updates the state as $s_{t+1} = f(s_t, a_t) = s_t | a_t$, where | is concatenation.\nFor those tasks requiring interacting with an external environment, like embodied agent control, the state and transition function is slightly different: if $a_t$ is the final token of the agent's response (e.g., \u201cgo to desk 1\u201d), then $s_{t+1} = f(s_t, a_t) = s_t | a_t | next observation.\nThe reward function $r(s_t, a_t)$ is generally defined for every state-action pair to provide feedback throughout the generation process. However, in this work, we focus on the challenging case where the reward is non-zero only at the terminal step $T$, reflecting the correctness of the reasoning chain, or whether the task is successfully accomplished.\nFollowing the standard setup in Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022; Rafailov et al., 2024b), a KL-regularization term is introduced to encourage the learned policy to remain close to a reference policy while optimizing for rewards. Therefore, the optimal policy $\\pi_{\\theta}$ can be described as follows:"}, {"title": "3.2 Soft Bellman Equation", "content": "Entropy-regularized reinforcement learning (RL) (Ziebart, 2010; Nachum et al., 2017; Haarnoja et al., 2017) augments the standard reward maximization objective with an entropy term to encourage exploration and improve the robustness of the learned policy. This formulation has a strong connection to entropy-regularized RL, as the Kullback-Leibler (KL) divergence between two distributions can be decomposed into a cross-entropy term and an entropy term, i.e., $D_{KL}(\\pi(\\cdot|s)||\\pi_{ref}(\\cdot|s)) = \u0395_{\\pi}[-log \\pi_{ref}(a|s)] \u2013 \u0395_{\\pi}[- log \\pi(a|s)]$.\nAdapting from the well-established theory in entropy-regularized RL to our setting, we first define the value function $V^{\\pi}$ of a policy, which quantifies the expected KL-regularized reward of a policy $\\pi$ from any given state:\nCompared to the value function in standard RL, which only includes expected rewards, the above definition incorporates an additional KL regularization term: $\\beta log \\frac{\\pi(a_{t+l}|s_{t+l})}{\\pi_{ref}(a_{t+l}|s_{t+l})}$.\nTheorem 1 The optimal policy and its value function satisfy the soft Bellman Equation:\nBuilding on Nachum et al. (2017); Haarnoja et al. (2017), we extend their theorem with a lightweight derivation tailored to our setting, with the proof provided in Appendix B.\nThis equation characterizes the relationship between the optimal policy and its value function, providing a theoretical basis for our proposed method. When $\\beta = 0$, the equation degenerates to the Bellman equation in standard RL. Importantly, when the soft Bellman Equation is always satisfied, the policy and the value function are guaranteed to be the optimal ones:\nTheorem 2 If a policy $\\pi(a | s)$ and state value function $V(s)$ satisfy the consistency property (3) for all states s and actions a (where s' = f(s, a)), then $\\pi = \\pi^*$ and $V = V^*$.", "latex": ["D_{KL}(\\pi(\\cdot|s)||\\pi_{ref}(\\cdot|s)) = \u0395_{\\pi}[-log \\pi_{ref}(a|s)] \u2013 \u0395_{\\pi}[- log \\pi(a|s)]"]}, {"title": "3.3 Connection to DPO", "content": "In this section, we introduce how DPO can be derived from the formulation above with two additional assumptions. This enables us to understand the limitation of DPO on LLM reasoning from the principle, and motivates us to propose the new method. Rafailov et al. (2024a) present a related derivation to analyze the properties of DPO.\nFirst, DPO relaxes the requirements of soft Bellman Equation by telescoping time steps:\nIt then introduces the Bradley-Terry preference model (Bradley and Terry, 1952), which assumes that the probability of one response being preferred over another is determined by the normalized relative exponential rewards of the responses:\nBy maximizing the log-likelihood that a winning response is preferred over a losing response with a preference dataset $D = {(\\tau^{w}, \\tau^{l})}$, the loss function of DPO can be derived:"}, {"title": "4 OREO: Offline Reasoning Optimization", "content": "Based on the theorems presented in Section 3, we present the detailed formulation of our method OREO. We further introduce two objective function variants, an iterative extension of our approach, and a test-time search strategy leveraging the value function."}, {"title": "4.1 Learning Objetive", "content": "We adopt a similar method as PCL (Nachum et al., 2017) to fine-tune the LLM. Inspired by Theorem 2, we optimize the policy by enforcing the soft Bellman Equation property given in Eq. 3. In our setting where the reward signal is sparse, we aim to enforce the telescoped version of Eq. 3, namely\nwhere $R_t = \\sum_{i>t}r(s_i, a_i)$. Note that DPO leverages Eq. 4, which is a special case of Eq. 7 with t = 0. We train a separate value network $V_{\\phi}$ together with the policy $\\pi_{\\theta}$. We adopt the MSE loss for the value network:\nThe policy objective is given by\nHere sg[.] denotes the stop gradient operator, which makes each step have the same scale in the gradient. $\\mathcal{L}_{reg} = \\sum_{t=0}^{T-1} KL[\\pi_{\\theta}(\\cdot|s_t)||\\pi_{ref}(\\cdot|s_t)]$ is a regularization term that helps stabilize training."}, {"title": "4.2 Loss Variants", "content": "In addition to our OREO learning objective, we present two variants: step-level OREO and response-level OREO.\nIn step-level OREO, an action is considered to be an entire reasoning step instead of a single generated token. For example, In May, Natalia sold 48"}, {"title": "4.3 Iterative OREO", "content": "Previous works have shown that offline LLM finetuning methods can be applied iteratively to improve model performance (Pang et al., 2024; Song et al., 2024; Xiong et al., 2024). After each iteration, a new dataset is collected using the updated policy model to generate responses or explore the environment, which is used for further training."}, {"title": "4.4 Test-Time Search with Value Function", "content": "Recently, inference-time scaling (Hao et al., 2024a; Snell et al., 2024; Wu et al., 2024) has received significant research attention. One notable approach is the use of Process Reward Models (PRM), which evaluate whether a reasoning step is correct. During the inference time, rather than decoding the reasoning chain autoregressively from the policy model, one can conduct a tree search (e.g., beam search) guided by the PRM. Our method provides a value model for free, which estimates the expected future reward and can be directly used to guide beam search.\nIn fact, previous PRM methods (Wang et al., 2024a,b; Luo et al., 2024) train their models using Monte Carlo rollouts, which are essentially similar to the objective used for training the value function in our approach (Eq. 8). Our principled formulation removes the need for the extensive heuristic designs commonly required in prior works.\nWe implement step-level beam search for math reasoning tasks. At each step, we maintain a set of B candidate partial trajectories. For each candidate, we generate B potential next reasoning steps. From the resulting B2 candidates, we retain the B with the highest values.\nFor embodied agent tasks, where environment dynamics are unknown, beam search is not applicable. Instead, we sample K actions at each step and select the action with the highest value."}, {"title": "5 Experiments", "content": "In this section, we evaluate our method OREO in math reasoning and embodied agent tasks. We also demonstrate that the value function trained alongside the policy can further improve model performance at test time through step-level beam search or choosing the best-of-K action.\nDatasets and Evaluation Metric. We adopt the GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) dataset for the task of math reasoning. GSM8K is a dataset of grade school math problems. It contains 7473 training problems and 1319 test problems. MATH consists of competition-level math problems, with a training set of size 7500 and a test set of size 5000. All problems in these datasets are labeled with step-by-step ground-truth solutions. We use the script from DeepSeekMath2to extract the final answer from the solution and evaluate its correctness.\nWe adopt ALFWorld (Shridhar et al., 2020) for the task of embodied agent control. ALFWorld provides interactive TextWorld environments for household tasks. Each task is labeled with an expert trajectory. However, these data do not contain any reasoning process. Song et al. (2024) annotates 3119 ALFWorld training trajectories with rationales for each step, allowing model training with ReAct-style (Yao et al., 2022) prompting. The evaluation set of ALFWorld contains 140 tasks in seen environments and 134 tasks in unseen environments. We evaluate the success rates of agents in completing the tasks within 40 steps.\nBase Models. For the math reasoning task, we select Qwen2.5-Math-1.5B (Yang et al., 2024) and DeepSeekMath-7B-Instruct (Shao et al., 2024) as our base model. For the embodied agent task, we use MiniCPM-2B-dpo-bf16 (Hu et al., 2024) as the base model.\nBaseline Methods. In addition to supervised finetuning, we compare our method against three other baselines:\n\u2022 Rejection Sampling: The method uses the successful trajectories in the offline dataset to su-"}, {"title": "5.1 Main Results", "content": "We present the experimental results on mathematical reasoning in Table 1. Consistent with prior research (Yuan et al., 2024; Pang et al., 2024), we observe that while DPO provides marginal improvements over the SFT checkpoint used for its initialization, simpler methods such as rejection sampling often outperform DPO. In contrast, OREO demonstrates consistent superiority over all baselines across both datasets (GSM8K and MATH). This improvement is also observed universally across models in the Qwen and DeepSeekMath families. Specifically, for Qwen-2.5-Math 1.5B, OREO achieves a 5.2% relative improvement over SFT on GSM8K and a 10.5% improvement on MATH. For DeepSeekMath 7B, despite the SFT checkpoint being heavily tuned with 776K samples (Shao et al., 2024), OREO still delivers meaningful improvements, with relative gains of 3.6% on GSM8K and 5.1% on MATH. These results highlight the robustness and effectiveness of our approach across different models and datasets.\nThe experimental results on ALFWorld, an embodied control task, are presented in Table 2. OREO outperforms all baselines in both settings. Interestingly, rejection sampling performs well in seen environments within ALFWorld. However, its improvement is marginal in unseen settings, whereas OREO achieves a significant 17.7% relative improvement over the baseline. Compared to SFT which only learns from successful experience,"}, {"title": "5.2 Iterative OREO", "content": "Figure 2 illustrates the performance of various algorithms on the math reasoning task across multiple iterations. OREO demonstrates steady and consistent improvements in accuracy over three iterations, showcasing its robustness in leveraging iterative training. While baseline methods also benefit from collecting additional data during each iteration, their performance consistently lags behind that of OREO. Notably, rejection sampling shows signs of saturation by the third iteration, with diminishing performance gains. In contrast, OREO continues to improve, likely due to its ability to effectively learn from failed trajectories. The updated policy model in each new iteration may be able to explore novel failure patterns, and incorporate these insights into the learning process. This potentially explains why OREO benefits more from multiple iterations compared to rejection sampling."}, {"title": "5.3 Implicit vs Explicit Value Functions", "content": "In DPO, the policy model is viewed as an implicit value function (Rafailov et al., 2024a). However, our results in Section 5.1 have demonstrated that OREO benefits from explicitly parameterizing a separate value function. In this section, we present"}, {"title": "5.4 Test-Time Search with Value Functions", "content": "The superiority of the explicit value function motivates its use to enhance inference through searchbased methods. For our experiments, we evaluate a subset of the MATH dataset containing 500 queries, a commonly used benchmark in prior work (Lightman et al., 2023; Sun et al., 2024).\nFigure 3 shows the performance of step-level beam search in math reasoning. OREO leverages the value function to achieve progressively higher accuracies as the computational budget increases. Compared to greedy decoding, beam search with B = 7 provides a 11.4% relative improvement in GSM8K and a 17.9% relative improvement in MATH. This indicates that the explicit value function is more effective than the policy in distinguishing between correct and incorrect reasoning steps.\nSimilarly, Figure 4 presents the success rates in ALFWorld when selecting the best-of-K actions. The success rates improve rapidly as the number of sampled actions increases, while stabilizing with five samples. Importantly, the value function is a natural byproduct of the OREO training framework, unlike prior work on PRM, which often involves substantial data engineering and heuristic design efforts."}, {"title": "6 Conclusion", "content": "In this paper, we present Offline REasoning Optimization (OREO), an offline RL algorithm for LLM reasoning and embodied LLM agent tasks. OREO leverages the idea of soft Q-learning. It trains an explicit value function together with the LLM policy by optimizing the soft Bellman Equation. This alleviates the need for paired preference data in DPO and enables fine-grained credit assignment among the reasoning steps. In addition, our value function can be used in test-time search to improve the model performance. We evaluate our method in GSM8K, MATH, and ALFWorld, demonstrating a consistent improvement compared to previous offline RLHF methods like DPO."}, {"title": "7 Limitations", "content": "Due to limited computation resources, some of our experiments, including ablation studies, iterative OREO, and test-times search, use 1.5B models. We plan to run experiments on larger scales in the future. Our method has primarily been evaluated on mathematical reasoning and embodied agent tasks. As future work, we aim to extend OREO to a wider variety of tasks, such as coding and web browsing, to explore its effectiveness in domains with different structures and requirements."}, {"title": "A Implementation Details", "content": "We use the datasets GSM8K and MATH to train the SFT model for math reasoning. For the task of math reasoning using 1.5B models, we sample 10 responses for each query in the GSM8K dataset and the MATH dataset. Only trajectories with correct answers receive a reward 1 at the terminal state. For DPO, we pair up the positive and negative instances for each query and sample at most 6 pairs without replacement.\nWhen training 7B models, we apply a different data-collecting strategy to balance the number of positive and negative instances. We sample 16 responses for each query and randomly select at most 4 positive instances and 4 negative instances. We then enforce that the number of positive instances does not exceed that of negative instances. For example, if there are only 3 negative instances in the 16 sampled responses, then we only select 3 positive instances instead of 4. However, we make sure that at least 1 positive instance is selected for the query (if there is any). For DPO, we then sample at most 10 pairs of data without replacement.\nFor the task ALFWorld, we use the annotated data from Song et al. (2024) to train our SFT model. We perform 5 rollouts for each task in the training set. Successful trajectories receive a reward 1 at the end of the trajectory. We sample at most 5 preference pairs for DPO."}, {"title": "C Safeguard Statement", "content": "In this paper, we primarily focus on the math reasoning tasks and embodied agent control tasks in a household simulator, posing no significant ethical or harmful concerns. We recognize that future research on border applications of multi-step reasoning may pose a risk of misuse, and we recommend careful consideration of all aspects of safety before it's applied in the real world."}], "A.1 Dataset Construction": [{"title": "A.2 Segmentation of Reasoning Steps", "content": "In step-level beam search and step-level OREO, we use line breaks and periods to indicate the end of a reasoning step. More specifically, we use line breaks in the GSM8K dataset and use both line breaks and periods in the MATH dataset. This is because the GSM8K dataset already split reasoning steps with line breaks."}, {"title": "A.3 Hyperparameters", "content": "The batch size is set to 128 for all experiments."}]}