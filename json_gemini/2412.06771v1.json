{"title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty", "authors": ["Meera Hahn", "Wenjun Zeng", "Nithish Kannen", "Rich Galt", "Kartikeya Badola", "Been Kim", "Zi Wang"], "abstract": "User prompts for generative AI models are often underspecified, leading to sub-optimal responses. This problem is particularly evident in text-to-image (T2I) generation, where users commonly struggle to articulate their precise intent. This disconnect between the user's vision and the model's interpretation often forces users to painstakingly and repeatedly refine their prompts. To address this, we propose a design for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their understanding of user intent as an understandable belief graph that a user can edit. We build simple prototypes for such agents and verify their effectiveness through both human studies and automated evaluation. We observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow. Moreover, we develop a scalable automated evaluation approach using two agents, one with a ground truth image and the other tries to ask as few questions as possible to align with the ground truth. On DesignBench, a benchmark we created for artists and designers, the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), we observed that these T2I agents were able to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2014) than the standard single-turn T2I generation.", "sections": [{"title": "1. Introduction", "content": "A fundamental challenge in the development of AI agents is how to foster effective and efficient multi-turn communication and collaboration with human users to achieve user-defined goals, especially when faced with the common issue of vague or incomplete instructions from humans. We focus specifically on text-to-image (T2I) generation, where recent advancements (Baldridge et al., 2024; Betker et al., 2023; Podell et al., 2023; Yu et al., 2023) have enabled the creation of stunning images from complex text descriptions. However, users often struggle to describe the image they would like to generate in a way that T2I systems can fully understand. This leads to unsatisfactory results and repeated iterations of prompts.\nThe prompt underspecification problem arises from the inherent ambiguity of natural language, the different assumptions that humans make and the vast space of potential images that can be generated from a single prompt (Hutchinson et al., 2022). Imagine a prompt generate an image of a rabbit next to a cat. This seemingly simple prompt leaves many important aspects underspecified: What kind of rabbit? What color is the cat? What is their relative positions? What is the background? While a T2I model can generate an image with a rabbit and a cat in it, it is unlikely that the image captures the specific details a specific user has in mind. For example, people in Holland might assume it is common for rabbits to have lop ears, but people in New England might expect to see a cottontail rabbit with straight ears. The combination of all these factors can lead to a frustrating cycle of trial-and-error, with the user repeatedly refining their prompt in an attempt to steer the model towards the desired output (Huang et al., 2024; Sun and Guo, 2023; Vodrahalli and Zou, 2024).\nInstead of relying on passive T2I models that simply generate images based on potentially vague user instructions, we pursue a quest for agency in T2I generation. The T2I agents should actively engage with human users to provide a collaborative and interactive experience for image creation. We envision that these T2I agents will be able to (1) express and visualize their beliefs and uncertainty about user intents, (2) allow human users to directly control their beliefs beyond just text descriptions, and (3) proactively seek clarification from the human user to iteratively align their understanding with what the human user intends to generate.\nIn this work, we develop simple prototypes of such agents. At the core of those agent prototypes, we build in a graph-based symbolic belief state, named belief graph, for agents to understand its own uncertainty about possible entities (e.g., rabbit) that might appear in the image, attributes of entities (e.g., rabbit's color), relations between entities and so on. Given a user prompt, we use an LLM and constrain its generation to the graph structure of beliefs, which include probability estimates on the appearance of entities and the possible values for attributes and relations. Figure 1 illustrates the interface and features of the prototypes. In particular, the agent can ask questions based on its uncertainty. For example, a very simple strategy is to find the most uncertain attribute of an entity (e.g., rabbit's color) and use an LLM to phrase a question about the attribute (e.g., What is the color of the rabbit?). The agent can also guide users to directly edit uncertain items in the graph.\nBased on user answers to agent questions or direct edits in the belief graph, the agent uses an LLM to update the prompt. It also transitions to a new belief graph by modifying the uncertainty of items clarified by the user. Based on the updated prompt, the agent calls an off-the-shelf T2I model to generate images. The structure of our agent prototypes is highly modular, making it easy to improve each component individually, e.g., changing the strategy for asking questions, updating the belief graph construction method, and switching to better LLMs or T2I models when they become available.\nTo evaluate the utility of our agent prototypes, we conduct both automatic evaluations and human studies. We develop automatic evaluation pipelines to assess the effectiveness and efficiency of the T2I agents when interacting with simulated users with underspecified prompts answering questions"}, {"title": "2. Related work", "content": "From the very outset of artificial intelligence, a core challenge has been to develop intelligent agents capable of representing knowledge and taking actions to acquire knowledge necessary for achieving their goals (McCarthy and Hayes, 1969; Minsky, 1974; Moore, 1985; Nilsson, 2009; Russell and Norvig, 2016). Our work is an attempt to address this challenge for intelligent T2I agents.\nIn machine learning and statistics, efficient data acquisition has been extensively studied for many problems, including active learning (Cohn et al., 1996; Gal et al., 2017; Houlsby et al., 2011; Ren et al., 2021; Settles, 2009; Wang et al., 2018), Bayesian optimization (Auer, 2002; Garnett, 2023; Hennig and Schuler, 2012; Kushner, 1964; Mo\u010dkus, 1974; Srinivas et al., 2010; Wang and Jegelka, 2017; Wang et al., 2024b), reinforcement learning (Ghavamzadeh et al., 2015; Kaelbling et al., 1996; Sutton, 2018) and experimental design (Chaloner and Verdinelli, 1995; Kirk, 2009). We reckon that T21 agents should also be capable of actively seeking important information from human users to quickly reduce uncertainty (Wang et al., 2024c) and generate satisfying images. In \u00a7E, we detail the implementation of action selection strategies for our T2I agents.\nIn human-computer interaction, researchers have been extensively studying how to best enable Human-AI interaction especially from user experience perspectives (Amershi et al., 2019; Cai et al., 2019; Chen et al., 2024; H\u00f6\u00f6k, 2000; Kim et al., 2023; Norman, 1994; Vi\u00e9gas and Wattenberg, 2023; Yang et al., 2020). Interface design for AI is becoming increasingly challenging due to the lack of transparency (Chen et al., 2024; Vi\u00e9gas and Wattenberg, 2023), uncertainty about AI capability and complex outputs (Yang et al., 2020). We aim to build user-friendly agents, and an indispensable component is their interface to enable them to effectively act and observe, as detailed in \u00a7G.\nInterpretebaility. Surfacing an agent's belief overlaps with interpretability as both aim to understand model or agent's internal. Some methods leverage LLM\u2019s natural language interface to surface their reasoning (e.g., chain of thought (Wei et al., 2023a)), sometime interactively (Wang et al.,"}, {"title": "3. Background", "content": "The belief graph in our work is closely related to symbolic world representations.\nWorld states. In classical AI, researchers use symbolic representations to describe the world state (Kaelbling and Lozano-P\u00e9rez, 2011; McCarthy and Hayes, 1969; Minsky, 1974, 1988; Pasula et al., 2007). For example, in the blocks world (Alkhazraji et al., 2020; Ginsberg and Smith, 1988; Gupta and Nau, 1992), a state can be\n$is\\_block(a) \\wedge is\\_red(a) \\wedge on\\_table(a) \\wedge is\\_block(b) \\wedge is\\_blue(b) \\wedge on(b, a)$,\ndescribing that there are a red block and a blue block, referred to as a and b, block a is on a table, and block b is on a. Such world states must include entities (e.g., a and b), their attributes (e.g., position on_table, characteristics is_block) and relations (e.g., on(b, a)) which are critical for enabling a robot to know and act in the world.\nIn linguistics, Davidson (1965, 1967a,b) introduce logic-based formalisms of meanings of sentences. The semantics of a sentence is decomposed to a set of atomic propositions, such that no propositions can be added or removed from the set to represent the meaning of the sentence. (Cho et al., 2023) propose Davidsonian Scene Graph (DSG) which represent an image description as a set of atomic propositions (and corresponding questions about each proposition) to evaluate T2I alignment.\nWe borrow the same concept as symbolic world representations and scene graphs, except that the agent needs to represent an imaginary world. The image generation problem can be viewed as taking a picture of the imaginary world. The world state should include all entities that are in the picture, together with their attributes and relations.\nBelief states. Term \"belief state\" (Kaelbling et al., 1998; Nilsson, 1986) has been used to describe a distribution over states. E.g., for block a, we might have $p(on\\_table(a)) = 0.5$ and $p(\\neg on\\_table(a)) = 0.5$, which means the agent is unsure whether the block is on a table. To represent the T2I agent's belief on which image to generate, we need to consider the distribution over all possible \u201cworlds\u201d in which the picture can be taken. This distribution can be described by the probabilities that an entity appears in the picture, an attribute gets assigned a certain value, etc.\nA core difference between our belief graphs and classic belief states is that belief graphs do not need pre-defined predicates (e.g., on(b, a)), but instead automatically produce useful predicates using LLMs. This makes belief graphs much more generalizable across domains. More details in \u00a7B."}, {"title": "4. Proactive T2I agent design", "content": "We provide high-level principles and design that guide our agent how to behave and interact with users to generate desired images from text through multi-turn interactions. The goal of the agent is to generate images that match the user's intended image as closely as possible with minimal back-and-forth, particularly in cases with underspecified prompts and the agent needs to gather information proactively. This requires a decision strategy on information gathering to trade off between the cost of interactions and the quality of generated images. The formal problem definition can be found in \u00a7C."}, {"title": "4.1. What kind of questions should be asked?", "content": "We explain considerations in question asking and examples of strategies in this section."}, {"title": "4.1.1. Principles", "content": "We identify the following principles for an agent to ask the user questions about the underspecified prompt and their intended image: (i) Relevance: The question should be based on the user prompt. (ii) Uncertainty Reduction: The question should aim to reduce the agent's uncertainty about the attributes and contents of the image, the objects, the spatial layout, and the style. (iii) Easy-to-Answer: The question should be as concise and direct as possible to ensure it is not too difficult for the user to answer. (iv) No Redundancy: The question should not collect information present in the history of interactions with the user. The Relevance and No Redundancy principles are self-explanatory, we detail the other two principles below.\nThe Uncertainty Reduction principle aims to let agent elicit information about various characteristics of the desired image, which the agent is unsure of.\nFirst, the agent needs to know what characteristics of images are important. Some examples include: (i) Attributes of the subjects, such as breed, size, or color, with questions like What kind of rabbit? What color is the cat?; (ii) Spatial relationships between the subjects, such as proximity and relative position (Are the rabbit and cat close to each other? Are they facing each other?); (iii) Background information, such as location, style and time of day (Are they in a park or at home?); and (iv) Implicit entities that might not be explicitly mentioned in the initial prompt but are relevant to the user's vision (Are there any other animals or people present?).\nSecond, the agent needs to know its own uncertainty about those characteristics. In the agent's belief, the uncertainty is explicit. One strategy is to form questions about the image characteristics that the agent is most uncertain about. We discuss more in \u00a7E.2.\nThird, the agent needs to update its own uncertainty once the user gives a response to its question (a.k.a. transition in \u00a74.3). Then, it can construct questions again based on its updated uncertainty estimates. This iterative clarification process allows the agent to progressively refine its understanding of the user's intent and generate an image that more accurately reflects their desired output.\nThe Easy-to-Answer principle aims to reduce users' effort to respond to questions. One way is to have the agent provide some answer options, where options are what the agent believes likely to appear. E.g., What color is the cat? (a) Black (b) Brown (c) Orange (d) Other (please specify)."}, {"title": "4.1.2. Examples of question-asking strategies", "content": "Given the agent belief constructed from the user prompt (more details in \u00a74.2), several basic approaches can be employed following the above principles. We construct simple agents with the following strategies, which are implemented and used in our experiments.\n\u2022 Ag1 (\u00a7E.5): Rule-based question generation, which leverages predefined rules or heuristics to identify salient attributes, entities, or relationships that require clarification. For example, an LLM could be used to estimate the importance and likelihood of different components within the belief,"}, {"title": "4.2. Interacting with the user based on agent beliefs", "content": "The Uncertainty Reduction principle inspires the usage of belief graphs for the agent to directly express uncertainty, in addition to reflecting uncertainty through questions. Instead of using hardcoded symbols in classic belief representations (Fikes and Nilsson, 1971) described in \u00a73, we employ LLMs to generate names and values for entities, attributes and relations. As a result, this belief construction method can generalize across any prompts. Algorithm 1 summarizes how an agent parses a prompt to a belief graph and allows user interaction\u00b3. All agents in \u00a74.1.2 use the same kind of belief graphs.\nEntities. In addition to (a) entities mentioned in the user prompt, a belief graph also includes (b) implicit entities not mentioned in the prompt but likely to appear, e.g., pet owner in the context of a pet-related scene; and (c) background entities, such as image style, time of day, location, which play important roles in constructing the image.\nAttributes and relations. While the prompt might mention some attributes of a certain entity, they are not enough to describe the exact details of that entity. Hence the agent have to imagine the relevant attributes for each entity, and construct a list of possible values along with their associated probabilities (e.g., the color attribute for the cat entity might have values like black, white, gray with corresponding probabilities). Similarly the agent may have to imagine the possible relations between entities, e.g., spatial relation between rabbit and cat might include values like close, far, touching.\nImportance scores. While the agent can be uncertain about many aspects of the user's intended image, some are more important than others. E.g., for prompt \u201ca rabbit and a cat\u201d, the agent might be very uncertain about the exact color of a carpet that might appear in the image, but rabbit and cat are more important than the carpet. We enable agents to estimate an importance score for each entity, attribute and relation.\nExtracting beliefs and enabling interactions. A simple idea is to use a large language model (LLM) via in-context learning. \u00a7E.1 details how an LLM may analyze the user prompt to identify entities, their attributes, and the relations between them, effectively translating the natural language input into a structured representation within the belief. Once the belief is extracted, a user can edit the belief to adjust uncertainty levels, confirm existence of entities etc, as shown in Figure 1."}, {"title": "4.3. Transition", "content": "The agent belief undergoes a transition whenever the agent receives new information through user feedback, either user answers from the agent question or user interactions with the graph-based"}, {"title": "5. Experiments", "content": "We conduct 2 types of experiments to study the effectiveness of the proposed agent design: automatic evaluation which uses a simulated user to converse with a T2I agent and human study which studies the efficacy of our framework with human subjects."}, {"title": "5.1. Automatic evaluation", "content": "We simulate the user-agent conversation using self-play (Shah et al., 2018) between two LLMs. The conversation starts with an arbitrarily chosen image to represent the goal image from a T2I model that the user has in mind\u2074. Along with this ground truth image, a user has a detailed prompt (i.e., the ground truth) in mind that describes the image in high-detail. We use the algorithm similar to Ag2 (detailed in \u00a7E.4) to simulate the user, where the questions are answered based on the ground truth prompt and the belief graph generated from the ground truth prompt. We run the agent-user conversation for a total of 15 turns and compute different metrics at the end of each turn. More details of the simulated user can be found in the appendix, including the prompts provided to the LLM when simulating the user are provided. Figure 2 part b shows the multi-turn set up that we use in our results."}, {"title": "5.1.1. Setups for agents and baseline", "content": "Baselines. We use a standard T2I model as a baseline, which directly generates an image based on a prompt without asking any questions. We refer to this baseline as 'T2I'.\nAgents. We use Ag1, Ag2 and Ag3 with question-asking strategies introduced in \u00a74.1.2. The creation and updates to the belief graph (\u00a74.2), as well as transitions to prompt (\u00a74.3) are consistent among all multi-turn agents. Further implementation details of each agent can be found in \u00a7E.\nModel Selection. In this work we use an off-the shelve Text-to-Image (T2I) model and a Multi-Modal Large Language (MLLM) model and build the different components of our agent on top of these models. We keep these models consistent across all agents for fair comparison. We implement the agent on top of the Gemini 1.5 (Gemini Team Google, 2024) using the default temperature and a 32K context length. The in-context examples and the exact prompt used at each step of the agent pipeline is detailed in \u00a7E.8 - \u00a7E.15. More agent implementation details are provided in \u00a7E. For T2I generation, we use Imagen 3 (Baldridge et al., 2024) across all baselines given it's recency and prompt-following capabilities. We used both the models served publically using the Vertex API."}, {"title": "5.3. Analysis of quantitative results", "content": "The evaluations on the COCO-captions, ImageInWords, DesignBench datasets show similar results and highlight the same patterns across the different agents.\nMulti-Turn agents show clear advantage: The immediate take away is the baseline which does not use multi-turn interaction and instead passes in the original prompt into the T2I model performs worse than the multi-turn agents on all metrics on both datasets. This confirms our hypothesis that the current T2I agents often produce less desirable images given ambiguity in prompts. In Figure 2 we see real outputs of the multi-turn set up with the Ag3 agent.\nLLMs being a part of agents play a significant role: The best performers (Ag2 and Ag3) both query and LLM to provide a question to ask the user based on contextual information such as the belief graph and conversation history. They query the LLM to construct a concise and clear question but don't impose further constraints on the question construction. Ag1 provides a programatic template for how the LLM should construct the question based on its belief graph and does not provide any conversation history information. Examples of dialogs and the generated questions produced by the three agents can be found in the Appendix in Figure 6. This figure demonstrates that the templated question creation leads to extremely specific questions that often gather minimal information in return. This is an intrinsic limitation of hard coded question selection strategy but also can be an issue of the heuristic scores we defined for question selection in Ag1. In contrast, Ag2 and Ag3 generate questions that are more open-ended thus allowing the user to provide more nuanced details which in"}, {"title": "5.4. Human studies on generated images and dialogues", "content": "To verify the automatic evaluations of the agents, we performed human studies in which participants were asked to rate the generated images and dialogues along different axes. The detailed design of the studies can be found in Figure 20, Figure 21 and Figure 22.\nParticipants are asked to rate the images produced by the three proposed multi-turn agents and a single-turn T2I model against a Ground Truth image for which the original prompt was derived and the answers to the agents questions were derived. Approximately 550 image-dialog pairs per"}, {"title": "5.5. Human studies on the agent interface", "content": "To get real user feedback on the agent interface, we performed a human survey with the objective of understanding user frustrations and validating our solutions. We gathered data from 143 participants who all identified to be regular T2I users (at least once a month). Participants were presented with four hypothesized frustrations (prompt misinterpretation, many iterations, inconsistent generations, incorrect assumptions) and three potential mitigating features (clarifications, entity graph, relationship graph; more details in \u00a7H).\nTable 4 in Appendix confirms the prevalence of hypothesized frustrations amongst users, with 83% experiencing occasional, frequent, or very frequent frustration due to prompt iterations, followed by 70% for misinterpretations, 71% for inconsistent generations, and 60% experiencing frustration due to incorrect assumptions. Most acutely 55% of participants reported frequent or very frequent frustration due to the prompt iteration frequency necessary. In Table 2, we report the mitigation features that are likely to help. Clarifications reported the highest likelihood to help current workflows (91% could / likely / very likely to be helpful), followed by entity graphs (88% could / likely / very likely to be helpful) and relationship graphs (86% could / likely / very likely to be helpful). Clarifications were expected to deliver value immediately / very soon by 58%.\nOverall these suggest strong user desire for & likelihood for success of features that reduce iterations and mitigate misinterpretations in T2I generation. Full explanations of the hypothesized frustrations, mitigation and responses splits are in \u00a7H. All respondents were compensated for their time as per market rates, and were recruited by our vendor to ensure diversity across age, gender, and T2I usage in terms of models, frequency and purpose (work and non work)."}, {"title": "6. Discussion and conclusion", "content": "This work introduces a design for agents that assist users in generating images through an interactive process of proactive question asking and belief graph refinement. By dynamically updating its understanding of the user's intent, the agent facilitates a more collaborative and precise approach to image generation. Moreover, presenting the agent's belief graph can be a generalizable method for AI transparancy, which is an important factor given the increasing complexity of modern AI models.\nModular design. Our agent prototypes are highly modular: the agents use frozen T2I models to generate images based on the prompts that the agent updated. Therefore when a better off-the-shelf T2I model becomes available, it can be directly plugged into the agents and the system will achieve better performance without any additional adaptation9.\nPersonalized content. By asking clarification questions, our agents enable a more customizable and personalized content creation experience. Because different groups of people may perceive helpfulness and harmfulness of contents differently, learning more about the user through clarification questions before generation can potentially mitigate risks of generating contents that are offensive to each specific user, and increase likelihoods of producing helpful outputs.\nFuture work. Alternative to the modular design, one can explore generating images directly from belief graphs and fine-tuning LLM/VLMs on text/image trajectories that include asking questions. These may require a) collecting data such as gold-standard trajectories or annotations on the quality of trajectories of human-agent conversations and b) new approaches to fine-tune the model on multi-turn trajectories of images and text, which can potentially improve the performance of the agent."}, {"title": "A. Author contributions", "content": "All authors contributed to writing.\n\u2022 Meera Hahn (meerahahn@google.com): Led automated evaluation and human evaluation of agents. Created DesignBench. Contributed to agent design and development.\n\u2022 Wenjun Zeng (wenjunzeng@google.com): Significant contribution to agent design and development, as well as open-sourcing.\n\u2022 Nithish Kannen (nitkan@google.com): Significant contribution to experiments and early versions of agents. Led open-sourcing.\n\u2022 Rich Galt (richgalt@google.com): Led agent interface design and human studies.\n\u2022 Kartikeya Badola (kbadola@google.com): Contributed to experiments and early versions of agents.\n\u2022 Been Kim (beenkim@google.com): Advised project direction with critical feedback.\n\u2022 Zi Wang (wangzi@google.com): Proposed and initiated project. Led agent design and development. Advised project. Contributed to evaluation."}, {"title": "B. Novelty and contributions", "content": "In this section, we emphasize the novelty and contributions of this work.\n1. System design of proactive T2I agents.\n\u2022 Novel human-agent interaction modalities: Prior to our work, human users typically interact with current T2I systems by giving additional instructions or refining the prompt. To the best of our knowledge, our work is the first to propose a proactive T2I agent system that is able to ask clarification questions and present its belief graph for the user to edit.\n\u2022 Novel human-agent interaction interface: We designed a new interface to best enable the clarification and belief graph interaction modalities. We have not seen these features in any T2I, or other generative media apps that are publicly live to date, signifying to us total uniqueness. Our human studies showed that at least 85% of raters found each component of the interface useful for their workflow, for us proving that these are both novel and useful.\n\u2022 Novel design of different T2I agents that enable the proposed interaction modalities. Please see \u00a74 for the full details of the design principles and construction of those T2I agent prototypes (Ag1, Ag2, Ag3).\n2. Our belief graphs significantly differs from classic belief states in the following ways:\n\u2022 Hardcoded predicates v.s. Automatically-generated predicates: Traditionally, constructing classic symbolic belief states requires a pre-defined set of predicates such as \u201con(a, b)\u201d, \"is_red(a)\", \"at_position(robot, x, y, z)\u201d and it is non-trivial to learn new predicates that can be used and generalized to new tasks (Pasula et al., 2007; Xia et al., 2019). Typically the pre-defined set of predicates are written by system developers and hardcoded into classic Al systems (Fikes and Nilsson, 1971). Our belief graphs do not require any pre-defined predicates. Instead, we propose to construct symbolic beliefs using a sequential in-context learning (ICL) method with LLMs (Algorithm 1). Our method can be generalized across a wide range of T2I tasks and achieve high performance (see our comprehensive results on Coco, Imageinwords, DesignBench).\n\u2022 Applications: To the best of our knowledge, classic symbolic beliefs are mostly used for robot planning, and we are the first to use symbolic belief graphs to assist T2I tasks.\n\u2022 Data structure: Because of the application to planning, a symbolic world state is usually implemented and stored as a set or list of literals, i.e., atoms or negation of atoms where"}, {"title": "C. Formalism of the agent and its objective", "content": "We define an interactive T2I agent as a (B, A, O, \u03c4, \u03c0) tuple, where we have\n\u2022 S: a representation space of images,\n\u2022 B: a space of agent beliefs,\n\u2022 A: a space of actions that the agent can take,\n\u2022 O: a space of agent observations of the user,\n\u2022 transition function \u03c4 : B \u00d7 A \u00d7 O \u2192 B for updating beliefs given new interactions,\n\u2022 action selection strategy \u03c0 : \u0392 \u2194 A, which specifies which action to take given a belief.\nFor each user-initiated interaction, we assume that there exists a specific intent s \u2208 S, where S is the space of all possible user intents. For a T2I task, we assume that the intent is the image the user would like to generate, and the intent stays the same throughout the interaction with an agent. We discuss more about the validity of this assumption in \u00a76.\nEach type of T2I agents can have a unique user intent representation, belief representation, construction of the action space, and user interface design to obtain observations of users.\nIn \u00a74, we show the examples for these components.\nWe use a score function, f : B \u00d7 S \u2192 R, to evaluate the alignment between an agent belief and a user intent at any turn of the interaction. Function f can only be evaluated in hindsight once the user intent is revealed. The agent does not have direct access to function f since the user intent is hidden"}, {"title": "E.1. Implementation of agent beliefs", "content": "Technically, an agent belief b \u2208 B is represented in two complementary forms: (i) Merged prompt: This is a natural language representation that summarizes the entire conversation history up to the current turn. It provides a comprehensive textual overview of the user's requests, feedback, and any clarifications exchanged with the agent. (ii) Belief graph: This is a symbolic representation derived from the merged prompt. It parses the natural language text into a structured format, capturing key elements like entities, attributes, relationships, and associated probabilities. This structured representation facilitates more precise reasoning and decision-making by the agent.\nPrompt Merging. An LLM (\u00a7E.11) summarizes the latest interaction, encapsulating the agent's question and the user's response into a concise textual representation. This step distills the essential information exchanged during the interaction. Another LLM (\u00a7E.12) merges the existing merged prompt (containing the accumulated information from previous interactions) and the summarized interaction at the current turn. This creates an updated prompt that reflects the evolving understanding of the user's intent.\nBelief Parsing. See an example of the belief graph fig. 11. We employ three specialized parsers trained via in-context learning (ICL): entity parser (\u00a7E.8) analyzes the user prompt to identify and extract a list of relevant entities.; attribute parser (\u00a7E.9) takes user prompt and an entity as the input to extract a list of attributes associated with that entity; relation parser (\u00a7E.10) takes the user prompt and a list of entities as input and identifies relationships between those entities. Each entity is associated with meta information like name, importance to ask score, description, probability of appearing, a list of attributes like color, position, etc 10. Each attribute contains meta information like name, importance to ask score, a list of possible values for the attribute along with their associated probabilities, etc. Each relation includes meta information such as: name, description, spatial relation, importance to ask score, entity 1 and entity 2, whether the relation is bidirectional, etc."}, {"title": "E.2. Implementation of action", "content": "From an information theoretic perspective, an optimal action is the one that maximizes the information gain between the observation and the belief, i.e. $a_{t} = arg\\ max_{a} H(o_{i-1}; b_{i-1} | a) \u2013 H(o_{i}; b_{i} | a)$. However, directly optimizing this objective can be computationally challenging. Therefore, we explore several heuristic strategies to effectively reduce uncertainty:\n\u2022 Maximize the overall heuristic importance score (MHIS):This strategy focuses on maximizing the overall importance score of the entities, attributes, and relations within the belief. We further ask a question regarding an attribute or relation by maximizing the overall heuristic importance score. The score can be modeled as:\n$max_{e,a,c,r} (IS(e) * IS(a) * P(e) * Ent(c), IS(r) * P(r) * Ent(c))$ (1)\nHere IS, P, Ent represents importance to ask score, probability of appearing, and entropy of the probabilities respectively and e, a, c, r represents entity, attribute, candidate list, relation respectively.\n\u2022 Ask Important Clarification Question based on belief (AICQB): This strategy leverages the structured information within the belief. We provide the LLM with the user prompt, conversation"}, {"title": "E.5. Ag1: Heuristic Score Agent", "content": "The Heuristic score agent leverages the importance scores and probabilities within the belief graph to guide its question-asking strategy. The underlying principle is to identify and inquire about the entity", "ask": "What color of the rabbit do you have in mind? a. black", "implementation": "i) Belief Representation: The agent's belief comprises the merged prompt and the current belief graph. (ii) Select Action: MHIS strategy is employed to identify the attribute or relation of interest"}]}