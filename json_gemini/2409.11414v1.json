{"title": "RTLRewriter: Methodologies for Large Models aided RTL Code Optimization", "authors": ["Xufeng Yao", "Yiwen Wang", "Xing Li", "Yingzhao Lian", "Ran Chen", "Lei Chen", "Mingxuan Yuan", "Hong Xu", "Bei Yu"], "abstract": "Register Transfer Level (RTL) code optimization is crucial for enhancing the efficiency and performance of digital circuits during early synthesis stages. Currently, optimization relies heavily on manual efforts by skilled engineers, often requiring multiple iterations based on synthesis feedback. In contrast, existing compiler-based methods fall short in addressing complex designs. This paper introduces RTLRewriter, an innovative framework that leverages large models to optimize RTL code. A circuit partition pipeline is utilized for fast synthesis and efficient rewriting. A multi-modal program analysis is proposed to incorporate vital visual diagram information as optimization cues. A specialized search engine is designed to identify useful optimization guides, algorithms, and code snippets that enhance the model's ability to generate optimized RTL. Additionally, we introduce a Cost-aware Monte Carlo Tree Search (C-MCTS) algorithm for efficient rewriting, managing diverse retrieved contents and steering the rewriting results. Furthermore, a fast verification pipeline is proposed to reduce verification cost. To cater to the needs of both industry and academia, we propose two benchmarking suites: the Large Rewriter Benchmark, targeting complex scenarios with extensive circuit partitioning, optimization trade-offs, and verification challenges, and the Small Rewriter Benchmark, designed for a wider range of scenarios and patterns. Our comparative analysis with established compilers such as Yosys and E-graph demonstrates significant improvements, highlighting the benefits of integrating large models into the early stages of circuit design. We provide our benchmarks at https://github.com/yaoxufeng/RTLRewriter-Bench.", "sections": [{"title": "1 INTRODUCTION", "content": "Optimizing Register Transfer Level (RTL) code is an essential step in the early stages of circuit design. This process involves multiple rounds of rewriting original RTL code snippets into optimized versions based on optimization patterns or synthesis feedback. Conventionally, this process relies heavily on the expertise of seasoned engineers. However, the growing complexity of design patterns has significantly hindered the efficiency of manual optimization. In comparison, existing compiler-based methods exhibit limited scope and effectiveness in optimizing complex designs, and fall short in optimizing code via synthesis feedback. Figure 1 illustrates a classic example of MUX optimization, where the revised version achieves a reduction in area by eliminating an adder. Nonetheless, certain open-source compilers, such as Yosys [1], struggle to effectively manage such scenarios.\nPrevious works on RTL code optimization mainly focus on specific scenarios such as data-path, MUX, memory [2-6]. Nevertheless,\nXufeng Yao and Yiwen Wang are equally contributed."}, {"title": "2 RELATED WORKS", "content": "The utilization of large models into electronic design automation (EDA) is an emerging field, as evidenced by recent advancements in hardware design [22\u201325], EDA script generation [26], RTL code generation and debugging [27\u201332], and other applications [33\u201337] These developments highlight the potential of large models to enhance and accelerate circuit design processes. However, the area of RTL code optimization, characterized by complex patterns and significant verification challenges, remains underexplored. This paper introduces a novel framework that leverages large models to address these issues, effectively filling this research gap.\nAdditionally, the evaluation of large models in practical industry applications presents new challenges. For instance, the Verilog generation benchmark, VerilogEval [38], uses overly simplistic cases from HDLbits. To address this, we introduce two new RTL code optimization benchmarks that include a wider range of case complexities, designed to meet the needs of both academia and industry."}, {"title": "2.1 RTL Code Optimization", "content": "RTL code optimization is a long-standing problem in VLSI that has a significant impact on final PPA (Power, Performance, Area). While previous works have made substantial efforts toward RTL optimization, thoroughly optimizing RTL code remains challenging. In real industry applications, RTL code optimization heavily relies on experienced Verilog engineers, often requiring multiple iterations of modification based on synthesis feedback. In the realm of data-path optimization, our focus primarily lies on techniques such as subexpression elimination [2, 7], Constant folding [7], Constant propagation [3, 8], Algebraic simplification [9, 10], Dead code elimination [11, 12], and Strength reduction [13]. When it comes to Mux optimization, our considerations revolve around Mux reduction [4, 6], Mux tree decomposition [5], and Mux tree restructuring [6]."}, {"title": "2.2 Large Models aided design and Challenges", "content": "In this section, we provide a comprehensive overview of the proposed RTLRewriter framework. The first step in our approach is partitioning the RTL design, such as a CPU, into smaller, manageable parts, as detailed in Section 3.1. For each sub-circuit, we introduce a novel RTL rewriting framework aided by large models. This framework integrates multi-modal program analysis, a search engine, and a cost-aware Monte Carlo Tree Search algorithm, as outlined in Section 3.2. To mitigate the inherent randomness of LLM generation and reduce verification costs, we implement a fast verification pipeline that filters out ineffective rewrites using automated test case generation and efficient program analysis-based verification, as described in Section 3.3."}, {"title": "3 METHODOLOGIES", "content": "The challenge of comprehending long contexts remains an unsolved problem in transformer-based large models [39] due to their quadratic complexity, and we have observed this phenomenon in RTL code optimization as well. Specifically, we have observed that the effects of rewriting are significantly improved when applied to partitioned sub-circuits, as opposed to the entire circuit. Furthermore, partitioning a large circuit is a common practice in the industry due to the high synthesis time cost. It involves dividing a large circuit into smaller parts that can be synthesized in parallel.\nAn effective circuit partitioning approach should aim to achieve an optimal total synthesis time while minimizing the loss of performance. Figure 2 shows the circuit partition pipeline. The RTL code"}, {"title": "3.1 Circuit Partition", "content": "passes an initial transformation into an abstract syntax tree (ast) using a parser, then transformed to an instance tree, from which one node represents a Verilog module. Subsequently, a circuit predictor is employed to estimate the synthesis time for each node. Based on these predictions, the circuit partitioner is designed to divide the circuit in a manner that balances the total synthesis workload while minimizing the loss of performance, and a scheduling algorithm is employed to efficiently allocate synthesis time slots to the sub-circuits, ensuring optimal resource utilization and minimizing overall synthesis time.\nTo evaluate the synthesis time of each module, we propose to establish a circuit predictor to predict the related synthesis time of each node. For RTL code, arithmetic and control operations such as binary and unary operators, conditionals, and element-selects with varying bit-widths (e.g., Add, Equality, ArithmeticShiftLeft, ElementSelect) are heavily utilized for word-level and bit-level synthesis. After building and simplifying the Abstract Syntax Tree (ast), the tree is traversed, and the bit-widths of ast nodes are compiled into a high-dimensional feature vector for subsequent prediction.\nWe train our circuit predictor in an offline manner where real industry-level data is leveraged. We adopt XGBoost [40] as the predictor and train the model to predict the node weights. Edge weight prediction involves estimating the performance, power, and area (PPA) effects using edge weights. These weights are calculated based on the type of connection between instances in a Verilog module, classified as direct, combinational, or sequential. This process supports workload prediction and chip partitioning.\nTo optimize parallel synthesis efficiency, we focus on partitioning the instance tree to balance synthesis time and PPA, which can be formulated as:\nmin C = L + \\lambda E\ns.t. N_{min} \\leq |S| \\leq N_{max},\nwhere C is the total cost, L represents overall synthesis time and E denotes the cost of edgecut. The edgecut is closely related to the granularity of the RTL code, and we generally prefer smaller granularity. S is the set of edgecuts, where the cardinality of S is equal to number of partitions. N_{min} and N_{max} represent the max and min number of partitions. By solving Equation (1), we aim to achieve balanced partitions with minimal synthesis time and optimal edgecut.\nTo address the problem, we propose a hierarchical tree partitioning algorithm. It starts by partitioning the instance tree in a top-down manner, where we partition the root node and associated sub-trees. Then, we evaluate the cost C using a bin-packing algorithm with a first-fit strategy. Subsequently, We iteratively partition the sub-tree with the largest weight and evaluate the cost C. The partitioning process continues until it meets the maximum partition"}, {"title": "3.2 Large Models aided RTL Rewriting", "content": "Figure 3 illustrates the whole large models aided RTL rewriting pipeline. We first introduce a multi-modal program analysis pipeline to extract optimization and verification patterns. Then the optimization pattern, along with RTL code and diagram are regarded as queries to a search engine. The retrieved contents from self-established database will act as optimization guides, thereby enhancing the capability of large models to generate improved rewriting code. To better utilize various sources of retrieved contents, we also introduce the Cost-aware Monte Carlo Tree Seach algorithm to efficiently direct optimization path.\nProgram analysis serves as a crucial role in circuit design. However, current compiler-based approaches fall short in extracting semantic information from raw code. For example, Visual information is an important clue in optimizing RTL code, Figure 4 shows a motivated visual example from which it's easy to recognize there existing optimization area for reducing critical path.\nInspired by chain-of-thought (COT) [41], we propose a large multi-modal model (LMM) COT program analysis pipeline. The pipeline first prompts large models to provide an in-depth analysis of diagrams based on the comprehension of given RTL code. The insights of proposed chain of analysis stem from the observation that large models can provide more accurate analysis of diagram when equipped with related RTL code, and combing diagram and RTL code can provide more informative and targeted guidance. Defining large models as \\pi_{\\theta}, visual diagram as x_v, RTL code as x_c, initial prompt as p_{init}, and the optimization and verification patterns as p_{opt} and p_{ver}, respectively, the process can be formulated as follows:\np_{chain} = \\pi_{\\theta} (x_v, x_c, p_{init}),\np_{opt}, p_{ver} = \\pi_{\\rho}(p_{chain}, x_v, x_c, p_{out}),\nwhere p_{chain} represents the chain of thoughts output, and p_{output} denotes the final output prompt.\nWhile program analysis can be applied to various scenarios in the RTL optimization process, this research primarily focuses on utilizing program analysis for optimization and verification pattern recognition. Given an RTL code and its associated diagram, we aim to leverage program analysis to identify relevant optimization patterns, such as data-path categorization and sub-expression elimination directions. Regarding verification, the large models aim to"}, {"title": "Search Engine.", "content": "Retrieval augmented generation (RAG) [42] effectively enhances the generation capabilities of large models. To facilitate this, we establish a RTL database that stores relevant diagrams, codes, optimization instructions, and algorithms. The details of database creation require a considerable amount of manual effort and are omitted due to page limits.\nTable 1 presents the various retrieval types, their representations, and query methods employed in our approach. For diagrams, we store the embeddings generated by a large vision model. As visual inputs are not directly suitable for providing optimization instructions, we consider them as bridges connecting related optimization instructions and codes. Consequently, we employ a join query approach [43], where the visual input is first searched, and then its mapped optimization instructions and codes are retrieved.\nFor codes and optimization instructions, we primarily utilize the embeddings generated by language models. Additionally, we incorporate the traditional TF-IDF [44] representation for code search to enhance retrieval effectiveness. When it comes to algorithms, we adopt a join query method. In this approach, optimization instructions or codes are initially searched and then serve as a bridge to retrieve related algorithms.\nFor diagrams and optimization instructions, we use the widely adopted cosine distance as the similarity metric and retrieve the top-k most associated instances. For codes, we design a two-stage ranking approach to identify the top-k most relevant code instances for a given RTL code query. In the first stage, for any query code x_d, we generate keyword and semantic vectors, \\overrightarrow{v} and \\overrightarrow{v'}, using the TF-IDF encoder and LLM, respectively. The similarity between"}, {"title": "C-MCTS.", "content": "Although search engines can retrieve a wealth of contents, including code, optimization instructions, and algorithms, we have observed that supplying large models with all the search results as context does not consistently yield satisfactory outcomes, and can sometimes even degrade performance. Table 2 shows different RAG context that proved to be effective in optimizing corresponding cases. Specifically, certain contexts, such as algorithms, are crucial for optimizing scenarios like finite state machines (FSM), where state reduction heavily depends on the optimization analysis of state transition tables."}, {"title": "3.3 Fast Verification", "content": "Verification is a crucial part in circuit design, ensuring the correctness of the implemented functionality. In our framework, verification becomes even more crucial due to the inherent randomness generation by large models. In the realm of RTL optimization frameworks, a primary focus lies in equivalence checking [46]. This process aims to validate the functional equivalence between the original Verilog code and its rewritten version. Modern open-source compilers mainly use SAT-based approaches [47], which translate the equivalence problem into a SAT problem. If the SAT finds the problem unsatisfiable, the RTL codes are not equivalent. However, the worst-case computational complexity of SAT is NP-complete, which means the time required to solve a problem can grow exponentially with the size of problem. For example, as shown in Figure 7, this arithmetic circuits lead to unexpected verification times due to the complexity of the Boolean representations of such operations. In contrast, this kind of arithmetic circuit can be efficiently solved by other approaches such as symbolic algebraic methods [48]. These approaches leverage algebraic techniques and tools to simplify and compare these arithmetic expressions.\nDue to the randomness of large model-generated rewritten codes, conducting equivalence checking for all cases is costly. To address this, we leverage fuzz testing [49], which generates random test cases to simulate Verilog codes. By comparing the outputs of the original and rewritten code, we can identify differences and filter out problematic cases, reducing the need for extensive verification. This approach helps optimize the RTL optimization framework by avoiding the burden of exhaustive verification.\nFigure 6 illustrates our fast verification pipeline. After generating multiple rewritten codes via large models, we employ open-source tools to randomly generate test cases. We then filter out those rewritten cases whose outputs differ from the original. Subsequently, we utilize the verification pattern generated by program analysis as detailed in Figure 4 to determine the Verilog code is combinational or sequential and choose the appropriate solver."}, {"title": "4 EXPERIMENTS", "content": "In this study, we introduce two benchmarks for RTL code optimization, designed to address both long and short code scenarios, crafted by experienced Verilog engineers. These benchmarks derive their optimization patterns from a comprehensive review of internal industry documents and a survey of approximately 50 scholarly articles on RTL optimization."}, {"title": "4.1 Experimental Settings", "content": "xd and each instance I_i \\in D_e in the code database D_e is defined as:\nsim(x_d, I_i) = \\lambda \\cdot cosine(\\overrightarrow{v}, \\overrightarrow{v}_i) + (1 - \\lambda) \\cdot cosine(\\overrightarrow{v'}, \\overrightarrow{v'}_i) + 1,\nwhere \\lambda \\in [0, 1] is a hyper-parameter between semantic similarity and keyword similarity and sim(x_q, I_i) \\in [0,2]. We select the top-N similar instances D_{ed} from the code database D_e based on the similarity score. In the second-ranking stage, our goal is to pinpoint the top-k relevant yet diverse code instances. These selections are aimed at providing a broader range of code patterns, which can help large models offer better optimizations. Formally, given N code instances D_{ed} and the query RTL code x_d, we select the top-k relevant and diverse code D_{ad} by maximizing the following objective:\n\\max_{D_{ad}} \\frac{1}{k} \\sum_{I_i \\in D_{ad}} sim(x_d, I_i) + \\sum_{I_i \\in D_{ed}} dis(I_i, D_{ad}),\nwhere distance dis(I_i, D_{ad}) = \\min_{I_j \\in D_{xd} \\backslash I_i} (2 \u2013 sim(I_i, I_j)) denotes the diversity value between each instance I_i and the other instances in D_{ad} and dis (I_i, D_{ad}) \\in [0,2]."}, {"title": "4.2 Performance Analysis", "content": "Table 4 illustrates the results of short RTL rewriting benchmarks across 16 cases, encompassing both baseline results and human implementations, validated by senior Verilog engineers. Wires and cells are two critical evaluation metrics in front-end RTL optimization due to their strong correlation with area and delay. In real-world applications, optimizing these metrics, even by 1%, represents significant progress.\nWe adopt Yosys results as our baseline, where fewer wires and cells indicate better results. The results with gray color mean that the method fails to optimize the code. Notably, our method outperforms all competitive baselines by a substantial margin. Among"}, {"title": "4.3 Ablation Studies", "content": "In this subsection we provide more ablation studies on each part within our framework. Table 6 presents the ablation studies on circuit partition. We observe that without partitioning, the synthesis time is ten times longer than with partitioning, owing to the parallel synthesis strategy. Additionally, the PPA loss is almost negligible compared to the w/o partition approach. More importantly, when testing on the long-benchmark, we found that it is nearly impossible for large models such as GPT4 or Claude3-Opus to optimize the entire code correctly, highlighting the importance and effectiveness of the partitioning strategy.\nMulti-modal program analysis is a crucial component of our framework, where integrating other modalities, such as visual information, is naturally aligned with RTL code analysis. Tables 7 and 8 present a comparison between GPT-4 and our method, on small benchmark. It is evident that our method achieves better pattern-finding performance than GPT-4 on both RTL optimization pattern finding and verification pattern finding.\nTable 9 shows the effectiveness of our search method compared to other baselines, including traditional methods such as TF-IDF [44], BM25 [57], and machine learning methods such as"}, {"title": "5 CONCLUSION", "content": "Traditional RTL code optimization relies heavily on the expertise of skilled engineers, often requiring multiple iterations based on synthesis feedback. In this paper, we present an automatic RTL code optimization framework leveraging large models. We introduce several key components to address the challenges, including a circuit partition pipeline, large model-aided RTL optimization encompassing multi-modal program analysis, a search engine, and a cost-aware search algorithm for efficient rewriting. Additionally, we have developed a fast verification pipeline to streamline the verification process and reduce costs. Moreover, we have created two datasets to foster further advancements in RTL code optimization. We hope our work can stimulate innovation in RTL code optimization."}]}