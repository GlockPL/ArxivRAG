{"title": "A Unified Platform for At-Home Post-Stroke Rehabilitation Enabled by Wearable Technologies and Artificial Intelligence", "authors": ["Chenyu Tang", "Ruizhi Zhang", "Shuo Gao", "Zihe Zhao", "Zibo Zhang", "Jiaqi Wang", "Cong Li", "Junliang Chen", "Yanning Dai", "Shengbo Wang", "Ruoyu Juan", "Qiaoying Li", "Ruimou Xie", "Xuhang Chen", "Xinkai Zhou", "Yunjia Xia", "Jianan Chen", "Fanghao Lu", "Xin Li", "Ninglli Wang", "Peter Smielewski", "Yu Pan", "Hubin Zhao", "Luigi G. Occhipinti"], "abstract": "At-home rehabilitation for post-stroke patients presents significant challenges, as continuous, personalized care is often limited outside clinical settings. Additionally, the absence of comprehensive solutions addressing diverse rehabilitation needs in home environments complicates recovery efforts. Here, we introduce a smart home platform that integrates wearable sensors, ambient monitoring, and large language model (LLM)-powered assistance to provide seamless health monitoring and intelligent support. The system leverages machine learning enabled plantar pressure arrays for motor recovery assessment (94% classification accuracy), a wearable eye-tracking module for cognitive evaluation, and ambient sensors for precise smart home control (100% operational success, <1 s latency). Additionally, the LLM-powered agent, Auto-Care, offers real-time interventions, such as health reminders and environmental adjustments, enhancing user satisfaction by 29%. This work establishes a fully integrated platform for long-term, personalized rehabilitation, offering new possibilities for managing chronic conditions and supporting aging populations.", "sections": [{"title": "I. Main", "content": "Stroke is the third leading cause of disability worldwide, affecting more than 101 million people [1, 2].\nSurvivors often experience motor impairments (60\u201380%), cognitive deficits (20\u201330%), and speech\ndifficulties (30\u201350%), which significantly compromise their independence and quality of life [3, 4].\nPost-stroke recovery is not only a prolonged process but also a resource-intensive one, imposing\nsignificant economic and caregiving burdens on families and healthcare systems a challenge\nexacerbated by global aging [5]. For many patients, the home becomes a critical environment for\nrehabilitation, as opportunities for continuous and personalized care are limited outside of clinical settings\n[6]. An ideal solution would resemble the intelligent, adaptive environments often envisioned in science\nfiction: a system that uses multi-modal, multi-dimensional sensing to continuously monitor patients'\nphysical and physiological states, while leveraging artificial intelligence to provide real-time health\nassessments and assistance [7, 8].\n\nRecent advancements in wearable sensors and artificial intelligence (AI) technologies have brought this\nvision within reach [9, 10, 11, 12]. Combined with AI methods, wearable devices such as force sensors,\naccelerometers, EMG sensors, and eye trackers have enabled tracking of motor recovery and provided\nvaluable insights into cognitive function [13-23]. Assistive tools, including robotic aids and smart home\ndevices, have been developed to address specific daily needs [24, 25, 26]. However, the comprehensive\nsolution envisioned is still absent. To unlock the full potential of at-home rehabilitation, we step further to\na unified, patient-centered system capable of navigating the complexities of real-world scenarios.\n\nHere, we report a smart home system specifically designed for long-term, at-home rehabilitation of\npost-stroke patients, integrating health monitoring and assistive functionalities into a single platform (Fig.\n1). By leveraging multi-sensor fusion, the system comprehensively addresses the needs of patients with\npost-stroke impairments. For rehabilitation monitoring, our plantar pressure array, coupled with a\nmachine learning model, evaluates motor recovery, achieving a classification accuracy of 94.1% across\nthree rehabilitation states. A wearable eye-tracking module extracts key indicators of cognitive function,\nwhile ambient sensors such as cameras and microphones, in collaboration with the eye-tracking module,\nenable seamless and precise smart home control (100% operational success rate with a latency of <1 s).\nThis multi-sensor collaborative design ensures accessibility for a diverse range of users, allowing them to\nchoose the most suitable interaction modality based on their specific needs. Additionally, we introduce an\nautonomous assistive agent, Auto-Care, powered by a large language model (LLM), which analyzes\nmulti-modal data to provide timely interventions such as health reminders, environmental adjustments, or\ncaregiver notifications, increasing user satisfaction by 29% compared to scenarios without the agent. This\nsystem represents the first fully integrated solution for simultaneous health monitoring and intelligent\nassistance in post-stroke home rehabilitation, offering a pathway toward comprehensive, patient-centered\nmanagement. Furthermore, it holds potential for broader applications in other chronic conditions, such as\namyotrophic lateral sclerosis (ALS) and Parkinson's disease, and aging populations."}, {"title": "II. Results", "content": "As shown in Fig. 1a, the platform integrates wearable devices, including plantar pressure insoles (SFig.\n1-4), wristband module (SFig. 5, 6), and eye-tracking module (SFig. 7), alongside ambient sensors such"}, {"title": "The multimodal rehabilitation system", "content": "as cameras and microphones, to enable comprehensive, round-the-clock monitoring of patients [27, 28,\n29]. This multimodal sensing system collects a full spectrum of patient information, providing intelligent\nassessments of rehabilitation progress and offering daily assistance to support independent living. The\ncore concept of the platform is realized through the IoT architecture depicted in Fig. 1b (SFig. 11), which\nconsolidates data from all sensing modalities into a local host server at home. Wearable devices and\nambient sensors transmit data locally via Bluetooth Low Energy (BLE) and WiFi protocols, ensuring\nseamless communication, minimal latency, protection of users' data. The host server processes the\naggregated data in real time, transforming it into actionable outputs for health monitoring and assistive\ndecision-making.\n\nTo evaluate the platform's capability in tracking motor recovery, 20 post-stroke patients with varying\ndegrees of motor impairments and diverse post-stroke complications including hemiplegia, knee valgus,\nand foot inversion were recruited. Patients were stratified based on their motor function scores obtained\nby using the Fugl-Meyer Assessment (FMA) scale [30] into three rehabilitation levels: mild, moderate,\nand severe. Plantar pressure data from a 48-channel sensor matrix were recorded during routine walking\ntasks, capturing detailed gait dynamics across different recovery stages. As shown in Fig. 2a, patients in\nthe mild rehabilitation level exhibited gait signals resembling those of healthy individuals, characterized\nby consistent amplitude and symmetry, indicating effective weight distribution and propulsion [29, 31]. In\ncontrast, patients in the moderate rehabilitation level showed irregular oscillations and reduced symmetry,\nreflecting instability during gait phases. Those in the severe rehabilitation level, such as individuals with\nleft-sided hemiplegia, exhibited diminished signals on the affected side and exaggerated signals on the\nunaffected side, indicative of compensatory mechanisms [32]. These distinctive signal patterns across\nrecovery states underpin the machine learning model's ability to objectively decode and monitor\nrehabilitation progress.\n\nFig. 2b shows the comparison of eye-tracking patterns between subjects with and without cognitive\nimpairments during interactions with a smart light. Among the 20 participants, 4 were identified by\nclinicians as exhibiting cognitive impairments based on professional evaluations of their behavior and\nneurological assessments. While the limited sample size of cognitive impairment cases does not allow for\nthe development of a statistically robust tracking model, the collected data still provide valuable insights\nfor clinicians via the IoT system. For subjects without cognitive impairment, gaze trajectories during\ninteraction were precise and efficient, with fixations rapidly converging on the target smart light, as\nreflected by the compact heatmap. In contrast, the subject with cognitive impairment demonstrated\ndispersed and irregular gaze patterns, with frequent distractions and prolonged fixations on irrelevant\nobjects before locating the target. The corresponding heatmap exhibits a broad, scattered distribution,\nconsistent with delayed visual attention and impaired decision-making, hallmark traits often associated\nwith cognitive deficits. Additionally, the platform recorded other statistical metrics, such as blink\nfrequency and duration patterns, providing further indicators of cognitive function (SFig. 8, 9). These data,\naggregated through the IoT system, can be securely shared with clinicians to analyze, offering valuable\nsupplementary information for understanding the patient's cognitive condition and guiding personalized\ninterventions."}, {"title": "Motor rehabilitation states monitoring via plantar pressure insoles and deep learning", "content": "To accurately track motor impairment recovery stages, we collected walking data from the subjects and\nsegmented it into 5-second samples to construct the dataset. Among the 20 participants, 6 were annotated"}, {"title": "Smart home control based on multi-sensor fusion", "content": "as being in the severe, 7 in the moderate, and 7 in the mild rehabilitation level. Fig. 3a and Fig. 3b\nvisualize key statistical characteristics of the gait data. Specifically, Fig. 3a displays the coefficient of\nvariation (CV) of plantar pressure, which quantifies the variability in foot pressure relative to the mean\npressure, with higher CV values reflecting greater instability in walking. Patients in the severe stage\nexhibit significantly higher CV compared to those in the moderate and mild stages, indicating less\nconsistent gait dynamics. Fig. 3b highlights the asymmetry in pressure distribution and stance phase ratio\nbetween the left and right feet, with more severe cases showing pronounced imbalances. This reflects the\nunderlying biomechanical challenges, where motor impairments disrupt load distribution and timing\nbetween the two legs.\n\nFig. 3c outlines our deep learning pipeline for decoding rehabilitation states. The 48-channel plantar\npressure signals from each foot are converted into 224 \u00d7 224 two-dimensional heatmaps, which are then\nfed into a convolutional neural network (CNN) to encode spatiotemporal gait features. This 2D\ntransformation allows the model to capture the spatial relationships between channels while preserving\ntemporal dynamics, a critical aspect for distinguishing gait patterns across recovery stages. The encoded\nfeatures from both feet are subsequently processed by a multi-layer perceptron (MLP) classifier to decode\nthe patient's rehabilitation status. Performance comparisons of various baseline models as gait feature\nencoders are shown in Fig. 3d, where ResNet-101 outperformed alternatives with the highest accuracy of\n94.1%, supporting its selection as the optimal encoder (optimal hyperparameters detailed in STable 1).\nThe confusion matrix in Fig. 3e demonstrates robust classification across all rehabilitation states, with\nminimal misclassification errors. Furthermore, the encoder's output feature representations, visualized\nusing UMAP in Fig. 3f, reveal clear clustering of the three rehabilitation states, underscoring the model's\nability to differentiate between mild, moderate, and severe motor impairment stages effectively. This\npipeline provides a robust, end-to-end, and data-driven approach to monitor recovery progress in\npost-stroke patients.\n\nTo address the challenges of monitoring stroke patients in home settings, we developed a real-time scene\ndetection and human action recognition system based on video streams (Fig. 4a). This system employs a\nlightweight neural network architecture to achieve efficient local computation while maintaining patient\nprivacy, balancing accuracy and computational efficiency. Scene images are extracted from video streams\ncaptured by home cameras and processed using a fine-tuned YOLOv8n model [33]. The model identifies\nthe patient and household objects with corresponding confidence scores. Objects with scores exceeding a\npredefined threshold are retained, and their spatial relationships are analyzed to infer the current home\nenvironment. For human action recognition, patient images are processed using the open-source\nMediaPipe model to extract pose landmarks, which are then converted into normalized 3D coordinates.\nThese coordinates serve as input for an MLP action classification model, enabling the detection of typical\nhuman actions with an inference latency of less than 50 ms. On a self-collected test dataset, the model\nachieved an accuracy of 99.3% (Fig. 4b, SFig. 10). By combining scene and action data, the system\nprovides real-time video feedback to a host device while storing data for retrospective analysis,\nfacilitating long-term rehabilitation monitoring.\n\nFor patients with speech impairments, the system extends its functionality to support multimodal\ninteraction by integrating audio and eye-tracking data. Patients with partial speech capability can issue\nvoice commands through a microphone, while those with severe speech impairments can rely on gaze"}, {"title": "LLM agent for autonomous rehabilitation management", "content": "direction and blinking patterns captured by the eye tracker (Fig. 4c). These signals are processed to\ngenerate control commands for smart home devices. This approach provides a seamless and intuitive\ninterface for patients with diverse disabilities, fostering independence and improving their quality of life\nduring at-home rehabilitation. The IoT-based architecture ensures reliable communication with smart\ndevices, enabling adaptive control of the home environment to meet individual needs.\n\nTo overcome the limitations of patients interacting with the platform solely based on subjective needs, we\nembedded an autonomous health management agent, Auto-Care, powered by GPT-4o Mini API. This\nagent operates continuously, analyzing multimodal data streams 24/7 to intelligently detect and address\nvarious patient needs. For example, as shown in Fig. 5a, during gait training (point 1), the agent detected\nrising heart rate and temperature coupled with decreasing heart rate variability (HRV), prompting it to\nrecommend hydration, pause training, and activate air conditioning to maintain comfort. At point 2, when\na fall was detected, the agent used a microphone to check the patient's status and, upon confirming the\nneed for assistance or receiving no response, alerted a caregiver. At point 3, as ambient light levels\ndecreased, the agent adjusted the smart lighting in the dining room based on the patient's location to\nensure adequate illumination.\n\nThe effectiveness of Auto-Care was further enhanced through prompt optimization, as shown in Fig. 5b.\nAdding Chain-of-Thought reasoning and pre-defined intervention demos to the prompts significantly\nimproved user satisfaction [34]. Multimodal data were downsampled to 1-minute intervals before being\ninput into the agent, ensuring computational efficiency without compromising real-time responsiveness.\nFig. 5c shows that a six-minute data context provided the optimal balance for decision-making accuracy\nand computational efficiency. Overall, as demonstrated in Fig. 5d, the integration of the Auto-Care Agent\ninto the platform significantly enhanced patient outcomes across multiple dimensions, including reduced\npsychological burden, improved operational efficiency, and increased overall satisfaction (evaluation\ncriteria detailed in STable 2). Compared to scenarios where the platform was not used, users reported a\n67% improvement in overall satisfaction, with an additional 29% increase upon the integration of the\nAuto-Care Agent. These results highlight the potential of the platform and agent to transform at-home\nrehabilitation, providing continuous, intelligent support tailored to individual patient needs."}, {"title": "III. Discussion", "content": "In this work, we present a smart home system specifically designed to address the multifaceted\nrehabilitation needs of post-stroke patients in a home environment. The system's integrated platform\ncombines advanced health monitoring and assistive functionalities, demonstrating exceptional\nperformance in real-world scenarios. Comprehensive evaluations confirm its efficacy: the plantar pressure\narray and machine learning model achieved 94.1% accuracy in classifying three rehabilitation states,\nwhile the wearable eye-tracking module and ambient sensors enabled seamless smart home control with a\n100% success rate and latency of <1 s. The introduction of Auto-Care, an LLM-powered autonomous\nagent, further enhances the system's functionality, improving user satisfaction by 29% through timely and\nintelligent interventions. These features collectively position the system as a groundbreaking solution for\npost-stroke rehabilitation, offering continuous, personalized care and assistance.\n\nFuture directions will focus on advancing the system's adaptability, scalability, and inclusivity. First,"}, {"title": "IV. Methods", "content": "expanding its application to other chronic conditions and diverse user populations, including aging\nindividuals and those with neurodegenerative diseases, will enhance its broader relevance. Second,\nimproving the system's robustness and interoperability with external assistive devices, such as robotic\naids and exoskeletons, will further enrich its capabilities. Lastly, optimizing its computational efficiency\nthrough edge computing will reduce power consumption and latency, enabling seamless operation and\nensuring privacy in at-home environments.\n\nLooking ahead, the system has the potential to transform long-term post-stroke care by improving both\nphysical and psychological well-being. Continuous, personalized monitoring and intelligent assistance\nempower patients to regain independence, enhance social interaction, and improve quality of life.\nAdditionally, the system's ability to collect and analyze long-term, multi-modal data opens new\npossibilities for predicting stroke progression, recovery trajectories, and even the risk of secondary\nstrokes. By identifying subtle patterns in motor performance, cognitive behaviors, physiological signals,\nand environmental factors, the system could enable proactive, personalized interventions to mitigate\nfuture risks and optimize recovery strategies. These capabilities position the system not only as a\nrehabilitation tool but as a predictive and comprehensive health management platform for post-stroke\ncare."}, {"title": "Fabrication of the plantar pressure insole", "content": "To detect plantar pressure, we developed a custom insole equipped with a 4 \u00d7 12 resistive pressure sensor\narray, comprising 48 sensing points with an average sensor density of 0.23 sensors/cm\u00b2. The structural\ndetails and sensor dimensions are reported in the figure. The topmost layer of the insole consists of a\npolyethylene terephthalate (PET) protective film, beneath which lies a layer of copper row and column\nelectrodes etched onto a polyimide (PI) substrate. An FSR (force-sensitive resistor) graphite layer is\nplaced between the electrode layers to form the resistive sensing elements. This flexible design ensures\nthat the insole can withstand frequent bending during walking without losing functionality. At just 100\n\u00b5m thick, the insole provides a comfortable wearing experience without causing discomfort.\n\nTo process the pressure data, we designed a custom resistive array detection circuit with the HC32F460\nmicrocontroller, based on the ARM Cortex-M4 architecture operating at 200 MHz. This MCU offers\nrobust computational capability for processing large volumes of pressure data. A low-dropout regulator\n(LDO) reduces the input voltage from 5.6 V to 5 V, ensuring a stable power supply to the ADC for\nprecise signal conversion. The circuit also includes an integrated TP4054 lithium battery charging chip,\nenabling recharging via a Micro-USB interface.\n\nFor high-speed data communication, all modules utilize fast GPIO and protocols such as SPI and I2C. T\u03bf\nenable wireless data transmission, the circuit integrates a CH9141 Bluetooth module that communicates\nwith the MCU via the USART protocol. This efficient and flexible design supports real-time plantar\npressure monitoring, with data transmission capabilities that are robust and optimized for rehabilitation\nscenarios."}, {"title": "Fabrication of the wristband", "content": "A custom-designed wireless wristband was developed to enable efficient, continuous acquisition and"}, {"title": "Fabrication of the wearable eye tracker", "content": "transmission of physiological and environmental data within the IoT system, ensuring seamless\nintegration and modularity for the specific multimodal sensing needs of the platform. Unlike commercial\nsolutions, the custom design ensures full compatibility with the IoT architecture and allows precise\ncustomization of sensing modalities to guarantee integrated functionality. The wristband integrates six\nfunctional modules: an STM32L412 microcontroller for system control and coordination of the overall\nsensing and data management functions, a CH9141 BLE module for bidirectional data and instruction\ntransmission, a power management module, and three sensing modules\u2014MAX30101 for PPG signal\nacquisition, AS7341 for environmental light detection, and MTS4B for temperature measurement.\n\nEach sensing module operates independently, collecting data at predefined sampling rates and temporarily\nstoring raw signals in internal registers. The STM32L412 microcontroller controls system functionality,\npolling each module at regular intervals via the I2C protocol and transmitting aggregated data to the host\ncomputer via the CH9141 Bluetooth module. The wristband is powered by a 4.2V rechargeable lithium\nbattery, charged through a TP4054 linear charger and protected against overcharging, over-discharging,\nand overcurrent using a DW01 chip. Real-time battery status is monitored by a BQ27220 coulometer,\nwhile voltage regulation is managed by XC6206P332MR and XC6206P182MR LDOs (3.3V and 1.8V,\nrespectively) and a ME2188A50XG linear regulator delivering 5V for the MAX30101's integrated LED.\n\nTo ensure reliability and user comfort during daily wear, the wristband features a compact six-layer PCB\nwith double-sided component mounting, measuring 40 \u00d7 30 \u00d7 1.6 mm. This design achieves a balance\nbetween unobtrusiveness and robust functionality, facilitating comfortable long-term use while\nmaintaining consistent performance.\n\nA custom head-mounted wireless eye tracker was developed for real-time gaze tracking and\nenvironmental scene analysis, tailored for applications in rehabilitation and smart home interaction. The\nsystem integrates two near-infrared (IR) cameras, each equipped with four edge-mounted IR LEDs, for\nprecise pupil and corner-of-eye detection, and a forward-facing visible-light camera for capturing the\nwearer's environmental context. All cameras utilize the IMX258 image sensor with an 80-degree\nfixed-focus lens, providing high-resolution (12 MP) imagery at 30 FPS.\n\nCentralized processing is performed by an OrangePi CM5 module, which incorporates the RK3588S SoC\n(quad-core Cortex-A76 at 2.4 GHz and quad-core Cortex-A55), ensuring efficient real-time data handling.\nThe module is powered by an RK806-1 power management IC, supporting stable operations for\ncomputationally intensive tasks. Wireless data transmission is facilitated by a CDW-20U5622 WiFi\nmodule, enabling seamless integration with the IoT system. The device is powered by a compact 5V 4A\nlithium battery, ensuring portability and extended use.\n\nKey eye features, such as pupil center and eye corner coordinates, are extracted in real time by the\nprocessing unit, which computes gaze coordinates using calibration data. The visible-light camera data is\nsynchronized with gaze coordinates, providing a robust mechanism for environmental interaction. The\nsystem achieves high accuracy and reliability in dynamic scenarios, supporting diverse applications such\nas rehabilitation monitoring, assistive device control, and cognitive assessments."}, {"title": "IoT Framework for Multimodal Data Integration", "content": "To enable seamless integration and processing of multimodal data in the home environment, we designed\na hierarchical IoT architecture comprising the following layers (SFig. 11):\n\nSensor Layer: This layer includes all data collection devices responsible for sensing user states and\nenvironmental conditions. Key components are wearable eye-tracking devices, wristbands, plantar\npressure insoles, and Hikvision DS-2SC2Q133MW cameras (integrated with microphones and speakers).\n\nData Transmission Layer: Communication across devices utilizes three main protocols: BLE, HTTP,\nand MiIO. The wristbands and insoles communicate with the gateway using BLE 4.2 via Bluetooth\nmodules. The eye-tracking devices and cameras connect to the gateway over a WiFi network using the\nHTTP protocol. Smart home devices, such as lights and air conditioners, use the MiIO protocol over WiFi\nfor communication.\n\nData Processing Layer: A custom gateway software is deployed on a host device equipped with both\nWiFi and BLE modules. This software aggregates data streams from all sensors, processes multimodal\ndata fusion, and distributes control commands to connected devices. The host device handles real-time\ndata synchronization and processing to ensure consistent and actionable outputs across modalities.\n\nEndpoint Layer: This layer consists of smart home devices such as smart TVs, air conditioners, and\ntable lamps, which are controlled via the MiIO protocol.\n\nTime Synchronization Server: A local network time protocol (NTP) server is set up on the host device\nto ensure precise time synchronization for all collected data. During data processing at the gateway,\ntimestamps generated from the NTP-synchronized server are embedded in the frame headers to maintain\ntemporal alignment across all modalities.\n\nThis framework ensures efficient, synchronized communication and integration of data from diverse\nsensing devices, enabling robust multimodal monitoring and interaction within the smart home\nrehabilitation system."}, {"title": "Plantar pressure data acquisition", "content": "We conducted a motor impairment study involving 20 stroke patients (mean age: 51.4 \u00b1 9.8 years; 14\nmales, 6 females) who were recruited in compliance with the Ethics Committee approval by the\nCommittee for Medical Research Ethics at the First Hospital of Shijiazhuang City, China (assigned\nproject number of 2020036). All participants provided written informed consent prior to enrollment.\nPatients were instructed to walk naturally on a flat surface while wearing plantar pressure insoles under\nthe supervision of medical professionals. Continuous plantar pressure signals were recorded during the\nwalking sessions. Following the data collection, the patients' motor recovery status was assessed using\nthe Fugl-Meyer Assessment (FMA) scale, a clinically validated tool for evaluating motor impairment\nrecovery. Based on their FMA scores, patients were categorized into three levels of motor impairment\nrecovery: mild (FMA score \u2265 85), moderate (FMA score 50\u201384), and severe (FMA score < 50).\n\nThe plantar pressure insoles captured data at a sampling frequency of 200 Hz. For analysis, continuous\nsignals from both feet were segmented into five-second intervals, with each interval constituting a single\nsample. A total of 1,543 gait samples were collected across the 20 participants (80% were selected as the\ntraining set, while 20% formed the test set), providing a robust dataset for subsequent analysis of motor"}, {"title": "Software environment for motor impairment monitoring model training", "content": "recovery patterns.\n\nSignal preprocessing was performed on a MacBook Pro equipped with an M1 Max CPU. Network\ntraining was conducted using Python 3.8.13, Miniconda 3, and PyTorch 2.0.1 in a performance-optimized\nenvironment. Training acceleration was enabled by CUDA on NVIDIA 4090 GPU."}, {"title": "The smart home control system", "content": "The smart home control system combines lightweight neural networks and wearable eye-tracking\ntechnology to achieve efficient, privacy-preserving, real-time interaction in home environments. For\nscene detection and action recognition, a fine-tuned YOLOv8n model was employed to analyze video\nstreams from a Hikvision DS-2SC2Q133MW camera. This lightweight model, with 3.2M parameters,\nachieves near 100% accuracy for detecting human actions (e.g., walking, sitting, standing, and falling)\nand household objects (e.g., sofas, lights, and TVs) while maintaining an inference time of less than 100\nms on a CPU. Action classification leverages MediaPipe for extracting 3D normalized pose coordinates\nfrom images and a compact 3-layer MLP with 0.7M parameters for action recognition, achieving 99.3%\naccuracy with an inference time below 50 ms.\n\nWearable eye trackers further enable intuitive control of smart home devices. These devices capture\ninfrared pupil images and field-of-view (FoV) images to compute real-time gaze coordinates. After a\nnine-point calibration process using least-squares fitting, gaze points are mapped onto detected household\nobjects. A decision is made if the gaze is continuously fixed on an object across five consecutive frames,\nprompting audio feedback like \u201cTV detected, please issue a command.\u201d For patients with speech\ncapabilities, commands are captured via a microphone, processed through Whisper-tiny (39M parameters)\n[35], and translated into device actions via MiIO protocol.\n\nFor patients with severe speech impairments, eye gestures and blinks are detected to enable interaction.\nGaze direction is classified by analyzing real-time pupil positions, while blinks are identified by tracking\nclosed-eye intervals using statistical thresholds derived from calibration data. These signals are translated\ninto control commands, allowing comprehensive interaction with smart home devices through a\ncombination of gaze, speech, and physical gestures, optimized for accessibility and efficiency."}, {"title": "Design of the Auto-Care agent", "content": "Auto-Care utilizes the GPT-40 Mini API to analyze multimodal patient data streams and deliver real-time,\ncontext-aware interventions. Prompts are dynamically generated to include a six-minute context window,\nsummarizing recent trends and events. CoT reasoning is embedded to enable stepwise analysis of patient\nstatus and recommended actions, such as pausing rehabilitation, suggesting hydration, or adjusting\nenvironmental conditions. The API is configured with a response token limit of 200 to ensure concise\noutputs, and a temperature setting of 0.7 balances variability and reliability. Predefined templates and\nmulti-step reasoning ensure robust and contextually relevant decisions across diverse rehabilitation\nscenarios. Feedback from real-world applications continuously refines prompt structures, enhancing\nprecision and system performance."}, {"title": "Data availability", "content": "The datasets supporting this study will be available from the GitHub repository before publication."}, {"title": "Code availability", "content": "The code supporting this study will be available from the GitHub repository before publication."}]}