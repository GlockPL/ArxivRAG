{"title": "LLM-PYSC2: STARCRAFT II LEARNING ENVIRONMENT FOR\nLARGE LANGUAGE MODELS", "authors": ["Zongyuan Li", "Yanan Ni", "Runnan Qi", "Lumin Jiang", "Chang Lu", "Xiaojie Xu", "Xiangbei Liu", "Pengfei Li", "Yunzheng Guo", "Zhe Ma", "Xian Guo", "Kuihua Huang", "Xuebo Zhang"], "abstract": "This paper introduces a new environment LLM-PySC2 (the Large Language Model StarCraft II\nLearning Environment), a platform derived from DeepMind's StarCraft II Learning Environment\nthat serves to develop Large Language Models (LLMs) based decision-making methodologies. This\nenvironment is the first to offer the complete StarCraft II action space, multi-modal observation\ninterfaces, and a structured game knowledge database, which are seamlessly connected with various\nLLMs to facilitate the research of LLMs-based decision-making. To further support multi-agent\nresearch, we developed an LLM collaborative framework that supports multi-agent concurrent queries\nand multi-agent communication. In our experiments, the LLM-PySC2 environment is adapted to\nbe compatible with the StarCraft Multi-Agent Challenge (SMAC) task group and provided eight\nnew scenarios focused on macro-decision abilities. We evaluated nine mainstream LLMs in the\nexperiments, and results show that sufficient parameters are necessary for LLMs to make decisions,\nbut improving reasoning ability does not directly lead to better decision-making outcomes. Our\nfindings further indicate the importance of enabling large models to learn autonomously in the\ndeployment environment through parameter training or train-free learning techniques. Ultimately,\nwe expect that the LLM-PySC2 environment can promote research on learning methods for LLMs,\nhelping LLM-based methods better adapt to task scenarios.", "sections": [{"title": "1 Introduction", "content": "In 2017, the StarCraft II Learning Environment (SC2LE)[1] was developed by DeepMind and Blizzard Entertainment.\nIt is the first environment that enables various reinforcement learning (RL) agents to compete with each other in the\nStarCraft II game, and promoted the emergence of decision-making methods such as QMix[2], Weighted QMIX[3],\nMAPPO[4], and the household name AlphaStar[5]. However, RL-trained agents typically require a substantial amount\nof data and prolonged interactions, but still lack generalization capabilities in most scenarios due to the task-relevant\nreward function. Consequently, it is urgent to develop new decision-making methods at the present.\n\nAdditionally, impressive research efforts such as Stanford Town[6], LLM plays MineCraft[7] and the game of\nDiplomacy[8] have demonstrated great potential in LLM-based decision-making in recent years. Considering that\nlarge models exhibit greater interactivity, interpretability, and reasoning capabilities, it is quite natural to apply large\nmodels in complex decision-making environments. However, there is no sufficiently comprehensive platform to support\nresearch on LLM decision-making methods in complex environments. Notably, the mainstream platform SC2LE\nenvironment does not yet support research on decision-making with large models.\n\nIn order to leverage the advantages of large models and circumvent the disadvantages of RL, researchers developed the\nSC2 module into TextStarCraft II (TSC2)[9], enabling LLMs to interact with the StarCraft II environment for the first\ntime. However, there are some restrictions in the environment. The LLM based agent can not use micro-operations and"}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Starcraft II", "content": "Starcraft II is a classic platform for evaluating algorithms. Specially, as a real-time strategy game, StarCraft II features a\nhigh-dimensional partially observable state space and a huge continuous action space. With three species and more than\n120 types of units, it is widely regarded as one of the most complex and challenging environments and is commonly\nused for evaluating advanced decision-making methods.\n\nTo support the research of learning methods, DeepMind and Blizzard Entertainment developed SC2LE, a comprehensive\nenvironment for RL research. This environment is designed to improve research in learning algorithms within complex\nstrategy games. It provides RL interfaces such as the observation, actions, and reward function, considered as one of the\nmost significant environments in the field of artificial intelligence.\n\nConsequently, after the introduction of SC2LE, more and more StarCraft II environments have emerged. Among\nthose environments, SMAC[10] and PyMARL are the most famous. SMAC is a benchmark comprising 23 tasks\nspecifically designed for multi-agent RL, mostly focusing on distributed multi-agent decision-making. To evaluate\nMARL algorithms, the SMAC team also developed PyMARL as their training platform. In the PyMARL framework,\nover five algorithms are integrated, and the framework is gradually expanded into a multi-environment available RL\nplatform.\n\nOverall, their work effectively advanced the research on multi-agent learning methods, made significant contributions\nto the field of intelligent decision-making, and motivated us to develop an environment for LLM-based methods."}, {"title": "2.2 LLM Decision-Making and Text StarCraft II", "content": "In recent years, the decision-making ability of LLMs has started to attract attention. In 2023, an LLM-based agent\ncalled the Ghost in Minecraft achieved 67.5% success in Minecraft's diamond challenge. After that, Agent-Pro[11], an\nLLM agent capable of using strategies like bluffing in Poker, was developed. Additionally, researchers deployed LLM\nagents in Werewolf[12], a game with deception and counter-deception through communication, and developed LLM\nagents in the game of Diplomacy, a game of collaboration and competition.\n\nThese works inspire researchers to develop LLM-based decision-making methods in games. As one of the most\nfamous real-time strategy games, StarCraft II was first developed into an LLM-interactable environment called TSC2.\nThis environment enables LLMs to make macro-decisions in StarCraft II and proves that LLMs can make decisions\nand defeat build-in bots at level-4 in StarCraft-II. However, TSC2 does not support micro-operations on units and\nmulti-agent collaboration and faces limitations in observation and action space.\n\nUnder these circumstances, we constructed the LLM-PySC2 environment, aiming to solve these problems and provide\na new StarCraft-II environment. We also make our environment compatible with SMAC tasks, facilitating comparisons\nwith algorithms developed in the StarCraft environment."}, {"title": "3 LLM-PySC2 environment", "content": null}, {"title": "3.1 Framework", "content": "The LLM-PySC2 environment is built on the PySC2 module's agent level. In Figure 1, the MainAgent plays the role of\ncontrolling the camera, selecting units, collecting observations, and executing actions, while the LLM agent plays the\nrole of the actual decision maker that observes game situations, analyzes, and gives actions. Each LLM agent connects\nto an LLM, getting a text or multimodal observations from a wrapper, querying the LLM in an independent thread, and\nfinally getting game analysis and actions."}, {"title": "3.1.1 Interact with environment", "content": "An interaction step consists of two phases: auxiliary management and decision-making (and it consists of many game\nsteps). In the auxiliary management phase, no LLM will be involved. The MainAgent will control the PySC2 camera\nand finish works like grouping newly trained units and managing idle workers to avoid excessive involvement of large\nmodels in simple and repetitive labour.\n\nObservations for each agent's unit teams will be collected in the decision-making phase. After all teams' observations\nare collected, the agents use the Observation Wrapper to translate the structured observations into a text observation.\nThen, all agents query remote or local LLMs concurrently, waiting until all the agents get the response.\n\nAfter all agents get the response, they will use the Action Recognizer to detect valid actions and translate the text actions\ninto a structured form. Then, the MainAgent moves the camera to the same position when collecting observations and\nexecutes each agent's stored actions. After executing all the actions, the LLM-PySC2 environment will enter the next\ninteraction step and repeat the work mentioned above."}, {"title": "3.1.2 Multi-agent communication", "content": "Considering that LLMs have inherent advantages in interaction, we designed a communication system for the multi-\nagent framework. In the communication system, agents communicate with each other using 'communication actions', a\nkind of text action similar to unit control actions shown in Figure 1."}, {"title": "3.2 Observation", "content": "Observation is indispensable for decision making. Different types of information are necessary for agents with different\ntasks. Roughly, we categorize observational information into two types: local observations for micro-level operations\nand global observations for macro-level decision-making. These observations can be divided into text and image\nobservations according to form."}, {"title": "3.2.1 Text Observation", "content": "Observation Wrapper for micro-operations This wrapper focuses on local observations. It provides detailed\ninformation of controlled unit, nearby ally and nearby enemy unit for an agent. It extracts unit information from PySC2\nobs object and the relevant game knowledge from the knowledge base. As shown in Figure 2 The text observation\ngenerated by the wrapper includes game time, unit information, unit knowledge, valid actions, short-term memory,\ncommunication data, and task descriptions. Agents using the wrapper are designed for micro-operations like fighting\nwith enemy units or constructing buildings in a specific position.\n\nObservation Wrapper for macro-decisions This wrapper focuses on global observations. It provides deployment\ninformation, unit counts, and upgrade status that are similar to the text observation of the TSC2 environment. For the\nagent responsible for military deployment, text observation generated from the wrapper is used for supporting overall\nstrategy. For the agent responsible for development, the generated global observation will make the agent aware of the\ncurrent economy and technology situation, supporting the planning of future works of development."}, {"title": "3.2.2 Image Observation", "content": "In the complex environment of StarCraft II, relying solely on textual observations may prevent agents from fully\ncomprehending the battlefield dynamics. To enhance situational awareness, the LLM-PySC2 environment provides\nmultimodal observation. This feature enables multimodal large models to integrate visual information, leading to a\nmore accurate understanding of the situation. Figure 3 highlights two primary types of image observations: game image\nobservation and feature map observation. These visual inputs provide the agent with critical battlefield information,\nfacilitating tactical analysis and strategy development."}, {"title": "3.3 Action", "content": "In decision-making environments, the concept of \"action\" is pivotal to enable interactions between the agent and the\nenvironment. In our framework, LLMs engage with the environment through text-based actions, which must adhere to a\nspecific format to be recognized and converted into PySC2 action functions. The process of processing text action into\nPySC2 functions can be seen in Figure 4."}, {"title": "Text Actions", "content": "These actions are expressed in a syntax that is intuitive and descriptive, allowing the LLM to comprehend\nthe intended operation without additional context. A standard text action is encapsulated in angle brackets and several\narguments, shaped as <ActionName()>, <ActionName(arg0)>, or <ActionName(arg0, arg1)>. The arguments can\nrepresent various elements, such as a unit tag, a screen coordinate, or a minimap position, allowing these actions to\nencompass the complete continuous action space of PySC2."}, {"title": "Action Space", "content": "All kinds of actions in PySC2 are available in our environment, however, each agent does not have to\nface all the actions of its race. In our environment, the action space is agent-specific, allowing each agent to define a\nunique set of actions. For the agent that controls units such as Stalkers, the action space consists of text actions like\n<Stop()>, <No_Op()>, <Move_Screen(screen)>, <Move_Minimap(minimap)>, <Attack_Unit(tag)>, and do not consist\nof actions like training units or research."}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Experiment Scenarios", "content": "To facilitate research in LLM-based decision-making, we have provided two sets of experiments: LLM-SMAC tasks and\nLLM-PySC2 tasks. The LLM-SMAC tasks are the same as standard SMAC experiments, which serve as an excellent\nbridge for comparing with RL-based methods. LLM-PySC2 tasks are new scenarios, which, compared to the SMAC\ntasks designed specifically for micro-operations, place more emphasis on the large model's ability to understand the\ntask scenario and make macro-level decisions."}, {"title": "4.1.1 LLM-SMAC tasks", "content": "LLM-SMAC tasks share the same settings as the original SMAC tasks. These tasks initialize units for both sides\nand automatically raise attacks for enemy units. In these scenarios, the key to victory lies in concentrating firepower,\ncontrolling combat distance, and, sometimes, in interaction frequency. They are good scenarios for comparing the\ntraining data efficiency with RL-based methods but not good scenarios for utilizing the multitasking and macro\ndecision-making capabilities of LLMs."}, {"title": "4.1.2 LLM-PySC2 tasks", "content": "LLM-PySC2 tasks are the newly proposed experiment scenarios, a task group that tests agents' situation analysis\ncapabilities, planning abilities, the application of knowledge, communication, and collaboration. Some of the tasks are\nshown in Figure 5"}, {"title": "4.2 Experiment Results", "content": "To facilitate subsequent research, we tested the decision-making ability of various large models. All experiments were\nconducted in StarCraft II of Version 5.0.13 (92440), LLM-PySC2 v0.1. We recorded a ratio of resources of the killed\nunit over the dead unit (K/D rate) and the winning rate (WR, i.e. task completion rate). The combination of K/D rate\nand WR reflects the performance of LLM in decision-making scenarios.\n\nIn the LLM-PySC2 environment, we provide series of LLMs, such as GPT-3, GPT-4, GPT-01, GLM-4, Claude-3,\nLlama-3.1. We tested some representative models among them, tested the performance of models with different\nreasoning abilities in decision-making tasks (GPT-3.5, GPT-4o-mini, GPT-40), and tested the performance of models\nwith different parameters based on the same architecture (Llama3.1-8b, Llama3.1-70b, Llama3.1-405b).\n\nAll experiments use the default configuration of the open-sourced codes. As a benchmark, we do not specially design\nprompts to promote decision quality or instruct the LLMs to obtain victories, and all the LLMs are not fine-tuned in the\nLLM-PySC2 environment. Results show that large models can make decisions and generate text actions in the correct\nform. However, when the task is complex enough or requires a lot of micro-operations, large models may not perform\nwell, suggesting that training or other technical methods are necessary for improving their decision quality."}, {"title": "4.2.1 Experiment Results in LLM-SMAC tasks", "content": "In the LLM-SMAC tasks, we conducted 20 repeated experiments for 6 LLMs in each scenario. For scenarios where\ndecisions were made by GPT-3.5-turbo, we raised the number to 50 due to its good concurrency support and friendly\ncost. In these experiments, all large models used textual observations. This setting is completely sufficient for scenarios\nother than 2c_vs_64zg, as they basically did not need to utilize terrain information. Results are shown in Table 1."}, {"title": "4.2.2 Experiment Results in LLM-PySC2 tasks", "content": "The same as the experiments in SMAC, we conducted 20 repeated experiments for each large model in each scenario\nand 50 for GPT-3.5-turbo. Considering that all models cannot complete the multi-line attack in Task 8, we only listed\nthe data from Task 1 to Task 7. Results are shown in Table 2 and 3."}, {"title": "5 Discussion", "content": "In the experiments, we found that there are several deficiencies in LLM-based decision making.\n\nHallucinations. Hallucination is the first problem that leads to bad decisions. Sometimes, LLMs confuse screen\ncoordinates with minimap coordinates (as shown in Figure 6), or use unmentioned actions in the Valid Actions part\nof the observation. Sometimes, LLMs even damage teammate units. Hallucination has become an urgent problem in\nLLM-based decision making.\n\nPoor knowledge utilization. Large models generally exhibit a significant deficiency in leveraging game-related\nknowledge. In task 2, game knowledge shows that Phoenix's GravitonBeam ability will prevent the unit from moving\nand attacking. However, this ability is still overused, failing to obtain victories in task 2. In task 5, even the LLM knows\nthe PurificationNova of Disruptor deals a lot of damage, they use the skill on injured units, causing a large amount of\nspillover damage."}, {"title": "Poor understanding of the world", "content": "Lack of world understanding is a kind of lack of knowledge. Pre-trained LLMs are\ngenerally not trained in decision-making tasks. They do not know how to win in each task. In task 4, for example, the\nlarge model should use Stalker's Blink ability to transfer injured units to the rear. However, this ability is rarely used,\nresulting in the unit's death and a zero winning rate in task 4, even though the LLM is told that Blink is commonly used\nto pursue the enemy or retreat injured units."}, {"title": "Low quality collaboration", "content": "In the multi-agent tasks like task 5 to 8, LLM agents should collaborate with others and\ndefeat the enemy together. However, we found it difficult for these agents to reasonably allocate targets, coordinate\nattack timing, and coordinate retreat timing, no matter whether they collaborate with or without a leadership/commander.\nHow to improve the collaboration performance of LLM agents is important in building a high-level multi-agent\ndecision-making system.\n\nThese issues hinder the application of LLMs in decision-making scenarios. Fortunately, there are many ways to improve\nthe decision-making ability of large models. For example, directly providing knowledge to LLMs may directly improve\ntheir ability. However, providing LLMs knowledge or precisely annotated datasets usually demands quite a lot of\nresources. Self-supervised learning is still the most attractive way to enhance decision-making ability, either through\nreward-based or reward-free methods (such as LLM reflection), and either through parameter training or training-free\ntechniques."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a new environment for LLM decision-making, the first environment that accommodates\ncontinuous PySC2 actions, and the first LLM StarCraft II environment with a multi-agent framework and communication\nsystem. In experiments, we test mainstream LLMs' performance in both the LLM-SMAC and LLM-PySC2 task groups,\namong which the LLM-PySC2 task group is a brand-new experimental scenario that we designed for large models.\nResults of baseline tests show that LLMs can make decisions, generating actions in the correct form. Still, the decision\nquality is relatively low and there are several problems like hallucinations, poor utilization of game knowledge, and\nlack of world understanding. Results indicate that learning in the deployment environment is necessary for LLM-based\ndecision-making. We hope the LLM-PySC2 environment can promote research on LLM learning methods, helping\nLLM-based decision-making methods better adapt to task scenarios."}]}