{"title": "WCEBleedGen: A wireless capsule endoscopy dataset and its benchmarking for automatic bleeding classification, detection, and segmentation", "authors": ["Palak Handa", "Manas Dhir", "Amirreza Mahbod", "Florian Schwarzhans", "Ramona Woitek", "Nidhi Goel", "Deepak Gunjan"], "abstract": "Objective: Computer-based analysis of Wireless Capsule Endoscopy (WCE) is crucial. However, a medically annotated WCE dataset for training and evaluation of automatic classification, detection, and segmentation of bleeding and non-bleeding frames is currently lacking. Methods: The present work focused on development of a medically annotated WCE dataset called WCEbleedGen for automatic classification, detection, and segmentation of bleeding and non-bleeding frames. It comprises 2,618 WCE bleeding and non-bleeding frames which were collected from various internet resources and existing WCE datasets. A comprehensive benchmarking and evaluation of the developed dataset was done using nine classification-based, three detection-based, and three segmentation-based deep learning models. Results: The dataset is of high-quality, is class-balanced and contains single and multiple bleeding sites. Overall, our standard benchmark results show that Visual Geometric Group (VGG) 19, You Only Look Once version 8 nano (YOLOv8n), and Link network (Linknet) performed best in automatic classification, detection, and segmentation-based evaluations, respectively. Conclusion and significance: Automatic bleeding diagnosis is crucial for WCE video interpretations. This diverse dataset will aid in developing of real-time, multi-task learning-based innovative solutions for automatic bleeding diagnosis in WCE. The dataset and code are publicly available here", "sections": [{"title": "I. INTRODUCTION", "content": "Gastrointestinal (GI) bleeding is the most common GI diagnosis in both inpatient and emergency settings [1], [2]. It results in 400,000 hospital admissions and 300,000 deaths per year globally, with mortality rates between 5 and 10 percent [3]. It has been identified as the most common GI cause of 30-day readmission in U.S. hospitals during 2014-15 [4]. Locating the source of GI bleeding is crucial for timely treatment initiation.\nThe most common sources of GI bleeding are the upper and lower GI tracts. However, in approximately between 5 and 10 percent of patients, the source of GI bleeding is the small bowel, which is difficult to examine using endoscopic methods due to its location, length, and excessive mobility [5]. For GI bleeding within the small bowel, Wireless Capsule Endoscopy (WCE) is the state-of-the-art procedure with high sensitivity for detecting causes of bleeding [6]. WCE has also proven to be a useful tool for diagnosing bleeding, tumors, and inflammatory disease in other segments of the GI tract, such as the colon and stomach [7].\nIn WCE, a disposable capsule-shaped device travels inside the GI tract via peristalsis and comprises an optical dome, a battery, an illuminator, an imaging sensor, and a radio-frequency transmitter. During 8 - 12 hours of WCE, a video of the GI tract transit is recorded on a device attached to the patient's belt which produces about 57,000 - 100,000 frames that are subsequently analyzed visually by subspecialized gastroenterologists [8]. With two to three hours of reading time per WCE for a subspecialized gastroenterologist, this visual evaluation is not only time consuming but also suffers from substantial heterogeneity and sub-optimal inter- and intra-observer disagreement [9].\nDue to an increasing lack of subspecialized gastroenterologists worldwide and a need for robust and repeatable WCE analysis, there is an unmet need for developing robust, interpretable and generalized Artificial Intelligence (AI) models that can decrease the workload of gastroenterologists by providing computer-aided classification, detection, and segmentation of bleeding and non-bleeding frames in WCE.\nResearch on streamlining multi-task learning in automatic bleeding diagnosis in WCE i.e., the classification of bleeding and non-bleeding frames in WCE, and further detection, and segmentation of bleeding in WCE has been hindered by the absence of dedicated datasets.\nThese datasets are characterized by imbalanced classes and do not adequately address the challenges of this AI problem such as the need for frames with multiple bleeding sites, and different types of bleeding shades, mixed with intestinal fluids, chyme, etc. The frames within these existing datasets do not exhibit diverse dimensions and thus necessitate pre-processing prior to utilization of any machine learning or deep learning models. None of the existing datasets include a combination of class labels, bounding boxes, and medical markings which could promote multi-task learning-based research in this field.\nDue to the limited availability of dedicated WCE datasets that specifically classify bleeding as a label, studies have incorporated various lesions such as angioectasia, ulcers, and polyps to develop automated AI models. While these lesions can lead to bleeding, the bleeding itself may not be directly observable in the WCE frames [10], [11]. Consequently, AI models developed using these datasets, cannot be employed on real-world data. Thus, for a robust, interpretable and generalized AI development in this domain, a dedicated dataset which only contains bleeding and non-bleeding WCE frames is essential.\nThe present work introduces WCEbleedGen, an open-source dataset comprising a total of 2,618 WCE frames collected from various internet repositories and existing datasets. The dataset consists of high resolution frames of uniform size, equally divided into the two classes namely bleeding and non-bleeding. Additionally, the dataset has been meticulously validated by subspecialized gastroenterologists, ensuring high accuracy. It features comprehensive annotations, including class labels, manually generated binary masks, and precise bounding boxes, making it a robust resource for advanced analysis. These features facilitate the development of combined solutions for the classification, detection, and segmentation of bleeding regions in WCE frames.\nTable II presents an overview of the recent advancements in AI techniques developed for automating the analysis of bleeding regions in WCE frames. Although these research works have made valuable contributions, they are subject to certain constraints. The metrics obtained in these works pose challenges in terms of validation and comparison due to the scarcity of data and the lack of established benchmarks. In addition, none of the previous studies report results for classification, segmentation, and detection simultaneously. Addressing these gaps, the present work also encompasses comprehensive benchmarking and evaluation of the developed dataset using nine conventional classification models, three detection models, and three segmentation models employing standard performance metrics. This establishes a performance standard for researchers to use in comparing and assessing their own work."}, {"title": "II. METHODOLOGY", "content": "The objective of this work was to develop and benchmark a WCE dataset consisting of bleeding and non-bleeding frames. The dataset development process is detailed in sub-section II-A. The information related to pre-existing datasets is detailed in sub-section II-B. To benchmark and validate the effectiveness of the developed dataset, nine classification-based deep learning models, three detection-based deep learning models, and three segmentation-based deep learning models were evaluated for automatic classification, detection, and segmentation of bleeding and non-bleeding frames separately. This evaluation is described in sub-section II-C. The implementation settings are mentioned in sub-section II-D.\nA. Data Collection, Processing and its Annotation\nThe first phase of the dataset development involved the compilation and extraction of frames depicting both bleeding and non-bleeding instances from multiple sources including various internet repositories and pre-existing datasets. Table III presents the sources for each frame in the dataset, to provide transparency regarding the origin of the data. A total of 2,618 frames were collected, with the bleeding and non-bleeding classes consisting of 1,309 frames each to ensure class balance. Sample frames of the dataset are shown in Figure 1.\nAfter data collection, the frames were resized to a consistent size of 224 x 224 pixels, which corresponds to the default input size for common transfer learning techniques [24]. Then the binary masks were manually generated for all the frames followed by development of bounding boxes through an automated python script to ensure accurate detection and segmentation of bleeding regions and to obtain a comprehensive understanding of medical features. Additionally, the dataset was manually validated by subspecialized gastroenterologists to ensure correctness and reliability in clinical interpretation.\nThe data was then organized into a folder structure, with separate folders for bleeding and non-bleeding classes, as well as distinct folders for binary masks, images, and bounding box markings within each class folder. Three formats of bounding box markings were included inside the bounding box folder namely the text, extensible markup language (XML), and text markings compatible with You Only Look Once (YOLO) models.\nB. Pre-existing Datasets\n1) KID [27]: KID is the first dataset used in WCE-AI research, featuring over 2,500 WCE frames with a size of"}, {"title": "C. Bio-mathematical problem definition and evaluation metrics", "content": "Let $D = \\{(X_i, Y_i)|i = 1, ...N\\}$ represent the developed dataset, where N = 2618 is the total number of frames in the dataset. $X_i \\in R^{224\\times224\\times3}$ is the ith frame and $Y_i \\in \\{0,1\\}$ is the binary label indicating non-bleeding (0) or bleeding (1). The research problem is defined for three tasks:"}, {"title": "D. Implementation settings", "content": "The models were implemented using Python scripts with TensorFlow as the backend on a high-performance super-computer consisting of four port 40 GB DGX A100 NVIDIA workstations, running on a pre-installed Ubuntu Linux-based operating system. Additionally, the image labeler MATLAB toolbox was utilized for generating precise binary mask annotations for all the 1,309 bleeding frames. For some of the frames available in the Red Lesion Endoscopy Dataset (Set 2), direct rectangle boxes were drawn as binary mask annotations. The bounding boxes were generated using the binary masks with the help of a python script using libraries like pandas, and OpenCV. The script has been provided on our github.\nFor classification tasks, all the models were compiled with Adam optimizer and categorical cross-entropy loss with the default learning rate as 0.001. The batch size for training was kept as 32. On-the-fly data augmentations like rotation (up to \u00b110 degrees), width and height shifting (up to \u00b10.1), zoom (up to \u00b10.1), horizontal flipping, and normalization to [0,1] were done only to increase the quantum of the data for classification tasks. All classification models were utilized with pre-trained weights from imagenet dataset and detection models were loaded with weights from the Common Objects in Context (COCO) dataset [24]. For detection, the Adam optimizer was utilized with learning rate and momentum set to default which was automatically determined as 0.002 and 0.9 respectively. The batch size for training was kept as 16. All detection models were setup using the in-built functions available in the Ultralytics library. For segmentation, Adam optimizer was used with 0.0001 learning rate, binary cross-entropy loss and a batch size of 32.\nThe models were built using keras library. No separate on-the-fly augmentations were done for the detection and segmentation tasks.\nFor classification tasks, averaged accuracy, accuracy on the last epoch, averaged loss, and loss on the last epoch were considered for the training and validation data, while precision, recall, and F1 score were considered for the test set evaluation. For detection tasks, Box Loss (BL), Classification Loss (CLS), and Distribution Focal Loss (DFL) were used for training data, while precision, recall, and mean Average Precision (mAP) were considered for test dataset evaluation. For the validation data, BL, CLS, mAP, recall, and precision were reported. The evaluation of the segmentation tasks for the training, validation, and test dataset were based on accuracy, precision, recall, dice coefficient, loss, accuracy, and Intersection over Union (IoU).\nFor evaluation of the dataset for classification, segmentation and detection tasks, the dataset was randomly split into 70 - 20 - 10 ratio, where 70% was used for training, 20% was used for validation and the 10% subset was used for testing purposes. A fixed random seed was initialized in all the scripts to maintain reproducibility of the scripts and to ensure consistency. All models for classification, detection, and segmentation tasks were trained for 250 epochs without any modification, parameter sharing or hyper parameter tuning."}, {"title": "III. RESULTS", "content": "The results have been discussed as per the three tasks namely the classification, detection, and segmentation performed on the proposed datasets. Table V and VI detail the achieved performance metrics for classification task on training, validation and test set respectively. The training and validation performance metrics for detection tasks have been detailed in Table VII. Table VIII detail the results achieved upon testing the detection-based AI models. Table IX and X summarize the achieved evaluation metrics on training, validation and test set for LinkNet, SegNet and UNet segmentation models.\nA. Classification task\nOver the average of 250 epochs, a highest accuracy and lowest loss of up to 0.99 and 0.004 respectively were observed for the training dataset, whereas the highest and the lowest values for accuracy and loss on last epoch were found to be 1 and 3.25 \u00d7 10-10 respectively. The highest accuracy of 0.78 was recorded for the validation data, both as an average over 250 epochs and in the last epoch. This was accompanied by the lowest average loss of 6.46 and the lowest last epoch loss of 6.93 for the validation data. For the test set, the highest values achieved for the macro averages of precision, recall, and F1-score were 0.63, 0.63, and 0.63 respectively. Likewise, the highest value for weighted average of precision, recall and F1-score was also 0.63. For the classification task nearly all AI models achieved notably high accuracy ranging between 0.96 \u2013 1, with minimal loss values on the training data.\nB. Detection task\nOn the training data, the lowest values for BL, CLS, and DFL averaged over 250 epochs were found to be up to 1.15, 1.22, and 1.44, respectively. Similarly, 0.80, 0.66, and 1.19 were obtained as the lowest BL, CLS and DFL values respectively, for the last epoch. The highest precision, recall, mAP@50, and mAP@95 on the validation set, averaged over 250 epochs, were found to be up to 0.62, 0.55, 0.57, and 0.28, respectively. These were accompanied by lowest average BL, CLS and DFL values of up to 1.73, 1.68, and 1.99 respectively. Similarly for the last epoch, 0.69, 0.62, 0.65, and 0.33 were obtained as the highest values for precision, recall, mAP@50, and mAP@95 respectively. The lowest values for BL, CLS, and DFL were observed at the last epoch, reaching 1.67, 1.40, and 2.04, respectively. For the 10% test set, the highest values for precision, recall, mAP@50, and mAP@95 were 0.69, 0.64, 0.63, and 0.36, respectively.\nC. Segmentation task\nThe highest values for accuracy, dice coefficient, IoU, precision and recall averaged over 250 epochs were obtained as 0.99, 0.93, 0.89, 0.96, and 0.99 respectively, accompanied by lowest average loss value of 0.01 on the training data. Similarly 0.99, 0.98, 0.97, 0.99, and 0.99 were obtained as the highest accuracy, dice coefficient, IoU, precision and recall respectively, for the last epoch with the lowest loss of 0.004. For the validation set, 0.97, 0.87, 0.80, 0.89, and 0.90 were achieved as the highest accuracy, dice coefficient, IoU, precision and recall respectively, with the lowest loss value of up to 0.06, averaged over 250 epochs. Likewise for the last epoch, the highest values for accuracy, dice coefficient, IoU, precision and recall were achieved up to 0.98, 0.95, 0.90, 0.93, and 0.97 respectively, along with lowest loss of 0.04. Whereas for the test set, 0.99, 0.98, 0.95, 0.90, and 0.94 were obtained as the highest accuracy, precision, recall, IoU and dice coefficient respectively."}, {"title": "IV. DISCUSSION", "content": "Manual interpretation of WCE frames with bleeding abnormality is a laborious, cost ineffective and error prone process due to the large number of frames captured during the capsule travelling throughout the GI tract. A subspecialized gastroenterologist spends at least two to three hours to evaluate a WCE video of a single patient [47]\u2013[49]. Thus to overcome this challenge, various AI models have been developed in the past to automate the process of bleeding analysis in WCE [50]. However, the development of integrated systems for bleeding analysis has been hindered by the lack of dedicated datasets.\nOur proposed dataset is the first of its kind which facilitates the development of a combined solution for performing simultaneous classification, detection and segmentation of bleeding regions in WCE frames. The proposed dataset consists of a total of 2, 618 frames with balanced classes of bleeding and non-bleeding and contains multiple types of abnormalities and lesions which contribute to bleeding. Furthermore, the dataset consists of high quality frames standardized to a uniform dimension of 224 \u00d7 224 pixels along with medical annotations in the form of binary masks and bounding boxes. It is freely available and can be used to build integrated solutions to perform automatic classification, detection, and segmentation of bleeding region in WCE frames.\nThe proposed dataset was evaluated for classification, detection, and segmentation tasks using vanilla AI models to show the feasibility of AI in this field. The AI models used in this work were selected based on their promising performance in prior research works, particularly in classification, segmentation, and detection tasks on different biomedical datasets [43]\u2013[46]. All AI models were trained for 250 epochs on a super-computer without any modifications. Common evaluation metrics were reported for each of the AI model.\nVGG19, obtained an accuracy of up to 0.78 on the validation data, performing better than the other models. This showed that VGG19 was successful in capturing the underlying patterns in the data, but not to an exceptionally high degree. Other models with very high training accuracy and significantly lower validation accuracy showed that the models were over-fit on some features present in the training data, leading to poor generalizability performance on validation data. All the models achieved low values for weighted average and macro average of precision, recall and F1-score on the test set. This showed that the models had poor generalization capability and could not be used in practical applications for unseen data without any modification. The overall poor performance of all models for the classification of bleeding frames showed the need of refinement of the AI models, hyper-parameter tuning or using ensemble learning along with other AI models.\nThere was very little variation in the performance of the three detection AI models with minimal difference in the achieved evaluation metrics. The models showed uniform performance across both the validation and test sets, indicating the absence of overfitting of the model on training or validation data. Yet, the attained metrics were not up to the mark, highlighting the model's inability to capture the complex patterns present within the dataset. Based on the metrics obtained in the testing phase, YOLOv8n architecture performed the best followed by YOLOv5nu and YOLOv8x. However, the overall low performance indicated the need of fine-tuning of the AI model to improve their performance for practical applications in clinical settings.\nAll three models in the segmentation task namely, the UNet, SegNet, and LinkNet, exhibited high accuracies accompanied with low loss values with LinkNet having the least loss on both training and validation data followed by UNet and SegNet. This indicated their ability to accurately classify pixels. SegNet achieved relatively lower values for IoU and dice coefficient on both training and validation data which shows that there was a lower amount of overlap between the predicted and the true annotations. It was inferred from the high values obtained for accuracy, precision and recall for SegNet, that the model was able to correctly classify the pixels, but was unable to define the boundary correctly leading to lower values for IoU and dice coefficient. A similar trend was observed in the IoU and dice coefficient obtained on the test set for SegNet. LinkNet and UNet, on the other hand, exhibit a favorable balance between the metrics observed for both the training and validation sets, along with a similar performance on the test set with LinkNet performing slightly better than UNet. This implied that the architectures were able to effectively learn the features of the dataset during the training process and have good generalization capability on unseen data as well. Further these AI models may be tweaked and fine tuned to achieve enhanced performance for real world applications.\nOverall, the metrics obtained for classification and detection tasks were not up to the mark but demonstrated the potential of AI in this field with improvement and modification in the AI models. For the segmentation task, high metrics were obtained for the LinkNet model, thus further testing and tuning may be done for utilization in real world clinical setting. Furthermore, there is potential to enhance the evaluation metrics by incorporating image augmentation techniques and optimizing the model's hyper-parameters.\nThe evaluation metrics presented in this work serve as valuable benchmarks for future researchers, enabling them to compare their findings, refine AI models, and address emerging challenges in the field. Baseline models help in establishing the foundation of the research problem by providing necessary context which in turn guides decision making process of researchers. They further expedite the development of highly efficient and accurate real-world systems, building upon the established foundation.\nThe present work is subject to certain limitations, which present opportunities for future research to address and overcome. The WCEBleedGen dataset consists of frames collected from diverse sources ranging from internet repositories to pre-existing datasets, thus it does not support per patient medical analysis as the actual source of the images is not known. Furthermore, the dataset does not enable longitudinal analysis of bleeding region. The variation of the bleeding region with time cannot be captured using the proposed dataset, due to different non-sequential image frames. The absence of sequential frames also hinders the recognition of the source of the blood flow inside the human body."}, {"title": "V. CONCLUSION", "content": "Automatic bleeding analysis in WCE interpretation enhances diagnostic accuracy, reduces clinician workload, and ensures timely detection of GI bleeding, improving patient outcomes. The present work demonstrated the potential of AI for automatic classification, detection, and segmentation of bleeding and non-bleeding frames in WCE. The proposed dataset, WCEBleedGen, validated by proficient gastroenterologists, enables the development of robust integrated solutions for multi-task learning in this field. The obtained evaluation metrics set a benchmark for future researchers for further improvements. The future directions of this research are geared towards the development of a larger dataset that encompasses multiple classes for more specific analysis of GI diseases."}]}