{"title": "Multi-scale Temporal Fusion Transformer for Incomplete Vehicle Trajectory Prediction", "authors": ["Zhanwen Liu", "Chao Li", "Yang Wang", "Nan Yang", "Xing Fan", "Jiaqi Ma", "Xiangmo Zhao"], "abstract": "Motion prediction plays an essential role in autonomous driving systems, enabling autonomous vehicles to achieve more accurate local-path planning and driving decisions based on predictions of the surrounding vehicles. However, existing methods neglect the potential missing values caused by object occlusion, perception failures, etc., which inevitably degrades the trajectory prediction performance in real traffic scenarios. To address this limitation, we propose a novel end-to-end framework for incomplete vehicle trajectory prediction, named Multi-scale Temporal Fusion Transformer (MTFT), which consists of the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. Specifically, the MAH leverages the multi-head attention mechanism to parallelly capture multi-scale motion representation of trajectory from different temporal granularities, thus mitigating the adverse effect of missing values on prediction. Furthermore, the multi-scale motion representation is input into the CRMF module for multi-scale fusion to obtain the robust temporal feature of the vehicle. During the fusion process, the continuity representation of vehicle motion is first extracted across time steps to guide the fusion, ensuring that the resulting temporal feature incorporates both detailed information and the overall trend of vehicle motion, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion trend. We evaluate the proposed model on four datasets derived from highway and urban traffic scenarios. The experimental results demonstrate its superior performance in the incomplete vehicle trajectory prediction task compared with state-of-the-art models, e.g., a comprehensive performance improvement of more than 39% on the HighD dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the development of autonomous driving technologies, autonomous vehicles (AVs) have made great achievements. To guarantee the safety and efficiency in complex and dynamic traffic scenarios, AVs need to utilize the vehicle position and road topology information provided by roadside or onboard sensing systems to predict the future trajectories of surrounding vehicles [1], [2], enhancing their own local-path planning and collision warning.\nIn recent years, benefit from the development of deep learning technologies, various well-designed deep learning models have been proposed. Interaction-aware models are first proposed to consider inter-vehicle interaction in trajectory prediction, achieving promising prediction performance in simple highway scenarios [3], [4]. Subsequently, models leveraging high-definition (HD) maps have been introduced to incorporate the constraints imposed by complex road topologies, enabling flexible trajectory prediction of vehicles in non-Euclidean spaces [5]. With the advancement of Transformer, numerous prediction models based on Transformers have recently emerged, harnessing attention mechanisms to capture long-term dependency and achieve superior performance in long-term trajectory prediction [6].\nWhile achieving impressive achievements, the performance of existing methods highly depended on the completeness of observed history trajectory, neglecting the potential missing values caused by occlusion and perception failures in real traffic scenarios. The existence of missing values hinders the model from capturing the accurate temporal dependence between the adjacent time steps, undoubtedly leading to a significant negative impact on the trajectory prediction performance of existing models.\nTo achieve end-to-end incomplete vehicle trajectory prediction,the Multi-scale Temporal Fusion Transformer (MTFT) is presented to alleviate the negative impact of missing values on trajectory prediction by capturing and fusing the temporal dependency of trajectory across multiple time scales. Specifically, we design a novel Multi-scale Attention Head (MAH) utilizing the inherent padding mask mechanism in the Transformer. With the predefined scale mask, the MAH can perceive the temporal dependency of incomplete trajectory from different time scales parallelly, enabling the extraction of multi-scale motion representation. Furthermore, a Continuity Representation-guided Multi-scale Fusion (CRMF) module is proposed, which computes the multi-scale continuity representation and uses it to guide the multi-scale motion representation fusion, thereby extracting the robust temporal feature of the vehicle. The critical idea behind our method is that the observed trajectory at different time scales may skip some missing values, and predicting the current trajectory from different temporal granularities utilizing multi-scale motion representation can alleviate the negative impact of missing values. Furthermore, the continuity representation extracted across time steps reflects the overall trend of vehicle motion and is insensitive to the missing patterns of trajectory, albeit with a loss of some detail. It can guide the model to perform the fusion of multi-scale motion representation to extract temporal representation that balances detailed information with the overall motion trend to the fullest extent possible, which contributes to achieving accurate prediction of incomplete vehicle trajectory.\nThis work is extended on our previous conference version MSTF [8]. Compared to MSTF, we extend our work in the following three aspects: 1) we explore the positive significance of the novel multi-scale fusion method, CRMF, on incomplete trajectory prediction; 2) we incorporate the impact of inter-vehicle interaction to enhance accurate incomplete trajectory prediction; 3) We validated the effectiveness of MTFT by comparing it with state-of-art models on three public datasets and one self-constructed dataset. In summary, the main contributions of our work are summarized below:\n\u2022 We conduct a statistical analysis to reveal that the trajectory missing is inevitable in real traffic scenarios, which will disturb the accurate prediction of future vehicle trajectory. To address this issue, we develop an end-to-end framework named MTFT consisting of MAH and CRMF for incomplete trajectory prediction. The MAH is meticulously designed to capture the multi-scale dependency of trajectory from different temporal granularities, mitigating the negative impact of missing trajectory on vehicle trajectory prediction.\n\u2022 We propose a novel CRMF module, which adaptively extracts multi-scale continuity representation across time steps based on the missing pattern of trajectory. Utilizing the multi-scale continuity representation, CRMF fuses the multi-scale motion representation to obtain the temporal representation that both contains detailed information and the overall trend of motion, which facilitates the accurate incomplete trajectory prediction.\n\u2022 To validate the prediction performance of the proposed model on the real incomplete trajectory, we analyze and filter the trajectory of vehicle with the object type 'OTH-ERS' in the Argoverse dataset [9], constructing the In-complete Argoverse (IArgoverse) dataset. The IAgoverse dataset includes incomplete vehicle trajectory resulting from occlusions etc. in real traffic scenarios, providing a benchmark for the research in incomplete vehicle trajectory prediction.\n\u2022 Our method is comprehensively validated on four datasets spanning three different traffic scenarios. Both ablation experiments and comparative experiments indicate that the proposed MTFT outperforms state-of-the-art methods significantly, especially when the percentage of missing values is high."}, {"title": "II. RELATED WORK", "content": "A. Trajectory Prediction\nThe objective of trajectory prediction is to enable autonomous vehicles to anticipate the future positions of surrounding vehicles, thereby facilitating efficient and safe driving in dynamic traffic environments.\nIn previous research, physical models including single trajectory methods [10], Kalman filtering methods [11], and Monte Carlo methods [12] have been proposed for vehicle trajectory prediction, which are computationally relatively simple but cannot be flexibly applied to dynamically changing traffic scenarios, such that most of these methods are only suitable for short-term prediction (no more than 1 s) [13]. Unlike physics-based methods that use several physics models, machine learning-based methods apply data-driven models to predict trajectory, such as Gaussian Process [14], Support Vector Machine [15], Hidden Markov Model [16], Dynamic Bayesian Network [17], [18], and so on. These methods utilize feature extraction from data to estimate the distribution of future trajectory, thus offering novel insights into trajectory prediction and driving the advancement of learning-based approaches. Recently, trajectory prediction methods based on deep learning have become increasingly popular due to their ability to achieve superior predictive performance in more complex scenarios by considering interaction-related factors. As a typical representative of RNN-based models, Social-LSTM [19] innovatively embeds vehicle features by rasterizing traffic scenes for interaction extraction, and then sequentially decodes future trajectory through the recursive work mechanism of LSTM. Following this, Hyeon et al. [20] utilize an encoder-decoder LSTM architecture. The LSTM encoder encodes the historical trajectory features, while the LSTM decoder employs beam search algorithm to solve for the K most likely future trajectories. Xing et al. [21] use GMM to distinguish driving styles and utilize the LSTM followed by fully connected regression layers to predict trajectory based on driving styles. By calculating the distance between the vehicle and the centerline, Chang et al. [9] propose an LSTM encoder-decoder baseline that takes the map information and social information into account and compares it with the Nearest Neighbor (NN) regression method. Considering lane information, Kawasaki et al. [22] combine LSTM with KF for multi-modal trajectory prediction. Li et al. [23] propose a Two-stream LSTM structure, an improvement based on LSTM, aiming to achieve superior performance in long-term trajectory prediction. Although some methods using RNN have achieved significant success in extracting features from Euclidean spatial data, they lack flexibility when applied to complex traffic scenarios with complex road topologies such as roundabouts and intersections. Therefore, graph-based methods have been proposed to adapt to the complexity of road topology, facilitating the vehicle trajectory prediction in non-Euclidean space. A hierarchical GNN named VectorNet [24] encapsulates the sequential features of map elements and past trajectories with instance-wise subgraphs and models interaction with a global graph. MacFormer [25] is the extension of VectorNet focusing on predicting rational destinations. With the significant achievements of Transformer [26] in the field of NLP, Transformer-based models [6], [27] have been applied to this task to establish direct links for inputs via an attentional mechanism, enabling the models to capture long-term dependency of the trajectory for achieving more accurate prediction.\nHowever, these methods assume that vehicle observations are entirely complete, which is too strong an assumption to satisfy in practice. Existing methods are not applicable to the prediction of incomplete trajectory whose temporal dependency is disrupted by missing values.\nB. Sequence Imputation\nIn response to the issue of missing sequential data across various domains, three categories of sequential imputation methods have been proposed, encompassing statistical imputation methods, machine learning imputation approaches, and deep learning imputation algorithms.\nThe simple statistical imputation methods replace missing values with statistics or the most similar ones among the data. In contrast, traditional machine learning imputation solutions train a prediction model for missing value imputation, including tree-based imputation methods, regression-based imputation methods, compression-based imputation methods, and shallow neural network (SNN)-based imputation methods. The Tree-based imputation methods establish a decision tree model for each incomplete feature containing missing values, such as XGBI [28]. The regression-based methods utilize linear regression models with multiple imputations to estimate missing values, e.g., IIM [29]. Unlike other machine learning solutions, the compression-based methods construct a single prediction model for the whole incomplete dataset [30]. Additionally, the SNN-based methods make use of a shallow neural network to impute missing values, such as RRSI [31]. Moreover, deep learning models have been employed to address imputation problems, such as deep autoencoders (AEs) and generative adversarial networks (GANs). Hinton et al. [32] initially proposed deep AE models. It has powerful density estimators that capture complex distributions. Building upon the foundation of deep AE models, various AE-based imputation algorithms are presented, including HI-VAE [33] and MIWAE [34]. On the other hand, the GAN model introduced by Goodfellow et al. [35] builds an adversarial training framework between two players, namely the generator and the discriminator, to play a minimax game. The GAN model possesses powerful modeling capability to learn complex high-dimensional distributions [36]."}, {"title": "III. METHODS", "content": "A. Problem Formulation\nTo facilitate the study of the temporal evolution of trajectory, existing datasets have supplemented the vehicle trajectory that is missing due to target occlusion or perception failure through manual annotation, which results in the incomplete trajectory being unavailable. Therefore, we adopt the method of randomly masking to generate the incomplete trajectory in this work.\nSpecifically, considering the complete observed trajectory of the target vehicle and its surrounding N vehicles as $X = \\{x^{tar}, X_1, X_2, ..., X_N\\}$, and $x_i$ can be further denoted as $x_i = \\{x_i^{t+1}, x_i^{t+2}, ..., x_i^{t+T_h}\\}$. Where $x_i \\in \\mathbb{R}^2$ is the 2D coordinate of vehicle i at time t, and $T_h$ represents the observation horizon. To simulate the incomplete trajectory caused by the object occlusion and perception failures in real traffic scenarios, we define the sequence mask $M = \\{m^{tar}, M_1, M_2, ..., m_N\\}$ with the same dimension as X, where $m_i = \\{m_i^{t+1}, m_i^{t+2}, ..., m_i^{t+T_h}\\} \\in \\{0, 1\\}^{T_h}$ is the sequence mask for vehicle i. The variable $m_i^t$ is assigned a value of 0 if the observation of vehicle i at time t is missing, and 1 otherwise. The positions and number of missing observations are generated completely at random. Under this setting, the generated incomplete trajectory can be expressed as:\n$X_{in} = X \\otimes M$ (1)\nwhere $X_{in} = \\{X_{in}^{tar}, X_{1,in}, X_{2,in}, ..., X_{N,in}\\}$ is the incomplete trajectory of the target vehicle and its surrounding N vehicles after random masking, and $\\otimes$ represents element-wise multiplication.\nBased on the generated incomplete historical trajectory, this study aims to predict the future trajectory $\\hat{y} = \\{\\hat{y}_{t+T_h+1}, \\hat{y}_{t+T_h+2}, ..., \\hat{y}_{t+T_h+T_f}\\}$ of the target vehicle. Where $T_f$ is the prediction horizon, and $\\hat{y}_t \\in \\mathbb{R}^2$ is predicted the 2D coordinate at time t.\nB. Model Framework\nFig. 2 depicts the pipeline of the proposed MTFT. Firstly, the sequence mask is obtained by randomly generating the number and distribution position of masks, which is applied to mask the complete trajectory sourced from the large public dataset to obtain the required incomplete trajectory. Secondly, utilizing the predefined scale mask, MAH captures the temporal dependency of incomplete trajectory from different time scales parallelly, thereby forming multi-scale motion representation. Subsequently, the CRMF module obtains across attention based on the observation matrix computed with the sequence mask and the scale mask, and the across attention is used to fuse multi-scale motion representation across time steps, yielding multi-scale continuity representation. Leveraging multi-scale continuity representation as query vector, CRMF fuses multi-scale motion representation across time scales to derive the temporal feature of each vehicle. Finally, following the modeling of global interactions among all vehicles, the future trajectory decoder outputs the predicted trajectory of the target vehicle.\nC. Multi-scale Attention Head\nEffectively capturing the temporal dependency of the trajectory is crucial for the vehicle trajectory prediction task. However, the presence of missing values disrupts the local dependency between the adjacent time steps. We argue that recurrent neural networks (e.g., LSTM and GRU), which extract local dependency of the trajectory serially using the recursive mechanism, are more susceptible to the negative effect of missing values. In contrast, the Transformer utilizes the attention mechanism to process the input sequence in parallel, enabling each value in the sequence to directly aggregate information from all the remaining values to capture global dependency, which mitigates the negative impact of certain missing values on prediction to a certain extent. Therefore, adopting a Transformer-designed encoder in our work is a natural decision.\nSpecifically, we first compute the query vector $Q = \\{q^1, q^2, ..., q^n\\}$, the key vector $K = \\{k^1, k^2, ..., k^n\\}$, and the value vector $V = \\{v^1, v^2, ..., v^n\\}$ for n attention heads based on the incomplete trajectory $X_{in}$.\n$X_{in}' = \\beta(X_{in}) + Pos$\n$q^i = \\Phi_Q(X_{in}', W_Q)$\n$k^i = \\Phi_K(X_{in}', W_K)$\n$v^i = \\Phi_V(X_{in}', W_V)$ (2)\nwhere $\\beta$ denotes MLP, utilized to project the two-dimensional coordinate into higher dimensions, enhancing feature representation. Following the Transformer, positional encoding Pos is incorporated into the model to distinguish the order of the input sequence. $W_Q$, $W_K$, and $W_V$ are the learnable parameter matrices for corresponding transformations $\\Phi_Q$, $\\Phi_K$, and $\\Phi_V$, respectively.\nFurthermore, the scale mask $M = \\{m^1, m^2, ..., m^n\\}$ with different temporal granularities is predefined for n attention heads. Where, $m^i \\in \\mathbb{R}^{len \\times len}$ is the scale mask for the attention head i, len represents the length of the input sequence, and the value $m_{a,b}^i$ at position (a, b) in $m^i$ can be expressed as:\n$m_{a,b}^i = \\begin{cases}\n1, & a \\% i = b \\% i, a, b \\in \\{1, 2, ..., len\\} \\\\\n0, & Others\n\\end{cases}$ (3)\nwhere Z represents the set of integers.\nFinally, leveraging the scale mask $M = \\{m^1, m^2, ..., m^n\\}$, we design the mapping function $\\Psi$ to obtain the $ScaleAtten$, which enables the n attention heads to observe the incomplete trajectory from different temporal granularities, parallelly extracting multi-scale motion representation $R_m = \\{r^1_m, r^2_m, ..., r^n_m\\}$.\n$a^i = q^i (k^i)^T$\n$\\Psi(a^i, m^i)_{a,b} = \\begin{cases}\n\\alpha_{a,b}^i, & a, b, m_{a,b}^i = 1 \\\\\n-\\infty, & m_{a,b}^i = 0\n\\end{cases}$ (5)\nwhere $a_{a,b}^i$ and $m_{a,b}^i$ are the values at position (a,b) in $a^i$ and $m^i$, respectively.\nD. Continuity Representation-guided Multi-scale Fusion Module\nThe presence of missing values causes the encoded feature of the same incomplete trajectory sample to vary randomly with the missing pattern (the number and distributed positions of missing values), which poses a significant challenge to accurately decode the future trajectory. In this regard, we propose the continuity representation-guided multi-scale fusion (CRMF) module to extract continuity representation across time steps to guide the multi-scale representation fusion. The continuity representation does not contain detailed motion information but can reflect the overall trend of vehicle motion, and is insensitive to the trajectory missing. Guided by the continuity representation, the multi-scale motion representation fusion can obtain the temporal feature that both contains detailed motion information and reflects the overall trend of the motion to the greatest extent, which can enhance the prediction ability of the model.\nFormally, the observation matrix $S = \\{s^1, s^2, ..., s^n\\}$ is first computed based on the randomly generated sequence mask m and the predefined scale mask $M = \\{m^1, m^2, ..., m^n\\}$. The process of computing the observation matrix can be formulated as follows:\n$s^i = \\Lambda(m, m^i)$ (6)\nwhere the sequence mask m reflects the missing pattern of the trajectory, while the scale mask $m^i$ serves as the observation scale of attention head i. $s^i \\in \\mathbb{R}^{len \\times len}$ denotes the observation matrix at time scale i, which indicates whether the values in the sequence can be observed with each other under the constraints of the missing pattern and the time scale. The symbol $\\Lambda$ represents the element-wise multiplication of m and $m^i$ row by row.\nSubsequently, utilizing the observation matrix $s^i$, the information increment $\\Delta^i = [\\Delta_1^i, \\Delta_2^i, ..., \\Delta_{len}^i]$ of each value in the sequence is statistically analyzed at time scale i.\n$\\Delta_{j}^i = \\sum_{l=1}^{len} s_{j,l}^i$\n$\\delta_{j,l}^i \\in \\{0, 1\\}$\n$s_{j,l}^i = \\sum_{l=1}^{len} s_{j,l}^i$ (7)\nwhere $s_{j,l}^i$ represents the value of the observation matrix $s^i$ at row j and column l. $\\delta_{j,l}^i$ is assigned 1 if the l \u2013th value in the sequence can be observed by the j-th value, and 0 otherwise. $\\Delta_{j}^i$ represents the information increment of the j \u2013 th value in the sequence at time scale i.\nFurthermore, due to the capability of MAH to capture global dependency, the feature of each trajectory point in multi-scale motion representation can reflect the motion continuity to some extent, only that the trajectory points at different locations observe the motion continuity from different perspectives. Therefore, we aggregate multi-scale motion representation across time steps, which enables CRMF to synthesize different perspectives to obtain robust continuity representation. Specifically, considering that the missing values affect the trajectory points at different locations differently, we compute the according to the information increment $\\Delta^i = [\\Delta_1^i, \\Delta_2^i, ..., \\Delta_{len}^i]$, assigning greater weight to the trajectory points that are less affected by the missing values, and then deriving the multi-scale continuity representation $R_c = \\{r^1_c, r^2_c, ..., r^n_c\\}$ by weighted aggregation across time steps.\n$w^i = \\frac{exp(\\Delta^i)}{\\sum_{i=1}^{len} exp(\\Delta^i)}$ (8)\n$AcrossAtten(\\Delta^i) = \\{w_1^i, w_2^i, ..., w_{len}^i\\}$,\n$r^i_c = AcrossAtten(\\Delta^i) \\times (r^i_m)$. (10)\nwhere $r^i_c$ is the continuity representation at time scale i.\nFinally, to comprehensively capture the temporal feature $e^{temp}$ of the trajectory from multiple time scales based on the understanding of the overall continuity of the motion, we employ the multi-scale continuity representation $R_c$ as the query vector to fuse the multi-scale motion representation $R_m$.\n$Q_c = \\eta_Q(R_c, W_c)$ (11)\n$K_m = \\eta_K(R_m, W_K)$\n$V_m = \\eta_V(R_m, W_V)$\n$e^{temp} = softmax(\\frac{Q_c(K_m)^T}{\\sqrt{d_k}})V_m$ (14)\nwhere $\\eta_Q$, $\\eta_K$ and $\\eta_V$ are transformation functions, which are achieved through MLP in our work. $W_Q$, $W_K$ and $W_V$ are their corresponding learnable parameters. The temporal feature $e^{temp}$ contains both detailed information about motion and reflects the overall continuity of motion to the greatest extent possible, and can robustly represent the temporal motion of the vehicle based on the input incomplete trajectory.\nE. Interaction and Prediction\nAs a part of the transportation system, the target vehicle has to interact with the surrounding vehicles to achieve collision avoidance and efficient passage. Therefore, the extraction of inter-vehicle interaction is essential for accurate trajectory prediction. In this work, we model the inter-vehicle interaction with reference to the Global Interaction Module in Vectornet [24]. Ultimately, the future trajectory decoder outputs the predicted trajectory $\\hat{y} = \\{\\hat{y}_{t+T_h+1}, \\hat{y}_{t+T_h+2}, ..., \\hat{y}_{t+T_h+T_f}\\}$ of the target vehicle for the next $T_f$ time steps.\n$e_{i,j} = \\Gamma(\\mathcal{G}(e_i^{temp}), \\mathcal{G}(e_j^{temp}))$, (15)\n$A_{i,j} = \\frac{exp(e_{i,j})}{\\sum_{k \\in \\{tar,1,2,...,N\\}} exp(e_{i,k})}$, (16)\n$V_i = \\sigma(\\sum_{j \\in \\{tar,1,2,...,N\\}} A_{i,j} W e_j^{temp})$, (17)\n$\\hat{y} = P(V_{tar})$, (18)\nwhere $e_i^{temp}$ is the temporal feature of vehicle i, and $V_i$ represents its final encoding after interaction modeling. The symbol $\\measuredangle(\\cdot)$ denotes the computation of the inner product of vectors. P is the future trajectory decoder, which is implemented using LSTM in our work."}, {"title": "IV. EXPERIMENT", "content": "A. Datasets\nIn highway traffic scenarios, the simplicity of road alignment and lower vehicle density contribute to faster vehicle speed, characterized primarily by simple driving behaviors such as acceleration, deceleration, and lane changing. Conversely, within urban traffic scenarios, due to the complex road topology, high vehicle density, and strong interaction, the vehicle speed is slower but there are complex driving behaviors such as turning left, turning right, and U-turn. Considering the aforementioned distinctions, we conducted experiments to validate our proposed method on datasets corresponding to highway traffic scenarios, namely NGSIM [39], [40] and HighD [41], as well as datasets representing urban traffic scenarios, namely Argoverse [9] and IArgoverse.\nHighD: The HighD team uses a drone to collect vehicle trajectory from six different locations on the Germany highway"}, {"title": "V. CONCLUSION", "content": "This paper proposes a novel end-to-end framework named MTFT for incomplete vehicle trajectory prediction in real-world traffic scenarios, which comprises the Multi-scale Attention Head (MAH) and the Continuity Representation-guided Multi-scale Fusion (CRMF) module. The MAH is designed to extract multi-scale motion representation with global dependency from different time granularities, effectively alleviating the negative impact of missing values on prediction. Furthermore, the CRMF module can extract high-level continuity representation of vehicle motion and use it to guide the fusion of multi-scale motion representation. The temporal feature obtained after fusion not only contains detailed information about motion but also reflects the overall trend of vehicle motion to the greatest extent, which facilitates the accurate decoding of future trajectory that is consistent with the vehicle's motion continuity.\nThe future research direction is to explore the positive role of HD maps in the task of incomplete vehicle trajectory prediction. Since the vehicle trajectory generally follows the center of a lane, we attempt to use HD maps as prior knowledge to complement the missing information caused by missing trajectory points, enabling the model to output prediction consistent with the scene in complex urban traffic scenarios."}]}