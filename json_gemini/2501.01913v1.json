{"title": "Mingling with the Good to Backdoor Federated Learning", "authors": ["Nuno Neves"], "abstract": "Federated learning (FL) is a decentralized machine learning technique that allows multiple entities to jointly train a model while preserving dataset privacy. However, its distributed nature has raised various security concerns, which have been addressed by increasingly sophisticated defenses. These protections utilize a range of data sources and metrics to, for example, filter out malicious model updates, ensuring that the impact of attacks is minimized or eliminated.\nThis paper explores the feasibility of designing a generic attack method capable of installing backdoors in FL while evading a diverse array of defenses. Specifically, we focus on an attacker strategy called MIGO, which aims to produce model updates that subtly blend with legitimate ones. The resulting effect is a gradual integration of a backdoor into the global model, often ensuring its persistence long after the attack concludes, while generating enough ambiguity to hinder the effectiveness of defenses.\nMIGO was employed to implant three types of backdoors across five datasets and different model architectures. The results demonstrate the significant threat posed by these backdoors, as MIGO consistently achieved exceptionally high backdoor accuracy (exceeding 90%) while maintaining the utility of the main task. Moreover, MIGO exhibited strong evasion capabilities against ten defenses, including several state-of-the-art methods. When compared to four other attack strategies, MIGO consistently outperformed them across most configurations. Notably, even in extreme scenarios where the attacker controls just 0.1% of the clients, the results indicate that successful backdoor insertion is possible if the attacker can persist for a sufficient number of rounds.", "sections": [{"title": "I. INTRODUCTION", "content": "FL is a distributed learning paradigm that enables model training across multiple devices\u00b9 [1], [2]. This approach effectively addresses privacy concerns, such as those outlined in GDPR [3] and CCPA [4], as it ensures that stored samples remain on the respective devices while enabling collaborative training of a global deep learning model. Moreover, FL enhances model generalization by leveraging decentralized data collection, which often results in a more diverse set of samples. This diversity contributes to better coverage of the input space, ultimately enhancing model performance when deployed in production settings. Hence, FL has found application across a diverse spectrum of tasks, including critical domains. Examples such as autonomous driving [5] and healthcare [6] illustrate its pivotal role, yet numerous mainstream applications also explore its benefits. Platforms"}, {"title": "II. CONTEXT", "content": "A. Federated Learning\nFL is a distributed learning approach where a group of nodes collaborates to train a DNN model without sharing their local datasets [2]. A benefit is that privacy comes from the onset, as samples collected locally never leave the nodes, diminishing concerns over giving access to reserved information.\nIn FL, a server node coordinates the operations, while the rest act as clients doing the training tasks. In the beginning, the server initializes the global model $G_0$ randomly or with the parameter values of a pre-trained model (e.g., when performing fine-tuning). Then, FL proceeds in rounds r, where the global model $G_r$ is trained progressively. In a round, the server starts by choosing a subset of n participating clients $C_r^n$ out of the total group of clients $Cli$ (where $|Cli| = N$). Next, it sends them the current version of the model $G_r$. The clients then use their local datasets $D_i$ to train the model for a few epochs, eventually producing a new local version of the model $L_i^{r+1}$. The datasets normally differ $D_i \\neq D_j$, both in the number of samples and their distribution, as they were collected independently. Consequently, the learned local models also vary because they were built to achieve some specific objective, e.g., to maximize the accuracy of the classification of the local examples. In the end, the clients compute an update to the global model $U_i^{r+1} = L_i^{r+1} - G_r$, and forward it to the server. After receiving all updates, the server uses an aggregation algorithm to generate the next version of the global model $G_{r+1} = Agg(U_1^{r+1}, ..., U_n^{r+1})$ reflecting the various contributions. Several aggregation algorithms have been proposed, but the most often used solution is FedAvg [1], [43], which simply averages the updates to modify the global model (where $\\eta_r$ is the global learning rate)\u00b2:\n$G_{r+1} = G_r + \\eta_r \\sum_{i=1}^n U_i^{r+1}$ (1)\nIn a classification task, a local dataset contains pairs $(x_k, l_k) \\in D_i$, where $x_k$ is an instance from the input space $X$ and $l_k$ is a label in the label space $y$, which is associated to one of the considered classes $C_j \\in C$. For simplicity, we will say that example $x_k$ belongs to class $C_j$ (i.e., $x_k \\in C_j$) if its label $l_k$ equates with $C_j$. As training evolves, one would like the model to yield the mapping $G_r(x_k) = l_k$ with high probability. Similarly, at inference time, the model should predict $G(x_k) = l_k$ for any $x_k \\in X$, and $l_k$ should correspond to the appropriate class.\nTwo main scenarios are being considered in FL. In cross-device, only a small subset of the clients participates in each round (e.g., n = 10 smartphones out of N = 1000 willing to collaborate). The selection procedure uses random choice at its core but normally also takes into consideration a few extra criteria (e.g., the smartphone is connected to a power source). In cross-silo, all clients take part in every round (i.e., n = N). This last scenario is envisioned for situations when a group of institutions want to create an improved model without sharing their datasets. We will perform the MIGO attack in both scenarios, although most experiments will concentrate on cross-device as it is more commonly studied.\nB. Threat Model\nThe threat model assumes an adversary that controls a few nodes running FL clients. All the information in those nodes is available to the attacker, including the DNN model architecture, hyperparameters, training methods, and local datasets $D_i$. The adversary lacks detailed knowledge about the datasets of other clients and is unable to interfere with their"}, {"title": "III. MIGO", "content": "A. Challenges\nBackdoor attacks in FL encounter several significant challenges, such as limited control over the training procedure, the presence of a large number of legitimate clients compared to malicious ones, the requirement to remain stealthy, and the random selection of participating clients. The defenses, such as the ones evaluated in this paper, further complicate these attacks by employing a heterogeneous set of techniques that substantially restrict the actions available to attackers.\n(i) Diversity of the Data Analyzed: Defenses use a variety of evidence to differentiate malicious from legitimate models. This evidence may be derived from selected weights [32], [34] or all model parameters [35], [37], [46]. Alternatively, defenses may analyze model predictions, focusing on the logits [32] or considering all layers outputs [36]. To compute these predictions, inputs can be generated randomly [32] or taken from the current round's clients datasets [31], [36].\n(ii) Multitude of Metrics: Metrics are used to identify anomalies by assessing the (dis)similarity among the collected evidence. These metrics can be standard measures, such as a Lp norm, cosine similarity/distance, or statistical tests like the D-test, as well as specialized metrics like Division Differences [32]. A single metric may be applied [46] or a combination [36]. Additionally, the evidence may undergo pre-processing steps, such as Principal Component Analysis [36] or a frequency domain transformation [37].\n(iii) Control Impact: Aggregation can be performed through simple averaging (as in Eq.1), or the updates can be constrained beforehand by scaling the parameters with a factor less than or equal to one [16], [31], [32], [34], [35], thereby reducing the influence of specific models. Since the number of attacker clients is a minority, this further limits how the adversary affects the global model.\nThe next sections explain how the MIGO strategy addresses the above challenges, starting with the selection of the three types of backdoors that will be implanted. To the best of our knowledge, we are the first to study OUT-backdoors in FL.\nB. Backdoor Types\nOne of the goals of the attacker is to be stealthy, ensuring not only that the model corruption is hard to detect, but also that backdoor instances can be easily obtained at inference time (e.g., by not requiring access to the physical environment where the model is used). For this reason, we focus on three backdoor types that do not require any changes to the inputs, making poisoned data look natural (avoiding, e.g., the addition of pixel pattern triggers [18], [21] or the generation of clean-label instances [23]). Since it is expected that the malicious samples share (many) features with the elements in the benign datasets, inserting backdoors during training will require more subtle changes in the model, making it harder for defenses to distinguish bad from good updates through detailed parameter analyses (e.g., as in [37]).\nMIGO uses data poisoning to add the backdoor. The adversary produces a malicious dataset $D_o^r$ that serves to train the local model while the attack lasts. This dataset contains a mix of correctly labeled samples and backdoor instances. Again, the aim is to assist in dissimulating the attack as a backdoored local model continues to process valid samples as well as the benign counterparts (e.g., relevant with [32], [36]).\nBackdoor types are categorized based on how their instances relate to the distribution of samples within benign datasets:\nIn-distribution backdoor (IN): (also known as label-flipping [33], [47]) The adversary wants to corrupt the global model so that it associates an erroneous label $l_b$ to samples of a certain target class $C_t \\in C$ (e.g., classify as trucks the images of dogs). Here, the backdoor instances are examples from the class being poisoned (e.g., images of dogs), which have their label altered to $l_b$ (e.g., 9, corresponding to trucks).\nIn this setting, two competing groups of clients update the global model during FL training: (1) a large number of legitimate clients with datasets $D_i$, which may include correctly labeled instances of both $C_t$ and $C_l$; (2) a smaller number of poisoned clients with $D_o^r$. This attack is only successful if, even with a continuous stream of correct updates, the bad ones manage to influence the global model to do the malicious mapping.\nEdge-distribution backdoor (EDGE). (known as semantic backdoor [14], [17]) Here, the samples being targeted have characteristics that make them rare or unlikely to be included in the benign train or test datasets. They would be correctly classified under normal circumstances (with label $l_t$), but the adversary tries to change the behavior of the global model so that they are assigned a distinct malicious label $l_b$.\nFor example, the benign datasets may contain images of planes but very few or no examples from Southwest Airlines."}, {"title": "IV. EVALUATION", "content": "This section describes the experimental setup and discusses the MIGO results for three scenarios. The first evaluates various aspects of MIGO under normal FL operation; the second investigates the behavior of MIGO when FL is protected with ten different defenses; lastly, we compare MIGO with four other backdoor insertion strategies.\nA. Experimental setup\nWe organize the experiments following a similar approach as prior work to facilitate comparisons [12], [14], [32], [37].\nMetrics: The following metrics are used to evaluate MIGO:\nBackdoor Accuracy (BackAcc). In classification tasks, this metric represents the percentage of backdoor instances correctly classified with the backdoor label. In word prediction tasks, it measures the percentage of predicted words that belong to a set of backdoor words. In both cases, the objective is to gradually increase this metric value as the attack progresses."}, {"title": "V. RELATED WORK", "content": "This section reviews the literature on attacks and defenses in federated learning, with an emphasis on backdoor attacks.\na) Attacks.: Several attack methods have been developed for FL, and sometimes they may be combined to maximize impact. Some of these attacks can target the overall inference capability of the global model when training concludes (untargeted poison attacks) or change the model's behavior for specific classes (targeted poison attacks).\nModel/Data poisoning: With model-poisoning, the adversary manipulates the client-side training process by altering hyperparameters, modifying the loss function [13], [14], or directly changing model updates [10], [11], [54]. With data poisoning, the attacker changes the training data on a subset of clients, altering datasets to achieve specific goals, such as reducing accuracy for particular classes [9] or forcing the model to output a chosen label when a trigger is present in the input [18]. Data poisoning may be easier to execute than model poisoning, as it only requires compromising the device's storage rather than the FL procedure itself. Moreover, detection methods based on example inspection are challenging to implement in FL, as clients are not required to share their data.\nBackdoor attacks: Backdoor attacks aim to alter the global model's behavior for specific inputs or classes [12]-[18], making them targeted attacks. They can be introduced into the global model through various strategies. For example, a global backdoor trigger can be split into distinct patterns, each embedded in the local datasets of different malicious participants [18]. This approach ensures that the global model will respond as intended by the adversary when it encounters the complete trigger pattern. Alternatively, attackers can inject edge-case backdoors [14], [17], causing the model to misclassify examples on the tail end of the input distribution. In a model replacement attack, the adversary attempts to substitute the global model entirely with a malicious local update (as in MRepl [14]), exploiting the assumption that, as the global model converges, local (benign) updates start to cancel out. This cancellation creates an opening for a boosted (malicious) update to take over the global model. Boosting can also be combined with other adversarial goals, such as stealth, by framing it as an optimization problem solved during local training [13], [55]. Furthermore, local training can be tuned to ensure updates remain close to the global model (as in BackProj [16]) or to ensure gradients align within the bottom-K% of benign coordinates (as in Neurotoxin [12]). The attack can also be made adaptive, employing various countermeasures to evade particular defenses in a black-box setting (as in 3DFed [15]).\nMIGO aims to generate malicious updates that introduce sufficient ambiguity, making it challenging for defenses to reliably distinguish between legitimate and malicious local models. This strategy is implemented through several techniques, such as choosing backdoor types that closely resemble normal examples and incrementally introducing small updates to the global model.\nb) Protections.: Defenses in machine learning models typically focus on filtering attacks and/or mitigating their impact. Although the literature has attempted to categorize defenses, achieving this goal is challenging due to the fact that state-of-the-art safeguards often integrate multiple approaches.\nRobust aggregation: FedAvg uses the mean as the statistical operator to aggregate client updates, making it susceptible to outliers. One way to address this limitation is to use other operators, such as median or geometric-median [56]. Trimmed-mean is another alternative, which filters extreme values below and above the data distribution by a certain percentage and then calculates the mean of the remaining"}, {"title": "VI. CONCLUSIONS", "content": "Our research presents MIGO, an innovative method for embedding backdoors into machine learning models trained in FL. MIGO strategically selects backdoor types with inputs that naturally blend with legitimate examples. Backdoors are integrated progressively throughout the training process, enabling the generation of malicious model updates that closely resemble benign ones. MIGO successfully implanted distinct types of backdoors, even in the presence of robust and diverse FL defenses. These results highlight the substantial threat posed by this attack, which achieves high backdoor accuracy while preserving the utility of the main task."}]}