{"title": "Federated LLMs Fine-tuned with Adaptive Importance-Aware LoRA", "authors": ["Yang Su", "Na Yan", "Yansha Deng"], "abstract": "Federated fine-tuning of pre-trained Large Language Models (LLMs) enables task-specific adaptation across diverse datasets while preserving data privacy. However, the large model size and heterogeneity in client resources pose significant computational and communication challenges. To address these issues, in this paper, we propose a novel Heterogeneous Adaptive Federated Low-Rank Adaptation (LoRA) fine-tuned LLM framework (HAFL). To accommodate client resource heterogeneity, we first introduce an importance-based parameter truncation scheme, which allows clients to have different LoRA ranks, and smoothed sensitivity scores are used as importance indicators. Despite its flexibility, the truncation process may cause performance degradation. To tackle this problem, we develop an importance-based parameter freezing scheme. In this approach, both the cloud server and clients maintain the same LoRA rank, while clients selectively update only the most important decomposed LORA rank-1 matrices, keeping the rest frozen. To mitigate the information dilution caused by the zero-padding aggregation method, we propose an adaptive aggregation approach that operates at the decomposed rank-1 matrix level. Experiments on the 20 News Group classification task show that our method converges quickly with low communication size, and avoids performance degradation when distributing models to clients compared to truncation-based heterogeneous LoRA rank scheme. Additionally, our adaptive aggregation method achieves faster convergence compared to the zero-padding approach.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have exhibited exceptional performance in understanding and generating natural language across a wide range of tasks, including applications in chatbots and search engines [1]. To achieve optimal performance on specific tasks, these pre-trained models often require further fine-tuning. However, conventional fine-tuning methods are typically centralized, involving the collection of raw data on a single client, which raises significant privacy concerns. To address this issue, Federated Learning (FL) [2] has emerged as a promising solution by enabling collaborative model training across decentralized LLM agents, where only model weights are shared instead of raw data, thereby preserving data privacy. Integrating FL with LLMs allows for leveraging diverse data sources, resulting in more robust and generalized models.\nAs FL increasingly relies on wireless networks to support collaboration across clients, communication has emerged as a critical bottleneck in fine-tuning LLMs. Unlike centralized settings, FL requires frequent transmission of model updates over limited-bandwidth connections, posing significant challenges"}, {"title": "II. SYSTEM MODEL", "content": "As shown in Fig. 1, our proposed HAFL framework consists of a cloud server and a set of N clients, denoted as\n\\(N = \\{1, 2, ..., N\\}\\). Each client and the cloud server have a LoRA based LLM, where the parameters of the original pre-trained LLM are frozen, and only the incorporated low-rank decomposition matrices are trainable. Each client \\(k \\in N\\) has a local dataset \\(D_k = \\{(x_i, y_i)\\}_{i=1}^{P_k}\\), where \\(x_i\\) is the i-th input data sample, \\(y_i\\) is the corresponding labeled output, and \\(|D_k|\\) is the number of data samples. Considering the limited communication resource, in each communication round, a subset of clients, K of size is randomly selected from the entire set of clients N."}, {"title": "A. Cloud LORA Parameters Broadcast", "content": "The model updates of the LoRA based LLM at the cloud server and the clients can be expressed as\n\\(W = W_{pre} + \\Delta W = W_{pre} + BA,\n(1)\nwhere \\(W_{pre} \\in \\mathbb{R}^{d \\times l}\\) denotes the fixed pre-trained model weights, and \\(\\Delta W \\in \\mathbb{R}^{d \\times l}\\) is the trainable matrix. The low-rank decomposition of \\(\\Delta W\\) is given by \\(\\Delta W = BA\\), where \\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times l}\\) are the low-rank matrices with rank r. Here, d and l are the dimensions of the model's weight matrix, and r is the rank of the low-rank approximation, which is typically much smaller than both d and l.\nFor different clients k, the LoRA rank is denoted as \\(r_k\\). In our importance-based parameter truncation scheme, the client LoRA rank \\(r_k\\) is selected from the range \\([r_{min}, r_{max}]\\). In our importance-based parameter freezing scheme, the rank \\(r_k\\) is set to be equal to \\(r_{max}\\). Regardless of the scheme used, the"}, {"title": "B. Local Model Training and Global Aggregation", "content": "In each communication round, the selected set of clients K receives the LoRA parameters \\(\\Theta_{LORA}^{(t)}\\) broadcasted by the cloud server. Each client \\(k \\in K\\) then begins local model training using its local dataset \\(D_k\\). In this process, clients keep the pre-trained model weights \\(W_{pre}\\) frozen. They initialize their local LoRA parameters \\(B_k\\) and \\(A_k\\) with the received \\(\\Theta_{LORA}^{(t)}\\) and optimize these parameters by minimizing a local loss function.\nAfter completing local training, clients upload their updated LORA parameters \\(\\{B_k^{(t+1)}, A_k^{(t+1)}\\}\\) to the cloud server. Upon receiving updates from all clients, the cloud server aggregates these parameters to update the global LoRA parameters \\(\\Theta_{LORA}^{(t+1)}\\), preparing for the next communication round. This approach allows the HAFL framework to effectively optimize the model using distributed data while preserving data privacy."}, {"title": "III. IMPORTANCE-BASED PARAMETERS TRUNCATION AND FREEZING SCHEMES", "content": "In this section, to handle the heterogeneity of client resources, we first introduce an importance-based parameter truncation scheme at the local LLM. To address the issue of performance degradation resulting from the truncation scheme, we subsequently introduce an importance-based parameter freezing scheme at the local LLM."}, {"title": "A. Decomposed LoRA Rank-1 Matrices Importance Evaluation", "content": "In FL environments, clients often have different computational capacities, making it challenging to train all clients' LORA parameters at a uniform rank across all clients. To address this, the LoRA parameter product BA will be de-composed into smaller, rank-1 matrices. As shown in Fig. 1, we take the \\(LoRA_{rg} = 3\\) as an example. The cloud server calculates an importance score for each rank-1 matrix and broadcasts this information to the clients. Clients can then prioritize these rank-1 matrices, training the most important ones first."}, {"title": "1) Matrix Decomposition", "content": "The cloud server will decompose the product \\(B_gA_g\\) into rank-1 matrices as\n\\(B_gA_g = \\sum_{i=1}^{r} b_i a_i,\n(3)\nwhere \\(b_i \\in \\mathbb{R}^{d \\times 1}\\) is the ith column vector of \\(B_g\\), and \\(a_i \\in \\mathbb{R}^{1 \\times l}\\) is the ith row vector of \\(A_g\\)."}, {"title": "2) Element-wise Importance Calculation", "content": "To assess the significance of each element within the vectors \\(b_i\\) and \\(a_i\\), we employ a sensitivity-based metric [10]. This involves computing the gradient of the loss function L with respect to each element and using the product of the element's value and its gradient as a measure of importance. However, in the context of FL, the global model does not have a direct loss function L to calculate the gradient. Instead, we approximate the gradient by scaling the change in parameter values between consecutive communication rounds using the learning rate. Then, we define a unified importance function \\(I(w_{ij})\\) for any trainable parameter \\(w_{ij}\\) as follows:\n\\(I(w_{ij}) = w_{ij} \\frac{\\Delta w_{ij}}{\\eta}\n(4)\nwhere \\(w_{ij}\\) represents each element \\(b_{ji}\\) in \\(b_i\\), with \\(j \\in \\{1,...,d\\}\\), or each element \\(a_{iq}\\) in \\(a_i\\), with \\(q \\in \\{1, ...,l\\}\\). The term \\(\\Delta w_{ij} = w_{ij}^{(t)} - w_{ij}^{(t-1)}\\) represents the change in the parameter value between two consecutive global updates, and \\(\\eta\\) is the learning rate.\nHowever, due to the high variability and uncertainty in estimating sensitivity from mini-batch samples, it is crucial to smooth these importance scores to obtain more reliable indicators. We achieve this by applying an exponential moving average to the sensitivity scores and quantifying the uncertainty [11].\nFirst, the smoothed sensitivity \\(\\bar{I}^{(t)}(w_{ij})\\) is calculated using an exponential moving average:\n\\(\\bar{I}^{(t)}(w_{ij}) = \\beta_1 \\bar{I}^{(t-1)}(w_{ij}) + (1 - \\beta_1)I^{(t)}(w_{ij}),\n(5)\nwhere \\(\\beta_1\\) is a smoothing factor between 0 and 1.\nNext, the uncertainty \\(\\bar{U}^{(t)}(w_{ij})\\) is quantified to capture the variability in the sensitivity scores:\n\\(\\bar{U}^{(t)}(w_{ij}) = \\beta_2 \\bar{U}^{(t-1)}(w_{ij}) + (1-\\beta_2) \\mid I^{(t)}(w_{ij}) - \\bar{I}^{(t)}(w_{ij}) \\mid,\n(6)\nwhere \\(\\beta_2\\) is another smoothing factor between 0 and 1.\nFinally, the combined importance score \\(s^{(t)}(w_{ij})\\) is defined as the product of the smoothed sensitivity and the uncertainty:\n\\(s^{(t)}(w_{ij}) = \\bar{I}^{(t)}(w_{ij}) \\cdot \\bar{U}^{(t)}(w_{ij}).\n(7)\nBy incorporating these smoothing techniques, we enhance the reliability of the importance scores, making them more robust to the stochasticity in FL environments."}, {"title": "3) Aggregation of Importance Scores", "content": "To obtain the overall importance score for the rank-1 matrix \\(b_ia_i\\), we sum the smoothed and uncertainty adjusted importance scores for all elements in \\(b_i\\) and \\(a_i\\). This approach ensures that the aggregated score reflects the reliability and significance of each parameter. The overall importance score \\(S_i\\) of rank-1 matrix is calculated using:\n\\(S_i = \\sum_{j=1}^{d} s(b_{ji}) + \\sum_{q=1}^{l} s(a_{iq}),\n(8)\nwhere \\(s(b_{ji})\\) and \\(s(a_{iq})\\) are the smoothed and uncertainty-adjusted importance scores for the elements of \\(b_i\\) and \\(a_i\\), respectively. This aggregation method provides a robust measure of the importance of each rank-1 matrix in the decomposition of BA."}, {"title": "4) Broadcast Importance Scores", "content": "After calculating the importance scores S for each rank-1 matrix, we obtain a list of scores \\([S_1, S_2, ..., S_r]\\). This list represents the importance of each rank-1 matrix in the decomposition of BA. The list of importance scores is then broadcast to all clients."}, {"title": "B. Importance-based Parameters Truncation Scheme", "content": "To accommodate diverse computational capabilities among clients, we allow each client to have a different LoRA rank \\(r_k \\in [r_{min}, r_{max}]\\). However, this flexibility introduces the challenge of aligning the cloud server's global model with the different LoRA ranks of each client's local model. To address this issue, we propose an importance-based parameter truncation scheme."}, {"title": "1) Truncation Process", "content": "In each communication round t, the cloud server broadcasts the LORA parameters \\(\\Theta_{LORA}^{(t)}\\) to all clients. Upon receiving these parameters, each client \\(k \\in K\\) performs the following steps.\na) Selection of Significant rank-1 matrices: Each client \\(k \\in K\\) receives the list of importance scores \\([S_1, S_2, ..., S_r]\\) from the server. Based on these scores, client k selects the top \\(r_k\\) rank-1 matrices as\n\\(I_k = top_k ([S_1, S_2, ..., S_r], r_k),\n(9)\nwhere \\(I_k\\) represents the set of indices corresponding to the rank-1 matrices selected by client k.\nb) Truncation of Low-Rank Matrices: Client k then truncates the low-rank matrices \\(B_g\\) and \\(A_g\\) to align with the local model's LoRA rank requirements. Specifically, the client retains \\(r_k\\) column vectors \\(b_i\\) and \\(r_k\\) row vectors \\(a_i\\) corresponding to the highest important scores:\n\\(B_k = B_g[:, I_k], A_k = B_g[I_k, :].\n(10)"}, {"title": "2) Local Model Update", "content": "The truncated low-rank matrices \\(B_k\\) and \\(A_k\\) will be copied to the LoRA parameters of client k's local model. Subsequently, client k trains the local model using its local dataset \\(D_k\\). In the parameters truncation scheme, the local loss function for each client k is defined as\n\\(L_k (B_k, A_k) = \\frac{1}{|D_k|} \\sum_{\\xi \\in D_k} l(f(B_kA_k), W_{pre}) + \\frac{\\lambda}{2} (||B_k||^2 + ||A_k||^2),\n(11)\nwhere \\(|D_k|\\) denotes the size of this dataset. The function l measures the model's performance on a data sample \\(\\xi\\). The parameters \\(B_k\\) and \\(A_k\\) are the unfrozen LoRA rank-1 matrices that are being optimized. The term \\(\\lambda\\) is the weight decay coefficient, which is used to prevent overfitting by penalizing large parameter values. The regularization term \\((||B_k ||^2 + || A_k ||^2)\\) incorporates the L2 norm, || . ||, to measure the magnitude of the parameters, thereby encouraging smaller parameter values."}, {"title": "C. Importance-based Parameters Freezing Scheme", "content": "The truncation process introduced above will inevitably result in performance degradation when distributing the model to clients. To address this issue, we propose a novel importance-based parameters freezing scheme, where both local models and the global model utilize the same maximum LoRA rank, denoted as \\(r_k = r_g = r_{max}\\). To accommodate client resource constraints, clients will freeze a portion of the LoRA parameters based on their importance. We denote the freezing ratio as \\(\\alpha_k\\), which represents the proportion of LoRA parameters that are frozen relative to the total parameters."}, {"title": "1) Freezing Process", "content": "In the HAFL framework, each client updates its local LoRA parameters by selectively training a subset of the parameters based on the importance scores received from the cloud server. All LoRA parameters Lora = \\(\\{B_g, A_g\\}\\) broadcasted by the server are replicated to the client's local model, but only the most important rank-1 matrices are unfrozen and optimized by the client's local dataset. Based on the freezing ratio \\(\\alpha_k\\) of each client k, \\((1 - \\alpha_k)r_{max}\\) rank-1 matrices will be fine-tuned.\na) Selection of Significant rank-1 matrices: Each client \\(k \\in K\\) receives the list of importance scores \\([S_1, S_2, ..., S_r]\\) from the server. Based on these scores, client k selects the top \\((1 - \\alpha_k)r_{max}\\) rank-1 matrices to actively train, where \\(\\alpha_k\\) can vary between clients depending on their computational capacity and resource availability. The selection is performed by choosing the indices corresponding to the highest importance scores:\n\\(I_k = top_k ([S_1, S_2, ..., S_r], (1 - \\alpha_k)r_{max}),\n(12)\nwhere \\(I_k\\) is the set of indices of the selected rank-1 matrices for client k. The remaining rank-1 matrices' indices are denoted as \\(U_k = \\{1, 2, ..., r_{max}\\} \\setminus I_k\\).\n2) Local Model Update: Each client k updates its local model by minimizing a loss function \\(L_k\\) over its dataset \\(D_k\\). The loss function is defined as\n\\(L_k (B_k, A_k) = \\frac{1}{|D_k|} \\sum_{\\xi \\in D_k} l(f(B_k^{I_k} A_k^{I_k}), W_{pre}, \\xi) + \\frac{\\lambda}{2} (||B_k^{I_k}||^2 + ||A_k^{I_k}||^2),\n(13)\nwhere |D_k| represents the size of this dataset, the loss function l evaluates the model's performance on a data sample \\(\\xi\\). The matices \\(B_k^{I_k} = B_k[:, I_k]\\) and \\(A_k^{I_k} = A_k[I_k, :]\\) represent the trained LoRA parameter rank-1 matrices. Additionally, \\(B_k^{U_k} = B_k[:, U_k]\\) and \\(A_k^{U_k} = A_k[U_k, :]\\) are the untrained frozen LoRA parameter rank-1 matrices.\nIn the importance-based parameters freezing scheme, clients receive LoRA parameters \\(\\{B_g, A_g\\}\\) from the cloud server and replicate all of them to their local LoRA rank-1 matrices. However, only the rank-1 matrices indexed by \\(I_k\\) are trained and optimized, while those indexed by \\(U_k\\) remain frozen during local training."}, {"title": "D. Adaptive Global Aggregation at the Cloud", "content": "In the HAFL framework, after completing local training, each client uploads its updated LoRA parameters to the cloud server. These parameters are then aggregated to form a new global model. For the truncation-based scheme, the upload LoRA parameters are \\(B_k\\) and \\(A_k\\). In contrast, for the importance-based parameter freezing scheme, the upload LoRA parameters are \\(B_k^{I_k} = B_k[:, I_k]\\) and \\(A_k^{I_k} = A_k[I_k, :]\\).\na) Adaptive Aggregation of LoRA Parameters: The adaptive aggregation is shown in Algorithm 1, rather than aggregating LoRA parameters at the scale of the entire LORA matrices, we perform aggregation at the decomposed rank-1 matrix level. For each rank-1 matrix, only the clients that update and upload that rank-1 matrix will participate in the aggregation process. Each client k contributes to the global model based on a sparsity-based norm \\(z_k\\), defined as\n\\(z_k = \\begin{cases}||B_kA_k||_F, & \\text{Truncation scheme} \\\\\\ |B_k^{I_k} A_k^{I_k}||_F, & \\text{Freezing scheme} \\end{cases}\n(14)\nwhere \\(||.||_F\\) denotes the Frobenius norm. The total norm list Z has a length of \\(r_g\\), and Z[j] accumulates the \\(z_k\\) values from all clients that have updated the jth rank-1 matrix. This list is then used to normalize \\(z_k\\) to obtain the contribution weight for each rank-1 matrix."}, {"title": "IV. NUMERICAL RESULTS", "content": "In our experiments, we utilize GPT2-Large [12] as the backbone model and test on the 20 News Group classification"}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a HAFL framework to address privacy concerns in fine-tuning LLMs. To accommodate client resource heterogeneity, we first introduced an importance-based parameter truncation scheme at the local LLM. However, the truncation process inevitably results in performance degradation. To tackle this, we further developed an importance-based parameter frozen scheme at the local LLM. At the cloud server, we proposed an adaptive global model aggregation method to counter the information dilution problem caused by the zero-padding aggregation method. Our experimental results demonstrate that our method achieves rapid convergence with low communication overhead. Compared to the truncation approach, our importance-based partial freezing scheme maintains client model performance during global model parameter distribution. Furthermore, the adaptive aggregation method achieves faster convergence compared to the zero-padding approach."}]}