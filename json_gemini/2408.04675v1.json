{"title": "ACL Ready: RAG Based Assistant for the ACL Checklist", "authors": ["Michael Galarnyk", "Rutwik Routu", "Kosha Bheda", "Priyanshu Mehta", "Agam Shah", "Sudheer Chava"], "abstract": "The ARR Responsible NLP Research checklist website states that the \"checklist is designed to encourage best practices for responsible research, addressing issues of research ethics, societal impact and reproducibility.\" Answering the questions is an opportunity for authors to reflect on their work and make sure any shared scientific assets follow best practices. Ideally, considering the checklist before submission can favorably impact the writing of a research paper. However, the checklist is often filled out at the last moment. In this work, we introduce ACLReady, a retrieval-augmented language model application that can be used to empower authors to reflect on their work and assist authors with the ACL checklist. To test the effectiveness of the system, we conducted a qualitative study with 13 users which shows that 92% of users found the application useful and easy to use as well as 77% of the users found that the application provided the information they expected. Our code is publicly available under the CC BY-NC 4.0 license on GitHub.", "sections": [{"title": "1 Introduction", "content": "In order to submit an ACL paper, authors are required to submit their answers to the ARR Responsible NLP Research checklist. The checklist was mostly developed through a combination of the NLP Reproducibility Checklist (Dodge et al., 2019), the reproducible data checklist (Rogers et al., 2021), and the NeurIPS 2021 Paper Checklist Guidelines (neu, 2021). The goal of this process is to address reproducibility, societal impact, and potential ethical issues in the research work. This means researchers should discuss things like the limitations/risks of their research, scientific artifact usage, relevant details about computational experiments, annotators/human participants, and whether the authors used AI assistants in their writing."}, {"title": "2 ACL Ready", "content": "The checklist consists of up to 19 questions about the paper. For example, question A2 is the following: \"Did you discuss any potential risks of your work?\" If the answer is yes, the authors must provide the section number where the risks are discussed. If the answer is no, authors need to provide a reasonable justification. However, despite the checklist's importance, authors often fail to give each of the questions the careful consideration they deserve due to constraints such as a lack of time. One way to approach this challenge is to give users a question answering assistant.\nLarge Language Models (LLMs) like GPT-4 (OpenAI, 2023a) and Llama-3 (Touvron et al., 2023) have shown to be good at the question answering and generation tasks. To enhance their capabilities, Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) integrates information retrieval with generative models. When a question is posed, RAG first searches a large corpus for relevant text, which can be used by the generative model to produce informed responses. This approach improves accuracy and relevance, especially for question-answering tasks requiring up-to-date or domain-specific knowledge (Karpukhin et al., 2020).\nIn this work, we introduce ACLReady, a retrieval-augmented language model based application that can be used to empower authors to reflect on their work and assist authors with the ACL checklist. We also present a qualitative user study that demonstrates the efficacy of the application. The main contributions of our work are as follows:\n\u2022 Checklist Assistant: ACLReady can be used to assist users with the checklist questions, providing a way for authors to reflect on their work and give more thoughtful responses.\n\u2022 User-Friendly Tool: The user interface has undergone user testing and has been updated with user feedback, making the application easy to navigate.\n\u2022 Modular Design: ACLReady is a simple application that can be configured to use a wide variety of LLMs or even forked for similar checklist applications.\nFor a video demonstration of ACLReady, please visit https://youtu.be/_V0OV2E90FY."}, {"title": "2.1 Parsing, Chunking, and Embedding", "content": "The ACLReady tool depicted in Figure 1, operates as follows: (1) the user uploads a TeX file, (2) the file is chunked by section, (3) each section is semantically chunked, (4) metadata is added and text is embedded, (5) filtering, prompting, and querying occur, (6) LLM-generated checklist responses are sent to the frontend, and (7) the user modifies and exports the responses.\nParsing After users upload their paper's TeX file, the document is parsed to remove all comments and all text before the abstract. Additionally, sections like acknowledgments are removed. For figures and tables, only captions are kept. Finally, sections are numbered in order to mimic the section numbering that tools like overleaf.com perform when compiling from LaTeX to PDF.\nMaintaining relationships during chunking In order to best utilize the original structure of the TeX document while making it easier for the LLM to distinguish between sections, the chunking process is as follows:\n1. Every section is chunked into its own node.\n2. Metadata is added (section name, previous node, next node). This also makes it easier to filter out irrelevant nodes for some prompts.\n3. Section nodes are broken up into parent and child chunks by semantic chunking\u00b9. This chunking method takes embeddings of sentences and finds breakpoints between sequential sentences using embedding similarity.\nEmbeddings The text is embedded. When the application is configured for OpenAI models, the default embedding is \"text-embedding-ada-002\". The application can also be configured to use the open source embedding \"m2-bert-80M-8k-retrieval\". Nodes that are not relevant for a specific query can be filtered. For instance, for question A3 (\"Do the abstract and introduction summarize the paper's main claims\") all nodes that are not parent or child nodes of the abstract and introduction sections can be excluded. The app uses recursive retrieval with cosine similarity as the similarity metric. Queries retrieve the smaller child chunks and follow references to the parent chunks. The parent chunks are fed into the LLM.\nACLReady has been evaluated with leading LLMs like GPT-3.5 Turbo (\"gpt-3.5-turbo-0613\"). Due to the model being proprietary and not open-source, we built in a configuration for the application that allows the user the flexibility of selecting the LLM they want to use. Currently, the only open-source option that has been tested with the"}, {"title": "2.2 LLM Checklist Response", "content": "After inference, the LLM checklist response is sent to the frontend. This response is formatted and added to the corresponding sections (A-D) in the user interface. If the answer to the question is \"yes\", the response is formatted as \"section name\". If the answer to the question is \"no\", the response is formatted as \"None. LLM Generated Justification\".\nUser Checklist Modification The LLM answers are supposed to assist users with understanding their paper and simplifying the response process. Consequently, users should check each LLM generated answer for accuracy. Section E which deals with the use of AI assistants in research, coding, or writing is only to be answered by users.\nOnce the user is satisfied with the answers they can export the response to a markdown document. Markdown was chosen due to how easy it is to convert from markdown to other formats (e.g., PDF and LaTeX) and the widespread adoption of README markdown files on GitHub and model cards on Hugging Face (Yang et al., 2024)."}, {"title": "2.3 Implementation Details", "content": "This section details the technical details of the web application and how the frontend and backend are built."}, {"title": "3 User Interface and Experience Design", "content": "The user interface of ACLReady is shown in Figure 3. It consists of a upload function, a primary and secondary navigation to switch between sections, and a generated response field with a copy function.\nThe ACLReady user journey is shown in Figure 4. Users upload a TeX file, view the progress screen while they wait for the LLM response to generate, check/edit the response, and finally download a markdown file. The features for the platform and the rationale behind them are listed below:\n1. Side Bar/Upload: The upload function only allows users to upload their paper in TeX format. The side bar incorporates the visual identity of the platform. It has been visualized to resemble file tabs so that the users can connect with the overarching action being performed through the platform.\n2. Progress screen: After the user uploads their file, a progress screen appears to users on the backend progress. This feature was added to the platform after informal interviews where it was noted that users wanted to get some indication on how long they needed to wait for the document to be parsed and see results.\n3. Primary navigation: The top bar of the interface provides the users with functionality of switching between sections. It also indicates the progress for each section.\n4. Response sections: After the paper is parsed, the responses will be auto-filled into the response spaces. Here, we have provided a copy button which allows the users to copy the entire response for multiple use cases. These include: wanting to repurpose the generated response in another section or even sharing individual responses with others. In the first iteration of this platform, we had included an edit response button as well but chose not to retain it as it seemed more intuitive for the users to be able to edit by simply clicking on the generated text.\n5. Secondary navigation: The bottom of the platform also provides the user with linear navigation between sections. This secondary navigation aims to maintain the user's workflow while checking or editing responses, allowing movement to the next page without needing to return to the primary navigation at the top of the user interface.\n6. Export: After the user has reviewed each section, they can download all of their responses, which are then ready for submission."}, {"title": "4 Evaluation", "content": "We conducted a user study with 13 evaluators. Each evaluator used the tool and checked the 18 LLM vs human responses (sections A-D) for a randomly selected paper from the list of accepted 2023 EMNLP main conference papers. The only selection criteria was that each selected paper had to have an arxiv version so that the LLM responses could be generated from the TeX source. The human checklist responses were obtained from the ACL Anthology version of the paper.\nThe goal of this study was to qualitatively assess the user experience and utility of the app. A quantitative evaluation that compared accuracy of the LLM responses was not feasible due to frequent differences between the arxiv and the ACL Anthology versions of the paper.\nEvaluator Background Evaluation was performed independently by 13 different evaluators. 12 evaluators were graduate students and 1 was an undergraduate student (8% female, 92% male) in NLP. 85% of the participants are between the ages of 18-29, and 15% are between the ages of 30-39.\nTime A key aspect of the application is its ability to save users time. Consequently, the app logs and records the time taken to analyze each paper in the markdown file users output. In the user study, it took an average of 44 seconds from file upload to LLM-generated responses."}, {"title": "4.1 Qualitative Evaluation", "content": "In order to understand the user experience of the tool and know what to further improve, we had the users answer the following questions after using the tool.\n\u2022 Is the tool easy to use?\n\u2022 Did you find the application useful?\n\u2022 Did the tool provide the information you were expecting?\nFigure 5 presents the evaluators' responses to the questions. To help users better assess the LLM responses compared to human checklist answers, we required the LLM to provide justifications for both \"yes\" and \"no\" answers, rather than only for \"no\" answers.\n92% of users agreed or strongly agreed that the application is useful and easy to use. 77% of the users agreed or strongly agreed that the application provided the information they expected. The 23% of users that were neutral or didn't agree on the application providing the information they expected expressed that the application should give more specific answer justification."}, {"title": "5 Conclusion", "content": "This paper introduces ACLReady, a LLM-based system which can be used to empower authors reflect on their work and act as a assistant to help authors with the ACL checklist. With ACLReady, authors can get a LLM checklist response that they use to reflect on their work or modify before submitting. The application was tested using 13 evaluators who evaluated it on a qualitative level. It demonstrates that the application is easy to use, useful, and provides the expected information. We hope that the open-source application will be responsibly used as an assistant and tool for reflection."}, {"title": "Limitations", "content": "ACL Checklist The current version of the app only addresses the ACL checklist. However, the prompts could be modified for other conferences and applications.\nMulti-answer Some authors often provide a list of sections even when questions are only asking for a single section. The application currently doesn't mimic this behavior well.\nUser Study We only selected papers that have been accepted by EMNLP and have been published on arxiv. This likely biased our user study and RAG model design towards better structured papers."}, {"title": "Ethics Statement", "content": "Hallucination in LLMs Large language models are known to hallucinate and generate false or misleading information. For our application, it means the model can output incorrect sections. Users of our prototype application must only use it as a assistant or as a way to reflect on their work, not as a tool for automation."}]}