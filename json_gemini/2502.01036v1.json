{"title": "EAGLE: EARLY APPROXIMATED-GRADIENT-BASED LEARNING RATE ESTIMATOR", "authors": ["Takumi Fujimoto", "Hiroaki Nishi"], "abstract": "We propose EAGLE update rule, a novel optimization method that accelerates loss convergence during the early stages of training by leveraging both current and previous step parameter and gradient values. The update algorithm estimates optimal parameters by computing the changes in parameters and gradients between consecutive training steps and leveraging the local curvature of the loss landscape derived from these changes. However, this update rule has potential instability, and to address that, we introduce an adaptive switching mechanism that dynamically selects between Adam and EAGLE update rules to enhance training stability. Experiments on standard benchmark datasets demonstrate that EAGLE optimizer, which combines this novel update rule with the switching mechanism achieves rapid training loss convergence with fewer epochs, compared to conventional optimization methods.", "sections": [{"title": "INTRODUCTION", "content": null}, {"title": "BACKGROUND", "content": "Deep learning has been widely adopted in various fields, including computer vision and natural language processing. In computer vision, the evolution of Convolutional Neural Network (CNN) [1] has achieved near-human recognition accuracy in tasks such as image classification and object detection. In natural language processing, while early approaches utilized Recurrent Neural Network (RNN) [2] and Long Short-Term Memory (LSTM) [3] networks\u2014which addressed the long-term dependency problem, Transformers [4] have become the dominant architecture in recent years. Transformers enable parallel computation and serve as the foundation for large language models such as Bidirectional Encoder Representations from Transformers (BERT) [5] and the GPT series [6], which demonstrate human-comparable capabilities in text generation and language understanding.\nThe performance of these deep learning models is quantitatively evaluated using task- specific metrics. superior models demonstrate high performance on these metrics, specifically showing minimal loss (error) between predicted and true values. Conversely, models with insufficient performance exhibit low prediction reliability, making practical applications challenging. Thus, obtaining high-performance models remains a crucial challenge in deep learning.\nEssential to achieving high-performance models is the design of appropriate training processes. Model training is executed as an iterative process consisting of the following steps:"}, {"title": null, "content": "(i) Input data into the neural network to generate predictions\n(ii) Calculate loss by inputting predicted and true values into a loss function (or error function)\n(iii) Update model parameters to minimize the loss\n(iv) Repeat the above steps until convergence criteria are met\nParameter optimization in deep learning models involves searching for parameter values that minimize loss through this process. The final model performance is determined by the parameter values obtained at the completion of training. While identifying optimal parameters would be straightforward if we could obtain the loss landscape for all parameters, modern deep learning models contain millions to billions of parameters, making complete loss landscape analysis computationally infeasible.\nOptimizers in deep learning are algorithms designed to efficiently optimize parameters for such opaque loss functions. Efficient parameter updates by optimizers enhance model training efficiency, enabling faster model development and reduced computational resource requirements. The focus and objectives of optimizers have evolved over time. Early research primarily adopted simple optimization techniques such as Gradient Descent (GD) [7], which relied solely on gradient information. While theoretically straightforward, this approach faced challenges in convergence speed and computational efficiency, leading to the development of Stochastic Gradient Descent (SGD) [8] and the introduction of Momentum [9]. Subsequently, research focus shifted to adaptive learning rate adjustment. RMSprop [10] introduced mechanisms for adaptively adjusting learning rates for individual parameters, achieving efficient training. Notably, Adam [11] established a method combining the advantages of adaptive learning rate adjustment and momentum, demonstrating stable convergence generalization across various tasks. Recent trends, driven by the popularity of large-scale models, emphasize memory efficiency and computational cost reduction. Adafactor [12] reduces memory usage through low-rank approximation of second moments, while Lion [13] achieves computational cost reduction using update rules based solely on gradient sign information.\nThus, optimizer research has evolved from simple gradient information utilization to stable convergence through adaptive learning rate adjustment, and further to computational efficiency improvement, reflecting shifts in focus and objectives."}, {"title": "OBJECTIVES AND OVERVIEW OF THE PROPOSED METHOD", "content": "As discussed in Section 1.1, research on optimizers in deep learning has progressed under various objectives, including convergence stability, computational efficiency, and memory efficiency. In this study, we propose EAGLE, a parameter optimization method focused on loss landscape characteristics, aiming to achieve both rapid loss convergence in early epochs and enhanced overall training stability. EAGLE features two main characteristics:"}, {"title": null, "content": "(i) EAGLE Update Rule:\nThe update rule estimates optimal parameters by utilizing the local curvature of the loss function between consecutive steps, which is derived from the ratio of gradient changes to parameter changes in neural networks. This approach aims to accelerate loss convergence during the early stages of training.\n(ii) Adaptive Switching Mechanism:\nEAGLE update rule has constraints related to extremely small gradient differences and the shape of the loss function. When these constraints are met, indicating that EAGLE update rule may not function appropriately, the optimizer switches to the established Adam update rule. This switching mechanism ensures overall training stability."}, {"title": "EAGLE UPDATE RULE", "content": "In this study, we describe EAGLE, a parameter update rule that utilizes the ratio of gradient changes to parameter changes in neural networks. Hereafter, we refer to the proposed update rule as \"EAGLE update rule\" and simply use \"update\" when discussing parameter updates. While the term \"loss function\" generally encompasses both \"the relationship between loss and training steps\" and \"the relationship between loss and parameters,\" this paper uses it in the latter sense.\nThe mathematical formulation of EAGLE update rule is as follow:\n$\\theta_{n+1} = \\theta_n - \\frac{\\Delta\\theta}{\\Delta \\frac{\\partial L}{\\partial \\theta_n}} \\frac{\\partial L}{\\partial \\theta_n}$ \n$\\left(\\Delta\\theta = \\theta_n - \\theta_{n-1}, \\frac{\\Delta \\frac{\\partial L}{\\partial \\theta}}{\\partial L} = \\frac{\\frac{\\partial L}{\\partial \\theta_n} - \\frac{\\partial L}{\\partial \\theta_{n-1}}}{\\frac{\\partial L}{\\partial \\theta}} \\right)$ \nwhere $\\theta$ represents the neural network parameters, $\\frac{\\partial L}{\\partial \\theta}$ represents the gradient of the loss function $L$ with respect to $\\theta$, $\\Delta\\Theta \\cdot \\Delta(\\frac{\\partial L}{\\partial \\theta})$ represents their respective changes, and $n$ represents the training step. EAGLE update rule utilizes information from both the current parameters $\\theta_n$ and gradient $\\frac{\\partial L}{\\partial \\theta_n}$, as well as the previous step's parameters $\\theta_{n-1}$ and gradient $\\frac{\\partial L}{\\partial \\theta_{n-1}}$. By calculating and taking the ratio of their differences during updates, we can estimate the curvature of the loss function between the current and previous steps. This curvature is a crucial characteristic indicating the trend of loss function changes, enabling updates in directions where gradient reduction is predicted. This curvature-aware approach allows for improved model training speed compared to conventional methods.\nTo verify the effectiveness of EAGLE update rule, let us consider a simple quadratic function as an example:\n$L = (\\theta - 2)^2 + 2 = \\theta^2 - 4\\theta + 6$ \nTaking the derivative with respect to $\\theta$:\n$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} (\\theta^2 - 4\\theta + 6) = 2\\theta - 4$\nHereafter, let $\\partial L/\\partial \\theta_n = g_n$. We set the previous parameter $\\theta_{n-1}$ and current parameter $\\theta_n$ as follows, obtaining their respective gradients $g_{n-1}, g_n$:\n$\\begin{cases} \\theta_{n-1} = 10 \\\\ g_{n-1} = 2 \\cdot 10 - 4 = 20 - 4 = 16 \\end{cases}$ \n$\\begin{cases} \\theta_n = 8 \\\\ g_n = 2 \\cdot 8 - 4 = 16 - 4 = 12 \\end{cases}$"}, {"title": null, "content": "Using these values, we estimate the optimal parameters with EAGLE update rule:\n$\\theta_{n+1} = \\theta_n - \\frac{\\theta_n - \\theta_{n-1}}{g_n - g_{n-1}}g_n$ \n$= 8 - \\frac{8 - 10}{12 - 16} \\cdot 12$ \n$= 8 - \\frac{(-2)}{(-4)} \\cdot 12 = 8 - 6 = 2$\nLet us examine the parameter value $\\theta_{n+1} = 2$ obtained through this update. At this point, the gradient $g_{n+1}$ in the original function $L = (\\theta - 2)^2 + 2$ becomes zero ($g_{n+1} = 2 \\cdot 2 - 4 = 0$), coinciding with the function's minimum point. This sequence is illustrated in Figure 1.\nThis result demonstrates that EAGLE update rule can estimate parameters that minimize loss in a single update when the loss function has a quadratic shape. Generalizing this finding suggests that EAGLE update rule may improve convergence speed in neural network training when the loss function is convex.\nFurthermore, this update rule can be interpreted as a dynamic learning rate. While conventional optimization methods use fixed learning rates, in EAGLE update rule, the rate of change $\\Delta \\theta / \\Delta g$ functions as an adaptive learning rate.\nStochastic Gradient Descent (SGD) uses the following update equation:\n$\\theta_{n+1} = \\theta_n - \\alpha \\frac{\\partial L}{\\partial \\theta_n}$ \nwhere $\\alpha$ is a fixed learning rate. In contrast, EAGLE update rule uses the following update equation:"}, {"title": null, "content": "$\\theta_{n+1} = \\theta_n - \\beta \\cdot \\frac{\\partial L}{\\partial \\theta_n}$ \n$\\beta = \\frac{\\Delta\\theta}{\\Delta \\frac{\\partial L}{\\partial \\theta}} = \\frac{\\theta_n - \\theta_{n-1}}{\\frac{\\partial L}{\\partial \\theta_n} - \\frac{\\partial L}{\\partial \\theta_{n-1}}}$ \nThus, EAGLE update rule can be viewed as adaptively adjusting the learning rate $\\beta$ based on the respective changes in parameters and gradients."}, {"title": "ADAPTIVE SWITCHING MECHANISM", "content": "EAGLE update rule proposed in Section 2 faces several practical challenges. To address these challenges, we introduce the adaptive switching mechanism for the update rule. Specifically, we adopt both Adam and EAGLE update rules, switching to Adam update rule in situations where EAGLE update rule is deemed ineffective. This switching mechanism is implemented to enhance training stability in practical neural networks. While we chose Adam among conventional optimization methods due to its widely recognized general performance with high training stability, essentially any optimizer could be adopted as an alternative. In the following sections, we describe two major practical challenges and the corresponding conditions for determining update rule switching."}, {"title": "SELECTION BASED ON GRADIENT DIFFERENCE", "content": "The first challenge of EAGLE update rule is that when the gradient difference ratio $\\Delta (\\partial L/\\partial \\theta)$ in the denominator of the update term becomes extremely small, the update magnitude may diverge. This challenge can be expressed by the following relationship:\n$\\Delta \\frac{\\partial L}{\\partial \\theta} \\rightarrow 0$ \n$\\therefore \\frac{\\Delta \\theta}{\\Delta \\frac{\\partial L}{\\partial \\theta}} \\rightarrow +\\infty$ \n$\\therefore \\theta_{n+1} = \\theta_n - \\frac{\\Delta \\theta}{\\Delta \\frac{\\partial L}{\\partial \\theta}} \\frac{\\partial L}{\\partial \\theta_n} = \\theta_n \\pm \\infty \\rightarrow \\pm \\infty$"}, {"title": null, "content": "This issue occurs primarily in two training processes, illustrated in Figure 2. First, while gradient differences show relatively large values in the early stages of training, as training progresses and parameters approach the minimum point, the gradient gradually decreases, causing gradient differences to converge to small values. Consequently, the gradient difference between consecutive steps becomes extremely small, potentially causing update magnitude divergence (left graph in Figure 2). Second, in locally flat regions of the loss function, the gradient maintains small, nearly constant values, similarly resulting in extremely small gradient differences that may cause update magnitude divergence (right graph in Figure2).\nTo address this challenge, we introduce an adaptive switching mechanism using the following conditional branching:\ncondition1: $\\frac{\\Delta \\frac{\\partial L}{\\partial \\theta}}{\\partial \\theta} < threshold$ \nupdate_rule = $\\begin{cases} Adam & (if\\; condition1) \\\\ EAGLE & (otherwise) \\end{cases}$ \nwhere $threshold$ is a small positive threshold value. In this mechanism, Adam update rule is selected when the absolute value of the gradient difference is smaller than the $threshold$, and EAGLE update rule is selected otherwise. $threshold$ is a crucial hyperparameter. Setting an excessively large value biases the updates toward Adam, preventing the utilization of EAGLE update rule's advantages. Conversely, an excessively small value negates the intended function of avoiding division by extremely small values, making it impossible to suppress update magnitude divergence. Therefore, appropriate value setting is necessary. This switching enables flexible updates where Adam's superior stability is utilized in regions with minimal gradient changes, while EAGLE's rapid convergence is leveraged in regions with sufficient gradient changes."}, {"title": "SELECTION BASED ON LOSS LANDSCAPE", "content": "The second challenge of EAGLE update rule is that optimal parameter estimation fails when the loss landscape exhibits locally convex regions. While EAGLE update rule is expected to function effectively in simple concave structures as shown in Section 2, it fails to estimate optimal parameters in locally convex regions.\nTo address this challenge, we classify loss landscape shapes based on the signs and variation patterns of second derivatives of gradients (without directly computing them), analyzing EAGLE update rule's effectiveness in each case. As a result, we propose switching conditions for cases where the rule is considered ineffective."}, {"title": null, "content": "Therefore, the effectiveness of EAGLE update rule is confirmed in this specific example, suggesting its effectiveness in regions where negative gradients monotonically increase in actual neural networks.\nTransition 1-3: Point 2 \u2194 Point 4\nSimilarly, we set the previous parameter $\\theta_{n-1}$ and current parameter $\\theta_n$ as follows, obtaining their respective gradients $g_{n-1}, g_n$:"}, {"title": null, "content": "Similarly, $\\theta_{n+1} = 2$ is the parameter value that minimizes the function, indicating successful optimal parameter estimation. Therefore, this demonstrates that EAGLE update rule is effective even in regions where gradient signs change near the optimal point.\nFrom the above analysis, we have shown that when the loss landscape has global convexity with monotonic gradient changes, EAGLE update rule tends to converge to the global minimum across all basic transition patterns. This confirms the effectiveness of EAGLE update rule."}, {"title": null, "content": "These analysis results, focusing on the relationship between gradients and their changes in sign between consecutive steps, are summarized in Table 1, where VL represents the gradient of the loss landscape with respect to parameters, n indicates the step number, and AVL represents the change in gradients between consecutive steps. From this Table 1, EAGLE update rule is confirmed to be ineffective when the following two conditions are simultaneously satisfied:\nCondition 1: Consistency of gradient signs between consecutive steps: $VL(n - 1) * VL(n) \\geq 0$\nCondition 2: Consistency of gradient signs between current gradient and gradient difference: $VL(n) * AVL \\geq 0$\nTo address inappropriate updates under these conditions, we introduce an adaptive switching mechanism using the following conditional branching:\ncondition2: $(VL(n - 1) * VL(n) \\geq 0) \\wedge (L(n) * AVL \\geq 0)$ \nupdate_rule = $\\begin{cases} Adam & (if\\; condition2) \\\\ EAGLE & (otherwise) \\end{cases}$ \nIn this mechanism, Adam update rule is selected when the loss landscape shape is deemed unsuitable for EAGLE update rule, and EAGLE update rule is selected otherwise. Through this adaptive switching, even in cases where the loss landscape is"}, {"title": null, "content": "not necessarily concave, we achieve both training stability and convergence by combining Adam update rule's superior stability with EAGLE update rule's rapid convergence.\nFurthermore, the above switching mechanism simultaneously resolves an implementation challenge that EAGLE faces in the first update step. While EAGLE update rule requires gradient history indicating the gradient information from the previous step, this gradient history does not exist in the first step immediately after training begins. Therefore, the first update must use Adam update rule rather than EAGLE update rule, necessitating a separate branching process to use Adam update rule. However, by defining the previous gradient as zero in the first update, we can make the above switching mechanism function as a solution.\nLet us calculate the gradient change by defining the gradient at the first update as a real number k and the previous gradient as zero:\n$VL(1) = k, VL(0) = 0, AVL = VL(1) - VL(0) = k$\nSubstituting these values into condition2:\n$\\begin{cases} VL(n - 1) * VL(n) = VL(0) * VL(1) = 0 * k = 0 \\geq 0 \\\\ L(n) * \\Delta\\nabla L = L(1) * AVL = k * k = k^2 \\geq 0 \\end{cases}$ \nConsequently, since condition2 holds true for any real number k, the update rule necessarily becomes Adam.\nupdate_rule = Adam ($\\forall k \\in R: condition2 = true$)\nThis eliminates the need for separate branching logic for the first update step where gradient history does not exist. Additionally, from the second step onward, the necessary gradient history has been accumulated, enabling appropriate updates using EAGLE update rule. Thus, the switching mechanism based on gradient sign relationships functions not only as a mere update rule switching mechanism but also as a solution to the history dependency challenge in EAGLE update rule's first update."}, {"title": "INTEGRATION OF SWITCHING MECHANISMS", "content": "Based on the above analysis, we integrate the switching mechanisms for update rules. condition1 based on gradient difference threshold shown in Section 3.1 and condition2 based on loss landscape shape shown in Section 3.2 are both mechanisms for determining switches to Adam update rule. Therefore, we combine these conditions as a logical OR and formulate them as a single selection equation:\ncondition1: $\\Delta \\frac{\\frac{\\partial L}{\\partial \\theta}}{\\partial \\theta} < threshold$ \ncondition2: $(VL(n - 1) * \\nabla L(n) \\geq 0) \\wedge (L(n) * AVL \\geq 0)$ \ncondition: condition1 $\\vee$ condition2 \nupdate_rule = $\\begin{cases} Adam & (if\\; condition) \\\\ EAGLE & (otherwise) \\end{cases}$ \nThis integrated switching mechanism enables consistent handling of both challenges- update magnitude divergence due to gradient difference minimization and inappropriate updates due to locally convex loss landscape shapes-within a unified framework."}, {"title": "PSEUDO-CODE IMPLEMENTATION", "content": "The proposal organized as pseudo-code is shown below."}, {"title": "RELATED WORK: ADAM", "content": "Adam (Adaptive Moment Estimation) [11], proposed by Kingma et al. in 2015, is an optimization method that combines the advantages of AdaDelta [14], Momentum [9], and RMSprop [10]. It is expressed by the following update equations:\n$m_n = \\beta_1 m_{n-1} + (1 - \\beta_1) \\nabla L(\\theta_n) \tbinom{}{} (m_0 = 0)$\n$v_n = \\beta_2 v_{n-1} + (1 - \\beta_2){\\{\\nabla L(\\theta_n)\\}\\}^2 \tbinom{}{} (v_0 = 0)$\n$\\hat{m_n} = \\frac{m_n}{1 - \\beta_1^n} , \\hat{v_n} = \\frac{v_n}{1 - \\beta_2^n}$\n$\\theta_{n+1} = \\theta_n - \\eta \\frac{\\hat{m_n}}{\\sqrt{\\hat{v_n}} + \\epsilon}$"}, {"title": "First-Order Moment Calculation", "content": "The first-order moment (mean) incorporates the Momentum concept similar to AdaDelta, calculating the moving average of gradients to consider the directional trends of past gradients and appropriately determine the update direction. This mechanism enables stable updates even with significant gradient changes."}, {"title": "Second-Order Moment Calculation", "content": "The second-order moment (variance) also incorporates the RMSprop concept similar to AdaDelta, calculating the moving average of squared gradient values to adaptively adjust learning rates individually for each parameter. This mechanism enables efficient learning based on parameter importance and update necessity."}, {"title": "Bias Correction", "content": "While Adam has relatively many hyperparameters, they exhibit high robustness, and their default values ($\\beta_1$ = 0.9, $\\beta_2$ = 0.999, $\\epsilon$ = 1e \u2013 8) function well across a wide range of tasks. Consequently, it is widely used as the de facto standard in deep learning."}, {"title": "EXPERIMENTS", "content": "In this chapter, we evaluate the performance of our proposed method on multi-layer neural networks and convolutional neural networks, analyze the local shape of loss functions in the datasets and network models used in Sections 6.1 and 6.2, and examine the usage ratio of EAGLE update rule with the switching mechanism introduced for practical extensions. For all implementation evaluations, we adopted a single machine equipped with an NVIDIA A100 80GB PCIe."}, {"title": "EXPERIMENT: MULTI-LAYER NEURAL NETWORKS", "content": null}, {"title": "EXPERIMENTAL SETTINGS", "content": "In this experiment, we evaluate the performance of EAGLE based on our research objectives: rapid loss convergence in the early epochs of training and improved overall training stability. We adopted datasets from the UCI Machine Learning Repository: the Iris and Wine datasets. The Iris dataset is used for classifying three types of iris species based on four features (sepal length, sepal width, petal length, petal width). We adopt a three- layer neural network (input layer: 4, hidden layer: 25, output layer: 3) as our model. The Wine dataset is used for classifying wine types based on their chemical composition. It predicts which of three types a wine belongs to using 13 features (alcohol, malic acid, ash, alkalinity of ash, magnesium, total phenols, flavonoids, nonflavanoid phenols, proanthocyanins, color intensity, hue, OD280/OD315 of diluted wines, proline). We adopt a three-layer neural network (input layer: 13, hidden layer: 15, output layer: 3) as our model.\nThe gradient difference threshold in the switching mechanism was set to 0.0005 through preliminary experiments. As comparison baselines, we selected conventional optimizers: SGD with momentum and Adam, and advanced optimizers: RAdam, Lion, and Sophia. In the following sections, we refer to SGD with momentum simply as SGD.\nTo ensure reproducibility and reliability, we conducted independent experiments using 10 randomly selected seed values from 1 to 10,000. We evaluated the experimental results using the following three evaluation metrics:"}, {"title": null, "content": "Evaluation Metrics 1: Time-series visualization of mean and standard deviation for Training Loss, Training Accuracy, Test Loss, and Test Accuracy across all seeds (shown in top and middle panels). The standard deviation is represented by shaded regions around each curve.\nEvaluation Metrics 2: Early-stage learning progression graphs showing Training Loss and Training Accuracy, extracted from the above visualizations (shown in bottom panels).\nEvaluation Metrics 3: Comparative analysis of Training Loss changes across optimizers"}, {"title": "RESULTS AND DISCUSSION", "content": "First, we present the experimental results using the Iris dataset in Figure 5,6 and Table 2,3."}, {"title": "EXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS", "content": null}, {"title": "EXPERIMENTAL SETTINGS", "content": "We conduct the same experiments as in Section 6.1 using the MNIST dataset to evaluate the performance of EAGLE. The MNIST dataset is used for recognizing handwritten digits from 0 to 9. Each digit is represented as a 28\u00d728 pixel grayscale image. Each pixel has a brightness value from 0 to 255, which is normalized to the range [0,1] by dividing by 255 as preprocessing. We adopt a convolutional neural network (CNN) that combines two convolutional blocks and two fully connected layers. Each convolutional block includes a convolutional layer (kernel size 3\u00d73), batch normalization layer, ReLU activation function, and max pooling layer (kernel size 2\u00d72). The first fully connected layer has 3,136 (64\u00d77\u00d77) inputs and 128 outputs, followed by a batch normalization layer and ReLU activation function, while the second layer performs 10-class classification.\nTo ensure reproducibility and reliability, we conducted independent experiments using 5 randomly selected seed values from 1 to 10,000. For evaluation of the experimental results, we used Evaluation Metrics 1 and 3 from Section 6.1. However, in Metric 3, we present the training loss changes at one-epoch intervals during the early stages of learning."}, {"title": "RESULTS AND DISCUSSION", "content": "we present the experimental results using the MNIST dataset in Figure 9,10 and Table 8,9."}, {"title": "EXPERIMENT: ANALYSIS OF LOSS LANDSCAPE", "content": null}, {"title": "EXPERIMENTAL SETTINGS", "content": "This research assumes that EAGLE update rule functions effectively when the loss function shape can be approximated as convex. Therefore, to verify this convexity property of loss functions, we experimentally analyzed the shape of loss functions using the Iris, Wine, and MNIST datasets and their corresponding models from Sections 6.1 and 6.2. The analysis procedure is described below:"}, {"title": null, "content": "Model Training and Reference Point Setting\ni. Train the neural network model\nii. Set the optimal parameter values obtained by the optimizer (EAGLE in this experiment) as reference points\nParameter Sampling\ni. Sample a subset of parameters considering computational cost, rather than all parameters\nii. Iris: Randomly select 50 parameters from input layer hidden layer, 40 parameters from hidden layer output layer\niii. Wine: Randomly select 60 parameters from input layer hidden layer, 30 parameters from hidden layer output layer\niv. MNIST: For convolutional layers, randomly select 20 parameters for each of the two blocks; for fully connected layers, randomly select 20 parameters for each of the two layers\nSensitivity Analysis for Each Parameter\ni. Fix all parameter values at their reference points\nii. Select one parameter from the sampled set and vary it within \u00b15 range of the reference point, calculating loss values (with 1000-point uniform sampling, resulting in parameter intervals of 0.01)\niii. Return the varied parameter to its reference point\niv. Repeat steps ii and iii for all parameters selected in step 2\nResults Visualization\ni. Plot the loss function graphs for each parameter from steps 3\nii. Display the reference point with a red dashed line and the loss value at learning completion with a green dashed line"}, {"title": "RESULTS AND DISCUSSION FOR IRIS AND WINE", "content": "Experimental results are shown in Figure 11 and 12."}, {"title": null, "content": "We analyze the shape of loss functions obtained from experimental results from the following three perspectives."}, {"title": "Shapes with Convexity", "content": "As represented in Figure 13, multiple loss functions shown in Figures 11 and 12 were found to form curves that are convex around the reference point. This result supports the validity of our assumption that loss functions can be approximated as convex. This characteristic was particularly prominent in the Wine dataset. For parameters exhibiting such shapes, EAGLE update rule appears to function effectively, likely contributing to the rapid loss convergence in early training stages as shown in Section 6.1."}, {"title": "Oscillatory Shapes", "content": "As represented in Figure 14, multiple loss functions shown in Figures 11 and 12 were found to exhibit oscillatory behavior in response to parameter value changes, which appears to contradict our assumption that loss functions can be approximated as convex. However, since this experiment analyzes only the shape near the optimal solution, the overall shape of the loss function remains unconfirmed, and we cannot definitively conclude that it \"contradicts the assumption.\"\nHere, we consider the extent to which this oscillatory behavior affects model performance. Comparing Figures 13 and 14 with focus on the range of loss variations on the vertical axis, we observe a reduction from approximately 0.45 to 0.02 for the Iris dataset, and from approximately 0.02 to 0.000175 for the Wine dataset. Consequently, when adjusting the vertical scale of Figure 14 to match the convex shape graphs, the oscillating shapes can be interpreted as nearly flat. This indicates that the loss is close to its minimum value near the optimal solution, suggesting that parameter value fluctuations have minimal impact on the loss. In conclusion, parameters with such oscillatory loss function shapes likely have minimal impact on model performance through optimization, and precise parameter optimization may not be necessary.\nHowever, even if the loss changes for individual parameters are small, when multiple parameters simultaneously influence the loss, their effects may accumulate and have non- negligible impact on the final loss value. Therefore, insufficient control of such oscillations near the optimal solution in EAGLE may have contributed to the decreased final convergence performance shown in Section 6.1.\nAdditionally, comparing Figures 11 and 12, such oscillatory loss functions were frequently observed in the Iris dataset but were less common in the Wine dataset. According to Tables 6 and 7, the final loss values across all optimizers were lower for the Wine dataset compared to the Iris dataset. This suggests that such oscillations may be a factor making optimization challenging for all optimizers, not just EAGLE."}, {"title": "Shapes with Changing Signs of Second Derivatives", "content": "As represented in Figure 15, among the loss functions shown in Figures 11 and 12, we confirmed the existence of loss functions with changing signs in second derivatives as discussed in Section 3.2, particularly in the Wine dataset. From the position of the final loss value (green dashed line) in Figure 15, we can observe that despite the presence of oscillations in Iris, parameters are properly updated to minimize loss even when second derivatives change signs. This demonstrates that the switching mechanism proposed in Section 3.2 functions effectively even with changes in second derivatives.\nWe summarize the above three analyses. This experiment confirmed the existence of loss functions that can be approximated as convex, and suggests that EAGLE functions effectively for these cases, achieving rapid convergence in early training stages. Additionally, we confirmed proper updates even with loss functions having second derivative sign changes, demonstrating the effectiveness of the proposed switching mechanism in our update rule. However, the handling of parameters with oscillatory loss functions remains insufficient, which was analyzed as a factor contributing to decreased final convergence performance."}, {"title": "RESULTS AND DISCUSSION FOR MNIST", "content": "Experimental results are shown in Figures 16, 17, 18, 19 and 20."}, {"title": null, "content": "We analyze the lack of significant early-stage loss reduction in the MNIST dataset compared to that observed in the Iris and Wine datasets in Section 6.2, from the perspective of loss function shape characteristics."}, {"title": "Oscillatory Shapes", "content": "We analyze the parameter updates in the first fully connected layer shown in Figure 19. In Figures 17, 18, 19, and 20, while we used the same sampling number for each layer in this experiment, the parameters in the first fully connected layer actually account for 95.2% of"}, {"title": null, "content": "all parameters, and these parameters have oscillatory loss functions as shown in Figure 21. As discussed in Section 5.2, loss minimization by EAGLE is challenging for parameters with such oscillatory shapes. We conclude that this characteristic led to the absence of significant loss reduction in the early stages of training."}, {"title": "Linear and Flat Shapes", "content": "We analyze the parameter updates in the first convolutional block shown in Figure 17. As shown in Figure 22, these parameter groups commonly exhibit linear (right side of optimal solution) or flat (left side of optimal solution) shapes near the optimal solution. As discussed in Section 3.1, these shapes result in extremely small gradient differences, which could lead to divergent update magnitudes when applying EAGLE update rule. However, Figure 17 shows no such divergence, and as indicated by the loss value at learning completion (green dashed line), updates successfully reduced the loss. This suggests that the switching mechanism functioned properly, and Adam update rule was primarily used for updating these parameters."}, {"title": "Shapes with Convexity", "content": "As shown in Figures 18 and 20, the parameter groups in the second convolutional block and the second fully connected layer exhibit shapes with convexity, as illustrated in Figures 23 and 24. In these regions, EAGLE update rule appears to function effectively, contributing to accelerated learning."}, {"title": "EXPERIMENT: ANALYSIS OF EAGLE UPDATE RULE USAGE", "content": null}, {"title": "EXPERIMENTAL SETTINGS", "content": "As explained in Chapter 3, the proposed EAGLE optimizer introduces a switching mechanism that adaptively switches between EAGLE update rule and Adam update rule. The gradient difference threshold explained in Section 3.1 is a crucial hyperparameter that directly affects the usage ratio as shown in Figure 25. In this experiment, we analyze the changes in usage rate of EAGLE update rule during the training process. Specifically, using the Iris, Wine, and MNIST datasets and their models from Sections 6.1 and 6.2, we evaluate the progression of training loss, changes in usage rates during early training and at completion, and average usage rate throughout training when varying this threshold across [1e-3, 7e-4, 4e-4, 1e-4"}]}