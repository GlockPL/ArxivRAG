{"title": "ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration", "authors": ["Zixiang Wang", "Yinghao Zhu", "Huiya Zhao", "Xiaochen Zheng", "Tianlong Wang", "Wen Tang", "Yasha Wang", "Chengwei Pan", "Ewen M. Harrison", "Junyi Gao", "Liantao Ma"], "abstract": "We introduce ColaCare, a framework that enhances Electronic Health Record (EHR) modeling through multi-agent collaboration driven by Large Language Models (LLMs). Our approach seamlessly integrates domain-specific expert models with LLMs to bridge the gap between structured EHR data and text-based reasoning. Inspired by clinical consultations, ColaCare employs two types of agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data. Expert models process and generate predictions from numerical EHR data, while LLM agents produce reasoning references and decision-making reports within the collaborative consultation framework. We additionally incorporate the Merck Manual of Diagnosis and Therapy (MSD) medical guideline within a retrieval-augmented generation (RAG) module for authoritative evidence support. Extensive experiments conducted on four distinct EHR datasets demonstrate ColaCare's superior performance in mortality prediction tasks, underscoring its potential to revolutionize clinical decision support systems and advance personalized precision medicine. The code, complete prompt templates, more case studies, etc. are publicly available at the anonymous link: https://colacare.netlify.app/.", "sections": [{"title": "1 INTRODUCTION", "content": "Electronic Health Records (EHRs) modeling plays a crucial role in prognosis prediction and clinical treatment decision-making, serving as a cornerstone for data-driven healthcare advancements [11]. In recent years, with the development of deep learning, existing works on EHR modeling have achieved significant success, particularly in leveraging structured EHR data [25, 27, 43]. However, these efforts have primarily been pure data-driven, end-to-end methods that operate independently of external knowledge. Consequently, these modeling procedures fundamentally fail to comprehend the clinical significance of the recorded features, treating them as mere variables without semantic context [45]. This limits the real-world application of such \"black box\" methods due to their sensitivity to input data distribution and tendency to overfit, especially when training samples are limited in quantity and diversity\u2014a common scenario in real-world clinical practice. Moreover, for practical application, methods require clear interpretability to demonstrate the prediction process and evidence to human physicians, ensuring that deep learning methods can be integrated into real clinical decision-making. Existing methods' interpretability is limited, often relying on traditional feature importance analysis techniques such as attention mechanisms [26], SHAP (SHapley Additive exPlanations) [34], and activation level visualization [38], which only offer a basic level of interpretability insufficient for meaningful communication with physicians.\nSome works have attempted to embed knowledge through ICD-codes and knowledge graphs, representing a valuable effort [3, 24]. However, the deployment of such methods in real-world applications is highly challenging due to their reliance on manually crafted knowledge forms and slow knowledge updates, which are often not aligned with the latest medical research, clinical reports, or updated guidelines\u2014critical factors for clinical prediction tasks.\nRecently, the impressive capabilities of Large Language Models (LLMs) in handling general tasks [1] and medical applications [33] have motivated exploration into their potential for enhancing EHR analysis. Yet, existing works have mainly focused on medical question-answering (Q&A) as generative tasks or reasoning over unstructured notes [14, 36]. While medical Q&A tasks are widely trained and benchmarked [2, 9], the analysis and prediction of structured EHR"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 LLMs in Medical Tasks", "content": "Large Language Models (LLMs) have demonstrated significant success in the medical domain, particularly in medical question-answering (Q&A) tasks [2, 9, 39]. They have also excelled in medical evidence summarization [35, 37]. Notably, the advanced LLM GPT-4 [1] has outperformed medical students on standard medical board exams [20]. While these achievements primarily involve textual clinical notes, recent research has begun exploring LLMs' capabilities in handling structured Electronic Health Record (EHR) data. Approaches include prompting LLMs directly [13, 44] or ensembling machine learning models' outputs [12, 16]. These studies reveal that GPT-4 shows potential for zero-shot prediction on structured EHR data, although a significant performance gap remains compared to conventional deep learning methods trained on full datasets [44]. Additionally, LLM's direct outputs face challenges with hallucination, where generated content may not strictly adhere to instructions, resulting in unexpected outputs beyond numerical values [40]."}, {"title": "2.2 LLM-Driven Multi-Agent Collaboration in Medical Field", "content": "The development of LLM-driven agent systems, where multiple agents with distinct roles collaborate and utilize external tools [10], has garnered increasing attention in medical domains. Examples include Al Hospital [8] and Agent Hospital [22], which simulate real hospital environments through collaboration among several agents. Recent studies have also explored adversarial collaboration, incorporating debates and negotiations among multiple agents. MedAgents [36] proposes a medical collaboration framework where doctor agents vote on diagnoses, while ArgMed-Agents [14] constructs conflict relationship graphs and employs formal deduction to generate coherent Q&A conclusions.\nWhile most prior work has focused on Q&A tasks, where LLMs generate text-based diagnoses similar to those in reading comprehension tasks, real-life medical scenarios often require specific numerical outputs, such as disease mortality risk prediction. This highlights the need for designing a framework that can better incorporate structured EHR data to address these more complex, quantitative medical tasks."}, {"title": "3 PROBLEM DEFINITION", "content": ""}, {"title": "3.1 EHR Datasets Formulation", "content": "The EHR datasets are structured as multivariate time-series data with multiple features, denoted as $X = [X_1, X_2, \u2026, X_T] \\in R^{T\u00d7F}$, encapsulating information across T visits and F features, which includes static features (e.g., sex and age) and dynamic features (e.g., lab tests and vital signs)."}, {"title": "3.2 Predictive Objective Formulation", "content": "The prediction task is defined as a binary classification problem aimed at predicting patients' mortality outcomes. Our goal is to extract knowledge from EHR data, supplemented by auxiliary external medical knowledge (e.g., medical guidelines), to enhance predictive modeling of EHRs. Thus, the predictive objective is formulated as:\n$\\hat{y} = Framework(x_{EHR}, MedicalKnowledge)$                                                       (1)\nwhere $\\hat{y}$ represents the predicted mortality outcome, with values closer to 0 indicating an alive status, and values closer to 1 indicating deceased."}, {"title": "4 METHODOLOGY", "content": "Figure 1 and Figure 2 demonstrates our proposed LLM-based multi-agent collaboration ColaCare framework, where the key module: Multi-Agent Collaborative Consultation Module is detailedly illustrated in Figure 1. It gathers a group of doctor agents and a meta doctor agent for medical discussion about a certain patient's condition. Doctor agents reach an agreement after multiple rounds of debate and the meta doctor agent proposes a report finally."}, {"title": "4.1 Structured EHR Information Extraction", "content": "Given the EHR data of a patient, $X_{EHR}$, we utilize the EHR model, denoted as $Model$, to encode this temporally linked information:\n$h_{EHR} = Model(X_{EHR})$                                                             (2)\nThen we use a MLP layer to predict the output logit z and the SHAP strategy to obtain feature importance weights $\\alpha$:\n$z = MLP(h_{EHR})$\n$\\alpha = SHAP(Model, X_{EHR})$                                              (3)\nThe prediction result z and interpretation features $\\alpha$ by deep learning models are used to support the LLM-based collaborative consultation."}, {"title": "4.2 Multi-Agent Collaborative Consultation", "content": "As shown in Figure 1, in the multi-agent collaborative consultation, we define two distinct roles: the doctor agent (DoctorAgent) and the meta doctor agent (MetaAgent). Each DoctorAgent is linked to a domain-specific expert model.\nThe collaboration begins with each DoctorAgent providing an initial review of a patient's condition. Subsequently, the MetaAgent synthesizes these reviews to generate a comprehensive report and orchestrates the collaborative consultation process. During this iterative process, all DoctorAgents express their opinions on the current report. The MetaAgent then considers the feedback, accordingly revises the report, and determines whether more rounds of consultation are necessary. Figure 1 demonstrates the overview pipeline of the collaborative consultation."}, {"title": "4.2.1 Generation of Initial Reviews.", "content": "We first utilize output logits z and feature importance weights $\\alpha$ of EHR models, as well as basic information of a certain patient (e.g. Sex and Age), to build an initial patient record $x_{record}$:\n$X_{record} = Prompt(z, \\alpha, Info_{patient})$                                                           (4)\nNext, the retriever module matches the patient record with a series of documents in the corpus and retrieves the top-K relevant documents:\n$Docs = Retriever(X_{record})$                                                        (5)\nLastly, we instruct the DoctorAgent to generate a review:\n$r_{doctor} = DoctorAgent(Prompt(X_{record}, Docs))$                                                                        (6)"}, {"title": "4.2.2 Synthesized Preliminary Report.", "content": "The MetaAgent is instructed to leverage the patient's basic information and initial reviews from all DoctorAgents to generate a synthesized report:\n$r_{meta} = MetaAgent(Prompt(\\sum r_{doctor}, Info_{patient}))$                                                                                                            (7)"}, {"title": "4.2.3 Collaborative Consultation Process.", "content": "The DoctorAgents are instructed to evaluate their initial assessments and state their agreement or disagreement with the current report. In cases of disagreement, they are required to provide detailed rationales and support their arguments with relevant documents retrieved from the corpus:\n$r_{doctor}^{j} = DoctorAgent(Prompt(r_{meta}^{j-1},\\sum r_{doctor}^{j-1} \\cup Docs))$                                                                                                                               (8)\nwhere j stands for the j-th round in the process.\nSubsequently, the MetaAgent aggregates and analyzes the feedback from all DoctorAgents to determine whether further discussion is necessary:\n$Action = MetaAgent(Prompt(\\sum r_{doctor}^{j}))$                                                                                               (9)\nThe MetaAgent evaluates the statements of the DoctorAgents, focusing on their agreement or disagreement with the current report. If unanimous agreement is reached, the MetaAgent concludes that further discussion is unnecessary. However, in cases of disagreement, the MetaAgent conducts a more detailed analysis by carefully examining the evidence presented by the dissenting DoctorAgents"}, {"title": "4.3 Multimodal Fusion Network", "content": "We begin by leveraging hidden representations of EHR data from previous N EHR-specific expert models, denoted as $h_{EHR}$. Subsequently, we utilize a medical-domain pretrained language model, LM, to encode the final consensus report embedding:\n$h_{Report} = LM(x_{Report})$                                                                                                                                       (11)\nWe then concatenate the representations from both modalities and apply an MLP layer to obtain the final prediction, $\\hat{y}$:\n$\\hat{y} = MLP (Concat [h_{EHR_{1}},h_{EHR_{2}}...,h_{EHR_{N}},h_{Report}])$                                                                                           (12)\nThe loss function employed is the Binary Cross-Entropy (BCE) Loss for binary classification:\n$L(\\hat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N}(y_{i}log(\\hat{y_{i}}) + (1-y_{i}) log(1 \u2013 \\hat{y_{i}}))$                                                              (13)\nwhere N represents the number of patients in a batch, $\\hat{y} \u2208 [0, 1]$ is the predicted probability, and y is the ground truth."}, {"title": "5 EXPERIMENTAL SETUPS", "content": ""}, {"title": "5.1 Experimented Datasets and Utilized Medical Guideline", "content": "We adopt four real-world datasets: MIMIC-III, MIMIC-IV, CDSL, and PD datasets and additionally introduce the MSD (Merck Manual of Diagnosis and Therapy) medical guideline to enhance our experimental framework.\n\u2022 MIMIC-III, MIMIC-IV [18, 19]: These datasets are part of the Medical Information Mart for Intensive Care project. They contain comprehensive EHR data for intensive care unit patients, including demographic data, vital signs, laboratory results, procedures, medications, notes, and mortality statistics. MIMIC-IV is an updated version of MIMIC-III with enhanced data and restructured tables.\n\u2022 CDSL [15]: This dataset is derived from the HM Hospitales EHR system and consists of anonymized records of 4,479 patients admitted with a confirmed or suspected diagnosis of COVID-19. CDSL offers a rich variety of medical features, including comprehensive details on diagnoses, treatments, admissions, ICU stays, diagnostic imaging tests, laboratory results, and patient discharge or death status.\n\u2022 PD [26]: The Peritoneal Dialysis (PD) dataset comprises data from 656 peritoneal dialysis patients, including 13,091 visit records collected over a 12-year period, from January 1, 2006, to January 1, 2018. This longitudinal dataset features patients' baseline information, visit records, and clinical outcomes, offering a unique perspective on long-term peritoneal dialysis treatment and patient progression.\nWe incorporate the Merck Manual of Diagnosis and Therapy (MSD) medical guideline into our framework ColaCare. The MSD guideline is a comprehensive medical reference that provides detailed information on various diseases, corresponding diagnoses, and treatment protocols.\nWe adhere to the established EHR benchmark pipeline [11] for preprocessing time-series data, ensuring consistency and comparability in our data preparation across all datasets. The statistics of dataset split and label distribution for the four datasets are shown in Table 1."}, {"title": "5.2 Evaluation Metrics", "content": "We employ three widely-used evaluation metrics for binary classification tasks, all of which are interpreted as \"higher is better\""}, {"title": "5.3 EHR-Specific Baseline Models", "content": "We include following EHR-specific deep learning-based models as baselines: AdaCare [25], ConCare [27], RETAIN [4], GRASP [43], M3Care [42], SAFARI [28], AICare [26], mTAN [32], and PrimeNet [5]. These models employ various architectures and techniques such as GRU-based structures, attention mechanisms, cohort representation learning, and feature extraction and recalibration to address different aspects of EHR data analysis and patient health representation."}, {"title": "5.4 Implementation Details", "content": ""}, {"title": "5.4.1 Hardware and Software Configuration.", "content": "All experiments are conducted on a single Nvidia RTX 3090 GPU with CUDA 12.5. The server's system memory (RAM) size is 128GB. We implement the model in Python 3.9.19, PyTorch 2.3.1 [30], PyTorch Lightning 2.3.3 [7]."}, {"title": "5.4.2 Model Training and Hyperparameters.", "content": "AdamW [23] is employed with a batch size of 128 patients. All models are trained for 50 epochs with an early stopping strategy based on AUPRC after 10 epochs without improvement. For baseline models, the learning rate 0.01, 0.001, 0.0001 and hidden dimensions 64, 128 are tuned using a grid search strategy on the validation set. We employ AdaCare, ConCare and RETAIN as selected domain-specific expert models on MIMIC-III, MIMIC-IV and CDSL datasets, and AICare, ConCare and SAFARI on the PD dataset. The max round"}, {"title": "5.4.3 Utilized (Large) Language Models.", "content": "ColaCare incorporates both Language Models (LMs) and Large Language Models (LLMs) within its framework. For language models, we employ MedCPT [17] in the RAG system's biomedical information retrieval process. MedCPT is contrastively pre-trained on 255 million PubMed literature entries. Additionally, we utilize the GatorTron [41] model, which is pretrained on 82 billion de-identified clinical texts and further finetuned on MIMIC datasets for final clinical note embedding computation. For large language models, we employ the 200-billion-parameter DeepSeek-V2 Chat [6] to act as agents and conduct reasoning tasks."}, {"title": "6 EXPERIMENTAL RESULTS AND ANALYSIS", "content": "This section evaluates and analyzes the ColaCare framework by addressing the following Research Questions (RQs):\n\u2022 RQ1: Overall Performance: How does ColaCare perform compared to other EHR-specific deep learning models in predicting mortality?\n\u2022 RQ2: Comparing Output Methods: Is leveraging the fusion network superior to directly instructing the LLM to predict numerical logits?\n\u2022 RQ3: Sensitivity to Number of Agents: How does the number of agents affect overall performance?\n\u2022 RQ4: Case Studies: Does ColaCare provide reliable and authoritative clinical reports for interpretable analysis?\n\u2022 RQ5: Analysis of Consultation Process: Is the ColaCare framework efficient in facilitating agreement among agents?\n\u2022 RQ6: Cost Analysis: What is the cost required to implement ColaCare in clinical practice?"}, {"title": "6.1 RQ1: Overall Performance", "content": "To respond RQ1, we conduct the mortality prediction task on MIMIC-III, MIMIC-IV, CDSL and PD datasets. The overall performance of ColaCare is depicted in Table 2. The results demonstrate that ColaCare consistently outperforms all baseline models across all four datasets, with particularly notable improvements in the AUPRC metric. This superior performance underscores ColaCare's potential for practical application in clinical decision-making scenarios."}, {"title": "6.2 RQ2: Comparing Output Methods", "content": "To address RQ2, we examine two methods for obtaining prediction logits:\n\u2022 Instruct the LLM's direct outputs: Utilize the reasoning capabilities of the LLM by prompting it to produce prediction results based on the final report, as applied in previous studies [13, 44].\n\u2022 Applying a fusion network module: Integrate hidden representations from structured EHR data and the unstructured report generated by ColaCare through simple concatenation and an MLP layer."}, {"title": "6.3 RQ3: Sensitivity to Number of Agents", "content": "To answer RQ3, we explore the impact of number of agents participated in the collaborative consultation. Our findings indicate that the participation of one or two DoctorAgents has either no impact or a slightly negative impact on performance. This phenomenon can be attributed to the ease with which a small number of DoctorAgents can reach a consensus, even if their final opinion lacks robustness. As the number of DoctorAgents increases, the improvement can be explained by the incorporation of a wider range of perspectives and medical evidence, ultimately resulting in more comprehensive and reliable reports."}, {"title": "6.4 RQ4: Case Studies", "content": "Figure 3 illustrates two exemplary cases that demonstrate the MetaAgent's final report, along with the supporting rationale and evidence.\nIn the first case, the MetaAgent synthesizes the patient's overall condition and concludes a high mortality risk. This assessment is predominantly based on the opinions and evidence provided by DoctorAgents 2 and 3. Notably, the MetaAgent successfully persuades DoctorAgent 1, leading to a unanimous consensus.\nThe second case presents a more complex scenario. Initially, DoctorAgents 2 and 3 advocate for a high mortality risk, while DoctorAgent 1 dissents in the first round of deliberation. Despite this disagreement, the MetaAgent maintains the high-risk assessment, basing its decision on the majority consensus and corroborating medical evidence. The MetaAgent then formulates a final report that reflects this considered judgment."}, {"title": "6.5 RQ5: Analysis of Consultation Process", "content": "Table 4 presents an analysis of the consensus-building process across various datasets. Our findings indicate that the average number of rounds required to reach consensus differs among the datasets. For instance, the MIMIC-III and CDSL datasets require more rounds to achieve agreement. Additionally, we observe that the proportion of affirmative votes to disaffirmative votes from DoctorAgents is relatively balanced, rather than skewed towards agreement or disagreement, which would be less effective for consensus. This suggests that the MetaAgent can selectively incorporate opinions from multiple DoctorAgents to create and refine reports, resulting in more robust and well-considered decisions."}, {"title": "6.6 RQ6: Cost Analysis", "content": "To evaluate the practical viability of the ColaCare framework in real-world clinical settings, we conducted an analysis of its computational costs, as illustrated in Figure 4. A significant proportion of input tokens is allocated to prompting DoctorAgents, primarily due to the need for integrating extensive external medical knowledge. With DeepSeek-v2 Chat, we estimate the cost per sample to be approximately 0.07 US dollars, which is economical."}, {"title": "7 LIMITATIONS AND FUTURE WORK", "content": "While ColaCare demonstrates promising results in enhancing EHR modeling, several limitations and areas for future work should be acknowledged:\n\u2022 Generalizability: Our current study focused on mortality prediction tasks across four datasets and employed one large language model. Further research is needed to evaluate ColaCare's performance on a broader range of clinical prediction tasks and diverse EHR datasets. Additionally, more large language models such as GPT-4, Claude-3.5 and Llama-3.1 could be utilized as agents to enhance the model's capabilities.\n\u2022 Human Evaluation: Although ColaCare provides interpretable reports that were examined by expert doctors in our case studies, more comprehensive human evaluation from clinicians is required to ensure trustworthy and reliable decision-making.\n\u2022 Continuous Learning: The current EHR-specific models' parameters are fixed after training on full datasets. Developing mechanisms for ColaCare that enable continuous learning and parameter (knowledge) updates based on feedback from large language model agents could be further explored to enhance its adaptability and performance over time.\nOur future work aims to address these limitations to further improve the effectiveness and applicability of ColaCare in real-world clinical settings."}, {"title": "8 CONCLUSION", "content": "This paper introduces ColaCare, a framework that enhances Electronic Health Record (EHR) modeling through Large Language Model (LLM)-driven multi-agent collaboration. By integrating domain-specific expert models with general-purpose LLMs, ColaCare bridges the gap between structured EHR data analysis and LLM reasoning capabilities. The multi-agent collaborative process, involving doctor agents and a meta doctor agent, effectively simulates medical consultations, while retrieval-augmented generation techniques ensure predictions are grounded in current medical knowledge. Our experiments across four real-world EHR datasets demonstrate ColaCare's superior performance in mortality prediction tasks. Additional case studies illustrate the framework's ability to provide interpretable insights. ColaCare represents a significant advancement in EHR modeling and clinical prediction, offering a powerful tool for enhancing personalized precision medicine, which could transform clinical decision support systems and improve healthcare delivery."}]}