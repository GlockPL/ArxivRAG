{"title": "GALORE+: BOOSTING LOW-RANK ADAPTATION FOR LLMS WITH CROSS-HEAD PROJECTION*", "authors": ["Xutao Liao", "Shaohui Li", "Yuhui Xu", "Zhi Li", "Yu LIU", "You He"], "abstract": "Recent low-rank training methods, such as GaLore, have significantly reduced\nthe memory required to optimize large language models (LLMs). However, these\nmethods often suffer from time-consuming low-rank projection estimations. In\nparticular, the singular value decomposition (SVD) in GaLore can consume more\nthan 80% of the total training time. To address this issue, we propose GaLore+,\nwhich uses cross-head low-rank projection to reduce the substantial time con-\nsumption in estimating low-rank projections for multi-head attention. In addition,\nwe employ randomized subspace iteration to achieve fast SVD. To further enhance\nperformance, we propose sparsely coded residuals to reduce the errors caused by\nlow-rank approximation on the first- and second-order moments of the optimizers\nand weight updates. We evaluate GaLore+ on arithmetic reasoning and natural\nlanguage generation datasets. Our experiments demonstrate that GaLore+ de-\nlivers superior performance while achieving approximately 4\u00d7 fine-tuning speed\ncompared to vanilla GaLore.", "sections": [{"title": "1 INTRODUCTION", "content": "As the sizes of language models grow rapidly, training models from scratch for different tasks be-\ncomes impractical due to the significant time and computational resources required. To address this\nchallenge, current research and applications typically rely on pre-training large language models\n(LLMs) and subsequently fine-tuning them for specific downstream tasks. Such a paradigm has\nbeen proven highly efficient in a wide range of tasks, including, but not limited to, natural language\nprocessing, question-answering, and reasoning (Roziere et al., 2023; Li et al., 2023; Ouyang et al.,\n2022; Brown, 2020).\nHowever, full fine-tuning of entire LLMs\nrequires enormous memory, making it pro-\nhibitively expensive for individuals and\nstart-ups. Parameter-efficient fine-tuning\n(PEFT) methods only fine-tune a small num-\nber of the weights, significantly reducing the\nmemory requirements (Lialin et al., 2023;\nDing et al., 2023). Among them, the meth-\nods based on low-rank reparameterization,\nsuch as LoRA (Hu et al., 2022), have at-\ntracted much attention due to their impres-\nsive efficiency. Recently, a low-rank adap-\ntation method named GaLore (Zhao et al.,\n2024) demonstrated the ability to optimize\nan LLM with 7 billion parameters on a\nconsumer-level GPU with 24 GB of mem-\nory. Low-rank adaptation methods achieve\ndramatic memory reduction by updating the\nweights of LLMs in low-rank subspace, thus reducing the number of fine-tuned weights. We catego-"}, {"title": "2 RELATED WORK", "content": "Parameter Efficient Fine-Tuning. A variety of parameter-efficient fine-tuning methods have\nemerged in recent years, enabling an increasing number of institutions and researchers to fine-tune\nLLMs to meet their specific requirements. Adapters (Rebuffi et al., 2017; Houlsby et al., 2019;\nLin et al., 2020; Karimi Mahabadi et al., 2021b;a) enable parameter-efficient fine-tuning by insert-\ning trainable layers into LLMs while keeping other layers frozen. However, this approach also\nintroduces additional inference latency. BitFit (Zaken et al., 2021) selectively tunes only the bi-\nases within the network, significantly reducing the number of parameters involved in fine-tuning.\nPrompt tuning achieves parameter-efficient fine-tuning by optimizing a set of new input tokens or\nprompts for each task (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2021; Liu\net al., 2023). Hu et al. (2022) introduced LoRA, proposing that weight updates are low-rank and\ncan be expressed as the product of two low-rank matrices. Furthermore, the trainable parameters\ncan be merged with the original weights, eliminating additional inference latency. Recent studies\ncombined parameter-efficient fine-tuning methods with quantization to enhance memory efficiency\nduring the fine-tuning of LLMs (Kwon et al., 2022; Dettmers et al., 2023; Chai et al., 2023; Xu et al.,\n2023). And DoRA (Liu et al., 2024), or Weight-Decomposed Low-Rank Adaptation, is a parameter-\nefficient fine-tuning method designed to enhance learning capacity and stability by decomposing\npre-trained weights into magnitude and direction components, leveraging LoRA for directional up-\ndates, and achieving superior performance across tasks without additional inference costs.\nParameter Sharing. Adam-mini partitions the model parameters into blocks based on the struc-\nture of the Hessian matrix, assigning a unified second-order moment to all parameters within each\nblock (Zhang et al., 2024). This approach significantly reduces the memory footprint of the second-\norder moment, thereby decreasing the optimizer's memory usage. From a temporal perspective,\nGaLore shares the same projection across a fixed number of steps, reducing computational over-\nhead. The above discussion demonstrates that many parameters can be shared during fine-tuning,\nreducing memory usage or computational complexity.\nLow-Rank plus Sparse Matrix. Robust Principal Component Analysis (RPCA) decomposes a data\nmatrix into the sum of the product of low-rank matrices and a sparse matrix and has been extensively\nstudied in both theory and applications (Lin et al., 2010; Zhou & Tao, 2011; Liu et al., 2013; Aravkin\net al., 2014; Hinterm\u00fcller & Wu, 2015; Yi et al., 2016; Zhang & Yang, 2018). The recent Robust\nAdaptation (RoSA) method extends Low-Rank Adaptation (LoRA) by further decomposing weight\nupdates into the product of two low-rank matrices, with an additional sparse matrix (Nikdan et al.,\n2024). Using an optimizer to update both the low-rank and sparse matrices, RoSA achieves superior\nperformance compared to LoRA."}, {"title": "3 PRELIMINARIES", "content": "GaLore. Conventional PEFT methods, such as LoRA (Hu et al., 2022), reduce the number of\nparameters for fine-tuning LLMs. However, the fixed low-rank nature of these methods limits the\neffectiveness of weight updates, resulting in performance inferior to full fine-tuning. GaLore (Zhao\net al., 2024) addresses this limitation by leveraging the low-rank characteristics of gradients and\nprojecting them onto low-rank subspace, significantly reducing the memory requirements for fine-\ntuning LLMs, while still maintaining the capability for full-parameter tuning. This approach enables\npre-training an LLM with 7 billion parameters on a consumer-grade GPU, i.e., NVIDIA RTX 4090\nwith 24 GB memory. The low-rank projections in GaLore are calculated via SVD and updated at\nfixed intervals. Thus, the search space for parameters can dynamically change within the full-rank\nspace.\nTransformer. Multi-head attention is a key component in Transformer (Vaswani et al., 2017), en-\nabling the model to focus on different parts of the input sequence simultaneously. This mechanism\nemploys multiple attention heads, each processing the input sequence independently. Let Q, K, V\nbe dmodel-dimensional query, key, and value for the multi-head attention, the output of the i-th head\nis\n$head_{i} = Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$\n$= softmax(\\frac{(QW_{i}^{Q}(KW_{i}^{K})^{T})}{\\sqrt{d_{k}}})VW_{i}^{V},$\n(1)\nwhere $W_{i}^{Q} \\in \\mathbb{R}^{d_{model}\\times d_{k}}$, $W_{i}^{K} \\in \\mathbb{R}^{d_{model}\\times d_{k}}$, and $W_{i}^{V} \\in \\mathbb{R}^{d_{model}\\times d_{v}}$ are learned linear trans-\nforms. Moreover, the heads are concatenated and further transformed as follows.\n$MultiHead(Q, K, V) = [head_{1}, ..., head_{h}]W^{O},$\n(2)\nwhere $W^{O} \\in \\mathbb{R}^{hd_{v}\\times d_{model}}.$"}, {"title": "4 \u041c\u0415\u0422\u041dODS", "content": "To reduce the time consumption of GaLore while improving performance, we propose GaLore+,\nwhich introduces two key components, i.e., cross-head low-rank projection and sparsely coded resid-\nual. The cross-head low-rank projection enables efficient estimation of projection matrices with re-\nduced computational complexity. Meanwhile, the sparsely coded residual corrects the weight update\nerrors caused by low-rank projection, ensuring more accurate fine-tuning."}, {"title": "4.1 CROSS-HEAD LOW-RANK PROJECTION", "content": "GaLore demonstrates remarkable performance on PEFT, enabling the training of a 7B LLM on a\nGPU with just 24GB of memory. However, the SVD employed in GaLore (Zhao et al., 2024) is\ninherently time-consuming. Figure 2a presents the time consumption for SVD and other operations\nduring the fine-tuning of a LLaMA2-7B model using GaLore. The figure indicates that SVD ac-\ncounts for more than 80% of the time consumption of the whole fine-tuning process. Moreover,\nSVD for the multi-head attention (MHA) layers alone takes about half of the fine-tuning process.\n4.1.1 CROSS-HEAD SIMILARITY FOR SIMPLIFIED SVD\nSection 3 has introduced multi-head attention and linear transforms. Note that the three linear trans-\nformations across different heads share the same input and can be processed in parallel. Thus, in\npractical implementations, such as the PyTorch implementation, the linear transforms of different\nheads are concatenated into a single matrix. Here, we denote the concatenated transforms as\n$W^{Q} \\triangleq [W_{1}^{Q},W_{2}^{Q},...,W_{h}^{Q}],$\n$W^{K} \\triangleq [W_{1}^{K},W_{2}^{K},...,W_{h}^{K}],$\n$W^{V} \\triangleq [W_{1}^{V},W_{2}^{V},...,W_{h}^{V}].$\n(3)\nExisting low-rank based methods, such as LoRA (Hu et al., 2022) and GaLore (Zhao et al., 2024),\nconduct low-rank approximation on the updates of the concatenated multi-head transforms rather\nthan on the updates of each individual head. Specifically, GaLore applies SVD to the gradient\nof a concatenated transform to get the low-rank projection. For instance, the r-rank (r < hdk)\napproximation of the gradient $\\nabla W^{Q} \\in \\mathbb{R}^{d_{model}\\times h d_{k}}$ is\n$\\nabla W^{Q} = U\\Sigma V \\approx U_{:,r}\\Sigma_{r:r}V_{:,r}^{T} = PP^{T}\\nabla W^{Q},$\n(4)\nwhere $P \\triangleq U_{:,r}$ is the low-rank projection matrix that contains the first r columns of U. The\nlow-rank projections for $\\nabla W^{K}$ and $\\nabla W^{V}$ are calculated similarly.\nCordonnier et al. (2020) observed that the query or key transforms of different attention heads within\nthe same layer are similar. The authors show that the concatenated projection is low-rank even\nthough the projections of each head are of high ranks. Therefore, we hypothesize that the gradient\n$\\nabla W_{i}^{Q}$ (or $\\nabla W_{i}^{K}$) of different heads within the same layer are similar. Thus, we can obtain a\nlow-rank projection of the gradient of the concatenated transform $\\nabla W^{Q}$ (or $\\nabla W^{K}$) via SVD on a\nrandomly selected $W_{i}^{Q}$ (or $\\nabla W_{i}^{K}$), i.e.,\n$\\nabla W_{i}^{Q} = U_{i}\\Sigma_{i}V_{i} \\approx (U_{i})_{:,r}(\\Sigma_{i})_{:,r:r}(V_{i})_{:,r}^{T} = PP^{T}\\nabla W^{Q},$\n(5)\nwhere $P_{i} \\triangleq (U_{i})_{:,r}$. Thus, the low-rank approximation of $\\nabla W^{Q}$ in the proposed GaLore+ is\nachieved by with\n$\\nabla W^{Q} \\approx PP^{T}P_{i}^{T}\\nabla W^{Q}.$\n(6)\nFigure 2b illustrates the approximation error of the gradients $\\nabla W_{i}^{Q}$ with a randomly selected multi-\nhead attention head using the low-rank projections using Equation 4 (vanilla GaLore) and Equation 6\n(cross-head low-rank projection). The errors in Equation 6 are larger than those in Equation 4, as\nshown in Figure 2b. We further introduce sparsely coded residual to reduce for this discrepancy, as\ndiscussed in Subsection 4.2. Since the computational complexity of SVD is O(mn \u00d7 min(m, n))\nfor a matrix of m \u00d7 n elements, the computational complexity for the SVD operations of $\\nabla W^{Q} \\in$\n[$\\mathbb{R}^{d_{model}\\times h d_{k}}$ and $\\nabla W^{K} \\in$ [$\\mathbb{R}^{d_{model}\\times h d_{k}}$ can be reduced from O($h^{3}d$) to O($hd$). As a reference,\nh is set to 32 in LLaMA2-7B.\n4.1.2 FAST SVD WITH RANDOMIZED SUBSPACE ITERATION\nTo further reduce the time consumption on SVD, we adopt the randomized subspace iteration algo-\nrithm proposed by Halko et al. (2011), which is a fast implementation of SVD. To obtain the m\u00d7 r"}, {"title": "4.2 SPARSELY CODED RESIDUAL", "content": "The low-rank projection of the gradients can significantly reduce the memory consumption during\nfine-tuning, along with the low-rank first- and second-order moments for optimizers. However,\nthe low-rank projection may not always be the main component of practical implementations. For\ninstance, the low-rank projections are updated every hundred steps during fine-tuning due to the\nhigh computational complexity of SVD, making it challenging to apply low-rank projections at\nevery step. Moreover, the cross-head low-rank projection introduces increased approximation error\ndue to cross-head sharing and randomized subspace iterations, leading to less accurate SVD results.\nConsequently, the low-rank first- and second-order moments for the optimizers are also imprecise.\nTo address this, we estimate the residuals using a sparse representation, which improves the quality\nof the first- and second-order moments.\n4.2.1 LoW-RANK APPROXIMATION RESIDUAL OF MOMENTS\nLet $G_{t} \\in \\mathbb{R}^{m\\times n}$ be the gradient of t-th step, and $P_{t} \\in \\mathbb{R}^{m\\times r}$ be the low-rank projection of t-th\nstep, the residual of low-rank approximation of $G_{t}$ is\n$\\Delta G_{t} = G_{t} - P_{t}P_{t}^{T}G_{t}.$\n(7)\nThe first- and second-order moments (denoted as $M_{t}$ and $V_{t}$, with $M_{t}, V_{t} \\in \\mathbb{R}^{m\\times n}$) for common\noptimizers, such as Adam, AdaGrad, and AdamW, are estimated as\n$M_{t} = \\beta_{1}M_{t-1} + (1 - \\beta_{1})G_{t},$\n$V_{t} = \\beta_{2}V_{t-1} + (1 - \\beta_{2}) G_{t} \\odot G_{t},$\n(8)\nwhere $\\beta_{1}$ and $\\beta_{2}$ are decay rates of the moments, and $\\odot$ means element-wise multiplication. The\nlow-rank first- and second-order moments (denoted as $M_{t}^{'}$ and $V_{t}^{'}$, with $M_{t}^{'}, V_{t}^{'} \\in \\mathbb{R}^{r\\times n}$) used in\nGaLore are realized by\n$M_{t}^{'} = \\beta_{1}M_{t-1}^{'} + (1 - \\beta_{1})P_{t}^{T}G_{t},$\n$V_{t}^{'} = \\beta_{2}V_{t-1}^{'} + (1 - \\beta_{2})(P_{t}^{T}G_{t}) \\odot (P_{t}^{T}G_{t}).$\n(9)\nThe low-rank approximation residuals of the moments are\n$\\Delta M_{t} = M_{t} - P_{t}P_{t}^{T}M_{t}^{'}, \\Delta V_{t} = V_{t} - P_{t}P_{t}^{T}V_{t}^{'}.$\n(10)\nEquation 10 can be extended into the following form considering Equation 7, 8, and 9,\n$\\Delta M_{t} \\approx \\beta_{1}\\Delta M_{t-1} + (1 - \\beta_{1}) \\Delta G_{t},$\n$\\Delta V_{t} \\approx \\beta_{2}V_{t-1} + 2(1 - \\beta_{2}) (P_{t}P_{t}^{T} G_{t}) \\Delta G_{t},$\n(11)\nThe detailed inference is provided in Appendix A. Equation 11 depicts the evolution of the low-\nrank approximation residual of the moments during fine-tuning. Thus, we employ two additional\nvariables in fine-tuning as the estimate of the low-rank approximation residual, and the two variables\nupdate following Equation 11.\nFurthermore, the approximation residual of the moments leads to bias in parameter updates. The\nbias formulation depends on the specific form of the optimizer, and we use AdamW as an example."}, {"title": "5 EXPERIMENTS", "content": "In this section, we present a series of experiments to evaluate the effectiveness of GaLore+. We\ncompare our proposed method against a range of baselines in fine-tuning the LLaMA2-7B and\nLLaMA2-13B models (Touvron et al., 2023), specifically focusing on tasks related to arithmetic\nreasoning and natural language generation, to assess its overall performance.\nImplementation Details. We fine-tune all layers of the LLaMA2-7B and LLaMA2-13B models,\nadding sparsely coded residuals only in the query and key projections, and load the parameters in"}, {"title": "5.3 ABLATION STUDY", "content": "To evaluate the effectiveness of the proposed sparsely coded residual, we conduct experiments on\nGaLore+ with increasing number of non-zero elements in the sparse indexing matrix. All hyper-\nparameters except for the sparsity of the residuals are kept unchanged for reasonable ablation. For\nrobustness of the results, experiments were conducted on both LLaMA2-7B and LLaMA3-8B mod-\nels, with four random seeds set for each experiment. The standard deviations for LLaMA2-7B and\nthe results of LLaMA3-8B are provided in Appendix D.\nFigure 3 presents the results of the comparison, suggesting that the residuals with more non-zero\nelements lead to better performance in most cases which indicates that the sparsely coded residuals\nindeed mitigate projection-induced errors and enhance the model's performance."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we propose GaLore+, a low-rank adaptation method with fast yet precise estimation of\nthe low-rank projections. The estimation is achieved by cross-head low-rank projection and random-\nized subspace iteration. We further employ a sparsely coded residual to further reduce the error of\nlow-rank approximation, which works on the moments of the optimizer. Experiments on arithmetic\nreasoning and natural language generation indicate that GaLore+ outperforms existing low-rank\nadaptation methods, offering superior performance with reduced computational complexity."}]}