{"title": "The application of GPT-4 in grading design university students' assignment and providing feedback: An exploratory study", "authors": ["Qian Huang", "Thijs Willems", "Poon King Wang"], "abstract": "This study aims to investigate whether GPT-4 can effectively grade assignments for design university students and provide useful feedback. In design education, assignments do not have a single correct answer and often involve solving an open-ended design problem. This subjective nature of design projects often leads to grading problems, as grades can vary between different raters, for instance instructor from engineering background or architecture background. This study employs an iterative research approach in developing a Custom GPT with the aim of achieving more reliable results and testing whether it can provide design students with constructive feedback. The findings include: First, through several rounds of iterations the inter-reliability between GPT and human raters reached a level that is generally accepted by educators. This indicates that by providing accurate prompts to GPT, and continuously iterating to build a Custom GPT, it can be used to effectively grade students' design assignments, serving as a reliable complement to human raters. Second, the $intra$-reliability of GPT's scoring at different times is between 0.65 and 0.78. This indicates that, with adequate instructions, a Custom GPT gives consistent results which is a precondition for grading students. As consistency and comparability are the two main rules to ensure the reliability of educational assessment, this study has looked at whether a Custom GPT can be developed that adheres to these two rules. We finish the paper by testing whether Custom GPT can provide students with useful feedback and reflecting on how educators can develop and iterate a Custom GPT to serve as a complementary rater.", "sections": [{"title": "1.Introduction", "content": "Since its inception, ChatGPT has revolutionized various industries, including education (Montenegro-Rueda et al., 2023). The potential for ChatGPT to transform educational settings has sparked significant debate and discussion (Arif, Munaf & Ul-Haque, 2023; Hwang & Chen, 2023). A growing number of educators recognizes the necessity of embracing technology to enhance learning outcomes (Sim, 2023). Universities, such as Arizona State University, have integrated ChatGPT into their teaching and research frameworks, demonstrating its practical applications\u00b9. Moreover, organizations and educational institutions have begun establishing guidelines for the ethical and effective use of generative Al in educational contexts (Halaweh, 2023). The advent of GPT-4 has marked a significant leap forward in Al accuracy and utility in academic contexts. For instance, GPT-4 improved translation accuracy significantly (Chen, 2023); GPT-4 demonstrated superior performance on medical competency examinations (Nori et al., 2023); and Custom GPT models, for instance, \"Scholar GPT\u00b2\" has been developed to enhance research productivity and tailor solutions to specific professional needs.\nDespite these improvements, GPT-4 struggles with reasoning tasks and is often unable to consistently solve complex problems (Arkoudas, 2023). GPT-4, while improved from earlier versions, is still prone to hallucination and fabrication in responses (Currie, 2023). Concerns also persist regarding the potential erosion of students' critical thinking and problem-solving skills due to over-reliance on Al (Luckin, 2017). Ethical considerations, including data privacy and inherent biases in Al algorithms, remain significant (Binns & Veale, 2023). Al systems, including ChatGPT, can inadvertently perpetuate biases present in their training data, leading to unfair outcomes for certain student groups (Weidinger et al., 2022). Therefore, continuous efforts to audit and refine Al models are essential to ensure fairness and inclusivity (Halaweh, 2023).\nIn light of the rapid advancements brought by GPT-4 and the development of custom GPT models, the education sector stands at a critical juncture. While these technologies offer impressive enhancements to research and pedagogy, their potential downsides cannot be overlooked. This duality presents a complex challenge for educators and policymakers: how to harness the benefits of Al tools like ChatGPT while mitigating their risks. This is not only about leveraging technology for academic excellence but also about ensuring that it serves as a tool for equitable and ethical education (Luckin, Holmes, & Griffiths, 2022). Moving forward, it becomes crucial to develop robust mechanisms for monitoring Al's impact on learning environments and to ensure that educators are well-equipped to utilize these technologies in ways that truly enhance student learning and critical thinking skills.\nWhile the emergence of many studies discussing the impact of Gen-Al on education is important and crucial in assuring that its developments vis-a-vis learning and teaching are monitored (Hwang & Chen, 2023; Kadaruddin, 2023), we lack a robust understanding of Gen-Al in practice and how it either benefits or hampers educators and/or students. To this end, this study sets out to empirically study the extent to which a Custom GPT-4 model can assist teachers in one of their core competencies: the grading of students' assignments. Specifically, we look at how a Custom GPT can or cannot be of help in the grading of subjective and open-ended assignments, in our case design projects. While the grading of yes/no or multiple-choice questions is arguably more straightforward, assessing"}, {"title": "2.Literature", "content": "subjective assignments can even amongst human raters be a controversial topic with different raters giving different grades.\nThis research addresses the critical gap by developing and empirically testing custom GPT-4 models to evaluate their effectiveness as grading assistants in a design course at a Singaporean design university. Through multiple iterations, we aim to assess and enhance the accuracy and reliability of GPT in grading tasks. This study not only seeks to validate the use of GPT for educational assessments but also provides actionable guidelines to educators on leveraging this technology effectively. The goal of this exercise is to determine the conditions and GTP instructions under which a custom GPT can act as a reliable grader that human raters can use to complement their work. We deliberately use the word complement here, because even though we found that custom GPT-models can provide reliable and consistent assessments we think that the domain knowledge of educators remains crucial. However, assessments done by a custom GPT can serve as a useful benchmark or act as an additional evaluator if different human raters are in disagreement.\nThe two research questions of this study are: 1) Can GPT accurately grade students' assignments in a design course, and 2) under which conditions can a Custom GPT act as a reliable grader to serve as a complementary rater? By answering these questions, this research aims to verify through empirical studies, involving multiple iterations, whether GPT can be an accurate and reliable grading assistant. Additionally, it seeks to provide guiding principles for educators on how to achieve this reliability."}, {"title": "2.1 ChatGPT in education", "content": "ChatGPT, developed by OpenAl, is a generative pre-trained transformer that has seen widespread adoption in various sectors, including education. Its capabilities extend from providing personalized learning experiences to facilitating administrative tasks, thereby reshaping traditional educational landscapes (Halaweh, 2023; Poon, Willems, & Huang, 2024, Strzelecki, 2023; Zhu & Li, 2023).\nEducators can leverage ChatGPT to assist in creating and delivering content. The Al can generate lecture materials, interactive discussions, and tailor assessments to student needs (Chung & Park, 2023; Kohnke & Zou, 2023). For instance, Al-driven tools like ChatGPT can revolutionize traditional assessment methods. They support innovative testing approaches, such as adaptive testing, which aligns the difficulty of test items with the student's ability level, potentially leading to more accurate measurements of student knowledge and skills (Sok & Heng, 2024).\nThese transformative capabilities in the application of GPT in education can be categorized in several key areas. Academic writing: Studies have explored the potential of Chat GPT as a tool for academic editing, highlighting its abilities in error detection, writing improvement, and content generation (Tao & Zhang, 2023; Zhu & Wang, 2023). Personalized feedback: Studies have explored using GPT-3.5 to generate personalized feedback for programming assignments (Wang & Yang, 2023; Wilson & Evans, 2022). Preparing teaching material: It can be employed to design teaching contents, materials, quizzes (Anderson & Morgan, 2023). Analyzing qualitative data: Some studies have explored how ChatGPT can be used to enhance research capabilities in several fields by helping with data analysis (Wang & Liu, 2023).\nBuilding on the versatile applications of ChatGPT in education, it becomes evident that this technology not only supports traditional educational functions but also introduces innovative evaluation mechanisms that enhance reliability and precision in assessments. The transition from Al-assisted content creation and personalized learning tools to sophisticated grading systems exemplifies how Al can bridge the gap between educational content delivery and assessment accuracy. This shift underscores the importance of integrating reliable Al tools, like ChatGPT, to maintain the integrity and fairness of educational evaluations. By leveraging Al for both teaching and assessment, educators can ensure a more cohesive and supportive learning environment that aligns teaching methods with evaluation practices."}, {"title": "2.2 Reliability of Educational Assessment", "content": "Ensuring the reliability of educational assessments is critical for valid and equitable evaluation of student performance. This is even more so the case when evaluating the accuracy of a Custom GPT rater. To evaluate the above, we can draw on two key indicators that were initially developed in the field of psychometrics and have since then been applied in fields such as the social sciences, mathematics, education, etc.: inter-rater reliability, and intra-rater reliability. The latter concerns the consistency of a single rater over time, whereas the former concerns the consistency among multiple raters. Despite the theoretical discussions on GPT's potential as a grading assistant, empirical studies remain scarce. It is imperative to bridge this gap by translating theoretical insights into practical applications and providing clear guidelines for educators.\nIntra-Rater Reliability\nIntra-rater reliability refers to the degree to which the same examiner gives consistent ratings of the same subject on different occasions. High intra-rater reliability is essential as it indicates that assessments are not influenced by external factors such as mood, environment, or temporal context, which could potentially bias the rater's judgment. For instance, a study by Ling, Mollaun, and Xi (2014) found that fatigue could negatively affect a human rater's scoring quality on constructed responses.\nIntra-rater reliability is commonly measured using the intraclass correlation coefficient (ICC). An ICC above 0.70 is generally considered good, indicating strong consistency, while"}, {"title": "3.Research methods", "content": "values between 0.60 and 0.74 are considered acceptable for normal assessments. For high-stakes assessments, an ICC or kappa value above 0.85 is recommended to ensure maximum reliability (Saxton et al., 2012).\nIntraclass Correlation Coefficient (ICC) is a commonly used statistic for evaluating consistency or agreement among raters, which is, inter-reliability of multiple raters. It measures the reliability of ratings by comparing the variability of different ratings of the same subject to the total variation across all ratings and all subjects. The two-way ANOVA model is often used to estimate the ICC by treating both subjects and raters as random effects, which helps in understanding the variability attributed to different sources Cicchetti, 1994; Weir, 2005).\nInter-Rater Reliability\nInter-rater reliability is typically measured using intraclass correlation coefficient (ICC). For normal assessments, an ICC value of 0.70 to 0.80 is considered satisfactory (Cicchetti, 1994). Inter-rater reliability is crucial for ensuring consistency among multiple graders. A study showed that trained raters achieved acceptable levels of inter-rater reliability (a \u2265 0.70) across all rubric categories, highlighting the importance of comprehensive rater training and clear assessment criteria (Saxton et al., 2012).\nFurther, \u015eahan and Raz\u0131 (2020) explored how raters with different levels of experience approached the assessment of essays. They found that raters prioritized different aspects of essays based on their quality and that rater behaviors evolved with practice, indicating that training impacts raters differently depending on their experience. While Weigle (2002) emphasized that detailed rubrics and ongoing rater training are essential for maintaining high inter-rater reliability in language assessments.\nThe background of this research takes place in a design thinking course for first-year students at a university in Singapore. The researchers observed in this course that each class had one engineering instructor and one architecture instructor. Student feedback often indicated that the advice given by the two instructors was different, and the students did not know how to balance the feedback from both teachers. Therefore, the researchers attempted to use ChatGPT-4 to grade the design assignments of 10 students. Each student submitted 2 poster designs, making a total of 20 screenshots inputted into ChatGPT-4.\nIn terms of the testing the reliability of ChatGPT grading, this study employed a Design-Based Research (DBR) approach, which is commonly used in educational experiments and reforms (Campanella & Penuel, 2021). Through multiple iterations and prototypes, the researchers aimed to find better ways to address the research questions. This iterative process is fundamental in DBR, allowing for continuous refinement and adaptation of"}, {"title": "4. Findings", "content": "strategies based on the results obtained from each phase of testing and prototypes. Figure 1 shows the process of DBR in this study.\nAs Figure 1 shows, there are three parts to this study. Part I is the 1st comparison line of the GPT's performance without role-playing. The 1st iteration uses General GPT to grade students' design assignments; the 2nd iteration uses ChatGPT-4 to create a Custom GPT, incorporating rubrics set by instructors; the 3rd iteration provides a good example in the Custom GPT, which is an assignment both instructors agree is good. It is well known in the literature, that giving examples of what is deemed as good output is a crucial element of prompting, that the GPT can then use as a benchmark against which they assess subsequent input provided. (Reynolds, & McDonell, 2021).\nPart II is the second comparison line of GPT's performance, in which we have included role-play instructions, meaning that each iteration is evaluated from the perspectives of an instructor in a specific domain (in our case, architecture instructors and an engineering instructors). Specifically: the 1st iteration used General GPT for role-play grading; the 2nd"}, {"title": "4.1 (Part I) Inter-reliability between GPT and human raters without role-playing", "content": "iteration used Custom GPT for role-play grading (after adding Rubrics); the 3rd iteration used a Custom GPT with added rubrics and a good example for role-play.\nPart III is the third comparison line assessing the intra-reliability of GPT itself. Here, we checked if the custom GPT evaluates a specific assignment consistently over time.\nThis study aims to answer two overarching research questions:\n1) Can GPT accurately grade students' assignments in a design course, and 2) under which conditions can a Custom GPT act as a reliable grader to serve as a complementary rater?\nAssumption 1: In the iterations without role-play (Part I), inter-reliability increases progressively;\nAssumption 2: In the iterations with role-play (Part II), inter-reliability increases progressively;\nAssumption 3: When using GPT to score at different times (Part III), intra-reliability is greater than 0.5."}, {"title": "1st iteration: General GPT", "content": "Process:\nWe gave the following prompt to General GPT to score the students' assignments\u00b3.\nBecause we need to compare the reliability with instructors' scoring, General GPT was given the same standards and scoring range as the instructors. However, detailed rubrics were not provided to General GPT in this iteration.\n(Prompt given): Can you grade the following projects with the following three dimensions: 1) Design goal: whether it is appropriate, clear, and concise framing of the design goal from project, process, and skills perspectives (Full mark 15) 2) Site drawing: whether it shows at an appropriate level of abstraction with enough detail the key elements (Full mark 35) 3) Macro-AEIOU\u2074: whether it provides an impactful sequence of visuals and comprehensive layers (Full mark 50)\nResults:\nAs shown in Table 1, the inter-reliability between the Architect Instructor and Engineer Instructor is not high, as they are scoring from two different perspectives. In this iteration, the inter-reliability between GPT and the two instructors is also not high. The inter-reliability between GPT and Engineer instructor slightly higher with the average scores of the two instructors at 0.4730, but still below 0.5."}, {"title": "2nd iteration: Custom GPT (with rubrics added)", "content": "Process:\nResearchers used the rubrics provided by the instructors as prompts to build a Custom-GPT model. The building process included the following steps:\nAfter building the custom-GPT, the researchers took screenshots of each student's design assignment and fed these screenshots into the Custom-GPT one by one. The GPT was programmed to provide scores as well as feedback based on the rubrics for each student, including suggestions for their improvement.\nResults:\nIn this iteration, the inter-reliability between GPT and the two instructors has improved. The ICC with the Architect Instructors increased by 0.2378, and with the Engineer Instructors by 0.1073, reaching 0.5803. At the same time, the inter-reliability with the average scores of the two instructors also improved by 0.3055, exceeding 0.5 (0.5672)."}, {"title": "3rd iteration: Custom GPT (with a good example added)", "content": "Process:\nThe authors revised the instructions to the Custom-GPT as shown in the following screenshots, by giving it an example of what would be considered a good assignment.\n(Prompt given): Please rate the following assignments based on the rubrics given to you; and refer to the good example given to you.\nResults:\nIn this iteration, the inter-reliability between GPT and the two instructors continued to improve. The ICC with the Architect Instructors increased by 0.1018, and with the Engineer Instructors, it exceeded 0.7 (0.7652). At the same time, the inter-reliability with the average scores of the two instructors reached 0.7285. This already meets the requirements for raters' inter-reliability in high-stake exams.\n* All ICC data analysis in Appendix 1\nTo sum up, the 4.1 section discussed the above answers the Assumptions 1 that in the iterations without role-play (Part I), inter-reliability increases progressively."}, {"title": "4.2 (Part II) Inter-reliability between GPT and human raters with role-play", "content": "1st Iteration: General GPT with Role-play\nProcess:\nBased on the 1st iteration with General GPT, this time we let General GPT grade the students' assignments from the perspectives of both the architect instructor and the engineer instructor.\nResults:\nFrom the table below, it can be seen that the inter-reliability between Architect GPT and the Architect instructor is very low (0.1874), while the inter-reliability between Engineer GPT and the Engineer instructor is 0.5423 (Moderate reliability). The inter-reliability between the two GPTs (Architect GPT and Engineer GPT) is 0.5407 (Moderate reliability)."}, {"title": "2nd iteration: Custom GPT (with rubrics added, with role-playing)", "content": "Process:\nThis iteration involved adding the rubrics used by the two instructors, and then conducting role-play to score from the perspectives of both the architect instructor and the engineer instructor.\nResults:\nThe inter-reliability between Architect GPT and the Architect instructor increased by 0.2068, while the inter-reliability between Engineer GPT and the Engineer instructor decreased by 0.3024. The inter-reliability between the two GPTs (Architect GPT and Engineer GPT) decreased by 0.0132, but still exceeded 0.5. Therefore, in the next round, we will see if providing a good example to Custom-GPT will improve the reliability in each category.\nUpon re-examining the details of the study, the fact that the Inter-Class Correlation (ICC) results in Table 4 (which uses general GPT) are higher than those in Table 5 (which uses custom GPT) indeed seems counterintuitive. The possible reasons include: The role-playing aspect adds another layer of complexity, as it requires the GPT model to simulate the perspectives of different types of instructors (architect and engineer). The custom GPT, being more finely tuned to specific rubric criteria, might struggle to adapt its grading process when switching between these varied instructional perspectives. Secondly, a \"good example\u201d may be needed for the GPT to reference. Without this example, the Custom GPT might still have had difficulty fully aligning its grading with the nuanced expectations of the human instructors."}, {"title": "3rd iteration: Custom GPT (with a good example added, with role-play)", "content": "Process:\nThis iteration included a good example, and then scoring was conducted from the perspectives of both the architect instructor and the engineer the instructor.\nResults:"}, {"title": "4.3 (Part III) Intra-reliability of GPT accessed in three different times", "content": "To verify the intra-reliability of GPT's scoring over different times, researchers randomly conducted three comparisons on different days, using General GPT to grade the same student assignments.\nThe ICC among these 3 iteration is 0.6485. The ICC between 1st and 2nd: 0.7336. The ICC between 2nd and 3rd is 0.7750. The ICC between 1st and 3rd: 0.6983. Thus, the intra-reliability of GPT is high.\nTo sum up, this section discussed the above answers the Assumption 3 that when using GPT to score at different times (Part III), intra-reliability is greater than 0.5. The 4.1 through 4.3 sections answered RQ1 by verifying the three assumptions"}, {"title": "4.4 Custom GPT's role in providing feedback", "content": "Besides, while GPT was grading students' assignment, it provided with personalized feedback, no matter without or with role-playing. As Screenshot 4 shows, the Custom-GPT knows the perspectives from both engineer instructor and Architect instructor. It provides feedback from different perspectives so that students can receive the different opinions and balance different feedback in a more creative way (Screenshot 3).\nThus, GPT-4 is an effective tool for providing students with timely feedback, as instructors lack the capacity to: 1) provide individual feedback to a large number of students; 2) offer advice to students at all times (7 days a week, 24 hours a day). For design students, GPT can make a significant contribution to providing timely feedback to students, and it is worth considering its use on a large scale to continuously encourage students to promptly improve their design assignments and provide students with synthesis of instructors from different background \u2013 in this research, engineer instructor and architect instructors' perspective (shown in Screenshot 5). At the meantime, GPT can could serve as a reference or a co-worker for instructors. For instance, if instructors find a significant discrepancy in the scores of a particular student, they can focus on why this discrepancy occurs, and even question whether the rubrics were scientifically set."}, {"title": "Discussion", "content": "Furthermore, Custom-GPT can be given prompts to compare students' work with that of a renowned designer, providing directions and suggestions for emulation. This approach could involve GPT analyzing notable works in the field of design and drawing parallels between these works and the students' assignments. GPT could then offer constructive feedback on how students might incorporate certain elements or techniques from established designers into their own work to improve creativity, functionality, and aesthetic appeal. This comparative analysis could serve as an inspirational and educational tool, helping students understand the benchmarks in their field and aspire to reach or exceed them. For instance, Screenshot 8 shows the comparison between Teamlab and students' project and GPT provided further suggestions on improvement to students.\nThus, during GPT's grading process, it has many other roles: Firstly, it can provide 24-hour/day and 7-days/week personalized feedback to students. Secondly, through personalized feedback and being given prompts, it can stimulate students' ideas and inspirations. Thirdly, it can serve as a co-worker with instructors to provide references during instructors' grading process.\nThis study demonstrates through two lines of iteration how GPT can effectively grade students' design assignments. We found that with the appropriate instruction the evaluations of custom GPTs can have high inter-reliability and intra-reliability. Using educational design-based research, this study provides a detailed demonstration of each iteration's process and offers the following generalized principle guidelines:\n1) Provide GPT with accurate and consistent prompts;\n2) Establish Custom-GPT with rubrics consistent with those used by instructors;\n3) Give Custom-GPT a good example to clarify what constitutes a good project/assignment.\nThus, this study verifies using ChatGPT to mimic human raters in scoring and normalizing grading processes can achieve high inter-reliability with human raters. The implications of our findings indicate that GPT, under the right set of conditions, can serve as a co-worker to instructors in educational settings (Mollick, 2023). In terms of grading, both intra-rater and inter-rater reliability are two key rules for the validity and fairness of educational assessments. The studies reviewed herein underscore the importance of structured rater training, the use of clear rubrics, and the necessity for regular calibration and feedback"}, {"title": "Conclusion", "content": "mechanisms. As educational methods continue to evolve, so too must the approaches to ensuring that assessments are reliable and equitable. GPT models offer promising tools to support this process but must be integrated thoughtfully to avoid introducing new biases and complexities. Continuing to innovate in the tools and methods used to train and calibrate raters will be essential in maintaining the integrity and fairness of educational evaluations.\nOn the one hand, GPT models can support the process by providing consistent, unbiased scoring guidelines and assisting in the training and calibration of human raters. For example, automated feedback and analysis provided by GPT models can help raters understand and correct their biases, leading to improved reliability over time. On the other hand, the introduction of GPT models brings new complexities into the equation. There are concerns about the transparency of Al-driven decisions and the potential for the technology to perpetuate existing biases if not properly monitored. Additionally, reliance on Al tools may lead to reduced critical engagement by human raters, potentially affecting the overall quality of the assessments.\nBesides, as our findings show, the feedback provided by Custom-GPT can promote personalized learning for students. It aligns with insights from recent studies. One study, for instance, found that ChatGPT can promote personalized and interactive learning, particularly in generating prompts for formative assessment and ongoing feedback, although concerns exist regarding the potential for generating incorrect information and inherent biases (Baidoo-Anu & Owusu Ansah, 2023). ChatGPT has demonstrated efficacy in providing detailed feedback on language learning, with its evaluations closely aligning with those of experienced instructors (Dai et al., 2023). From this study, GPT can serve as a co-worker for educators to increase productivity.\nThis research has provided instructors and educators with insights on how to better utilize technology to assist in grading. Firstly, it emphasizes the need to embrace technology as a means to enhance work efficiency through interaction between humans and technology. Secondly, the study illustrates the iterative process of building a Custom-GPT in evaluating design students' assignments by showing hands-on empirical process. It offers practically meaningful prompts and process in using GPT-4 as co-workers. Thirdly, the study highlights the need for more empirical studies in demonstrating the impact of using GPT in design students' learning processes."}]}