{"title": "Provably Efficient Exploration in Inverse Constrained Reinforcement Learning", "authors": ["Bo Yue", "Jian Li", "Guiliang Liu"], "abstract": "To obtain the optimal constraints in complex environments, Inverse Constrained\nReinforcement Learning (ICRL) seeks to recover these constraints from expert\ndemonstrations in a data-driven manner. Existing ICRL algorithms collect training\nsamples from an interactive environment. However, the efficacy and efficiency\nof these sampling strategies remain unknown. To bridge this gap, we introduce a\nstrategic exploration framework with provable efficiency. Specifically, we define a\nfeasible constraint set for ICRL problems and investigate how expert policy and\nenvironmental dynamics influence the optimality of constraints. Motivated by our\nfindings, we propose two exploratory algorithms to achieve efficient constraint in-\nference via 1) dynamically reducing the bounded aggregate error of cost estimation\nand 2) strategically constraining the exploration policy. Both algorithms are theo-\nretically grounded with tractable sample complexity. We empirically demonstrate\nthe performance of our algorithms under various environments.", "sections": [{"title": "1 Introduction", "content": "Constrained Reinforcement Learning (CRL) addresses sequential decision-making problems within\nsafety constraints and achieves considerable success in various safety-critical applications [1]. How-\never, in many real-world environments, such as robot control [2, 3] and autonomous driving [4]\nspecifying the exact constraint that can consistently guarantee the safe control is challenging, which\nis further exacerbated when the ground-truth constraint is time-varying and context-dependent.\nInstead of utilizing a pre-defined constraint, an alternative approach, Inverse Constrained Reinforce-\nment Learning (ICRL) [5, 6], seeks to learn the constraint signals from the demonstrations of expert\nagents and imitate their behaviors by adopting the inferred constraint. ICRL effectively incorporates\nexpert experience into the online CRL paradigm and thus better explains how expert agents optimize\ncumulative rewards under their empirical constraints. Under this framework, existing ICRL algo-\nrithms often assume the presence of a known dynamics model [7, 8], or a generative transition model\nthat responds to queries for any state-action pair [9, 10]. However, this setting has a considerable gap\nwith scenarios in practice where the transition models are often not available, or even time-varying,\nnecessitating agents to physically navigate to new states to learn about them through exploration.\nTo mitigate the gap, some recent studies [5, 11, 12] explicitly maximized the policy entropy throughout\nthe learning process, yielding soft-optimal policy representations that favor less-selected actions.\nUnfortunately, such an uncertainty-driven exploration largely ignores the potential estimation errors\nin dynamic models or policies. To date, it still lacks a theoretical framework to demonstrate how well\nthe maximum entropy approaches facilitate the accurate estimation of constraints."}, {"title": "", "content": "In this paper, we introduce a strategic exploration framework to solve ICRL problems with provable\nefficiency. Recognizing the inherent challenge in uniquely identifying the exact constraint from expert\ndemonstration, the objective of our framework is to recover the set of feasible constraints where each\nelement can accurately align with expert preferences, rather than to identify an exact constraint. By\nexplicitly representing these constraint sets with the reward advantages and the transition model, we\nmanage to confine the constraint estimation error with the discrepancy by comparing the estimated\nenvironmental dynamics and expert policy with the ground-truth ones. This strategy provides a\nquantifiable measure of error for our constraint estimation, linking it directly to a computationally\ntractable upper bound.\nUnder our framework, we design two strategic exploration algorithms for solving ICRL problems:\n1) A Bounded Error Aggregate Reduction (BEAR) strategy, which guides the exploration policy\nto minimize the upper bound of discounted cumulative constraint estimation error; and 2) Policy-\nConstrained Strategic Exploration (PCSE), which diminishes the estimation error by selecting an\nexploration policy from a predefined set of candidate policies. This collection of policies is rigorously\nestablished to encompass the optimal policy, thereby promising to accelerate the training process\nsignificantly. For both algorithms, we provide a rigorous sample complexity analysis, furnishing a\ndeeper understanding of the training efficiency of these algorithms.\nTo empirically study how well our method captures the accurate constraint, we conduct evaluations un-\nder different environments. The experimental results show that PCSE significantly outperforms other\nexploration strategies and is applicable to continuous environments. Limitations, future directions\nand broader impacts are discussed in Appendix E and F."}, {"title": "2 Related Work", "content": "In this section, we introduce previous works that are most related to our algorithms.\nExploration in Inverse Reinforcement Learning (IRL). Compared to the exploration strategies\nin RL for forward control [13, 14], the exploration algorithms in IRL has relatively limited studies.\n[15] utilized Bayesian optimization to identify multiple IRL solutions by efficiently exploring the\nreward function space. To learn a transferable reward function, [16] introduced an active sampling\nmethodology that is designed to target the most informative regions with a generative model to\nfacilitate effective approximations of the transition model and the expert policy. A subsequent\nresearch [17] expanded this concept to finite-horizon MDPs with non-stationary policies, crafting\ninnovative strategies to accelerate the exploration process. To better quantify the precision of\nrecovered feasible rewards, [18] recently devised a framework that employs the Hausdorff metric\nand provides more rigorous theoretical bounds. These methods study only reward function under a\nregular MDP without considering the safety of control or the constraints in the environments.\nInverse Constrained Reinforcement Learning (ICRL). Unlike IRL that solely focuses on the\nrecovery of reward functions, ICRL seeks to elucidate the preference of expert agents by inferring\nwhich constraints they follow. The majority of ICRL algorithms update the cost functions by\nmaximizing the likelihood of generating the expert dataset under the maximum (causal) entropy\nframework [7]. This method can be efficiently scaled to both discrete [8] and continuous state-action\nspace [5, 10, 11, 12, 19, 20]. [9] recently proposed a Bayesian approach for learning a posterior\ndistribution of constraints. To improve training efficiency, recent studies combined ICRL with bi-level\noptimization techniques [21, 22]. However, current ICRL methods have not explored exploration\nstrategies or conducted theoretical studies about the sample complexity of their algorithms."}, {"title": "3 Preliminaries and Problem Formulation", "content": "Notation. Let X be a finite set and Y be a space. The notation $Y^X$ represents the set of functions\n$f : X \\rightarrow Y$. The simplex over X is denoted as $\\Delta^X = {\\varphi \\in [0,1]^X : \\sum_{x \\in X} \\varphi(x) = 1}$ and we\ndenote $\\Delta^\\mathcal{X}$ as the set of functions $X \\rightarrow \\Delta^{\\mathcal{A}}$. Let $\\psi \\in \\Delta^X$ and $f \\in \\mathbb{R}^X$. We use the shorthand\nnotation $\\psi^T f = \\sum_{x \\in X} \\psi(x)f(x)$, where $T$ serves as an operator. We define the $L_\\infty$-norm of\n$f$ as $|| f ||_\\infty = \\max_{x \\in X} |f(x)|$. Concretely, for a vector a, we define the vector infinity norm as\n$||a||_\\infty = \\max_i|a_i|$. For a matrix A, we define the matrix infinity norm as $||A||_\\infty = \\max_i \\sum_j |A_{ij}|$.\nConstrained Markov Decision Process (CMDP). We model the environment as a stationary CMDP\n$\\mathcal{M}^\\mathcal{U}_c := (\\mathcal{S}, \\mathcal{A}, P_T, r, c, \\epsilon, \\mu_0, \\gamma)$, where $\\mathcal{S}$ and $\\mathcal{A}$ are the finite state and action spaces, with the\ncardinality denoted as $S = |\\mathcal{S}|$ and $A = |\\mathcal{A}|$; $P_T(s'|s, a) \\in \\Delta^\\mathcal{S}$ defines the transition distribution;\n$r(s,a) \\in [0, R_{\\text{max}}]$ and $c(s, a) \\in [0, C_{\\text{max}}]$ denote the reward and cost functions; $\\epsilon$ defines the\nthreshold (budget) of the constraint; $\\mu_0 \\in \\Delta^\\mathcal{S}$ denotes the initial state distribution; and $\\gamma \\in [0, 1)$ is\nthe discount factor. In this paper, we are interested in the stationary MDP where the planning horizon\n$H$ goes to infinity. Without loss of generality, we denote the CMDP with known cost as $\\mathcal{M}^\\mathcal{U}_c$, and\nthe CMDP without cost (i.e., CMDP$\\backslash c$) as $\\mathcal{M}$. The agent's behavior can be modeled by a policy\n$\\pi \\in \\Delta^\\mathcal{A}$. In this work, our theoretical results are mainly based on a discrete finite state-action space.\nGiven the CMDP, we define the discounted normalized occupancy measure [23] as $\\rho^{\\pi}(s,a) =$\n$(1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t P(S_t = s, A_t = a)$ so that $(1 - \\gamma) V^{\\pi} (r, \\mu_0) = (\\rho^\\pi, r)$ and $(1 - \\gamma) V^{\\pi} (c, \\mu_0) =$\n$(\\rho^\\pi, c)$, where $(1 - \\gamma)$ is the normalizer for $\\rho^\\pi$ to be a probability measure and $V^{\\pi}$ is a reward or\ncost state-value function under the policy $\\pi$ and the initial distribution $\\mu_0$.\nConstrained Reinforcement Learning (CRL). Within a CMDP environment, CRL learns a policy\n$\\pi$ that maximizes the cumulative rewards subject to a known constraint:\n$$\n\\mathop{\\text{arg max}} \\limits_{\\pi} \\mathbb{E}_{\\mu_0,\\pi,P_T} [\\sum_{t=0}^{H} r(S_t, a_t)] \\text{ s.t. } \\mathbb{E}_{\\mu_0,\\pi,P_T} [\\sum_{t=0}^{H} c(S_t, a_t)] \\le \\epsilon.\n$$\nIn this paper, we primarily focus on the cumulative constraint as in (1) instead of instantaneous\nconstraints due to its broader applications [24]. In particular, since $c > 0$, by setting $\\epsilon > 0$, the\nconstraint in (1) denotes a soft constraint, enabling its application to the environment with stochastic\ndynamics. On the other hand, we convert this constraint into a hard one when setting $\\epsilon = 0$, which\nfacilitates the enforcement of absolute constraints at each decision step.\nInverse Constrained Reinforcement Learning (ICRL). In many realistic applications, the constraint\nis not readily available, and we must infer the constraints respected by the expert agents from their\nbehaviors. This problem is known as the ICRL, which can be formally defined as follows:\nDefinition 3.1. (ICRL problem [5]). An ICRL problem is a pair $P = (\\mathcal{M},\\pi^E,r)$, where $\\mathcal{M}$ is a\nCMDP$\\backslash c$ (CMDP without knowing the cost) and $\\pi^E \\in \\Delta^{\\mathcal{A}}$ is an expert's policy. A cost representation\n$c \\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ is feasible for $P$ if $\\pi^E$ is an optimal policy for the CMDP $\\mathcal{M}^\\mathcal{U}_c$, i.e., $\\pi^E \\in \\Pi_{\\mathcal{M}^\\mathcal{U}_c}$. We\ndenote by $\\mathcal{C}_P$ the minimal set of feasible cost functions for $P$, named feasible cost set. In this sense,\n$\\mathcal{C}_P$ includes only the necessary cost functions to explain expert policy.\nTo better formulate the feasible cost set $\\mathcal{C}_P$, we define the value functions for the costs and rewards,\ndenoted by c and r in the superscript, respectively. Specifically, the action-value Q-function for costs\nis defined as: $\\mathcal{Q}^\\mathcal{M}^\\mathcal{U}_c(s, a) = \\mathbb{E}_{\\pi,P_T} [\\sum_{t=0}^{\\infty} \\gamma^t c(s_t, a_t) | s_0 = s, a_0 = a]$, and the state-value function\nis $\\mathcal{V}^\\mathcal{M}^\\mathcal{U}_c(s) = \\mathbb{E}_{\\pi}[\\mathcal{Q}^\\mathcal{M}^\\mathcal{U}_c(s, a)]$. Similarly, we define the action-value Q-function and advantage\nfunction for rewards under the $\\mathcal{M}$=CMDP$\\backslash c$ (without acknowledging the constraint) as:\n$$\n\\mathcal{Q}^\\mathcal{M}(s,a) = \\mathbb{E}_{\\pi,P_T} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) | s_0 = s, a_0 = a],\n$$\nand the reward advantage function as $\\mathcal{A}^{\\pi}(s,a) = \\mathcal{Q}^\\mathcal{M}(s,a) - \\mathcal{V}^\\mathcal{M}(s)$, where the state-value\nfunction is $\\mathcal{V}^{\\pi} (s) = \\mathbb{E}_{\\pi}[\\mathcal{Q}^{\\pi} (s, a)]$"}, {"title": "4 Learning Feasible Constraints", "content": "In this section, we introduce the feasible cost set which represents the physical constraint set. To\nquantify the accuracy of an estimated cost set, we introduce how its estimation error can be bounded\nby the estimation error of the environmental dynamics and the expert policy."}, {"title": "4.1 Feasible Costs in CMDP", "content": "Considering the expert policy is the\nconstraint-satisfying policy with the\nlargest cumulative rewards (see the\nCRL objective in (1)), we define the\nfeasible cost functions based on these\nintuitions: 1) if a policy achieves\nlarger rewards than the expert policy,\nas illustrated in Figure 1 (left), the\nunderlying constraints in the environ-\nment must be violated, and we can\ndetect unsafe state-action pairs by ex-\namining the trajectories; 2) if a pol-\nicy achieves the same rewards as the\nexpert policy, its movements must vi-\nolate the underlying constraints if its\ntrajectory does not overlap with the\nexpert trajectory, as illustrated in Figure 1(middle); and 3) if a policy achieves smaller rewards than\nthe expert policy as illustrated in Figure 1 (right), this suggests an absence of notable constraint-\nviolating actions. Since ICRL focuses on identifying the minimal set of constraints sufficient to\nexplain expert behaviors [7], such a policy is not utilized for expanding the cost set.\nBefore formulating the cost function, we introduce the necessary assumptions for different constraints.\nAssumption 4.1. Either of the following two statements holds:\n(i) The constraint in (1) is a hard constraint such that $\\epsilon = 0$;\n(ii) The constraint in (1) is a soft constraint such that $\\epsilon > 0$, and the expert policy is deterministic.\nThis assumption requires either a hard constraint or a soft constraint accompanied by a deterministic\nexpert policy. Furthermore, note that in some states, expert policy is not defined if all actions lead to\nconstraint violation. Since feasible cost functions are defined to explain expert behaviors, we do not\nutilize them to explain the non-existing expert policy in such states. In this work, $\\mathcal{S}$ denotes all the\nstates where the expert policy is available. To better accommodate the soft constraint, we establish\nthe conditions under which any state-action pair violates the constraint.\nLemma 4.2. Suppose the expert policy $\\pi^E$ of a CMDP $\\mathcal{M}^\\mathcal{U}_c$ is known and the current state-action\npair is $(s', a')$. Let $A^E(s')$ denote the set containing all expert actions at state $s'$, i.e., $A^E(s')={a\\in\n\\mathcal{A} | \\pi^E(a|s') >0}$. Then, at least one of the following two conditions must be satisfied: 1) The cost\nfunction ensures $\\mathbb{E}_{\\mu_0,\\pi^E,P_T} [\\sum_{t=0}^{H} c(s_t, a_t)] = \\epsilon$; 2) $\\forall a'' \\notin A^E(s'), \\mathcal{A}^{\\pi}(s', a'') \\le 0$.\nThis lemma shows that if there exists an action yielding larger rewards than the expert action, the\ncumulative expert costs must reach the threshold. Hence, enforcing the constraint-violating action\nwith greater costs than the expert action suffices to establish the constraint violating condition. Based\non these findings, we are ready to establish the implicit formulation of the feasible cost set.\nLemma 4.3. (Feasible Cost Set Implicit). Under Assumption 4.1, let $P = (\\mathcal{M},\\pi^E, r)$ be an ICRL\nproblem. $c \\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ is a feasible cost, i.e., $c \\in \\mathcal{C}_P$ if and only if $\\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A}$:\n1) (Expert Consistent $(s, a)$): If $\\pi^E (a|s) > 0, \\mathcal{Q}_c^{\\mathcal{M}^\\mathcal{U}_c}(s, a) - \\mathcal{V}_c^{\\mathcal{M}^\\mathcal{U}_c}(s) = 0$;\n2) (Constraint Violating $(s, a)$): If $\\pi^E (a|s) = 0$ and $\\mathcal{A}^\\mathcal{M} (s, a) > 0, \\mathcal{Q}_c^{\\mathcal{M}^\\mathcal{U}_c}(s, a) - \\mathcal{V}_c^{\\mathcal{M}^\\mathcal{U}_c}(s) > 0$;\n3) (Non-Critical $(s, a)$): If $\\pi^E (a|s) = 0$ and $\\mathcal{A}^\\mathcal{M} (s, a) \\le 0, \\mathcal{Q}_c^{\\mathcal{M}^\\mathcal{U}_c}(s, a) - \\mathcal{V}_c^{\\mathcal{M}^\\mathcal{U}_c}(s) \\le 0$.\nAppendix B provides proofs of Lemma 4.3 and subsequent theoretical results. Based on Lemma 4.3,\nthe corresponding cost Q-function can be defined as follows:\nLemma 4.4. Let $P = (\\mathcal{M},\\pi^E, r)$ be an ICRL problem. A Q-function satisfies the condition of\nLemma 4.3 if and only if there exist $\\zeta \\in \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}$ and $\\mathcal{V}^c \\in \\mathbb{R}^{\\mathcal{S}}$ such that:\n$$\n\\mathcal{Q}^\\mathcal{M}^\\mathcal{U}_c = \\zeta + E \\mathcal{V}^c,\n$$\nwhere the expansion operator E satisfies $(Ef)(s, a) = f(s)$."}, {"title": "", "content": "Here, the term $\\zeta$ ensures 1) (when $\\mathcal{A}^{\\mathcal{M}}_{T^{\\mathcal{A}}} \\mathcal{A}^\\mathcal{M}_{M^{\\mathcal{A}}} > 0$) the constraint condition in (1) is violated at $(s, a)$\npairs that achieve larger rewards than the expert policy, and 2) (when $\\mathcal{A}^{\\mathcal{M}}_{T^{\\mathcal{A}}} < 0$) only necessary cost\nfunctions are captured by feasible cost set $\\mathcal{C}_P$. Once the cost Q-function is defined, we are ready to\ndemonstrate the explicit feasible cost set as follows:\nLemma 4.5. (Feasible Cost Set Explicit). Let $P = (\\mathcal{M}, \\pi^E, r)$ be an ICRL problem. Let $c \\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$,\nthen c is a feasible cost, i.e., $c \\in \\mathcal{C}_P$ if and only if $\\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A}$:\n$$\nc = \\mathcal{A}^{\\mathcal{M}} + (E - \\gamma P_T) \\mathcal{V}^c.\n$$\nIntuitively, the first term in (3) penalizes constraint-violating movements which not only deviate\nfrom the expert's preference but also have larger rewards (i.e., $\\mathcal{A}^{\\mathcal{M}}_{T^{\\mathcal{A}}} > 0$). This penalty ensures the\nviolation of constraint condition in (1), thereby prohibiting any policies following these movements.\nThe second term $\\mathcal{V}^c \\in \\mathbb{R}^{\\mathcal{S}}$ can be interpreted as a cost-shaping operator that depends on the CMDP\nbut not on the expert policy. To represent hard constraints, $\\mathcal{V}^c$ is a zero matrix whose entries are all\nzeros, i.e., $\\mathcal{V}^c = 0$. However, if the target constraint is soft, we must ensure that $\\mathcal{V}^c(s) = \\mathcal{V}^\\mathcal{M}^\\mathcal{U}_c(s)."}, {"title": "4.2 Error Propagation", "content": "Our primary objective is to minimize the estimation error of constraints (i.e., the feasible cost sets $\\mathcal{C}_P$).\nTo define this error, based on Lemma 4.5, we first bound the estimation error of the cost functions\n(i.e., elements in the set) with some theoretically manageable terms in the following:\nLemma 4.6. (Error Propagation). Let $P = (\\mathcal{M},\\pi^E, r)$ and $\\hat{P} = (\\hat{\\mathcal{M}},\\hat{\\pi}^E, r)$ be two ICRL problems.\nFor any $c \\in \\mathcal{C}_P$ satisfying $c = \\mathcal{A}^{\\mathcal{M}} + (E - \\gamma P_T) \\mathcal{V}^c$ and $||c||_\\infty < C_{\\text{max}}$, there exists $\\hat{c} \\in \\hat{\\mathcal{C}}_P$ such\nthat element-wise it holds that:\n$$\n|c - \\hat{c}| \\le |(\\hat{P}_T - P_T) \\mathcal{V}^c| + |\\mathcal{A}^{\\hat{\\mathcal{M}}} - \\mathcal{A}^{\\mathcal{M}} |.\n$$\nFurthermore, $|\\mathcal{V}^c(s)||_\\infty \\le \\frac{C_{\\text{max}}}{(1 - \\gamma)}$ and $|\\zeta||_\\infty \\le \\frac{C_{\\text{max}}}{\\text{min}_{_{(s,a)}} \\mathcal{A}^{\\mathcal{M}} }$.\nThis lemma states the existence of a cost $\\hat{c}$ in the estimated feasible set $\\hat{\\mathcal{C}}_P$ fulfilling the bound\ncomposed by two terms. The first term concerns the estimation error of the transition model. The\nsecond term depends on both the expert policy approximation and the estimated MDP, which can be\nfurther decomposed as follows:\nLemma 4.7. For a given policy $\\pi$, let $\\mathcal{A}^{\\mathcal{M}}_{TT}$ denote the reward advantage function based on the\noriginal MDP $\\mathcal{M}$. For an estimated policy $\\hat{\\pi}$, let $\\mathcal{A}^{\\hat{\\mathcal{M}}}$ denote the reward advantage function based\non the estimated MDP $\\hat{\\mathcal{M}}$. Then, we have\n$$\n|\\mathcal{A}^{\\mathcal{M}} - \\mathcal{A}^{\\hat{\\mathcal{M}}} | \\le \\frac{2}{(1-\\gamma)} |(P_T - \\hat{P}_T) \\mathcal{V}^{\\hat{\\pi}} | + \\frac{\\gamma(1 + \\gamma)}{(1-\\gamma)} \\Vert (\\pi - \\hat{\\pi}) P_T \\mathcal{V}^{\\hat{\\pi}} \\Vert + \\gamma \\Vert (\\pi - \\hat{\\pi}) \\Vert_\\infty \\Vert P_T \\mathcal{V}^{\\hat{\\pi}} \\Vert.\n$$\nWith the estimation error of cost functions bounded as in Lemma 4.6, we next analyze the estimation\nerrors of optimal policies $\\pi^*$ between CMDP with true cost and estimated cost, i.e., $\\mathcal{M}^\\mathcal{U}_c$ and $\\mathcal{M}^\\mathcal{U}_{\\hat{c}}$.\nThis error quantifies the extent to which the estimated cost function captures expert behaviors.\nLemma 4.8. Let $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P_T, r, \\epsilon, \\mu_0, \\gamma)$ be a CMDP without the knowledge of the cost (i.e.,\nCMDP$\\backslash c$). For every given policy $\\pi$, the first inequality below holds element-wise. For every optimal\npolicies $\\pi^* \\in \\Pi_{\\mathcal{M}^\\mathcal{U}_c}$ and $\\hat{\\pi}^* \\in \\Pi_{\\hat{\\mathcal{M}}^\\mathcal{U}_{\\hat{c}}}$ of CMDPS $\\mathcal{M}^\\mathcal{U}_c$ and $\\hat{\\mathcal{M}}^\\mathcal{U}_{\\hat{c}}$ respectively, the second\ninequality below holds.\n$$\n|\\mathcal{Q}^{\\pi}_{\\mathcal{M}^\\mathcal{U}_c} - \\mathcal{Q}^{\\pi}_{\\hat{\\mathcal{M}}^\\mathcal{U}_{\\hat{c}}} | \\le || (I_{\\mathcal{S}\\times \\mathcal{A}} - \\gamma P_T \\pi)^{-1}|| \\cdot ||c -\\hat{c}||,\n$$\n$$\n\\mathop{\\text{max}} \\limits_{\\pi \\in {\\pi^*,\\hat{\\pi}^*}} || \\mathcal{Q}^{\\pi}_{\\mathcal{M}^\\mathcal{U}_c} - \\mathcal{Q}^{\\pi}_{\\hat{\\mathcal{M}}^\\mathcal{U}_{\\hat{c}}} ||_\\infty \\le \\frac{1}{1 - \\gamma} ||c-\\hat{c}||_\\infty.\n$$\nWith the above results, we can define the optimality of the estimated cost sets based on the Probably\nApproximately Correct (PAC) condition [25, 26]. The estimated feasible set $\\hat{\\mathcal{C}}_P$ is \u201cclose\u201d to the\nexact feasible set $\\mathcal{C}_P$, if for every cost $c \\in \\mathcal{C}_P$, there exists one estimated cost $\\hat{c} \\in \\hat{\\mathcal{C}}_P$ that is \u201cclose\u201d\nto c, and vice versa."}, {"title": "5 Efficient Exploration for ICRL", "content": "In this section, we introduce algorithms for efficient exploration by leveraging the aforementioned\ncost set and estimation error. Our objective is to collect high-quality samples from interactions with\nthe environment, thereby improving the accuracy of our cost set estimations. Unlike most existing\nICRL works [9, 27] that rely on a generative model for collecting samples, our exploration strategy\nmust determine which states require more frequent visits and how to traverse to them starting from\nthe initial state so. To achieve this goal, we first define the estimated transition model and the expert\npolicy (Section 5.1), based on which we develop a BEAR (Bounded Error Aggregate Reduction)\nstrategy algorithm (Section 5.2) and a PCSE (Policy-Constrained Strategic Exploration) algorithm\n(Section 5.3) for solving ICRL problems, respectively."}, {"title": "5.1 Estimating Transition Dynamics and Expert Model", "content": "We consider a model-based setting where the agent strategically explores the environment to learn\ntransition dynamics and expert policy. These components are vital for bounding the estimation error\nof the feasible cost set (Lemma 4.6). To achieve this, we record the returns of a state-action pair\n$(s, a)$ by observing a next state $s' \\sim P(\\cdot|s, a)$, and the preference of expert agents $a^E \\sim \\pi^E (.\\vert s)$\nin each visited state. For iteration $\\forall k$, we denote by $n_k(s, a, s')$ the number of times we observe\nthe transition $(s, a, s')$. Denote $n_k(s, a) = \\sum_{s'\\in \\mathcal{S}} n_k(s, a, s')$ and $n_k(s) = \\sum_{a\\in \\mathcal{A}} n_k(s, a)$. For the\nexpert policy and the transition model estimation, we define the cumulative counts $N_k(s, a, s') =$\n$\\sum_{j=1}^{k} n_j(s, a, s'), N_k(s, a) = \\sum_{j=1}^{k} n_j(s, a)$ and $N_k(s) = \\sum_{j=1}^{k} n_j(s)$. Accordingly, we can\nrepresent the estimated transition model and expert policy as follows:\n$$\n\\hat{P}_k(s'\\vert s, a) = \\frac{N_k(s, a, s')^+}{N_k(s,a)},\\quad \\hat{\\pi}^E(a\\vert s) = \\frac{N_k(s,a)}{N_k(s)},\n$$\nwhere $x^+ = \\text{max}{1, x}$. With these estimations, we derive the confidence intervals for the transition\nmodel and expert policy using the Hoeffding inequality (see Lemma B.4). We guarantee that the true\ntransition model and the expert policy fall into these intervals with high probability. Based on these\nresults, we derive an upper bound on the estimation error of feasible cost sets and prove that this\nupper bound can be guaranteed with high probability as follows:\nLemma 5.1. Let $\\delta \\in (0, 1)$, with probability at least $1 - \\delta$, for any pair of cost functions $c \\in \\mathcal{C}_P$ and\n$\\hat{c}_k \\in \\hat{\\mathcal{C}}_P$ at iteration k, we have\n$$\n|c(s, a) - \\hat{c}_k(s, a)| \\le C_k(s,a), \\text{ C}_k(s,a) = \\text{min} {\\sigma \\sqrt{\\frac{l_k(s,a)}{2 N_k(s,a)}}, C_{\\text{max}}},\n$$\nwhere $\\sigma = \\frac{\\gamma(R_{\\text{max}}(3 + \\gamma) ||\\zeta||_\\infty + C_{\\text{max}}(1 - \\gamma))}{(1 - \\gamma)^2}$ and $l_k (s, a) = \\text{log}(\\frac{36SA(N_k(s,a))^2}{\\delta})$.\nIt is worth noting that $C_k (s, a)$ typically decreases after the number of samples collected for a specific\n$(s, a)$ pair reaches a peak. To efficiently allocate a fixed number of samples to meet the demand of\nDefinition 4.9, we introduce the exploration strategy next."}, {"title": "5.2 Exploration via Reducing Bounded Errors", "content": "Based on the above upper bound", "follows": "nLemma 5.2. At iteration k"}, {"follows": "n$$\n||e_k(s", "satisfied": "n(i) $\\frac{1}{1 - \\gamma} \\mathop{\\text{max}} \\limits_{(s,a)\\in \\mathcal{S}\\times \\mathcal{A}} C_k(s, a) \\le \\epsilon$, (ii) $||\\mu_0 (I_{\\mathcal{S}\\times \\mathcal{A}} - \\gamma P_T \\pi^*)^{-1} C_k||_\\infty \\le \\epsilon$. (8)\nSample Complexity. Next, we analyze the sample complexity of Algorithm BEAR. The updated\naccuracy $\\epsilon_k$ in Algorithm 1 equals to (i) of ("}]}