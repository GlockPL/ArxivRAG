{"title": "Provably Efficient Exploration in Inverse Constrained\nReinforcement Learning", "authors": ["Bo Yue", "Jian Li", "Guiliang Liu"], "abstract": "To obtain the optimal constraints in complex environments, Inverse Constrained\nReinforcement Learning (ICRL) seeks to recover these constraints from expert\ndemonstrations in a data-driven manner. Existing ICRL algorithms collect training\nsamples from an interactive environment. However, the efficacy and efficiency\nof these sampling strategies remain unknown. To bridge this gap, we introduce a\nstrategic exploration framework with provable efficiency. Specifically, we define a\nfeasible constraint set for ICRL problems and investigate how expert policy and\nenvironmental dynamics influence the optimality of constraints. Motivated by our\nfindings, we propose two exploratory algorithms to achieve efficient constraint in-\nference via 1) dynamically reducing the bounded aggregate error of cost estimation\nand 2) strategically constraining the exploration policy. Both algorithms are theo-\nretically grounded with tractable sample complexity. We empirically demonstrate\nthe performance of our algorithms under various environments.", "sections": [{"title": "1 Introduction", "content": "Constrained Reinforcement Learning (CRL) addresses sequential decision-making problems within\nsafety constraints and achieves considerable success in various safety-critical applications [1]. How-\never, in many real-world environments, such as robot control [2, 3] and autonomous driving [4]\nspecifying the exact constraint that can consistently guarantee the safe control is challenging, which\nis further exacerbated when the ground-truth constraint is time-varying and context-dependent.\nInstead of utilizing a pre-defined constraint, an alternative approach, Inverse Constrained Reinforce-\nment Learning (ICRL) [5, 6], seeks to learn the constraint signals from the demonstrations of expert\nagents and imitate their behaviors by adopting the inferred constraint. ICRL effectively incorporates\nexpert experience into the online CRL paradigm and thus better explains how expert agents optimize\ncumulative rewards under their empirical constraints. Under this framework, existing ICRL algo-\nrithms often assume the presence of a known dynamics model [7, 8], or a generative transition model\nthat responds to queries for any state-action pair [9, 10]. However, this setting has a considerable gap\nwith scenarios in practice where the transition models are often not available, or even time-varying,\nnecessitating agents to physically navigate to new states to learn about them through exploration.\nTo mitigate the gap, some recent studies [5, 11, 12] explicitly maximized the policy entropy throughout\nthe learning process, yielding soft-optimal policy representations that favor less-selected actions.\nUnfortunately, such an uncertainty-driven exploration largely ignores the potential estimation errors\nin dynamic models or policies. To date, it still lacks a theoretical framework to demonstrate how well\nthe maximum entropy approaches facilitate the accurate estimation of constraints."}, {"title": "2 Related Work", "content": "In this section, we introduce previous works that are most related to our algorithms.\nExploration in Inverse Reinforcement Learning (IRL). Compared to the exploration strategies\nin RL for forward control [13, 14], the exploration algorithms in IRL has relatively limited studies.\n[15] utilized Bayesian optimization to identify multiple IRL solutions by efficiently exploring the\nreward function space. To learn a transferable reward function, [16] introduced an active sampling\nmethodology that is designed to target the most informative regions with a generative model to\nfacilitate effective approximations of the transition model and the expert policy. A subsequent\nresearch [17] expanded this concept to finite-horizon MDPs with non-stationary policies, crafting\ninnovative strategies to accelerate the exploration process. To better quantify the precision of\nrecovered feasible rewards, [18] recently devised a framework that employs the Hausdorff metric\nand provides more rigorous theoretical bounds. These methods study only reward function under a\nregular MDP without considering the safety of control or the constraints in the environments.\nInverse Constrained Reinforcement Learning (ICRL). Unlike IRL that solely focuses on the\nrecovery of reward functions, ICRL seeks to elucidate the preference of expert agents by inferring\nwhich constraints they follow. The majority of ICRL algorithms update the cost functions by\nmaximizing the likelihood of generating the expert dataset under the maximum (causal) entropy\nframework [7]. This method can be efficiently scaled to both discrete [8] and continuous state-action\nspace [5, 10, 11, 12, 19, 20]. [9] recently proposed a Bayesian approach for learning a posterior\ndistribution of constraints. To improve training efficiency, recent studies combined ICRL with bi-level\noptimization techniques [21, 22]. However, current ICRL methods have not explored exploration\nstrategies or conducted theoretical studies about the sample complexity of their algorithms."}, {"title": "3 Preliminaries and Problem Formulation", "content": "Notation. Let X be a finite set and Y be a space. The notation $Y^x$ represents the set of functions\nf : X \u2192 Y. The simplex over X is denoted as $\u2206 = {\u03c6 \u2208 [0,1]^X: \u03a3_{x\u2208X} \u03c6(x) = 1}$ and we\ndenote $A$ as the set of functions X \u2192 \u0394^A. Let $\u03c8 \u2208 \u2206^X$ and f \u2208 RX. We use the shorthand\nnotation $\u03c8^T f = \u03a3_{x\u2208X}\u03c8(x)f(x)$, where $T$ serves as an operator. We define the L\u221e-norm of\nf as $|| f ||_\u221e = max_{x\u2208X} |f(x)|$. Concretely, for a vector a, we define the vector infinity norm as\n$||a||_\u221e = max_i|a_i|$. For a matrix A, we define the matrix infinity norm as $||A||_\u221e = max_i \u2211_j |A_{ij}|$.\nConstrained Markov Decision Process (CMDP). We model the environment as a stationary CMDP\n$\\\\mathcal{M}_{U^c} := (S, A, P_T, r, c, \\\\epsilon, \\\\mu_0, \\\\gamma)$, where S and A are the finite state and action spaces, with the\ncardinality denoted as S = |S| and A = |A|; $P_T(s'|s, a) \u2208 \u0394^{SxA}$ defines the transition distribution;\nr(s, a) \u2208 [0, Rmax] and c(s, a) \u2208 [0, Cmax] denote the reward and cost functions; \\\\epsilon defines the\nthreshold (budget) of the constraint; $\u00b5_0 \u2208 \u0394^S$ denotes the initial state distribution; and $\u03b3 \u2208 [0, 1)$ is\nthe discount factor. In this paper, we are interested in the stationary MDP where the planning horizon\nH goes to infinity. Without loss of generality, we denote the CMDP with known cost as $M_{U^c}$, and\nthe CMDP without cost (i.e., CMDP\\c) as M. The agent's behavior can be modeled by a policy\n$\u03c0\u2208 \u0394^A$. In this work, our theoretical results are mainly based on a discrete finite state-action space.\nGiven the CMDP, we define the discounted normalized occupancy measure [23] as $\u03c1^\u03c0(s, a) =$\n$(1 \u2013 \u03b3) \u03a3_{t=0}^{\u221e} P(S_t = s, A_t = a)$ so that $(1 \u2212 \u03b3)V^\u03c0(r, \u00b5_0) = (\u03c1^\u03c0, r)$ and $(1 \u2013 \u03b3)V^\u03c0(c, \u00b5_0) =$\n$(\u03c1^\u03c0, c)$, where (1 \u2013 \u03b3) is the normalizer for $\u03c1^\u03c0$ to be a probability measure and V\u03c0 is a reward or\ncost state-value function under the policy \u03c0 and the initial distribution $\u00b5_0$.\nConstrained Reinforcement Learning (CRL). Within a CMDP environment, CRL learns a policy\n\u03c0 that maximizes the cumulative rewards subject to a known constraint:\n$\\underset{\\pi}{arg \\\\max}~\\\\mathbb{E}_{\\\\mu_0, \\\\pi, P_T} \\left[ \\\\sum_{t=0}^H r(S_t, a_t) \\\\right]~s.t.~\\\\mathbb{E}_{\\\\mu_0, \\\\pi, P_T} \\left[ \\\\sum_{t=0}^H c(S_t, a_t) \\\\right] \\\\leq \\\\epsilon.$\nIn this paper, we primarily focus on the cumulative constraint as in (1) instead of instantaneous\nconstraints due to its broader applications [24]. In particular, since c > 0, by setting \\\\epsilon > 0, the\nconstraint in (1) denotes a soft constraint, enabling its application to the environment with stochastic\ndynamics. On the other hand, we convert this constraint into a hard one when setting \\\\epsilon = 0, which\nfacilitates the enforcement of absolute constraints at each decision step.\nInverse Constrained Reinforcement Learning (ICRL). In many realistic applications, the constraint\nis not readily available, and we must infer the constraints respected by the expert agents from their\nbehaviors. This problem is known as the ICRL, which can be formally defined as follows:\nDefinition 3.1. (ICRL problem [5]). An ICRL problem is a pair $P = (M,\u03c0^E,r)$, where M is a\nCMDP\\c (CMDP without knowing the cost) and $\u03c0^E \u2208 \u0394^{SXA}$ is an expert's policy. A cost representation\nc\u2208 RS\u00d7A is feasible for P if $\u03c0^E$ is an optimal policy for the CMDP $M_{U^c}$, i.e., $\u03c0^E \u2208 \u03a0_{M_{U^c}}$. We\ndenote by $C_P$ the minimal set of feasible cost functions for P, named feasible cost set. In this sense,\n$C_P$ includes only the necessary cost functions to explain expert policy.\nTo better formulate the feasible cost set $C_P$, we define the value functions for the costs and rewards,\ndenoted by c and r in the superscript, respectively. Specifically, the action-value Q-function for costs\nis defined as: $Q_{\\mathcal{M}_{U^c}}^\u03c0(s, a) = \\\\mathbb{E}_{\u03c0, P_T} [\\\\sum_{t=0}^{\u221e} \\\\gamma^t c(S_t, a_t) | S_0 = s, A_0 = a]$, and the state-value function\nis $V_{\\mathcal{M}_{U^c}}^\u03c0(s) = \\\\mathbb{E}_{\u03c0}[Q_{\\mathcal{M}_{U^c}}^\u03c0(s, a)]$. Similarly, we define the action-value Q-function and advantage\nfunction for rewards under the M=CMDP\\c (without acknowledging the constraint) as:\n$Q_{\\\\mathcal{M}}^\u03c0(s, a) = \\\\mathbb{E}_{\u03c0, P_T} [\\\\sum_{t=0}^{\u221e} \\\\gamma^t r(s_t, a_t) | s_0 = s, a_0 = a]$,\nand the reward advantage function as $A_{\\\\mathcal{M}}^\u03c0(s, a) = Q_{\\\\mathcal{M}}^\u03c0(s, a) - V_{\\\\mathcal{M}}^\u03c0(s)$, where the state-value\nfunction is $V_{\\\\mathcal{M}}^\u03c0(s) = \\\\mathbb{E}_{\u03c0}[Q_{\\\\mathcal{M}}^\u03c0(s, a)]$.\n4 Learning Feasible Constraints"}, {"title": "4 Learning Feasible Constraints", "content": "In this section, we introduce the feasible cost set which represents the physical constraint set. To\nquantify the accuracy of an estimated cost set, we introduce how its estimation error can be bounded\nby the estimation error of the environmental dynamics and the expert policy."}, {"title": "4.1 Feasible Costs in CMDP", "content": "Considering the expert policy is the\nconstraint-satisfying policy with the\nlargest cumulative rewards (see the\nCRL objective in (1)), we define the\nfeasible cost functions based on these\nintuitions: 1) if a policy achieves\nlarger rewards than the expert policy,\nas illustrated in Figure 1 (left), the\nunderlying constraints in the environ-\nment must be violated, and we can\ndetect unsafe state-action pairs by ex-\namining the trajectories; 2) if a pol-\nicy achieves the same rewards as the\nexpert policy, its movements must vi-\nolate the underlying constraints if its\ntrajectory does not overlap with the\nexpert trajectory, as illustrated in Figure 1(middle); and 3) if a policy achieves smaller rewards than\nthe expert policy as illustrated in Figure 1 (right), this suggests an absence of notable constraint-\nviolating actions. Since ICRL focuses on identifying the minimal set of constraints sufficient to\nexplain expert behaviors [7], such a policy is not utilized for expanding the cost set.\nBefore formulating the cost function, we introduce the necessary assumptions for different constraints.\nAssumption 4.1. Either of the following two statements holds:\n(i) The constraint in (1) is a hard constraint such that \\\\epsilon = 0;\n(ii) The constraint in (1) is a soft constraint such that \\\\epsilon > 0, and the expert policy is deterministic.\nThis assumption requires either a hard constraint or a soft constraint accompanied by a deterministic\nexpert policy. Furthermore, note that in some states, expert policy is not defined if all actions lead to\nconstraint violation. Since feasible cost functions are defined to explain expert behaviors, we do not\nutilize them to explain the non-existing expert policy in such states. In this work, S denotes all the\nstates where the expert policy is available. To better accommodate the soft constraint, we establish\nthe conditions under which any state-action pair violates the constraint.\nLemma 4.2. Suppose the expert policy $\u03c0^E$ of a CMDP $M_{U^c}$ is known and the current state-action\npair is (s', a'). Let AE (s') denote the set containing all expert actions at state s', i.e., $A^E (s')={a\u2208\n\u0391|\u03c0^E(a|s') >0}$. Then, at least one of the following two conditions must be satisfied: 1) The cost\nfunction ensures $\\\\mathbb{E}_{\u00b5_0, \u03c0^E, P_T} [\\\\sum_{t=0}^{H}c(s_t, a_t)] = \\\\epsilon$; 2) $\u2200a'' \\\\notin A^E (s'), A^M (s', a'') \u2264 0$.\nThis lemma shows that if there exists an action yielding larger rewards than the expert action, the\ncumulative expert costs must reach the threshold. Hence, enforcing the constraint-violating action\nwith greater costs than the expert action suffices to establish the constraint violating condition. Based\non these findings, we are ready to establish the implicit formulation of the feasible cost set.\nLemma 4.3. (Feasible Cost Set Implicit). Under Assumption 4.1, let $P = (M,\u03c0^E, r)$ be an ICRL\nproblem. c \u2208 RS\u00d7A is a feasible cost, i.e., \u0441 \u2208 $C_P$ if and only if $\u2200(s, a) \u2208 S \u00d7 A:\n1) (Expert Consistent (s, a)): If $\u03c0^E (a|s) > 0, Q_{\\mathcal{M}_{U^c}}^\u03c0(s, a) - V_{\\mathcal{M}_{U^c}}^\u03c0(s) = 0$;\n2) (Constraint Violating (s, a)): If $\u03c0^E (a|s) = 0$ and $A_{\\\\mathcal{M}}^\u03c0 (s, a) > 0, Q_{\\mathcal{M}_{U^c}}^\u03c0(s, a) \u2013 V_{\\mathcal{M}_{U^c}}^\u03c0(s) > 0$;\n3) (Non-Critical (s, a)): If $\u03c0^E (a|s) = 0$ and $A_{\\\\mathcal{M}}^\u03c0 (s, a) \u2264 0, Q_{\\mathcal{M}_{U^c}}^\u03c0(s, a) \u2264 V_{\\mathcal{M}_{U^c}}^\u03c0(s)$.\nAppendix B provides proofs of Lemma 4.3 and subsequent theoretical results. Based on Lemma 4.3,\nthe corresponding cost Q-function can be defined as follows:\nLemma 4.4. Let $P = (M,\u03c0^E,r)$ be an ICRL problem. A Q-function satisfies the condition of\nLemma 4.3 if and only if there exist $\u03b6 \u2208 R^{SxA}$ and $V^c \u2208 R^S$ such that:\n$Q_{\\mathcal{M}_{U^c}} = A_{\\\\mathcal{M}}^\u03c0 + EV^c,$\nwhere the expansion operator E satisfies (Ef)(s, a) = f(s)."}, {"title": "4.2 Error Propagation", "content": "Our primary objective is to minimize the estimation error of constraints (i.e., the feasible cost sets $C_P$).\nTo define this error, based on Lemma 4.5, we first bound the estimation error of the cost functions\n(i.e., elements in the set) with some theoretically manageable terms in the following.\nLemma 4.6. (Error Propagation). Let $P = (M,\u03c0^E, r)$ and $\\hat{P} = (\\\\hat{M},\u03c0^E, r)$ be two ICRL problems.\nFor any $c \u2208 C_P$ satisfying $c = A_{\\\\mathcal{M}}^\u03c0 + (E \u2013 \u03b3P_T)V^c$ and $||c||_\u221e < C_{max}$, there exists $\\\\hat{c} \u2208 C_P$ such\nthat element-wise it holds that:\n$||c-\\hat{c}||_\u221e\u2264\\\\frac{2}{(1-\\gamma)}||(P_T-\\hat{P}_T)V^c||_\u221e+||A_{\\\\mathcal{M}}^{\\pi}-\\\\hat{A}_{\\\\mathcal{M}}^{\\pi}||_\u221e$.\nFurthermore, $||V^c(s)||_\u221e \u2264 \\\\frac{C_{max}}{(1 \u2013 \u03b3)}$ and $||\u03b6||_\u221e \u2264 \\\\frac{C_{max}}{min_{(s,a)}|A_{\\\\mathcal{M}}^{\\pi}|}$.\nThis lemma states the existence of a cost $\\\\hat{c}$in the estimated feasible set $\\\\hat{C}_P$ fulfilling the bound\ncomposed by two terms. The first term concerns the estimation error of the transition model. The\nsecond term depends on both the expert policy approximation and the estimated MDP, which can be\nfurther decomposed as follows:\nLemma 4.7. For a given policy \u03c0, let $A_{\\\\mathcal{M}}^{\\pi}$ denote the reward advantage function based on the\noriginal MDP M. For an estimated policy $\\hat{\u03c0}$, let $\\hat{A}_{\\\\mathcal{M}}^{\\hat{\\pi}}$ denote the reward advantage function based\non the estimated MDP $\\hat{M}$. Then, we have\n$||A_{\\\\mathcal{M}}^{\\pi}-\\\\hat{A}_{\\\\mathcal{M}}^{\\hat{\\pi}}||_\u221e\u2264\\\\frac{2 \\\\gamma}{(1-\\gamma)}||(P_T-\\hat{P}_T)V^r||_\u221e+ \\\\frac{\\\\gamma(1 + \\\\gamma)}{(1-\\gamma)^2} ||(\u03c0-\\hat{\u03c0})P_T V^r ||_\u221e+ ||\\\\gamma P_T \\\\hat{V}^r ||_\u221e V.$\nWith the estimation error of cost functions bounded as in Lemma 4.6, we next analyze the estimation\nerrors of optimal policies $\u03c0^*$ between CMDP with true cost and estimated cost, i.e., $M_{U^c}$and $\\\\hat{M}_{U^c}$.\nThis error quantifies the extent to which the estimated cost function captures expert behaviors.\nLemma 4.8. Let M = (S, A, PT, r, \\\\epsilon, \u00b50, \u03b3) be a CMDP without the knowledge of the cost (i.e.,\nCMDP\\c). For every given policy \u03c0, the first inequality below holds element-wise. For every optimal\npolicies $\u03c0^* \u2208 \u03a0_{M_{U^c}}$ and $\\hat{\u03c0}^* \u2208 \u03a0 _{\\\\hat{M}_{U^c}}$ of CMDPS $M_{U^c}$ and $\\\\hat{M}_{U^c}$ respectively, the second\ninequality below holds.\n$||Q_{\\mathcal{M}_{U^c}}^{\\pi} - Q_{\\\\hat{M}_{U^c}}^{\\pi}||_\u221e\u2264 |(I_{SxA} \u2013 \u03b3P_T\u03c0)^{-1}|||c -\\\\hat{c}||,$\n$\\underset{\\pi\u2208{\u03c0*,\u03c0\u02c6*}}{max}|Q_{\\mathcal{M}_{U^c}}^{\\pi} - Q_{\\\\hat{M}_{U^c}}^{\\pi}||_\u221e\u2264\\\\frac{1}{(1-\\gamma)}||Q_{\\mathcal{M}_{U^c}} - Q_{\\\\hat{M}_{U^c}}||_\u221e.$\nWith the above results, we can define the optimality of the estimated cost sets based on the Probably\nApproximately Correct (PAC) condition [25, 26]. The estimated feasible set $\\\\hat{C}_P$ is \u201cclose\u201d to the\nexact feasible set $C_P$, if for every cost $c \u2208 C_P$, there exists one estimated cost $\\\\hat{c} \u2208 \\\\hat{C}_P$ that is \u201cclose\u201d\nto c, and vice versa."}, {"title": "5 Efficient Exploration for ICRL", "content": "In this section, we introduce algorithms for efficient exploration by leveraging the aforementioned\ncost set and estimation error. Our objective is to collect high-quality samples from interactions with\nthe environment, thereby improving the accuracy of our cost set estimations. Unlike most existing\nICRL works [9, 27] that rely on a generative model for collecting samples, our exploration strategy\nmust determine which states require more frequent visits and how to traverse to them starting from\nthe initial state $s_0$. To achieve this goal, we first define the estimated transition model and the expert\npolicy (Section 5.1), based on which we develop a BEAR (Bounded Error Aggregate Reduction)\nstrategy algorithm (Section 5.2) and a PCSE (Policy-Constrained Strategic Exploration) algorithm\n(Section 5.3) for solving ICRL problems, respectively."}, {"title": "5.1 Estimating Transition Dynamics and Expert Model", "content": "We consider a model-based setting where the agent strategically explores the environment to learn\ntransition dynamics and expert policy. These components are vital for bounding the estimation error\nof the feasible cost set (Lemma 4.6). To achieve this, we record the returns of a state-action pair\n(s, a) by observing a next state s' ~ P(\u00b7|s, a), and the preference of expert agents $\u03b1^E$ ~ $\u03c0^E (.|s)$\nin each visited state. For iteration $\u2200k$, we denote by $n_k(s, a, s')$ the number of times we observe\nthe transition (s, a, s'). Denote $n_k(s, a) = \u2211_{s'\u2208s} n_k(s, a, s')$ and $n_k(s) = \u2211_{a\u2208a}n_k(s, a)$. For the\nexpert policy and the transition model estimation, we define the cumulative counts $N_k (s, a, s') =$\n$\u2211_{j=1}^{k}n_j(s,a, s'), N_k(s,a) = \u2211_{j=1}^{k}n_j(s,a)$ and $N_k(s) = \u2211_{j=1}^{k}n_j(s)$. Accordingly, we can\nrepresent the estimated transition model and expert policy as follows:\n$\\hat{P}_{T_k}(s'|s, a) = \\\\frac{N_k(s, a, s')^+}{N_k(s,a)},$\n$\u03c0_k(a|s) = \\\\frac{N_k(s,a)}{N_k(s)},$\nwhere $x^+ = max{1, x}$. With these estimations, we derive the confidence intervals for the transition\nmodel and expert policy using the Hoeffding inequality (see Lemma B.4). We guarantee that the true\ntransition model and the expert policy fall into these intervals with high probability. Based on these\nresults, we derive an upper bound on the estimation error of feasible cost sets and prove that this\nupper bound can be guaranteed with high probability as follows:\nLemma 5.1. Let $\u03b4 \u2208 (0, 1)$, with probability at least 1 \u2013 \u03b4, for any pair of cost functions c \u2208 $C_P$ and\n$c_k \u2208 \\\\hat{C}_P$ at iteration k, we have\n$|c(s, a) - ck(s, a)| \u2264 C_k(s,a), C_k(s,a) = min \\\\left { \u03c3 \\\\sqrt{\\\\frac{l_k(s,a)}{2N_k^+(s, a)}}, C_{max}\\\\right },$\nwhere \u03c3 = $\\\\frac{(R_{max}(3+\\\\gamma)||||\u03b6|||\\\\infty +C_{max}(1-\\\\gamma))}{(1-\\\\gamma)^2}$ and $l_k (s, a) = log(\\frac{36SA(N_k(s,a))^2}{\\\\delta})$.\nIt is worth noting that $C_k (s, a)$ typically decreases after the number of samples collected for a specific\n(s, a) pair reaches a peak. To efficiently allocate a fixed number of samples to meet the demand of\nDefinition 4.9, we introduce the exploration strategy next."}, {"title": "5.2 Exploration via Reducing Bounded Errors", "content": "Based on the above upper bound, we are ready to design algorithms for efficiently solving the\nICRL problem. Since our primary goal is to fulfill the PAC-condition in Definition 4.9, we begin\nby establishing an upper bound on the estimation error, which pertains to the disparity for the\nperformance of optimal policy $\u03c0^*$ between CMDP with true cost and CMDP with estimated cost at\niteration k, i.e., $M_{U^c}$ and $\\\\hat{M}_{U^{ck}}$. Our key results are presented as follows:\nLemma 5.2. At iteration k, let $ek(s, a; \u03c0^*) = |Q_{\\mathcal{M}_{U^c}}^{\u03c0^*}(s, a) \u2013 Q_{\\\\hat{M}_{U^{ck}}}^{\\pi^*}(s, a) |$ defines the estimation\nerror of discounted cumulative costs within the true CMDP\\c M. For any policy $\u03c0^* \u2208 \u03a0_{M_{U^c}}$ we\nupper bound the above estimation error $e_k(\u00b7)$ as follows:\n$||ek(s, a; \u03c0^*)||_\u221e\u2264 ||\u00b50 (I_{sxa}-\u03b3PT\u03c0^*)^{-1}C_k ||_\u221e .$\nTo reduce this error bound, we introduce BEAR exploration strategy for ICRL in Algorithm 1\n(represented in teal color), which explores to reduce the bounded error. This is equivalent to solving\nthe RL problem defined by $\\hat{M}_{Ck} = (M\\\\r)UC_k$, where we replace the reward r in MDP M with $C_k$.\nWe can use any RL solver to find the exploration policy in practice. We show in Corollary B.5 that\nthe exploration algorithm converges (satisfies Definition 4.9) when either of the following statements\nis satisfied:\n(i) $\\\\frac{1}{1- \\\\gamma}$  max  $C_k(s, a) \u2264 \\\\epsilon$,\n(s,a)\u2208SXA\n(ii) $||\u00b50 (I_{sxa} - \u03b3P_T\u03c0)^{-1}C_k||_\u221e \u2264 \\\\epsilon$.\nSample Complexity. Next, we analyze the sample complexity of Algorithm BEAR. The updated\naccuracy $\\\\epsilon_k$ in Algorithm 1 equals to (i) of (8). Let $\u03b7_h^k(s, a|s_0)$, h \u2208 [nmax] be the probability of\nstate-action pair (s, a) reached in the h-th step following a policy $\u03c0_k \u2208 \u03a0_{\\\\hat{M}_{Ck}}$ starting in state s0.\nWe can compute it recursively:\n$\u03b7_0^k(s, a|s_0) := \u03c0_k(a|s)1{s=s0}, \u03b7_{h+1}^k(s, a|s_0) := \u2211_{a',s'}\u03c0_k(a|s) P_T(s'|s, a')\u03b7_h^k(s', a'|s_0),$\nwhere $\u03c0_k$ is the exploration policy in iteration k. We then define the pseudo-counts that are crucial to\ndeal with the uncertainty of the transition dynamics in our analysis.\nDefinition 5.3. (Pseudo-counts) We introduce the pseudo-counts of visiting a specific state-action\npair (s, a) in the h-th step within the first k iterations as:\n$\\tilde{N}_{h}^k(s, a) = \u00b5_0 \u2211_{h=1}^{N_{max}} \u2211_{i=1}^{k}\u03b7_h^i (s, a|s_0).$\nSimilar to (5), we define $\u00d1_k^+(s, a) = max{0, \u00d1_k(s,a)}$. The following lemma upper bounds the\nestimation error of feasible costs with the pseudo-counts under a certain confidence interval.\nLemma 5.4. With probability at least 1 \u2013 \u03b4/2, $\u2200s,a,h,k \u2208 S \u00d7 A \u00d7 [0, nmax] \u00d7 N^+$, we have:\nmin \\\\left { \u03c3 \\\\sqrt{\\\\frac{l_k(s, a)}{2\\tilde{N}^+_k(s,a)}}, C_{max}\\\\right }\u2264 \u03c3 \\\\sqrt{\\\\frac{2l_k(s, a)}{\\tilde{N}^+_k(s,a)}} ,$\nwhere $l_k(s, a) = log(36SA(\u00d1_k^+(s, a))^2/\u03b4)$ and $\\\\tilde{\u03c3} = max{0, \u221a2C_{max}}$.\nSubsequently, the sample complexity of Algorithm 1 is presented as follows:\nTheorem 5.5. (Sample Complexity of BEAR). If Algorithm BEAR terminates at iteration K with the\nupdated accuracy $\u03b5_K$, then with probability at least 1 \u2013 d, it fulfills Definition 4.9 with a number of\nsamples upper bounded by\nn\u2264\u00d5 \\\\frac{\u03c3^2SA}{(1 \u2013 \u03b3)^2\u03b5_K} .\nThe above theorem has taken into account the sample complexity of the RL phase. This result matches\nthe sample complexity of the uniform sampling strategy with a generative model (see Appendix B.8),\nwhich indicates that a generative model is not necessary to achieve favorable sample complexity\nin ICRL. This is also verified regarding IRL in [17]. In fact, further improvements can be made to\nenhance the algorithm's performance."}, {"title": "5.3 Exploration via Constraining Candidate Policies", "content": "The above exploration strategy has limitations", "as": "nsup $(I_{SxA"}, "\u03b3P_T\u03c0)C_{k+1},~s.t.~\u03c0\u2208 \u03a0_k,$\n$\\\\Pi_k = \\\\Pi \\\\cap \\\\Pi$,\n$\\\\Pi = \\\\left{\u03c0 \u2208 \u0394^{SxA}: sup_{\u00b5_0\u2208\u0394^{SXA}}\u00b5_0 \\\\left( V_{\\mathcal{M}_{U^{c}}}^{\u03c0} \u2013 V_{\\mathcal{M}_{U^{c}},*}^{\u03c0} \\\\right) \u2264 4\u03f5_k + 2\u03f5 \\\\right},$\n$\\\\Pi = \\\\left{\u03c0 \u2208 \u0394^{SxA}: inf_{\u00b5_0\u2208\u0394^{SXA}}\u00b5_0 \\\\left( V_{\\mathcal{M}}^{\u03c0} - V_{\\mathcal{M}}^{*\u03c0} \\\\right) \u2265 R_k \\\\right},$\nwhere $R_k = \\\\frac{2\u03b3R_{max}}{(1-\u03b3)^2}||P_T \u2013 \\\\hat{P}_T||_{\u221e}+ \\\\frac{\u03b3 R_{max}}{(1-\u03b3)^2}|| \\\\left( \u03c0^* \u2013 \\\\hat{\u03c0} \\\\right)||_{\u221e}$. The rationale in \u03a0 can be attributed\nto the intersection of two aspects: 1) If constrains exploration policies to visit states within an\nadditional budget, thereby ensuring resilience to estimation error when searching for optimal policies;\n2) \u03a0 states that exploration policies should focus on states with potentially higher cumulative\nrewards, where possible constraints lie. As the estimation error decreases, the gap (i.e., Rk) also\ndiminishes, eventually converging to zero, which ensures the optimality of constrained policies. We\nhave shown in Appendix B.13 that optimality policies can be captured by subsequent \u03a0.\nTo solve the optimization problem (10), we represent its Lagrangian objective as L(\u03c1\u039c, \u03bb) =\n$\\left( \u03c1_M, (I_{SxA} \u2013 \u03b3P_T\u03c0)C_{k+1} \\\\right)+\u03bb_1 \\\\left[-(1-\u03b3) sup_{\u00b5_0} (V_{\\mathcal{M}_{U^c}}^{\u03c0} \u2013 V_{\\mathcal{M}_{U^{c}},*}^{\u03c0}) + 4\u03f5_k + 2\u03f5 \\\\right"]}