{"title": "Environment Complexity and Nash Equilibria in a Sequential Social Dilemma", "authors": ["Mustafa Yasir", "Andrew Howes", "Vasilios Mavroudis", "Chris Hicks"], "abstract": "Multi-agent reinforcement learning (MARL) methods, while effective in zero-sum or positive-sum games, often yield suboptimal outcomes in general-sum games where cooperation is essential for achieving globally optimal outcomes. Matrix game social dilemmas, which abstract key aspects of general-sum interactions, such as cooperation, risk, and trust, fail to model the temporal and spatial dynamics characteristic of real-world scenarios. In response, our study extends matrix game social dilemmas into more complex, higher-dimensional MARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma to more closely match the decision-space of a one-shot matrix game while also introducing variable environment complexity. Our findings indicate that as complexity increases, MARL agents trained in these environments converge to suboptimal strategies, consistent with the risk-dominant Nash equilibria strategies found in matrix games. Our work highlights the impact of environment complexity on achieving optimal outcomes in higher-dimensional game-theoretic MARL environments.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL) is concerned with training agents to maximise individual or shared rewards in environments with multiple concurrent learners. A critical aspect of MARL is the agent motive, which can be categorised as competitive (zero-sum), purely cooperative (positive-sum), or mixed (general-sum). MARL methods have shown considerable success both in competitive environments [9, 32] and in purely cooperative environments [27, 4]. In these settings, agent goals are either directly opposed or aligned, and learning paradigms like self-play, which train agents to compute best responses to rational opponents, empirically converge to stable strategies, i.e., Nash equilibria, that correspond to optimal outcomes. However, Nash equilibria in general-sum games often coincide with suboptimal outcomes, rendering the direct application of such methods ineffective for finding desirable solutions [8, 6, 14]. Instead, in many general-sum games, optimal outcomes are often only achievable through cooperation-by explicitly favouring collective gains over stable individual rewards.\nMatrix game social dilemmas (MGSDs), a class of general-sum games, represent situations where individual rationality may lead to collective irrationality. These dilemmas are widely used to explore cooperation in various disciplines [23, 26], including MARL, where numerous methods have been proposed to induce cooperation among learning agents [7]. While MGSDs are valuable for abstracting key aspects of real-world multi-agent interactions\u2014such as cooperation, risk, and trust\u2014they lack the complexity of real-world temporal and spatial dynamics. Unlike real-world actions, matrix game"}, {"title": "2 Related Work", "content": "Addressing the viability of MGSDs in modelling key aspects of multi-agent interaction, numerous studies have proposed higher-dimensional game-theoretic MARL environments to explore coopera-tion under more realistic settings. The Coin Game, a widely used gridworld environment based on repeated MGSDs, has been used to examine cooperative MARL methods by Foerster et al. [8], and the Melting Pot suite combines various MGSDs in gridworld formats to benchmark MARL algorithms [13]. Recently, Khan et al. [11] highlight the challenges in adapting matrix games to gridworld environments by demonstrating the shortcomings of the Coin Game. Relevant to our focus on the Stag Hunt, is the Level-Based Foraging environment [5], a gridworld foraging game which targets multi-agent cooperation and coordination, and a study by Nica et al. [20], which successfully demon-strates enhanced learning in a Minecraft-based stag hunt through gridworld abstraction. Although existing works provide useful game-theoretic environments, primarily for proposing new MARL methods for repeated and partially observable settings, we simplify the learning problem to one-shot and fully observable games; to focus on modifying environment dynamics, whilst maintaining the relation between our environment and its matrix game formulation. To this end, we leverage an existing game-theoretic MARL environment [19, 22], based on the Stag Hunt MGSD, chosen for its overlap with existing environments and accessibility of manipulating game dynamics, which was briefly explored by Peysakhovich and Lerer [22]."}, {"title": "3 Background", "content": "3.1 Matrix Game Social Dilemmas\nA matrix game is a formal representation of strategic interactions between two players, where each player's decision affects the other's outcome. The game is represented by a matrix in which each cell details the outcomes (or payoffs) for the players based on their combined choices. Players choose a strategy without knowledge of the other's simultaneous decision, and the combination of their choices leads to a specific payoff for each. In matrix game social dilemmas (MGSDs), the two actions, or strategies, available to each player are 'cooperate' and 'defect', and the exact payoffs for each player are given by the values {R, P, S, T} \u2208 R, detailed in Table 1. Formally, a matrix game is a social dilemma when its four payoffs satisfy the following inequalities:\n$$R > P, R > S, 2R > T + S, \\text{ and either: } T > R, \\text{ or, } P > S.$$\nCentral to our work, is the MGSD, Stag Hunt [31]; where the 'cooperate' and 'defect' actions correspond to 'hunt' and 'forage', respectively. Conceptually, this represents a scenario where players are tasked with either hunting a stag or foraging a plant. A bilateral decision to hunt offers the highest payoff, h, to both players, but carries the risk of being mauled by the stag, yielding the lowest possible reward, m, to a player hunting unilaterally. In mauling scenarios, the opposing player may receive a unique foraging reward f* \u2265 f. Formally, the Stag Hunt is characterised by Table 2 and corresponds to an MGSD where R = h, P = f, S = m and T = f*."}, {"title": "3.2 Nash Equilibria", "content": "Nash Equilibria, a fundamental concept in game theory introduced by Nash [18], refers to a situation in which each player in a game makes an optimal choice considering the choices of the other players, and no player has anything to gain by changing only their own strategy unilaterally. Formally, a Nash Equilibrium in a game with n players is defined as a strategy profile ($s_1, s_2, ..., s_n$) such that for each player i, the strategy $s_i$ is a best response to the strategies $s_{-i}^*$ chosen by the other players. This can be expressed as:\n$$s_i^* \\in \\arg\\max_{s_i} u_i(s_i, s_{-i}^*), \\forall i \\in \\{1, 2,..., n\\}$$\nwhere $s_i$ represents the strategy chosen by player i, $s_{-i}^*$ represents the strategies chosen by all other players except i, and $u_i(s_i, s_{-i})$ is the payoff function for player i. In the Stag Hunt, players are faced with the decision between hunting a stag or foraging for plants, i.e., $s_i \\in \\{Hunt, Forage\\}$, and the game is particularly valuable for containing two Nash Equilibria which reflect different resolutions of its inherent social dilemma:"}, {"title": "3.3 Markov Games", "content": "We model the MARL adaptation of Stag Hunt as a two-player Markov Game, also known as a Stochastic Game [25]; a natural extension of Markov Decision Processes to multi-agent settings [17]. A Markov game for two players is a tuple (S, A, R, P, \u03b3), where S is the set of states describing the environment, A is the combined action space across all players, given by A := A1 \u00d7 A2, where the A is is the set of actions available to player i, R is the reward function, given by R := R1 \u00d7 R2, where $R_i : S \u00d7 A \u00d7 S \u2192 R$ is the reward function for player i, which maps a state and action tuple (including the resulting state) to a real number and defines the reward that player i receives after all players choose their actions. $P : S \u00d7 A \u00d7 S \u2192 \u25b3(S)$ is the state transition probability function, and \u03b3 is the discount factor, which is typically constrained to 0 \u2264 \u03b3 \u2264 1.\nIn a two-player Markov Game, each player chooses a policy that determines their action based on the current state of the game. The joint policy vector \ud835\udf0b combines the individual policies of each player, where \ud835\udf0b = (\u03c01, \u03c02). For player i, the value function under this joint policy, $V_\u03c0$, is defined as the expected sum of discounted rewards when both players adhere to their respective parts of \ud835\udf0b. The formal definition is:\n$$V_i^\\pi(s) = E_{a_t \\sim \\pi, s_{t+1} \\sim P(s_t, a_t)}[\\sum_{t=0}^{\\infty} \\gamma^t R_i(s_t, a_t, s_{t+1}) | s_0=s]$$\nwhere st is the state at time t, at is the joint actions taken by Player 1 and Player 2 at time t, determined by their policies \u03c0\u2081 and \u03c02, respectively, st+1 is the state resulting from the actions at, $R_i(s_t, a_t, s_{t+1})$ is the reward received by player i after the actions are taken and the state transitions, and \u03b3 is the discount factor."}, {"title": "3.4 Sequential Social Dilemmas", "content": "We also use the Sequential Social Dilemma (SSD) model, introduced by Leibo et al. [12], which characterises when a Markov Game contains embedded MGSDs, thus extending the MGSD model to temporally and spatially extended settings. Formally, an SSD is a tuple \u3008M, \u03a0C, \u03a0D\u3009, where M is a Markov Game with state space S, and, \u03a0C and \u03a0D, are disjoint sets of policies that implement cooperative and defective strategies, respectively. These policy sets are defined by selecting a metric, \u03b1 : \u03a0 \u2192 R, and threshold values \u03b1c and \u03b1d such that \u03b1(\u03c0) < \u03b1c \u21d4 \u03c0\u2208 \u03a0c and \u03b1(\u03c0) > \u03b1d \u21d4 \u03c0\u2208 \u03a0\u03c1. For a given Markov Game, M, and state, s \u2208 M, we can construct an empirical payoff matrix, (R(s), P(s), S(s), T(s)), induced by the outcomes of policies, (\u03c0C \u2208 \u03a0C, \u03c0D \u2208 \u03a0D), via the following equations: $R(s) := V_{\\pi^C, \\pi^C}(s)$, $P(s) := V_{\\pi^D, \\pi^D}(s)$, $S(s) := V_{\\pi^C, \\pi^D}(s)$, and $T(s) := V_{\\pi^D, \\pi^C}(s)$. Then, M is an SSD when there exist states s \u2208 S and policies (\u03c0C \u2208 \u03a0C, \u03c0D \u2208 \u03a0D) such that the induced empirical payoff matrix satisfies the five MGSD inequalities in Equation 1. Hence, a Markov Game is an SSD when it can be mapped by empirical game-theoretic analysis [33] to an MGSD."}, {"title": "3.5 Proximal Policy Optimisation", "content": "Proximal Policy Optimisation (PPO) is a widely used policy gradient method for single-agent reinforcement learning, introduced by Schulman et al. [24]. The core idea behind PPO is the use of a clipped surrogate objective function to prevent the policy from deviating too far from the current policy during updates. Through its extension to the multi-agent paradigm, PPO has proven effective in numerous MARL environments, including those that target cooperation [35]."}, {"title": "4 Methods", "content": "4.1 Gridworld Stag Hunt\nWe adapt a gridworld implementation of the Stag Hunt matrix game, initially developed by Nesterov-Rappoport [19] and Peysakhovich and Lerer [22]. The selected environment, built on the PettingZoo [30] and Gym [3] frameworks, simulates a 5x5 grid game featuring two agents-red and blue, visualised in Appendix A. Each episode begins with placing both agents, along with non-agent entities: one stag and two plants, on distinct cells of the grid. The agents and the stag can move in any of the four cardinal directions or remain stationary at every timestep. The state of the environment is fully observable for each agent, with integer arrays representing the coordinates of all agents and entities on the grid. Similarly, the action space is represented by five unique integers, corresponding to the four cardinal directions and the option to remain stationary. Plants remain stationary within an episode and are randomly re-spawned at every episode start. Individual agent rewards are dependent on the actions of both agents and are determined at the end of every timestep as follows:\n\u2022 Agents receive an individual hunting reward, h, by jointly occupying the stag's grid cell.\n\u2022 An agent receives a foraging reward, f, by independently, or jointly, occupying a plant's grid cell.\n\u2022 An agent receives a mauling reward, m, by independently occupying the stag's grid cell.\nThe rewards values assigned satisfy the generalised two-player stag hunt inequality in Table 2, with values set at h = 25, f = f* = 2 and m = -1. This configuration significantly favours hunting strategies over foraging strategies, whilst inducing an element of risk to hunting with a negative mauling reward. Furthermore, to align the gridworld stag hunt with a one-shot matrix game, we implemented an infinite-horizon setup. In this format, episodes conclude only when both agents have received a reward: through hunting, foraging, or being mauled. In asynchronous episode terminations, an agent that has received a reward is frozen on the grid, while the other agent continues to act.\n4.1.1 Variable Complexity\nIn Table 3 we introduce randomised and deterministic configurations for three environment parameters in the gridworld Stag Hunt: the spawn locations of agents, the spawn locations of the stag, and movement of the stag, where agent and stag spawn parameters apply to the start of every episode, and the stag movement applies to every timestep. Enumerating all combinations of values for these parameters provides eight unique environment variants, each with a specified degree of randomisation. These environment variants are outlined in Table 4 and we use the corresponding environment label to denote the degree of randomisation in a given environment, i.e., the label FFR denotes an environment with: deterministic agent spawn (F), deterministic stag spawn (F) and randomised stag movement (R). We characterise the spectrum of randomisation across these eight variants as environment complexity in our study, where environments with randomisation on multiple parameters, such as environment RRR, are considered more complex than those with deterministic dynamics, such as environment FFF."}, {"title": "4.2 Experimental Setup", "content": "Our experiments use RLLib's [16] Proximal Policy Optimisation (PPO) [24] implementation to train pairs of agents in the gridworld Stag Hunt environment. We consider the simplest multi-agent variant of PPO, Independent PPO (IPPO), in which each agent uses an independent instance of PPO to optimise its own policy, through maximising individual rewards [29]. In particular, no parameters are shared between the multiple policies and each agent treats other agents as part of their environment"}, {"title": "5 Results", "content": "5.1 Environment Complexity and Strategy Convergence\nWe conduct five training runs of IPPO and PPO in each of the eight environments listed in Table 4, training for 1,000 iterations, each consisting of 4,000 environment timesteps. In each trial we use a distinct random seed for determining environment randomisation dynamics where applicable. We defer the results of our baseline, PPO, and the hyperparameters used, to Appendices B and C.1.\nIn Figure 1, which shows the training performance of IPPO agents, measured by mean combined episode reward per iteration, averaged across 5 trials; we group environments based on observed reward patterns. Environments yielding high rewards (i.e., FFF, RFF, FRF, RRF, FRR) are denoted as Group A, while those with lower rewards (FFR, RFR, RRR) constitute Group B. These groupings reveal two insights into the learnt strategies of IPPO agents:\nObservation 1: Agents trained in Group B converge to globally suboptimal strategies, consisting of bilateral foraging. By analysing the mean episode rewards at the end of 1,000 training iterations, we infer agent strategies based on the previously established payoff structure. Specifically, exclusive bilateral hunting yields a combined reward of 50, significantly higher than the 4 from bilateral foraging. Consequently, stable convergence to a mean episode reward over 4 indicates that agents have adopted strategies that include bilateral hunting. Thus, in Group B, where the mean combined rewards rarely"}, {"title": "5.2 Escaping Suboptimal Strategies", "content": "We consider only Group B environments from the previous subsection, in which we observed IPPO agents converging to suboptimal strategies (Observation 1). To assess the feasibility of higher-reward strategies in these environments, we bias agents toward bilateral hunting strategies through a simple curriculum. We introduce a new environment variant, cXXX, designed to enforce a purely cooperative Nash equilibrium by eliminating the reward for foraging, thus exclusively incentivising bilateral hunting. This corresponds to a Stag Hunt game, as formalised in Table 2, where h = 25, f = f* = 0 and m-1.\nFor each environment in Group B, a corresponding cXXX variant is created: cFFR, cRFR and cRRR. These variants maintain their respective randomisation dynamics described in Table 4. Initially, agents are trained in their respective cXXX environments for 500 iterations to learn hunting strategies (Stage 1). To discourage passive strategies and further promote hunting, a timestep penalty of -0.5 is introduced in these environments. Following this step, the pre-trained policies are then trained in their original environments (FFR, RFR, RRR) for another 500 iterations, during which the timestep penalty is removed (Stage 2). We use only IPPO for these experiments.\nObservation 3: Curriculum learning allows agents to escape suboptimal strategies in Group B. Our results in Figure 2 demonstrate that our curriculum enables agents to find strategies in Group B environments that yield significantly higher rewards than those observed previously. In particular, during the second training stage, performance begins to converge toward average rewards that substantially exceed those attainable through exclusive foraging. Consistent learning of higher-reward strategies demonstrates that agents have not only discovered, but also converged to strategies that include bilateral hunting-effectively escaping the suboptimal strategies that were previously observed."}, {"title": "5.3 Empirical Game-Theoretic Analysis", "content": "Here we further characterise the Group B environments with an empirical game-theoretic analysis following the methodology of Leibo et al. [12]. Recall from Section 5.1 that agents trained in these environments converged to suboptimal foraging strategies (Observation 1). In the context of the Stag Hunt, foraging corresponds to the defect action within a matrix game social dilemma (MGSD). Consequently, we categorise the strategies that these policies implement as defective. Conversely, in Section 5.2, we present a learning curriculum that shifts agents from these suboptimal strategies towards cooperative hunting-focused strategies. Thus we can train two distinct types of policy for each environment in Group B: cooperative policies, \u03a0C, and defective policies, \u03a0D. This allows us to construct a meta-game with cooperate and defect actions that involve sampling a policy from either category and determining payoffs based on the resulting pairwise average rewards. This meta-game represents an MGSD, induced by the policy sets, \u03a0 and \u03a0D, embedded within the underlying Markov Game, if such a mapping exists. We compute the empirical payoffs of this meta-game as follows:\nFor each environment, two pairs of policies, ($\\pi^C, \\pi^D$) and ($\\pi^D, \\pi^D$), are sampled, where $\\pi^C \\in \\Pi$ and $\\pi^D \\in \\Pi_D$. These policy pairs represent the actions of a row player and a column player in a two-player matrix game with cooperate or defect actions (i.e., an MGSD), respectively. Specifically, $\\pi^C$ corresponds to the cooperate action for player i, and $\\pi^D$ corresponds to the defect action for player j. The outcomes (payoffs) for each combination of actions are determined by playing the respective policies against one another for an episode, in the respective Stag Hunt environment. The rewards obtained in each episode are then averaged in the corresponding cell in the payoff matrix. This procedure is repeated for each environment until convergence of all cell values, yielding the set of induced empirical payoff matrices shown in Table 5."}, {"title": "6 Discussion", "content": "Our results indicate that agents trained in Group B environments converge to suboptimal strategies (Observation 1) and that these policies empirically represent Nash equilibria strategies (Observation 5). However, we also discover that agents are in-fact capable of learning more-optimal strategies within these environments (Observation 3). Despite the viability of higher-reward strategies, we observe that agents systematically converge to the suboptimal Nash equilibrium strategy (i.e., mutual foraging). We suggest that this behaviour is a result of the increased complexity introduced by our modifications to the gridworld Stag Hunt. To support this, we explore three alternative explanations for our empirical findings:\nFirst, although the observation space for each agent includes the positions of all entities within the grid, converging on higher-reward strategies in more complex environments, such as those in Group B, may require previous observations, actions, or rewards to be included in the agent's observation. However, this information is not available in a one-shot version of the game and thus cannot be considered a viable solution. Second, it could be that performing higher-reward strategies in Group B environments is significantly improbable due to highly randomised environment dynamics, and hence agents are behaving as expected, converging to the most optimal strategy (i.e., maximising long-term reward). We mitigate this proposition (and also the first explanation) by demonstrating empirically that through curriculum learning the same agents are able to learn significantly more cooperative strategies (Observation 3). Third, the dynamics of Group B environments might simply require more suitable hyperparameter values or additional training time, e.g., to increase exploration behaviour or allow more time to discover better strategies. Thus, in Appendix C.2, we run further training experiments in Group B, including multiple hyperparameter searches, for 12,000 iterations. The results show that no further improvements are made under any circumstances, indicating that the observed behaviour is likely not an artifact of hyperparameter selection or insufficient training time, but a systematic defection toward lower-reward strategies due to increased environment complexity.\nThus, our findings suggest that the observed suboptimal convergence behaviour is a direct result of the increased complexity found in the Group B environments. Examining the environment dynamics within Group B indicates that the complexity introduced by a randomly moving stag\u2014environments labelled by xxR\u2014induces significant uncertainty which constrains learned strategies to suboptimal equilibria. Indeed, in three of the four environments featuring a randomly moving stag, our agents"}, {"title": "C Environment Complexity and Strategy Convergence", "content": "In this section, we provide results of the supplementary experiments we conducted to substantiate our claims in Section 5.1.\nC.1 Centralised PPO\nTo establish a baseline for our earlier results in Section 5.1, which used the IPPO training methodology, we trained a single joint-action policy that controls both agents, labelled PPO. The distinction between IPPO and PPO is that while IPPO agents treat other learners as part of their own environment, PPO controls both agents under a single policy. This distinction allows us to assess the extent to which independent multi-agent interaction is necessary to achieve globally optimal outcomes in the gridworld Stag Hunt.\nOur results, presented in Figure 4, further suggest that environment complexity promotes convergence to suboptimal strategies. We find that the same grouping of environments from IPPO (Figure 1) applies to PPO agents. Although the performance of PPO in Group B resembles that of IPPO in Group B, we observe that performance in Group A exhibits significantly larger variance between trials compared to IPPO. Specifically, multiple trials in the same environment, including the fully deterministic environment, FFF, show convergence to vastly different strategies. This observation highlights the necessity of independent multi-agent interaction in navigating the social dilemma aspects of our environment, where PPO may be overly influenced by early biases in training runs. Despite this, in Figure 4 we continue to observe that an increase in environment complexity correlates with decreased pay-offs.\nC.2 Extended Training in Group B Environments\nIn Section 5.1, we found that IPPO agents converged to suboptimal strategies in Group B, whilst converging to more optimal strategies in Group A. To investigate whether the observed suboptimal convergence behaviour could be explained through either: inappropriate hyperparameters for the given environment variants, or insufficient training time to discover more optimal strategies, we conducted an extended training experiment with hyperparameter tuning.\nInitially, we trained IPPO agents in each of the group B environments (FFR, RFR, RRR) for five trials, each consisting of 1,000 iterations. Each of these trials used HyperOpt's Tree Parzen Estimator (TSE) search algorithm [2] and the Asynchronous Hyperband (ASHA) scheduler [15] to estimate the optimal hyperparameter values for each trial, where the hyperparameter ranges used are listed in the second column of Table 6. Subsequently, we selected the best performing trial in each environment,"}]}