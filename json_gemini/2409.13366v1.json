{"title": "RingMo-Aerial: An Aerial Remote Sensing Foundation Model With A Affine Transformation Contrastive Learning", "authors": ["Wenhui Diao", "Haichen Yu", "Kaiyue Kang", "Tong Ling", "Di Liu", "Yingchao Feng", "Hanbo Bi", "Libo Ren", "Xuexue Li", "Yongqiang Mao", "Xian Sun"], "abstract": "Aerial Remote Sensing (ARS) vision tasks pose significant challenges due to the unique characteristics of their viewing angles. Existing research has primarily focused on algorithms for specific tasks, which have limited applicability in a broad range of ARS vision applications. This paper proposes the RingMo-Aerial model, aiming to fill the gap in foundation model research in the field of ARS vision. By introducing the Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) mechanism and an affine transformation-based contrastive learning pre-training method, the model's detection capability for small targets is enhanced and optimized for the tilted viewing angles characteristic of ARS. Furthermore, the ARS-Adapter, an efficient parameter fine-tuning method, is proposed to improve the model's adaptability and effectiveness in various ARS vision tasks. Experimental results demonstrate that RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This indicates the practicality and effectiveness of RingMo-Aerial in enhancing the performance of ARS vision tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "ERIAL Remote Sensing (ARS) is a significant method of information acquisition that, unlike satellite remote sensing, is typically not constrained by revisit cycles, satellite sensor incidence angles, or other observation timing and viewing angle limitations. Aerial remote sensing sensors are commonly mounted on Unmanned Aerial Vehicles (UAVs), airplanes, or balloons. Compared to remote sensing satellites, these aerial platforms can swiftly reach the observation area and conduct comprehensive observations from multiple perspectives and at various resolutions, thereby capturing real-time and detailed information about the target area. Consequently, aerial remote sensing finds extensive applications in emergency response, environmental monitoring, and national defense security. Therefore, research on enhancing aerial remote-sensing images' intelligent information extraction capabilities holds significant academic and practical value.\nHowever, despite the advantages and value of aerial remote sensing imagery, our research reveals that the current focus in intelligent remote sensing interpretation remains predominantly on space-borne remote sensing. As shown in Figure 1, although research on aerial remote sensing has seen rapid growth in recent years, studies related to its intelligent interpretation remain relatively scarce. The number of published papers in this area accounts for less than 10% of space-borne remote sensing. Compared to space-borne remote sensing, aerial remote sensing possesses distinct physical characteristics, such as multi-view, multi-resolution, and occlusion effects, owing to its unique observational perspectives and sensor properties, as illustrated in Figure 2. In Figure 2(a), various perspectives are depicted, including oblique views (i and ii), a vertical view (iii), and a horizontal view (iv). Unlike the predominantly vertical perspective of satellite remote sensing, aerial remote sensing imagery captures surface information from oblique angles, enhancing the three-dimensional perception of the targets. However, this also results in distant objects appearing smaller and more densely arranged, increasing the complexity of image interpretation. Figure 2(b) compares images of different resolutions: low-resolution (i, iv), medium-resolution (ii, v), and high-resolution (iii, vi). These multi-resolution characteristics result from the varying data resolutions of aerial sensors operating at different flight altitudes and speeds. The variation in resolution affects the clarity and detail representation of the same target across different images, thereby placing higher demands on the adaptability of image interpretation models. Furthermore, aerial imagery is often characterized by significant occlusion, as shown in Figure 2(c). In this Figure, some targets are partially or fully occluded by other objects (i and iii illustrate occluded scenarios, while ii and iv show non-occluded scenarios). Such occlusions further complicate the interpretation of ARS images.\nGiven the challenges mentioned above, it is difficult to design specialized intelligent models for aerial remote sensing that are tailored to different applications. Existing foundational models in computer vision and remote sensing vision are no longer sufficient to meet the demands of aerial remote sensing applications. The recently developed foundational models, which have gained significant attention in remote sensing interpretation, are considered an effective solution to these challenges. These models benefit from pre-training on vast amounts of data and large-scale parameters, enabling them to adapt more rapidly and effectively to various changes.\nHowever, existing foundational model studies in the remote sensing domain primarily focus on the interpretation of space-borne remote sensing imagery, as evidenced by studies like RingMo[1] and Skysense[2]. This underscores the pressing need to develop a pre-trained foundational model specifically tailored for aerial remote sensing to address its unique application requirements. When adapting current foundational models to aerial remote sensing, several critical challenges persist:\n(1) Issues in the Pre-training Phase: The pre-training phase typically relies on large-scale datasets for self-supervised learning, primarily adopting two paradigms: Masked Image Modeling (MIM) and Contrastive Learning (CL). MIM, as exemplified by models like MAE [3] and Sim-MIM [4], involves applying masks to images and training the model to reconstruct the masked parts, thereby enhancing the model's feature extraction capabilities. On the other hand, CL, as seen in models like MOCO [5], improves feature extraction by creating positive and negative sample pairs and training the model to learn the relationships between these pairs. Both paradigms have advantages and limitations, with MIM being more widely utilized in most visual scenarios. For instance, Swin Transformer and RingMo both utilize MIM-based pre-\ntraining methods. However, for aerial remote sensing imagery, the characteristics of images at different perspectives and resolutions differ significantly at the feature level. As shown in Figure 3, vertical images tend to exhibit more structured and continuous features, often characterized by large areas of color and texture consistency, with fewer variations in detail. Such images typically contain abundant low-frequency information due to their broad display range and relatively uniform visual presentation. In contrast, oblique images capture the sides of buildings and their height information, offering a more dynamic and three-dimensional perspective. These images are rich in detail variation and contain relatively less low-frequency information. The current self-supervised learning methods rely on reconstructing local information in images and struggle to capture these variations effectively.\n(2) Issues in the Fine-tuning Phase: The traditional approach of fine-tuning the entire model presents significant challenges, primarily due to the time-consuming nature of the process. Additionally, since the foundational model is pre-trained on a diverse set of downstream tasks, fine-tuning the model globally for a specific downstream task can lead to overfitting, especially in aerial remote sensing. Unlike the single perspective and resolution typically encountered in spaceborne remote sensing, aerial remote sensing models are more prone to overfitting to specific perspectives and resolutions of a particular downstream task. This overfitting can reduce the ability of the model to generalize to different perspectives and resolutions in various downstream tasks.\nTo address these challenges, this study introduces RingMo-Aerial, the world's first foundational large model designed for aerial remote sensing tasks, based on the existing spaceborne remote sensing foundational model, RingMo. In response to the unique characteristics and difficulties of aerial remote sensing tasks, we propose several novel theories and methods, as shown in Figure4.\nFirst, we introduce Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) to address the multi-scale and occlusion issues caused by the oblique perspective in aerial remote sensing images. Unlike traditional multi-head self-attention mechanisms, FE-MSA incorporates an additional image patch expansion layer, ensuring that dense small objects are captured by more tokens when patches are divided into tokens for the Transformer model. Furthermore, FE-MSA leverages the Swin Transformer's window attention and shifted window mechanisms. These improvements significantly enhance the model's ability to detect dense small objects and occluded targets.\nSecond, to solve the problem of detecting and recognizing small distant targets in oblique perspectives, we propose a Contrastive Learning Affine Framework (CLAF) that scales and enlarges distant targets through affine transformations to construct positive samples. By inputting the transformed and original images as sample pairs into the model, this method guides the model to learn the unique visual characteristics of ARS images, improving its ability to recognize small targets at a distance.\nIn addition, during the fine-tuning phase, we propose an efficient fine-tuning method for basic aerial remote sensing foundational models, called ARS-Adapter. Unlike traditional comprehensive fine-tuning methods, ARS-Adapter achieves efficient model fine-tuning by adjusting fewer than 5% of the parameters, significantly improving the training efficiency of downstream tasks while reducing the risk of overfitting. ARS-Adapter applies to all layers of the model: in the attention layers, it employs a traditional Adapter [6] structure that operates parallel to the attention layers; in the Multi-Layer Perceptron (MLP) layers, it utilizes a bottleneck structure that includes MLP downsampling, deep convolutional feature extraction, and MLP upsampling. This approach optimizes the performance of the aerial remote sensing foundational model, making it more adaptable and effective across various aerial remote sensing vision tasks.\nIn conclusion, the core contributions of this paper are as follows:"}, {"title": "II. RELATED WORKS", "content": "Aerial remote sensing offers unique characteristics and has found widespread applications across various domains. Common tasks in aerial remote sensing include object detection, object tracking, semantic segmentation, and 3D reconstruction. The most frequently utilized modalities are optical imaging [7], infrared imaging [8], and synthetic aperture radar (SAR) [9].\nCompared to satellite remote sensing, aerial remote sensing provides higher image resolution and more flexible spatiotem- poral coverage. However, it also presents distinct charac- teristics that set it apart from satellite-based methods. One notable difference is that aerial imagery is typically captured at oblique angles with varying shooting distances. This results in significant variations in the size of similar targets and increases the likelihood of target occlusion. [10\u201313] have specifically modeled occluded targets at the structural level, significantly improving the detection and segmentation performance of occluded instances. [14-17] have focused on optimizing the oblique perspective through multi-scale transformations, en- hancing the resolution of small targets. Another key distinction lies in the frequency content of aerial imagery, which contains both high- and low-frequency signals, unlike satellite imagery that predominantly captures low-frequency information. Effec- tively leveraging high-frequency signals can greatly aid in the interpretation of aerial images. [18, 19] introduced frequency- domain enhancement modules to balance high- and low- frequency signals, leading to improved performance across various aerial remote sensing tasks.\nVisual foundation models have gained attention for their ability to serve as general-purpose solutions in various com- puter vision tasks. Convolutional neural networks (CNNs) were among the earliest architectures used for visual foun- dation models, with well-known examples such as VGG [20] and ResNet [21]. These models rely on convolutional layers for feature extraction and are typically pre-trained on classification tasks using the ImageNet [22] dataset. With the advent of Transformers in natural language processing, visual Trans- former models like ViT [23] and Swin Transformer [24] have gained prominence, utilizing masked image modeling (MIM) or contrastive learning (CL) for self-supervised pre-training. In the field of remote sensing (RS), models such as RingMo [1] and SkySense [2] have adopted the visual Transformer architecture and self-supervised pre-training strategies. Addi- tionally, a number of novel architectures, such as ConvNeXt [25] and Mamba[26], have been proposed. However, due to technical immaturity, these newer models have yet to achieve widespread adoption.\nThe key issue with these models lies in the pre-training phase, where they learn broad visual representations from large datasets. This chapter will also summarize three types of self-supervised visual training methods: CL, MIM, and their combination, with each method addressing both natural and RS."}, {"title": "III. PROBLEM DESCRIPTION", "content": "The unique imaging characteristics of ARS photography stem from the inherent properties of aerial perspective. Specifically, this perspective manifests as a size disparity within the same image due to the inclined angle of capture. Objects that are closer to the ARS (towards the bottom of the image) appear larger, while those further away (towards the top) appear smaller, as illustrated in Figure 5.\nTo elaborate, as depicted in Figure 6, when a ARS captures two objects of equal actual size at inclined angles of $\\theta_1$ and $\\theta_2$, the resulting pixel dimensions and their ratio can be mathematically expressed. For illustration, consider two identical cars on a highway, both with a height of L. The vertical component of the cars in the camera's view can be expressed as $L \\times sin(\\theta_1)$ and $L \\times sin(\\theta_2)$. Given the drone's flight height is H, the focal length of the drone's camera is f, and the horizontal distances between the two cars and the drone are $d_1$ and $d_2$ respectively, the slant distances between the cars and the drone are calculated as $D_1 = \\sqrt{H^2 + d_1^2}$ and $D_1 = \\sqrt{H^2 + d_2^2}$. According to the principle of perspective projection, the height of the objects in pixels in the image is calculated as follows:\n$h_1 = \\frac{f \\cdot L \\cdot sin(\\theta_1)}{D_1}$        (1)\n$h_2 = \\frac{f \\cdot L \\cdot sin(\\theta_2)}{D_2}$       (2)\nThe ratio of the heights of the pixels occupied by the cars is:\n$\\frac{h_1}{h_2} = \\frac{D_2 \\cdot sin(\\theta_1)}{D_1 \\cdot sin(\\theta_2)} = \\frac{\\sqrt{H^2 + d_2^2} \\cdot sin(\\theta_1)}{\\sqrt{H^2 + d_2^2} \\cdot sin(\\theta_2)}$         (3)\nThen the proportion of pixels occupied by car 1 and car 2 can be estimated as:\n$\\frac{P_1}{P_2} = \\frac{h_1^2}{h_2^2}$       (4)\nConsidering that in the visual transformer model, the number of pixels occupied by an object is related to the number of tokens it occupies, as 5. Therefore, it is obviously useful to adopt a method to balance the number of tokens occupied by similar objects of different sizes in an image."}, {"title": "IV. METHODOLOGY", "content": "The proposed RingMo-Aerial model serves as a foundational framework with broad applicability. Its implementation involves three main stages: model design, pretraining, and fine-tuning for downstream tasks. In Section IV-B, we address the challenge of detecting small objects in ARS imagery caused by oblique viewing angles during the model design phase. To enhance small object detection, we introduce the Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) layer by in-corporating a patch expansion layer, shown as Figure 7. In Section IV-C, we combine the commonly used masked image modeling (MIM) pretraining with an additional contrastive learning (CL) method based on affine transformations during the pretraining stage. This hybrid pretraining approach signif- icantly improves both training speed and the performance of the converged model on downstream tasks, shown as Figure 7. Furthermore, in Section IV-D, we introduce a parallel Adapter fine-tuning method during the fine-tuning phase. The proposed ARS-Adapter module includes downsampling, channel-wise convolution, and upsampling components, enhancing its adapt- ability and learning capacity for specific tasks, shown as Figure 10.\nRingMo-Aerial's design is based on Swin Transformer. The architecture of Swin Transformer consists of four stages, each comprising either a linear encoder or Patch Merging, along with an even number of Swin Transformer Blocks. Each block includes Window Multi-Head Self-Attention (W-MSA) and Shifted Window Multi-Head Self-Attention (SW-MSA). These attention mechanisms effectively compute at- tention maps within the windows, extracting spatial feature information between pixels while reducing the computational complexity of the model.\nAlthough the classical Swin Transformer introduces spatial interactions through two types of window attention mech- anisms, emphasizing high-level semantic information within feature vectors, challenges arise in ARS imagery. The varying shooting distances, angles, and scale differences between dis- tant and nearby objects present significant difficulties. Relying solely on semantic information may not be sufficient to achieve optimal performance in downstream tasks.\nTo address the aforementioned challenges, we propose the overall architecture of RingMo-Aerial. As shown in Figure 7(a), the overall architecture of RingMo-Aerial is based on the Swin Transformer. Considering the unique characteristics of the ARS perspective and based on the window attention mechanisms (W-MSA and SW-MSA), a new attention mech- anism called Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) is introduced to enhance the model's capability in handling diverse aerial scenarios.\nTo better understand the FE-MSA mechanism, it is essential to first recognize the limitations of the existing window attention mechanisms. These mechanisms often strug- gle to balance the semantic information in feature vectors of objects of different sizes. For instance, as illustrated in Figure 8, larger objects may occupy 4 \u00d7 4 tokens, while smaller distant objects may occupy only 3 \u00d7 3 tokens. The FE-MSA mechanism introduces frequency domain processing to the existing window attention, indirectly expanding the receptive field for smaller objects.\nAs shown in Figure 7(b), the RingMo-Aerial Block structure is illustrated, including two window attention mecha- nisms optimized by the FE-MSA mechanism. Given $z^{(l-1)} \\in R^{H \\times W \\times C}$, where l is the id of each RingMo-Aerial Block. Taking the first stage as an example, the input feature vector $z^l$ adopts the same slicing parameters as the stan- dard Swin-Transformer, resulting in four-channel slices $C = \\{C_1, C_2, C_3, C_4\\}$. After linear normalization, it enters the FE- MSA module. For each patch with a dimension of C, a 7 \u00d7 7 convolution kernel is used for depthwise convolution to obtain the feature map $F_{z^{(l-1)}} = Concat\\{F_{C_1}, F_{C_2}, F_{C_3}, F_{C_4}\\}$. Then, W-MSA or SW-MSA is used to compute the attention map. After the FE-MSA, the edge and texture features of small objects are further enhanced, improving the model's ability to perceive both local and global features. The computation process for each token patch through the RingMo-Aerial block is as follows:\n$z^l = FE-MSA(z^{l-1})$      (5)\n$FE-MSA(z^{l-1}) = SoftMax(\\frac{q k^T}{\\sqrt{d}} + B)$        (6)\nThe FE-MSA mechanism enables RingMo-Aerial to effec- tively capture edge and texture features of different targets in ARS images, enhancing the model's performance in complex aerial scenes.\nIn the methodological section of the paper, we expand upon the fine-tuning strategies employed for downstream tasks. Beyond the conventional approach of fine-tuning the entire model, this study introduces an adapter-based fine- tuning method tailored for an aviation foundation model. The proposed ARS-Adapter comprises a standard Adapter module coupled with an additional depthwise convolution module, as 10. The latter is specifically designed to enhance the model's performance in tasks such as detection and tracking. In a fashion akin to AdaptFormer [60], the introduced ARS- Adapter module operates in parallel with the model. The Adapter module featuring the depthwise convolution is paral- leled with the MLP layer, while the Adapter module without this convolution module is paralleled with the Attention layer. Therefore, the forward propagation is:\n$Adapter-MSA = Up(GELU(Down(x)))$      (12a)\n$Adapter-FFN = Up(GELU(Conv(GELU(Down(x))))) $     (12b)\nSpecifically, the downsampling layer performs 16 times downsampling in the embedding dimension, and the corre- sponding upsampling layer performs 16 times upsampling. The Adapter in parallel with FFN introduces an additional convo- lution layer, which uses 7\u00d77 depth-separated convolution to reduce fine-tuning parameters while introducing more spatial information.\nDuring training, the parameters of the entire pre-trained model will be frozen, and only the parameters of the above- mentioned ARS-Adapter module will be trained and updated. This approach allows the pre-trained model to achieve the same effect as full fine-tuning by only fine-tuning less than 0.5% of the parameters."}, {"title": "V. EXPERIMENTS", "content": "In the pretraining phase of our model, approximately 5.4 million image patches, as mentioned earlier, are utilized. Prior to pretraining, we initialize the model parameters using Kaiming initialization. Subsequently, we perform 100 epochs of masked image modeling (MIM) pretraining. During MIM"}]}